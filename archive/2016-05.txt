From viktor.klang at gmail.com  Sun May  1 04:55:48 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 1 May 2016 10:55:48 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8 Livelock on
 computeIfAbsent() ?
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E4900102E59C73@HQMBX5.eur.ad.sag>
References: <5715E8DA.8020403@javaspecialists.eu>
	<CAGu0=MN-qrM46acU=dgNJXdMmLWNejMHrYZPELXbT2SJBJe24g@mail.gmail.com>
	<5715FBF8.80307@javaspecialists.eu>
	<5716192D.4090601@cs.oswego.edu>
	<6701E505-7F1F-4364-B3E1-39A8C2EB554B@sics.se>
	<5717759C.1010100@cs.oswego.edu>
	<CANPzfU9b7ikXOKJcuMO-PV8unLzkEN4rx94vYr9obXuPu8n9eA@mail.gmail.com>
	<571B0884.1010706@javaspecialists.eu>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E576AE@HQMBX5.eur.ad.sag>
	<CANPzfU91BD4DxWRajFL-AANOYVk_fSqRfThhe2PV2zoWQ6sxcQ@mail.gmail.com>
	<CADZL2=u9z3kY+YaYKDSnAX0-Pbn+6x1H_-E-Czn4FQ8xV1poyw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E584D1@HQMBX5.eur.ad.sag>
	<CANPzfU8QrjHQUikwhvrC-P0=myu5cXzmsUKpE31gWpFP+jCFgw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E587A2@HQMBX5.eur.ad.sag>
	<CANPzfU9xnbiTnEbsmUGa7=u0zw+EtY6t5LUGYMMzvzkxzTyeSw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58A89@HQMBX5.eur.ad.sag>
	<CANPzfU-aAdVw5aTWAanGER_NgLRqoKyk_E6GCvuw-oXBrnFz=Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58D74@HQMBX5.eur.ad.sag>
	<CANPzfU9FMXDqF6BLzor7awmJE5+MD5UrvCdPC0uiyrjCOV9n8g@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E59C73@HQMBX5.eur.ad.sag>
Message-ID: <CANPzfU_QS1XaNxrip_YmBKLRx3-8qgno=O6OLktSqcD43KNuUQ@mail.gmail.com>

Hi Sebastian,

I suspect the use of thenComposeAsync is required to trampoline on top of
the Executors submission queue rather than exhausting the stack.

I completely missed that you were testing the scalability, I'm sorry. Let
me have a look later today or tomorrow.

Thanks for keeping the topic alive,

V

-- 
Cheers,
?
On Apr 30, 2016 11:19 PM, "Millies, Sebastian" <
Sebastian.Millies at softwareag.com> wrote:

> Hi Viktor,
>
>
>
> I?m sorry, I?ve made a mistake: there hasn?t been a deadlock at all. When
> running the JMH benchmark, I don?t get as much stack space as when running
> stand-alone, so the recursion depth of 6000 was causing a
> StackoverflowError. However, I didn?t see that error, it was hidden by JMH,
> I just saw the benchmark hanging and the JMH worker threads all being
> parked, and jumped to the wrong conclusion.
>
>
>
> Anyway, computing the 3500th Fibonacci number, I consistently do not see
> any advantage of the async version over the synchronous one. In fact, it is
> the other way around:
>
>
>
> 2 Threads
>
> Benchmark                                 Mode  Cnt  Score     Error  Units
>
> FibCachedConcurrentBenchmark.cf3500         ss   20   4.132 ?  1.421  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20   9.134 ?  0.862  ms/op
>
> FibCachedConcurrentBenchmark.cf3500         ss   20   2.887 ?  0.571  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20  10.345 ? 12.954  ms/op
>
> FibCachedConcurrentBenchmark.cf3500         ss   20   3.500 ?  1.291  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20   8.803 ?  1.679  ms/op
>
>
>
> 4 Threads
>
> Benchmark                                 Mode  Cnt  Score   Error  Units
>
> FibCachedConcurrentBenchmark.cf3500         ss   20  2.780 ? 0.430  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.850 ? 1.595  ms/op
>
> FibCachedConcurrentBenchmark.cf3500         ss   20  3.034 ? 0.451  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20  9.744 ? 1.669  ms/op
>
> FibCachedConcurrentBenchmark.cf3500         ss   20  3.965 ? 1.380  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.430 ? 2.396  ms/op
>
>
>
> Perhaps adding to BigIntegers just isn?t expensive enough to warrant the
> overhead of going async.
>
>
>
> n  Sebastian
>
>
>
> PS: Your code below I think doesn?t address the problem, namely as you
> suggested ?to use the same Fib from multiple threads to show how it
> behaves under contention?. Your test(int) method below produces a new CF
> instance for each thread, so there is no contention. Or does it?
>
>
>
>
>
> *From:* Viktor Klang [mailto:viktor.klang at gmail.com
> <viktor.klang at gmail.com>]
> *Sent:* Friday, April 29, 2016 5:59 PM
> *To:* Millies, Sebastian
> *Subject:* Re: [concurrency-interest] ConcurrentHashMapV8 Livelock on
> computeIfAbsent() ?
>
>
>
> Hi Sebastian,
>
>
>
> could it be a thread-pool issue?
>
>
>
> This works just fine for me:
>
>
>
>
>
> package yava.klang;
>
>
>
> import java.math.BigInteger;
>
> import java.util.Map;
>
> import java.util.concurrent.CompletableFuture;
>
> import java.util.concurrent.CompletionStage;
>
> import java.util.concurrent.ConcurrentHashMap;
>
> import java.util.concurrent.ForkJoinPool;
>
> import java.util.concurrent.Executor;
>
> import java.util.function.Function;
>
>
>
> /**
>
>  * Demonstrates ways of caching recursive functions.
>
>  *
>
>  * @author Andrew Haley, Viktor Klang, Sebastian Millies
>
>  * @see triggered by <a href=
>
>  *      "
> http://concurrency.markmail.org/search/?q=#query:%20list%3Aedu.oswego.cs.concurrency-interest+page:3+mid:tf7xddfa6i6ow6d3+state:results
> ">
>
>  *      this discussion</a> on concurrency-interest
>
>  *
>
>  */
>
> public class FibCached {
>
>
>
>     private static class Memoizer<T, R> {
>
>         private final Map<T, R> memo;
>
>
>
>         public Memoizer(Map<T, R> memo) {
>
>             this.memo = memo;
>
>         }
>
>
>
>         public Function<T, R> memoize(Function<T, R> f) {
>
>             return t -> {
>
>                             R r = memo.get(t);
>
>                             if (r == null) {
>
>                                 r = f.apply(t);
>
>                                 memo.put(t, r);
>
>                             }
>
>                             return r;
>
>                         };
>
>         }
>
>     }
>
>
>
>     public static class FibonacciSimple {
>
>         private final Memoizer<Integer, BigInteger> m;
>
>
>
>         public FibonacciSimple(Map<Integer, BigInteger> cache) {
>
>             m = new Memoizer<>(cache);
>
>         }
>
>
>
>         public BigInteger fib(int n) {
>
>             if (n <= 2) return BigInteger.ONE;
>
>             return m.memoize(this::fib).apply(n - 1).add(
>
>                    m.memoize(this::fib).apply(n - 2));
>
>         }
>
>     }
>
>
>
>     public static class CF {
>
>         private final static CompletionStage<BigInteger> csOne =
> CompletableFuture.completedFuture(BigInteger.ONE);
>
>         private final Map<Integer, CompletionStage<BigInteger>> cache;
>
>
>
>         public CF(Map<Integer, CompletionStage<BigInteger>> cache) {
>
>             this.cache = cache;
>
>         }
>
>
>
>         public CompletionStage<BigInteger> fib(int n) {
>
>             if (n <= 2) return csOne;
>
>
>
>             CompletionStage<BigInteger> ret = cache.get(n);
>
>             if (ret == null) {
>
>                 final CompletableFuture<BigInteger> compute = new
> CompletableFuture<>();
>
>                 ret = cache.putIfAbsent(n, compute);
>
>                 if (ret == null) {
>
>                     ret = fib(n - 1).thenCompose(x ->
>
>                           fib(n - 2).thenCompose(y -> {
>
>                                 compute.complete(x.add(y));
>
>                                 return compute;
>
>                     }));
>
>                 }
>
>             }
>
>             return ret;
>
>         }
>
>
>
>         // async version. It's very much possible and recommended to not
> make the first thenCompose an async one,
>
>         // as only the addition of x and y might be "expensive" (for large
> values).
>
>         public CompletionStage<BigInteger> fib(int n, Executor e) {
>
>             if (n <= 2) return csOne;
>
>
>
>             CompletionStage<BigInteger> ret = cache.get(n);
>
>             if (ret == null) {
>
>                 final CompletableFuture<BigInteger> compute = new
> CompletableFuture<>();
>
>                 ret = cache.putIfAbsent(n, compute);
>
>                 if (ret == null) {
>
>                     ret = fib(n - 1, e).thenComposeAsync(x ->
>
>                           fib(n - 2, e).thenComposeAsync(y -> {
>
>                                 compute.complete(x.add(y));
>
>                                 return compute;
>
>                     }, e));
>
>                 }
>
>             }
>
>             return ret;
>
>         }
>
>     }
>
>
>
>     public static CompletionStage<BigInteger> test(final int n) {
>
>         final CF fib = new CF(new ConcurrentHashMap<>());
>
>         return fib.fib(n, ForkJoinPool.commonPool());
>
>     }
>
>
>
> }
>
>
>
> On Thu, Apr 28, 2016 at 10:38 AM, Millies, Sebastian <
> Sebastian.Millies at softwareag.com> wrote:
>
> Dropbox may not have been a good idea L Here?s a thread-dump from
> deadlock in the cf6000Async benchmark:
>
>
>
> 2016-04-28 10:29:36
>
> Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.92-b14 mixed mode):
>
>
>
> "java8.concurrent.FibCachedConcurrentBenchmark.cfAsync6000-jmh-worker-2"
> #14 daemon prio=5 os_prio=0 tid=0x000000001fc95800 nid=0x2418 waiting on
> condition [0x000000001f62e000]
>
>    java.lang.Thread.State: WAITING (parking)
>
>                 at sun.misc.Unsafe.park(Native Method)
>
>                 - parking to wait for  <0x000000076bc429b8> (a
> java.util.concurrent.CompletableFuture$Signaller)
>
>                 at
> java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
>
>                 at
> java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1693)
>
>                 at
> java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
>
>                 at
> java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1729)
>
>                 at
> java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1934)
>
>                 at
> java8.concurrent.FibCachedConcurrentBenchmark.cfAsync6000(FibCachedConcurrentBenchmark.java:76)
>
>                 at
> java8.concurrent.generated.FibCachedConcurrentBenchmark_cfAsync6000_jmhTest.cfAsync6000_ss_jmhStub(FibCachedConcurrentBenchmark_cfAsync6000_jmhTest.java:490)
>
>                 at
> java8.concurrent.generated.FibCachedConcurrentBenchmark_cfAsync6000_jmhTest.cfAsync6000_SingleShotTime(FibCachedConcurrentBenchmark_cfAsync6000_jmhTest.java:433)
>
>                 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native
> Method)
>
>                 at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
>
>                 at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>
>                 at java.lang.reflect.Method.invoke(Method.java:498)
>
>                 at
> org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:430)
>
>                 at
> org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:412)
>
>                 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
>
>                 at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
>
>                 at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
>
>                 at java.lang.Thread.run(Thread.java:745)
>
>
>
>    Locked ownable synchronizers:
>
>                 - <0x000000076b9cda18> (a
> java.util.concurrent.ThreadPoolExecutor$Worker)
>
>
>
> "java8.concurrent.FibCachedConcurrentBenchmark.cfAsync6000-jmh-worker-1"
> #13 daemon prio=5 os_prio=0 tid=0x000000001fc94800 nid=0x2414 waiting on
> condition [0x000000001f3df000]
>
>    java.lang.Thread.State: WAITING (parking)
>
>                 at sun.misc.Unsafe.park(Native Method)
>
>                 - parking to wait for  <0x000000076b9517a0> (a
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
>
>                 at
> java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
>
>                 at
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
>
>                 at
> java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
>
>                 at
> java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
>
>                 at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
>
>                 at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
>
>                 at java.lang.Thread.run(Thread.java:745)
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "Service Thread" #10 daemon prio=9 os_prio=0 tid=0x000000001d97d000
> nid=0x2404 runnable [0x0000000000000000]
>
>    java.lang.Thread.State: RUNNABLE
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "C1 CompilerThread3" #9 daemon prio=9 os_prio=2 tid=0x000000001c71f000
> nid=0x1cb0 waiting on condition [0x0000000000000000]
>
>    java.lang.Thread.State: RUNNABLE
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "C2 CompilerThread2" #8 daemon prio=9 os_prio=2 tid=0x000000001c71e000
> nid=0x19cc waiting on condition [0x0000000000000000]
>
>    java.lang.Thread.State: RUNNABLE
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "C2 CompilerThread1" #7 daemon prio=9 os_prio=2 tid=0x000000001c71b000
> nid=0x1d78 waiting on condition [0x0000000000000000]
>
>    java.lang.Thread.State: RUNNABLE
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "C2 CompilerThread0" #6 daemon prio=9 os_prio=2 tid=0x000000001d8db000
> nid=0xfec waiting on condition [0x0000000000000000]
>
>    java.lang.Thread.State: RUNNABLE
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "Attach Listener" #5 daemon prio=5 os_prio=2 tid=0x000000001d8d9800
> nid=0x200c waiting on condition [0x0000000000000000]
>
>    java.lang.Thread.State: RUNNABLE
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "Signal Dispatcher" #4 daemon prio=9 os_prio=2 tid=0x000000001d8d8000
> nid=0x620 runnable [0x0000000000000000]
>
>    java.lang.Thread.State: RUNNABLE
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "Finalizer" #3 daemon prio=8 os_prio=1 tid=0x000000001c711000 nid=0x10a4
> in Object.wait() [0x000000001ec7f000]
>
>    java.lang.Thread.State: WAITING (on object monitor)
>
>                 at java.lang.Object.wait(Native Method)
>
>                 - waiting on <0x000000076b208ee0> (a
> java.lang.ref.ReferenceQueue$Lock)
>
>                 at
> java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
>
>                 - locked <0x000000076b208ee0> (a
> java.lang.ref.ReferenceQueue$Lock)
>
>                 at
> java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
>
>                 at
> java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "Reference Handler" #2 daemon prio=10 os_prio=2 tid=0x000000001c70a000
> nid=0x1b48 in Object.wait() [0x000000001e91f000]
>
>    java.lang.Thread.State: WAITING (on object monitor)
>
>                 at java.lang.Object.wait(Native Method)
>
>                 - waiting on <0x000000076b206b50> (a
> java.lang.ref.Reference$Lock)
>
>                 at java.lang.Object.wait(Object.java:502)
>
>                 at
> java.lang.ref.Reference.tryHandlePending(Reference.java:191)
>
>                 - locked <0x000000076b206b50> (a
> java.lang.ref.Reference$Lock)
>
>                 at
> java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "main" #1 prio=5 os_prio=0 tid=0x000000000020f800 nid=0x1e14 waiting on
> condition [0x0000000002b1e000]
>
>    java.lang.Thread.State: TIMED_WAITING (parking)
>
>                 at sun.misc.Unsafe.park(Native Method)
>
>                 - parking to wait for  <0x000000076b9cd9f8> (a
> java.util.concurrent.FutureTask)
>
>                 at
> java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
>
>                 at
> java.util.concurrent.FutureTask.awaitDone(FutureTask.java:426)
>
>                 at java.util.concurrent.FutureTask.get(FutureTask.java:204)
>
>                 at
> org.openjdk.jmh.runner.BenchmarkHandler.runIteration(BenchmarkHandler.java:376)
>
>                 at
> org.openjdk.jmh.runner.BaseRunner.runBenchmark(BaseRunner.java:263)
>
>                 at
> org.openjdk.jmh.runner.BaseRunner.runBenchmark(BaseRunner.java:235)
>
>                 at
> org.openjdk.jmh.runner.BaseRunner.doSingle(BaseRunner.java:142)
>
>                 at
> org.openjdk.jmh.runner.BaseRunner.runBenchmarksForked(BaseRunner.java:76)
>
>                 at
> org.openjdk.jmh.runner.ForkedRunner.run(ForkedRunner.java:72)
>
>                 at
> org.openjdk.jmh.runner.ForkedMain.main(ForkedMain.java:84)
>
>
>
>    Locked ownable synchronizers:
>
>                 - None
>
>
>
> "VM Thread" os_prio=2 tid=0x000000001c701000 nid=0x2038 runnable
>
>
>
> "GC task thread#0 (ParallelGC)" os_prio=0 tid=0x000000000266e800
> nid=0x1ba0 runnable
>
>
>
> "GC task thread#1 (ParallelGC)" os_prio=0 tid=0x0000000002670000 nid=0x4a0
> runnable
>
>
>
> "GC task thread#2 (ParallelGC)" os_prio=0 tid=0x0000000002671800 nid=0xfb4
> runnable
>
>
>
> "GC task thread#3 (ParallelGC)" os_prio=0 tid=0x0000000002673000 nid=0x1b8
> runnable
>
>
>
> "GC task thread#4 (ParallelGC)" os_prio=0 tid=0x0000000002676800
> nid=0x1168 runnable
>
>
>
> "GC task thread#5 (ParallelGC)" os_prio=0 tid=0x0000000002677800 nid=0xa84
> runnable
>
>
>
> "GC task thread#6 (ParallelGC)" os_prio=0 tid=0x000000000267b000
> nid=0x17dc runnable
>
>
>
> "GC task thread#7 (ParallelGC)" os_prio=0 tid=0x000000000267c000
> nid=0x2090 runnable
>
>
>
> "VM Periodic Task Thread" os_prio=2 tid=0x000000001d981800 nid=0x2408
> waiting on condition
>
>
>
> JNI global references: 334
>
>
>
>
>
> *From:* Viktor Klang [mailto:viktor.klang at gmail.com]
> *Sent:* Wednesday, April 27, 2016 3:21 PM
> *To:* Millies, Sebastian
> *Cc:* concurrency-interest
> *Subject:* Re: [concurrency-interest] ConcurrentHashMapV8 Livelock on
> computeIfAbsent() ?
>
>
>
> Do you have a thread dump? (sorry, I don't have any spare cycles to have a
> stab at running it right now)
>
>
>
> On Wed, Apr 27, 2016 at 3:09 PM, Millies, Sebastian <
> Sebastian.Millies at softwareag.com> wrote:
>
> I have added
> https://gist.github.com/smillies/0cceb17501f74c4f53bf4930eba61889#file-fibcachedconcurrentbenchmark-java
>
> to compute concurrent benchmarks of the 6000th Fibonacci number. Only the
> CF versions of course.
>
>
>
> The same Fib is used by two threads in each case, the pool for the async
> version gets another two.
>
>
>
> The bad news is they?re prone to  deadlock. Now I am no expert in JMH,
> perhaps it?s just the way I?ve set up the tests .(I hope so.)
>
>
>
> n  Sebastian
>
>
>
>
>
> Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt,
> Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 -
> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>
>
>
>
>
> --
>
> Cheers,
>
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160501/84af4b36/attachment-0001.html>

From heinz at javaspecialists.eu  Sun May  1 12:18:49 2016
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Sun, 01 May 2016 17:18:49 +0100
Subject: [concurrency-interest] ConcurrentHashMapV8 Livelock on
 computeIfAbsent() ?
In-Reply-To: <CANPzfU_QS1XaNxrip_YmBKLRx3-8qgno=O6OLktSqcD43KNuUQ@mail.gmail.com>
References: <5715E8DA.8020403@javaspecialists.eu>	<571B0884.1010706@javaspecialists.eu>	<32F15738E8E5524DA4F01A0FA4A8E4900102E576AE@HQMBX5.eur.ad.sag>	<CANPzfU91BD4DxWRajFL-AANOYVk_fSqRfThhe2PV2zoWQ6sxcQ@mail.gmail.com>	<CADZL2=u9z3kY+YaYKDSnAX0-Pbn+6x1H_-E-Czn4FQ8xV1poyw@mail.gmail.com>	<32F15738E8E5524DA4F01A0FA4A8E4900102E584D1@HQMBX5.eur.ad.sag>	<CANPzfU8QrjHQUikwhvrC-P0=myu5cXzmsUKpE31gWpFP+jCFgw@mail.gmail.com>	<32F15738E8E5524DA4F01A0FA4A8E4900102E587A2@HQMBX5.eur.ad.sag>	<CANPzfU9xnbiTnEbsmUGa7=u0zw+EtY6t5LUGYMMzvzkxzTyeSw@mail.gmail.com>	<32F15738E8E5524DA4F01A0FA4A8E4900102E58A89@HQMBX5.eur.ad.sag>	<CANPzfU-aAdVw5aTWAanGER_NgLRqoKyk_E6GCvuw-oXBrnFz=Q@mail.gmail.com>	<32F15738E8E5524DA4F01A0FA4A8E4900102E58D74@HQMBX5.eur.ad.sag>	<CANPzfU9FMXDqF6BLzor7awmJE5+MD5UrvCdPC0uiyrjCOV9n8g@mail.gmail.com>	<32F15738E8E5524DA4F01A0FA4A8E4900102E59C73@HQMBX5.eur.ad.sag>
	<CANPzfU_QS1XaNxrip_YmBKLRx3-8qgno=O6OLktSqcD43KNuUQ@mail.gmail.com>
Message-ID: <57262C69.4000102@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160501/48fcb8f8/attachment-0001.html>

From Sebastian.Millies at softwareag.com  Mon May  2 06:03:53 2016
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Mon, 2 May 2016 10:03:53 +0000
Subject: [concurrency-interest] ConcurrentHashMapV8 Livelock on
 computeIfAbsent() ?
In-Reply-To: <CANPzfU_QS1XaNxrip_YmBKLRx3-8qgno=O6OLktSqcD43KNuUQ@mail.gmail.com>
References: <5715E8DA.8020403@javaspecialists.eu>
	<CAGu0=MN-qrM46acU=dgNJXdMmLWNejMHrYZPELXbT2SJBJe24g@mail.gmail.com>
	<5715FBF8.80307@javaspecialists.eu>	<5716192D.4090601@cs.oswego.edu>
	<6701E505-7F1F-4364-B3E1-39A8C2EB554B@sics.se>
	<5717759C.1010100@cs.oswego.edu>
	<CANPzfU9b7ikXOKJcuMO-PV8unLzkEN4rx94vYr9obXuPu8n9eA@mail.gmail.com>
	<571B0884.1010706@javaspecialists.eu>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E576AE@HQMBX5.eur.ad.sag>
	<CANPzfU91BD4DxWRajFL-AANOYVk_fSqRfThhe2PV2zoWQ6sxcQ@mail.gmail.com>
	<CADZL2=u9z3kY+YaYKDSnAX0-Pbn+6x1H_-E-Czn4FQ8xV1poyw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E584D1@HQMBX5.eur.ad.sag>
	<CANPzfU8QrjHQUikwhvrC-P0=myu5cXzmsUKpE31gWpFP+jCFgw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E587A2@HQMBX5.eur.ad.sag>
	<CANPzfU9xnbiTnEbsmUGa7=u0zw+EtY6t5LUGYMMzvzkxzTyeSw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58A89@HQMBX5.eur.ad.sag>
	<CANPzfU-aAdVw5aTWAanGER_NgLRqoKyk_E6GCvuw-oXBrnFz=Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58D74@HQMBX5.eur.ad.sag>
	<CANPzfU9FMXDqF6BLzor7awmJE5+MD5UrvCdPC0uiyrjCOV9n8g@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E59C73@HQMBX5.eur.ad.sag>
	<CANPzfU_QS1XaNxrip_YmBKLRx3-8qgno=O6OLktSqcD43KNuUQ@mail.gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E4900102E59F03@HQMBX5.eur.ad.sag>

Hi Viktor,

as I?m getting StackOverflowErrors I don?t think the code we?ve seen trampolines. However, below I show code that really does trampoline. That is, it combines concurrent memorization and  trampolining. With it, I can get to about the 500.000th Fibonacci number on my box, before all the cached BigInteger values cause an OutOfMemoryError. This code scales OK, I have single-shot times for  the 100.000th Fibonacci number with 2, 4, and 8 concurrent threads of about a quarter of a second.

This is somewhat off on a tangent, I guess. Bouncing off an Executor queue inside CF is not the most efficient implementation of a trampoline. But I think it?s still interesting.


n  Sebastian

static class ConcurrentTrampoliningMemoizer<T,R> {
  private static final Executor TRAMPOLINE = newSingleThreadExecutor(new ThreadFactoryBuilder().setDaemon(true).build());
  private final ConcurrentMap<T, CompletableFuture<R>> memo;

  public ConcurrentTrampoliningMemoizer(ConcurrentMap<T, CompletableFuture<R>> cache) {
    this.memo = cache;
  }

  public Function<T, CompletableFuture<R>> memoize(Function<T, CompletableFuture<R>> f) {
    return t -> {
      CompletableFuture<R> r = memo.get(t);
      if (r == null) {
        final CompletableFuture<R> compute = new CompletableFuture<>();
        r = memo.putIfAbsent(t, compute);
        if (r == null) {
          r = CompletableFuture.supplyAsync(() -> f.apply(t), TRAMPOLINE).thenCompose(Function.identity())
              .thenCompose(x -> {
                compute.complete(x);
                return compute;
              });
        }
       }
       return r;
    };
  }
}

static class Fibonacci {
  private final ConcurrentTrampoliningMemoizer<Integer, BigInteger> m;

  public Fibonacci(ConcurrentMap<Integer, CompletableFuture<BigInteger>> cache) {
    m = new ConcurrentTrampoliningMemoizer<>(cache);
  }

  public CompletableFuture<BigInteger> fib(int n) {
    if (n <= 2) return CompletableFuture.completedFuture(BigInteger.ONE);
    return m.memoize(this::fib).apply(n - 1).thenCompose(x ->
           m.memoize(this::fib).apply(n - 2).thenApply(y ->
             x.add(y)));
  }
}

BigInteger fib = new Fibonacci(new ConcurrentHashMap<>()).fib(500_000).join();

From: Viktor Klang [mailto:viktor.klang at gmail.com]
Sent: Sunday, May 01, 2016 10:56 AM
To: Millies, Sebastian
Cc: concurrency-interest
Subject: RE: [concurrency-interest] ConcurrentHashMapV8 Livelock on computeIfAbsent() ?


Hi Sebastian,

I suspect the use of thenComposeAsync is required to trampoline on top of the Executors submission queue rather than exhausting the stack.

I completely missed that you were testing the scalability, I'm sorry. Let me have a look later today or tomorrow.

Thanks for keeping the topic alive,

V

--
Cheers,
?
On Apr 30, 2016 11:19 PM, "Millies, Sebastian" <Sebastian.Millies at softwareag.com<mailto:Sebastian.Millies at softwareag.com>> wrote:
Hi Viktor,

I?m sorry, I?ve made a mistake: there hasn?t been a deadlock at all. When running the JMH benchmark, I don?t get as much stack space as when running stand-alone, so the recursion depth of 6000 was causing a StackoverflowError. However, I didn?t see that error, it was hidden by JMH, I just saw the benchmark hanging and the JMH worker threads all being parked, and jumped to the wrong conclusion.

Anyway, computing the 3500th Fibonacci number, I consistently do not see any advantage of the async version over the synchronous one. In fact, it is the other way around:

2 Threads
Benchmark                                 Mode  Cnt  Score     Error  Units
FibCachedConcurrentBenchmark.cf3500         ss   20   4.132 ?  1.421  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20   9.134 ?  0.862  ms/op
FibCachedConcurrentBenchmark.cf3500         ss   20   2.887 ?  0.571  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20  10.345 ? 12.954  ms/op
FibCachedConcurrentBenchmark.cf3500         ss   20   3.500 ?  1.291  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20   8.803 ?  1.679  ms/op

4 Threads
Benchmark                                 Mode  Cnt  Score   Error  Units
FibCachedConcurrentBenchmark.cf3500         ss   20  2.780 ? 0.430  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.850 ? 1.595  ms/op
FibCachedConcurrentBenchmark.cf3500         ss   20  3.034 ? 0.451  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20  9.744 ? 1.669  ms/op
FibCachedConcurrentBenchmark.cf3500         ss   20  3.965 ? 1.380  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.430 ? 2.396  ms/op

Perhaps adding to BigIntegers just isn?t expensive enough to warrant the overhead of going async.


?  Sebastian

PS: Your code below I think doesn?t address the problem, namely as you suggested ?to use the same Fib from multiple threads to show how it behaves under contention?. Your test(int) method below produces a new CF instance for each thread, so there is no contention. Or does it?


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160502/1271e675/attachment-0001.html>

From viktor.klang at gmail.com  Mon May  2 06:30:35 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 2 May 2016 12:30:35 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8 Livelock on
 computeIfAbsent() ?
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E4900102E59F03@HQMBX5.eur.ad.sag>
References: <5715E8DA.8020403@javaspecialists.eu>
	<CAGu0=MN-qrM46acU=dgNJXdMmLWNejMHrYZPELXbT2SJBJe24g@mail.gmail.com>
	<5715FBF8.80307@javaspecialists.eu> <5716192D.4090601@cs.oswego.edu>
	<6701E505-7F1F-4364-B3E1-39A8C2EB554B@sics.se>
	<5717759C.1010100@cs.oswego.edu>
	<CANPzfU9b7ikXOKJcuMO-PV8unLzkEN4rx94vYr9obXuPu8n9eA@mail.gmail.com>
	<571B0884.1010706@javaspecialists.eu>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E576AE@HQMBX5.eur.ad.sag>
	<CANPzfU91BD4DxWRajFL-AANOYVk_fSqRfThhe2PV2zoWQ6sxcQ@mail.gmail.com>
	<CADZL2=u9z3kY+YaYKDSnAX0-Pbn+6x1H_-E-Czn4FQ8xV1poyw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E584D1@HQMBX5.eur.ad.sag>
	<CANPzfU8QrjHQUikwhvrC-P0=myu5cXzmsUKpE31gWpFP+jCFgw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E587A2@HQMBX5.eur.ad.sag>
	<CANPzfU9xnbiTnEbsmUGa7=u0zw+EtY6t5LUGYMMzvzkxzTyeSw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58A89@HQMBX5.eur.ad.sag>
	<CANPzfU-aAdVw5aTWAanGER_NgLRqoKyk_E6GCvuw-oXBrnFz=Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58D74@HQMBX5.eur.ad.sag>
	<CANPzfU9FMXDqF6BLzor7awmJE5+MD5UrvCdPC0uiyrjCOV9n8g@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E59C73@HQMBX5.eur.ad.sag>
	<CANPzfU_QS1XaNxrip_YmBKLRx3-8qgno=O6OLktSqcD43KNuUQ@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E59F03@HQMBX5.eur.ad.sag>
Message-ID: <CANPzfU_dv0e62aCnxHM3M7oS0nKAs8n40Zmwo1tvkvtpq6jCqA@mail.gmail.com>

Hi Sebastian,

Controlling the bouncing from the outside seems desirable, and I think the
following implementation might generate a bit less allocations than the
previous one:

package fib;

import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CompletionStage;
import java.util.concurrent.Executor;
import java.util.concurrent.ConcurrentMap;
import java.util.function.Function;
import java.math.BigInteger;

class ConcurrentTrampoliningMemoizer<T,R> {
  private final ConcurrentMap<T, CompletionStage<R>> memo;
  private final Executor executor;

  public ConcurrentTrampoliningMemoizer(final ConcurrentMap<T,
CompletionStage<R>> cache, final Executor e) {
    this.memo = cache;
    this.executor = e;
  }

  public Function<T, CompletionStage<R>> memoize(final Function<T,
CompletionStage<R>> f) {
    return t -> {
      CompletionStage<R> r = memo.get(t);
      if (r == null) {
        final CompletableFuture<R> compute = new CompletableFuture<>();
        r = memo.putIfAbsent(t, compute);
        if (r == null) {
          r = CompletableFuture.supplyAsync(() -> f.apply(t),
executor).thenCompose(Function.identity())
              .thenCompose(x -> {
                compute.complete(x);
                return compute;
              });
        }
       }
       return r;
    };
  }
}

public final class FibCached {
  private final Function<Integer, CompletionStage<BigInteger>> m;
  private final CompletionStage<BigInteger> ONE =
CompletableFuture.completedFuture(BigInteger.ONE);

  public FibCached(final ConcurrentMap<Integer,
CompletionStage<BigInteger>> cache, final Executor executor) {
    m = new ConcurrentTrampoliningMemoizer<Integer, BigInteger>(cache,
executor).memoize(this::fib);
  }

  public CompletionStage<BigInteger> fib(int n) {
    if (n <= 2) return ONE;
    else return m.apply(n - 1).thenCompose(x -> m.apply(n - 2).thenApply(y
-> x.add(y)));
  }
}

On Mon, May 2, 2016 at 12:03 PM, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:

> Hi Viktor,
>
>
>
> as I?m getting StackOverflowErrors I don?t think the code we?ve seen
> trampolines. However, below I show code that really does trampoline. That
> is, it combines concurrent memorization and  trampolining. With it, I can
> get to about the 500.000th Fibonacci number on my box, before all the
> cached BigInteger values cause an OutOfMemoryError. This code scales OK, I
> have single-shot times for  the 100.000th Fibonacci number with 2, 4, and
> 8 concurrent threads of about a quarter of a second.
>
>
>
> This is somewhat off on a tangent, I guess. Bouncing off an Executor queue
> inside CF is not the most efficient implementation of a trampoline. But I
> think it?s still interesting.
>
>
>
> n  Sebastian
>
>
>
> static class ConcurrentTrampoliningMemoizer<T,R> {
>
>   private static final Executor TRAMPOLINE = newSingleThreadExecutor(new
> ThreadFactoryBuilder().setDaemon(true).build());
>
>   private final ConcurrentMap<T, CompletableFuture<R>> memo;
>
>
>
>   public ConcurrentTrampoliningMemoizer(ConcurrentMap<T,
> CompletableFuture<R>> cache) {
>
>     this.memo = cache;
>
>   }
>
>
>
>   public Function<T, CompletableFuture<R>> memoize(Function<T,
> CompletableFuture<R>> f) {
>
>     return t -> {
>
>       CompletableFuture<R> r = memo.get(t);
>
>       if (r == null) {
>
>         final CompletableFuture<R> compute = new CompletableFuture<>();
>
>         r = memo.putIfAbsent(t, compute);
>
>         if (r == null) {
>
>           r = CompletableFuture.supplyAsync(() -> f.apply(t),
> TRAMPOLINE).thenCompose(Function.identity())
>
>               .thenCompose(x -> {
>
>                 compute.complete(x);
>
>                 return compute;
>
>               });
>
>         }
>
>        }
>
>        return r;
>
>     };
>
>   }
>
> }
>
>
>
> static class Fibonacci {
>
>   private final ConcurrentTrampoliningMemoizer<Integer, BigInteger> m;
>
>
>
>   public Fibonacci(ConcurrentMap<Integer, CompletableFuture<BigInteger>>
> cache) {
>
>     m = new ConcurrentTrampoliningMemoizer<>(cache);
>
>   }
>
>
>
>   public CompletableFuture<BigInteger> fib(int n) {
>
>     if (n <= 2) return CompletableFuture.completedFuture(BigInteger.ONE);
>
>     return m.memoize(this::fib).apply(n - 1).thenCompose(x ->
>
>            m.memoize(this::fib).apply(n - 2).thenApply(y ->
>
>              x.add(y)));
>
>   }
>
> }
>
>
>
> BigInteger fib = new Fibonacci(new
> ConcurrentHashMap<>()).fib(500_000).join();
>
>
>
> *From:* Viktor Klang [mailto:viktor.klang at gmail.com]
> *Sent:* Sunday, May 01, 2016 10:56 AM
> *To:* Millies, Sebastian
> *Cc:* concurrency-interest
> *Subject:* RE: [concurrency-interest] ConcurrentHashMapV8 Livelock on
> computeIfAbsent() ?
>
>
>
> Hi Sebastian,
>
> I suspect the use of thenComposeAsync is required to trampoline on top of
> the Executors submission queue rather than exhausting the stack.
>
> I completely missed that you were testing the scalability, I'm sorry. Let
> me have a look later today or tomorrow.
>
> Thanks for keeping the topic alive,
>
> V
>
> --
> Cheers,
> ?
>
> On Apr 30, 2016 11:19 PM, "Millies, Sebastian" <
> Sebastian.Millies at softwareag.com> wrote:
>
> Hi Viktor,
>
>
>
> I?m sorry, I?ve made a mistake: there hasn?t been a deadlock at all. When
> running the JMH benchmark, I don?t get as much stack space as when running
> stand-alone, so the recursion depth of 6000 was causing a
> StackoverflowError. However, I didn?t see that error, it was hidden by JMH,
> I just saw the benchmark hanging and the JMH worker threads all being
> parked, and jumped to the wrong conclusion.
>
>
>
> Anyway, computing the 3500th Fibonacci number, I consistently do not see
> any advantage of the async version over the synchronous one. In fact, it is
> the other way around:
>
>
>
> 2 Threads
>
> Benchmark                                 Mode  Cnt  Score     Error  Units
>
> FibCachedConcurrentBenchmark.cf3500         ss   20   4.132 ?  1.421  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20   9.134 ?  0.862  ms/op
>
> FibCachedConcurrentBenchmark.cf3500         ss   20   2.887 ?  0.571  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20  10.345 ? 12.954  ms/op
>
> FibCachedConcurrentBenchmark.cf3500         ss   20   3.500 ?  1.291  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20   8.803 ?  1.679  ms/op
>
>
>
> 4 Threads
>
> Benchmark                                 Mode  Cnt  Score   Error  Units
>
> FibCachedConcurrentBenchmark.cf3500         ss   20  2.780 ? 0.430  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.850 ? 1.595  ms/op
>
> FibCachedConcurrentBenchmark.cf3500         ss   20  3.034 ? 0.451  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20  9.744 ? 1.669  ms/op
>
> FibCachedConcurrentBenchmark.cf3500         ss   20  3.965 ? 1.380  ms/op
>
> FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.430 ? 2.396  ms/op
>
>
>
> Perhaps adding to BigIntegers just isn?t expensive enough to warrant the
> overhead of going async.
>
>
>
> n  Sebastian
>
>
>
> PS: Your code below I think doesn?t address the problem, namely as you
> suggested ?to use the same Fib from multiple threads to show how it
> behaves under contention?. Your test(int) method below produces a new CF
> instance for each thread, so there is no contention. Or does it?
>
>
>
> Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt,
> Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 -
> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160502/5e13ba46/attachment-0001.html>

From Sebastian.Millies at softwareag.com  Mon May  2 07:25:02 2016
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Mon, 2 May 2016 11:25:02 +0000
Subject: [concurrency-interest] ConcurrentHashMapV8 Livelock on
 computeIfAbsent() ?
In-Reply-To: <CANPzfU_dv0e62aCnxHM3M7oS0nKAs8n40Zmwo1tvkvtpq6jCqA@mail.gmail.com>
References: <5715E8DA.8020403@javaspecialists.eu>
	<CAGu0=MN-qrM46acU=dgNJXdMmLWNejMHrYZPELXbT2SJBJe24g@mail.gmail.com>
	<5715FBF8.80307@javaspecialists.eu> <5716192D.4090601@cs.oswego.edu>
	<6701E505-7F1F-4364-B3E1-39A8C2EB554B@sics.se>
	<5717759C.1010100@cs.oswego.edu>
	<CANPzfU9b7ikXOKJcuMO-PV8unLzkEN4rx94vYr9obXuPu8n9eA@mail.gmail.com>
	<571B0884.1010706@javaspecialists.eu>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E576AE@HQMBX5.eur.ad.sag>
	<CANPzfU91BD4DxWRajFL-AANOYVk_fSqRfThhe2PV2zoWQ6sxcQ@mail.gmail.com>
	<CADZL2=u9z3kY+YaYKDSnAX0-Pbn+6x1H_-E-Czn4FQ8xV1poyw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E584D1@HQMBX5.eur.ad.sag>
	<CANPzfU8QrjHQUikwhvrC-P0=myu5cXzmsUKpE31gWpFP+jCFgw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E587A2@HQMBX5.eur.ad.sag>
	<CANPzfU9xnbiTnEbsmUGa7=u0zw+EtY6t5LUGYMMzvzkxzTyeSw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58A89@HQMBX5.eur.ad.sag>
	<CANPzfU-aAdVw5aTWAanGER_NgLRqoKyk_E6GCvuw-oXBrnFz=Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58D74@HQMBX5.eur.ad.sag>
	<CANPzfU9FMXDqF6BLzor7awmJE5+MD5UrvCdPC0uiyrjCOV9n8g@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E59C73@HQMBX5.eur.ad.sag>
	<CANPzfU_QS1XaNxrip_YmBKLRx3-8qgno=O6OLktSqcD43KNuUQ@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E59F03@HQMBX5.eur.ad.sag>
	<CANPzfU_dv0e62aCnxHM3M7oS0nKAs8n40Zmwo1tvkvtpq6jCqA@mail.gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E4900102E59FB0@HQMBX5.eur.ad.sag>

Hi Viktor,

I don?t get much more mileage. What?s swamping the heap is really the BigIntegers, not the function instances (and ONE gets allocated only once anyway).
As for passing in an executor: On my system, performance degrades when I pass in anything but a single thread executor, so I?m not sure that extra parameter really helps.


n  Sebastian

From: Viktor Klang [mailto:viktor.klang at gmail.com]
Sent: Monday, May 02, 2016 12:31 PM
To: Millies, Sebastian
Cc: concurrency-interest
Subject: Re: [concurrency-interest] ConcurrentHashMapV8 Livelock on computeIfAbsent() ?

Hi Sebastian,

Controlling the bouncing from the outside seems desirable, and I think the following implementation might generate a bit less allocations than the previous one:

package fib;

import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CompletionStage;
import java.util.concurrent.Executor;
import java.util.concurrent.ConcurrentMap;
import java.util.function.Function;
import java.math.BigInteger;

class ConcurrentTrampoliningMemoizer<T,R> {
  private final ConcurrentMap<T, CompletionStage<R>> memo;
  private final Executor executor;

  public ConcurrentTrampoliningMemoizer(final ConcurrentMap<T, CompletionStage<R>> cache, final Executor e) {
    this.memo = cache;
    this.executor = e;
  }

  public Function<T, CompletionStage<R>> memoize(final Function<T, CompletionStage<R>> f) {
    return t -> {
      CompletionStage<R> r = memo.get(t);
      if (r == null) {
        final CompletableFuture<R> compute = new CompletableFuture<>();
        r = memo.putIfAbsent(t, compute);
        if (r == null) {
          r = CompletableFuture.supplyAsync(() -> f.apply(t), executor).thenCompose(Function.identity())
              .thenCompose(x -> {
                compute.complete(x);
                return compute;
              });
        }
       }
       return r;
    };
  }
}

public final class FibCached {
  private final Function<Integer, CompletionStage<BigInteger>> m;
  private final CompletionStage<BigInteger> ONE = CompletableFuture.completedFuture(BigInteger.ONE);

  public FibCached(final ConcurrentMap<Integer, CompletionStage<BigInteger>> cache, final Executor executor) {
    m = new ConcurrentTrampoliningMemoizer<Integer, BigInteger>(cache, executor).memoize(this::fib);
  }

  public CompletionStage<BigInteger> fib(int n) {
    if (n <= 2) return ONE;
    else return m.apply(n - 1).thenCompose(x -> m.apply(n - 2).thenApply(y -> x.add(y)));
  }
}

On Mon, May 2, 2016 at 12:03 PM, Millies, Sebastian <Sebastian.Millies at softwareag.com<mailto:Sebastian.Millies at softwareag.com>> wrote:
Hi Viktor,

as I?m getting StackOverflowErrors I don?t think the code we?ve seen trampolines. However, below I show code that really does trampoline. That is, it combines concurrent memorization and  trampolining. With it, I can get to about the 500.000th Fibonacci number on my box, before all the cached BigInteger values cause an OutOfMemoryError. This code scales OK, I have single-shot times for  the 100.000th Fibonacci number with 2, 4, and 8 concurrent threads of about a quarter of a second.

This is somewhat off on a tangent, I guess. Bouncing off an Executor queue inside CF is not the most efficient implementation of a trampoline. But I think it?s still interesting.


?  Sebastian

static class ConcurrentTrampoliningMemoizer<T,R> {
  private static final Executor TRAMPOLINE = newSingleThreadExecutor(new ThreadFactoryBuilder().setDaemon(true).build());
  private final ConcurrentMap<T, CompletableFuture<R>> memo;

  public ConcurrentTrampoliningMemoizer(ConcurrentMap<T, CompletableFuture<R>> cache) {
    this.memo = cache;
  }

  public Function<T, CompletableFuture<R>> memoize(Function<T, CompletableFuture<R>> f) {
    return t -> {
      CompletableFuture<R> r = memo.get(t);
      if (r == null) {
        final CompletableFuture<R> compute = new CompletableFuture<>();
        r = memo.putIfAbsent(t, compute);
        if (r == null) {
          r = CompletableFuture.supplyAsync(() -> f.apply(t), TRAMPOLINE).thenCompose(Function.identity())
              .thenCompose(x -> {
                compute.complete(x);
                return compute;
              });
        }
       }
       return r;
    };
  }
}

static class Fibonacci {
  private final ConcurrentTrampoliningMemoizer<Integer, BigInteger> m;

  public Fibonacci(ConcurrentMap<Integer, CompletableFuture<BigInteger>> cache) {
    m = new ConcurrentTrampoliningMemoizer<>(cache);
  }

  public CompletableFuture<BigInteger> fib(int n) {
    if (n <= 2) return CompletableFuture.completedFuture(BigInteger.ONE);
    return m.memoize(this::fib).apply(n - 1).thenCompose(x ->
           m.memoize(this::fib).apply(n - 2).thenApply(y ->
             x.add(y)));
  }
}

BigInteger fib = new Fibonacci(new ConcurrentHashMap<>()).fib(500_000).join();

From: Viktor Klang [mailto:viktor.klang at gmail.com<mailto:viktor.klang at gmail.com>]
Sent: Sunday, May 01, 2016 10:56 AM
To: Millies, Sebastian
Cc: concurrency-interest
Subject: RE: [concurrency-interest] ConcurrentHashMapV8 Livelock on computeIfAbsent() ?


Hi Sebastian,

I suspect the use of thenComposeAsync is required to trampoline on top of the Executors submission queue rather than exhausting the stack.

I completely missed that you were testing the scalability, I'm sorry. Let me have a look later today or tomorrow.

Thanks for keeping the topic alive,

V

--
Cheers,
?
On Apr 30, 2016 11:19 PM, "Millies, Sebastian" <Sebastian.Millies at softwareag.com<mailto:Sebastian.Millies at softwareag.com>> wrote:
Hi Viktor,

I?m sorry, I?ve made a mistake: there hasn?t been a deadlock at all. When running the JMH benchmark, I don?t get as much stack space as when running stand-alone, so the recursion depth of 6000 was causing a StackoverflowError. However, I didn?t see that error, it was hidden by JMH, I just saw the benchmark hanging and the JMH worker threads all being parked, and jumped to the wrong conclusion.

Anyway, computing the 3500th Fibonacci number, I consistently do not see any advantage of the async version over the synchronous one. In fact, it is the other way around:

2 Threads
Benchmark                                 Mode  Cnt  Score     Error  Units
FibCachedConcurrentBenchmark.cf3500         ss   20   4.132 ?  1.421  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20   9.134 ?  0.862  ms/op
FibCachedConcurrentBenchmark.cf3500         ss   20   2.887 ?  0.571  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20  10.345 ? 12.954  ms/op
FibCachedConcurrentBenchmark.cf3500         ss   20   3.500 ?  1.291  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20   8.803 ?  1.679  ms/op

4 Threads
Benchmark                                 Mode  Cnt  Score   Error  Units
FibCachedConcurrentBenchmark.cf3500         ss   20  2.780 ? 0.430  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.850 ? 1.595  ms/op
FibCachedConcurrentBenchmark.cf3500         ss   20  3.034 ? 0.451  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20  9.744 ? 1.669  ms/op
FibCachedConcurrentBenchmark.cf3500         ss   20  3.965 ? 1.380  ms/op
FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.430 ? 2.396  ms/op

Perhaps adding to BigIntegers just isn?t expensive enough to warrant the overhead of going async.


?  Sebastian

PS: Your code below I think doesn?t address the problem, namely as you suggested ?to use the same Fib from multiple threads to show how it behaves under contention?. Your test(int) method below produces a new CF instance for each thread, so there is no contention. Or does it?


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com




--
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160502/2023cec2/attachment-0001.html>

From viktor.klang at gmail.com  Mon May  2 08:36:01 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 2 May 2016 14:36:01 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8 Livelock on
 computeIfAbsent() ?
In-Reply-To: <CANPzfU8bKGVV6TeWM8vRnaXU5mvHxhuxP83d7Gg9edE8iUddxQ@mail.gmail.com>
References: <5715E8DA.8020403@javaspecialists.eu>
	<CAGu0=MN-qrM46acU=dgNJXdMmLWNejMHrYZPELXbT2SJBJe24g@mail.gmail.com>
	<5715FBF8.80307@javaspecialists.eu>
	<5716192D.4090601@cs.oswego.edu>
	<6701E505-7F1F-4364-B3E1-39A8C2EB554B@sics.se>
	<5717759C.1010100@cs.oswego.edu>
	<CANPzfU9b7ikXOKJcuMO-PV8unLzkEN4rx94vYr9obXuPu8n9eA@mail.gmail.com>
	<571B0884.1010706@javaspecialists.eu>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E576AE@HQMBX5.eur.ad.sag>
	<CANPzfU91BD4DxWRajFL-AANOYVk_fSqRfThhe2PV2zoWQ6sxcQ@mail.gmail.com>
	<CADZL2=u9z3kY+YaYKDSnAX0-Pbn+6x1H_-E-Czn4FQ8xV1poyw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E584D1@HQMBX5.eur.ad.sag>
	<CANPzfU8QrjHQUikwhvrC-P0=myu5cXzmsUKpE31gWpFP+jCFgw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E587A2@HQMBX5.eur.ad.sag>
	<CANPzfU9xnbiTnEbsmUGa7=u0zw+EtY6t5LUGYMMzvzkxzTyeSw@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58A89@HQMBX5.eur.ad.sag>
	<CANPzfU-aAdVw5aTWAanGER_NgLRqoKyk_E6GCvuw-oXBrnFz=Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E58D74@HQMBX5.eur.ad.sag>
	<CANPzfU9FMXDqF6BLzor7awmJE5+MD5UrvCdPC0uiyrjCOV9n8g@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E59C73@HQMBX5.eur.ad.sag>
	<CANPzfU_QS1XaNxrip_YmBKLRx3-8qgno=O6OLktSqcD43KNuUQ@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E59F03@HQMBX5.eur.ad.sag>
	<CANPzfU_dv0e62aCnxHM3M7oS0nKAs8n40Zmwo1tvkvtpq6jCqA@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E4900102E59FB0@HQMBX5.eur.ad.sag>
	<CANPzfU-rj0sENsX+b+JBhbtOTngAuzv_GwMGj_0YTpqhEHOJQQ@mail.gmail.com>
	<CANPzfU8bKGVV6TeWM8vRnaXU5mvHxhuxP83d7Gg9edE8iUddxQ@mail.gmail.com>
Message-ID: <CANPzfU-B6RMNKYArXzgoengRLc8Fx+cc9JtJhRK_cTL82p554Q@mail.gmail.com>

Hi Sebastian,

I always restructure code along my own neural pathways so don't view all my
changes as major optimizations. :)

The reason for passing in the Executor is to make it much easier to measure
performance/scalability separately from implementation.

It would also be possible to create an Executor which trampolines on the
current/calling thread.

I guess next step is to change the encoding of the storage?

If we step back to the original topic: it does seem like the current
approach is workable compared to computeIfAbsent.

-- 
Cheers,
?
On May 2, 2016 1:25 PM, "Millies, Sebastian" <
Sebastian.Millies at softwareag.com> wrote:

Hi Viktor,



I don?t get much more mileage. What?s swamping the heap is really the
BigIntegers, not the function instances (and ONE gets allocated only once
anyway).

As for passing in an executor: On my system, performance degrades when I
pass in anything but a single thread executor, so I?m not sure that extra
parameter really helps.



n  Sebastian



*From:* Viktor Klang [mailto:viktor.klang at gmail.com]
*Sent:* Monday, May 02, 2016 12:31 PM
*To:* Millies, Sebastian
*Cc:* concurrency-interest
*Subject:* Re: [concurrency-interest] ConcurrentHashMapV8 Livelock on
computeIfAbsent() ?



Hi Sebastian,



Controlling the bouncing from the outside seems desirable, and I think the
following implementation might generate a bit less allocations than the
previous one:



package fib;



import java.util.concurrent.CompletableFuture;

import java.util.concurrent.CompletionStage;

import java.util.concurrent.Executor;

import java.util.concurrent.ConcurrentMap;

import java.util.function.Function;

import java.math.BigInteger;



class ConcurrentTrampoliningMemoizer<T,R> {

  private final ConcurrentMap<T, CompletionStage<R>> memo;

  private final Executor executor;



  public ConcurrentTrampoliningMemoizer(final ConcurrentMap<T,
CompletionStage<R>> cache, final Executor e) {

    this.memo = cache;

    this.executor = e;

  }



  public Function<T, CompletionStage<R>> memoize(final Function<T,
CompletionStage<R>> f) {

    return t -> {

      CompletionStage<R> r = memo.get(t);

      if (r == null) {

        final CompletableFuture<R> compute = new CompletableFuture<>();

        r = memo.putIfAbsent(t, compute);

        if (r == null) {

          r = CompletableFuture.supplyAsync(() -> f.apply(t),
executor).thenCompose(Function.identity())

              .thenCompose(x -> {

                compute.complete(x);

                return compute;

              });

        }

       }

       return r;

    };

  }

}



public final class FibCached {

  private final Function<Integer, CompletionStage<BigInteger>> m;

  private final CompletionStage<BigInteger> ONE =
CompletableFuture.completedFuture(BigInteger.ONE);



  public FibCached(final ConcurrentMap<Integer,
CompletionStage<BigInteger>> cache, final Executor executor) {

    m = new ConcurrentTrampoliningMemoizer<Integer, BigInteger>(cache,
executor).memoize(this::fib);

  }



  public CompletionStage<BigInteger> fib(int n) {

    if (n <= 2) return ONE;

    else return m.apply(n - 1).thenCompose(x -> m.apply(n - 2).thenApply(y
-> x.add(y)));

  }

}



On Mon, May 2, 2016 at 12:03 PM, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:

Hi Viktor,



as I?m getting StackOverflowErrors I don?t think the code we?ve seen
trampolines. However, below I show code that really does trampoline. That
is, it combines concurrent memorization and  trampolining. With it, I can
get to about the 500.000th Fibonacci number on my box, before all the
cached BigInteger values cause an OutOfMemoryError. This code scales OK, I
have single-shot times for  the 100.000th Fibonacci number with 2, 4, and 8
concurrent threads of about a quarter of a second.



This is somewhat off on a tangent, I guess. Bouncing off an Executor queue
inside CF is not the most efficient implementation of a trampoline. But I
think it?s still interesting.



n  Sebastian



static class ConcurrentTrampoliningMemoizer<T,R> {

  private static final Executor TRAMPOLINE = newSingleThreadExecutor(new
ThreadFactoryBuilder().setDaemon(true).build());

  private final ConcurrentMap<T, CompletableFuture<R>> memo;



  public ConcurrentTrampoliningMemoizer(ConcurrentMap<T,
CompletableFuture<R>> cache) {

    this.memo = cache;

  }



  public Function<T, CompletableFuture<R>> memoize(Function<T,
CompletableFuture<R>> f) {

    return t -> {

      CompletableFuture<R> r = memo.get(t);

      if (r == null) {

        final CompletableFuture<R> compute = new CompletableFuture<>();

        r = memo.putIfAbsent(t, compute);

        if (r == null) {

          r = CompletableFuture.supplyAsync(() -> f.apply(t),
TRAMPOLINE).thenCompose(Function.identity())

              .thenCompose(x -> {

                compute.complete(x);

                return compute;

              });

        }

       }

       return r;

    };

  }

}



static class Fibonacci {

  private final ConcurrentTrampoliningMemoizer<Integer, BigInteger> m;



  public Fibonacci(ConcurrentMap<Integer, CompletableFuture<BigInteger>>
cache) {

    m = new ConcurrentTrampoliningMemoizer<>(cache);

  }



  public CompletableFuture<BigInteger> fib(int n) {

    if (n <= 2) return CompletableFuture.completedFuture(BigInteger.ONE);

    return m.memoize(this::fib).apply(n - 1).thenCompose(x ->

           m.memoize(this::fib).apply(n - 2).thenApply(y ->

             x.add(y)));

  }

}



BigInteger fib = new Fibonacci(new
ConcurrentHashMap<>()).fib(500_000).join();



*From:* Viktor Klang [mailto:viktor.klang at gmail.com]
*Sent:* Sunday, May 01, 2016 10:56 AM
*To:* Millies, Sebastian
*Cc:* concurrency-interest
*Subject:* RE: [concurrency-interest] ConcurrentHashMapV8 Livelock on
computeIfAbsent() ?



Hi Sebastian,

I suspect the use of thenComposeAsync is required to trampoline on top of
the Executors submission queue rather than exhausting the stack.

I completely missed that you were testing the scalability, I'm sorry. Let
me have a look later today or tomorrow.

Thanks for keeping the topic alive,

V

-- 
Cheers,
?

On Apr 30, 2016 11:19 PM, "Millies, Sebastian" <
Sebastian.Millies at softwareag.com> wrote:

Hi Viktor,



I?m sorry, I?ve made a mistake: there hasn?t been a deadlock at all. When
running the JMH benchmark, I don?t get as much stack space as when running
stand-alone, so the recursion depth of 6000 was causing a
StackoverflowError. However, I didn?t see that error, it was hidden by JMH,
I just saw the benchmark hanging and the JMH worker threads all being
parked, and jumped to the wrong conclusion.



Anyway, computing the 3500th Fibonacci number, I consistently do not see
any advantage of the async version over the synchronous one. In fact, it is
the other way around:



2 Threads

Benchmark                                 Mode  Cnt  Score     Error  Units

FibCachedConcurrentBenchmark.cf3500         ss   20   4.132 ?  1.421  ms/op

FibCachedConcurrentBenchmark.cfAsync3500    ss   20   9.134 ?  0.862  ms/op

FibCachedConcurrentBenchmark.cf3500         ss   20   2.887 ?  0.571  ms/op

FibCachedConcurrentBenchmark.cfAsync3500    ss   20  10.345 ? 12.954  ms/op

FibCachedConcurrentBenchmark.cf3500         ss   20   3.500 ?  1.291  ms/op

FibCachedConcurrentBenchmark.cfAsync3500    ss   20   8.803 ?  1.679  ms/op



4 Threads

Benchmark                                 Mode  Cnt  Score   Error  Units

FibCachedConcurrentBenchmark.cf3500         ss   20  2.780 ? 0.430  ms/op

FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.850 ? 1.595  ms/op

FibCachedConcurrentBenchmark.cf3500         ss   20  3.034 ? 0.451  ms/op

FibCachedConcurrentBenchmark.cfAsync3500    ss   20  9.744 ? 1.669  ms/op

FibCachedConcurrentBenchmark.cf3500         ss   20  3.965 ? 1.380  ms/op

FibCachedConcurrentBenchmark.cfAsync3500    ss   20  8.430 ? 2.396  ms/op



Perhaps adding to BigIntegers just isn?t expensive enough to warrant the
overhead of going async.



n  Sebastian



PS: Your code below I think doesn?t address the problem, namely as you
suggested ?to use the same Fib from multiple threads to show how it behaves
under contention?. Your test(int) method below produces a new CF instance
for each thread, so there is no contention. Or does it?





Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt,
Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 -
Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>





-- 

Cheers,

?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160502/e9cf4aa1/attachment-0001.html>

From rco at quartetfs.com  Wed May  4 04:42:06 2016
From: rco at quartetfs.com (Romain Colle)
Date: Wed, 4 May 2016 10:42:06 +0200
Subject: [concurrency-interest] About putOrdered and its meaning
Message-ID: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>

Hi all,

I've been recently trying to wrap my head around the meaning of
Unsafe.putOrderedXXX() and how it differs from a volatile store.

If I understand correctly, putOrdered shares some guarantees with a
volatile store, notably that all the preceding stores from the same thread
will be made visible to other threads before the ordered store.
The main difference (from a hardware point of view) seems to be that the
ordered store (and preceding ones) are not immediately flushed to main
memory and may only be visible locally for a while.
Is that correct?

In this case, do we need anything more to guarantee safe Object publication
and happens-before relationships? More specifically:

1) I want to safely publish an Object, i.e. make sure it has been fully
built and initialized before making it visible to other threads.
In the absence of final fields (which should be enough by themselves), can
I simply use a putOrdered instead of a volatile write?

2) If I want a happens-before relationship before a write and a read:
Consider two variables X and Y. I first store a value 'a' in X and then a
value 'b' to Y. I want to make sure that if a thread reads 'b' from Y, the
'a' value of X will be visible to this thread.
The usual and "supported" way to go is to have Y be a volatile variable and
perform volatile loads and stores to Y.
However, wouldn't it be enough to simply perform an ordered store and a
volatile load?
In java 9, would it also be enough to perform a release store
(putObjectRelease) and an acquire load (getObjectAcquire)?

Thanks a lot for your insight!

-- 
Romain Colle
R&D Project Manager
QuartetFS
46 rue de l'Arbre Sec, 75001 Paris, France
http://www.quartetfs.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/d7cef19a/attachment.html>

From aleksey.shipilev at oracle.com  Wed May  4 10:08:11 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 4 May 2016 17:08:11 +0300
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
Message-ID: <572A024B.9090101@oracle.com>

On 05/04/2016 11:42 AM, Romain Colle wrote:
> I've been recently trying to wrap my head around the meaning of
> Unsafe.putOrderedXXX() and how it differs from a volatile store.

> If I understand correctly, putOrdered shares some guarantees with a
> volatile store, notably that all the preceding stores from the same
> thread will be made visible to other threads before the ordered store.
> The main difference (from a hardware point of view) seems to be that the
> ordered store (and preceding ones) are not immediately flushed to main
> memory and may only be visible locally for a while.
> Is that correct?

putOrdered is a release in disguise, most of the C++11 std::atomic(...,
mem_order_release) reasoning applies here. "Flushed to main memory" is a
very unhelpful model on current hardware.

There is no way to specify this in the realm of current Java Memory
Model, so explanation should really deviate from it. My own mental model
is this: acquire/release are the relaxations from the usual volatile
rules -- while producing happens-before-s, they drop from total
synchronization order, thus breaking sequential consistency.

In practice, this means at least the absence of total order of
ordered/volatile reads/writes; or, as Javadoc says, the release writes
might not be visible to other threads [in the order you'd expect from
volatile reads/writes -- Edit: me] immediately, until the next
synchronization action happens.

See e.g.:
  http://cs.oswego.edu/pipermail/concurrency-interest/2016-March/015037.html


> In this case, do we need anything more to guarantee safe Object
> publication and happens-before relationships? More specifically:
> 
> 1) I want to safely publish an Object, i.e. make sure it has been fully
> built and initialized before making it visible to other threads.
> In the absence of final fields (which should be enough by themselves),
> can I simply use a putOrdered instead of a volatile write?

I think you are conflating safe construction and safe publication there.

Safe publication still works:

                       int x; volatile int y;
-----------------------------------------------------------------------
    put(x, 1);                   |  r1 = get{Acquire|Volatile}(y);
    put{Release|Volatile}(y, 2); |  r2 = get(x);

(r1, r2) = (2, 0) is forbidden.

But anything trickier that requires sequential consistency fails. IRIW
fails, because no consistent write order observed by all threads. Dekker
fails, because release stores followed by loads may or may not be
visible in program order:

                     volatile int x; volatile int y;
-----------------------------------------------------------------------
    putRelease(x, 1);            |    putRelease(y, 1);
    r1 = getAcquire(y);          |    r2 = getAcquire(x);

(r1, r2) = (0, 0) is allowed. Forbidden if all ops are volatile.


Safe construction still does not work (even for volatiles!):

                                A global;
-----------------------------------------------------------------------
    A a = <alloc>;                  |  A a = global;
    put{Release|Volatile}(a.x, 1);  |  r1 = get{Acquire|Volatile}(a.x);
    global = a;                     |

(r1) = (0) is allowed.


> 2) If I want a happens-before relationship before a write and a read:
> Consider two variables X and Y. I first store a value 'a' in X and then
> a value 'b' to Y. I want to make sure that if a thread reads 'b' from Y,
> the 'a' value of X will be visible to this thread.
> The usual and "supported" way to go is to have Y be a volatile variable
> and perform volatile loads and stores to Y.

See the first example above.

> However, wouldn't it be enough to simply perform an ordered store and a
> volatile load?

Two answers:
 a) Yes, unless you need sequential consistency;
 b) No, unless you can give up sequential consistency;

> In java 9, would it also be enough to perform a release store
> (putObjectRelease) and an acquire load (getObjectAcquire)?

Same two answers.

The bottom-line is that acq/rel are very sharp tools, and their
advantages overcome the maintainability/reasoning downsides only in a
few selected cases and/or on weak memory model hardware platforms that
do not have fast SC primitives.

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/cec501d9/attachment.bin>

From david.dice at gmail.com  Wed May  4 10:56:33 2016
From: david.dice at gmail.com (David Dice)
Date: Wed, 4 May 2016 10:56:33 -0400
Subject: [concurrency-interest] For those of us who bench on Linux
Message-ID: <CANbRUciDYR4-n_wLmovfC2uRcz3r-tBXniv_ndORL9idJKRpDQ@mail.gmail.com>

The linux scheduler(s) -- O(1) and then CFS -- have been non work
conserving for some time, often leading to odd behavior for java
applications.   See
https://blogs.oracle.com/dave/entry/a_scheduling_and_dispatching_oddity.

I'll sometimes tell afflicted customers about the following work-around :

sudo sh -c ?echo 5000 > /proc/sys/kernel/sched_migration_cost_ns"

The above is palliative, but can be helpful.  Caveat utor.

Sched_migration_cost_ns represents a trade-off between eagerness to migrate
(and a tendency toward being work conserving) vs affinity to a CPU where a
thread might have some residual cache affinity.   It also reflects
migration costs, as evidenced by the name.

When a thread has been migrated, the scheduler will prevent the thread from
re-migrating for the duration specified via the sched_migration_cost_ns
pseudo-file.  IIRC, this holds even if the thread is blocked, or preempted,
btw.     Even if there are other idle CPUs on which a ready thread T1 might
run,  the scheduler will refrain from migrating T1 during T1's
sched_migration_cost_ns interval.  This damps migration rates (avoiding
rampant migration) which is normally a good thing, but it also prevents the
scheduler from being work conserving.  Lower sched_migration_cost_ns values
make the system tend toward being more work conserving, but also increase
migration rates, so there's a trade-off.

For the JVM, I?ve found that I can reduce the ?wasted dead time? ? time
where we have both idle CPUs and runnable threads languishing on ready
queues ? by reducing the value.  It?s not prefect, but it can make a
considerable difference for some benchmarks.   Beware, however, that it's
been a few years since I've looked at this issue, so YMMV.

Solaris, in contrast, preserves residual affinity to the extent possible,
but it's always work conserving. Affinity-aware scheduling is a win, but
being work conserving  generally needs to have precedence over affinity.
 (You can contrive cases where that claim doesn?t hold, but it?s a good
rule of thumb).
Dave


David Holmes wrote:


> Message: 1
> Date: Wed, 27 Apr 2016 18:50:54 +1000
> From: "David Holmes" <davidcholmes at aapt.net.au>
> To: "'Viktor Klang'" <viktor.klang at gmail.com>,
>         "'concurrency-interest'" <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] For those of us who bench on Linux
> Message-ID: <002901d1a061$e8faafb0$baf00f10$@aapt.net.au>
> Content-Type: text/plain; charset="utf-8"
>
> Very interesting. In particular that scheduling affinity (to take
> advantage of a warm cache) can be a performance hit.
>
>
>
> Thanks for the link.
>
>
>
> David
>
>
>
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Viktor Klang
> Sent: Wednesday, April 27, 2016 5:24 PM
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: [concurrency-interest] For those of us who bench on Linux
>
>
>
> TL;DR: problems found and fixed with Linux scheduler affecting multicore
> systems.
>
> Story & paper:
> https://blog.acolyer.org/2016/04/26/the-linux-scheduler-a-decade-of-wasted-cores/
>
> Code: https://github.com/jplozi/wastedcores
>
> Would love to hear your thoughts.
>
> --
> Cheers,
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/23c675a1/attachment.html>

From thurston at nomagicsoftware.com  Wed May  4 10:22:17 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 4 May 2016 07:22:17 -0700 (MST)
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <572A024B.9090101@oracle.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
Message-ID: <1462371737417-13432.post@n7.nabble.com>

Aleksey Shipilev-2 wrote
> Safe construction still does not work (even for volatiles!):
> 
>                                 A global;
> -----------------------------------------------------------------------
>     A a = 
> <alloc>
> ;                  |  A a = global;
>     put{Release|Volatile}(a.x, 1);  |  r1 = get{Acquire|Volatile}(a.x);
>     global = a;                     |
> 
> (r1) = (0) is allowed.
> 
> 
> 
> Thanks,
> -Aleksey

Let's change your example slightly:
 A global;
-----------------------------------------------------------------------
    A a = <alloc>;                  |  A a = getAcquire(global);
    putRelease(a.x, 1);           |   r1 = a.x;
    global = a;                      |

(r1) = (0) is *not* allowed.

Ignoring the possibility of a null pointer in thread on right.

release is a storestore and acquire is a loadload (at least that's my
understanding), and if so, then r1 should never be allowed to be 0





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/About-putOrdered-and-its-meaning-tp13429p13432.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Wed May  4 10:40:51 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 4 May 2016 07:40:51 -0700 (MST)
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <1462371737417-13432.post@n7.nabble.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
Message-ID: <1462372851737-13433.post@n7.nabble.com>

thurstonn wrote
> 
> Aleksey Shipilev-2 wrote
>> Safe construction still does not work (even for volatiles!):
>> 
>>                                 A global;
>> -----------------------------------------------------------------------
>>     A a = 
>> <alloc>
>> ;                  |  A a = global;
>>     put{Release|Volatile}(a.x, 1);  |  r1 = get{Acquire|Volatile}(a.x);
>>     global = a;                     |
>> 
>> (r1) = (0) is allowed.
>> 
>> 
>> 
>> Thanks,
>> -Aleksey
> Let's change your example slightly:
>  A global;
> -----------------------------------------------------------------------
>     A a = 
> <alloc>
> ;                  |  A a = getAcquire(global);
>     putRelease(a.x, 1);           |   r1 = a.x;
>     global = a;                      |
> 
> (r1) = (0) is *not* allowed.
> 
> Ignoring the possibility of a null pointer in thread on right.
> 
> release is a storestore and acquire is a loadload (at least that's my
> understanding), and if so, then r1 should never be allowed to be 0

I realize that I'm assuming that the barriers are emitted *after* the
respective memory actions, so above code becomes:
 A global;
-----------------------------------------------------------------------
    A a = <alloc>;                  |  A a = global;
    a.x = 1                            |  LoadLoad()
    StoreStore()                     |   r1 = a.x;
    global = a;                      |

Maybe that assumption is wrong?



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/About-putOrdered-and-its-meaning-tp13429p13433.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From aph at redhat.com  Wed May  4 12:02:44 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 4 May 2016 17:02:44 +0100
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <1462372851737-13433.post@n7.nabble.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com> <1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com>
Message-ID: <572A1D24.20109@redhat.com>

On 05/04/2016 03:40 PM, thurstonn wrote:
> I realize that I'm assuming that the barriers are emitted *after* the
> respective memory actions, so above code becomes:
>  A global;
> -----------------------------------------------------------------------
>     A a = <alloc>;                  |  A a = global;
>     a.x = 1                            |  LoadLoad()
>     StoreStore()                     |   r1 = a.x;
>     global = a;                      |
> 
> Maybe that assumption is wrong?

StoreRelease is (LoadStore|StoreStore ; store)
LoadAcquire is (load ; LoadStore|LoadLoad)

Andrew.


From aleksey.shipilev at oracle.com  Wed May  4 12:12:06 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 4 May 2016 19:12:06 +0300
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <1462371737417-13432.post@n7.nabble.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com> <1462371737417-13432.post@n7.nabble.com>
Message-ID: <572A1F56.9070609@oracle.com>

On 05/04/2016 05:22 PM, thurstonn wrote:
> Let's change your example slightly:
>  A global;
> -----------------------------------------------------------------------
>     A a = <alloc>;                  |  A a = getAcquire(global);
>     putRelease(a.x, 1);           |   r1 = a.x;
>     global = a;                      |
> 
> (r1) = (0) is *not* allowed.

Nope, (0) is still allowed, no matter how hard you try at the reader
side -- the ship on writer side <strike>had already sailed</strike> off
to the races ;) Only finals would preclude (0).

> release is a storestore and acquire is a loadload (at least that's my
> understanding), and if so, then r1 should never be allowed to be 0

Remember how putOrdered says nothing is guaranteed for the subsequent
ops? Assuming it is even fair to talk with barriers at this level (I
hate those), this is how it is subtly different from final fields:

 A a = <alloc>;
 [LoadStore|StoreStore] // putRelease a.x
 a.x = 1;
 global = a; // <--- oopsies, no barriers

(A similar example may be constructed for volatiles, where trailing
StoreLoad does not help either)

...whereas should a.x be final:

 A a = <alloc>;
 a.x = 1;
 [LoadStore|StoreStore] // <--- a mythical "freeze action"
 global = a; // yeah!


Thanks,
-Aleksey


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/75717f1a/attachment.bin>

From thurston at nomagicsoftware.com  Wed May  4 11:37:04 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 4 May 2016 08:37:04 -0700 (MST)
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <572A1F56.9070609@oracle.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<572A1F56.9070609@oracle.com>
Message-ID: <1462376224723-13436.post@n7.nabble.com>

Aleksey Shipilev-2 wrote
> On 05/04/2016 05:22 PM, thurstonn wrote:
>> Let's change your example slightly:
>>  A global;
>> -----------------------------------------------------------------------
>>     A a = 
> <alloc>
> ;                  |  A a = getAcquire(global);
>>     putRelease(a.x, 1);           |   r1 = a.x;
>>     global = a;                      |
>> 
>> (r1) = (0) is *not* allowed.
> 
> Nope, (0) is still allowed, no matter how hard you try at the reader
> side -- the ship on writer side 
> <strike>
> had already sailed
> </strike>
>  off
> to the races ;) Only finals would preclude (0).
> 
>> release is a storestore and acquire is a loadload (at least that's my
>> understanding), and if so, then r1 should never be allowed to be 0
> 
> Remember how putOrdered says nothing is guaranteed for the subsequent
> ops? Assuming it is even fair to talk with barriers at this level (I
> hate those), this is how it is subtly different from final fields:
> 
>  A a = 
> <alloc>
> ;
>  [LoadStore|StoreStore] // putRelease a.x
>  a.x = 1;
>  global = a; // <--- oopsies, no barriers
> 
> (A similar example may be constructed for volatiles, where trailing
> StoreLoad does not help either)
> 
> ...whereas should a.x be final:
> 
>  A a = 
> <alloc>
> ;
>  a.x = 1;
>  [LoadStore|StoreStore] // <--- a mythical "freeze action"
>  global = a; // yeah!
> 
> 
> Thanks,
> -Aleksey
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list

> Concurrency-interest at .oswego

> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> signature.asc (853 bytes)
> &lt;http://jsr166-concurrency.10961.n7.nabble.com/attachment/13435/0/signature.asc&gt;

OK, I think I got it, but surely then the following would work:

Let's change your example slightly (again):
A global;
-----------------------------------------------------------------------
     A a = <alloc>;                  |  A a = getAcquire(global);
     a.x = 1                           |   r1 = a.x;
     putRelease(global, a);        
 
    (r1) = (0) is *not* allowed.

Note: obviously this works if you replace Release/Acquire with volatile

I was confused about the order of the memory actions relative to the
respective barriers provided by putRelease, but I think the code with the
explicit barriers provides for the r1 = 1 guarantee




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/About-putOrdered-and-its-meaning-tp13429p13436.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From aleksey.shipilev at oracle.com  Wed May  4 13:16:24 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 4 May 2016 20:16:24 +0300
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <1462376224723-13436.post@n7.nabble.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com> <1462371737417-13432.post@n7.nabble.com>
	<572A1F56.9070609@oracle.com> <1462376224723-13436.post@n7.nabble.com>
Message-ID: <572A2E68.4070403@oracle.com>

On 05/04/2016 06:37 PM, thurstonn wrote:
> OK, I think I got it, but surely then the following would work:
> 
> Let's change your example slightly (again):
> A global;
> -----------------------------------------------------------------------
>      A a = <alloc>;                  |  A a = getAcquire(global);
>      a.x = 1                           |   r1 = a.x;
>      putRelease(global, a);        
>  
>     (r1) = (0) is *not* allowed.
> 
> Note: obviously this works if you replace Release/Acquire with volatile

Yes, you have arrived at my first example under "Safe publication still
works" (it does not matter if a.x is a field, it could be another
variable as well) here:
  http://cs.oswego.edu/pipermail/concurrency-interest/2016-May/015104.html

-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/891919b8/attachment.bin>

From boehm at acm.org  Wed May  4 13:35:27 2016
From: boehm at acm.org (Hans Boehm)
Date: Wed, 4 May 2016 10:35:27 -0700
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <572A1D24.20109@redhat.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com>
	<572A1D24.20109@redhat.com>
Message-ID: <CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>

On Wed, May 4, 2016 at 9:02 AM, Andrew Haley <aph at redhat.com> wrote:

> On 05/04/2016 03:40 PM, thurstonn wrote:
> > I realize that I'm assuming that the barriers are emitted *after* the
> > respective memory actions, so above code becomes:
> >  A global;
> > -----------------------------------------------------------------------
> >     A a = <alloc>;                  |  A a = global;
> >     a.x = 1                            |  LoadLoad()
> >     StoreStore()                     |   r1 = a.x;
> >     global = a;                      |
> >
> > Maybe that assumption is wrong?
>
> StoreRelease is (LoadStore|StoreStore ; store)
> LoadAcquire is (load ; LoadStore|LoadLoad)
>
>
But only when that abstraction works :-)

x = 1;
y =release 1;
z = 1;

does not order the stores to x and z.  (Neither in theory nor in practice.)

In the C++ model at least,

Thread 1: y =release 2; x =release 1;

Thread 2: x =release 2; y =release 1;

allows a final state of x = y = 2.  Memory_order_release doesn't mean
anything in the absence of a corresponding acquire or consume load.
 (Hardware implementations are unlikely to allow that; compiler
optimizations might.) Acquire/release make the "message passing" idiom
work, not much more than that.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/bd36fbcd/attachment.html>

From rco at quartetfs.com  Wed May  4 13:41:35 2016
From: rco at quartetfs.com (Romain Colle)
Date: Wed, 04 May 2016 19:41:35 +0200
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <572A2E68.4070403@oracle.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com> <1462371737417-13432.post@n7.nabble.com>
	<572A1F56.9070609@oracle.com> <1462376224723-13436.post@n7.nabble.com>
	<572A2E68.4070403@oracle.com>
Message-ID: <1462383697248-224e77e1-4f38398e-b53022f8@quartetfs.com>

Thanks a lot Aleksey (and all), that's exactly the explanation I was 
looking for.
Clear sentences with examples are always very appreciated!

Cheers
Romain

On Wed, May 04, 2016 at 7:29 PM, Aleksey Shipilev < 
aleksey.shipilev at oracle.com [aleksey.shipilev at oracle.com] > wrote:
On 05/04/2016 06:37 PM, thurstonn wrote:
 > OK, I think I got it, but surely then the following would work:
 >
 > Let's change your example slightly (again):
 > A global;
 > -----------------------------------------------------------------------
 > A a = <alloc>; | A a = getAcquire(global);
 > a.x = 1 | r1 = a.x;
 > putRelease(global, a);
 >
 > (r1) = (0) is *not* allowed.
 >
 > Note: obviously this works if you replace Release/Acquire with volatile

Yes, you have arrived at my first example under "Safe publication still
works" (it does not matter if a.x is a field, it could be another
variable as well) here:
http://cs.oswego.edu/pipermail/concurrency-interest/2016-May/015104.html

-Aleksey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/5b70867d/attachment-0001.html>

From thurston at nomagicsoftware.com  Wed May  4 12:56:55 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 4 May 2016 09:56:55 -0700 (MST)
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com>
	<572A1D24.20109@redhat.com>
	<CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
Message-ID: <1462381015529-13440.post@n7.nabble.com>

Hans Boehm wrote
> On Wed, May 4, 2016 at 9:02 AM, Andrew Haley &lt;

> aph@

> &gt; wrote:
> 
> 
>>
>> StoreRelease is (LoadStore|StoreStore ; store)
>> LoadAcquire is (load ; LoadStore|LoadLoad)
>>
>>
> But only when that abstraction works :-)
> 
> x = 1;
> y =release 1;
> z = 1;
> 
> does not order the stores to x and z.  (Neither in theory nor in
> practice.)
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list

> Concurrency-interest at .oswego

> http://cs.oswego.edu/mailman/concurrency-interest

Just to be clear, do you mean that (in another thread):
r1 = z
LoadLoad
r2 = x

can result in r1 = 1, r2 = 0?
Surely not




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/About-putOrdered-and-its-meaning-tp13429p13440.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From vitalyd at gmail.com  Wed May  4 14:40:24 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 4 May 2016 14:40:24 -0400
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <1462381015529-13440.post@n7.nabble.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com>
	<572A1D24.20109@redhat.com>
	<CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
	<1462381015529-13440.post@n7.nabble.com>
Message-ID: <CAHjP37Eaw3NKswX3c98vS_j5AEWXn3Q0ccx+0LNAmhy0-Mq+9A@mail.gmail.com>

Yes it can.  Writer can be reordered as:

z = 1;
x = 1;
y =release 1;

The releasing store to y only orders y and x (assuming reader observes that
via an acquire on y), but not z and x.  This is basically the same thing as
the constructor+field write earlier in this thread - the LoadLoad in the
reader is irrelevant since writer was reordered.


On Wed, May 4, 2016 at 12:56 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> Hans Boehm wrote
> > On Wed, May 4, 2016 at 9:02 AM, Andrew Haley &lt;
>
> > aph@
>
> > &gt; wrote:
> >
> >
> >>
> >> StoreRelease is (LoadStore|StoreStore ; store)
> >> LoadAcquire is (load ; LoadStore|LoadLoad)
> >>
> >>
> > But only when that abstraction works :-)
> >
> > x = 1;
> > y =release 1;
> > z = 1;
> >
> > does not order the stores to x and z.  (Neither in theory nor in
> > practice.)
> >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
>
> > Concurrency-interest at .oswego
>
> > http://cs.oswego.edu/mailman/concurrency-interest
>
> Just to be clear, do you mean that (in another thread):
> r1 = z
> LoadLoad
> r2 = x
>
> can result in r1 = 1, r2 = 0?
> Surely not
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/About-putOrdered-and-its-meaning-tp13429p13440.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/9af82678/attachment.html>

From boehm at acm.org  Wed May  4 17:17:08 2016
From: boehm at acm.org (Hans Boehm)
Date: Wed, 4 May 2016 14:17:08 -0700
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <CAHjP37Eaw3NKswX3c98vS_j5AEWXn3Q0ccx+0LNAmhy0-Mq+9A@mail.gmail.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com>
	<572A1D24.20109@redhat.com>
	<CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
	<1462381015529-13440.post@n7.nabble.com>
	<CAHjP37Eaw3NKswX3c98vS_j5AEWXn3Q0ccx+0LNAmhy0-Mq+9A@mail.gmail.com>
Message-ID: <CAPUmR1Y9C1W8Mqm+r61GCuo4Tt6mVaMGW4q2j5gayKEzk6-fRg@mail.gmail.com>

Yes.  And it probably bears repeating here that by similar reasoning

x = 1;
synchronized(foo) {}
z = 1;

also doesn't order the stores. Historically, it mostly did, but that was a
historical accident. It will not in the future.

As usual, none of this matters in the absence of data races.

On Wed, May 4, 2016 at 11:40 AM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> Yes it can.  Writer can be reordered as:
>
> z = 1;
> x = 1;
> y =release 1;
>
> The releasing store to y only orders y and x (assuming reader observes
> that via an acquire on y), but not z and x.  This is basically the same
> thing as the constructor+field write earlier in this thread - the LoadLoad
> in the reader is irrelevant since writer was reordered.
>
>
> On Wed, May 4, 2016 at 12:56 PM, thurstonn <thurston at nomagicsoftware.com>
> wrote:
>
>> Hans Boehm wrote
>> > On Wed, May 4, 2016 at 9:02 AM, Andrew Haley &lt;
>>
>> > aph@
>>
>> > &gt; wrote:
>> >
>> >
>> >>
>> >> StoreRelease is (LoadStore|StoreStore ; store)
>> >> LoadAcquire is (load ; LoadStore|LoadLoad)
>> >>
>> >>
>> > But only when that abstraction works :-)
>> >
>> > x = 1;
>> > y =release 1;
>> > z = 1;
>> >
>> > does not order the stores to x and z.  (Neither in theory nor in
>> > practice.)
>> >
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>>
>> > Concurrency-interest at .oswego
>>
>> > http://cs.oswego.edu/mailman/concurrency-interest
>>
>> Just to be clear, do you mean that (in another thread):
>> r1 = z
>> LoadLoad
>> r2 = x
>>
>> can result in r1 = 1, r2 = 0?
>> Surely not
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/About-putOrdered-and-its-meaning-tp13429p13440.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/b4d4a09d/attachment.html>

From peter.levart at gmail.com  Wed May  4 17:20:58 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 4 May 2016 23:20:58 +0200
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com> <1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com> <572A1D24.20109@redhat.com>
	<CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
Message-ID: <feb5817f-258b-c1cb-21b8-7442474bcfd2@gmail.com>



On 05/04/2016 07:35 PM, Hans Boehm wrote:
>
>
> On Wed, May 4, 2016 at 9:02 AM, Andrew Haley <aph at redhat.com 
> <mailto:aph at redhat.com>> wrote:
>
>     On 05/04/2016 03:40 PM, thurstonn wrote:
>     > I realize that I'm assuming that the barriers are emitted
>     *after* the
>     > respective memory actions, so above code becomes:
>     >  A global;
>     >
>     -----------------------------------------------------------------------
>     >     A a = <alloc>;                  |  A a = global;
>     >     a.x = 1                            | LoadLoad()
>     >     StoreStore()                     |   r1 = a.x;
>     >     global = a;                      |
>     >
>     > Maybe that assumption is wrong?
>
>     StoreRelease is (LoadStore|StoreStore ; store)
>     LoadAcquire is (load ; LoadStore|LoadLoad)
>
> But only when that abstraction works :-)
>
> x = 1;
> y =release 1;
> z = 1;
>
> does not order the stores to x and z.  (Neither in theory nor in 
> practice.)
>
> In the C++ model at least,
>
> Thread 1: y =release 2; x =release 1;
>
> Thread 2: x =release 2; y =release 1;
>
> allows a final state of x = y = 2. Memory_order_release doesn't mean 
> anything in the absence of a corresponding acquire or consume load. 
>  (Hardware implementations are unlikely to allow that; compiler 
> optimizations might.) Acquire/release make the "message passing" idiom 
> work, not much more than that.

Ok, but in the presence of store-release / load-acquire pairs, does a 
single such pair guarantee ordering of other relaxed load/stores that 
are in program order before store-release to be strictly before 
load/stores that are in program order after corresponding load-acquire. 
For example:

Thread1: construct an object graph with relaxed load/stores then publish 
the reference to data structure via store-release to 'global'

Thread2: load a reference from 'global' via load-acquire then use 
relaxed load/stores to read/modify the data structure navigated through 
the reference

Does this guarantee that:
- Thread2 sees all stores performed on data structure by Thread1 before 
publication
- Thread1 sees no modifications of data structure performed by Thread2 
after loading the reference to it


Or, very similar, but not quite the same:

Thread1: process some shared state with relaxed load/stores then 
store-release a true value into a 'global' flag (that was initially false)

Thread2: after observing 'global' flag read via load-acquire to be true, 
perform relaxed load/stores of the shared state

Does this guarantee that:
- Thread2 sees all stores performed on shared state by Thread1 before 
storing true to 'global'
- Thread1 sees no modifications of shared state performed by Thread2 
after loading true from 'global'

?

Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/98d85300/attachment-0001.html>

From peter.levart at gmail.com  Wed May  4 17:27:51 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 4 May 2016 23:27:51 +0200
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <feb5817f-258b-c1cb-21b8-7442474bcfd2@gmail.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com> <1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com> <572A1D24.20109@redhat.com>
	<CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
	<feb5817f-258b-c1cb-21b8-7442474bcfd2@gmail.com>
Message-ID: <4fa07d23-a5a4-9d2a-b49b-b1b4cbaf8a2e@gmail.com>



On 05/04/2016 11:20 PM, Peter Levart wrote:
>
>
>
> On 05/04/2016 07:35 PM, Hans Boehm wrote:
>>
>>
>> On Wed, May 4, 2016 at 9:02 AM, Andrew Haley <aph at redhat.com 
>> <mailto:aph at redhat.com>> wrote:
>>
>>     On 05/04/2016 03:40 PM, thurstonn wrote:
>>     > I realize that I'm assuming that the barriers are emitted
>>     *after* the
>>     > respective memory actions, so above code becomes:
>>     >  A global;
>>     >
>>     -----------------------------------------------------------------------
>>     >     A a = <alloc>;                  |  A a = global;
>>     >     a.x = 1                            | LoadLoad()
>>     >     StoreStore()                     |   r1 = a.x;
>>     >     global = a;                      |
>>     >
>>     > Maybe that assumption is wrong?
>>
>>     StoreRelease is (LoadStore|StoreStore ; store)
>>     LoadAcquire is (load ; LoadStore|LoadLoad)
>>
>> But only when that abstraction works :-)
>>
>> x = 1;
>> y =release 1;
>> z = 1;
>>
>> does not order the stores to x and z.  (Neither in theory nor in 
>> practice.)
>>
>> In the C++ model at least,
>>
>> Thread 1: y =release 2; x =release 1;
>>
>> Thread 2: x =release 2; y =release 1;
>>
>> allows a final state of x = y = 2. Memory_order_release doesn't mean 
>> anything in the absence of a corresponding acquire or consume load. 
>>  (Hardware implementations are unlikely to allow that; compiler 
>> optimizations might.) Acquire/release make the "message passing" 
>> idiom work, not much more than that.
>
> Ok, but in the presence of store-release / load-acquire pairs, does a 
> single such pair guarantee ordering of other relaxed load/stores that 
> are in program order before store-release to be strictly before 
> load/stores that are in program order after corresponding 
> load-acquire. For example:
>
> Thread1: construct an object graph with relaxed load/stores then 
> publish the reference to data structure via store-release to 'global'
>
> Thread2: load a reference from 'global' via load-acquire then use 
> relaxed load/stores to read/modify the data structure navigated 
> through the reference
>
> Does this guarantee that:
> - Thread2 sees all stores performed on data structure by Thread1 
> before publication
> - Thread1 sees no modifications of data structure performed by Thread2 
> after loading the reference to it
>
>
> Or, very similar, but not quite the same:
>
> Thread1: process some shared state with relaxed load/stores then 
> store-release a true value into a 'global' flag (that was initially false)
>
> Thread2: after observing 'global' flag read via load-acquire to be 
> true, perform relaxed load/stores of the shared state
>
> Does this guarantee that:
> - Thread2 sees all stores performed on shared state by Thread1 before 
> storing true to 'global'
> - Thread1 sees no modifications of shared state performed by Thread2 
> after loading true from 'global'
>
> ?
>
> Regards, Peter
>

Ok, I see this already answered by Aleksey. Is this true for Java 
VarHandles only or for C++ too?

Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/7cb7b0b9/attachment.html>

From boehm at acm.org  Wed May  4 17:52:46 2016
From: boehm at acm.org (Hans Boehm)
Date: Wed, 4 May 2016 14:52:46 -0700
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <4fa07d23-a5a4-9d2a-b49b-b1b4cbaf8a2e@gmail.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com>
	<572A1D24.20109@redhat.com>
	<CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
	<feb5817f-258b-c1cb-21b8-7442474bcfd2@gmail.com>
	<4fa07d23-a5a4-9d2a-b49b-b1b4cbaf8a2e@gmail.com>
Message-ID: <CAPUmR1axO_OKm60iqE+P300Nq-1GsyUi0-gZie0HFVvpz3wF4A@mail.gmail.com>

The answers are pretty consistent across Java, C, and C++ (and one or two
others, notably OpenCL). An acquire load guarantees that all memory effects
preceding the corresponding release store are visible (and none of the
memory affects following the acquire load are visible before the release
store). That's essentially all it guarantees. In my opinion, it's usually
best not to think in terms of fences, though fence-based thinking sometimes
exposes some useful rough intuitions.

There may be subtle differences/uncertainties as to what happens when an
acquire load actually sees the results of a later (in coherence order)
store that is not itself ordered. The C++ rules (see "release sequence")
predate a modern hardware understanding, and the Java memory model
description probably isn't as general as it now needs to be. But these are
relatively esoteric issues that typically don't matter.

On Wed, May 4, 2016 at 2:27 PM, Peter Levart <peter.levart at gmail.com> wrote:

>
>
> On 05/04/2016 11:20 PM, Peter Levart wrote:
>
>
>
> On 05/04/2016 07:35 PM, Hans Boehm wrote:
>
>
>
> On Wed, May 4, 2016 at 9:02 AM, Andrew Haley < <aph at redhat.com>
> aph at redhat.com> wrote:
>
>> On 05/04/2016 03:40 PM, thurstonn wrote:
>> > I realize that I'm assuming that the barriers are emitted *after* the
>> > respective memory actions, so above code becomes:
>> >  A global;
>> > -----------------------------------------------------------------------
>> >     A a = <alloc>;                  |  A a = global;
>> >     a.x = 1                            |  LoadLoad()
>> >     StoreStore()                     |   r1 = a.x;
>> >     global = a;                      |
>> >
>> > Maybe that assumption is wrong?
>>
>> StoreRelease is (LoadStore|StoreStore ; store)
>> LoadAcquire is (load ; LoadStore|LoadLoad)
>>
>>
> But only when that abstraction works :-)
>
> x = 1;
> y =release 1;
> z = 1;
>
> does not order the stores to x and z.  (Neither in theory nor in practice.)
>
> In the C++ model at least,
>
> Thread 1: y =release 2; x =release 1;
>
> Thread 2: x =release 2; y =release 1;
>
> allows a final state of x = y = 2.  Memory_order_release doesn't mean
> anything in the absence of a corresponding acquire or consume load.
>  (Hardware implementations are unlikely to allow that; compiler
> optimizations might.) Acquire/release make the "message passing" idiom
> work, not much more than that.
>
>
> Ok, but in the presence of store-release / load-acquire pairs, does a
> single such pair guarantee ordering of other relaxed load/stores that are
> in program order before store-release to be strictly before load/stores
> that are in program order after corresponding load-acquire. For example:
>
> Thread1: construct an object graph with relaxed load/stores then publish
> the reference to data structure via store-release to 'global'
>
> Thread2: load a reference from 'global' via load-acquire then use relaxed
> load/stores to read/modify the data structure navigated through the
> reference
>
> Does this guarantee that:
> - Thread2 sees all stores performed on data structure by Thread1 before
> publication
> - Thread1 sees no modifications of data structure performed by Thread2
> after loading the reference to it
>
>
> Or, very similar, but not quite the same:
>
> Thread1: process some shared state with relaxed load/stores then
> store-release a true value into a 'global' flag (that was initially false)
>
> Thread2: after observing 'global' flag read via load-acquire to be true,
> perform relaxed load/stores of the shared state
>
> Does this guarantee that:
> - Thread2 sees all stores performed on shared state by Thread1 before
> storing true to 'global'
> - Thread1 sees no modifications of shared state performed by Thread2 after
> loading true from 'global'
>
> ?
>
> Regards, Peter
>
>
> Ok, I see this already answered by Aleksey. Is this true for Java
> VarHandles only or for C++ too?
>
> Regards, Peter
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/1cd0c79f/attachment.html>

From thurston at nomagicsoftware.com  Wed May  4 18:55:47 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 4 May 2016 15:55:47 -0700 (MST)
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <572A2E68.4070403@oracle.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<572A1F56.9070609@oracle.com>
	<1462376224723-13436.post@n7.nabble.com>
	<572A2E68.4070403@oracle.com>
Message-ID: <1462402547674-13446.post@n7.nabble.com>

Right.

So given that acquire/releases are not part of the total synchronization
order:

 A global;
-----------------------------------------------------------------------
     A a = <alloc>;                  |  A a = getAcquire(global);
      a.x = 1                           |   r1 = a.x;
      putRelease(global, a);         


and let's assume there are 2 reader/acquire threads, and they execute  in
absolute time in the following order:

Writer 
Reader A (r1 = 1)
. . .
Reader B (a is null)

At least in theory this should be possible?




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/About-putOrdered-and-its-meaning-tp13429p13446.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From boehm at acm.org  Wed May  4 20:38:28 2016
From: boehm at acm.org (Hans Boehm)
Date: Wed, 4 May 2016 17:38:28 -0700
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <1462402547674-13446.post@n7.nabble.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<572A1F56.9070609@oracle.com>
	<1462376224723-13436.post@n7.nabble.com>
	<572A2E68.4070403@oracle.com>
	<1462402547674-13446.post@n7.nabble.com>
Message-ID: <CAPUmR1aPLtsYephVBaLOsPsxrKM+_RUYq5vfn2bpy5zqxC=9MQ@mail.gmail.com>

"absolute time" is not a well-defined notion.  But I think the answer you
want here is "yes".

Returning to another well-known litmus test:

Thread 1:
x =release 1;

Thread 2:
y =release 1;

Thread 3:
r1 =acquire x;
r2 =acquire y;

Thread 4:
r3 =acquire y;
r4 =acquire x;

r1 = r3 = 1; r2 = r4 = 0 is a possible outcome.

I.e. the release stores do not need to be observed in a consistent order.
And that's critical to the performance gain on Power and ARM.


On Wed, May 4, 2016 at 3:55 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> Right.
>
> So given that acquire/releases are not part of the total synchronization
> order:
>
>  A global;
> -----------------------------------------------------------------------
>      A a = <alloc>;                  |  A a = getAcquire(global);
>       a.x = 1                           |   r1 = a.x;
>       putRelease(global, a);
>
>
> and let's assume there are 2 reader/acquire threads, and they execute  in
> absolute time in the following order:
>
> Writer
> Reader A (r1 = 1)
> . . .
> Reader B (a is null)
>
> At least in theory this should be possible?
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/About-putOrdered-and-its-meaning-tp13429p13446.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/f936bdf5/attachment-0001.html>

From vitalyd at gmail.com  Wed May  4 20:56:27 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 4 May 2016 20:56:27 -0400
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <1462402547674-13446.post@n7.nabble.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<572A1F56.9070609@oracle.com>
	<1462376224723-13436.post@n7.nabble.com>
	<572A2E68.4070403@oracle.com>
	<1462402547674-13446.post@n7.nabble.com>
Message-ID: <CAHjP37FVhM_6iLofy=-8u3roEEqDiHm3v-ycpsfo3wXhJ6qoBA@mail.gmail.com>

On Wednesday, May 4, 2016, thurstonn <thurston at nomagicsoftware.com> wrote:

> Right.
>
> So given that acquire/releases are not part of the total synchronization
> order:
>
>  A global;
> -----------------------------------------------------------------------
>      A a = <alloc>;                  |  A a = getAcquire(global);
>       a.x = 1                           |   r1 = a.x;
>       putRelease(global, a);
>
>
> and let's assume there are 2 reader/acquire threads, and they execute  in
> absolute time in the following order:
>
> Writer
> Reader A (r1 = 1)
> . . .
> Reader B (a is null)
>
> At least in theory this should be possible?

Yes, that's correct - there's no total order.  In C++11 the seq_cst memory
order would be required to prevent this (but it's the most expensive
ordering).

>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/About-putOrdered-and-its-meaning-tp13429p13446.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <javascript:;>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160504/77084a51/attachment.html>

From aph at redhat.com  Thu May  5 03:39:20 2016
From: aph at redhat.com (Andrew Haley)
Date: Thu, 5 May 2016 08:39:20 +0100
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com> <1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com> <572A1D24.20109@redhat.com>
	<CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
Message-ID: <572AF8A8.4000601@redhat.com>

On 04/05/16 18:35, Hans Boehm wrote:
> On Wed, May 4, 2016 at 9:02 AM, Andrew Haley <aph at redhat.com> wrote:
> 
>> > On 05/04/2016 03:40 PM, thurstonn wrote:
>>> > > I realize that I'm assuming that the barriers are emitted *after* the
>>> > > respective memory actions, so above code becomes:
>>> > >  A global;
>>> > > -----------------------------------------------------------------------
>>> > >     A a = <alloc>;                  |  A a = global;
>>> > >     a.x = 1                            |  LoadLoad()
>>> > >     StoreStore()                     |   r1 = a.x;
>>> > >     global = a;                      |
>>> > >
>>> > > Maybe that assumption is wrong?
>> >
>> > StoreRelease is (LoadStore|StoreStore ; store)
>> > LoadAcquire is (load ; LoadStore|LoadLoad)
>> >
>> >
> But only when that abstraction works :-)
> 
> x = 1;
> y =release 1;
> z = 1;

Aww, I should have said "is approximately".

One of the problems with HotSpot's C2 complier is that it does not
model the JMM internally with happens-before: instead, it inserts
fences immediately after parsing.  All of this works pretty well on
TSO machines, but getting good code on AArch64 has been a challenge.
I would love to fix this, but the complexities of replacing all the
delicate code make things rather difficult.

Andrew.

From gergg at cox.net  Thu May  5 16:32:00 2016
From: gergg at cox.net (Gregg Wonderly)
Date: Thu, 5 May 2016 15:32:00 -0500
Subject: [concurrency-interest] About putOrdered and its meaning
In-Reply-To: <qMuF1s00j02hR0p01MuHrS>
References: <CAJp3eRBU8M2dGxvqaeTPR8Vr6PV_2mYmWU4QQ8=TVEsFRxcqsw@mail.gmail.com>
	<572A024B.9090101@oracle.com>
	<1462371737417-13432.post@n7.nabble.com>
	<1462372851737-13433.post@n7.nabble.com>
	<572A1D24.20109@redhat.com>
	<CAPUmR1aCT-o15eyxadENXcEG3wQp4HhbZsDWMmgD9cJDvNMGHA@mail.gmail.com>
	<feb5817f-258b-c1cb-21b8-7442474bcfd2@gmail.com>
	<4fa07d23-a5a4-9d2a-b49b-b1b4cbaf8a2e@gmail.com>
	<qMuF1s00j02hR0p01MuHrS>
Message-ID: <8A029E8A-BA78-43A0-A05C-B5B8D3F577AB@cox.net>

Its really interesting to see that the early work on Dataflow architectures, some 30 years ago is starting to appear in conversations like this.  Relating the computational paths of individual values is convenient for our brains, but convoluted for hardware which has become focused on parallelization.  

I am still firmly committed to totally ordered execution for languages which depict such relationships with code structure.

But when I was working with some of the AT&T EMSP investigators in college in the '80s, I was hoping that we would eventually get to nothing but Dataflow based programming where complex objects were considered single values with hardware support for them instead of hardware support for only words or pages.

Right now the hardware and the software systems are struggling for a unifying concept which reaches beyond the current "memory" concepts expressed in each. We struggle with very different viewpoints on what exactly a value is and exactly how to manage the view that the software needs vs the view that the hardware provides.

Instruction level management of details which are actually storage attributes (volatile, shared etc) grates pretty hard against the software concepts.

Gregg

Sent from my iPhone

> On May 4, 2016, at 4:52 PM, Hans Boehm <boehm at acm.org> wrote:
> 
> The answers are pretty consistent across Java, C, and C++ (and one or two others, notably OpenCL). An acquire load guarantees that all memory effects preceding the corresponding release store are visible (and none of the memory affects following the acquire load are visible before the release store). That's essentially all it guarantees. In my opinion, it's usually best not to think in terms of fences, though fence-based thinking sometimes exposes some useful rough intuitions.
> 
> There may be subtle differences/uncertainties as to what happens when an acquire load actually sees the results of a later (in coherence order) store that is not itself ordered. The C++ rules (see "release sequence") predate a modern hardware understanding, and the Java memory model description probably isn't as general as it now needs to be. But these are relatively esoteric issues that typically don't matter.
> 
>> On Wed, May 4, 2016 at 2:27 PM, Peter Levart <peter.levart at gmail.com> wrote:
>> 
>> 
>>> On 05/04/2016 11:20 PM, Peter Levart wrote:
>>> 
>>> 
>>>> On 05/04/2016 07:35 PM, Hans Boehm wrote:
>>>> 
>>>> 
>>>>> On Wed, May 4, 2016 at 9:02 AM, Andrew Haley <aph at redhat.com> wrote:
>>>>> On 05/04/2016 03:40 PM, thurstonn wrote:
>>>>> > I realize that I'm assuming that the barriers are emitted *after* the
>>>>> > respective memory actions, so above code becomes:
>>>>> >  A global;
>>>>> > -----------------------------------------------------------------------
>>>>> >     A a = <alloc>;                  |  A a = global;
>>>>> >     a.x = 1                            |  LoadLoad()
>>>>> >     StoreStore()                     |   r1 = a.x;
>>>>> >     global = a;                      |
>>>>> >
>>>>> > Maybe that assumption is wrong?
>>>>> 
>>>>> StoreRelease is (LoadStore|StoreStore ; store)
>>>>> LoadAcquire is (load ; LoadStore|LoadLoad)
>>>>  
>>>> But only when that abstraction works :-)
>>>> 
>>>> x = 1;
>>>> y =release 1;
>>>> z = 1;
>>>> 
>>>> does not order the stores to x and z.  (Neither in theory nor in practice.)
>>>> 
>>>> In the C++ model at least,
>>>> 
>>>> Thread 1: y =release 2; x =release 1;
>>>> 
>>>> Thread 2: x =release 2; y =release 1;
>>>> 
>>>> allows a final state of x = y = 2.  Memory_order_release doesn't mean anything in the absence of a corresponding acquire or consume load.  (Hardware implementations are unlikely to allow that; compiler optimizations might.) Acquire/release make the "message passing" idiom work, not much more than that.
>>> 
>>> Ok, but in the presence of store-release / load-acquire pairs, does a single such pair guarantee ordering of other relaxed load/stores that are in program order before store-release to be strictly before load/stores that are in program order after       corresponding load-acquire. For example:
>>> 
>>> Thread1: construct an object graph with relaxed load/stores then publish the reference to data structure via store-release to 'global'
>>> 
>>> Thread2: load a reference from 'global' via load-acquire then use relaxed load/stores to read/modify the data structure navigated through the reference
>>> 
>>> Does this guarantee that:
>>> - Thread2 sees all stores performed on data structure by Thread1 before publication
>>> - Thread1 sees no modifications of data structure performed by Thread2 after loading the reference to it
>>> 
>>> 
>>> Or, very similar, but not quite the same:
>>> 
>>> Thread1: process some shared state with relaxed load/stores then store-release a true value into a 'global' flag (that was initially false)
>>> 
>>> Thread2: after observing 'global' flag read via load-acquire to be true, perform relaxed load/stores of the shared state
>>> 
>>> Does this guarantee that:
>>> - Thread2 sees all stores performed on shared state by Thread1 before storing true to 'global'
>>> - Thread1 sees no modifications of shared state performed by Thread2 after loading true from 'global'
>>> 
>>> ?
>>> 
>>> Regards, Peter
>> 
>> Ok, I see this already answered by Aleksey. Is this true for Java VarHandles only or for C++ too?
>> 
>> Regards, Peter
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160505/acb4f9b2/attachment.html>

From pavel.rappo at gmail.com  Wed May 11 06:42:43 2016
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Wed, 11 May 2016 11:42:43 +0100
Subject: [concurrency-interest] Exceptions in an async code
Message-ID: <CAChcVun_jK7RLexCmogm40kdewDAeRVVk-ghTPsQhS-L4jXZkw@mail.gmail.com>

Hello,

Does anybody have any experience with designing exception contracts for
signatures like this?

    CompletableFuture<E> someMethod(T1 argOfTypeT1, ... Tn
argOfTypeTn) throws X;

As I understand, there is a generally accepted idea on throwing exceptions from
methods, which basically says that an exception should generally be thrown as
early as possible. It seems to work fine with an "ordinary code", or synchronous
for that matter.

With a CompletableFuture we have a bit of a different situation. Not only it
provides its own channel for exceptions, which are not thrown to, but passed to
as arguments, it also allows an ultimate deferral. That is, if not checked for,
an exception might stay unobserved forever.

I heard an opinion that once one goes with methods like the above, one should
better go the full way and relay _all_ exceptions through the returned CF. The
rationale behind this is that this way one will only have a single place to deal
with a failure thus would be able handle it in a more robust way.

This sounds very rational. But as usual, the devil is in the detail. What about
all these NullPointerException, IllegalArgumentException, etc.?

For instance, java.util.concurrent.Flow.Subscription.request _relay_ the
exception to another method, rather than throws it in-place:

    /**
     * Adds the given number {@code n} of items to the current
     * unfulfilled demand for this subscription.  If {@code n} is
     * negative, the Subscriber will receive an {@code onError}
     * signal with an {@link IllegalArgumentException} argument.
    ...
    public void request(long n);

On the other hand,
java.util.concurrent.ExecutorService.submit(java.util.concurrent.Callable<T>)
throws exceptions (related to the executor itself) in-place.

    ...
     * @throws RejectedExecutionException if the task cannot be
     *         scheduled for execution
     * @throws NullPointerException if the task is null
     */
    <T> Future<T> submit(Callable<T> task);

Is there any comparative analysis available on the web, or known pitfalls of
going one way or another?

Thanks,
-Pavel

From vitalyd at gmail.com  Wed May 11 07:58:07 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 11 May 2016 07:58:07 -0400
Subject: [concurrency-interest] Exceptions in an async code
In-Reply-To: <CAChcVun_jK7RLexCmogm40kdewDAeRVVk-ghTPsQhS-L4jXZkw@mail.gmail.com>
References: <CAChcVun_jK7RLexCmogm40kdewDAeRVVk-ghTPsQhS-L4jXZkw@mail.gmail.com>
Message-ID: <CAHjP37GQnBRpk1CRMB49HzxMmTqnFE4RS3_uxS5_YRHU0jXA3Q@mail.gmail.com>

I've seen both approaches (all exceptions signaled via futures vs hybrid).
I'm on the fence regarding runtime exceptions such as NPE, IAE, etc that
flat out violate an API's contract (passing a null arg, passing an out of
range parameter, etc).  Signaling those exceptions via a future may result
in significant "distance" between where the exception occurred and where
it's checked, and of course can possibly go unobserved entirely, as you
said.  In some ways, throwing those types of exceptions inline is better.

What I *do* find very annoying is when the same error condition is signaled
sometimes synchronously and other times asynchronously.  For instance,
initiating an I/O operation against a socket.  I've seen APIs that will
throw an inline exception if the socket is already closed, but if the
socket is closed while the I/O is still pending, it'll signal that via the
future.  You end up with the same handler logic for both cases, which is
very annoying.  So for these types of cases, I prefer signaling the error
via the future.  I guess the generalization is "if the error condition can
occur while the async operation is in flight but also immediately, always
signal via the future".

So for me, non-runtime exceptions should always be signaled via the future,
even if the future completes with the error immediately.  Runtime
exceptions I can see both ways, but would likely prefer immediate feedback
via an inline exception.



On Wed, May 11, 2016 at 6:42 AM, Pavel Rappo <pavel.rappo at gmail.com> wrote:

> Hello,
>
> Does anybody have any experience with designing exception contracts for
> signatures like this?
>
>     CompletableFuture<E> someMethod(T1 argOfTypeT1, ... Tn
> argOfTypeTn) throws X;
>
> As I understand, there is a generally accepted idea on throwing exceptions
> from
> methods, which basically says that an exception should generally be thrown
> as
> early as possible. It seems to work fine with an "ordinary code", or
> synchronous
> for that matter.
>
> With a CompletableFuture we have a bit of a different situation. Not only
> it
> provides its own channel for exceptions, which are not thrown to, but
> passed to
> as arguments, it also allows an ultimate deferral. That is, if not checked
> for,
> an exception might stay unobserved forever.
>
> I heard an opinion that once one goes with methods like the above, one
> should
> better go the full way and relay _all_ exceptions through the returned CF.
> The
> rationale behind this is that this way one will only have a single place
> to deal
> with a failure thus would be able handle it in a more robust way.
>
> This sounds very rational. But as usual, the devil is in the detail. What
> about
> all these NullPointerException, IllegalArgumentException, etc.?
>
> For instance, java.util.concurrent.Flow.Subscription.request _relay_ the
> exception to another method, rather than throws it in-place:
>
>     /**
>      * Adds the given number {@code n} of items to the current
>      * unfulfilled demand for this subscription.  If {@code n} is
>      * negative, the Subscriber will receive an {@code onError}
>      * signal with an {@link IllegalArgumentException} argument.
>     ...
>     public void request(long n);
>
> On the other hand,
>
> java.util.concurrent.ExecutorService.submit(java.util.concurrent.Callable<T>)
> throws exceptions (related to the executor itself) in-place.
>
>     ...
>      * @throws RejectedExecutionException if the task cannot be
>      *         scheduled for execution
>      * @throws NullPointerException if the task is null
>      */
>     <T> Future<T> submit(Callable<T> task);
>
> Is there any comparative analysis available on the web, or known pitfalls
> of
> going one way or another?
>
> Thanks,
> -Pavel
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160511/01a7eda4/attachment.html>

From oleksandr.otenko at gmail.com  Wed May 11 09:14:24 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 11 May 2016 14:14:24 +0100
Subject: [concurrency-interest] Exceptions in an async code
In-Reply-To: <CAChcVun_jK7RLexCmogm40kdewDAeRVVk-ghTPsQhS-L4jXZkw@mail.gmail.com>
References: <CAChcVun_jK7RLexCmogm40kdewDAeRVVk-ghTPsQhS-L4jXZkw@mail.gmail.com>
Message-ID: <3F0CFB67-4FDB-40D9-9540-B515BD192FB8@gmail.com>

Once you are into Futures, you are essentially into continuation-passing. Your code is cleaner if you always consider E someMethod(T1 argOfTypeT1, ? Tn argOfTypeTn) throws X; to be a continuation-passing transform of CompletableFuture<Either<X,E>> someMethod(T1 argOfTypeT1, ? Tn argOfTypeTn); (and vice versa). If you code like that, you never ?forget? to check for exceptions - they are just part of what the CompletableFuture type means. But, of course, it isn?t coded like that.

Making sure all runtime exceptions propagate to the consumer of the Future is more consistent. If they are thrown to the caller, it is potentially leaking abstraction, but in the wrong direction, which is worse than leaking abstractions in the right direction.

Alex

> On 11 May 2016, at 11:42, Pavel Rappo <pavel.rappo at gmail.com> wrote:
> 
> Hello,
> 
> Does anybody have any experience with designing exception contracts for
> signatures like this?
> 
>    CompletableFuture<E> someMethod(T1 argOfTypeT1, ... Tn
> argOfTypeTn) throws X;
> 
> As I understand, there is a generally accepted idea on throwing exceptions from
> methods, which basically says that an exception should generally be thrown as
> early as possible. It seems to work fine with an "ordinary code", or synchronous
> for that matter.
> 
> With a CompletableFuture we have a bit of a different situation. Not only it
> provides its own channel for exceptions, which are not thrown to, but passed to
> as arguments, it also allows an ultimate deferral. That is, if not checked for,
> an exception might stay unobserved forever.
> 
> I heard an opinion that once one goes with methods like the above, one should
> better go the full way and relay _all_ exceptions through the returned CF. The
> rationale behind this is that this way one will only have a single place to deal
> with a failure thus would be able handle it in a more robust way.
> 
> This sounds very rational. But as usual, the devil is in the detail. What about
> all these NullPointerException, IllegalArgumentException, etc.?
> 
> For instance, java.util.concurrent.Flow.Subscription.request _relay_ the
> exception to another method, rather than throws it in-place:
> 
>    /**
>     * Adds the given number {@code n} of items to the current
>     * unfulfilled demand for this subscription.  If {@code n} is
>     * negative, the Subscriber will receive an {@code onError}
>     * signal with an {@link IllegalArgumentException} argument.
>    ...
>    public void request(long n);
> 
> On the other hand,
> java.util.concurrent.ExecutorService.submit(java.util.concurrent.Callable<T>)
> throws exceptions (related to the executor itself) in-place.
> 
>    ...
>     * @throws RejectedExecutionException if the task cannot be
>     *         scheduled for execution
>     * @throws NullPointerException if the task is null
>     */
>    <T> Future<T> submit(Callable<T> task);
> 
> Is there any comparative analysis available on the web, or known pitfalls of
> going one way or another?
> 
> Thanks,
> -Pavel
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From viktor.klang at gmail.com  Wed May 11 13:35:30 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 11 May 2016 13:35:30 -0400
Subject: [concurrency-interest] Exceptions in an async code
In-Reply-To: <3F0CFB67-4FDB-40D9-9540-B515BD192FB8@gmail.com>
References: <CAChcVun_jK7RLexCmogm40kdewDAeRVVk-ghTPsQhS-L4jXZkw@mail.gmail.com>
	<3F0CFB67-4FDB-40D9-9540-B515BD192FB8@gmail.com>
Message-ID: <CANPzfU_euDo7oc1YD=xfcGvh1=hWaofU+Jfh_LJ1rUyLjQ7JeQ@mail.gmail.com>

Also, if you want to shield the calling code for spurious exceptions, you
can always invoke the method within a future and then do a
thenComposeAsync(identity)

On Wed, May 11, 2016 at 9:14 AM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> Once you are into Futures, you are essentially into continuation-passing.
> Your code is cleaner if you always consider E someMethod(T1 argOfTypeT1, ?
> Tn argOfTypeTn) throws X; to be a continuation-passing transform of
> CompletableFuture<Either<X,E>> someMethod(T1 argOfTypeT1, ? Tn
> argOfTypeTn); (and vice versa). If you code like that, you never ?forget?
> to check for exceptions - they are just part of what the CompletableFuture
> type means. But, of course, it isn?t coded like that.
>
> Making sure all runtime exceptions propagate to the consumer of the Future
> is more consistent. If they are thrown to the caller, it is potentially
> leaking abstraction, but in the wrong direction, which is worse than
> leaking abstractions in the right direction.
>
> Alex
>
> > On 11 May 2016, at 11:42, Pavel Rappo <pavel.rappo at gmail.com> wrote:
> >
> > Hello,
> >
> > Does anybody have any experience with designing exception contracts for
> > signatures like this?
> >
> >    CompletableFuture<E> someMethod(T1 argOfTypeT1, ... Tn
> > argOfTypeTn) throws X;
> >
> > As I understand, there is a generally accepted idea on throwing
> exceptions from
> > methods, which basically says that an exception should generally be
> thrown as
> > early as possible. It seems to work fine with an "ordinary code", or
> synchronous
> > for that matter.
> >
> > With a CompletableFuture we have a bit of a different situation. Not
> only it
> > provides its own channel for exceptions, which are not thrown to, but
> passed to
> > as arguments, it also allows an ultimate deferral. That is, if not
> checked for,
> > an exception might stay unobserved forever.
> >
> > I heard an opinion that once one goes with methods like the above, one
> should
> > better go the full way and relay _all_ exceptions through the returned
> CF. The
> > rationale behind this is that this way one will only have a single place
> to deal
> > with a failure thus would be able handle it in a more robust way.
> >
> > This sounds very rational. But as usual, the devil is in the detail.
> What about
> > all these NullPointerException, IllegalArgumentException, etc.?
> >
> > For instance, java.util.concurrent.Flow.Subscription.request _relay_ the
> > exception to another method, rather than throws it in-place:
> >
> >    /**
> >     * Adds the given number {@code n} of items to the current
> >     * unfulfilled demand for this subscription.  If {@code n} is
> >     * negative, the Subscriber will receive an {@code onError}
> >     * signal with an {@link IllegalArgumentException} argument.
> >    ...
> >    public void request(long n);
> >
> > On the other hand,
> >
> java.util.concurrent.ExecutorService.submit(java.util.concurrent.Callable<T>)
> > throws exceptions (related to the executor itself) in-place.
> >
> >    ...
> >     * @throws RejectedExecutionException if the task cannot be
> >     *         scheduled for execution
> >     * @throws NullPointerException if the task is null
> >     */
> >    <T> Future<T> submit(Callable<T> task);
> >
> > Is there any comparative analysis available on the web, or known
> pitfalls of
> > going one way or another?
> >
> > Thanks,
> > -Pavel
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160511/57e8528e/attachment.html>

From dl at cs.oswego.edu  Mon May 16 15:54:26 2016
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 16 May 2016 15:54:26 -0400
Subject: [concurrency-interest] list outage
Message-ID: <573A2572.9000702@cs.oswego.edu>


Sorry that this list was out after a hardware failure late friday.
Things should be back to normal now, although there could still
be a few glitches due to upgrading Mailman and running it on a
different mailer (postfix) while we had to rebuild anyway.

-Doug

From dl at cs.oswego.edu  Sat May 21 10:50:57 2016
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 21 May 2016 10:50:57 -0400
Subject: [concurrency-interest] list outage
Message-ID: <574075D1.9070301@cs.oswego.edu>


Sorry that this list was out after a hardware failure.
Things should be back to normal now, although there could still
be a few glitches due to upgrading Mailman and running it on a
different mailer (postfix) while we had to rebuild anyway.

-Doug

From thurston at nomagicsoftware.com  Mon May 30 10:40:56 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 30 May 2016 07:40:56 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict* Total
	Order?
Message-ID: <1464619256580-13457.post@n7.nabble.com>

In the JMM documentation, synchronization order is defined as a total order;
perhaps it's just semantics, but I've always been curious whether it is
intentional that it isn't defined as a *strict* total order.

Without getting into arcane mathematics, I guess what I'm asking is:

Given a program that contains only synchronization actions, but they are
independent, e.g.

volatile int x, y

Thread A       Thread B
x = 1             y = 1

Is the following true?

I. *Every* execution of the program has only one and exactly one valid
(accurate?) history?

[either

A(x=1)
B(y=1)

or 

B(y=1)
A(x=1)
]  

II. or is it possible that it can be said that for some theoretical
execution, both histories are equally valid?

My inference has always been that the JMM prescribes I (which implies strict
total order) and proscribes II.

Of course, one might say, who cares? or how could you tell the difference?
or the "results" of the program are the same . . .








--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From joe.bowbeer at gmail.com  Tue May 31 02:43:48 2016
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 30 May 2016 23:43:48 -0700
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464619256580-13457.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
Message-ID: <CAHzJPEqPAfDW1jJXfP3X=g0Vnd+=8UTKpbP02F8ZNPL4rSHCVw@mail.gmail.com>

The JMM is written to guarantee sequential consistency for data-race-free
programs. As I understand it, sequential consistency (SC) requires that
every execution *could* be the result of *some* sequential execution. But,
in general, multiple possible sequential executions may exist.

Do you have a different understanding of SC? Or are you questioning why SC
was chosen for JMM?

On Mon, May 30, 2016 at 7:40 AM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> In the JMM documentation, synchronization order is defined as a total
> order;
> perhaps it's just semantics, but I've always been curious whether it is
> intentional that it isn't defined as a *strict* total order.
>
> Without getting into arcane mathematics, I guess what I'm asking is:
>
> Given a program that contains only synchronization actions, but they are
> independent, e.g.
>
> volatile int x, y
>
> Thread A       Thread B
> x = 1             y = 1
>
> Is the following true?
>
> I. *Every* execution of the program has only one and exactly one valid
> (accurate?) history?
>
> [either
>
> A(x=1)
> B(y=1)
>
> or
>
> B(y=1)
> A(x=1)
> ]
>
> II. or is it possible that it can be said that for some theoretical
> execution, both histories are equally valid?
>
> My inference has always been that the JMM prescribes I (which implies
> strict
> total order) and proscribes II.
>
> Of course, one might say, who cares? or how could you tell the difference?
> or the "results" of the program are the same . . .
>
>
>
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160530/0a5a2d6b/attachment.html>

From peter.levart at gmail.com  Tue May 31 06:44:09 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 31 May 2016 12:44:09 +0200
Subject: [concurrency-interest] LinearProbeHashtable
Message-ID: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>

Hi concurrency experts,

As part of an attempt to optimize the ClassValue API for small number of 
elements per Class in OpenJDK, I came up with a proposal to leverage 
ConcurrentHashMap, which improves the footprint overhead considerably 
(has ~ 1/2 footprint overhead of current ClassValue), simplifies 
ClassValue implementation and is only barely slower with lookups than 
existing ClassValue implementation. Here's the code if anyone is 
interested to see it:

http://cr.openjdk.java.net/~plevart/misc/ClassValue.Alternative2/webrev.02/

But a lookup slowdown of about 10-15 % is not acceptable, they say. So I 
created a simple specialized map-like construct to replace CHM in this 
ClassValue implementation - LinearProbeHashtable which improves lookup 
performance. As there was interest to use it elsewhere in the inner 
workings of JDK infrastructure, namely in MethodType interning support, 
to be able to use VarHandles for implementation of CHM without fear of 
circular dependencies, I though of presenting the code of 
LinearProbeHashtable on this list in hope that someone could spot any 
obvious issues with it:

http://cr.openjdk.java.net/~plevart/misc/ClassValue.Alternative2/webrev.04.2/src/java.base/share/classes/java/lang/LinearProbeHashtable.java.html

This is not a general-purpose Map implementation. It does not even 
implement the Map interface. The methods it has in common with Map 
interface should adhere to Map's specification though. The purpose of 
this implementation is:

- quick lock-free lookups
- compact footprint
- resonable for modifications

To be used in situations that are mostly lookup-based such as interning, 
caching, etc. Where the number of entries is not large and keys already 
implement good hash code(s) for power-of-2 length table(s).

So, do you see any issues with this implementation?

Regards, Peter


From thurston at nomagicsoftware.com  Tue May 31 05:52:52 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 31 May 2016 02:52:52 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464619256580-13457.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
Message-ID: <1464688372978-13460.post@n7.nabble.com>

Let me try another way:

Imagine we have the previously mentioned program:

volatile int x, y

Thread A       Thread B
x = 1             y = 1 

And we have two "observers" Bob and Jill; both of them have the magical
ability to observe all hardware state (registers, caches, memory, etc)
during the execution of any program *without interfering in said execution*

They record each execution of a program by writing down a history of what
they observed.

The above program is run a 1000 times; after each execution, Bob and Jill
compare their two histories; they are always in agreement.

For some executions, the history is:
A(x=1)
B(y=1)

for others, it's:
B(y=1)
A(x=1)

Again, Jill's and Bob's histories are always the same.

Then the program is executed for the 1001 time.
Again they compare histories:
Bob:
A(x=1)
B(y=1)

Jill:
B(y=1)
A(x=1)

Bob:  "Hmm, our Java platform is broken; this violates the JMM guarantee of
a total order among synchronization actions, i.e. it violates
synchronization order"

Jill:   "I'm not so sure; a total order (which is just a binary operator)
allows for A(x=1) <= B(y=1) AND B(y=1) <= A(x=1). i.e. it allows for A(x=1)
= B(y=1). Our Java platform is OK"

My question is:  who is right (according to the JMM)?







--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13460.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From rk at rkuhn.info  Tue May 31 08:39:16 2016
From: rk at rkuhn.info (Roland Kuhn)
Date: Tue, 31 May 2016 14:39:16 +0200
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464688372978-13460.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com>
Message-ID: <749920E2-861C-4AAD-B9D0-D88C3A44CFCB@rkuhn.info>

Your hypothetical observers cannot exist in this universe because the two writes can occur concurrently, with true parallelism: each core writes to its cache, there is no synchronization necessary and hence it is not done. Since the difference is not observable (due to special relativity if nothing else) the question is moot as far as I can see. The JMM only regulates observable behavior, and that is hard enough ;-)

Is there another question behind the one you are currently asking?

Regards, Roland 

Sent from my iPhone

> On 31 May 2016, at 11:52, thurstonn <thurston at nomagicsoftware.com> wrote:
> 
> Let me try another way:
> 
> Imagine we have the previously mentioned program:
> 
> volatile int x, y
> 
> Thread A       Thread B
> x = 1             y = 1 
> 
> And we have two "observers" Bob and Jill; both of them have the magical
> ability to observe all hardware state (registers, caches, memory, etc)
> during the execution of any program *without interfering in said execution*
> 
> They record each execution of a program by writing down a history of what
> they observed.
> 
> The above program is run a 1000 times; after each execution, Bob and Jill
> compare their two histories; they are always in agreement.
> 
> For some executions, the history is:
> A(x=1)
> B(y=1)
> 
> for others, it's:
> B(y=1)
> A(x=1)
> 
> Again, Jill's and Bob's histories are always the same.
> 
> Then the program is executed for the 1001 time.
> Again they compare histories:
> Bob:
> A(x=1)
> B(y=1)
> 
> Jill:
> B(y=1)
> A(x=1)
> 
> Bob:  "Hmm, our Java platform is broken; this violates the JMM guarantee of
> a total order among synchronization actions, i.e. it violates
> synchronization order"
> 
> Jill:   "I'm not so sure; a total order (which is just a binary operator)
> allows for A(x=1) <= B(y=1) AND B(y=1) <= A(x=1). i.e. it allows for A(x=1)
> = B(y=1). Our Java platform is OK"
> 
> My question is:  who is right (according to the JMM)?
> 
> 
> 
> 
> 
> 
> 
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13460.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aleksey.shipilev at oracle.com  Tue May 31 09:39:56 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 31 May 2016 16:39:56 +0300
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
 Total Order?
In-Reply-To: <1464688372978-13460.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com>
Message-ID: <574D942C.5060305@oracle.com>

On 05/31/2016 12:52 PM, thurstonn wrote:
> And we have two "observers" Bob and Jill; both of them have the magical
> ability to observe all hardware state (registers, caches, memory, etc)
> during the execution of any program *without interfering in said execution*

Your question boils down to: are Bob and Jill -- the CPU demons --
observe events "x=1" and "y=1" in the same relative order?

Since there is communication delay involved, you cannot say they have to
observe the events in any particular order. Or, that is to say, the
physical universe where machines exist does not force us to detect both
events in the same relative order. (Aside: this is similar how
introducing the no-faster-than-a-speed-of-light axiom in special
relativity gives raise to relativity of simultaneity itself)

But the actual question that programmer care of is, are *programs*
allowed to observe the different relative order? I.e. if we introduce
the actual reads in the program, do we see the different order of x=1
and y=1? IRIW under JMM says "no". And this is a critical thing: model
is described as behavior observed by *programs*, not by CPU demons.

Now, it is a machine problem how to map the actual physical sequence of
events towards the program-observable behaviors. Barriers and other HW
synchronization primitives are the ways to communicate where that
observable order matters.


> My question is:  who is right (according to the JMM)?

I think you have to get back to Lamport's definition of sequential
consistency and its difference against the strict consistency.

Notably, SC definition states "... the result of any execution is the
same *as if* [emphasis is mine] the operations of all the processors
were executed in some sequential order, and the operations of each
individual processor appear in this sequence in the order specified by
its program."

That "as if"-ness is a crucial part here: under SC-DRF, the outcome of
the data-race-free program is *as if* there was a total order. The exact
order in which the actions were executed is not required to be
consistent with that order (_strict_ consistency requires that).

Introducing external observers that can magically observe the "blurry"
interim state of the system does not violate SC property, because the
program outcome still stays the same.


Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160531/c0facc2d/attachment.sig>

From jw_list at headissue.com  Tue May 31 10:53:52 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Tue, 31 May 2016 16:53:52 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
Message-ID: <3825850.COdVxmxUDJ@tapsy>

Peter,

thanks for sharing this. It's a nice afternoon read :)

On Tuesday 31 May 2016 12:44:09 Peter Levart wrote:
> So, do you see any issues with this implementation?

I have a "non concurrency" related remark. It took me quite a while, to see whether the table
gets expanded when tombstones get to much. It does, probably. Maybe the code can be more 
readable here? E.g.:

         if (2 * capacity(inserted + 1) > curLen) {
            int newLen = 2 * capacity((inserted - tombstoned) * 3 / 2 + 1);

The numbers 2 and 3 repeat at other places, too, probably you mean always the same... When implementing hash tables,
I personally always like to have the load factor as parameter to use it in unit tests (and benchmarks!).

> - quick lock-free lookups

This only holds, if you don't have too much tombstones around. Right now, your worst case lookup speed will be much worse then
CHM. From the use case you describe probably its expected to have not much removals and updates. I would limit the number 
of tombstones and rehash if it is reached to have a better worst case scenario guarantee.

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From peter.levart at gmail.com  Tue May 31 12:50:53 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 31 May 2016 18:50:53 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <3825850.COdVxmxUDJ@tapsy>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy>
Message-ID: <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>

Hi Jens,

Thanks for taking a look. I'm happy to answer your comments...

On 05/31/2016 04:53 PM, Jens Wilke wrote:
> Peter,
>
> thanks for sharing this. It's a nice afternoon read :)
>
> On Tuesday 31 May 2016 12:44:09 Peter Levart wrote:
>> So, do you see any issues with this implementation?
> I have a "non concurrency" related remark. It took me quite a while, to see whether the table
> gets expanded when tombstones get to much. It does, probably. Maybe the code can be more
> readable here? E.g.:
>
>           if (2 * capacity(inserted + 1) > curLen) {
>              int newLen = 2 * capacity((inserted - tombstoned) * 3 / 2 + 1);

A tombstone is just a deleted entry (well a key, its corresponding value 
is cleared) - it occupies the slot. 'inserted' is used to track the 
number of entries in the table (including tombstones) and the table is 
rehashed (possibly resized too) when the load is about to grow beyond 2/3.
>
> The numbers 2 and 3 repeat at other places, too, probably you mean always the same... When implementing hash tables,
> I personally always like to have the load factor as parameter to use it in unit tests (and benchmarks!).

The only place where load factor is coded is in the capacity() method. 
The '2' in if (2 * capacity(inserted + 1) > curLen) above is to convert 
from capacity to table length (as two table slots are used for each 
key/value pair in the table). The 3/2 factor in the next line is just 
coincidentally equal to 1/loadFactor and has nothing to do with it. It 
is used as a kind of "hysteresis loop" that prevents oscillating around 
the tipping point (the point where capacity() function jumps by a factor 
of 2). The table is rehashed when, as mentioned, the number of inserted 
entries (including tombstoned entries) grows over loadFactor * capacity. 
Rehashing also gets rid of tombstones.

Imagine a table that contains just one tombstone when it is rehashed. if 
the new table length was computed simply as newLen = 2 * 
capacity(inserted - tombstoned + 1), the new table would have the same 
length as the old one and we would end up with new rehashed table with 
space for a single entry before another rehashing was necessary. The 3/2 
factor in newLen = 2 * capacity((inserted - tombstoned) * 3 / 2 + 1) 
guarantees that rehashing is not needed for at least another size()/2 + 
1 insertions. When this factor is 1 <= factor < 2, it does not affect 
normal growth when entries are just inserted and never removed.

>
>> - quick lock-free lookups
> This only holds, if you don't have too much tombstones around. Right now, your worst case lookup speed will be much worse then
> CHM. From the use case you describe probably its expected to have not much removals and updates. I would limit the number
> of tombstones and rehash if it is reached to have a better worst case scenario guarantee.

Tombstones don't worsen lookup speed compared to a table state before 
removing an entry that became a tombstone. They can actually improve 
lookup speed compared to a pre-remove state as they contain this 'skip' 
int field that tells how many slots to jump to get over the whole run of 
consecutive tombstones in one go. Planting the tombstone does not 
improve lookup as much as removing an entry right away would, of course. 
They are a necessary evil to enable lock-free lookups.

But your idea is a good one, thanks. Why would only insertions trigger 
rehashing. Removals could do that too. I'll think about the strategy...

Note: "load" in above sentences pertains to occupancy of the hash table 
- not the CPU.

>
> Cheers,
>
> Jens
>

Regards, Peter


From thurston at nomagicsoftware.com  Tue May 31 12:55:44 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 31 May 2016 09:55:44 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <574D942C.5060305@oracle.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
Message-ID: <1464713744101-13465.post@n7.nabble.com>

Quoting from the JLS:

17.4.4. Synchronization Order

"Every execution has a synchronization order. A synchronization order is a
total order over all of the synchronization actions of an execution. "

That seems like a categorical statement; no references to data-races,
sequential consistency, observability, program results, nor, God forbid,
special relativity (which isn't relevant since CPUs are not in relative
motion with each other). 

So for a program with only synchronization actions, the JMM requires some
binary operator (the total order) over each pair of actions in the program
(in this case, the set {A(x=1), B(y=1}).

What could that binary operator be?
In other words, what, within a program execution, does A(x=1) <= B(y=1)
mean?
Could CPUcost(action-x, action-y) be it?  I don't think so.

It seems to me, no matter how hard I might try to avoid it, it means the
numerical comparison of time(stamp) of each action, which presents
difficulty in precise definition (since, e.g. writes and reads are not
instantaneous).

The JMM authors had 2 choices (again when it comes to synchronization
actions):

No execution with simultaneous (A(x=1) = B(y=1)) actions are allowed (Bob's
understanding, and implied by a *strict* total order) or

Simultaneous (A(x=1) = B(y=1)) actions are allowed (Jill's understanding,
and implied by a *non-strict* total order)

I'm assuming from your reply that it's the latter (Jill's) meaning, but I
fail to see how any reasonable interpretation of the sentence I quoted above
could be:
"Well, since no threads observe any other thread's actions (true in this
case), then scratch the whole requirement about synchronization order" which
you also seem to be suggesting




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13465.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From joe.bowbeer at gmail.com  Tue May 31 14:21:25 2016
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 31 May 2016 11:21:25 -0700
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464713744101-13465.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com>
 <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
Message-ID: <CAHzJPEr5j9zrwJ9GrAV6usZf_B9JNxt-eS260Vyyt1NbjbK9KA@mail.gmail.com>

The program you are referring to can be obtained by making every field
volatile. However, unobservable synchronization actions, including volatile
reads and writes, can be optimized and/or elided.
On May 31, 2016 11:06 AM, "thurstonn" <thurston at nomagicsoftware.com> wrote:

> Quoting from the JLS:
>
> 17.4.4. Synchronization Order
>
> "Every execution has a synchronization order. A synchronization order is a
> total order over all of the synchronization actions of an execution. "
>
> That seems like a categorical statement; no references to data-races,
> sequential consistency, observability, program results, nor, God forbid,
> special relativity (which isn't relevant since CPUs are not in relative
> motion with each other).
>
> So for a program with only synchronization actions, the JMM requires some
> binary operator (the total order) over each pair of actions in the program
> (in this case, the set {A(x=1), B(y=1}).
>
> What could that binary operator be?
> In other words, what, within a program execution, does A(x=1) <= B(y=1)
> mean?
> Could CPUcost(action-x, action-y) be it?  I don't think so.
>
> It seems to me, no matter how hard I might try to avoid it, it means the
> numerical comparison of time(stamp) of each action, which presents
> difficulty in precise definition (since, e.g. writes and reads are not
> instantaneous).
>
> The JMM authors had 2 choices (again when it comes to synchronization
> actions):
>
> No execution with simultaneous (A(x=1) = B(y=1)) actions are allowed (Bob's
> understanding, and implied by a *strict* total order) or
>
> Simultaneous (A(x=1) = B(y=1)) actions are allowed (Jill's understanding,
> and implied by a *non-strict* total order)
>
> I'm assuming from your reply that it's the latter (Jill's) meaning, but I
> fail to see how any reasonable interpretation of the sentence I quoted
> above
> could be:
> "Well, since no threads observe any other thread's actions (true in this
> case), then scratch the whole requirement about synchronization order"
> which
> you also seem to be suggesting
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13465.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160531/c1c2c285/attachment.html>

From jw_list at headissue.com  Tue May 31 14:48:41 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Tue, 31 May 2016 20:48:41 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
Message-ID: <15269016.0KoCuDZ9Z7@tapsy>

On Tuesday 31 May 2016 18:50:53 Peter Levart wrote:

> The only place where load factor is coded is in the capacity() method. 
> The '2' in if (2 * capacity(inserted + 1) > curLen) above is to convert 
> from capacity to table length (as two table slots are used for each 
> key/value pair in the table). The 3/2 factor in the next line is just 
> coincidentally equal to 1/loadFactor and has nothing to do with it. It 
> is used as a kind of "hysteresis loop" that prevents oscillating around 
> the tipping point (the point where capacity() function jumps by a factor 
> of 2). The table is rehashed when, as mentioned, the number of inserted 
> entries (including tombstoned entries) grows over loadFactor * capacity. 
> Rehashing also gets rid of tombstones.
>
> Imagine a table that contains just one tombstone when it is rehashed. if 
> the new table length was computed simply as newLen = 2 * 
> capacity(inserted - tombstoned + 1), the new table would have the same 
> length as the old one and we would end up with new rehashed table with 
> space for a single entry before another rehashing was necessary. The 3/2 
> factor in newLen = 2 * capacity((inserted - tombstoned) * 3 / 2 + 1) 
> guarantees that rehashing is not needed for at least another size()/2 + 
> 1 insertions. When this factor is 1 <= factor < 2, it does not affect 
> normal growth when entries are just inserted and never removed.

Exactly, that is the strange thing. But, usually the load factor only determines _when_ 
to extend, you do not need it to calculate the next size. The next expansion 
size is all the time  tab.length * 2, since your size is a power of two, and you
do not insert more then one element at a time, so that there is a need to expand more.

The load factor is a "magic number". It is inside expand(), in checkSize() and in capacity().

I don't see a real need for the methods checkSize() and capacity() in that form. At least
for the current functionality without shrink support. Just an idea how this could look 
more obvious (hopefully...):

static final int SLOTS_PER_ENTRY = 2;
static final int LOAD_PERCENT = 66;
. . .
int maximumCapacity = tab.length / SLOTS_PER_ENTRY * LOAD_PERCENT / 100;
if (inserted > maximumCapactiy) {
  // needs expansion or tombstone cleanup!
  int newLen = tab.length;
  if (size() > maximumCapacity) {
    // needs expansion!
    newLen = tab.length * 2;
    if (newLen < 0) { throw new OutOfMemoryException("...."); }
  }
  Object[] newTab = new Object[newLen];
  // do the rehash
}

Interestingly your current code would also shrink the table, since
you calculate the next table length based on the real size. But its in the put() path :)

BTW:

   public int size() {
     synchronized (lock) {
       return inserted - tombstoned;
     }
   }

Why not have the size as variable, and calculate occupation = size + tombstone for the internal
purposes?

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From thurston at nomagicsoftware.com  Tue May 31 13:42:54 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 31 May 2016 10:42:54 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <CAHzJPEr5j9zrwJ9GrAV6usZf_B9JNxt-eS260Vyyt1NbjbK9KA@mail.gmail.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
 <CAHzJPEr5j9zrwJ9GrAV6usZf_B9JNxt-eS260Vyyt1NbjbK9KA@mail.gmail.com>
Message-ID: <1464716574039-13467.post@n7.nabble.com>

joe.bowbeer wrote
> The program you are referring to can be obtained by making every field
> volatile. However, unobservable synchronization actions, including
> volatile
> reads and writes, can be optimized and/or elided.
> On May 31, 2016 11:06 AM, "thurstonn" &lt;

> thurston@

> &gt; wrote:

Of course, hence the program (repeated here)
volatile int x, y

Thread A       Thread B
x = 1             y = 1 


Elision is an interesting case;  I interpret that with respect to
synchronization order as:

"A synchronization order is a total order over all of the synchronization
actions of an execution" where the synchronization actions (after elision)
is {}.
No problem there.
Optimization, viz. turning a volatile read/write into a plain one, and
therefore not a synchronization action, similarly reduces the execution's SA
set.





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13467.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From aleksey.shipilev at oracle.com  Tue May 31 15:21:04 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 31 May 2016 22:21:04 +0300
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
 Total Order?
In-Reply-To: <1464713744101-13465.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
Message-ID: <574DE420.1000502@oracle.com>

On 05/31/2016 07:55 PM, thurstonn wrote:
> "Every execution has a synchronization order. A synchronization order is a
> total order over all of the synchronization actions of an execution. "
> 
> That seems like a categorical statement; no references to data-races,
> sequential consistency, observability, program results, nor, God forbid,
> special relativity (which isn't relevant since CPUs are not in relative
> motion with each other). 

Notice there are also no references as to whether those actions are the
actual machine actions that Bob and Jill are supposed to observe. And
this is for reason: the program actions in JMM are abstract, and have
little connection with what real systems do. As long as runtime/hardware
can keep the appearances of satisfying JMM requirements, it can do whatever.

The point of my previous note was to highlight that: you are asking the
question what could happen on physical level, using the entities
(actions) on abstract model level. These have little in common. What
happens on physical level is all smoke and mirrors.


> So for a program with only synchronization actions, the JMM requires some
> binary operator (the total order) over each pair of actions in the program
> (in this case, the set {A(x=1), B(y=1}).
> 
> What could that binary operator be?
> In other words, what, within a program execution, does A(x=1) <= B(y=1)
> mean?
> Could CPUcost(action-x, action-y) be it?  I don't think so.
> 
> It seems to me, no matter how hard I might try to avoid it, it means the
> numerical comparison of time(stamp) of each action, which presents
> difficulty in precise definition (since, e.g. writes and reads are not
> instantaneous).

I think you are trying to drag the abstract notion of action towards its
physical manifestation (e.g. changes in machine state, or associated
ticks in a global time) -- a very dangerous and confusing route.


> The JMM authors had 2 choices (again when it comes to synchronization
> actions):
> 
> No execution with simultaneous (A(x=1) = B(y=1)) actions are allowed (Bob's
> understanding, and implied by a *strict* total order) or
> 
> Simultaneous (A(x=1) = B(y=1)) actions are allowed (Jill's understanding,
> and implied by a *non-strict* total order)

That equality sign is confusing.

I am probably forgetting a significant part of my training, but these
are the definitions that are consistent with my own memory:

(Weak) total order implies that for all A, B, C in set:
 a) Reflexive:     (A op A)
 b) Antisymmetric: (A op B) and (B op A) => (A = B)
 c) Transitive:    (A op B) and (B op C) => (A op C)
 d) Totality:      (A op B) or (B op A)

Strict total order implies that for all A, B, C in set:
 a) Irreflexive:   !(A op A)
 b) Asymmetric:    (A op B) => !(B op A)
 c) Transitive:    (A op B) and (B op C) => (A op C)
 d) Trichotomy:    Exactly one of (A op B), (B op A), (A = B) is true

Notice (A = B) reads as "A and B are the *same*", not "simultaneous".

The difference is in reflexivity. Does it matter if SO is reflexive or
not? I don't think so.

What does it mean to be simultaneous in SO? Does it mean there exist two
non-equal actions that are *not* ordered by SO relation? This
contradicts totality/trichotomy in either. Does it mean two non-equal
actions have SO in different orders? This breaks anti/asymmetry in either.

Thanks,
-Aleksey


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160531/428ef795/attachment-0001.sig>

From dragonsinth at gmail.com  Tue May 31 15:25:19 2016
From: dragonsinth at gmail.com (Scott Blum)
Date: Tue, 31 May 2016 15:25:19 -0400
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464716574039-13467.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com>
 <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
 <CAHzJPEr5j9zrwJ9GrAV6usZf_B9JNxt-eS260Vyyt1NbjbK9KA@mail.gmail.com>
 <1464716574039-13467.post@n7.nabble.com>
Message-ID: <CALuNCpjfZtw0AmkjPGn31nUzd9UCQaxdLDMOYxLVNYAc_hcVvw@mail.gmail.com>

I would posit that there is only a strict total order if you try to observe
it within the program.  If you add sufficient code to determine which
history actually happened, you will in effect force the instructions to be
generated to insist on a strict total ordering.  In the absence of that
extra code, it's meaningless to ask which happened first.

Imagine a hypothetical implementation where you have two threads running on
two processors each with their own data caches.  Each thread might either
write its x=1, y=1 to its local cache, or to main memory.  If they write to
local cache, there's no rules around when that needs to get pushed through
to main memory-- provided no one is doing a volatile read on those fields.
Without the volatile reads you're never forcing the system to reach an
absolutely "determined" state.

But I use all these terms loosely because there truly isn't a clear 1:1
mapping between JMM and hardware. The JMM is all about reads and writes and
visibility within a program; how and when things actually get written to
memory or cache or whatever is an implementation detail of a particular JVM
and machine.

TLDR: the JMM doesn't require a strict total ordering unless you try to
observe it.

On Tue, May 31, 2016 at 1:42 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> joe.bowbeer wrote
> > The program you are referring to can be obtained by making every field
> > volatile. However, unobservable synchronization actions, including
> > volatile
> > reads and writes, can be optimized and/or elided.
> > On May 31, 2016 11:06 AM, "thurstonn" &lt;
>
> > thurston@
>
> > &gt; wrote:
>
> Of course, hence the program (repeated here)
> volatile int x, y
>
> Thread A       Thread B
> x = 1             y = 1
>
>
> Elision is an interesting case;  I interpret that with respect to
> synchronization order as:
>
> "A synchronization order is a total order over all of the synchronization
> actions of an execution" where the synchronization actions (after elision)
> is {}.
> No problem there.
> Optimization, viz. turning a volatile read/write into a plain one, and
> therefore not a synchronization action, similarly reduces the execution's
> SA
> set.
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13467.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160531/b3c8a4cf/attachment.html>

From thurston at nomagicsoftware.com  Tue May 31 15:37:20 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 31 May 2016 12:37:20 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <574DE420.1000502@oracle.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
Message-ID: <1464723440028-13471.post@n7.nabble.com>

Aleksey Shipilev-2 wrote
> 
> I am probably forgetting a significant part of my training, but these
> are the definitions that are consistent with my own memory:
> 
> (Weak) total order implies that for all A, B, C in set:
>  a) Reflexive:     (A op A)
>  b) Antisymmetric: (A op B) and (B op A) => (A = B)
>  c) Transitive:    (A op B) and (B op C) => (A op C)
>  d) Totality:      (A op B) or (B op A)
> 
> Strict total order implies that for all A, B, C in set:
>  a) Irreflexive:   !(A op A)
>  b) Asymmetric:    (A op B) => !(B op A)
>  c) Transitive:    (A op B) and (B op C) => (A op C)
>  d) Trichotomy:    Exactly one of (A op B), (B op A), (A = B) is true
> 
> Notice (A = B) reads as "A and B are the *same*", not "simultaneous".

Ah, I think that last statement might be the rub; my understanding is that A
= B does not mean A and B are the *same*, the irreflexive rule deals with
that
"=" is defined by the total order.
The phraseology is extremely awkward; let's replace total order with "binary
operator" (subject to the rules you've enumerated), and let's dispense with
(ir)reflexivity, as I agree it's completely unnecessary to speak of ordering
a SA with respect to itself.

"=" is analogous to equals() vis a vis Java identity (==) (at least as I
understand it)

Say I have a set of laptops {Laptop-A, Laptop-B}, 2 physically distinct
laptops.
It is meaningless to speak of a total order over that set.
I have to define a binary operator, right?

Say, weight-of-laptop.
In such a case, I can say weight-of-laptop is a total order over {Laptop-A,
Laptop-B}; it's not difficult to interpret "=" in that case.

Now my understanding is that weight-of-laptop is *not* a strict total order
over say {Laptop-A 1.1kg, Laptop-B 1.1kg}
Because in order to be a total order, either Laptop-A :wol: Laptop-B => "<" 
or Laptop-B :wol: Laptop-A => "<".

Similarly, let's define an execution as a set of actions. e.g. {A(x=1),
B(y=1)}.
What's a total order over it?
Meaningless right, i.e. I have to define a binary operator.
Now, the JMM doesn't define one; it simply adds that the relations produced
by it, must be consistent with (intra-thread) program order; which in the
sample trivial program is no restriction at all, since each thread contains
a single SA.

The only logical binary operator "implementation" I can think of is: 
timestamp of action.
So, my interpretation of synchronization order has been, every possible
legal execution's SAs are ordered by "timestamp of action", with the
unsolved question being:
does the JMM allow an execution to have a pair of SA's with the same
timestamp?

Is that thinking too close to the "physical layer"?  Maybe; it only requires
that each SA have a timestamp associated with it, and those timestamps be
comparable in the common-sense way









--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13471.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From rk at rkuhn.info  Tue May 31 17:10:41 2016
From: rk at rkuhn.info (Roland Kuhn)
Date: Tue, 31 May 2016 23:10:41 +0200
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464723440028-13471.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com>
Message-ID: <F854CF1E-4115-4E99-A920-D27FCCAB6CD7@rkuhn.info>


> 31 maj 2016 kl. 21:37 skrev thurstonn <thurston at nomagicsoftware.com>:
> 
> Aleksey Shipilev-2 wrote
>> 
>> I am probably forgetting a significant part of my training, but these
>> are the definitions that are consistent with my own memory:
>> 
>> (Weak) total order implies that for all A, B, C in set:
>> a) Reflexive:     (A op A)
>> b) Antisymmetric: (A op B) and (B op A) => (A = B)
>> c) Transitive:    (A op B) and (B op C) => (A op C)
>> d) Totality:      (A op B) or (B op A)
>> 
>> Strict total order implies that for all A, B, C in set:
>> a) Irreflexive:   !(A op A)
>> b) Asymmetric:    (A op B) => !(B op A)
>> c) Transitive:    (A op B) and (B op C) => (A op C)
>> d) Trichotomy:    Exactly one of (A op B), (B op A), (A = B) is true
>> 
>> Notice (A = B) reads as "A and B are the *same*", not "simultaneous".
> 
> Ah, I think that last statement might be the rub; my understanding is that A
> = B does not mean A and B are the *same*, the irreflexive rule deals with
> that
> "=" is defined by the total order.

You can’t have it both ways: if you’re talking mathematics, = means “they can be used interchangeably”, they are the same. And that is exactly what antisymmetry gives you for a total order.

> The phraseology is extremely awkward; let's replace total order with "binary
> operator" (subject to the rules you've enumerated), and let's dispense with
> (ir)reflexivity, as I agree it's completely unnecessary to speak of ordering
> a SA with respect to itself.
> 
> "=" is analogous to equals() vis a vis Java identity (==) (at least as I
> understand it)
> 
> Say I have a set of laptops {Laptop-A, Laptop-B}, 2 physically distinct
> laptops.
> It is meaningless to speak of a total order over that set.
> I have to define a binary operator, right?
> 
> Say, weight-of-laptop.
> In such a case, I can say weight-of-laptop is a total order over {Laptop-A,
> Laptop-B}; it's not difficult to interpret "=" in that case.
> 
> Now my understanding is that weight-of-laptop is *not* a strict total order
> over say {Laptop-A 1.1kg, Laptop-B 1.1kg}
> Because in order to be a total order, either Laptop-A :wol: Laptop-B => "<" 
> or Laptop-B :wol: Laptop-A => "<".
> 
> Similarly, let's define an execution as a set of actions. e.g. {A(x=1),
> B(y=1)}.
> What's a total order over it?
> Meaningless right, i.e. I have to define a binary operator.

Nope, not meaningless: for every execution there is at least one total order that results in the observed outcome. That binary operator is defined for you—synchronization order. What is meaningless is to ask how many such orders there can be, given that they all result in the same observed outcome, i.e. they cannot be distinguished by definition.

> Now, the JMM doesn't define one; it simply adds that the relations produced
> by it, must be consistent with (intra-thread) program order; which in the
> sample trivial program is no restriction at all, since each thread contains
> a single SA.
> 
> The only logical binary operator "implementation" I can think of is: 
> timestamp of action.
> So, my interpretation of synchronization order has been, every possible
> legal execution's SAs are ordered by "timestamp of action", with the
> unsolved question being:
> does the JMM allow an execution to have a pair of SA's with the same
> timestamp?
> 
> Is that thinking too close to the "physical layer"?  Maybe; it only requires
> that each SA have a timestamp associated with it, and those timestamps be
> comparable in the common-sense way

There is no such thing as a common-sense timestamp for these operations. And for timestamps to not be comparable due to special relativity the two devices do not actually have to be in motion relative to each other, the only thing that matters is that the observed order of events is not invariant under Lorentz transformations (i.e. different external observers could see conflicting sequences depending on their movement relative to the devices).

If you write timestamps from the CPU cores into memory, you’re adding the synchronization steps needed to collapse those multiple undistinguishable histories into distinct ones.

Coming back to your initial question: I don’t think it matters whether synchronization order is described in strict or non-strict form, as one gives rise to the other.

Regards,

Roland

> 
> 
> 
> 
> 
> 
> 
> 
> 
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13471.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

--
Simplicity and elegance are unpopular because they require hard work and discipline to achieve and education to be appreciated.
  -- Dijkstra


From thurston at nomagicsoftware.com  Tue May 31 16:35:36 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 31 May 2016 13:35:36 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <F854CF1E-4115-4E99-A920-D27FCCAB6CD7@rkuhn.info>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com>
 <F854CF1E-4115-4E99-A920-D27FCCAB6CD7@rkuhn.info>
Message-ID: <1464726936248-13473.post@n7.nabble.com>

Nope, not meaningless: for every execution there is at least one total order
that results in the observed outcome. That binary operator is defined for
you—synchronization order. What is meaningless is to ask how many such
orders there can be, given that they all result in the same observed
outcome, i.e. they cannot be distinguished by definition.


OK, what's the "observed outcome" of the program?
x=1, y=1  ?
So then what's  the total order of that execution?
Total orders can be enumerated pairwise




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13473.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Tue May 31 16:57:12 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 31 May 2016 13:57:12 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <CALuNCpjfZtw0AmkjPGn31nUzd9UCQaxdLDMOYxLVNYAc_hcVvw@mail.gmail.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
 <CAHzJPEr5j9zrwJ9GrAV6usZf_B9JNxt-eS260Vyyt1NbjbK9KA@mail.gmail.com>
 <1464716574039-13467.post@n7.nabble.com>
 <CALuNCpjfZtw0AmkjPGn31nUzd9UCQaxdLDMOYxLVNYAc_hcVvw@mail.gmail.com>
Message-ID: <1464728232216-13474.post@n7.nabble.com>

Scott Blum wrote
> I would posit that there is only a strict total order if you try to
> observe
> it within the program.  If you add sufficient code to determine which
> history actually happened, you will in effect force the instructions to be
> generated to insist on a strict total ordering.  In the absence of that
> extra code, it's meaningless to ask which happened first.
> 
> Imagine a hypothetical implementation where you have two threads running
> on
> two processors each with their own data caches.  Each thread might either
> write its x=1, y=1 to its local cache, or to main memory.  If they write
> to
> local cache, there's no rules around when that needs to get pushed through
> to main memory-- provided no one is doing a volatile read on those fields.
> Without the volatile reads you're never forcing the system to reach an
> absolutely "determined" state.
> 
> But I use all these terms loosely because there truly isn't a clear 1:1
> mapping between JMM and hardware. The JMM is all about reads and writes
> and
> visibility within a program; how and when things actually get written to
> memory or cache or whatever is an implementation detail of a particular
> JVM
> and machine.
> 
> TLDR: the JMM doesn't require a strict total ordering unless you try to
> observe it.


Here's the issue, let's presume you're correct.
Where does it stipulate that in  JLS$17?

I read the JMM very simply:
There's an (legal) execution.
Does the execution contain SA's?  Yes, 2 of them
Then there is a total order among those SA's. Period. Categorical

As I wrote in the OP, I agree that it's reasonable to ask:
who cares?  how can one tell the difference (i.e. one would have to
"observe" the total order, which requires reads, which changes the program)?






--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13474.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jsampson at guidewire.com  Tue May 31 18:12:24 2016
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 31 May 2016 22:12:24 +0000
Subject: [concurrency-interest] Is Synchronization Order A
 *Strict*	Total Order?
In-Reply-To: <1464728232216-13474.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
 <CAHzJPEr5j9zrwJ9GrAV6usZf_B9JNxt-eS260Vyyt1NbjbK9KA@mail.gmail.com>
 <1464716574039-13467.post@n7.nabble.com>
 <CALuNCpjfZtw0AmkjPGn31nUzd9UCQaxdLDMOYxLVNYAc_hcVvw@mail.gmail.com>
 <1464728232216-13474.post@n7.nabble.com>
Message-ID: <DM2PR05MB6860B9AA60E30DDAF0897C7D1460@DM2PR05MB686.namprd05.prod.outlook.com>

Your program has no reads in it, so the Java Memory Model doesn't constrain the implementation in any way whatsoever:

"A memory model describes, given a program and an execution trace of that program, whether the execution trace is a legal execution of the program. The Java programming language memory model works by examining each read in an execution trace and checking that the write observed by that read is valid according to certain rules.

"The memory model describes possible behaviors of a program. An implementation is free to produce any code it likes, as long as all resulting executions of a program produce a result that can be predicted by the memory model.

"This provides a great deal of freedom for the implementor to perform a myriad of code transformations, including the reordering of actions and removal of unnecessary synchronization."

https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4

Cheers,
Justin


-----Original Message-----
From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of thurstonn
Sent: Tuesday, May 31, 2016 1:57 PM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Is Synchronization Order A *Strict* Total Order?

Scott Blum wrote
> I would posit that there is only a strict total order if you try to
> observe
> it within the program.  If you add sufficient code to determine which
> history actually happened, you will in effect force the instructions to be
> generated to insist on a strict total ordering.  In the absence of that
> extra code, it's meaningless to ask which happened first.
> 
> Imagine a hypothetical implementation where you have two threads running
> on
> two processors each with their own data caches.  Each thread might either
> write its x=1, y=1 to its local cache, or to main memory.  If they write
> to
> local cache, there's no rules around when that needs to get pushed through
> to main memory-- provided no one is doing a volatile read on those fields.
> Without the volatile reads you're never forcing the system to reach an
> absolutely "determined" state.
> 
> But I use all these terms loosely because there truly isn't a clear 1:1
> mapping between JMM and hardware. The JMM is all about reads and writes
> and
> visibility within a program; how and when things actually get written to
> memory or cache or whatever is an implementation detail of a particular
> JVM
> and machine.
> 
> TLDR: the JMM doesn't require a strict total ordering unless you try to
> observe it.


Here's the issue, let's presume you're correct.
Where does it stipulate that in  JLS$17?

I read the JMM very simply:
There's an (legal) execution.
Does the execution contain SA's?  Yes, 2 of them
Then there is a total order among those SA's. Period. Categorical

As I wrote in the OP, I agree that it's reasonable to ask:
who cares?  how can one tell the difference (i.e. one would have to
"observe" the total order, which requires reads, which changes the program)?






--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13474.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From dragonsinth at gmail.com  Tue May 31 18:16:03 2016
From: dragonsinth at gmail.com (Scott Blum)
Date: Tue, 31 May 2016 18:16:03 -0400
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464728232216-13474.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com>
 <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
 <CAHzJPEr5j9zrwJ9GrAV6usZf_B9JNxt-eS260Vyyt1NbjbK9KA@mail.gmail.com>
 <1464716574039-13467.post@n7.nabble.com>
 <CALuNCpjfZtw0AmkjPGn31nUzd9UCQaxdLDMOYxLVNYAc_hcVvw@mail.gmail.com>
 <1464728232216-13474.post@n7.nabble.com>
Message-ID: <CALuNCpibkC+U4iv6EQNkNUqA-vytuPdc6C59O0KEAOWEEneErw@mail.gmail.com>

I think you're reading too much into it.  JLS 17 doesn't say there's one
particular execution or even one particular total order.  In fact it
implies the opposite: there may be a lot of executions and a lot of
possible total orderings, but:

"If a program has no data races, then all executions of the program will
appear to be sequentially consistent."

In a loosely defined program such as your example, there is more than one
sequentially consistent total ordering of events.



On Tue, May 31, 2016 at 4:57 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> Scott Blum wrote
> > I would posit that there is only a strict total order if you try to
> > observe
> > it within the program.  If you add sufficient code to determine which
> > history actually happened, you will in effect force the instructions to
> be
> > generated to insist on a strict total ordering.  In the absence of that
> > extra code, it's meaningless to ask which happened first.
> >
> > Imagine a hypothetical implementation where you have two threads running
> > on
> > two processors each with their own data caches.  Each thread might either
> > write its x=1, y=1 to its local cache, or to main memory.  If they write
> > to
> > local cache, there's no rules around when that needs to get pushed
> through
> > to main memory-- provided no one is doing a volatile read on those
> fields.
> > Without the volatile reads you're never forcing the system to reach an
> > absolutely "determined" state.
> >
> > But I use all these terms loosely because there truly isn't a clear 1:1
> > mapping between JMM and hardware. The JMM is all about reads and writes
> > and
> > visibility within a program; how and when things actually get written to
> > memory or cache or whatever is an implementation detail of a particular
> > JVM
> > and machine.
> >
> > TLDR: the JMM doesn't require a strict total ordering unless you try to
> > observe it.
>
>
> Here's the issue, let's presume you're correct.
> Where does it stipulate that in  JLS$17?
>
> I read the JMM very simply:
> There's an (legal) execution.
> Does the execution contain SA's?  Yes, 2 of them
> Then there is a total order among those SA's. Period. Categorical
>
> As I wrote in the OP, I agree that it's reasonable to ask:
> who cares?  how can one tell the difference (i.e. one would have to
> "observe" the total order, which requires reads, which changes the
> program)?
>
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13474.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160531/2476b744/attachment.html>

From aleksey.shipilev at oracle.com  Tue May 31 18:16:33 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 1 Jun 2016 01:16:33 +0300
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
 Total Order?
In-Reply-To: <1464726936248-13473.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com>
 <F854CF1E-4115-4E99-A920-D27FCCAB6CD7@rkuhn.info>
 <1464726936248-13473.post@n7.nabble.com>
Message-ID: <574E0D41.1030405@oracle.com>

On 05/31/2016 11:35 PM, thurstonn wrote:
> Nope, not meaningless: for every execution there is at least one total order
> that results in the observed outcome. That binary operator is defined for
> you—synchronization order. What is meaningless is to ask how many such
> orders there can be, given that they all result in the same observed
> outcome, i.e. they cannot be distinguished by definition.
> 
> 
> OK, what's the "observed outcome" of the program?
> x=1, y=1  ?
> So then what's  the total order of that execution?

Follow-up question: "If a tree falls in a forest and no one is around to
hear it, does it make a sound?"

But really, you can observe the finished state is (x = 1, y = 1). Is it
relevant which order had produced it? You can justify this outcome with
either juxtaposition, and so cannot distinguish which one "really
happened". The model does not concern itself with what "really happens",
it only cares that outcomes are governed by some abstract rules (that
is, "as if" there was a total order indeed).

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160601/c816e859/attachment.sig>

From thurston at nomagicsoftware.com  Tue May 31 17:21:45 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 31 May 2016 14:21:45 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A
	*Strict*	Total Order?
In-Reply-To: <DM2PR05MB6860B9AA60E30DDAF0897C7D1460@DM2PR05MB686.namprd05.prod.outlook.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
 <CAHzJPEr5j9zrwJ9GrAV6usZf_B9JNxt-eS260Vyyt1NbjbK9KA@mail.gmail.com>
 <1464716574039-13467.post@n7.nabble.com>
 <CALuNCpjfZtw0AmkjPGn31nUzd9UCQaxdLDMOYxLVNYAc_hcVvw@mail.gmail.com>
 <1464728232216-13474.post@n7.nabble.com>
 <DM2PR05MB6860B9AA60E30DDAF0897C7D1460@DM2PR05MB686.namprd05.prod.outlook.com>
Message-ID: <1464729705169-13478.post@n7.nabble.com>


"17.4.4. Synchronization Order

Every execution has a synchronization order. A synchronization order is a
total order over all of the synchronization actions of an execution. For
each thread t, the synchronization order of the synchronization actions
(§17.4.2) in t is consistent with the program order (§17.4.3) of t. "

Every execution (a program without reads falls within "every")
Free to do whatever it wants? yes, as long as there is a total order over
the set of synchronization actions.
My English is pretty good



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13478.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Tue May 31 17:43:41 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 31 May 2016 14:43:41 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <574E0D41.1030405@oracle.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com>
 <F854CF1E-4115-4E99-A920-D27FCCAB6CD7@rkuhn.info>
 <1464726936248-13473.post@n7.nabble.com> <574E0D41.1030405@oracle.com>
Message-ID: <1464731021884-13479.post@n7.nabble.com>

Aleksey Shipilev-2 wrote
> On 05/31/2016 11:35 PM, thurstonn wrote:
>> Nope, not meaningless: for every execution there is at least one total
>> order
>> that results in the observed outcome. That binary operator is defined for
>> you—synchronization order. What is meaningless is to ask how many such
>> orders there can be, given that they all result in the same observed
>> outcome, i.e. they cannot be distinguished by definition.
>> 
>> 
>> OK, what's the "observed outcome" of the program?
>> x=1, y=1  ?
>> So then what's  the total order of that execution?
> 
> Follow-up question: "If a tree falls in a forest and no one is around to
> hear it, does it make a sound?"
> 
> But really, you can observe the finished state is (x = 1, y = 1). Is it
> relevant which order had produced it? You can justify this outcome with
> either juxtaposition, and so cannot distinguish which one "really
> happened". The model does not concern itself with what "really happens",
> it only cares that outcomes are governed by some abstract rules (that
> is, "as if" there was a total order indeed).
> 
> Thanks,
> -Aleksey
> 
> 
> _______________________________________________
> Concurrency-interest mailing list

> Concurrency-interest at .oswego

> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> signature.asc (853 bytes)
> &lt;http://jsr166-concurrency.10961.n7.nabble.com/attachment/13477/0/signature.asc&gt;

Those are perfectly fair questions and I essentially raised them in the OP
if you look back.
For such a trivial program, I agree it's not an issue; it just seems very
difficult to "prove" to myself at least that it doesn't present some issue
in some hypothetical "concurrency puzzler" program that I inadvertently
write.








--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13479.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From aleksey.shipilev at oracle.com  Tue May 31 19:21:26 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 1 Jun 2016 02:21:26 +0300
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
 Total Order?
In-Reply-To: <1464723440028-13471.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com>
Message-ID: <574E1C76.3050106@oracle.com>

On 05/31/2016 10:37 PM, thurstonn wrote:
> Aleksey Shipilev-2 wrote
>>
>> I am probably forgetting a significant part of my training, but these
>> are the definitions that are consistent with my own memory:
>>
>> (Weak) total order implies that for all A, B, C in set:
>>  a) Reflexive:     (A op A)
>>  b) Antisymmetric: (A op B) and (B op A) => (A = B)
>>  c) Transitive:    (A op B) and (B op C) => (A op C)
>>  d) Totality:      (A op B) or (B op A)
>>
>> Strict total order implies that for all A, B, C in set:
>>  a) Irreflexive:   !(A op A)
>>  b) Asymmetric:    (A op B) => !(B op A)
>>  c) Transitive:    (A op B) and (B op C) => (A op C)
>>  d) Trichotomy:    Exactly one of (A op B), (B op A), (A = B) is true
>>
>> Notice (A = B) reads as "A and B are the *same*", not "simultaneous".
> 
> Ah, I think that last statement might be the rub; my understanding is that A
> = B does not mean A and B are the *same*, the irreflexive rule deals with
> that
> "=" is defined by the total order.

Set X is totally ordered under "op" binary relation if conditions above
are met. Equality is not defined by that binary relation (no less
because equality requires symmetry, which is at odds with a/antisymmetry
you already have), it should be defined separately.


> let's replace total order with "binary operator" (subject to the 
> rules you've enumerated), and let's dispense with (ir)reflexivity, as
> I agree it's completely unnecessary to speak of ordering a SA with
> respect to itself.
> "=" is analogous to equals() vis a vis Java identity (==) (at least
> as I understand it)

I see that you are inventing the definitions that were not used to
define JMM. On these grounds alone the rest of the derivation that tries
to analyze if "SO is (strict) total order" is rather suspicious.


> Say I have a set of laptops {Laptop-A, Laptop-B}, 2 physically
> distinct laptops. It is meaningless to speak of a total order over
> that set. I have to define a binary operator, right?
> 
> Say, weight-of-laptop. In such a case, I can say weight-of-laptop is
>  a total order over {Laptop-A, Laptop-B}; it's not difficult to 
> interpret "=" in that case.

It is actually difficult to interpret, because it allows for at least
two interpretations:
  a) "equal by name";
  b) "equal by weight";

Once you have chosen one, you have to run with it.

> Now my understanding is that weight-of-laptop is *not* a strict total
> order over say {Laptop-A 1.1kg, Laptop-B 1.1kg} Because in order to
> be a total order, either Laptop-A :wol: Laptop-B => "<" or Laptop-B
> :wol: Laptop-A => "<".

I don't understand the semantics of "weight-of-laptop" relation. What
does it mean Laptop-A weight-of-laptop Laptop-B?! Let's instead define
"is-heavier-than" relation, which has meaning in Laptop-A
is-heavier-than Laptop-B, comparing weights.

With "equal by weight" definition of equality, "is-heavier-than" defines
a *strict* total order over the set of laptops: it is irreflexive,
asymmetric, transitive, and trichotomous.

Note that it is not a *weak* total order: it lacks totality,
demonstrated by (Laptop-A is-heavier-than Laptop-B) and (Laptop-B
is-heavier-than Laptop-A) = false. The saving grace for this binary
relation is that equality by weight allows the escape hatch in third
case (A = B) in trichotomy.

With "equal by name" definition of equality, "is-heavier-than" is not
even a _strict_ total order anymore, because it could not satisfy the
trichotomy property. So it loses all opportunities to be total.

Doing the same for "is-heavier-than-or-equal-weight" relation is left as
an exercise for the reader :) Hint: you will run into problems with
reflexivity and antisymmetry under "equal by name" there.

So, choose: either you want a consistent equality and total order, or
auxiliary equality and the deconstructed total order.


> Similarly, let's define an execution as a set of actions. e.g. 
> {A(x=1), B(y=1)}. What's a total order over it? Meaningless right, 
> i.e. I have to define a binary operator. Now, the JMM doesn't define 
> one; it simply adds that the relations produced by it, must be
> consistent with (intra-thread) program order; which in the sample
> trivial program is no restriction at all, since each thread contains
> a single SA.

The definition of total order means there exists a binary relation that
satisfy the requirements...


> The only logical binary operator "implementation" I can think of is: 
> timestamp of action. So, my interpretation of synchronization order
> has been, every possible legal execution's SAs are ordered by
> "timestamp of action", with the unsolved question being: does the JMM
> allow an execution to have a pair of SA's with the same timestamp?

...and the fact that JMM does not advertise the exact relation used to
produce the total order does not allow you to pull a cozy one out of
thin air. You have to prove that whatever candidate relation you
invented satisfies the original "SO is total order" definition. Can you
do that for timestamps?

I don't think you can, pretty much by the reasons explained in the
laptop example above. The SA equality has a trivial property: distinct
actions are not equal -- this is roughly similar to "equals by name" in
the laptop example. If you allow two SAs to have equal timestamps, then
it derails the SO totality under whatever binary relation over timestamps.


> Is that thinking too close to the "physical layer"?  Maybe; it only requires
> that each SA have a timestamp associated with it, and those timestamps be
> comparable in the common-sense way

This is why we have formal logic, so that we can keep "common sense"
"physical" intuitions from running afoul, producing garbage interpretations.

Cheers,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160601/8a8f1bda/attachment-0001.sig>

From thurston at nomagicsoftware.com  Tue May 31 20:59:28 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 31 May 2016 17:59:28 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <574E1C76.3050106@oracle.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com> <574E1C76.3050106@oracle.com>
Message-ID: <1464742768769-13481.post@n7.nabble.com>

I think most of the confusion rests with the A = B.
Roland made it clear that = means "identity".
I wasn't inventing anything, I simply misunderstood the meaning of "=".

Now in the context of strict total order, you seem to suggest that there can
be some adhoc equals relation (like "equal-by-name") that is associated with
but distinct from the strict binary operator?
If "=" means identity in the weak total order rules (b) antisymmetric), then
why wouldn't it mean identity in d) Trichotomy ?
I didn't follow that part.

When I talk of the "definition of a total order", this is what I mean:
You can think of a total order "definition" in two ways:
1.  Is just the enumeration of all of the pair-wise relations; so when
evaluating x op y, simply search through the enumerated pair-wise relations. 
That's not what I meant by "defining a total order"

2.  Specify the definition of a "rule" (algorithm, whatever), e.g. the
mathematical operator <= applied to the set of integers (I  shouldn't need
to explain that); when evaluating x <= y, I can simply apply the rule - it
is this that the JMM does not do - it leaves it up to each execution, again
as long as it satisfies the formal mathematical rules and a few additional
constraints (program order, memory sequential consistency).

I hope that difference is clear, it is to me.
Now I suppose the lack of a "rule" doesn't matter,  after all using a rule
one can generate the enumerated pair-wise relations, and then just apply
step 1.

Upon thinking about it more deeply, what bothers me is the potential
divergence between the "physical reality" of a CPU executing an instruction
(at a time x) and the execution's total order.

Specifically, the JMM allows the following for an execution:
Time x:   A(x=1)
Time x + 100:  B(y=1)

yet the execution can reify its total order as {(B, A)}; this seems at odds
with the (flawed) notion of a volatile write as entailing an "immediate
flush to memory"







--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13481.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

