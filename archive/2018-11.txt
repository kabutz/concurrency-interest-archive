From radhakrishnan.mohan at gmail.com  Thu Nov  1 05:43:07 2018
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Thu, 1 Nov 2018 15:13:07 +0530
Subject: [concurrency-interest] Changes to concurrenxy API in newer versions
Message-ID: <CAOoXFP_Qv0hz6RBdsK5RanKS-x5z1QAtfZH+TtzfeqXK285_pA@mail.gmail.com>

Hello,
               General question. Apology.

Now that the release cycles of the JDK
are shorter I was wondering how one knows what concurrency changes are
being made.
Should I just read the bug database or the dev. threads ?

Thanks,
Mohan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181101/a82a4fbb/attachment.html>

From martinrb at google.com  Thu Nov  1 08:10:49 2018
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 1 Nov 2018 05:10:49 -0700
Subject: [concurrency-interest] Changes to concurrenxy API in newer
	versions
In-Reply-To: <CAOoXFP_Qv0hz6RBdsK5RanKS-x5z1QAtfZH+TtzfeqXK285_pA@mail.gmail.com>
References: <CAOoXFP_Qv0hz6RBdsK5RanKS-x5z1QAtfZH+TtzfeqXK285_pA@mail.gmail.com>
Message-ID: <CA+kOe0_Pxa7RjD4Q692bkHdyWwV8mJSVKPbirTELYOq_5FE1iA@mail.gmail.com>

You can follow along in CVS, mercurial, or JIRA.

Here's a JIRA filter https://bugs.openjdk.java.net/issues/?filter=27326
that might help get you started.  You can watch a JIRA subcomponent.

On Thu, Nov 1, 2018 at 2:43 AM, Mohan Radhakrishnan via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:

> Hello,
>                General question. Apology.
>
> Now that the release cycles of the JDK
> are shorter I was wondering how one knows what concurrency changes are
> being made.
> Should I just read the bug database or the dev. threads ?
>
> Thanks,
> Mohan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181101/3eefd7eb/attachment.html>

From akarnokd at gmail.com  Fri Nov  2 09:25:15 2018
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Fri, 2 Nov 2018 14:25:15 +0100
Subject: [concurrency-interest] Can Stream.close() be called at any time
	from any thread?
Message-ID: <CAAWwtm-29EcSuz2dZ-_-GLyNJsC0OUPQ1OjKWqi5UPtf5aoQ3A@mail.gmail.com>

Hello,

I couldn't find any specific description of whether Stream.close() can be
called at any time from any thread, or it has to be stricty in serialized
with an ongoing poll in it (of its iterator())?

 i.e., is generally the following safe/allowed with the Stream API?

Stream<String> stream = Files.lines(...);
Iterator<String> iterator = stream.iterator();

//...

// Thread 1, repeatedly do
if (iterator.hasNext()) {
   handle(iterator.next());
}

// Thread 2
stream.close();

(I already know how to work around if this is not allowed, just wanted to
know if I should workaround this concurrency problem).
-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181102/9568e5ec/attachment.html>

From rba at activeviam.com  Tue Nov  6 04:11:55 2018
From: rba at activeviam.com (=?UTF-8?Q?R=C3=A9mi_Barat?=)
Date: Tue, 6 Nov 2018 10:11:55 +0100
Subject: [concurrency-interest] ForkJoinTask does not re-check task status
	entailing overhead
Message-ID: <CAKsH56UriM9O7SG5MFR=bcTvGTQEgpqHpYA2he9qWGJCMHKhHg@mail.gmail.com>

Hi,

In one of our test we create around 260,000 ForkJoinTasks from one parent
task. The parent task sets a flag which causes every child task to cancel
itself (using super.cancel).

Please find attached a class to reproduce the test. In Java 8, this test is
instantaneous, whereas it takes around 25s on my machine with Java 11.

After investigation, it seems that this difference in duration comes from a
change in the ForkJoinPool#awaitJoin(WorkQueue, ForkJoinTask<?>, long)
function, which does not check that the task status is DONE anymore:

* in Java 8, before trying to help or removeAndExec, we check the current
task status:
for (;;) {
    if ((s = task.status) < 0)
        break;
    // else, try to help other tasks
}
As the task was cancelled, the function returns immediately.

* in Java 11, the check is not performed, which causes the execution of the
ForkJoinPool#tryRemoveAndExec function, which vainly browses the list of
tasks and induces the overhead.

The question is: is the check on the task status necessary, or did we do
something wrong?

Thanks,
Rémi Barat
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181106/2cfee344/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: InterruptedForkJoinTasks.java
Type: text/x-java
Size: 2559 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181106/2cfee344/attachment.java>

From dl at cs.oswego.edu  Tue Nov  6 07:10:03 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 6 Nov 2018 07:10:03 -0500
Subject: [concurrency-interest] ForkJoinTask does not re-check task
 status entailing overhead
In-Reply-To: <CAKsH56UriM9O7SG5MFR=bcTvGTQEgpqHpYA2he9qWGJCMHKhHg@mail.gmail.com>
References: <CAKsH56UriM9O7SG5MFR=bcTvGTQEgpqHpYA2he9qWGJCMHKhHg@mail.gmail.com>
Message-ID: <ab3629f9-548b-1f7f-1033-d9fbb188f89e@cs.oswego.edu>

On 11/6/18 4:11 AM, Rémi Barat via Concurrency-interest wrote:
> Hi,
> 
> In one of our test we create around 260,000 ForkJoinTasks from one
> parent task. The parent task sets a flag which causes every child task
> to cancel itself (using super.cancel).
> 
> Please find attached a class to reproduce the test. In Java 8, this test
> is instantaneous, whereas it takes around 25s on my machine with Java 11.
> ...

> The question is: is the check on the task status necessary, or did we do
> something wrong?

The check is not necessary assuming that exec() returns true if done
(either normally or exceptionally), which it is required to do. You
should change the line:

        @Override
        protected boolean exec() {
            if (!interrupted.get()) {
                executeTask();
                return true;
            } else {
                super.cancel(false);
                return true; // was false
            }
        }

(In jdk8, it happened to work OK even without doing this.)
You could also add a line in your checkInterrupt method:

        protected void checkInterruption() {
            if (interrupted.get()) {
                cancel(false);    // added
                throw new CancellationException();
            }
        }


-Doug



From rba at activeviam.com  Tue Nov  6 09:50:21 2018
From: rba at activeviam.com (=?UTF-8?Q?R=C3=A9mi_Barat?=)
Date: Tue, 6 Nov 2018 15:50:21 +0100
Subject: [concurrency-interest] ForkJoinTask does not re-check task
 status entailing overhead
In-Reply-To: <ab3629f9-548b-1f7f-1033-d9fbb188f89e@cs.oswego.edu>
References: <CAKsH56UriM9O7SG5MFR=bcTvGTQEgpqHpYA2he9qWGJCMHKhHg@mail.gmail.com>
 <ab3629f9-548b-1f7f-1033-d9fbb188f89e@cs.oswego.edu>
Message-ID: <CAKsH56UdiTB+4Sbg4h-CShrL0VdxJx3+A9zDwuXkLKKQbNQzrA@mail.gmail.com>

Hi, Doug,

Thank you very much for your quick answer. We originally made exec() return
false since the javadoc says "Returns: true if this task is known to have
completed *normally*". Maybe it should be updated so that it matches your
answer, that "... assuming that exec() returns true if done (either
normally or exceptionally)".

Do you see any potential issue we might have after changing this return
value from false to true?
It looks like it will simply set the DONE flag in the status through
setDone in doExec, which should be idempotent with the already recorded
abnormal completion of the super.cancel call.
However I'm not 100% sure of this, and we're using ForkJoinTasks and
CountedCompleters quite extensively in our software. Hopefully our test
suite will cover this!

Rémi

On Tue, Nov 6, 2018 at 1:12 PM Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 11/6/18 4:11 AM, Rémi Barat via Concurrency-interest wrote:
> > Hi,
> >
> > In one of our test we create around 260,000 ForkJoinTasks from one
> > parent task. The parent task sets a flag which causes every child task
> > to cancel itself (using super.cancel).
> >
> > Please find attached a class to reproduce the test. In Java 8, this test
> > is instantaneous, whereas it takes around 25s on my machine with Java 11.
> > ...
>
> > The question is: is the check on the task status necessary, or did we do
> > something wrong?
>
> The check is not necessary assuming that exec() returns true if done
> (either normally or exceptionally), which it is required to do. You
> should change the line:
>
>         @Override
>         protected boolean exec() {
>             if (!interrupted.get()) {
>                 executeTask();
>                 return true;
>             } else {
>                 super.cancel(false);
>                 return true; // was false
>             }
>         }
>
> (In jdk8, it happened to work OK even without doing this.)
> You could also add a line in your checkInterrupt method:
>
>         protected void checkInterruption() {
>             if (interrupted.get()) {
>                 cancel(false);    // added
>                 throw new CancellationException();
>             }
>         }
>
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181106/f9751a7f/attachment.html>

From dl at cs.oswego.edu  Tue Nov  6 11:38:22 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 6 Nov 2018 11:38:22 -0500
Subject: [concurrency-interest] ForkJoinTask does not re-check task
 status entailing overhead
In-Reply-To: <CAKsH56UdiTB+4Sbg4h-CShrL0VdxJx3+A9zDwuXkLKKQbNQzrA@mail.gmail.com>
References: <CAKsH56UriM9O7SG5MFR=bcTvGTQEgpqHpYA2he9qWGJCMHKhHg@mail.gmail.com>
 <ab3629f9-548b-1f7f-1033-d9fbb188f89e@cs.oswego.edu>
 <CAKsH56UdiTB+4Sbg4h-CShrL0VdxJx3+A9zDwuXkLKKQbNQzrA@mail.gmail.com>
Message-ID: <7e921d9b-a0e0-ee2c-23de-831c7a1eb619@cs.oswego.edu>

On 11/6/18 9:50 AM, Rémi Barat via Concurrency-interest wrote:
> Hi, Doug,
> 
> Thank you very much for your quick answer. We originally made exec()
> return false since the javadoc says "Returns: true if this task is known
> to have completed /normally/". Maybe it should be updated so that it
> matches your answer, that "... assuming that exec() returns true if done
> (either normally or exceptionally)".

Yes; thanks -- "normally" should/will be deleted in that sentence
(especially since it does not mesh with the following sentence
clarification.) Sorry for the confusion!

> 
> Do you see any potential issue we might have after changing this return
> value from false to true?

No.

-Doug



From rba at activeviam.com  Thu Nov  8 11:10:58 2018
From: rba at activeviam.com (=?UTF-8?Q?R=C3=A9mi_Barat?=)
Date: Thu, 8 Nov 2018 17:10:58 +0100
Subject: [concurrency-interest] ForkJoinTask does not re-check task
 status entailing overhead
In-Reply-To: <7e921d9b-a0e0-ee2c-23de-831c7a1eb619@cs.oswego.edu>
References: <CAKsH56UriM9O7SG5MFR=bcTvGTQEgpqHpYA2he9qWGJCMHKhHg@mail.gmail.com>
 <ab3629f9-548b-1f7f-1033-d9fbb188f89e@cs.oswego.edu>
 <CAKsH56UdiTB+4Sbg4h-CShrL0VdxJx3+A9zDwuXkLKKQbNQzrA@mail.gmail.com>
 <7e921d9b-a0e0-ee2c-23de-831c7a1eb619@cs.oswego.edu>
Message-ID: <CAKsH56VmMvZ8nhXOSM7GTHURH91B3+hsUtKRJP_B6eTz6kd-BA@mail.gmail.com>

Hi, Doug,

Thank you for the clarification and for your time. It seems all clear on
our side when setting the return value to true.

Have a good day,
Rémi

On Tue, Nov 6, 2018 at 5:39 PM Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 11/6/18 9:50 AM, Rémi Barat via Concurrency-interest wrote:
> > Hi, Doug,
> >
> > Thank you very much for your quick answer. We originally made exec()
> > return false since the javadoc says "Returns: true if this task is known
> > to have completed /normally/". Maybe it should be updated so that it
> > matches your answer, that "... assuming that exec() returns true if done
> > (either normally or exceptionally)".
>
> Yes; thanks -- "normally" should/will be deleted in that sentence
> (especially since it does not mesh with the following sentence
> clarification.) Sorry for the confusion!
>
> >
> > Do you see any potential issue we might have after changing this return
> > value from false to true?
>
> No.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181108/b6528c4b/attachment.html>

From notcarl at google.com  Thu Nov  8 18:16:31 2018
From: notcarl at google.com (Carl Mastrangelo)
Date: Thu, 8 Nov 2018 15:16:31 -0800
Subject: [concurrency-interest] ForkJoinPool with custom number of WorkQueues
Message-ID: <CAAcqB+vVBDT5xMgd6Gs1hnG=OSUquTL1Masw5C_XnW-9wAL1Eg@mail.gmail.com>

Hi,

I am using ForkJoinPool as a means to avoid lock contention for submitting
tasks to an executor.  FJP is much faster than before, but has some
unexpected slowdown when there is not enough work.   In particular, A
significant amount of time is spent waiting parking and unparking threads
when there no work to do.  When there is no active work, it seems each
worker scans the other work queues looking for work before going to sleep.

In my program I have a parallelism of 64, because when the work load is
high, each of the threads can be active.  However, when work load is low,
the workers spend too much time looking for more work.

One way to fix this (I think) is to lower the number of worker queues, but
keep the same number of workers.   In my case, having 32 or 16 queues
rather than having exactly 64 might help, but I have no way of testing it
out.   Has this ever been brought up, or is it possible for me to easily
patch FJP to see if it would help?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181108/d1379a8b/attachment.html>

From nigro.fra at gmail.com  Fri Nov  9 02:52:25 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Fri, 9 Nov 2018 08:52:25 +0100
Subject: [concurrency-interest] ForkJoinPool with custom number of
	WorkQueues
In-Reply-To: <CAAcqB+vVBDT5xMgd6Gs1hnG=OSUquTL1Masw5C_XnW-9wAL1Eg@mail.gmail.com>
References: <CAAcqB+vVBDT5xMgd6Gs1hnG=OSUquTL1Masw5C_XnW-9wAL1Eg@mail.gmail.com>
Message-ID: <CAKxGtTWEbPUd6sV5u1V9i_rhXVuAsvws_Xq8152hzBorAis+VQ@mail.gmail.com>

In the past implementations of the FJ pool there was a SPIN static variable
used for this purpose that where quite good (with parallelism<available
cores) to not park immediately the executor threads where is nothing to do.

I have the same problem of Carl and I have found a similar "solution":
reduce the parallelism hoping to get the queues more busy, but it would in
incour in an higher contention on offer side.

It wouldn't make sense to inject some WaitStrategy to allow the user to
choose what to do before parking (if parking)?

Il giorno ven 9 nov 2018, 00:18 Carl Mastrangelo via Concurrency-interest <
concurrency-interest at cs.oswego.edu> ha scritto:

> Hi,
>
> I am using ForkJoinPool as a means to avoid lock contention for submitting
> tasks to an executor.  FJP is much faster than before, but has some
> unexpected slowdown when there is not enough work.   In particular, A
> significant amount of time is spent waiting parking and unparking threads
> when there no work to do.  When there is no active work, it seems each
> worker scans the other work queues looking for work before going to sleep.
>
> In my program I have a parallelism of 64, because when the work load is
> high, each of the threads can be active.  However, when work load is low,
> the workers spend too much time looking for more work.
>
> One way to fix this (I think) is to lower the number of worker queues, but
> keep the same number of workers.   In my case, having 32 or 16 queues
> rather than having exactly 64 might help, but I have no way of testing it
> out.   Has this ever been brought up, or is it possible for me to easily
> patch FJP to see if it would help?
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181109/e43fbef7/attachment.html>

From viktor.klang at gmail.com  Fri Nov  9 03:03:20 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 9 Nov 2018 09:03:20 +0100
Subject: [concurrency-interest] ForkJoinPool with custom number of
	WorkQueues
In-Reply-To: <CAKxGtTWEbPUd6sV5u1V9i_rhXVuAsvws_Xq8152hzBorAis+VQ@mail.gmail.com>
References: <CAAcqB+vVBDT5xMgd6Gs1hnG=OSUquTL1Masw5C_XnW-9wAL1Eg@mail.gmail.com>
 <CAKxGtTWEbPUd6sV5u1V9i_rhXVuAsvws_Xq8152hzBorAis+VQ@mail.gmail.com>
Message-ID: <CANPzfU_JihVhYt7-8S-bPFLdqv_Ey9shhJbPWS=mqq3BOV6-Nw@mail.gmail.com>

I've also wanted a WaitStrategy so if that's on the table then I'd be
voting for that.

On Fri, Nov 9, 2018 at 8:54 AM Francesco Nigro via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> In the past implementations of the FJ pool there was a SPIN static
> variable used for this purpose that where quite good (with
> parallelism<available cores) to not park immediately the executor threads
> where is nothing to do.
>
> I have the same problem of Carl and I have found a similar "solution":
> reduce the parallelism hoping to get the queues more busy, but it would in
> incour in an higher contention on offer side.
>
> It wouldn't make sense to inject some WaitStrategy to allow the user to
> choose what to do before parking (if parking)?
>
> Il giorno ven 9 nov 2018, 00:18 Carl Mastrangelo via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> ha scritto:
>
>> Hi,
>>
>> I am using ForkJoinPool as a means to avoid lock contention for
>> submitting tasks to an executor.  FJP is much faster than before, but has
>> some unexpected slowdown when there is not enough work.   In particular, A
>> significant amount of time is spent waiting parking and unparking threads
>> when there no work to do.  When there is no active work, it seems each
>> worker scans the other work queues looking for work before going to sleep.
>>
>> In my program I have a parallelism of 64, because when the work load is
>> high, each of the threads can be active.  However, when work load is low,
>> the workers spend too much time looking for more work.
>>
>> One way to fix this (I think) is to lower the number of worker queues,
>> but keep the same number of workers.   In my case, having 32 or 16 queues
>> rather than having exactly 64 might help, but I have no way of testing it
>> out.   Has this ever been brought up, or is it possible for me to easily
>> patch FJP to see if it would help?
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181109/3391db65/attachment.html>

From dl at cs.oswego.edu  Sun Nov 11 09:23:31 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 11 Nov 2018 09:23:31 -0500
Subject: [concurrency-interest] ForkJoinPool with custom number of
 WorkQueues
In-Reply-To: <CAKxGtTWEbPUd6sV5u1V9i_rhXVuAsvws_Xq8152hzBorAis+VQ@mail.gmail.com>
References: <CAAcqB+vVBDT5xMgd6Gs1hnG=OSUquTL1Masw5C_XnW-9wAL1Eg@mail.gmail.com>
 <CAKxGtTWEbPUd6sV5u1V9i_rhXVuAsvws_Xq8152hzBorAis+VQ@mail.gmail.com>
Message-ID: <816ed264-6f54-8950-acec-5cc07b7d104d@cs.oswego.edu>


I agree that something should be done to improve performance in cases
where the optimal steady state is to engage only a subset of available
worker threads. I'll explore other options (which has been on todo-list
for too long). None of those we've tried so far are very good:


On 11/9/18 2:52 AM, Francesco Nigro via Concurrency-interest wrote:
> In the past implementations of the FJ pool there was a SPIN static
> variable used for this purpose that where quite good (with
> parallelism<available cores) to not park immediately the executor
> threads where is nothing to do.

... with the disadvantage of unacceptably high CPU wastage on machines
running near CPU saturation, as we discovered only after releasing, and
so later removed. There are now only a few j.u.c classes that include
paths allowing a non-trivial amount of spinning, and some of these might
be subject to reconsideration in light of Loom, in which spinning is
rarely a good idea.

> 
> I have the same problem of Carl and I have found a similar "solution":
> reduce the parallelism hoping to get the queues more busy, but it would
> in incour in an higher contention on offer side. 

When steady state has little variance in task execution rates, reducing
parallelism *is* the right solution. But has the disadvantage of poorer
throughput when a lot of tasks suddenly appear.

> 
> It wouldn't make sense to inject some WaitStrategy to allow the user to
> choose what to do before parking (if parking)?

I'm not a big fan of forcing users to make such decisions, since they
can/will encounter the same issues as algorithmic solutions do, as in
Carl's situation below.

> 
> Il giorno ven 9 nov 2018, 00:18 Carl Mastrangelo via
> Concurrency-interest <concurrency-interest at cs.oswego.edu
> <mailto:concurrency-interest at cs.oswego.edu>> ha scritto:
> 
>     Hi,
> 
>     I am using ForkJoinPool as a means to avoid lock contention for
>     submitting tasks to an executor.  FJP is much faster than before,
>     but has some unexpected slowdown when there is not enough work.   In
>     particular, A significant amount of time is spent waiting parking
>     and unparking threads when there no work to do.  When there is no
>     active work, it seems each worker scans the other work queues
>     looking for work before going to sleep.  
> 
>     In my program I have a parallelism of 64, because when the work load
>     is high, each of the threads can be active.  However, when work load
>     is low, the workers spend too much time looking for more work.

Besides keeping too many threads alive, the main performance issue here
is reduced locality because related tasks are taken by different
threads. So ...
   
> 
>     One way to fix this (I think) is to lower the number of worker
>     queues, but keep the same number of workers.   In my case, having 32
>     or 16 queues rather than having exactly 64 might help, but I have no
>     way of testing it out.   Has this ever been brought up, or is it
>     possible for me to easily patch FJP to see if it would help?

... I don't think this will be a dramatic improvement, but if you are
interested in experimenting, you could try building a jsr166.jar after
changing ForkJoinPool line 703 from:
    static final int SQMASK       = 0x007e;   // max 64 (even) slots
    static final int SQMASK       = 0x001e;   // max 16 (even) slots

-Doug



From nigro.fra at gmail.com  Mon Nov 12 09:19:03 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Mon, 12 Nov 2018 15:19:03 +0100
Subject: [concurrency-interest] ForkJoinPool with custom number of
	WorkQueues
In-Reply-To: <816ed264-6f54-8950-acec-5cc07b7d104d@cs.oswego.edu>
References: <CAAcqB+vVBDT5xMgd6Gs1hnG=OSUquTL1Masw5C_XnW-9wAL1Eg@mail.gmail.com>
 <CAKxGtTWEbPUd6sV5u1V9i_rhXVuAsvws_Xq8152hzBorAis+VQ@mail.gmail.com>
 <816ed264-6f54-8950-acec-5cc07b7d104d@cs.oswego.edu>
Message-ID: <CAKxGtTWGpj95_9ZOExcCijwT0GR=Cz7yM053mW9LQkJUn4RBOA@mail.gmail.com>

Answers inline

There are now only a few j.u.c classes that include
> paths allowing a non-trivial amount of spinning, and some of these might
> be subject to reconsideration in light of Loom, in which spinning is
> rarely a good idea


Good point: most busy spin approaches (or Thread::onSpinWait patterns)
should be avoided just for this reason:
to not count the recent issues re Intel and the pause instrinsic IIRC.

I'm not a big fan of forcing users to make such decisions, since they
> can/will encounter the same issues as algorithmic solutions do, as in
> Carl's situation below.


That's true for any fine tuning related to specific performance issue I
believe.
That's why I propose to not have this tuning in the hands of the "average"
(2 be defined) user, but instead to default
on the park approach and just via a protected constructor accessible only
via inheritance the full fat feature
(ie customizable IdleStrategy/WaitStrategy et similia). Any defensive
approach is wellcome here :)
I perfectly understand that there are just few devs in the world that need
it, but given that I'm one of
them I can't avoid to propose it respecting your need to not complicate the
life to the other 99,9999% of users,
but I do understand if it won't seems safe enough.


Il giorno dom 11 nov 2018 alle ore 15:25 Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> ha scritto:

>
> I agree that something should be done to improve performance in cases
> where the optimal steady state is to engage only a subset of available
> worker threads. I'll explore other options (which has been on todo-list
> for too long). None of those we've tried so far are very good:
>
>
> On 11/9/18 2:52 AM, Francesco Nigro via Concurrency-interest wrote:
> > In the past implementations of the FJ pool there was a SPIN static
> > variable used for this purpose that where quite good (with
> > parallelism<available cores) to not park immediately the executor
> > threads where is nothing to do.
>
> ... with the disadvantage of unacceptably high CPU wastage on machines
> running near CPU saturation, as we discovered only after releasing, and
> so later removed. There are now only a few j.u.c classes that include
> paths allowing a non-trivial amount of spinning, and some of these might
> be subject to reconsideration in light of Loom, in which spinning is
> rarely a good idea.
>
> >
> > I have the same problem of Carl and I have found a similar "solution":
> > reduce the parallelism hoping to get the queues more busy, but it would
> > in incour in an higher contention on offer side.
>
> When steady state has little variance in task execution rates, reducing
> parallelism *is* the right solution. But has the disadvantage of poorer
> throughput when a lot of tasks suddenly appear.
>
> >
> > It wouldn't make sense to inject some WaitStrategy to allow the user to
> > choose what to do before parking (if parking)?
>
> I'm not a big fan of forcing users to make such decisions, since they
> can/will encounter the same issues as algorithmic solutions do, as in
> Carl's situation below.
>
> >
> > Il giorno ven 9 nov 2018, 00:18 Carl Mastrangelo via
> > Concurrency-interest <concurrency-interest at cs.oswego.edu
> > <mailto:concurrency-interest at cs.oswego.edu>> ha scritto:
> >
> >     Hi,
> >
> >     I am using ForkJoinPool as a means to avoid lock contention for
> >     submitting tasks to an executor.  FJP is much faster than before,
> >     but has some unexpected slowdown when there is not enough work.   In
> >     particular, A significant amount of time is spent waiting parking
> >     and unparking threads when there no work to do.  When there is no
> >     active work, it seems each worker scans the other work queues
> >     looking for work before going to sleep.
> >
> >     In my program I have a parallelism of 64, because when the work load
> >     is high, each of the threads can be active.  However, when work load
> >     is low, the workers spend too much time looking for more work.
>
> Besides keeping too many threads alive, the main performance issue here
> is reduced locality because related tasks are taken by different
> threads. So ...
>
> >
> >     One way to fix this (I think) is to lower the number of worker
> >     queues, but keep the same number of workers.   In my case, having 32
> >     or 16 queues rather than having exactly 64 might help, but I have no
> >     way of testing it out.   Has this ever been brought up, or is it
> >     possible for me to easily patch FJP to see if it would help?
>
> ... I don't think this will be a dramatic improvement, but if you are
> interested in experimenting, you could try building a jsr166.jar after
> changing ForkJoinPool line 703 from:
>     static final int SQMASK       = 0x007e;   // max 64 (even) slots
>     static final int SQMASK       = 0x001e;   // max 16 (even) slots
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181112/252fb22b/attachment.html>

From nathanila at gmail.com  Mon Nov 12 10:31:18 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Mon, 12 Nov 2018 08:31:18 -0700
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <0264213e-314f-5d4b-5b9c-30bad89d1605@anarres.org>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
 <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>
 <71395ea9-79fa-cf83-174f-3bdc99af834d@gmail.com>
 <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>
 <0264213e-314f-5d4b-5b9c-30bad89d1605@anarres.org>
Message-ID: <5bf1cc7c-402b-bc2c-cef9-c5ba30f694b1@gmail.com>

What about replacing ThreadLocal.ThreadLocalMap with WeakHashMap?  
WeakHashMap uses a ReferenceQueue to track stale entries and WeakHashMap 
gets rid of stale entries with every access.  WeakHashMap checks the 
entry's hash before checking the key for equality.  Using WeakHashMap 
eliminates the cost of maintaining ThreadLocalMap which is essentially a 
WeakHashMap.

-Nathan

On 10/19/2018 11:45 AM, Shevek wrote:
> This sounds VERY like the behaviour we observed - the WeakReferences 
> not getting cleared would make total sense for causing the linear scans.
>
> On 10/19/18 8:09 AM, Gil Tene via Concurrency-interest wrote:
>> The problem with “short lived” thread locals that do not explicitly 
>> use remove() when they are no longer needed, is that in those 
>> situations “short lived” means “until the next GC detects that the 
>> ThreadLocal is no longer reachable, and the weak reference to that 
>> ThreadLocal starts returning nulls”. Scanning for stale entries more 
>> frequently, or on an API call, won’t help these situations because 
>> the map entries are not “stale” until the collector determines the 
>> unreachability of their related ThreadLocal instances. When short 
>> lived ThreadLocals are created and die at a rate linear to 
>> application throughout, this leads to ThreadLocal maps in active 
>> threads holding thousands or millions of entries, to extremely long 
>> collision chains, and to situations where half the CPU is spent 
>> walking those chains in ThreadLocal.get().
>>
>> Where would such a ThreadLocal instance creation rate come from? 
>> We’ve seen it happen in the wild in quite a few places. I initially 
>> thought of it as an application or library-level misuse of a 
>> ThreadLocal, but with each instance of juc ReentrantReadWriteLock 
>> potentially using a ThreadLocal instance for internal coordination, 
>> application and library writers that use ReentrantReadWriteLock in a 
>> seemingly idiomatic way are often unaware of the ThreadLocal 
>> implications. A simple, seemingly valid pattern for using 
>> ReentrantReadWriteLock would be a system where arriving work 
>> “packets” are handled by parallel threads (doing parallel work within 
>> the work packet), where each work packet uses its own of 
>> ReentrantReadWriteLock instance for coordination of the parallelism 
>> within the packet.
>>
>> With G1 (and C4, and likely Shenandoah and ZGC), with the current 
>> ThreadLocal implementation, the “means “until the next GC detects 
>> that the ThreadLocal is no longer reachable” meaning gets compounded 
>> by the fact that collisions in the map will actually prevent 
>> otherwise-unreachable ThreadLocal instances from becoming 
>> unreachable. As long as get() calls for other (chain colliding) 
>> ThreadLocals are actively performed on any thread, where “actively” 
>> means “more than once during a mark cycle”, the weakrefs from the 
>> colliding entries get strengthened during the mark, preventing the 
>> colliding ThreadLocal instances from dying in the given GC cycle.. 
>> Stop-the-world newgen provides a bit of a filter for 
>> short-lived-enough ThreadLocals for G1, but for ThreadLocals with 
>> mid-term lifecycles (which will naturally occur as queues in a work 
>> system as described above grow under load), or without such a STW 
>> newgen (which none of the newer collectors have or want to have 
>> have), this situation leads to explosive ThreadLocal map growth under 
>> high-enough throughout situations.
>>
>> We’ve created a tweaked implementation of ThreadLocal that avoids the 
>> weakref get()-strengthening problem with no semantic change, by 
>> having ThreadLocal get() determine identity by comparing a 
>> ThreadLocal ID (a unique long value per ThreadLocal instance, 
>> assigned at ThreadLocal instantiation) rather than comparing the 
>> outcome of a weakref get() for each entry in the chain walked when 
>> looking for a match. This makes weakref get()s only occur on the 
>> actual entry you are looking up, and not on any colliding entries, 
>> this preventing the gets from keeping dead ThreadLocals alive. We’ve 
>> been using this implementation successfully with C4 in Zing, and 
>> would be happy to share and upstream if there is interest.
>>
>> Sent from my iPad
>>
>>> On Oct 19, 2018, at 6:45 AM, Nathan and Ila Reynolds via 
>>> Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>>>
>>> A while ago, I raised the request for a processor local. Striping a 
>>> highly contended cache line, reduces contention but the cache line 
>>> still bounces around cores.  Using processor local, each cache line 
>>> can be assigned to a core and it remains in the exclusive state for 
>>> that core.  Atomic operations on that cache line are the fastest 
>>> possible.
>>>
>>> My original example was a reader-writer lock which is almost 
>>> exclusively used for read operations.  However, with processor 
>>> local, we can implement uncontended counters for heavily used queues.
>>>
>>> -Nathan
>>>
>>>> On 10/19/2018 7:06 AM, Brian Goetz wrote:
>>>> The problem is deeper than that; people using TL because it's what 
>>>> we've got, when really what people want (in various situations) is 
>>>> { processor, frame, task } locals, and TL is the best approximation 
>>>> we have.  Over in Project Loom, there's a deeper exploration going 
>>>> on of the use cases, and what additional mechanisms might help.
>>>>
>>>>> On 10/19/2018 6:12 AM, Peter Levart via Concurrency-interest wrote:
>>>>> It is interesting that people have problems with short-lived 
>>>>> ThreadLocals because ThreadLocal tries hard to expunge stale 
>>>>> entries gradually during use. Even get() tries to do it if the 
>>>>> access happens to miss the home slot. Repeated access to the same 
>>>>> entry should not have to skip stale entries over and over again. 
>>>>> There seems to be special use pattern(s) that provoke problems. 
>>>>> Studying them more deeply could show us the weakness of current 
>>>>> ThreadLocal auto-expunge strategy so it could be improved and new 
>>>>> method would not be needed...
>>>>>
>>>>> Maybe the problem is not caused by unexpunged stale entries. It 
>>>>> may be related to degeneration of hashtable that leads to 
>>>>> sub-optimal placement of live entries. In that case, perhaps 
>>>>> triggering a re-hash automatically when get() encounters many live 
>>>>> entries it has to skip would help.
>>>>>
>>>>> So those with problems, please be more specific about them. Can 
>>>>> you show us a reproducer?
>>>>>
>>>>> Regards, Peter
>>>>>
>>>>>> On 10/17/2018 08:26 PM, Doug Lea via Concurrency-interest wrote:
>>>>>> [+list]
>>>>>>
>>>>>>> On 10/17/18 11:44 AM, Nathan and Ila Reynolds wrote:
>>>>>>> Can we add the following method to ThreadLocal?
>>>>>>>
>>>>>>> public static void expungeStaleEntries()
>>>>>> This seems like a reasonable request (although perhaps with an 
>>>>>> improved
>>>>>> name).  The functionality exists internally, and it seems overly
>>>>>> parental not to export it for use as a band-aid by those people 
>>>>>> who have
>>>>>> tried and otherwise failed to solve the zillions of short-lived
>>>>>> ThreadLocals in long-lived threads problem.
>>>>>>
>>>>>> Can anyone think of a reason not to do this?
>>>>>>
>>>>>> -Doug
>>>>>>
>>>>>>> This method will call 
>>>>>>> ThreadLocal.ThreadLocalMap.expungeStaleEntries()
>>>>>>> for the ThreadLocalMap of the current thread.  Thread pools can 
>>>>>>> then
>>>>>>> call this method when the thread finishes processing a job after 
>>>>>>> GC.
>>>>>>> This solves the problem of zillions of short-lived ThreadLocals in
>>>>>>> long-lived threads.
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>

From notcarl at google.com  Thu Nov 15 20:08:53 2018
From: notcarl at google.com (Carl Mastrangelo)
Date: Thu, 15 Nov 2018 17:08:53 -0800
Subject: [concurrency-interest] ForkJoinPool with custom number of
	WorkQueues
In-Reply-To: <816ed264-6f54-8950-acec-5cc07b7d104d@cs.oswego.edu>
References: <CAAcqB+vVBDT5xMgd6Gs1hnG=OSUquTL1Masw5C_XnW-9wAL1Eg@mail.gmail.com>
 <CAKxGtTWEbPUd6sV5u1V9i_rhXVuAsvws_Xq8152hzBorAis+VQ@mail.gmail.com>
 <816ed264-6f54-8950-acec-5cc07b7d104d@cs.oswego.edu>
Message-ID: <CAAcqB+uwCX2Xu8KH8sg+vYrwKJoX7=iLX8w=+HsqX84byifhgQ@mail.gmail.com>

Response inline:

On Sun, Nov 11, 2018 at 6:25 AM Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

>
> I agree that something should be done to improve performance in cases
> where the optimal steady state is to engage only a subset of available
> worker threads. I'll explore other options (which has been on todo-list
> for too long). None of those we've tried so far are very good:
>
>
> On 11/9/18 2:52 AM, Francesco Nigro via Concurrency-interest wrote:
> > In the past implementations of the FJ pool there was a SPIN static
> > variable used for this purpose that where quite good (with
> > parallelism<available cores) to not park immediately the executor
> > threads where is nothing to do.
>
> ... with the disadvantage of unacceptably high CPU wastage on machines
> running near CPU saturation, as we discovered only after releasing, and
> so later removed. There are now only a few j.u.c classes that include
> paths allowing a non-trivial amount of spinning, and some of these might
> be subject to reconsideration in light of Loom, in which spinning is
> rarely a good idea.
>
> >
> > I have the same problem of Carl and I have found a similar "solution":
> > reduce the parallelism hoping to get the queues more busy, but it would
> > in incour in an higher contention on offer side.
>
> When steady state has little variance in task execution rates, reducing
> parallelism *is* the right solution. But has the disadvantage of poorer
> throughput when a lot of tasks suddenly appear.
>
> >
> > It wouldn't make sense to inject some WaitStrategy to allow the user to
> > choose what to do before parking (if parking)?
>
> I'm not a big fan of forcing users to make such decisions, since they
> can/will encounter the same issues as algorithmic solutions do, as in
> Carl's situation below.
>
> >
> > Il giorno ven 9 nov 2018, 00:18 Carl Mastrangelo via
> > Concurrency-interest <concurrency-interest at cs.oswego.edu
> > <mailto:concurrency-interest at cs.oswego.edu>> ha scritto:
> >
> >     Hi,
> >
> >     I am using ForkJoinPool as a means to avoid lock contention for
> >     submitting tasks to an executor.  FJP is much faster than before,
> >     but has some unexpected slowdown when there is not enough work.   In
> >     particular, A significant amount of time is spent waiting parking
> >     and unparking threads when there no work to do.  When there is no
> >     active work, it seems each worker scans the other work queues
> >     looking for work before going to sleep.
> >
> >     In my program I have a parallelism of 64, because when the work load
> >     is high, each of the threads can be active.  However, when work load
> >     is low, the workers spend too much time looking for more work.
>
> Besides keeping too many threads alive, the main performance issue here
> is reduced locality because related tasks are taken by different
> threads. So ...
>
> >
> >     One way to fix this (I think) is to lower the number of worker
> >     queues, but keep the same number of workers.   In my case, having 32
> >     or 16 queues rather than having exactly 64 might help, but I have no
> >     way of testing it out.   Has this ever been brought up, or is it
> >     possible for me to easily patch FJP to see if it would help?
>
> ... I don't think this will be a dramatic improvement, but if you are
> interested in experimenting, you could try building a jsr166.jar after
> changing ForkJoinPool line 703 from:
>     static final int SQMASK       = 0x007e;   // max 64 (even) slots
>     static final int SQMASK       = 0x001e;   // max 16 (even) slots
>

Thanks for the tip.   I tried this out, (and 0x003e), and as you said it
hardly helped out.  In my profile, most of the time was moved from
park/unpark over to scan, making it mostly a wash.

Here is what my profiler says:

32 parallelism, 16 workqueues
8.43 s 23.33% ForkJoinPool2.scan(ForkJoinPool2$WorkQueue, int)
(ForkJoinPool2.java)
360 ms 0.996% ForkJoinPool2.externalPush(ForkJoinTask2) (ForkJoinPool2.java)
300 ms 0.830% ForkJoinPool2.signalWork(ForkJoinPool2$WorkQueue[],
ForkJoinPool2$WorkQueue) (ForkJoinPool2.java)
270 ms 0.747% ForkJoinPool2.awaitWork(ForkJoinPool2$WorkQueue, int)
(ForkJoinPool2.java)
160 ms 0.443% ForkJoinPool2$WorkQueue.runTask(ForkJoinTask2)
(ForkJoinPool2.java)
140 ms 0.387% ForkJoinPool2.runWorker(ForkJoinPool2$WorkQueue)
(ForkJoinPool2.java)
90 ms 0.249% ForkJoinPool2.execute(java.lang.Runnable) (ForkJoinPool2.java)
50 ms 0.138% ForkJoinTask2.setCompletion(int) (ForkJoinTask2.java)
20 ms 0.055% ForkJoinTask2.doExec (ForkJoinTask2.java)
10 ms 0.028% ForkJoinPool2.tryRelease(long, ForkJoinPool2$WorkQueue, long)
(ForkJoinPool2.java)
10 ms 0.028% ForkJoinTask2$RunnableExecuteAction.exec (ForkJoinTask2.java)


32 parallelism, 32 workqueues
6.82 s 18.80% ForkJoinPool2.scan(ForkJoinPool2$WorkQueue, int)
(ForkJoinPool2.java)
480 ms 1.323% ForkJoinPool2.externalPush(ForkJoinTask2) (ForkJoinPool2.java)
400 ms 1.103% ForkJoinPool2.awaitWork(ForkJoinPool2$WorkQueue, int)
(ForkJoinPool2.java)
320 ms 0.882% ForkJoinPool2.signalWork(ForkJoinPool2$WorkQueue[],
ForkJoinPool2$WorkQueue) (ForkJoinPool2.java)
160 ms 0.441% ForkJoinTask2.doExec (ForkJoinTask2.java)
110 ms 0.303% ForkJoinPool2$WorkQueue.runTask(ForkJoinTask2)
(ForkJoinPool2.java)
50 ms 0.138% ForkJoinTask2.setCompletion(int) (ForkJoinTask2.java)
20 ms 0.055% ForkJoinPool2.tryRelease(long, ForkJoinPool2$WorkQueue, long)
(ForkJoinPool2.java)
10 ms 0.028% ForkJoinPool2.runWorker(ForkJoinPool2$WorkQueue)
(ForkJoinPool2.java)
10 ms 0.028% ForkJoinPool2$WorkQueue.execLocalTasks (ForkJoinPool2.java)
10 ms 0.028% ForkJoinPool2.execute(java.lang.Runnable) (ForkJoinPool2.java)




>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181115/ca516297/attachment.html>

From dl at cs.oswego.edu  Sun Nov 18 17:38:37 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 18 Nov 2018 17:38:37 -0500
Subject: [concurrency-interest] ForkJoinPool with custom number of
 WorkQueues
In-Reply-To: <CAAcqB+uwCX2Xu8KH8sg+vYrwKJoX7=iLX8w=+HsqX84byifhgQ@mail.gmail.com>
References: <CAAcqB+vVBDT5xMgd6Gs1hnG=OSUquTL1Masw5C_XnW-9wAL1Eg@mail.gmail.com>
 <CAKxGtTWEbPUd6sV5u1V9i_rhXVuAsvws_Xq8152hzBorAis+VQ@mail.gmail.com>
 <816ed264-6f54-8950-acec-5cc07b7d104d@cs.oswego.edu>
 <CAAcqB+uwCX2Xu8KH8sg+vYrwKJoX7=iLX8w=+HsqX84byifhgQ@mail.gmail.com>
Message-ID: <425388c0-8bf3-c43d-e7bb-d3d1205c3b78@cs.oswego.edu>

On 11/15/18 8:08 PM, Carl Mastrangelo wrote:
> On Sun, Nov 11, 2018 at 6:25 AM Doug Lea via Concurrency-interest
> <concurrency-interest at cs.oswego.edu
> <mailto:concurrency-interest at cs.oswego.edu>> wrote:
> 
>     When steady state has little variance in task execution rates, reducing
>     parallelism *is* the right solution. 

(Although this is not always actionable advice because burstinesss can
be related to choice of Garbage Collector.)

>     >
>     >     One way to fix this (I think) is to lower the number of worker
>     >     queues, but keep the same number of workers.   
> 
> Thanks for the tip.   I tried this out, (and 0x003e), and as you said it
> hardly helped out.  In my profile, most of the time was moved from
> park/unpark over to scan, making it mostly a wash.

I've been working on reducing the excessive scans and park/unparks. If
anyone experiencing this issue could approximate programs showing impact
as performance tests, please get in touch off-list so I can better check
impact.

-Doug


