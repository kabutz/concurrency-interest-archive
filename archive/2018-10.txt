From oleksandr.otenko at gmail.com  Mon Oct  1 04:12:36 2018
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Mon, 1 Oct 2018 09:12:36 +0100
Subject: [concurrency-interest] Linearizability of synchronization
	actions
In-Reply-To: <5A64A994-05C0-4010-82D6-1FC745E826E5@gmail.com>
References: <996621C8-40C3-485C-A4FF-9FE38A0BA59B@gmail.com>
 <A1D91EEA-D8C1-449F-BF16-EA972A434EBE@gmail.com>
 <EB55F396-C00B-42C0-A5DD-135713A4F337@gmail.com>
 <6EFB1ADF-22CE-4AD4-BC65-4B26A2B565A3@gmail.com>
 <6B279585-73BF-491C-ADFA-6746B4C5D04E@gmail.com>
 <5A64A994-05C0-4010-82D6-1FC745E826E5@gmail.com>
Message-ID: <ABEB9324-9DB8-4EC7-B4D5-73B25C54C275@gmail.com>

To elaborate some more, JMM is not about the behaviour of processes separated by space (running on different CPU cores communicating using physical connections limited by the speed of light), or time (running on a single CPU with preemptive multitasking). JMM is about the behaviour of processes subject to optimizations. Java program then is only a model of computation.

Measuring time delta then becomes an impossible task. If you place any sort of beacon taking the wall clock reading at two points in the Java program, what is the optimizer meant to do? Keep all the computations appearing between them in program order strictly between them at run time? What makes the time measurement so special?

But then what’s the meaning of “socket write appears after q.enq(x)”? Appears to whom? Java language spec only requires that it should appear so to T1. It doesn’t mean T1 measures time to observe that is so. It only means that the side effects of q.enq and socket write are observable by T1 as if appearing in that order. That’s the meaning of program order: things appearing in the program before the socket write cannot see side effects of the socket write.

T2 reading from socket is not bound by that constraint. So even an existence of an obvious constraint (but out-of-bounds as far as JMM is concerned) that the read of the socket cannot see the write “before” the write to the socket happens, it is not sufficient to agree that q.enq also happened before that read. If you were to demonstrate that within JMM, you would need a synchronizes-with edge joining the program orders of the two threads.



Also, linearizability is not meant to be in opposition to sequential consistency. Linearizable FIFO queue is (I think! I am not too strong on theory here) sequentially consistent, too - that is, any execution corresponds to some possible sequential ordering of enq and deq.

Linearizability is meant to describe a property of composable computations, unlike sequential consistency. If you look at the coarse grained events, you may agree the queue is sequentially consistent; but when you look into the finer grained events, you may not see sequential consistency there - CAS failing does not correspond to any possible sequential execution of enq and deq. Converse is also true: if you see the building blocks behave sequentially consistently (ok, CAS failing does correspond to a possible sequential execution of the lock acquire attempts), you will not necessarily see the whole behave sequentially consistently.

But linearizability is composable: if the building blocks are linearizable, then the whole is linearizable, too. It is not a substitute of sequential consistency. It is just a different correctness guarantee.


Alex

> On 28 Sep 2018, at 18:26, Andrew Ershov <andrershov at gmail.com> wrote:
> 
> 
> Thanks for your reply.
> Regarding your third answer: do you mean synchronizes-with edge between writeToSocket and readFromSocket? I don’t see in the spec, that external actions could form synchronizes-with edge.
> 
>> On 28 Sep 2018, at 19:17, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
>> 
>> 
>>> On 28 Sep 2018, at 17:41, Andrew Ershov <andrershov at gmail.com> wrote:
>>> 
>>> If there is no global time, does it make sense to talk about linearizability?
>> 
>> Yes. By defining a total order of synchronization events which all the threads agree on.
>> 
>> Global time is more than that. Global time is about agreeing on the time elapsed between any two events in such an order. But this is not a necessary condition for linearizability.
>> 
>>> For humans time is mostly about causality.
>> 
>> Happens-before captures that.
>> 
>>> What if we enrich my previous example with external actions (socket read/write) like this:
>>> T1
>>> q.enq(x)
>>> writeToSocket1()
>>> 
>>> T2
>>> readFromSocket2()
>>> q.enq(y)
>>> q.deq(y)
>>> 
>>> And there is an external actor, that receives message from socket1 as soon as enqueuing in the first thread is done and immediately writes to socket2 to unblock thread T2.
>>> What in JMM prohibits such execution?
>> 
>> For JMM to prohibit such execution, you need a synchronizes-with edge that from which absurd follows.
>> 
>> 
>> Alex
>> 
>> 
>>>> On 28 Sep 2018, at 17:53, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
>>>> 
>>>> On a serious note, there needs to be some clarity what time is meant in the definition of linearizability you are looking at.
>>>> 
>>>> There is no physical global time line in the real world. The models may assume an even weaker notion of time. (Hence a reference to happens-before and total orders - that’s a way to get out of the “what is time?” conundrum and at the same time specify what it means that they are atomic, and that everyone agrees on the order. Such a definition also allows for weaker-than-real-world time.)
>>>> 
>>>> Alex
>>>> 
>>>>> On 28 Sep 2018, at 16:47, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
>>>>> 
>>>>> Time is an illusion.
>>>>> 
>>>>> Alex
>>>>> 
>>>>>> On 28 Sep 2018, at 16:30, Andrew Ershov via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>>>>>> 
>>>>>> The “Art of Multiprocessor programming” claims that “synchronization events are linearizable: they are totally ordered, and all threads agree on the ordering”. JMM also agrees that there is a total order of synchronization actions, but linearizability assumes also time ordering, JMM says nothing that synchronization actions order is consistent with time.
>>>>>> In other words, if there is a queue q (protected by lock) and two threads T1 and T1. In the time order (assuming no method invocations overlap):
>>>>>> 1) T1 q.enq(x)
>>>>>> 2) T2 q.enq(y)
>>>>>> 3) T3 q.deq(y)
>>>>>> 
>>>>>> This execution is sequentially consistent, but it’s not linearizable. The total order of synchronization actions that allows this execution is 2 lock - 2 unlock - 3 lock - 3 unlock - 1 lock - 1 unlock. But it’s not consistent with time.
>>>>>> 
>>>>>> Any ideas on that?
>>>>>> 
>>>>>> Andrey
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>> 
>>>> 
>> 


From pavel.rappo at gmail.com  Mon Oct  1 10:47:54 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Mon, 1 Oct 2018 15:47:54 +0100
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <CAAWwtm-vfa41Vq+8JBygi2dvKkEqDXu-Oze0OvH42Rht51Fr-A@mail.gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CAAWwtm-vfa41Vq+8JBygi2dvKkEqDXu-Oze0OvH42Rht51Fr-A@mail.gmail.com>
Message-ID: <57B29429-F8AA-43E1-9809-F96844949ED8@gmail.com>



> On 28 Sep 2018, at 15:25, Dávid Karnok <akarnokd at gmail.com> wrote:
> 

> We call this trampolining or queue-drain in ReactiveX and the concept was already invented by the time we switched to such lock-free coordination. 

Wow! Did you recognize the idea from the description I gave or from those
implementations I referred to? Anyhow, it's good to learn one more name of this
idea. For the sake of tracing the origins and discovering more use cases could 
you maybe remember how exactly you learned about this concept? You said it had 
been already invented by the time you switched to it in ReactiveX? Did you see 
it somewhere?

> There are two typical implementations.

And more flavours thereof. Having or not having an executor is only one of the
choices an implementor has to make[^1]. The nature of the task, the way the 
backlog is represented, reentrancy, fairness and error handling -- all these are
important. The result is that a particular implementation may look quite
different from the examples you've provided. However, no matter which 
implementation we pick, the core stays the same. An executor, a cumulating 
backlog, and a state machine with at least 3 states[^2].

> The reason we don't have one abstraction for these in general is the inlining of other features/behavior of reactive operators while making them more concise and performant.

Maybe you could provide a couple of examples showing that abstracting out the
common bits would singnificantly hinder performance or conciseness? The reason
I'm asking is that I find discovering this idea is mentally difficult and 
implementing it is error-prone. I still hope we can do something about it.



[^1]: Though once again, a "calling thread executor" may help with the 
"always use an executor" choice.

[^2]: For example, we can't get away with switching to `AtomicBoolean wip` in
the examples you've provided, as this way we will risk missing tasks added to
the backlog. By the way, does "wip" stand for "Work In Progress"?


From pavel.rappo at gmail.com  Mon Oct  1 12:12:00 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Mon, 1 Oct 2018 17:12:00 +0100
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <CANPzfU8Z0cSOn4KArpNGiJccUyvCt9_Wx-scEHQX+CR9eLZ=9w@mail.gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CANPzfU8Z0cSOn4KArpNGiJccUyvCt9_Wx-scEHQX+CR9eLZ=9w@mail.gmail.com>
Message-ID: <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>

> On 30 Sep 2018, at 23:30, Viktor Klang <viktor.klang at gmail.com> wrote:
> 
> Come to think about it, besides the MutexExecutor, I've also got this and that.
> 

Oh my! It will take me some time to read Scala code. Sorry about that.
Meanwhile, maybe you could answer some questions I have on `MutexExecutor`?

1. I don't know whether this question should go to you or James Roper. The way
`MutexExecutor`[^1] is used in microprofile-reactive-streams-lightbend[^2]
hints that we could probably use a simpler specialization of the `MutexExecutor`
leveraging the fact that the task is always the same. Maybe only for the sake of 
generating less garbage?

2. Looking at line 63. Was there a particular reason you used busy-waiting
instead of, say, reusing `ConcurrentLinkedQueue` or making threads help each
other?

3. I have little experience with the VarHandle API, but I have a question :-) 
Would it broke the code if we switched to `getAcquire()` instead of `get()`,
line 63? The corresponding operation is `lazySet`, or VarHandle `setRelease`,
line 31.



[^1]: https://github.com/lightbend/microprofile-reactive-streams/blob/master/zerodep/src/main/java/com/lightbend/microprofile/reactive/streams/zerodep/MutexExecutor.java
[^2]: https://github.com/lightbend/microprofile-reactive-streams/blob/master/zerodep/src/main/java/com/lightbend/microprofile/reactive/streams/zerodep/BuiltGraph.java#L108

From gergg at cox.net  Mon Oct  1 13:54:42 2018
From: gergg at cox.net (Gregg Wonderly)
Date: Mon, 1 Oct 2018 12:54:42 -0500
Subject: [concurrency-interest] Linearizability of synchronization
	actions
In-Reply-To: <i8bs1y00H02hR0p018buHw>
References: <996621C8-40C3-485C-A4FF-9FE38A0BA59B@gmail.com>
 <A1D91EEA-D8C1-449F-BF16-EA972A434EBE@gmail.com>
 <EB55F396-C00B-42C0-A5DD-135713A4F337@gmail.com>
 <6EFB1ADF-22CE-4AD4-BC65-4B26A2B565A3@gmail.com>
 <6B279585-73BF-491C-ADFA-6746B4C5D04E@gmail.com>
 <5A64A994-05C0-4010-82D6-1FC745E826E5@gmail.com> <i8bs1y00H02hR0p018buHw>
Message-ID: <50FEC5CE-A2E7-448C-A7B5-684857AD2FA9@cox.net>

What you hint at here, is data flow program order.  That is, only the computations that a data item flow though are actually ordered relative to others.  Data flow is not really the way that many programmers consider anything they write in Java, since the programming language/environment is very much about linear statements (ordered by lines) and strictly sequential programming (loops, ifs, function calls) paradigms.  We’ve had this discussion before and I still think it’s not a good idea to really let the language and the JMM be so disconnected that you can have discussions about the JMM which discard language constructs as a form of execution order/control.

The reason why the lack of consistency is interesting is because it makes the hardware more effective when you hand it (or infer) details to allow better/parallel use of hardware resources.  However, the largest issue is that different hardware has very different requirements to be most effectively used.  And you are continuing to weaken the guarantees that the implementation used to “provide” by making “happens before” be the only guaranteed ordering that the “language” can provide, even though it’s the JMM that is implemented this way.  

In effect, the JMM is so weak in controllability by any language compiled to it, that allowing a language to have more strict linear/ordering execution guarantees than the JMM has, is extremely difficult.   So in the end, we only have the JMM.  It’s really no longer about a language compiling to the JMM being able to guarantee any observable execution beyond happens-before.

Thus the Java language is really not the consideration any more.  All we should discuss is the JMM and just stop confusion.  

Gregg

> On Oct 1, 2018, at 3:12 AM, Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> To elaborate some more, JMM is not about the behaviour of processes separated by space (running on different CPU cores communicating using physical connections limited by the speed of light), or time (running on a single CPU with preemptive multitasking). JMM is about the behaviour of processes subject to optimizations. Java program then is only a model of computation.
> 
> Measuring time delta then becomes an impossible task. If you place any sort of beacon taking the wall clock reading at two points in the Java program, what is the optimizer meant to do? Keep all the computations appearing between them in program order strictly between them at run time? What makes the time measurement so special?
> 
> But then what’s the meaning of “socket write appears after q.enq(x)”? Appears to whom? Java language spec only requires that it should appear so to T1. It doesn’t mean T1 measures time to observe that is so. It only means that the side effects of q.enq and socket write are observable by T1 as if appearing in that order. That’s the meaning of program order: things appearing in the program before the socket write cannot see side effects of the socket write.
> 
> T2 reading from socket is not bound by that constraint. So even an existence of an obvious constraint (but out-of-bounds as far as JMM is concerned) that the read of the socket cannot see the write “before” the write to the socket happens, it is not sufficient to agree that q.enq also happened before that read. If you were to demonstrate that within JMM, you would need a synchronizes-with edge joining the program orders of the two threads.
> 
> 
> 
> Also, linearizability is not meant to be in opposition to sequential consistency. Linearizable FIFO queue is (I think! I am not too strong on theory here) sequentially consistent, too - that is, any execution corresponds to some possible sequential ordering of enq and deq.
> 
> Linearizability is meant to describe a property of composable computations, unlike sequential consistency. If you look at the coarse grained events, you may agree the queue is sequentially consistent; but when you look into the finer grained events, you may not see sequential consistency there - CAS failing does not correspond to any possible sequential execution of enq and deq. Converse is also true: if you see the building blocks behave sequentially consistently (ok, CAS failing does correspond to a possible sequential execution of the lock acquire attempts), you will not necessarily see the whole behave sequentially consistently.
> 
> But linearizability is composable: if the building blocks are linearizable, then the whole is linearizable, too. It is not a substitute of sequential consistency. It is just a different correctness guarantee.
> 
> 
> Alex
> 
>> On 28 Sep 2018, at 18:26, Andrew Ershov <andrershov at gmail.com> wrote:
>> 
>> 
>> Thanks for your reply.
>> Regarding your third answer: do you mean synchronizes-with edge between writeToSocket and readFromSocket? I don’t see in the spec, that external actions could form synchronizes-with edge.
>> 
>>> On 28 Sep 2018, at 19:17, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
>>> 
>>> 
>>>> On 28 Sep 2018, at 17:41, Andrew Ershov <andrershov at gmail.com> wrote:
>>>> 
>>>> If there is no global time, does it make sense to talk about linearizability?
>>> 
>>> Yes. By defining a total order of synchronization events which all the threads agree on.
>>> 
>>> Global time is more than that. Global time is about agreeing on the time elapsed between any two events in such an order. But this is not a necessary condition for linearizability.
>>> 
>>>> For humans time is mostly about causality.
>>> 
>>> Happens-before captures that.
>>> 
>>>> What if we enrich my previous example with external actions (socket read/write) like this:
>>>> T1
>>>> q.enq(x)
>>>> writeToSocket1()
>>>> 
>>>> T2
>>>> readFromSocket2()
>>>> q.enq(y)
>>>> q.deq(y)
>>>> 
>>>> And there is an external actor, that receives message from socket1 as soon as enqueuing in the first thread is done and immediately writes to socket2 to unblock thread T2.
>>>> What in JMM prohibits such execution?
>>> 
>>> For JMM to prohibit such execution, you need a synchronizes-with edge that from which absurd follows.
>>> 
>>> 
>>> Alex
>>> 
>>> 
>>>>> On 28 Sep 2018, at 17:53, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
>>>>> 
>>>>> On a serious note, there needs to be some clarity what time is meant in the definition of linearizability you are looking at.
>>>>> 
>>>>> There is no physical global time line in the real world. The models may assume an even weaker notion of time. (Hence a reference to happens-before and total orders - that’s a way to get out of the “what is time?” conundrum and at the same time specify what it means that they are atomic, and that everyone agrees on the order. Such a definition also allows for weaker-than-real-world time.)
>>>>> 
>>>>> Alex
>>>>> 
>>>>>> On 28 Sep 2018, at 16:47, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
>>>>>> 
>>>>>> Time is an illusion.
>>>>>> 
>>>>>> Alex
>>>>>> 
>>>>>>> On 28 Sep 2018, at 16:30, Andrew Ershov via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>>>>>>> 
>>>>>>> The “Art of Multiprocessor programming” claims that “synchronization events are linearizable: they are totally ordered, and all threads agree on the ordering”. JMM also agrees that there is a total order of synchronization actions, but linearizability assumes also time ordering, JMM says nothing that synchronization actions order is consistent with time.
>>>>>>> In other words, if there is a queue q (protected by lock) and two threads T1 and T1. In the time order (assuming no method invocations overlap):
>>>>>>> 1) T1 q.enq(x)
>>>>>>> 2) T2 q.enq(y)
>>>>>>> 3) T3 q.deq(y)
>>>>>>> 
>>>>>>> This execution is sequentially consistent, but it’s not linearizable. The total order of synchronization actions that allows this execution is 2 lock - 2 unlock - 3 lock - 3 unlock - 1 lock - 1 unlock. But it’s not consistent with time.
>>>>>>> 
>>>>>>> Any ideas on that?
>>>>>>> 
>>>>>>> Andrey
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>> 
>>>>> 
>>> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jsampson at guidewire.com  Mon Oct  1 16:06:05 2018
From: jsampson at guidewire.com (Justin Sampson)
Date: Mon, 1 Oct 2018 20:06:05 +0000
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CANPzfU8Z0cSOn4KArpNGiJccUyvCt9_Wx-scEHQX+CR9eLZ=9w@mail.gmail.com>
 <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>
Message-ID: <1A87AC6B-5E25-4ECC-94D0-D2A69F08BA03@guidewire.com>

Interesting to see this discussion of a MutexExecutor concept, because
I've implemented something called GuardedExecutor that I'm planning to
share soon. I'll put it up in its own GitHub repo at some point, but for
now I've posted the API as a Guava feature request (since I see it as
the natural evolution of Guava's Monitor, which I wrote several years
ago):

https://github.com/google/guava/issues/3265

Once the code is posted I will start a new thread here to drum up
interest. Just wanted to share the idea on this thread in case anyone's
exploring similar use cases.

Cheers,
Justin


On 10/1/18, 9:12 AM, Pavel Rappo wrote:

> > On 30 Sep 2018, at 23:30, Viktor Klang <viktor.klang at gmail.com> wrote:
> > 
> > Come to think about it, besides the MutexExecutor, I've also got this and that.
> > 
> 
> Oh my! It will take me some time to read Scala code. Sorry about that.
> Meanwhile, maybe you could answer some questions I have on `MutexExecutor`?
> 
> 1. I don't know whether this question should go to you or James Roper. The way
> `MutexExecutor`[^1] is used in microprofile-reactive-streams-lightbend[^2]
> hints that we could probably use a simpler specialization of the `MutexExecutor`
> leveraging the fact that the task is always the same. Maybe only for the sake of 
> generating less garbage?
> 
> 2. Looking at line 63. Was there a particular reason you used busy-waiting
> instead of, say, reusing `ConcurrentLinkedQueue` or making threads help each
> other?
> 
> 3. I have little experience with the VarHandle API, but I have a question :-) 
> Would it broke the code if we switched to `getAcquire()` instead of `get()`,
> line 63? The corresponding operation is `lazySet`, or VarHandle `setRelease`,
> line 31.


From oleksandr.otenko at gmail.com  Mon Oct  1 17:28:59 2018
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Mon, 1 Oct 2018 22:28:59 +0100
Subject: [concurrency-interest] Linearizability of synchronization
	actions
In-Reply-To: <50FEC5CE-A2E7-448C-A7B5-684857AD2FA9@cox.net>
References: <996621C8-40C3-485C-A4FF-9FE38A0BA59B@gmail.com>
 <A1D91EEA-D8C1-449F-BF16-EA972A434EBE@gmail.com>
 <EB55F396-C00B-42C0-A5DD-135713A4F337@gmail.com>
 <6EFB1ADF-22CE-4AD4-BC65-4B26A2B565A3@gmail.com>
 <6B279585-73BF-491C-ADFA-6746B4C5D04E@gmail.com>
 <5A64A994-05C0-4010-82D6-1FC745E826E5@gmail.com> <i8bs1y00H02hR0p018buHw>
 <50FEC5CE-A2E7-448C-A7B5-684857AD2FA9@cox.net>
Message-ID: <FBA533D8-5C3F-4A87-9EA6-3E4F211959A4@gmail.com>

I am not sure I see what the problem is.

Single-threaded executions are unable to observe what the “real” execution order is through the means of the language. Multi-threaded executions don’t become more correct or substantially easier to reason about, if you make all accesses totally ordered. Race conditions are possible in Javascript just as well. Javascript can have concurrency unimaginable in Java, although its memory accesses are totally ordered, and its concurrency is cooperative.


Alex

> On 1 Oct 2018, at 18:54, Gregg Wonderly <gergg at cox.net> wrote:
> 
> What you hint at here, is data flow program order.  That is, only the computations that a data item flow though are actually ordered relative to others.  Data flow is not really the way that many programmers consider anything they write in Java, since the programming language/environment is very much about linear statements (ordered by lines) and strictly sequential programming (loops, ifs, function calls) paradigms.  We’ve had this discussion before and I still think it’s not a good idea to really let the language and the JMM be so disconnected that you can have discussions about the JMM which discard language constructs as a form of execution order/control.
> 
> The reason why the lack of consistency is interesting is because it makes the hardware more effective when you hand it (or infer) details to allow better/parallel use of hardware resources.  However, the largest issue is that different hardware has very different requirements to be most effectively used.  And you are continuing to weaken the guarantees that the implementation used to “provide” by making “happens before” be the only guaranteed ordering that the “language” can provide, even though it’s the JMM that is implemented this way.  
> 
> In effect, the JMM is so weak in controllability by any language compiled to it, that allowing a language to have more strict linear/ordering execution guarantees than the JMM has, is extremely difficult.   So in the end, we only have the JMM.  It’s really no longer about a language compiling to the JMM being able to guarantee any observable execution beyond happens-before.
> 
> Thus the Java language is really not the consideration any more.  All we should discuss is the JMM and just stop confusion.  
> 
> Gregg
> 
>> On Oct 1, 2018, at 3:12 AM, Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>> 
>> To elaborate some more, JMM is not about the behaviour of processes separated by space (running on different CPU cores communicating using physical connections limited by the speed of light), or time (running on a single CPU with preemptive multitasking). JMM is about the behaviour of processes subject to optimizations. Java program then is only a model of computation.
>> 
>> Measuring time delta then becomes an impossible task. If you place any sort of beacon taking the wall clock reading at two points in the Java program, what is the optimizer meant to do? Keep all the computations appearing between them in program order strictly between them at run time? What makes the time measurement so special?
>> 
>> But then what’s the meaning of “socket write appears after q.enq(x)”? Appears to whom? Java language spec only requires that it should appear so to T1. It doesn’t mean T1 measures time to observe that is so. It only means that the side effects of q.enq and socket write are observable by T1 as if appearing in that order. That’s the meaning of program order: things appearing in the program before the socket write cannot see side effects of the socket write.
>> 
>> T2 reading from socket is not bound by that constraint. So even an existence of an obvious constraint (but out-of-bounds as far as JMM is concerned) that the read of the socket cannot see the write “before” the write to the socket happens, it is not sufficient to agree that q.enq also happened before that read. If you were to demonstrate that within JMM, you would need a synchronizes-with edge joining the program orders of the two threads.
>> 
>> 
>> 
>> Also, linearizability is not meant to be in opposition to sequential consistency. Linearizable FIFO queue is (I think! I am not too strong on theory here) sequentially consistent, too - that is, any execution corresponds to some possible sequential ordering of enq and deq.
>> 
>> Linearizability is meant to describe a property of composable computations, unlike sequential consistency. If you look at the coarse grained events, you may agree the queue is sequentially consistent; but when you look into the finer grained events, you may not see sequential consistency there - CAS failing does not correspond to any possible sequential execution of enq and deq. Converse is also true: if you see the building blocks behave sequentially consistently (ok, CAS failing does correspond to a possible sequential execution of the lock acquire attempts), you will not necessarily see the whole behave sequentially consistently.
>> 
>> But linearizability is composable: if the building blocks are linearizable, then the whole is linearizable, too. It is not a substitute of sequential consistency. It is just a different correctness guarantee.
>> 
>> 
>> Alex
>> 
>>> On 28 Sep 2018, at 18:26, Andrew Ershov <andrershov at gmail.com> wrote:
>>> 
>>> 
>>> Thanks for your reply.
>>> Regarding your third answer: do you mean synchronizes-with edge between writeToSocket and readFromSocket? I don’t see in the spec, that external actions could form synchronizes-with edge.
>>> 
>>>> On 28 Sep 2018, at 19:17, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
>>>> 
>>>> 
>>>>> On 28 Sep 2018, at 17:41, Andrew Ershov <andrershov at gmail.com> wrote:
>>>>> 
>>>>> If there is no global time, does it make sense to talk about linearizability?
>>>> 
>>>> Yes. By defining a total order of synchronization events which all the threads agree on.
>>>> 
>>>> Global time is more than that. Global time is about agreeing on the time elapsed between any two events in such an order. But this is not a necessary condition for linearizability.
>>>> 
>>>>> For humans time is mostly about causality.
>>>> 
>>>> Happens-before captures that.
>>>> 
>>>>> What if we enrich my previous example with external actions (socket read/write) like this:
>>>>> T1
>>>>> q.enq(x)
>>>>> writeToSocket1()
>>>>> 
>>>>> T2
>>>>> readFromSocket2()
>>>>> q.enq(y)
>>>>> q.deq(y)
>>>>> 
>>>>> And there is an external actor, that receives message from socket1 as soon as enqueuing in the first thread is done and immediately writes to socket2 to unblock thread T2.
>>>>> What in JMM prohibits such execution?
>>>> 
>>>> For JMM to prohibit such execution, you need a synchronizes-with edge that from which absurd follows.
>>>> 
>>>> 
>>>> Alex
>>>> 
>>>> 
>>>>>> On 28 Sep 2018, at 17:53, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
>>>>>> 
>>>>>> On a serious note, there needs to be some clarity what time is meant in the definition of linearizability you are looking at.
>>>>>> 
>>>>>> There is no physical global time line in the real world. The models may assume an even weaker notion of time. (Hence a reference to happens-before and total orders - that’s a way to get out of the “what is time?” conundrum and at the same time specify what it means that they are atomic, and that everyone agrees on the order. Such a definition also allows for weaker-than-real-world time.)
>>>>>> 
>>>>>> Alex
>>>>>> 
>>>>>>> On 28 Sep 2018, at 16:47, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
>>>>>>> 
>>>>>>> Time is an illusion.
>>>>>>> 
>>>>>>> Alex
>>>>>>> 
>>>>>>>> On 28 Sep 2018, at 16:30, Andrew Ershov via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>>>>>>>> 
>>>>>>>> The “Art of Multiprocessor programming” claims that “synchronization events are linearizable: they are totally ordered, and all threads agree on the ordering”. JMM also agrees that there is a total order of synchronization actions, but linearizability assumes also time ordering, JMM says nothing that synchronization actions order is consistent with time.
>>>>>>>> In other words, if there is a queue q (protected by lock) and two threads T1 and T1. In the time order (assuming no method invocations overlap):
>>>>>>>> 1) T1 q.enq(x)
>>>>>>>> 2) T2 q.enq(y)
>>>>>>>> 3) T3 q.deq(y)
>>>>>>>> 
>>>>>>>> This execution is sequentially consistent, but it’s not linearizable. The total order of synchronization actions that allows this execution is 2 lock - 2 unlock - 3 lock - 3 unlock - 1 lock - 1 unlock. But it’s not consistent with time.
>>>>>>>> 
>>>>>>>> Any ideas on that?
>>>>>>>> 
>>>>>>>> Andrey
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>> 
>>>>>> 
>>>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From notcarl at google.com  Mon Oct  1 19:35:49 2018
From: notcarl at google.com (Carl Mastrangelo)
Date: Mon, 1 Oct 2018 16:35:49 -0700
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <CAAWwtm-vfa41Vq+8JBygi2dvKkEqDXu-Oze0OvH42Rht51Fr-A@mail.gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CAAWwtm-vfa41Vq+8JBygi2dvKkEqDXu-Oze0OvH42Rht51Fr-A@mail.gmail.com>
Message-ID: <CAAcqB+uY1_6A3xTUq44bD-FvNYVS9VktAfP5dyMYm6peSB0e_Q@mail.gmail.com>

I have a similar Executor to your dedicated example.  It's called
SerializingExecutor
<https://github.com/grpc/grpc-java/blob/v1.15.0/core/src/main/java/io/grpc/internal/SerializingExecutor.java#L36>
and does the same thing but with one fewer atomic operation.

I don't think it is easily extractable because some decisions cannot be
made automatically:

- What should happen on an exception?  Does the queue stop draining?
Should it power through it?  If it stops, should the next execute() call
keep going?
- Can a serializing executor wrap another serializing executor?  The
"serial" ness of this gets tricky fast with multiple, because the order of
executions can change.
- CLQ is MPMC, but it could feasibly be optimized to MPSC, since there is
only one thread draining the queue.  (and in special circumstances, be
SPSC).
- For single thread scenarios, serial execution can be done without
synchronization.   Its possible to avoid reentrancy more cheaply,
especially when wrapping a direct executor.  It's unclear what the
commonality is between the the properly synchronized and unsynchronized one
is.


On Fri, Sep 28, 2018 at 7:58 AM Dávid Karnok via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> We call this trampolining or queue-drain in ReactiveX and the concept was
> already invented by the time we switched to such lock-free coordination.
>
> There are two typical implementations. The first reuses one of the
> involved threads to do as much work as possible:
>
> concurrentQueue.offer(ThingsToDo);
> if (wip.getAndIncrement() == 0) {
>    do {
>       var todo =  concurrentQueue.poll();
>       handle(todo);
>    } while (wip.decrementAndGet() != 0);
> }
>
> Another one uses a dedicated executor (scheduler, worker, actor, etc.) to
> perform the loop body:
>
> concurrentQueue.offer(ThingsToDo);
> if (wip.getAndIncrement() == 0) {
>    executor.execute(() -> {
>       do {
>          var todo =  concurrentQueue.poll();
>          handle(todo);
>       } while (wip.decrementAndGet() != 0);
>    });
> }
>
> The latter has the benefit that the handle() method will run
> non-overlappingly even if the executor is multi-threaded; it will occupy
> one of its threads as long as possible and the atomic operation on wip
> makes sure the do-loop body gets executed exclusively as well. Also unlike
> the first one, this variant could be made fair by allowing it to get
> interleaved by other tasks on the same executor.
>
> The reason we don't have one abstraction for these in general is the
> inlining of other features/behavior of reactive operators while making them
> more concise and performant.
>
> Pavel Rappo via Concurrency-interest <concurrency-interest at cs.oswego.edu>
> ezt írta (időpont: 2018. szept. 28., P, 15:24):
>
>> Hello,
>>
>> Let me start this discussion as a branch of the "Reactive Streams utility
>> API"
>> thread here:
>>
>>
>> http://mail.openjdk.java.net/pipermail/core-libs-dev/2018-September/055671.html
>>
>> I would like to talk about a particular synchronization primitive I find
>> to be
>> repeatedly reinvented in Reactive Streams implementations. I hope that we
>> could
>> discuss different incarnations of this primitive and maybe extract it
>> into a
>> reusable component.
>>
>> The primitive in question assists a number of parties in executing tasks
>> in a
>> sequential and non-blocking fashion without having to use a dedicated
>> thread of
>> execution. Though specifics may vary, the core of the primitive consists
>> of a
>> tiny protocol for bidirectional communication between parties, a backlog
>> of
>> tasks and an executor to execute the tasks in. All this is packed into a
>> single
>> method for a party to invoke. When a party invokes the method, the method
>> adds
>> the passed task to the backlog and then checks whether the method must
>> handle
>> the task or the method may simply return. If the method must handle the
>> task,
>> then the method executes a special service task in the executor and
>> returns.
>> From that moment and up until the service task has finished all tasks
>> that are
>> added to the backlog are handled by this service task. The service task
>> finishes
>> when the backlog becomes empty.
>>
>> In other words, parties are busy with dumping their tasks on each other.
>> Either
>> a party gets lucky and can rest assured the task it has just added will be
>> looked after, or it ends up with executing its task and, possibly, many
>> others
>> dumped on it by its luckier peers. In an extreme case where the
>> underlying
>> executor is a "calling thread executor" (`Executor executor =
>> Runnable::run`),
>> an unlucky party can get really buried itself in work. A metaphor for this
>> strategy could be "work dumping" (or "work foisting").
>>
>> Without diving further into too much detail here are 2 components I would
>> like
>> to start with:
>>
>>   1. MutexExecutor
>>
>> https://github.com/lightbend/microprofile-reactive-streams/blob/master/zerodep/src/main/java/com/lightbend/microprofile/reactive/streams/zerodep/MutexExecutor.java
>>   2. SequentialScheduler[^1]
>>
>> http://hg.openjdk.java.net/jdk/jdk11/file/ee6f7a61f3a5/src/java.net.http/share/classes/jdk/internal/net/http/common/SequentialScheduler.java
>>
>> I believe that a similar (though a bit more sophisticated) primitive was
>> implemented
>> in `java.util.concurrent.SubmissionPublisher`.
>>
>> Do you think they fit into the idea described above? Could there be some
>> common
>> mechanics extracted? Is there any potentially reusable component? Are
>> there any
>> other contexts in which this can be used?
>>
>> Thanks,
>> -Pavel
>>
>> [^1]: `SequentialScheduler` evolved from a much simpler
>> `CooperativeHandler`
>> http://hg.openjdk.java.net/jdk9/sandbox/jdk/file/7c7a3c48196e/src/jdk.incubator.httpclient/share/classes/jdk/incubator/http/internal/websocket/CooperativeHandler.java
>> adding asynchronous tasks and an underlying executor.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> --
> Best regards,
> David Karnok
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181001/f1bbe393/attachment.html>

From pavel.rappo at gmail.com  Tue Oct  2 14:43:48 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Tue, 2 Oct 2018 19:43:48 +0100
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <CAAcqB+uY1_6A3xTUq44bD-FvNYVS9VktAfP5dyMYm6peSB0e_Q@mail.gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CAAWwtm-vfa41Vq+8JBygi2dvKkEqDXu-Oze0OvH42Rht51Fr-A@mail.gmail.com>
 <CAAcqB+uY1_6A3xTUq44bD-FvNYVS9VktAfP5dyMYm6peSB0e_Q@mail.gmail.com>
Message-ID: <7EB2B928-6F0C-4949-8E50-02D69B155BC7@gmail.com>

> On 2 Oct 2018, at 00:35, Carl Mastrangelo <notcarl at google.com> wrote:
> 
> I have a similar Executor to your dedicated example.  It's called SerializingExecutor and does the same thing but with one fewer atomic operation.   

Thanks for the link. On an unrelated note, I've looked through the repository
and now have a question. Are these related?

  https://github.com/grpc/grpc-java/blob/60a0b0c471d720b0546ee3a5b4fa4283635dfbcf/core/src/main/java/io/grpc/internal/SerializingExecutor.java

and

  https://github.com/google/guava/blob/0cd4e9faa1360da4a343f84cb275d6eda0c5e732/android/guava/src/com/google/common/util/concurrent/SequentialExecutor.java	

> I don't think it is easily extractable because some decisions cannot be made automatically:

What do you mean by "making decisions automatically"? I hope we could make these
decisions configurable.

> - What should happen on an exception?  Does the queue stop draining?  Should it power through it?  If it stops, should the next execute() call keep going?

How many practical strategies of handling execution errors in Executors are
there? If there are only a couple of strategies, we can enumerate them,
otherwise provide an extension mechanism (patterns Strategy, Template method, 
etc.)?

> - Can a serializing executor wrap another serializing executor?  The "serial" ness of this gets tricky fast with multiple, because the order of executions can change.

Could you maybe provide an example where the order of executions changes?

> - CLQ is MPMC, but it could feasibly be optimized to MPSC, since there is only one thread draining the queue.  (and in special circumstances, be SPSC).   

Right. One thread at a time. The said optimizations in queueing are possible,
sure. I'd be happy to discuss them.

> - For single thread scenarios, serial execution can be done without synchronization.   Its possible to avoid reentrancy more cheaply, especially when wrapping a direct executor.

I don't see why this can't be configurable.

>  It's unclear what the commonality is between the the properly synchronized and unsynchronized one is.

Do you mean the single-threaded case? If so, then more likely "it's unclear what
the difference is."


From pavel.rappo at gmail.com  Tue Oct  2 14:59:33 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Tue, 2 Oct 2018 19:59:33 +0100
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <1A87AC6B-5E25-4ECC-94D0-D2A69F08BA03@guidewire.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CANPzfU8Z0cSOn4KArpNGiJccUyvCt9_Wx-scEHQX+CR9eLZ=9w@mail.gmail.com>
 <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>
 <1A87AC6B-5E25-4ECC-94D0-D2A69F08BA03@guidewire.com>
Message-ID: <95912D11-D3AD-4FE1-9124-CA41C0F46417@gmail.com>

Hi Justin,

I've skimmed through the `Monitor` javadoc here

  https://google.github.io/guava/releases/snapshot-jre/api/docs/com/google/common/util/concurrent/Monitor.html

It looks like it's a neat tool. Unfortunately, I haven't had a chance to use it.
I'd be interested in looking at `GuardedExecutor` too. Please keep us posted.

Thanks,
-Pavel

> On 1 Oct 2018, at 21:06, Justin Sampson <jsampson at guidewire.com> wrote:
> 
> Interesting to see this discussion of a MutexExecutor concept, because
> I've implemented something called GuardedExecutor that I'm planning to
> share soon. I'll put it up in its own GitHub repo at some point, but for
> now I've posted the API as a Guava feature request (since I see it as
> the natural evolution of Guava's Monitor, which I wrote several years
> ago):
> 
> https://github.com/google/guava/issues/3265
> 
> Once the code is posted I will start a new thread here to drum up
> interest. Just wanted to share the idea on this thread in case anyone's
> exploring similar use cases.
> 
> Cheers,
> Justin


From notcarl at google.com  Tue Oct  2 20:12:27 2018
From: notcarl at google.com (Carl Mastrangelo)
Date: Tue, 2 Oct 2018 17:12:27 -0700
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <7EB2B928-6F0C-4949-8E50-02D69B155BC7@gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CAAWwtm-vfa41Vq+8JBygi2dvKkEqDXu-Oze0OvH42Rht51Fr-A@mail.gmail.com>
 <CAAcqB+uY1_6A3xTUq44bD-FvNYVS9VktAfP5dyMYm6peSB0e_Q@mail.gmail.com>
 <7EB2B928-6F0C-4949-8E50-02D69B155BC7@gmail.com>
Message-ID: <CAAcqB+td6dK1PE2dQXZm5MAuZ8QS0YZFVEfnq28gnbWPpCx79A@mail.gmail.com>

Responses inline
On Tue, Oct 2, 2018 at 11:43 AM Pavel Rappo <pavel.rappo at gmail.com> wrote:

> > On 2 Oct 2018, at 00:35, Carl Mastrangelo <notcarl at google.com> wrote:
> >
> > I have a similar Executor to your dedicated example.  It's called
> SerializingExecutor and does the same thing but with one fewer atomic
> operation.
>
> Thanks for the link. On an unrelated note, I've looked through the
> repository
> and now have a question. Are these related?
>
>
> https://github.com/grpc/grpc-java/blob/60a0b0c471d720b0546ee3a5b4fa4283635dfbcf/core/src/main/java/io/grpc/internal/SerializingExecutor.java
>
> and
>
>
> https://github.com/google/guava/blob/0cd4e9faa1360da4a343f84cb275d6eda0c5e732/android/guava/src/com/google/common/util/concurrent/SequentialExecutor.java
>
>
> > I don't think it is easily extractable because some decisions cannot be
> made automatically:
>
> What do you mean by "making decisions automatically"? I hope we could make
> these
> decisions configurable.
>
> > - What should happen on an exception?  Does the queue stop draining?
> Should it power through it?  If it stops, should the next execute() call
> keep going?
>
> How many practical strategies of handling execution errors in Executors are
> there? If there are only a couple of strategies, we can enumerate them,
> otherwise provide an extension mechanism (patterns Strategy, Template
> method,
> etc.)?
>

One of the cases that came up is what to do for interruption?  If the
answer is reset the interrupted bit and throw IE, the the executor has to
declare it in the the throws.  If it keeps going, any blocking operations
in the runnables are going to fail, cause all submitted tasks that block to
exit early.  (Which is weird).  SerializingExceutor is not thread
oriented.    Additionally, does it bubble up to the Thread's uncaught
exception handler?



>
> > - Can a serializing executor wrap another serializing executor?  The
> "serial" ness of this gets tricky fast with multiple, because the order of
> executions can change.
>
> Could you maybe provide an example where the order of executions changes?
>


Sure.  Suppose as an optimization, you wanted to avoid allocation
SerializingExecutors if one wraps another.  I.e. SE(SE(E)), where E is the
innermost.  There is not a reason to allocate a new outermost SE because
the inner one guarantees the sequential ordering.  Alternatively, If you
assumed wrapping one SE in another SE returned a new one, using them in
separate places may result in accidental serial execution even if you did
not want it.   This came up when Guava's sequentialExecutor() method was
made public.  The optimization (and any possibly confusion) was lost since
MoreExecutors.newSequentialExecutor() always returns a new instance.


>
> > - CLQ is MPMC, but it could feasibly be optimized to MPSC, since there
> is only one thread draining the queue.  (and in special circumstances, be
> SPSC).
>
> Right. One thread at a time. The said optimizations in queueing are
> possible,
> sure. I'd be happy to discuss them.
>

To wrap up the queue for external usage, you have to trust the caller knows
how many readers and writers there are.   That would make the API uglier,
for the possibility of having a faster queue.


>
> > - For single thread scenarios, serial execution can be done without
> synchronization.   Its possible to avoid reentrancy more cheaply,
> especially when wrapping a direct executor.
>
> I don't see why this can't be configurable.
>

You're right.  The issue is that with so many configuration options, it
doesn't really feel like there is much reuse, which was the original
point.  Also, too many options would be bad API design.


>
> >  It's unclear what the commonality is between the the properly
> synchronized and unsynchronized one is.
>
> Do you mean the single-threaded case? If so, then more likely "it's
> unclear what
> the difference is."
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181002/7e5ccba2/attachment.html>

From james at lightbend.com  Wed Oct  3 01:23:33 2018
From: james at lightbend.com (James Roper)
Date: Wed, 3 Oct 2018 15:23:33 +1000
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <57B29429-F8AA-43E1-9809-F96844949ED8@gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CAAWwtm-vfa41Vq+8JBygi2dvKkEqDXu-Oze0OvH42Rht51Fr-A@mail.gmail.com>
 <57B29429-F8AA-43E1-9809-F96844949ED8@gmail.com>
Message-ID: <CABY0rKMn3QYs9O=pxiffEWGA7DQ4c3TRT_8Hk63BGHQ75b7Ccg@mail.gmail.com>

On Tue., 2 Oct. 2018, 01:48 Pavel Rappo via Concurrency-interest, <
concurrency-interest at cs.oswego.edu> wrote:

>
>
> > On 28 Sep 2018, at 15:25, Dávid Karnok <akarnokd at gmail.com> wrote:
> >
>
> > We call this trampolining or queue-drain in ReactiveX and the concept
> was already invented by the time we switched to such lock-free
> coordination.
>
> Wow! Did you recognize the idea from the description I gave or from those
> implementations I referred to? Anyhow, it's good to learn one more name of
> this
> idea. For the sake of tracing the origins and discovering more use cases
> could
> you maybe remember how exactly you learned about this concept? You said it
> had
> been already invented by the time you switched to it in ReactiveX? Did you
> see
> it somewhere?
>

While all the executors mentioned are trampolining executors, trampolining
itself is an orthogonal concept to the mutually exclusive serialisation of
executed tasks - trampolining is a technique used to solve infinite
recursion problems, implemented by unrolling the stack and then bouncing
back to the callback. Not every trampolining executor implements mutually
exclusive execution of tasks, for example, you can implement a trampoline
using a thread local to hold the task queue, this achieves trampolining
(unrolling the stack to prevent infinite recursions), but does not achieve
mutual exclusion. One reason executors used by Reactive Streams tend to be
trampolining is that the Reactive Streams spec requires implementations to
prevent infinite recursion, so trampolines are useful for that.

This 2009 blog post by a former colleague talks about trampolines:

http://blog.richdougherty.com/2009/04/tail-calls-tailrec-and-trampolines.html?m=1

It references an earlier discussion of trampolines in Scala 2.8, but that
link is dead.


> > There are two typical implementations.
>
> And more flavours thereof. Having or not having an executor is only one of
> the
> choices an implementor has to make[^1]. The nature of the task, the way
> the
> backlog is represented, reentrancy, fairness and error handling -- all
> these are
> important. The result is that a particular implementation may look quite
> different from the examples you've provided. However, no matter which
> implementation we pick, the core stays the same. An executor, a cumulating
> backlog, and a state machine with at least 3 states[^2].
>
> > The reason we don't have one abstraction for these in general is the
> inlining of other features/behavior of reactive operators while making them
> more concise and performant.
>
> Maybe you could provide a couple of examples showing that abstracting out
> the
> common bits would singnificantly hinder performance or conciseness? The
> reason
> I'm asking is that I find discovering this idea is mentally difficult and
> implementing it is error-prone. I still hope we can do something about it.
>
>
>
> [^1]: Though once again, a "calling thread executor" may help with the
> "always use an executor" choice.
>
> [^2]: For example, we can't get away with switching to `AtomicBoolean wip`
> in
> the examples you've provided, as this way we will risk missing tasks added
> to
> the backlog. By the way, does "wip" stand for "Work In Progress"?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181003/eb7e8e10/attachment-0001.html>

From viktor.klang at gmail.com  Wed Oct  3 03:11:26 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 3 Oct 2018 09:11:26 +0200
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CANPzfU8Z0cSOn4KArpNGiJccUyvCt9_Wx-scEHQX+CR9eLZ=9w@mail.gmail.com>
 <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>
Message-ID: <CANPzfU-FdUq0mu72M8Lz1xMbnvPtzuxOL-ZHX87=d46_fc+A=A@mail.gmail.com>

On Mon, Oct 1, 2018 at 6:12 PM Pavel Rappo <pavel.rappo at gmail.com> wrote:

> > On 30 Sep 2018, at 23:30, Viktor Klang <viktor.klang at gmail.com> wrote:
> >
> > Come to think about it, besides the MutexExecutor, I've also got this
> and that.
> >
>
> Oh my! It will take me some time to read Scala code. Sorry about that.
> Meanwhile, maybe you could answer some questions I have on `MutexExecutor`?
>
> 1. I don't know whether this question should go to you or James Roper. The
> way
> `MutexExecutor`[^1] is used in microprofile-reactive-streams-lightbend[^2]
> hints that we could probably use a simpler specialization of the
> `MutexExecutor`
> leveraging the fact that the task is always the same. Maybe only for the
> sake of
> generating less garbage?
>

We do that in Akka—non-blocking "resubmission" of cached "Runnables".


>
> 2. Looking at line 63. Was there a particular reason you used busy-waiting
> instead of, say, reusing `ConcurrentLinkedQueue` or making threads help
> each
> other?
>

63 is to close the gap between 29 and 31.


>
> 3. I have little experience with the VarHandle API, but I have a question
> :-)
> Would it broke the code if we switched to `getAcquire()` instead of
> `get()`,
> line 63? The corresponding operation is `lazySet`, or VarHandle
> `setRelease`,
> line 31.
>

At first glance, yes that should be possible. Not sure there'll be any
material gain though. Others: please chime in.


>
>
>
> [^1]:
> https://github.com/lightbend/microprofile-reactive-streams/blob/master/zerodep/src/main/java/com/lightbend/microprofile/reactive/streams/zerodep/MutexExecutor.java
> [^2]:
> https://github.com/lightbend/microprofile-reactive-streams/blob/master/zerodep/src/main/java/com/lightbend/microprofile/reactive/streams/zerodep/BuiltGraph.java#L108



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181003/ed868ba6/attachment.html>

From pavel.rappo at gmail.com  Thu Oct  4 06:38:15 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Thu, 4 Oct 2018 11:38:15 +0100
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <CANPzfU-FdUq0mu72M8Lz1xMbnvPtzuxOL-ZHX87=d46_fc+A=A@mail.gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CANPzfU8Z0cSOn4KArpNGiJccUyvCt9_Wx-scEHQX+CR9eLZ=9w@mail.gmail.com>
 <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>
 <CANPzfU-FdUq0mu72M8Lz1xMbnvPtzuxOL-ZHX87=d46_fc+A=A@mail.gmail.com>
Message-ID: <D61FE631-4F84-400E-97B1-2C8ED3CE0030@gmail.com>

> On 3 Oct 2018, at 08:11, Viktor Klang <viktor.klang at gmail.com> wrote:
> 
> We do that in Akka—non-blocking "resubmission" of cached "Runnables".

Could you please send me a link to the corresponding code?

When the task is the same, not only does it make recreating this task
unnecessary, it also eliminates the need for the task queue. For example,
consider we were to implement a `Publisher<ByteBuffer>` of data read from a
socket. This publisher would publish data only if there are both unfulfilled
demand and data available. The check-and-publish task could then be executed
every time either the demand increases, or the socket becomes readable. If all
satisfied the task arranges for `SocketChannel.read` and `Subscriber.onNext` to
be called.

Since I started from this point of view I didn't see this primitive as an
executor, but rather a consumer of pending signals. No doubt the sequential
executor can be implemented on top of this primitive and vice-versa.

> 63 is to close the gap between 29 and 31.

Pardon, I might not have been clear. I understand the purpose of this particular
busy-waiting. What I was intending to ask is: why did you chose this approach
over others? I might seem irrational here, but I don't feel comfortable when I
see busy-waiting[^1]. So anytime it appears on the horizon, I try to think of
some other way of achieving the goal.

For example, one option could be to make threads finish updates for each other.
To achieve this the order of updates needs to be changed. First the "next" link
is updated, then the `last`. The downside is increased complexity. I can try to
sketch something if you are interested.

> Others: please chime in.

Yes, please! I'd like to understand the difference between visibility guarantees
for `lazySet` -> `get` and `setRelease` -> `getAcquire`. For some reason this 
document[^2] hasn't helped much.



[^1]: I realize that it may seem overkill to protect against a failure happening
between lines 29 and 31 that renders some other thread spinning forever. I also
realize that extra strain this puts on the CPU will probably be negligible in
most cases.
[^2]: http://gee.cs.oswego.edu/dl/html/j9mm.html


From viktor.klang at gmail.com  Thu Oct  4 10:06:41 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 4 Oct 2018 16:06:41 +0200
Subject: [concurrency-interest] Fwd: Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <CANPzfU9vjt6yxM+Z2Q6MjrHiEqFgJaagAW0O4A1JpTnBzB1Bhg@mail.gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CANPzfU8Z0cSOn4KArpNGiJccUyvCt9_Wx-scEHQX+CR9eLZ=9w@mail.gmail.com>
 <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>
 <CANPzfU-FdUq0mu72M8Lz1xMbnvPtzuxOL-ZHX87=d46_fc+A=A@mail.gmail.com>
 <D61FE631-4F84-400E-97B1-2C8ED3CE0030@gmail.com>
 <CANPzfU9vjt6yxM+Z2Q6MjrHiEqFgJaagAW0O4A1JpTnBzB1Bhg@mail.gmail.com>
Message-ID: <CANPzfU8ectSzjCSLWFXXTaTL=1c2SN-x6jA-H-Y8KZ5aiYuy1A@mail.gmail.com>

(Replying to list this time :-))


Pavel,

On Thu, Oct 4, 2018 at 12:38 PM Pavel Rappo <pavel.rappo at gmail.com> wrote:

> > On 3 Oct 2018, at 08:11, Viktor Klang <viktor.klang at gmail.com> wrote:
> >
> > We do that in Akka—non-blocking "resubmission" of cached "Runnables".
>
> Could you please send me a link to the corresponding code?
>

Yeah, I could probably see if I can link direct to it, however I'm not sure
it is applicable here since it deals with submissions to *any* Executor, no
matter if they are multithreaded or not.
It's just a mechanism for being able to resubmit (conditionally of course)
Runnables if they have not yet been scheduled (or if they are currently
running).


>
> When the task is the same, not only does it make recreating this task
> unnecessary, it also eliminates the need for the task queue. For example,
> consider we were to implement a `Publisher<ByteBuffer>` of data read from a
> socket. This publisher would publish data only if there are both
> unfulfilled
> demand and data available. The check-and-publish task could then be
> executed
> every time either the demand increases, or the socket becomes readable. If
> all
> satisfied the task arranges for `SocketChannel.read` and
> `Subscriber.onNext` to
> be called.
>

You also need to deal with fairness.


>
> Since I started from this point of view I didn't see this primitive as an
> executor, but rather a consumer of pending signals. No doubt the sequential
> executor can be implemented on top of this primitive and vice-versa.
>
> > 63 is to close the gap between 29 and 31.
>
> Pardon, I might not have been clear. I understand the purpose of this
> particular
> busy-waiting. What I was intending to ask is: why did you chose this
> approach
> over others?


It's the standard way of dealing with that situation. The other alternative
is to exit even if you know that there is a pending write, but you'll need
to recheck later anyway, and the only way to know when that is is either by
implementing Monitors/wait-lists and then do the park/unpark dance, which
is definitely going to be more expensive in the general case for a queue.

Also, remember that the "missed" write (or rather the busy-spin) will only
occur when the reader has caught up with the writers.


> I might seem irrational here, but I don't feel comfortable when I
> see busy-waiting[^1]. So anytime it appears on the horizon, I try to think
> of
> some other way of achieving the goal.
>

Sure, I'm not a huge fan of busy-waiting either, but for all intents and
purposes, a naïve implementation of something like
incrementAndGet/getAndIncrement could potentially be busy-waiting forever
(assuming some form of unfair scheduling). Same for many CAS+retry
constructs.


>
> For example, one option could be to make threads finish updates for each
> other.
> To achieve this the order of updates needs to be changed. First the "next"
> link
> is updated, then the `last`. The downside is increased complexity. I can
> try to
> sketch something if you are interested.
>

I haven't found it to be a problem, but if you have any measurement that it
is, I'd be more than happy to have a look.


>
> > Others: please chime in.
>
> Yes, please! I'd like to understand the difference between visibility
> guarantees
> for `lazySet` -> `get` and `setRelease` -> `getAcquire`. For some reason
> this
> document[^2] hasn't helped much.
>
>
>
> [^1]: I realize that it may seem overkill to protect against a failure
> happening
> between lines 29 and 31 that renders some other thread spinning forever. I
> also
> realize that extra strain this puts on the CPU will probably be negligible
> in
> most cases.
> [^2]: http://gee.cs.oswego.edu/dl/html/j9mm.html
>
>

-- 
Cheers,
√
-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181004/f5fe4950/attachment.html>

From dl at cs.oswego.edu  Thu Oct  4 13:14:21 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 4 Oct 2018 13:14:21 -0400
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <D61FE631-4F84-400E-97B1-2C8ED3CE0030@gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CANPzfU8Z0cSOn4KArpNGiJccUyvCt9_Wx-scEHQX+CR9eLZ=9w@mail.gmail.com>
 <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>
 <CANPzfU-FdUq0mu72M8Lz1xMbnvPtzuxOL-ZHX87=d46_fc+A=A@mail.gmail.com>
 <D61FE631-4F84-400E-97B1-2C8ED3CE0030@gmail.com>
Message-ID: <9be0e267-1e2b-db37-924d-57341f4a075e@cs.oswego.edu>

On 10/04/2018 06:38 AM, Pavel Rappo via Concurrency-interest wrote:

> Yes, please! I'd like to understand the difference between visibility guarantees
> for `lazySet` -> `get` and `setRelease` -> `getAcquire`. For some reason this 
> document[^2] hasn't helped much.
> [^2]: http://gee.cs.oswego.edu/dl/html/j9mm.html

In general setRelease must be matched with getAcquire for anything
producer-consumer-like that doesn't include Dekker-like scenarios
encountered with blocking synchronization (in which case you generally
need {get,set,cas}Volatile). It is a little confusing here because the
default "get" for AtomicReference is equivalent to VarHandle getVolatile
(coping the best we could with evolving language/APIs). Also, whenever a
read will be validated with a CAS, you could weaken to
AtomicReference.getPlain === VarHandle.get. And the name lazySet is
being retired in part because the name is so bad (use setRelease). Maybe
we should denigrate (deprecate not-for-removal)

While I'm at it: I've been staying out of discussion so far about added
j.u.c support for reactive streams applications, to see where it heads.
One underlying issue seems to be whether any of the component
functionality of SubmissionPublisher should/could be made available in
streamlined form. (BTW, note that it can works with an
Executors.newSingleThreadExecutor.)

Also, as a secondary aside. I like Justin's Guava GuardedExecutor, but
I'm undecided whether it or something like it should be introduced in
j.u.c.

-Doug


From pavel.rappo at gmail.com  Thu Oct  4 13:41:10 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Thu, 4 Oct 2018 18:41:10 +0100
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <CABY0rKMn3QYs9O=pxiffEWGA7DQ4c3TRT_8Hk63BGHQ75b7Ccg@mail.gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CAAWwtm-vfa41Vq+8JBygi2dvKkEqDXu-Oze0OvH42Rht51Fr-A@mail.gmail.com>
 <57B29429-F8AA-43E1-9809-F96844949ED8@gmail.com>
 <CABY0rKMn3QYs9O=pxiffEWGA7DQ4c3TRT_8Hk63BGHQ75b7Ccg@mail.gmail.com>
Message-ID: <FF4D50AF-3149-469D-9B5C-7D893F10F683@gmail.com>

> On 3 Oct 2018, at 06:23, James Roper <james at lightbend.com> wrote:
> 
> 
> While all the executors mentioned are trampolining executors, trampolining itself is an orthogonal concept to the mutually exclusive serialisation of executed tasks - trampolining is a technique used to solve infinite recursion problems, implemented by unrolling the stack and then bouncing back to the callback. Not every trampolining executor implements mutually exclusive execution of tasks, for example, you can implement a trampoline using a thread local to hold the task queue, this achieves trampolining (unrolling the stack to prevent infinite recursions), but does not achieve mutual exclusion. One reason executors used by Reactive Streams tend to be trampolining is that the Reactive Streams spec requires implementations to prevent infinite recursion, so trampolines are useful for that.
> 
> This 2009 blog post by a former colleague talks about trampolines:
> 
> http://blog.richdougherty.com/2009/04/tail-calls-tailrec-and-trampolines.html?m=1
> 
> It references an earlier discussion of trampolines in Scala 2.8, but that link is dead.

Thanks for the important clarification and the links. Thankfully, not all dead
links have gone forever:

  https://web.archive.org/web/20090812090720/http://www.nabble.com:80/-scala--Manifest,-implicits-and-apply-for-DSL-td22381717.html


From pavel.rappo at gmail.com  Fri Oct  5 10:29:09 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Fri, 5 Oct 2018 15:29:09 +0100
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <CAAcqB+td6dK1PE2dQXZm5MAuZ8QS0YZFVEfnq28gnbWPpCx79A@mail.gmail.com>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CAAWwtm-vfa41Vq+8JBygi2dvKkEqDXu-Oze0OvH42Rht51Fr-A@mail.gmail.com>
 <CAAcqB+uY1_6A3xTUq44bD-FvNYVS9VktAfP5dyMYm6peSB0e_Q@mail.gmail.com>
 <7EB2B928-6F0C-4949-8E50-02D69B155BC7@gmail.com>
 <CAAcqB+td6dK1PE2dQXZm5MAuZ8QS0YZFVEfnq28gnbWPpCx79A@mail.gmail.com>
Message-ID: <F7390EF2-D5BF-4CEC-8EBA-B2DEEBDBF83F@gmail.com>

> On 3 Oct 2018, at 01:12, Carl Mastrangelo <notcarl at google.com> wrote:
> 
> One of the cases that came up is what to do for interruption?  If the answer is reset the interrupted bit and throw IE, the the executor has to declare it in the the throws.  If it keeps going, any blocking operations in the runnables are going to fail, cause all submitted tasks that block to exit early.  (Which is weird).  SerializingExceutor is not thread oriented.    Additionally, does it bubble up to the Thread's uncaught exception handler?

This question should be answered in the course of implementation. I must say
that it's getting more difficult for me to answer questions like this one
without writing the corresponding code.
 
> Could you maybe provide an example where the order of executions changes?
> 
> 
> Sure.  Suppose as an optimization, you wanted to avoid allocation SerializingExecutors if one wraps another.  I.e. SE(SE(E)), where E is the innermost.  There is not a reason to allocate a new outermost SE because the inner one guarantees the sequential ordering.  Alternatively, If you assumed wrapping one SE in another SE returned a new one, using them in separate places may result in accidental serial execution even if you did not want it.   This came up when Guava's sequentialExecutor() method was made public.  The optimization (and any possibly confusion) was lost since MoreExecutors.newSequentialExecutor() always returns a new instance.

I'm not following. Whether a factory method creates a new executor or reuses an
existing one shouldn't be a problem, if an end result doesn't violate the
properties of the said executor. What do you mean by "accidental serial
execution"? From the point of view of a user, once they have an instance of an
executor that executes their tasks sequentially, they should not care whether or
not this executor is shared.

Please provide relevant links to "This came up when Guava's sequentialExecutor()
method was made public" discussions, so we could better understand the problem.
I've only found the revisions where the method was introduced[^1] and then
renamed[^2]. Unfortunately, those changes do not tell the whole story.

> To wrap up the queue for external usage, you have to trust the caller knows how many readers and writers there are.   That would make the API uglier, for the possibility of having a faster queue.

As you've said, the queue can be drained by (at most) one thread at a time.
Hence, there doesn't seem to be the need for a "Multiple Producers - Multiple
Consumers" queue. Again, we could start with something and iterate, gradually
answering these questions.



[^1]: https://github.com/google/guava/commit/89fb0654edc987741441c5329fee606a4a2a9224#diff-7a493427c77df959295f1d55574dbd50
[^2]: https://github.com/google/guava/commit/1c760101a7fcd8798eb0c6ad3c277ff190bcb22f#diff-7a493427c77df959295f1d55574dbd50


From pavel.rappo at gmail.com  Fri Oct  5 10:55:15 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Fri, 5 Oct 2018 15:55:15 +0100
Subject: [concurrency-interest] Synchronization primitives in Reactive
 Streams implementations
In-Reply-To: <9be0e267-1e2b-db37-924d-57341f4a075e@cs.oswego.edu>
References: <9B8CD5A4-9259-478F-B35E-06CE87CFE008@gmail.com>
 <CANPzfU8Z0cSOn4KArpNGiJccUyvCt9_Wx-scEHQX+CR9eLZ=9w@mail.gmail.com>
 <371908D1-7BD8-417B-B004-894637EADBB1@gmail.com>
 <CANPzfU-FdUq0mu72M8Lz1xMbnvPtzuxOL-ZHX87=d46_fc+A=A@mail.gmail.com>
 <D61FE631-4F84-400E-97B1-2C8ED3CE0030@gmail.com>
 <9be0e267-1e2b-db37-924d-57341f4a075e@cs.oswego.edu>
Message-ID: <4E1D9FCA-9519-498C-B35C-EB082F95EA0F@gmail.com>

> On 4 Oct 2018, at 18:14, Doug Lea via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> (BTW, note that it can works with an
> Executors.newSingleThreadExecutor.)

Yes, the executor returned by the `Executors.newSingleThreadExecutor` method
provides the same _service_ for the user as does the `SequentialExecutor` (or
similar). It's the _implementation_ that might not be satisfactory for some
cases. I think resource-wise this has a lot in common with the problem of
effective mapping of M lightweight threads to N threads.


From rl.stpuu at gmail.com  Thu Oct  4 12:34:28 2018
From: rl.stpuu at gmail.com (Roussanka Loukanova)
Date: Thu, 4 Oct 2018 18:34:28 +0200
Subject: [concurrency-interest] CfP: Natural Language Processing in
	Artificial Intelligence - NLPinAI 2019
Message-ID: <CACAe74iOHLmoUQpPOqee8c_O-sQNk02zF24Jx0CF3K3AoB8gow@mail.gmail.com>

CALL FOR PAPERS

Natural Language Processing in Artificial Intelligence - NLPinAI 2019
19 - 21 February, 2019 - Prague, Czech Republic

http://www.icaart.org/NLPinAI.aspx

Special Session within the 11th International Conference on Agents and
Artificial Intelligence - ICAART 2019
http://www.icaart.org

-------------------------------------------------------------
SCOPE:
Computational and technological developments that incorporate natural
language are proliferating. Adequate coverage encounters difficult problems
related to partiality, underspecification, and context-dependency, which
are signature features of information in nature and natural languages.
Furthermore, agents (humans or computational systems) are information
conveyors, interpreters, or participate as components of informational
content. Generally, language processing depends on agents' knowledge,
reasoning, perspectives, and interactions.

The session covers theoretical work, applications, approaches, and
techniques for computational models of information and its presentation by
language (artificial, human, or natural in other ways). The goal is to
promote intelligent natural language processing and related models of
thought, mental states, reasoning, and other cognitive processes.

TOPICS:
We invite contributions relevant to the following topics, without being
limited to them:

- Type theories for applications to language and information processing
- Computational grammar
- Computational syntax
- Computational semantics of natural languages
- Computational syntax-semantics interface
- Interfaces between morphology, lexicon, syntax, semantics, speech, text,
pragmatics
- Parsing
- Multilingual processing
- Large-scale grammars of natural languages
- Interfaces between morphology, lexicon, syntax, semantics, speech, text,
pragmatics
- Models of computation and algorithms for natural language processing
- Computational models of partiality, underspecification, and
context-dependency
- Models of situations, contexts, and agents, for applications to language
processing
- Information about space and time in language models and processing
- Models of computation and algorithms for linguistics
- Data science in language processing
- Machine learning of language
- Interdisciplinary methods
- Integration of formal, computational, model theoretic, graphical,
diagrammatic, statistical, and other related methods
- Logic for information extraction or expression in written and spoken
language
- Language processing based on biological fundamentals of information and
languages
- Computational neuroscience of language

IMPORTANT DATES:
Paper Submission:                      December 20, 2018
Authors Notification:                  January 7, 2019
Camera Ready and Registration: January 15, 2019

PAPER SUBMISSION:
Authors can submit their work in the form of a Regular Paper, representing
completed and validated research, or as a Position Paper, for preliminary
work in progress.

Regular Papers
- Submission:
It is recommended that Regular Papers are submitted for review with around
8 to 10 pages
- Acceptance:
After a double-blind peer review, qualifying Regular Papers may be accepted
as either Full Papers or Short Papers
- Publication:
Regular Papers classified as Full Papers will be assigned a 12-page limit
in the Conference Proceedings, while Regular Papers classified as Short
Papers have an 8-page limit

Position Papers
- Submission:
Position Papers should be submitted for review with around 6 or 7 pages
- Acceptance:
After a double-blind peer review, qualifying Position Papers will be
accepted as Short Papers
- Publication: Position Papers will be assigned a 8-page limit in the
Conference Proceedings

Instructions for preparing the manuscript (in Word and Latex formats) are
available at the page with paper Templates:

http://www.icaart.org/Templates.aspx

Please also check the Guidelines:

http://www.icaart.org/Guidelines.aspx

Papers must be submitted electronically via the web-based submission system
using the appropriated button Submit Paper on the pages of NLPinAI 2019.

The Conference Proceedings will be published under an ISBN number by
SCITEPRESS and include final versions of all accepted papers, adjusted to
satisfy reviewers' recommendations. They will be obtainable on paper and
CD-Rom support, and made available for online consultation at the
SCITEPRESS Digital Library. Online publication is exclusive to papers which
have been both published and presented at the event.

Indexation: The proceedings will be submitted to Thomson Reuters Conference
Proceedings Citation Index (CPCI/ISI), INSPEC, DBLP, EI (Elsevier
Engineering Village Index) and Scopus for indexation.

-------------------------------------------------------------
CHAIRS:
Roussanka Loukanova
Stockholm University, Sweden

CONTACT:
Roussanka Loukanova (rloukanova at gmail.com)
-------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181004/4239ade4/attachment.html>

From shevek at anarres.org  Sun Oct 14 23:23:04 2018
From: shevek at anarres.org (Shevek)
Date: Sun, 14 Oct 2018 20:23:04 -0700
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
Message-ID: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>

We hit a serious performance degradation when we used too many 
ThreadLocal instances in a process, as a consequence of which we are 
avoiding using instance-ThreadLocal(s), but we are looking for an 
alternative pattern for caching sub-computation state.

Would:
   static ThreadLocal<WeakHashMap<Instance, Value>>
be a good pattern?

What other observations or suggestions do people have?

S.

From andrey.a.pavlenko at gmail.com  Mon Oct 15 11:31:16 2018
From: andrey.a.pavlenko at gmail.com (Andrey Pavlenko)
Date: Mon, 15 Oct 2018 18:31:16 +0300
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
In-Reply-To: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
References: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
Message-ID: <CAHSUVA7k5S7dETqE2zXoc20MyvoWhRz42a8w+yy-HxDKP1vEiw@mail.gmail.com>

I would suggest a custom ThreadFactory. I believe the following code is
much faster than ThreadLocals:

MyAppThread t = (MyAppThread) Thread.currentThread();
t.getSomething()

On Mon, Oct 15, 2018 at 6:27 AM Shevek via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> We hit a serious performance degradation when we used too many
> ThreadLocal instances in a process, as a consequence of which we are
> avoiding using instance-ThreadLocal(s), but we are looking for an
> alternative pattern for caching sub-computation state.
>
> Would:
>    static ThreadLocal<WeakHashMap<Instance, Value>>
> be a good pattern?
>
> What other observations or suggestions do people have?
>
> S.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181015/d47ab53b/attachment.html>

From gergg at cox.net  Mon Oct 15 12:48:12 2018
From: gergg at cox.net (Gregg Wonderly)
Date: Mon, 15 Oct 2018 11:48:12 -0500
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
In-Reply-To: <nrYQ1y00w02hR0p01rYSXG>
References: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
 <nrYQ1y00w02hR0p01rYSXG>
Message-ID: <CD087543-3B9B-49D0-9836-F8D0F8505F27@cox.net>

Subclassing Thread invalidates the use of the security manager.  If there is thread specific context, but it into a class that implements Runnable.  You can then, already have view of this other class in its Runnable implementation. And have a single object to store as a thread local instead of many different values.  

When you need more optimal execution, pass this Runnable class reference (implementing another interface or as a value class instance) in your API design.

Also note that you can also use a worker pattern which is constructed with this object and the invoked with the thread do that the context is always present without the Runnable implementation class owning that logic.

Gregg

> On Oct 15, 2018, at 10:31 AM, Andrey Pavlenko via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> I would suggest a custom ThreadFactory. I believe the following code is much faster than ThreadLocals:
> 
> MyAppThread t = (MyAppThread) Thread.currentThread();
> t.getSomething()
> 
>> On Mon, Oct 15, 2018 at 6:27 AM Shevek via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>> We hit a serious performance degradation when we used too many 
>> ThreadLocal instances in a process, as a consequence of which we are 
>> avoiding using instance-ThreadLocal(s), but we are looking for an 
>> alternative pattern for caching sub-computation state.
>> 
>> Would:
>>    static ThreadLocal<WeakHashMap<Instance, Value>>
>> be a good pattern?
>> 
>> What other observations or suggestions do people have?
>> 
>> S.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181015/071baa14/attachment.html>

From david.lloyd at redhat.com  Mon Oct 15 12:54:48 2018
From: david.lloyd at redhat.com (David Lloyd)
Date: Mon, 15 Oct 2018 11:54:48 -0500
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
In-Reply-To: <CD087543-3B9B-49D0-9836-F8D0F8505F27@cox.net>
References: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
 <CD087543-3B9B-49D0-9836-F8D0F8505F27@cox.net>
Message-ID: <CANghgrS-pic_o9ZbgBFtNx0+gq6qzkgLwaCurbK78EvfmwSuXQ@mail.gmail.com>

On Mon, Oct 15, 2018 at 11:50 AM Gregg Wonderly via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> Subclassing Thread invalidates the use of the security manager.

I'm pretty sure that this is false.  Even if it were somehow true, it
seems unlikely to be a significant consideration in most modern
applications.
-- 
- DML

From shevek at anarres.org  Mon Oct 15 14:04:16 2018
From: shevek at anarres.org (Shevek)
Date: Mon, 15 Oct 2018 11:04:16 -0700
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
In-Reply-To: <5e993a7d-eeb7-2e6b-be49-64783ecc8cba@gmail.com>
References: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
 <5e993a7d-eeb7-2e6b-be49-64783ecc8cba@gmail.com>
Message-ID: <9d4310eb-f435-daf2-38b9-bb1513b35a35@anarres.org>

The trouble is that ThreadLocalMap isn't a HashMap, and does not have 
elegant degradation on over-size. perf and related tools told me that we 
were spending the majority of the application time in 
ThreadLocalMap.getEntryAfterMiss() after many thousand instance 
ThreadLocal(s) had been created and GC'd; this wasn't an accident of the 
inliner or anything, it's real.

Short version: Long-lived threads with many short-lived ThreadLocals 
sucks really hard.

At a wild guess, the table filled up with tombstones and either the 
tombstones aren't GCing fast enough due to touching the WeakHashMap 
(vague memory you used to be able to inhibit Reference cleanup that way) 
or ...? And the consequence of all this was that we went full linear 
search. I ran out of both hypotheses and time at this stage, but I still 
need a practical solution for the application.

Based on a set of other great replies, here are the requirements:

* Must garbage-collect if the thread ends.
* Must garbage-collect if the instance garbage-collects.
* Must be compatible with ForkJoinPool threads, i.e. no subclasses of 
Thread.

So at this point, unless I'm missing something, we're pretty much 
restricted to a WeakHashMap<Thread, ?> in the instance or a 
WeakHashMap<Instance, ?> in the Thread.

Then my question is, do I have a singleton PerInstanceThreadLocal so 
that there is exactly one WeakHashMap per Thread, for all clients, and 
just key that on instance, or do I want to have a separate WeakHashMap 
per client in its own non-singleton (but still static) 
PerInstanceThreadLocal?

// Do I make this class be a singleton?
// Is one big WeakHashMap better than many small ones?
// Does my code suck and did I miss a dumb trick (again)?

public class PerInstanceThreadLocal<Instance, Value>
     extends ThreadLocal<Map<Instance, Value>> {

     public class Accessor {

         private final Instance instance;
         private final Function<Instance, Value> constructor;

         public Accessor(@Nonnull Instance instance, @Nonnull 
Supplier<Value> supplier) {
             this.instance = Preconditions.checkNotNull(instance, 
"Instance was null.");
             this.constructor = i -> supplier.get();
         }

         public Accessor(@Nonnull Instance instance) {
             this(instance, Suppliers.ofInstance(null));
         }

         @CheckForNull
         public Value get() {
             Map<Instance, Value> map = PerInstanceThreadLocal.this.get();
             return map.computeIfAbsent(instance, constructor);
         }
     }

     @Override
     protected Map<Instance, Value> initialValue() {
         return new WeakHashMap<>();
     }
}

S.

On 10/15/2018 06:28 AM, Nathan and Ila Reynolds wrote:
> ThreadLocal works by calling Thread.currentThread() and then getting the 
> Map field in Thread.  From there, it calls Map.get(ThreadLocal) to get 
> the value.  This alternative pattern is going to execute 
> WeakHashMap.get(Instance) and cause further memory indirections.  These 
> memory indirections could hurt performance.
> 
> Another pattern is ConcurrentHashMap<Thread, WeakHashMap<Instance, 
> Value>>.  This is probably not an improvement over what you already 
> proposed.
> 
> I am curious as to the details of the "serious performance 
> degradation".  Please share what you can.
> 
> -Nathan
> 
> On 10/14/2018 9:23 PM, Shevek via Concurrency-interest wrote:
>> We hit a serious performance degradation when we used too many 
>> ThreadLocal instances in a process, as a consequence of which we are 
>> avoiding using instance-ThreadLocal(s), but we are looking for an 
>> alternative pattern for caching sub-computation state.
>>
>> Would:
>>   static ThreadLocal<WeakHashMap<Instance, Value>>
>> be a good pattern?
>>
>> What other observations or suggestions do people have?
>>
>> S.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From andrey.a.pavlenko at gmail.com  Mon Oct 15 14:21:39 2018
From: andrey.a.pavlenko at gmail.com (Andrey Pavlenko)
Date: Mon, 15 Oct 2018 21:21:39 +0300
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
In-Reply-To: <9d4310eb-f435-daf2-38b9-bb1513b35a35@anarres.org>
References: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
 <5e993a7d-eeb7-2e6b-be49-64783ecc8cba@gmail.com>
 <9d4310eb-f435-daf2-38b9-bb1513b35a35@anarres.org>
Message-ID: <CAHSUVA5kKts138riOvgoybTANZ8HuE7g8rJJoZB526vsBCD2yQ@mail.gmail.com>

> Must be compatible with ForkJoinPool threads, i.e. no subclasses of
Thread.

Why no subclasses of Thread.? You may have a custom
ForkJoinWorkerThreadFactory.

On Mon, Oct 15, 2018 at 9:08 PM Shevek via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> The trouble is that ThreadLocalMap isn't a HashMap, and does not have
> elegant degradation on over-size. perf and related tools told me that we
> were spending the majority of the application time in
> ThreadLocalMap.getEntryAfterMiss() after many thousand instance
> ThreadLocal(s) had been created and GC'd; this wasn't an accident of the
> inliner or anything, it's real.
>
> Short version: Long-lived threads with many short-lived ThreadLocals
> sucks really hard.
>
> At a wild guess, the table filled up with tombstones and either the
> tombstones aren't GCing fast enough due to touching the WeakHashMap
> (vague memory you used to be able to inhibit Reference cleanup that way)
> or ...? And the consequence of all this was that we went full linear
> search. I ran out of both hypotheses and time at this stage, but I still
> need a practical solution for the application.
>
> Based on a set of other great replies, here are the requirements:
>
> * Must garbage-collect if the thread ends.
> * Must garbage-collect if the instance garbage-collects.
> * Must be compatible with ForkJoinPool threads, i.e. no subclasses of
> Thread.
>
> So at this point, unless I'm missing something, we're pretty much
> restricted to a WeakHashMap<Thread, ?> in the instance or a
> WeakHashMap<Instance, ?> in the Thread.
>
> Then my question is, do I have a singleton PerInstanceThreadLocal so
> that there is exactly one WeakHashMap per Thread, for all clients, and
> just key that on instance, or do I want to have a separate WeakHashMap
> per client in its own non-singleton (but still static)
> PerInstanceThreadLocal?
>
> // Do I make this class be a singleton?
> // Is one big WeakHashMap better than many small ones?
> // Does my code suck and did I miss a dumb trick (again)?
>
> public class PerInstanceThreadLocal<Instance, Value>
>      extends ThreadLocal<Map<Instance, Value>> {
>
>      public class Accessor {
>
>          private final Instance instance;
>          private final Function<Instance, Value> constructor;
>
>          public Accessor(@Nonnull Instance instance, @Nonnull
> Supplier<Value> supplier) {
>              this.instance = Preconditions.checkNotNull(instance,
> "Instance was null.");
>              this.constructor = i -> supplier.get();
>          }
>
>          public Accessor(@Nonnull Instance instance) {
>              this(instance, Suppliers.ofInstance(null));
>          }
>
>          @CheckForNull
>          public Value get() {
>              Map<Instance, Value> map = PerInstanceThreadLocal.this.get();
>              return map.computeIfAbsent(instance, constructor);
>          }
>      }
>
>      @Override
>      protected Map<Instance, Value> initialValue() {
>          return new WeakHashMap<>();
>      }
> }
>
> S.
>
> On 10/15/2018 06:28 AM, Nathan and Ila Reynolds wrote:
> > ThreadLocal works by calling Thread.currentThread() and then getting the
> > Map field in Thread.  From there, it calls Map.get(ThreadLocal) to get
> > the value.  This alternative pattern is going to execute
> > WeakHashMap.get(Instance) and cause further memory indirections.  These
> > memory indirections could hurt performance.
> >
> > Another pattern is ConcurrentHashMap<Thread, WeakHashMap<Instance,
> > Value>>.  This is probably not an improvement over what you already
> > proposed.
> >
> > I am curious as to the details of the "serious performance
> > degradation".  Please share what you can.
> >
> > -Nathan
> >
> > On 10/14/2018 9:23 PM, Shevek via Concurrency-interest wrote:
> >> We hit a serious performance degradation when we used too many
> >> ThreadLocal instances in a process, as a consequence of which we are
> >> avoiding using instance-ThreadLocal(s), but we are looking for an
> >> alternative pattern for caching sub-computation state.
> >>
> >> Would:
> >>   static ThreadLocal<WeakHashMap<Instance, Value>>
> >> be a good pattern?
> >>
> >> What other observations or suggestions do people have?
> >>
> >> S.
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181015/c0c6e1d5/attachment-0001.html>

From gergg at cox.net  Mon Oct 15 15:19:52 2018
From: gergg at cox.net (Gregg Wonderly)
Date: Mon, 15 Oct 2018 14:19:52 -0500
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
In-Reply-To: <nsvN1y03i3v99Ah01svPfm>
References: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
 <CD087543-3B9B-49D0-9836-F8D0F8505F27@cox.net> <nsvN1y03i3v99Ah01svPfm>
Message-ID: <F650F61D-CE01-490B-9D12-EF58F22BE17A@cox.net>

Okay I see that there is now a subclass permission required.  I seem to recall that check at line 394 or throwing a security exception without the permission check in the past.

Are you suggesting that security is not a need in modern apps?

Gregg

Sent from my iPhone

> On Oct 15, 2018, at 11:54 AM, David Lloyd <david.lloyd at redhat.com> wrote:
> 
> On Mon, Oct 15, 2018 at 11:50 AM Gregg Wonderly via
> Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>> Subclassing Thread invalidates the use of the security manager.
> 
> I'm pretty sure that this is false.  Even if it were somehow true, it
> seems unlikely to be a significant consideration in most modern
> applications.
> -- 
> - DML


From nathanila at gmail.com  Tue Oct 16 09:51:42 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Tue, 16 Oct 2018 07:51:42 -0600
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
In-Reply-To: <9d4310eb-f435-daf2-38b9-bb1513b35a35@anarres.org>
References: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
 <5e993a7d-eeb7-2e6b-be49-64783ecc8cba@gmail.com>
 <9d4310eb-f435-daf2-38b9-bb1513b35a35@anarres.org>
Message-ID: <054888b5-76ee-00f0-9dd1-cc7aa372d292@gmail.com>

I recently hit that same issue: long living threads with many 
short-lived ThreadLocals.  I ended up accessing Thread.threadLocals and 
calling expungeStaleEntries() when the thread finishes executing a job.  
I haven't seen any performance and memory issues since doing this.  The 
downside is that it is a hack that could break in a future version of Java.

-Nathan

On 10/15/2018 12:04 PM, Shevek wrote:
> The trouble is that ThreadLocalMap isn't a HashMap, and does not have 
> elegant degradation on over-size. perf and related tools told me that 
> we were spending the majority of the application time in 
> ThreadLocalMap.getEntryAfterMiss() after many thousand instance 
> ThreadLocal(s) had been created and GC'd; this wasn't an accident of 
> the inliner or anything, it's real.
>
> Short version: Long-lived threads with many short-lived ThreadLocals 
> sucks really hard.
>
> At a wild guess, the table filled up with tombstones and either the 
> tombstones aren't GCing fast enough due to touching the WeakHashMap 
> (vague memory you used to be able to inhibit Reference cleanup that 
> way) or ...? And the consequence of all this was that we went full 
> linear search. I ran out of both hypotheses and time at this stage, 
> but I still need a practical solution for the application.
>
> Based on a set of other great replies, here are the requirements:
>
> * Must garbage-collect if the thread ends.
> * Must garbage-collect if the instance garbage-collects.
> * Must be compatible with ForkJoinPool threads, i.e. no subclasses of 
> Thread.
>
> So at this point, unless I'm missing something, we're pretty much 
> restricted to a WeakHashMap<Thread, ?> in the instance or a 
> WeakHashMap<Instance, ?> in the Thread.
>
> Then my question is, do I have a singleton PerInstanceThreadLocal so 
> that there is exactly one WeakHashMap per Thread, for all clients, and 
> just key that on instance, or do I want to have a separate WeakHashMap 
> per client in its own non-singleton (but still static) 
> PerInstanceThreadLocal?
>
> // Do I make this class be a singleton?
> // Is one big WeakHashMap better than many small ones?
> // Does my code suck and did I miss a dumb trick (again)?
>
> public class PerInstanceThreadLocal<Instance, Value>
>     extends ThreadLocal<Map<Instance, Value>> {
>
>     public class Accessor {
>
>         private final Instance instance;
>         private final Function<Instance, Value> constructor;
>
>         public Accessor(@Nonnull Instance instance, @Nonnull 
> Supplier<Value> supplier) {
>             this.instance = Preconditions.checkNotNull(instance, 
> "Instance was null.");
>             this.constructor = i -> supplier.get();
>         }
>
>         public Accessor(@Nonnull Instance instance) {
>             this(instance, Suppliers.ofInstance(null));
>         }
>
>         @CheckForNull
>         public Value get() {
>             Map<Instance, Value> map = PerInstanceThreadLocal.this.get();
>             return map.computeIfAbsent(instance, constructor);
>         }
>     }
>
>     @Override
>     protected Map<Instance, Value> initialValue() {
>         return new WeakHashMap<>();
>     }
> }
>
> S.
>
> On 10/15/2018 06:28 AM, Nathan and Ila Reynolds wrote:
>> ThreadLocal works by calling Thread.currentThread() and then getting 
>> the Map field in Thread.  From there, it calls Map.get(ThreadLocal) 
>> to get the value.  This alternative pattern is going to execute 
>> WeakHashMap.get(Instance) and cause further memory indirections.  
>> These memory indirections could hurt performance.
>>
>> Another pattern is ConcurrentHashMap<Thread, WeakHashMap<Instance, 
>> Value>>.  This is probably not an improvement over what you already 
>> proposed.
>>
>> I am curious as to the details of the "serious performance 
>> degradation".  Please share what you can.
>>
>> -Nathan
>>
>> On 10/14/2018 9:23 PM, Shevek via Concurrency-interest wrote:
>>> We hit a serious performance degradation when we used too many 
>>> ThreadLocal instances in a process, as a consequence of which we are 
>>> avoiding using instance-ThreadLocal(s), but we are looking for an 
>>> alternative pattern for caching sub-computation state.
>>>
>>> Would:
>>>   static ThreadLocal<WeakHashMap<Instance, Value>>
>>> be a good pattern?
>>>
>>> What other observations or suggestions do people have?
>>>
>>> S.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>

From david.lloyd at redhat.com  Tue Oct 16 10:39:29 2018
From: david.lloyd at redhat.com (David Lloyd)
Date: Tue, 16 Oct 2018 09:39:29 -0500
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
In-Reply-To: <F650F61D-CE01-490B-9D12-EF58F22BE17A@cox.net>
References: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
 <CD087543-3B9B-49D0-9836-F8D0F8505F27@cox.net>
 <F650F61D-CE01-490B-9D12-EF58F22BE17A@cox.net>
Message-ID: <CANghgrRdiOuTRtA12XpSQsJU0axKih8HXSgeqBASp_L=qBWy6w@mail.gmail.com>

On Mon, Oct 15, 2018 at 2:20 PM Gregg Wonderly <gergg at cox.net> wrote:
> Are you suggesting that security is not a need in modern apps?

Definitely not.  I am suggesting that the current consensus (right or
wrong) appears to be that most practical applications of a security
manager do not appreciably enhance a given application's security,
hence the recent move towards deprecating it altogether.

-- 
- DML

From peter.levart at gmail.com  Wed Oct 17 03:09:05 2018
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 17 Oct 2018 09:09:05 +0200
Subject: [concurrency-interest] Per-thread per-instance ThreadLocal data
In-Reply-To: <9d4310eb-f435-daf2-38b9-bb1513b35a35@anarres.org>
References: <a976f61b-e37c-7975-2663-3064620a79bf@anarres.org>
 <5e993a7d-eeb7-2e6b-be49-64783ecc8cba@gmail.com>
 <9d4310eb-f435-daf2-38b9-bb1513b35a35@anarres.org>
Message-ID: <17d5643c-3f65-331e-1668-2d6d3fc0088c@gmail.com>

Hi Shevek,

I you can explicitly control the lifecycle of ThreadLocal instances, it 
might help to call ThreadLocal::remove when you're done with them.

Regards, Peter

On 10/15/2018 08:04 PM, Shevek via Concurrency-interest wrote:
> The trouble is that ThreadLocalMap isn't a HashMap, and does not have 
> elegant degradation on over-size. perf and related tools told me that 
> we were spending the majority of the application time in 
> ThreadLocalMap.getEntryAfterMiss() after many thousand instance 
> ThreadLocal(s) had been created and GC'd; this wasn't an accident of 
> the inliner or anything, it's real.
>
> Short version: Long-lived threads with many short-lived ThreadLocals 
> sucks really hard.
>
> At a wild guess, the table filled up with tombstones and either the 
> tombstones aren't GCing fast enough due to touching the WeakHashMap 
> (vague memory you used to be able to inhibit Reference cleanup that 
> way) or ...? And the consequence of all this was that we went full 
> linear search. I ran out of both hypotheses and time at this stage, 
> but I still need a practical solution for the application.
>
> Based on a set of other great replies, here are the requirements:
>
> * Must garbage-collect if the thread ends.
> * Must garbage-collect if the instance garbage-collects.
> * Must be compatible with ForkJoinPool threads, i.e. no subclasses of 
> Thread.
>
> So at this point, unless I'm missing something, we're pretty much 
> restricted to a WeakHashMap<Thread, ?> in the instance or a 
> WeakHashMap<Instance, ?> in the Thread.
>
> Then my question is, do I have a singleton PerInstanceThreadLocal so 
> that there is exactly one WeakHashMap per Thread, for all clients, and 
> just key that on instance, or do I want to have a separate WeakHashMap 
> per client in its own non-singleton (but still static) 
> PerInstanceThreadLocal?
>
> // Do I make this class be a singleton?
> // Is one big WeakHashMap better than many small ones?
> // Does my code suck and did I miss a dumb trick (again)?
>
> public class PerInstanceThreadLocal<Instance, Value>
>     extends ThreadLocal<Map<Instance, Value>> {
>
>     public class Accessor {
>
>         private final Instance instance;
>         private final Function<Instance, Value> constructor;
>
>         public Accessor(@Nonnull Instance instance, @Nonnull 
> Supplier<Value> supplier) {
>             this.instance = Preconditions.checkNotNull(instance, 
> "Instance was null.");
>             this.constructor = i -> supplier.get();
>         }
>
>         public Accessor(@Nonnull Instance instance) {
>             this(instance, Suppliers.ofInstance(null));
>         }
>
>         @CheckForNull
>         public Value get() {
>             Map<Instance, Value> map = PerInstanceThreadLocal.this.get();
>             return map.computeIfAbsent(instance, constructor);
>         }
>     }
>
>     @Override
>     protected Map<Instance, Value> initialValue() {
>         return new WeakHashMap<>();
>     }
> }
>
> S.
>
> On 10/15/2018 06:28 AM, Nathan and Ila Reynolds wrote:
>> ThreadLocal works by calling Thread.currentThread() and then getting 
>> the Map field in Thread.  From there, it calls Map.get(ThreadLocal) 
>> to get the value.  This alternative pattern is going to execute 
>> WeakHashMap.get(Instance) and cause further memory indirections.  
>> These memory indirections could hurt performance.
>>
>> Another pattern is ConcurrentHashMap<Thread, WeakHashMap<Instance, 
>> Value>>.  This is probably not an improvement over what you already 
>> proposed.
>>
>> I am curious as to the details of the "serious performance 
>> degradation".  Please share what you can.
>>
>> -Nathan
>>
>> On 10/14/2018 9:23 PM, Shevek via Concurrency-interest wrote:
>>> We hit a serious performance degradation when we used too many 
>>> ThreadLocal instances in a process, as a consequence of which we are 
>>> avoiding using instance-ThreadLocal(s), but we are looking for an 
>>> alternative pattern for caching sub-computation state.
>>>
>>> Would:
>>>   static ThreadLocal<WeakHashMap<Instance, Value>>
>>> be a good pattern?
>>>
>>> What other observations or suggestions do people have?
>>>
>>> S.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/d5ac16e6/attachment.html>

From aph at redhat.com  Wed Oct 17 07:50:39 2018
From: aph at redhat.com (Andrew Haley)
Date: Wed, 17 Oct 2018 12:50:39 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
Message-ID: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>

Some of you might be interested to know the overhead of the "fast
path" of ThreadLocals. Some of you might be terrified of the VM
innards and panic when you see assembly code: this post is not for
you. Everybody else, read on...


Here's what happens when you say int n = ThreadLocal<Integer>::get :

         ││ ;; B11: #	B24 B12 <- B3 B10 	Loop: B11-B10 inner  Freq: 997.566
 10.51%  ││  0x000003ff68b38170: ldr	x10, [xthread,#856]         ;*invokestatic currentThread {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.ThreadLocal::get at 0 (line 162)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)

## Read the pointer to java.lang.Thread from thread metadata.

         ││  0x000003ff68b38174: ldr	w11, [x10,#76]
         ││  0x000003ff68b38178: lsl	x24, x11, #3            ;*getfield threadLocals {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.ThreadLocal::getMap at 1 (line 254)
         ││                                                            ; - java.lang.ThreadLocal::get at 6 (line 163)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)
  2.12%  ││  0x000003ff68b3817c: cbz	x24, 0x000003ff68b3824c  ;*ifnull {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.ThreadLocal::get at 11 (line 164)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)

## Read the pointer to Thread.threadLocals from the Thread. Check it's
   not zero.

         ││ ;; B12: #	B32 B13 <- B11  Freq: 997.551
         ││  0x000003ff68b38180: ldr	w10, [x24,#20]
         ││  0x000003ff68b38184: lsl	x10, x10, #3                ;*getfield table {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.ThreadLocal$ThreadLocalMap::getEntry at 5 (line 434)
         ││                                                            ; - java.lang.ThreadLocal::get at 16 (line 165)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)

## Read the pointer to ThreadLocals.table from Thread.threadLocals


  2.12%  ││  0x000003ff68b38188: ldr	w11, [x10,#12]              ;*arraylength {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.ThreadLocal$ThreadLocalMap::getEntry at 8 (line 434)
         ││                                                            ; - java.lang.ThreadLocal::get at 16 (line 165)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)
         ││                                                            ; implicit exception: dispatches to 0x000003ff68b38324

## Read the length field from table

         ││ ;; B13: #	B26 B14 <- B12  Freq: 997.55
         ││  0x000003ff68b3818c: ldr	w13, [x23,#12]

## Read ThreadLocal.threadLocalHashCode

         ││  0x000003ff68b38190: sub	w12, w11, #0x1
         ││  0x000003ff68b38194: and	w20, w13, w12               ;*iand {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.ThreadLocal$ThreadLocalMap::getEntry at 11 (line 434)
         ││                                                            ; - java.lang.ThreadLocal::get at 16 (line 165)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)
  1.37%  ││  0x000003ff68b38198: add	x12, x10, w20, sxtw #2

## int i = key.threadLocalHashCode & (table.length - 1);


         ││  0x000003ff68b3819c: cmp	w11, #0x0
         ││  0x000003ff68b381a0: b.ls	0x000003ff68b38270

## make sure table.length is not <= 0. (Can't happen, but VM doesn't know that.)

         ││ ;; B14: #	B20 B15 <- B13  Freq: 997.549
 11.88%  ││  0x000003ff68b381a4: ldr	w10, [x12,#16]

## Entry e = table[i];

         ││  0x000003ff68b381a8: lsl	x25, x10, #3          ;*aaload {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.ThreadLocal$ThreadLocalMap::getEntry at 18 (line 435)
         ││                                                            ; - java.lang.ThreadLocal::get at 16 (line 165)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)
         ││  0x000003ff68b381ac: cbz	x25, 0x000003ff68b38208  ;*ifnull {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.ThreadLocal$ThreadLocalMap::getEntry at 21 (line 436)
         ││                                                            ; - java.lang.ThreadLocal::get at 16 (line 165)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)

## if (e != null

         ││ ;; B15: #	B5 B16 <- B14  Freq: 997.489
  6.37%  ││  0x000003ff68b381b0: ldr	w11, [x25,#12]
         ││  0x000003ff68b381b4: ldrsb	w10, [xthread,#48]
         ││  0x000003ff68b381b8: lsl	x26, x11, #3
  9.41%  ╰│  0x000003ff68b381bc: cbz	w10, 0x000003ff68b38130

## G1 garbage collector special case for weak references: if we're
   doing parallel marking, take a slow path.

            ;; B5: #	B27 B6 <- B18 B4 B16 B15  top-of-loop Freq: 997.489
 24.71%  ↗↗  0x000003ff68b38130: cmp	x26, x23
         ││  0x000003ff68b38134: b.ne	0x000003ff68b38294          ;*invokevirtual getEntry {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.ThreadLocal::get at 16 (line 165)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)

##  && e.get() == key)

         ││ ;; B6: #	B7 <- B5 B22  Freq: 997.549
         ││  0x000003ff68b38138: ldr	w11, [x25,#28]
         ││  0x000003ff68b3813c: lsl	x0, x11, #3                 ;*invokevirtual get {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 14 (line 31)

## We now have our ThreadLocal.

         ││ ;; B7: #	B31 B8 <- B6 B25  Freq: 997.564
  1.71%  ││  0x000003ff68b38140: ldr	w11, [x0,#8]                ; implicit exception: dispatches to 0x000003ff68b3830c
         ││ ;; B8: #	B30 B9 <- B7  Freq: 997.563
         ││  0x000003ff68b38144: mov	x12, #0x10000               	// #65536
         ││                                                            ;   {metadata(&apos;java/lang/Integer&apos;)}
         ││  0x000003ff68b38148: movk	x12, #0x3de8
         ││  0x000003ff68b3814c: cmp	w11, w12
  3.15%  ││  0x000003ff68b38150: b.ne	0x000003ff68b382f4          ;*checkcast {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 17 (line 31)

## checkcast to make sure it really is an Integer.


         ││ ;; B9: #	B28 B10 <- B8  Freq: 997.563
         ││  0x000003ff68b38154: ldr	w10, [x0,#12]               ;*getfield value {reexecute=0 rethrow=0 return_oop=0}
         ││                                                            ; - java.lang.Integer::intValue at 1 (line 1132)
         ││                                                            ; - org.sample.ThreadLocalTest::floss at 20 (line 31)

Read the int field. We're done.

12 field loads, 5 conditional branches. That's the overhead of a
single ThreadLocal.get(). Conditional branches depending on the result
of a load from memory are expensive, and we have a lot of them.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From nigro.fra at gmail.com  Wed Oct 17 08:07:51 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Wed, 17 Oct 2018 14:07:51 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
Message-ID: <CAKxGtTXyvZjQs39m9MhgUUxMybFP9n4OtqiB0uPTjq6nkP9fpA@mail.gmail.com>

That's super nice and somehow expected: that's why some (in this same list)
have suggested to extends Thread and provide an ad hoc field directly I
suppose...
Many thanks for this analysis!!!!

Il giorno mer 17 ott 2018 alle ore 13:58 Andrew Haley via
Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:

> Some of you might be interested to know the overhead of the "fast
> path" of ThreadLocals. Some of you might be terrified of the VM
> innards and panic when you see assembly code: this post is not for
> you. Everybody else, read on...
>
>
> Here's what happens when you say int n = ThreadLocal<Integer>::get :
>
>          ││ ;; B11: #   B24 B12 <- B3 B10       Loop: B11-B10 inner  Freq:
> 997.566
>  10.51%  ││  0x000003ff68b38170: ldr    x10, [xthread,#856]
>  ;*invokestatic currentThread {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.ThreadLocal::get at 0 (line 162)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>
> ## Read the pointer to java.lang.Thread from thread metadata.
>
>          ││  0x000003ff68b38174: ldr    w11, [x10,#76]
>          ││  0x000003ff68b38178: lsl    x24, x11, #3            ;*getfield
> threadLocals {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.ThreadLocal::getMap at 1 (line 254)
>          ││                                                            ; -
> java.lang.ThreadLocal::get at 6 (line 163)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>   2.12%  ││  0x000003ff68b3817c: cbz    x24, 0x000003ff68b3824c  ;*ifnull
> {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.ThreadLocal::get at 11 (line 164)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>
> ## Read the pointer to Thread.threadLocals from the Thread. Check it's
>    not zero.
>
>          ││ ;; B12: #   B32 B13 <- B11  Freq: 997.551
>          ││  0x000003ff68b38180: ldr    w10, [x24,#20]
>          ││  0x000003ff68b38184: lsl    x10, x10, #3
> ;*getfield table {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.ThreadLocal$ThreadLocalMap::getEntry at 5 (line 434)
>          ││                                                            ; -
> java.lang.ThreadLocal::get at 16 (line 165)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>
> ## Read the pointer to ThreadLocals.table from Thread.threadLocals
>
>
>   2.12%  ││  0x000003ff68b38188: ldr    w11, [x10,#12]
> ;*arraylength {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.ThreadLocal$ThreadLocalMap::getEntry at 8 (line 434)
>          ││                                                            ; -
> java.lang.ThreadLocal::get at 16 (line 165)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>          ││                                                            ;
> implicit exception: dispatches to 0x000003ff68b38324
>
> ## Read the length field from table
>
>          ││ ;; B13: #   B26 B14 <- B12  Freq: 997.55
>          ││  0x000003ff68b3818c: ldr    w13, [x23,#12]
>
> ## Read ThreadLocal.threadLocalHashCode
>
>          ││  0x000003ff68b38190: sub    w12, w11, #0x1
>          ││  0x000003ff68b38194: and    w20, w13, w12               ;*iand
> {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.ThreadLocal$ThreadLocalMap::getEntry at 11 (line 434)
>          ││                                                            ; -
> java.lang.ThreadLocal::get at 16 (line 165)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>   1.37%  ││  0x000003ff68b38198: add    x12, x10, w20, sxtw #2
>
> ## int i = key.threadLocalHashCode & (table.length - 1);
>
>
>          ││  0x000003ff68b3819c: cmp    w11, #0x0
>          ││  0x000003ff68b381a0: b.ls   0x000003ff68b38270
>
> ## make sure table.length is not <= 0. (Can't happen, but VM doesn't know
> that.)
>
>          ││ ;; B14: #   B20 B15 <- B13  Freq: 997.549
>  11.88%  ││  0x000003ff68b381a4: ldr    w10, [x12,#16]
>
> ## Entry e = table[i];
>
>          ││  0x000003ff68b381a8: lsl    x25, x10, #3          ;*aaload
> {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.ThreadLocal$ThreadLocalMap::getEntry at 18 (line 435)
>          ││                                                            ; -
> java.lang.ThreadLocal::get at 16 (line 165)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>          ││  0x000003ff68b381ac: cbz    x25, 0x000003ff68b38208  ;*ifnull
> {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.ThreadLocal$ThreadLocalMap::getEntry at 21 (line 436)
>          ││                                                            ; -
> java.lang.ThreadLocal::get at 16 (line 165)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>
> ## if (e != null
>
>          ││ ;; B15: #   B5 B16 <- B14  Freq: 997.489
>   6.37%  ││  0x000003ff68b381b0: ldr    w11, [x25,#12]
>          ││  0x000003ff68b381b4: ldrsb  w10, [xthread,#48]
>          ││  0x000003ff68b381b8: lsl    x26, x11, #3
>   9.41%  ╰│  0x000003ff68b381bc: cbz    w10, 0x000003ff68b38130
>
> ## G1 garbage collector special case for weak references: if we're
>    doing parallel marking, take a slow path.
>
>             ;; B5: #    B27 B6 <- B18 B4 B16 B15  top-of-loop Freq: 997.489
>  24.71%  ↗↗  0x000003ff68b38130: cmp    x26, x23
>          ││  0x000003ff68b38134: b.ne   0x000003ff68b38294
> ;*invokevirtual getEntry {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.ThreadLocal::get at 16 (line 165)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>
> ##  && e.get() == key)
>
>          ││ ;; B6: #    B7 <- B5 B22  Freq: 997.549
>          ││  0x000003ff68b38138: ldr    w11, [x25,#28]
>          ││  0x000003ff68b3813c: lsl    x0, x11, #3
>  ;*invokevirtual get {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 14 (line 31)
>
> ## We now have our ThreadLocal.
>
>          ││ ;; B7: #    B31 B8 <- B6 B25  Freq: 997.564
>   1.71%  ││  0x000003ff68b38140: ldr    w11, [x0,#8]                ;
> implicit exception: dispatches to 0x000003ff68b3830c
>          ││ ;; B8: #    B30 B9 <- B7  Freq: 997.563
>          ││  0x000003ff68b38144: mov    x12, #0x10000                   //
> #65536
>          ││                                                            ;
>  {metadata(&apos;java/lang/Integer&apos;)}
>          ││  0x000003ff68b38148: movk   x12, #0x3de8
>          ││  0x000003ff68b3814c: cmp    w11, w12
>   3.15%  ││  0x000003ff68b38150: b.ne   0x000003ff68b382f4
> ;*checkcast {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 17 (line 31)
>
> ## checkcast to make sure it really is an Integer.
>
>
>          ││ ;; B9: #    B28 B10 <- B8  Freq: 997.563
>          ││  0x000003ff68b38154: ldr    w10, [x0,#12]
>  ;*getfield value {reexecute=0 rethrow=0 return_oop=0}
>          ││                                                            ; -
> java.lang.Integer::intValue at 1 (line 1132)
>          ││                                                            ; -
> org.sample.ThreadLocalTest::floss at 20 (line 31)
>
> Read the int field. We're done.
>
> 12 field loads, 5 conditional branches. That's the overhead of a
> single ThreadLocal.get(). Conditional branches depending on the result
> of a load from memory are expensive, and we have a lot of them.
>
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/2115143d/attachment-0001.html>

From david.lloyd at redhat.com  Wed Oct 17 08:47:03 2018
From: david.lloyd at redhat.com (David Lloyd)
Date: Wed, 17 Oct 2018 07:47:03 -0500
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
Message-ID: <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>

On Wed, Oct 17, 2018 at 6:58 AM Andrew Haley via Concurrency-interest
<concurrency-interest at cs.oswego.edu> wrote:
>
> Some of you might be interested to know the overhead of the "fast
> path" of ThreadLocals. Some of you might be terrified of the VM
> innards and panic when you see assembly code: this post is not for
> you. Everybody else, read on...
> [...]
> 12 field loads, 5 conditional branches. That's the overhead of a
> single ThreadLocal.get(). Conditional branches depending on the result
> of a load from memory are expensive, and we have a lot of them.

It sure would be nice to have static thread local fields, similarly to
C/C++ thread/_Thread_local, where there are just one or two pointer
indirections to get at the data even in dynamically-loaded class
loaders.

That said, I don't think there's any good way to implement
thread-local instance fields; at least, not anything that's likely to
be much better than the existing ThreadLocal situation.  I would love
to be proven wrong though.
-- 
- DML

From aph at redhat.com  Wed Oct 17 10:01:16 2018
From: aph at redhat.com (Andrew Haley)
Date: Wed, 17 Oct 2018 15:01:16 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
Message-ID: <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>

On 10/17/2018 01:47 PM, David Lloyd wrote:
> It sure would be nice to have static thread local fields, similarly to
> C/C++ thread/_Thread_local, where there are just one or two pointer
> indirections to get at the data even in dynamically-loaded class
> loaders.

Mmm. I wonder if we could piggyback this on Project Panama (or
Valhalla?)

> That said, I don't think there's any good way to implement
> thread-local instance fields; at least, not anything that's likely to
> be much better than the existing ThreadLocal situation.  I would love
> to be proven wrong though.

I can think of some cheaper ways for static thread locals. You'd
wouldn't have features like inheritance and creation on first use, but
you'd gain performance.

Here's how: Thread.currentThread() is extremely fast on all of the
machines we care about, it's usually a single instruction. So, we'd
need a growable per-thread map from static thread-local field ID to
the field contents: that's not terribly difficult. It should be
possible to do all of that in fewer than ten instructions, maybe only
five. If C++ can do it, so can we.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From oleksandr.otenko at gmail.com  Wed Oct 17 10:53:07 2018
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 17 Oct 2018 15:53:07 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
Message-ID: <4BE67B3D-FB33-495C-A2EE-73E41999EE8D@gmail.com>

I wonder why such a focus on ThreadLocals? Is this the recommended way to capture state?


Alex


> On 17 Oct 2018, at 15:01, Andrew Haley via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> On 10/17/2018 01:47 PM, David Lloyd wrote:
>> It sure would be nice to have static thread local fields, similarly to
>> C/C++ thread/_Thread_local, where there are just one or two pointer
>> indirections to get at the data even in dynamically-loaded class
>> loaders.
> 
> Mmm. I wonder if we could piggyback this on Project Panama (or
> Valhalla?)
> 
>> That said, I don't think there's any good way to implement
>> thread-local instance fields; at least, not anything that's likely to
>> be much better than the existing ThreadLocal situation.  I would love
>> to be proven wrong though.
> 
> I can think of some cheaper ways for static thread locals. You'd
> wouldn't have features like inheritance and creation on first use, but
> you'd gain performance.
> 
> Here's how: Thread.currentThread() is extremely fast on all of the
> machines we care about, it's usually a single instruction. So, we'd
> need a growable per-thread map from static thread-local field ID to
> the field contents: that's not terribly difficult. It should be
> possible to do all of that in fewer than ten instructions, maybe only
> five. If C++ can do it, so can we.
> 
> -- 
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Wed Oct 17 11:07:52 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 17 Oct 2018 11:07:52 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
Message-ID: <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>

On 10/17/18 10:01 AM, Andrew Haley via Concurrency-interest wrote:
> On 10/17/2018 01:47 PM, David Lloyd wrote:
>> It sure would be nice to have static thread local fields, similarly
>> to C/C++ thread/_Thread_local, where there are just one or two
>> pointer indirections to get at the data even in dynamically-loaded
>> class loaders.
> 
> Mmm. I wonder if we could piggyback this on Project Panama (or 
> Valhalla?)
> 
>> That said, I don't think there's any good way to implement 
>> thread-local instance fields; at least, not anything that's likely
>> to be much better than the existing ThreadLocal situation.  I would
>> love to be proven wrong though.
> 
> I can think of some cheaper ways for static thread locals. You'd 
> wouldn't have features like inheritance and creation on first use,
> but you'd gain performance.
> 
> Here's how: Thread.currentThread() is extremely fast on all of the 
> machines we care about, it's usually a single instruction. So, we'd 
> need a growable per-thread map from static thread-local field ID to 
> the field contents: that's not terribly difficult. It should be 
> possible to do all of that in fewer than ten instructions, maybe
> only five. If C++ can do it, so can we.
> 

I agree that Panama/Valhalla makes possible things like this that were
dismissed years ago when we contemplated further improvements.

This alone would not address long-standing problems with zillions of
short-lived ThreadLocals in long-lived threads.  Right now, ThreadLocal
is as fast as we know how to make it while still not completely falling
over under such usages. The only solution I know for this is to create a
new GC-aware storage class, which is not very likely to be adopted.

In the mean time, I agree with the suggestions of, when possible
creating Thread subclasses, and/or calling ThreadLocal.remove. Also
consider restructuring code to use task-local classes (perhaps with
linkages among them) that are GCable when tasks complete.

-Doug

From nigro.fra at gmail.com  Wed Oct 17 11:21:21 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Wed, 17 Oct 2018 17:21:21 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <4BE67B3D-FB33-495C-A2EE-73E41999EE8D@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <4BE67B3D-FB33-495C-A2EE-73E41999EE8D@gmail.com>
Message-ID: <CAKxGtTXrkKxcLrEoWFFFZUsYPU-4BR_y9hye18suWHmFq89U_g@mail.gmail.com>

@alex I suppose that there are several concurrent algorithms that are
simpler to be implemented using it eg Combiner

Il giorno mer 17 ott 2018 alle ore 17:12 Alex Otenko via
Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:

> I wonder why such a focus on ThreadLocals? Is this the recommended way to
> capture state?
>
>
> Alex
>
>
> > On 17 Oct 2018, at 15:01, Andrew Haley via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
> >
> > On 10/17/2018 01:47 PM, David Lloyd wrote:
> >> It sure would be nice to have static thread local fields, similarly to
> >> C/C++ thread/_Thread_local, where there are just one or two pointer
> >> indirections to get at the data even in dynamically-loaded class
> >> loaders.
> >
> > Mmm. I wonder if we could piggyback this on Project Panama (or
> > Valhalla?)
> >
> >> That said, I don't think there's any good way to implement
> >> thread-local instance fields; at least, not anything that's likely to
> >> be much better than the existing ThreadLocal situation.  I would love
> >> to be proven wrong though.
> >
> > I can think of some cheaper ways for static thread locals. You'd
> > wouldn't have features like inheritance and creation on first use, but
> > you'd gain performance.
> >
> > Here's how: Thread.currentThread() is extremely fast on all of the
> > machines we care about, it's usually a single instruction. So, we'd
> > need a growable per-thread map from static thread-local field ID to
> > the field contents: that's not terribly difficult. It should be
> > possible to do all of that in fewer than ten instructions, maybe only
> > five. If C++ can do it, so can we.
> >
> > --
> > Andrew Haley
> > Java Platform Lead Engineer
> > Red Hat UK Ltd. <https://www.redhat.com>
> > EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/32722352/attachment.html>

From tim at peierls.net  Wed Oct 17 11:41:18 2018
From: tim at peierls.net (Tim Peierls)
Date: Wed, 17 Oct 2018 11:41:18 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
Message-ID: <CA+F8eeTb1BpSPV1ZAUL5YJFf4NUwpPFPE2WgoYYBAyHRLoZR1A@mail.gmail.com>

On Wed, Oct 17, 2018 at 11:28 AM Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> Also consider restructuring code to use task-local classes (perhaps with
> linkages among them) that are GCable when tasks complete.
>

I bet a *lot* of the ThreadLocal uses under consideration would benefit
from this kind of restructuring, avoiding ThreadLocal entirely.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/1735e81e/attachment.html>

From david.lloyd at redhat.com  Wed Oct 17 12:10:43 2018
From: david.lloyd at redhat.com (David Lloyd)
Date: Wed, 17 Oct 2018 11:10:43 -0500
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CA+F8eeTb1BpSPV1ZAUL5YJFf4NUwpPFPE2WgoYYBAyHRLoZR1A@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <CA+F8eeTb1BpSPV1ZAUL5YJFf4NUwpPFPE2WgoYYBAyHRLoZR1A@mail.gmail.com>
Message-ID: <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>

On Wed, Oct 17, 2018 at 11:01 AM Tim Peierls via Concurrency-interest
<concurrency-interest at cs.oswego.edu> wrote:
> On Wed, Oct 17, 2018 at 11:28 AM Doug Lea via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>> Also consider restructuring code to use task-local classes (perhaps with
>> linkages among them) that are GCable when tasks complete.
>
> I bet a lot of the ThreadLocal uses under consideration would benefit from this kind of restructuring, avoiding ThreadLocal entirely.

How would one access task-local data if not by way of a ThreadLocal?
-- 
- DML

From aph at redhat.com  Wed Oct 17 12:29:13 2018
From: aph at redhat.com (Andrew Haley)
Date: Wed, 17 Oct 2018 17:29:13 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
Message-ID: <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>

On 10/17/2018 04:07 PM, Doug Lea via Concurrency-interest wrote:
> This alone would not address long-standing problems with zillions of
> short-lived ThreadLocals in long-lived threads.  Right now, ThreadLocal
> is as fast as we know how to make it while still not completely falling
> over under such usages. The only solution I know for this is to create a
> new GC-aware storage class, which is not very likely to be adopted.

Well, yeah, but creating zillions of short-lived ThreadLocals seems
like an antipattern to me.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Wed Oct 17 12:30:28 2018
From: aph at redhat.com (Andrew Haley)
Date: Wed, 17 Oct 2018 17:30:28 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <4BE67B3D-FB33-495C-A2EE-73E41999EE8D@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <4BE67B3D-FB33-495C-A2EE-73E41999EE8D@gmail.com>
Message-ID: <6ae2c09a-8464-8b3a-dee9-f4246aa5b024@redhat.com>

On 10/17/2018 03:53 PM, Alex Otenko wrote:
> I wonder why such a focus on ThreadLocals? Is this the recommended way to capture state?

It seems seductively simple. However, it's going to break very badly
with new concurrency models like fibers, so subclassing seems like a
better plan.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Wed Oct 17 12:36:01 2018
From: aph at redhat.com (Andrew Haley)
Date: Wed, 17 Oct 2018 17:36:01 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <CA+F8eeTb1BpSPV1ZAUL5YJFf4NUwpPFPE2WgoYYBAyHRLoZR1A@mail.gmail.com>
 <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>
Message-ID: <10f552e2-b024-a07b-bb11-ccf7222f37d7@redhat.com>

On 10/17/2018 05:10 PM, David Lloyd via Concurrency-interest wrote:
> How would one access task-local data if not by way of a ThreadLocal?

((MyThread)(Thread.currentThread()).getFoo()

would work, surely. It's true that you'd need to know that the
current thread was an instance of MyThread.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From jbloch at gmail.com  Wed Oct 17 12:50:15 2018
From: jbloch at gmail.com (Joshua Bloch)
Date: Wed, 17 Oct 2018 12:50:15 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
Message-ID: <CAP0L=UR+D_ewg1RvwwKF=gOPQ=TOLvgrDz_mrc96iz7usCH5bw@mail.gmail.com>

I agree. For your amusement, when I first designed and implemented
ThreadLocal, I assumed that no VM would ever have more that ~10 thread
locals over its lifetime. We reimplemented it several times as my initial
estimate proved further and further from the truth. The API has held up
pretty well, though.

Josh

On Wed, Oct 17, 2018 at 12:45 PM Andrew Haley via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 10/17/2018 04:07 PM, Doug Lea via Concurrency-interest wrote:
> > This alone would not address long-standing problems with zillions of
> > short-lived ThreadLocals in long-lived threads.  Right now, ThreadLocal
> > is as fast as we know how to make it while still not completely falling
> > over under such usages. The only solution I know for this is to create a
> > new GC-aware storage class, which is not very likely to be adopted.
>
> Well, yeah, but creating zillions of short-lived ThreadLocals seems
> like an antipattern to me.
>
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/9416e613/attachment.html>

From tim at peierls.net  Wed Oct 17 13:01:59 2018
From: tim at peierls.net (Tim Peierls)
Date: Wed, 17 Oct 2018 13:01:59 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <CA+F8eeTb1BpSPV1ZAUL5YJFf4NUwpPFPE2WgoYYBAyHRLoZR1A@mail.gmail.com>
 <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>
Message-ID: <CA+F8eeRHqOLomrQp7eNaK6Vv8Gzr5jwRwJ5Zq02rupdJJeBJtg@mail.gmail.com>

On Wed, Oct 17, 2018 at 12:11 PM David Lloyd <david.lloyd at redhat.com> wrote:

> On Wed, Oct 17, 2018 at 11:01 AM Tim Peierls wrote:
> > On Wed, Oct 17, 2018 at 11:28 AM Doug Lea via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
> >> Also consider restructuring code to use task-local classes (perhaps
> with linkages among them) that are GCable when tasks complete.
> >
> > I bet a lot of the ThreadLocal uses under consideration would benefit
> from this kind of restructuring, avoiding ThreadLocal entirely.
>
> How would one access task-local data if not by way of a ThreadLocal?
>

I'm thinking of settings where a task instance is confined to a single
thread, so that the fields of that instance are safely accessible and
modifiable without need of any other machinery.

I suspect that it is common for people to use per-thread state simply
because it's easy, rather than trying to structure things in terms of tasks
that run in a single thread at a time (with *happens-before* edges wherever
the task is passed to another thread).

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/bd26e754/attachment.html>

From nathanila at gmail.com  Wed Oct 17 13:06:29 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Wed, 17 Oct 2018 11:06:29 -0600
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
Message-ID: <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>

 > creating zillions of short-lived ThreadLocals seems like an 
antipattern to me.

Perhaps, you can share another way to solve this problem.

I have a ByteBuffer that maps a large file.  I have multiple threads 
reading the ByteBuffer at different positions.  As long as the threads 
don't call ByteBuffer.position(), they can operate concurrently on the 
ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an absolute 
method hence the thread has to call position().

Attempt #1: I started by putting a lock around the ByteBuffer. This 
causes a lot of contention.

Attempt #2: I started by slicing the ByteBuffer.  This created a lot of 
garbage.

Attempt #3: I put the sliced ByteBuffers into ThreadLocal but with many 
files mapped, consumed and unmapped rapidly, this leads to zillions of 
short-lived ThreadLocals.

Attempt #4: I put the sliced ByteBuffers into a LinkedTransferQueue but 
this created a lot of garbage for creating nodes in the queue.

Attempt #5: I put the sliced ByteBuffers into a ConcurrentHashMap keyed 
on the Thread.  I cannot remember why this didn't work.  I think the 
overhead of ConcurrentHashMap created a lot of garbage.

Attempt #6: I went back to attempt #3 (ThreadLocal) and call expunge 
when the thread returns to the thread pool.  Yes, this creates zillions 
of short-lived ThreadLocals but they get cleaned out quickly so there is 
performance degradation for ThreadLocal lookup.

Each thread cannot have its sliced ByteBuffer passed through the stack 
as an argument.  This would create a lot of garbage from duplicate 
structures.

-Nathan

On 10/17/2018 10:29 AM, Andrew Haley via Concurrency-interest wrote:
> On 10/17/2018 04:07 PM, Doug Lea via Concurrency-interest wrote:
>> This alone would not address long-standing problems with zillions of
>> short-lived ThreadLocals in long-lived threads.  Right now, ThreadLocal
>> is as fast as we know how to make it while still not completely falling
>> over under such usages. The only solution I know for this is to create a
>> new GC-aware storage class, which is not very likely to be adopted.
> Well, yeah, but creating zillions of short-lived ThreadLocals seems
> like an antipattern to me.
>

From david.lloyd at redhat.com  Wed Oct 17 13:40:53 2018
From: david.lloyd at redhat.com (David Lloyd)
Date: Wed, 17 Oct 2018 12:40:53 -0500
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CA+F8eeRHqOLomrQp7eNaK6Vv8Gzr5jwRwJ5Zq02rupdJJeBJtg@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <CA+F8eeTb1BpSPV1ZAUL5YJFf4NUwpPFPE2WgoYYBAyHRLoZR1A@mail.gmail.com>
 <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>
 <CA+F8eeRHqOLomrQp7eNaK6Vv8Gzr5jwRwJ5Zq02rupdJJeBJtg@mail.gmail.com>
Message-ID: <CANghgrTNwuv-4yipomaV+AzXFQM0dK5S69FqpV3RLuGR2O359w@mail.gmail.com>

On Wed, Oct 17, 2018 at 12:01 PM Tim Peierls <tim at peierls.net> wrote:
> On Wed, Oct 17, 2018 at 12:11 PM David Lloyd <david.lloyd at redhat.com> wrote:
>> On Wed, Oct 17, 2018 at 11:01 AM Tim Peierls wrote:
>> > On Wed, Oct 17, 2018 at 11:28 AM Doug Lea via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>> >> Also consider restructuring code to use task-local classes (perhaps with linkages among them) that are GCable when tasks complete.
>> >
>> > I bet a lot of the ThreadLocal uses under consideration would benefit from this kind of restructuring, avoiding ThreadLocal entirely.
>>
>> How would one access task-local data if not by way of a ThreadLocal?
>
> I'm thinking of settings where a task instance is confined to a single thread, so that the fields of that instance are safely accessible and modifiable without need of any other machinery.

That's fair, but I'm not talking about safety; I was asking
specifically how task-local classes can be accessible at all.  One can
allocate objects on the heap, and use them either within a single
lexical scope, pass them among methods as explicit method parameters,
or passed implicitly between methods by direct or indirect usage of a
thread local (either ThreadLocal or through some other path from
Thread to the data in question, including fields on a Thread
subclass).

Doug's mention of "task local classes" sounds like he was alluding to
some new mode of access other than these three, so I would be
interested to know of such a thing.

> I suspect that it is common for people to use per-thread state simply because it's easy, rather than trying to structure things in terms of tasks that run in a single thread at a time (with happens-before edges wherever the task is passed to another thread).

Maybe.

-- 
- DML

From oleksandr.otenko at gmail.com  Wed Oct 17 13:41:36 2018
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 17 Oct 2018 18:41:36 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
Message-ID: <F019EE32-1846-447C-8A5D-2E09A1A68DA4@gmail.com>

I was about to say that sharing Buffers would qualify as an example of a good use of ThreadLocals.

The important thing here is not having concurrent way of obtaining a preallocated Buffer, but the ability to use the Buffer whose content is very likely not shared with any other CPU. (Yes, the thread may migrate, but generally speaking...)

Alex

> On 17 Oct 2018, at 18:06, Nathan and Ila Reynolds via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> > creating zillions of short-lived ThreadLocals seems like an antipattern to me.
> 
> Perhaps, you can share another way to solve this problem.
> 
> I have a ByteBuffer that maps a large file.  I have multiple threads reading the ByteBuffer at different positions.  As long as the threads don't call ByteBuffer.position(), they can operate concurrently on the ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an absolute method hence the thread has to call position().
> 
> Attempt #1: I started by putting a lock around the ByteBuffer. This causes a lot of contention.
> 
> Attempt #2: I started by slicing the ByteBuffer.  This created a lot of garbage.
> 
> Attempt #3: I put the sliced ByteBuffers into ThreadLocal but with many files mapped, consumed and unmapped rapidly, this leads to zillions of short-lived ThreadLocals.
> 
> Attempt #4: I put the sliced ByteBuffers into a LinkedTransferQueue but this created a lot of garbage for creating nodes in the queue.
> 
> Attempt #5: I put the sliced ByteBuffers into a ConcurrentHashMap keyed on the Thread.  I cannot remember why this didn't work.  I think the overhead of ConcurrentHashMap created a lot of garbage.
> 
> Attempt #6: I went back to attempt #3 (ThreadLocal) and call expunge when the thread returns to the thread pool.  Yes, this creates zillions of short-lived ThreadLocals but they get cleaned out quickly so there is performance degradation for ThreadLocal lookup.
> 
> Each thread cannot have its sliced ByteBuffer passed through the stack as an argument.  This would create a lot of garbage from duplicate structures.
> 
> -Nathan
> 
> On 10/17/2018 10:29 AM, Andrew Haley via Concurrency-interest wrote:
>> On 10/17/2018 04:07 PM, Doug Lea via Concurrency-interest wrote:
>>> This alone would not address long-standing problems with zillions of
>>> short-lived ThreadLocals in long-lived threads.  Right now, ThreadLocal
>>> is as fast as we know how to make it while still not completely falling
>>> over under such usages. The only solution I know for this is to create a
>>> new GC-aware storage class, which is not very likely to be adopted.
>> Well, yeah, but creating zillions of short-lived ThreadLocals seems
>> like an antipattern to me.
>> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From david.lloyd at redhat.com  Wed Oct 17 13:44:41 2018
From: david.lloyd at redhat.com (David Lloyd)
Date: Wed, 17 Oct 2018 12:44:41 -0500
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
Message-ID: <CANghgrQpdkL2vUUA2NgbiWobux_axzz5dtjUmAcmNPP8swZEig@mail.gmail.com>

On Wed, Oct 17, 2018 at 12:36 PM Nathan and Ila Reynolds via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> Perhaps, you can share another way to solve this problem.
>
> I have a ByteBuffer that maps a large file.  I have multiple threads
> reading the ByteBuffer at different positions.  As long as the threads
> don't call ByteBuffer.position(), they can operate concurrently on the
> ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an absolute
> method hence the thread has to call position().

The obvious solution would seem to be that we should enhance
ByteBuffer to have such a method.

But, your #2 should work if you are careful to do it like this:

   buf.duplicate().position(newPos).get(byteArray);

In such cases, HotSpot can sometimes delete the allocation of the new
ByteBuffer altogether.  I seem to recall that my colleague Andrew
Haley (on this thread) did some work/research in this area a while
ago.

-- 
- DML

From jbloch at gmail.com  Wed Oct 17 13:49:35 2018
From: jbloch at gmail.com (Joshua Bloch)
Date: Wed, 17 Oct 2018 13:49:35 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
Message-ID: <CAP0L=UR87FW-Sj-MPqPJjVmv-Zr7hcvVZGyzd3t+mzRGCRfGaA@mail.gmail.com>

You are fighting with ByteBuffer, which is not my favorite API. You do not
need fancy thread-locals; you need a decent memory-mapped file.

On Wed, Oct 17, 2018 at 1:35 PM Nathan and Ila Reynolds via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:

>  > creating zillions of short-lived ThreadLocals seems like an
> antipattern to me.
>
> Perhaps, you can share another way to solve this problem.
>
> I have a ByteBuffer that maps a large file.  I have multiple threads
> reading the ByteBuffer at different positions.  As long as the threads
> don't call ByteBuffer.position(), they can operate concurrently on the
> ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an absolute
> method hence the thread has to call position().
>
> Attempt #1: I started by putting a lock around the ByteBuffer. This
> causes a lot of contention.
>
> Attempt #2: I started by slicing the ByteBuffer.  This created a lot of
> garbage.
>
> Attempt #3: I put the sliced ByteBuffers into ThreadLocal but with many
> files mapped, consumed and unmapped rapidly, this leads to zillions of
> short-lived ThreadLocals.
>
> Attempt #4: I put the sliced ByteBuffers into a LinkedTransferQueue but
> this created a lot of garbage for creating nodes in the queue.
>
> Attempt #5: I put the sliced ByteBuffers into a ConcurrentHashMap keyed
> on the Thread.  I cannot remember why this didn't work.  I think the
> overhead of ConcurrentHashMap created a lot of garbage.
>
> Attempt #6: I went back to attempt #3 (ThreadLocal) and call expunge
> when the thread returns to the thread pool.  Yes, this creates zillions
> of short-lived ThreadLocals but they get cleaned out quickly so there is
> performance degradation for ThreadLocal lookup.
>
> Each thread cannot have its sliced ByteBuffer passed through the stack
> as an argument.  This would create a lot of garbage from duplicate
> structures.
>
> -Nathan
>
> On 10/17/2018 10:29 AM, Andrew Haley via Concurrency-interest wrote:
> > On 10/17/2018 04:07 PM, Doug Lea via Concurrency-interest wrote:
> >> This alone would not address long-standing problems with zillions of
> >> short-lived ThreadLocals in long-lived threads.  Right now, ThreadLocal
> >> is as fast as we know how to make it while still not completely falling
> >> over under such usages. The only solution I know for this is to create a
> >> new GC-aware storage class, which is not very likely to be adopted.
> > Well, yeah, but creating zillions of short-lived ThreadLocals seems
> > like an antipattern to me.
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/8778bc6e/attachment.html>

From tim at peierls.net  Wed Oct 17 14:15:35 2018
From: tim at peierls.net (Tim Peierls)
Date: Wed, 17 Oct 2018 14:15:35 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANghgrTNwuv-4yipomaV+AzXFQM0dK5S69FqpV3RLuGR2O359w@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <CA+F8eeTb1BpSPV1ZAUL5YJFf4NUwpPFPE2WgoYYBAyHRLoZR1A@mail.gmail.com>
 <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>
 <CA+F8eeRHqOLomrQp7eNaK6Vv8Gzr5jwRwJ5Zq02rupdJJeBJtg@mail.gmail.com>
 <CANghgrTNwuv-4yipomaV+AzXFQM0dK5S69FqpV3RLuGR2O359w@mail.gmail.com>
Message-ID: <CA+F8eeQrfX91Za6rnOLrsBnKwcWtAVbSuM6xrd=fFXfN-THzKw@mail.gmail.com>

On Wed, Oct 17, 2018 at 1:41 PM David Lloyd <david.lloyd at redhat.com> wrote:

> On Wed, Oct 17, 2018 at 12:01 PM Tim Peierls <tim at peierls.net> wrote:
> > On Wed, Oct 17, 2018 at 12:11 PM David Lloyd <david.lloyd at redhat.com>
> wrote:
> >> On Wed, Oct 17, 2018 at 11:01 AM Tim Peierls wrote:
> >> > On Wed, Oct 17, 2018 at 11:28 AM Doug Lea via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
> >> >> Also consider restructuring code to use task-local classes (perhaps
> with linkages among them) that are GCable when tasks complete.
> >> >
> >> > I bet a lot of the ThreadLocal uses under consideration would benefit
> from this kind of restructuring, avoiding ThreadLocal entirely.
> >>
> >> How would one access task-local data if not by way of a ThreadLocal?
> >
> > I'm thinking of settings where a task instance is confined to a single
> thread, so that the fields of that instance are safely accessible and
> modifiable without need of any other machinery.
>
> That's fair, but I'm not talking about safety; I was asking specifically
> how task-local classes can be accessible at all.  One can allocate objects
> on the heap, and use them either within a single lexical scope, pass them
> among methods as explicit method parameters, or passed implicitly between
> methods by direct or indirect usage of a thread local (either ThreadLocal
> or through some other path from Thread to the data in question, including
> fields on a Thread subclass).
>

I'm talking about submitting tasks to execution services that wrap thread
pools:

class MyTask implements Supplier<Result> {
    // task-local state as fields of MyTask:
    ...

    MyTask(...) { ... }

    @Override public Result get() {
        ... do some work involving task-local state and eventually
(maybe) produce a Result ...
    }
}

CompletableFuture<Result> future = CompletableFuture.supplyAsync(new
MyTask(...));

That's all I meant.



> Doug's mention of "task local classes" sounds like he was alluding to some
> new mode of access other than these three, so I would be interested to know
> of such a thing.
>

Not positive what Doug meant, but I don't think he was talking about a new
mode of access when he wrote "task-local classes".


I suspect that it is common for people to use per-thread state simply
> because it's easy, rather than trying to structure things in terms of tasks
> that run in a single thread at a time (with happens-before edges wherever
> the task is passed to another thread).
>
> Maybe.


Sounds like a good question to pose to people who can search large
codebases for patterns of use! Kevin Bourrillion, are you listening?

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/2daad425/attachment-0001.html>

From dl at cs.oswego.edu  Wed Oct 17 14:26:19 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 17 Oct 2018 14:26:19 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
Message-ID: <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>

[+list]

On 10/17/18 11:44 AM, Nathan and Ila Reynolds wrote:
> Can we add the following method to ThreadLocal?
> 
> public static void expungeStaleEntries()

This seems like a reasonable request (although perhaps with an improved
name).  The functionality exists internally, and it seems overly
parental not to export it for use as a band-aid by those people who have
tried and otherwise failed to solve the zillions of short-lived
ThreadLocals in long-lived threads problem.

Can anyone think of a reason not to do this?

-Doug

> 
> This method will call ThreadLocal.ThreadLocalMap.expungeStaleEntries()
> for the ThreadLocalMap of the current thread.  Thread pools can then
> call this method when the thread finishes processing a job after GC.
> This solves the problem of zillions of short-lived ThreadLocals in
> long-lived threads.  


From nathanila at gmail.com  Wed Oct 17 14:33:14 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Wed, 17 Oct 2018 12:33:14 -0600
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANghgrQpdkL2vUUA2NgbiWobux_axzz5dtjUmAcmNPP8swZEig@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CANghgrQpdkL2vUUA2NgbiWobux_axzz5dtjUmAcmNPP8swZEig@mail.gmail.com>
Message-ID: <c8c185ab-82d1-9270-4d1b-041c670d1240@gmail.com>

I would love it if ByteBuffer had absolute versions of all the relative 
methods.  I have wanted the absolute versions in the past for other 
situations.

My #2 is similar what you said.  Replace duplicate with slice.

    buf.slice().position(newPos).get(byteArray);

This produces a lot of garbage unless I keep it in a ThreadLocal or some 
other data structure.  I was running on Java 8.  Perhaps, Java 10 or 11 
would not produce as much garbage.

-Nathan

On 10/17/2018 11:44 AM, David Lloyd wrote:
> On Wed, Oct 17, 2018 at 12:36 PM Nathan and Ila Reynolds via
> Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>> Perhaps, you can share another way to solve this problem.
>>
>> I have a ByteBuffer that maps a large file.  I have multiple threads
>> reading the ByteBuffer at different positions.  As long as the threads
>> don't call ByteBuffer.position(), they can operate concurrently on the
>> ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an absolute
>> method hence the thread has to call position().
> The obvious solution would seem to be that we should enhance
> ByteBuffer to have such a method.
>
> But, your #2 should work if you are careful to do it like this:
>
>     buf.duplicate().position(newPos).get(byteArray);
>
> In such cases, HotSpot can sometimes delete the allocation of the new
> ByteBuffer altogether.  I seem to recall that my colleague Andrew
> Haley (on this thread) did some work/research in this area a while
> ago.
>

From andrey.a.pavlenko at gmail.com  Wed Oct 17 14:35:02 2018
From: andrey.a.pavlenko at gmail.com (Andrey Pavlenko)
Date: Wed, 17 Oct 2018 21:35:02 +0300
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
Message-ID: <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>

Are you sure that MappedByteBuffer is well suitable for concurrent random
reading of large files? MappedByteBuffer.get() is potentially blocking
operation. Perhaps AsynchronousFileChannel is more suitable for this case.

On Wed, Oct 17, 2018 at 8:36 PM Nathan and Ila Reynolds via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:

>  > creating zillions of short-lived ThreadLocals seems like an
> antipattern to me.
>
> Perhaps, you can share another way to solve this problem.
>
> I have a ByteBuffer that maps a large file.  I have multiple threads
> reading the ByteBuffer at different positions.  As long as the threads
> don't call ByteBuffer.position(), they can operate concurrently on the
> ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an absolute
> method hence the thread has to call position().
>
> Attempt #1: I started by putting a lock around the ByteBuffer. This
> causes a lot of contention.
>
> Attempt #2: I started by slicing the ByteBuffer.  This created a lot of
> garbage.
>
> Attempt #3: I put the sliced ByteBuffers into ThreadLocal but with many
> files mapped, consumed and unmapped rapidly, this leads to zillions of
> short-lived ThreadLocals.
>
> Attempt #4: I put the sliced ByteBuffers into a LinkedTransferQueue but
> this created a lot of garbage for creating nodes in the queue.
>
> Attempt #5: I put the sliced ByteBuffers into a ConcurrentHashMap keyed
> on the Thread.  I cannot remember why this didn't work.  I think the
> overhead of ConcurrentHashMap created a lot of garbage.
>
> Attempt #6: I went back to attempt #3 (ThreadLocal) and call expunge
> when the thread returns to the thread pool.  Yes, this creates zillions
> of short-lived ThreadLocals but they get cleaned out quickly so there is
> performance degradation for ThreadLocal lookup.
>
> Each thread cannot have its sliced ByteBuffer passed through the stack
> as an argument.  This would create a lot of garbage from duplicate
> structures.
>
> -Nathan
>
> On 10/17/2018 10:29 AM, Andrew Haley via Concurrency-interest wrote:
> > On 10/17/2018 04:07 PM, Doug Lea via Concurrency-interest wrote:
> >> This alone would not address long-standing problems with zillions of
> >> short-lived ThreadLocals in long-lived threads.  Right now, ThreadLocal
> >> is as fast as we know how to make it while still not completely falling
> >> over under such usages. The only solution I know for this is to create a
> >> new GC-aware storage class, which is not very likely to be adopted.
> > Well, yeah, but creating zillions of short-lived ThreadLocals seems
> > like an antipattern to me.
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/d40047bc/attachment.html>

From nathanila at gmail.com  Wed Oct 17 14:29:51 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Wed, 17 Oct 2018 12:29:51 -0600
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CAGmsiP4VSgyf3RLPNyJS9QMqkrzHapptW5uo2KifTwbKLdLWGQ@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAGmsiP4VSgyf3RLPNyJS9QMqkrzHapptW5uo2KifTwbKLdLWGQ@mail.gmail.com>
Message-ID: <691f958f-f9f5-2ca9-a8eb-d439d2a5d39e@gmail.com>

I wasn't aware of ByteBuffer.duplicate().  That makes more sense.  
However, with the way I am using ByteBuffer.slice() it is essentially 
the same as using duplicate().

-Nathan

On 10/17/2018 11:41 AM, Bob Lee wrote:
> Can you use ByteBuffer.duplicate()?
>
> Bob
>
> On Wed, Oct 17, 2018 at 10:35 AM Nathan and Ila Reynolds via 
> Concurrency-interest <concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>
>      > creating zillions of short-lived ThreadLocals seems like an
>     antipattern to me.
>
>     Perhaps, you can share another way to solve this problem.
>
>     I have a ByteBuffer that maps a large file.  I have multiple threads
>     reading the ByteBuffer at different positions.  As long as the
>     threads
>     don't call ByteBuffer.position(), they can operate concurrently on
>     the
>     ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an
>     absolute
>     method hence the thread has to call position().
>
>     Attempt #1: I started by putting a lock around the ByteBuffer. This
>     causes a lot of contention.
>
>     Attempt #2: I started by slicing the ByteBuffer.  This created a
>     lot of
>     garbage.
>
>     Attempt #3: I put the sliced ByteBuffers into ThreadLocal but with
>     many
>     files mapped, consumed and unmapped rapidly, this leads to
>     zillions of
>     short-lived ThreadLocals.
>
>     Attempt #4: I put the sliced ByteBuffers into a
>     LinkedTransferQueue but
>     this created a lot of garbage for creating nodes in the queue.
>
>     Attempt #5: I put the sliced ByteBuffers into a ConcurrentHashMap
>     keyed
>     on the Thread.  I cannot remember why this didn't work.  I think the
>     overhead of ConcurrentHashMap created a lot of garbage.
>
>     Attempt #6: I went back to attempt #3 (ThreadLocal) and call expunge
>     when the thread returns to the thread pool.  Yes, this creates
>     zillions
>     of short-lived ThreadLocals but they get cleaned out quickly so
>     there is
>     performance degradation for ThreadLocal lookup.
>
>     Each thread cannot have its sliced ByteBuffer passed through the
>     stack
>     as an argument.  This would create a lot of garbage from duplicate
>     structures.
>
>     -Nathan
>
>     On 10/17/2018 10:29 AM, Andrew Haley via Concurrency-interest wrote:
>     > On 10/17/2018 04:07 PM, Doug Lea via Concurrency-interest wrote:
>     >> This alone would not address long-standing problems with
>     zillions of
>     >> short-lived ThreadLocals in long-lived threads. Right now,
>     ThreadLocal
>     >> is as fast as we know how to make it while still not completely
>     falling
>     >> over under such usages. The only solution I know for this is to
>     create a
>     >> new GC-aware storage class, which is not very likely to be adopted.
>     > Well, yeah, but creating zillions of short-lived ThreadLocals seems
>     > like an antipattern to me.
>     >
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/4f4ead63/attachment-0001.html>

From dl at cs.oswego.edu  Wed Oct 17 14:36:18 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 17 Oct 2018 14:36:18 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CA+F8eeQrfX91Za6rnOLrsBnKwcWtAVbSuM6xrd=fFXfN-THzKw@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <CA+F8eeTb1BpSPV1ZAUL5YJFf4NUwpPFPE2WgoYYBAyHRLoZR1A@mail.gmail.com>
 <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>
 <CA+F8eeRHqOLomrQp7eNaK6Vv8Gzr5jwRwJ5Zq02rupdJJeBJtg@mail.gmail.com>
 <CANghgrTNwuv-4yipomaV+AzXFQM0dK5S69FqpV3RLuGR2O359w@mail.gmail.com>
 <CA+F8eeQrfX91Za6rnOLrsBnKwcWtAVbSuM6xrd=fFXfN-THzKw@mail.gmail.com>
Message-ID: <3dad0ffe-6872-a2f7-722f-2fbf62690d68@cs.oswego.edu>

On 10/17/18 2:15 PM, Tim Peierls wrote:
>  
> 
>     Doug's mention of "task local classes" sounds like he was alluding
>     to some new mode of access other than these three, so I would be
>     interested to know of such a thing.
> 
> 
> Not positive what Doug meant, but I don't think he was talking about a
> new mode of access when he wrote "task-local classes".

Right; as in (among other options) passing in a "TaskContext" object to
each task, so task body code would not need to rely implicitly on the
thread's ThreadLocalMap. This might in some cases be more ugly/awkward
but is still a trade-off worth considering.

-Doug


From nathanila at gmail.com  Wed Oct 17 14:52:49 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Wed, 17 Oct 2018 12:52:49 -0600
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
Message-ID: <369c2d07-c350-34e1-6101-941911ee2567@gmail.com>

The reads need the data before the rest of the operation could 
continue.  Hence, the thread would block on the returned Future. It 
seems FileChannel would be a better fit.

Does the read() operation from either class cause a round-trip through 
the kernel?  If so, MappedByteBuffer.get() avoids the kernel trip if the 
page is in RAM and hence can perform better.

-Nathan

On 10/17/2018 12:35 PM, Andrey Pavlenko wrote:
> Are you sure that MappedByteBuffer is well suitable for concurrent 
> random reading of large files? MappedByteBuffer.get() is potentially 
> blocking operation. Perhaps AsynchronousFileChannel is more suitable 
> for this case.
>
> On Wed, Oct 17, 2018 at 8:36 PM Nathan and Ila Reynolds via 
> Concurrency-interest <concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>
>      > creating zillions of short-lived ThreadLocals seems like an
>     antipattern to me.
>
>     Perhaps, you can share another way to solve this problem.
>
>     I have a ByteBuffer that maps a large file.  I have multiple threads
>     reading the ByteBuffer at different positions.  As long as the
>     threads
>     don't call ByteBuffer.position(), they can operate concurrently on
>     the
>     ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an
>     absolute
>     method hence the thread has to call position().
>
>     Attempt #1: I started by putting a lock around the ByteBuffer. This
>     causes a lot of contention.
>
>     Attempt #2: I started by slicing the ByteBuffer.  This created a
>     lot of
>     garbage.
>
>     Attempt #3: I put the sliced ByteBuffers into ThreadLocal but with
>     many
>     files mapped, consumed and unmapped rapidly, this leads to
>     zillions of
>     short-lived ThreadLocals.
>
>     Attempt #4: I put the sliced ByteBuffers into a
>     LinkedTransferQueue but
>     this created a lot of garbage for creating nodes in the queue.
>
>     Attempt #5: I put the sliced ByteBuffers into a ConcurrentHashMap
>     keyed
>     on the Thread.  I cannot remember why this didn't work.  I think the
>     overhead of ConcurrentHashMap created a lot of garbage.
>
>     Attempt #6: I went back to attempt #3 (ThreadLocal) and call expunge
>     when the thread returns to the thread pool.  Yes, this creates
>     zillions
>     of short-lived ThreadLocals but they get cleaned out quickly so
>     there is
>     performance degradation for ThreadLocal lookup.
>
>     Each thread cannot have its sliced ByteBuffer passed through the
>     stack
>     as an argument.  This would create a lot of garbage from duplicate
>     structures.
>
>     -Nathan
>
>     On 10/17/2018 10:29 AM, Andrew Haley via Concurrency-interest wrote:
>     > On 10/17/2018 04:07 PM, Doug Lea via Concurrency-interest wrote:
>     >> This alone would not address long-standing problems with
>     zillions of
>     >> short-lived ThreadLocals in long-lived threads. Right now,
>     ThreadLocal
>     >> is as fast as we know how to make it while still not completely
>     falling
>     >> over under such usages. The only solution I know for this is to
>     create a
>     >> new GC-aware storage class, which is not very likely to be adopted.
>     > Well, yeah, but creating zillions of short-lived ThreadLocals seems
>     > like an antipattern to me.
>     >
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/933d1970/attachment.html>

From brian at briangoetz.com  Wed Oct 17 15:33:42 2018
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 17 Oct 2018 15:33:42 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
Message-ID: <f55e4ef7-61d1-8538-67b0-f7721dee5cc7@briangoetz.com>

Or with a worse name, to ward off use by people who think that calling 
`System.gc()` "helps" the GC...

On 10/17/2018 2:26 PM, Doug Lea via Concurrency-interest wrote:
> [+list]
> 
> On 10/17/18 11:44 AM, Nathan and Ila Reynolds wrote:
>> Can we add the following method to ThreadLocal?
>>
>> public static void expungeStaleEntries()
> 
> This seems like a reasonable request (although perhaps with an improved
> name).  The functionality exists internally, and it seems overly
> parental not to export it for use as a band-aid by those people who have
> tried and otherwise failed to solve the zillions of short-lived
> ThreadLocals in long-lived threads problem.
> 
> Can anyone think of a reason not to do this?
> 
> -Doug
> 
>>
>> This method will call ThreadLocal.ThreadLocalMap.expungeStaleEntries()
>> for the ThreadLocalMap of the current thread.  Thread pools can then
>> call this method when the thread finishes processing a job after GC.
>> This solves the problem of zillions of short-lived ThreadLocals in
>> long-lived threads.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From nigro.fra at gmail.com  Wed Oct 17 15:48:08 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Wed, 17 Oct 2018 21:48:08 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <c8c185ab-82d1-9270-4d1b-041c670d1240@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CANghgrQpdkL2vUUA2NgbiWobux_axzz5dtjUmAcmNPP8swZEig@mail.gmail.com>
 <c8c185ab-82d1-9270-4d1b-041c670d1240@gmail.com>
Message-ID: <CAKxGtTUs=b4_Av-FX7zw8t4ofEp9YXrGm2AC5uwRMJyBszCUpA@mail.gmail.com>

@nathan

Re Absolute BytrBuffer ops AFAIK there is somethinf moving on
http://mail.openjdk.java.net/pipermail/nio-dev/2018-October/005511.html

Il mer 17 ott 2018, 21:26 Nathan and Ila Reynolds via Concurrency-interest <
concurrency-interest at cs.oswego.edu> ha scritto:

> I would love it if ByteBuffer had absolute versions of all the relative
> methods.  I have wanted the absolute versions in the past for other
> situations.
>
> My #2 is similar what you said.  Replace duplicate with slice.
>
>     buf.slice().position(newPos).get(byteArray);
>
> This produces a lot of garbage unless I keep it in a ThreadLocal or some
> other data structure.  I was running on Java 8.  Perhaps, Java 10 or 11
> would not produce as much garbage.
>
> -Nathan
>
> On 10/17/2018 11:44 AM, David Lloyd wrote:
> > On Wed, Oct 17, 2018 at 12:36 PM Nathan and Ila Reynolds via
> > Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> >> Perhaps, you can share another way to solve this problem.
> >>
> >> I have a ByteBuffer that maps a large file.  I have multiple threads
> >> reading the ByteBuffer at different positions.  As long as the threads
> >> don't call ByteBuffer.position(), they can operate concurrently on the
> >> ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an absolute
> >> method hence the thread has to call position().
> > The obvious solution would seem to be that we should enhance
> > ByteBuffer to have such a method.
> >
> > But, your #2 should work if you are careful to do it like this:
> >
> >     buf.duplicate().position(newPos).get(byteArray);
> >
> > In such cases, HotSpot can sometimes delete the allocation of the new
> > ByteBuffer altogether.  I seem to recall that my colleague Andrew
> > Haley (on this thread) did some work/research in this area a while
> > ago.
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/d0f332e9/attachment.html>

From david.lloyd at redhat.com  Wed Oct 17 16:07:51 2018
From: david.lloyd at redhat.com (David Lloyd)
Date: Wed, 17 Oct 2018 15:07:51 -0500
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <369c2d07-c350-34e1-6101-941911ee2567@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <369c2d07-c350-34e1-6101-941911ee2567@gmail.com>
Message-ID: <CANghgrRtJgY4O9nYoA=NHk0_OHrX1mL7AjY4wvfHZjvMdMnm2w@mail.gmail.com>

On Linux at least, nearly *any* memory access is a potentially
blocking operation.  It's really hard to say which is better than the
other without profiling in as realistic of a deployment situation as
possible.
On Wed, Oct 17, 2018 at 3:02 PM Nathan and Ila Reynolds via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>
> The reads need the data before the rest of the operation could continue.  Hence, the thread would block on the returned Future.  It seems FileChannel would be a better fit.
>
> Does the read() operation from either class cause a round-trip through the kernel?  If so, MappedByteBuffer.get() avoids the kernel trip if the page is in RAM and hence can perform better.
>
> -Nathan
>
> On 10/17/2018 12:35 PM, Andrey Pavlenko wrote:
>
> Are you sure that MappedByteBuffer is well suitable for concurrent random reading of large files? MappedByteBuffer.get() is potentially blocking operation. Perhaps AsynchronousFileChannel is more suitable for this case.
>
> On Wed, Oct 17, 2018 at 8:36 PM Nathan and Ila Reynolds via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>>
>>  > creating zillions of short-lived ThreadLocals seems like an
>> antipattern to me.
>>
>> Perhaps, you can share another way to solve this problem.
>>
>> I have a ByteBuffer that maps a large file.  I have multiple threads
>> reading the ByteBuffer at different positions.  As long as the threads
>> don't call ByteBuffer.position(), they can operate concurrently on the
>> ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an absolute
>> method hence the thread has to call position().
>>
>> Attempt #1: I started by putting a lock around the ByteBuffer. This
>> causes a lot of contention.
>>
>> Attempt #2: I started by slicing the ByteBuffer.  This created a lot of
>> garbage.
>>
>> Attempt #3: I put the sliced ByteBuffers into ThreadLocal but with many
>> files mapped, consumed and unmapped rapidly, this leads to zillions of
>> short-lived ThreadLocals.
>>
>> Attempt #4: I put the sliced ByteBuffers into a LinkedTransferQueue but
>> this created a lot of garbage for creating nodes in the queue.
>>
>> Attempt #5: I put the sliced ByteBuffers into a ConcurrentHashMap keyed
>> on the Thread.  I cannot remember why this didn't work.  I think the
>> overhead of ConcurrentHashMap created a lot of garbage.
>>
>> Attempt #6: I went back to attempt #3 (ThreadLocal) and call expunge
>> when the thread returns to the thread pool.  Yes, this creates zillions
>> of short-lived ThreadLocals but they get cleaned out quickly so there is
>> performance degradation for ThreadLocal lookup.
>>
>> Each thread cannot have its sliced ByteBuffer passed through the stack
>> as an argument.  This would create a lot of garbage from duplicate
>> structures.
>>
>> -Nathan
>>
>> On 10/17/2018 10:29 AM, Andrew Haley via Concurrency-interest wrote:
>> > On 10/17/2018 04:07 PM, Doug Lea via Concurrency-interest wrote:
>> >> This alone would not address long-standing problems with zillions of
>> >> short-lived ThreadLocals in long-lived threads.  Right now, ThreadLocal
>> >> is as fast as we know how to make it while still not completely falling
>> >> over under such usages. The only solution I know for this is to create a
>> >> new GC-aware storage class, which is not very likely to be adopted.
>> > Well, yeah, but creating zillions of short-lived ThreadLocals seems
>> > like an antipattern to me.
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-- 
- DML

From Alan.Bateman at oracle.com  Wed Oct 17 16:17:22 2018
From: Alan.Bateman at oracle.com (Alan Bateman)
Date: Wed, 17 Oct 2018 21:17:22 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
Message-ID: <6dd7eea0-8757-92c5-c0ba-521cc03e4e0f@oracle.com>

On 17/10/2018 18:06, Nathan and Ila Reynolds via Concurrency-interest wrote:
> > creating zillions of short-lived ThreadLocals seems like an 
> antipattern to me.
>
> Perhaps, you can share another way to solve this problem.
>
> I have a ByteBuffer that maps a large file.  I have multiple threads 
> reading the ByteBuffer at different positions.  As long as the threads 
> don't call ByteBuffer.position(), they can operate concurrently on the 
> ByteBuffer.  However, ByteBuffer.get(byte[]) does not have an absolute 
> method
Efforts to add absolute variants of the bulk get/put operations is under 
discussion on nio-dev. The intention isn't to make bufffers thread safe 
but it should help with many cases where buffer position is a hindrance.

-Alan

From ben.manes at gmail.com  Wed Oct 17 16:42:37 2018
From: ben.manes at gmail.com (Benjamin Manes)
Date: Wed, 17 Oct 2018 13:42:37 -0700
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <f55e4ef7-61d1-8538-67b0-f7721dee5cc7@briangoetz.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <f55e4ef7-61d1-8538-67b0-f7721dee5cc7@briangoetz.com>
Message-ID: <CAGu0=MPascEo6F+fvNrUJOFGwSoZZxjsX9ZNXgVjithw8hJzNw@mail.gmail.com>

ArrayList.trimToSize() is benign and almost entirely forgotten. Might find
something similar.

On Wed, Oct 17, 2018 at 1:40 PM Brian Goetz via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> Or with a worse name, to ward off use by people who think that calling
> `System.gc()` "helps" the GC...
>
> On 10/17/2018 2:26 PM, Doug Lea via Concurrency-interest wrote:
> > [+list]
> >
> > On 10/17/18 11:44 AM, Nathan and Ila Reynolds wrote:
> >> Can we add the following method to ThreadLocal?
> >>
> >> public static void expungeStaleEntries()
> >
> > This seems like a reasonable request (although perhaps with an improved
> > name).  The functionality exists internally, and it seems overly
> > parental not to export it for use as a band-aid by those people who have
> > tried and otherwise failed to solve the zillions of short-lived
> > ThreadLocals in long-lived threads problem.
> >
> > Can anyone think of a reason not to do this?
> >
> > -Doug
> >
> >>
> >> This method will call ThreadLocal.ThreadLocalMap.expungeStaleEntries()
> >> for the ThreadLocalMap of the current thread.  Thread pools can then
> >> call this method when the thread finishes processing a job after GC.
> >> This solves the problem of zillions of short-lived ThreadLocals in
> >> long-lived threads.
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181017/85ae1f59/attachment.html>

From forax at univ-mlv.fr  Wed Oct 17 17:05:37 2018
From: forax at univ-mlv.fr (Remi Forax)
Date: Wed, 17 Oct 2018 23:05:37 +0200 (CEST)
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <CA+F8eeTb1BpSPV1ZAUL5YJFf4NUwpPFPE2WgoYYBAyHRLoZR1A@mail.gmail.com>
 <CANghgrTV2SWcyimj128WxxAJVKkH2OZLv6Brfi4XhbiKxTsKvQ@mail.gmail.com>
Message-ID: <420601237.541365.1539810337363.JavaMail.zimbra@u-pem.fr>

The other possible API is an abstraction over a stack that grows at the same time the stack grows

  class Context<V> {
    public Context(Supplier<? extends V> initialValueSupplier);
    public V getValue();
    public void setValue(V value);
    public void enter(Runnable runnable);
  }

The idea is that if you want to access (read/write) to the value, you have to do it in the Runnable after calling enter().
By example,

  var context = new Context<>(() -> 1);
  var context2 = new Context<>(() -> 2);
    
  context.enter(() -> {
    context2.enter(() -> {
      assertEquals(1, (int)context.getValue());
      assertEquals(2, (int)context2.getValue());
    });
  });

The way to implement this API is to have a growable array of pairs Context/Value in java.lang.Thread, finding/replacing the value is a loop from the end of the array to the beginning that returns the value associated to the context.

This API has the advantage to be explicit about the cost (you see the Context.enter in the stacktrace), usages of ThreadLocal is to be easy to hide IMO, you have even a lot of codes that initialize a ThreadLocal in a static block even if you never calls the method that using it (this may be fixed when lazy static final field will be implemented [1]).

Rémi

[1] https://bugs.openjdk.java.net/browse/JDK-8209964

----- Mail original -----
> De: "concurrency-interest" <concurrency-interest at cs.oswego.edu>
> À: tim at peierls.net
> Cc: "Doug Lea" <dl at cs.oswego.edu>, "concurrency-interest" <concurrency-interest at cs.oswego.edu>
> Envoyé: Mercredi 17 Octobre 2018 18:10:43
> Objet: Re: [concurrency-interest] Overhead of ThreadLocal data

> On Wed, Oct 17, 2018 at 11:01 AM Tim Peierls via Concurrency-interest
> <concurrency-interest at cs.oswego.edu> wrote:
>> On Wed, Oct 17, 2018 at 11:28 AM Doug Lea via Concurrency-interest
>> <concurrency-interest at cs.oswego.edu> wrote:
>>> Also consider restructuring code to use task-local classes (perhaps with
>>> linkages among them) that are GCable when tasks complete.
>>
>> I bet a lot of the ThreadLocal uses under consideration would benefit from this
>> kind of restructuring, avoiding ThreadLocal entirely.
> 
> How would one access task-local data if not by way of a ThreadLocal?
> --
> - DML
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From aph at redhat.com  Thu Oct 18 04:21:55 2018
From: aph at redhat.com (Andrew Haley)
Date: Thu, 18 Oct 2018 09:21:55 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
Message-ID: <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>

On 10/17/2018 07:35 PM, Andrey Pavlenko via Concurrency-interest wrote:
> Are you sure that MappedByteBuffer is well suitable for concurrent random
> reading of large files?

Yes.

> MappedByteBuffer.get() is potentially blocking
> operation. 

So is any access to memory if you're running out of space.

> Perhaps AsynchronousFileChannel is more suitable for this case.

Why? You can prefetch the file if you need to have it ready.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Thu Oct 18 04:24:04 2018
From: aph at redhat.com (Andrew Haley)
Date: Thu, 18 Oct 2018 09:24:04 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
Message-ID: <918307b2-bb15-d279-ffa0-c06d9978d09f@redhat.com>

On 10/17/2018 06:06 PM, Nathan and Ila Reynolds via Concurrency-interest wrote:
> Each thread cannot have its sliced ByteBuffer passed through the stack 
> as an argument.  This would create a lot of garbage from duplicate 
> structures.

Why would it create a lot of garbage when passed through the stack?

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Thu Oct 18 04:36:37 2018
From: aph at redhat.com (Andrew Haley)
Date: Thu, 18 Oct 2018 09:36:37 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CAP0L=UR+D_ewg1RvwwKF=gOPQ=TOLvgrDz_mrc96iz7usCH5bw@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <CAP0L=UR+D_ewg1RvwwKF=gOPQ=TOLvgrDz_mrc96iz7usCH5bw@mail.gmail.com>
Message-ID: <4e56515b-a898-4c82-8325-5d52e1ac82a0@redhat.com>

On 10/17/2018 05:50 PM, Joshua Bloch wrote:
> For your amusement, when I first designed and implemented
> ThreadLocal, I assumed that no VM would ever have more that ~10
> thread locals over its lifetime. We reimplemented it several times
> as my initial estimate proved further and further from the truth.

:-)

> The API has held up pretty well, though.

Indeed it has. The big problem is that unless you have a very deep
knowledge of the VM, you're not going to realize that
Thread.currentThread() costs nothing, but ThreadLocal.get() is
expensive.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From andrey.a.pavlenko at gmail.com  Thu Oct 18 05:29:52 2018
From: andrey.a.pavlenko at gmail.com (Andrey Pavlenko)
Date: Thu, 18 Oct 2018 12:29:52 +0300
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
Message-ID: <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>

In case of a non-large file and/or sequential reads MappedByteBuffer is
definitely faster, but the question here is about *concurrent* *random*
reads of *large* files. For this purpose MappedByteBuffer may be
inefficient.

On Thu, Oct 18, 2018 at 11:21 AM Andrew Haley <aph at redhat.com> wrote:

> On 10/17/2018 07:35 PM, Andrey Pavlenko via Concurrency-interest wrote:
> > Are you sure that MappedByteBuffer is well suitable for concurrent random
> > reading of large files?
>
> Yes.
>
> > MappedByteBuffer.get() is potentially blocking
> > operation.
>
> So is any access to memory if you're running out of space.
>
> > Perhaps AsynchronousFileChannel is more suitable for this case.
>
> Why? You can prefetch the file if you need to have it ready.
>
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181018/e687be55/attachment-0001.html>

From aph at redhat.com  Thu Oct 18 05:58:42 2018
From: aph at redhat.com (Andrew Haley)
Date: Thu, 18 Oct 2018 10:58:42 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
Message-ID: <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>

On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
> In case of a non-large file and/or sequential reads MappedByteBuffer is
> definitely faster, but the question here is about *concurrent* *random*
> reads of *large* files. For this purpose MappedByteBuffer may be
> inefficient.

Perhaps. It might be that manually managing memory (by reading and
writing the parts of the file you need) works better than letting the
kernel do it, but the kernel will cache as much as it can anyway, so
it's not as if it'll necessarily save memory or reduce disk activity.
There are advantages to using read() for sequential file access because
the kernel can automatically read and cache the next part of the file.

There were some problems with inefficient code generation for byte
buffers but we have worked on that and it's better now, with (even)
more improvements to come. Unless there are kernel issues I don't know
about, mapped files are excellent for random access, and the Java
ByteBuffer operations generate excellent code. (This isn't guaranteed
because C2 uses a bunch of heuristics, but it's usually good.)

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Thu Oct 18 06:13:41 2018
From: aph at redhat.com (Andrew Haley)
Date: Thu, 18 Oct 2018 11:13:41 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
Message-ID: <4007fd12-fa0e-7e19-8e2c-be0732947298@redhat.com>

On 10/18/2018 10:58 AM, Andrew Haley wrote:
> Perhaps. It might be that manually managing memory (by reading and
> writing the parts of the file you need) works better than letting the
> kernel do it, but the kernel will cache as much as it can anyway, so
> it's not as if it'll necessarily save memory or reduce disk activity.

Thinking about this some more, it would be nice to have an interface to
madvise(2), in particular MADV_DONTNEED.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Thu Oct 18 10:06:30 2018
From: aph at redhat.com (Andrew Haley)
Date: Thu, 18 Oct 2018 15:06:30 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
Message-ID: <699f6a40-2912-3858-cf52-860b9a64be11@redhat.com>

On 10/18/2018 10:58 AM, Andrew Haley via Concurrency-interest wrote:
> On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>> In case of a non-large file and/or sequential reads MappedByteBuffer is
>> definitely faster, but the question here is about *concurrent* *random*
>> reads of *large* files. For this purpose MappedByteBuffer may be
>> inefficient.
> 
> Perhaps.

Mea culpa. I went out for a walk and thought about this, and one
important problem with using MappedByteBuffers for random access
dawned on me: you're looking at extended time-to-safepoint
pauses. This would happen on any system with slow storage or a heavily
overloaded I/O subsystem which blocked reading a page from memory. It
wouldn't happen with a File interface using simply read() calls
because threads blocked in read() don't delay safepoints.

So yes, I take your point.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From nathanila at gmail.com  Thu Oct 18 10:45:57 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Thu, 18 Oct 2018 08:45:57 -0600
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
Message-ID: <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>

When accessing a file through the kernel's file I/O, the thread context 
switches from user land into kernel land.  The thread then goes to the 
file cache to see if the data is there.  If not, the kernel blocks the 
thread and pulls the data from disk.  Once the data is in the file 
cache, the thread copies the data into the user land buffer and context 
switches back to user land.

When accessing a file through memory mapped I/O, the thread does a load 
instruction against RAM.  If the data is not in RAM, then the thread 
switches to the kernel, blocks while pulling data from disk and resumes 
operation.

File I/O and memory mapped I/O do the same operations but in a different 
order.  The difference is key.  With file I/O, the thread has to context 
switch into the kernel with every access. Thus, we use large buffers to 
minimize the performance impact of the kernel round trip.  It is the 
context switch with every operation that hurts file I/O and where memory 
mapped I/O shines. So, memory mapped I/O does well at concurrent random 
reads of large files, but comes with an initialization cost and isn't 
the best solution for all file access.  I have found that file I/O 
considerably outperforms memory mapped I/O when sequentially reading and 
writing to a file unless you can map the entire file in one large piece.

-Nathan

On 10/18/2018 3:58 AM, Andrew Haley wrote:
> On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>> In case of a non-large file and/or sequential reads MappedByteBuffer is
>> definitely faster, but the question here is about *concurrent* *random*
>> reads of *large* files. For this purpose MappedByteBuffer may be
>> inefficient.
> Perhaps. It might be that manually managing memory (by reading and
> writing the parts of the file you need) works better than letting the
> kernel do it, but the kernel will cache as much as it can anyway, so
> it's not as if it'll necessarily save memory or reduce disk activity.
> There are advantages to using read() for sequential file access because
> the kernel can automatically read and cache the next part of the file.
>
> There were some problems with inefficient code generation for byte
> buffers but we have worked on that and it's better now, with (even)
> more improvements to come. Unless there are kernel issues I don't know
> about, mapped files are excellent for random access, and the Java
> ByteBuffer operations generate excellent code. (This isn't guaranteed
> because C2 uses a bunch of heuristics, but it's usually good.)
>

From nigro.fra at gmail.com  Thu Oct 18 12:06:33 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Thu, 18 Oct 2018 18:06:33 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
Message-ID: <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>

@nathan
The point that Andrew raised is more related to safepoints: on
MappedByteBuffer access you're in the JVM land and you're working between
safepoints ie you got blocked by the kernel means that you won't arrive
soon to the next safepoint, delaying the others that are waiting for you to
join.
With FileChannel you're in the JNI that is worlking *in* a safepoint and
won't delay any java mutator threads to reach it (if needed). Makes sense?

Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via
Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:

> When accessing a file through the kernel's file I/O, the thread context
> switches from user land into kernel land.  The thread then goes to the
> file cache to see if the data is there.  If not, the kernel blocks the
> thread and pulls the data from disk.  Once the data is in the file
> cache, the thread copies the data into the user land buffer and context
> switches back to user land.
>
> When accessing a file through memory mapped I/O, the thread does a load
> instruction against RAM.  If the data is not in RAM, then the thread
> switches to the kernel, blocks while pulling data from disk and resumes
> operation.
>
> File I/O and memory mapped I/O do the same operations but in a different
> order.  The difference is key.  With file I/O, the thread has to context
> switch into the kernel with every access. Thus, we use large buffers to
> minimize the performance impact of the kernel round trip.  It is the
> context switch with every operation that hurts file I/O and where memory
> mapped I/O shines. So, memory mapped I/O does well at concurrent random
> reads of large files, but comes with an initialization cost and isn't
> the best solution for all file access.  I have found that file I/O
> considerably outperforms memory mapped I/O when sequentially reading and
> writing to a file unless you can map the entire file in one large piece.
>
> -Nathan
>
> On 10/18/2018 3:58 AM, Andrew Haley wrote:
> > On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
> >> In case of a non-large file and/or sequential reads MappedByteBuffer is
> >> definitely faster, but the question here is about *concurrent* *random*
> >> reads of *large* files. For this purpose MappedByteBuffer may be
> >> inefficient.
> > Perhaps. It might be that manually managing memory (by reading and
> > writing the parts of the file you need) works better than letting the
> > kernel do it, but the kernel will cache as much as it can anyway, so
> > it's not as if it'll necessarily save memory or reduce disk activity.
> > There are advantages to using read() for sequential file access because
> > the kernel can automatically read and cache the next part of the file.
> >
> > There were some problems with inefficient code generation for byte
> > buffers but we have worked on that and it's better now, with (even)
> > more improvements to come. Unless there are kernel issues I don't know
> > about, mapped files are excellent for random access, and the Java
> > ByteBuffer operations generate excellent code. (This isn't guaranteed
> > because C2 uses a bunch of heuristics, but it's usually good.)
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181018/5a9f3b98/attachment.html>

From oleksandr.otenko at gmail.com  Thu Oct 18 13:06:04 2018
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 18 Oct 2018 18:06:04 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
Message-ID: <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>

No, it doesn't. Accessing memory mapped area is off heap, so what makes it
less safe than JNI?
On 18 Oct 2018 17:58, "Francesco Nigro via Concurrency-interest" <
concurrency-interest at cs.oswego.edu> wrote:

> @nathan
> The point that Andrew raised is more related to safepoints: on
> MappedByteBuffer access you're in the JVM land and you're working between
> safepoints ie you got blocked by the kernel means that you won't arrive
> soon to the next safepoint, delaying the others that are waiting for you to
> join.
> With FileChannel you're in the JNI that is worlking *in* a safepoint and
> won't delay any java mutator threads to reach it (if needed). Makes sense?
>
> Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via
> Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:
>
>> When accessing a file through the kernel's file I/O, the thread context
>> switches from user land into kernel land.  The thread then goes to the
>> file cache to see if the data is there.  If not, the kernel blocks the
>> thread and pulls the data from disk.  Once the data is in the file
>> cache, the thread copies the data into the user land buffer and context
>> switches back to user land.
>>
>> When accessing a file through memory mapped I/O, the thread does a load
>> instruction against RAM.  If the data is not in RAM, then the thread
>> switches to the kernel, blocks while pulling data from disk and resumes
>> operation.
>>
>> File I/O and memory mapped I/O do the same operations but in a different
>> order.  The difference is key.  With file I/O, the thread has to context
>> switch into the kernel with every access. Thus, we use large buffers to
>> minimize the performance impact of the kernel round trip.  It is the
>> context switch with every operation that hurts file I/O and where memory
>> mapped I/O shines. So, memory mapped I/O does well at concurrent random
>> reads of large files, but comes with an initialization cost and isn't
>> the best solution for all file access.  I have found that file I/O
>> considerably outperforms memory mapped I/O when sequentially reading and
>> writing to a file unless you can map the entire file in one large piece.
>>
>> -Nathan
>>
>> On 10/18/2018 3:58 AM, Andrew Haley wrote:
>> > On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>> >> In case of a non-large file and/or sequential reads MappedByteBuffer is
>> >> definitely faster, but the question here is about *concurrent* *random*
>> >> reads of *large* files. For this purpose MappedByteBuffer may be
>> >> inefficient.
>> > Perhaps. It might be that manually managing memory (by reading and
>> > writing the parts of the file you need) works better than letting the
>> > kernel do it, but the kernel will cache as much as it can anyway, so
>> > it's not as if it'll necessarily save memory or reduce disk activity.
>> > There are advantages to using read() for sequential file access because
>> > the kernel can automatically read and cache the next part of the file.
>> >
>> > There were some problems with inefficient code generation for byte
>> > buffers but we have worked on that and it's better now, with (even)
>> > more improvements to come. Unless there are kernel issues I don't know
>> > about, mapped files are excellent for random access, and the Java
>> > ByteBuffer operations generate excellent code. (This isn't guaranteed
>> > because C2 uses a bunch of heuristics, but it's usually good.)
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181018/a3f3c741/attachment.html>

From nathanila at gmail.com  Thu Oct 18 13:27:22 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Thu, 18 Oct 2018 11:27:22 -0600
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
Message-ID: <dd66cb89-b6be-a3c3-569c-4b1c111712a9@gmail.com>

Yes, thank you for clarifying.  Besides taking the thread's callstack 
(?) and GC, what other operations wait for a Java thread to reach a 
safepoint?

-Nathan

On 10/18/2018 10:06 AM, Francesco Nigro wrote:
> @nathan
> The point that Andrew raised is more related to safepoints: on 
> MappedByteBuffer access you're in the JVM land and you're working 
> between safepoints ie you got blocked by the kernel means that you 
> won't arrive soon to the next safepoint, delaying the others that are 
> waiting for you to join.
> With FileChannel you're in the JNI that is worlking *in* a safepoint 
> and won't delay any java mutator threads to reach it (if needed). 
> Makes sense?
>
> Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via 
> Concurrency-interest <concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>> ha scritto:
>
>     When accessing a file through the kernel's file I/O, the thread
>     context
>     switches from user land into kernel land.  The thread then goes to
>     the
>     file cache to see if the data is there.  If not, the kernel blocks
>     the
>     thread and pulls the data from disk.  Once the data is in the file
>     cache, the thread copies the data into the user land buffer and
>     context
>     switches back to user land.
>
>     When accessing a file through memory mapped I/O, the thread does a
>     load
>     instruction against RAM.  If the data is not in RAM, then the thread
>     switches to the kernel, blocks while pulling data from disk and
>     resumes
>     operation.
>
>     File I/O and memory mapped I/O do the same operations but in a
>     different
>     order.  The difference is key.  With file I/O, the thread has to
>     context
>     switch into the kernel with every access. Thus, we use large
>     buffers to
>     minimize the performance impact of the kernel round trip.  It is the
>     context switch with every operation that hurts file I/O and where
>     memory
>     mapped I/O shines. So, memory mapped I/O does well at concurrent
>     random
>     reads of large files, but comes with an initialization cost and isn't
>     the best solution for all file access.  I have found that file I/O
>     considerably outperforms memory mapped I/O when sequentially
>     reading and
>     writing to a file unless you can map the entire file in one large
>     piece.
>
>     -Nathan
>
>     On 10/18/2018 3:58 AM, Andrew Haley wrote:
>     > On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>     >> In case of a non-large file and/or sequential reads
>     MappedByteBuffer is
>     >> definitely faster, but the question here is about *concurrent*
>     *random*
>     >> reads of *large* files. For this purpose MappedByteBuffer may be
>     >> inefficient.
>     > Perhaps. It might be that manually managing memory (by reading and
>     > writing the parts of the file you need) works better than
>     letting the
>     > kernel do it, but the kernel will cache as much as it can anyway, so
>     > it's not as if it'll necessarily save memory or reduce disk
>     activity.
>     > There are advantages to using read() for sequential file access
>     because
>     > the kernel can automatically read and cache the next part of the
>     file.
>     >
>     > There were some problems with inefficient code generation for byte
>     > buffers but we have worked on that and it's better now, with (even)
>     > more improvements to come. Unless there are kernel issues I
>     don't know
>     > about, mapped files are excellent for random access, and the Java
>     > ByteBuffer operations generate excellent code. (This isn't
>     guaranteed
>     > because C2 uses a bunch of heuristics, but it's usually good.)
>     >
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181018/14badda9/attachment-0001.html>

From nigro.fra at gmail.com  Thu Oct 18 13:33:38 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Thu, 18 Oct 2018 19:33:38 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
Message-ID: <CAKxGtTXsBG_5=Q6Qb5NPR2w7WLJznCmknJCUSH32e9T=_wh2Mg@mail.gmail.com>

Isn't less "safe" but if a MappedByteBuffer::get got inlined and the Unsafe
call is an intrinsic I'm expecting it to not have any safepoints poll in it
(if not inlined it "should" have a poll_return on method exit AFAIK). I
have to verify it with newer versions of the JVM, but that's a quite big
difference AFAIK if compared with a JNI call.
Am I missing anything? I apologize if I've written something incorrect or
not precise enough

Sorry I have answered privately this one :)

Il gio 18 ott 2018, 19:06 Alex Otenko <oleksandr.otenko at gmail.com> ha
scritto:

> No, it doesn't. Accessing memory mapped area is off heap, so what makes it
> less safe than JNI?
> On 18 Oct 2018 17:58, "Francesco Nigro via Concurrency-interest" <
> concurrency-interest at cs.oswego.edu> wrote:
>
>> @nathan
>> The point that Andrew raised is more related to safepoints: on
>> MappedByteBuffer access you're in the JVM land and you're working between
>> safepoints ie you got blocked by the kernel means that you won't arrive
>> soon to the next safepoint, delaying the others that are waiting for you to
>> join.
>> With FileChannel you're in the JNI that is worlking *in* a safepoint and
>> won't delay any java mutator threads to reach it (if needed). Makes sense?
>>
>> Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via
>> Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:
>>
>>> When accessing a file through the kernel's file I/O, the thread context
>>> switches from user land into kernel land.  The thread then goes to the
>>> file cache to see if the data is there.  If not, the kernel blocks the
>>> thread and pulls the data from disk.  Once the data is in the file
>>> cache, the thread copies the data into the user land buffer and context
>>> switches back to user land.
>>>
>>> When accessing a file through memory mapped I/O, the thread does a load
>>> instruction against RAM.  If the data is not in RAM, then the thread
>>> switches to the kernel, blocks while pulling data from disk and resumes
>>> operation.
>>>
>>> File I/O and memory mapped I/O do the same operations but in a different
>>> order.  The difference is key.  With file I/O, the thread has to context
>>> switch into the kernel with every access. Thus, we use large buffers to
>>> minimize the performance impact of the kernel round trip.  It is the
>>> context switch with every operation that hurts file I/O and where memory
>>> mapped I/O shines. So, memory mapped I/O does well at concurrent random
>>> reads of large files, but comes with an initialization cost and isn't
>>> the best solution for all file access.  I have found that file I/O
>>> considerably outperforms memory mapped I/O when sequentially reading and
>>> writing to a file unless you can map the entire file in one large piece.
>>>
>>> -Nathan
>>>
>>> On 10/18/2018 3:58 AM, Andrew Haley wrote:
>>> > On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>>> >> In case of a non-large file and/or sequential reads MappedByteBuffer
>>> is
>>> >> definitely faster, but the question here is about *concurrent*
>>> *random*
>>> >> reads of *large* files. For this purpose MappedByteBuffer may be
>>> >> inefficient.
>>> > Perhaps. It might be that manually managing memory (by reading and
>>> > writing the parts of the file you need) works better than letting the
>>> > kernel do it, but the kernel will cache as much as it can anyway, so
>>> > it's not as if it'll necessarily save memory or reduce disk activity.
>>> > There are advantages to using read() for sequential file access because
>>> > the kernel can automatically read and cache the next part of the file.
>>> >
>>> > There were some problems with inefficient code generation for byte
>>> > buffers but we have worked on that and it's better now, with (even)
>>> > more improvements to come. Unless there are kernel issues I don't know
>>> > about, mapped files are excellent for random access, and the Java
>>> > ByteBuffer operations generate excellent code. (This isn't guaranteed
>>> > because C2 uses a bunch of heuristics, but it's usually good.)
>>> >
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181018/f61c16b4/attachment.html>

From nigro.fra at gmail.com  Thu Oct 18 13:34:47 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Thu, 18 Oct 2018 19:34:47 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <dd66cb89-b6be-a3c3-569c-4b1c111712a9@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <dd66cb89-b6be-a3c3-569c-4b1c111712a9@gmail.com>
Message-ID: <CAKxGtTWRK9t+GCrN5WdOHPV+-zciFEC+xO=vRM0p8oEu4LU1ww@mail.gmail.com>

@nathan I do not have a complete list and I was asking to Andrew this same
question when

Il gio 18 ott 2018, 19:27 Nathan and Ila Reynolds <nathanila at gmail.com> ha
scritto:

> Yes, thank you for clarifying.  Besides taking the thread's callstack (?)
> and GC, what other operations wait for a Java thread to reach a safepoint?
>
> -Nathan
>
> On 10/18/2018 10:06 AM, Francesco Nigro wrote:
>
> @nathan
> The point that Andrew raised is more related to safepoints: on
> MappedByteBuffer access you're in the JVM land and you're working between
> safepoints ie you got blocked by the kernel means that you won't arrive
> soon to the next safepoint, delaying the others that are waiting for you to
> join.
> With FileChannel you're in the JNI that is worlking *in* a safepoint and
> won't delay any java mutator threads to reach it (if needed). Makes sense?
>
> Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via
> Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:
>
>> When accessing a file through the kernel's file I/O, the thread context
>> switches from user land into kernel land.  The thread then goes to the
>> file cache to see if the data is there.  If not, the kernel blocks the
>> thread and pulls the data from disk.  Once the data is in the file
>> cache, the thread copies the data into the user land buffer and context
>> switches back to user land.
>>
>> When accessing a file through memory mapped I/O, the thread does a load
>> instruction against RAM.  If the data is not in RAM, then the thread
>> switches to the kernel, blocks while pulling data from disk and resumes
>> operation.
>>
>> File I/O and memory mapped I/O do the same operations but in a different
>> order.  The difference is key.  With file I/O, the thread has to context
>> switch into the kernel with every access. Thus, we use large buffers to
>> minimize the performance impact of the kernel round trip.  It is the
>> context switch with every operation that hurts file I/O and where memory
>> mapped I/O shines. So, memory mapped I/O does well at concurrent random
>> reads of large files, but comes with an initialization cost and isn't
>> the best solution for all file access.  I have found that file I/O
>> considerably outperforms memory mapped I/O when sequentially reading and
>> writing to a file unless you can map the entire file in one large piece.
>>
>> -Nathan
>>
>> On 10/18/2018 3:58 AM, Andrew Haley wrote:
>> > On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>> >> In case of a non-large file and/or sequential reads MappedByteBuffer is
>> >> definitely faster, but the question here is about *concurrent* *random*
>> >> reads of *large* files. For this purpose MappedByteBuffer may be
>> >> inefficient.
>> > Perhaps. It might be that manually managing memory (by reading and
>> > writing the parts of the file you need) works better than letting the
>> > kernel do it, but the kernel will cache as much as it can anyway, so
>> > it's not as if it'll necessarily save memory or reduce disk activity.
>> > There are advantages to using read() for sequential file access because
>> > the kernel can automatically read and cache the next part of the file.
>> >
>> > There were some problems with inefficient code generation for byte
>> > buffers but we have worked on that and it's better now, with (even)
>> > more improvements to come. Unless there are kernel issues I don't know
>> > about, mapped files are excellent for random access, and the Java
>> > ByteBuffer operations generate excellent code. (This isn't guaranteed
>> > because C2 uses a bunch of heuristics, but it's usually good.)
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181018/acc3c565/attachment.html>

From nathanila at gmail.com  Thu Oct 18 13:38:11 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Thu, 18 Oct 2018 11:38:11 -0600
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
Message-ID: <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>

When a thread reads a memory mapped area, it does a load instruction.  
If the page is not in RAM or the process's page table, then the thread 
will exit user land and enter the kernel. The thread could then load the 
page from disk.  This could take a while especially if the disk is 
heavily loaded.  Meanwhile, the JVM could be waiting for the thread to 
reach a safepoint.  It won't be able to until the thread returns from 
loading the page from disk.

With regular file I/O, the thread can be blocked inside the kernel.  
When it returns from the kernel, it checks a flag and sees that it 
should block.  Thus, the thread does not execute Java code during a 
stop-the-world operation.  Hence, the JVM can assume the thread has 
reached a safepoint.

-Nathan

On 10/18/2018 11:06 AM, Alex Otenko wrote:
>
> No, it doesn't. Accessing memory mapped area is off heap, so what 
> makes it less safe than JNI?
>
> On 18 Oct 2018 17:58, "Francesco Nigro via Concurrency-interest" 
> <concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>
>     @nathan
>     The point that Andrew raised is more related to safepoints: on
>     MappedByteBuffer access you're in the JVM land and you're working
>     between safepoints ie you got blocked by the kernel means that you
>     won't arrive soon to the next safepoint, delaying the others that
>     are waiting for you to join.
>     With FileChannel you're in the JNI that is worlking *in* a
>     safepoint and won't delay any java mutator threads to reach it (if
>     needed). Makes sense?
>
>     Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds
>     via Concurrency-interest <concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>> ha scritto:
>
>         When accessing a file through the kernel's file I/O, the
>         thread context
>         switches from user land into kernel land.  The thread then
>         goes to the
>         file cache to see if the data is there.  If not, the kernel
>         blocks the
>         thread and pulls the data from disk.  Once the data is in the
>         file
>         cache, the thread copies the data into the user land buffer
>         and context
>         switches back to user land.
>
>         When accessing a file through memory mapped I/O, the thread
>         does a load
>         instruction against RAM.  If the data is not in RAM, then the
>         thread
>         switches to the kernel, blocks while pulling data from disk
>         and resumes
>         operation.
>
>         File I/O and memory mapped I/O do the same operations but in a
>         different
>         order.  The difference is key.  With file I/O, the thread has
>         to context
>         switch into the kernel with every access. Thus, we use large
>         buffers to
>         minimize the performance impact of the kernel round trip. It
>         is the
>         context switch with every operation that hurts file I/O and
>         where memory
>         mapped I/O shines. So, memory mapped I/O does well at
>         concurrent random
>         reads of large files, but comes with an initialization cost
>         and isn't
>         the best solution for all file access.  I have found that file
>         I/O
>         considerably outperforms memory mapped I/O when sequentially
>         reading and
>         writing to a file unless you can map the entire file in one
>         large piece.
>
>         -Nathan
>
>         On 10/18/2018 3:58 AM, Andrew Haley wrote:
>         > On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>         >> In case of a non-large file and/or sequential reads
>         MappedByteBuffer is
>         >> definitely faster, but the question here is about
>         *concurrent* *random*
>         >> reads of *large* files. For this purpose MappedByteBuffer
>         may be
>         >> inefficient.
>         > Perhaps. It might be that manually managing memory (by
>         reading and
>         > writing the parts of the file you need) works better than
>         letting the
>         > kernel do it, but the kernel will cache as much as it can
>         anyway, so
>         > it's not as if it'll necessarily save memory or reduce disk
>         activity.
>         > There are advantages to using read() for sequential file
>         access because
>         > the kernel can automatically read and cache the next part of
>         the file.
>         >
>         > There were some problems with inefficient code generation
>         for byte
>         > buffers but we have worked on that and it's better now, with
>         (even)
>         > more improvements to come. Unless there are kernel issues I
>         don't know
>         > about, mapped files are excellent for random access, and the
>         Java
>         > ByteBuffer operations generate excellent code. (This isn't
>         guaranteed
>         > because C2 uses a bunch of heuristics, but it's usually good.)
>         >
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181018/8a3eafa8/attachment-0001.html>

From oleksandr.otenko at gmail.com  Thu Oct 18 13:42:31 2018
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 18 Oct 2018 18:42:31 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
 <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>
Message-ID: <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>

This is strange. While the page is being loaded, the thread is not on the
CPU. Does the GC wait for all the threads to get back on the CPU? Or is
there some other reason that doesn't allow to find out whether the thread
is on the CPU?

Alex
On 18 Oct 2018 18:38, "Nathan and Ila Reynolds" <nathanila at gmail.com> wrote:

> When a thread reads a memory mapped area, it does a load instruction.  If
> the page is not in RAM or the process's page table, then the thread will
> exit user land and enter the kernel.  The thread could then load the page
> from disk.  This could take a while especially if the disk is heavily
> loaded.  Meanwhile, the JVM could be waiting for the thread to reach a
> safepoint.  It won't be able to until the thread returns from loading the
> page from disk.
>
> With regular file I/O, the thread can be blocked inside the kernel.  When
> it returns from the kernel, it checks a flag and sees that it should
> block.  Thus, the thread does not execute Java code during a stop-the-world
> operation.  Hence, the JVM can assume the thread has reached a safepoint.
>
> -Nathan
>
> On 10/18/2018 11:06 AM, Alex Otenko wrote:
>
> No, it doesn't. Accessing memory mapped area is off heap, so what makes it
> less safe than JNI?
> On 18 Oct 2018 17:58, "Francesco Nigro via Concurrency-interest" <
> concurrency-interest at cs.oswego.edu> wrote:
>
>> @nathan
>> The point that Andrew raised is more related to safepoints: on
>> MappedByteBuffer access you're in the JVM land and you're working between
>> safepoints ie you got blocked by the kernel means that you won't arrive
>> soon to the next safepoint, delaying the others that are waiting for you to
>> join.
>> With FileChannel you're in the JNI that is worlking *in* a safepoint and
>> won't delay any java mutator threads to reach it (if needed). Makes sense?
>>
>> Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via
>> Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:
>>
>>> When accessing a file through the kernel's file I/O, the thread context
>>> switches from user land into kernel land.  The thread then goes to the
>>> file cache to see if the data is there.  If not, the kernel blocks the
>>> thread and pulls the data from disk.  Once the data is in the file
>>> cache, the thread copies the data into the user land buffer and context
>>> switches back to user land.
>>>
>>> When accessing a file through memory mapped I/O, the thread does a load
>>> instruction against RAM.  If the data is not in RAM, then the thread
>>> switches to the kernel, blocks while pulling data from disk and resumes
>>> operation.
>>>
>>> File I/O and memory mapped I/O do the same operations but in a different
>>> order.  The difference is key.  With file I/O, the thread has to context
>>> switch into the kernel with every access. Thus, we use large buffers to
>>> minimize the performance impact of the kernel round trip.  It is the
>>> context switch with every operation that hurts file I/O and where memory
>>> mapped I/O shines. So, memory mapped I/O does well at concurrent random
>>> reads of large files, but comes with an initialization cost and isn't
>>> the best solution for all file access.  I have found that file I/O
>>> considerably outperforms memory mapped I/O when sequentially reading and
>>> writing to a file unless you can map the entire file in one large piece.
>>>
>>> -Nathan
>>>
>>> On 10/18/2018 3:58 AM, Andrew Haley wrote:
>>> > On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>>> >> In case of a non-large file and/or sequential reads MappedByteBuffer
>>> is
>>> >> definitely faster, but the question here is about *concurrent*
>>> *random*
>>> >> reads of *large* files. For this purpose MappedByteBuffer may be
>>> >> inefficient.
>>> > Perhaps. It might be that manually managing memory (by reading and
>>> > writing the parts of the file you need) works better than letting the
>>> > kernel do it, but the kernel will cache as much as it can anyway, so
>>> > it's not as if it'll necessarily save memory or reduce disk activity.
>>> > There are advantages to using read() for sequential file access because
>>> > the kernel can automatically read and cache the next part of the file.
>>> >
>>> > There were some problems with inefficient code generation for byte
>>> > buffers but we have worked on that and it's better now, with (even)
>>> > more improvements to come. Unless there are kernel issues I don't know
>>> > about, mapped files are excellent for random access, and the Java
>>> > ByteBuffer operations generate excellent code. (This isn't guaranteed
>>> > because C2 uses a bunch of heuristics, but it's usually good.)
>>> >
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181018/dd264d8d/attachment.html>

From nigro.fra at gmail.com  Thu Oct 18 13:48:05 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Thu, 18 Oct 2018 19:48:05 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
 <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>
 <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
Message-ID: <CAKxGtTUyj+Pq2OdQxOXshs3rurx_8RW4CCTte+K_ezqh3q6+jw@mail.gmail.com>

It depends if has been triggered a global safepoints operation (or the the
JVM is triggering its usual safepoint at interval, 1 sec by default IIRC):
in that case the answer is yes, all the threads need to reach their
safepoint poll to be *in* the safepoint.

Il gio 18 ott 2018, 19:42 Alex Otenko <oleksandr.otenko at gmail.com> ha
scritto:

> This is strange. While the page is being loaded, the thread is not on the
> CPU. Does the GC wait for all the threads to get back on the CPU? Or is
> there some other reason that doesn't allow to find out whether the thread
> is on the CPU?
>
> Alex
> On 18 Oct 2018 18:38, "Nathan and Ila Reynolds" <nathanila at gmail.com>
> wrote:
>
>> When a thread reads a memory mapped area, it does a load instruction.  If
>> the page is not in RAM or the process's page table, then the thread will
>> exit user land and enter the kernel.  The thread could then load the page
>> from disk.  This could take a while especially if the disk is heavily
>> loaded.  Meanwhile, the JVM could be waiting for the thread to reach a
>> safepoint.  It won't be able to until the thread returns from loading the
>> page from disk.
>>
>> With regular file I/O, the thread can be blocked inside the kernel.  When
>> it returns from the kernel, it checks a flag and sees that it should
>> block.  Thus, the thread does not execute Java code during a stop-the-world
>> operation.  Hence, the JVM can assume the thread has reached a safepoint.
>>
>> -Nathan
>>
>> On 10/18/2018 11:06 AM, Alex Otenko wrote:
>>
>> No, it doesn't. Accessing memory mapped area is off heap, so what makes
>> it less safe than JNI?
>> On 18 Oct 2018 17:58, "Francesco Nigro via Concurrency-interest" <
>> concurrency-interest at cs.oswego.edu> wrote:
>>
>>> @nathan
>>> The point that Andrew raised is more related to safepoints: on
>>> MappedByteBuffer access you're in the JVM land and you're working between
>>> safepoints ie you got blocked by the kernel means that you won't arrive
>>> soon to the next safepoint, delaying the others that are waiting for you to
>>> join.
>>> With FileChannel you're in the JNI that is worlking *in* a safepoint and
>>> won't delay any java mutator threads to reach it (if needed). Makes sense?
>>>
>>> Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via
>>> Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:
>>>
>>>> When accessing a file through the kernel's file I/O, the thread context
>>>> switches from user land into kernel land.  The thread then goes to the
>>>> file cache to see if the data is there.  If not, the kernel blocks the
>>>> thread and pulls the data from disk.  Once the data is in the file
>>>> cache, the thread copies the data into the user land buffer and context
>>>> switches back to user land.
>>>>
>>>> When accessing a file through memory mapped I/O, the thread does a load
>>>> instruction against RAM.  If the data is not in RAM, then the thread
>>>> switches to the kernel, blocks while pulling data from disk and resumes
>>>> operation.
>>>>
>>>> File I/O and memory mapped I/O do the same operations but in a
>>>> different
>>>> order.  The difference is key.  With file I/O, the thread has to
>>>> context
>>>> switch into the kernel with every access. Thus, we use large buffers to
>>>> minimize the performance impact of the kernel round trip.  It is the
>>>> context switch with every operation that hurts file I/O and where
>>>> memory
>>>> mapped I/O shines. So, memory mapped I/O does well at concurrent random
>>>> reads of large files, but comes with an initialization cost and isn't
>>>> the best solution for all file access.  I have found that file I/O
>>>> considerably outperforms memory mapped I/O when sequentially reading
>>>> and
>>>> writing to a file unless you can map the entire file in one large piece.
>>>>
>>>> -Nathan
>>>>
>>>> On 10/18/2018 3:58 AM, Andrew Haley wrote:
>>>> > On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>>>> >> In case of a non-large file and/or sequential reads MappedByteBuffer
>>>> is
>>>> >> definitely faster, but the question here is about *concurrent*
>>>> *random*
>>>> >> reads of *large* files. For this purpose MappedByteBuffer may be
>>>> >> inefficient.
>>>> > Perhaps. It might be that manually managing memory (by reading and
>>>> > writing the parts of the file you need) works better than letting the
>>>> > kernel do it, but the kernel will cache as much as it can anyway, so
>>>> > it's not as if it'll necessarily save memory or reduce disk activity.
>>>> > There are advantages to using read() for sequential file access
>>>> because
>>>> > the kernel can automatically read and cache the next part of the file.
>>>> >
>>>> > There were some problems with inefficient code generation for byte
>>>> > buffers but we have worked on that and it's better now, with (even)
>>>> > more improvements to come. Unless there are kernel issues I don't know
>>>> > about, mapped files are excellent for random access, and the Java
>>>> > ByteBuffer operations generate excellent code. (This isn't guaranteed
>>>> > because C2 uses a bunch of heuristics, but it's usually good.)
>>>> >
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181018/27e08828/attachment-0001.html>

From aph at redhat.com  Thu Oct 18 16:09:35 2018
From: aph at redhat.com (Andrew Haley)
Date: Thu, 18 Oct 2018 21:09:35 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
 <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>
 <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
Message-ID: <04b1de05-c5e4-6b20-0151-2358ca2f2ae9@redhat.com>

On 10/18/2018 06:42 PM, Alex Otenko via Concurrency-interest wrote:
> This is strange. While the page is being loaded, the thread is not on the
> CPU. Does the GC wait for all the threads to get back on the CPU?

Yes, absolutely. It has to do that.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Thu Oct 18 16:15:45 2018
From: aph at redhat.com (Andrew Haley)
Date: Thu, 18 Oct 2018 21:15:45 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
 <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>
 <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
Message-ID: <00039e3a-f5a0-1b90-3d3f-348c78908447@redhat.com>

On 18 Oct 2018 18:38, "Nathan and Ila Reynolds" <nathanila at gmail.com> wrote:

> When a thread reads a memory mapped area, it does a load instruction.  If
> the page is not in RAM or the process's page table, then the thread will
> exit user land and enter the kernel.  The thread could then load the page
> from disk.  This could take a while especially if the disk is heavily
> loaded.  Meanwhile, the JVM could be waiting for the thread to reach a
> safepoint.  It won't be able to until the thread returns from loading the
> page from disk.
>
> With regular file I/O, the thread can be blocked inside the kernel.  When
> it returns from the kernel, it checks a flag and sees that it should
> block.  Thus, the thread does not execute Java code during a stop-the-world
> operation.  Hence, the JVM can assume the thread has reached a safepoint.

Not exactly. A native call *is* a safepoint: the GC can run even while a
native method is blocked in the kernel. The GC cannot run while we are
running Java code, even if that Java code is blocked by the kernel,
because we do not know when the kernel will restart that Java code. It
could be at any time. So we have to wait for the Java code to reach a
safepoint, and that means we have to wait for it to be unblocked by the
kernel.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From davidcholmes at aapt.net.au  Thu Oct 18 17:02:29 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 19 Oct 2018 07:02:29 +1000
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
 <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>
 <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
Message-ID: <025601d46725$e19d72b0$a4d85810$@aapt.net.au>

The VM doesn’t try to determine if a thread is on CPU and if it did it would do no good as it can’t know when it comes back on CPU. A thread executes in a number of possible safepoint related states, mainly: in_java, in_VM, in_native and blocked. The first two require that the VMThread wait until the thread notices the safepoint request and changes to a safe state – there is where delay can occur. The latter two states are safepoint-safe and a thread can’t transition out of those states if a safepoint is in progress. The “blocked” state is for synchronization operations and sleeps, not I/O, which is typically performed in_Native. There are times when a thread goes “native” without a state transition (e.g. fast JNI field accesses) so those operations have to be short to avoid safepoint delays.

 

Hotspot is moving away from global safepoints where possible to use thread-directed handshakes. New GC’s will take advantage of this but old ones will still utilize a global safepoint for at least part of the GC operation.

 

David

 

From: Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu> On Behalf Of Alex Otenko via Concurrency-interest
Sent: Friday, October 19, 2018 3:43 AM
To: Nathan and Ila Reynolds <nathanila at gmail.com>
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Overhead of ThreadLocal data

 

This is strange. While the page is being loaded, the thread is not on the CPU. Does the GC wait for all the threads to get back on the CPU? Or is there some other reason that doesn't allow to find out whether the thread is on the CPU?

Alex

On 18 Oct 2018 18:38, "Nathan and Ila Reynolds" <nathanila at gmail.com <mailto:nathanila at gmail.com> > wrote:

When a thread reads a memory mapped area, it does a load instruction.  If the page is not in RAM or the process's page table, then the thread will exit user land and enter the kernel.  The thread could then load the page from disk.  This could take a while especially if the disk is heavily loaded.  Meanwhile, the JVM could be waiting for the thread to reach a safepoint.  It won't be able to until the thread returns from loading the page from disk.

With regular file I/O, the thread can be blocked inside the kernel.  When it returns from the kernel, it checks a flag and sees that it should block.  Thus, the thread does not execute Java code during a stop-the-world operation.  Hence, the JVM can assume the thread has reached a safepoint.

-Nathan

On 10/18/2018 11:06 AM, Alex Otenko wrote:

No, it doesn't. Accessing memory mapped area is off heap, so what makes it less safe than JNI?

On 18 Oct 2018 17:58, "Francesco Nigro via Concurrency-interest" <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> > wrote:

@nathan  

The point that Andrew raised is more related to safepoints: on MappedByteBuffer access you're in the JVM land and you're working between safepoints ie you got blocked by the kernel means that you won't arrive soon to the next safepoint, delaying the others that are waiting for you to join.

With FileChannel you're in the JNI that is worlking *in* a safepoint and won't delay any java mutator threads to reach it (if needed). Makes sense? 

 

Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via Concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> > ha scritto:

When accessing a file through the kernel's file I/O, the thread context 
switches from user land into kernel land.  The thread then goes to the 
file cache to see if the data is there.  If not, the kernel blocks the 
thread and pulls the data from disk.  Once the data is in the file 
cache, the thread copies the data into the user land buffer and context 
switches back to user land.

When accessing a file through memory mapped I/O, the thread does a load 
instruction against RAM.  If the data is not in RAM, then the thread 
switches to the kernel, blocks while pulling data from disk and resumes 
operation.

File I/O and memory mapped I/O do the same operations but in a different 
order.  The difference is key.  With file I/O, the thread has to context 
switch into the kernel with every access. Thus, we use large buffers to 
minimize the performance impact of the kernel round trip.  It is the 
context switch with every operation that hurts file I/O and where memory 
mapped I/O shines. So, memory mapped I/O does well at concurrent random 
reads of large files, but comes with an initialization cost and isn't 
the best solution for all file access.  I have found that file I/O 
considerably outperforms memory mapped I/O when sequentially reading and 
writing to a file unless you can map the entire file in one large piece.

-Nathan

On 10/18/2018 3:58 AM, Andrew Haley wrote:
> On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>> In case of a non-large file and/or sequential reads MappedByteBuffer is
>> definitely faster, but the question here is about *concurrent* *random*
>> reads of *large* files. For this purpose MappedByteBuffer may be
>> inefficient.
> Perhaps. It might be that manually managing memory (by reading and
> writing the parts of the file you need) works better than letting the
> kernel do it, but the kernel will cache as much as it can anyway, so
> it's not as if it'll necessarily save memory or reduce disk activity.
> There are advantages to using read() for sequential file access because
> the kernel can automatically read and cache the next part of the file.
>
> There were some problems with inefficient code generation for byte
> buffers but we have worked on that and it's better now, with (even)
> more improvements to come. Unless there are kernel issues I don't know
> about, mapped files are excellent for random access, and the Java
> ByteBuffer operations generate excellent code. (This isn't guaranteed
> because C2 uses a bunch of heuristics, but it's usually good.)
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181019/5decb5b0/attachment.html>

From davidcholmes at aapt.net.au  Thu Oct 18 22:27:12 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 19 Oct 2018 12:27:12 +1000
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <025601d46725$e19d72b0$a4d85810$@aapt.net.au>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
 <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>
 <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
 <025601d46725$e19d72b0$a4d85810$@aapt.net.au>
Message-ID: <027e01d46753$3ec129a0$bc437ce0$@aapt.net.au>

Correction: new GCs will still have some global safepoints.

 

David

 

From: Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu> On Behalf Of David Holmes via Concurrency-interest
Sent: Friday, October 19, 2018 7:02 AM
To: 'Alex Otenko' <oleksandr.otenko at gmail.com>
Cc: David Holmes <davidcholmes at aapt.net.au>; 'concurrency-interest' <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Overhead of ThreadLocal data

 

The VM doesn’t try to determine if a thread is on CPU and if it did it would do no good as it can’t know when it comes back on CPU. A thread executes in a number of possible safepoint related states, mainly: in_java, in_VM, in_native and blocked. The first two require that the VMThread wait until the thread notices the safepoint request and changes to a safe state – there is where delay can occur. The latter two states are safepoint-safe and a thread can’t transition out of those states if a safepoint is in progress. The “blocked” state is for synchronization operations and sleeps, not I/O, which is typically performed in_Native. There are times when a thread goes “native” without a state transition (e.g. fast JNI field accesses) so those operations have to be short to avoid safepoint delays.

 

Hotspot is moving away from global safepoints where possible to use thread-directed handshakes. New GC’s will take advantage of this but old ones will still utilize a global safepoint for at least part of the GC operation.

 

David

 

From: Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu <mailto:concurrency-interest-bounces at cs.oswego.edu> > On Behalf Of Alex Otenko via Concurrency-interest
Sent: Friday, October 19, 2018 3:43 AM
To: Nathan and Ila Reynolds <nathanila at gmail.com <mailto:nathanila at gmail.com> >
Cc: concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> 
Subject: Re: [concurrency-interest] Overhead of ThreadLocal data

 

This is strange. While the page is being loaded, the thread is not on the CPU. Does the GC wait for all the threads to get back on the CPU? Or is there some other reason that doesn't allow to find out whether the thread is on the CPU?

Alex

On 18 Oct 2018 18:38, "Nathan and Ila Reynolds" <nathanila at gmail.com <mailto:nathanila at gmail.com> > wrote:

When a thread reads a memory mapped area, it does a load instruction.  If the page is not in RAM or the process's page table, then the thread will exit user land and enter the kernel.  The thread could then load the page from disk.  This could take a while especially if the disk is heavily loaded.  Meanwhile, the JVM could be waiting for the thread to reach a safepoint.  It won't be able to until the thread returns from loading the page from disk.

With regular file I/O, the thread can be blocked inside the kernel.  When it returns from the kernel, it checks a flag and sees that it should block.  Thus, the thread does not execute Java code during a stop-the-world operation.  Hence, the JVM can assume the thread has reached a safepoint.

-Nathan

On 10/18/2018 11:06 AM, Alex Otenko wrote:

No, it doesn't. Accessing memory mapped area is off heap, so what makes it less safe than JNI?

On 18 Oct 2018 17:58, "Francesco Nigro via Concurrency-interest" <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> > wrote:

@nathan  

The point that Andrew raised is more related to safepoints: on MappedByteBuffer access you're in the JVM land and you're working between safepoints ie you got blocked by the kernel means that you won't arrive soon to the next safepoint, delaying the others that are waiting for you to join.

With FileChannel you're in the JNI that is worlking *in* a safepoint and won't delay any java mutator threads to reach it (if needed). Makes sense? 

 

Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via Concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> > ha scritto:

When accessing a file through the kernel's file I/O, the thread context 
switches from user land into kernel land.  The thread then goes to the 
file cache to see if the data is there.  If not, the kernel blocks the 
thread and pulls the data from disk.  Once the data is in the file 
cache, the thread copies the data into the user land buffer and context 
switches back to user land.

When accessing a file through memory mapped I/O, the thread does a load 
instruction against RAM.  If the data is not in RAM, then the thread 
switches to the kernel, blocks while pulling data from disk and resumes 
operation.

File I/O and memory mapped I/O do the same operations but in a different 
order.  The difference is key.  With file I/O, the thread has to context 
switch into the kernel with every access. Thus, we use large buffers to 
minimize the performance impact of the kernel round trip.  It is the 
context switch with every operation that hurts file I/O and where memory 
mapped I/O shines. So, memory mapped I/O does well at concurrent random 
reads of large files, but comes with an initialization cost and isn't 
the best solution for all file access.  I have found that file I/O 
considerably outperforms memory mapped I/O when sequentially reading and 
writing to a file unless you can map the entire file in one large piece.

-Nathan

On 10/18/2018 3:58 AM, Andrew Haley wrote:
> On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
>> In case of a non-large file and/or sequential reads MappedByteBuffer is
>> definitely faster, but the question here is about *concurrent* *random*
>> reads of *large* files. For this purpose MappedByteBuffer may be
>> inefficient.
> Perhaps. It might be that manually managing memory (by reading and
> writing the parts of the file you need) works better than letting the
> kernel do it, but the kernel will cache as much as it can anyway, so
> it's not as if it'll necessarily save memory or reduce disk activity.
> There are advantages to using read() for sequential file access because
> the kernel can automatically read and cache the next part of the file.
>
> There were some problems with inefficient code generation for byte
> buffers but we have worked on that and it's better now, with (even)
> more improvements to come. Unless there are kernel issues I don't know
> about, mapped files are excellent for random access, and the Java
> ByteBuffer operations generate excellent code. (This isn't guaranteed
> because C2 uses a bunch of heuristics, but it's usually good.)
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181019/64cac4a2/attachment-0001.html>

From nigro.fra at gmail.com  Fri Oct 19 03:09:57 2018
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Fri, 19 Oct 2018 09:09:57 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <027e01d46753$3ec129a0$bc437ce0$@aapt.net.au>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
 <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>
 <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
 <025601d46725$e19d72b0$a4d85810$@aapt.net.au>
 <027e01d46753$3ec129a0$bc437ce0$@aapt.net.au>
Message-ID: <CAKxGtTXjkTHsPWT_24wbF8-iio8A0kJj7pAf=xLjX7qh=KMWuw@mail.gmail.com>

But at least now is possible to trigger "local safepoints" too ie
http://openjdk.java.net/jeps/312: other JVMs has this feature from a long
time, but I'm happy we're getting there :) eg Zing

Il ven 19 ott 2018, 05:53 David Holmes via Concurrency-interest <
concurrency-interest at cs.oswego.edu> ha scritto:

> Correction: new GCs will still have some global safepoints.
>
>
>
> David
>
>
>
> *From:* Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu> *On
> Behalf Of *David Holmes via Concurrency-interest
> *Sent:* Friday, October 19, 2018 7:02 AM
> *To:* 'Alex Otenko' <oleksandr.otenko at gmail.com>
> *Cc:* David Holmes <davidcholmes at aapt.net.au>; 'concurrency-interest' <
> concurrency-interest at cs.oswego.edu>
> *Subject:* Re: [concurrency-interest] Overhead of ThreadLocal data
>
>
>
> The VM doesn’t try to determine if a thread is on CPU and if it did it
> would do no good as it can’t know when it comes back on CPU. A thread
> executes in a number of possible safepoint related states, mainly: in_java,
> in_VM, in_native and blocked. The first two require that the VMThread wait
> until the thread notices the safepoint request and changes to a safe state
> – there is where delay can occur. The latter two states are safepoint-safe
> and a thread can’t transition out of those states if a safepoint is in
> progress. The “blocked” state is for synchronization operations and sleeps,
> not I/O, which is typically performed in_Native. There are times when a
> thread goes “native” without a state transition (e.g. fast JNI field
> accesses) so those operations have to be short to avoid safepoint delays.
>
>
>
> Hotspot is moving away from global safepoints where possible to use
> thread-directed handshakes. New GC’s will take advantage of this but old
> ones will still utilize a global safepoint for at least part of the GC
> operation.
>
>
>
> David
>
>
>
> *From:* Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu> *On
> Behalf Of *Alex Otenko via Concurrency-interest
> *Sent:* Friday, October 19, 2018 3:43 AM
> *To:* Nathan and Ila Reynolds <nathanila at gmail.com>
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Overhead of ThreadLocal data
>
>
>
> This is strange. While the page is being loaded, the thread is not on the
> CPU. Does the GC wait for all the threads to get back on the CPU? Or is
> there some other reason that doesn't allow to find out whether the thread
> is on the CPU?
>
> Alex
>
> On 18 Oct 2018 18:38, "Nathan and Ila Reynolds" <nathanila at gmail.com>
> wrote:
>
> When a thread reads a memory mapped area, it does a load instruction.  If
> the page is not in RAM or the process's page table, then the thread will
> exit user land and enter the kernel.  The thread could then load the page
> from disk.  This could take a while especially if the disk is heavily
> loaded.  Meanwhile, the JVM could be waiting for the thread to reach a
> safepoint.  It won't be able to until the thread returns from loading the
> page from disk.
>
> With regular file I/O, the thread can be blocked inside the kernel.  When
> it returns from the kernel, it checks a flag and sees that it should
> block.  Thus, the thread does not execute Java code during a stop-the-world
> operation.  Hence, the JVM can assume the thread has reached a safepoint.
>
> -Nathan
>
> On 10/18/2018 11:06 AM, Alex Otenko wrote:
>
> No, it doesn't. Accessing memory mapped area is off heap, so what makes it
> less safe than JNI?
>
> On 18 Oct 2018 17:58, "Francesco Nigro via Concurrency-interest" <
> concurrency-interest at cs.oswego.edu> wrote:
>
> @nathan
>
> The point that Andrew raised is more related to safepoints: on
> MappedByteBuffer access you're in the JVM land and you're working between
> safepoints ie you got blocked by the kernel means that you won't arrive
> soon to the next safepoint, delaying the others that are waiting for you to
> join.
>
> With FileChannel you're in the JNI that is worlking *in* a safepoint and
> won't delay any java mutator threads to reach it (if needed). Makes sense?
>
>
>
> Il giorno gio 18 ott 2018 alle ore 18:03 Nathan and Ila Reynolds via
> Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:
>
> When accessing a file through the kernel's file I/O, the thread context
> switches from user land into kernel land.  The thread then goes to the
> file cache to see if the data is there.  If not, the kernel blocks the
> thread and pulls the data from disk.  Once the data is in the file
> cache, the thread copies the data into the user land buffer and context
> switches back to user land.
>
> When accessing a file through memory mapped I/O, the thread does a load
> instruction against RAM.  If the data is not in RAM, then the thread
> switches to the kernel, blocks while pulling data from disk and resumes
> operation.
>
> File I/O and memory mapped I/O do the same operations but in a different
> order.  The difference is key.  With file I/O, the thread has to context
> switch into the kernel with every access. Thus, we use large buffers to
> minimize the performance impact of the kernel round trip.  It is the
> context switch with every operation that hurts file I/O and where memory
> mapped I/O shines. So, memory mapped I/O does well at concurrent random
> reads of large files, but comes with an initialization cost and isn't
> the best solution for all file access.  I have found that file I/O
> considerably outperforms memory mapped I/O when sequentially reading and
> writing to a file unless you can map the entire file in one large piece.
>
> -Nathan
>
> On 10/18/2018 3:58 AM, Andrew Haley wrote:
> > On 10/18/2018 10:29 AM, Andrey Pavlenko wrote:
> >> In case of a non-large file and/or sequential reads MappedByteBuffer is
> >> definitely faster, but the question here is about *concurrent* *random*
> >> reads of *large* files. For this purpose MappedByteBuffer may be
> >> inefficient.
> > Perhaps. It might be that manually managing memory (by reading and
> > writing the parts of the file you need) works better than letting the
> > kernel do it, but the kernel will cache as much as it can anyway, so
> > it's not as if it'll necessarily save memory or reduce disk activity.
> > There are advantages to using read() for sequential file access because
> > the kernel can automatically read and cache the next part of the file.
> >
> > There were some problems with inefficient code generation for byte
> > buffers but we have worked on that and it's better now, with (even)
> > more improvements to come. Unless there are kernel issues I don't know
> > about, mapped files are excellent for random access, and the Java
> > ByteBuffer operations generate excellent code. (This isn't guaranteed
> > because C2 uses a bunch of heuristics, but it's usually good.)
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181019/7afdc4ed/attachment.html>

From aph at redhat.com  Fri Oct 19 03:57:54 2018
From: aph at redhat.com (Andrew Haley)
Date: Fri, 19 Oct 2018 08:57:54 +0100
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <027e01d46753$3ec129a0$bc437ce0$@aapt.net.au>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <6d30a324-9b83-a7ce-17bc-1a987c907bdb@redhat.com>
 <46a68b16-2e2d-98d8-a203-e1f03e0fa585@gmail.com>
 <CAHSUVA7xmLO_Z8Y4ZiiWhkx=h5v2-w9sA81EXKtF6RcthyVeUA@mail.gmail.com>
 <5cb48096-f69c-3d59-0b59-7ad3ef2b64ed@redhat.com>
 <CAHSUVA6TZNZTqo+QX_ukS0h8_JbLEUe3p0k2U9WPGtw4yFCvGg@mail.gmail.com>
 <8631a964-1b23-120a-6a3a-69d12bdefcbb@redhat.com>
 <829c9ff9-ed7f-bd9a-9880-e23aae53e6e7@gmail.com>
 <CAKxGtTWBDd+QAEvwbDSORme9m=Nh=4BunYtjD05HN0gbtiYhuQ@mail.gmail.com>
 <CANkgWKgTUJYC4oppf-NhOiQe05+oKoCt5fQd6R4gwT-JdK_jdw@mail.gmail.com>
 <ec24e886-fdd6-1c89-919f-1ed404cc0e36@gmail.com>
 <CANkgWKjJ--YePP9dr6bFyf_Tgt7+=gWtY5ND_AimtkPA8-k0qg@mail.gmail.com>
 <025601d46725$e19d72b0$a4d85810$@aapt.net.au>
 <027e01d46753$3ec129a0$bc437ce0$@aapt.net.au>
Message-ID: <6a05cedb-8230-54d9-5776-dfab0f2ec250@redhat.com>

On 10/19/2018 03:27 AM, David Holmes via Concurrency-interest wrote:
> Correction: new GCs will still have some global safepoints.

And it's really important to remember that it's not just about the GCs: there is much
shared metadata that has to be scanned at safepoints.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From peter.levart at gmail.com  Fri Oct 19 06:12:02 2018
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 19 Oct 2018 12:12:02 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
Message-ID: <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>

It is interesting that people have problems with short-lived 
ThreadLocals because ThreadLocal tries hard to expunge stale entries 
gradually during use. Even get()  tries to do it if the access happens 
to miss the home slot. Repeated access to the same entry should not have 
to skip stale entries over and over again. There seems to be special use 
pattern(s) that provoke problems. Studying them more deeply could show 
us the weakness of current ThreadLocal auto-expunge strategy so it could 
be improved and new method would not be needed...

Maybe the problem is not caused by unexpunged stale entries. It may be 
related to degeneration of hashtable that leads to sub-optimal placement 
of live entries. In that case, perhaps triggering a re-hash 
automatically when get() encounters many live entries it has to skip 
would help.

So those with problems, please be more specific about them. Can you show 
us a reproducer?

Regards, Peter

On 10/17/2018 08:26 PM, Doug Lea via Concurrency-interest wrote:
> [+list]
>
> On 10/17/18 11:44 AM, Nathan and Ila Reynolds wrote:
>> Can we add the following method to ThreadLocal?
>>
>> public static void expungeStaleEntries()
> This seems like a reasonable request (although perhaps with an improved
> name).  The functionality exists internally, and it seems overly
> parental not to export it for use as a band-aid by those people who have
> tried and otherwise failed to solve the zillions of short-lived
> ThreadLocals in long-lived threads problem.
>
> Can anyone think of a reason not to do this?
>
> -Doug
>
>> This method will call ThreadLocal.ThreadLocalMap.expungeStaleEntries()
>> for the ThreadLocalMap of the current thread.  Thread pools can then
>> call this method when the thread finishes processing a job after GC.
>> This solves the problem of zillions of short-lived ThreadLocals in
>> long-lived threads.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181019/b6222882/attachment-0001.html>

From brian at briangoetz.com  Fri Oct 19 09:06:07 2018
From: brian at briangoetz.com (Brian Goetz)
Date: Fri, 19 Oct 2018 09:06:07 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
Message-ID: <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>

The problem is deeper than that; people using TL because it's what we've 
got, when really what people want (in various situations) is { 
processor, frame, task } locals, and TL is the best approximation we 
have.  Over in Project Loom, there's a deeper exploration going on of 
the use cases, and what additional mechanisms might help.

On 10/19/2018 6:12 AM, Peter Levart via Concurrency-interest wrote:
> It is interesting that people have problems with short-lived 
> ThreadLocals because ThreadLocal tries hard to expunge stale entries 
> gradually during use. Even get()  tries to do it if the access happens 
> to miss the home slot. Repeated access to the same entry should not have 
> to skip stale entries over and over again. There seems to be special use 
> pattern(s) that provoke problems. Studying them more deeply could show 
> us the weakness of current ThreadLocal auto-expunge strategy so it could 
> be improved and new method would not be needed...
> 
> Maybe the problem is not caused by unexpunged stale entries. It may be 
> related to degeneration of hashtable that leads to sub-optimal placement 
> of live entries. In that case, perhaps triggering a re-hash 
> automatically when get() encounters many live entries it has to skip 
> would help.
> 
> So those with problems, please be more specific about them. Can you show 
> us a reproducer?
> 
> Regards, Peter
> 
> On 10/17/2018 08:26 PM, Doug Lea via Concurrency-interest wrote:
>> [+list]
>>
>> On 10/17/18 11:44 AM, Nathan and Ila Reynolds wrote:
>>> Can we add the following method to ThreadLocal?
>>>
>>> public static void expungeStaleEntries()
>> This seems like a reasonable request (although perhaps with an improved
>> name).  The functionality exists internally, and it seems overly
>> parental not to export it for use as a band-aid by those people who have
>> tried and otherwise failed to solve the zillions of short-lived
>> ThreadLocals in long-lived threads problem.
>>
>> Can anyone think of a reason not to do this?
>>
>> -Doug
>>
>>> This method will call ThreadLocal.ThreadLocalMap.expungeStaleEntries()
>>> for the ThreadLocalMap of the current thread.  Thread pools can then
>>> call this method when the thread finishes processing a job after GC.
>>> This solves the problem of zillions of short-lived ThreadLocals in
>>> long-lived threads.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From nathanila at gmail.com  Fri Oct 19 09:43:28 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Fri, 19 Oct 2018 07:43:28 -0600
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
 <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>
Message-ID: <71395ea9-79fa-cf83-174f-3bdc99af834d@gmail.com>

A while ago, I raised the request for a processor local. Striping a 
highly contended cache line, reduces contention but the cache line still 
bounces around cores.  Using processor local, each cache line can be 
assigned to a core and it remains in the exclusive state for that core.  
Atomic operations on that cache line are the fastest possible.

My original example was a reader-writer lock which is almost exclusively 
used for read operations.  However, with processor local, we can 
implement uncontended counters for heavily used queues.

-Nathan

On 10/19/2018 7:06 AM, Brian Goetz wrote:
> The problem is deeper than that; people using TL because it's what 
> we've got, when really what people want (in various situations) is { 
> processor, frame, task } locals, and TL is the best approximation we 
> have.  Over in Project Loom, there's a deeper exploration going on of 
> the use cases, and what additional mechanisms might help.
>
> On 10/19/2018 6:12 AM, Peter Levart via Concurrency-interest wrote:
>> It is interesting that people have problems with short-lived 
>> ThreadLocals because ThreadLocal tries hard to expunge stale entries 
>> gradually during use. Even get() tries to do it if the access happens 
>> to miss the home slot. Repeated access to the same entry should not 
>> have to skip stale entries over and over again. There seems to be 
>> special use pattern(s) that provoke problems. Studying them more 
>> deeply could show us the weakness of current ThreadLocal auto-expunge 
>> strategy so it could be improved and new method would not be needed...
>>
>> Maybe the problem is not caused by unexpunged stale entries. It may 
>> be related to degeneration of hashtable that leads to sub-optimal 
>> placement of live entries. In that case, perhaps triggering a re-hash 
>> automatically when get() encounters many live entries it has to skip 
>> would help.
>>
>> So those with problems, please be more specific about them. Can you 
>> show us a reproducer?
>>
>> Regards, Peter
>>
>> On 10/17/2018 08:26 PM, Doug Lea via Concurrency-interest wrote:
>>> [+list]
>>>
>>> On 10/17/18 11:44 AM, Nathan and Ila Reynolds wrote:
>>>> Can we add the following method to ThreadLocal?
>>>>
>>>> public static void expungeStaleEntries()
>>> This seems like a reasonable request (although perhaps with an improved
>>> name).  The functionality exists internally, and it seems overly
>>> parental not to export it for use as a band-aid by those people who 
>>> have
>>> tried and otherwise failed to solve the zillions of short-lived
>>> ThreadLocals in long-lived threads problem.
>>>
>>> Can anyone think of a reason not to do this?
>>>
>>> -Doug
>>>
>>>> This method will call ThreadLocal.ThreadLocalMap.expungeStaleEntries()
>>>> for the ThreadLocalMap of the current thread.  Thread pools can then
>>>> call this method when the thread finishes processing a job after GC.
>>>> This solves the problem of zillions of short-lived ThreadLocals in
>>>> long-lived threads.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>

From gil at azul.com  Fri Oct 19 11:09:20 2018
From: gil at azul.com (Gil Tene)
Date: Fri, 19 Oct 2018 15:09:20 +0000
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <71395ea9-79fa-cf83-174f-3bdc99af834d@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
 <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>,
 <71395ea9-79fa-cf83-174f-3bdc99af834d@gmail.com>
Message-ID: <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>

The problem with “short lived” thread locals that do not explicitly use remove() when they are no longer needed, is that in those situations “short lived” means “until the next GC detects that the ThreadLocal is no longer reachable, and the weak reference to that ThreadLocal starts returning nulls”. Scanning for stale entries more frequently, or on an API call, won’t help these situations because the map entries are not “stale” until the collector determines the unreachability of their related ThreadLocal instances. When short lived ThreadLocals are created and die at a rate linear to application throughout, this leads to ThreadLocal maps in active threads holding thousands or millions of entries, to extremely long collision chains, and to situations where half the CPU is spent walking those chains in ThreadLocal.get().

Where would such a ThreadLocal instance creation rate come from? We’ve seen it happen in the wild in quite a few places. I initially thought of it as an application or library-level misuse of a ThreadLocal, but with each instance of juc ReentrantReadWriteLock potentially using a ThreadLocal instance for internal coordination, application and library writers that use ReentrantReadWriteLock in a seemingly idiomatic way are often unaware of the ThreadLocal implications. A simple, seemingly valid pattern for using ReentrantReadWriteLock would be a system where arriving work “packets” are handled by parallel threads (doing parallel work within the work packet), where each work packet uses its own of ReentrantReadWriteLock instance for coordination of the parallelism within the packet.

With G1 (and C4, and likely Shenandoah and ZGC), with the current ThreadLocal implementation, the “means “until the next GC detects that the ThreadLocal is no longer reachable” meaning gets compounded by the fact that collisions in the map will actually prevent otherwise-unreachable ThreadLocal instances from becoming unreachable. As long as get() calls for other (chain colliding) ThreadLocals are actively performed on any thread, where “actively” means “more than once during a mark cycle”, the weakrefs from the colliding entries get strengthened during the mark, preventing the colliding ThreadLocal instances from dying in the given GC cycle.. Stop-the-world newgen provides a bit of a filter for short-lived-enough ThreadLocals for G1, but for ThreadLocals with mid-term lifecycles (which will naturally occur as queues in a work system as described above grow under load), or without such a STW newgen (which none of the newer collectors have or want to have have), this situation leads to explosive ThreadLocal map growth under high-enough throughout situations.

We’ve created a tweaked implementation of ThreadLocal that avoids the weakref get()-strengthening problem with no semantic change, by having ThreadLocal get() determine identity by comparing a ThreadLocal ID (a unique long value per ThreadLocal instance, assigned at ThreadLocal instantiation) rather than comparing the outcome of a weakref get() for each entry in the chain walked when looking for a match. This makes weakref get()s only occur on the actual entry you are looking up, and not on any colliding entries, this preventing the gets from keeping dead ThreadLocals alive. We’ve been using this implementation successfully with C4 in Zing, and would be happy to share and upstream if there is interest.

Sent from my iPad

> On Oct 19, 2018, at 6:45 AM, Nathan and Ila Reynolds via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> A while ago, I raised the request for a processor local. Striping a highly contended cache line, reduces contention but the cache line still bounces around cores.  Using processor local, each cache line can be assigned to a core and it remains in the exclusive state for that core.  Atomic operations on that cache line are the fastest possible.
> 
> My original example was a reader-writer lock which is almost exclusively used for read operations.  However, with processor local, we can implement uncontended counters for heavily used queues.
> 
> -Nathan
> 
>> On 10/19/2018 7:06 AM, Brian Goetz wrote:
>> The problem is deeper than that; people using TL because it's what we've got, when really what people want (in various situations) is { processor, frame, task } locals, and TL is the best approximation we have.  Over in Project Loom, there's a deeper exploration going on of the use cases, and what additional mechanisms might help.
>> 
>>> On 10/19/2018 6:12 AM, Peter Levart via Concurrency-interest wrote:
>>> It is interesting that people have problems with short-lived ThreadLocals because ThreadLocal tries hard to expunge stale entries gradually during use. Even get() tries to do it if the access happens to miss the home slot. Repeated access to the same entry should not have to skip stale entries over and over again. There seems to be special use pattern(s) that provoke problems. Studying them more deeply could show us the weakness of current ThreadLocal auto-expunge strategy so it could be improved and new method would not be needed...
>>> 
>>> Maybe the problem is not caused by unexpunged stale entries. It may be related to degeneration of hashtable that leads to sub-optimal placement of live entries. In that case, perhaps triggering a re-hash automatically when get() encounters many live entries it has to skip would help.
>>> 
>>> So those with problems, please be more specific about them. Can you show us a reproducer?
>>> 
>>> Regards, Peter
>>> 
>>>> On 10/17/2018 08:26 PM, Doug Lea via Concurrency-interest wrote:
>>>> [+list]
>>>> 
>>>>> On 10/17/18 11:44 AM, Nathan and Ila Reynolds wrote:
>>>>> Can we add the following method to ThreadLocal?
>>>>> 
>>>>> public static void expungeStaleEntries()
>>>> This seems like a reasonable request (although perhaps with an improved
>>>> name).  The functionality exists internally, and it seems overly
>>>> parental not to export it for use as a band-aid by those people who have
>>>> tried and otherwise failed to solve the zillions of short-lived
>>>> ThreadLocals in long-lived threads problem.
>>>> 
>>>> Can anyone think of a reason not to do this?
>>>> 
>>>> -Doug
>>>> 
>>>>> This method will call ThreadLocal.ThreadLocalMap.expungeStaleEntries()
>>>>> for the ThreadLocalMap of the current thread.  Thread pools can then
>>>>> call this method when the thread finishes processing a job after GC.
>>>>> This solves the problem of zillions of short-lived ThreadLocals in
>>>>> long-lived threads.
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From peter.levart at gmail.com  Fri Oct 19 12:06:10 2018
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 19 Oct 2018 18:06:10 +0200
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
 <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>
 <71395ea9-79fa-cf83-174f-3bdc99af834d@gmail.com>
 <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>
Message-ID: <1e8112bd-b422-dc36-d628-29fee144b0f8@gmail.com>

Hi Gil,

On 10/19/2018 05:09 PM, Gil Tene wrote:
> We’ve created a tweaked implementation of ThreadLocal that avoids the weakref get()-strengthening problem with no semantic change, by having ThreadLocal get() determine identity by comparing a ThreadLocal ID (a unique long value per ThreadLocal instance, assigned at ThreadLocal instantiation) rather than comparing the outcome of a weakref get() for each entry in the chain walked when looking for a match. This makes weakref get()s only occur on the actual entry you are looking up, and not on any colliding entries, this preventing the gets from keeping dead ThreadLocals alive. We’ve been using this implementation successfully with C4 in Zing, and would be happy to share and upstream if there is interest.

Very interesting. One question though. How does your implementation do 
expunging of stale entries? Just when there is a re-hash needed because 
of capacity growing over threshold? I ask because in order to detect 
that and entry is stale, you have to call get(). The only other variant 
is to have the weakrefs enqueued and then poll the queue for them. But 
that already interacts with a reference handling thread and I had the 
impression that ThreadLocal is designed to have no synchronization 
whatsoever.

Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181019/21a735fa/attachment.html>

From gil at azul.com  Fri Oct 19 12:45:58 2018
From: gil at azul.com (Gil Tene)
Date: Fri, 19 Oct 2018 16:45:58 +0000
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <1e8112bd-b422-dc36-d628-29fee144b0f8@gmail.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
 <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>
 <71395ea9-79fa-cf83-174f-3bdc99af834d@gmail.com>
 <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>,
 <1e8112bd-b422-dc36-d628-29fee144b0f8@gmail.com>
Message-ID: <E7A87FB7-AF9C-4A48-AD2A-54A97AC0D780@azul.com>



Sent from Gil's iPhone

> On Oct 19, 2018, at 9:06 AM, Peter Levart <peter.levart at gmail.com> wrote:
> 
> Hi Gil,
> 
>> On 10/19/2018 05:09 PM, Gil Tene wrote:
>> We’ve created a tweaked implementation of ThreadLocal that avoids the weakref get()-strengthening problem with no semantic change, by having ThreadLocal get() determine identity by comparing a ThreadLocal ID (a unique long value per ThreadLocal instance, assigned at ThreadLocal instantiation) rather than comparing the outcome of a weakref get() for each entry in the chain walked when looking for a match. This makes weakref get()s only occur on the actual entry you are looking up, and not on any colliding entries, this preventing the gets from keeping dead ThreadLocals alive. We’ve been using this implementation successfully with C4 in Zing, and would be happy to share and upstream if there is interest.
> 
> Very interesting. One question though. How does your implementation do expunging of stale entries? Just when there is a re-hash needed because of capacity growing over threshold? I ask because in order to detect that and entry is stale, you have to call get(). The only other variant is to have the weakrefs enqueued and then poll the queue for them. But that already interacts with a reference handling thread and I had the impression that ThreadLocal is designed to have no synchronization whatsoever.

We use a reference queue, and poll on it to detect and deal with entries that (weakly) refereed to dead ThreadLocals.

> 
> Regards, Peter
> 

From shevek at anarres.org  Fri Oct 19 13:45:42 2018
From: shevek at anarres.org (Shevek)
Date: Fri, 19 Oct 2018 10:45:42 -0700
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
 <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>
 <71395ea9-79fa-cf83-174f-3bdc99af834d@gmail.com>
 <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>
Message-ID: <0264213e-314f-5d4b-5b9c-30bad89d1605@anarres.org>

This sounds VERY like the behaviour we observed - the WeakReferences not 
getting cleared would make total sense for causing the linear scans.

On 10/19/18 8:09 AM, Gil Tene via Concurrency-interest wrote:
> The problem with “short lived” thread locals that do not explicitly use remove() when they are no longer needed, is that in those situations “short lived” means “until the next GC detects that the ThreadLocal is no longer reachable, and the weak reference to that ThreadLocal starts returning nulls”. Scanning for stale entries more frequently, or on an API call, won’t help these situations because the map entries are not “stale” until the collector determines the unreachability of their related ThreadLocal instances. When short lived ThreadLocals are created and die at a rate linear to application throughout, this leads to ThreadLocal maps in active threads holding thousands or millions of entries, to extremely long collision chains, and to situations where half the CPU is spent walking those chains in ThreadLocal.get().
> 
> Where would such a ThreadLocal instance creation rate come from? We’ve seen it happen in the wild in quite a few places. I initially thought of it as an application or library-level misuse of a ThreadLocal, but with each instance of juc ReentrantReadWriteLock potentially using a ThreadLocal instance for internal coordination, application and library writers that use ReentrantReadWriteLock in a seemingly idiomatic way are often unaware of the ThreadLocal implications. A simple, seemingly valid pattern for using ReentrantReadWriteLock would be a system where arriving work “packets” are handled by parallel threads (doing parallel work within the work packet), where each work packet uses its own of ReentrantReadWriteLock instance for coordination of the parallelism within the packet.
> 
> With G1 (and C4, and likely Shenandoah and ZGC), with the current ThreadLocal implementation, the “means “until the next GC detects that the ThreadLocal is no longer reachable” meaning gets compounded by the fact that collisions in the map will actually prevent otherwise-unreachable ThreadLocal instances from becoming unreachable. As long as get() calls for other (chain colliding) ThreadLocals are actively performed on any thread, where “actively” means “more than once during a mark cycle”, the weakrefs from the colliding entries get strengthened during the mark, preventing the colliding ThreadLocal instances from dying in the given GC cycle.. Stop-the-world newgen provides a bit of a filter for short-lived-enough ThreadLocals for G1, but for ThreadLocals with mid-term lifecycles (which will naturally occur as queues in a work system as described above grow under load), or without such a STW newgen (which none of the newer collectors have or want to have have), this situation leads to explosive ThreadLocal map growth under high-enough throughout situations.
> 
> We’ve created a tweaked implementation of ThreadLocal that avoids the weakref get()-strengthening problem with no semantic change, by having ThreadLocal get() determine identity by comparing a ThreadLocal ID (a unique long value per ThreadLocal instance, assigned at ThreadLocal instantiation) rather than comparing the outcome of a weakref get() for each entry in the chain walked when looking for a match. This makes weakref get()s only occur on the actual entry you are looking up, and not on any colliding entries, this preventing the gets from keeping dead ThreadLocals alive. We’ve been using this implementation successfully with C4 in Zing, and would be happy to share and upstream if there is interest.
> 
> Sent from my iPad
> 
>> On Oct 19, 2018, at 6:45 AM, Nathan and Ila Reynolds via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>>
>> A while ago, I raised the request for a processor local. Striping a highly contended cache line, reduces contention but the cache line still bounces around cores.  Using processor local, each cache line can be assigned to a core and it remains in the exclusive state for that core.  Atomic operations on that cache line are the fastest possible.
>>
>> My original example was a reader-writer lock which is almost exclusively used for read operations.  However, with processor local, we can implement uncontended counters for heavily used queues.
>>
>> -Nathan
>>
>>> On 10/19/2018 7:06 AM, Brian Goetz wrote:
>>> The problem is deeper than that; people using TL because it's what we've got, when really what people want (in various situations) is { processor, frame, task } locals, and TL is the best approximation we have.  Over in Project Loom, there's a deeper exploration going on of the use cases, and what additional mechanisms might help.
>>>
>>>> On 10/19/2018 6:12 AM, Peter Levart via Concurrency-interest wrote:
>>>> It is interesting that people have problems with short-lived ThreadLocals because ThreadLocal tries hard to expunge stale entries gradually during use. Even get() tries to do it if the access happens to miss the home slot. Repeated access to the same entry should not have to skip stale entries over and over again. There seems to be special use pattern(s) that provoke problems. Studying them more deeply could show us the weakness of current ThreadLocal auto-expunge strategy so it could be improved and new method would not be needed...
>>>>
>>>> Maybe the problem is not caused by unexpunged stale entries. It may be related to degeneration of hashtable that leads to sub-optimal placement of live entries. In that case, perhaps triggering a re-hash automatically when get() encounters many live entries it has to skip would help.
>>>>
>>>> So those with problems, please be more specific about them. Can you show us a reproducer?
>>>>
>>>> Regards, Peter
>>>>
>>>>> On 10/17/2018 08:26 PM, Doug Lea via Concurrency-interest wrote:
>>>>> [+list]
>>>>>
>>>>>> On 10/17/18 11:44 AM, Nathan and Ila Reynolds wrote:
>>>>>> Can we add the following method to ThreadLocal?
>>>>>>
>>>>>> public static void expungeStaleEntries()
>>>>> This seems like a reasonable request (although perhaps with an improved
>>>>> name).  The functionality exists internally, and it seems overly
>>>>> parental not to export it for use as a band-aid by those people who have
>>>>> tried and otherwise failed to solve the zillions of short-lived
>>>>> ThreadLocals in long-lived threads problem.
>>>>>
>>>>> Can anyone think of a reason not to do this?
>>>>>
>>>>> -Doug
>>>>>
>>>>>> This method will call ThreadLocal.ThreadLocalMap.expungeStaleEntries()
>>>>>> for the ThreadLocalMap of the current thread.  Thread pools can then
>>>>>> call this method when the thread finishes processing a job after GC.
>>>>>> This solves the problem of zillions of short-lived ThreadLocals in
>>>>>> long-lived threads.
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From gil at azul.com  Fri Oct 19 14:11:22 2018
From: gil at azul.com (Gil Tene)
Date: Fri, 19 Oct 2018 18:11:22 +0000
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <E7A87FB7-AF9C-4A48-AD2A-54A97AC0D780@azul.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
 <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>
 <71395ea9-79fa-cf83-174f-3bdc99af834d@gmail.com>
 <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>
 <1e8112bd-b422-dc36-d628-29fee144b0f8@gmail.com>
 <E7A87FB7-AF9C-4A48-AD2A-54A97AC0D780@azul.com>
Message-ID: <A7020C46-4318-4600-A78C-2B06E778FCA4@azul.com>

I've posted a full OpenJDK8-based ThreadLocal.java implementation for reference here: https://github.com/giltene/GilExamples/blob/master/ModifiedThreadLocal/src/main/java/java/lang/ThreadLocal.java <https://github.com/giltene/GilExamples/blob/master/ModifiedThreadLocal/src/main/java/java/lang/ThreadLocal.java> , and as noted, we'd be happy to upstream it (into 12?).

> On Oct 19, 2018, at 9:45 AM, Gil Tene via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> 
> 
> Sent from Gil's iPhone
> 
>> On Oct 19, 2018, at 9:06 AM, Peter Levart <peter.levart at gmail.com> wrote:
>> 
>> Hi Gil,
>> 
>>> On 10/19/2018 05:09 PM, Gil Tene wrote:
>>> We’ve created a tweaked implementation of ThreadLocal that avoids the weakref get()-strengthening problem with no semantic change, by having ThreadLocal get() determine identity by comparing a ThreadLocal ID (a unique long value per ThreadLocal instance, assigned at ThreadLocal instantiation) rather than comparing the outcome of a weakref get() for each entry in the chain walked when looking for a match. This makes weakref get()s only occur on the actual entry you are looking up, and not on any colliding entries, this preventing the gets from keeping dead ThreadLocals alive. We’ve been using this implementation successfully with C4 in Zing, and would be happy to share and upstream if there is interest.
>> 
>> Very interesting. One question though. How does your implementation do expunging of stale entries? Just when there is a re-hash needed because of capacity growing over threshold? I ask because in order to detect that and entry is stale, you have to call get(). The only other variant is to have the weakrefs enqueued and then poll the queue for them. But that already interacts with a reference handling thread and I had the impression that ThreadLocal is designed to have no synchronization whatsoever.
> 
> We use a reference queue, and poll on it to detect and deal with entries that (weakly) refereed to dead ThreadLocals.
> 
>> 
>> Regards, Peter
>> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181019/cc32382a/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: Message signed with OpenPGP
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181019/cc32382a/attachment-0001.sig>

From dl at cs.oswego.edu  Sat Oct 20 07:44:34 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 20 Oct 2018 07:44:34 -0400
Subject: [concurrency-interest] Overhead of ThreadLocal data
In-Reply-To: <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>
References: <0e99cbcb-80a4-2dde-b490-bfb7044de008@redhat.com>
 <CANghgrQsPJ00ZVOE6vcP+LCRjVd4kaXHg-MUGwPLk7pHZGx4Lg@mail.gmail.com>
 <467f85c8-1258-d442-b3e1-749a833ec86c@redhat.com>
 <95014194-f725-1c26-d5c9-c50cb21fc479@cs.oswego.edu>
 <185bd310-033c-36cb-8df8-14cec82c57fc@gmail.com>
 <9eb9e54a-cddd-a62b-6d18-5441b975d2c0@cs.oswego.edu>
 <799046bd-9362-39a8-add5-c45e9ffb0c46@gmail.com>
 <26b262bb-dc2b-8b07-305d-7297c5219da1@briangoetz.com>
 <71395ea9-79fa-cf83-174f-3bdc99af834d@gmail.com>
 <011A9D8C-55F6-4DB7-8E98-51894BCBB8E3@azul.com>
Message-ID: <175ec6e9-73b9-3a39-359c-6a2197e873e3@cs.oswego.edu>

On 10/19/18 11:09 AM, Gil Tene wrote:

> 
> With G1 (and C4, and likely Shenandoah and ZGC), with the current
> ThreadLocal implementation, the “means “until the next GC detects
> that the ThreadLocal is no longer reachable” meaning gets compounded
> by the fact that collisions in the map will actually prevent
> otherwise-unreachable ThreadLocal instances from becoming
> unreachable. 

Thanks for pointing this out. Yes, this should be addressed (the
implementation long predated such collectors). There's a simpler way
than you showed though. We can extend the Fibonacci hash to "long" to
avoid ever wrapping, define a tombstone value, and use as id to avoid
WeakRef.get calls on collision. The reduces a bit of the time/space
overhead that this scheme imposes.

I'm also a little suspicious about whether using a reference queue is
worth the overhead. Because GCs clear weak refs in bunches, cleaning a
few usually usually leads to finding more without need to poll. Any data
showing otherwise would be helpful.

I'm guessing that Peter Levart has had some similar thoughts; we'll work
off-list to see where these go.

-Doug



From sudhakarg79spam at gmail.com  Mon Oct 22 23:21:36 2018
From: sudhakarg79spam at gmail.com (Sudhakar Govindavajhala)
Date: Mon, 22 Oct 2018 23:21:36 -0400
Subject: [concurrency-interest] Happens before and stampedlocks
Message-ID: <CAGdJZsDW2v3mEn5G0UttPv9+q2TbRQWpuU4jicv1xY2yX3sjPA@mail.gmail.com>

Dear sirs and madams


Please accept my humble respects.


Please let me know if stampedlock class guarantees any happens before
relationship.   Yes or no.    Kindly provide some reading resources on this
subject

Sincerely
Sudhakar Krsna
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181022/0d7d662f/attachment.html>

From valentin.male.kovalenko at gmail.com  Tue Oct 23 12:24:22 2018
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Tue, 23 Oct 2018 10:24:22 -0600
Subject: [concurrency-interest] Happens before and stampedlocks
In-Reply-To: <mailman.1.1540310401.21050.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1540310401.21050.concurrency-interest@cs.oswego.edu>
Message-ID: <CAO-wXw+MEhG_m=xq-Qvw=DmtqJOSknmpaT9KH060q-7damg_oA@mail.gmail.com>

 Hi Sudhakar,

I understand that this question is specifically about the optimistic read
(because HB for read/write stamped lock is obvious).

Optimistic read does provide a HB between the previous (in synchronization
order) release of a write lock and a successful optimistic read. This is
also obvious, btw, because optimistic read is essentially a volatile read,
and release of the write lock has a volatile write to the same field.

The problem with optimistic reads is that neither tryOptimisticRead nor
validate methods have a volatile write, thus allowing data races
with subsequent (again, in synchronization order) acquire actions of the
write lock (and as a result with writes happening under those write locks).
This is the reason why we can only safely read data before (in program
order) calling validate method, and not after it. OpenJDK 11 solves this
problem (removes such data races for reads inside
tryOptimisticRead/validate block) by introducing VarHandle.acquireFence()
into validate method before actually validating the stamp.

A more detailed and very good explanation is here:
"Can Seqlocks Get Along With Programming Language Memory Models?"
http://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf

Regards,
Valentin
[image: LinkedIn] <https://www.linkedin.com/in/stIncMale>   [image: GitHub]
<https://github.com/stIncMale>   [image: YouTube]
<https://www.youtube.com/user/stIncMale>


On Tue, 23 Oct 2018 at 10:05, <concurrency-interest-request at cs.oswego.edu>
wrote:

> Send Concurrency-interest mailing list submissions to
>         concurrency-interest at cs.oswego.edu
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>         concurrency-interest-request at cs.oswego.edu
>
> You can reach the person managing the list at
>         concurrency-interest-owner at cs.oswego.edu
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>    1. Happens before and stampedlocks (Sudhakar Govindavajhala)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 22 Oct 2018 23:21:36 -0400
> From: Sudhakar Govindavajhala <sudhakarg79spam at gmail.com>
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Happens before and stampedlocks
> Message-ID:
>         <
> CAGdJZsDW2v3mEn5G0UttPv9+q2TbRQWpuU4jicv1xY2yX3sjPA at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Dear sirs and madams
>
>
> Please accept my humble respects.
>
>
> Please let me know if stampedlock class guarantees any happens before
> relationship.   Yes or no.    Kindly provide some reading resources on this
> subject
>
> Sincerely
> Sudhakar Krsna
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181022/0d7d662f/attachment-0001.html
> >
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> ------------------------------
>
> End of Concurrency-interest Digest, Vol 164, Issue 28
> *****************************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181023/fbaeb0ab/attachment.html>

From martinrb at google.com  Tue Oct 23 13:13:09 2018
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 23 Oct 2018 10:13:09 -0700
Subject: [concurrency-interest] Happens before and stampedlocks
In-Reply-To: <CAO-wXw+MEhG_m=xq-Qvw=DmtqJOSknmpaT9KH060q-7damg_oA@mail.gmail.com>
References: <mailman.1.1540310401.21050.concurrency-interest@cs.oswego.edu>
 <CAO-wXw+MEhG_m=xq-Qvw=DmtqJOSknmpaT9KH060q-7damg_oA@mail.gmail.com>
Message-ID: <CA+kOe0-L+VPgA+0V2etC5DcjOk1nP=WLEN+Jm2fk_kF+hyu1DA@mail.gmail.com>

We "should" have more memory model level docs in StampedLock.
We "should" say that only those who have completely understood
"Can Seqlocks Get Along With Programming Language Memory Models?"
are qualified to use optimistic reads.

But we're likely to run into human deadlocks trying to come up with wording.

On Tue, Oct 23, 2018 at 9:24 AM, Valentin Kovalenko via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:

>  Hi Sudhakar,
>
> I understand that this question is specifically about the optimistic read
> (because HB for read/write stamped lock is obvious).
>
> Optimistic read does provide a HB between the previous (in synchronization
> order) release of a write lock and a successful optimistic read. This is
> also obvious, btw, because optimistic read is essentially a volatile read,
> and release of the write lock has a volatile write to the same field.
>
> The problem with optimistic reads is that neither tryOptimisticRead nor
> validate methods have a volatile write, thus allowing data races
> with subsequent (again, in synchronization order) acquire actions of the
> write lock (and as a result with writes happening under those write locks).
> This is the reason why we can only safely read data before (in program
> order) calling validate method, and not after it. OpenJDK 11 solves this
> problem (removes such data races for reads inside
> tryOptimisticRead/validate block) by introducing VarHandle.acquireFence()
> into validate method before actually validating the stamp.
>
> A more detailed and very good explanation is here:
> "Can Seqlocks Get Along With Programming Language Memory Models?"
> http://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf
>
> Regards,
> Valentin
> [image: LinkedIn] <https://www.linkedin.com/in/stIncMale>   [image:
> GitHub] <https://github.com/stIncMale>   [image: YouTube]
> <https://www.youtube.com/user/stIncMale>
>
>
> On Tue, 23 Oct 2018 at 10:05, <concurrency-interest-request at cs.oswego.edu>
> wrote:
>
>> Send Concurrency-interest mailing list submissions to
>>         concurrency-interest at cs.oswego.edu
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> or, via email, send a message with subject or body 'help' to
>>         concurrency-interest-request at cs.oswego.edu
>>
>> You can reach the person managing the list at
>>         concurrency-interest-owner at cs.oswego.edu
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of Concurrency-interest digest..."
>>
>>
>> Today's Topics:
>>
>>    1. Happens before and stampedlocks (Sudhakar Govindavajhala)
>>
>>
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Mon, 22 Oct 2018 23:21:36 -0400
>> From: Sudhakar Govindavajhala <sudhakarg79spam at gmail.com>
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Happens before and stampedlocks
>> Message-ID:
>>         <CAGdJZsDW2v3mEn5G0UttPv9+q2TbRQWpuU4jicv1xY2yX3sjPA@
>> mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>>
>> Dear sirs and madams
>>
>>
>> Please accept my humble respects.
>>
>>
>> Please let me know if stampedlock class guarantees any happens before
>> relationship.   Yes or no.    Kindly provide some reading resources on
>> this
>> subject
>>
>> Sincerely
>> Sudhakar Krsna
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <http://cs.oswego.edu/pipermail/concurrency-
>> interest/attachments/20181022/0d7d662f/attachment-0001.html>
>>
>> ------------------------------
>>
>> Subject: Digest Footer
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> ------------------------------
>>
>> End of Concurrency-interest Digest, Vol 164, Issue 28
>> *****************************************************
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181023/24f77871/attachment-0001.html>

From Sebastian.Millies at softwareag.com  Tue Oct 23 13:33:37 2018
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Tue, 23 Oct 2018 17:33:37 +0000
Subject: [concurrency-interest] Happens before and stampedlocks
In-Reply-To: <CA+kOe0-L+VPgA+0V2etC5DcjOk1nP=WLEN+Jm2fk_kF+hyu1DA@mail.gmail.com>
References: <mailman.1.1540310401.21050.concurrency-interest@cs.oswego.edu>
 <CAO-wXw+MEhG_m=xq-Qvw=DmtqJOSknmpaT9KH060q-7damg_oA@mail.gmail.com>
 <CA+kOe0-L+VPgA+0V2etC5DcjOk1nP=WLEN+Jm2fk_kF+hyu1DA@mail.gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E4900156FF9318@daeexmbx1.eur.ad.sag>

I’d be in favor of removing API’s that can only be understood by reading a ten-page conference paper or journal article.
But how much knowledge should documentation really presuppose? (Perhaps an understanding of “Java Concurrency in Practice”?)

Seriously, I’d be interested to hear what guidelines people follow (consciously or as a matter of habit) when writing the docs.

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin Buchholz via Concurrency-interest
Sent: Tuesday, October 23, 2018 7:13 PM
To: Valentin Kovalenko
Cc: concurrency-interest
Subject: Re: [concurrency-interest] Happens before and stampedlocks

We "should" have more memory model level docs in StampedLock.
We "should" say that only those who have completely understood
"Can Seqlocks Get Along With Programming Language Memory Models?"
are qualified to use optimistic reads.

But we're likely to run into human deadlocks trying to come up with wording.

On Tue, Oct 23, 2018 at 9:24 AM, Valentin Kovalenko via Concurrency-interest <concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>> wrote:
 Hi Sudhakar,

I understand that this question is specifically about the optimistic read (because HB for read/write stamped lock is obvious).

Optimistic read does provide a HB between the previous (in synchronization order) release of a write lock and a successful optimistic read. This is also obvious, btw, because optimistic read is essentially a volatile read, and release of the write lock has a volatile write to the same field.

The problem with optimistic reads is that neither tryOptimisticRead nor validate methods have a volatile write, thus allowing data races with subsequent (again, in synchronization order) acquire actions of the write lock (and as a result with writes happening under those write locks). This is the reason why we can only safely read data before (in program order) calling validate method, and not after it. OpenJDK 11 solves this problem (removes such data races for reads inside tryOptimisticRead/validate block) by introducing VarHandle.acquireFence() into validate method before actually validating the stamp.

A more detailed and very good explanation is here:
"Can Seqlocks Get Along With Programming Language Memory Models?" http://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf

Regards,
Valentin
[LinkedIn]<https://www.linkedin.com/in/stIncMale>   [GitHub] <https://github.com/stIncMale>    [YouTube] <https://www.youtube.com/user/stIncMale>

On Tue, 23 Oct 2018 at 10:05, <concurrency-interest-request at cs.oswego.edu<mailto:concurrency-interest-request at cs.oswego.edu>> wrote:
Send Concurrency-interest mailing list submissions to
        concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>

To subscribe or unsubscribe via the World Wide Web, visit
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest
or, via email, send a message with subject or body 'help' to
        concurrency-interest-request at cs.oswego.edu<mailto:concurrency-interest-request at cs.oswego.edu>

You can reach the person managing the list at
        concurrency-interest-owner at cs.oswego.edu<mailto:concurrency-interest-owner at cs.oswego.edu>

When replying, please edit your Subject line so it is more specific
than "Re: Contents of Concurrency-interest digest..."


Today's Topics:

   1. Happens before and stampedlocks (Sudhakar Govindavajhala)


----------------------------------------------------------------------

Message: 1
Date: Mon, 22 Oct 2018 23:21:36 -0400
From: Sudhakar Govindavajhala <sudhakarg79spam at gmail.com<mailto:sudhakarg79spam at gmail.com>>
To: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] Happens before and stampedlocks
Message-ID:
        <CAGdJZsDW2v3mEn5G0UttPv9+q2TbRQWpuU4jicv1xY2yX3sjPA at mail.gmail.com<mailto:CAGdJZsDW2v3mEn5G0UttPv9%2Bq2TbRQWpuU4jicv1xY2yX3sjPA at mail.gmail.com>>
Content-Type: text/plain; charset="utf-8"


Dear sirs and madams


Please accept my humble respects.


Please let me know if stampedlock class guarantees any happens before
relationship.   Yes or no.    Kindly provide some reading resources on this
subject

Sincerely
Sudhakar Krsna
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181022/0d7d662f/attachment-0001.html>

------------------------------

Subject: Digest Footer

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


------------------------------

End of Concurrency-interest Digest, Vol 164, Issue 28
*****************************************************

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt, Germany – Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Sanjay Brahmawar (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt, Dr. Stefan Sigg; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181023/91c3b77d/attachment.html>

From sudhakarg79spam at gmail.com  Wed Oct 24 22:39:12 2018
From: sudhakarg79spam at gmail.com (Sudhakar Govindavajhala)
Date: Wed, 24 Oct 2018 22:39:12 -0400
Subject: [concurrency-interest] Happens before and stampedlocks
In-Reply-To: <CAO-wXw+MEhG_m=xq-Qvw=DmtqJOSknmpaT9KH060q-7damg_oA@mail.gmail.com>
References: <mailman.1.1540310401.21050.concurrency-interest@cs.oswego.edu>
 <CAO-wXw+MEhG_m=xq-Qvw=DmtqJOSknmpaT9KH060q-7damg_oA@mail.gmail.com>
Message-ID: <CAGdJZsCiKyk3HVzC7J2nX5pqkvdU2teAEUwQW+T0riayA9JLGg@mail.gmail.com>

hi all

I wrote privately to Valentin that my interest is not in optimistic reads.
 My interest is in plain read and write locks.

Valentin wrote as follows:


what is the basis for this statement?
Lock interface (
https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/locks/Lock.html)
specifies this in "Memory Synchronization" paragraph. Since both
ReentrantReadWriteLock.ReadLock and ReentrantReadWriteLock.WriteLock
implement Lock interface, that specification is also applied here.

With StampedLock lock the docs, unfortunately, docs are less clear.
StampedLock does not implement Lock interface and has no mentions of memory
synchronization effects. However, it has methods asReadLock/asWriteLock
which return plain Lock views of this StampedLock in which the Lock.lock()
method is mapped to readLock()/writeLock(), and similarly for other
methods. So since asReadLock/asWriteLock just map methods of StamedLock to
methods of Lock interface, we can infer that StampedLock provides the same
memory synchronization effects for read/write locks. This is all we have in
the current docs. Everything else comes from common sense and the knowledge
about the implementation.



Thanks
Sudhakar

On Tue, Oct 23, 2018, 12:27 PM Valentin Kovalenko via Concurrency-interest <
concurrency-interest at cs.oswego.edu wrote:

>  Hi Sudhakar,
>
> I understand that this question is specifically about the optimistic read
> (because HB for read/write stamped lock is obvious).
>
> Optimistic read does provide a HB between the previous (in synchronization
> order) release of a write lock and a successful optimistic read. This is
> also obvious, btw, because optimistic read is essentially a volatile read,
> and release of the write lock has a volatile write to the same field.
>
> The problem with optimistic reads is that neither tryOptimisticRead nor
> validate methods have a volatile write, thus allowing data races
> with subsequent (again, in synchronization order) acquire actions of the
> write lock (and as a result with writes happening under those write locks).
> This is the reason why we can only safely read data before (in program
> order) calling validate method, and not after it. OpenJDK 11 solves this
> problem (removes such data races for reads inside
> tryOptimisticRead/validate block) by introducing VarHandle.acquireFence()
> into validate method before actually validating the stamp.
>
> A more detailed and very good explanation is here:
> "Can Seqlocks Get Along With Programming Language Memory Models?"
> http://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf
>
> Regards,
> Valentin
> [image: LinkedIn] <https://www.linkedin.com/in/stIncMale>   [image:
> GitHub] <https://github.com/stIncMale>   [image: YouTube]
> <https://www.youtube.com/user/stIncMale>
>
>
> On Tue, 23 Oct 2018 at 10:05, <concurrency-interest-request at cs.oswego.edu>
> wrote:
>
>> Send Concurrency-interest mailing list submissions to
>>         concurrency-interest at cs.oswego.edu
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> or, via email, send a message with subject or body 'help' to
>>         concurrency-interest-request at cs.oswego.edu
>>
>> You can reach the person managing the list at
>>         concurrency-interest-owner at cs.oswego.edu
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of Concurrency-interest digest..."
>>
>>
>> Today's Topics:
>>
>>    1. Happens before and stampedlocks (Sudhakar Govindavajhala)
>>
>>
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Mon, 22 Oct 2018 23:21:36 -0400
>> From: Sudhakar Govindavajhala <sudhakarg79spam at gmail.com>
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Happens before and stampedlocks
>> Message-ID:
>>         <
>> CAGdJZsDW2v3mEn5G0UttPv9+q2TbRQWpuU4jicv1xY2yX3sjPA at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Dear sirs and madams
>>
>>
>> Please accept my humble respects.
>>
>>
>> Please let me know if stampedlock class guarantees any happens before
>> relationship.   Yes or no.    Kindly provide some reading resources on
>> this
>> subject
>>
>> Sincerely
>> Sudhakar Krsna
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <
>> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181022/0d7d662f/attachment-0001.html
>> >
>>
>> ------------------------------
>>
>> Subject: Digest Footer
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> ------------------------------
>>
>> End of Concurrency-interest Digest, Vol 164, Issue 28
>> *****************************************************
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181024/5c5d296c/attachment.html>

From martinrb at google.com  Wed Oct 24 22:55:46 2018
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 24 Oct 2018 19:55:46 -0700
Subject: [concurrency-interest] Happens before and stampedlocks
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E4900156FF9318@daeexmbx1.eur.ad.sag>
References: <mailman.1.1540310401.21050.concurrency-interest@cs.oswego.edu>
 <CAO-wXw+MEhG_m=xq-Qvw=DmtqJOSknmpaT9KH060q-7damg_oA@mail.gmail.com>
 <CA+kOe0-L+VPgA+0V2etC5DcjOk1nP=WLEN+Jm2fk_kF+hyu1DA@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E4900156FF9318@daeexmbx1.eur.ad.sag>
Message-ID: <CA+kOe0-3gUiZF5wOF7EbXa0B=tLYkuReYqCFT0iJ-U7WhkMGRQ@mail.gmail.com>

On Tue, Oct 23, 2018 at 10:33 AM, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:

> I’d be in favor of removing API’s that can only be understood by reading a
> ten-page conference paper or journal article.
>

You make compromises for 10x performance.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181024/a041aacd/attachment.html>

From aph at redhat.com  Thu Oct 25 07:24:10 2018
From: aph at redhat.com (Andrew Haley)
Date: Thu, 25 Oct 2018 12:24:10 +0100
Subject: [concurrency-interest] Happens before and stampedlocks
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E4900156FF9318@daeexmbx1.eur.ad.sag>
References: <mailman.1.1540310401.21050.concurrency-interest@cs.oswego.edu>
 <CAO-wXw+MEhG_m=xq-Qvw=DmtqJOSknmpaT9KH060q-7damg_oA@mail.gmail.com>
 <CA+kOe0-L+VPgA+0V2etC5DcjOk1nP=WLEN+Jm2fk_kF+hyu1DA@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E4900156FF9318@daeexmbx1.eur.ad.sag>
Message-ID: <f733027a-4567-7c68-dd96-1c0d2869caa2@redhat.com>

On 10/23/2018 06:33 PM, Millies, Sebastian via Concurrency-interest wrote:
> I’d be in favor of removing API’s that can only be understood by
> reading a ten-page conference paper or journal article.  But how
> much knowledge should documentation really presuppose? (Perhaps an
> understanding of “Java Concurrency in Practice”?)

I don't think this is really fair. It's not a good idea to over-
specify APIs like this one: the guarantees in the documentation are
all you've got, and all you should rely on in your code.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From dl at cs.oswego.edu  Fri Oct 26 10:00:25 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 26 Oct 2018 10:00:25 -0400
Subject: [concurrency-interest] Happens before and stampedlocks
In-Reply-To: <CAGdJZsDW2v3mEn5G0UttPv9+q2TbRQWpuU4jicv1xY2yX3sjPA@mail.gmail.com>
References: <CAGdJZsDW2v3mEn5G0UttPv9+q2TbRQWpuU4jicv1xY2yX3sjPA@mail.gmail.com>
Message-ID: <2bacae6c-a8f7-42cd-ddb6-e9b2dec5d926@cs.oswego.edu>

Sorry for delay answering:

On 10/22/18 11:21 PM, Sudhakar Govindavajhala via Concurrency-interest
wrote:
> 
> Please let me know if stampedlock class guarantees any happens before
> relationship.   Yes or no.    Kindly provide some reading resources on
> this subject
> 

Yes. As mentioned in the java.util.concurrent package documentation
(https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/package-summary.html),
all j.u.c locks conform to happens-before requirements. This is not
repeated in any lock APIs, but perhaps it should be. Additionally, this
could/should be made explicit for optimistic mode:

  *   in write mode. Method {@link #validate} returns true if the lock
  *   has not been acquired in write mode since obtaining a given
- *   stamp.  This mode can be thought of as an extremely weak version
- *   of a read-lock, that can be broken by a writer at any time.

+ *   stamp, in which case all actions prior to the most recent write
+ *   lock release happen-before actions following the call to
+ *   validate. This mode can be thought of as an extremely weak
+ *   ...

Some of the difficulty in documenting this and some other APIs is that
the official JMM is still languishing without incorporating specs for
the jdk9+ VarHandle modes and related methods falling between plain and
volatile, that could be used to more clearly specify behavior. Some day.
In the mean time, I added something in the guide at
http://gee.cs.oswego.edu/dl/html/j9mm.html indicating the techniques
used here.

-Doug


From martinrb at google.com  Fri Oct 26 15:23:27 2018
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 26 Oct 2018 12:23:27 -0700
Subject: [concurrency-interest] Happens before and stampedlocks
In-Reply-To: <2bacae6c-a8f7-42cd-ddb6-e9b2dec5d926@cs.oswego.edu>
References: <CAGdJZsDW2v3mEn5G0UttPv9+q2TbRQWpuU4jicv1xY2yX3sjPA@mail.gmail.com>
 <2bacae6c-a8f7-42cd-ddb6-e9b2dec5d926@cs.oswego.edu>
Message-ID: <CA+kOe0_EAv3_BmF9=NG06G-maRribUqu+qphNVyPtQZ0APs9LQ@mail.gmail.com>

On Fri, Oct 26, 2018 at 7:00 AM, Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

>
> + *   stamp, in which case all actions prior to the most recent write
> + *   lock release happen-before actions following the call to
> + *   validate.
> + *   ...
>

It's trickier.

The happens-before is actually established by the tryOptimisticRead, but
because there's no mutual exclusion, you can't tell that a read
between tryOptimisticRead
and validate didn't come from the future -i.e. validate establishes the
mutual exclusion that allows you to conclude that reads prior to validate
"happened after" the release of the lock held when the corresponding write
occurred.

Coming up with good wording here is hard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20181026/9341ecd8/attachment.html>

From dl at cs.oswego.edu  Sun Oct 28 09:34:12 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 28 Oct 2018 09:34:12 -0400
Subject: [concurrency-interest] Happens before and stampedlocks
In-Reply-To: <2bacae6c-a8f7-42cd-ddb6-e9b2dec5d926@cs.oswego.edu>
References: <CAGdJZsDW2v3mEn5G0UttPv9+q2TbRQWpuU4jicv1xY2yX3sjPA@mail.gmail.com>
 <2bacae6c-a8f7-42cd-ddb6-e9b2dec5d926@cs.oswego.edu>
Message-ID: <28e0dfc2-c539-a85b-fce1-5d99c97669eb@cs.oswego.edu>

On 10/26/18 10:00 AM, Doug Lea via Concurrency-interest wrote:

> Yes. As mentioned in the java.util.concurrent package documentation
> (https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/package-summary.html),
> all j.u.c locks conform to happens-before requirements. This is not
> repeated in any lock APIs, but perhaps it should be.

On second thought, we should add this explicitly to avoid future
confusion (also improving the wording in previous post):

 * <p><b>Memory Synchronization.</b> Methods with the effect of
 * successfully locking in any mode have the same memory
 * synchronization effects as a <em>Lock</em> action described <a
 *
href="https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4">
 * Chapter 17 of <cite>The Java&trade; Language
 * Specification</cite></a>.  Methods successfully unlocking in write
 * mode have the same memory synchronization effects as
 * <em>Unlock</em> actions.  In optimistic usages, actions prior to
 * the most recent write mode unlock action are guaranteed to
 * happen-before those following a tryOptimisticRead only if a later
 * validate returns true; otherwise there is no guarantee that the
 * reads between tryOptimisticRead and validate obtain a consistent
 * snapshot.
 *


