From william.louth at jinspired.com  Sun Aug  1 14:00:21 2010
From: william.louth at jinspired.com (William Louth (JINSPIRED.COM))
Date: Sun, 01 Aug 2010 20:00:21 +0200
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <4C51D02C.3050807@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>
	<4C4DDE3D.8040508@cox.net> <4C4DE96F.2070600@jinspired.com>
	<4C4E0494.4050309@cytetech.com> <4C4E09B8.6010007@jinspired.com>
	<4C51D02C.3050807@cytetech.com>
Message-ID: <4C55B635.1030407@jinspired.com>

  Hi Gregg,

We employ many techniques some of which have enabled us to support Java 
1.4 VM's up to the release of OpenCore 6.0. But as in any endeavor that 
has constraints the key is to start with a good design in terms of 
model, interface and runtime. OpenCore has achieved this via its 
metering concepts of activities (probes and cost group hierarchies) and 
meters (typically thread specific counters, measures, resources). With 
any global state management the goal should always to balance cost and 
value especially when value itself allows one to dynamic reduce cost 
itself. We employ various low level techniques as well as many 
architectural ones which are touched on in this article.

http://williamlouth.wordpress.com/2010/03/07/three-degrees-of-separation-in-apm/

I will be giving a Google tech talk next week on the usage of smart 
metering within software stacks and across cloud service interactions 
next week which hopefully be made available the following week. During 
this talk I will discuss the techniques employed today to reduce such 
overhead and new techniques we are currently investigating which nearly 
eliminate this via a synch(point) probes provider optional add-on to our 
runtime.

Some other tools employ the use of non-application threads (and pools) 
for collection & update but unfortunately these approaches are generally 
more expensive (globally) than ones that do not but are much smarter and 
efficient in how it is does within the thread context - a benefit of 
which is that scalability is predictable and data is real-time which can 
be important if the metering information is used itself within a 
grid/task scheduler.

http://williamlouth.wordpress.com/2010/01/12/too-expensive-to-meter-too-slow-to-profile/
http://williamlouth.wordpress.com/2009/12/24/too-cheap-to-meter-too-fast-to-profile/
http://williamlouth.wordpress.com/2009/11/30/self-optimizing-java-hotspot-detection/
http://williamlouth.wordpress.com/2009/10/29/hybrid-probe-profiling-execution-sampling/

William

On 29/07/2010 21:02, Gregg Wonderly wrote:
> Again, one of the big issues for me, is that typically, such metering 
> requires atomic counting, which, itself, is a form of contention.  
> This contention can get injected into previously uncontended blocks 
> and redistribute the timing through particular parts of the code which 
> then don't reveal the actual behavior in normal execution.
>
> Where can I see more information about how issues like this are dealt 
> with?
>
> Gregg Wonderly
>
> William Louth (JINSPIRED.COM) wrote:
>>  It can do this and more but maybe I should have factor in the 
>> likelihood that you would have only skimmed over the links I email - 
>> time is money. Sorry for not directly pointing this out but as the 
>> architect of both the Probes and Metrics Open API's I thought this 
>> would be rather obvious.
>>
>> OpenCore supports many meters some of which are contention related: 
>> http://opencore.jinspired.com/?page_id=981#p:core A lot of these are 
>> not time related.
>>
>> OpenCore's Probes metering can be automatically mapped to Metrics to 
>> correlate across multiple threads with other measurements not metered 
>> based.
>> http://opencore.jinspired.com/?page_id=377
>>
>> OpenCore supports the reporting of metering at thread and probes 
>> level. You indicate transaction points to the runtime via config and 
>> then see which particular probes (activities) and meters (resources: 
>> monitors) contributed the most. See transaction probes provider under 
>> Guides.
>> http://opencore.jinspired.com/?page_id=772
>>
>> On 26/07/2010 23:56, Gregg Wonderly wrote:
>>> This doesn't really point at concurrency issues, only in execution 
>>> time of compute bound execution, and perhaps some simple linear 
>>> scaling of complexity.
>>>
>>> When you have 1000s of instructions and multiple code paths, which 
>>> include synchronization, and random latency injection, I am not sure 
>>> that you can see how threads are "waiting in line" until you see the 
>>> quadratic change in execution time that is typically visible when 
>>> high contention occurs.
>>>
>>> Maybe you can point out where this tool provides for the ability to 
>>> monitor lock contention and count stalled threads and otherwise see 
>>> the real contention that develops over time as load increases on a 
>>> highly contended code segment?
>>>
>>> If you could "time" the execution interval through all code paths, 
>>> and then look at the "percentage" of threads and time through each 
>>> path, and then see that the "largest" latency block was in a very 
>>> common code path, you could perhaps then say this was the area to 
>>> look at.  All of that analysis might not really be possible though, 
>>> because of the exponential potential complexity of code path coverage.
>>>
>>> Gregg Wonderly
>>>
>>> William Louth (JINSPIRED.COM) wrote:
>>>>  JINSPIRED'S OpenCore (http://opencore.jinspired.com) metering & 
>>>> metrics runtime has built in meters for thread contention metering 
>>>> (activity based) @see blocking.time and blocking.count.
>>>>
>>>> You can easily extend it with your own custom counters or resource 
>>>> measures mapped to meters. And you don't always need to measure 
>>>> time: 
>>>> http://williamlouth.wordpress.com/2010/06/11/no-latency-application-performance-analysis-when-wall-clock-time-is-simply-too-slow/ 
>>>>
>>>>
>>>> Queuing can also be aggregated at various namespace levels: 
>>>> http://williamlouth.wordpress.com/2010/05/20/metered-software-service-queues/ 
>>>>
>>>>
>>>> More related articles: 
>>>> http://williamlouth.wordpress.com/category/profiling/
>>>>
>>>> On 26/07/2010 21:13, Gregg Wonderly wrote:
>>>>> Per thread latency measurements with 1, 2, 10 and 100 threads will 
>>>>> often tell you a lot about how contention is affecting the 
>>>>> execution time.  When you get to 100, a thread dump will often 
>>>>> reveal where everyone is standing in line...
>>>>>
>>>>> Gregg Wonderly
>>>>>
>>>>> David Holmes wrote:
>>>>>> Kendall,
>>>>>>
>>>>>> In my opinion a monitoring tool looking at the lock acquisition 
>>>>>> time, or CAS attempts, won't give you much insight into whether 
>>>>>> to use a blocking or non-blocking approach. You need to measure 
>>>>>> the performance of your application logic as a whole, utilising 
>>>>>> the two different approaches. Afterall how can you compare 
>>>>>> locking times with number of CAS attempts in general?
>>>>>>
>>>>>> David Holmes
>>>>>>
>>>>>>     -----Original Message-----
>>>>>>     *From:* concurrency-interest-bounces at cs.oswego.edu
>>>>>>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>>>>>>     *Kendall Moore
>>>>>>     *Sent:* Sunday, 25 July 2010 4:14 PM
>>>>>>     *To:* concurrency-interest at cs.oswego.edu
>>>>>>     *Subject:* [concurrency-interest] Monitoring Tool
>>>>>>
>>>>>>     Greetings all,
>>>>>>
>>>>>>     Is there a common consensus on which monitoring tools are 
>>>>>> best to
>>>>>>     use when writing parallel apps?  To be more specific, I would 
>>>>>> like
>>>>>>     to be able to know how many times a given thread has to try 
>>>>>> to CAS
>>>>>>     before succeeding.   Also, the ability to see how long a thread
>>>>>>     waits to acquire a lock would be useful as well.  The end goal
>>>>>>     would, in my particular case, would be to compare these in 
>>>>>> order to
>>>>>>     determine if a non-blocking approach would be more effective 
>>>>>> in a
>>>>>>     given situation than a lock-based approach.  Any help would 
>>>>>> be much
>>>>>>     appreciated!
>>>>>>
>>>>>>     --     Kendall Moore
>>>>>>
>>>>>>
>>>>>> ------------------------------------------------------------------------ 
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>>
>>
>>
>
>
>

From matthias at mernst.org  Mon Aug  2 09:41:19 2010
From: matthias at mernst.org (Matthias Ernst)
Date: Mon, 2 Aug 2010 15:41:19 +0200
Subject: [concurrency-interest] Safety of initializations in Servlet init
In-Reply-To: <4E7C4E9E-4D1D-4A22-8A37-D504A04AC018@trifork.com>
References: <992148B0-2668-4535-8253-15A23D285D93@trifork.com>
	<AANLkTimeFWNepVTPQkXJtZkx02Q=XRhcrbJ6Gf6PVnjn@mail.gmail.com>
	<4E7C4E9E-4D1D-4A22-8A37-D504A04AC018@trifork.com>
Message-ID: <AANLkTim=4uDqcavOqT3d9XaLUUdYnjQMmW7rNuz2v2dL@mail.gmail.com>

On Sat, Jul 31, 2010 at 7:19 AM, Karl Krukow <kkr at trifork.com> wrote:
>
> On 30/07/2010, at 20.15, Kris Schneider wrote:
>
>> On Fri, Jul 30, 2010 at 1:05 PM, Karl Krukow <kkr at trifork.com> wrote:
> [snip]
>
>> The article you reference below is almost 10 years old and common (or
>> at least better) practice has moved on to use ServletContextListener.
>
> Even so, the question is still valid: why is it safe to do an initialization of an instance variable in one thread (in the init method) and read the variable in another thread (in the service method) without synchronization. There must be two scenarios: (i) either it's isn't really safe and the common pattern is really broken; or (ii) app servers perform some form of synchronization under the hood to ensure a happens-before relation between the write in the init and the read in the service method.
>
> Hopefully it is not (i), but the problem with (ii) is that as far as I can see, it isn't mentioned in the servlet or portlet specifications so if coding strictly to the spec the programmer would have to synchronize.
>
> Regarding: ServletContextListener: I didn't know that using it is common best practice, and the servlet spec still writes about the init method (servlet 2.5, SRV.2.3.2):
>
> ?>>After the servlet object is instantiated, the container must initialize the servlet before it can handle requests from clients. Initialization is provided so that a servlet can read persistent configuration data, initialize costly resources (such as JDBCTM API- based connections), and perform other one-time activities. The container initializes the servlet instance by calling the init method of the Servlet interface with a unique (per servlet declaration) object implementing the ServletConfig interface.<<

I think the keywords are "must...before". I'd argue that this prose
naturally translates to "happens before" in JMM lingo. The only
reasonable ways to implement this requirement introduce happens-before
relationships: for example the "worker pool threads" are only started
after the init methods have run or the container worker safely
communicates an initialization bit to the workers or a new,
initialized servlet instance is safely published into some
servlet-mapping.

Then "Write to servlet instance variables" happens-before "init-method
finishes" happens-before "request is dispatched to servlet"
happens-before "read from instance variable".

If a container instead performed an unsafe busy wait loop on the
servlet instance, there are probably more things to run away
screamingly from such container...

That said, back in the 4.x times, a chief Tomcat maintainer resisted
to necessary synchronization related to session timeout and on-demand
JSP compilation if I remember correctly. But those simply were bugs
IMHO that undermined my trust in the implementation and not the spec.

Matthias



> In either case, in the servlet context listener, the same question applies, but Brian's article you mention below shows that it is up to the user to synchronize if storing an object in the servlet context.
>
> [snip]
>
>>
>> Although I don't recall that Servlet.init is specifically covered,
>> this is a more recent and much more informative article on state and
>> concurrency in the context of servlet containers:
>>
>> http://www.ibm.com/developerworks/library/j-jtp09238.html
>
> Yes, thanks for that (good) reference. I remember reading that some time ago. It doesn't mention the init method.
>
> [snip..]
>
> Thanks for taking the time,
> - Karl
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From kkr at trifork.com  Tue Aug  3 01:23:08 2010
From: kkr at trifork.com (Karl Krukow)
Date: Tue, 3 Aug 2010 07:23:08 +0200
Subject: [concurrency-interest] Safety of initializations in Servlet init
In-Reply-To: <AANLkTim=4uDqcavOqT3d9XaLUUdYnjQMmW7rNuz2v2dL@mail.gmail.com>
References: <992148B0-2668-4535-8253-15A23D285D93@trifork.com>
	<AANLkTimeFWNepVTPQkXJtZkx02Q=XRhcrbJ6Gf6PVnjn@mail.gmail.com>
	<4E7C4E9E-4D1D-4A22-8A37-D504A04AC018@trifork.com>
	<AANLkTim=4uDqcavOqT3d9XaLUUdYnjQMmW7rNuz2v2dL@mail.gmail.com>
Message-ID: <A71C78B6-7D5E-4F0C-90D2-D8E02816A76A@trifork.com>


On 02/08/2010, at 15.41, Matthias Ernst wrote:
[snip]
> I think the keywords are "must...before". I'd argue that this prose
> naturally translates to "happens before" in JMM lingo. The only
> reasonable ways to implement this requirement introduce happens-before
> relationships: for example the "worker pool threads" are only started
> after the init methods have run or the container worker safely
> communicates an initialization bit to the workers or a new,
> initialized servlet instance is safely published into some
> servlet-mapping.

If there are several servlets sharing a thread pool, then chances are it is already started, right?

But, yes, I agree it would be hard to implement this without creating a happens-before relationship since the request handling logic must check that the servlet has been initialized before calling the service method (I guess, at least this would require a volatile boolean read/write).

> 
> Then "Write to servlet instance variables" happens-before "init-method
> finishes" happens-before "request is dispatched to servlet"
> happens-before "read from instance variable".
> 
> If a container instead performed an unsafe busy wait loop on the
> servlet instance, there are probably more things to run away
> screamingly from such container...
> 
> That said, back in the 4.x times, a chief Tomcat maintainer resisted
> to necessary synchronization related to session timeout and on-demand
> JSP compilation if I remember correctly. But those simply were bugs
> IMHO that undermined my trust in the implementation and not the spec.
> 
> Matthias

;) Yes, that happened to me too: I recently saw the broken double checked locking in apache myfaces portlet bridge - the natural thing to think is "so what else is broken ..."

private void initBridge()
http://svn.apache.org/repos/asf/myfaces/portlet-bridge/core/tags/2.0.0-beta/api/src/main/java/javax/portlet/faces/GenericFacesPortlet.java 

Perhaps many of the jsr specs would be more clear if JMM considerations were explicit.

Thanks for taking the time.

Karl.

From gergg at cox.net  Tue Aug  3 17:57:54 2010
From: gergg at cox.net (Gregg Wonderly)
Date: Tue, 03 Aug 2010 16:57:54 -0500
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <879050.54032.qm@web38804.mail.mud.yahoo.com>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>	<4C4DDE3D.8040508@cox.net>
	<4C4DE96F.2070600@jinspired.com>	<4C4E0494.4050309@cytetech.com>
	<4C4E09B8.6010007@jinspired.com>	<4C51D02C.3050807@cytetech.com>
	<879050.54032.qm@web38804.mail.mud.yahoo.com>
Message-ID: <4C5890E2.60501@cox.net>

I know this seems pretty harmless, but for extremely light CPU use applications, 
even the use of the read/write lock on a thread local value can create a marked 
change in the contention points of an application.  I know that it can be useful 
information, but there are places where these change invert the real world 
places of interest and can cause any metered code to suddenly become the hot spot.

Gregg Wonderly

Ben Manes wrote:
> Most likely their using an inverted read/write lock approach where the 
> count is kept thread-local and aggregated by a monitoring thread. I 
> haven't looked at their implementation, but that's a standard idiom for 
> avoiding contention for capturing statistical information.
> 
> ------------------------------------------------------------------------
> *From:* Gregg Wonderly <gregg at cytetech.com>
> *To:* William Louth (JINSPIRED.COM) <william.louth at jinspired.com>
> *Cc:* concurrency-interest at cs.oswego.edu; gregg.wonderly at pobox.com
> *Sent:* Thu, July 29, 2010 12:02:04 PM
> *Subject:* Re: [concurrency-interest] Monitoring Tool
> 
> Again, one of the big issues for me, is that typically, such metering 
> requires atomic counting, which, itself, is a form of contention.  This 
> contention can get injected into previously uncontended blocks and 
> redistribute the timing through particular parts of the code which then 
> don't reveal the actual behavior in normal execution.
> 
> Where can I see more information about how issues like this are dealt with?
> 
> Gregg Wonderly
> 
> William Louth (JINSPIRED.COM <http://JINSPIRED.COM>) wrote:
>  >  It can do this and more but maybe I should have factor in the 
> likelihood that you would have only skimmed over the links I email - 
> time is money. Sorry for not directly pointing this out but as the 
> architect of both the Probes and Metrics Open API's I thought this would 
> be rather obvious.
>  >
>>  OpenCore supports many meters some of which are contention related: 
> http://opencore.jinspired.com/?page_id=981#p:core A lot of these are not 
> time related.
>  >
>  > OpenCore's Probes metering can be automatically mapped to Metrics to 
> correlate across multiple threads with other measurements not metered based.
>>  http://opencore.jinspired.com/?page_id=377
>  >
>  > OpenCore supports the reporting of metering at thread and probes 
> level. You indicate transaction points to the runtime via config and 
> then see which particular probes (activities) and meters (resources: 
> monitors) contributed the most. See transaction probes provider under 
> Guides.
>>  http://opencore.jinspired.com/?page_id=772
>  >
>  > On 26/07/2010 23:56, Gregg Wonderly wrote:
>  >> This doesn't really point at concurrency issues, only in execution 
> time of compute bound execution, and perhaps some simple linear scaling 
> of complexity.
>  >>
>  >> When you have 1000s of instructions and multiple code paths, which 
> include synchronization, and random latency injection, I am not sure 
> that you can see how threads are "waiting in line" until you see the 
> quadratic change in execution time that is typically visible when high 
> contention occurs.
>  >>
>  >> Maybe you can point out where this tool provides for the ability to 
> monitor lock contention and count stalled threads and otherwise see the 
> real contention that develops over time as load increases on a highly 
> contended code segment?
>  >>
>  >> If you could "time" the execution interval through all code paths, 
> and then look at the "percentage" of threads and time through each path, 
> and then see that the "largest" latency block was in a very common code 
> path, you could perhaps then say this was the area to look at.  All of 
> that analysis might not really be possible though, because of the 
> exponential potential complexity of code path coverage.
>  >>
>  >> Gregg Wonderly
>  >>
>  >> William Louth (JINSPIRED.COM) wrote:
>> >>  JINSPIRED'S OpenCore (http://opencore.jinspired.com) metering & 
> metrics runtime has built in meters for thread contention metering 
> (activity based) @see blocking.time and blocking.count.
>  >>>
>> >> You can easily extend it with your own custom counters or resource 
> measures mapped to meters. And you don't always need to measure time: 
> http://williamlouth.wordpress.com/2010/06/11/no-latency-application-performance-analysis-when-wall-clock-time-is-simply-too-slow/ 
> 
>  >>>
>> >> Queuing can also be aggregated at various namespace levels: 
> http://williamlouth.wordpress.com/2010/05/20/metered-software-service-queues/ 
> 
>  >>>
>> >> More related articles: 
> http://williamlouth.wordpress.com/category/profiling/
>  >>>
>  >>> On 26/07/2010 21:13, Gregg Wonderly wrote:
>  >>>> Per thread latency measurements with 1, 2, 10 and 100 threads will 
> often tell you a lot about how contention is affecting the execution 
> time.  When you get to 100, a thread dump will often reveal where 
> everyone is standing in line...
>  >>>>
>  >>>> Gregg Wonderly
>  >>>>
>  >>>> David Holmes wrote:
>  >>>>> Kendall,
>  >>>>>
>  >>>>> In my opinion a monitoring tool looking at the lock acquisition 
> time, or CAS attempts, won't give you much insight into whether to use a 
> blocking or non-blocking approach. You need to measure the performance 
> of your application logic as a whole, utilising the two different 
> approaches. Afterall how can you compare locking times with number of 
> CAS attempts in general?
>  >>>>>
>  >>>>> David Holmes
>  >>>>>
>  >>>>>    -----Original Message-----
>  >>>>>    *From:* concurrency-interest-bounces at cs.oswego.edu 
> <mailto:concurrency-interest-bounces at cs.oswego.edu>
>  >>>>>    [mailto:concurrency-interest-bounces at cs.oswego.edu 
> <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On Behalf Of
>  >>>>>    *Kendall Moore
>  >>>>>    *Sent:* Sunday, 25 July 2010 4:14 PM
>  >>>>>    *To:* concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>
>  >>>>>    *Subject:* [concurrency-interest] Monitoring Tool
>  >>>>>
>  >>>>>    Greetings all,
>  >>>>>
>  >>>>>    Is there a common consensus on which monitoring tools are best to
>  >>>>>    use when writing parallel apps?  To be more specific, I would like
>  >>>>>    to be able to know how many times a given thread has to try to CAS
>  >>>>>    before succeeding.  Also, the ability to see how long a thread
>  >>>>>    waits to acquire a lock would be useful as well.  The end goal
>  >>>>>    would, in my particular case, would be to compare these in 
> order to
>  >>>>>    determine if a non-blocking approach would be more effective in a
>  >>>>>    given situation than a lock-based approach.  Any help would be 
> much
>  >>>>>    appreciated!
>  >>>>>
>  >>>>>    --    Kendall Moore
>  >>>>>
>  >>>>>
>  >>>>> 
> ------------------------------------------------------------------------
>  >>>>>
>  >>>>> _______________________________________________
>  >>>>> Concurrency-interest mailing list
>  >>>>> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
>> >>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>  >>>>
>  >>>> _______________________________________________
>  >>>> Concurrency-interest mailing list
>  >>>> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
>  >>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>  >>>>
>  >>>>
>  >>> _______________________________________________
>  >>> Concurrency-interest mailing list
>  >>> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
>  >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>  >>>
>  >>>
>  >>
>  >>
>  >>
>  >
>  >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From william.louth at jinspired.com  Tue Aug  3 20:05:02 2010
From: william.louth at jinspired.com (William Louth (JINSPIRED.COM))
Date: Wed, 04 Aug 2010 02:05:02 +0200
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <4C5890E2.60501@cox.net>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>	<4C4DDE3D.8040508@cox.net>
	<4C4DE96F.2070600@jinspired.com>	<4C4E0494.4050309@cytetech.com>
	<4C4E09B8.6010007@jinspired.com>	<4C51D02C.3050807@cytetech.com>
	<879050.54032.qm@web38804.mail.mud.yahoo.com>
	<4C5890E2.60501@cox.net>
Message-ID: <4C58AEAE.7070603@jinspired.com>

  Hi Gregg,

It would be great if you could publish real-world data that actual 
quantifies your concerns and opinion.

Metered code can't become a hotspot if the metering runtime is strategy 
based, configured correctly (in terms of meter, threshold, warm-up, & 
statistic), and communication between the instrumentation & measurement 
code is bi-directional allowing for evaluation short cutting to execute 
in under 1 nanosecond. Which is the case for OpenCore under extreme 
micro-benchmarking tests.

That said OpenCore does front thread locals with fast & small caches 
that play on the thread context & cpu coupling to some degree.

You might be surprised to find out that for some of our customers who 
avail of the open interface to our metering data within the runtime and 
& their scheduling code they actually make performance gains.

In addition you should try to look at the big picture which goes beyond 
a single process execution lifecycle. The metering data produced by 
OpenCore leads to significant gains across releases something which 
would never happen if one was thinking short-term and (thread) local.

For our customers having production quality data is the biggest 
contributor to increases in quality of the code and its execution. I 
can't recall a case in which the metering hotspot strategy produced a 
red herring after a few iterations of a fully warmed up runtime.

I would also like to point out that in two recent proof of concepts by 
major fx trading platforms it was found that OpenCore's strategy based 
metering runtime was the only realistic solution on the market for 
production monitoring of applications with tx time intervals in the 
microsecond range (approx 300). Hopefully we can eventually get these 
reports published.

There will always be exceptions to the rule but for those I doubt there 
is any solution available today (we come as close as is possible when 
used & configured appropriately). Please do not get me started on call 
stack sampling.

William

On 03/08/2010 23:57, Gregg Wonderly wrote:
> I know this seems pretty harmless, but for extremely light CPU use 
> applications, even the use of the read/write lock on a thread local 
> value can create a marked change in the contention points of an 
> application.  I know that it can be useful information, but there are 
> places where these change invert the real world places of interest and 
> can cause any metered code to suddenly become the hot spot.
>
> Gregg Wonderly
>
> Ben Manes wrote:
>> Most likely their using an inverted read/write lock approach where 
>> the count is kept thread-local and aggregated by a monitoring thread. 
>> I haven't looked at their implementation, but that's a standard idiom 
>> for avoiding contention for capturing statistical information.
>>
>> ------------------------------------------------------------------------
>> *From:* Gregg Wonderly <gregg at cytetech.com>
>> *To:* William Louth (JINSPIRED.COM) <william.louth at jinspired.com>
>> *Cc:* concurrency-interest at cs.oswego.edu; gregg.wonderly at pobox.com
>> *Sent:* Thu, July 29, 2010 12:02:04 PM
>> *Subject:* Re: [concurrency-interest] Monitoring Tool
>>
>> Again, one of the big issues for me, is that typically, such metering 
>> requires atomic counting, which, itself, is a form of contention.  
>> This contention can get injected into previously uncontended blocks 
>> and redistribute the timing through particular parts of the code 
>> which then don't reveal the actual behavior in normal execution.
>>
>> Where can I see more information about how issues like this are dealt 
>> with?
>>
>> Gregg Wonderly
>>
>> William Louth (JINSPIRED.COM <http://JINSPIRED.COM>) wrote:
>> >  It can do this and more but maybe I should have factor in the 
>> likelihood that you would have only skimmed over the links I email - 
>> time is money. Sorry for not directly pointing this out but as the 
>> architect of both the Probes and Metrics Open API's I thought this 
>> would be rather obvious.
>> >
>>>  OpenCore supports many meters some of which are contention related: 
>> http://opencore.jinspired.com/?page_id=981#p:core A lot of these are 
>> not time related.
>> >
>> > OpenCore's Probes metering can be automatically mapped to Metrics 
>> to correlate across multiple threads with other measurements not 
>> metered based.
>>>  http://opencore.jinspired.com/?page_id=377
>> >
>> > OpenCore supports the reporting of metering at thread and probes 
>> level. You indicate transaction points to the runtime via config and 
>> then see which particular probes (activities) and meters (resources: 
>> monitors) contributed the most. See transaction probes provider under 
>> Guides.
>>>  http://opencore.jinspired.com/?page_id=772
>> >
>> > On 26/07/2010 23:56, Gregg Wonderly wrote:
>> >> This doesn't really point at concurrency issues, only in execution 
>> time of compute bound execution, and perhaps some simple linear 
>> scaling of complexity.
>> >>
>> >> When you have 1000s of instructions and multiple code paths, which 
>> include synchronization, and random latency injection, I am not sure 
>> that you can see how threads are "waiting in line" until you see the 
>> quadratic change in execution time that is typically visible when 
>> high contention occurs.
>> >>
>> >> Maybe you can point out where this tool provides for the ability 
>> to monitor lock contention and count stalled threads and otherwise 
>> see the real contention that develops over time as load increases on 
>> a highly contended code segment?
>> >>
>> >> If you could "time" the execution interval through all code paths, 
>> and then look at the "percentage" of threads and time through each 
>> path, and then see that the "largest" latency block was in a very 
>> common code path, you could perhaps then say this was the area to 
>> look at.  All of that analysis might not really be possible though, 
>> because of the exponential potential complexity of code path coverage.
>> >>
>> >> Gregg Wonderly
>> >>
>> >> William Louth (JINSPIRED.COM) wrote:
>>> >>  JINSPIRED'S OpenCore (http://opencore.jinspired.com) metering & 
>> metrics runtime has built in meters for thread contention metering 
>> (activity based) @see blocking.time and blocking.count.
>> >>>
>>> >> You can easily extend it with your own custom counters or resource 
>> measures mapped to meters. And you don't always need to measure time: 
>> http://williamlouth.wordpress.com/2010/06/11/no-latency-application-performance-analysis-when-wall-clock-time-is-simply-too-slow/ 
>>
>> >>>
>>> >> Queuing can also be aggregated at various namespace levels: 
>> http://williamlouth.wordpress.com/2010/05/20/metered-software-service-queues/ 
>>
>> >>>
>>> >> More related articles: 
>> http://williamlouth.wordpress.com/category/profiling/
>> >>>
>> >>> On 26/07/2010 21:13, Gregg Wonderly wrote:
>> >>>> Per thread latency measurements with 1, 2, 10 and 100 threads 
>> will often tell you a lot about how contention is affecting the 
>> execution time.  When you get to 100, a thread dump will often reveal 
>> where everyone is standing in line...
>> >>>>
>> >>>> Gregg Wonderly
>> >>>>
>> >>>> David Holmes wrote:
>> >>>>> Kendall,
>> >>>>>
>> >>>>> In my opinion a monitoring tool looking at the lock acquisition 
>> time, or CAS attempts, won't give you much insight into whether to 
>> use a blocking or non-blocking approach. You need to measure the 
>> performance of your application logic as a whole, utilising the two 
>> different approaches. Afterall how can you compare locking times with 
>> number of CAS attempts in general?
>> >>>>>
>> >>>>> David Holmes
>> >>>>>
>> >>>>>    -----Original Message-----
>> >>>>>    *From:* concurrency-interest-bounces at cs.oswego.edu 
>> <mailto:concurrency-interest-bounces at cs.oswego.edu>
>> >>>>>    [mailto:concurrency-interest-bounces at cs.oswego.edu 
>> <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On Behalf Of
>> >>>>>    *Kendall Moore
>> >>>>>    *Sent:* Sunday, 25 July 2010 4:14 PM
>> >>>>>    *To:* concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>
>> >>>>>    *Subject:* [concurrency-interest] Monitoring Tool
>> >>>>>
>> >>>>>    Greetings all,
>> >>>>>
>> >>>>>    Is there a common consensus on which monitoring tools are 
>> best to
>> >>>>>    use when writing parallel apps?  To be more specific, I 
>> would like
>> >>>>>    to be able to know how many times a given thread has to try 
>> to CAS
>> >>>>>    before succeeding.  Also, the ability to see how long a thread
>> >>>>>    waits to acquire a lock would be useful as well.  The end goal
>> >>>>>    would, in my particular case, would be to compare these in 
>> order to
>> >>>>>    determine if a non-blocking approach would be more effective 
>> in a
>> >>>>>    given situation than a lock-based approach.  Any help would 
>> be much
>> >>>>>    appreciated!
>> >>>>>
>> >>>>>    --    Kendall Moore
>> >>>>>
>> >>>>>
>> >>>>> 
>> ------------------------------------------------------------------------
>> >>>>>
>> >>>>> _______________________________________________
>> >>>>> Concurrency-interest mailing list
>> >>>>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> >>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>
>> >>>> _______________________________________________
>> >>>> Concurrency-interest mailing list
>> >>>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> >>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>
>> >>>>
>> >>> _______________________________________________
>> >>> Concurrency-interest mailing list
>> >>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>
>> >>>
>> >>
>> >>
>> >>
>> >
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

From william.louth at jinspired.com  Wed Aug  4 00:07:23 2010
From: william.louth at jinspired.com (William Louth (JINSPIRED.COM))
Date: Wed, 04 Aug 2010 06:07:23 +0200
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <4C58AEAE.7070603@jinspired.com>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>	<4C4DDE3D.8040508@cox.net>	<4C4DE96F.2070600@jinspired.com>	<4C4E0494.4050309@cytetech.com>	<4C4E09B8.6010007@jinspired.com>	<4C51D02C.3050807@cytetech.com>	<879050.54032.qm@web38804.mail.mud.yahoo.com>	<4C5890E2.60501@cox.net>
	<4C58AEAE.7070603@jinspired.com>
Message-ID: <4C58E77B.5020106@jinspired.com>

  [last post on this thread]

I wanted to add that I have designed OpenCore's Probes (Metering) and 
Metrics (Monitoring) API's with the hope that JVM could directly 
support  meter integration (low level thread specific jvm counters) as 
well as providing an actual implementation of each API directly within 
the runtime via a SPI.

My vision is for activity based metering to be present in all future 
runtimes (whatever the language or environment) and to be used to create 
new dynamic libraries (and services) which are much more aware of the 
performance, cost (monetary or unit based) and capacity (on-demand) 
requirements and characteristics of a software's execution whether it 
relates to in-process task scheduling, out-of-process metered service 
interactions, or distributed job queuing & routing.

I have for a number of years tried to get various JVM vendors on board a 
possible standardization but that's a sad & depressing tale for another day.

William

On 04/08/2010 02:05, William Louth (JINSPIRED.COM) wrote:
>  Hi Gregg,
>
> It would be great if you could publish real-world data that actual 
> quantifies your concerns and opinion.
>
> Metered code can't become a hotspot if the metering runtime is 
> strategy based, configured correctly (in terms of meter, threshold, 
> warm-up, & statistic), and communication between the instrumentation & 
> measurement code is bi-directional allowing for evaluation short 
> cutting to execute in under 1 nanosecond. Which is the case for 
> OpenCore under extreme micro-benchmarking tests.
>
> That said OpenCore does front thread locals with fast & small caches 
> that play on the thread context & cpu coupling to some degree.
>
> You might be surprised to find out that for some of our customers who 
> avail of the open interface to our metering data within the runtime 
> and & their scheduling code they actually make performance gains.
>
> In addition you should try to look at the big picture which goes 
> beyond a single process execution lifecycle. The metering data 
> produced by OpenCore leads to significant gains across releases 
> something which would never happen if one was thinking short-term and 
> (thread) local.
>
> For our customers having production quality data is the biggest 
> contributor to increases in quality of the code and its execution. I 
> can't recall a case in which the metering hotspot strategy produced a 
> red herring after a few iterations of a fully warmed up runtime.
>
> I would also like to point out that in two recent proof of concepts by 
> major fx trading platforms it was found that OpenCore's strategy based 
> metering runtime was the only realistic solution on the market for 
> production monitoring of applications with tx time intervals in the 
> microsecond range (approx 300). Hopefully we can eventually get these 
> reports published.
>
> There will always be exceptions to the rule but for those I doubt 
> there is any solution available today (we come as close as is possible 
> when used & configured appropriately). Please do not get me started on 
> call stack sampling.
>
> William
>
> On 03/08/2010 23:57, Gregg Wonderly wrote:
>> I know this seems pretty harmless, but for extremely light CPU use 
>> applications, even the use of the read/write lock on a thread local 
>> value can create a marked change in the contention points of an 
>> application.  I know that it can be useful information, but there are 
>> places where these change invert the real world places of interest 
>> and can cause any metered code to suddenly become the hot spot.
>>
>> Gregg Wonderly
>>
>> Ben Manes wrote:
>>> Most likely their using an inverted read/write lock approach where 
>>> the count is kept thread-local and aggregated by a monitoring 
>>> thread. I haven't looked at their implementation, but that's a 
>>> standard idiom for avoiding contention for capturing statistical 
>>> information.
>>>
>>> ------------------------------------------------------------------------ 
>>>
>>> *From:* Gregg Wonderly <gregg at cytetech.com>
>>> *To:* William Louth (JINSPIRED.COM) <william.louth at jinspired.com>
>>> *Cc:* concurrency-interest at cs.oswego.edu; gregg.wonderly at pobox.com
>>> *Sent:* Thu, July 29, 2010 12:02:04 PM
>>> *Subject:* Re: [concurrency-interest] Monitoring Tool
>>>
>>> Again, one of the big issues for me, is that typically, such 
>>> metering requires atomic counting, which, itself, is a form of 
>>> contention.  This contention can get injected into previously 
>>> uncontended blocks and redistribute the timing through particular 
>>> parts of the code which then don't reveal the actual behavior in 
>>> normal execution.
>>>
>>> Where can I see more information about how issues like this are 
>>> dealt with?
>>>
>>> Gregg Wonderly
>>>
>>> William Louth (JINSPIRED.COM <http://JINSPIRED.COM>) wrote:
>>> >  It can do this and more but maybe I should have factor in the 
>>> likelihood that you would have only skimmed over the links I email - 
>>> time is money. Sorry for not directly pointing this out but as the 
>>> architect of both the Probes and Metrics Open API's I thought this 
>>> would be rather obvious.
>>> >
>>>>  OpenCore supports many meters some of which are contention related: 
>>> http://opencore.jinspired.com/?page_id=981#p:core A lot of these are 
>>> not time related.
>>> >
>>> > OpenCore's Probes metering can be automatically mapped to Metrics 
>>> to correlate across multiple threads with other measurements not 
>>> metered based.
>>>>  http://opencore.jinspired.com/?page_id=377
>>> >
>>> > OpenCore supports the reporting of metering at thread and probes 
>>> level. You indicate transaction points to the runtime via config and 
>>> then see which particular probes (activities) and meters (resources: 
>>> monitors) contributed the most. See transaction probes provider 
>>> under Guides.
>>>>  http://opencore.jinspired.com/?page_id=772
>>> >
>>> > On 26/07/2010 23:56, Gregg Wonderly wrote:
>>> >> This doesn't really point at concurrency issues, only in 
>>> execution time of compute bound execution, and perhaps some simple 
>>> linear scaling of complexity.
>>> >>
>>> >> When you have 1000s of instructions and multiple code paths, 
>>> which include synchronization, and random latency injection, I am 
>>> not sure that you can see how threads are "waiting in line" until 
>>> you see the quadratic change in execution time that is typically 
>>> visible when high contention occurs.
>>> >>
>>> >> Maybe you can point out where this tool provides for the ability 
>>> to monitor lock contention and count stalled threads and otherwise 
>>> see the real contention that develops over time as load increases on 
>>> a highly contended code segment?
>>> >>
>>> >> If you could "time" the execution interval through all code 
>>> paths, and then look at the "percentage" of threads and time through 
>>> each path, and then see that the "largest" latency block was in a 
>>> very common code path, you could perhaps then say this was the area 
>>> to look at.  All of that analysis might not really be possible 
>>> though, because of the exponential potential complexity of code path 
>>> coverage.
>>> >>
>>> >> Gregg Wonderly
>>> >>
>>> >> William Louth (JINSPIRED.COM) wrote:
>>>> >>  JINSPIRED'S OpenCore (http://opencore.jinspired.com) metering & 
>>> metrics runtime has built in meters for thread contention metering 
>>> (activity based) @see blocking.time and blocking.count.
>>> >>>
>>>> >> You can easily extend it with your own custom counters or resource 
>>> measures mapped to meters. And you don't always need to measure 
>>> time: 
>>> http://williamlouth.wordpress.com/2010/06/11/no-latency-application-performance-analysis-when-wall-clock-time-is-simply-too-slow/ 
>>>
>>> >>>
>>>> >> Queuing can also be aggregated at various namespace levels: 
>>> http://williamlouth.wordpress.com/2010/05/20/metered-software-service-queues/ 
>>>
>>> >>>
>>>> >> More related articles: 
>>> http://williamlouth.wordpress.com/category/profiling/
>>> >>>
>>> >>> On 26/07/2010 21:13, Gregg Wonderly wrote:
>>> >>>> Per thread latency measurements with 1, 2, 10 and 100 threads 
>>> will often tell you a lot about how contention is affecting the 
>>> execution time.  When you get to 100, a thread dump will often 
>>> reveal where everyone is standing in line...
>>> >>>>
>>> >>>> Gregg Wonderly
>>> >>>>
>>> >>>> David Holmes wrote:
>>> >>>>> Kendall,
>>> >>>>>
>>> >>>>> In my opinion a monitoring tool looking at the lock 
>>> acquisition time, or CAS attempts, won't give you much insight into 
>>> whether to use a blocking or non-blocking approach. You need to 
>>> measure the performance of your application logic as a whole, 
>>> utilising the two different approaches. Afterall how can you compare 
>>> locking times with number of CAS attempts in general?
>>> >>>>>
>>> >>>>> David Holmes
>>> >>>>>
>>> >>>>>    -----Original Message-----
>>> >>>>>    *From:* concurrency-interest-bounces at cs.oswego.edu 
>>> <mailto:concurrency-interest-bounces at cs.oswego.edu>
>>> >>>>>    [mailto:concurrency-interest-bounces at cs.oswego.edu 
>>> <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On Behalf Of
>>> >>>>>    *Kendall Moore
>>> >>>>>    *Sent:* Sunday, 25 July 2010 4:14 PM
>>> >>>>>    *To:* concurrency-interest at cs.oswego.edu 
>>> <mailto:concurrency-interest at cs.oswego.edu>
>>> >>>>>    *Subject:* [concurrency-interest] Monitoring Tool
>>> >>>>>
>>> >>>>>    Greetings all,
>>> >>>>>
>>> >>>>>    Is there a common consensus on which monitoring tools are 
>>> best to
>>> >>>>>    use when writing parallel apps?  To be more specific, I 
>>> would like
>>> >>>>>    to be able to know how many times a given thread has to try 
>>> to CAS
>>> >>>>>    before succeeding.  Also, the ability to see how long a thread
>>> >>>>>    waits to acquire a lock would be useful as well.  The end goal
>>> >>>>>    would, in my particular case, would be to compare these in 
>>> order to
>>> >>>>>    determine if a non-blocking approach would be more 
>>> effective in a
>>> >>>>>    given situation than a lock-based approach.  Any help would 
>>> be much
>>> >>>>>    appreciated!
>>> >>>>>
>>> >>>>>    --    Kendall Moore
>>> >>>>>
>>> >>>>>
>>> >>>>> 
>>> ------------------------------------------------------------------------ 
>>>
>>> >>>>>
>>> >>>>> _______________________________________________
>>> >>>>> Concurrency-interest mailing list
>>> >>>>> Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>> >>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >>>>
>>> >>>> _______________________________________________
>>> >>>> Concurrency-interest mailing list
>>> >>>> Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> >>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >>>>
>>> >>>>
>>> >>> _______________________________________________
>>> >>> Concurrency-interest mailing list
>>> >>> Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >>>
>>> >>>
>>> >>
>>> >>
>>> >>
>>> >
>>> >
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> ------------------------------------------------------------------------ 
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From davidcholmes at aapt.net.au  Wed Aug  4 08:31:29 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 4 Aug 2010 22:31:29 +1000
Subject: [concurrency-interest] Threads should not be Cloneable
Message-ID: <NFBBKALFDCPFIDBNKAPCAEKFIHAA.davidcholmes@aapt.net.au>

Cloning a started thread makes no sense and is fundamentally broken at the
moment - you can get various weird things to occur. Cloning an unstarted
thread would be harmless but achieves nothing that a constructor doesn't
already achieve - and avoids icky issues like managing names and ids.

To close the door on this weirdness and remove any doubt as to whether
Threads should be cloneable or not, the core-libs folks in Oracle (plus
myself) are proposing that for Java 7 we simply specify that Thread.clone()
throws CloneNotSupportedException.

We expect that this would go unnoticed by 99.99999% of developers but wanted
to solicit comments in case we had overlooked an important use case (which
is doubtful considering it doesn't actually work :) ).

Thanks,
David Holmes
Java Platform Group
Oracle



From peter.kovacs.1.0rc at gmail.com  Sun Aug  8 08:04:17 2010
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Sun, 8 Aug 2010 14:04:17 +0200
Subject: [concurrency-interest] Asynchronous-nature of
	ConcurrentLinkedQueue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEHIIGAA.davidcholmes@aapt.net.au>
References: <4BF31481.4050204@cox.net>
	<NFBBKALFDCPFIDBNKAPCIEHIIGAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTik5f-riEgA5sEOsvHWXzbzGo3OTnOfatJ_zXeCq@mail.gmail.com>

David,

> The present code only penalises the misguided client that thinks they need
to know the size.

Is there a simple, abstract way to demonstrate one shouldn't need this
information? (But not too formal, please. :-) )

I have a scenario where I found useful knowing an approximate size of a
LinkedBlockingQueue:

A number of worker threads

   1. put an empty result-holder object on the LBQ,
   2. take their input from a producer,
   3. do some processing and
   4. put the result into the empty result-holder object.

The first two steps are executed in a synchronized block, so the order of
results on the LBQ reflects that of the corresponding inputs.

The LBQ is consumed by a single thread.

I found that

   1. having the worker wait in step#1, when the LBQ is full ( if
   (!lbq.offer(resultHolder)) { wait(); } )  and
   2. notifying workers waiting, when lbq.size() < maxLbqSize * 4 / 5

gives about 15% improvement in throughput -- as opposed to the algorithm,
where I let workers block in lbq.put(resultHolder).

My naive hypothesis to explain the gain is that having -- during *each
*lbq.take()
-- to constantly manage the threads which block in lbq.put() might be more
expensive than having workers wait and notifying them *less frequently*.

I've been having the haunting feeling that this queue-size-checking trick is
just a hack to compensate for some non-trivial design (or implementation)
flaw elsewhere. But I haven't been able to find a(n easy) way to either
prove or refute my hypothesis.

I'd be grateful if you could offer some thoughts on this.

Thanks
Peter

On Wed, May 19, 2010 at 12:35 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

> Greg,
>
> Gregg Wonderly writes:
> > Doug Lea wrote:
> > > On 05/18/10 08:03, Kai Meder wrote:
> > >> Hello
> > >>
> > >> reading the Java-Docs of ConcurrentLinkedQueue I wonder what the
> > >> "asynchronous nature" mentioned in the size()-doc is?
> > >>
> > >> "Beware that, unlike in most collections, this method is NOT a
> > >> constant-time operation. Because of the asynchronous nature of these
> > >> queues, determining the current number of elements requires an O(n)
> > >> traversal. "
> > >>
> > >
> > > Because insertion and removal operations can occur concurrently
> > > (even while you are asking about the size), you generally
> > > don't want to ask about the size (although isEmpty is usually
> > > still useful). But if you do ask, the queue
> > > will provide an answer by counting up the elements. The
> > > answer it returns might not have much bearing to the actual
> > > number of elements upon return of the method.
> >
> > And I guess I am always curious why there is no "counter"
> > associated with the queue length.  It would provide the
> > same "rough" estimate as the "traversal" without the
> >  repeated overhead would it not?
>
> Yes but at a cost to every queue operation to maintain that counter. This
> penalises the main queue operations to support another operation (size())
> that is for most intents and purposes completely useless.
>
> The present code only penalises the misguided client that thinks they need
> to know the size.
>
> David
>
> >
> > Gregg Wonderly
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100808/42707146/attachment.html>

From davidcholmes at aapt.net.au  Sun Aug  8 19:03:15 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 9 Aug 2010 09:03:15 +1000
Subject: [concurrency-interest] Asynchronous-nature
	ofConcurrentLinkedQueue
In-Reply-To: <AANLkTik5f-riEgA5sEOsvHWXzbzGo3OTnOfatJ_zXeCq@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEKNIHAA.davidcholmes@aapt.net.au>

Hi Peter,

I should have said "need to know the size for correctness". There are times when queries like this can be useful for performance heuristics as you describe. 

>From the usage you describe you seem to be supplanting the blocking-queue semantics with your own - in which case a customized wait/notify strategy may well perform better than the general one behind put/take. 

David Holmes 


 -----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of P?ter Kov?cs
Sent: Sunday, 8 August 2010 10:04 PM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Asynchronous-nature ofConcurrentLinkedQueue


  David,


  > The present code only penalises the misguided client that thinks they need
  to know the size.


  Is there a simple, abstract way to demonstrate one shouldn't need this information? (But not too formal, please. :-) )


  I have a scenario where I found useful knowing an approximate size of a LinkedBlockingQueue:


  A number of worker threads
    1.. put an empty result-holder object on the LBQ, 
    2.. take their input from a producer,
    3.. do some processing and
    4.. put the result into the empty result-holder object.
  The first two steps are executed in a synchronized block, so the order of results on the LBQ reflects that of the corresponding inputs.


  The LBQ is consumed by a single thread.


  I found that
    1.. having the worker wait in step#1, when the LBQ is full ( if (!lbq.offer(resultHolder)) { wait(); } )  and 
    2.. notifying workers waiting, when lbq.size() < maxLbqSize * 4 / 5
  gives about 15% improvement in throughput -- as opposed to the algorithm, where I let workers block in lbq.put(resultHolder).


  My naive hypothesis to explain the gain is that having -- during each lbq.take() -- to constantly manage the threads which block in lbq.put() might be more expensive than having workers wait and notifying them less frequently.


  I've been having the haunting feeling that this queue-size-checking trick is just a hack to compensate for some non-trivial design (or implementation) flaw elsewhere. But I haven't been able to find a(n easy) way to either prove or refute my hypothesis.


  I'd be grateful if you could offer some thoughts on this.


  Thanks
  Peter


  On Wed, May 19, 2010 at 12:35 AM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Greg,


    Gregg Wonderly writes:
    > Doug Lea wrote:
    > > On 05/18/10 08:03, Kai Meder wrote:
    > >> Hello
    > >>
    > >> reading the Java-Docs of ConcurrentLinkedQueue I wonder what the
    > >> "asynchronous nature" mentioned in the size()-doc is?
    > >>
    > >> "Beware that, unlike in most collections, this method is NOT a
    > >> constant-time operation. Because of the asynchronous nature of these
    > >> queues, determining the current number of elements requires an O(n)
    > >> traversal. "
    > >>
    > >
    > > Because insertion and removal operations can occur concurrently
    > > (even while you are asking about the size), you generally
    > > don't want to ask about the size (although isEmpty is usually
    > > still useful). But if you do ask, the queue
    > > will provide an answer by counting up the elements. The
    > > answer it returns might not have much bearing to the actual
    > > number of elements upon return of the method.
    >
    > And I guess I am always curious why there is no "counter"
    > associated with the queue length.  It would provide the
    > same "rough" estimate as the "traversal" without the
    >  repeated overhead would it not?


    Yes but at a cost to every queue operation to maintain that counter. This
    penalises the main queue operations to support another operation (size())
    that is for most intents and purposes completely useless.

    The present code only penalises the misguided client that thinks they need
    to know the size.

    David


    >
    > Gregg Wonderly
    > _______________________________________________
    > Concurrency-interest mailing list
    > Concurrency-interest at cs.oswego.edu
    > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100809/60f28bc3/attachment.html>

From djg at cs.washington.edu  Sun Aug  8 19:16:07 2010
From: djg at cs.washington.edu (Dan Grossman)
Date: Sun, 8 Aug 2010 16:16:07 -0700
Subject: [concurrency-interest] Sophomoric Introduction to Parallelism and
	Concurrency
Message-ID: <AANLkTinBQh=khPsgvOLvifsdrZUXwfFSRTTAKjqqmFtJ@mail.gmail.com>

Several months ago I posted a couple questions about using JSR166 with
students being exposed to threads, parallelism, and concurrency for
the first time.  Thanks again for all the help.  I wanted to "give
back" by advertising what I did and the materials I have put together
that I hope other educators (or students) can use.

Short version: Check out

   http://www.cs.washington.edu/homes/djg/teachingMaterials/

I would be grateful for your constructive feedback as well as
suggestions for other places to discuss/advertise this approach.

Slightly longer version:

As part of a major curriculum revision at the University Washington,
our required sophomore-level core data structures course now includes,
among many other more traditional topics, an introduction to threads,
parallelism, concurrency, and synchronization.  As the first
instructor, I "stood on the shoulders of giants" to distill what I
believed were the essential concepts into a format suitable for
students with modest experience with sequential programming in Java.
Finding no appropriate textbook, I developed my own materials,
including 60 pages of written explanation (something between lecture
notes and a first draft of something textbook-like), lecture slides,
homeworks, and a programming project.  I would love for others not to
have to re-invent these wheels.

A large part of this 3-week unit relies on the ForkJoin framework
(though porting to similar frameworks in other languages would be
simple).  I wrote my own notes for using the framework because I
believed the (otherwise high-quality) documentation for JSR166 would
not be a good match for classroom use.

Overall, I was very pleased with the entire experience -- and so were
the students.

--Dan

From peter.kovacs.1.0rc at gmail.com  Mon Aug  9 02:53:46 2010
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Mon, 9 Aug 2010 08:53:46 +0200
Subject: [concurrency-interest] Asynchronous-nature
	ofConcurrentLinkedQueue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEKNIHAA.davidcholmes@aapt.net.au>
References: <AANLkTik5f-riEgA5sEOsvHWXzbzGo3OTnOfatJ_zXeCq@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEKNIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTimPcmRP0pz+htdnU6wo68hUpzoqq2k=qY20C7OJ@mail.gmail.com>

Thank you, David!
Peter

2010/8/9 David Holmes <davidcholmes at aapt.net.au>

>  Hi Peter,
>
> I should have said "need to know the size for correctness". There are times
> when queries like this can be useful for performance heuristics as you
> describe.
>
> From the usage you describe you seem to be supplanting the blocking-queue
> semantics with your own - in which case a customized wait/notify strategy
> may well perform better than the general one behind put/take.
>
> David Holmes
>
>
>  -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *P?ter Kov?cs
> *Sent:* Sunday, 8 August 2010 10:04 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Asynchronous-nature
> ofConcurrentLinkedQueue
>
>  David,
>
> > The present code only penalises the misguided client that thinks they
> need
> to know the size.
>
> Is there a simple, abstract way to demonstrate one shouldn't need this
> information? (But not too formal, please. :-) )
>
> I have a scenario where I found useful knowing an approximate size of a
> LinkedBlockingQueue:
>
> A number of worker threads
>
>    1. put an empty result-holder object on the LBQ,
>    2. take their input from a producer,
>    3. do some processing and
>    4. put the result into the empty result-holder object.
>
> The first two steps are executed in a synchronized block, so the order of
> results on the LBQ reflects that of the corresponding inputs.
>
> The LBQ is consumed by a single thread.
>
> I found that
>
>    1. having the worker wait in step#1, when the LBQ is full ( if
>    (!lbq.offer(resultHolder)) { wait(); } )  and
>    2. notifying workers waiting, when lbq.size() < maxLbqSize * 4 / 5
>
> gives about 15% improvement in throughput -- as opposed to the algorithm,
> where I let workers block in lbq.put(resultHolder).
>
> My naive hypothesis to explain the gain is that having -- during *each *lbq.take()
> -- to constantly manage the threads which block in lbq.put() might be more
> expensive than having workers wait and notifying them *less frequently*.
>
> I've been having the haunting feeling that this queue-size-checking trick
> is just a hack to compensate for some non-trivial design (or implementation)
> flaw elsewhere. But I haven't been able to find a(n easy) way to either
> prove or refute my hypothesis.
>
> I'd be grateful if you could offer some thoughts on this.
>
> Thanks
> Peter
>
> On Wed, May 19, 2010 at 12:35 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>> Greg,
>>
>> Gregg Wonderly writes:
>> > Doug Lea wrote:
>> > > On 05/18/10 08:03, Kai Meder wrote:
>> > >> Hello
>> > >>
>> > >> reading the Java-Docs of ConcurrentLinkedQueue I wonder what the
>> > >> "asynchronous nature" mentioned in the size()-doc is?
>> > >>
>> > >> "Beware that, unlike in most collections, this method is NOT a
>> > >> constant-time operation. Because of the asynchronous nature of these
>> > >> queues, determining the current number of elements requires an O(n)
>> > >> traversal. "
>> > >>
>> > >
>> > > Because insertion and removal operations can occur concurrently
>> > > (even while you are asking about the size), you generally
>> > > don't want to ask about the size (although isEmpty is usually
>> > > still useful). But if you do ask, the queue
>> > > will provide an answer by counting up the elements. The
>> > > answer it returns might not have much bearing to the actual
>> > > number of elements upon return of the method.
>> >
>> > And I guess I am always curious why there is no "counter"
>> > associated with the queue length.  It would provide the
>> > same "rough" estimate as the "traversal" without the
>> >  repeated overhead would it not?
>>
>> Yes but at a cost to every queue operation to maintain that counter. This
>> penalises the main queue operations to support another operation (size())
>> that is for most intents and purposes completely useless.
>>
>> The present code only penalises the misguided client that thinks they need
>> to know the size.
>>
>> David
>>
>> >
>> > Gregg Wonderly
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100809/a20edb53/attachment-0001.html>

From ihkris at gmail.com  Mon Aug  9 03:18:49 2010
From: ihkris at gmail.com (HariKrishna)
Date: Mon, 9 Aug 2010 08:18:49 +0100
Subject: [concurrency-interest] Books, tutorials for beginers?
Message-ID: <AANLkTikSe1ZY=Ls9V1NLfqshcbc+-6W3GsgHzNBotQyh@mail.gmail.com>

Hi All,
    I am a new bie to Multhi Threading and concurrency. Kindly let me know
the best resources,tutorials,books,websites,groups that can help me.

-- 
Regards,
HariKrishna
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100809/43f7748d/attachment.html>

From davidcholmes at aapt.net.au  Mon Aug  9 03:32:42 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 9 Aug 2010 17:32:42 +1000
Subject: [concurrency-interest] Books, tutorials for beginers?
In-Reply-To: <AANLkTikSe1ZY=Ls9V1NLfqshcbc+-6W3GsgHzNBotQyh@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCELDIHAA.davidcholmes@aapt.net.au>

Totally biased but ...

If you have a good book on Java Programming I'd start with it's Threading
chapter(s) - eg "The Java Programming Language, 4th Edition".

Then to get really into it read "Java Concurrency in Practice". Then if
you're really into things "Concurrent Programming in Java".

For websites see:

http://jcip.net
http://gee.cs.oswego.edu/dl/

for starters.

By the way it is very impolite to spam people's personal email addresses
with the same request you put on the mailing list.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of HariKrishna
  Sent: Monday, 9 August 2010 5:19 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Books, tutorials for beginers?


  Hi All,
      I am a new bie to Multhi Threading and concurrency. Kindly let me know
the best resources,tutorials,books,websites,groups that can help me.

  --
  Regards,
  HariKrishna

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100809/da2f5aad/attachment.html>

From ihkris at gmail.com  Mon Aug  9 03:50:38 2010
From: ihkris at gmail.com (HariKrishna)
Date: Mon, 9 Aug 2010 08:50:38 +0100
Subject: [concurrency-interest] Books, tutorials for beginers?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCELDIHAA.davidcholmes@aapt.net.au>
References: <AANLkTikSe1ZY=Ls9V1NLfqshcbc+-6W3GsgHzNBotQyh@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCELDIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTiksLgWL2AXB-nYXM0RgrLbdoeJHnqHmao_sS_tz@mail.gmail.com>

Hi David,
    Thanks for mail. When i emailed to mailing list i didn't get any email
so i got misconception that the mail didn't go any where and i thought of
asking individuals. Apologies for personally asking.

Regards
HariKrishna

On Mon, Aug 9, 2010 at 8:32 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

>  Totally biased but ...
>
> If you have a good book on Java Programming I'd start with it's Threading
> chapter(s) - eg "The Java Programming Language, 4th Edition".
>
> Then to get really into it read "Java Concurrency in Practice". Then if
> you're really into things "Concurrent Programming in Java".
>
> For websites see:
>
> http://jcip.net
> http://gee.cs.oswego.edu/dl/
>
> for starters.
>
> By the way it is very impolite to spam people's personal email addresses
> with the same request you put on the mailing list.
>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *HariKrishna
> *Sent:* Monday, 9 August 2010 5:19 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Books, tutorials for beginers?
>
> Hi All,
>     I am a new bie to Multhi Threading and concurrency. Kindly let me know
> the best resources,tutorials,books,websites,groups that can help me.
>
> --
> Regards,
> HariKrishna
>
>


-- 
Regards,
HariKrishna
07404367902
02085524594
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100809/ddf97d48/attachment.html>

From kasper at kav.dk  Sun Aug 15 07:38:17 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Sun, 15 Aug 2010 13:38:17 +0200
Subject: [concurrency-interest] The need for a default ForkJoinPool
Message-ID: <4C67D1A9.4030606@kav.dk>

Hi,

Having used the FJ classes for a couple of libraries. The one thing that 
is really annoying me is the fact that there are no default ForkJoinPool 
I can use.

If you are designing a library a.la. a parallel ju.Arrays library that 
uses the FJ classes. You have two choices: either you can create a new 
ForkJoinPool and use it internally in your library. Or you can ask the 
user to provide one, something like sort(ForkJoinPool pool, int[] 
array). And I really don't like any of the alternatives.

The first one because I see more and more projects that uses 20-30 or 
even more different libraries. If they all need to create a ForkJoinPool 
internally it is going to get very messy with too many threads 
especially for processors with many cores. And it there is a problem now 
it is only going to get worse in the future.

The second one because it is just annoying for users that pass 
ForkJoinPools around all the time. Normal users do not want to get 
bugged down by details like this.

So i suggest a single static method on ForkJoinPool
ForkJoinPool defaultForkJoinPool();

Which returns a lazy initialized default ForkJoinPool that can be used 
across of different libraries/code. Maybe coupled with a security 
manager check before returning the ForkJoinPool

I also think it will make people more willing to use it in their 
libraries. Forcing people to create availableProcessors() threads, at 
least makes me think if I can solve the problem in any other way.

For those 5 % of users that needs something advanced they can still 
create their own ForkJoinPool.

Its probably not an optimal solution, but I think the alternative of not 
providing a default pool is much worse.

Cheers
   Kasper

From tim at peierls.net  Sun Aug 15 08:04:40 2010
From: tim at peierls.net (Tim Peierls)
Date: Sun, 15 Aug 2010 08:04:40 -0400
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C67D1A9.4030606@kav.dk>
References: <4C67D1A9.4030606@kav.dk>
Message-ID: <AANLkTimm7D_j9ZBYHJ8UanLTdj-DBoBUkPS77TjGO9mc@mail.gmail.com>

I disagree. There's a potential for a "tragedy of the commons" lurking in
the provision of a default thread pool. I think the problem is better solved
with dependency injection.

--tim

On Aug 15, 2010 7:41 AM, "Kasper Nielsen" <kasper at kav.dk> wrote:

Hi,

Having used the FJ classes for a couple of libraries. The one thing that is
really annoying me is the fact that there are no default ForkJoinPool I can
use.

If you are designing a library a.la. a parallel ju.Arrays library that uses
the FJ classes. You have two choices: either you can create a new
ForkJoinPool and use it internally in your library. Or you can ask the user
to provide one, something like sort(ForkJoinPool pool, int[] array). And I
really don't like any of the alternatives.

The first one because I see more and more projects that uses 20-30 or even
more different libraries. If they all need to create a ForkJoinPool
internally it is going to get very messy with too many threads especially
for processors with many cores. And it there is a problem now it is only
going to get worse in the future.

The second one because it is just annoying for users that pass ForkJoinPools
around all the time. Normal users do not want to get bugged down by details
like this.

So i suggest a single static method on ForkJoinPool
ForkJoinPool defaultForkJoinPool();

Which returns a lazy initialized default ForkJoinPool that can be used
across of different libraries/code. Maybe coupled with a security manager
check before returning the ForkJoinPool

I also think it will make people more willing to use it in their libraries.
Forcing people to create availableProcessors() threads, at least makes me
think if I can solve the problem in any other way.

For those 5 % of users that needs something advanced they can still create
their own ForkJoinPool.

Its probably not an optimal solution, but I think the alternative of not
providing a default pool is much worse.

Cheers
 Kasper
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100815/3f7b0f19/attachment.html>

From dl at cs.oswego.edu  Sun Aug 15 18:30:14 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 15 Aug 2010 18:30:14 -0400
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C67D1A9.4030606@kav.dk>
References: <4C67D1A9.4030606@kav.dk>
Message-ID: <4C686A76.8030604@cs.oswego.edu>

On 08/15/10 07:38, Kasper Nielsen wrote:
> Hi,
>
> Having used the FJ classes for a couple of libraries. The one thing that is
> really annoying me is the fact that there are no default ForkJoinPool I can use.
>

I think this will become a common concern. I'm not sure what
the best solution is though. In most ways, it makes sense
to have exactly one pool per program constructed with default
target parallelism (i.e., equal to #CPUs/cores). If you have
more than one, then they will eat up unnecessary resources
and will contend with each other. It also can make sense to
to occasionally use others with lower parallelism or different
parameters.

On the other hand, some methods (like shutdown) on a single global
pool could not be made accessible.

Also, as implied by Tim, having a single global pool invites
surprise and unhappiness when unknown other parts of a system
are abusing it -- for example running tasks that never terminate
or lock themselves up.

It might be reasonable to compromise along the lines you suggest
-- to have a default one (that we would need to support by somehow
disabling shutdown etc), so that people at least knowingly expose
themselves to the risks.

Other suggestions welcome.

-Doug

From davidcholmes at aapt.net.au  Sun Aug 15 19:50:18 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 16 Aug 2010 09:50:18 +1000
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C67D1A9.4030606@kav.dk>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEMHIHAA.davidcholmes@aapt.net.au>

Kasper Nielsen writes:
> Having used the FJ classes for a couple of libraries. The one thing that
> is really annoying me is the fact that there are no default ForkJoinPool
> I can use.
>
> If you are designing a library a.la. a parallel ju.Arrays library that
> uses the FJ classes. You have two choices: either you can create a new
> ForkJoinPool and use it internally in your library. Or you can ask the
> user to provide one, something like sort(ForkJoinPool pool, int[]
> array). And I really don't like any of the alternatives.

I think as a library provider you must do the latter. You can't assume that
you know how pools should be created or used, so all you can do is have the
user supply you with one. I understand this is somewhat awkward as you don't
necessarily want this on every method that is an entry-point to your
library; nor is it usual to have an explicit "init" method for a library,
that could be used to set this. But you need some technique for the library
user to "configure" your library with regard to the pool to be used. It may
also be that different parts of the application may want to use your library
with different underlying pools - again indicating the user must supply the
pool to be used somehow.

I do agree that there should be a default pool easily accessible to the
user - and created only on demand - but other libraries should only use that
default as a fall-back for when the user does not provide one, or uses the
library in a way that says "use the default pool for this" - the user has to
have the control here.

Cheers,
David Holmes

> The first one because I see more and more projects that uses 20-30 or
> even more different libraries. If they all need to create a ForkJoinPool
> internally it is going to get very messy with too many threads
> especially for processors with many cores. And it there is a problem now
> it is only going to get worse in the future.
>
> The second one because it is just annoying for users that pass
> ForkJoinPools around all the time. Normal users do not want to get
> bugged down by details like this.
>
> So i suggest a single static method on ForkJoinPool
> ForkJoinPool defaultForkJoinPool();
>
> Which returns a lazy initialized default ForkJoinPool that can be used
> across of different libraries/code. Maybe coupled with a security
> manager check before returning the ForkJoinPool
>
> I also think it will make people more willing to use it in their
> libraries. Forcing people to create availableProcessors() threads, at
> least makes me think if I can solve the problem in any other way.
>
> For those 5 % of users that needs something advanced they can still
> create their own ForkJoinPool.
>
> Its probably not an optimal solution, but I think the alternative of not
> providing a default pool is much worse.
>
> Cheers
>    Kasper
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Sun Aug 15 20:04:46 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 16 Aug 2010 10:04:46 +1000
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C686A76.8030604@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEMIIHAA.davidcholmes@aapt.net.au>

Hi Doug,

As a slight refinement on the default pool, I would suggest adding a way to
set the default pool. That way the "default default" can prohibit shutdown,
while a "custom default" would allow user-defined policies, including
allowing shutdown. Being able to set the default via a system property, in
addition to an explicit setXXX method, would provide more flexibility.

   volatile ForkJoinPool fjp;

   public void setDefaultPool(ForkJoinPool p) {
     fjp = p;
   }
   public static ForkJoinPool getDefaultPool() {
     ForkJoinPool def = fjp;
     if (def != null) return def;

     String fjpc = (String) System.getProperty("xxxx.ForkJoinPool.default");
     if (fjpc != null) {
        try {
            // spi-style reflective construction
            ...
            fjp = def;
            return def;
         } catch (...) { ...}
     }
     def = new ForkJoinPool() {
        public void shutdown() {
           // ignore or throw
        }
         ...
     };
     return def;
   }

Though this implies the custom default has its own means of determining the
desirable construction parameters.

Cheers,
David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug Lea
> Sent: Monday, 16 August 2010 8:30 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] The need for a default ForkJoinPool
>
>
> On 08/15/10 07:38, Kasper Nielsen wrote:
> > Hi,
> >
> > Having used the FJ classes for a couple of libraries. The one
> thing that is
> > really annoying me is the fact that there are no default
> ForkJoinPool I can use.
> >
>
> I think this will become a common concern. I'm not sure what
> the best solution is though. In most ways, it makes sense
> to have exactly one pool per program constructed with default
> target parallelism (i.e., equal to #CPUs/cores). If you have
> more than one, then they will eat up unnecessary resources
> and will contend with each other. It also can make sense to
> to occasionally use others with lower parallelism or different
> parameters.
>
> On the other hand, some methods (like shutdown) on a single global
> pool could not be made accessible.
>
> Also, as implied by Tim, having a single global pool invites
> surprise and unhappiness when unknown other parts of a system
> are abusing it -- for example running tasks that never terminate
> or lock themselves up.
>
> It might be reasonable to compromise along the lines you suggest
> -- to have a default one (that we would need to support by somehow
> disabling shutdown etc), so that people at least knowingly expose
> themselves to the risks.
>
> Other suggestions welcome.
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From djg at cs.washington.edu  Sun Aug 15 23:49:03 2010
From: djg at cs.washington.edu (Dan Grossman)
Date: Sun, 15 Aug 2010 20:49:03 -0700
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C686A76.8030604@cs.oswego.edu>
References: <4C67D1A9.4030606@kav.dk>
	<4C686A76.8030604@cs.oswego.edu>
Message-ID: <AANLkTin681CULdRAPxCfVtwCpLTJXPMk4PN6yyfw8T29@mail.gmail.com>

>From a beginner's pedagogy perspective, having a default pool is
tempting and none of the downsides seem to apply.  It would save a
couple lines of boilerplate, which is always nice.

But it doesn't seem like such a big deal.  I don't hink I told my
students what this "pool" was, just that it was necessary for
initializing the library and then calling into it.  Because some
libraries /do/ have a mandatory "init" function, living with the need
to create a pool doesn't seem any worse.

On Sun, Aug 15, 2010 at 3:30 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 08/15/10 07:38, Kasper Nielsen wrote:
>>
>> Hi,
>>
>> Having used the FJ classes for a couple of libraries. The one thing that
>> is
>> really annoying me is the fact that there are no default ForkJoinPool I
>> can use.
>>
>
> I think this will become a common concern. I'm not sure what
> the best solution is though. In most ways, it makes sense
> to have exactly one pool per program constructed with default
> target parallelism (i.e., equal to #CPUs/cores). If you have
> more than one, then they will eat up unnecessary resources
> and will contend with each other. It also can make sense to
> to occasionally use others with lower parallelism or different
> parameters.
>
> On the other hand, some methods (like shutdown) on a single global
> pool could not be made accessible.
>
> Also, as implied by Tim, having a single global pool invites
> surprise and unhappiness when unknown other parts of a system
> are abusing it -- for example running tasks that never terminate
> or lock themselves up.
>
> It might be reasonable to compromise along the lines you suggest
> -- to have a default one (that we would need to support by somehow
> disabling shutdown etc), so that people at least knowingly expose
> themselves to the risks.
>
> Other suggestions welcome.
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From kasper at kav.dk  Mon Aug 16 04:50:12 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Mon, 16 Aug 2010 10:50:12 +0200
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C686A76.8030604@cs.oswego.edu>
References: <4C67D1A9.4030606@kav.dk> <4C686A76.8030604@cs.oswego.edu>
Message-ID: <4C68FBC4.2050903@kav.dk>

On 16/08/10 00.30, Doug Lea wrote:
>
> On the other hand, some methods (like shutdown) on a single global
> pool could not be made accessible.
>
> Also, as implied by Tim, having a single global pool invites
> surprise and unhappiness when unknown other parts of a system
> are abusing it -- for example running tasks that never terminate
> or lock themselves up.

Im not to worried about this. Is somebody going to mess up with a single 
global pool. Of course, but there will always be badly written code that 
locks up the system, leaks memory etc. And I don't except problems with 
misuse of a single threadpool will be too hard to track down. A simple 
profiler should do. IMHO things such as memory leaks/classloading issues 
are 10 times more difficult to track down.

Besides it is not like we do not already have en single global 
threadpool in Java. A single instance 
AsynchronousChannelProvider.provider() was introduced via JSR 203. The 
class wraps an Executor and ScheduledExecutorService internally.

And other platforms also share single threadpools. .NET Parallel 
Extensions/PLINQ uses a single (pr CLR instance) shared threadpool for 
their Parallel.Invoke/Parallel.For/... constructs. OSXs Grand Central 
Dispatch uses a single system wide thread-pool. And i expect that 
languages such as Scala/Groovy/X10 also has a single shared threadpool 
for their various parallel constructs.



> It might be reasonable to compromise along the lines you suggest
> -- to have a default one (that we would need to support by somehow
> disabling shutdown etc), so that people at least knowingly expose
> themselves to the risks.
>
We might also want disable the various setXXX methods of ForkJoinPool or 
least put some security checks in there.


Cheers
   Kasper

From kasper at kav.dk  Mon Aug 16 05:15:45 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Mon, 16 Aug 2010 11:15:45 +0200
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEMHIHAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCEMHIHAA.davidcholmes@aapt.net.au>
Message-ID: <4C6901C1.5000003@kav.dk>

On 16/08/10 01.50, David Holmes wrote:
> Kasper Nielsen writes:
>> Having used the FJ classes for a couple of libraries. The one thing that
>> is really annoying me is the fact that there are no default ForkJoinPool
>> I can use.
>>
>> If you are designing a library a.la. a parallel ju.Arrays library that
>> uses the FJ classes. You have two choices: either you can create a new
>> ForkJoinPool and use it internally in your library. Or you can ask the
>> user to provide one, something like sort(ForkJoinPool pool, int[]
>> array). And I really don't like any of the alternatives.
>
> I think as a library provider you must do the latter. You can't assume that
> you know how pools should be created or used, so all you can do is have the
> user supply you with one. I understand this is somewhat awkward as you don't
> necessarily want this on every method that is an entry-point to your
> library; nor is it usual to have an explicit "init" method for a library,
> that could be used to set this. But you need some technique for the library
> user to "configure" your library with regard to the pool to be used. It may
> also be that different parts of the application may want to use your library
> with different underlying pools - again indicating the user must supply the
> pool to be used somehow.
>
> I do agree that there should be a default pool easily accessible to the
> user - and created only on demand - but other libraries should only use that
> default as a fall-back for when the user does not provide one, or uses the
> library in a way that says "use the default pool for this" - the user has to
> have the control here.
>
> Cheers,
> David Holmes

I agree, for my particular use I have a setForkJoinPool method. I just
suspect that 95 % of users will never use it. And I really don't want to 
make my library more complex then it already is, by requiring them to 
always set one.

Cheers,
  Kasper

From kasper at kav.dk  Mon Aug 16 05:33:56 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Mon, 16 Aug 2010 11:33:56 +0200
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C67D1A9.4030606@kav.dk>
References: <4C67D1A9.4030606@kav.dk>
Message-ID: <4C690604.7070703@kav.dk>

Im just throwing this into the discussion, it might be really bad idea. 
Haven't thought it through completely.

This is primarily thought to be a way for applications servers such as 
Tomcat/Weblogic/Websphere to have better control over which threadpools 
should be used in case we decide on a single system-wide threadpool.

Again i really want to just expose a simple api such as 
parallelSort(int[] array) to users. Now, for example, when I'm using 
Weblogic i often use its build-in functionality for prioritizing 
requests. So request from user A uses high priority Threadpool 1, and 
requests from user B uses low priority Threadpool 2. If both users 
requires calls to parallelSort they effectively have the same priority 
because they both use the single shared threadpool.

Something like this would allow complete control to containers.

public class ForkJoins {

     private final static ThreadLocal<SoftReference<ForkJoinPool>> 
FJP_THREAD_DEFAULT = new ThreadLocal<SoftReference<ForkJoinPool>>();
     private final static ForkJoinPool DEFAULT = new ForkJoinPool();// 
not lazy for simplicity issues

     public static ForkJoinPool get() {
         SoftReference<ForkJoinPool> sr = FJP_THREAD_DEFAULT.get();
         if (sr != null) {
             ForkJoinPool fjp = sr.get();
             return fjp != null ? fjp : DEFAULT;
         }
         return DEFAULT;
     }

     public static void setThreadDefault(ForkJoinPool fjp) {
         //Security checks
         FJP_THREAD_DEFAULT.set(new SoftReference<ForkJoinPool>(fjp));
     }
}

Cheers
   Kasper

From dl at cs.oswego.edu  Mon Aug 16 11:08:56 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 16 Aug 2010 11:08:56 -0400
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C68FBC4.2050903@kav.dk>
References: <4C67D1A9.4030606@kav.dk> <4C686A76.8030604@cs.oswego.edu>
	<4C68FBC4.2050903@kav.dk>
Message-ID: <4C695488.9010601@cs.oswego.edu>


Even if we don't establish a JDK-wide default ForkJoinPool,
I now see that, minimally, ForkJoinPool should be a better
citizen when used by others as a global default, by (very) slowly
shedding resources (mainly worker threads) when it is not used.
So expect a CVS update very soon that does this. While this
might not be a good final answer, it enables the choice to
establish global defaults at some point to be a matter of policy,
not implementation.

-Doug


From brian at briangoetz.com  Mon Aug 16 12:39:15 2010
From: brian at briangoetz.com (Brian Goetz)
Date: Mon, 16 Aug 2010 12:39:15 -0400
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C686A76.8030604@cs.oswego.edu>
References: <4C67D1A9.4030606@kav.dk> <4C686A76.8030604@cs.oswego.edu>
Message-ID: <4C6969B3.6050708@briangoetz.com>

And this is going to become a far more widespread problem as we start to put 
parallel collections in the class library, which we want to do.  It seems an 
absurd abstraction leak that before you can construct a parallel collection, 
you have to instantiate some thread-pool thingy.  (Of course, if you want more 
control, by all means make your own pool and the libraries should be able to 
use that.)

Pool pollution is a valid concern, but note that the concern doesn't go away 
just by making more threads.  If someone drops a runaway task in a pool, 
doesn't matter what pool it is -- contention for cycles just went up.

On 8/15/2010 6:30 PM, Doug Lea wrote:
> On 08/15/10 07:38, Kasper Nielsen wrote:
>> Hi,
>>
>> Having used the FJ classes for a couple of libraries. The one thing
>> that is
>> really annoying me is the fact that there are no default ForkJoinPool
>> I can use.
>>
>
> I think this will become a common concern. I'm not sure what
> the best solution is though. In most ways, it makes sense
> to have exactly one pool per program constructed with default
> target parallelism (i.e., equal to #CPUs/cores). If you have
> more than one, then they will eat up unnecessary resources
> and will contend with each other. It also can make sense to
> to occasionally use others with lower parallelism or different
> parameters.
>
> On the other hand, some methods (like shutdown) on a single global
> pool could not be made accessible.
>
> Also, as implied by Tim, having a single global pool invites
> surprise and unhappiness when unknown other parts of a system
> are abusing it -- for example running tasks that never terminate
> or lock themselves up.
>
> It might be reasonable to compromise along the lines you suggest
> -- to have a default one (that we would need to support by somehow
> disabling shutdown etc), so that people at least knowingly expose
> themselves to the risks.
>
> Other suggestions welcome.
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From gregg at cytetech.com  Mon Aug 16 13:32:12 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 16 Aug 2010 12:32:12 -0500
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C690604.7070703@kav.dk>
References: <4C67D1A9.4030606@kav.dk> <4C690604.7070703@kav.dk>
Message-ID: <4C69761C.9040301@cytetech.com>

It seems like there should be a default to use which falls out of the context of 
the executing thread too.  Using a ThreadLocal, as here, helps to keep previous 
'knowledge" of the pool intact for subsequent uses without all the burden of 
maintaining all the context necessary for every point of execution to know what 
the previous did.

Gregg Wonderly

Kasper Nielsen wrote:
> Im just throwing this into the discussion, it might be really bad idea. 
> Haven't thought it through completely.
> 
> This is primarily thought to be a way for applications servers such as 
> Tomcat/Weblogic/Websphere to have better control over which threadpools 
> should be used in case we decide on a single system-wide threadpool.
> 
> Again i really want to just expose a simple api such as 
> parallelSort(int[] array) to users. Now, for example, when I'm using 
> Weblogic i often use its build-in functionality for prioritizing 
> requests. So request from user A uses high priority Threadpool 1, and 
> requests from user B uses low priority Threadpool 2. If both users 
> requires calls to parallelSort they effectively have the same priority 
> because they both use the single shared threadpool.
> 
> Something like this would allow complete control to containers.
> 
> public class ForkJoins {
> 
>     private final static ThreadLocal<SoftReference<ForkJoinPool>> 
> FJP_THREAD_DEFAULT = new ThreadLocal<SoftReference<ForkJoinPool>>();
>     private final static ForkJoinPool DEFAULT = new ForkJoinPool();// 
> not lazy for simplicity issues
> 
>     public static ForkJoinPool get() {
>         SoftReference<ForkJoinPool> sr = FJP_THREAD_DEFAULT.get();
>         if (sr != null) {
>             ForkJoinPool fjp = sr.get();
>             return fjp != null ? fjp : DEFAULT;
>         }
>         return DEFAULT;
>     }
> 
>     public static void setThreadDefault(ForkJoinPool fjp) {
>         //Security checks
>         FJP_THREAD_DEFAULT.set(new SoftReference<ForkJoinPool>(fjp));
>     }
> }
> 
> Cheers
>   Kasper
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From joe.bowbeer at gmail.com  Mon Aug 16 14:56:12 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 16 Aug 2010 11:56:12 -0700
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C68FBC4.2050903@kav.dk>
References: <4C67D1A9.4030606@kav.dk> <4C686A76.8030604@cs.oswego.edu>
	<4C68FBC4.2050903@kav.dk>
Message-ID: <AANLkTik-kUR+CJ0KsZF5d34+Lxw0gKUvEHvfnew-tKko@mail.gmail.com>

Other background services that are managed by default:

SwingWorkers share a single executor (with no option to configure another
one, unfortunately)

RMI & gc

For things like SwingWorker, which is a fundamental service in some contexts
and a layered service in other contexts, I would like to have a convenient
default available, but I would also like the ability to configure my own.

Joe

On Mon, Aug 16, 2010 at 1:50 AM, Kasper Nielsen wrote:

> On 16/08/10 00.30, Doug Lea wrote:
>
>>
>> On the other hand, some methods (like shutdown) on a single global
>> pool could not be made accessible.
>>
>> Also, as implied by Tim, having a single global pool invites
>> surprise and unhappiness when unknown other parts of a system
>> are abusing it -- for example running tasks that never terminate
>> or lock themselves up.
>>
>
> Im not to worried about this. Is somebody going to mess up with a single
> global pool. Of course, but there will always be badly written code that
> locks up the system, leaks memory etc. And I don't except problems with
> misuse of a single threadpool will be too hard to track down. A simple
> profiler should do. IMHO things such as memory leaks/classloading issues are
> 10 times more difficult to track down.
>
> Besides it is not like we do not already have en single global threadpool
> in Java. A single instance AsynchronousChannelProvider.provider() was
> introduced via JSR 203. The class wraps an Executor and
> ScheduledExecutorService internally.
>
> And other platforms also share single threadpools. .NET Parallel
> Extensions/PLINQ uses a single (pr CLR instance) shared threadpool for their
> Parallel.Invoke/Parallel.For/... constructs. OSXs Grand Central Dispatch
> uses a single system wide thread-pool. And i expect that languages such as
> Scala/Groovy/X10 also has a single shared threadpool for their various
> parallel constructs.
>
>
>
>
>  It might be reasonable to compromise along the lines you suggest
>> -- to have a default one (that we would need to support by somehow
>> disabling shutdown etc), so that people at least knowingly expose
>> themselves to the risks.
>>
>>  We might also want disable the various setXXX methods of ForkJoinPool or
> least put some security checks in there.
>
>
> Cheers
>  Kasper
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100816/4f24a773/attachment.html>

From davidcholmes at aapt.net.au  Mon Aug 16 18:53:10 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 17 Aug 2010 08:53:10 +1000
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C69761C.9040301@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMENAIHAA.davidcholmes@aapt.net.au>

If threads explicitly attached and detached from libraries then a
Threadlocal might be appropriate, but in general I think it is a very bad
mechanism because the right pool is a function of the library not of the
thread using it!

I think libraries must allow for a default pool while also supporting an
explicit pool.

I think what differentiates this case from other global "thread pools" is
that the scope of usage is potentially much broader and that the pool will
not necessarily grow as demand increases.

I also think, and perhaps it is just my misunderstanding here, that having
multiple simultaneous users of a FJPool is going to be counter-productive. I
can get good speedup doing a parallel sort of one huge array, but if I try
to sort two huge arrays at the same time then I'm just stepping on my own
toes. If a pool processes one task then all stealing aids that task; with
multiple tasks you really have no idea how long each one will take.

Which would perform better: one pool of N thread processing 2 arrays, or 2
pools of N/2 threads processing 1 array each?

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg
> Wonderly
> Sent: Tuesday, 17 August 2010 3:32 AM
> To: Kasper Nielsen
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] The need for a default ForkJoinPool
>
>
> It seems like there should be a default to use which falls out of
> the context of
> the executing thread too.  Using a ThreadLocal, as here, helps to
> keep previous
> 'knowledge" of the pool intact for subsequent uses without all
> the burden of
> maintaining all the context necessary for every point of
> execution to know what
> the previous did.
>
> Gregg Wonderly
>
> Kasper Nielsen wrote:
> > Im just throwing this into the discussion, it might be really bad idea.
> > Haven't thought it through completely.
> >
> > This is primarily thought to be a way for applications servers such as
> > Tomcat/Weblogic/Websphere to have better control over which threadpools
> > should be used in case we decide on a single system-wide threadpool.
> >
> > Again i really want to just expose a simple api such as
> > parallelSort(int[] array) to users. Now, for example, when I'm using
> > Weblogic i often use its build-in functionality for prioritizing
> > requests. So request from user A uses high priority Threadpool 1, and
> > requests from user B uses low priority Threadpool 2. If both users
> > requires calls to parallelSort they effectively have the same priority
> > because they both use the single shared threadpool.
> >
> > Something like this would allow complete control to containers.
> >
> > public class ForkJoins {
> >
> >     private final static ThreadLocal<SoftReference<ForkJoinPool>>
> > FJP_THREAD_DEFAULT = new ThreadLocal<SoftReference<ForkJoinPool>>();
> >     private final static ForkJoinPool DEFAULT = new ForkJoinPool();//
> > not lazy for simplicity issues
> >
> >     public static ForkJoinPool get() {
> >         SoftReference<ForkJoinPool> sr = FJP_THREAD_DEFAULT.get();
> >         if (sr != null) {
> >             ForkJoinPool fjp = sr.get();
> >             return fjp != null ? fjp : DEFAULT;
> >         }
> >         return DEFAULT;
> >     }
> >
> >     public static void setThreadDefault(ForkJoinPool fjp) {
> >         //Security checks
> >         FJP_THREAD_DEFAULT.set(new SoftReference<ForkJoinPool>(fjp));
> >     }
> > }
> >
> > Cheers
> >   Kasper
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jason_mehrens at hotmail.com  Mon Aug 16 22:23:48 2010
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Mon, 16 Aug 2010 21:23:48 -0500
Subject: [concurrency-interest] Reference/IdentityMap toString
Message-ID: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>


 
Should the string representation of a map (entries and collection views) describe strictly just the contents or express the equivalence relationship of how the map views its entries? Specificity, since the CustomConcurrentHashMap can be created with an IDENTITY equivalence or EQUALS equivalence it would seem to me that one natural way to express this difference would be to have the map create an identityToString for a key or value when using an IDENTITY equivalence. Is it worth adding another method to CCHM.Equivalence to support custom strings?


Regards,
 

Jason Mehrens

 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100816/0daa3705/attachment.html>

From gregg at cytetech.com  Tue Aug 17 16:47:22 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 17 Aug 2010 15:47:22 -0500
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMENAIHAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMENAIHAA.davidcholmes@aapt.net.au>
Message-ID: <4C6AF55A.50905@cytetech.com>

David Holmes wrote:
> If threads explicitly attached and detached from libraries then a
> Threadlocal might be appropriate, but in general I think it is a very bad
> mechanism because the right pool is a function of the library not of the
> thread using it!

Work dispatched directly from a FJTask, or otherwise with explicit knowledge of 
the pool it is running in, has no problem staying inside of the environment it 
was launched into.

The original question was about how to generalize code to not have explicit 
knowledge, but rather implicit knowledge of what context (FJPool) it might use.

A Thread of execution represents a code path which ultimately can end up running 
through unknown software libraries.  Container architectures such as JEE, JME, 
Applet and others have ended up wrapping all the context into some instance of a 
class/interface which provides the global view which can help compartmentalize 
environment of execution so that there can be very clean separation between 
"code" and "environment".

Having to "pass" a FJPool around presupposes that this is the only method of 
concurrency control that will be applicable, and it means that you have to know 
ahead of time that one is needed, and that, ultimately is not possible.

So, establishing an out of band mechanism for selecting a pool, would provide 
much easier deployment control.  DI can be a useful mechanism, and I'm all for 
using that technique where possible.  But, DI comes in multiple flavors, and 
ones own choice may not mesh with, or interact well with the mechanisms of another.

So, I'm still interested in what might be possible to have FJ provide some level 
of out of band knowledge.  I'm thinking about something like the following in 
FJPool.

public static FJPool getThreadsPool() {
	for( FJPool pool : allPools.elements() ) {
		if( pool.contains( Thread.currentThread() ) != null ) {
			return pool;
		}
	}
}

This would look through active instances to see if the current thread is 
dispatched from that pool.  If it is, then return the pool.  This would then 
allow a pretty low level in the software structure to find the environment it 
might use for dispatching work, without having to pass around such details 
through countless APIs.

> I think libraries must allow for a default pool while also supporting an
> explicit pool.
> 
> I think what differentiates this case from other global "thread pools" is
> that the scope of usage is potentially much broader and that the pool will
> not necessarily grow as demand increases.

Scaling is always a challenging detail.  But, providing some explicit 
controlling effects, by disallowing a low level flow of execution to "raise" the 
performance of it's work, might be a very desirable thing.

For example, it might make sense to add Permission checks to FJPool creation to 
limit thread pool creation.  With the method above, a thread of execution could 
then discover what environment it should submit additional work to.  This would 
help with DOS activities on mobile code applications by allowing them only 
marginal opportunity to abuse the machine with thread creation.

> I also think, and perhaps it is just my misunderstanding here, that having
> multiple simultaneous users of a FJPool is going to be counter-productive. I
> can get good speedup doing a parallel sort of one huge array, but if I try
> to sort two huge arrays at the same time then I'm just stepping on my own
> toes. If a pool processes one task then all stealing aids that task; with
> multiple tasks you really have no idea how long each one will take.
> 
> Which would perform better: one pool of N thread processing 2 arrays, or 2
> pools of N/2 threads processing 1 array each?

I don't live in a world of "fixed work loads".  Everything I do involves network 
oriented distributed systems with inter-machine, remotely dispatched activities. 
  I need ways to confine such applications to limited thread resources more than 
I need to provide an ultimate sorting engine.

Not everything done in such systems have short running threads of execution with 
out delays/pauses.  So, I still have to manage some threading explicitly.  But 
for certain applications and work, FJPool is a big help, and I, like Kasper, am 
trying to figure out a good "container view" for my applications which would 
make passing of the FJPool a lot less necessary.

Gregg Wonderly

> David Holmes
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg
>> Wonderly
>> Sent: Tuesday, 17 August 2010 3:32 AM
>> To: Kasper Nielsen
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] The need for a default ForkJoinPool
>>
>>
>> It seems like there should be a default to use which falls out of
>> the context of
>> the executing thread too.  Using a ThreadLocal, as here, helps to
>> keep previous
>> 'knowledge" of the pool intact for subsequent uses without all
>> the burden of
>> maintaining all the context necessary for every point of
>> execution to know what
>> the previous did.
>>
>> Gregg Wonderly
>>
>> Kasper Nielsen wrote:
>>> Im just throwing this into the discussion, it might be really bad idea.
>>> Haven't thought it through completely.
>>>
>>> This is primarily thought to be a way for applications servers such as
>>> Tomcat/Weblogic/Websphere to have better control over which threadpools
>>> should be used in case we decide on a single system-wide threadpool.
>>>
>>> Again i really want to just expose a simple api such as
>>> parallelSort(int[] array) to users. Now, for example, when I'm using
>>> Weblogic i often use its build-in functionality for prioritizing
>>> requests. So request from user A uses high priority Threadpool 1, and
>>> requests from user B uses low priority Threadpool 2. If both users
>>> requires calls to parallelSort they effectively have the same priority
>>> because they both use the single shared threadpool.
>>>
>>> Something like this would allow complete control to containers.
>>>
>>> public class ForkJoins {
>>>
>>>     private final static ThreadLocal<SoftReference<ForkJoinPool>>
>>> FJP_THREAD_DEFAULT = new ThreadLocal<SoftReference<ForkJoinPool>>();
>>>     private final static ForkJoinPool DEFAULT = new ForkJoinPool();//
>>> not lazy for simplicity issues
>>>
>>>     public static ForkJoinPool get() {
>>>         SoftReference<ForkJoinPool> sr = FJP_THREAD_DEFAULT.get();
>>>         if (sr != null) {
>>>             ForkJoinPool fjp = sr.get();
>>>             return fjp != null ? fjp : DEFAULT;
>>>         }
>>>         return DEFAULT;
>>>     }
>>>
>>>     public static void setThreadDefault(ForkJoinPool fjp) {
>>>         //Security checks
>>>         FJP_THREAD_DEFAULT.set(new SoftReference<ForkJoinPool>(fjp));
>>>     }
>>> }
>>>
>>> Cheers
>>>   Kasper
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From kasper at kav.dk  Tue Aug 17 16:51:07 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Tue, 17 Aug 2010 22:51:07 +0200
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMENAIHAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMENAIHAA.davidcholmes@aapt.net.au>
Message-ID: <4C6AF63B.3040404@kav.dk>

On 17/08/10 00.53, David Holmes wrote:
> If threads explicitly attached and detached from libraries then a
> Threadlocal might be appropriate, but in general I think it is a very bad
> mechanism because the right pool is a function of the library not of the
> thread using it!
>
I disagree, the right pool depends on the context you use the library in 
not on the library it self. At least when in comes to ForkJoinPools 
because all tasks are supposed to be CPU bound. So if you processing 
your request in a thread with a high priority, you also want to be able 
to sort you big array with a high priority.
Likewise while running with a thread with a low priority you don't
want to block high priority requests because your library uses the same 
pool for all your requests.

Thats why I think something like
Thread.currentThread().getForkJoinPool();
would be appropiate

>
> I think what differentiates this case from other global "thread pools" is
> that the scope of usage is potentially much broader and that the pool will
> not necessarily grow as demand increases.
>
> I also think, and perhaps it is just my misunderstanding here, that having
> multiple simultaneous users of a FJPool is going to be counter-productive. I
> can get good speedup doing a parallel sort of one huge array, but if I try
> to sort two huge arrays at the same time then I'm just stepping on my own
> toes. If a pool processes one task then all stealing aids that task; with
> multiple tasks you really have no idea how long each one will take.
>

Normally you will just have a single FJPool in your system. Ideally a 
single FJPool with NumberOfProccessors threads should be adequate for 
most programs. Job submissions are queued up FIFO wise internally.

> Which would perform better: one pool of N thread processing 2 arrays, or 2
> pools of N/2 threads processing 1 array each?
The first one. Say you have 2 pools and the first pool sorts the array 
real quick because it is easier to sort (smaller/ semisorted/...). The 
second pool has a more work to do while sorting its array. But because 
you are using two pool the threads from the first pool are unable to 
steal tasks from the second pool and you are basically only using half 
of your CPUS/cores.


Cheers
   Kasper

From kasper at kav.dk  Tue Aug 17 17:11:07 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Tue, 17 Aug 2010 23:11:07 +0200
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C6AF55A.50905@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCMENAIHAA.davidcholmes@aapt.net.au>
	<4C6AF55A.50905@cytetech.com>
Message-ID: <4C6AFAEB.1030200@kav.dk>

On 17/08/10 22.47, Gregg Wonderly wrote:
>
> So, I'm still interested in what might be possible to have FJ provide
> some level of out of band knowledge. I'm thinking about something like
> the following in FJPool.
>
> public static FJPool getThreadsPool() {
> for( FJPool pool : allPools.elements() ) {
> if( pool.contains( Thread.currentThread() ) != null ) {
> return pool;
> }
> }
> }
>
> This would look through active instances to see if the current thread is
> dispatched from that pool. If it is, then return the pool. This would
> then allow a pretty low level in the software structure to find the
> environment it might use for dispatching work, without having to pass
> around such details through countless APIs.

Gregg,

I'm not sure I understand what you are trying to do.
If the current thread is dispatch from a FJPool you can only be running 
a ForkJoinTask in which case the pool that is executing the task can be 
obtained via a call to static ForkJoinPool ForkJoinTask.getPool();


Cheers
  Kasper

From davidcholmes at aapt.net.au  Tue Aug 17 18:15:35 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 18 Aug 2010 08:15:35 +1000
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C6AF63B.3040404@kav.dk>
Message-ID: <NFBBKALFDCPFIDBNKAPCIENDIHAA.davidcholmes@aapt.net.au>

Kasper Nielsen writes:
> On 17/08/10 00.53, David Holmes wrote:
> > If threads explicitly attached and detached from libraries then a
> > Threadlocal might be appropriate, but in general I think it is
> a very bad
> > mechanism because the right pool is a function of the library not of the
> > thread using it!
> >
> I disagree, the right pool depends on the context you use the library in
> not on the library it self.

Not if the library is trying to encapsulate the pool the way some libs
currently encapsulate executors. It does indeed depend on context, and my
point is that thread identity alone may not be sufficient context to make a
choice. (And if people will be "overwhelmed" by passing around pools,
imagine how they will feel trying to figure out ThreadLocals!).

> At least when in comes to ForkJoinPools
> because all tasks are supposed to be CPU bound. So if you processing
> your request in a thread with a high priority, you also want to be able
> to sort you big array with a high priority.
> Likewise while running with a thread with a low priority you don't
> want to block high priority requests because your library uses the same
> pool for all your requests.
> Thats why I think something like
> Thread.currentThread().getForkJoinPool();
> would be appropiate

That is one context and obviously in this context you are making a property
of the thread the determining factor. But expand that a little and suppose
that some actions of a "low priority" thread can actually be high-priority,
so that task may need a different pool - now thread identity no longer
serves.

I do not see a way to generalize the mechanism by which you can have library
method like getForkJoinPool()  always return the "right" pool.

> >
> > I think what differentiates this case from other global "thread
> > pools" is that the scope of usage is potentially much broader
> > and that the pool will not necessarily grow as demand increases.
> >
> > I also think, and perhaps it is just my misunderstanding here,
> > that having multiple simultaneous users of a FJPool is going
> > to be counter-productive. I can get good speedup doing a parallel
> > sort of one huge array, but if I try to sort two huge arrays at
> > the same time then I'm just stepping on my own toes. If a pool
> > processes one task then all stealing aids that task; with
> > multiple tasks you really have no idea how long each one will take.
> >
>
> Normally you will just have a single FJPool in your system. Ideally a
> single FJPool with NumberOfProccessors threads should be adequate for
> most programs. Job submissions are queued up FIFO wise internally.

But you just said you wanted different pools for different priority tasks ?

I don't agree we only need one FJPool per system - it depends on how it is
used. It depends on whether it needs to encapsulate something inherent to
the particular library involved. It depends on how it performs for a given
workload.

> > Which would perform better: one pool of N thread processing 2
> > arrays, or 2 pools of N/2 threads processing 1 array each?
> The first one. Say you have 2 pools and the first pool sorts the array
> real quick because it is easier to sort (smaller/ semisorted/...). The
> second pool has a more work to do while sorting its array. But because
> you are using two pool the threads from the first pool are unable to
> steal tasks from the second pool and you are basically only using half
> of your CPUS/cores.

I was asking in the context of equal workloads. Also performance here is
relative - the "system" may perform better in one context or the other, but
what about individual threads? What if you are concerned about response
times as well as system throughput?

I do not see a one size fits all solution here, or even a one-size fits
many. There is so much context dependency here, the choices about how pools
are created and how they are used, has to come from a knowledgeable entity
in the "application".

Cheers,
David

> Cheers
>    Kasper
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Wed Aug 18 08:35:07 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 18 Aug 2010 08:35:07 -0400
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIENDIHAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCIENDIHAA.davidcholmes@aapt.net.au>
Message-ID: <4C6BD37B.5080402@cs.oswego.edu>

A few notes on this:

ForkJoinPool is entirely geared to maximizing throughput,
not responsiveness/latency/fairness. In default configuration,
it tries to fully employ/saturate all CPUs/cores on the machine.
Because of the FIFO submission admission policy, it will normally
perform submissions (which are expected to burst into many
subtasks) one at a time unless there is insufficient parallelism
or stalls with the current submission, in which case another
may be accepted to overlap. In general, this will maximize
overall throughput.

So, in pure throughput-oriented programming, you really do
want only one ForkJoinPool per program. Having more than one
would waste resources and create useless contention. (Note
however that the work-stealing techniques used internally
are still better than alternatives even when there is CPU
contention, since they adapt to cases where some worker
threads progress much more quickly than others, so long
as users use fine-grained enough tasks for this to kick in.)

Even if there were one global pool, we'd still allow
construction of others for nichy purposes of using
ForkJoinWorkerThread subclasses that add execution context,
or special UncaughtException handlers, or locally-fifo
processing (thus these non-default constructors.)

But otherwise, the only user-level decision I know of
is whether to configure the ForkJoinPool to use all available
CPUS/cores, or whether to use a smaller number to
improve chances of better responsiveness of other parts of a
system. I'm not positive that even this decision is best
left to users though. In the future (with better JVM/OS
support) we might be able to do a better job than
users could by internally automating dynamic sizing/throttling
of default-constructed or global ForkJoinPools. If
JDK libraries start using ForkJoinPool for parallel
apply-to-all, map, reduce, etc, we will surely need
to further explore doing this.

Note: it is possible, and even common for ForkJoinPool
to create more threads than the given parallelism
level to ensure liveness while preserving join dependencies.
(As I've said before, this is a feature, not a bug!)
However, this results in only transient periods in which
more than the target parallelism level number of threads
are actually running.

-Doug


From gregg at cytetech.com  Wed Aug 18 09:44:06 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 18 Aug 2010 08:44:06 -0500
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C6AFAEB.1030200@kav.dk>
References: <NFBBKALFDCPFIDBNKAPCMENAIHAA.davidcholmes@aapt.net.au>
	<4C6AF55A.50905@cytetech.com> <4C6AFAEB.1030200@kav.dk>
Message-ID: <4C6BE3A6.5010207@cytetech.com>

Kasper Nielsen wrote:
> On 17/08/10 22.47, Gregg Wonderly wrote:
>>
>> So, I'm still interested in what might be possible to have FJ provide
>> some level of out of band knowledge. I'm thinking about something like
>> the following in FJPool.
>>
>> public static FJPool getThreadsPool() {
>> for( FJPool pool : allPools.elements() ) {
>> if( pool.contains( Thread.currentThread() ) != null ) {
>> return pool;
>> }
>> }
>> }
>>
>> This would look through active instances to see if the current thread is
>> dispatched from that pool. If it is, then return the pool. This would
>> then allow a pretty low level in the software structure to find the
>> environment it might use for dispatching work, without having to pass
>> around such details through countless APIs.
> 
> Gregg,
> 
> I'm not sure I understand what you are trying to do.
> If the current thread is dispatch from a FJPool you can only be running 
> a ForkJoinTask in which case the pool that is executing the task can be 
> obtained via a call to static ForkJoinPool ForkJoinTask.getPool();

Yes, but a library which I have dispatched from that task to do something, in 
addition, won't have a reference to the FJTask, unless the library has some kind 
of initialization that lets me tell it what pool to use.  In this case, I want 
that library to be able to find the FJPool, and I'd even say I don't want it to 
be able to create its own instance in some cases, as I alluded to.

Gregg Wonderly

From kasper at kav.dk  Wed Aug 18 11:22:41 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Wed, 18 Aug 2010 17:22:41 +0200
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>
Message-ID: <4C6BFAC1.4050502@kav.dk>

On 17/08/10 04.23, Jason Mehrens wrote:
> Should the string representation of a map (entries and collection views)
> describe strictly just the contents or express the equivalence
> relationship of how the map views its entries? Specificity, since the
> CustomConcurrentHashMap can be created with an IDENTITY equivalence or
> EQUALS equivalence it would seem to me that one natural way to express
> this difference would be to have the map create an identityToString for
> a key or value when using an IDENTITY equivalence. Is it worth adding
> another method to CCHM.Equivalence to support custom strings?
>
Could you come up with some kind of usage scenario?

I can't recall i've ever used map.toString() for anything but debugging.
And if I ever wanted another string representation I would just copy the 
one from AbstractMap/AbstractCollection and customize it.

Cheers
   Kasper

From jason_mehrens at hotmail.com  Wed Aug 18 14:14:54 2010
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Wed, 18 Aug 2010 13:14:54 -0500
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <4C6BFAC1.4050502@kav.dk>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>, <4C6BFAC1.4050502@kav.dk>
Message-ID: <SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>


Kasper,
 
Debuging and logging are key areas.  Take the following:
 
//=========================================
Map<Integer, Integer> eq = new HashMap<Integer, Integer>();
eq.put(new Integer(1), new Integer(1));

Map<Integer, Integer> id = new IdentityHashMap<Integer, Integer>();
id.put(new Integer(1), new Integer(1));
 
System.out.println(id.equals(eq));
System.out.println(id.toString().equals(eq.toString()));
//=========================================
 
I think it would be ideal if this printed false, false and not false, true.  Changing the toString implementation to use identity strings would mean that CCHM.toString would be safe from stack overflow, would not unknowingly acquire an internal lock, and would have a more predictable runtime (Map<K, Collection<E>>.  CCHM hashCode and equals enjoy those benefits when in identity mode so why shouldn't CCHM.toString?
 
Regards,
 
Jason
 
> Date: Wed, 18 Aug 2010 17:22:41 +0200
> From: kasper at kav.dk
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Reference/IdentityMap toString
> 
> On 17/08/10 04.23, Jason Mehrens wrote:
> > Should the string representation of a map (entries and collection views)
> > describe strictly just the contents or express the equivalence
> > relationship of how the map views its entries? Specificity, since the
> > CustomConcurrentHashMap can be created with an IDENTITY equivalence or
> > EQUALS equivalence it would seem to me that one natural way to express
> > this difference would be to have the map create an identityToString for
> > a key or value when using an IDENTITY equivalence. Is it worth adding
> > another method to CCHM.Equivalence to support custom strings?
> >
> Could you come up with some kind of usage scenario?
> 
> I can't recall i've ever used map.toString() for anything but debugging.
> And if I ever wanted another string representation I would just copy the 
> one from AbstractMap/AbstractCollection and customize it.
> 
> Cheers
> Kasper
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100818/711d0590/attachment.html>

From kasper at kav.dk  Wed Aug 18 15:40:54 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Wed, 18 Aug 2010 21:40:54 +0200
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>,
	<4C6BFAC1.4050502@kav.dk>
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>
Message-ID: <4C6C3746.9050406@kav.dk>

On 18/08/10 20.14, Jason Mehrens wrote:
> Kasper,
>
> Debuging and logging are key areas. Take the following:
>
> //=========================================
> Map<Integer, Integer> eq = new HashMap<Integer, Integer>();
> eq.put(new Integer(1), new Integer(1));
>
> Map<Integer, Integer> id = new IdentityHashMap<Integer, Integer>();
> id.put(new Integer(1), new Integer(1));
>
> System.out.println(id.equals(eq));
> System.out.println(id.toString().equals(eq.toString()));
> //=========================================
>
> I think it would be ideal if this printed false, false and not false,
> true. Changing the toString implementation to use identity strings would
> mean that CCHM.toString would be safe from stack overflow, would not
> unknowingly acquire an internal lock, and would have a more predictable
> runtime (Map<K, Collection<E>>. CCHM hashCode and equals enjoy those
> benefits when in identity mode so why shouldn't CCHM.toString?

Jason,

I must understand I still can't see any use cases for this.
Why would you want to print something else but {1=1} in your 
IdentityHashMap but not in your HashMap. And why isn't CCHM.toString 
safe from stack overflow, unless someone is dumb enough to insert the 
map into itself.

Besides, you don't want to ever compare string representations of Maps. 
Maps are free to list the elements in any order they want.

Cheers
   Kasper

From gregg at cytetech.com  Wed Aug 18 17:06:55 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 18 Aug 2010 16:06:55 -0500
Subject: [concurrency-interest] The need for a default ForkJoinPool
In-Reply-To: <4C6BD37B.5080402@cs.oswego.edu>
References: <NFBBKALFDCPFIDBNKAPCIENDIHAA.davidcholmes@aapt.net.au>
	<4C6BD37B.5080402@cs.oswego.edu>
Message-ID: <4C6C4B6F.7080302@cytetech.com>

Doug Lea wrote:
> A few notes on this:
> 
> ForkJoinPool is entirely geared to maximizing throughput,
> not responsiveness/latency/fairness. In default configuration,
> it tries to fully employ/saturate all CPUs/cores on the machine.
> Because of the FIFO submission admission policy, it will normally
> perform submissions (which are expected to burst into many
> subtasks) one at a time unless there is insufficient parallelism
> or stalls with the current submission, in which case another
> may be accepted to overlap. In general, this will maximize
> overall throughput.
> 
> So, in pure throughput-oriented programming, you really do
> want only one ForkJoinPool per program. Having more than one
> would waste resources and create useless contention. (Note
> however that the work-stealing techniques used internally
> are still better than alternatives even when there is CPU
> contention, since they adapt to cases where some worker
> threads progress much more quickly than others, so long
> as users use fine-grained enough tasks for this to kick in.)
> 
> Even if there were one global pool, we'd still allow
> construction of others for nichy purposes of using
> ForkJoinWorkerThread subclasses that add execution context,
> or special UncaughtException handlers, or locally-fifo
> processing (thus these non-default constructors.)
> 
> But otherwise, the only user-level decision I know of
> is whether to configure the ForkJoinPool to use all available
> CPUS/cores, or whether to use a smaller number to
> improve chances of better responsiveness of other parts of a
> system. I'm not positive that even this decision is best
> left to users though. In the future (with better JVM/OS
> support) we might be able to do a better job than
> users could by internally automating dynamic sizing/throttling
> of default-constructed or global ForkJoinPools. If
> JDK libraries start using ForkJoinPool for parallel
> apply-to-all, map, reduce, etc, we will surely need
> to further explore doing this.
> 
> Note: it is possible, and even common for ForkJoinPool
> to create more threads than the given parallelism
> level to ensure liveness while preserving join dependencies.
> (As I've said before, this is a feature, not a bug!)
> However, this results in only transient periods in which
> more than the target parallelism level number of threads
> are actually running.

So, the specific use case that I think makes the most sense to stare at, is 
where an existing API has no use of FJ, but decides to change to use it, in the 
background for work dispatch.  When you place such code into an existing app 
with FJ use, along with other threads, there is no difference between FJ use and 
thread pools now, from the perspective of unmanageable (because the problem can 
grow to be intractable in complexity) CPU contention.

Doug is correct that there doesn't seem to be a "general rule" that simply 
solves the problem of CPU contention.  But, I think that falls out of all the 
control that Java provides for letting threads get created and in how we design 
APIs with continuous execution as a "mechanism" for managing the complexity of 
multi-staged operations.

If FJPool did provide a mechanism for providing a default pool, it still seems 
that it would be possible for it to look at which pool the current thread was 
in, and use such if it knew.

enum { FJ_DEFAULT_POOL, FJ_CONTEXT_POOL } FJPoolSelectionMode;

public static FJPool getContextPool() {
	return poolCurrentThreadIsIn();
}

public static FJPool getDefaultPool() {
	return selMode == FJ_DEFAULT_POOL ?
		defaultPool :
		getContextPool();
}

public static FJPool setPoolSelectionMode( FJPoolSelectionMode mode ) {
	selMode = mode;
}

Some other choices, or fewer methods with the enum as a parameter would work as 
well.

It just seems to me that choosing to return a singleton as the only 
implementation is not really a solution at all, because it doesn't let the 
application segregate "general computation" from "real work".

It could divide the CPU usage between multiple pools, and then use the context 
mechanism to keep related work using the same pool where stealing can be most 
effective.

Gregg Wonderly

From jason_mehrens at hotmail.com  Wed Aug 18 18:12:52 2010
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Wed, 18 Aug 2010 17:12:52 -0500
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <4C6C3746.9050406@kav.dk>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>, ,
	<4C6BFAC1.4050502@kav.dk>,
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>,
	<4C6C3746.9050406@kav.dk>
Message-ID: <SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>


Kasper,
 
> I must understand I still can't see any use cases for this.
> Why would you want to print something else but {1=1} in your 
> IdentityHashMap but not in your HashMap.
 
Because identity/reference maps view entries by what they reference and not the contents of what is referenced.  It's a code illusion to print the contents of each entry (my initial point).  Plus the other reasons listed in the last post.
 
> And why isn't CCHM.toString 
> safe from stack overflow, unless someone is dumb enough to insert the 
> map into itself.
 
You don't necessarily have to be dumb, you could be evil.  Back to the point, identity maps are used in object graph traversals to track what has been visited.  What happens when the tracker map tracks itself? Equals and hashCode work fine but lookout toString.  There is more than one way to poison the map:
 
//==============================================
m.put(K, Collections.nCopies(Integer.MAX_VALUE, Integer.MAX_VALUE));
//==============================================
 

> Besides, you don't want to ever compare string representations of Maps. 
> Maps are free to list the elements in any order they want.
 
Maps, entries, and collections are free to create whatever toString they want, it's not defined in the corresponding interface contracts at all.  They don't have to list any entries.
Comparing the result of the representations of the maps is just to prove that it is a code illusion.  One way or another, map.toString ends up in a log or in a debug watch and that is when the illusion begins.
 
Regards,
 
Jason 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100818/8d2a213f/attachment.html>

From kasper at kav.dk  Thu Aug 19 02:49:33 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Thu, 19 Aug 2010 08:49:33 +0200
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>, ,
	<4C6BFAC1.4050502@kav.dk>,
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>,
	<4C6C3746.9050406@kav.dk> <SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>
Message-ID: <4C6CD3FD.5030803@kav.dk>

On 19/08/10 00.12, Jason Mehrens wrote:
> Kasper,
>
>  > I must understand I still can't see any use cases for this.
>  > Why would you want to print something else but {1=1} in your
>  > IdentityHashMap but not in your HashMap.
>
> Because identity/reference maps view entries by what they reference and
> not the contents of what is referenced. It's a code illusion to print
> the contents of each entry (my initial point). Plus the other reasons
> listed in the last post.
>
>  > And why isn't CCHM.toString
>  > safe from stack overflow, unless someone is dumb enough to insert the
>  > map into itself.
>
> You don't necessarily have to be dumb, you could be evil. Back to the
> point, identity maps are used in object graph traversals to track what
> has been visited. What happens when the tracker map tracks itself?
> Equals and hashCode work fine but lookout toString. There is more than
> one way to poison the map:
>
> //==============================================
> m.put(K, Collections.nCopies(Integer.MAX_VALUE, Integer.MAX_VALUE));
> //==============================================
>
>
>  > Besides, you don't want to ever compare string representations of Maps.
>  > Maps are free to list the elements in any order they want.
>
> Maps, entries, and collections are free to create whatever toString they
> want, it's not defined in the corresponding interface contracts at all.
> They don't have to list any entries.
> Comparing the result of the representations of the maps is just to prove
> that it is a code illusion. One way or another, map.toString ends up in
> a log or in a debug watch and that is when the illusion begins.
>
> Regards,
>
> Jason
Jason,

You still haven't come up with any convincing use case. I've never seen 
any bugs related to this behavior. And if you want a custom toString 
method for CCHM you can just use the CCHM.entrySet().iterator to iterate 
through all elements and print whatever you want. Requiring people to 
implement a toString(Object) method whenever they implement the 
Equivalence interface is just a waste of peoples time IMO.

Cheers
   Kasper

From jason_mehrens at hotmail.com  Thu Aug 19 11:25:09 2010
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Thu, 19 Aug 2010 10:25:09 -0500
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <4C6CD3FD.5030803@kav.dk>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>,
	, , <4C6BFAC1.4050502@kav.dk>, ,
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>, ,
	<4C6C3746.9050406@kav.dk>
	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>, <4C6CD3FD.5030803@kav.dk>
Message-ID: <SNT114-W490FAD6D794521FE029728839E0@phx.gbl>


Kasper,
 
> You still haven't come up with any convincing use case. 
 
Feel free to counter each of my previous points.
 
>I've never seen any bugs related to this behavior. 
 
Take the output of this code:
//===============
Map<Integer, Integer> id = new IdentityHashMap<Integer, Integer>();
id.put(new Integer(1), new Integer(1));
id.put(new Integer(1), new Integer(1));
id.put(new Integer(1), new Integer(1));
System.out.println(id.keySet());
//==============
 
Shouldn't a set contain unique items? If you are tracking items by identity or reference displaying the contents of a key/value has no meaning.  I don't think I would get any positive feedback if I wrote the output of that code on a piece of paper and presented it as an example of a unique set of elements.
 
 
>And if you want a custom toString 
> method for CCHM you can just use the CCHM.entrySet().iterator to iterate 
> through all elements and print whatever you want. 
 
That applies both ways.
 
>Requiring people to 
> implement a toString(Object) method whenever they implement the 
> Equivalence interface is just a waste of peoples time IMO.
 
Equivalence is extracting methods from Object.  The object documentation states they subclasses should always override toString.  Currently, there is no way translate that requirement from Object to Equivalence.
 
Regards,

Jason 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100819/b53b5cd6/attachment-0001.html>

From jwesleysmith at atlassian.com  Fri Aug 20 02:39:38 2010
From: jwesleysmith at atlassian.com (Jed Wesley-Smith)
Date: Fri, 20 Aug 2010 16:39:38 +1000
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <SNT114-W490FAD6D794521FE029728839E0@phx.gbl>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>, , ,
	<4C6BFAC1.4050502@kav.dk>, ,
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>, ,
	<4C6C3746.9050406@kav.dk>	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>,
	<4C6CD3FD.5030803@kav.dk> <SNT114-W490FAD6D794521FE029728839E0@phx.gbl>
Message-ID: <4C6E232A.3040707@atlassian.com>

Jason,

I think you're basically arguing that IdentityHashMap (and any "map" or 
"set" that uses identity == rather than the equals contract) is broken. 
Which, is of course true ? and there is a lot of prior discussion 
recognising that it was probably a mistake but a pragmatically useful 
one nonetheless in expert hands.

Personally I find the fact that equals is defined at all on Object (or 
at least to be a virtual method) a problem. It is extremely hard to 
maintain the symmetric and transitive properties of the contract if your 
classes are open. It is maintainable when you tightly control your class 
hierarchy and object instantiation but can be the source of lots of 
problems in a n open environment.

So, should CCHM and IHM provide a special toString() to indicate they 
are broken? Personally it seems a quite esoteric requirement. Better not 
to use them in the first place unless you really, really need to and 
even then hide it from the hoi polloi.

If you really need to have the example below work both conceptually and 
print a nice toString have a Ref class that contains a single final 
reference and delegates its equality to the contained reference identity. eg

class Ref<T> {
final T ref;

Ref(T ref) {
this.ref = ref;
}

T get() {return ref;}

public String toString() {return ref.getClass().getName() + "@" + 
hashCode();)

public int hashCode() {return System.identityHashCode(ref);}

public boolean equals(Object o) {return ref == ((Ref) o).ref);}
}

This gives you total control and means you don't need to use the IHM or 
CCHM. In the IHM case this means the extra indirection of a wrapper, but 
CCHM I believe does this anyway. (Of course, CCHM can also do weak/soft 
refs and a few other things?)

cheers,
jed.

Jason Mehrens wrote:
> Kasper,
>
> > You still haven't come up with any convincing use case.
>
> Feel free to counter each of my previous points.
>
> >I've never seen any bugs related to this behavior.
>
> Take the output of this code:
> //===============
> Map<Integer, Integer> id = new IdentityHashMap<Integer, Integer>();
> id.put(new Integer(1), new Integer(1));
> id.put(new Integer(1), new Integer(1));
> id.put(new Integer(1), new Integer(1));
> System.out.println(id.keySet());
> //==============
>
> Shouldn't a set contain unique items? If you are tracking items by 
> identity or reference displaying the contents of a key/value has no 
> meaning. I don't think I would get any positive feedback if I wrote 
> the output of that code on a piece of paper and presented it as an 
> example of a unique set of elements.
>
>
> >And if you want a custom toString
> > method for CCHM you can just use the CCHM.entrySet().iterator to 
> iterate
> > through all elements and print whatever you want.
>
> That applies both ways.
>
> >Requiring people to
> > implement a toString(Object) method whenever they implement the
> > Equivalence interface is just a waste of peoples time IMO.
>
> Equivalence is extracting methods from Object. The object 
> documentation states they subclasses should always override toString. 
> Currently, there is no way translate that requirement from Object to 
> Equivalence.
>
> Regards,
>
> Jason
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   


From jason_mehrens at hotmail.com  Fri Aug 20 10:20:38 2010
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Fri, 20 Aug 2010 09:20:38 -0500
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <4C6E232A.3040707@atlassian.com>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>,	, ,
	<4C6BFAC1.4050502@kav.dk>, ,
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>, 
	, <4C6C3746.9050406@kav.dk>	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>,
	<4C6CD3FD.5030803@kav.dk>
	<SNT114-W490FAD6D794521FE029728839E0@phx.gbl>,
	<4C6E232A.3040707@atlassian.com>
Message-ID: <SNT114-W62EC3F26ADFC34FC684D21839F0@phx.gbl>


Jed,
 
> I think you're basically arguing that IdentityHashMap (and any "map" or 
> "set" that uses identity == rather than the equals contract) is broken. 
 
IdentityHashMap has a nice warning in the javadocs which is great.  Specifically, I'm arguing that any map that doesn't use the equals contract but uses the AbstractMap.toString is broken.  The reason is that map containment is determined by the equivalence relationship and AbstractMap.toString lists what is contained in the map.  Therefore, the string representation of a map must accurately describe the equivalence relationship, otherwise the string representation is broken.
 
Regards,
 
Jason 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100820/d39260b6/attachment.html>

From tim at peierls.net  Fri Aug 20 10:56:03 2010
From: tim at peierls.net (Tim Peierls)
Date: Fri, 20 Aug 2010 10:56:03 -0400
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <SNT114-W62EC3F26ADFC34FC684D21839F0@phx.gbl>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>
	<4C6BFAC1.4050502@kav.dk>
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>
	<4C6C3746.9050406@kav.dk>
	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>
	<4C6CD3FD.5030803@kav.dk>
	<SNT114-W490FAD6D794521FE029728839E0@phx.gbl>
	<4C6E232A.3040707@atlassian.com>
	<SNT114-W62EC3F26ADFC34FC684D21839F0@phx.gbl>
Message-ID: <AANLkTikB8eb_-jLKPGonn-0W=VytSO31V9b7fZkWLtG1@mail.gmail.com>

On Fri, Aug 20, 2010 at 10:20 AM, Jason Mehrens
<jason_mehrens at hotmail.com>wrote:

> ...I'm arguing that any map that doesn't use the equals contract but uses
> the AbstractMap.toString is broken.  The reason is that map containment is
> determined by the equivalence relationship and AbstractMap.toString lists
> what is contained in the map.  Therefore, the string representation of a
> map must accurately describe the equivalence relationship, otherwise the
> string representation is broken.
>

While bijectivity is an attractive property for a string representation
function, it's not something anyone should depend on.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100820/083acd3d/attachment.html>

From jason_mehrens at hotmail.com  Fri Aug 20 14:08:20 2010
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Fri, 20 Aug 2010 13:08:20 -0500
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <AANLkTikB8eb_-jLKPGonn-0W=VytSO31V9b7fZkWLtG1@mail.gmail.com>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>, <4C6BFAC1.4050502@kav.dk>,
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>,
	<4C6C3746.9050406@kav.dk>,
	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>,
	<4C6CD3FD.5030803@kav.dk>,
	<SNT114-W490FAD6D794521FE029728839E0@phx.gbl>,
	<4C6E232A.3040707@atlassian.com>,
	<SNT114-W62EC3F26ADFC34FC684D21839F0@phx.gbl>,
	<AANLkTikB8eb_-jLKPGonn-0W=VytSO31V9b7fZkWLtG1@mail.gmail.com>
Message-ID: <SNT114-W285C99497C113C628AF52A839F0@phx.gbl>


Tim,
 
>While bijectivity is an attractive property for a string representation function, it's not something anyone should depend on.

 
Agreed.  A key toString implementation lacking sufficient detail could cause AbstractMap.toString to lose that property (at no fault of the map).
 
Regards,
 
Jason

  		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100820/afe96726/attachment.html>

From tim at peierls.net  Fri Aug 20 16:18:27 2010
From: tim at peierls.net (Tim Peierls)
Date: Fri, 20 Aug 2010 16:18:27 -0400
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <SNT114-W285C99497C113C628AF52A839F0@phx.gbl>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>
	<4C6BFAC1.4050502@kav.dk>
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>
	<4C6C3746.9050406@kav.dk>
	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>
	<4C6CD3FD.5030803@kav.dk>
	<SNT114-W490FAD6D794521FE029728839E0@phx.gbl>
	<4C6E232A.3040707@atlassian.com>
	<SNT114-W62EC3F26ADFC34FC684D21839F0@phx.gbl>
	<AANLkTikB8eb_-jLKPGonn-0W=VytSO31V9b7fZkWLtG1@mail.gmail.com>
	<SNT114-W285C99497C113C628AF52A839F0@phx.gbl>
Message-ID: <AANLkTikci=OasofoKOexpts-ATDAuSwHPztLwjVD-XHt@mail.gmail.com>

On Fri, Aug 20, 2010 at 2:08 PM, Jason Mehrens <jason_mehrens at hotmail.com>wrote:

> [Tim wrote:] While bijectivity is an attractive property for a string
> representation function, it's not something anyone should depend on.
>
> [Jason wrote:] Agreed.  A key toString implementation lacking sufficient
> detail could cause AbstractMap.toString to lose that property (at no fault
> of the map).
>

I meant that because no one should be depending on toString being bijective,
there's not much point in striving for bijectivity in toString
implementations in the first place. So I don't think AbstractMap.toString()
can be fairly described as "broken", even for identity maps.

As others have mentioned, if you have a specific need to preserve a key's
identity in a string representation -- e.g., debugging, logging -- there are
several approaches one could use that don't involve complicating
AbstractMap.toString.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100820/618ba2fb/attachment.html>

From gregg at cytetech.com  Mon Aug 23 10:10:24 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 23 Aug 2010 09:10:24 -0500
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <AANLkTikci=OasofoKOexpts-ATDAuSwHPztLwjVD-XHt@mail.gmail.com>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>
	<4C6BFAC1.4050502@kav.dk>
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>
	<4C6C3746.9050406@kav.dk>
	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>
	<4C6CD3FD.5030803@kav.dk>
	<SNT114-W490FAD6D794521FE029728839E0@phx.gbl>
	<4C6E232A.3040707@atlassian.com>
	<SNT114-W62EC3F26ADFC34FC684D21839F0@phx.gbl>
	<AANLkTikB8eb_-jLKPGonn-0W=VytSO31V9b7fZkWLtG1@mail.gmail.com>
	<SNT114-W285C99497C113C628AF52A839F0@phx.gbl>
	<AANLkTikci=OasofoKOexpts-ATDAuSwHPztLwjVD-XHt@mail.gmail.com>
Message-ID: <4C728150.60007@cytetech.com>

Tim Peierls wrote:
> On Fri, Aug 20, 2010 at 2:08 PM, Jason Mehrens 
> <jason_mehrens at hotmail.com <mailto:jason_mehrens at hotmail.com>> wrote:
> 
>     [Tim wrote:] While bijectivity is an attractive property for a
>     string representation function, it's not something anyone should
>     depend on.
>      
>     [Jason wrote:] Agreed.  A key toString implementation lacking
>     sufficient detail could cause AbstractMap.toString to lose that
>     property (at no fault of the map).
> 
> 
> I meant that because no one should be depending on toString being 
> bijective, there's not much point in striving for bijectivity in 
> toString implementations in the first place. So I don't think 
> AbstractMap.toString() can be fairly described as "broken", even for 
> identity maps.
> 
> As others have mentioned, if you have a specific need to preserve a 
> key's identity in a string representation -- e.g., debugging, logging -- 
> there are several approaches one could use that don't involve 
> complicating AbstractMap.toString.

I am not sure I understand the opposing views in this conversation, but I will 
say that more often then not, I do use object instance toString() variations so 
that I can see how many new instances of something are appearing within logging 
or debugging messages to the console.  I often do have things like

public String toString() {
	return prettyToString()+
		(logger.isLoggable( Level.FINE ) ? (":"+hashCode()) : "");
}

or some such on Object subclasses to do this.  This makes it trivial to see this 
kind of information when I need it, but for it to not be visible otherwise.

Gregg Wonderly

From viktor.klang at gmail.com  Mon Aug 23 16:19:02 2010
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 23 Aug 2010 22:19:02 +0200
Subject: [concurrency-interest] Bounded TransferQueue
Message-ID: <AANLkTiku=U2JcRksYm7VmKi7uyTesQDRN=B90mnj0Sgr@mail.gmail.com>

Hi folks,

It's late here and I've rummaged through the Internet in it's entirety so
I'll get straight to the point,

I have a dire need for a bounded TransferQueue, and the LinkedTransferQueue
states that it's unbounded,
does anyone have any suggestions where I can find a (highly performant)
bounded TransferQueue?

Best regards,
-- 
Viktor Klang,
Code Connoisseur
Work:   www.akkasource.com
Code:   github.com/viktorklang
Follow: twitter.com/viktorklang
Read:   klangism.tumblr.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100823/3b7f8064/attachment.html>

From jason_mehrens at hotmail.com  Mon Aug 23 16:35:04 2010
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Mon, 23 Aug 2010 15:35:04 -0500
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <AANLkTikci=OasofoKOexpts-ATDAuSwHPztLwjVD-XHt@mail.gmail.com>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>, <4C6BFAC1.4050502@kav.dk>,
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>,
	<4C6C3746.9050406@kav.dk>,
	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>,
	<4C6CD3FD.5030803@kav.dk>,
	<SNT114-W490FAD6D794521FE029728839E0@phx.gbl>,
	<4C6E232A.3040707@atlassian.com>,
	<SNT114-W62EC3F26ADFC34FC684D21839F0@phx.gbl>,
	<AANLkTikB8eb_-jLKPGonn-0W=VytSO31V9b7fZkWLtG1@mail.gmail.com>,
	<SNT114-W285C99497C113C628AF52A839F0@phx.gbl>,
	<AANLkTikci=OasofoKOexpts-ATDAuSwHPztLwjVD-XHt@mail.gmail.com>
Message-ID: <SNT114-W1422B073017D6B47FC97D483820@phx.gbl>


Tim,
 




[Tim wrote:] While bijectivity is an attractive property for a string representation function, it's not something anyone should depend on.

[Tim wrote:] I meant that because no one should be depending on toString being bijective, there's not much point in striving for bijectivity in toString implementations in the first place. 
 
 
>From the callers point of view right?  Otherwise, I'm (mis)interpreting the above as "bijectivity is attractive property but not worth doing".
 
The reason to do so is to walk like a Map (get/put) and talk like a Map (toString).  ToString is dependent on the fact 'this' context is a map.  The map implementation of a toString method is testable (which is one caller that depends on toString).  If we throw all bijectivity out then there is no way to unit test a map implementation of toString (assuming it chooses to iterate over the contents).  That would allow a map to legally iterate of the first element N times where N is the size and size is greater than one.  Making the map look like it contains duplicate entries.
 

>So I don't think AbstractMap.toString() can be fairly described as "broken", even for identity maps.
Bringing it back my original question, you mean that because AbstractMap.toString is not required to be bijective it is free to not represent the equivalence relationship, therefore is not "broken".  Correct?
 
 
>As others have mentioned, if you have a specific need to preserve a key's identity in a string representation -- e.g., debugging, logging -- there are several approaches one could use that don't involve complicating AbstractMap.toString.
The examples are great.  They show that everyone is eager to solve a problem and help others.  The core issue is what is the right thing for the map itself to do (callee not caller)?  100 more workarounds will not answer my original question.
 
I guess the practical version of my original question is: "Say some day the next great reference/identity map is being integrated into the java of the future.  Should the implementation use AbstractMap.toString that suffers from some issues in this context or since it is a specialized map perhaps it should provide a specialized toString to fix these issues?"
 
Respectfully,
 
Jason 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100823/c6c1eae7/attachment.html>

From joe.bowbeer at gmail.com  Mon Aug 23 16:56:09 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 23 Aug 2010 13:56:09 -0700
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <AANLkTiku=U2JcRksYm7VmKi7uyTesQDRN=B90mnj0Sgr@mail.gmail.com>
References: <AANLkTiku=U2JcRksYm7VmKi7uyTesQDRN=B90mnj0Sgr@mail.gmail.com>
Message-ID: <AANLkTi=T0-S04XkFp7D+xv13Dj+quRALBNm8qOcOzkK5@mail.gmail.com>

In a previous discussion, Doug Lea wrote:

"For the bounded case, it's hard to do any better than use a Semaphore in
front of a LinkedTransferQueue."

http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html

I don't know if the thinking has changed any since then.

Joe

On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:

> Hi folks,
>
> It's late here and I've rummaged through the Internet in it's entirety so
> I'll get straight to the point,
>
> I have a dire need for a bounded TransferQueue, and the LinkedTransferQueue
> states that it's unbounded,
> does anyone have any suggestions where I can find a (highly performant)
> bounded TransferQueue?
>
> Best regards,
> --
> Viktor Klang
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100823/ed32173b/attachment.html>

From tim at peierls.net  Mon Aug 23 21:11:46 2010
From: tim at peierls.net (Tim Peierls)
Date: Mon, 23 Aug 2010 21:11:46 -0400
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <SNT114-W1422B073017D6B47FC97D483820@phx.gbl>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>
	<4C6BFAC1.4050502@kav.dk>
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>
	<4C6C3746.9050406@kav.dk>
	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>
	<4C6CD3FD.5030803@kav.dk>
	<SNT114-W490FAD6D794521FE029728839E0@phx.gbl>
	<4C6E232A.3040707@atlassian.com>
	<SNT114-W62EC3F26ADFC34FC684D21839F0@phx.gbl>
	<AANLkTikB8eb_-jLKPGonn-0W=VytSO31V9b7fZkWLtG1@mail.gmail.com>
	<SNT114-W285C99497C113C628AF52A839F0@phx.gbl>
	<AANLkTikci=OasofoKOexpts-ATDAuSwHPztLwjVD-XHt@mail.gmail.com>
	<SNT114-W1422B073017D6B47FC97D483820@phx.gbl>
Message-ID: <AANLkTinEttR6600RLCDyiPD-jupgFZoCjMR5DjEQMg-o@mail.gmail.com>

On Mon, Aug 23, 2010 at 4:35 PM, Jason Mehrens <jason_mehrens at hotmail.com>wrote:

> I guess the practical version of my original question is: "Say some day the
> next great reference/identity map is being integrated into the java of the
> future.  Should the implementation use AbstractMap.toString that suffers
> from some issues in this context or since it is a specialized map perhaps it
> should provide a specialized toString to fix these issues?"
>

I don't accept the premise that AbstractMap.toString "suffers from some
issues" when used as part of an implementation of an identity map.
AbstractMap.toString is not broken or deficient; you are asking too much of
it. You're trying to turn it into a serialization mechanism, something it is
not meant for.

If you feel you really do need uniqueness of string representations for your
identity maps, you can achieve it without imposing extra complications for
those who don't. Several people have given examples of how you might
accomplish this in practice.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100823/8ebbace1/attachment.html>

From viktor.klang at gmail.com  Tue Aug 24 03:39:16 2010
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 24 Aug 2010 09:39:16 +0200
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <AANLkTi=T0-S04XkFp7D+xv13Dj+quRALBNm8qOcOzkK5@mail.gmail.com>
References: <AANLkTiku=U2JcRksYm7VmKi7uyTesQDRN=B90mnj0Sgr@mail.gmail.com>
	<AANLkTi=T0-S04XkFp7D+xv13Dj+quRALBNm8qOcOzkK5@mail.gmail.com>
Message-ID: <AANLkTin9N2_D09O8KCb-BDx3ayWc2Jt4EePqT6VNEOra@mail.gmail.com>

Thanks Joe,

The problem with that would be that I'd have to wrap the queue and make sure
I have the Semaphore in place in all places that matter, and then implement
remainingCapacity etc, I'd rather not do that so if it's possible to
avoid...

If someone has an impl under a permissive license (ApacheV2, MIT, BSD or
such) I'd be very thankful for it.

Cheers,

On Mon, Aug 23, 2010 at 10:56 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> In a previous discussion, Doug Lea wrote:
>
> "For the bounded case, it's hard to do any better than use a Semaphore in
> front of a LinkedTransferQueue."
>
> http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html
>
> I don't know if the thinking has changed any since then.
>
> Joe
>
> On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:
>
> Hi folks,
>>
>> It's late here and I've rummaged through the Internet in it's entirety so
>> I'll get straight to the point,
>>
>> I have a dire need for a bounded TransferQueue, and the
>> LinkedTransferQueue states that it's unbounded,
>> does anyone have any suggestions where I can find a (highly performant)
>> bounded TransferQueue?
>>
>> Best regards,
>> --
>> Viktor Klang
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Viktor Klang,
Code Connoisseur
Work:   www.akkasource.com
Code:   github.com/viktorklang
Follow: twitter.com/viktorklang
Read:   klangism.tumblr.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100824/3292e250/attachment.html>

From davidcholmes at aapt.net.au  Tue Aug 24 03:48:53 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 24 Aug 2010 17:48:53 +1000
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <AANLkTin9N2_D09O8KCb-BDx3ayWc2Jt4EePqT6VNEOra@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEOFIHAA.davidcholmes@aapt.net.au>

Not sure I understand your "problem" here. You write a BoundedTransferQueue
that has-a LinkedTransferQueue and a Semaphore.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Viktor Klang
  Sent: Tuesday, 24 August 2010 5:39 PM
  To: Joe Bowbeer
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Bounded TransferQueue


  Thanks Joe,

  The problem with that would be that I'd have to wrap the queue and make
sure I have the Semaphore in place in all places that matter, and then
implement remainingCapacity etc, I'd rather not do that so if it's possible
to avoid...

  If someone has an impl under a permissive license (ApacheV2, MIT, BSD or
such) I'd be very thankful for it.

  Cheers,


  On Mon, Aug 23, 2010 at 10:56 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
wrote:

    In a previous discussion, Doug Lea wrote:


    "For the bounded case, it's hard to do any better than use a Semaphore
in front of a LinkedTransferQueue."


    http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html


    I don't know if the thinking has changed any since then.


    Joe


    On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:


      Hi folks,

      It's late here and I've rummaged through the Internet in it's entirety
so I'll get straight to the point,

      I have a dire need for a bounded TransferQueue, and the
LinkedTransferQueue states that it's unbounded,
      does anyone have any suggestions where I can find a (highly
performant) bounded TransferQueue?

      Best regards,
      --
      Viktor Klang


    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest





  --
  Viktor Klang,
  Code Connoisseur
  Work:   www.akkasource.com
  Code:   github.com/viktorklang
  Follow: twitter.com/viktorklang
  Read:   klangism.tumblr.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100824/8bffa12c/attachment-0001.html>

From viktor.klang at gmail.com  Tue Aug 24 03:54:43 2010
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 24 Aug 2010 09:54:43 +0200
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEOFIHAA.davidcholmes@aapt.net.au>
References: <AANLkTin9N2_D09O8KCb-BDx3ayWc2Jt4EePqT6VNEOra@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEOFIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTikFCFchBaHEittxDWMf6FBycQt2vkWOGUrekp2j@mail.gmail.com>

On Tue, Aug 24, 2010 at 9:48 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

>  Not sure I understand your "problem" here. You write a
> BoundedTransferQueue that has-a LinkedTransferQueue and a Semaphore.
>

Yes, that's my fall-back option, but if I don't need to reinvent the wheel
I'd rather not.


>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Viktor Klang
> *Sent:* Tuesday, 24 August 2010 5:39 PM
> *To:* Joe Bowbeer
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Bounded TransferQueue
>
> Thanks Joe,
>
> The problem with that would be that I'd have to wrap the queue and make
> sure I have the Semaphore in place in all places that matter, and then
> implement remainingCapacity etc, I'd rather not do that so if it's possible
> to avoid...
>
> If someone has an impl under a permissive license (ApacheV2, MIT, BSD or
> such) I'd be very thankful for it.
>
> Cheers,
>
> On Mon, Aug 23, 2010 at 10:56 PM, Joe Bowbeer <joe.bowbeer at gmail.com>wrote:
>
>> In a previous discussion, Doug Lea wrote:
>>
>> "For the bounded case, it's hard to do any better than use a Semaphore in
>> front of a LinkedTransferQueue."
>>
>> http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html
>>
>> I don't know if the thinking has changed any since then.
>>
>> Joe
>>
>> On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:
>>
>> Hi folks,
>>>
>>> It's late here and I've rummaged through the Internet in it's entirety so
>>> I'll get straight to the point,
>>>
>>> I have a dire need for a bounded TransferQueue, and the
>>> LinkedTransferQueue states that it's unbounded,
>>> does anyone have any suggestions where I can find a (highly performant)
>>> bounded TransferQueue?
>>>
>>> Best regards,
>>> --
>>> Viktor Klang
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Viktor Klang,
> Code Connoisseur
> Work:   www.akkasource.com
> Code:   github.com/viktorklang
> Follow: twitter.com/viktorklang
> Read:   klangism.tumblr.com
>
>


-- 
Viktor Klang,
Code Connoisseur
Work:   www.akkasource.com
Code:   github.com/viktorklang
Follow: twitter.com/viktorklang
Read:   klangism.tumblr.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100824/222fc9ea/attachment.html>

From gustav.trede at gmail.com  Tue Aug 24 04:35:02 2010
From: gustav.trede at gmail.com (gustav)
Date: Tue, 24 Aug 2010 10:35:02 +0200
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <AANLkTikFCFchBaHEittxDWMf6FBycQt2vkWOGUrekp2j@mail.gmail.com>
References: <AANLkTin9N2_D09O8KCb-BDx3ayWc2Jt4EePqT6VNEOra@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEOFIHAA.davidcholmes@aapt.net.au>
	<AANLkTikFCFchBaHEittxDWMf6FBycQt2vkWOGUrekp2j@mail.gmail.com>
Message-ID: <AANLkTimjM6fNvUcsvDjLdSAdidp0yr_zYt_T_5cMndZ+@mail.gmail.com>

On 24 August 2010 09:54, Viktor Klang <viktor.klang at gmail.com> wrote:

>
>
> On Tue, Aug 24, 2010 at 9:48 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>>  Not sure I understand your "problem" here. You write a
>> BoundedTransferQueue that has-a LinkedTransferQueue and a Semaphore.
>>
>
> Yes, that's my fall-back option, but if I don't need to reinvent the wheel
> I'd rather not.
>
>
>>
>> David Holmes
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Viktor Klang
>> *Sent:* Tuesday, 24 August 2010 5:39 PM
>> *To:* Joe Bowbeer
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Bounded TransferQueue
>>
>> Thanks Joe,
>>
>> The problem with that would be that I'd have to wrap the queue and make
>> sure I have the Semaphore in place in all places that matter, and then
>> implement remainingCapacity etc, I'd rather not do that so if it's possible
>> to avoid...
>>
>> If someone has an impl under a permissive license (ApacheV2, MIT, BSD or
>> such) I'd be very thankful for it.
>>
>> Cheers,
>>
>> On Mon, Aug 23, 2010 at 10:56 PM, Joe Bowbeer <joe.bowbeer at gmail.com>wrote:
>>
>>> In a previous discussion, Doug Lea wrote:
>>>
>>> "For the bounded case, it's hard to do any better than use a Semaphore in
>>> front of a LinkedTransferQueue."
>>>
>>> http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html
>>>
>>> I don't know if the thinking has changed any since then.
>>>
>>> Joe
>>>
>>> On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:
>>>
>>> Hi folks,
>>>>
>>>> It's late here and I've rummaged through the Internet in it's entirety
>>>> so I'll get straight to the point,
>>>>
>>>> I have a dire need for a bounded TransferQueue, and the
>>>> LinkedTransferQueue states that it's unbounded,
>>>> does anyone have any suggestions where I can find a (highly performant)
>>>> bounded TransferQueue?
>>>>
>>>> Best regards,
>>>> --
>>>>
>>>
There is a  QueueLimitedThreadPool.java in sun io framework grizzly ,
glassfish appserver  that is using LTQ with an Atomic to limit the queue.
Unless its changed after i implemented it , Its used if your not putting -1
as queue limit while using equal min max pool size in glassfish,
If you set pool min and max to same value , and -1 as queue limit a pure LTQ
is used.

I cant find a working link to it now.

At high contention the Atomic counter is ending up looping in its internal
methods, the throughput can become similar to a synchronized design.
Its a lot better if its possible to handle the limit at other places, lets
say you have many connections that can produce work,
if you limit the per outstanding work per connection, you will indirectly
control the overall limit and don't need to kill concurrent scalability in
the global queue.

The avoid to re inventing the weel is a good concept, but should never be an
excuse to not learn new things due to fear.
You can always ask for feedback here, to help you QA the implementation if
you decide to do it.

regards
 gustav trede
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100824/bdc73bfc/attachment.html>

From viktor.klang at gmail.com  Tue Aug 24 04:43:38 2010
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 24 Aug 2010 10:43:38 +0200
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <AANLkTimjM6fNvUcsvDjLdSAdidp0yr_zYt_T_5cMndZ+@mail.gmail.com>
References: <AANLkTin9N2_D09O8KCb-BDx3ayWc2Jt4EePqT6VNEOra@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEOFIHAA.davidcholmes@aapt.net.au>
	<AANLkTikFCFchBaHEittxDWMf6FBycQt2vkWOGUrekp2j@mail.gmail.com>
	<AANLkTimjM6fNvUcsvDjLdSAdidp0yr_zYt_T_5cMndZ+@mail.gmail.com>
Message-ID: <AANLkTikEB+oM3k0Jx62SXgBMuLmaL_La8fEiKGSgg7G1@mail.gmail.com>

On Tue, Aug 24, 2010 at 10:35 AM, gustav <gustav.trede at gmail.com> wrote:

>
>
> On 24 August 2010 09:54, Viktor Klang <viktor.klang at gmail.com> wrote:
>
>>
>>
>> On Tue, Aug 24, 2010 at 9:48 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>>
>>>  Not sure I understand your "problem" here. You write a
>>> BoundedTransferQueue that has-a LinkedTransferQueue and a Semaphore.
>>>
>>
>> Yes, that's my fall-back option, but if I don't need to reinvent the wheel
>> I'd rather not.
>>
>>
>>>
>>> David Holmes
>>>
>>> -----Original Message-----
>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Viktor Klang
>>> *Sent:* Tuesday, 24 August 2010 5:39 PM
>>> *To:* Joe Bowbeer
>>> *Cc:* concurrency-interest at cs.oswego.edu
>>> *Subject:* Re: [concurrency-interest] Bounded TransferQueue
>>>
>>> Thanks Joe,
>>>
>>> The problem with that would be that I'd have to wrap the queue and make
>>> sure I have the Semaphore in place in all places that matter, and then
>>> implement remainingCapacity etc, I'd rather not do that so if it's possible
>>> to avoid...
>>>
>>> If someone has an impl under a permissive license (ApacheV2, MIT, BSD or
>>> such) I'd be very thankful for it.
>>>
>>> Cheers,
>>>
>>> On Mon, Aug 23, 2010 at 10:56 PM, Joe Bowbeer <joe.bowbeer at gmail.com>wrote:
>>>
>>>> In a previous discussion, Doug Lea wrote:
>>>>
>>>> "For the bounded case, it's hard to do any better than use a Semaphore
>>>> in front of a LinkedTransferQueue."
>>>>
>>>> http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html
>>>>
>>>> I don't know if the thinking has changed any since then.
>>>>
>>>> Joe
>>>>
>>>> On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:
>>>>
>>>> Hi folks,
>>>>>
>>>>> It's late here and I've rummaged through the Internet in it's entirety
>>>>> so I'll get straight to the point,
>>>>>
>>>>> I have a dire need for a bounded TransferQueue, and the
>>>>> LinkedTransferQueue states that it's unbounded,
>>>>> does anyone have any suggestions where I can find a (highly performant)
>>>>> bounded TransferQueue?
>>>>>
>>>>> Best regards,
>>>>> --
>>>>>
>>>>
> There is a  QueueLimitedThreadPool.java in sun io framework grizzly ,
> glassfish appserver  that is using LTQ with an Atomic to limit the queue.
> Unless its changed after i implemented it , Its used if your not putting -1
> as queue limit while using equal min max pool size in glassfish,
> If you set pool min and max to same value , and -1 as queue limit a pure
> LTQ is used.
>
> I cant find a working link to it now.
>
> At high contention the Atomic counter is ending up looping in its internal
> methods, the throughput can become similar to a synchronized design.
> Its a lot better if its possible to handle the limit at other places, lets
> say you have many connections that can produce work,
> if you limit the per outstanding work per connection, you will indirectly
> control the overall limit and don't need to kill concurrent scalability in
> the global queue.
>
> The avoid to re inventing the weel is a good concept, but should never be
> an excuse to not learn new things due to fear.
> You can always ask for feedback here, to help you QA the implementation if
> you decide to do it.
>

I agree, but I don't need it for some hobby project, I need it to fit into a
high-performance concurrency framework, so I'd rather avoid to waste a lot
of time to tune a makeshift implementation.


>
> regards
>  gustav trede
>



-- 
Viktor Klang,
Code Connoisseur
Work:   www.akkasource.com
Code:   github.com/viktorklang
Follow: twitter.com/viktorklang
Read:   klangism.tumblr.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100824/63c3d6e5/attachment-0001.html>

From gustav.trede at gmail.com  Tue Aug 24 04:57:53 2010
From: gustav.trede at gmail.com (gustav)
Date: Tue, 24 Aug 2010 10:57:53 +0200
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <AANLkTikEB+oM3k0Jx62SXgBMuLmaL_La8fEiKGSgg7G1@mail.gmail.com>
References: <AANLkTin9N2_D09O8KCb-BDx3ayWc2Jt4EePqT6VNEOra@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEOFIHAA.davidcholmes@aapt.net.au>
	<AANLkTikFCFchBaHEittxDWMf6FBycQt2vkWOGUrekp2j@mail.gmail.com>
	<AANLkTimjM6fNvUcsvDjLdSAdidp0yr_zYt_T_5cMndZ+@mail.gmail.com>
	<AANLkTikEB+oM3k0Jx62SXgBMuLmaL_La8fEiKGSgg7G1@mail.gmail.com>
Message-ID: <AANLkTikwGau+1MNz_EdF-9brTvgm4=oYXT-=gahXv-8=@mail.gmail.com>

On 24 August 2010 10:43, Viktor Klang <viktor.klang at gmail.com> wrote:

>
>
> On Tue, Aug 24, 2010 at 10:35 AM, gustav <gustav.trede at gmail.com> wrote:
>
>>
>>
>> On 24 August 2010 09:54, Viktor Klang <viktor.klang at gmail.com> wrote:
>>
>>>
>>>
>>> On Tue, Aug 24, 2010 at 9:48 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>>>
>>>>  Not sure I understand your "problem" here. You write a
>>>> BoundedTransferQueue that has-a LinkedTransferQueue and a Semaphore.
>>>>
>>>
>>> Yes, that's my fall-back option, but if I don't need to reinvent the
>>> wheel I'd rather not.
>>>
>>>
>>>>
>>>> David Holmes
>>>>
>>>> -----Original Message-----
>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Viktor Klang
>>>> *Sent:* Tuesday, 24 August 2010 5:39 PM
>>>> *To:* Joe Bowbeer
>>>> *Cc:* concurrency-interest at cs.oswego.edu
>>>> *Subject:* Re: [concurrency-interest] Bounded TransferQueue
>>>>
>>>> Thanks Joe,
>>>>
>>>> The problem with that would be that I'd have to wrap the queue and make
>>>> sure I have the Semaphore in place in all places that matter, and then
>>>> implement remainingCapacity etc, I'd rather not do that so if it's possible
>>>> to avoid...
>>>>
>>>> If someone has an impl under a permissive license (ApacheV2, MIT, BSD or
>>>> such) I'd be very thankful for it.
>>>>
>>>> Cheers,
>>>>
>>>> On Mon, Aug 23, 2010 at 10:56 PM, Joe Bowbeer <joe.bowbeer at gmail.com>wrote:
>>>>
>>>>> In a previous discussion, Doug Lea wrote:
>>>>>
>>>>> "For the bounded case, it's hard to do any better than use a Semaphore
>>>>> in front of a LinkedTransferQueue."
>>>>>
>>>>>
>>>>> http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html
>>>>>
>>>>> I don't know if the thinking has changed any since then.
>>>>>
>>>>> Joe
>>>>>
>>>>> On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:
>>>>>
>>>>> Hi folks,
>>>>>>
>>>>>> It's late here and I've rummaged through the Internet in it's entirety
>>>>>> so I'll get straight to the point,
>>>>>>
>>>>>> I have a dire need for a bounded TransferQueue, and the
>>>>>> LinkedTransferQueue states that it's unbounded,
>>>>>> does anyone have any suggestions where I can find a (highly
>>>>>> performant) bounded TransferQueue?
>>>>>>
>>>>>> Best regards,
>>>>>> --
>>>>>>
>>>>>
>> There is a  QueueLimitedThreadPool.java in sun io framework grizzly ,
>> glassfish appserver  that is using LTQ with an Atomic to limit the queue.
>> Unless its changed after i implemented it , Its used if your not putting
>> -1 as queue limit while using equal min max pool size in glassfish,
>> If you set pool min and max to same value , and -1 as queue limit a pure
>> LTQ is used.
>>
>> I cant find a working link to it now.
>>
>> At high contention the Atomic counter is ending up looping in its internal
>> methods, the throughput can become similar to a synchronized design.
>> Its a lot better if its possible to handle the limit at other places, lets
>> say you have many connections that can produce work,
>> if you limit the per outstanding work per connection, you will indirectly
>> control the overall limit and don't need to kill concurrent scalability in
>> the global queue.
>>
>> The avoid to re inventing the weel is a good concept, but should never be
>> an excuse to not learn new things due to fear.
>> You can always ask for feedback here, to help you QA the implementation if
>> you decide to do it.
>>
>
> I agree, but I don't need it for some hobby project, I need it to fit into
> a high-performance concurrency framework, so I'd rather avoid to waste a lot
> of time to tune a makeshift implementation.
>
>
I implemented it myself and its used in real projects, no problems since its
indeed a very easy 10 min job to do =).
Whats the QA on some random code from the net anyway , that your asking for
?.
you cant use some random external code and blame problems on it...
Its your choice to use it and hence the same need for understanding as it
takes to write it yourself, how else can you possible know it will work ?.
But i guess it feels better to be able to put blame somebody else
regardless, and hope management dont see through it ? =).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100824/ce47ca87/attachment.html>

From viktor.klang at gmail.com  Tue Aug 24 05:09:19 2010
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 24 Aug 2010 11:09:19 +0200
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <AANLkTikwGau+1MNz_EdF-9brTvgm4=oYXT-=gahXv-8=@mail.gmail.com>
References: <AANLkTin9N2_D09O8KCb-BDx3ayWc2Jt4EePqT6VNEOra@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEOFIHAA.davidcholmes@aapt.net.au>
	<AANLkTikFCFchBaHEittxDWMf6FBycQt2vkWOGUrekp2j@mail.gmail.com>
	<AANLkTimjM6fNvUcsvDjLdSAdidp0yr_zYt_T_5cMndZ+@mail.gmail.com>
	<AANLkTikEB+oM3k0Jx62SXgBMuLmaL_La8fEiKGSgg7G1@mail.gmail.com>
	<AANLkTikwGau+1MNz_EdF-9brTvgm4=oYXT-=gahXv-8=@mail.gmail.com>
Message-ID: <AANLkTi=M8V7arzUJEajEs9+Eq-58eyM83aUkLeZERkKX@mail.gmail.com>

On Tue, Aug 24, 2010 at 10:57 AM, gustav <gustav.trede at gmail.com> wrote:

>
>
> On 24 August 2010 10:43, Viktor Klang <viktor.klang at gmail.com> wrote:
>
>>
>>
>> On Tue, Aug 24, 2010 at 10:35 AM, gustav <gustav.trede at gmail.com> wrote:
>>
>>>
>>>
>>> On 24 August 2010 09:54, Viktor Klang <viktor.klang at gmail.com> wrote:
>>>
>>>>
>>>>
>>>> On Tue, Aug 24, 2010 at 9:48 AM, David Holmes <davidcholmes at aapt.net.au
>>>> > wrote:
>>>>
>>>>>  Not sure I understand your "problem" here. You write a
>>>>> BoundedTransferQueue that has-a LinkedTransferQueue and a Semaphore.
>>>>>
>>>>
>>>> Yes, that's my fall-back option, but if I don't need to reinvent the
>>>> wheel I'd rather not.
>>>>
>>>>
>>>>>
>>>>> David Holmes
>>>>>
>>>>> -----Original Message-----
>>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Viktor Klang
>>>>> *Sent:* Tuesday, 24 August 2010 5:39 PM
>>>>> *To:* Joe Bowbeer
>>>>> *Cc:* concurrency-interest at cs.oswego.edu
>>>>> *Subject:* Re: [concurrency-interest] Bounded TransferQueue
>>>>>
>>>>> Thanks Joe,
>>>>>
>>>>> The problem with that would be that I'd have to wrap the queue and make
>>>>> sure I have the Semaphore in place in all places that matter, and then
>>>>> implement remainingCapacity etc, I'd rather not do that so if it's possible
>>>>> to avoid...
>>>>>
>>>>> If someone has an impl under a permissive license (ApacheV2, MIT, BSD
>>>>> or such) I'd be very thankful for it.
>>>>>
>>>>> Cheers,
>>>>>
>>>>> On Mon, Aug 23, 2010 at 10:56 PM, Joe Bowbeer <joe.bowbeer at gmail.com>wrote:
>>>>>
>>>>>> In a previous discussion, Doug Lea wrote:
>>>>>>
>>>>>> "For the bounded case, it's hard to do any better than use a Semaphore
>>>>>> in front of a LinkedTransferQueue."
>>>>>>
>>>>>>
>>>>>> http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html
>>>>>>
>>>>>> I don't know if the thinking has changed any since then.
>>>>>>
>>>>>> Joe
>>>>>>
>>>>>> On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:
>>>>>>
>>>>>> Hi folks,
>>>>>>>
>>>>>>> It's late here and I've rummaged through the Internet in it's
>>>>>>> entirety so I'll get straight to the point,
>>>>>>>
>>>>>>> I have a dire need for a bounded TransferQueue, and the
>>>>>>> LinkedTransferQueue states that it's unbounded,
>>>>>>> does anyone have any suggestions where I can find a (highly
>>>>>>> performant) bounded TransferQueue?
>>>>>>>
>>>>>>> Best regards,
>>>>>>> --
>>>>>>>
>>>>>>
>>> There is a  QueueLimitedThreadPool.java in sun io framework grizzly ,
>>> glassfish appserver  that is using LTQ with an Atomic to limit the queue.
>>> Unless its changed after i implemented it , Its used if your not putting
>>> -1 as queue limit while using equal min max pool size in glassfish,
>>> If you set pool min and max to same value , and -1 as queue limit a pure
>>> LTQ is used.
>>>
>>> I cant find a working link to it now.
>>>
>>> At high contention the Atomic counter is ending up looping in its
>>> internal methods, the throughput can become similar to a synchronized
>>> design.
>>> Its a lot better if its possible to handle the limit at other places,
>>> lets say you have many connections that can produce work,
>>> if you limit the per outstanding work per connection, you will indirectly
>>> control the overall limit and don't need to kill concurrent scalability in
>>> the global queue.
>>>
>>> The avoid to re inventing the weel is a good concept, but should never be
>>> an excuse to not learn new things due to fear.
>>> You can always ask for feedback here, to help you QA the implementation
>>> if you decide to do it.
>>>
>>
>> I agree, but I don't need it for some hobby project, I need it to fit into
>> a high-performance concurrency framework, so I'd rather avoid to waste a lot
>> of time to tune a makeshift implementation.
>>
>>
> I implemented it myself and its used in real projects, no problems since
> its indeed a very easy 10 min job to do =).
> Whats the QA on some random code from the net anyway , that your asking for
> ?.
> you cant use some random external code and blame problems on it...
> Its your choice to use it and hence the same need for understanding as it
> takes to write it yourself, how else can you possible know it will work ?.
> But i guess it feels better to be able to put blame somebody else
> regardless, and hope management dont see through it ? =).
>


Haha, you've got a very good point, I'll write it myself.

And on that topic I think it'd be a very good idea to provide pre-built
tests for the different interfaces, so anyone who implements TransferQueue
or whatnot already has a harness for it.



-- 
Viktor Klang,
Code Connoisseur
Work:   www.akkasource.com
Code:   github.com/viktorklang
Follow: twitter.com/viktorklang
Read:   klangism.tumblr.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100824/9725c21f/attachment-0001.html>

From gustav.trede at gmail.com  Tue Aug 24 05:33:54 2010
From: gustav.trede at gmail.com (gustav)
Date: Tue, 24 Aug 2010 11:33:54 +0200
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <AANLkTi=M8V7arzUJEajEs9+Eq-58eyM83aUkLeZERkKX@mail.gmail.com>
References: <AANLkTin9N2_D09O8KCb-BDx3ayWc2Jt4EePqT6VNEOra@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEOFIHAA.davidcholmes@aapt.net.au>
	<AANLkTikFCFchBaHEittxDWMf6FBycQt2vkWOGUrekp2j@mail.gmail.com>
	<AANLkTimjM6fNvUcsvDjLdSAdidp0yr_zYt_T_5cMndZ+@mail.gmail.com>
	<AANLkTikEB+oM3k0Jx62SXgBMuLmaL_La8fEiKGSgg7G1@mail.gmail.com>
	<AANLkTikwGau+1MNz_EdF-9brTvgm4=oYXT-=gahXv-8=@mail.gmail.com>
	<AANLkTi=M8V7arzUJEajEs9+Eq-58eyM83aUkLeZERkKX@mail.gmail.com>
Message-ID: <AANLkTikEcemjaHRkrNHNYPxJ9fuS0fcMa0yjDPDTzR4o@mail.gmail.com>

On 24 August 2010 11:09, Viktor Klang <viktor.klang at gmail.com> wrote:

>
>
> On Tue, Aug 24, 2010 at 10:57 AM, gustav <gustav.trede at gmail.com> wrote:
>
>>
>>
>> On 24 August 2010 10:43, Viktor Klang <viktor.klang at gmail.com> wrote:
>>
>>>
>>>
>>> On Tue, Aug 24, 2010 at 10:35 AM, gustav <gustav.trede at gmail.com> wrote:
>>>
>>>>
>>>>
>>>> On 24 August 2010 09:54, Viktor Klang <viktor.klang at gmail.com> wrote:
>>>>
>>>>>
>>>>>
>>>>> On Tue, Aug 24, 2010 at 9:48 AM, David Holmes <
>>>>> davidcholmes at aapt.net.au> wrote:
>>>>>
>>>>>>  Not sure I understand your "problem" here. You write a
>>>>>> BoundedTransferQueue that has-a LinkedTransferQueue and a Semaphore.
>>>>>>
>>>>>
>>>>> Yes, that's my fall-back option, but if I don't need to reinvent the
>>>>> wheel I'd rather not.
>>>>>
>>>>>
>>>>>>
>>>>>> David Holmes
>>>>>>
>>>>>> -----Original Message-----
>>>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Viktor
>>>>>> Klang
>>>>>> *Sent:* Tuesday, 24 August 2010 5:39 PM
>>>>>> *To:* Joe Bowbeer
>>>>>> *Cc:* concurrency-interest at cs.oswego.edu
>>>>>> *Subject:* Re: [concurrency-interest] Bounded TransferQueue
>>>>>>
>>>>>> Thanks Joe,
>>>>>>
>>>>>> The problem with that would be that I'd have to wrap the queue and
>>>>>> make sure I have the Semaphore in place in all places that matter, and then
>>>>>> implement remainingCapacity etc, I'd rather not do that so if it's possible
>>>>>> to avoid...
>>>>>>
>>>>>> If someone has an impl under a permissive license (ApacheV2, MIT, BSD
>>>>>> or such) I'd be very thankful for it.
>>>>>>
>>>>>> Cheers,
>>>>>>
>>>>>> On Mon, Aug 23, 2010 at 10:56 PM, Joe Bowbeer <joe.bowbeer at gmail.com>wrote:
>>>>>>
>>>>>>> In a previous discussion, Doug Lea wrote:
>>>>>>>
>>>>>>> "For the bounded case, it's hard to do any better than use a
>>>>>>> Semaphore in front of a LinkedTransferQueue."
>>>>>>>
>>>>>>>
>>>>>>> http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html
>>>>>>>
>>>>>>> I don't know if the thinking has changed any since then.
>>>>>>>
>>>>>>> Joe
>>>>>>>
>>>>>>> On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:
>>>>>>>
>>>>>>> Hi folks,
>>>>>>>>
>>>>>>>> It's late here and I've rummaged through the Internet in it's
>>>>>>>> entirety so I'll get straight to the point,
>>>>>>>>
>>>>>>>> I have a dire need for a bounded TransferQueue, and the
>>>>>>>> LinkedTransferQueue states that it's unbounded,
>>>>>>>> does anyone have any suggestions where I can find a (highly
>>>>>>>> performant) bounded TransferQueue?
>>>>>>>>
>>>>>>>> Best regards,
>>>>>>>> --
>>>>>>>>
>>>>>>>
>>>> There is a  QueueLimitedThreadPool.java in sun io framework grizzly ,
>>>> glassfish appserver  that is using LTQ with an Atomic to limit the queue.
>>>> Unless its changed after i implemented it , Its used if your not putting
>>>> -1 as queue limit while using equal min max pool size in glassfish,
>>>> If you set pool min and max to same value , and -1 as queue limit a pure
>>>> LTQ is used.
>>>>
>>>> I cant find a working link to it now.
>>>>
>>>> At high contention the Atomic counter is ending up looping in its
>>>> internal methods, the throughput can become similar to a synchronized
>>>> design.
>>>> Its a lot better if its possible to handle the limit at other places,
>>>> lets say you have many connections that can produce work,
>>>> if you limit the per outstanding work per connection, you will
>>>> indirectly control the overall limit and don't need to kill concurrent
>>>> scalability in the global queue.
>>>>
>>>> The avoid to re inventing the weel is a good concept, but should never
>>>> be an excuse to not learn new things due to fear.
>>>> You can always ask for feedback here, to help you QA the implementation
>>>> if you decide to do it.
>>>>
>>>
>>> I agree, but I don't need it for some hobby project, I need it to fit
>>> into a high-performance concurrency framework, so I'd rather avoid to waste
>>> a lot of time to tune a makeshift implementation.
>>>
>>>
>> I implemented it myself and its used in real projects, no problems since
>> its indeed a very easy 10 min job to do =).
>> Whats the QA on some random code from the net anyway , that your asking
>> for ?.
>> you cant use some random external code and blame problems on it...
>> Its your choice to use it and hence the same need for understanding as it
>> takes to write it yourself, how else can you possible know it will work ?.
>> But i guess it feels better to be able to put blame somebody else
>> regardless, and hope management dont see through it ? =).
>>
>
>
> Haha, you've got a very good point, I'll write it myself.
>
> That's the spirit !.

For a simple wrapper its enough to ensure the increment and decrements on
the atomics are balanced around the offer, poll methods.
Test harnesses for concurrent stuff is only good up to a limit,
Its probably impossible to truly ensure you trigger all theoretical special
cases in a concurrent environment on all platforms.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100824/bf7810df/attachment.html>

From fslzdd at gmail.com  Tue Aug 24 07:26:09 2010
From: fslzdd at gmail.com (=?GB2312?B?tO/X0w==?=)
Date: Tue, 24 Aug 2010 19:26:09 +0800
Subject: [concurrency-interest] Useful multithreaded tool for Java
Message-ID: <AANLkTinWUrGR9C+6fCLwQTqfE6G9GmHGPqMbYPNrH7LD@mail.gmail.com>

All,

I am happy to announce that MulticoreSDK v2.3.3 is released in
alphaworks<http://www.alphaworks.ibm.com/tech/msdk>.


MulticoreSDK toolkit includes various valuable features, such as data race
detection, deadlock detection, unit test framework for parallel programs,
lock profiling tool, etc. These tools are provably useful to test, debug and
analyze applications targeted for multicore hardware systems.

Release Highlight:
-- Unified Java lock contention profiler, capable of handling j.u.c lock and
Java lock monitor.
-- Unified installation package for all command-line tools.
-- All command-line tools support Windows/x86_32, Linux/x86_32,
Linux/x86_64, AIX/PPC_64 platforms.

Any comment or feedback is warmly welcome. Thanks!

Regards,
Daniel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100824/0e4618ab/attachment.html>

From jason_mehrens at hotmail.com  Wed Aug 25 14:26:43 2010
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Wed, 25 Aug 2010 13:26:43 -0500
Subject: [concurrency-interest] Reference/IdentityMap toString
In-Reply-To: <AANLkTinEttR6600RLCDyiPD-jupgFZoCjMR5DjEQMg-o@mail.gmail.com>
References: <SNT114-W83219BFFB72BD91C066A2839C0@phx.gbl>, <4C6BFAC1.4050502@kav.dk>,
	<SNT114-W31A7E1EEE0D40513A7D1A1839D0@phx.gbl>,
	<4C6C3746.9050406@kav.dk>,
	<SNT114-W89C5F706778171AFB9BBF839D0@phx.gbl>,
	<4C6CD3FD.5030803@kav.dk>,
	<SNT114-W490FAD6D794521FE029728839E0@phx.gbl>,
	<4C6E232A.3040707@atlassian.com>,
	<SNT114-W62EC3F26ADFC34FC684D21839F0@phx.gbl>,
	<AANLkTikB8eb_-jLKPGonn-0W=VytSO31V9b7fZkWLtG1@mail.gmail.com>,
	<SNT114-W285C99497C113C628AF52A839F0@phx.gbl>,
	<AANLkTikci=OasofoKOexpts-ATDAuSwHPztLwjVD-XHt@mail.gmail.com>,
	<SNT114-W1422B073017D6B47FC97D483820@phx.gbl>,
	<AANLkTinEttR6600RLCDyiPD-jupgFZoCjMR5DjEQMg-o@mail.gmail.com>
Message-ID: <SNT114-W6493EDC15EC947AEE5FC1083840@phx.gbl>


Tim,
 
>>"I don't accept the premise that AbstractMap.toString "suffers from some issues" when used as part of an implementation of an identity map. AbstractMap.toString is not broken or deficient;" 

I listed four failure cases and backed it up with examples (uniqueness was only one aspect).  Where is the counter argument that AbstractMap.toString is right for an identity context?  No one has defined why it is right, best, or only choice.
 
 
>>you are asking too much of it. You're trying to turn it into a serialization mechanism, something it is not meant for. 
I was hoping for creative insight on map theory but, I've only succeeded in stirring up creative insight on what I'm doing and why I'm doing it.


 
>>If you feel you really do need uniqueness of string representations for your identity maps, you can achieve it without imposing extra complications for those who don't. Several people have given examples of how you might accomplish this in practice.
This doesn't answer first question of my first post on this thread.  I'm not going to accept counter arguments of "why do that", "don't do that", "don't see the need", "use a workaround", or "don't mess with the JDK" when I'm asking about theory.
The points you raised on bijection were very helpful.
 
Thanks,
 
Jason

  		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100825/054c63e6/attachment.html>

From mohit.riverstone at gmail.com  Thu Aug 26 23:49:40 2010
From: mohit.riverstone at gmail.com (Mohit Kumar)
Date: Fri, 27 Aug 2010 09:19:40 +0530
Subject: [concurrency-interest] Numa and ReentrantLock
Message-ID: <4c7735e4.10c98e0a.47d4.ffffe2fe@mx.google.com>

Hi Doug,

 

I have a requirement where-in I spin by calling "trylock()" a configured
number of times before i go in for a blocking version.

Something like adaptive spinning on intrinsic locks.

 

My only concern is this spinning is on a single variable which would be on
remote memory for all but one thread on NUMA machines.

 

Regards

Mohit

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100827/b5387151/attachment.html>

From dl at cs.oswego.edu  Fri Aug 27 08:37:40 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 27 Aug 2010 08:37:40 -0400 (EDT)
Subject: [concurrency-interest] Numa and ReentrantLock
In-Reply-To: <4c7735e4.10c98e0a.47d4.ffffe2fe@mx.google.com>
References: <4c7735e4.10c98e0a.47d4.ffffe2fe@mx.google.com>
Message-ID: <56210.98.211.93.92.1282912660.squirrel@altair.cs.oswego.edu>

> Hi Doug,
>
>
>
> I have a requirement where-in I spin by calling "trylock()" a configured
> number of times before i go in for a blocking version.
>
> Something like adaptive spinning on intrinsic locks.

Yes, this is an intended common use for tryLock -- we in the
library don't know if/how you'd like to do adaptive spins,
but we try to make it easy to do so.

>
> My only concern is this spinning is on a single variable which would be on
> remote memory for all but one thread on NUMA machines.

This is unlikely to be a major concern, but you might heuristically
improve good-citizenship by occasionally invoking Thread.yield()
if you spin a lot. For example, yield every 64 spins. This has no
guaranteed effects, but in practice helps.

-Doug






>
>
> Regards
>
> Mohit
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



From viktor.klang at gmail.com  Fri Aug 27 08:48:56 2010
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 27 Aug 2010 14:48:56 +0200
Subject: [concurrency-interest] Bounded TransferQueue
In-Reply-To: <AANLkTikEcemjaHRkrNHNYPxJ9fuS0fcMa0yjDPDTzR4o@mail.gmail.com>
References: <AANLkTin9N2_D09O8KCb-BDx3ayWc2Jt4EePqT6VNEOra@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEOFIHAA.davidcholmes@aapt.net.au>
	<AANLkTikFCFchBaHEittxDWMf6FBycQt2vkWOGUrekp2j@mail.gmail.com>
	<AANLkTimjM6fNvUcsvDjLdSAdidp0yr_zYt_T_5cMndZ+@mail.gmail.com>
	<AANLkTikEB+oM3k0Jx62SXgBMuLmaL_La8fEiKGSgg7G1@mail.gmail.com>
	<AANLkTikwGau+1MNz_EdF-9brTvgm4=oYXT-=gahXv-8=@mail.gmail.com>
	<AANLkTi=M8V7arzUJEajEs9+Eq-58eyM83aUkLeZERkKX@mail.gmail.com>
	<AANLkTikEcemjaHRkrNHNYPxJ9fuS0fcMa0yjDPDTzR4o@mail.gmail.com>
Message-ID: <AANLkTimwxRj8MbhYKiq4k3NNjnPT=g-8JytvUskd0u9C@mail.gmail.com>

In case someone needs a bounded transfer queue (scala), here's one:

https://gist.github.com/2ee0b136fe17ff4414b7

On Tue, Aug 24, 2010 at 11:33 AM, gustav <gustav.trede at gmail.com> wrote:

>
>
> On 24 August 2010 11:09, Viktor Klang <viktor.klang at gmail.com> wrote:
>
>>
>>
>> On Tue, Aug 24, 2010 at 10:57 AM, gustav <gustav.trede at gmail.com> wrote:
>>
>>>
>>>
>>> On 24 August 2010 10:43, Viktor Klang <viktor.klang at gmail.com> wrote:
>>>
>>>>
>>>>
>>>> On Tue, Aug 24, 2010 at 10:35 AM, gustav <gustav.trede at gmail.com>wrote:
>>>>
>>>>>
>>>>>
>>>>> On 24 August 2010 09:54, Viktor Klang <viktor.klang at gmail.com> wrote:
>>>>>
>>>>>>
>>>>>>
>>>>>> On Tue, Aug 24, 2010 at 9:48 AM, David Holmes <
>>>>>> davidcholmes at aapt.net.au> wrote:
>>>>>>
>>>>>>>  Not sure I understand your "problem" here. You write a
>>>>>>> BoundedTransferQueue that has-a LinkedTransferQueue and a Semaphore.
>>>>>>>
>>>>>>
>>>>>> Yes, that's my fall-back option, but if I don't need to reinvent the
>>>>>> wheel I'd rather not.
>>>>>>
>>>>>>
>>>>>>>
>>>>>>> David Holmes
>>>>>>>
>>>>>>> -----Original Message-----
>>>>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Viktor
>>>>>>> Klang
>>>>>>> *Sent:* Tuesday, 24 August 2010 5:39 PM
>>>>>>> *To:* Joe Bowbeer
>>>>>>> *Cc:* concurrency-interest at cs.oswego.edu
>>>>>>> *Subject:* Re: [concurrency-interest] Bounded TransferQueue
>>>>>>>
>>>>>>> Thanks Joe,
>>>>>>>
>>>>>>> The problem with that would be that I'd have to wrap the queue and
>>>>>>> make sure I have the Semaphore in place in all places that matter, and then
>>>>>>> implement remainingCapacity etc, I'd rather not do that so if it's possible
>>>>>>> to avoid...
>>>>>>>
>>>>>>> If someone has an impl under a permissive license (ApacheV2, MIT, BSD
>>>>>>> or such) I'd be very thankful for it.
>>>>>>>
>>>>>>> Cheers,
>>>>>>>
>>>>>>> On Mon, Aug 23, 2010 at 10:56 PM, Joe Bowbeer <joe.bowbeer at gmail.com
>>>>>>> > wrote:
>>>>>>>
>>>>>>>> In a previous discussion, Doug Lea wrote:
>>>>>>>>
>>>>>>>> "For the bounded case, it's hard to do any better than use a
>>>>>>>> Semaphore in front of a LinkedTransferQueue."
>>>>>>>>
>>>>>>>>
>>>>>>>> http://cs.oswego.edu/pipermail/concurrency-interest/2007-May/004108.html
>>>>>>>>
>>>>>>>> I don't know if the thinking has changed any since then.
>>>>>>>>
>>>>>>>> Joe
>>>>>>>>
>>>>>>>> On Mon, Aug 23, 2010 at 1:19 PM, Viktor Klang wrote:
>>>>>>>>
>>>>>>>> Hi folks,
>>>>>>>>>
>>>>>>>>> It's late here and I've rummaged through the Internet in it's
>>>>>>>>> entirety so I'll get straight to the point,
>>>>>>>>>
>>>>>>>>> I have a dire need for a bounded TransferQueue, and the
>>>>>>>>> LinkedTransferQueue states that it's unbounded,
>>>>>>>>> does anyone have any suggestions where I can find a (highly
>>>>>>>>> performant) bounded TransferQueue?
>>>>>>>>>
>>>>>>>>> Best regards,
>>>>>>>>> --
>>>>>>>>>
>>>>>>>>
>>>>> There is a  QueueLimitedThreadPool.java in sun io framework grizzly ,
>>>>> glassfish appserver  that is using LTQ with an Atomic to limit the queue.
>>>>> Unless its changed after i implemented it , Its used if your not
>>>>> putting -1 as queue limit while using equal min max pool size in glassfish,
>>>>> If you set pool min and max to same value , and -1 as queue limit a
>>>>> pure LTQ is used.
>>>>>
>>>>> I cant find a working link to it now.
>>>>>
>>>>> At high contention the Atomic counter is ending up looping in its
>>>>> internal methods, the throughput can become similar to a synchronized
>>>>> design.
>>>>> Its a lot better if its possible to handle the limit at other places,
>>>>> lets say you have many connections that can produce work,
>>>>> if you limit the per outstanding work per connection, you will
>>>>> indirectly control the overall limit and don't need to kill concurrent
>>>>> scalability in the global queue.
>>>>>
>>>>> The avoid to re inventing the weel is a good concept, but should never
>>>>> be an excuse to not learn new things due to fear.
>>>>> You can always ask for feedback here, to help you QA the implementation
>>>>> if you decide to do it.
>>>>>
>>>>
>>>> I agree, but I don't need it for some hobby project, I need it to fit
>>>> into a high-performance concurrency framework, so I'd rather avoid to waste
>>>> a lot of time to tune a makeshift implementation.
>>>>
>>>>
>>> I implemented it myself and its used in real projects, no problems since
>>> its indeed a very easy 10 min job to do =).
>>> Whats the QA on some random code from the net anyway , that your asking
>>> for ?.
>>> you cant use some random external code and blame problems on it...
>>> Its your choice to use it and hence the same need for understanding as it
>>> takes to write it yourself, how else can you possible know it will work ?.
>>> But i guess it feels better to be able to put blame somebody else
>>> regardless, and hope management dont see through it ? =).
>>>
>>
>>
>> Haha, you've got a very good point, I'll write it myself.
>>
>> That's the spirit !.
>
> For a simple wrapper its enough to ensure the increment and decrements on
> the atomics are balanced around the offer, poll methods.
> Test harnesses for concurrent stuff is only good up to a limit,
> Its probably impossible to truly ensure you trigger all theoretical special
> cases in a concurrent environment on all platforms.
>
>


-- 
Viktor Klang,
Code Connoisseur
Work:   www.akkasource.com
Code:   github.com/viktorklang
Follow: twitter.com/viktorklang
Read:   klangism.tumblr.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100827/ef682a5e/attachment.html>

From sjlee at apache.org  Tue Aug 31 01:12:50 2010
From: sjlee at apache.org (Sangjin Lee)
Date: Mon, 30 Aug 2010 22:12:50 -0700
Subject: [concurrency-interest] the remove-on-cancel policy on
	ScheduledThreadPoolExecutor
Message-ID: <AANLkTi=o_vFG6mkjToq+hqu23EOy-6N6=R6cJ-D5aotc@mail.gmail.com>

Maybe this has been discussed in the past...

I've always been curious why ScheduledFuture.cancel() would not also remove
tasks from the queue (so would keep them live until the delay elapses) as an
optional behavior. I know one can override the decorateTask() method to
achieve the same effect, but it's slightly less than optimal as you need to
create an additional delegate object to achieve the same goal. Then I
checked the upcoming JDK 7 changes, and there seems to be such a setting
called the remove-on-cancel policy!

I looked some old versions of ScheduledThreadPoolExecutor, and it seems this
behavior was in but was removed at some point? Could someone shed light on
why it was decided not to include it in the existing versions of JDK? And
why the change in JDK 7? I would love to have this behavior, but I'm just
curious about the reason behind these decisions. Thanks!

Regards,
Sangjin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100830/e48c9d0f/attachment.html>

From trustin at gmail.com  Tue Aug 31 05:18:50 2010
From: trustin at gmail.com (=?UTF-8?B?IuydtO2drOyKuSAoVHJ1c3RpbiBMZWUpIg==?=)
Date: Tue, 31 Aug 2010 18:18:50 +0900
Subject: [concurrency-interest] the remove-on-cancel policy
	on	ScheduledThreadPoolExecutor
In-Reply-To: <AANLkTi=o_vFG6mkjToq+hqu23EOy-6N6=R6cJ-D5aotc@mail.gmail.com>
References: <AANLkTi=o_vFG6mkjToq+hqu23EOy-6N6=R6cJ-D5aotc@mail.gmail.com>
Message-ID: <4C7CC8FA.8040500@gmail.com>

ScheduledThreadPoolExecutor's queue implementation is heap, and it means
removing an arbitrary task from the queue takes O(logN) time which is
not very optimal.  If removal takes place very often, you will observe
very high CPU usage.

If remove-on-cancel policy became available, someone might have
addressed this issue in JDK 7.  However, I did not read the code yet, so
someone else will step up and let us know.

Anyway, you can call ThreadPoolExecutor.purge() to remove the cancelled
tasks from the queue manually.  Calling it for every 1000th cancellation
for example helped me a lot.

Alternatively, you can try a different timer implementation that has
minimal cancellation cost.  I find hashed wheel timer very useful:

    http://www.cse.wustl.edu/~cdgill/courses/cs6874/TimingWheels.ppt

and the following is my implementation for Netty:


http://docs.jboss.org/netty/3.2/xref/org/jboss/netty/util/HashedWheelTimer.html

HTH,
Trustin

On 08/31/2010 02:12 PM, Sangjin Lee wrote:
> Maybe this has been discussed in the past...
> 
> I've always been curious why ScheduledFuture.cancel() would not also
> remove tasks from the queue (so would keep them live until the delay
> elapses) as an optional behavior. I know one can override the
> decorateTask() method to achieve the same effect, but it's slightly less
> than optimal as you need to create an additional delegate object to
> achieve the same goal. Then I checked the upcoming JDK 7 changes, and
> there seems to be such a setting called the remove-on-cancel policy!
> 
> I looked some old versions of ScheduledThreadPoolExecutor, and it seems
> this behavior was in but was removed at some point? Could someone shed
> light on why it was decided not to include it in the existing versions
> of JDK? And why the change in JDK 7? I would love to have this behavior,
> but I'm just curious about the reason behind these decisions. Thanks!
> 
> Regards,
> Sangjin
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
what we call human nature in actuality is human habit
http://gleamynode.net/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 293 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100831/412efa97/attachment.bin>

From dl at cs.oswego.edu  Tue Aug 31 11:51:24 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 31 Aug 2010 11:51:24 -0400
Subject: [concurrency-interest] the remove-on-cancel policy
	on	ScheduledThreadPoolExecutor
In-Reply-To: <AANLkTi=o_vFG6mkjToq+hqu23EOy-6N6=R6cJ-D5aotc@mail.gmail.com>
References: <AANLkTi=o_vFG6mkjToq+hqu23EOy-6N6=R6cJ-D5aotc@mail.gmail.com>
Message-ID: <4C7D24FC.9070004@cs.oswego.edu>

On 08/31/10 01:12, Sangjin Lee wrote:
> Maybe this has been discussed in the past...
>
> I've always been curious why ScheduledFuture.cancel() would not also remove
> tasks from the queue (so would keep them live until the delay elapses) as an
> optional behavior. I know one can override the decorateTask() method to achieve
> the same effect, but it's slightly less than optimal as you need to create an
> additional delegate object to achieve the same goal. Then I checked the upcoming
> JDK 7 changes, and there seems to be such a setting called the remove-on-cancel
> policy!
>
> I looked some old versions of ScheduledThreadPoolExecutor, and it seems this
> behavior was in but was removed at some point? Could someone shed light on why
> it was decided not to include it in the existing versions of JDK? And why the
> change in JDK 7? I would love to have this behavior, but I'm just curious about
> the reason behind these decisions. Thanks!
>

The story on this is that it was (and presumably still is)
Sun/Oracle policy not to introduce API changes between
major releases. So even though we have had a version with
setRemoveOnCancelPolicy in our jsr166 CVS for years now,
the only distributed version is in openjdk7 (as well as
the jdk7 binary snapshots). And while it is a little
painful to set up, you can always get a copy from us
(either standalone or part of jsr166.jar) and place in your
bootclasspath to use it with JDK6+. (Get it via links at
http://gee.cs.oswego.edu/dl/concurrency-interest/index.html)

-Doug

From paulo.silveira at gmail.com  Tue Aug 31 14:14:41 2010
From: paulo.silveira at gmail.com (Paulo Silveira)
Date: Tue, 31 Aug 2010 15:14:41 -0300
Subject: [concurrency-interest] retrofitted wait/notify?
Message-ID: <AANLkTik6DVifaWWD-Z5TYJ6G9c_Nn7F88PO5LYYtA0Bx@mail.gmail.com>

Hello

Nowadays every concurrent collection will use ReentrantLock/Syncs (or
something similar) where we could also use wait/notify. I do
understand the advantages in using the concurrency primitives, so why
not using AbstractQueuedSynchronizer /UnfairSyncs to implement
wait/notify instead of its native implementations? There is probably
an obvious answer, but I cant figuer it out.

Paulo
--
Paulo Silveira
Caelum | Ensino e Inova??o
www.caelum.com.br
www.arquiteturajava.com.br


From kasper at kav.dk  Tue Aug 31 14:44:11 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Tue, 31 Aug 2010 20:44:11 +0200
Subject: [concurrency-interest] the remove-on-cancel policy on
	ScheduledThreadPoolExecutor
In-Reply-To: <4C7D24FC.9070004@cs.oswego.edu>
References: <AANLkTi=o_vFG6mkjToq+hqu23EOy-6N6=R6cJ-D5aotc@mail.gmail.com>
	<4C7D24FC.9070004@cs.oswego.edu>
Message-ID: <4C7D4D7B.9040002@kav.dk>

On 31/08/10 17.51, Doug Lea wrote:
>
> The story on this is that it was (and presumably still is)
> Sun/Oracle policy not to introduce API changes between
> major releases. So even though we have had a version with
> setRemoveOnCancelPolicy in our jsr166 CVS for years now,
> the only distributed version is in openjdk7 (as well as
> the jdk7 binary snapshots). And while it is a little
> painful to set up, you can always get a copy from us
> (either standalone or part of jsr166.jar) and place in your
> bootclasspath to use it with JDK6+. (Get it via links at
> http://gee.cs.oswego.edu/dl/concurrency-interest/index.html)
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

Would it make sense to at least clear the reference to the 
runnable/callable on future.cancel() to allow it to be gc'ed?

I haven't ever been bugged down by this but I could imagine some people 
might have.

Cheers
   Kasper

From jason_mehrens at hotmail.com  Tue Aug 31 18:03:14 2010
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Tue, 31 Aug 2010 17:03:14 -0500
Subject: [concurrency-interest] the remove-on-cancel policy on
 ScheduledThreadPoolExecutor
In-Reply-To: <4C7D4D7B.9040002@kav.dk>
References: <AANLkTi=o_vFG6mkjToq+hqu23EOy-6N6=R6cJ-D5aotc@mail.gmail.com>,
	<4C7D24FC.9070004@cs.oswego.edu>, <4C7D4D7B.9040002@kav.dk>
Message-ID: <SNT114-W390504658C2D0A1C2D4B80838A0@phx.gbl>


Kasper,
 
Agreed.  One would think that the stored callable could be a root of a larger object graph.  I suppose the preferred method is to create nice callable or create wrapper callable around the offending target callable.
 
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6213323
 
If futures and executors replace thread creation I don't really see why nulling a future task callable is any different from nulling the target field in a thread.
 
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4006245
 
I suppose that given the age difference between the two bugs there has been a lot of JVM changes.
 
Regards,
 
Jason
 
> Date: Tue, 31 Aug 2010 20:44:11 +0200
> From: kasper at kav.dk
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] the remove-on-cancel policy on ScheduledThreadPoolExecutor
> 
> On 31/08/10 17.51, Doug Lea wrote:
> >
> > The story on this is that it was (and presumably still is)
> > Sun/Oracle policy not to introduce API changes between
> > major releases. So even though we have had a version with
> > setRemoveOnCancelPolicy in our jsr166 CVS for years now,
> > the only distributed version is in openjdk7 (as well as
> > the jdk7 binary snapshots). And while it is a little
> > painful to set up, you can always get a copy from us
> > (either standalone or part of jsr166.jar) and place in your
> > bootclasspath to use it with JDK6+. (Get it via links at
> > http://gee.cs.oswego.edu/dl/concurrency-interest/index.html)
> >
> > -Doug
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> 
> Would it make sense to at least clear the reference to the 
> runnable/callable on future.cancel() to allow it to be gc'ed?
> 
> I haven't ever been bugged down by this but I could imagine some people 
> might have.
> 
> Cheers
> Kasper
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100831/5b4b5407/attachment.html>

From davidcholmes at aapt.net.au  Tue Aug 31 18:28:16 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 1 Sep 2010 08:28:16 +1000
Subject: [concurrency-interest] retrofitted wait/notify?
In-Reply-To: <AANLkTik6DVifaWWD-Z5TYJ6G9c_Nn7F88PO5LYYtA0Bx@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEAHIIAA.davidcholmes@aapt.net.au>

Paulo Silveira writes:
>
> Nowadays every concurrent collection will use ReentrantLock/Syncs (or
> something similar) where we could also use wait/notify.

While there are reasons to replace use of synchronized+wait/notify with
Locks+Conditions you should only do so when it makes sense - it is not
intended that all such uses can or should be converted.

>  I do
> understand the advantages in using the concurrency primitives, so why
> not using AbstractQueuedSynchronizer /UnfairSyncs to implement
> wait/notify instead of its native implementations? There is probably
> an obvious answer, but I cant figuer it out.

wait/notify are tightly intetegrated with monitors so you couldn't use AQS
unless you also replaced monitors with AQS - and that doesn't make sense.

David Holmes


From sjlee at apache.org  Tue Aug 31 19:41:44 2010
From: sjlee at apache.org (Sangjin Lee)
Date: Tue, 31 Aug 2010 16:41:44 -0700
Subject: [concurrency-interest] the remove-on-cancel policy on
	ScheduledThreadPoolExecutor
In-Reply-To: <4C7CC8FA.8040500@gmail.com>
References: <AANLkTi=o_vFG6mkjToq+hqu23EOy-6N6=R6cJ-D5aotc@mail.gmail.com>
	<4C7CC8FA.8040500@gmail.com>
Message-ID: <AANLkTi=HkLFzPegmqQeKD0d64YXESn-kLLETJC5EQeXi@mail.gmail.com>

Thanks. I'll check out the hashed wheel timer. Sounds quite interesting.

Yes, I know that trying to remove it every time you cancel can be expensive
(remove(Object) is actually linear). But we do support cases where the delay
is quite large and most of the tasks are promptly canceled. In this case, it
causes a pretty significant increase in used JVM heap and the associated
increases in GC cost. So, we want to provide an *option* where they can
remove on cancel, and they would come out ahead all things considered.

Thanks,
Sangjin

On Tue, Aug 31, 2010 at 2:18 AM, "??? (Trustin Lee)" <trustin at gmail.com>wrote:

> ScheduledThreadPoolExecutor's queue implementation is heap, and it means
> removing an arbitrary task from the queue takes O(logN) time which is
> not very optimal.  If removal takes place very often, you will observe
> very high CPU usage.
>
> If remove-on-cancel policy became available, someone might have
> addressed this issue in JDK 7.  However, I did not read the code yet, so
> someone else will step up and let us know.
>
> Anyway, you can call ThreadPoolExecutor.purge() to remove the cancelled
> tasks from the queue manually.  Calling it for every 1000th cancellation
> for example helped me a lot.
>
> Alternatively, you can try a different timer implementation that has
> minimal cancellation cost.  I find hashed wheel timer very useful:
>
>    http://www.cse.wustl.edu/~cdgill/courses/cs6874/TimingWheels.ppt
>
> and the following is my implementation for Netty:
>
>
>
> http://docs.jboss.org/netty/3.2/xref/org/jboss/netty/util/HashedWheelTimer.html
>
> HTH,
> Trustin
>
> On 08/31/2010 02:12 PM, Sangjin Lee wrote:
> > Maybe this has been discussed in the past...
> >
> > I've always been curious why ScheduledFuture.cancel() would not also
> > remove tasks from the queue (so would keep them live until the delay
> > elapses) as an optional behavior. I know one can override the
> > decorateTask() method to achieve the same effect, but it's slightly less
> > than optimal as you need to create an additional delegate object to
> > achieve the same goal. Then I checked the upcoming JDK 7 changes, and
> > there seems to be such a setting called the remove-on-cancel policy!
> >
> > I looked some old versions of ScheduledThreadPoolExecutor, and it seems
> > this behavior was in but was removed at some point? Could someone shed
> > light on why it was decided not to include it in the existing versions
> > of JDK? And why the change in JDK 7? I would love to have this behavior,
> > but I'm just curious about the reason behind these decisions. Thanks!
> >
> > Regards,
> > Sangjin
> >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
> what we call human nature in actuality is human habit
> http://gleamynode.net/
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100831/7db0b844/attachment.html>

From crazybob at crazybob.org  Tue Aug 31 22:34:48 2010
From: crazybob at crazybob.org (Bob Lee)
Date: Tue, 31 Aug 2010 19:34:48 -0700
Subject: [concurrency-interest] retrofitted wait/notify?
In-Reply-To: <AANLkTik6DVifaWWD-Z5TYJ6G9c_Nn7F88PO5LYYtA0Bx@mail.gmail.com>
References: <AANLkTik6DVifaWWD-Z5TYJ6G9c_Nn7F88PO5LYYtA0Bx@mail.gmail.com>
Message-ID: <AANLkTinBshaanwrn36NjoEOnTW92Q8bxPNytKWuokzP6@mail.gmail.com>

On Tue, Aug 31, 2010 at 11:14 AM, Paulo Silveira
<paulo.silveira at gmail.com>wrote:

> why
> not using AbstractQueuedSynchronizer /UnfairSyncs to implement
> wait/notify instead of its native implementations? There is probably
> an obvious answer, but I cant figuer it out.
>

wait() and notify() are typically highly optimized for memory and
performance. For example, the lock state bits are typically encoded in the
object header.

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100831/4c9d2f70/attachment-0001.html>

