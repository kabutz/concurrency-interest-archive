From al-javaconcurrencyinterest at none.at  Thu Mar  1 08:14:32 2012
From: al-javaconcurrencyinterest at none.at (Aleksandar Lazic)
Date: Thu, 01 Mar 2012 14:14:32 +0100
Subject: [concurrency-interest] Recursive Directory checker
In-Reply-To: <5f6a6d2ea2122aff907d07afd68a3f1d@none.at>
References: <31a188f4e9a53f784a280a7735877cd1@none.at>
	<4F47CE5E.3010805@oracle.com>
	<CACr06N0tPX-3tvBOpGPt-8EtmHDBrkYibC48CgyyegFFaBfRkA@mail.gmail.com>
	<5f6a6d2ea2122aff907d07afd68a3f1d@none.at>
Message-ID: <aa809496d28b600e7634184ef9587f26@none.at>

 

Dear List member, 

I have now a part solution


http://www.none.at/NasChecker02.zip 

with the follwoing libs.


http://pholser.github.com/jopt-simple/ 

http://logback.qos.ch/


http://www.slf4j.org/ 

http://jackson.codehaus.org/


http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar 

I have still the
question which Data structure (Queue,CopyOnWriteArrayList,...) can I
handle the recursive directory walk. 

Does I need a 'HandlingClass'?


####Code snipped main 

... 

File myFile = new
File(maConfigReal.getStartDir());
 mainLogger.debug(" myFile getStartDir
{}",myFile.listFiles().toString());

// Here I would add all dirs &
files into a queue and iterate over that queue
 File[] myFiles =
myFile.listFiles(); 

... 

for (File mF: myFiles){
 mainLogger.debug("
mF {} {} " ,mF, mF.isDirectory()?"Dir":"File");
 if(mF.isDirectory()){

toGoDirs.add(mF);
 sumHash.put(mF.toString(), new myRT(mF,toGoDirs));

mainPool.execute(sumHash.get(mF.toString()));
 }
 } 

.... 

#### 

###
code myRT 

... 

protected File compute() { 

... 

myFJFiles =
curDir.listFiles(); 

... 

for (File myFJFile: myFJFiles){

if(!clqft.contains(myFJFile))
 if(myFJFile.isDirectory()){

myRTLogger.debug(" <<< ADD mF isDir {}" ,myFJFile); 

// >>>>> How can I
put this DIR into the MAIN Queue 

// clqft.add(myFJFile); 

 }else
if(myFJFile.isFile()){

dirSizeHash.get(tmp).addAndGet(myFJFile.length());
// myRTLogger.debug("
mF isFile {} size {}" ,myFJFile,myFJFile.length());
 }else{

myRTLogger.info(" mF Unknown type {}" ,myFJFile);
 }
 } 

... 

###


Does I nee the 

mainPool.awaitTermination(10, TimeUnit.SECONDS); 

to
be on the save site that all processes are done with there work. 

Many
thanks for your help. 

Best regards 

Aleks 

On 24-02-2012 20:21,
Aleksandar Lazic wrote: 

> Hi, 
> 
> we scan over a NAS Share (NFS
Netapp Filer), due to this fact I don't think that the deep 
> 
> disk
handling is in my hand. 
> 
> I use currently the IO:AIO program
treescan from the IO:AIO perl module 
> 
>
http://cvs.schmorp.de/IO-AIO/bin/treescan?view=markup 
> 
> which use 8
thread to collect the necessary data. 
> 
> The both links below shows
my description from the perl point of view 
> 
>
http://lists.schmorp.de/pipermail/anyevent/2012q1/000227.html 
> 
>
http://lists.schmorp.de/pipermail/anyevent/2012q1/000231.html 
> 
> The
reason why I want to switch to Java is that i need solution which I
'just' 
> 
> need to extract and run not to install a lot of modules for
the dedicated script language. 
> 
> Please can you tell me what do you
suggest to handle the directories which are already scanned? 
> 
> Best
regards 
> 
> Aleks 
> 
> On 24-02-2012 19:29, Benedict Elliott Smith
wrote: 
> 
>> I hate to nitpick, but this is only true for sequential
reads; as soon as you devolve to random IO (and for large directory
trees metadata traversal is unlikely at best to remain sequential, even
if there are no other competing IO requests) you are much better with
multiple ops in flight so the disk can select the order it services them
and to some degree maximize throughput. When performance testing new
file servers I have found single threaded random IOPs are typically
dreadful, even with dozens of disks. 
>> In my experience a
multi-threaded directory traversal has usually been considerably faster
than single threaded. 
>> I don't think the choice of queue is likely to
have a material impact on the performance of this algorithm, Aleksandar;
IO will be your bottleneck. However, I think the use of a queue defeats
the point of using the ForkJoin framework. 
>> On 24 February 2012
17:52, Nathan Reynolds <nathan.reynolds at oracle.com [9]> wrote:
>> 
>>> I
would like to point out that hard disks perform best when accessed in a
single threaded manner. If you have 2 threads making requests, then the
disk head will have to swing back and forth between the 2 locations.
With only 1 thread, the disk head doesn't have to travel as much. Flash
disks (SSDs) are a different story. We have seen optimal throughput when
16 threads hit the disk concurrently. Your mileage will vary depending
upon the SSD. So, you may not get much better performance from your
directory size counter by using multiple threads.
>>> 
>>> I have found
on Windows that defragmenting the hard drive and placing all of the
directory meta data together makes this kind of thing run really fast.
(See MyDefrag). The disk head simply has to sit on the directory meta
data section of the hard disk. I realize you aren't running on Windows.
But, you might consider something similar.
>>> 
>>> Nathan Reynolds [5]
| Consulting Member of Technical Staff | 602.333.9091
>>> Oracle PSR
Engineering [6] | Server Technology 
>>> 
>>> On 2/24/2012 8:59 AM,
Aleksandar Lazic wrote: 
>>> 
>>>> Dear list members, 
>>>> 
>>>> I'm on
the way to write a directory counter. 
>>>> 
>>>> I'm new to all this
thread/fork stuff, so please accept my apologize 
>>>> for such a
'simple' question ;-) 
>>>> 
>>>> What is the 'best' Class for such a
program. 
>>>> 
>>>> ForkJoinTask 
>>>> RecursiveAction 
>>>>
RecursiveTask 
>>>> 
>>>> I plan to use for the main program. 
>>>>

>>>> pseudocode 
>>>> ### 
>>>> main: 
>>>> 
>>>> File startdir = new
File("/home/user/"); 
>>>> File[] files = file.listFiles() 
>>>> 
>>>>
add directories to the Queue. 
>>>> 
>>>> ----- 
>>>> I'm unsure which
Queue is the best for this? 
>>>> 
>>>>
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/Queue.html [1]

>>>> 
>>>> I tend to BlockingDeque 
>>>> ----- 
>>>> 
>>>> ForkJoinPool
fjp = new ForkJoinPool(5); 
>>>> 
>>>> foreach worker 
>>>> get
filesizes and $SummAtomicLong.addAndGet(filesizes); 
>>>> 
>>>> print
"the Directory and there subdirs have {} Mbytes", $SummAtomicLong 
>>>>

>>>> #### 
>>>> 
>>>> Worker: 
>>>> 
>>>> foreach directory 
>>>> if
directory is not in queue 
>>>> add directory to the Queue. 
>>>> 
>>>>
foreach file 
>>>> add filesize to
$workerAtomicLong.addAndGet(file.size); 
>>>> ### 
>>>> 
>>>> I hope it
is a little bit clear what I want to do ;-) 
>>>> 
>>>> No this is not a
Homework ;-) 
>>>> 
>>>> Should I use a global variable for the
SummAtomicLong? 
>>>> Should I use a global variable for the
DirectoryQueue? 
>>>> 
>>>> I expect that there are not more then
'ForkJoinPool(5)'-Threads/Processes which work 
>>>> on the disk, is
that right? 
>>>> 
>>>> I have try to understand some of the 
>>>> 
>>>>
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/ [2]

>>>> 
>>>> but I have still some questions. 
>>>> 
>>>> Many thanks for
all your help. 
>>>> 
>>>> Cheers 
>>>> Aleks 
>>>>
_______________________________________________ 
>>>>
Concurrency-interest mailing list 
>>>>
Concurrency-interest at cs.oswego.edu [3] 
>>>>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest [4]
>>> 
>>>
_______________________________________________
>>> Concurrency-interest
mailing list
>>> Concurrency-interest at cs.oswego.edu [7]
>>>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest [8]



Links:
------
[1]
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/Queue.html
[2]
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/
[3]
mailto:Concurrency-interest at cs.oswego.edu
[4]
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
[5]
http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds
[6]
http://psr.us.oracle.com/
[7]
mailto:Concurrency-interest at cs.oswego.edu
[8]
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
[9]
mailto:nathan.reynolds at oracle.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120301/89fb3b12/attachment-0001.html>

From yechielf at gigaspaces.com  Sun Mar  4 04:56:13 2012
From: yechielf at gigaspaces.com (Yechiel Feffer)
Date: Sun, 4 Mar 2012 01:56:13 -0800
Subject: [concurrency-interest] regarding synchronized command and object's
	footprint
Message-ID: <F5A3854EDBB4D440A098DEF1EFFC922B1F312C9CFD@IE2RD2XVS661.red002.local>

Hi
I heard a claim that execution of synchronized(object)  command  (maybe contended synchronized) raises the  object footprint in the java heap- a raise the (partially ?) remains even after all threads exited the synchronized block.
Is it correct ? Is it related to barrier info java maintains ?

Regards
Yechiel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120304/f289498d/attachment.html>

From davidcholmes at aapt.net.au  Sun Mar  4 05:49:59 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 4 Mar 2012 20:49:59 +1000
Subject: [concurrency-interest] regarding synchronized command and
	object'sfootprint
In-Reply-To: <F5A3854EDBB4D440A098DEF1EFFC922B1F312C9CFD@IE2RD2XVS661.red002.local>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHGJDAA.davidcholmes@aapt.net.au>

That is correct in Hotspot. If contention occurs a heavyweight monitor has
to be created (this is called monitor inflation). There is a freelist of
inflated monitors - if the list is non-empty then one of those monitors is
used, else a new one is created.  If the monitor becomes idle it can be
returned to the freelist during a safepoint. The monitors once created are
never destroyed.

It has nothing to do with "barrier info" whatever that might be.

David Holmes

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Yechiel
Feffer
Sent: Sunday, 4 March 2012 7:56 PM
To: 'Concurrency-interest at cs.oswego.edu'
Subject: [concurrency-interest] regarding synchronized command and
object'sfootprint


  Hi

  I heard a claim that execution of synchronized(object)  command  (maybe
contended synchronized) raises the  object footprint in the java heap- a
raise the (partially ?) remains even after all threads exited the
synchronized block.

  Is it correct ? Is it related to barrier info java maintains ?



  Regards

  Yechiel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120304/49822bab/attachment.html>

From viktor.klang at gmail.com  Sun Mar  4 06:05:46 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sun, 4 Mar 2012 12:05:46 +0100
Subject: [concurrency-interest] regarding synchronized command and
	object'sfootprint
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEHGJDAA.davidcholmes@aapt.net.au>
References: <F5A3854EDBB4D440A098DEF1EFFC922B1F312C9CFD@IE2RD2XVS661.red002.local>
	<NFBBKALFDCPFIDBNKAPCGEHGJDAA.davidcholmes@aapt.net.au>
Message-ID: <CANPzfU9KgKq+JO4ZY4QzMYgHTXMkT7gBKHTfnj7ccVaMXhaLgA@mail.gmail.com>

On Sun, Mar 4, 2012 at 11:49 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> That is correct in Hotspot. If contention occurs a heavyweight monitor has
> to be created (this is called monitor inflation). There is a freelist of
> inflated monitors - if the list is non-empty then one of those monitors is
> used, else a new one is created.  If the monitor becomes idle it can be
> returned to the freelist during a safepoint. The monitors once created are
> never destroyed.
>

Another very good reason to stick to CAS unless the exact semantics of
synchronized is desirable. But technically, if sequential access is
desired, one can still avoid synchronization by switching to the correct
model for that - actors.

Cheers,
?


>
> It has nothing to do with "barrier info" whatever that might be.
>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Yechiel Feffer
> *Sent:* Sunday, 4 March 2012 7:56 PM
> *To:* 'Concurrency-interest at cs.oswego.edu'
> *Subject:* [concurrency-interest] regarding synchronized command and
> object'sfootprint
>
>  Hi****
>
> I heard a claim that execution of synchronized(object)  command  (maybe
> contended synchronized) raises the  object footprint in the java heap- a
> raise the (partially ?) remains even after all threads exited the
> synchronized block.****
>
> Is it correct ? Is it related to barrier info java maintains ? ****
>
> ** **
>
> Regards****
>
> Yechiel****
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120304/7fb6ad5c/attachment.html>

From william.louth at jinspired.com  Sun Mar  4 06:21:05 2012
From: william.louth at jinspired.com (William Louth (JINSPIRED.COM))
Date: Sun, 04 Mar 2012 12:21:05 +0100
Subject: [concurrency-interest] regarding synchronized command and
	object'sfootprint
In-Reply-To: <CANPzfU9KgKq+JO4ZY4QzMYgHTXMkT7gBKHTfnj7ccVaMXhaLgA@mail.gmail.com>
References: <F5A3854EDBB4D440A098DEF1EFFC922B1F312C9CFD@IE2RD2XVS661.red002.local>
	<NFBBKALFDCPFIDBNKAPCGEHGJDAA.davidcholmes@aapt.net.au>
	<CANPzfU9KgKq+JO4ZY4QzMYgHTXMkT7gBKHTfnj7ccVaMXhaLgA@mail.gmail.com>
Message-ID: <4F535021.9000908@jinspired.com>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120304/18a1334e/attachment-0001.html>

From yechielf at gigaspaces.com  Sun Mar  4 06:23:37 2012
From: yechielf at gigaspaces.com (Yechiel Feffer)
Date: Sun, 4 Mar 2012 03:23:37 -0800
Subject: [concurrency-interest] regarding synchronized command and
 object'sfootprint
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEHGJDAA.davidcholmes@aapt.net.au>
References: <F5A3854EDBB4D440A098DEF1EFFC922B1F312C9CFD@IE2RD2XVS661.red002.local>
	<NFBBKALFDCPFIDBNKAPCGEHGJDAA.davidcholmes@aapt.net.au>
Message-ID: <F5A3854EDBB4D440A098DEF1EFFC922B1F312C9D0B@IE2RD2XVS661.red002.local>

So its not so bad- meaning the number of such monitors in the system =  maximum number of concurrent synchronized objects  since the jvm started. This cannot be more than several thousands  even for a very busy server.

From: David Holmes [mailto:davidcholmes at aapt.net.au]
Sent: ??? ? 04 ??? 2012 12:50
To: Yechiel Feffer; Concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] regarding synchronized command and object'sfootprint

That is correct in Hotspot. If contention occurs a heavyweight monitor has to be created (this is called monitor inflation). There is a freelist of inflated monitors - if the list is non-empty then one of those monitors is used, else a new one is created.  If the monitor becomes idle it can be returned to the freelist during a safepoint. The monitors once created are never destroyed.

It has nothing to do with "barrier info" whatever that might be.

David Holmes

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-interest-bounces at cs.oswego.edu]<mailto:[mailto:concurrency-interest-bounces at cs.oswego.edu]>On Behalf Of Yechiel Feffer
Sent: Sunday, 4 March 2012 7:56 PM
To: 'Concurrency-interest at cs.oswego.edu'
Subject: [concurrency-interest] regarding synchronized command and object'sfootprint
Hi
I heard a claim that execution of synchronized(object)  command  (maybe contended synchronized) raises the  object footprint in the java heap- a raise the (partially ?) remains even after all threads exited the synchronized block.
Is it correct ? Is it related to barrier info java maintains ?

Regards
Yechiel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120304/6b2cd9f0/attachment.html>

From viktor.klang at gmail.com  Sun Mar  4 06:35:53 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sun, 4 Mar 2012 12:35:53 +0100
Subject: [concurrency-interest] regarding synchronized command and
	object'sfootprint
In-Reply-To: <4F535021.9000908@jinspired.com>
References: <F5A3854EDBB4D440A098DEF1EFFC922B1F312C9CFD@IE2RD2XVS661.red002.local>
	<NFBBKALFDCPFIDBNKAPCGEHGJDAA.davidcholmes@aapt.net.au>
	<CANPzfU9KgKq+JO4ZY4QzMYgHTXMkT7gBKHTfnj7ccVaMXhaLgA@mail.gmail.com>
	<4F535021.9000908@jinspired.com>
Message-ID: <CANPzfU-1HXL7OgCJa5u85kdQJhFT7BWP4FhwFJdzuU8hHhnMrw@mail.gmail.com>

Parking threads is cheap...
On Mar 4, 2012 12:24 PM, "William Louth (JINSPIRED.COM)" <
william.louth at jinspired.com> wrote:

>  fix a relatively small memory allocation retention problem with a
> significant memory allocation rate problem...sounds great
>
> On 04/03/2012 12:05, ?iktor ?lang wrote:
>
>
>
> On Sun, Mar 4, 2012 at 11:49 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>>  That is correct in Hotspot. If contention occurs a heavyweight monitor
>> has to be created (this is called monitor inflation). There is a freelist
>> of inflated monitors - if the list is non-empty then one of those monitors
>> is used, else a new one is created.?  If the monitor becomes idle it can be
>> returned to the freelist during a safepoint. The monitors once created are
>> never destroyed.
>>
>
>  Another very good reason to stick to CAS unless the exact semantics of
> synchronized is desirable. But technically, if sequential access is
> desired, one can still avoid synchronization by switching to the correct
> model for that - actors.
>
>  Cheers,
> ???
> ?
>
>>  ?
>> It has nothing to do with "barrier info" whatever that might be.
>>  ?
>> David Holmes
>>   ?
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Yechiel Feffer
>> *Sent:* Sunday, 4 March 2012 7:56 PM
>> *To:* 'Concurrency-interest at cs.oswego.edu'
>> *Subject:* [concurrency-interest] regarding synchronized command and
>> object'sfootprint
>>
>>   Hi
>>
>> I heard a claim that execution of synchronized(object)?  command ? (maybe
>> contended synchronized) raises the?  object footprint in the java heap- a
>> raise the (partially ?) remains even after all threads exited the
>> synchronized block.
>>
>> Is it correct ? Is it related to barrier info java maintains ?
>>
>> ?
>>
>> Regards
>>
>> Yechiel
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>  --
> Viktor Klang
>
> Akka Tech Lead
> Typesafe <http://www.typesafe.com/>? - The software stack for
> applications that scale
>
> Twitter: @viktorklang
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120304/86979998/attachment.html>

From dl at cs.oswego.edu  Sun Mar  4 11:07:11 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 04 Mar 2012 11:07:11 -0500
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <4F20A70A.7030204@cs.oswego.edu>
References: <4F20A70A.7030204@cs.oswego.edu>
Message-ID: <4F53932F.7070004@cs.oswego.edu>

On 01/25/12 20:06, Doug Lea wrote:
> As promised for a while now, some updates to ForkJoin
> are available from the usual places linked from
> http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
>
>
> 3. One small API addition: Explicit support for task marking.
> It was cruel to tell people that they could use FJ for things
> like graph traversal but not have a simple way to mark tasks
> so they won't be revisited while processing a graph (among a few
> other common use cases).

... and almost equally cruel to only allow a single kind of mark.
These have been changed to methods that operate on any "short" tag,
which allows marking with different flavors, marking multiple subtasks,
etc. The use of "short" here is enough to support a much broader
range of uses. The methods are a little annoying to use though because
there are no "short" literals, so you have to say for example:
   if (task.compareAndSetForkJoinTaskTag((short)0, (short)1)) ...
to conditionally add a simple mark.

(No, we can't support the nicer-to-use "int" without hurting other
FJ usages.)

Sorry to those who have already been using the "mark" versions.
(But this is one reason why we let improvements sit in our repository
for a while before integrating with OpenJDK etc.)
It should be easy to change your code to use tags instead.

-Doug


From alexlamsl at gmail.com  Sun Mar  4 14:50:30 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Sun, 4 Mar 2012 19:50:30 +0000
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <4F53932F.7070004@cs.oswego.edu>
References: <4F20A70A.7030204@cs.oswego.edu> <4F53932F.7070004@cs.oswego.edu>
Message-ID: <CAGpACNvKBB95LVkLr+jXh2vMFrp3pDeNq9sPJh8n2HiqimDc0A@mail.gmail.com>

On Sun, Mar 4, 2012 at 4:07 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 01/25/12 20:06, Doug Lea wrote:
>>
>> As promised for a while now, some updates to ForkJoin
>> are available from the usual places linked from
>> http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
>>
>>
>> 3. One small API addition: Explicit support for task marking.
>> It was cruel to tell people that they could use FJ for things
>> like graph traversal but not have a simple way to mark tasks
>> so they won't be revisited while processing a graph (among a few
>> other common use cases).
>
>
> ... and almost equally cruel to only allow a single kind of mark.
> These have been changed to methods that operate on any "short" tag,
> which allows marking with different flavors, marking multiple subtasks,
> etc. The use of "short" here is enough to support a much broader
> range of uses. The methods are a little annoying to use though because
> there are no "short" literals, so you have to say for example:
> ?if (task.compareAndSetForkJoinTaskTag((short)0, (short)1)) ...
> to conditionally add a simple mark.
>
> (No, we can't support the nicer-to-use "int" without hurting other
> FJ usages.)

Would you mind elaborate more on this? Is this something to do with
the 2-byte overhead being too significantly when the number of tasks
are large?


Thanks,
Alex.


From dl at cs.oswego.edu  Sun Mar  4 14:55:21 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 04 Mar 2012 14:55:21 -0500
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <CAGpACNvKBB95LVkLr+jXh2vMFrp3pDeNq9sPJh8n2HiqimDc0A@mail.gmail.com>
References: <4F20A70A.7030204@cs.oswego.edu> <4F53932F.7070004@cs.oswego.edu>
	<CAGpACNvKBB95LVkLr+jXh2vMFrp3pDeNq9sPJh8n2HiqimDc0A@mail.gmail.com>
Message-ID: <4F53C8A9.7060702@cs.oswego.edu>

On 03/04/12 14:50, Alex Lam S.L. wrote:
>> (No, we can't support the nicer-to-use "int" without hurting other
>> FJ usages.)
>
> Would you mind elaborate more on this? Is this something to do with
> the 2-byte overhead being too significantly when the number of tasks
> are large?


No -- we have some spare bits in a control word that we must keep
atomically updated anyway. So we can let users use 16 of them for tags.
But we can't magically add any more bits and still maintain atomic
updates that are both cheap and coordinated with state changes.

-Doug

From karmazilla at gmail.com  Sun Mar  4 15:46:33 2012
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Sun, 4 Mar 2012 21:46:33 +0100
Subject: [concurrency-interest] regarding synchronized command and
	object'sfootprint
In-Reply-To: <CANPzfU-1HXL7OgCJa5u85kdQJhFT7BWP4FhwFJdzuU8hHhnMrw@mail.gmail.com>
References: <F5A3854EDBB4D440A098DEF1EFFC922B1F312C9CFD@IE2RD2XVS661.red002.local>
	<NFBBKALFDCPFIDBNKAPCGEHGJDAA.davidcholmes@aapt.net.au>
	<CANPzfU9KgKq+JO4ZY4QzMYgHTXMkT7gBKHTfnj7ccVaMXhaLgA@mail.gmail.com>
	<4F535021.9000908@jinspired.com>
	<CANPzfU-1HXL7OgCJa5u85kdQJhFT7BWP4FhwFJdzuU8hHhnMrw@mail.gmail.com>
Message-ID: <CACyP5Pfp6T5gq8kOgu=GZ+yh2S0t1jso33mDDJaoFzBmnV33aA@mail.gmail.com>

The LockSupport.park methods do a similar thing with the permits, but in a
racy maner, where it is possible that a new permit is allocated even though
the free-list isn't "supposed to be" empty. According to the code comments,
things are done this way to avoid an ABA problem with the free list, IIRC.

On Sun, Mar 4, 2012 at 12:35, ?iktor ?lang <viktor.klang at gmail.com> wrote:

> Parking threads is cheap...
> On Mar 4, 2012 12:24 PM, "William Louth (JINSPIRED.COM)" <
> william.louth at jinspired.com> wrote:
>
>>  fix a relatively small memory allocation retention problem with a
>> significant memory allocation rate problem...sounds great
>>
>> On 04/03/2012 12:05, ?iktor ?lang wrote:
>>
>>
>>
>> On Sun, Mar 4, 2012 at 11:49 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>>
>>>  That is correct in Hotspot. If contention occurs a heavyweight monitor
>>> has to be created (this is called monitor inflation). There is a freelist
>>> of inflated monitors - if the list is non-empty then one of those monitors
>>> is used, else a new one is created.?  If the monitor becomes idle it can be
>>> returned to the freelist during a safepoint. The monitors once created are
>>> never destroyed.
>>>
>>
>>  Another very good reason to stick to CAS unless the exact semantics of
>> synchronized is desirable. But technically, if sequential access is
>> desired, one can still avoid synchronization by switching to the correct
>> model for that - actors.
>>
>>  Cheers,
>> ???
>> ?
>>
>>>  ?
>>> It has nothing to do with "barrier info" whatever that might be.
>>>  ?
>>> David Holmes
>>>   ?
>>> -----Original Message-----
>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Yechiel Feffer
>>> *Sent:* Sunday, 4 March 2012 7:56 PM
>>> *To:* 'Concurrency-interest at cs.oswego.edu'
>>> *Subject:* [concurrency-interest] regarding synchronized command and
>>> object'sfootprint
>>>
>>>   Hi
>>>
>>> I heard a claim that execution of synchronized(object)?  command
>>> ? (maybe contended synchronized) raises the?  object footprint in the java
>>> heap- a raise the (partially ?) remains even after all threads exited the
>>> synchronized block.
>>>
>>> Is it correct ? Is it related to barrier info java maintains ?
>>>
>>> ?
>>>
>>> Regards
>>>
>>> Yechiel
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>>  --
>> Viktor Klang
>>
>> Akka Tech Lead
>> Typesafe <http://www.typesafe.com/>? - The software stack for
>> applications that scale
>>
>> Twitter: @viktorklang
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120304/41578fca/attachment.html>

From alexlamsl at gmail.com  Sun Mar  4 15:58:31 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Sun, 4 Mar 2012 20:58:31 +0000
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <4F53C8A9.7060702@cs.oswego.edu>
References: <4F20A70A.7030204@cs.oswego.edu> <4F53932F.7070004@cs.oswego.edu>
	<CAGpACNvKBB95LVkLr+jXh2vMFrp3pDeNq9sPJh8n2HiqimDc0A@mail.gmail.com>
	<4F53C8A9.7060702@cs.oswego.edu>
Message-ID: <CAGpACNsEhphGme0g2bKz==qBVgQh6O9DvJsVC+80_VuSBNLWPA@mail.gmail.com>

On Sun, Mar 4, 2012 at 7:55 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 03/04/12 14:50, Alex Lam S.L. wrote:
>>>
>>> (No, we can't support the nicer-to-use "int" without hurting other
>>> FJ usages.)
>>
>>
>> Would you mind elaborate more on this? Is this something to do with
>> the 2-byte overhead being too significantly when the number of tasks
>> are large?
>
>
>
> No -- we have some spare bits in a control word that we must keep
> atomically updated anyway. So we can let users use 16 of them for tags.
> But we can't magically add any more bits and still maintain atomic
> updates that are both cheap and coordinated with state changes.

I see - thanks for the explanation.

As for the API - in OutputStream for example, they provide the write()
method using "int", but document them such that only the lowest byte
is used:

http://docs.oracle.com/javase/7/docs/api/java/io/OutputStream.html#write(int)

Would that be a good pattern to adopt here, for convenience?


Alex.


>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From davidcholmes at aapt.net.au  Sun Mar  4 16:17:58 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 5 Mar 2012 07:17:58 +1000
Subject: [concurrency-interest] regarding synchronized command and
	object'sfootprint
In-Reply-To: <F5A3854EDBB4D440A098DEF1EFFC922B1F312C9D0B@IE2RD2XVS661.red002.local>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHJJDAA.davidcholmes@aapt.net.au>

In a well designed server it should be significantly less. The number of
active monitors depends on the interval between safepoints as well. In
theory you only need as many inflated monitors as there are threads in the
system, but because reclamation is not instantaneous ( a good thing to avoid
ping-pong effects) this number will be larger. There are a number of tuning
knobs for this code, and two different strategies for reclamation. You can
use -XX:+TraceMonitorInflation to see when inflation/deflation occurs. See
synchronizer.cpp for all the gory details.

David
  -----Original Message-----
  From: Yechiel Feffer [mailto:yechielf at gigaspaces.com]
  Sent: Sunday, 4 March 2012 9:24 PM
  To: dholmes at ieee.org; Concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] regarding synchronized command and
object'sfootprint


  So its not so bad- meaning the number of such monitors in the system =
maximum number of concurrent synchronized objects  since the jvm started.
This cannot be more than several thousands  even for a very busy server.



  From: David Holmes [mailto:davidcholmes at aapt.net.au]
  Sent: ??? ? 04 ??? 2012 12:50
  To: Yechiel Feffer; Concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] regarding synchronized command and
object'sfootprint



  That is correct in Hotspot. If contention occurs a heavyweight monitor has
to be created (this is called monitor inflation). There is a freelist of
inflated monitors - if the list is non-empty then one of those monitors is
used, else a new one is created.  If the monitor becomes idle it can be
returned to the freelist during a safepoint. The monitors once created are
never destroyed.



  It has nothing to do with "barrier info" whatever that might be.



  David Holmes



  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Yechiel
Feffer
  Sent: Sunday, 4 March 2012 7:56 PM
  To: 'Concurrency-interest at cs.oswego.edu'
  Subject: [concurrency-interest] regarding synchronized command and
object'sfootprint

    Hi

    I heard a claim that execution of synchronized(object)  command  (maybe
contended synchronized) raises the  object footprint in the java heap- a
raise the (partially ?) remains even after all threads exited the
synchronized block.

    Is it correct ? Is it related to barrier info java maintains ?



    Regards

    Yechiel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120305/db5f73a3/attachment-0001.html>

From david.dice at gmail.com  Sun Mar  4 16:38:01 2012
From: david.dice at gmail.com (David Dice)
Date: Sun, 4 Mar 2012 16:38:01 -0500
Subject: [concurrency-interest] regarding synchronized command and
	object'sfootprint
Message-ID: <CANbRUciyEFdRsWbApbcK5uv3psua0FGH806dUS1yu3Q8b42RVw@mail.gmail.com>

Message: 5
> Date: Mon, 5 Mar 2012 07:17:58 +1000
> From: "David Holmes" <davidcholmes at aapt.net.au>
> To: "Yechiel Feffer" <yechielf at gigaspaces.com>,
>        <Concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] regarding synchronized command and
>        object'sfootprint
> Message-ID: <NFBBKALFDCPFIDBNKAPCGEHJJDAA.davidcholmes at aapt.net.au>
> Content-Type: text/plain; charset="windows-1255"
>
> In a well designed server it should be significantly less. The number of
> active monitors depends on the interval between safepoints as well. In
> theory you only need as many inflated monitors as there are threads in the
> system, but because reclamation is not instantaneous ( a good thing to
> avoid
> ping-pong effects) this number will be larger. There are a number of tuning
> knobs for this code, and two different strategies for reclamation. You can
> use -XX:+TraceMonitorInflation to see when inflation/deflation occurs. See
> synchronizer.cpp for all the gory details.
>
>
Following up, the objectmonitor structure is a normal C++ object and
doesn't reside in the normal heap.    The lifecycle is such that we inflate
on initial contention (and a few other conditions) and, if the object is
uncontended and there are no wait()ers, we deflate at the next
stop-the-world safepoint, at which point the objectmonitor is eligible for
re-use.   Currently, the memory underlying objectmonitor instances is
type-stable.    To avoid contention on the main objectmonitor free list we
also have bounded per-thread free lists.   That can increase the # of
objectmonitors in circulation at any given time, but it's a good trade-off.
  In recent JVMs there's also a back-pressure facility where we can induce
a GC as the number of extant monitors passes a configurable bound.

Regards
Dave

p.s., other JVMs deflate on-the-fly instead of at the next safepoint.
On-the-fly is arguably better, but hotspot has a good deal of day-one code
that assumes the object:monitor relationship is stable except over
safepoints.   Even with on-the-fly deflation, we'd still need to reduce
contention on the central free lists with per-thread free lists.   And with
on-the-fly we have to worry about the case where monitors migrate between
threads, which we found was quite common.    In that case we need to take
actions to trim the free lists.   Still, on-the-fly remains better because
it decouples the synchronization subsystem from the safepointing mechanism.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120304/2313b308/attachment.html>

From dl at cs.oswego.edu  Sun Mar  4 17:35:52 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 04 Mar 2012 17:35:52 -0500
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <CAGpACNsEhphGme0g2bKz==qBVgQh6O9DvJsVC+80_VuSBNLWPA@mail.gmail.com>
References: <4F20A70A.7030204@cs.oswego.edu>
	<4F53932F.7070004@cs.oswego.edu>	<CAGpACNvKBB95LVkLr+jXh2vMFrp3pDeNq9sPJh8n2HiqimDc0A@mail.gmail.com>	<4F53C8A9.7060702@cs.oswego.edu>
	<CAGpACNsEhphGme0g2bKz==qBVgQh6O9DvJsVC+80_VuSBNLWPA@mail.gmail.com>
Message-ID: <4F53EE48.8000401@cs.oswego.edu>


> The methods are a little annoying to use though because there are no "short"
> literals, so you have to say for example: if
> (task.compareAndSetForkJoinTaskTag((short)0, (short)1)) ... to conditionally
> add a simple mark. ...

> As for the API - in OutputStream for example, they provide the write() method
> using "int", but document them such that only the lowest byte is used:
>

In the bondage-and-discipline spirit of most of j.u.c :-),
I'd rather force the users of this API to explicitly acknowledge that
they are using a narrower-than-usual type. This seems OK mainly because
these tags are uninterpreted, and are made available so that thin layers
on top of FJT can be more time/space efficient than they could otherwise
be in the vastly most common case where there is only a small range of
possible tags. For example, those that internally translate enums that
are known to have only a few values:

enum Relationship { LIKES, FRIENDS, INVITES }
abstract class SocialNetworkGraphTask extends ForkJoinTask<Void> {
   void visit(Relationship r) {
      short tag = (short) (1 << r.ordinal());
      ...
      if (compareAndSetForkJoinTaskTag(..., tag))
          process();
   }
   ...
}

More compelling use cases include various forms of completion-task
designs. I'll post more about these hopefully soon.

On the other hand, there is no reason to make common sensible
CAS-based idioms on tags any more tedious than necessary. So it seems
reasonable to at least provide a more streamlined way to add a value:
   short getAndAddToForkJoinTaskTag(short delta)


-Doug



From alexlamsl at gmail.com  Sun Mar  4 18:13:20 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Sun, 4 Mar 2012 23:13:20 +0000
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <4F53EE48.8000401@cs.oswego.edu>
References: <4F20A70A.7030204@cs.oswego.edu> <4F53932F.7070004@cs.oswego.edu>
	<CAGpACNvKBB95LVkLr+jXh2vMFrp3pDeNq9sPJh8n2HiqimDc0A@mail.gmail.com>
	<4F53C8A9.7060702@cs.oswego.edu>
	<CAGpACNsEhphGme0g2bKz==qBVgQh6O9DvJsVC+80_VuSBNLWPA@mail.gmail.com>
	<4F53EE48.8000401@cs.oswego.edu>
Message-ID: <CAGpACNtsNiF8FmivJiNoJ8T4=8LKW_sgpLXv53T20AZ_zPDNWg@mail.gmail.com>

On Sun, Mar 4, 2012 at 10:35 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> The methods are a little annoying to use though because there are no
>> "short"
>> literals, so you have to say for example: if
>> (task.compareAndSetForkJoinTaskTag((short)0, (short)1)) ... to
>> conditionally
>> add a simple mark. ...
>
>
>> As for the API - in OutputStream for example, they provide the write()
>> method
>> using "int", but document them such that only the lowest byte is used:
>>
>
> In the bondage-and-discipline spirit of most of j.u.c :-),
> I'd rather force the users of this API to explicitly acknowledge that
> they are using a narrower-than-usual type. This seems OK mainly because
> these tags are uninterpreted, and are made available so that thin layers
> on top of FJT can be more time/space efficient than they could otherwise
> be in the vastly most common case where there is only a small range of
> possible tags. For example, those that internally translate enums that
> are known to have only a few values:
>
> enum Relationship { LIKES, FRIENDS, INVITES }
> abstract class SocialNetworkGraphTask extends ForkJoinTask<Void> {
> ?void visit(Relationship r) {
> ? ? short tag = (short) (1 << r.ordinal());
> ? ? ...
> ? ? if (compareAndSetForkJoinTaskTag(..., tag))
> ? ? ? ? process();
> ?}
> ?...
> }
>
> More compelling use cases include various forms of completion-task
> designs. I'll post more about these hopefully soon.

Looking forward to seeing them.

I have a computation-intensive app dealing with 2D geometry which I
have been evaluating various design for ForkJoin on JavaSE 7, so would
welcome any new ideas.

(Slightly OT - one of these days I will try to fix those rounding bugs
in java.awt.geom.Area for good!)


> On the other hand, there is no reason to make common sensible
> CAS-based idioms on tags any more tedious than necessary. So it seems
> reasonable to at least provide a more streamlined way to add a value:
> ?short getAndAddToForkJoinTaskTag(short delta)
>

About those really long method names - is the original reason for
"mark" still applies to "tag"? Can we just come up with an obscure
term instead of "tag" so we can do away with the "ForkJoinTask" bit?

Doesn't hurt my fingers that much since I use an IDE, but my eyes...!


Alex.


From alexlamsl at gmail.com  Sun Mar  4 18:30:50 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Sun, 4 Mar 2012 23:30:50 +0000
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <4F53EE48.8000401@cs.oswego.edu>
References: <4F20A70A.7030204@cs.oswego.edu> <4F53932F.7070004@cs.oswego.edu>
	<CAGpACNvKBB95LVkLr+jXh2vMFrp3pDeNq9sPJh8n2HiqimDc0A@mail.gmail.com>
	<4F53C8A9.7060702@cs.oswego.edu>
	<CAGpACNsEhphGme0g2bKz==qBVgQh6O9DvJsVC+80_VuSBNLWPA@mail.gmail.com>
	<4F53EE48.8000401@cs.oswego.edu>
Message-ID: <CAGpACNtnqm6YC_G9ZuBfUghkbbr4fiQxpTxM1_3rYc_m8a=JJw@mail.gmail.com>

On Sun, Mar 4, 2012 at 10:35 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> 3. One small API addition: Explicit support for task marking.
> It was cruel to tell people that they could use FJ for things
> like graph traversal but not have a simple way to mark tasks
> so they won't be revisited while processing a graph (among a few
> other common use cases). Because they weren't supported initially,
> marking methods need crummy names that won't conflict with
> existing usages: markForkJoinTask and isMarkedForkJoinTask.

Actually, now that I read this again - by conflicts do you mean
inconvenience when modifying existing user code, or do you mean
existing applications will break?

Because for the latter case, I would expect the new methods will
simply be overridden by user code, hence should not cause any
breakage.

And if it is the former case - given that they are updating the source
code already, it doesn't seem like too much of an inconvenience.

The worst case I can think of is if one is using a 3rd party library
which is now overriding the new ForkJoinTask methods...


Alex.

From dl at cs.oswego.edu  Sun Mar  4 18:41:36 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 04 Mar 2012 18:41:36 -0500
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <CAGpACNtnqm6YC_G9ZuBfUghkbbr4fiQxpTxM1_3rYc_m8a=JJw@mail.gmail.com>
References: <4F20A70A.7030204@cs.oswego.edu> <4F53932F.7070004@cs.oswego.edu>
	<CAGpACNvKBB95LVkLr+jXh2vMFrp3pDeNq9sPJh8n2HiqimDc0A@mail.gmail.com>
	<4F53C8A9.7060702@cs.oswego.edu>
	<CAGpACNsEhphGme0g2bKz==qBVgQh6O9DvJsVC+80_VuSBNLWPA@mail.gmail.com>
	<4F53EE48.8000401@cs.oswego.edu>
	<CAGpACNtnqm6YC_G9ZuBfUghkbbr4fiQxpTxM1_3rYc_m8a=JJw@mail.gmail.com>
Message-ID: <4F53FDB0.3090109@cs.oswego.edu>

On 03/04/12 18:30, Alex Lam S.L. wrote:
> On Sun, Mar 4, 2012 at 10:35 PM, Doug Lea<dl at cs.oswego.edu>  wrote:
>> 3. One small API addition: Explicit support for task marking.
>> It was cruel to tell people that they could use FJ for things
>> like graph traversal but not have a simple way to mark tasks
>> so they won't be revisited while processing a graph (among a few
>> other common use cases). Because they weren't supported initially,
>> marking methods need crummy names that won't conflict with
>> existing usages: markForkJoinTask and isMarkedForkJoinTask.
>
> Actually, now that I read this again - by conflicts do you mean
> inconvenience when modifying existing user code, or do you mean
> existing applications will break?

The convention we operate under is that if you add new methods
to existing subclassable library classes, then you have to pick
names that no existing subclass could conceivably have used.
Even still, it amounts to a bet that no programmer on the planet
has such poor taste to pick such a crummy name.
I think that bet is safe with compareAndSetForkJoinTaskTag
etc.

Upcoming extension methods in JDK8 will help with more cases along
these lines, but even if they existed, we'd probably do it this
way in this case.

-Doug

From wolfgang.baltes at laposte.net  Sun Mar  4 23:26:44 2012
From: wolfgang.baltes at laposte.net (Wolfgang Baltes)
Date: Sun, 04 Mar 2012 20:26:44 -0800
Subject: [concurrency-interest] ForkJoinTask enhancement suggestion
Message-ID: <4F544084.5020308@laposte.net>

Hi,

Looking at the ForkJoin framework, I miss the ability to create a new 
'done' task with a given value. This is useful if - at a certain point 
of recursion - the result value of a task to be created is already 
known, but one wants to create the task anyway so that it integrates 
more easily with the existing algorithm, or to hand over the value to 
another process wrapped in the task.

I know one can already do this using the complete(v) method of 
ForkJoinTask, but this method requires a CAS instruction to change the 
tasks state, which I would like to avoid by setting the state/status 
before construction finishes. My proposal is to consider adding a new 
constructor ForkJoinTask(V) that a) sets the value, and b) sets the 
state to NORMAL. Although it may be consistent to also add a constructor 
to create an "exception task", I do not think this is necessary because 
of less frequent use, and hence less overall performance impact.

Noticing that tagging will also be available in a forthcoming version of 
the framework, it may make sense - for better performance and 
consistency - to also add the constructor ForkJoinTask(V, short) and the 
method complete(V, short), which allow setting both the value and the 
tag simultaneously.

Regards,
Wolfgang.

From dl at cs.oswego.edu  Mon Mar  5 06:39:13 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 05 Mar 2012 06:39:13 -0500
Subject: [concurrency-interest] ForkJoinTask enhancement suggestion
In-Reply-To: <4F544084.5020308@laposte.net>
References: <4F544084.5020308@laposte.net>
Message-ID: <4F54A5E1.6060808@cs.oswego.edu>

On 03/04/12 23:26, Wolfgang Baltes wrote:
>  My
> proposal is to consider adding a new constructor ForkJoinTask(V) that a) sets
> the value, and b) sets the state to NORMAL. Although it may be consistent to
> also add a constructor to create an "exception task", I do not think this is
> necessary because of less frequent use, and hence less overall performance impact.
>
> Noticing that tagging will also be available in a forthcoming version of the
> framework, it may make sense - for better performance and consistency - to also
> add the constructor ForkJoinTask(V, short) and the method complete(V, short),
> which allow setting both the value and the tag simultaneously.
>

Stay tuned. One of the reasons for adding integrated tagging support
is to allow introduction of one or more new flavors of
ForkJoinTask (besides RecursiveAction and RecursiveTask)
that trigger completion actions when some number (including zero)
of subtasks complete. So you should be able to efficiently get these
kinds of effects using these classes without us having to expose
more representation-dependent methods/constructors.

These upcoming classes are part of the efforts I mentioned in posts
around 6 months ago to improve support for the many FJ usages
that have sprung up that fall outside of the initial target
uses.

-Doug



From wolfgang.baltes at laposte.net  Mon Mar  5 10:47:44 2012
From: wolfgang.baltes at laposte.net (Wolfgang Baltes)
Date: Mon, 05 Mar 2012 07:47:44 -0800
Subject: [concurrency-interest] ForkJoinTask enhancement suggestion
In-Reply-To: <4F54A5E1.6060808@cs.oswego.edu>
References: <4F544084.5020308@laposte.net> <4F54A5E1.6060808@cs.oswego.edu>
Message-ID: <4F54E020.3080904@laposte.net>

Thanks, Doug, for the quick reply. I look forward hearing more about 
this. In the meantime, I will look for the posts you refer to.

W.

On 2012-03-05 03:39, Doug Lea wrote:
> On 03/04/12 23:26, Wolfgang Baltes wrote:
>>  My
>> proposal is to consider adding a new constructor ForkJoinTask(V) that 
>> a) sets
>> the value, and b) sets the state to NORMAL. Although it may be 
>> consistent to
>> also add a constructor to create an "exception task", I do not think 
>> this is
>> necessary because of less frequent use, and hence less overall 
>> performance impact.
>>
>> Noticing that tagging will also be available in a forthcoming version 
>> of the
>> framework, it may make sense - for better performance and consistency 
>> - to also
>> add the constructor ForkJoinTask(V, short) and the method complete(V, 
>> short),
>> which allow setting both the value and the tag simultaneously.
>>
>
> Stay tuned. One of the reasons for adding integrated tagging support
> is to allow introduction of one or more new flavors of
> ForkJoinTask (besides RecursiveAction and RecursiveTask)
> that trigger completion actions when some number (including zero)
> of subtasks complete. So you should be able to efficiently get these
> kinds of effects using these classes without us having to expose
> more representation-dependent methods/constructors.
>
> These upcoming classes are part of the efforts I mentioned in posts
> around 6 months ago to improve support for the many FJ usages
> that have sprung up that fall outside of the initial target
> uses.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From radhakrishnan.mohan at gmail.com  Tue Mar  6 09:29:09 2012
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Tue, 6 Mar 2012 19:59:09 +0530
Subject: [concurrency-interest] ForkJoinTask enhancement suggestion
In-Reply-To: <4F54A5E1.6060808@cs.oswego.edu>
References: <4F544084.5020308@laposte.net>
	<4F54A5E1.6060808@cs.oswego.edu>
Message-ID: <CAOoXFP8m6FPcZ3Wve09+VyUD770RXBCRODULkQZq0bD+PTLsJA@mail.gmail.com>

Looking forward to some advanced examples.

Thanks,
Mohan

On Mon, Mar 5, 2012 at 5:09 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 03/04/12 23:26, Wolfgang Baltes wrote:
>
>>  My
>> proposal is to consider adding a new constructor ForkJoinTask(V) that a)
>> sets
>> the value, and b) sets the state to NORMAL. Although it may be consistent
>> to
>> also add a constructor to create an "exception task", I do not think this
>> is
>> necessary because of less frequent use, and hence less overall
>> performance impact.
>>
>> Noticing that tagging will also be available in a forthcoming version of
>> the
>> framework, it may make sense - for better performance and consistency -
>> to also
>> add the constructor ForkJoinTask(V, short) and the method complete(V,
>> short),
>> which allow setting both the value and the tag simultaneously.
>>
>>
> Stay tuned. One of the reasons for adding integrated tagging support
> is to allow introduction of one or more new flavors of
> ForkJoinTask (besides RecursiveAction and RecursiveTask)
> that trigger completion actions when some number (including zero)
> of subtasks complete. So you should be able to efficiently get these
> kinds of effects using these classes without us having to expose
> more representation-dependent methods/constructors.
>
> These upcoming classes are part of the efforts I mentioned in posts
> around 6 months ago to improve support for the many FJ usages
> that have sprung up that fall outside of the initial target
> uses.
>
> -Doug
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120306/a512c88c/attachment.html>

From sebastien.bocq at gmail.com  Wed Mar  7 09:42:49 2012
From: sebastien.bocq at gmail.com (=?ISO-8859-1?Q?S=E9bastien_Bocq?=)
Date: Wed, 7 Mar 2012 15:42:49 +0100
Subject: [concurrency-interest] How to set the thread group of the
	ForkJoinPool?
Message-ID: <CAFJkGuEzwQjvSPCFquWd05zgOE82JyHneGp3RSkPeeSke82kAg@mail.gmail.com>

Hi,

I looked up through the sources of the jsr166y and I can't find a way
to do this. In case it is not, will it be possible in the future?

Thanks,
Sebastien

From peter.firmstone at zeus.net.au  Wed Mar  7 12:48:05 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Thu, 08 Mar 2012 03:48:05 +1000
Subject: [concurrency-interest] How to set the thread group of the
	ForkJoinPool?
In-Reply-To: <mailman.1.1331139600.28012.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1331139600.28012.concurrency-interest@cs.oswego.edu>
Message-ID: <1331142484.26225.3.camel@bluto>

Implement ForkJoinPool.ForkJoinWorkerThreadFactory and extend
ForkJoinWorkerThread

Cheers,

Peter.

On Thu, 2012-03-08 at 03:00, concurrency-interest-request at cs.oswego.edu
wrote:
> Send Concurrency-interest mailing list submissions to
> 	concurrency-interest at cs.oswego.edu
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
> 	concurrency-interest-request at cs.oswego.edu
> 
> You can reach the person managing the list at
> 	concurrency-interest-owner at cs.oswego.edu
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
> 
> 
> Today's Topics:
> 
>    1. How to set the thread group of the	ForkJoinPool? (S?bastien Bocq)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Wed, 7 Mar 2012 15:42:49 +0100
> From: S?bastien Bocq <sebastien.bocq at gmail.com>
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: [concurrency-interest] How to set the thread group of the
> 	ForkJoinPool?
> Message-ID:
> 	<CAFJkGuEzwQjvSPCFquWd05zgOE82JyHneGp3RSkPeeSke82kAg at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> Hi,
> 
> I looked up through the sources of the jsr166y and I can't find a way
> to do this. In case it is not, will it be possible in the future?
> 
> Thanks,
> Sebastien
> 
> 
> ------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> End of Concurrency-interest Digest, Vol 86, Issue 7
> ***************************************************


From mthornton at optrak.com  Wed Mar  7 13:13:12 2012
From: mthornton at optrak.com (Mark Thornton)
Date: Wed, 07 Mar 2012 18:13:12 +0000
Subject: [concurrency-interest] How to set the thread group of the
	ForkJoinPool?
In-Reply-To: <1331142484.26225.3.camel@bluto>
References: <mailman.1.1331139600.28012.concurrency-interest@cs.oswego.edu>
	<1331142484.26225.3.camel@bluto>
Message-ID: <4F57A538.9080902@optrak.com>

On 07/03/12 17:48, Peter Firmstone wrote:
> Implement ForkJoinPool.ForkJoinWorkerThreadFactory and extend
> ForkJoinWorkerThread
>
>
Doesn't work because ForkJoinWorkerThread doesn't have a constructor 
which takes the ThreadGroup as a parameter.

Mark Thornton

From peter.firmstone at zeus.net.au  Wed Mar  7 13:20:40 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Thu, 08 Mar 2012 04:20:40 +1000
Subject: [concurrency-interest] How to set the thread group of
	the	ForkJoinPool?
In-Reply-To: <4F57A538.9080902@optrak.com>
References: <mailman.1.1331139600.28012.concurrency-interest@cs.oswego.edu>
	<1331142484.26225.3.camel@bluto>  <4F57A538.9080902@optrak.com>
Message-ID: <1331144439.26225.9.camel@bluto>

Hmm, you're right, I'm guessing you want to use the system thread group
or something similar?

Cheers,

Peter.



On Thu, 2012-03-08 at 04:13, Mark Thornton wrote:
> On 07/03/12 17:48, Peter Firmstone wrote:
> > Implement ForkJoinPool.ForkJoinWorkerThreadFactory and extend
> > ForkJoinWorkerThread
> >
> >
> Doesn't work because ForkJoinWorkerThread doesn't have a constructor 
> which takes the ThreadGroup as a parameter.
> 
> Mark Thornton


From sebastien.bocq at gmail.com  Wed Mar  7 14:28:45 2012
From: sebastien.bocq at gmail.com (=?ISO-8859-1?Q?S=E9bastien_Bocq?=)
Date: Wed, 7 Mar 2012 20:28:45 +0100
Subject: [concurrency-interest] How to set the thread group of the
	ForkJoinPool?
In-Reply-To: <1331144439.26225.9.camel@bluto>
References: <mailman.1.1331139600.28012.concurrency-interest@cs.oswego.edu>
	<1331142484.26225.3.camel@bluto> <4F57A538.9080902@optrak.com>
	<1331144439.26225.9.camel@bluto>
Message-ID: <CAFJkGuHQ2yzjf++Ki4xCNkKfwNU2M4w9oL_QhKaCvb=1PDt9QA@mail.gmail.com>

I use different pools and I need to identify from which pool threads
are coming from. For instance, I want to use my own naming scheme.
Right now, I use a standard ThreadPoolExecutor and pass my factory as
last parameter to its constructor:

ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long
keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue,
ThreadFactory threadFactory)

But I'd like to replace it with the ForkJoinPool.

Sebastien

Le 7 mars 2012 19:20, Peter Firmstone <peter.firmstone at zeus.net.au> a ?crit :
> Hmm, you're right, I'm guessing you want to use the system thread group
> or something similar?
>
> Cheers,
>
> Peter.
>
>
>
> On Thu, 2012-03-08 at 04:13, Mark Thornton wrote:
>> On 07/03/12 17:48, Peter Firmstone wrote:
>> > Implement ForkJoinPool.ForkJoinWorkerThreadFactory and extend
>> > ForkJoinWorkerThread
>> >
>> >
>> Doesn't work because ForkJoinWorkerThread doesn't have a constructor
>> which takes the ThreadGroup as a parameter.
>>
>> Mark Thornton
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From rco at quartetfs.com  Wed Mar  7 14:40:55 2012
From: rco at quartetfs.com (Romain Colle)
Date: Wed, 7 Mar 2012 14:40:55 -0500
Subject: [concurrency-interest] How to set the thread group of the
	ForkJoinPool?
In-Reply-To: <CAFJkGuHQ2yzjf++Ki4xCNkKfwNU2M4w9oL_QhKaCvb=1PDt9QA@mail.gmail.com>
References: <mailman.1.1331139600.28012.concurrency-interest@cs.oswego.edu>
	<1331142484.26225.3.camel@bluto> <4F57A538.9080902@optrak.com>
	<1331144439.26225.9.camel@bluto>
	<CAFJkGuHQ2yzjf++Ki4xCNkKfwNU2M4w9oL_QhKaCvb=1PDt9QA@mail.gmail.com>
Message-ID: <CAJp3eRCYhz+bEin4e9qjTuyDwLScS83-Or1tEPqhL9OutA+1fA@mail.gmail.com>

On Wed, Mar 7, 2012 at 2:28 PM, S?bastien Bocq <sebastien.bocq at gmail.com>wrote:

> I use different pools and I need to identify from which pool threads
> are coming from. For instance, I want to use my own naming scheme.
>

Hi Sebastien,

We are using a thread factory to do so, and extend ForkJoinWorkerThread.
We simply call "setName" when the thread starts to get the required naming.
See sample code below.

    protected class Worker extends ForkJoinWorkerThread {
        protected Worker(ForkJoinPool pool) {
            super(pool);
        }

        /**
         * Called when the thread is started
         */
        protected void onStart() {
            super.onStart();
            setName(poolName + "-" + poolIndex + "-worker-" +
workerCount.getAndIncrement());
        }
    }

-- 
Romain Colle
Senior R&D Engineer
QuartetFS
2 rue Jean Lantier, 75001 Paris, France
http://www.quartetfs.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120307/8a2b9aa6/attachment.html>

From sebastien.bocq at gmail.com  Wed Mar  7 14:46:58 2012
From: sebastien.bocq at gmail.com (=?ISO-8859-1?Q?S=E9bastien_Bocq?=)
Date: Wed, 7 Mar 2012 20:46:58 +0100
Subject: [concurrency-interest] How to set the thread group of the
	ForkJoinPool?
In-Reply-To: <CAJp3eRCYhz+bEin4e9qjTuyDwLScS83-Or1tEPqhL9OutA+1fA@mail.gmail.com>
References: <mailman.1.1331139600.28012.concurrency-interest@cs.oswego.edu>
	<1331142484.26225.3.camel@bluto> <4F57A538.9080902@optrak.com>
	<1331144439.26225.9.camel@bluto>
	<CAFJkGuHQ2yzjf++Ki4xCNkKfwNU2M4w9oL_QhKaCvb=1PDt9QA@mail.gmail.com>
	<CAJp3eRCYhz+bEin4e9qjTuyDwLScS83-Or1tEPqhL9OutA+1fA@mail.gmail.com>
Message-ID: <CAFJkGuGoFN5m8Z4H1KDgiZTq2NcWyoaEVtRgJ98KMHSi1Nwe4A@mail.gmail.com>

Le 7 mars 2012 20:40, Romain Colle <rco at quartetfs.com> a ?crit :
> On Wed, Mar 7, 2012 at 2:28 PM, S?bastien Bocq <sebastien.bocq at gmail.com>
> wrote:
>>
>> I use different pools and I need to identify from which pool threads
>> are coming from. For instance, I want to use my own naming scheme.
>
>
> Hi Sebastien,
>
> We are using a thread factory to do so, and extend ForkJoinWorkerThread.
> We simply call "setName" when the thread starts to get the required naming.
> See sample code below.
>
> ? ? protected class Worker extends ForkJoinWorkerThread {
> ? ? ? ? protected Worker(ForkJoinPool pool) {
> ? ? ? ? ? ? super(pool);
> ? ? ? ? }
>
> ? ? ? ? /**
> ? ? ? ? ?* Called when the thread is started
> ? ? ? ? ?*/
> ? ? ? ? protected void onStart() {
> ? ? ? ? ? ? super.onStart();
> ? ? ? ? ? ? setName(poolName + "-" + poolIndex + "-worker-" +
> workerCount.getAndIncrement());
> ? ? ? ? }
> ? ? }
>
> --
> Romain Colle
> Senior R&D Engineer
> QuartetFS
> 2 rue Jean Lantier, 75001 Paris, France
> http://www.quartetfs.com

Hi Romain,

Thanks. That is one thing but I need to set my own ThreadGroup as well
for other reasons.

S?bastien


From davidcholmes at aapt.net.au  Wed Mar  7 15:43:51 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 8 Mar 2012 06:43:51 +1000
Subject: [concurrency-interest] How to set the thread group of
	theForkJoinPool?
In-Reply-To: <CAFJkGuGoFN5m8Z4H1KDgiZTq2NcWyoaEVtRgJ98KMHSi1Nwe4A@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEIKJDAA.davidcholmes@aapt.net.au>

The only way I can see to set the ThreadGroup is for your ThreadFactory to
use its own thread, which itself is placed in the right group. That thread
then constructs the ForkJoinWorkerThreads and they inherit the ThreadGroup.

Arguably ForkJoinWorkerThread should expose a constructor that allows any
Thread parameters to be set (group, name, stack-size)

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> S?bastien Bocq
> Sent: Thursday, 8 March 2012 5:47 AM
> To: Romain Colle
> Cc: concurrency-interest at cs.oswego.edu; Peter Firmstone
> Subject: Re: [concurrency-interest] How to set the thread group of
> theForkJoinPool?
>
>
> Le 7 mars 2012 20:40, Romain Colle <rco at quartetfs.com> a ?crit :
> > On Wed, Mar 7, 2012 at 2:28 PM, S?bastien Bocq
> <sebastien.bocq at gmail.com>
> > wrote:
> >>
> >> I use different pools and I need to identify from which pool threads
> >> are coming from. For instance, I want to use my own naming scheme.
> >
> >
> > Hi Sebastien,
> >
> > We are using a thread factory to do so, and extend ForkJoinWorkerThread.
> > We simply call "setName" when the thread starts to get the
> required naming.
> > See sample code below.
> >
> > ? ? protected class Worker extends ForkJoinWorkerThread {
> > ? ? ? ? protected Worker(ForkJoinPool pool) {
> > ? ? ? ? ? ? super(pool);
> > ? ? ? ? }
> >
> > ? ? ? ? /**
> > ? ? ? ? ?* Called when the thread is started
> > ? ? ? ? ?*/
> > ? ? ? ? protected void onStart() {
> > ? ? ? ? ? ? super.onStart();
> > ? ? ? ? ? ? setName(poolName + "-" + poolIndex + "-worker-" +
> > workerCount.getAndIncrement());
> > ? ? ? ? }
> > ? ? }
> >
> > --
> > Romain Colle
> > Senior R&D Engineer
> > QuartetFS
> > 2 rue Jean Lantier, 75001 Paris, France
> > http://www.quartetfs.com
>
> Hi Romain,
>
> Thanks. That is one thing but I need to set my own ThreadGroup as well
> for other reasons.
>
> S?bastien
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



From sebastien.bocq at gmail.com  Wed Mar  7 15:51:40 2012
From: sebastien.bocq at gmail.com (=?ISO-8859-1?Q?S=E9bastien_Bocq?=)
Date: Wed, 7 Mar 2012 21:51:40 +0100
Subject: [concurrency-interest] How to set the thread group of
	theForkJoinPool?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEIKJDAA.davidcholmes@aapt.net.au>
References: <CAFJkGuGoFN5m8Z4H1KDgiZTq2NcWyoaEVtRgJ98KMHSi1Nwe4A@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOEIKJDAA.davidcholmes@aapt.net.au>
Message-ID: <CAFJkGuHgyaonTmfx7EyHertibtxn_RtsfFtKb+w4LSCezDaM7w@mail.gmail.com>

Thank you all for the suffestions, I have enough ideas to work out a
solution now.

Sebastien

Le 7 mars 2012 21:43, David Holmes <davidcholmes at aapt.net.au> a ?crit :
> The only way I can see to set the ThreadGroup is for your ThreadFactory to
> use its own thread, which itself is placed in the right group. That thread
> then constructs the ForkJoinWorkerThreads and they inherit the ThreadGroup.
>
> Arguably ForkJoinWorkerThread should expose a constructor that allows any
> Thread parameters to be set (group, name, stack-size)
>
> David
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> S?bastien Bocq
>> Sent: Thursday, 8 March 2012 5:47 AM
>> To: Romain Colle
>> Cc: concurrency-interest at cs.oswego.edu; Peter Firmstone
>> Subject: Re: [concurrency-interest] How to set the thread group of
>> theForkJoinPool?
>>
>>
>> Le 7 mars 2012 20:40, Romain Colle <rco at quartetfs.com> a ?crit :
>> > On Wed, Mar 7, 2012 at 2:28 PM, S?bastien Bocq
>> <sebastien.bocq at gmail.com>
>> > wrote:
>> >>
>> >> I use different pools and I need to identify from which pool threads
>> >> are coming from. For instance, I want to use my own naming scheme.
>> >
>> >
>> > Hi Sebastien,
>> >
>> > We are using a thread factory to do so, and extend ForkJoinWorkerThread.
>> > We simply call "setName" when the thread starts to get the
>> required naming.
>> > See sample code below.
>> >
>> > ? ? protected class Worker extends ForkJoinWorkerThread {
>> > ? ? ? ? protected Worker(ForkJoinPool pool) {
>> > ? ? ? ? ? ? super(pool);
>> > ? ? ? ? }
>> >
>> > ? ? ? ? /**
>> > ? ? ? ? ?* Called when the thread is started
>> > ? ? ? ? ?*/
>> > ? ? ? ? protected void onStart() {
>> > ? ? ? ? ? ? super.onStart();
>> > ? ? ? ? ? ? setName(poolName + "-" + poolIndex + "-worker-" +
>> > workerCount.getAndIncrement());
>> > ? ? ? ? }
>> > ? ? }
>> >
>> > --
>> > Romain Colle
>> > Senior R&D Engineer
>> > QuartetFS
>> > 2 rue Jean Lantier, 75001 Paris, France
>> > http://www.quartetfs.com
>>
>> Hi Romain,
>>
>> Thanks. That is one thing but I need to set my own ThreadGroup as well
>> for other reasons.
>>
>> S?bastien
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>



-- 
S?bastien


From djg at cs.washington.edu  Thu Mar  8 21:40:58 2012
From: djg at cs.washington.edu (Dan Grossman)
Date: Thu, 8 Mar 2012 18:40:58 -0800
Subject: [concurrency-interest] help show speed-up on a trivial but manual
	array-map operation?
Message-ID: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>

Hi all,

Short version:

Any help on why the attached program (also pasted at the bottom) runs
the sequential version faster than the parallel version?  Apologies if
I've missed something obvious.

Longer version:

As I've mentioned a couple times on this list, I have materials for
introducing parallelism and concurrency in undergraduate data
structures courses
(http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/).  The
parallel portion uses the ForkJoin Framework, including how to
manually implement maps and reduces over arrays.  (Why manually?  For
the same reason students in data structures courses often implement
hashtables and priority queues.)  Several schools are using the
materials and the most common question I get from instructors is "why
didn't this code run faster in parallel" since they understandably
like to show the "easy pay-off for an easy problem".  I've
remote-debugged a couple -- bad sequential cutoffs or wonky timing
code -- without bothering all of you, but this time I'm stuck.

The attached code (and pasted below for convenience) is a simple
vector addition.  Kim (cc'ed) is getting slower results than the naive
code on a 48-processor Linux box.  The attached results are from his
machine.  I see similar behavior on a 4-processor Linux box with
openJDK7.  We have more detailed machine specs or Java versions if
that would help, but I imagine either:

(a) We've done something silly with the code that we've both
overlooked (but varying the sequential cutoff or the array size
doesn't seem to be it)

or

(b) This is not a "winning" example for some reason and we should
tweak the problem.

Any thoughts?

Much thanks!

--Dan

=====
import java.util.concurrent.*;

public class VecAdd extends RecursiveAction {
       private static final long NPS = (1000L * 1000 * 1000);
       private static final int VECTOR_SIZE = 200000;
       private static final ForkJoinPool fjPool = new ForkJoinPool();
       private static int SEQUENTIAL_CUTOFF = 100;
       int lo;
       int hi;
       int[] res;
       int[] arr1;
       int[] arr2;

       public VecAdd(int l, int h, int[] r, int[] a1, int[] a2) {
               lo = l;
               hi = h;
               res = r;
               arr1 = a1;
               arr2 = a2;
       }

       protected void compute() {
               if ((hi - lo) < SEQUENTIAL_CUTOFF) {
                       for (int i = lo; i < hi; i++) {
                               res[i] = arr1[i] + arr2[i];
                       }
               } else {
                       int mid = (hi + lo) / 2;
                       VecAdd left = new VecAdd(lo, mid, res, arr1, arr2);
                       VecAdd right = new VecAdd(mid, hi, res, arr1, arr2);
                       left.fork();
                       right.compute();
                       left.join();
               }
       }

       public static int[] add(int[] arr1, int[] arr2) {
               assert (arr1.length == arr2.length);
               int[] ans = new int[arr1.length];
               fjPool.invoke(new VecAdd(0, arr1.length, ans, arr1, arr2));
               return ans;
       }

       public static int[] staticadd(int[] arr1, int[] arr2) {
               assert (arr1.length == arr2.length);
               int[] ans = new int[arr1.length];
               for (int i = 0; i < arr1.length; i++) {
                       ans[i] = arr1[i] + arr2[i];
               }
               return ans;
       }

       public static void main(String[] args) {
               int[] first = new int[VECTOR_SIZE];
               int[] second = new int[VECTOR_SIZE];
               for (int count = 0; count < VECTOR_SIZE; count++) {
                       first[count] = count;
                       second[count] = 2 * count;
               }
               for (int reps = 0; reps < 20; reps++) {
                       long last = System.nanoTime();
                       int[] answer = add(first, second);
                       double elapsed = (double) (System.nanoTime() -
last) / NPS;
                       System.out.println("Parallel add in time " + elapsed);
               }
               for (int reps = 0; reps < 20; reps++){
                       SEQUENTIAL_CUTOFF = 2 * SEQUENTIAL_CUTOFF;
                       long last = System.nanoTime();
                       int[] answer = add(first,second);
                       double elapsed = (double)(System.nanoTime() - last)/NPS;
                       System.out.println("Parallel add in time "+elapsed+
                                       " with cutoff "+SEQUENTIAL_CUTOFF);
               }
               System.out.println(fjPool);
               /*
                * for(int val:answer) { System.out.print(val+", "); }
                */
               for (int reps = 0; reps < 20; reps++) {
                       long last = System.nanoTime();
                       int[] answer = staticadd(first, second);
                       double elapsed = (double) (System.nanoTime() -
last) / NPS;
                       System.out.println("Sequential add in time " + elapsed);
               }
       }
}
-------------- next part --------------
A non-text attachment was scrubbed...
Name: VecAdd.java
Type: application/octet-stream
Size: 3531 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120308/312e10db/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: results
Type: application/octet-stream
Size: 2593 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120308/312e10db/attachment-0001.obj>

From davidcholmes at aapt.net.au  Thu Mar  8 21:49:17 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 9 Mar 2012 12:49:17 +1000
Subject: [concurrency-interest] help show speed-up on a trivial but
	manualarray-map operation?
In-Reply-To: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEJBJDAA.davidcholmes@aapt.net.au>

Dan,

Have you tried making the vector much much larger? On some problems you need
to switch to 64-bit to get data structures big enough to warrant parallel
evaluation.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dan
> Grossman
> Sent: Friday, 9 March 2012 12:41 PM
> To: concurrency-interest at cs.oswego.edu
> Cc: Kim Bruce
> Subject: [concurrency-interest] help show speed-up on a trivial but
> manualarray-map operation?
>
>
> Hi all,
>
> Short version:
>
> Any help on why the attached program (also pasted at the bottom) runs
> the sequential version faster than the parallel version?  Apologies if
> I've missed something obvious.
>
> Longer version:
>
> As I've mentioned a couple times on this list, I have materials for
> introducing parallelism and concurrency in undergraduate data
> structures courses
> (http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/).  The
> parallel portion uses the ForkJoin Framework, including how to
> manually implement maps and reduces over arrays.  (Why manually?  For
> the same reason students in data structures courses often implement
> hashtables and priority queues.)  Several schools are using the
> materials and the most common question I get from instructors is "why
> didn't this code run faster in parallel" since they understandably
> like to show the "easy pay-off for an easy problem".  I've
> remote-debugged a couple -- bad sequential cutoffs or wonky timing
> code -- without bothering all of you, but this time I'm stuck.
>
> The attached code (and pasted below for convenience) is a simple
> vector addition.  Kim (cc'ed) is getting slower results than the naive
> code on a 48-processor Linux box.  The attached results are from his
> machine.  I see similar behavior on a 4-processor Linux box with
> openJDK7.  We have more detailed machine specs or Java versions if
> that would help, but I imagine either:
>
> (a) We've done something silly with the code that we've both
> overlooked (but varying the sequential cutoff or the array size
> doesn't seem to be it)
>
> or
>
> (b) This is not a "winning" example for some reason and we should
> tweak the problem.
>
> Any thoughts?
>
> Much thanks!
>
> --Dan
>
> =====
> import java.util.concurrent.*;
>
> public class VecAdd extends RecursiveAction {
>        private static final long NPS = (1000L * 1000 * 1000);
>        private static final int VECTOR_SIZE = 200000;
>        private static final ForkJoinPool fjPool = new ForkJoinPool();
>        private static int SEQUENTIAL_CUTOFF = 100;
>        int lo;
>        int hi;
>        int[] res;
>        int[] arr1;
>        int[] arr2;
>
>        public VecAdd(int l, int h, int[] r, int[] a1, int[] a2) {
>                lo = l;
>                hi = h;
>                res = r;
>                arr1 = a1;
>                arr2 = a2;
>        }
>
>        protected void compute() {
>                if ((hi - lo) < SEQUENTIAL_CUTOFF) {
>                        for (int i = lo; i < hi; i++) {
>                                res[i] = arr1[i] + arr2[i];
>                        }
>                } else {
>                        int mid = (hi + lo) / 2;
>                        VecAdd left = new VecAdd(lo, mid, res, arr1, arr2);
>                        VecAdd right = new VecAdd(mid, hi, res,
> arr1, arr2);
>                        left.fork();
>                        right.compute();
>                        left.join();
>                }
>        }
>
>        public static int[] add(int[] arr1, int[] arr2) {
>                assert (arr1.length == arr2.length);
>                int[] ans = new int[arr1.length];
>                fjPool.invoke(new VecAdd(0, arr1.length, ans, arr1, arr2));
>                return ans;
>        }
>
>        public static int[] staticadd(int[] arr1, int[] arr2) {
>                assert (arr1.length == arr2.length);
>                int[] ans = new int[arr1.length];
>                for (int i = 0; i < arr1.length; i++) {
>                        ans[i] = arr1[i] + arr2[i];
>                }
>                return ans;
>        }
>
>        public static void main(String[] args) {
>                int[] first = new int[VECTOR_SIZE];
>                int[] second = new int[VECTOR_SIZE];
>                for (int count = 0; count < VECTOR_SIZE; count++) {
>                        first[count] = count;
>                        second[count] = 2 * count;
>                }
>                for (int reps = 0; reps < 20; reps++) {
>                        long last = System.nanoTime();
>                        int[] answer = add(first, second);
>                        double elapsed = (double) (System.nanoTime() -
> last) / NPS;
>                        System.out.println("Parallel add in time "
> + elapsed);
>                }
>                for (int reps = 0; reps < 20; reps++){
>                        SEQUENTIAL_CUTOFF = 2 * SEQUENTIAL_CUTOFF;
>                        long last = System.nanoTime();
>                        int[] answer = add(first,second);
>                        double elapsed =
> (double)(System.nanoTime() - last)/NPS;
>                        System.out.println("Parallel add in time "+elapsed+
>                                        " with cutoff "+SEQUENTIAL_CUTOFF);
>                }
>                System.out.println(fjPool);
>                /*
>                 * for(int val:answer) { System.out.print(val+", "); }
>                 */
>                for (int reps = 0; reps < 20; reps++) {
>                        long last = System.nanoTime();
>                        int[] answer = staticadd(first, second);
>                        double elapsed = (double) (System.nanoTime() -
> last) / NPS;
>                        System.out.println("Sequential add in time
> " + elapsed);
>                }
>        }
> }
>


From djg at cs.washington.edu  Thu Mar  8 21:56:45 2012
From: djg at cs.washington.edu (Dan Grossman)
Date: Thu, 8 Mar 2012 18:56:45 -0800
Subject: [concurrency-interest] help show speed-up on a trivial but
 manualarray-map operation?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEJBJDAA.davidcholmes@aapt.net.au>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJBJDAA.davidcholmes@aapt.net.au>
Message-ID: <CADruQ+hr4mF+0YR0wNBydov8NRaQ5WewfgS05jmF9envQdfm9A@mail.gmail.com>

I tried 100x larger (20 million array elements) and saw similar
behavior.  At 1000x (200 million array elements), I got an
out-of-memory error with the default heap configuration.

--Dan

On Thu, Mar 8, 2012 at 6:49 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Dan,
>
> Have you tried making the vector much much larger? On some problems you need
> to switch to 64-bit to get data structures big enough to warrant parallel
> evaluation.
>
> David Holmes
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dan
>> Grossman
>> Sent: Friday, 9 March 2012 12:41 PM
>> To: concurrency-interest at cs.oswego.edu
>> Cc: Kim Bruce
>> Subject: [concurrency-interest] help show speed-up on a trivial but
>> manualarray-map operation?
>>
>>
>> Hi all,
>>
>> Short version:
>>
>> Any help on why the attached program (also pasted at the bottom) runs
>> the sequential version faster than the parallel version? ?Apologies if
>> I've missed something obvious.
>>
>> Longer version:
>>
>> As I've mentioned a couple times on this list, I have materials for
>> introducing parallelism and concurrency in undergraduate data
>> structures courses
>> (http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/). ?The
>> parallel portion uses the ForkJoin Framework, including how to
>> manually implement maps and reduces over arrays. ?(Why manually? ?For
>> the same reason students in data structures courses often implement
>> hashtables and priority queues.) ?Several schools are using the
>> materials and the most common question I get from instructors is "why
>> didn't this code run faster in parallel" since they understandably
>> like to show the "easy pay-off for an easy problem". ?I've
>> remote-debugged a couple -- bad sequential cutoffs or wonky timing
>> code -- without bothering all of you, but this time I'm stuck.
>>
>> The attached code (and pasted below for convenience) is a simple
>> vector addition. ?Kim (cc'ed) is getting slower results than the naive
>> code on a 48-processor Linux box. ?The attached results are from his
>> machine. ?I see similar behavior on a 4-processor Linux box with
>> openJDK7. ?We have more detailed machine specs or Java versions if
>> that would help, but I imagine either:
>>
>> (a) We've done something silly with the code that we've both
>> overlooked (but varying the sequential cutoff or the array size
>> doesn't seem to be it)
>>
>> or
>>
>> (b) This is not a "winning" example for some reason and we should
>> tweak the problem.
>>
>> Any thoughts?
>>
>> Much thanks!
>>
>> --Dan
>>
>> =====
>> import java.util.concurrent.*;
>>
>> public class VecAdd extends RecursiveAction {
>> ? ? ? ?private static final long NPS = (1000L * 1000 * 1000);
>> ? ? ? ?private static final int VECTOR_SIZE = 200000;
>> ? ? ? ?private static final ForkJoinPool fjPool = new ForkJoinPool();
>> ? ? ? ?private static int SEQUENTIAL_CUTOFF = 100;
>> ? ? ? ?int lo;
>> ? ? ? ?int hi;
>> ? ? ? ?int[] res;
>> ? ? ? ?int[] arr1;
>> ? ? ? ?int[] arr2;
>>
>> ? ? ? ?public VecAdd(int l, int h, int[] r, int[] a1, int[] a2) {
>> ? ? ? ? ? ? ? ?lo = l;
>> ? ? ? ? ? ? ? ?hi = h;
>> ? ? ? ? ? ? ? ?res = r;
>> ? ? ? ? ? ? ? ?arr1 = a1;
>> ? ? ? ? ? ? ? ?arr2 = a2;
>> ? ? ? ?}
>>
>> ? ? ? ?protected void compute() {
>> ? ? ? ? ? ? ? ?if ((hi - lo) < SEQUENTIAL_CUTOFF) {
>> ? ? ? ? ? ? ? ? ? ? ? ?for (int i = lo; i < hi; i++) {
>> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?res[i] = arr1[i] + arr2[i];
>> ? ? ? ? ? ? ? ? ? ? ? ?}
>> ? ? ? ? ? ? ? ?} else {
>> ? ? ? ? ? ? ? ? ? ? ? ?int mid = (hi + lo) / 2;
>> ? ? ? ? ? ? ? ? ? ? ? ?VecAdd left = new VecAdd(lo, mid, res, arr1, arr2);
>> ? ? ? ? ? ? ? ? ? ? ? ?VecAdd right = new VecAdd(mid, hi, res,
>> arr1, arr2);
>> ? ? ? ? ? ? ? ? ? ? ? ?left.fork();
>> ? ? ? ? ? ? ? ? ? ? ? ?right.compute();
>> ? ? ? ? ? ? ? ? ? ? ? ?left.join();
>> ? ? ? ? ? ? ? ?}
>> ? ? ? ?}
>>
>> ? ? ? ?public static int[] add(int[] arr1, int[] arr2) {
>> ? ? ? ? ? ? ? ?assert (arr1.length == arr2.length);
>> ? ? ? ? ? ? ? ?int[] ans = new int[arr1.length];
>> ? ? ? ? ? ? ? ?fjPool.invoke(new VecAdd(0, arr1.length, ans, arr1, arr2));
>> ? ? ? ? ? ? ? ?return ans;
>> ? ? ? ?}
>>
>> ? ? ? ?public static int[] staticadd(int[] arr1, int[] arr2) {
>> ? ? ? ? ? ? ? ?assert (arr1.length == arr2.length);
>> ? ? ? ? ? ? ? ?int[] ans = new int[arr1.length];
>> ? ? ? ? ? ? ? ?for (int i = 0; i < arr1.length; i++) {
>> ? ? ? ? ? ? ? ? ? ? ? ?ans[i] = arr1[i] + arr2[i];
>> ? ? ? ? ? ? ? ?}
>> ? ? ? ? ? ? ? ?return ans;
>> ? ? ? ?}
>>
>> ? ? ? ?public static void main(String[] args) {
>> ? ? ? ? ? ? ? ?int[] first = new int[VECTOR_SIZE];
>> ? ? ? ? ? ? ? ?int[] second = new int[VECTOR_SIZE];
>> ? ? ? ? ? ? ? ?for (int count = 0; count < VECTOR_SIZE; count++) {
>> ? ? ? ? ? ? ? ? ? ? ? ?first[count] = count;
>> ? ? ? ? ? ? ? ? ? ? ? ?second[count] = 2 * count;
>> ? ? ? ? ? ? ? ?}
>> ? ? ? ? ? ? ? ?for (int reps = 0; reps < 20; reps++) {
>> ? ? ? ? ? ? ? ? ? ? ? ?long last = System.nanoTime();
>> ? ? ? ? ? ? ? ? ? ? ? ?int[] answer = add(first, second);
>> ? ? ? ? ? ? ? ? ? ? ? ?double elapsed = (double) (System.nanoTime() -
>> last) / NPS;
>> ? ? ? ? ? ? ? ? ? ? ? ?System.out.println("Parallel add in time "
>> + elapsed);
>> ? ? ? ? ? ? ? ?}
>> ? ? ? ? ? ? ? ?for (int reps = 0; reps < 20; reps++){
>> ? ? ? ? ? ? ? ? ? ? ? ?SEQUENTIAL_CUTOFF = 2 * SEQUENTIAL_CUTOFF;
>> ? ? ? ? ? ? ? ? ? ? ? ?long last = System.nanoTime();
>> ? ? ? ? ? ? ? ? ? ? ? ?int[] answer = add(first,second);
>> ? ? ? ? ? ? ? ? ? ? ? ?double elapsed =
>> (double)(System.nanoTime() - last)/NPS;
>> ? ? ? ? ? ? ? ? ? ? ? ?System.out.println("Parallel add in time "+elapsed+
>> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?" with cutoff "+SEQUENTIAL_CUTOFF);
>> ? ? ? ? ? ? ? ?}
>> ? ? ? ? ? ? ? ?System.out.println(fjPool);
>> ? ? ? ? ? ? ? ?/*
>> ? ? ? ? ? ? ? ? * for(int val:answer) { System.out.print(val+", "); }
>> ? ? ? ? ? ? ? ? */
>> ? ? ? ? ? ? ? ?for (int reps = 0; reps < 20; reps++) {
>> ? ? ? ? ? ? ? ? ? ? ? ?long last = System.nanoTime();
>> ? ? ? ? ? ? ? ? ? ? ? ?int[] answer = staticadd(first, second);
>> ? ? ? ? ? ? ? ? ? ? ? ?double elapsed = (double) (System.nanoTime() -
>> last) / NPS;
>> ? ? ? ? ? ? ? ? ? ? ? ?System.out.println("Sequential add in time
>> " + elapsed);
>> ? ? ? ? ? ? ? ?}
>> ? ? ? ?}
>> }
>>
>


From jws at csse.unimelb.edu.au  Thu Mar  8 22:24:27 2012
From: jws at csse.unimelb.edu.au (Jeff Schultz)
Date: Fri, 9 Mar 2012 14:24:27 +1100
Subject: [concurrency-interest] help show speed-up on a trivial
	but	manual array-map operation?
In-Reply-To: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
Message-ID: <20120309032427.GA16976@mulga.csse.unimelb.edu.au>

On Thu, Mar 08, 2012 at 06:40:58PM -0800, Dan Grossman wrote:
> The attached code (and pasted below for convenience) is a simple
> vector addition.  Kim (cc'ed) is getting slower results than the naive
> code on a 48-processor Linux box.  The attached results are from his
> machine.  I see similar behavior on a 4-processor Linux box with
> openJDK7.  We have more detailed machine specs or Java versions if
> that would help, but I imagine either:

I'd have expected that one int vector add loop would pretty much
saturate memory on most current processors, so more than one CPU on
the same chip won't do much better than a single CPU.

The 48 CPU machine is presumably a four or more socket arrangement
with memory attached directly to each socket.  Unless the OS can do
something very clever about moving parts of first and second in main
between the different memories, it's likely that they're completely
allocated on one socket's memory.  Even ignoring any effects of the
generally slower inter-socket interconnect, this still leaves the
problem of no more memory bandwidth than a single socket.

To show speedup, you need an operation that costs a lot more cycles
than integer add, while being cache-friendly.  (Naively) searching for
a pattern in an array might work.  Fill the input with mostly 1s and
the occasional 2 and look for patterns of N 1s followed by a 2.  As a
pedagogical bonus, you can show the effect of memory bandwidth
limitations by changing N.  Larger N means more reuse of each cache
line.


    Jeff Schultz

From ron.pressler at gmail.com  Fri Mar  9 07:09:22 2012
From: ron.pressler at gmail.com (Ron Pressler)
Date: Fri, 9 Mar 2012 14:09:22 +0200
Subject: [concurrency-interest] David Ungar talk: Everything You Know about
	Parallel Programming Is Wrong!
Message-ID: <CABg6-qh68Vot0kNqfmFTTz5UNLZxbqAcf-Z65KTA5jb_5StV6Q@mail.gmail.com>

Hi.
Just wanted to let you know about this video of a talk by David Ungar from
IBM research:
https://cmusv.adobeconnect.com/_a829716469/p1vdztyrp8e/?launcher=false&fcsContent=true&pbMode=normal

He basically discusses a new concurrent programming style for many-core
systems that completely does away with concurrency primitives, including
CAS. The idea is to do everything you can to mitigate the chance of a race
condition, and then, if a race happens, try to fix it later and/or live
with some wrong results.
What I found particularly interesting is the "probabilistic linked-list" he
discusses towards the end, where he shows a simple data structure that
might miss a few inserts, but on the whole has many more "successes per
millisecond" (a metric he thinks is valuable) than CAS-ing. He doesn't
mention memory fences at all, though, which I found a little strange.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120309/9531b4f1/attachment.html>

From dl at cs.oswego.edu  Fri Mar  9 08:11:02 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 09 Mar 2012 08:11:02 -0500
Subject: [concurrency-interest] help show speed-up on a trivial but
 manual array-map operation?
In-Reply-To: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
Message-ID: <4F5A0166.9080400@cs.oswego.edu>

On 03/08/12 21:40, Dan Grossman wrote:
> Hi all,
>
> Short version:
>
> Any help on why the attached program (also pasted at the bottom) runs
> the sequential version faster than the parallel version?  Apologies if
> I've missed something obvious.
>

Nothing that I'd call "obvious" but...

1. Microbenchmarking artifacts: The results of the
computations are never used so JVMs can kill some of the
code. The attached edited version includes a "checkSum" method
that combats this.

2. The 200K array size is relatively small compared to timing
precision so shows a big variance even on seq runs. Try 1 million.

3. The sequential loop form of adding elements along
constant-stride locations is heavily optimized on modern hardware.
Try different functions besides adding that disrupt some of the
point-wise hardware optimization. The attached version
includes a few choices. Even with above 2 changes, the
speedups for plain add are small, but those for sum
of squares or magnitudes are substantial. Having students play
around with  different choices might be instructive.

-Doug




-------------- next part --------------
A non-text attachment was scrubbed...
Name: VecAdd.java
Type: text/x-java
Size: 3949 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120309/f8e8298b/attachment.bin>

From dl at cs.oswego.edu  Fri Mar  9 08:20:22 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 09 Mar 2012 08:20:22 -0500
Subject: [concurrency-interest] help show speed-up on a trivial but
 manual array-map operation?
In-Reply-To: <4F5A0166.9080400@cs.oswego.edu>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
	<4F5A0166.9080400@cs.oswego.edu>
Message-ID: <4F5A0396.6070601@cs.oswego.edu>

On 03/09/12 08:11, Doug Lea wrote:

> 1. Microbenchmarking artifacts: The results of the
> computations are never used so JVMs can kill some of the
> code. The attached edited version includes a "checkSum" method
> that combats this.

Plus, as I should have checked before, the use of a linear
stream of initial values is also subject to microbenchmarking
artifacts. Filling with random values instead combats this.

The main moral is that reliably measuring anything on modern
systems is harder than you'd think it should be.

-Doug
-------------- next part --------------
A non-text attachment was scrubbed...
Name: VecAdd.java
Type: text/x-java
Size: 4059 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120309/5fd7960c/attachment.bin>

From mwh at cs.umd.edu  Fri Mar  9 08:24:50 2012
From: mwh at cs.umd.edu (Michael Hicks)
Date: Fri, 9 Mar 2012 08:24:50 -0500
Subject: [concurrency-interest] help show speed-up on a trivial but
	manual array-map operation?
In-Reply-To: <4F5A0396.6070601@cs.oswego.edu>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
	<4F5A0166.9080400@cs.oswego.edu> <4F5A0396.6070601@cs.oswego.edu>
Message-ID: <94CF0CEE-625D-46B6-8773-D0ABE05A3C2A@cs.umd.edu>

Mitkowicz et al also present a very interesting cautionary tale in this regard:

http://www.multicoreinfo.com/research/papers/2009/asplos09-producing-data.pdf

>From the paper: changing the order in which you link your .o files can change performance by up to 10%  Given that many compiler optimizations make "improvements" in the range of 5% makes this result a bit disturbing.

-Mike

On Mar 9, 2012, at 8:20 AM, Doug Lea wrote:

> On 03/09/12 08:11, Doug Lea wrote:
> 
>> 1. Microbenchmarking artifacts: The results of the
>> computations are never used so JVMs can kill some of the
>> code. The attached edited version includes a "checkSum" method
>> that combats this.
> 
> Plus, as I should have checked before, the use of a linear
> stream of initial values is also subject to microbenchmarking
> artifacts. Filling with random values instead combats this.
> 
> The main moral is that reliably measuring anything on modern
> systems is harder than you'd think it should be.
> 
> -Doug
> <VecAdd.java>_______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From djg at cs.washington.edu  Fri Mar  9 21:40:14 2012
From: djg at cs.washington.edu (Dan Grossman)
Date: Fri, 9 Mar 2012 18:40:14 -0800
Subject: [concurrency-interest] help show speed-up on a trivial but
 manual array-map operation?
In-Reply-To: <88AE6996-195A-483D-9E7A-3CB2971A0968@cs.pomona.edu>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
	<4F5A0166.9080400@cs.oswego.edu> <4F5A0396.6070601@cs.oswego.edu>
	<88AE6996-195A-483D-9E7A-3CB2971A0968@cs.pomona.edu>
Message-ID: <CADruQ+jemh19Mn-jAriV_8Y4q2OWvMyD_w6QWoJVs0CJmfOs2w@mail.gmail.com>

Let me echo Kim's word of thanks.

When I have time in a couple weeks, I hope to extend my materials to
include some of the excellent points and example made here: gotchas
with respect to timing, the need for large enough problem sizes, and
-- most important in my mind -- the need for beefy enough arithmetic
operations to overcome memory-bandwidth issues.

As Kim points out, any simple-enough-for-undergraduate-lecture
examples are welcome.

--Dan

On Fri, Mar 9, 2012 at 5:47 PM, Kim Bruce <kim at cs.pomona.edu> wrote:
> Thanks to Doug and everyone else for their suggestions. ?Doug's changes seem to buy me about a 12 times speed-up with a 48 core machine. ?This will definitely be more impressive for my students than the slowdowns or marginal speed-ups we were getting before.
>
> I'd be thrilled to see a set of relatively simple (to explain) programs using ForkJoin that could be used to demonstrate significant speedups with many-core computers. ?I'm going to be asking students to do some timing in a lab in a couple of weeks and would like them to be able to do things that do show significant speedup.
>
> Also valuable would be pointers to sites that might have good parallel/concurrency class projects (especially week-long assignments) for students in a data structures course learning about how to use these constructs.
>
> Thanks again for everyone's help!
>
> Kim
>
>
>
> On Mar 9, 2012, at 5:20 AM, Doug Lea wrote:
>
>> On 03/09/12 08:11, Doug Lea wrote:
>>
>>> 1. Microbenchmarking artifacts: The results of the
>>> computations are never used so JVMs can kill some of the
>>> code. The attached edited version includes a "checkSum" method
>>> that combats this.
>>
>> Plus, as I should have checked before, the use of a linear
>> stream of initial values is also subject to microbenchmarking
>> artifacts. Filling with random values instead combats this.
>>
>> The main moral is that reliably measuring anything on modern
>> systems is harder than you'd think it should be.
>>
>> -Doug
>> <VecAdd.java>
>


From dl at cs.oswego.edu  Sat Mar 10 09:09:52 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 10 Mar 2012 09:09:52 -0500
Subject: [concurrency-interest] help show speed-up on a trivial but
 manual array-map operation?
In-Reply-To: <CADruQ+jemh19Mn-jAriV_8Y4q2OWvMyD_w6QWoJVs0CJmfOs2w@mail.gmail.com>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>	<4F5A0166.9080400@cs.oswego.edu>	<4F5A0396.6070601@cs.oswego.edu>	<88AE6996-195A-483D-9E7A-3CB2971A0968@cs.pomona.edu>
	<CADruQ+jemh19Mn-jAriV_8Y4q2OWvMyD_w6QWoJVs0CJmfOs2w@mail.gmail.com>
Message-ID: <4F5B60B0.1040507@cs.oswego.edu>

On 03/09/12 21:40, Dan Grossman wrote:

> When I have time in a couple weeks, I hope to extend my materials to
> include some of the excellent points and example made here: gotchas
> with respect to timing, the need for large enough problem sizes, and
> -- most important in my mind -- the need for beefy enough arithmetic
> operations to overcome memory-bandwidth issues.
>

Mostly as a digression: These issues come to the forefront
when developing upcoming parallel aggregate operations on
collections using opaque (to us) functions. As a minimal
goal, a parallel version of, say, c.map(f).reduce(r, b)) should
never be slower than sequential version; and of course should
nicely scale up from there under more favorable conditions.
This is hard to pull off, but I've been working on almost always
hitting this goal for at least the most commonly used underlying
data structures -- array-based (possibly as an enhanced version
of ArrayList) and hash-based (further improving the implementation
of ConcurrentHashMapV8). I'm doubtful that we can come very
close to meeting this for arbitrary data structures. And even
given cleverer FJ-based parallel algorithms for aggregate operations,
improvements are still modulo a long list of underlying issues:
initial thread and memory allocation, megamorphic virtual call
overhead, compilation plans (ensuring that JITs compile the code
that actually matters), method handle performance,
GC cardmark  contention, anti-locality consequences of boxing,
and so on. The gap between what you'd naively think ought to be faster
by exploiting parallelism and what we can actually do will surely
shrink over time, but it will take a lot of hard work on a lot of fronts.
In the mean time, when applying time-consuming operations on many
elements of arrays etc in parallel, people routinely get good
parallel performance. But winning in other cases remains an art form.

-Doug



From radhakrishnan.mohan at gmail.com  Tue Mar 13 01:11:45 2012
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Tue, 13 Mar 2012 10:41:45 +0530
Subject: [concurrency-interest] help show speed-up on a trivial but
 manual array-map operation?
In-Reply-To: <94CF0CEE-625D-46B6-8773-D0ABE05A3C2A@cs.umd.edu>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
	<4F5A0166.9080400@cs.oswego.edu> <4F5A0396.6070601@cs.oswego.edu>
	<94CF0CEE-625D-46B6-8773-D0ABE05A3C2A@cs.umd.edu>
Message-ID: <CAOoXFP-Ao9xSjqaSKw=WfYBC222UAdyPCmU4o_aC+ArsMh6baw@mail.gmail.com>

The 'References' section of this paper mentiones another paper "
http://sc2000.org/techpapr/papers/pap.pap256.pdf"
which describes a tool suite(http://icl.cs.utk.edu/projects/papi) to get
performance information from hardware counters.
Figure 3. in this paper shows a Java frontend to analyze cache misses.
Seems it could be useful for basic research.

Thanks,
Mohan

On Fri, Mar 9, 2012 at 6:54 PM, Michael Hicks <mwh at cs.umd.edu> wrote:

> Mitkowicz et al also present a very interesting cautionary tale in this
> regard:
>
>
> http://www.multicoreinfo.com/research/papers/2009/asplos09-producing-data.pdf
>
> From the paper: changing the order in which you link your .o files can
> change performance by up to 10%  Given that many compiler optimizations
> make "improvements" in the range of 5% makes this result a bit disturbing.
>
> -Mike
>
> On Mar 9, 2012, at 8:20 AM, Doug Lea wrote:
>
> > On 03/09/12 08:11, Doug Lea wrote:
> >
> >> 1. Microbenchmarking artifacts: The results of the
> >> computations are never used so JVMs can kill some of the
> >> code. The attached edited version includes a "checkSum" method
> >> that combats this.
> >
> > Plus, as I should have checked before, the use of a linear
> > stream of initial values is also subject to microbenchmarking
> > artifacts. Filling with random values instead combats this.
> >
> > The main moral is that reliably measuring anything on modern
> > systems is harder than you'd think it should be.
> >
> > -Doug
> > <VecAdd.java>_______________________________________________
>  > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120313/948573cf/attachment.html>

From viktor.klang at gmail.com  Mon Mar 19 12:52:29 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 19 Mar 2012 17:52:29 +0100
Subject: [concurrency-interest] Advice on reuse of ForkJoinTask instances
Message-ID: <CANPzfU-AxF0OxWahnGuBcfQKDCAjqZFh39QV7dxKkKm7+ySrhg@mail.gmail.com>

Hey gang,

We're using the latest incarnation of the ForkJoinPool in Akka,
and I was looking at reducing allocations in the hot path, and essentially
the only thing left to remove is to remove the creation of a FJT for every
submission (and we have _lots_ of those), but unfortunately it seems rather
non-straightforward as I'll have to call reinitialize at the end of
executing the FJT.

What I essentially want to do is to treat the FJT as a coroutine.
Ideas?

Cheers,
?

-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120319/d9969045/attachment.html>

From aleksey.shipilev at gmail.com  Tue Mar 20 05:31:52 2012
From: aleksey.shipilev at gmail.com (Aleksey Shipilev)
Date: Tue, 20 Mar 2012 13:31:52 +0400
Subject: [concurrency-interest] ForkJoinTask.fork() enforces visibility?
Message-ID: <CA+1LWGEYjx1otyPEX5ASAOnAvG73m-Wr_W_xN553xBW_PKpC_g@mail.gmail.com>

Hi,

I'm looking over ForkJoinTask.fork() JavaDoc, and reading this:

     * Arranges to asynchronously execute this task.  While it is not
     * necessarily enforced, it is a usage error to fork a task more
     * than once unless it has completed and been reinitialized.
     * Subsequent modifications to the state of this task or any data
     * it operates on are not necessarily consistently observable by
     * any thread other than the one executing it unless preceded by a
     * call to {@link #join} or related methods, or a call to {@link
     * #isDone} returning {@code true}.

...is that implying that the data passed in ForkJoinTask.fork() *is*
visible (i.e. fork() enforces visibility)?

To be more precise:

public void submit() {
     Data data1 = new Data();
     new MyForkJoinTask(data1).fork(); // fork, submitting with data1
}

class MyForkJoinTask<Void> {

    private Data data;  // note this is not final nor volatile

    public class MyForkJoinTask(Data data) {
        this.data = data;
    }

    @Override
    public final boolean exec() {
        // is $data guaranteed to be visible as "data1" here?
    }

    ...
}

I had looked over FJP javadoc and saw no other visibility guarantees
there. I remember Brian Goetz' article describing FJP is guaranteeing
visibility in cases like this. It would be perfect to reflect that in
javadocs.

-Aleksey.

From aleksey.shipilev at gmail.com  Tue Mar 20 06:15:31 2012
From: aleksey.shipilev at gmail.com (Aleksey Shipilev)
Date: Tue, 20 Mar 2012 14:15:31 +0400
Subject: [concurrency-interest] ForkJoinTask.fork() enforces visibility?
In-Reply-To: <CA+1LWGEYjx1otyPEX5ASAOnAvG73m-Wr_W_xN553xBW_PKpC_g@mail.gmail.com>
References: <CA+1LWGEYjx1otyPEX5ASAOnAvG73m-Wr_W_xN553xBW_PKpC_g@mail.gmail.com>
Message-ID: <CA+1LWGGOKCwXD2Cf4qKkKjcuEk5THMK+CdXEFKdXwJhmVCjPyA@mail.gmail.com>

On Tue, Mar 20, 2012 at 1:31 PM, Aleksey Shipilev
<aleksey.shipilev at gmail.com> wrote:
> I had looked over FJP javadoc and saw no other visibility guarantees
> there. I remember Brian Goetz' article describing FJP is guaranteeing
> visibility in cases like this. It would be perfect to reflect that in
> javadocs.

Aha, ExecutorService javadoc is saying:

"Memory consistency effects: Actions in a thread prior to the
submission of a Runnable or Callable task to an ExecutorService
happen-before any actions taken by that task, which in turn
happen-before the result is retrieved via Future.get()."

Does this extend to ForkJoinTask.fork()?

-Aleksey.

From dl at cs.oswego.edu  Tue Mar 20 06:47:04 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 20 Mar 2012 06:47:04 -0400
Subject: [concurrency-interest] Advice on reuse of ForkJoinTask instances
In-Reply-To: <CANPzfU-AxF0OxWahnGuBcfQKDCAjqZFh39QV7dxKkKm7+ySrhg@mail.gmail.com>
References: <CANPzfU-AxF0OxWahnGuBcfQKDCAjqZFh39QV7dxKkKm7+ySrhg@mail.gmail.com>
Message-ID: <4F686028.6050505@cs.oswego.edu>

On 03/19/12 12:52, ?iktor ?lang wrote:
> We're using the latest incarnation of the ForkJoinPool in Akka,
> and I was looking at reducing allocations in the hot path, and essentially the
> only thing left to remove is to remove the creation of a FJT for every
> submission (and we have _lots_ of those), but unfortunately it seems rather
> non-straightforward as I'll have to call reinitialize at the end of executing
> the FJT.
>

There are a few common cases where it is easy and effective
to reuse FJTs, for example when building trees of recurring
computations. (The FJJacobi program in our CVS src/tests/loops
has one example.) But in most other cases, GC will do a better
job of managing space than you can. FJ is extremely friendly
to generational collectors -- most tasks become garbage and
reclaimable almost immediately, and managing the others is done
better using GC mechanics than almost anything else you could do.

Usually a better tactic is to minimize any auxiliary object
creation needed to start or process a task. Often enough,
these other objects are less GC-friendly. Among other steps,
it is usually worthwhile to "flatten" tasks so that they
contain all necessary state for execution without creating other
transient objects. For example, in graph algorithms, it often
works nicely to make each node itself a ForkJoinTask.

-Doug



From dl at cs.oswego.edu  Tue Mar 20 06:47:14 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 20 Mar 2012 06:47:14 -0400
Subject: [concurrency-interest] ForkJoinTask.fork() enforces visibility?
In-Reply-To: <CA+1LWGGOKCwXD2Cf4qKkKjcuEk5THMK+CdXEFKdXwJhmVCjPyA@mail.gmail.com>
References: <CA+1LWGEYjx1otyPEX5ASAOnAvG73m-Wr_W_xN553xBW_PKpC_g@mail.gmail.com>
	<CA+1LWGGOKCwXD2Cf4qKkKjcuEk5THMK+CdXEFKdXwJhmVCjPyA@mail.gmail.com>
Message-ID: <4F686032.6010107@cs.oswego.edu>

On 03/20/12 06:15, Aleksey Shipilev wrote:
> On Tue, Mar 20, 2012 at 1:31 PM, Aleksey Shipilev
> <aleksey.shipilev at gmail.com>  wrote:
>> I had looked over FJP javadoc and saw no other visibility guarantees
>> there. I remember Brian Goetz' article describing FJP is guaranteeing
>> visibility in cases like this. It would be perfect to reflect that in
>> javadocs.
>
> Aha, ExecutorService javadoc is saying:
>
> "Memory consistency effects: Actions in a thread prior to the
> submission of a Runnable or Callable task to an ExecutorService
> happen-before any actions taken by that task, which in turn
> happen-before the result is retrieved via Future.get()."
>
> Does this extend to ForkJoinTask.fork()?
>

Yes. Thanks for pointing out that we need an explicit statement
to this effect in ForkJoinTask. We'll add one.

-Doug

From viktor.klang at gmail.com  Tue Mar 20 08:09:33 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 20 Mar 2012 13:09:33 +0100
Subject: [concurrency-interest] Advice on reuse of ForkJoinTask instances
In-Reply-To: <4F686028.6050505@cs.oswego.edu>
References: <CANPzfU-AxF0OxWahnGuBcfQKDCAjqZFh39QV7dxKkKm7+ySrhg@mail.gmail.com>
	<4F686028.6050505@cs.oswego.edu>
Message-ID: <CANPzfU8u6UKnN1Q421+L4VG64hCbnjJ_7GecHTgkc7msdhZHfQ@mail.gmail.com>

On Tue, Mar 20, 2012 at 11:47 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 03/19/12 12:52, ?iktor ?lang wrote:
>
>> We're using the latest incarnation of the ForkJoinPool in Akka,
>> and I was looking at reducing allocations in the hot path, and
>> essentially the
>> only thing left to remove is to remove the creation of a FJT for every
>> submission (and we have _lots_ of those), but unfortunately it seems
>> rather
>> non-straightforward as I'll have to call reinitialize at the end of
>> executing
>> the FJT.
>>
>>
> There are a few common cases where it is easy and effective
> to reuse FJTs, for example when building trees of recurring
> computations. (The FJJacobi program in our CVS src/tests/loops
> has one example.) But in most other cases, GC will do a better
> job of managing space than you can. FJ is extremely friendly
> to generational collectors -- most tasks become garbage and
> reclaimable almost immediately, and managing the others is done
> better using GC mechanics than almost anything else you could do.
>
> Usually a better tactic is to minimize any auxiliary object
> creation needed to start or process a task. Often enough,
> these other objects are less GC-friendly. Among other steps,
> it is usually worthwhile to "flatten" tasks so that they
> contain all necessary state for execution without creating other
> transient objects. For example, in graph algorithms, it often
> works nicely to make each node itself a ForkJoinTask.
>

We're talking about _alot_ of submissions here :-)
To get 0 allocations in my hot path I'd want to be able to resubmit the
current FJT,
I can probably implement it myself, but I believe that it could be useful
for others as well.

Cheers,
?


>
> -Doug
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120320/d495ad6d/attachment.html>

From viktor.klang at gmail.com  Tue Mar 20 18:03:53 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 20 Mar 2012 23:03:53 +0100
Subject: [concurrency-interest] Advice on reuse of ForkJoinTask instances
In-Reply-To: <CANPzfU8u6UKnN1Q421+L4VG64hCbnjJ_7GecHTgkc7msdhZHfQ@mail.gmail.com>
References: <CANPzfU-AxF0OxWahnGuBcfQKDCAjqZFh39QV7dxKkKm7+ySrhg@mail.gmail.com>
	<4F686028.6050505@cs.oswego.edu>
	<CANPzfU8u6UKnN1Q421+L4VG64hCbnjJ_7GecHTgkc7msdhZHfQ@mail.gmail.com>
Message-ID: <CANPzfU9J3197nwrjR2cfh-db0_f2ACjcPyxSNM2NpXdJ2VTSJg@mail.gmail.com>

Would it make sense to use a status bit for "resubmit" in the general sense?

Cheers,
?

2012/3/20 ?iktor ?lang <viktor.klang at gmail.com>

>
>
> On Tue, Mar 20, 2012 at 11:47 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 03/19/12 12:52, ?iktor ?lang wrote:
>>
>>> We're using the latest incarnation of the ForkJoinPool in Akka,
>>> and I was looking at reducing allocations in the hot path, and
>>> essentially the
>>> only thing left to remove is to remove the creation of a FJT for every
>>> submission (and we have _lots_ of those), but unfortunately it seems
>>> rather
>>> non-straightforward as I'll have to call reinitialize at the end of
>>> executing
>>> the FJT.
>>>
>>>
>> There are a few common cases where it is easy and effective
>> to reuse FJTs, for example when building trees of recurring
>> computations. (The FJJacobi program in our CVS src/tests/loops
>> has one example.) But in most other cases, GC will do a better
>> job of managing space than you can. FJ is extremely friendly
>> to generational collectors -- most tasks become garbage and
>> reclaimable almost immediately, and managing the others is done
>> better using GC mechanics than almost anything else you could do.
>>
>> Usually a better tactic is to minimize any auxiliary object
>> creation needed to start or process a task. Often enough,
>> these other objects are less GC-friendly. Among other steps,
>> it is usually worthwhile to "flatten" tasks so that they
>> contain all necessary state for execution without creating other
>> transient objects. For example, in graph algorithms, it often
>> works nicely to make each node itself a ForkJoinTask.
>>
>
> We're talking about _alot_ of submissions here :-)
> To get 0 allocations in my hot path I'd want to be able to resubmit the
> current FJT,
> I can probably implement it myself, but I believe that it could be useful
> for others as well.
>
> Cheers,
> ?
>
>
>>
>> -Doug
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
>
>
> --
> Viktor Klang
>
> Akka Tech Lead
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
>
> Twitter: @viktorklang
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120320/c7fc7ce3/attachment.html>

From dl at cs.oswego.edu  Tue Mar 20 19:16:40 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 20 Mar 2012 19:16:40 -0400
Subject: [concurrency-interest] Advice on reuse of ForkJoinTask instances
In-Reply-To: <CANPzfU8u6UKnN1Q421+L4VG64hCbnjJ_7GecHTgkc7msdhZHfQ@mail.gmail.com>
References: <CANPzfU-AxF0OxWahnGuBcfQKDCAjqZFh39QV7dxKkKm7+ySrhg@mail.gmail.com>	<4F686028.6050505@cs.oswego.edu>
	<CANPzfU8u6UKnN1Q421+L4VG64hCbnjJ_7GecHTgkc7msdhZHfQ@mail.gmail.com>
Message-ID: <4F690FD8.8040303@cs.oswego.edu>

On 03/20/12 08:09, ?iktor ?lang wrote:
> We're talking about _alot_ of submissions here :-)

We have toy test examples (like Fib) that execute over
400million tasks per second on some machines without big
GC pauses, so it is not a sure thing that GC is hurting you
enough to  be worth a lot of effort. But...

> To get 0 allocations in my hot path I'd want to be able to resubmit the current FJT,
> I can probably implement it myself, but I believe that it could be useful for
> others as well.
>

As it states in the javadocs, you have to be sure a task is done and
that no other thread/task is depending on its completion before you
can reinitialize/refork. In general, you can't automate this rule.
But ff you are using FJ for async tasks that are never joined (which I
recall holds in your case), you probably don't need to worry about
the task completion dependencies. Which means that it might be
possible to do better than a GC mechanism that would handle
these dependencies (this is the main thing that GCs do better
than you can).

One possibility is to create a ThreadLocal cache of them,
but you'd still need to check isDone to when selecting them
from the cache.

-Doug




From andrew.gomilko at gmail.com  Tue Mar 20 19:26:12 2012
From: andrew.gomilko at gmail.com (andrew.gomilko at gmail.com)
Date: Tue, 20 Mar 2012 16:26:12 -0700
Subject: [concurrency-interest] ForkJoinTask.fork() enforces visibility?
In-Reply-To: <CA+1LWGEYjx1otyPEX5ASAOnAvG73m-Wr_W_xN553xBW_PKpC_g@mail.gmail.com>
References: <CA+1LWGEYjx1otyPEX5ASAOnAvG73m-Wr_W_xN553xBW_PKpC_g@mail.gmail.com>
Message-ID: <4FC3F6FA-309B-41B6-AE2E-5624A351F2EB@gmail.com>

Hi,

Probably, my question is silly.
But, Is the data field being set in the constructor, doesn't imply it's visibility to     the class members?

- Andrew

On Mar 20, 2012, at 2:31 AM, Aleksey Shipilev <aleksey.shipilev at gmail.com> wrote:

> Hi,
> 
> I'm looking over ForkJoinTask.fork() JavaDoc, and reading this:
> 
>     * Arranges to asynchronously execute this task.  While it is not
>     * necessarily enforced, it is a usage error to fork a task more
>     * than once unless it has completed and been reinitialized.
>     * Subsequent modifications to the state of this task or any data
>     * it operates on are not necessarily consistently observable by
>     * any thread other than the one executing it unless preceded by a
>     * call to {@link #join} or related methods, or a call to {@link
>     * #isDone} returning {@code true}.
> 
> ...is that implying that the data passed in ForkJoinTask.fork() *is*
> visible (i.e. fork() enforces visibility)?
> 
> To be more precise:
> 
> public void submit() {
>     Data data1 = new Data();
>     new MyForkJoinTask(data1).fork(); // fork, submitting with data1
> }
> 
> class MyForkJoinTask<Void> {
> 
>    private Data data;  // note this is not final nor volatile
> 
>    public class MyForkJoinTask(Data data) {
>        this.data = data;
>    }
> 
>    @Override
>    public final boolean exec() {
>        // is $data guaranteed to be visible as "data1" here?
>    }
> 
>    ...
> }
> 
> I had looked over FJP javadoc and saw no other visibility guarantees
> there. I remember Brian Goetz' article describing FJP is guaranteeing
> visibility in cases like this. It would be perfect to reflect that in
> javadocs.
> 
> -Aleksey.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From viktor.klang at gmail.com  Tue Mar 20 19:46:44 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 21 Mar 2012 00:46:44 +0100
Subject: [concurrency-interest] Advice on reuse of ForkJoinTask instances
In-Reply-To: <4F690FD8.8040303@cs.oswego.edu>
References: <CANPzfU-AxF0OxWahnGuBcfQKDCAjqZFh39QV7dxKkKm7+ySrhg@mail.gmail.com>
	<4F686028.6050505@cs.oswego.edu>
	<CANPzfU8u6UKnN1Q421+L4VG64hCbnjJ_7GecHTgkc7msdhZHfQ@mail.gmail.com>
	<4F690FD8.8040303@cs.oswego.edu>
Message-ID: <CANPzfU8b76Jk7RiPhLDmNrEz06_R-ubXxzKi7p+nVbu5QgLpwg@mail.gmail.com>

On Wed, Mar 21, 2012 at 12:16 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 03/20/12 08:09, ?iktor ?lang wrote:
>
>> We're talking about _alot_ of submissions here :-)
>>
>
> We have toy test examples (like Fib) that execute over
> 400million tasks per second on some machines without big
> GC pauses, so it is not a sure thing that GC is hurting you
> enough to  be worth a lot of effort. But...


Yeah, I guess it's one of those scenarios where you won't know how much
difference it makes unless you actually try it ;-)


>
>
>  To get 0 allocations in my hot path I'd want to be able to resubmit the
>> current FJT,
>> I can probably implement it myself, but I believe that it could be useful
>> for
>> others as well.
>>
>>
> As it states in the javadocs, you have to be sure a task is done and
> that no other thread/task is depending on its completion before you
> can reinitialize/refork. In general, you can't automate this rule.
> But ff you are using FJ for async tasks that are never joined (which I
> recall holds in your case), you probably don't need to worry about
> the task completion dependencies. Which means that it might be
> possible to do better than a GC mechanism that would handle
> these dependencies (this is the main thing that GCs do better
> than you can).
>

Yup, essentially I just want to be able to, within the FJT, to signal that
it should be resubmitted after execution.


>
> One possibility is to create a ThreadLocal cache of them,
> but you'd still need to check isDone to when selecting them
> from the cache.


Yeah, would like to avoid object pooling, it always gives me the
Heebie-Jeebies.


Cheers,
?


>
>
> -Doug
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120321/c677b675/attachment.html>

From davidcholmes at aapt.net.au  Tue Mar 20 20:15:22 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 21 Mar 2012 10:15:22 +1000
Subject: [concurrency-interest] ForkJoinTask.fork() enforces visibility?
In-Reply-To: <4FC3F6FA-309B-41B6-AE2E-5624A351F2EB@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMELEJDAA.davidcholmes@aapt.net.au>

andrew.gomilko at gmail.com writes:
>
> Probably, my question is silly.
> But, Is the data field being set in the constructor, doesn't
> imply it's visibility to     the class members?

No, not in general. If an object is constructed and then published for use
across multiple threads, then for the constructed fields to be properly
visible a "safe publication" mechanism must be used. Here fork() is a safe
means of sharing the FJT across threads.

David Holmes

> - Andrew
>
> On Mar 20, 2012, at 2:31 AM, Aleksey Shipilev
> <aleksey.shipilev at gmail.com> wrote:
>
> > Hi,
> >
> > I'm looking over ForkJoinTask.fork() JavaDoc, and reading this:
> >
> >     * Arranges to asynchronously execute this task.  While it is not
> >     * necessarily enforced, it is a usage error to fork a task more
> >     * than once unless it has completed and been reinitialized.
> >     * Subsequent modifications to the state of this task or any data
> >     * it operates on are not necessarily consistently observable by
> >     * any thread other than the one executing it unless preceded by a
> >     * call to {@link #join} or related methods, or a call to {@link
> >     * #isDone} returning {@code true}.
> >
> > ...is that implying that the data passed in ForkJoinTask.fork() *is*
> > visible (i.e. fork() enforces visibility)?
> >
> > To be more precise:
> >
> > public void submit() {
> >     Data data1 = new Data();
> >     new MyForkJoinTask(data1).fork(); // fork, submitting with data1
> > }
> >
> > class MyForkJoinTask<Void> {
> >
> >    private Data data;  // note this is not final nor volatile
> >
> >    public class MyForkJoinTask(Data data) {
> >        this.data = data;
> >    }
> >
> >    @Override
> >    public final boolean exec() {
> >        // is $data guaranteed to be visible as "data1" here?
> >    }
> >
> >    ...
> > }
> >
> > I had looked over FJP javadoc and saw no other visibility guarantees
> > there. I remember Brian Goetz' article describing FJP is guaranteeing
> > visibility in cases like this. It would be perfect to reflect that in
> > javadocs.
> >
> > -Aleksey.
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From bryan at systap.com  Wed Mar 21 15:25:14 2012
From: bryan at systap.com (Bryan Thompson)
Date: Wed, 21 Mar 2012 14:25:14 -0500
Subject: [concurrency-interest] Advice on tracking down the caller of
	Thread.interrupt()?
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED467386DDF5F@AUSP01VMBX06.collaborationhost.net>

Hello,

I am curious if there are any tricks for tracking down the source of an interrupt. That is, the Thread which actually invoked Thread.interrupt() on some thread.  

We use a disk IO in a database with concurrent readers on a shared file. IO, of course, can be interrupted.  And when it is interrupted the backing channel is closed, so we have to jump through hoops to ensure that the channel is reopened for those threads which see the AsynchronousCloseException (or ClosedChannelException), while ensuring that the thread which sees the ClosedByInterruptException terminates its activity.

Normally I can spot the source of an interrupt by looking at the context in which the interrupt occurs and work backward through the logic of the application to figure out why the Thread might have been interrupted.  However, this is difficult when our code is embedded into other applications when we have little to know idea what else might be going on.  In particular, I am looking at an issue now where I think that the interrupt might be coming from some other component.  For example, by holding onto a Thread reference beyond the life of a worker task and then interrupting that thread.

Is it possible to somehow instrument the JVM to track and report the caller of Thread.interrupt()?  Are there profilers which can do this?  Can this be done somehow by registering a SecurityManager to log stack fames in Thread.checkAccess()?

Thanks in advance,
Bryan

From genman at noderunner.net  Wed Mar 21 15:55:36 2012
From: genman at noderunner.net (Elias Ross)
Date: Wed, 21 Mar 2012 12:55:36 -0700
Subject: [concurrency-interest] Advice on tracking down the caller of
	Thread.interrupt()?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED467386DDF5F@AUSP01VMBX06.collaborationhost.net>
References: <DE10B00CCE0DC54883734F3060AC9ED467386DDF5F@AUSP01VMBX06.collaborationhost.net>
Message-ID: <CAKsEmEPwcKEYW3BXH5gMNQqMc_tjot4As_=Rq=Yg24Ea+O7JCA@mail.gmail.com>

On Wed, Mar 21, 2012 at 12:25 PM, Bryan Thompson <bryan at systap.com> wrote:
> Hello,
>
> I am curious if there are any tricks for tracking down the source of an interrupt.
>That is, the Thread which actually invoked Thread.interrupt() on some thread.

Check out JBoss Byteman. There is an example very close to what you want to do:

https://community.jboss.org/wiki/ABytemanTutorial#how_do_i_inject_code_into_jvm_classes

I have used this tool to track down allocation issues before.

From forax at univ-mlv.fr  Wed Mar 21 16:00:54 2012
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Wed, 21 Mar 2012 21:00:54 +0100
Subject: [concurrency-interest] Advice on tracking down the caller of
 Thread.interrupt()?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED467386DDF5F@AUSP01VMBX06.collaborationhost.net>
References: <DE10B00CCE0DC54883734F3060AC9ED467386DDF5F@AUSP01VMBX06.collaborationhost.net>
Message-ID: <4F6A3376.3090003@univ-mlv.fr>

On 03/21/2012 08:25 PM, Bryan Thompson wrote:
> Hello,
>
> I am curious if there are any tricks for tracking down the source of an interrupt. That is, the Thread which actually invoked Thread.interrupt() on some thread.
>
> We use a disk IO in a database with concurrent readers on a shared file. IO, of course, can be interrupted.  And when it is interrupted the backing channel is closed, so we have to jump through hoops to ensure that the channel is reopened for those threads which see the AsynchronousCloseException (or ClosedChannelException), while ensuring that the thread which sees the ClosedByInterruptException terminates its activity.
>
> Normally I can spot the source of an interrupt by looking at the context in which the interrupt occurs and work backward through the logic of the application to figure out why the Thread might have been interrupted.  However, this is difficult when our code is embedded into other applications when we have little to know idea what else might be going on.  In particular, I am looking at an issue now where I think that the interrupt might be coming from some other component.  For example, by holding onto a Thread reference beyond the life of a worker task and then interrupting that thread.
>
> Is it possible to somehow instrument the JVM to track and report the caller of Thread.interrupt()?  Are there profilers which can do this?  Can this be done somehow by registering a SecurityManager to log stack fames in Thread.checkAccess()?
>
> Thanks in advance,
> Bryan

You can do it by yourself, it's not hard :)

 From the directory that contains all JDK files, find the src.zip, which 
is roughly
a zip of the source of the public classes of the JDK.
Extract java/lang/Thread.java
Now, as you can see Thread::interrupt() is written in Java, so
you can add all the instructions you want :)
To get the caller class, use sun.reflect.reflection.getCallerClass(2) 
(see [1]),
because you are in a trusted class, you can call it.
Compile your class and prepend it in front of the bootclasspath,
so your classwill be loaded instead of the one from rt.jar
(see java -X and java -Xbootclasspath/p )

cheers,
R?mi

[1] 
http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/tip/src/share/classes/sun/reflect/Reflection.java


From nathan.reynolds at oracle.com  Wed Mar 21 16:05:48 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 21 Mar 2012 13:05:48 -0700
Subject: [concurrency-interest] Advice on tracking down the caller of
 Thread.interrupt()?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED467386DDF5F@AUSP01VMBX06.collaborationhost.net>
References: <DE10B00CCE0DC54883734F3060AC9ED467386DDF5F@AUSP01VMBX06.collaborationhost.net>
Message-ID: <4F6A349C.4090405@oracle.com>

Thread.interrupt() is a Java method.  You could use BTrace to instrument 
the method and print the call stack and thread interrupted and interruptee.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 3/21/2012 12:25 PM, Bryan Thompson wrote:
> Hello,
>
> I am curious if there are any tricks for tracking down the source of an interrupt. That is, the Thread which actually invoked Thread.interrupt() on some thread.
>
> We use a disk IO in a database with concurrent readers on a shared file. IO, of course, can be interrupted.  And when it is interrupted the backing channel is closed, so we have to jump through hoops to ensure that the channel is reopened for those threads which see the AsynchronousCloseException (or ClosedChannelException), while ensuring that the thread which sees the ClosedByInterruptException terminates its activity.
>
> Normally I can spot the source of an interrupt by looking at the context in which the interrupt occurs and work backward through the logic of the application to figure out why the Thread might have been interrupted.  However, this is difficult when our code is embedded into other applications when we have little to know idea what else might be going on.  In particular, I am looking at an issue now where I think that the interrupt might be coming from some other component.  For example, by holding onto a Thread reference beyond the life of a worker task and then interrupting that thread.
>
> Is it possible to somehow instrument the JVM to track and report the caller of Thread.interrupt()?  Are there profilers which can do this?  Can this be done somehow by registering a SecurityManager to log stack fames in Thread.checkAccess()?
>
> Thanks in advance,
> Bryan
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120321/a4b1b311/attachment.html>

From radhakrishnan.mohan at gmail.com  Sun Mar 25 00:54:35 2012
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Sun, 25 Mar 2012 10:24:35 +0530
Subject: [concurrency-interest] Lock type question
Message-ID: <CAOoXFP-p-rj3qd1rTC22A2kE8X8Pk3FiCK_4D8Nm5vWqF2LdhA@mail.gmail.com>

Hi,
         Does lock striping have any advantages in a multi-core
environment. Is there any material to read to understand this ?

         What papers describe types of locks ( thin locks vs fat locks etc.
) ? I have seen brief blog entries about these and I am aware of biased
locks.

Thanks,
Mohan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120325/f3768ec5/attachment.html>

From davidcholmes at aapt.net.au  Sun Mar 25 06:14:04 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 25 Mar 2012 20:14:04 +1000
Subject: [concurrency-interest] Lock type question
In-Reply-To: <CAOoXFP-p-rj3qd1rTC22A2kE8X8Pk3FiCK_4D8Nm5vWqF2LdhA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEMNJDAA.davidcholmes@aapt.net.au>

Use citeseerx to find papers on "thin locks", "meta locks" etc eg:

http://citeseerx.ist.psu.edu/search?q=thin+locks&submit=Search&sort=rlv&t=do
c

David Holmes

  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mohan
Radhakrishnan
  Sent: Sunday, 25 March 2012 2:55 PM
  To: Concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Lock type question


  Hi,
           Does lock striping have any advantages in a multi-core
environment. Is there any material to read to understand this ?


           What papers describe types of locks ( thin locks vs fat locks
etc. ) ? I have seen brief blog entries about these and I am aware of biased
locks.


  Thanks,
  Mohan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120325/930ac090/attachment.html>

From djg at cs.washington.edu  Sun Mar 25 16:48:10 2012
From: djg at cs.washington.edu (Dan Grossman)
Date: Sun, 25 Mar 2012 13:48:10 -0700
Subject: [concurrency-interest] help show speed-up on a trivial but
 manual array-map operation?
In-Reply-To: <CADruQ+jemh19Mn-jAriV_8Y4q2OWvMyD_w6QWoJVs0CJmfOs2w@mail.gmail.com>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
	<4F5A0166.9080400@cs.oswego.edu> <4F5A0396.6070601@cs.oswego.edu>
	<88AE6996-195A-483D-9E7A-3CB2971A0968@cs.pomona.edu>
	<CADruQ+jemh19Mn-jAriV_8Y4q2OWvMyD_w6QWoJVs0CJmfOs2w@mail.gmail.com>
Message-ID: <CADruQ+gazmYGsR-B_ARnzk9-3QvBkLQF4-QNB7W+Yu_Ok=q2Hw@mail.gmail.com>

Just to close the loop on this thread, today I edited

 http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html

which is part of the materials at

 http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/

I added a new section "timing issues" that lists various reasons why
you might see a /simple/ fork-join computation run slower than a
corresponding sequential implementation.  This distills the
suggestions on this thread and other well-known ideas.  Additional
suggestions and clarifications are welcome.

--Dan

On Fri, Mar 9, 2012 at 6:40 PM, Dan Grossman <djg at cs.washington.edu> wrote:
> Let me echo Kim's word of thanks.
>
> When I have time in a couple weeks, I hope to extend my materials to
> include some of the excellent points and example made here: gotchas
> with respect to timing, the need for large enough problem sizes, and
> -- most important in my mind -- the need for beefy enough arithmetic
> operations to overcome memory-bandwidth issues.
>
> As Kim points out, any simple-enough-for-undergraduate-lecture
> examples are welcome.
>
> --Dan
>
> On Fri, Mar 9, 2012 at 5:47 PM, Kim Bruce <kim at cs.pomona.edu> wrote:
>> Thanks to Doug and everyone else for their suggestions. ?Doug's changes seem to buy me about a 12 times speed-up with a 48 core machine. ?This will definitely be more impressive for my students than the slowdowns or marginal speed-ups we were getting before.
>>
>> I'd be thrilled to see a set of relatively simple (to explain) programs using ForkJoin that could be used to demonstrate significant speedups with many-core computers. ?I'm going to be asking students to do some timing in a lab in a couple of weeks and would like them to be able to do things that do show significant speedup.
>>
>> Also valuable would be pointers to sites that might have good parallel/concurrency class projects (especially week-long assignments) for students in a data structures course learning about how to use these constructs.
>>
>> Thanks again for everyone's help!
>>
>> Kim
>>
>>
>>
>> On Mar 9, 2012, at 5:20 AM, Doug Lea wrote:
>>
>>> On 03/09/12 08:11, Doug Lea wrote:
>>>
>>>> 1. Microbenchmarking artifacts: The results of the
>>>> computations are never used so JVMs can kill some of the
>>>> code. The attached edited version includes a "checkSum" method
>>>> that combats this.
>>>
>>> Plus, as I should have checked before, the use of a linear
>>> stream of initial values is also subject to microbenchmarking
>>> artifacts. Filling with random values instead combats this.
>>>
>>> The main moral is that reliably measuring anything on modern
>>> systems is harder than you'd think it should be.
>>>
>>> -Doug
>>> <VecAdd.java>
>>


From concurrency-interest at stefan-marr.de  Mon Mar 26 09:58:22 2012
From: concurrency-interest at stefan-marr.de (Stefan Marr)
Date: Mon, 26 Mar 2012 15:58:22 +0200
Subject: [concurrency-interest] help show speed-up on a trivial but
	manual array-map operation?
In-Reply-To: <CADruQ+gazmYGsR-B_ARnzk9-3QvBkLQF4-QNB7W+Yu_Ok=q2Hw@mail.gmail.com>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
	<4F5A0166.9080400@cs.oswego.edu> <4F5A0396.6070601@cs.oswego.edu>
	<88AE6996-195A-483D-9E7A-3CB2971A0968@cs.pomona.edu>
	<CADruQ+jemh19Mn-jAriV_8Y4q2OWvMyD_w6QWoJVs0CJmfOs2w@mail.gmail.com>
	<CADruQ+gazmYGsR-B_ARnzk9-3QvBkLQF4-QNB7W+Yu_Ok=q2Hw@mail.gmail.com>
Message-ID: <C8918A1C-1AD6-4AEA-9789-53A6A9ED0576@stefan-marr.de>

Hi Dan:

On 25 Mar 2012, at 22:48, Dan Grossman wrote:

> Just to close the loop on this thread, today I edited
> 
> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html
> 
> which is part of the materials at
> 
> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/
> 
> I added a new section "timing issues" that lists various reasons why
> you might see a /simple/ fork-join computation run slower than a
> corresponding sequential implementation.  This distills the
> suggestions on this thread and other well-known ideas.  Additional
> suggestions and clarifications are welcome.

An additional suggestion might be to use some 'established' benchmarking framework like http://code.google.com/p/caliper/
They also list common pitfalls for benchmarking on the JVMs.

Best regards
Stefan


-- 
Stefan Marr
Software Languages Lab
Vrije Universiteit Brussel
Pleinlaan 2 / B-1050 Brussels / Belgium
http://soft.vub.ac.be/~smarr
Phone: +32 2 629 2974
Fax:   +32 2 629 3525



From bryan at systap.com  Tue Mar 27 14:33:53 2012
From: bryan at systap.com (Bryan Thompson)
Date: Tue, 27 Mar 2012 13:33:53 -0500
Subject: [concurrency-interest] Advice on tracking down the caller of
 Thread.interrupt()?
In-Reply-To: <4F6A3376.3090003@univ-mlv.fr>
References: <DE10B00CCE0DC54883734F3060AC9ED467386DDF5F@AUSP01VMBX06.collaborationhost.net>
	<4F6A3376.3090003@univ-mlv.fr>
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED46739FF0AE0@AUSP01VMBX06.collaborationhost.net>

Thanks to everyone who wrote back on this.  One other suggestion (off list) was that Thread.interrupt() is not final.   You can specialize the thread pool factory and override Thread.interrupt to report the Thread which triggered the interrupt.

Thanks again,
Bryan

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of R?mi Forax
> Sent: Wednesday, March 21, 2012 4:01 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Advice on tracking down 
> the caller of Thread.interrupt()?
> 
> On 03/21/2012 08:25 PM, Bryan Thompson wrote:
> > Hello,
> >
> > I am curious if there are any tricks for tracking down the 
> source of an interrupt. That is, the Thread which actually 
> invoked Thread.interrupt() on some thread.
> >
> > We use a disk IO in a database with concurrent readers on a 
> shared file. IO, of course, can be interrupted.  And when it 
> is interrupted the backing channel is closed, so we have to 
> jump through hoops to ensure that the channel is reopened for 
> those threads which see the AsynchronousCloseException (or 
> ClosedChannelException), while ensuring that the thread which 
> sees the ClosedByInterruptException terminates its activity.
> >
> > Normally I can spot the source of an interrupt by looking 
> at the context in which the interrupt occurs and work 
> backward through the logic of the application to figure out 
> why the Thread might have been interrupted.  However, this is 
> difficult when our code is embedded into other applications 
> when we have little to know idea what else might be going on. 
>  In particular, I am looking at an issue now where I think 
> that the interrupt might be coming from some other component. 
>  For example, by holding onto a Thread reference beyond the 
> life of a worker task and then interrupting that thread.
> >
> > Is it possible to somehow instrument the JVM to track and 
> report the caller of Thread.interrupt()?  Are there profilers 
> which can do this?  Can this be done somehow by registering a 
> SecurityManager to log stack fames in Thread.checkAccess()?
> >
> > Thanks in advance,
> > Bryan
> 
> You can do it by yourself, it's not hard :)
> 
>  From the directory that contains all JDK files, find the 
> src.zip, which is roughly a zip of the source of the public 
> classes of the JDK.
> Extract java/lang/Thread.java
> Now, as you can see Thread::interrupt() is written in Java, 
> so you can add all the instructions you want :) To get the 
> caller class, use sun.reflect.reflection.getCallerClass(2)
> (see [1]),
> because you are in a trusted class, you can call it.
> Compile your class and prepend it in front of the 
> bootclasspath, so your classwill be loaded instead of the one 
> from rt.jar (see java -X and java -Xbootclasspath/p )
> 
> cheers,
> R?mi
> 
> [1]
> http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/tip/src/share/cl
> asses/sun/reflect/Reflection.java
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From yakov.zhdanov at gmail.com  Thu Mar 29 01:48:35 2012
From: yakov.zhdanov at gmail.com (Yakov Zhdanov)
Date: Thu, 29 Mar 2012 09:48:35 +0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 86,
	Issue 18
In-Reply-To: <mailman.1.1332950400.13218.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1332950400.13218.concurrency-interest@cs.oswego.edu>
Message-ID: <CAMhQLZq6FAgkdOyZqULeLHyWoDzVxhLZs+zs_x6xX65aYrrFtQ@mail.gmail.com>

You may also override interrupt() this way:

        @Override public void interrupt() {
            // Get full stack on interruption.
            new Exception().printStackTrace();

            super.interrupt();
        }

Yakov


2012/3/28 <concurrency-interest-request at cs.oswego.edu>

> Send Concurrency-interest mailing list submissions to
>        concurrency-interest at cs.oswego.edu
>
> To subscribe or unsubscribe via the World Wide Web, visit
>        http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>        concurrency-interest-request at cs.oswego.edu
>
> You can reach the person managing the list at
>        concurrency-interest-owner at cs.oswego.edu
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>   1. Re: Advice on tracking down the caller of Thread.interrupt()?
>      (Bryan Thompson)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 27 Mar 2012 13:33:53 -0500
> From: Bryan Thompson <bryan at systap.com>
> To: "concurrency-interest at cs.oswego.edu"
>        <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Advice on tracking down the caller
>        of Thread.interrupt()?
> Message-ID:
>        <
> DE10B00CCE0DC54883734F3060AC9ED46739FF0AE0 at AUSP01VMBX06.collaborationhost.net
> >
>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Thanks to everyone who wrote back on this.  One other suggestion (off
> list) was that Thread.interrupt() is not final.   You can specialize the
> thread pool factory and override Thread.interrupt to report the Thread
> which triggered the interrupt.
>
> Thanks again,
> Bryan
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> > Of R?mi Forax
> > Sent: Wednesday, March 21, 2012 4:01 PM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] Advice on tracking down
> > the caller of Thread.interrupt()?
> >
> > On 03/21/2012 08:25 PM, Bryan Thompson wrote:
> > > Hello,
> > >
> > > I am curious if there are any tricks for tracking down the
> > source of an interrupt. That is, the Thread which actually
> > invoked Thread.interrupt() on some thread.
> > >
> > > We use a disk IO in a database with concurrent readers on a
> > shared file. IO, of course, can be interrupted.  And when it
> > is interrupted the backing channel is closed, so we have to
> > jump through hoops to ensure that the channel is reopened for
> > those threads which see the AsynchronousCloseException (or
> > ClosedChannelException), while ensuring that the thread which
> > sees the ClosedByInterruptException terminates its activity.
> > >
> > > Normally I can spot the source of an interrupt by looking
> > at the context in which the interrupt occurs and work
> > backward through the logic of the application to figure out
> > why the Thread might have been interrupted.  However, this is
> > difficult when our code is embedded into other applications
> > when we have little to know idea what else might be going on.
> >  In particular, I am looking at an issue now where I think
> > that the interrupt might be coming from some other component.
> >  For example, by holding onto a Thread reference beyond the
> > life of a worker task and then interrupting that thread.
> > >
> > > Is it possible to somehow instrument the JVM to track and
> > report the caller of Thread.interrupt()?  Are there profilers
> > which can do this?  Can this be done somehow by registering a
> > SecurityManager to log stack fames in Thread.checkAccess()?
> > >
> > > Thanks in advance,
> > > Bryan
> >
> > You can do it by yourself, it's not hard :)
> >
> >  From the directory that contains all JDK files, find the
> > src.zip, which is roughly a zip of the source of the public
> > classes of the JDK.
> > Extract java/lang/Thread.java
> > Now, as you can see Thread::interrupt() is written in Java,
> > so you can add all the instructions you want :) To get the
> > caller class, use sun.reflect.reflection.getCallerClass(2)
> > (see [1]),
> > because you are in a trusted class, you can call it.
> > Compile your class and prepend it in front of the
> > bootclasspath, so your classwill be loaded instead of the one
> > from rt.jar (see java -X and java -Xbootclasspath/p )
> >
> > cheers,
> > R?mi
> >
> > [1]
> > http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/tip/src/share/cl
> > asses/sun/reflect/Reflection.java
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>
> ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> End of Concurrency-interest Digest, Vol 86, Issue 18
> ****************************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120329/751d832a/attachment.html>

From djg at cs.washington.edu  Fri Mar 30 01:41:20 2012
From: djg at cs.washington.edu (Dan Grossman)
Date: Thu, 29 Mar 2012 22:41:20 -0700
Subject: [concurrency-interest] help show speed-up on a trivial but
 manual array-map operation?
In-Reply-To: <C8918A1C-1AD6-4AEA-9789-53A6A9ED0576@stefan-marr.de>
References: <CADruQ+gEeruRti_PgoBqgzcFbGzb-NJMHP5QDLrYoiDoPzue5Q@mail.gmail.com>
	<4F5A0166.9080400@cs.oswego.edu> <4F5A0396.6070601@cs.oswego.edu>
	<88AE6996-195A-483D-9E7A-3CB2971A0968@cs.pomona.edu>
	<CADruQ+jemh19Mn-jAriV_8Y4q2OWvMyD_w6QWoJVs0CJmfOs2w@mail.gmail.com>
	<CADruQ+gazmYGsR-B_ARnzk9-3QvBkLQF4-QNB7W+Yu_Ok=q2Hw@mail.gmail.com>
	<C8918A1C-1AD6-4AEA-9789-53A6A9ED0576@stefan-marr.de>
Message-ID: <CADruQ+jNGs7yW5LTV6V_C0ebCp3=Aj-hQTP6YnrELcs-8XzYqA@mail.gmail.com>

Thanks, Stefan.  This is good advice and when I first wrote the page I
thought I would discuss issues like this, but I ended up not getting
into anything related to how to do the timing.  I'll keep it in mind
if I get questions about how to do timing.

--Dan

On Mon, Mar 26, 2012 at 6:58 AM, Stefan Marr
<concurrency-interest at stefan-marr.de> wrote:
> Hi Dan:
>
> On 25 Mar 2012, at 22:48, Dan Grossman wrote:
>
>> Just to close the loop on this thread, today I edited
>>
>> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html
>>
>> which is part of the materials at
>>
>> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/
>>
>> I added a new section "timing issues" that lists various reasons why
>> you might see a /simple/ fork-join computation run slower than a
>> corresponding sequential implementation. ?This distills the
>> suggestions on this thread and other well-known ideas. ?Additional
>> suggestions and clarifications are welcome.
>
> An additional suggestion might be to use some 'established' benchmarking framework like http://code.google.com/p/caliper/
> They also list common pitfalls for benchmarking on the JVMs.
>
> Best regards
> Stefan
>
>
> --
> Stefan Marr
> Software Languages Lab
> Vrije Universiteit Brussel
> Pleinlaan 2 / B-1050 Brussels / Belgium
> http://soft.vub.ac.be/~smarr
> Phone: +32 2 629 2974
> Fax: ? +32 2 629 3525
>


