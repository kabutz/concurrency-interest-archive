From mthornton at optrak.co.uk  Wed Dec  5 06:23:41 2007
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Wed, 05 Dec 2007 11:23:41 +0000
Subject: [concurrency-interest] Fork/Join framework with acyclic task graph
Message-ID: <47568A3D.3010600@optrak.co.uk>

Some of my tasks have common subtasks. In these cases I would like tasks 
to execute subtasks which have not already been submitted, but merely 
wait on those which have already been submitted by other tasks. For 
example task A has sub tasks C, D, while task B has subtasks D,E.
Does the fork/join framework permit this type of use?

Mark Thornton


From dl at cs.oswego.edu  Wed Dec  5 07:37:23 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 5 Dec 2007 07:37:23 -0500 (EST)
Subject: [concurrency-interest] Fork/Join framework with acyclic task
 graph
In-Reply-To: <47568A3D.3010600@optrak.co.uk>
References: <47568A3D.3010600@optrak.co.uk>
Message-ID: <2109.64.61.43.116.1196858243.squirrel@cs.oswego.edu>

> Some of my tasks have common subtasks. In these cases I would like tasks
> to execute subtasks which have not already been submitted, but merely
> wait on those which have already been submitted by other tasks. For
> example task A has sub tasks C, D, while task B has subtasks D,E.
> Does the fork/join framework permit this type of use?
>

If you mean that A does: C.join(); D.join(); and
similarly for B, then, sure, this is fine.
So long as the waits are acyclic, the underlying
helping-join mechanics suffice. Non-block-structured
cases might (or might not) use more internal
stack/work-queue space
than other cases, but this rarely an issue.

-Doug



From mthornton at optrak.co.uk  Wed Dec  5 10:08:43 2007
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Wed, 05 Dec 2007 15:08:43 +0000
Subject: [concurrency-interest] Fork/Join framework with acyclic task
 graph
In-Reply-To: <2109.64.61.43.116.1196858243.squirrel@cs.oswego.edu>
References: <47568A3D.3010600@optrak.co.uk>
	<2109.64.61.43.116.1196858243.squirrel@cs.oswego.edu>
Message-ID: <4756BEFB.3070901@optrak.co.uk>

Doug Lea wrote:
>> Some of my tasks have common subtasks. In these cases I would like tasks
>> to execute subtasks which have not already been submitted, but merely
>> wait on those which have already been submitted by other tasks. For
>> example task A has sub tasks C, D, while task B has subtasks D,E.
>> Does the fork/join framework permit this type of use?
>>
>>     
>
> If you mean that A does: C.join(); D.join(); and
> similarly for B, then, sure, this is fine.
> So long as the waits are acyclic, the underlying
> helping-join mechanics suffice. Non-block-structured
> cases might (or might not) use more internal
> stack/work-queue space
> than other cases, but this rarely an issue.
>
> -Doug
>
>
>   
Thanks, that is exactly what I was hoping. Most of the jobs will be 
block structured, so a small amount of overhead in the other cases is 
not an issue.

Mark Thornton


From alarmnummer at gmail.com  Wed Dec  5 12:59:10 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 5 Dec 2007 18:59:10 +0100
Subject: [concurrency-interest] question the about the JMM
Message-ID: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>

I'm having a discussion with a a very smart colleague about some
example (from iBatis) that in my opinion has a visibility problem and
in his opinion doesn't.

This is the example:

 private boolean flag = true;

 public void something() {
   if (flag) {
     synchronized (this) { // synchronized block
       // do something
     }
   } else {
     // do something else with very little impact, NOT inside a
 synchronized block
   }
 }

public void setFlag(boolean newFlag) {
   this.flag = newFlag;
   synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
     // unimportant
   }
 }

With my current understanding of the JMM I would expect that the flag
variable has a visibility problem. Because there is no happens before
relation between the write in the setFlag method, and the read in the
something method, a thread that runs the something method doesn't need
to see the written value by a different thread ever.

The problem could be solved by using the the monitor lock rule (using
the lock of the object itself) or the volatile read/write rule. So the
simplest thing to remove the visibility problem is to make the flag
volatile.

The question is, am I correct? Does this example contain a visibility problem?

To make things more complicated, we have asked Brian Goetz and
according to him my colleague is right.

this is the mail conversion between them:

rom: Brian Goetz [mailto:brian at briangoetz.com]
Sent: Wed 12/5/2007 16:41
To: Erwin Bolwidt
Subject: Re: JMM question (wrt to a statement you made at JavaPolis)

Your colleague is wrong.

Both of the statements you refer to are correct.  The key is that only
synchronization actions (reads and writes of volatiles, and acquire and
release of locks) need to be totally ordered.  So if thread A release a
lock "before" thread B acquires one, that doesn't mean anything, because
the threads don't share your concept of "before".

The primary mistake people make in approaching concurrency is to falsely
assume "sequential consistency", meaning that "everything happens in a
fixed, global order".  In reality, it is more like the thought
experiments you did in special relativity, where ordering is relative to
the observer.  This happens in modern CPUs too, in the absence of
adequate synchronization.  The only total ordering you can rely on is
synchronization actions.  This is why if we synchronize on a common
lock, things work properly -- because it is sensible to speak of "when
you release the lock and I subsequently acquire it."  When we acquire
different locks, there is no way to guarantee "subsequent".  (And when
we don't acquire locks at all, it isn't even meaningful to talk about
"subsequent.")

Any logic that relies on unsynchronized reads of shared references to
mutable objects is committing the same error as double-checked locking,
with the same possible consequences.

The rules for the new memory model (happens-before) are actually fairly
simple to reason through.  Its the "I (think I) know what's going on in
the machine" part that leads people into trouble.  Such reasoning
invariably amounts to "I don't believe these are really the rules."

Erwin Bolwidt wrote:
> Hi Brian,
>
> A collegue and are having a discussion about one aspect of the new Java
> Memory Model.
> After one of your presentations on the JMM I asked you about how
> specific volatile or synchronized is (at JavaPolis I believe but it
> could have been JavaOne)
>
> Specifically, I wondered if synchronized on different monitors or access
> to different volatile variables were really independent: if these would
> only cause related data to be synchronized (written out to/invalidated
> L1/2 caches) with "global" memory or if they would effectively
> synchronize the whole thread state (caches) with "global" memory.
> You response as I remember it was that the latter was the case: the
> whole thread state (caches) would be synchronized. The former would be
> too complicated to implement.
>
> The JSR 133 FAQ (co-authored by you) at
> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html states:
>
>         *Important Note:* Note that it is important for both threads to
>         synchronize on the same monitor in order to set up the
>         happens-before relationship properly. It is not the case that
>         everything visible to thread A when it synchronizes on object X
>         becomes visible to thread B after it synchronizes on object Y.
>         The release and acquire have to "match" (i.e., be performed on
>         the same monitor) to have the right semantics. Otherwise, the
>         code has a data race.
>
> Now I realize that possibly both statements are true, and that this
> boils down to a misunderstanding of terms like "happens-before
> relationship".
>
> I have an example and I'm not sure what would happen here. In the real
> case, "flag" was called "cacheEnabled" and the question was whether the
> cache could be turned off in a way that would be guaranteed to work.
> This looks a little like the double-check locking issue. My statement
> was that if "setFlag(false)" was called on one thread, the something()
> method on another thread would be guaranteed to see this
> _the_second_time_ the method was entered, because the synchronization
> would invalidate the CPU caches on the thread that runs something(), and
> likewise the synchronized on a different monitor in setFlag would write
> out the change to the field "flag" to global memory.
>
> My collegue disagrees. Can you tell who is right?
>
> Thanks a lot if you can spend some time on this.
>
> Regards,
>   Erwin Bolwidt
>
> -------------- Example code --------------
>
> private boolean flag = true;
>
> public void something() {
>   if (flag) {
>     synchronized (this) { // synchronized block
>       // do something
>     }
>   } else {
>     // do something else with very little impact, NOT inside a
> synchronized block
>   }
> }
>
> public void setFlag(boolean newFlag) {
>   this.flag = newFlag;
>   synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
>     // unimportant
>   }
> }
>
>
> --
> Erwin Bolwidt
> business consultant
>

From joe.bowbeer at gmail.com  Wed Dec  5 15:11:35 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 5 Dec 2007 12:11:35 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>
References: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>
Message-ID: <31f2a7bd0712051211o1230d0c0id0348bbc19556c0b@mail.gmail.com>

as i've already replied offlist, if both methods are called by
separate threads without additional sync than what is shown,
visibility of the flag value is not guaranteed. i recommend volatile.

On 12/5/07, Peter Veentjer <alarmnummer at gmail.com> wrote:
> I'm having a discussion with a a very smart colleague about some
> example (from iBatis) that in my opinion has a visibility problem and
> in his opinion doesn't.
>
> This is the example:
>
>  private boolean flag = true;
>
>  public void something() {
>    if (flag) {
>      synchronized (this) { // synchronized block
>        // do something
>      }
>    } else {
>      // do something else with very little impact, NOT inside a
>  synchronized block
>    }
>  }
>
> public void setFlag(boolean newFlag) {
>    this.flag = newFlag;
>    synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
>      // unimportant
>    }
>  }
>
> With my current understanding of the JMM I would expect that the flag
> variable has a visibility problem. Because there is no happens before
> relation between the write in the setFlag method, and the read in the
> something method, a thread that runs the something method doesn't need
> to see the written value by a different thread ever.
>
> The problem could be solved by using the the monitor lock rule (using
> the lock of the object itself) or the volatile read/write rule. So the
> simplest thing to remove the visibility problem is to make the flag
> volatile.
>
> The question is, am I correct? Does this example contain a visibility
> problem?
>
> To make things more complicated, we have asked Brian Goetz and
> according to him my colleague is right.
>
> this is the mail conversion between them:
>
> rom: Brian Goetz [mailto:brian at briangoetz.com]
> Sent: Wed 12/5/2007 16:41
> To: Erwin Bolwidt
> Subject: Re: JMM question (wrt to a statement you made at JavaPolis)
>
> Your colleague is wrong.
>
> Both of the statements you refer to are correct.  The key is that only
> synchronization actions (reads and writes of volatiles, and acquire and
> release of locks) need to be totally ordered.  So if thread A release a
> lock "before" thread B acquires one, that doesn't mean anything, because
> the threads don't share your concept of "before".
>
> The primary mistake people make in approaching concurrency is to falsely
> assume "sequential consistency", meaning that "everything happens in a
> fixed, global order".  In reality, it is more like the thought
> experiments you did in special relativity, where ordering is relative to
> the observer.  This happens in modern CPUs too, in the absence of
> adequate synchronization.  The only total ordering you can rely on is
> synchronization actions.  This is why if we synchronize on a common
> lock, things work properly -- because it is sensible to speak of "when
> you release the lock and I subsequently acquire it."  When we acquire
> different locks, there is no way to guarantee "subsequent".  (And when
> we don't acquire locks at all, it isn't even meaningful to talk about
> "subsequent.")
>
> Any logic that relies on unsynchronized reads of shared references to
> mutable objects is committing the same error as double-checked locking,
> with the same possible consequences.
>
> The rules for the new memory model (happens-before) are actually fairly
> simple to reason through.  Its the "I (think I) know what's going on in
> the machine" part that leads people into trouble.  Such reasoning
> invariably amounts to "I don't believe these are really the rules."
>
> Erwin Bolwidt wrote:
> > Hi Brian,
> >
> > A collegue and are having a discussion about one aspect of the new Java
> > Memory Model.
> > After one of your presentations on the JMM I asked you about how
> > specific volatile or synchronized is (at JavaPolis I believe but it
> > could have been JavaOne)
> >
> > Specifically, I wondered if synchronized on different monitors or access
> > to different volatile variables were really independent: if these would
> > only cause related data to be synchronized (written out to/invalidated
> > L1/2 caches) with "global" memory or if they would effectively
> > synchronize the whole thread state (caches) with "global" memory.
> > You response as I remember it was that the latter was the case: the
> > whole thread state (caches) would be synchronized. The former would be
> > too complicated to implement.
> >
> > The JSR 133 FAQ (co-authored by you) at
> > http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html states:
> >
> >         *Important Note:* Note that it is important for both threads to
> >         synchronize on the same monitor in order to set up the
> >         happens-before relationship properly. It is not the case that
> >         everything visible to thread A when it synchronizes on object X
> >         becomes visible to thread B after it synchronizes on object Y.
> >         The release and acquire have to "match" (i.e., be performed on
> >         the same monitor) to have the right semantics. Otherwise, the
> >         code has a data race.
> >
> > Now I realize that possibly both statements are true, and that this
> > boils down to a misunderstanding of terms like "happens-before
> > relationship".
> >
> > I have an example and I'm not sure what would happen here. In the real
> > case, "flag" was called "cacheEnabled" and the question was whether the
> > cache could be turned off in a way that would be guaranteed to work.
> > This looks a little like the double-check locking issue. My statement
> > was that if "setFlag(false)" was called on one thread, the something()
> > method on another thread would be guaranteed to see this
> > _the_second_time_ the method was entered, because the synchronization
> > would invalidate the CPU caches on the thread that runs something(), and
> > likewise the synchronized on a different monitor in setFlag would write
> > out the change to the field "flag" to global memory.
> >
> > My collegue disagrees. Can you tell who is right?
> >
> > Thanks a lot if you can spend some time on this.
> >
> > Regards,
> >   Erwin Bolwidt
> >
> > -------------- Example code --------------
> >
> > private boolean flag = true;
> >
> > public void something() {
> >   if (flag) {
> >     synchronized (this) { // synchronized block
> >       // do something
> >     }
> >   } else {
> >     // do something else with very little impact, NOT inside a
> > synchronized block
> >   }
> > }
> >
> > public void setFlag(boolean newFlag) {
> >   this.flag = newFlag;
> >   synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD
> something
> >     // unimportant
> >   }
> > }
> >
> >
> > --
> > Erwin Bolwidt
> > business consultant
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From alarmnummer at gmail.com  Wed Dec  5 15:26:10 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 5 Dec 2007 21:26:10 +0100
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <31f2a7bd0712051211o1230d0c0id0348bbc19556c0b@mail.gmail.com>
References: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>
	<31f2a7bd0712051211o1230d0c0id0348bbc19556c0b@mail.gmail.com>
Message-ID: <1466c1d60712051226r573684a3gdedde50acde1e26d@mail.gmail.com>

Hi Joe,

I'm grateful for your answer.

The reason why I'm reposting the example on the mailinglist is that my
colleague didn't believe it after I told him your answer (and gave him
all my arguments). After a lot of mailbased discussion he decided to
write a mail to Brian Goetz and Brian told him that he was right. That
is why I decided to get a more detailed answer to convince him that
reasoning in 'caches and invalidation' is not the way to go with the
new JMM. Everything is defined in happens before relations and
according to that definition, I think I'm right (in the example there
is no happens before relation between the write and the read, so
visibility problem).

My impression is that the question he asked to Brian, is not the one
we discussed about (so there is some confusion). The question Brian is
answering is that when a thread releases a lock, or does a volatile
write, all changes within that thread are visible (and maybe even
changes that occurred after) within a different thread that does the
acquire lock or volatile read. This is something I also agree on;
without this property it would be impossible to do a safe hand off for
example.

ps:
This happens before relation only is true when the lock
release/acquire is done on the same monitor.. or on the same volatile
variable.

On Dec 5, 2007 9:11 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> as i've already replied offlist, if both methods are called by
> separate threads without additional sync than what is shown,
> visibility of the flag value is not guaranteed. i recommend volatile.
>
>
> On 12/5/07, Peter Veentjer <alarmnummer at gmail.com> wrote:
> > I'm having a discussion with a a very smart colleague about some
> > example (from iBatis) that in my opinion has a visibility problem and
> > in his opinion doesn't.
> >
> > This is the example:
> >
> >  private boolean flag = true;
> >
> >  public void something() {
> >    if (flag) {
> >      synchronized (this) { // synchronized block
> >        // do something
> >      }
> >    } else {
> >      // do something else with very little impact, NOT inside a
> >  synchronized block
> >    }
> >  }
> >
> > public void setFlag(boolean newFlag) {
> >    this.flag = newFlag;
> >    synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
> >      // unimportant
> >    }
> >  }
> >
> > With my current understanding of the JMM I would expect that the flag
> > variable has a visibility problem. Because there is no happens before
> > relation between the write in the setFlag method, and the read in the
> > something method, a thread that runs the something method doesn't need
> > to see the written value by a different thread ever.
> >
> > The problem could be solved by using the the monitor lock rule (using
> > the lock of the object itself) or the volatile read/write rule. So the
> > simplest thing to remove the visibility problem is to make the flag
> > volatile.
> >
> > The question is, am I correct? Does this example contain a visibility
> > problem?
> >
> > To make things more complicated, we have asked Brian Goetz and
> > according to him my colleague is right.
> >
> > this is the mail conversion between them:
> >
> > rom: Brian Goetz [mailto:brian at briangoetz.com]
> > Sent: Wed 12/5/2007 16:41
> > To: Erwin Bolwidt
> > Subject: Re: JMM question (wrt to a statement you made at JavaPolis)
> >
> > Your colleague is wrong.
> >
> > Both of the statements you refer to are correct.  The key is that only
> > synchronization actions (reads and writes of volatiles, and acquire and
> > release of locks) need to be totally ordered.  So if thread A release a
> > lock "before" thread B acquires one, that doesn't mean anything, because
> > the threads don't share your concept of "before".
> >
> > The primary mistake people make in approaching concurrency is to falsely
> > assume "sequential consistency", meaning that "everything happens in a
> > fixed, global order".  In reality, it is more like the thought
> > experiments you did in special relativity, where ordering is relative to
> > the observer.  This happens in modern CPUs too, in the absence of
> > adequate synchronization.  The only total ordering you can rely on is
> > synchronization actions.  This is why if we synchronize on a common
> > lock, things work properly -- because it is sensible to speak of "when
> > you release the lock and I subsequently acquire it."  When we acquire
> > different locks, there is no way to guarantee "subsequent".  (And when
> > we don't acquire locks at all, it isn't even meaningful to talk about
> > "subsequent.")
> >
> > Any logic that relies on unsynchronized reads of shared references to
> > mutable objects is committing the same error as double-checked locking,
> > with the same possible consequences.
> >
> > The rules for the new memory model (happens-before) are actually fairly
> > simple to reason through.  Its the "I (think I) know what's going on in
> > the machine" part that leads people into trouble.  Such reasoning
> > invariably amounts to "I don't believe these are really the rules."
> >
> > Erwin Bolwidt wrote:
> > > Hi Brian,
> > >
> > > A collegue and are having a discussion about one aspect of the new Java
> > > Memory Model.
> > > After one of your presentations on the JMM I asked you about how
> > > specific volatile or synchronized is (at JavaPolis I believe but it
> > > could have been JavaOne)
> > >
> > > Specifically, I wondered if synchronized on different monitors or access
> > > to different volatile variables were really independent: if these would
> > > only cause related data to be synchronized (written out to/invalidated
> > > L1/2 caches) with "global" memory or if they would effectively
> > > synchronize the whole thread state (caches) with "global" memory.
> > > You response as I remember it was that the latter was the case: the
> > > whole thread state (caches) would be synchronized. The former would be
> > > too complicated to implement.
> > >
> > > The JSR 133 FAQ (co-authored by you) at
> > > http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html states:
> > >
> > >         *Important Note:* Note that it is important for both threads to
> > >         synchronize on the same monitor in order to set up the
> > >         happens-before relationship properly. It is not the case that
> > >         everything visible to thread A when it synchronizes on object X
> > >         becomes visible to thread B after it synchronizes on object Y.
> > >         The release and acquire have to "match" (i.e., be performed on
> > >         the same monitor) to have the right semantics. Otherwise, the
> > >         code has a data race.
> > >
> > > Now I realize that possibly both statements are true, and that this
> > > boils down to a misunderstanding of terms like "happens-before
> > > relationship".
> > >
> > > I have an example and I'm not sure what would happen here. In the real
> > > case, "flag" was called "cacheEnabled" and the question was whether the
> > > cache could be turned off in a way that would be guaranteed to work.
> > > This looks a little like the double-check locking issue. My statement
> > > was that if "setFlag(false)" was called on one thread, the something()
> > > method on another thread would be guaranteed to see this
> > > _the_second_time_ the method was entered, because the synchronization
> > > would invalidate the CPU caches on the thread that runs something(), and
> > > likewise the synchronized on a different monitor in setFlag would write
> > > out the change to the field "flag" to global memory.
> > >
> > > My collegue disagrees. Can you tell who is right?
> > >
> > > Thanks a lot if you can spend some time on this.
> > >
> > > Regards,
> > >   Erwin Bolwidt
> > >
> > > -------------- Example code --------------
> > >
> > > private boolean flag = true;
> > >
> > > public void something() {
> > >   if (flag) {
> > >     synchronized (this) { // synchronized block
> > >       // do something
> > >     }
> > >   } else {
> > >     // do something else with very little impact, NOT inside a
> > > synchronized block
> > >   }
> > > }
> > >
> > > public void setFlag(boolean newFlag) {
> > >   this.flag = newFlag;
> > >   synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD
> > something
> > >     // unimportant
> > >   }
> > > }
> > >
> > >
> > > --
> > > Erwin Bolwidt
> > > business consultant
> > >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>

From joe.bowbeer at gmail.com  Wed Dec  5 16:06:36 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 5 Dec 2007 13:06:36 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1466c1d60712051226r573684a3gdedde50acde1e26d@mail.gmail.com>
References: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>
	<31f2a7bd0712051211o1230d0c0id0348bbc19556c0b@mail.gmail.com>
	<1466c1d60712051226r573684a3gdedde50acde1e26d@mail.gmail.com>
Message-ID: <31f2a7bd0712051306g358a900ck399a48607bce301c@mail.gmail.com>

On Dec 5, 2007 12:26 PM, Peter Veentjer <alarmnummer at gmail.com> wrote:

> The reason why I'm reposting the example on the mailinglist is that my
> colleague didn't believe it after I told him your answer (and gave him
> all my arguments).

No problem. I reposted my answer in order to make my position known.

At this point, perhaps a small wager is in order?

--
Joe Bowbeer

From larryr at saturn.sdsu.edu  Wed Dec  5 16:37:38 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Wed, 05 Dec 2007 13:37:38 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1466c1d60712051226r573684a3gdedde50acde1e26d@mail.gmail.com>
Message-ID: <1196890658.646740.6557.nullmailer@home35>


> reasoning in 'caches and invalidation' is not the way to
> go with the new JMM.

I think it should be ok to reason that way, as long as it is
done in a way which is consistent with the actual semantics,
which I think is viable.  It seems to me kind of like saying
a lock is "protecting a critical section of code" rather
than "protecting some objects".  If I say thinking the
latter is the way to go, maybe it would not be unreasonable
for someone to say to me "but it is the critical sections of
code that access the objects".

If somebody was to think like "whenever I release a lock,
the caches get invalidated, and by the time another thread
has acquired a lock after that, the caches will have been
updated", where "after" is defined in terms of real world
time, it seems to me like something close to that could
be part of a valid model for somebody who wants to think
in terms of one single real world time, as long as they
are careful about which caches and locks, and which object
values are in the caches.

If someone thinks whenever a thread acquires/releases
a lock, it will see the latest changes any other thread
made to any value prior to the most recent acquire/release
of a lock in that thread, it seems like they may just be
misinformed, rather than using a fundamentally poor model.


Larry


From brian at briangoetz.com  Wed Dec  5 16:56:55 2007
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 05 Dec 2007 16:56:55 -0500
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>
References: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>
Message-ID: <47571EA7.9040505@briangoetz.com>

> I'm having a discussion with a a very smart colleague about some
> example (from iBatis) that in my opinion has a visibility problem and
> in his opinion doesn't.

I reviewed your colleague's e-mail again.  I had mistakenly switched the 
positions: I thought _he_ was saying "there is a visibility problem" and 
his colleague (you, apparently) that there was not.  But my intention 
was to say "this code is broken", and the comments in the message should 
make that clear.

Unless something() is called from the same thread as setFlag(), it is 
not guaranteed to _ever_ see flag be true unless it acquires the lock 
acquired by the writing thread.

Oh, and by the way, in:

 >    synchronized (new Object()) {
 >      // unimportant
 >    }

the synchronization has no effect in any case.  The compiler can prove 
that no other thread could synchronize on the same object, and is thus 
permitted to eliminate the sync entirely.

On a tangentially related note, it is in very poor taste to forward a 
private message to a public list without the author's permission; doubly 
so because it was sent to someone else, not you.


> 
> This is the example:
> 
>  private boolean flag = true;
> 
>  public void something() {
>    if (flag) {
>      synchronized (this) { // synchronized block
>        // do something
>      }
>    } else {
>      // do something else with very little impact, NOT inside a
>  synchronized block
>    }
>  }
> 
> public void setFlag(boolean newFlag) {
>    this.flag = newFlag;
>    synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
>      // unimportant
>    }
>  }
> 
> With my current understanding of the JMM I would expect that the flag
> variable has a visibility problem. Because there is no happens before
> relation between the write in the setFlag method, and the read in the
> something method, a thread that runs the something method doesn't need
> to see the written value by a different thread ever.
> 
> The problem could be solved by using the the monitor lock rule (using
> the lock of the object itself) or the volatile read/write rule. So the
> simplest thing to remove the visibility problem is to make the flag
> volatile.
> 
> The question is, am I correct? Does this example contain a visibility problem?
> 
> To make things more complicated, we have asked Brian Goetz and
> according to him my colleague is right.
> 
> this is the mail conversion between them:
> 
> rom: Brian Goetz [mailto:brian at briangoetz.com]
> Sent: Wed 12/5/2007 16:41
> To: Erwin Bolwidt
> Subject: Re: JMM question (wrt to a statement you made at JavaPolis)
> 
> Your colleague is wrong.
> 
> Both of the statements you refer to are correct.  The key is that only
> synchronization actions (reads and writes of volatiles, and acquire and
> release of locks) need to be totally ordered.  So if thread A release a
> lock "before" thread B acquires one, that doesn't mean anything, because
> the threads don't share your concept of "before".
> 
> The primary mistake people make in approaching concurrency is to falsely
> assume "sequential consistency", meaning that "everything happens in a
> fixed, global order".  In reality, it is more like the thought
> experiments you did in special relativity, where ordering is relative to
> the observer.  This happens in modern CPUs too, in the absence of
> adequate synchronization.  The only total ordering you can rely on is
> synchronization actions.  This is why if we synchronize on a common
> lock, things work properly -- because it is sensible to speak of "when
> you release the lock and I subsequently acquire it."  When we acquire
> different locks, there is no way to guarantee "subsequent".  (And when
> we don't acquire locks at all, it isn't even meaningful to talk about
> "subsequent.")
> 
> Any logic that relies on unsynchronized reads of shared references to
> mutable objects is committing the same error as double-checked locking,
> with the same possible consequences.
> 
> The rules for the new memory model (happens-before) are actually fairly
> simple to reason through.  Its the "I (think I) know what's going on in
> the machine" part that leads people into trouble.  Such reasoning
> invariably amounts to "I don't believe these are really the rules."
> 
> Erwin Bolwidt wrote:
>> Hi Brian,
>>
>> A collegue and are having a discussion about one aspect of the new Java
>> Memory Model.
>> After one of your presentations on the JMM I asked you about how
>> specific volatile or synchronized is (at JavaPolis I believe but it
>> could have been JavaOne)
>>
>> Specifically, I wondered if synchronized on different monitors or access
>> to different volatile variables were really independent: if these would
>> only cause related data to be synchronized (written out to/invalidated
>> L1/2 caches) with "global" memory or if they would effectively
>> synchronize the whole thread state (caches) with "global" memory.
>> You response as I remember it was that the latter was the case: the
>> whole thread state (caches) would be synchronized. The former would be
>> too complicated to implement.
>>
>> The JSR 133 FAQ (co-authored by you) at
>> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html states:
>>
>>         *Important Note:* Note that it is important for both threads to
>>         synchronize on the same monitor in order to set up the
>>         happens-before relationship properly. It is not the case that
>>         everything visible to thread A when it synchronizes on object X
>>         becomes visible to thread B after it synchronizes on object Y.
>>         The release and acquire have to "match" (i.e., be performed on
>>         the same monitor) to have the right semantics. Otherwise, the
>>         code has a data race.
>>
>> Now I realize that possibly both statements are true, and that this
>> boils down to a misunderstanding of terms like "happens-before
>> relationship".
>>
>> I have an example and I'm not sure what would happen here. In the real
>> case, "flag" was called "cacheEnabled" and the question was whether the
>> cache could be turned off in a way that would be guaranteed to work.
>> This looks a little like the double-check locking issue. My statement
>> was that if "setFlag(false)" was called on one thread, the something()
>> method on another thread would be guaranteed to see this
>> _the_second_time_ the method was entered, because the synchronization
>> would invalidate the CPU caches on the thread that runs something(), and
>> likewise the synchronized on a different monitor in setFlag would write
>> out the change to the field "flag" to global memory.
>>
>> My collegue disagrees. Can you tell who is right?
>>
>> Thanks a lot if you can spend some time on this.
>>
>> Regards,
>>   Erwin Bolwidt
>>
>> -------------- Example code --------------
>>
>> private boolean flag = true;
>>
>> public void something() {
>>   if (flag) {
>>     synchronized (this) { // synchronized block
>>       // do something
>>     }
>>   } else {
>>     // do something else with very little impact, NOT inside a
>> synchronized block
>>   }
>> }
>>
>> public void setFlag(boolean newFlag) {
>>   this.flag = newFlag;
>>   synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
>>     // unimportant
>>   }
>> }
>>
>>
>> --
>> Erwin Bolwidt
>> business consultant
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From brian at briangoetz.com  Wed Dec  5 17:01:32 2007
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 05 Dec 2007 17:01:32 -0500
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196890658.646740.6557.nullmailer@home35>
References: <1196890658.646740.6557.nullmailer@home35>
Message-ID: <47571FBC.9090704@briangoetz.com>

It is OK to reason that way -- as long as you know _exactly_ in which 
ways such reasoning is inconsistent with the actual semantics.  Which 
requires understanding all the details of the actual semantics...in 
which case it is probably easier (at least to me) to reason correctly 
about happens-before rather than caches and invalidation.

But your next paragraph illustrates exactly what's wrong with this 
mental model, and that is, there is almost always that hidden assumption 
of sequential consistency.  "Before" and "after" simply have no meaning 
under the JMM except as defined by happens-before.


Larry Riedel wrote:
>> reasoning in 'caches and invalidation' is not the way to
>> go with the new JMM.
> 
> I think it should be ok to reason that way, as long as it is
> done in a way which is consistent with the actual semantics,
> which I think is viable.  It seems to me kind of like saying
> a lock is "protecting a critical section of code" rather
> than "protecting some objects".  If I say thinking the
> latter is the way to go, maybe it would not be unreasonable
> for someone to say to me "but it is the critical sections of
> code that access the objects".
> 
> If somebody was to think like "whenever I release a lock,
> the caches get invalidated, and by the time another thread
> has acquired a lock after that, the caches will have been
> updated", where "after" is defined in terms of real world
> time, it seems to me like something close to that could
> be part of a valid model for somebody who wants to think
> in terms of one single real world time, as long as they
> are careful about which caches and locks, and which object
> values are in the caches.
> 
> If someone thinks whenever a thread acquires/releases
> a lock, it will see the latest changes any other thread
> made to any value prior to the most recent acquire/release
> of a lock in that thread, it seems like they may just be
> misinformed, rather than using a fundamentally poor model.
> 
> 
> Larry
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From alarmnummer at gmail.com  Wed Dec  5 17:05:02 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 5 Dec 2007 23:05:02 +0100
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <47571EA7.9040505@briangoetz.com>
References: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>
	<47571EA7.9040505@briangoetz.com>
Message-ID: <1466c1d60712051405x136c346bna29a25197b5f63c9@mail.gmail.com>

On Dec 5, 2007 10:56 PM, Brian Goetz <brian at briangoetz.com> wrote:
> > I'm having a discussion with a a very smart colleague about some
> > example (from iBatis) that in my opinion has a visibility problem and
> > in his opinion doesn't.
>
> I reviewed your colleague's e-mail again.  I had mistakenly switched the
> positions: I thought _he_ was saying "there is a visibility problem" and
> his colleague (you, apparently) that there was not.  But my intention
> was to say "this code is broken", and the comments in the message should
> make that clear.
>
> Unless something() is called from the same thread as setFlag(), it is
> not guaranteed to _ever_ see flag be true unless it acquires the lock
> acquired by the writing thread.

>
> Oh, and by the way, in:
>
>  >    synchronized (new Object()) {
>  >      // unimportant
>  >    }
>
> the synchronization has no effect in any case.  The compiler can prove
> that no other thread could synchronize on the same object, and is thus
> permitted to eliminate the sync entirely.

That is true and something I saw. But a new object is not created just
for synchronization sake, instead some argument that is passed is
used, but it was removed to simplify this example.

> On a tangentially related note, it is in very poor taste to forward a
> private message to a public list without the author's permission; doubly
> so because it was sent to someone else, not you.

I can understand that and I'm sorry.

>
>
>
> >
> > This is the example:
> >
> >  private boolean flag = true;
> >
> >  public void something() {
> >    if (flag) {
> >      synchronized (this) { // synchronized block
> >        // do something
> >      }
> >    } else {
> >      // do something else with very little impact, NOT inside a
> >  synchronized block
> >    }
> >  }
> >
> > public void setFlag(boolean newFlag) {
> >    this.flag = newFlag;
> >    synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
> >      // unimportant
> >    }
> >  }
> >
> > With my current understanding of the JMM I would expect that the flag
> > variable has a visibility problem. Because there is no happens before
> > relation between the write in the setFlag method, and the read in the
> > something method, a thread that runs the something method doesn't need
> > to see the written value by a different thread ever.
> >
> > The problem could be solved by using the the monitor lock rule (using
> > the lock of the object itself) or the volatile read/write rule. So the
> > simplest thing to remove the visibility problem is to make the flag
> > volatile.
> >
> > The question is, am I correct? Does this example contain a visibility problem?
> >
> > To make things more complicated, we have asked Brian Goetz and
> > according to him my colleague is right.
> >
> > this is the mail conversion between them:
> >
> > rom: Brian Goetz [mailto:brian at briangoetz.com]
> > Sent: Wed 12/5/2007 16:41
> > To: Erwin Bolwidt
> > Subject: Re: JMM question (wrt to a statement you made at JavaPolis)
> >
> > Your colleague is wrong.
> >
> > Both of the statements you refer to are correct.  The key is that only
> > synchronization actions (reads and writes of volatiles, and acquire and
> > release of locks) need to be totally ordered.  So if thread A release a
> > lock "before" thread B acquires one, that doesn't mean anything, because
> > the threads don't share your concept of "before".
> >
> > The primary mistake people make in approaching concurrency is to falsely
> > assume "sequential consistency", meaning that "everything happens in a
> > fixed, global order".  In reality, it is more like the thought
> > experiments you did in special relativity, where ordering is relative to
> > the observer.  This happens in modern CPUs too, in the absence of
> > adequate synchronization.  The only total ordering you can rely on is
> > synchronization actions.  This is why if we synchronize on a common
> > lock, things work properly -- because it is sensible to speak of "when
> > you release the lock and I subsequently acquire it."  When we acquire
> > different locks, there is no way to guarantee "subsequent".  (And when
> > we don't acquire locks at all, it isn't even meaningful to talk about
> > "subsequent.")
> >
> > Any logic that relies on unsynchronized reads of shared references to
> > mutable objects is committing the same error as double-checked locking,
> > with the same possible consequences.
> >
> > The rules for the new memory model (happens-before) are actually fairly
> > simple to reason through.  Its the "I (think I) know what's going on in
> > the machine" part that leads people into trouble.  Such reasoning
> > invariably amounts to "I don't believe these are really the rules."
> >
> > Erwin Bolwidt wrote:
> >> Hi Brian,
> >>
> >> A collegue and are having a discussion about one aspect of the new Java
> >> Memory Model.
> >> After one of your presentations on the JMM I asked you about how
> >> specific volatile or synchronized is (at JavaPolis I believe but it
> >> could have been JavaOne)
> >>
> >> Specifically, I wondered if synchronized on different monitors or access
> >> to different volatile variables were really independent: if these would
> >> only cause related data to be synchronized (written out to/invalidated
> >> L1/2 caches) with "global" memory or if they would effectively
> >> synchronize the whole thread state (caches) with "global" memory.
> >> You response as I remember it was that the latter was the case: the
> >> whole thread state (caches) would be synchronized. The former would be
> >> too complicated to implement.
> >>
> >> The JSR 133 FAQ (co-authored by you) at
> >> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html states:
> >>
> >>         *Important Note:* Note that it is important for both threads to
> >>         synchronize on the same monitor in order to set up the
> >>         happens-before relationship properly. It is not the case that
> >>         everything visible to thread A when it synchronizes on object X
> >>         becomes visible to thread B after it synchronizes on object Y.
> >>         The release and acquire have to "match" (i.e., be performed on
> >>         the same monitor) to have the right semantics. Otherwise, the
> >>         code has a data race.
> >>
> >> Now I realize that possibly both statements are true, and that this
> >> boils down to a misunderstanding of terms like "happens-before
> >> relationship".
> >>
> >> I have an example and I'm not sure what would happen here. In the real
> >> case, "flag" was called "cacheEnabled" and the question was whether the
> >> cache could be turned off in a way that would be guaranteed to work.
> >> This looks a little like the double-check locking issue. My statement
> >> was that if "setFlag(false)" was called on one thread, the something()
> >> method on another thread would be guaranteed to see this
> >> _the_second_time_ the method was entered, because the synchronization
> >> would invalidate the CPU caches on the thread that runs something(), and
> >> likewise the synchronized on a different monitor in setFlag would write
> >> out the change to the field "flag" to global memory.
> >>
> >> My collegue disagrees. Can you tell who is right?
> >>
> >> Thanks a lot if you can spend some time on this.
> >>
> >> Regards,
> >>   Erwin Bolwidt
> >>
> >> -------------- Example code --------------
> >>
> >> private boolean flag = true;
> >>
> >> public void something() {
> >>   if (flag) {
> >>     synchronized (this) { // synchronized block
> >>       // do something
> >>     }
> >>   } else {
> >>     // do something else with very little impact, NOT inside a
> >> synchronized block
> >>   }
> >> }
> >>
> >> public void setFlag(boolean newFlag) {
> >>   this.flag = newFlag;
> >>   synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
> >>     // unimportant
> >>   }
> >> }
> >>
> >>
> >> --
> >> Erwin Bolwidt
> >> business consultant
> >>
>
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From alarmnummer at gmail.com  Wed Dec  5 17:20:59 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 5 Dec 2007 23:20:59 +0100
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <47571EA7.9040505@briangoetz.com>
References: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>
	<47571EA7.9040505@briangoetz.com>
Message-ID: <1466c1d60712051420k3838eaccp37204e91721706e7@mail.gmail.com>

And making the variable volatile, or using it in a synchronized
context should solve all problems, right?

On Dec 5, 2007 10:56 PM, Brian Goetz <brian at briangoetz.com> wrote:
> > I'm having a discussion with a a very smart colleague about some
> > example (from iBatis) that in my opinion has a visibility problem and
> > in his opinion doesn't.
>
> I reviewed your colleague's e-mail again.  I had mistakenly switched the
> positions: I thought _he_ was saying "there is a visibility problem" and
> his colleague (you, apparently) that there was not.  But my intention
> was to say "this code is broken", and the comments in the message should
> make that clear.
>
> Unless something() is called from the same thread as setFlag(), it is
> not guaranteed to _ever_ see flag be true unless it acquires the lock
> acquired by the writing thread.
>
> Oh, and by the way, in:
>
>  >    synchronized (new Object()) {
>  >      // unimportant
>  >    }
>
> the synchronization has no effect in any case.  The compiler can prove
> that no other thread could synchronize on the same object, and is thus
> permitted to eliminate the sync entirely.
>
> On a tangentially related note, it is in very poor taste to forward a
> private message to a public list without the author's permission; doubly
> so because it was sent to someone else, not you.
>
>
>
> >
> > This is the example:
> >
> >  private boolean flag = true;
> >
> >  public void something() {
> >    if (flag) {
> >      synchronized (this) { // synchronized block
> >        // do something
> >      }
> >    } else {
> >      // do something else with very little impact, NOT inside a
> >  synchronized block
> >    }
> >  }
> >
> > public void setFlag(boolean newFlag) {
> >    this.flag = newFlag;
> >    synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
> >      // unimportant
> >    }
> >  }
> >
> > With my current understanding of the JMM I would expect that the flag
> > variable has a visibility problem. Because there is no happens before
> > relation between the write in the setFlag method, and the read in the
> > something method, a thread that runs the something method doesn't need
> > to see the written value by a different thread ever.
> >
> > The problem could be solved by using the the monitor lock rule (using
> > the lock of the object itself) or the volatile read/write rule. So the
> > simplest thing to remove the visibility problem is to make the flag
> > volatile.
> >
> > The question is, am I correct? Does this example contain a visibility problem?
> >
> > To make things more complicated, we have asked Brian Goetz and
> > according to him my colleague is right.
> >
> > this is the mail conversion between them:
> >
> > rom: Brian Goetz [mailto:brian at briangoetz.com]
> > Sent: Wed 12/5/2007 16:41
> > To: Erwin Bolwidt
> > Subject: Re: JMM question (wrt to a statement you made at JavaPolis)
> >
> > Your colleague is wrong.
> >
> > Both of the statements you refer to are correct.  The key is that only
> > synchronization actions (reads and writes of volatiles, and acquire and
> > release of locks) need to be totally ordered.  So if thread A release a
> > lock "before" thread B acquires one, that doesn't mean anything, because
> > the threads don't share your concept of "before".
> >
> > The primary mistake people make in approaching concurrency is to falsely
> > assume "sequential consistency", meaning that "everything happens in a
> > fixed, global order".  In reality, it is more like the thought
> > experiments you did in special relativity, where ordering is relative to
> > the observer.  This happens in modern CPUs too, in the absence of
> > adequate synchronization.  The only total ordering you can rely on is
> > synchronization actions.  This is why if we synchronize on a common
> > lock, things work properly -- because it is sensible to speak of "when
> > you release the lock and I subsequently acquire it."  When we acquire
> > different locks, there is no way to guarantee "subsequent".  (And when
> > we don't acquire locks at all, it isn't even meaningful to talk about
> > "subsequent.")
> >
> > Any logic that relies on unsynchronized reads of shared references to
> > mutable objects is committing the same error as double-checked locking,
> > with the same possible consequences.
> >
> > The rules for the new memory model (happens-before) are actually fairly
> > simple to reason through.  Its the "I (think I) know what's going on in
> > the machine" part that leads people into trouble.  Such reasoning
> > invariably amounts to "I don't believe these are really the rules."
> >
> > Erwin Bolwidt wrote:
> >> Hi Brian,
> >>
> >> A collegue and are having a discussion about one aspect of the new Java
> >> Memory Model.
> >> After one of your presentations on the JMM I asked you about how
> >> specific volatile or synchronized is (at JavaPolis I believe but it
> >> could have been JavaOne)
> >>
> >> Specifically, I wondered if synchronized on different monitors or access
> >> to different volatile variables were really independent: if these would
> >> only cause related data to be synchronized (written out to/invalidated
> >> L1/2 caches) with "global" memory or if they would effectively
> >> synchronize the whole thread state (caches) with "global" memory.
> >> You response as I remember it was that the latter was the case: the
> >> whole thread state (caches) would be synchronized. The former would be
> >> too complicated to implement.
> >>
> >> The JSR 133 FAQ (co-authored by you) at
> >> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html states:
> >>
> >>         *Important Note:* Note that it is important for both threads to
> >>         synchronize on the same monitor in order to set up the
> >>         happens-before relationship properly. It is not the case that
> >>         everything visible to thread A when it synchronizes on object X
> >>         becomes visible to thread B after it synchronizes on object Y.
> >>         The release and acquire have to "match" (i.e., be performed on
> >>         the same monitor) to have the right semantics. Otherwise, the
> >>         code has a data race.
> >>
> >> Now I realize that possibly both statements are true, and that this
> >> boils down to a misunderstanding of terms like "happens-before
> >> relationship".
> >>
> >> I have an example and I'm not sure what would happen here. In the real
> >> case, "flag" was called "cacheEnabled" and the question was whether the
> >> cache could be turned off in a way that would be guaranteed to work.
> >> This looks a little like the double-check locking issue. My statement
> >> was that if "setFlag(false)" was called on one thread, the something()
> >> method on another thread would be guaranteed to see this
> >> _the_second_time_ the method was entered, because the synchronization
> >> would invalidate the CPU caches on the thread that runs something(), and
> >> likewise the synchronized on a different monitor in setFlag would write
> >> out the change to the field "flag" to global memory.
> >>
> >> My collegue disagrees. Can you tell who is right?
> >>
> >> Thanks a lot if you can spend some time on this.
> >>
> >> Regards,
> >>   Erwin Bolwidt
> >>
> >> -------------- Example code --------------
> >>
> >> private boolean flag = true;
> >>
> >> public void something() {
> >>   if (flag) {
> >>     synchronized (this) { // synchronized block
> >>       // do something
> >>     }
> >>   } else {
> >>     // do something else with very little impact, NOT inside a
> >> synchronized block
> >>   }
> >> }
> >>
> >> public void setFlag(boolean newFlag) {
> >>   this.flag = newFlag;
> >>   synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
> >>     // unimportant
> >>   }
> >> }
> >>
> >>
> >> --
> >> Erwin Bolwidt
> >> business consultant
> >>
>
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From brian at briangoetz.com  Wed Dec  5 17:26:05 2007
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 05 Dec 2007 17:26:05 -0500
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1466c1d60712051420k3838eaccp37204e91721706e7@mail.gmail.com>
References: <1466c1d60712050959p53a41da4seefb0e31d17b7765@mail.gmail.com>	
	<47571EA7.9040505@briangoetz.com>
	<1466c1d60712051420k3838eaccp37204e91721706e7@mail.gmail.com>
Message-ID: <4757257D.5090004@briangoetz.com>

> And making the variable volatile, or using it in a synchronized
> context should solve all problems, right?

Not sure about _all_ problems, but...

Making the flag volatile will ensure that writes to flag in setFlag()
are visible in something() ASAP.

> On Dec 5, 2007 10:56 PM, Brian Goetz <brian at briangoetz.com> wrote:
>>> I'm having a discussion with a a very smart colleague about some
>>> example (from iBatis) that in my opinion has a visibility problem and
>>> in his opinion doesn't.
>> I reviewed your colleague's e-mail again.  I had mistakenly switched the
>> positions: I thought _he_ was saying "there is a visibility problem" and
>> his colleague (you, apparently) that there was not.  But my intention
>> was to say "this code is broken", and the comments in the message should
>> make that clear.
>>
>> Unless something() is called from the same thread as setFlag(), it is
>> not guaranteed to _ever_ see flag be true unless it acquires the lock
>> acquired by the writing thread.
>>
>> Oh, and by the way, in:
>>
>>  >    synchronized (new Object()) {
>>  >      // unimportant
>>  >    }
>>
>> the synchronization has no effect in any case.  The compiler can prove
>> that no other thread could synchronize on the same object, and is thus
>> permitted to eliminate the sync entirely.
>>
>> On a tangentially related note, it is in very poor taste to forward a
>> private message to a public list without the author's permission; doubly
>> so because it was sent to someone else, not you.
>>
>>
>>
>>> This is the example:
>>>
>>>  private boolean flag = true;
>>>
>>>  public void something() {
>>>    if (flag) {
>>>      synchronized (this) { // synchronized block
>>>        // do something
>>>      }
>>>    } else {
>>>      // do something else with very little impact, NOT inside a
>>>  synchronized block
>>>    }
>>>  }
>>>
>>> public void setFlag(boolean newFlag) {
>>>    this.flag = newFlag;
>>>    synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
>>>      // unimportant
>>>    }
>>>  }
>>>
>>> With my current understanding of the JMM I would expect that the flag
>>> variable has a visibility problem. Because there is no happens before
>>> relation between the write in the setFlag method, and the read in the
>>> something method, a thread that runs the something method doesn't need
>>> to see the written value by a different thread ever.
>>>
>>> The problem could be solved by using the the monitor lock rule (using
>>> the lock of the object itself) or the volatile read/write rule. So the
>>> simplest thing to remove the visibility problem is to make the flag
>>> volatile.
>>>
>>> The question is, am I correct? Does this example contain a visibility problem?
>>>
>>> To make things more complicated, we have asked Brian Goetz and
>>> according to him my colleague is right.
>>>
>>> this is the mail conversion between them:
>>>
>>> rom: Brian Goetz [mailto:brian at briangoetz.com]
>>> Sent: Wed 12/5/2007 16:41
>>> To: Erwin Bolwidt
>>> Subject: Re: JMM question (wrt to a statement you made at JavaPolis)
>>>
>>> Your colleague is wrong.
>>>
>>> Both of the statements you refer to are correct.  The key is that only
>>> synchronization actions (reads and writes of volatiles, and acquire and
>>> release of locks) need to be totally ordered.  So if thread A release a
>>> lock "before" thread B acquires one, that doesn't mean anything, because
>>> the threads don't share your concept of "before".
>>>
>>> The primary mistake people make in approaching concurrency is to falsely
>>> assume "sequential consistency", meaning that "everything happens in a
>>> fixed, global order".  In reality, it is more like the thought
>>> experiments you did in special relativity, where ordering is relative to
>>> the observer.  This happens in modern CPUs too, in the absence of
>>> adequate synchronization.  The only total ordering you can rely on is
>>> synchronization actions.  This is why if we synchronize on a common
>>> lock, things work properly -- because it is sensible to speak of "when
>>> you release the lock and I subsequently acquire it."  When we acquire
>>> different locks, there is no way to guarantee "subsequent".  (And when
>>> we don't acquire locks at all, it isn't even meaningful to talk about
>>> "subsequent.")
>>>
>>> Any logic that relies on unsynchronized reads of shared references to
>>> mutable objects is committing the same error as double-checked locking,
>>> with the same possible consequences.
>>>
>>> The rules for the new memory model (happens-before) are actually fairly
>>> simple to reason through.  Its the "I (think I) know what's going on in
>>> the machine" part that leads people into trouble.  Such reasoning
>>> invariably amounts to "I don't believe these are really the rules."
>>>
>>> Erwin Bolwidt wrote:
>>>> Hi Brian,
>>>>
>>>> A collegue and are having a discussion about one aspect of the new Java
>>>> Memory Model.
>>>> After one of your presentations on the JMM I asked you about how
>>>> specific volatile or synchronized is (at JavaPolis I believe but it
>>>> could have been JavaOne)
>>>>
>>>> Specifically, I wondered if synchronized on different monitors or access
>>>> to different volatile variables were really independent: if these would
>>>> only cause related data to be synchronized (written out to/invalidated
>>>> L1/2 caches) with "global" memory or if they would effectively
>>>> synchronize the whole thread state (caches) with "global" memory.
>>>> You response as I remember it was that the latter was the case: the
>>>> whole thread state (caches) would be synchronized. The former would be
>>>> too complicated to implement.
>>>>
>>>> The JSR 133 FAQ (co-authored by you) at
>>>> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html states:
>>>>
>>>>         *Important Note:* Note that it is important for both threads to
>>>>         synchronize on the same monitor in order to set up the
>>>>         happens-before relationship properly. It is not the case that
>>>>         everything visible to thread A when it synchronizes on object X
>>>>         becomes visible to thread B after it synchronizes on object Y.
>>>>         The release and acquire have to "match" (i.e., be performed on
>>>>         the same monitor) to have the right semantics. Otherwise, the
>>>>         code has a data race.
>>>>
>>>> Now I realize that possibly both statements are true, and that this
>>>> boils down to a misunderstanding of terms like "happens-before
>>>> relationship".
>>>>
>>>> I have an example and I'm not sure what would happen here. In the real
>>>> case, "flag" was called "cacheEnabled" and the question was whether the
>>>> cache could be turned off in a way that would be guaranteed to work.
>>>> This looks a little like the double-check locking issue. My statement
>>>> was that if "setFlag(false)" was called on one thread, the something()
>>>> method on another thread would be guaranteed to see this
>>>> _the_second_time_ the method was entered, because the synchronization
>>>> would invalidate the CPU caches on the thread that runs something(), and
>>>> likewise the synchronized on a different monitor in setFlag would write
>>>> out the change to the field "flag" to global memory.
>>>>
>>>> My collegue disagrees. Can you tell who is right?
>>>>
>>>> Thanks a lot if you can spend some time on this.
>>>>
>>>> Regards,
>>>>   Erwin Bolwidt
>>>>
>>>> -------------- Example code --------------
>>>>
>>>> private boolean flag = true;
>>>>
>>>> public void something() {
>>>>   if (flag) {
>>>>     synchronized (this) { // synchronized block
>>>>       // do something
>>>>     }
>>>>   } else {
>>>>     // do something else with very little impact, NOT inside a
>>>> synchronized block
>>>>   }
>>>> }
>>>>
>>>> public void setFlag(boolean newFlag) {
>>>>   this.flag = newFlag;
>>>>   synchronized (new Object()) { // ANOTHER MONITOR THAN IN METHOD something
>>>>     // unimportant
>>>>   }
>>>> }
>>>>
>>>>
>>>> --
>>>> Erwin Bolwidt
>>>> business consultant
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at altair.cs.oswego.edu
>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From alarmnummer at gmail.com  Wed Dec  5 17:31:21 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 5 Dec 2007 23:31:21 +0100
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196890658.646740.6557.nullmailer@home35>
References: <1466c1d60712051226r573684a3gdedde50acde1e26d@mail.gmail.com>
	<1196890658.646740.6557.nullmailer@home35>
Message-ID: <1466c1d60712051431g406ede78g64569052e5327421@mail.gmail.com>

Hi Larry.

one of the problems with the difference between reasoning in cache
invalidation and happens before relations, is that with the former the
following sounds very plausible:  a release of lock X (that causes a
cache invalidation) and an acquire of lock Y (that causes a cache
update) is a usable mechanism to make all changes from one thread
visible in another.

The new JMM clearly states that this isn't the case (only if they are
the same lock). So reasoning with cache invalidation could lead to
wrong assumptions. I also reasoned about caches in the beginning (and
in some cases it still slips my mind), but I was discouraged to do so.
After some time reasoning about happens before rules is a lot easier
than reasoning about cache invalidations.

On Dec 5, 2007 10:37 PM, Larry Riedel <larryr at saturn.sdsu.edu> wrote:
>
> > reasoning in 'caches and invalidation' is not the way to
> > go with the new JMM.
>
> I think it should be ok to reason that way, as long as it is
> done in a way which is consistent with the actual semantics,
> which I think is viable.  It seems to me kind of like saying
> a lock is "protecting a critical section of code" rather
> than "protecting some objects".  If I say thinking the
> latter is the way to go, maybe it would not be unreasonable
> for someone to say to me "but it is the critical sections of
> code that access the objects".
>
> If somebody was to think like "whenever I release a lock,
> the caches get invalidated, and by the time another thread
> has acquired a lock after that, the caches will have been
> updated", where "after" is defined in terms of real world
> time, it seems to me like something close to that could
> be part of a valid model for somebody who wants to think
> in terms of one single real world time, as long as they
> are careful about which caches and locks, and which object
> values are in the caches.
>
> If someone thinks whenever a thread acquires/releases
> a lock, it will see the latest changes any other thread
> made to any value prior to the most recent acquire/release
> of a lock in that thread, it seems like they may just be
> misinformed, rather than using a fundamentally poor model.
>
>
> Larry
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From larryr at saturn.sdsu.edu  Wed Dec  5 18:37:36 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Wed, 05 Dec 2007 15:37:36 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1466c1d60712051431g406ede78g64569052e5327421@mail.gmail.com>
Message-ID: <1196897856.871578.7701.nullmailer@home35>


> one of the problems with the difference between reasoning in
> cache invalidation and happens before relations, is that with
> the former the following sounds very plausible: a release of
> lock X (that causes a cache invalidation) and an acquire of
> lock Y (that causes a cache update) is a usable mechanism to
> make all changes from one thread visible in another.

I hope it is fair to presume any sane cache coherence
protocol maintains consistency at a level of granularity far
far smaller than the entire cache!  I would agree it would
be ludicrous to model the entire memory space of a thread
as one giant blob which gets invalidated every time a lock
is acquired/released!  The only changes I would expect to
be made visible in the above case are those made to values
in the memory blocks associated with the acquire/release of
both lock X and lock Y.  And in Java, if X is not Y, I do
not think any presumptions are supposed to be made about
which values those are, without additional information.


Larry


From larryr at saturn.sdsu.edu  Wed Dec  5 19:21:20 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Wed, 05 Dec 2007 16:21:20 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <47571FBC.9090704@briangoetz.com>
Message-ID: <1196900480.326336.8157.nullmailer@home35>


> [...] your next paragraph illustrates exactly what's wrong
> with this mental model, and that is, there is almost
> always that hidden assumption of sequential consistency.
> "Before" and "after" simply have no meaning under the JMM
> except as defined by happens-before.

I am inclined to expect any intuitive model based on
the JMM, with a local clock and "happens-before", will
be homomorphic to a graph for a model based on a global
clock and local caches (using a typical cache coherence
protocol), with meaningful and well-defined "before" and
"after".  But I would rather not try to prove it. (-:


Larry


From jmanson at cs.umd.edu  Thu Dec  6 02:18:24 2007
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Wed, 05 Dec 2007 23:18:24 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196900480.326336.8157.nullmailer@home35>
References: <1196900480.326336.8157.nullmailer@home35>
Message-ID: <4757A240.8070601@cs.umd.edu>

Larry Riedel wrote:
>> [...] your next paragraph illustrates exactly what's wrong
>> with this mental model, and that is, there is almost
>> always that hidden assumption of sequential consistency.
>> "Before" and "after" simply have no meaning under the JMM
>> except as defined by happens-before.
> 
> I am inclined to expect any intuitive model based on
> the JMM, with a local clock and "happens-before", will
> be homomorphic to a graph for a model based on a global
> clock and local caches (using a typical cache coherence
> protocol), with meaningful and well-defined "before" and
> "after".  But I would rather not try to prove it. (-:
> 

Larry,

This isn't just about cache coherence.  It's also about compiler 
optimization.  Consider the following example.  Initially, p == q, and 
they are some object with a field x.

Thread 1:

r1 = p.x;
r2 = q.x;
r3 = p.x;

Thread 2:

q.x = 1;

Now, the compiler can remove redundant reads as follows:

Thread 1:
r1 = r3 = p.x;
r2 = q.x;

Thread 2:
q.x = 1;

This means that r1 and r3 can have the value 0 and r2 can have the value 
1, even though they are all reading from the same memory location.  No 
sane cache coherence policy would allow this when reading from the same 
variable (although I may be misremembering some), but the JMM does.

					Jeremy

From alarmnummer at gmail.com  Thu Dec  6 03:46:03 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 6 Dec 2007 09:46:03 +0100
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196897856.871578.7701.nullmailer@home35>
References: <1466c1d60712051431g406ede78g64569052e5327421@mail.gmail.com>
	<1196897856.871578.7701.nullmailer@home35>
Message-ID: <1466c1d60712060046q367c8ecdibb0b39b35e5872c5@mail.gmail.com>

Hi Larry,

according to the JMM, the complete state of a thread when it releases
a lock X (or does a volatile write or ...) is visible in a different
thread when it acquires lock X (or does a volatile read or ...). The
changes made after the lock has been releases ( or. ) could even be
visibile when the lock is acquired ( or ...).

So the level of granulairy is big. This property is required to do a
save hand off: to pass a structure with visibility problems (an
ordinary POJO without any knowledge of concurrency) from one thread to
another. A BlockingQueue is a concurrency structure that provides a
save hand off and you could use it to pass an ordinary Person for
example.

On Dec 6, 2007 12:37 AM, Larry Riedel <larryr at saturn.sdsu.edu> wrote:
>
> > one of the problems with the difference between reasoning in
> > cache invalidation and happens before relations, is that with
> > the former the following sounds very plausible: a release of
> > lock X (that causes a cache invalidation) and an acquire of
> > lock Y (that causes a cache update) is a usable mechanism to
> > make all changes from one thread visible in another.
>
> I hope it is fair to presume any sane cache coherence
> protocol maintains consistency at a level of granularity far
> far smaller than the entire cache!  I would agree it would
> be ludicrous to model the entire memory space of a thread
> as one giant blob which gets invalidated every time a lock
> is acquired/released!  The only changes I would expect to
> be made visible in the above case are those made to values
> in the memory blocks associated with the acquire/release of
> both lock X and lock Y.  And in Java, if X is not Y, I do
> not think any presumptions are supposed to be made about
> which values those are, without additional information.
>
>
>
> Larry
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From larryr at saturn.sdsu.edu  Thu Dec  6 13:24:35 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Thu, 06 Dec 2007 10:24:35 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <4757A240.8070601@cs.umd.edu>
Message-ID: <1196965475.577850.4964.nullmailer@home35>


> This isn't just about cache coherence.  It's also about
> compiler optimization.  Consider the following example.
> Initially, p == q, and they are some object with a field x.
> Thread 1:
>     r1 = p.x;
>     r2 = q.x;
>     r3 = p.x;
> Thread 2:
>     q.x = 1;
> 
> This means that r1 and r3 can have the value 0 and r2 can
> have the value 1, even though they are all reading from the
> same memory location.  No sane cache coherence policy would
> allow this when reading from the same variable (although I
> may be misremembering some), but the JMM does.

I do not see that code using any constructs which bring
cache coherence /or/ happens-before into the picture,
unless x is "volatile", in which case I think ending up
with (r2==1) and (r3==0) should not be valid.  What
would be a good example where there is a happens-before
but still r1==r3==0 and r2==1?


Larry


From jmanson at cs.umd.edu  Thu Dec  6 15:50:40 2007
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Thu, 06 Dec 2007 12:50:40 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196965475.577850.4964.nullmailer@home35>
References: <1196965475.577850.4964.nullmailer@home35>
Message-ID: <475860A0.3010409@cs.umd.edu>

Larry Riedel wrote:
>> This isn't just about cache coherence.  It's also about
>> compiler optimization.  Consider the following example.
>> Initially, p == q, and they are some object with a field x.
>> Thread 1:
>>     r1 = p.x;
>>     r2 = q.x;
>>     r3 = p.x;
>> Thread 2:
>>     q.x = 1;
>>
>> This means that r1 and r3 can have the value 0 and r2 can
>> have the value 1, even though they are all reading from the
>> same memory location.  No sane cache coherence policy would
>> allow this when reading from the same variable (although I
>> may be misremembering some), but the JMM does.
> 
> I do not see that code using any constructs which bring
> cache coherence /or/ happens-before into the picture,
> unless x is "volatile", in which case I think ending up
> with (r2==1) and (r3==0) should not be valid.  What
> would be a good example where there is a happens-before
> but still r1==r3==0 and r2==1?

I think your definition of cache coherence and mine might be different. 
  To me, cache coherence is a protocol used by a processor to address 
the consistency of its caches.  It defines the behavior of reads and 
writes to the same memory locations.  This code reads and writes to 
memory locations, and therefore relies on a cache coherence protocol. 
Its behavior under the Java memory model is different from its behavior 
under any sane cache coherence protocol.

If you are looking for an example where happens-before edges differ from 
explicit memory barriers, that's a different question.  It's also a 
question that doesn't really have an answer, because you really can't 
map anything to a "memory barrier" in Java, because the notion doesn't 
exist.   An example would be that this:

synchronized (new Object()) {
   x = 1;
}

Does not have the same memory semantics as this:

mfence;  // or whatever you want to use
x = 1;
sfence;

Because synchronization on an Object that can't be accessed by multiple 
threads can be removed entirely.

					Jeremy

From larryr at saturn.sdsu.edu  Thu Dec  6 18:29:43 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Thu, 06 Dec 2007 15:29:43 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <475860A0.3010409@cs.umd.edu>
Message-ID: <1196983783.704266.8716.nullmailer@home35>


> To me, cache coherence is a protocol used by a processor
> to address the consistency of its caches.  It defines the
> behavior of reads and writes to the same memory locations.

That sounds like a pretty good description to me.  I
think I might just change "a processor" to "a set of
threads", and "memory locations" to "sets of objects".


> [p==q]
> Thread 1:
>     r1 = p.x;
>     r2 = q.x;
>     r3 = p.x;
> Thread 2:
>     q.x = 1;
> 
> This code reads and writes to memory locations, and
> therefore relies on a cache coherence protocol.  Its
> behavior under the Java memory model is different from
> its behavior under any sane cache coherence protocol.

The code is not utilizing any inter-thread relative
ordering constraint mechanisms of the protocol, so
I think the cache coherence protocol is agnostic
about whether Thread1 sees r1==r3==0 and r2==1.
If "x" is volatile though, I would expect the cache
coherence protocol to make sure r3==1 if r2==1.


> > What would be a good example where there is a
> > happens-before but still r1==r3==0 and r2==1?
> 
> If you are looking for an example where happens-before
> edges differ from explicit memory barriers,

An example where there is a happens-before but still
r1==r3==0 and r2==1, or something conceptually analogous.

> edges differ from explicit memory barriers, that's a
> different question.  It's also a question that doesn't
> really have an answer, because you really can't map
> anything to a "memory barrier" in Java, because the
> notion doesn't exist.

I did not say anything about "memory barrier", so I am
ok with the idea that the notion does not exist. (-:


> An example would be that this:
>    synchronized (new Object()) {
>       x = 1;
>    }
> Does not have the same memory semantics as this:
>    mfence;  // or whatever you want to use
>    x = 1;
>    sfence;
> 
> Because synchronization on an Object that can't be
> accessed by multiple threads can be removed entirely.

Guessing what role "mfence" and "sfence" are playing
there, I imagine I might want to use more like:

    Object o = new Object();
    mfence(o);
    x = 1;
    sfence(o);

Presuming no reference to "o" leaks out via [ms]fence,
I would not expect the cache coherence protocol to need
to impose any actual ordering constraints,


Larry


From dcholmes at optusnet.com.au  Thu Dec  6 18:58:51 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 7 Dec 2007 09:58:51 +1000
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196983783.704266.8716.nullmailer@home35>
Message-ID: <NFBBKALFDCPFIDBNKAPCGELMHJAA.dcholmes@optusnet.com.au>

Larry Riedel wrote:
> If "x" is volatile though, I would expect the cache
> coherence protocol to make sure r3==1 if r2==1.

The cache coherence protocol knows nothing about "volatile". If x is
volatile then it is up to the VM to issue the right instructions to ensure
visibility whatever the coherence protocol is. On many systems the VM
doesn't have to do anything special for this to happen.

The problem with using "cache coherence" as a logical model for the JMM is
two-fold. First, it is insufficient as it doesn't address reorderings that
the runtime compiler is allowed to do. Second it leads people into false
conclusions because they apply knowledge of real cache coherency protocols
to the model eg:

1. The JMM says if "x" isn't volatile then it need get flushed to main
memory and so won't be visible.
2. My system (like most) uses a write-through cache and so visibility is
never a problem.
3. Therefore on my systems I don't need to use "volatile".

As I've said many times on this list now, the old JMM model based on "main
memory" and "flushing" was appealing in its simplicity and intuitiveness,
but it was an oversimplification that misleads people. If you only think in
terms of happens-before then you can't go wrong.

David Holmes


From online at stolsvik.com  Thu Dec  6 20:13:04 2007
From: online at stolsvik.com (=?UTF-8?B?RW5kcmUgU3TDuGxzdmlr?=)
Date: Fri, 07 Dec 2007 02:13:04 +0100
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196900480.326336.8157.nullmailer@home35>
References: <1196900480.326336.8157.nullmailer@home35>
Message-ID: <47589E20.8050408@Stolsvik.com>

Larry Riedel wrote:
>> [...] your next paragraph illustrates exactly what's wrong
>> with this mental model, and that is, there is almost
>> always that hidden assumption of sequential consistency.
>> "Before" and "after" simply have no meaning under the JMM
>> except as defined by happens-before.
> 
> I am inclined to expect any intuitive model based on
> the JMM, with a local clock and "happens-before", will
> be homomorphic to a graph for a model based on a global
> clock and local caches (using a typical cache coherence
> protocol), with meaningful and well-defined "before" and
> "after".  But I would rather not try to prove it. (-:

I think the problem might be that one can end up with ideas that "*all* 
cache will be flushed to main memory upon *any* synch exit", and "*all* 
cache will be invalidated upon *any* synch entry", and that this means 
that as long as all threads in the VM are doing some synching or some 
access on volatiles, all shared variables will sooner or later be 
transferred between all threads.

This is not the case, however: threads must establish a happens-before 
edge between a *shared* object, or read/write on a *shared* volatile 
variable for the "transfer" to occur. But as long as this holds, then at 
least I agree that the cache logic mentioned above hold (this because 
the logic where "happens-before are transitive": everything written 
anywhere BEFORE the edge on the one thread will be visible AFTER the 
edge on the other thread).

The crucial point that the "happens-before" logic conveys is that the 
two threads in question must synch on some *common* object/volatile 
variable - the "transfer" won't happen on just any random synch.

Lets say we have four threads, where two and two of these threads are 
communicating properly with lots of synchs and volatile variables. 
However, the two groups also share a common (non-volatile) variable or 
object - but the two groups *never* synchs on that, or any other, common 
object, nor read-write on any shared volatile variable.

Then, the two groups might still see fully different values for the 
shared (non-volatile) variables and objects. The simple cache logic 
discussed above would at least in my head not allow this to happen, but 
the happens-before model will allow it just fine: there is never 
established any happens-before edge between the two groups of threads, 
and hence they aren't required to communicate/"transfer" the values of 
those shared variables.

Endre.

From larryr at saturn.sdsu.edu  Thu Dec  6 20:27:39 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Thu, 06 Dec 2007 17:27:39 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGELMHJAA.dcholmes@optusnet.com.au>
Message-ID: <1196990859.760082.9701.nullmailer@home35>


> > If "x" is volatile though, I would expect the cache
> > coherence protocol to make sure r3==1 if r2==1.
> 
> The cache coherence protocol knows nothing about "volatile".

Then I think "The cache coherence protocol" may not
be a good choice for use as a cache coherence protocol
in a model for the behavior of a Java program.


> [...] My system (like most) uses a write-through cache [...]

Then I think the cache in "My system" may not be
a good choice for use as the cache in a model for
the behavior of a Java program.


> [...] "cache coherence" doesn't address reorderings
> that the runtime compiler is allowed to do.

Is there a canonical or ubiquitous example of this?  I
am interested in the difficulties with using a model
which includes, for example, a simple object cache
coherence protocol based on a distributed directory.


Larry


From horowitz2 at llnl.gov  Thu Dec  6 21:25:09 2007
From: horowitz2 at llnl.gov (Ben Horowitz)
Date: Thu, 06 Dec 2007 18:25:09 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196990859.760082.9701.nullmailer@home35>
References: <1196990859.760082.9701.nullmailer@home35>
Message-ID: <4758AF05.3080501@llnl.gov>


>> [...] "cache coherence" doesn't address reorderings
>> that the runtime compiler is allowed to do.
>>     
>
> Is there a canonical or ubiquitous example of this?  I
> am interested in the difficulties with using a model
> which includes, for example, a simple object cache
> coherence protocol based on a distributed directory.
>   
I believe the following comes close to a canonical example.  This 
example is borrowed from Jeremy Manson.  I like it; please don't make me 
return it. :)

Initially, o.x == o.y == 0.
Thread 1:
    o.x = 1; // 1a
    j = o.y; // 1b
Thread 2:
    o.y = 1; // 2a
    i = o.x; // 2b

In the absence of happens-before constraints, it is possible that i == 0 
and j == 0 after both threads execute.

This would seem to be impossible: i == 0 seems to imply that 2b precedes 
1a; j == 0 seems to imply 1b precedes 2a; and in program order 1a 
precedes 1b and 2a precedes 2b - put these together and it follows that 
1a precedes 1a - clearly impossible...

...but: In the absence of happens-before constraints, a compiler or 
run-time environment is free to reorder the operations of thread 2 as 
follows:

Thread 2:
    i = o.x; // 2b
    o.y = 1; // 2a

Executing the operations in the order 2b, 1a, 1b, 2a then yields i == 0 
and j == 0 after both threads execute.

The compiler or run-time reordering could occur even on a system with 
one core and no cache.

Ben

From dcholmes at optusnet.com.au  Thu Dec  6 21:44:52 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 7 Dec 2007 12:44:52 +1000
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196990859.760082.9701.nullmailer@home35>
Message-ID: <NFBBKALFDCPFIDBNKAPCGELNHJAA.dcholmes@optusnet.com.au>

Larry,

You are completely losing me here. All I can say to address your last
question is:

Given:
   bool flag = false;  // a field

   while (!flag) {
      work();
   }

The compiler can, in the absence of volatile, rewrite this as:

   if (!flag) {
      while (true) {
         work();
      }
   }

Now what has any of that got to do with a "cache coherence protocol"?

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Larry
> Riedel
> Sent: Friday, 7 December 2007 11:28 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] question the about the JMM
>
>
>
> > > If "x" is volatile though, I would expect the cache
> > > coherence protocol to make sure r3==1 if r2==1.
> >
> > The cache coherence protocol knows nothing about "volatile".
>
> Then I think "The cache coherence protocol" may not
> be a good choice for use as a cache coherence protocol
> in a model for the behavior of a Java program.
>
>
> > [...] My system (like most) uses a write-through cache [...]
>
> Then I think the cache in "My system" may not be
> a good choice for use as the cache in a model for
> the behavior of a Java program.
>
>
> > [...] "cache coherence" doesn't address reorderings
> > that the runtime compiler is allowed to do.
>
> Is there a canonical or ubiquitous example of this?  I
> am interested in the difficulties with using a model
> which includes, for example, a simple object cache
> coherence protocol based on a distributed directory.
>
>
> Larry
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From larryr at saturn.sdsu.edu  Thu Dec  6 23:12:04 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Thu, 06 Dec 2007 20:12:04 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <4758AF05.3080501@llnl.gov>
Message-ID: <1197000724.479942.10921.nullmailer@home35>


> > > [...] "cache coherence" doesn't address reorderings
> > > that the runtime compiler is allowed to do.
> > >     
> >
> > Is there a canonical or ubiquitous example of this?  I
> > am interested in the difficulties with using a model
> > which includes, for example, a simple object cache
> > coherence protocol based on a distributed directory.
> 
> Initially, o.x == o.y == 0.
> Thread 1:
>     o.x = 1; // 1a
>     j = o.y; // 1b
> Thread 2:
>     o.y = 1; // 2a
>     i = o.x; // 2b
> 
> In the absence of happens-before constraints

Presuming none of those are volatile, I would expect
Thread1 can feel free to read o.y whenever it wants to
before it does something with j, and write o.x whenever
it gets around to it.  Correspondingly for Thread2.
This seems tangential to cache coherence protocols since
neither thread is expressing any interest in seeing the
latest shared version, or acknowledging the potential
need for anyone else to see its cached local version.


Larry


From larryr at saturn.sdsu.edu  Thu Dec  6 23:12:06 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Thu, 06 Dec 2007 20:12:06 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGELNHJAA.dcholmes@optusnet.com.au>
Message-ID: <1197000726.895753.10925.nullmailer@home35>


> > > The problem with using "cache coherence" as a logical model
> > > for the JMM is two-fold. First, it is insufficient as it
> > > doesn't address reorderings that the runtime compiler is
> > > allowed to do. [...]
> >
> > Is there a canonical or ubiquitous example of this?  I am
> > interested in the difficulties with using a model which
> > includes, for example, a simple object cache coherence
> > protocol based on a distributed directory.
> 
> All I can say to address your last question is:
> 
> Given:
>    bool flag = false;  // a field
>    while (!flag) {
>       work();
>    }
> The compiler can, in the absence of volatile, rewrite this as:
>    if (!flag) {
>       while (true) {
>          work();
>       }
>    }
> Now what has any of that got to do with a "cache
> coherence protocol"?

I presumed a "logical model for the JMM" would be addressing
propagation of object state changes between threads.  The
thread above does not seem to be interested in making sure
it has the latest state information for any objects...


Larry


From dcholmes at optusnet.com.au  Thu Dec  6 23:32:37 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 7 Dec 2007 14:32:37 +1000
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1197000726.895753.10925.nullmailer@home35>
Message-ID: <NFBBKALFDCPFIDBNKAPCEELOHJAA.dcholmes@optusnet.com.au>

Larry,

> I presumed a "logical model for the JMM" would be addressing
> propagation of object state changes between threads.  The
> thread above does not seem to be interested in making sure
> it has the latest state information for any objects...

The model explains *why* the code shown does not ensure the latest state
information is communicated. The thread *is* interested, but the code is
wrong under the JMM.

David Holmes


From dcholmes at optusnet.com.au  Fri Dec  7 00:03:53 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 7 Dec 2007 15:03:53 +1000
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGELMHJAA.dcholmes@optusnet.com.au>
Message-ID: <ABEHILABNFKEAJNKLENCAEGFCFAA.dcholmes@optusnet.com.au>

I wrote:
> The problem with using "cache coherence" as a logical model for the JMM is
> two-fold. First, it is insufficient as it doesn't address reorderings that
> the runtime compiler is allowed to do.

I misspoke. I was confusing hardware cache-coherency protocols (which know
nothing about compiler reorderings) and the logical "cache" model as used in
the original JMM. The latter can of course address compiler re-orderings.

Sorry for the confusion.

David Holmes


From larryr at saturn.sdsu.edu  Fri Dec  7 01:04:35 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Thu, 06 Dec 2007 22:04:35 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEELOHJAA.dcholmes@optusnet.com.au>
Message-ID: <1197007475.833944.3487.nullmailer@home35>


> > I presumed a "logical model for the JMM" would be addressing
> > propagation of object state changes between threads.  The
> > thread above does not seem to be interested in making sure
> > it has the latest state information for any objects...
> 
> The model explains *why* the code shown does not ensure the
> latest state information is communicated. The thread *is*
> interested, but the code is wrong under the JMM.

I am inclined to think the way to make it not wrong is to use
mechanisms which introduce something which can be described
in terms of a "happens-before" model, or in terms of a cache
coherent distributed shared memory model, with comparable
levels of intuitiveness and accuracy.  I am not sure though.


Larry


From jmanson at cs.umd.edu  Fri Dec  7 03:31:31 2007
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Fri, 07 Dec 2007 00:31:31 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1196983783.704266.8716.nullmailer@home35>
References: <1196983783.704266.8716.nullmailer@home35>
Message-ID: <475904E3.5060509@cs.umd.edu>

Larry Riedel wrote:
>> To me, cache coherence is a protocol used by a processor
>> to address the consistency of its caches.  It defines the
>> behavior of reads and writes to the same memory locations.
> 
> That sounds like a pretty good description to me.  I
> think I might just change "a processor" to "a set of
> threads", and "memory locations" to "sets of objects".

Not really.  A cache coherence protocol is for hardware, not a 
programming language.  See below.

>> [p==q]
>> Thread 1:
>>     r1 = p.x;
>>     r2 = q.x;
>>     r3 = p.x;
>> Thread 2:
>>     q.x = 1;
>>
>> This code reads and writes to memory locations, and
>> therefore relies on a cache coherence protocol.  Its
>> behavior under the Java memory model is different from
>> its behavior under any sane cache coherence protocol.
> 
> The code is not utilizing any inter-thread relative
> ordering constraint mechanisms of the protocol, so
> I think the cache coherence protocol is agnostic
> about whether Thread1 sees r1==r3==0 and r2==1.
> If "x" is volatile though, I would expect the cache
> coherence protocol to make sure r3==1 if r2==1.

So, something which I am not sure I am communicating effectively: all 
stores and loads in Java, regardless of whether they imply a 
happens-before ordering between different threads, have a behavior which 
can be described by the JMM.  All stores and loads executed directly on 
any modern processor have a behavior which can be described by the cache 
coherence protocol, regardless of whether they use atomic operations or 
memory fences / barriers.

The JMM controls what behaviors are allowed by a Java program.  The 
cache coherence protocol controls what behaviors are allowed by 
instructions executing on the hardware.  These are not the same things. 
  Sometimes the JMM makes more guarantees, and sometimes it makes fewer.

(The JMM can make more guarantees because it sometimes inserts explicit 
memory coherence operations where they would not obviously be indicated 
by the code.)

In this case, were these threads executed directly on x86 hardware 
(changing everything to appropriate x86 assembly instructions), without 
any reordering by the compiler, the x86 memory consistency model and 
cache coherence protocol would ensure that that result did not occur.

However, the JMM allows that result, because it allows the compiler to 
reorder the operations.

It's worth repeating this: The JMM controls what behaviors are allowed 
by a Java program.  The cache coherence protocol controls what behaviors 
are allowed by instructions executing on the hardware.  These are not 
the same things.  Sometimes the JMM makes more guarantees, and sometimes 
it makes fewer.

>>> What would be a good example where there is a
>>> happens-before but still r1==r3==0 and r2==1?
>> If you are looking for an example where happens-before
>> edges differ from explicit memory barriers,
> 
> An example where there is a happens-before but still
> r1==r3==0 and r2==1, or something conceptually analogous.
> 
>> edges differ from explicit memory barriers, that's a
>> different question.  It's also a question that doesn't
>> really have an answer, because you really can't map
>> anything to a "memory barrier" in Java, because the
>> notion doesn't exist.
> 
> I did not say anything about "memory barrier", so I am
> ok with the idea that the notion does not exist. (-:

Memory barriers are how you ensure cache coherence at the machine level 
(although there are some other operations which provide certain 
guarantees).  I don't actually know what you mean by using a cache 
coherence protocol without using some sort of cache coherence operation.

As "happens-before" doesn't exist as a concept for cache coherence 
protocols, "memory barriers" don't exist as a concept in Java.


>> An example would be that this:
>>    synchronized (new Object()) {
>>       x = 1;
>>    }
>> Does not have the same memory semantics as this:
>>    mfence;  // or whatever you want to use
>>    x = 1;
>>    sfence;
>>
>> Because synchronization on an Object that can't be
>> accessed by multiple threads can be removed entirely.
> 
> Guessing what role "mfence" and "sfence" are playing
> there, I imagine I might want to use more like:
> 
>     Object o = new Object();
>     mfence(o);
>     x = 1;
>     sfence(o);
> 
> Presuming no reference to "o" leaks out via [ms]fence,
> I would not expect the cache coherence protocol to need
> to impose any actual ordering constraints,

That doesn't really make any sense with respect to memory fences.  They 
are cache coherence operations; they don't have anything to do with a 
given object.

A good, albeit aging, primer on memory consistency is Shared Memory 
Consistency Models: A Tutorial, by Sarita Adve and Kourosh Gharachorloo. 
  It can be found here: http://citeseer.ist.psu.edu/adve95shared.html .

					Jeremy

From larryr at saturn.sdsu.edu  Fri Dec  7 12:59:26 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Fri, 07 Dec 2007 09:59:26 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <475904E3.5060509@cs.umd.edu>
Message-ID: <1197050366.478938.4926.nullmailer@home35>


> > > To me, cache coherence is a protocol used by a processor
> > > to address the consistency of its caches.  It defines the
> > > behavior of reads and writes to the same memory locations.
> > 
> > That sounds like a pretty good description to me.  I
> > think I might just change "a processor" to "a set of
> > threads", and "memory locations" to "sets of objects".
>
> Not really.  A cache coherence protocol is for hardware, not
> a programming language.

A cache is a container of local unshared snapshots of the states
of non-local shared objects, and a cache coherence protocol is
a mechanism for supporting the externally visible object state
consistency model.  Cache coherence protocols are fundamental
to contemporary network filesystems and relational databases,
and are no more exclusive to hardware than content-addressable
storage or myriad other abstract data types which happen to have
implementations in hardware.  I agree though it seems better to
say the memory consistency model is defined and provided by the
runtime environment, not the Java language itself; I never meant
to imply the latter; I think of constructs like "volatile" and
"synchronized" as language-level mechanisms for interfacing with
the memory consistency mechanism of the runtime environment.


Larry


From jmanson at cs.umd.edu  Fri Dec  7 13:35:01 2007
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Fri, 07 Dec 2007 10:35:01 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1197050366.478938.4926.nullmailer@home35>
References: <1197050366.478938.4926.nullmailer@home35>
Message-ID: <47599255.60002@cs.umd.edu>

Larry Riedel wrote:
>>>> To me, cache coherence is a protocol used by a processor
>>>> to address the consistency of its caches.  It defines the
>>>> behavior of reads and writes to the same memory locations.
>>> That sounds like a pretty good description to me.  I
>>> think I might just change "a processor" to "a set of
>>> threads", and "memory locations" to "sets of objects".
>> Not really.  A cache coherence protocol is for hardware, not
>> a programming language.
> 
> A cache is a container of local unshared snapshots of the states
> of non-local shared objects, and a cache coherence protocol is
> a mechanism for supporting the externally visible object state
> consistency model.  Cache coherence protocols are fundamental
> to contemporary network filesystems and relational databases,
> and are no more exclusive to hardware than content-addressable
> storage or myriad other abstract data types which happen to have
> implementations in hardware.  

That's perfectly fair in general, and perhaps I stated the case too 
broadly.  In this specific case, I was referring to things that are 
pertinent to this discussion and actually have a cache.  The hardware 
has a cache.  The semantics of the Java programming language describe 
nothing that resembles a cache, and the Java memory model allows 
behaviors that a reasonable hardware cache coherence policy would not allow.

> I agree though it seems better to
> say the memory consistency model is defined and provided by the
> runtime environment, not the Java language itself; I never meant
> to imply the latter; I think of constructs like "volatile" and
> "synchronized" as language-level mechanisms for interfacing with
> the memory consistency mechanism of the runtime environment.

The memory consistency model for a Java program is defined by the 
semantics of the Java programming language, not the runtime environment; 
if an implementation is compliant, it will only allow behaviors defined 
by the semantics, and if it is not compliant, it cannot be called Java. 
  Constructs like "volatile" and "synchronized" provide behaviors that 
are described by the semantics, and any effect they may have on caches 
is purely implementation-dependent.

There's a reason I'm hammering on this so hard.  If programmers are to 
be able to write correct programs without understanding all of the 
behaviors of the compiler and the processor, it is absolutely necessary 
for us to have a distinction between what the language allows and what a 
given implementation actually does.  With the first, there is only one 
thing to understand, it is relatively consistent, and can be explained 
relatively easily.  With the second, it can change from VM to VM and 
from one hardware implementation to another.

That's why C++ is adding a memory model.  Otherwise, it is simply 
impossible to write correct code.

That's also why I don't like to talk about this in terms of cache 
coherence -- it doesn't map well to what the JMM actually describes.

					Jeremy

From larryr at saturn.sdsu.edu  Fri Dec  7 15:21:26 2007
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Fri, 07 Dec 2007 12:21:26 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <47599255.60002@cs.umd.edu>
Message-ID: <1197058886.079504.6494.nullmailer@home35>


> In this specific case, I was referring to things that are
> pertinent to this discussion and actually have a cache.

I was referring, as I have been from the outset, to
a logical model for reasoning about the behavior of
the software, and the use of caches in that model.
Since that logical model "actually has a cache", it
is indubitably pertinent.


> > I agree though it seems better to say the memory consistency
> > model is defined and provided by the runtime environment, not
> > the Java language itself; I never meant to imply the latter;
> > I think of constructs like "volatile" and "synchronized" as
> > language-level mechanisms for interfacing with the memory
> > consistency mechanism of the runtime environment.
>
> The memory consistency model for a Java program is defined by
> the semantics of the Java programming language, not the runtime
> environment; if an implementation is compliant, it will only
> allow behaviors defined by the semantics, and if it is not
> compliant, it cannot be called Java.

That sounds ok to me.  The runtime environment is the
mechanism of behavior and access to state information.
The memory consistency model imposes requirements and
constraints on the runtime environment.  Definition of
the the language itself, in the absence the definition
of a compliant environment, is not sufficient to ensure
behavior compliant with the memory consistency model.

I find it intuitively easier to think of the memory
consistency model as part of the runtime environment,
and the language as providing a means for interfacing
with and utilizing it.  But neither the JLS or JVMS
in isolation as solely defining it.


> Constructs like "volatile" and "synchronized" provide
> behaviors that are described by the semantics

Indeed, as I said at the outset:

    > I decided to get a more detailed answer to convince
    > him that reasoning in 'caches and invalidation' is not
    > the way to go with the new JMM.

    I think it should be ok to reason that way, as long as
    it is done in a way which is consistent with the actual
    semantics, which I think is viable.


> If programmers are to be able to write correct programs
> without understanding all of the behaviors of the compiler
> and the processor, it is absolutely necessary for us to have
> a distinction between what the language allows and what a
> given implementation actually does.

I do not think of Java as defining much about behavior
of compilers or of "processors", if "processors" means
what is used by the implementation of the runtime
environment.  I think of it as describing the relationship
between the input and output of compilers, and describing
the runtime environment provided by the machine which
interprets/executes the output of the compiler to produce
the behavior of the software.


> That's why C++ is adding a memory model.  Otherwise, it is
> simply impossible to write correct code.

In that context I tend to think of C++ as mostly adding a
memory model for the runtime environment, especially since
there is not something which seems comparable to "The Java
Virtual Machine Specification" to define it.  But by this
I am not suggesting the language will not have changed,


Larry


From joe.bowbeer at gmail.com  Fri Dec  7 16:04:05 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 7 Dec 2007 13:04:05 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1197058886.079504.6494.nullmailer@home35>
References: <47599255.60002@cs.umd.edu>
	<1197058886.079504.6494.nullmailer@home35>
Message-ID: <31f2a7bd0712071304y7e98cff6h533e35e77bffbc59@mail.gmail.com>

In case those interested are not already aware, there is an mailing
list for in-depth discussions of the JMM:

 http://www.cs.umd.edu/~pugh/java/memoryModel/index.html#mailing

On Dec 7, 2007 12:21 PM, Larry Riedel <larryr at saturn.sdsu.edu> wrote:
>
> > In this specific case, I was referring to things that are
> > pertinent to this discussion and actually have a cache.
>
> I was referring, as I have been from the outset, to
> a logical model for reasoning about the behavior of
> the software, and the use of caches in that model.
> Since that logical model "actually has a cache", it
> is indubitably pertinent.
>
> [ ... ]

From dl at cs.oswego.edu  Sat Dec  8 08:59:09 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 08 Dec 2007 08:59:09 -0500
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <1197058886.079504.6494.nullmailer@home35>
References: <1197058886.079504.6494.nullmailer@home35>
Message-ID: <475AA32D.9080307@cs.oswego.edu>


A couple of thoughts on this discussion. Maybe they'd be
better on JMM list, but I think membership overlaps a lot.

Even though the JMM stands alone, regardless of underlying
caches, fences, and other memory system control, many people
seem to want a way to reconcile what they know about memory
systems with the JMM. Some of these people know less about
memory systems than is required to understand how they do
fit together. In which case, suggesting they ignore memory
system control and focus on JMM rules is good advice.

But some do know enough, and know what they want in terms
of memory control, but don't know how to achieve it in Java code.
So there ought to be a good high-level account of the JMM
targeted to such people. In addition to mapping reads
and writes of final, volatile, plain fields to fences
etc (as in my http://gee.cs.oswego.edu/dl/jmm/cookbook.html),
this mainly revolves around how/why optimization via program
analysis adds to the straight mapping story.
For example, that if a compiler sees
"local a = x.field; local b = x.field;" that it is normally
allowed (but not required) to transform to: "local a = x.field;
local b = a;". Or maybe kill "b" entirely and just use "a".
Or maybe not even  actually perform the x.field read at all
if its value is known for all possible sequential executions
(in which case it need not even actually write it).
And so on. Plus the fun/weird causal cycle issues such
analyses can encounter. Is there a simple way to characterize
these to arrive at a good short answer to questions from
this sort of audience?

-Doug

From jmanson at cs.umd.edu  Sat Dec  8 16:36:38 2007
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Sat, 08 Dec 2007 13:36:38 -0800
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <475AA32D.9080307@cs.oswego.edu>
References: <1197058886.079504.6494.nullmailer@home35>
	<475AA32D.9080307@cs.oswego.edu>
Message-ID: <475B0E66.1090701@cs.umd.edu>

Doug,

My feeling about this has always been that your cookbook is an excellent 
way for implementors to understand the barriers that need to be 
injected, but that the only way for a programmer to understand this is 
to talk about happens-before.

Otherwise, presumably, if you know enough to know why you need fences, 
you would also know enough to know what straight-line compiler 
optimization can do.  Most fences I've seen are associated with some 
form of compiler barrier -- in gcc, for example, there is a asm volatile 
that prevents the compiler from doing reordering around it.  If the 
programmer is that clever, then the only thing you really need to 
describe is when fences can be removed.

You could presumably say something as simple as the idea that a fence 
can be removed if it can be determined that the results of the [final, 
volatile, synchronized] access associated with that fence can never be 
seen by another thread.  So, for example, if a write to a volatile is 
never read by another thread, then the fence can be removed.

Another example would be an unlocked lock that is not subsequently 
locked by another thread (at least, not without being acquired by the 
same thread first).  If no thread "sees" the unlock, then you don't have 
to perform the associated fence.

So, if you have

synchronized(new Object()) { ... }

the compiler can prove no other thread will see that unlock, and remove 
the associated fence.  Further, if you have:

synchronized (a) {
   synchronized (a) {
   }
}

Then the unlock in the inner block won't occur, therefore no one will 
see it, therefore the fence can be removed.

(It's worth pointing out that if the same thread writes to the volatile 
again, and *then* it is read by another thread, then the results of the 
initial write aren't read by another thread.  So, if you have a program 
that consists entirely of:

volatile int x, y;

Thread 1:
x = 1;
y = 1;
x = 2;
y = 2;

Thread 2:
r1 = y;
if (r1 == 2) {
   r2 = x;
}

Then you know that you don't have to perform the barrier after the first 
write to x, because no one will read the value written there.)

					Jeremy


Doug Lea wrote:
> A couple of thoughts on this discussion. Maybe they'd be
> better on JMM list, but I think membership overlaps a lot.
> 
> Even though the JMM stands alone, regardless of underlying
> caches, fences, and other memory system control, many people
> seem to want a way to reconcile what they know about memory
> systems with the JMM. Some of these people know less about
> memory systems than is required to understand how they do
> fit together. In which case, suggesting they ignore memory
> system control and focus on JMM rules is good advice.
> 
> But some do know enough, and know what they want in terms
> of memory control, but don't know how to achieve it in Java code.
> So there ought to be a good high-level account of the JMM
> targeted to such people. In addition to mapping reads
> and writes of final, volatile, plain fields to fences
> etc (as in my http://gee.cs.oswego.edu/dl/jmm/cookbook.html),
> this mainly revolves around how/why optimization via program
> analysis adds to the straight mapping story.
> For example, that if a compiler sees
> "local a = x.field; local b = x.field;" that it is normally
> allowed (but not required) to transform to: "local a = x.field;
> local b = a;". Or maybe kill "b" entirely and just use "a".
> Or maybe not even  actually perform the x.field read at all
> if its value is known for all possible sequential executions
> (in which case it need not even actually write it).
> And so on. Plus the fun/weird causal cycle issues such
> analyses can encounter. Is there a simple way to characterize
> these to arrive at a good short answer to questions from
> this sort of audience?
> 
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From J.Sevcik at sms.ed.ac.uk  Mon Dec 10 06:14:56 2007
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Mon, 10 Dec 2007 11:14:56 +0000
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <475AA32D.9080307@cs.oswego.edu>
References: <1197058886.079504.6494.nullmailer@home35>
	<475AA32D.9080307@cs.oswego.edu>
Message-ID: <475D1FB0.9070908@sms.ed.ac.uk>

Doug, we have already discussed this before, and I still maintain that 
the JMM is flawed with respect to transformations.

For example, the program

(initially x=y=0)
Thread1: r1=x; y=r1
Thread2: r2=y; x=(r2==1)?y:1;print r2

should never print 1 according to the JMM. However, an optimizing 
compiler can rewrite the second thread:

r2=y; x=(r2==1)?y:1;print r2 ==> r2=y; x=(r2==1)?r2:1; print r2 ==> 
r2=y; x=1; print r2 ==> x=1;print y

After this transformation, it's certainly possible to get 1 printed even 
on sequentially consistent hardware - just use the interleaving x=y=0; 
x=1; r1=x; y=r1; print y. Even Hotspot performs this transformation (see 
the attachment).

That said, I believe that Hotspot still satisfies the guarantee of 
sequential consistency for correctly synchronised programs, although it 
does not implement the JMM properly.

The lesson is still the same (and Jeremy should emphasize it more often) 
- synchronize properly and you'll be fine.

Jaroslav
 
Doug Lea wrote:
> A couple of thoughts on this discussion. Maybe they'd be
> better on JMM list, but I think membership overlaps a lot.
>
> Even though the JMM stands alone, regardless of underlying
> caches, fences, and other memory system control, many people
> seem to want a way to reconcile what they know about memory
> systems with the JMM. Some of these people know less about
> memory systems than is required to understand how they do
> fit together. In which case, suggesting they ignore memory
> system control and focus on JMM rules is good advice.
>
> But some do know enough, and know what they want in terms
> of memory control, but don't know how to achieve it in Java code.
> So there ought to be a good high-level account of the JMM
> targeted to such people. In addition to mapping reads
> and writes of final, volatile, plain fields to fences
> etc (as in my http://gee.cs.oswego.edu/dl/jmm/cookbook.html),
> this mainly revolves around how/why optimization via program
> analysis adds to the straight mapping story.
> For example, that if a compiler sees
> "local a = x.field; local b = x.field;" that it is normally
> allowed (but not required) to transform to: "local a = x.field;
> local b = a;". Or maybe kill "b" entirely and just use "a".
> Or maybe not even  actually perform the x.field read at all
> if its value is known for all possible sequential executions
> (in which case it need not even actually write it).
> And so on. Plus the fun/weird causal cycle issues such
> analyses can encounter. Is there a simple way to characterize
> these to arrive at a good short answer to questions from
> this sort of audience?
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: Tester.java
Type: text/x-java
Size: 918 bytes
Desc: not available
Url : /pipermail/attachments/20071210/a66b63a9/attachment.bin 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: hotspot.log
Type: text/x-log
Size: 15403 bytes
Desc: not available
Url : /pipermail/attachments/20071210/a66b63a9/attachment-0001.bin 

From dl at cs.oswego.edu  Mon Dec 10 07:22:02 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 10 Dec 2007 07:22:02 -0500
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <475D1FB0.9070908@sms.ed.ac.uk>
References: <1197058886.079504.6494.nullmailer@home35>
	<475AA32D.9080307@cs.oswego.edu> <475D1FB0.9070908@sms.ed.ac.uk>
Message-ID: <475D2F6A.8050102@cs.oswego.edu>

Jaroslav Sevcik wrote:
> Doug, we have already discussed this before, and I still maintain that 
> the JMM is flawed with respect to transformations.
> 

Right. It's probably best to move that part of discussion
(i.e., diagnosing and fixing possible technical flaws in JMM
rules for transformations) to the JMM list
(Javamemorymodel-discussion at mcfeely.cs.umd.edu --
See http://mailman.cs.umd.edu/mailman/listinfo/javamemorymodel-discussion)

-Doug

From sberlin at gmail.com  Mon Dec 10 10:53:47 2007
From: sberlin at gmail.com (Sam Berlin)
Date: Mon, 10 Dec 2007 10:53:47 -0500
Subject: [concurrency-interest] How bad can volatile long++ be?
Message-ID: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>

Hi Folks,

I'm fairly certain that ++ is not an atomic operation, even on
volatile variables, on longs (and quite possibly ints).  Given that is
true (which it very well might not be), is it suspectible to problems
where a wildly wrong number can be produced (due to different bytes
being updated at different times from different threads), or will it
just cause some increments to effectively not happen?

Thanks!
 Sam

From chris.purcell.39 at gmail.com  Mon Dec 10 13:04:10 2007
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Mon, 10 Dec 2007 18:04:10 +0000
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
References: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
Message-ID: <4856e16d0712101004h1d5b75fcv73424dc791af7480@mail.gmail.com>

I can't answer with authority whether ++ is atomic; I don't believe
so. However, I can answer the rest of the question.

The reads and writes will be atomic. Even so, the non-atomicity of the
increment may still result in a wildly wrong number being produced.

As an example, consider two threads incrementing a shared counter like
this a thousand times each. If the counter starts at zero, in one
unlikely but possible execution history, the final value of the
counter may be only 2. That is, one thousand nine hundred and ninety
eight increments can be lost. (The same final value is possible for
any number of threads and any number of increments.)

If you're only maintaining statistics, however, the inaccuracy of the
counter shouldn't be an issue.

Cheers,
Chris

On 12/10/07, Sam Berlin <sberlin at gmail.com> wrote:
> Hi Folks,
>
> I'm fairly certain that ++ is not an atomic operation, even on
> volatile variables, on longs (and quite possibly ints).  Given that is
> true (which it very well might not be), is it suspectible to problems
> where a wildly wrong number can be produced (due to different bytes
> being updated at different times from different threads), or will it
> just cause some increments to effectively not happen?
>
> Thanks!
>  Sam
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From joe.bowbeer at gmail.com  Mon Dec 10 13:30:09 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 10 Dec 2007 10:30:09 -0800
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
References: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
Message-ID: <31f2a7bd0712101030g6a5857m546fad48711c7da7@mail.gmail.com>

++ of any numeric field is not atomic with or without volatile.
however, all numeric fields except longs are read and written
atomically. volatile adds this feature to longs; otherwise the two int
halves might be processed separately.

On 12/10/07, Sam Berlin <sberlin at gmail.com> wrote:
> Hi Folks,
>
> I'm fairly certain that ++ is not an atomic operation, even on
> volatile variables, on longs (and quite possibly ints).  Given that is
> true (which it very well might not be), is it suspectible to problems
> where a wildly wrong number can be produced (due to different bytes
> being updated at different times from different threads), or will it
> just cause some increments to effectively not happen?
>
> Thanks!
>  Sam
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From dgallardo at gmail.com  Mon Dec 10 13:49:42 2007
From: dgallardo at gmail.com (David Gallardo)
Date: Mon, 10 Dec 2007 10:49:42 -0800
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <4856e16d0712101004h1d5b75fcv73424dc791af7480@mail.gmail.com>
References: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
	<4856e16d0712101004h1d5b75fcv73424dc791af7480@mail.gmail.com>
Message-ID: <24dec92b0712101049t51aacce9s54da0b54ca93fe12@mail.gmail.com>

++ is not atomic; while it may effectively be so on a single processor
machine, this is not the case on multiprocessor machines.

@D

On Dec 10, 2007 10:04 AM, Chris Purcell <chris.purcell.39 at gmail.com> wrote:
> I can't answer with authority whether ++ is atomic; I don't believe
> so. However, I can answer the rest of the question.
>
> The reads and writes will be atomic. Even so, the non-atomicity of the
> increment may still result in a wildly wrong number being produced.
>
> As an example, consider two threads incrementing a shared counter like
> this a thousand times each. If the counter starts at zero, in one
> unlikely but possible execution history, the final value of the
> counter may be only 2. That is, one thousand nine hundred and ninety
> eight increments can be lost. (The same final value is possible for
> any number of threads and any number of increments.)
>
> If you're only maintaining statistics, however, the inaccuracy of the
> counter shouldn't be an issue.
>
> Cheers,
> Chris
>
>
> On 12/10/07, Sam Berlin <sberlin at gmail.com> wrote:
> > Hi Folks,
> >
> > I'm fairly certain that ++ is not an atomic operation, even on
> > volatile variables, on longs (and quite possibly ints).  Given that is
> > true (which it very well might not be), is it suspectible to problems
> > where a wildly wrong number can be produced (due to different bytes
> > being updated at different times from different threads), or will it
> > just cause some increments to effectively not happen?
> >
> > Thanks!
> >  Sam
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From gdipersio at nyc.saic.com  Mon Dec 10 14:09:29 2007
From: gdipersio at nyc.saic.com (Glen Di Persio)
Date: Mon, 10 Dec 2007 14:09:29 -0500
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
References: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
Message-ID: <12AE7DE71168B8419F619FF3079F969C03390693@nyc-1250-srv04.nyc.com>

There are plenty of java gurus on this thread who can tell you every
scenario that can happen in theory - I'm not one of those people :)  I
will tell you that in practice, you *need* to use an Atomic type or a
synchronized block.  Yes, even updating a shared int using ++myInt isn't
safe!

Run my attached code to see some missing increments.  It may appear ok
for small iterations, but it fails as you add a few threads and crank up
the iterations.  Although your question was "can this go really really
haywire", the answer is simply "it definitely goes a little haywire!"
Although the only side-effect I see is lost increments, that alone is
enough - time to make friends with Atomic :)

-Glen



Example using int, since the problem is bad enough with int:

C:>java -showversion IntRace
java version "1.5.0_11"
Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_11-b03)
Java HotSpot(TM) Server VM (build 1.5.0_11-b03, mixed mode)

Usage: java IntRace <threads> <reps>

C:\>java IntRace 2 10
Expected result: 20
  Actual result: 20

C:\>java IntRace 3 1000
Expected result: 3000
  Actual result: 2800
                 ^^^^ 200 lost increments!


-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Sam
Berlin
Sent: Monday, December 10, 2007 10:54 AM
To: Concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] How bad can volatile long++ be?

Hi Folks,

I'm fairly certain that ++ is not an atomic operation, even on volatile
variables, on longs (and quite possibly ints).  Given that is true
(which it very well might not be), is it suspectible to problems where a
wildly wrong number can be produced (due to different bytes being
updated at different times from different threads), or will it just
cause some increments to effectively not happen?

Thanks!
 Sam
-------------- next part --------------
A non-text attachment was scrubbed...
Name: IntRace.java
Type: application/octet-stream
Size: 1452 bytes
Desc: IntRace.java
Url : /pipermail/attachments/20071210/94bb13a2/attachment.obj 

From hans.boehm at hp.com  Mon Dec 10 14:19:29 2007
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 10 Dec 2007 19:19:29 +0000
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <4856e16d0712101004h1d5b75fcv73424dc791af7480@mail.gmail.com>
References: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
	<4856e16d0712101004h1d5b75fcv73424dc791af7480@mail.gmail.com>
Message-ID: <EB8E0FF63AB2414693DB20D50E863AE809050AB8@G3W0634.americas.hpqcorp.net>

In Java, ++ on volatiles is not atomic.  I strongly recommend against maintaining even statistical counters that way.  Use either java.util.concurrent.atomic.AtomicLong or per-thread counters.

The following non-Java anecdote from several years ago convinced me of this:

Our conservative garbage collector used to optionally maintain approximate counts of live objects this way, by having each GC thread increment a shared counter as it found a live object.  There are as many GC threads as processors.  After observing some misbehavior on a dual processor (2 sockets, 1 core/socket) machine, and tracking down what was going on, I discovered that:

1) Maintaining the shared counter slowed down the collector by about a factor of two, negating the benefit of running on two processors.

2) The final count was consistently off by about a factor of two.

The cause for (1) was no doubt contention on the cache line containing the counter.  As a result of that, both threads spent a lot of their time waiting on that cache line, and presumably the timing worked out such that there was a high probability the line got stolen between the load and the store of the ++, resulting in the loss of about half the updates.

(In the proposed C++ standard, ++ on atomic<long> WILL be atomic, as for java.util.concurrent.atomic.getAndAdd.  However, in the C++ case, ++ on a shared ordinary or volatile variable will result in completely undefined behavior, meaning your e-commerce app is allowed to start playing Rogue-O-Matic instead if you do this.  Indeed, those are the current rules for (C or C++) and pthreads.  Thus you definitely don't want to do this in C or C++ code.  Clearly I did not follow this advice in the above example, but the code has since been fixed. )

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> Of Chris Purcell
> Sent: Monday, December 10, 2007 10:04 AM
> To: Sam Berlin
> Cc: Concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] How bad can volatile long++ be?
>
> I can't answer with authority whether ++ is atomic; I don't
> believe so. However, I can answer the rest of the question.
>
> The reads and writes will be atomic. Even so, the
> non-atomicity of the increment may still result in a wildly
> wrong number being produced.
>
> As an example, consider two threads incrementing a shared
> counter like this a thousand times each. If the counter
> starts at zero, in one unlikely but possible execution
> history, the final value of the counter may be only 2. That
> is, one thousand nine hundred and ninety eight increments can
> be lost. (The same final value is possible for any number of
> threads and any number of increments.)
>
> If you're only maintaining statistics, however, the
> inaccuracy of the counter shouldn't be an issue.
>
> Cheers,
> Chris
>
> On 12/10/07, Sam Berlin <sberlin at gmail.com> wrote:
> > Hi Folks,
> >
> > I'm fairly certain that ++ is not an atomic operation, even on
> > volatile variables, on longs (and quite possibly ints).
> Given that is
> > true (which it very well might not be), is it suspectible
> to problems
> > where a wildly wrong number can be produced (due to different bytes
> > being updated at different times from different threads),
> or will it
> > just cause some increments to effectively not happen?
> >
> > Thanks!
> >  Sam
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From Thomas.Hawtin at Sun.COM  Mon Dec 10 16:06:37 2007
From: Thomas.Hawtin at Sun.COM (Thomas Hawtin)
Date: Mon, 10 Dec 2007 21:06:37 +0000
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <EB8E0FF63AB2414693DB20D50E863AE809050AB8@G3W0634.americas.hpqcorp.net>
References: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
	<4856e16d0712101004h1d5b75fcv73424dc791af7480@mail.gmail.com>
	<EB8E0FF63AB2414693DB20D50E863AE809050AB8@G3W0634.americas.hpqcorp.net>
Message-ID: <475DAA5D.8020800@Sun.COM>

Boehm, Hans wrote:
> In Java, ++ on volatiles is not atomic.  I strongly recommend against maintaining even statistical counters that way.  Use either java.util.concurrent.atomic.AtomicLong or per-thread counters.
> 
> The following non-Java anecdote from several years ago convinced me of this:

So, before java.util.concurrent?

> Our conservative garbage collector used to optionally maintain approximate counts of live objects this way, by having each GC thread increment a shared counter as it found a live object.  There are as many GC threads as processors.  After observing some misbehavior on a dual processor (2 sockets, 1 core/socket) machine, and tracking down what was going on, I discovered that:
> 
> 1) Maintaining the shared counter slowed down the collector by about a factor of two, negating the benefit of running on two processors.
> 
> 2) The final count was consistently off by about a factor of two.
> 
> The cause for (1) was no doubt contention on the cache line containing the counter.  As a result of that, both threads spent a lot of their time waiting on that cache line, and presumably the timing worked out such that there was a high probability the line got stolen between the load and the store of the ++, resulting in the loss of about half the updates.

I don't think AtomicLong is going to help there, as it is still sharing 
a volatile. So thread-local (ThreadLocal) counters are presumably the 
way to go, possibly using an unshared AtomicLong.

Tom Hawtin

From hans.boehm at hp.com  Mon Dec 10 16:24:47 2007
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 10 Dec 2007 21:24:47 +0000
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <475DAA5D.8020800@Sun.COM>
References: <19196d860712100753q3826eb26y65cbee554cf02ba6@mail.gmail.com>
	<4856e16d0712101004h1d5b75fcv73424dc791af7480@mail.gmail.com>
	<EB8E0FF63AB2414693DB20D50E863AE809050AB8@G3W0634.americas.hpqcorp.net>
	<475DAA5D.8020800@Sun.COM>
Message-ID: <EB8E0FF63AB2414693DB20D50E863AE809050CA2@G3W0634.americas.hpqcorp.net>



> -----Original Message-----
> From: Thomas.Hawtin at Sun.COM [mailto:Thomas.Hawtin at Sun.COM]
> Sent: Monday, December 10, 2007 1:07 PM
> To: Boehm, Hans
> Cc: Chris Purcell; Sam Berlin; Concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] How bad can volatile long++ be?
>
> Boehm, Hans wrote:
> > In Java, ++ on volatiles is not atomic.  I strongly
> recommend against maintaining even statistical counters that
> way.  Use either java.util.concurrent.atomic.AtomicLong or
> per-thread counters.
> >
> > The following non-Java anecdote from several years ago
> convinced me of this:
>
> So, before java.util.concurrent?
>
> > Our conservative garbage collector used to optionally
> maintain approximate counts of live objects this way, by
> having each GC thread increment a shared counter as it found
> a live object.  There are as many GC threads as processors.
> After observing some misbehavior on a dual processor (2
> sockets, 1 core/socket) machine, and tracking down what was
> going on, I discovered that:
> >
> > 1) Maintaining the shared counter slowed down the collector
> by about a factor of two, negating the benefit of running on
> two processors.
> >
> > 2) The final count was consistently off by about a factor of two.
> >
> > The cause for (1) was no doubt contention on the cache line
> containing the counter.  As a result of that, both threads
> spent a lot of their time waiting on that cache line, and
> presumably the timing worked out such that there was a high
> probability the line got stolen between the load and the
> store of the ++, resulting in the loss of about half the updates.
>
> I don't think AtomicLong is going to help there, as it is
> still sharing a volatile. So thread-local (ThreadLocal)
> counters are presumably the way to go, possibly using an
> unshared AtomicLong.
AtomicLong will definitely help with (2).  It might help with (1), especially on something like X86, since the cache line can only be stolen between ++ operations, not in the middle.  Thus the number of coherence misses might be reduced.  You are adding some needless memory ordering overhead in the process, but I think that's often cheaper on recent processors.  But I haven't done the measurements.

I tend to worry more about (2) than (1), especially if I don't expect the counting operation to be that frequently executed.  Having a statistical counter that lies systematically in what are probably precisely the most interesting cases is really not very useful.  So long as I get the right answer, I at least know that the counter got unexpectedly hot and is probably causing a problem.

Other than that, I agree with your conclusions.

I suspect that before java.util.concurrent, you really wanted thread-local counters.  If performance is really not much of an issue, synchronizing the updates seems like a distant second choice.

Hans

>
> Tom Hawtin
>


From dcholmes at optusnet.com.au  Mon Dec 10 18:07:59 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 11 Dec 2007 09:07:59 +1000
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <24dec92b0712101049t51aacce9s54da0b54ca93fe12@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEMHHJAA.dcholmes@optusnet.com.au>

David Gallardo writes:
> ++ is not atomic; while it may effectively be so on a single processor
> machine, this is not the case on multiprocessor machines.

It isn't the case on single processor machines either. ++ is
read-modify-write sequence and a thread can be preempted at any point in the
sequence.

++ is just syntatic short-hand. Write it out in full and you'd never expect
it to be atomic.

Cheers,
David Holmes


From mark at twistedbanana.demon.co.uk  Mon Dec 10 21:09:05 2007
From: mark at twistedbanana.demon.co.uk (Mark Mahieu)
Date: Tue, 11 Dec 2007 02:09:05 +0000
Subject: [concurrency-interest] jsr166y bug - NPE sorting a ParallelArray
Message-ID: <5C73106C-6AEF-47C0-B44F-DA0804FBAB09@twistedbanana.demon.co.uk>

Hi,

Apologies if this is already known about, or if I should be sending  
this somewhere else (did look, couldn't see anywhere obvious).

I believe I've tripped over a bug calling ParallelArray.sort() - it  
only appears to work with arrays of certain lengths, and throws  
NullPointerExceptions for all other cases.  I'm using the current  
code from CVS.

It should be possible to reproduce this by altering the value of the  
variable 'n' at the start of main() in the SortDemo class, for  
example to 10, which for me results in something like the following:

Exception in thread "main" java.lang.NullPointerException
         at jsr166y.forkjoin.ParallelArray$FJComparableMerger.merge 
(ParallelArray.java:4275)
         at jsr166y.forkjoin.ParallelArray$FJComparableMerger.compute 
(ParallelArray.java:4256)
         at jsr166y.forkjoin.ParallelArray$FJComparableSorter.compute 
(ParallelArray.java:4136)
         at jsr166y.forkjoin.ParallelArray 
$FJComparableSubSorter.compute(ParallelArray.java:4200)
         at jsr166y.forkjoin.RecursiveAction.exec 
(RecursiveAction.java:226)
         at jsr166y.forkjoin.ForkJoinWorkerThread.run 
(ForkJoinWorkerThread.java:286)


I think the problem lies in the calculation of the size of the 2nd of  
the 4 'quarters' which FJSorter.compute() and  
FJComparableSorter.compute() attempt to divide a subarray into.  The  
following code is taken directly from ParallelArray, starting at line  
3913:

     (new FJSubSorter<T>
      (new FJSorter<T>(cmp, a, w, origin,   q,   g),
       new FJSorter<T>(cmp, a, w, origin+q, q,   g),
       new FJMerger<T>(cmp, a, w, origin,   q,
                       origin+q, q, origin, g)
       ),

The problem being that it is only works if the first and second  
quarters are equal in size, which will depend on the size of the  
array.  The code immediately following the above, which deals with  
the 3rd and 4th quarters, appears to do this correctly already.   
Changing the code (and its counterpart in FJComparableSorter.compute 
()) to the following certainly seems to fix the problem:

     (new FJSubSorter<T>
      (new FJSorter<T>(cmp, a, w, origin,   q,   g),
       new FJSorter<T>(cmp, a, w, origin+q, h-q,   g),
       new FJMerger<T>(cmp, a, w, origin,   q,
                       origin+q, h-q, origin, g)
       ),


(If it's of any interest, I ran into this whilst experimenting with  
Neal Gafter's port of jsr166y to use BGGA closures.)

Best regards,

Mark Mahieu


From jv at cs.purdue.edu  Tue Dec 11 00:54:04 2007
From: jv at cs.purdue.edu (Jan Vitek)
Date: Tue, 11 Dec 2007 00:54:04 -0500
Subject: [concurrency-interest] Summer school on Concurrency
Message-ID: <FF0FDCF1-E1D6-469D-8CBC-E730C8E79875@cs.purdue.edu>



            The Second International Summer School on
                      Trends in Concurrency
                           TiC 2008

                    Prague, Czech Republic

   Concurrency is a pervasive and essential characteristic of modern
   computer systems. Whether it is the design of new hyper-threading
   techniques in computer architectures, specification of non-blocking
   data structures and algorithms, implementation of scalable computer
   farms for handling massive data sets, or the design of a robust
   software architecture for distributed business processes, a deep
   understanding of mechanisms and foundations for expressing and
   controlling concurrency is required.  Recent architectural advances
   in multi-core and many-core architectures have made this an essential
   topic for any serious student of computer science.

   This summer school will bring together outstanding researchers from
   academia and industry to discuss current research and future trends
   in concurrent systems design and implementation.  All instructors
   have had significant impact in the area of concurrency, and play
   an active role in substantial ongoing research and commercial
   efforts.

   The speakers include the following researchers.

      - Byron Cook, Microsoft Research
      - Neal Glew, intel Research
      - Jan-willem Maessen, Sun Laboratories.
      - Martin Odersky, EPFL.
      - Vivek Sarkar, Rice University.
      - Peter Sewell, Cambridge University.

   Additional information is available from:

       http://www.cs.purdue.edu/homes/jv/events/TiC08



From gkorland at gmail.com  Tue Dec 11 02:39:59 2007
From: gkorland at gmail.com (Guy Korland)
Date: Tue, 11 Dec 2007 09:39:59 +0200
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
	issue
Message-ID: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>

Hi,
We built an application in a SEDA fashion, working in stages from one
ThreadPool to another.
We found out that the BlockingQueue used by the ThreadPoolExecutor became a
major concurrency killer when we start working on 4 cores machines and
above.
The thing is that we don't really need the strong FIFO behavior forced by
all the BlockingQueue implementations available, some kind of fairness will
be good enough.
Any ideas?

Thanks,
Guy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071211/9dcd0fab/attachment.html 

From holger at wizards.de  Tue Dec 11 03:33:21 2007
From: holger at wizards.de (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Tue, 11 Dec 2007 09:33:21 +0100
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
 issue
In-Reply-To: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
Message-ID: <475E4B51.5010807@wizards.de>

Guy Korland wrote:
> We built an application in a SEDA fashion, working in stages from one
> ThreadPool to another.
> We found out that the BlockingQueue used by the ThreadPoolExecutor
> became a major concurrency killer when we start working on 4 cores
> machines and above.

Well, one could argue that that is sort of the point of a blocking queue. :)

> The thing is that we don't really need the strong FIFO behavior forced
> by all the BlockingQueue implementations available, some kind of
> fairness will be good enough.
> Any ideas?

If you just need handoff and have fixed-size (or growth-controlled) pools
then SynchronousQueue will probably work; in Mule we default to it as long
as no explicit queue size has been configured. If you really need a "true"
blocking queue try LinkedBlockingDeque, which should have less contention.
Unfortunately it is only in 1.6 or the backport, but not in 1.5..

regards
Holger

From dcholmes at optusnet.com.au  Tue Dec 11 04:15:17 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 11 Dec 2007 19:15:17 +1000
Subject: [concurrency-interest] ThreadPoolExecutor workQueue
	concurrencyissue
In-Reply-To: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEMJHJAA.dcholmes@optusnet.com.au>

How do you define "some kind of fairness" ?

When staged/pipelined designs start to saturate like that, you might see if
you can duplicate the stages to achieve greater parallelism.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Guy Korland
  Sent: Tuesday, 11 December 2007 5:40 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] ThreadPoolExecutor workQueue
concurrencyissue


  Hi,
  We built an application in a SEDA fashion, working in stages from one
ThreadPool to another.
  We found out that the BlockingQueue used by the ThreadPoolExecutor became
a major concurrency killer when we start working on 4 cores machines and
above.
  The thing is that we don't really need the strong FIFO behavior forced by
all the BlockingQueue implementations available, some kind of fairness will
be good enough.
  Any ideas?

  Thanks,
  Guy

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071211/d3844f91/attachment.html 

From gkorland at gmail.com  Tue Dec 11 04:33:46 2007
From: gkorland at gmail.com (Guy Korland)
Date: Tue, 11 Dec 2007 11:33:46 +0200
Subject: [concurrency-interest] ThreadPoolExecutor workQueue
	concurrencyissue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEMJHJAA.dcholmes@optusnet.com.au>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEMJHJAA.dcholmes@optusnet.com.au>
Message-ID: <79be5fa30712110133l59724a24m4d7dd18a755101ad@mail.gmail.com>

David,

I didn't defined "some kind of fairness" yet, but I guess that "no more than
X tasks can pass this a task" or "task Y arrived after more than X mills
won't pass another.".

>duplicate the stages to achieve greater parallelism
Seems like a waste to me and might cause from time to time that a task will
wait in the queue even though that the other queue is empty.

Guy Korland

On Dec 11, 2007 11:15 AM, David Holmes <dcholmes at optusnet.com.au> wrote:

>  How do you define "some kind of fairness" ?
>
> When staged/pipelined designs start to saturate like that, you might see
> if you can duplicate the stages to achieve greater parallelism.
>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Guy Korland
> *Sent:* Tuesday, 11 December 2007 5:40 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] ThreadPoolExecutor workQueue
> concurrencyissue
>
> Hi,
> We built an application in a SEDA fashion, working in stages from one
> ThreadPool to another.
> We found out that the BlockingQueue used by the ThreadPoolExecutor became
> a major concurrency killer when we start working on 4 cores machines and
> above.
> The thing is that we don't really need the strong FIFO behavior forced by
> all the BlockingQueue implementations available, some kind of fairness will
> be good enough.
> Any ideas?
>
> Thanks,
> Guy
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071211/ec4158b3/attachment.html 

From dcholmes at optusnet.com.au  Tue Dec 11 05:04:58 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 11 Dec 2007 20:04:58 +1000
Subject: [concurrency-interest] ThreadPoolExecutor
	workQueueconcurrencyissue
In-Reply-To: <79be5fa30712110133l59724a24m4d7dd18a755101ad@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEMKHJAA.dcholmes@optusnet.com.au>

Your "fairness" measure sounds quite involved :)

As for being a waste ... well it all depends on the load. If the pipeline
isn't kept reasonably full it may be a waste. Its your app, and your load.
:) I was simply pointing out another dimension to the solution space.

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Guy Korland
  Sent: Tuesday, 11 December 2007 7:34 PM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] ThreadPoolExecutor
workQueueconcurrencyissue


  David,

  I didn't defined "some kind of fairness" yet, but I guess that "no more
than X tasks can pass this a task" or "task Y arrived after more than X
mills won't pass another.".

  >duplicate the stages to achieve greater parallelism
  Seems like a waste to me and might cause from time to time that a task
will wait in the queue even though that the other queue is empty.

  Guy Korland


  On Dec 11, 2007 11:15 AM, David Holmes <dcholmes at optusnet.com.au> wrote:

    How do you define "some kind of fairness" ?

    When staged/pipelined designs start to saturate like that, you might see
if you can duplicate the stages to achieve greater parallelism.

    David Holmes
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Guy Korland
      Sent: Tuesday, 11 December 2007 5:40 PM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] ThreadPoolExecutor workQueue
concurrencyissue


      Hi,
      We built an application in a SEDA fashion, working in stages from one
ThreadPool to another.
      We found out that the BlockingQueue used by the ThreadPoolExecutor
became a major concurrency killer when we start working on 4 cores machines
and above.
      The thing is that we don't really need the strong FIFO behavior forced
by all the BlockingQueue implementations available, some kind of fairness
will be good enough.
      Any ideas?

      Thanks,
      Guy



-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071211/25c1c975/attachment-0001.html 

From osvaldo at visionnaire.com.br  Tue Dec 11 06:28:49 2007
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Tue, 11 Dec 2007 08:28:49 -0300
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEMHHJAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCMEMHHJAA.dcholmes@optusnet.com.au>
Message-ID: <475E7471.6030705@visionnaire.com.br>

David Holmes escreveu:
> David Gallardo writes:
>   
>> ++ is not atomic; while it may effectively be so on a single processor
>> machine, this is not the case on multiprocessor machines.
>>     
>
> It isn't the case on single processor machines either. ++ is
> read-modify-write sequence and a thread can be preempted at any point in the
> sequence.
>
> ++ is just syntatic short-hand. Write it out in full and you'd never expect
> it to be atomic.
>
>   
Perhaps the problem is that on CISC platforms like the over-popular x86, 
this /can/ be compiled down to a single instruction that does the fetch, 
increment and store on a memory address operand. People get used to 
this, they often read assembly output from compilers and see a single 
pretty, atomic instruction like INC DWORD PTR  [EBX], and expect this to 
be the rule - "it's atomic in practice". The problems is, it's not a 
portable assumption. And even in the platforms that allow this code 
generation, I'd expect the best optimizers to often /not/ do it, for 
example because they see that a new read is unnecessary on a previously 
used field, or the write can be delayed (e.g. if the increments are 
inside a loop this would provide a huge boost). I wonder, though, if any 
optimizers that could do that avoid it - giving more priority to perform 
an atomic increment - just to compensate for buggy application code?...

A+
Osvaldo
> Cheers,
> David Holmes
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>   


-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071211/7255e237/attachment.html 

From jseigh_cp00 at xemaps.com  Tue Dec 11 06:11:45 2007
From: jseigh_cp00 at xemaps.com (Joseph Seigh)
Date: Tue, 11 Dec 2007 06:11:45 -0500
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
 issue
In-Reply-To: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
Message-ID: <475E7071.604@xemaps.com>

Guy Korland wrote:
> Hi,
> We built an application in a SEDA fashion, working in stages from one 
> ThreadPool to another.
> We found out that the BlockingQueue used by the ThreadPoolExecutor 
> became a major concurrency killer when we start working on 4 cores 
> machines and above.
> The thing is that we don't really need the strong FIFO behavior forced 
> by all the BlockingQueue implementations available, some kind of 
> fairness will be good enough.
> Any ideas?
>

I made a blocking queue out of a fast semaphore and 
ConcurrentLinkedQueue which did pretty
good on a single core anyway.   Code for fast semaphore is here.
http://altair.cs.oswego.edu/pipermail/concurrency-interest/2007-April/003866.html

You could implement a BlockingQueue and try plugging that in.

--
Joe Seigh




From dl at cs.oswego.edu  Tue Dec 11 06:14:25 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 11 Dec 2007 06:14:25 -0500
Subject: [concurrency-interest] jsr166y bug - NPE sorting a ParallelArray
In-Reply-To: <5C73106C-6AEF-47C0-B44F-DA0804FBAB09@twistedbanana.demon.co.uk>
References: <5C73106C-6AEF-47C0-B44F-DA0804FBAB09@twistedbanana.demon.co.uk>
Message-ID: <475E7111.1090809@cs.oswego.edu>

Mark Mahieu wrote:
> Hi,
> 
> 
> I believe I've tripped over a bug calling ParallelArray.sort() - 

>        new FJSorter<T>(cmp, a, w, origin+q, q,   g),
should be
>        new FJSorter<T>(cmp, a, w, origin+q, h-q,   g),
etc

Yes; thanks! Somehow that crept in while adapting the dl.u.c
FJTask version of sort. Sorry. Fixed in updated CVS.

BTW, I'm still sitting on some of the other changes in
ParallelArray mentioned a few months ago. It's likely
that ParallelArray will support List API, which is
a slightly marginal call, but seems worthwhile.

-Doug


From dl at cs.oswego.edu  Tue Dec 11 06:23:07 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 11 Dec 2007 06:23:07 -0500
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
 issue
In-Reply-To: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
Message-ID: <475E731B.6040603@cs.oswego.edu>

Guy Korland wrote:
> Hi,
> We built an application in a SEDA fashion, working in stages from one 
> ThreadPool to another.
> We found out that the BlockingQueue used by the ThreadPoolExecutor 
> became a major concurrency killer when we start working on 4 cores 
> machines and above.
> The thing is that we don't really need the strong FIFO behavior forced 
> by all the BlockingQueue implementations available, some kind of 
> fairness will be good enough.
> Any ideas?
> 

You might consider trying LinkedTransferQueue that is out
in preliminary form in jsr166y. It eases up guarantees
to merely promise FIFOness wrt any given producer. Internally,
it extends some of the ideas from the java6 overhaul of
SynchronousQueue, and tends to have better throughput
under contention than LinkedBlockingQueue. In other words,
it was designed exactly to solve the problems you are seeing.
Javadoc at:
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
It is available in the jar for jsr166y listed at
http://gee.cs.oswego.edu/dl/concurrency-interest/index.html


-Doug

From gkorland at gmail.com  Tue Dec 11 15:57:32 2007
From: gkorland at gmail.com (Guy Korland)
Date: Tue, 11 Dec 2007 22:57:32 +0200
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
	issue
In-Reply-To: <475E731B.6040603@cs.oswego.edu>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
	<475E731B.6040603@cs.oswego.edu>
Message-ID: <79be5fa30712111257r624feb01qb18c73384ef59c74@mail.gmail.com>

Thanks for the hint, I'll try it ASAP.
BTW, one small question can you explain the reason behind the
PaddeAtomicReference?

Guy

On Dec 11, 2007 1:23 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> Guy Korland wrote:
> > Hi,
> > We built an application in a SEDA fashion, working in stages from one
> > ThreadPool to another.
> > We found out that the BlockingQueue used by the ThreadPoolExecutor
> > became a major concurrency killer when we start working on 4 cores
> > machines and above.
> > The thing is that we don't really need the strong FIFO behavior forced
> > by all the BlockingQueue implementations available, some kind of
> > fairness will be good enough.
> > Any ideas?
> >
>
> You might consider trying LinkedTransferQueue that is out
> in preliminary form in jsr166y. It eases up guarantees
> to merely promise FIFOness wrt any given producer. Internally,
> it extends some of the ideas from the java6 overhaul of
> SynchronousQueue, and tends to have better throughput
> under contention than LinkedBlockingQueue. In other words,
> it was designed exactly to solve the problems you are seeing.
> Javadoc at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
> It is available in the jar for jsr166y listed at
> http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
>
>
> -Doug
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071211/c87f03db/attachment.html 

From dcholmes at optusnet.com.au  Tue Dec 11 18:09:45 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 12 Dec 2007 09:09:45 +1000
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <475E7471.6030705@visionnaire.com.br>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEMLHJAA.dcholmes@optusnet.com.au>

Hi Osvaldo,

Taking a simple example:

   int x;  // field

    public inc() { x++; }

the bytecode generated by javac is:

public void inc();
  Code:
   0:   aload_0
   1:   dup
   2:   getfield        #2; //Field x:I
   5:   iconst_1
   6:   iadd
   7:   putfield        #2; //Field x:I
   10:  return

}

And you can see that there nothing atomic in that. Trying to recognize that
the above might be replaced by a single atomic assembly instruction is not a
worthwhile "optimization":
a) if the field is not accessed concurrently then there is no need for the
atomic update, and atomic instructions have a cost in terms being atomic, so
such a change would actually degrade performance;
b) if the field is accessed concurrently then either:
   i) there is synchronization protecting the field - in which case we're in
the same boat as (a), the atomic is unnecessary and expensive.; or
   ii) there is no sync, so the code is broken anyway and making this atomic
is unlikely to actually make the overall program correct.

Hence no point even attempting such an "optimization". :)

Cheers,
David

-----Original  Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Osvaldo
Pinali Doederlein
Sent: Tuesday, 11 December 2007 9:29 PM
To: dholmes at ieee.org
Cc: Concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] How bad can volatile long++ be?


  David Holmes escreveu:
David Gallardo writes:
  ++ is not atomic; while it may effectively be so on a single processor
machine, this is not the case on multiprocessor machines.

It isn't the case on single processor machines either. ++ is
read-modify-write sequence and a thread can be preempted at any point in the
sequence.

++ is just syntatic short-hand. Write it out in full and you'd never expect
it to be atomic.

  Perhaps the problem is that on CISC platforms like the over-popular x86,
this can be compiled down to a single instruction that does the fetch,
increment and store on a memory address operand. People get used to this,
they often read assembly output from compilers and see a single pretty,
atomic instruction like INC DWORD PTR  [EBX], and expect this to be the
rule - "it's atomic in practice". The problems is, it's not a portable
assumption. And even in the platforms that allow this code generation, I'd
expect the best optimizers to often not do it, for example because they see
that a new read is unnecessary on a previously used field, or the write can
be delayed (e.g. if the increments are inside a loop this would provide a
huge boost). I wonder, though, if any optimizers that could do that avoid
it - giving more priority to perform an atomic increment - just to
compensate for buggy application code?...

  A+
  Osvaldo

Cheers,
David Holmes

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest




--
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071212/c0cfaefd/attachment.html 

From dl at cs.oswego.edu  Tue Dec 11 19:33:07 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 11 Dec 2007 19:33:07 -0500
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
 issue
In-Reply-To: <79be5fa30712111257r624feb01qb18c73384ef59c74@mail.gmail.com>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>	
	<475E731B.6040603@cs.oswego.edu>
	<79be5fa30712111257r624feb01qb18c73384ef59c74@mail.gmail.com>
Message-ID: <475F2C43.9010607@cs.oswego.edu>

Guy Korland wrote:
> 
> BTW, one small question can you explain the reason behind the 
> PaddeAtomicReference?
> 

The Exchanger internal documentation has a better description
of rationale -- see
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/Exchanger.java?view=log)

But the main idea is that when you are spreading memory contention out
from a single location, the last thing you want is to use
to another location on the same cache line as the first --
you'll get the same amount of memory contention. The padded
versions just add some filler fields to avoid this. They lead
to a very noticeable improvement on some machines, and are used
sparingly enough that they don't noticeably impact footprint.

-Doug


From gkorland at gmail.com  Wed Dec 12 01:49:27 2007
From: gkorland at gmail.com (Guy Korland)
Date: Wed, 12 Dec 2007 08:49:27 +0200
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
	issue
In-Reply-To: <475F2C43.9010607@cs.oswego.edu>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>
	<475E731B.6040603@cs.oswego.edu>
	<79be5fa30712111257r624feb01qb18c73384ef59c74@mail.gmail.com>
	<475F2C43.9010607@cs.oswego.edu>
Message-ID: <79be5fa30712112249j3b83fc6dld0a16f630f61a79@mail.gmail.com>

Thanks!

I suspected this is reason, in fact we saw this phenomena in another
internal concurrent data structure we developed, but weren't sure how to
resolve it.

Guy
  On Dec 12, 2007 2:33 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> Guy Korland wrote:
> >
> > BTW, one small question can you explain the reason behind the
> > PaddeAtomicReference?
> >
>
> The Exchanger internal documentation has a better description
> of rationale -- see
>
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/Exchanger.java?view=log
> )
>
> But the main idea is that when you are spreading memory contention out
> from a single location, the last thing you want is to use
> to another location on the same cache line as the first --
> you'll get the same amount of memory contention. The padded
> versions just add some filler fields to avoid this. They lead
> to a very noticeable improvement on some machines, and are used
> sparingly enough that they don't noticeably impact footprint.
>
> -Doug
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071212/ed775650/attachment.html 

From online at stolsvik.com  Wed Dec 12 03:43:50 2007
From: online at stolsvik.com (=?UTF-8?B?RW5kcmUgU3TDuGxzdmlr?=)
Date: Wed, 12 Dec 2007 09:43:50 +0100
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
 issue
In-Reply-To: <475F2C43.9010607@cs.oswego.edu>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>		<475E731B.6040603@cs.oswego.edu>	<79be5fa30712111257r624feb01qb18c73384ef59c74@mail.gmail.com>
	<475F2C43.9010607@cs.oswego.edu>
Message-ID: <475F9F46.4050201@Stolsvik.com>

Doug Lea wrote:
> Guy Korland wrote:
>> BTW, one small question can you explain the reason behind the 
>> PaddeAtomicReference?
>>
> 
> The Exchanger internal documentation has a better description
> of rationale -- see
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/Exchanger.java?view=log)

This might be a bit off-topic question, but isn't it unfortunate that 
this class stores the number of CPUs in a static, final?

I thought that one was supposed be able to dynamically scale this number 
- allocate a different set of processors to different processes over 
time (e.g. when you "stick" a processor onto two of four processors). 
Checking the JavaDoc, it indeed states:
   "This value may change during a particular invocation of the virtual 
machine. Applications that are sensitive to the number of available 
processors should therefore occasionally poll this property and adjust 
their resource usage appropriately."

I guess a response will be that it won't matter much - but nevertheless, 
this might end up being one of those gotchas when you have 64 
processors, start a process, and then scale it back heavily to only run 
on 2 processors. Or the other case - you start out with very few 
processors, and then scale it way up because of demand..

The static final-ness of the field (and further its internal static 
scaling of structures) means that even though you create it again if the 
available processors change, the new instance won't honor the new number.

It just hit me when reading that first line of code - that this could 
seem like one of those java internals that you will end up 
special-handling in years to come.

Endre.

From dl at cs.oswego.edu  Wed Dec 12 07:02:42 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 12 Dec 2007 07:02:42 -0500
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
 issue
In-Reply-To: <475F9F46.4050201@Stolsvik.com>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>		<475E731B.6040603@cs.oswego.edu>	<79be5fa30712111257r624feb01qb18c73384ef59c74@mail.gmail.com>	<475F2C43.9010607@cs.oswego.edu>
	<475F9F46.4050201@Stolsvik.com>
Message-ID: <475FCDE2.8030408@cs.oswego.edu>

Endre St?lsvik wrote:
> 
> This might be a bit off-topic question, but isn't it unfortunate that 
> this class stores the number of CPUs in a static, final?
> 

Maybe. But all uses of this in j.u.c code are the forms:
1. Providing overridable defaults (as in new ForkJoinPool()).
2. Providing initial internal sizing or spinning constants
that affect footprint and performance but not correctness
(as in Exchanger).

The current cases of second form mainly distinguish
uniprocessors vs multiprocessors. So if you have a lot
of processors on first use of, for example, an Exchanger,
and then shut all but one of them down, then you will get
worse performance that you'd get if you started with only
one. And vice versa. Changes from say 4 to 8 processors
have almost no effect on these tunings though.
This doesn't seem like an important enough practical
problem to warrant slowing down main functionality
in all cases in order to dynamically adapt.

> It just hit me when reading that first line of code - that this could 
> seem like one of those java internals that you will end up 
> special-handling in years to come.
> 

You are right in general -- there are a few aspects
of some j.u.c. classes that are uncomfortably tied to typical
platform and usage characteristics; for example, using exactly
16 segments in ConcurrentHashMap. These will surely change
over time. I think it just comes with the territory.

-Doug



From osvaldo at visionnaire.com.br  Wed Dec 12 08:54:53 2007
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Wed, 12 Dec 2007 10:54:53 -0300
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEMLHJAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCKEMLHJAA.dcholmes@optusnet.com.au>
Message-ID: <475FE82D.20200@visionnaire.com.br>

Hi David,

I am aware of the produced bytecode and other issues you mention. So I 
think I was not very clear in my comment, so trying again:
1) Code like the inc() below can be compiled down to a single 
instruction, which (at least on singleprocessors as several people 
pointed) is an atomic instruction. (*) Even on 
multicore/multiprocessors, I think the instruction will be atomic, 
provided that the long value is aligned to 8 bytes, which typically 
(always?) has the nice side effect of making sure the value doesn't 
straddle cache-line boundaries, so, no atomicity problems with the 
propagation of writes through the memory hierararchy.
2) But a good JIT compiler may see that this single instruction is not 
the optimal compilation, it can get better performance with separate 
instructions for the load, inc, and store.
3) Now, the confusing part of my argument is: perhaps the JIT compiler 
knows how to do the optimizations that result in separate instructions, 
but decides not to do it, preferring instead to emit single instructions 
for ++ and -- just because they happen to be atomic (in singleprocessors 
anyway) and there are enough applications with concurrency bugs.
I am probably being paranoid, I don't think any major optimizer would 
make this kind of tradeoff between performance and support for broken 
code, at least in scenarios like concurrency where "broken" usually 
means "may fail in rare circumstances" - in other scenarios, where 
"broken" means "will always fail", it's well known that Sun and other 
JVM implementers often hold enhancements and even bugfixes because they 
know of important applications which would break because they're buggy.

(*) BTW, most JVMs can and will run method in interpreted mode many 
times before doing native compilation. So the code is buggy anyway as it 
may fail in interpreted runs. ANY safety assumption that depends on JIT 
compilation is flawed, but people often don't realize that, because the 
chances to catch the error are abismally low - you gotta be really 
unlucky to hit that dreadful thread/CPU interleaving with a few hundreds 
or thousands of interpreted executions of a buggy method.

A+
Osvaldo
> Hi Osvaldo,
>  
> Taking a simple example:
>  
>    int x;  // field
>  
>     public inc() { x++; }
>  
> the bytecode generated by javac is:
>  
> public void inc();
>   Code:
>    0:   aload_0
>    1:   dup
>    2:   getfield        #2; //Field x:I
>    5:   iconst_1
>    6:   iadd
>    7:   putfield        #2; //Field x:I
>    10:  return
>  
> }
>  
> And you can see that there nothing atomic in that. Trying to recognize 
> that the above might be replaced by a single atomic assembly 
> instruction is not a worthwhile "optimization":
> a) if the field is not accessed concurrently then there is no need for 
> the atomic update, and atomic instructions have a cost in terms being 
> atomic, so such a change would actually degrade performance;
> b) if the field is accessed concurrently then either:
>    i) there is synchronization protecting the field - in which case 
> we're in the same boat as (a), the atomic is unnecessary and 
> expensive.; or
>    ii) there is no sync, so the code is broken anyway and making this 
> atomic is unlikely to actually make the overall program correct.
>  
> Hence no point even attempting such an "optimization". :)
>  
> Cheers,
> David
>  
> -----Original  Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of 
> *Osvaldo Pinali Doederlein
> *Sent:* Tuesday, 11 December 2007 9:29 PM
> *To:* dholmes at ieee.org
> *Cc:* Concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] How bad can volatile long++ be?
>
>     David Holmes escreveu:
>>     David Gallardo writes:
>>       
>>>     ++ is not atomic; while it may effectively be so on a single processor
>>>     machine, this is not the case on multiprocessor machines.
>>>         
>>
>>     It isn't the case on single processor machines either. ++ is
>>     read-modify-write sequence and a thread can be preempted at any point in the
>>     sequence.
>>
>>     ++ is just syntatic short-hand. Write it out in full and you'd never expect
>>     it to be atomic.
>>
>>       
>     Perhaps the problem is that on CISC platforms like the
>     over-popular x86, this /can/ be compiled down to a single
>     instruction that does the fetch, increment and store on a memory
>     address operand. People get used to this, they often read assembly
>     output from compilers and see a single pretty, atomic instruction
>     like INC DWORD PTR  [EBX], and expect this to be the rule - "it's
>     atomic in practice". The problems is, it's not a portable
>     assumption. And even in the platforms that allow this code
>     generation, I'd expect the best optimizers to often /not/ do it,
>     for example because they see that a new read is unnecessary on a
>     previously used field, or the write can be delayed (e.g. if the
>     increments are inside a loop this would provide a huge boost). I
>     wonder, though, if any optimizers that could do that avoid it -
>     giving more priority to perform an atomic increment - just to
>     compensate for buggy application code?...
>
>     A+
>     Osvaldo
>>     Cheers,
>>     David Holmes
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at altair.cs.oswego.edu
>>     http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>       
>
>
>     -- 
>     -----------------------------------------------------------------------
>     Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
>     osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
>     Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>         
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>   


-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071212/149ca71f/attachment.html 

From online at stolsvik.com  Wed Dec 12 08:28:25 2007
From: online at stolsvik.com (=?UTF-8?B?RW5kcmUgU3TDuGxzdmlr?=)
Date: Wed, 12 Dec 2007 14:28:25 +0100
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
 issue
In-Reply-To: <475FCDE2.8030408@cs.oswego.edu>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>		<475E731B.6040603@cs.oswego.edu>	<79be5fa30712111257r624feb01qb18c73384ef59c74@mail.gmail.com>	<475F2C43.9010607@cs.oswego.edu>
	<475F9F46.4050201@Stolsvik.com> <475FCDE2.8030408@cs.oswego.edu>
Message-ID: <475FE1F9.3000603@Stolsvik.com>

Doug Lea wrote:

> Changes from say 4 to 8 processors
> have almost no effect on these tunings though.
> This doesn't seem like an important enough practical
> problem to warrant slowing down main functionality
> in all cases in order to dynamically adapt.

I get the general point about not slowing the general case down due to 
fringe situations. However, letting this be decided at instantiation 
time instead of in a static final would at least make it possible for 
application code to adapt to changing situations by recreating the 
setup. The current code fully removes that possibility.

And 4 to 8 might not be a big problem, but 2 to 64, or 64 to 2 could be, 
though?

> 
>> It just hit me when reading that first line of code - that this could 
>> seem like one of those java internals that you will end up 
>> special-handling in years to come.
>>
> 
> You are right in general -- there are a few aspects
> of some j.u.c. classes that are uncomfortably tied to typical
> platform and usage characteristics; for example, using exactly
> 16 segments in ConcurrentHashMap. These will surely change
> over time. I think it just comes with the territory.

As long as it is possible to change the constant 16 in CHM in future 
releases without breaking existing code, and existing understanding, I 
don't really see that particular example as problematic.

When I was talking about special-handling in years to come, I was 
thinking more of the user side: I've personally been bitten quite 
annoyingly (as in "wasted lots of time on completely unnecessary bug 
hunting and kludge-making") by ThreadLocals' that wont let go of users' 
ClassLoaders, DriverManager's retention of drivers, 
Introspector.flushCaches() and so on. Also, you have the discussion of 
the AWT EDT - how and when to construct GUI objects - for which 
recommendations (or requirements) have changed several times, and which 
have been a problem pretty much since Java was born.
   All things that just wasn't obvious in any way, gave lots of 
headaches (and maybe missed Java enthusiasts!), and that happened due to 
some small implementation detail years before.

My point is really that I can envision there will be threads in the 
future complaining about "Oh my god - why is that damn class fetching as 
a constant a number that in the javadoc is clearly stated as being 
dynamic???"..

4 processors will be normal on desktops within two years. Uniprocessor 
desktops will be far apart rather soon ("soon" are of course slightly 
relative here - but think 5 years). In some time, maybe 32 CPUs on a 
desktop system will be normal, and OSes might start controlling 
processes not only on nice levels, but on the number of CPUs they are 
allowed to run on..!


As a philosophical question, and pretty much off on a tangent: How long 
time do you think Java will live? Because if the answer is "a real good 
long time", then things should (obviously) strive to be rather perfect 
on the first time round, APIs being adjustable to future situations, and 
easy to learn and use - striving to making future API additions unnecessary.
   The j.u and of course in particular j.u.c are pretty intense already, 
learning curve wise, even usage wise to a degree. The same have 
obviously started to go for "Java itself" - witness the different 
frameworks popping up trying to be a shell over Java.
   Looking at Java these days, I can surely understand the argument 
about killing Java by making the recruitment criteria impossible: Java 
could end up being the best platform there is for many tasks, but it is 
just way to hard to learn.

Endre.

From dl at cs.oswego.edu  Wed Dec 12 10:42:43 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 12 Dec 2007 10:42:43 -0500
Subject: [concurrency-interest] ThreadPoolExecutor workQueue concurrency
 issue
In-Reply-To: <475FE1F9.3000603@Stolsvik.com>
References: <79be5fa30712102339o5eeea65do7fca6e37c90113ae@mail.gmail.com>		<475E731B.6040603@cs.oswego.edu>	<79be5fa30712111257r624feb01qb18c73384ef59c74@mail.gmail.com>	<475F2C43.9010607@cs.oswego.edu>	<475F9F46.4050201@Stolsvik.com>
	<475FCDE2.8030408@cs.oswego.edu> <475FE1F9.3000603@Stolsvik.com>
Message-ID: <47600173.6080904@cs.oswego.edu>

Endre St?lsvik wrote:
> Doug Lea wrote:
> 
>> Changes from say 4 to 8 processors
>> have almost no effect on these tunings though.
>> This doesn't seem like an important enough practical
>> problem to warrant slowing down main functionality
>> in all cases in order to dynamically adapt.
> 
> I get the general point about not slowing the general case down due to 
> fringe situations. However, letting this be decided at instantiation 
> time instead of in a static final would at least make it possible for 
> application code to adapt to changing situations by recreating the 
> setup. The current code fully removes that possibility.
> 
> And 4 to 8 might not be a big problem, but 2 to 64, or 64 to 2 could be, 
> though?
> 

Probably not, but could you try this and let me know how it goes?

In other words, if I thought that it would lead to serious problems,
I would not have done it this way. But no one seems to
actually turn processors on and off during execution of
systems with highly contended sync code. At least no one does
it and tells me about it. So I don't really know.


> When I was talking about special-handling in years to come, I was 
> thinking more of the user side: I've personally been bitten quite 
> annoyingly (as in "wasted lots of time on completely unnecessary bug 
> hunting and kludge-making") by ThreadLocals' that wont let go of users' 
> ClassLoaders,

A side note; There is some progress on this front. GC folks are
looking into support for a variant of "ephemerons" (google it)
that can be used to avoid uncollectable weak key-value chains.
If/when available (as a new form of java.lang.ref denoting
refs that don't contribute to the reachability of Weak refs),
then ThreadLocal, WeakHashMap, and planned follow-ons can be to
changed to use them; finally really solving this problem.

> 
> As a philosophical question, and pretty much off on a tangent: How long 
> time do you think Java will live? 

Much too hard for me to answer. :-) But those of us who write them
do think about core library classes as never actually being finished.

-Doug




From hans.boehm at hp.com  Wed Dec 12 14:17:41 2007
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 12 Dec 2007 19:17:41 +0000
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <475FE82D.20200@visionnaire.com.br>
References: <NFBBKALFDCPFIDBNKAPCKEMLHJAA.dcholmes@optusnet.com.au>
	<475FE82D.20200@visionnaire.com.br>
Message-ID: <EB8E0FF63AB2414693DB20D50E863AE80908BC12@G3W0634.americas.hpqcorp.net>

________________________________
From: Osvaldo Pinali Doederlein

I am aware of the produced bytecode and other issues you mention. So I think I was not very clear in my comment, so trying again:
1) Code like the inc() below can be compiled down to a single instruction, which (at least on singleprocessors as several people pointed) is an atomic instruction. (*) Even on multicore/multiprocessors, I think the instruction will be atomic, provided that the long value is aligned to 8 bytes, which typically (always?) has the nice side effect of making sure the value doesn't straddle cache-line boundaries, so, no atomicity problems with the propagation of writes through the memory hierararchy.

On X86, an increment instruction is not atomic on a multiprocessor (or presumably with "hyperthreading") unless it has a "lock" prefix.   The lock prefix normally adds SUBSTANTIAL (order of a dozen to hundreds of cycles) overhead, and is unlikely to be generated without good reason.

Hans
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071212/11840c5b/attachment.html 

From osvaldo at visionnaire.com.br  Wed Dec 12 16:51:15 2007
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Wed, 12 Dec 2007 18:51:15 -0300
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <EB8E0FF63AB2414693DB20D50E863AE80908BC12@G3W0634.americas.hpqcorp.net>
References: <NFBBKALFDCPFIDBNKAPCKEMLHJAA.dcholmes@optusnet.com.au>
	<475FE82D.20200@visionnaire.com.br>
	<EB8E0FF63AB2414693DB20D50E863AE80908BC12@G3W0634.americas.hpqcorp.net>
Message-ID: <476057D3.6060705@visionnaire.com.br>

Hi,
>
>     ------------------------------------------------------------------------
>     *From:* Osvaldo Pinali Doederlein
>
>     I am aware of the produced bytecode and other issues you mention.
>     So I think I was not very clear in my comment, so trying again:
>     1) Code like the inc() below can be compiled down to a single
>     instruction, which (at least on singleprocessors as several people
>     pointed) is an atomic instruction. (*) Even on
>     multicore/multiprocessors, I think the instruction will be atomic,
>     provided that the long value is aligned to 8 bytes, which
>     typically (always?) has the nice side effect of making sure the
>     value doesn't straddle cache-line boundaries, so, no atomicity
>     problems with the propagation of writes through the memory
>     hierararchy. 
>      
>
> On X86, an increment instruction is not atomic on a multiprocessor (or 
> presumably with "hyperthreading") unless it has a "lock" prefix.   The 
> lock prefix normally adds SUBSTANTIAL (order of a dozen to hundreds of 
> cycles) overhead, and is unlikely to be generated without good reason.
>  
> Hans
Sorry, when I wrote that comment I was thinking too hard about 
Consistency, not Atomicity. I meant that the alignment should prevent, 
for example, that thread 1 writes 0x0000000000000000 and thread 2 
concurrently writes 0xFFFFFFFFFFFFFFFF, and the result in memory is 
something like 0x00000000FFFFFFFF due to bad luck in cache-RAM sync'ing; 
but that's all (but correct me again if I am wrong even in this assumption).

A+
Osvaldo

-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071212/d47b697d/attachment.html 

From dl at cs.oswego.edu  Wed Dec 12 16:10:13 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 12 Dec 2007 16:10:13 -0500
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <475AA32D.9080307@cs.oswego.edu>
References: <1197058886.079504.6494.nullmailer@home35>
	<475AA32D.9080307@cs.oswego.edu>
Message-ID: <47604E35.5080000@cs.oswego.edu>


I forgot to mention as another follow-up to that discussion --
Ulrich Drepper has a good (and very long) introduction
to caches among other lower-level aspects of memory at
   part 1: http://lwn.net/Articles/250967/
   part 2: http://lwn.net/Articles/252125/
See links near end of part one for parts 3-9.
This is focused mainly on C programming on commodity MP
and multicore X86s but relevant to most systems.
Well worth reading if you want a good sense of memory
systems underlying execution of your code, but oddly
has little to say about fences/barriers.

-Doug



From elihusmails at gmail.com  Wed Dec 12 16:26:26 2007
From: elihusmails at gmail.com (Mark)
Date: Wed, 12 Dec 2007 16:26:26 -0500
Subject: [concurrency-interest] Concurrent Programming in Java, 3rd Edition
Message-ID: <9f066ee90712121326j5a73f485x597420d096f1fd5e@mail.gmail.com>

Does anyone know where I can find this book?  Amazon sent me a notice that
they cannot get copies of it, so I am not sure if there is a problem with
the publisher or the book is just not available.

Doug, any insight on this?

Thanks
Mark Webb

-- 
--------------------------------
The adjuration to be "normal" seems shockingly repellent to me; I see
neither hope nor comfort in sinking to that low level. I think it is
ignorance that makes people think of abnormality only with horror and allows
them to remain undismayed at the proximity of "normal" to average and
mediocre. For surely anyone who achieves anything is, essentially, abnormal.
    Dr. Karl Menninger
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071212/584a52a4/attachment.html 

From dgallardo at gmail.com  Wed Dec 12 16:30:10 2007
From: dgallardo at gmail.com (David Gallardo)
Date: Wed, 12 Dec 2007 13:30:10 -0800
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <476057D3.6060705@visionnaire.com.br>
References: <NFBBKALFDCPFIDBNKAPCKEMLHJAA.dcholmes@optusnet.com.au>
	<475FE82D.20200@visionnaire.com.br>
	<EB8E0FF63AB2414693DB20D50E863AE80908BC12@G3W0634.americas.hpqcorp.net>
	<476057D3.6060705@visionnaire.com.br>
Message-ID: <24dec92b0712121330l73cda6bbvefd72fd564e03cc7@mail.gmail.com>

 Goetz et al., in Java Concurrency in Practice, as this to say about that:

"When a thread reads a variable without synchronization, it may see a
stale value, but at least it sees a value that was actually placed
there by some thread rather than some random value. This safety
guarantee is called out-of-thin-air safety.

"Out-of-thin-air safety applies to all variables, with one exception:
64-bit numeric variables (double and long) that are not declared
volatile (see Section 3.1.4). The Java Memory Model requires fetch
and store operations to be atomic, but for nonvolatile long and
double variables, the JVM is permitted to treat a 64-bit read or
write as two separate 32-bit operations. If the reads and writes
occur in different threads, it is therefore possible to read a
nonvolatile long and get back the high 32 bits of one value and the
low 32 bits of another.[3] Thus, even if you don't care about stale
values, it is not safe to use shared mutable long and double
variables in multithreaded programs unless they are declared volatile
or guarded by a lock."


On Dec 12, 2007 1:51 PM, Osvaldo Pinali Doederlein
<osvaldo at visionnaire.com.br> wrote:
>
>  Hi,
>
>
>
>  ________________________________
>  From: Osvaldo Pinali Doederlein
>
>
>  I am aware of the produced bytecode and other issues you mention. So I
> think I was not very clear in my comment, so trying again:
>  1) Code like the inc() below can be compiled down to a single instruction,
> which (at least on singleprocessors as several people pointed) is an atomic
> instruction. (*) Even on multicore/multiprocessors, I think the instruction
> will be atomic, provided that the long value is aligned to 8 bytes, which
> typically (always?) has the nice side effect of making sure the value
> doesn't straddle cache-line boundaries, so, no atomicity problems with the
> propagation of writes through the memory hierararchy.
>
> On X86, an increment instruction is not atomic on a multiprocessor (or
> presumably with "hyperthreading") unless it has a "lock" prefix.   The lock
> prefix normally adds SUBSTANTIAL (order of a dozen to hundreds of cycles)
> overhead, and is unlikely to be generated without good reason.
>
> Hans Sorry, when I wrote that comment I was thinking too hard about
> Consistency, not Atomicity. I meant that the alignment should prevent, for
> example, that thread 1 writes 0x0000000000000000 and thread 2 concurrently
> writes 0xFFFFFFFFFFFFFFFF, and the result in memory is something like
> 0x00000000FFFFFFFF due to bad luck in cache-RAM sync'ing; but that's all
> (but correct me again if I am wrong even in this assumption).
>
>  A+
>  Osvaldo
>
>  --
> -----------------------------------------------------------------------
> Osvaldo Pinali Doederlein Visionnaire Inform?tica S/A
> osvaldo at visionnaire.com.br http://www.visionnaire.com.br
> Arquiteto de Tecnologia +55 (41) 337-1000 #223
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From dl at cs.oswego.edu  Wed Dec 12 16:33:25 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 12 Dec 2007 16:33:25 -0500
Subject: [concurrency-interest] Concurrent Programming in Java,
	3rd Edition
In-Reply-To: <9f066ee90712121326j5a73f485x597420d096f1fd5e@mail.gmail.com>
References: <9f066ee90712121326j5a73f485x597420d096f1fd5e@mail.gmail.com>
Message-ID: <476053A5.2040606@cs.oswego.edu>

Mark wrote:
> Does anyone know where I can find this book?  Amazon sent me a notice 
> that they cannot get copies of it, so I am not sure if there is a 
> problem with the publisher or the book is just not available. 
> 
> Doug, any insight on this? 

There is no third edition. There were plans for it, but Java
Concurrency in Practice (http://jcip.net/) included all
of the updates to cover Java5 that CPJ3e would
have included. You are better off getting that instead.
The CPJ book is possibly better about a few underlying design
issues, but the 2nd edition suffices for that.

One of these days, maybe I'll write 3rd edition though.

-Doug




From r.samuel.klatchko at gmail.com  Wed Dec 12 16:45:09 2007
From: r.samuel.klatchko at gmail.com (R Samuel Klatchko)
Date: Wed, 12 Dec 2007 13:45:09 -0800
Subject: [concurrency-interest] Concurrent Programming in Java,
	3rd Edition
In-Reply-To: <9f066ee90712121326j5a73f485x597420d096f1fd5e@mail.gmail.com>
References: <9f066ee90712121326j5a73f485x597420d096f1fd5e@mail.gmail.com>
Message-ID: <8eddd5030712121345t3f61e0bch711ca7cfad0bdf78@mail.gmail.com>

I'm pretty sure I noticed a copy on the shelf at Stacey's
Bookstore<http://www.staceys.com/>(a wonderful independent bookstore
in San Francisco) when I was browsing
books last weekend.  If you're not near hear, they do ship as well.

samuel


On Dec 12, 2007 1:26 PM, Mark <elihusmails at gmail.com> wrote:

> Does anyone know where I can find this book?  Amazon sent me a notice that
> they cannot get copies of it, so I am not sure if there is a problem with
> the publisher or the book is just not available.
>
> Doug, any insight on this?
>
> Thanks
> Mark Webb
>
> --
> --------------------------------
> The adjuration to be "normal" seems shockingly repellent to me; I see
> neither hope nor comfort in sinking to that low level. I think it is
> ignorance that makes people think of abnormality only with horror and allows
> them to remain undismayed at the proximity of "normal" to average and
> mediocre. For surely anyone who achieves anything is, essentially, abnormal.
>
>     Dr. Karl Menninger
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071212/839b8f2a/attachment-0001.html 

From osvaldo at visionnaire.com.br  Wed Dec 12 17:50:45 2007
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Wed, 12 Dec 2007 19:50:45 -0300
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <24dec92b0712121330l73cda6bbvefd72fd564e03cc7@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCKEMLHJAA.dcholmes@optusnet.com.au>	
	<475FE82D.20200@visionnaire.com.br>	
	<EB8E0FF63AB2414693DB20D50E863AE80908BC12@G3W0634.americas.hpqcorp.net>	
	<476057D3.6060705@visionnaire.com.br>
	<24dec92b0712121330l73cda6bbvefd72fd564e03cc7@mail.gmail.com>
Message-ID: <476065C5.2060105@visionnaire.com.br>

David Gallardo escreveu:
>  Goetz et al., in Java Concurrency in Practice, as this to say about that:
>
> "When a thread reads a variable without synchronization, it may see a
> stale value, but at least it sees a value that was actually placed
> there by some thread rather than some random value. This safety
> guarantee is called out-of-thin-air safety.
>
> "Out-of-thin-air safety applies to all variables, with one exception:
> 64-bit numeric variables (double and long) that are not declared
> volatile (see Section 3.1.4). The Java Memory Model requires fetch
> and store operations to be atomic, but for nonvolatile long and
> double variables, the JVM is permitted to treat a 64-bit read or
> write as two separate 32-bit operations. If the reads and writes
> occur in different threads, it is therefore possible to read a
> nonvolatile long and get back the high 32 bits of one value and the
> low 32 bits of another.[3] Thus, even if you don't care about stale
> values, it is not safe to use shared mutable long and double
> variables in multithreaded programs unless they are declared volatile
> or guarded by a lock."
>   
Right, but the discussion was not over the JMM and formal guarantees, it 
was about "what happens in practice", remarkably in best-case scenarios 
like: the system is a singleprocessor, the JVM generates optimal native 
code, and this code is the only code that matters (ignoring interpreted 
executions) - i.e., the assumptions that (unfortunately) many 
programmers make and then decide that some code is safe at least in a 
certain platform. Which is always wrong even for specific platforms, at 
least due to the interpreted code factor.

A+
Osvaldo
>
> On Dec 12, 2007 1:51 PM, Osvaldo Pinali Doederlein
> <osvaldo at visionnaire.com.br> wrote:
>   
>>  Hi,
>>
>>
>>
>>  ________________________________
>>  From: Osvaldo Pinali Doederlein
>>
>>
>>  I am aware of the produced bytecode and other issues you mention. So I
>> think I was not very clear in my comment, so trying again:
>>  1) Code like the inc() below can be compiled down to a single instruction,
>> which (at least on singleprocessors as several people pointed) is an atomic
>> instruction. (*) Even on multicore/multiprocessors, I think the instruction
>> will be atomic, provided that the long value is aligned to 8 bytes, which
>> typically (always?) has the nice side effect of making sure the value
>> doesn't straddle cache-line boundaries, so, no atomicity problems with the
>> propagation of writes through the memory hierararchy.
>>
>> On X86, an increment instruction is not atomic on a multiprocessor (or
>> presumably with "hyperthreading") unless it has a "lock" prefix.   The lock
>> prefix normally adds SUBSTANTIAL (order of a dozen to hundreds of cycles)
>> overhead, and is unlikely to be generated without good reason.
>>
>> Hans Sorry, when I wrote that comment I was thinking too hard about
>> Consistency, not Atomicity. I meant that the alignment should prevent, for
>> example, that thread 1 writes 0x0000000000000000 and thread 2 concurrently
>> writes 0xFFFFFFFFFFFFFFFF, and the result in memory is something like
>> 0x00000000FFFFFFFF due to bad luck in cache-RAM sync'ing; but that's all
>> (but correct me again if I am wrong even in this assumption).
>>
>>  A+
>>  Osvaldo
>>
>>  --
>> -----------------------------------------------------------------------
>> Osvaldo Pinali Doederlein Visionnaire Inform?tica S/A
>> osvaldo at visionnaire.com.br http://www.visionnaire.com.br
>> Arquiteto de Tecnologia +55 (41) 337-1000 #223
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>     
>
>   


-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071212/fb8a9919/attachment.html 

From mthornton at optrak.co.uk  Wed Dec 12 17:00:45 2007
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Wed, 12 Dec 2007 22:00:45 +0000
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <24dec92b0712121330l73cda6bbvefd72fd564e03cc7@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCKEMLHJAA.dcholmes@optusnet.com.au>	<475FE82D.20200@visionnaire.com.br>	<EB8E0FF63AB2414693DB20D50E863AE80908BC12@G3W0634.americas.hpqcorp.net>	<476057D3.6060705@visionnaire.com.br>
	<24dec92b0712121330l73cda6bbvefd72fd564e03cc7@mail.gmail.com>
Message-ID: <47605A0D.7000201@optrak.co.uk>

David Gallardo wrote:
>  Goetz et al., in Java Concurrency in Practice, as this to say about that:
>
> "When a thread reads a variable without synchronization, it may see a
> stale value, but at least it sees a value that was actually placed
> there by some thread rather than some random value. This safety
> guarantee is called out-of-thin-air safety.
>
> "Out-of-thin-air safety applies to all variables, with one exception:
> 64-bit numeric variables (double and long) that are not declared
> volatile (see Section 3.1.4). The Java Memory Model requires fetch
> and store operations to be atomic, but for nonvolatile long and
> double variables, the JVM is permitted to treat a 64-bit read or
> write as two separate 32-bit operations. If the reads and writes
> occur in different threads, it is therefore possible to read a
> nonvolatile long and get back the high 32 bits of one value and the
> low 32 bits of another.[3] Thus, even if you don't care about stale
> values, it is not safe to use shared mutable long and double
> variables in multithreaded programs unless they are declared volatile
> or guarded by a lock."
>
>   
I think Osvaldo was assuming that long (or double) reads/writes would be 
atomic on a 64 bit JVM. It is certainly not true on a typical 32 bit x86 
JVM.

Mark Thornton


From gregg at cytetech.com  Wed Dec 12 17:40:00 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 12 Dec 2007 16:40:00 -0600
Subject: [concurrency-interest] How bad can volatile long++ be?
In-Reply-To: <476057D3.6060705@visionnaire.com.br>
References: <NFBBKALFDCPFIDBNKAPCKEMLHJAA.dcholmes@optusnet.com.au>
	<475FE82D.20200@visionnaire.com.br>
	<EB8E0FF63AB2414693DB20D50E863AE80908BC12@G3W0634.americas.hpqcorp.net>
	<476057D3.6060705@visionnaire.com.br>
Message-ID: <47606340.4010508@cytetech.com>

Osvaldo Pinali Doederlein wrote:
> Sorry, when I wrote that comment I was thinking too hard about 
> Consistency, not Atomicity. I meant that the alignment should prevent, 
> for example, that thread 1 writes 0x0000000000000000 and thread 2 
> concurrently writes 0xFFFFFFFFFFFFFFFF, and the result in memory is 
> something like 0x00000000FFFFFFFF due to bad luck in cache-RAM sync'ing; 
> but that's all (but correct me again if I am wrong even in this assumption).

Okay, my perspective on all of this is that these types of comments and 
assumptions are what have created really poor mappings between software and what 
hardware can and can not vs will and will not do.  The classes, concepts and 
constructs that are being generated as part of the concurrency research that 
Doug and others are doing, are finally creating a nice layer of separation 
between hardware and software which allows hardware and software realities to be 
completely separated.

The software is finally expressing exactly what its intent is, and the classes 
and JMM implementations can provide the low level details of implementation 
without the software having to worry about what those are.

There are obviously use cases which can not be distinctly programmed as 100% 
optimal use of hardware capabilities still today.  But, the things that 
multi-core, multi-path memory are meant to address are directly accessible by 
the concurrency software which is flowing out of this work, and I think it's 
time to stop talking about hardware, and only talk about exactly what we want 
the software to do, and then design classes (or JMM specs if really needed) 
which provide the manifestation of that need which can then be targeted to the 
hardware that is available at that time.

Gregg Wonderly

From espen.sommerfelt at gmail.com  Thu Dec 13 04:31:17 2007
From: espen.sommerfelt at gmail.com (Espen Sommerfelt)
Date: Thu, 13 Dec 2007 10:31:17 +0100
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <47604E35.5080000@cs.oswego.edu>
References: <1197058886.079504.6494.nullmailer@home35>
	<475AA32D.9080307@cs.oswego.edu> <47604E35.5080000@cs.oswego.edu>
Message-ID: <82c257040712130131q49d32623g4ba0b42c764aa35b@mail.gmail.com>

The full paper in PDF format can be found here:
http://udrepper.livejournal.com/19557.html

Cheers,
Espen

On Dec 12, 2007 10:10 PM, Doug Lea <dl at cs.oswego.edu> wrote:

>
> I forgot to mention as another follow-up to that discussion --
> Ulrich Drepper has a good (and very long) introduction
> to caches among other lower-level aspects of memory at
>   part 1: http://lwn.net/Articles/250967/
>   part 2: http://lwn.net/Articles/252125/
> See links near end of part one for parts 3-9.
> This is focused mainly on C programming on commodity MP
> and multicore X86s but relevant to most systems.
> Well worth reading if you want a good sense of memory
> systems underlying execution of your code, but oddly
> has little to say about fences/barriers.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071213/8e7876ae/attachment.html 

From TEREKHOV at de.ibm.com  Thu Dec 13 05:13:52 2007
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Thu, 13 Dec 2007 11:13:52 +0100
Subject: [concurrency-interest] question the about the JMM
In-Reply-To: <47604E35.5080000@cs.oswego.edu>
Message-ID: <OFF864294B.DE67B04C-ONC12573B0.0037841C-C12573B0.00383E78@de.ibm.com>

>  part 1: http://lwn.net/Articles/250967/

"... Ulrich's copy of the document ..."

http://people.redhat.com/drepper/cpumemory.pdf
("What Every Programmer Should Know About Memory")

regards
alexander.

Doug Lea <dl at cs.oswego.edu>@cs.oswego.edu on 12.12.2007 22:10:13

Sent by:    concurrency-interest-bounces at cs.oswego.edu


To:    concurrency-interest <concurrency-interest at cs.oswego.edu>
cc:
Subject:    Re: [concurrency-interest] question the about the JMM



I forgot to mention as another follow-up to that discussion --
Ulrich Drepper has a good (and very long) introduction
to caches among other lower-level aspects of memory at
   part 1: http://lwn.net/Articles/250967/
   part 2: http://lwn.net/Articles/252125/
See links near end of part one for parts 3-9.
This is focused mainly on C programming on commodity MP
and multicore X86s but relevant to most systems.
Well worth reading if you want a good sense of memory
systems underlying execution of your code, but oddly
has little to say about fences/barriers.

-Doug


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



From mthornton at optrak.co.uk  Thu Dec 13 07:03:47 2007
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Thu, 13 Dec 2007 12:03:47 +0000
Subject: [concurrency-interest] ForkJoinExecutor.invoke and
	ForkJoinTask.invoke
Message-ID: <47611FA3.9060507@optrak.co.uk>

If you call ForkJoinExecutor.invoke from within a task being run by that 
executor should it behave as ForkJoinTask.invoke?
The current behaviour is that it blocks. This means that for some tasks 
I need to provide two routes for invoking them depending on whether the 
invoker is another task or not.

Regards,
Mark Thornton


From dl at cs.oswego.edu  Thu Dec 13 07:32:03 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 13 Dec 2007 07:32:03 -0500
Subject: [concurrency-interest] ForkJoinExecutor.invoke
	and	ForkJoinTask.invoke
In-Reply-To: <47611FA3.9060507@optrak.co.uk>
References: <47611FA3.9060507@optrak.co.uk>
Message-ID: <47612643.20307@cs.oswego.edu>

Mark Thornton wrote:
> If you call ForkJoinExecutor.invoke from within a task being run by that 
> executor should it behave as ForkJoinTask.invoke?
> The current behaviour is that it blocks. This means that for some tasks 
> I need to provide two routes for invoking them depending on whether the 
> invoker is another task or not.
> 

This is a good question; thanks.
Since you are the third person I've head from
who got stuck on this, it seem worth revisiting.

The issue is whether ForkJoinPool (FJP) invoke(task) should
(as now) serve as gateway between FJ and non-FJ threads,
or whether it should check to see if caller is an FJ thread
and if so act like task.invoke().
The main visible difference is that the former blocks
on join, while the latter helps on join (i.e., may execute
that or some other task until done). But there is another
difference that some people may need to rely on:
FJP.invoke schedules tasks for execution in submission order,
while plain task.invoke executes whenever it wants to
(which turns out to always be  "immediately").

So, special-casing FJP.invoke on the basis of caller would
eliminate the ability for an FJ task to submit a new poolwide
task that would be executed when one or more worker threads
don't have existing (sub)tasks to execute. I'd still like
to support a way to do this. But perhaps it would be less
error-prone for FJP.invoke(task) to act like task.invoke() if
caller is an FJ thread, and to have another method, say
ForkJoinTask.invokeLater(task) that schedules for poolwide
submission.

Further thoughts and suggestions would be welcome.

-Doug


From tim at peierls.net  Thu Dec 13 09:02:11 2007
From: tim at peierls.net (Tim Peierls)
Date: Thu, 13 Dec 2007 09:02:11 -0500
Subject: [concurrency-interest] ForkJoinExecutor.invoke and
	ForkJoinTask.invoke
In-Reply-To: <47612643.20307@cs.oswego.edu>
References: <47611FA3.9060507@optrak.co.uk> <47612643.20307@cs.oswego.edu>
Message-ID: <63b4e4050712130602t29c6572w67ee69aba56381f6@mail.gmail.com>

On Dec 13, 2007 7:32 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> ...perhaps it would be less error-prone for FJP.invoke(task) to act like
> task.invoke() if caller is an FJ thread, and to have another method, say
> ForkJoinTask.invokeLater(task) that schedules for poolwide submission.


FJTask.invoke is really fork + join, but possibly more efficient. Renaming
it to forkJoin() would remove the potential confusion with FJP.invoke() and
flatten the learning curve a bit. Then FJPool.invoke could act like
FJTask.invoke[Later] when called in a non-FJ thread and like forkJoin when
called in an FJ thread.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071213/c5a1ffd8/attachment.html 

From rob.griffin at quest.com  Thu Dec 13 22:00:53 2007
From: rob.griffin at quest.com (Rob Griffin)
Date: Thu, 13 Dec 2007 19:00:53 -0800
Subject: [concurrency-interest] TheadLocal variable usage in
	ReentrantReadWriteLock
In-Reply-To: <63b4e4050712130602t29c6572w67ee69aba56381f6@mail.gmail.com>
References: <47611FA3.9060507@optrak.co.uk> <47612643.20307@cs.oswego.edu>
	<63b4e4050712130602t29c6572w67ee69aba56381f6@mail.gmail.com>
Message-ID: <21CDF59FC7A26F4FAB7A0B867833DC792DCF396F@alvxmbw01.prod.quest.corp>

Hello,

I'm seeing a significant number (dozens) of ThreadLocal variables containing values of type ReentrantReadWriteLock$Sync$HoldCounter in a single thread. From the code it would appear that each new ReentrantReadWriteLock instance creates a new ThreadLocal variable named readHolds which is never cleared. Is my interpretation correct?

This is in Java 1.6.0_03

Regards,

Rob Griffin.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071213/d136421c/attachment.html 

From dcholmes at optusnet.com.au  Fri Dec 14 00:04:07 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 14 Dec 2007 15:04:07 +1000
Subject: [concurrency-interest] TheadLocal variable usage
	inReentrantReadWriteLock
In-Reply-To: <21CDF59FC7A26F4FAB7A0B867833DC792DCF396F@alvxmbw01.prod.quest.corp>
Message-ID: <NFBBKALFDCPFIDBNKAPCGENFHJAA.dcholmes@optusnet.com.au>

Hi Rob,

Yes this is a known issue in Java 6. I think there's been some discussion on
the list if you search the archive. Otherwise see:

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6625723

The number of RRWL used in an application was expected to be relatively
small given that their benefits only become apparent in certain use-cases.
So the ThreadLocal overhead wasn't expected to be a significant issue.

For Java 7 the best solution might be to make the read counts optional.
Other approaches are also being looked at.

David Holmes

  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Rob Griffin
  Sent: Friday, 14 December 2007 1:01 PM
  To: Concurrency Interest
  Subject: [concurrency-interest] TheadLocal variable usage
inReentrantReadWriteLock


  Hello,



  I'm seeing a significant number (dozens) of ThreadLocal variables
containing values of type ReentrantReadWriteLock$Sync$HoldCounter in a
single thread. From the code it would appear that each new
ReentrantReadWriteLock instance creates a new ThreadLocal variable named
readHolds which is never cleared. Is my interpretation correct?



  This is in Java 1.6.0_03



  Regards,



  Rob Griffin.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071214/2e437250/attachment.html 

From duplon at majorgames.de  Fri Dec 14 21:39:35 2007
From: duplon at majorgames.de (Darius Polonis)
Date: Sat, 15 Dec 2007 03:39:35 +0100
Subject: [concurrency-interest] Swing translates InterruptedException to
	Error
Message-ID: <2F734A03-3171-4D56-B22A-5E539E7191A4@majorgames.de>

Hello there,

basically I have the same question as Tom Cargill in June 06, 2003  
(http://altair.cs.oswego.edu/mailman/private/concurrency-interest/ 
2003-June/000454.html). I am inserting strings from different threads  
and if such a thread is cancelled at the right time, the  
InterruptedException will be translated into this Error.

It looks like it may be safely ignored, doesn't it? If it may be  
ignored, would it be possible to change the Error into a specific  
Error, so that catching code doesn't have to catch all Errors and  
make a decision based on the message string, but rather may catch the  
InterruptedException for example by themselfes?

Best wishes,

Darius


From joe.bowbeer at gmail.com  Fri Dec 14 21:26:56 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 14 Dec 2007 18:26:56 -0800
Subject: [concurrency-interest] Swing translates InterruptedException to
	Error
In-Reply-To: <2F734A03-3171-4D56-B22A-5E539E7191A4@majorgames.de>
References: <2F734A03-3171-4D56-B22A-5E539E7191A4@majorgames.de>
Message-ID: <31f2a7bd0712141826u3c39adf6w82a4b9648e8977c9@mail.gmail.com>

See:

http://altair.cs.oswego.edu/mailman/private/concurrency-interest/2003-June/000456.html

In short: This code was originally even more broken, simply ignoring
interrupts. The IE is later transcoded to preserve method signatures.

In my opinion, it is not safe to modify documents asynchronously (that
is, on threads other than the EDT).  I think the Swing folks have
eventually come to this conclusion, too.

Given that this is broken, what's the advantage in throwing a
different kind of error?

--Joe

On Dec 14, 2007 6:39 PM, Darius Polonis <duplon at majorgames.de> wrote:
> Hello there,
>
> basically I have the same question as Tom Cargill in June 06, 2003
> (http://altair.cs.oswego.edu/mailman/private/concurrency-interest/
> 2003-June/000454.html). I am inserting strings from different threads
> and if such a thread is cancelled at the right time, the
> InterruptedException will be translated into this Error.
>
> It looks like it may be safely ignored, doesn't it? If it may be
> ignored, would it be possible to change the Error into a specific
> Error, so that catching code doesn't have to catch all Errors and
> make a decision based on the message string, but rather may catch the
> InterruptedException for example by themselfes?
>
> Best wishes,
>
> Darius
>

From duplon at majorgames.de  Sat Dec 15 08:40:02 2007
From: duplon at majorgames.de (Darius Polonis)
Date: Sat, 15 Dec 2007 14:40:02 +0100
Subject: [concurrency-interest] Swing translates InterruptedException to
	Error
In-Reply-To: <31f2a7bd0712141826u3c39adf6w82a4b9648e8977c9@mail.gmail.com>
References: <2F734A03-3171-4D56-B22A-5E539E7191A4@majorgames.de>
	<31f2a7bd0712141826u3c39adf6w82a4b9648e8977c9@mail.gmail.com>
Message-ID: <C3A12306-72F2-46BF-8F6B-B796FBCE3E10@majorgames.de>

Allright, in that case I'll just wrap my code and use  
SwingUtilities.invokeLater(). But I am not sure the Swing folks have  
to come to your conclusion, too, as the javadoc still states  
AbstractDocument.insertString() would be thread safe...

Anyway, thank you for your fast response.

Best wishes,

Darius


Am 15.12.2007 um 03:26 schrieb Joe Bowbeer:

> See:
>
> http://altair.cs.oswego.edu/mailman/private/concurrency-interest/ 
> 2003-June/000456.html
>
> In short: This code was originally even more broken, simply ignoring
> interrupts. The IE is later transcoded to preserve method signatures.
>
> In my opinion, it is not safe to modify documents asynchronously (that
> is, on threads other than the EDT).  I think the Swing folks have
> eventually come to this conclusion, too.
>
> Given that this is broken, what's the advantage in throwing a
> different kind of error?
>
> --Joe
>
> On Dec 14, 2007 6:39 PM, Darius Polonis <duplon at majorgames.de> wrote:
>> Hello there,
>>
>> basically I have the same question as Tom Cargill in June 06, 2003
>> (http://altair.cs.oswego.edu/mailman/private/concurrency-interest/
>> 2003-June/000454.html). I am inserting strings from different threads
>> and if such a thread is cancelled at the right time, the
>> InterruptedException will be translated into this Error.
>>
>> It looks like it may be safely ignored, doesn't it? If it may be
>> ignored, would it be possible to change the Error into a specific
>> Error, so that catching code doesn't have to catch all Errors and
>> make a decision based on the message string, but rather may catch the
>> InterruptedException for example by themselfes?
>>
>> Best wishes,
>>
>> Darius
>>
>


From dl at cs.oswego.edu  Sat Dec 15 09:46:01 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 15 Dec 2007 09:46:01 -0500
Subject: [concurrency-interest]
	ForkJoinExecutor.invoke	and	ForkJoinTask.invoke
In-Reply-To: <47612643.20307@cs.oswego.edu>
References: <47611FA3.9060507@optrak.co.uk> <47612643.20307@cs.oswego.edu>
Message-ID: <4763E8A9.8010707@cs.oswego.edu>

Doug Lea wrote:
> 
> The issue is whether ForkJoinPool (FJP) invoke(task) should
> (as now) serve as gateway between FJ and non-FJ threads,
> or whether it should check to see if caller is an FJ thread
> and if so act like task.invoke().
> The main visible difference is that the former blocks
> on join, while the latter helps on join (i.e., may execute
> that or some other task until done). But there is another
> difference that some people may need to rely on:
> FJP.invoke schedules tasks for execution in submission order,
> while plain task.invoke executes whenever it wants to
> (which turns out to always be  "immediately").
> 

There's an (in retrospect) obvious solution:
ForkJoinPool.invoke should allow helping (non-blocking) joins
when called from FJ tasks, but need not immediately
invoke the task, but instead process tasks in pool submission
order until completion. This is not only what people would
intuitively expect given the ForkJoinPool and ForkJoinTask
specs, but also avoids thread depletion dues to unnrecessary
blockages.

Revised versions that do this are now in CVS and jsr166y.jar.

While I'm at it: they also include some documentation clarification
in response to another (off-list) question. For ForkJoinTask t,
t.isDone() guarantees only to *eventually* return true upon
t's completion. This means that for example, concurrent calls
to t.isDone() from other tasks/threads may transiently disagree
about whether t.isDone() yet, but will always eventually agree.
Usages where this issue arises though are a little suspicious;
normally you want to call join() rather than polling isDone().

-Doug

From tim at peierls.net  Sat Dec 15 10:39:15 2007
From: tim at peierls.net (Tim Peierls)
Date: Sat, 15 Dec 2007 10:39:15 -0500
Subject: [concurrency-interest] ForkJoinExecutor.invoke and
	ForkJoinTask.invoke
In-Reply-To: <4763E8A9.8010707@cs.oswego.edu>
References: <47611FA3.9060507@optrak.co.uk> <47612643.20307@cs.oswego.edu>
	<4763E8A9.8010707@cs.oswego.edu>
Message-ID: <63b4e4050712150739x43db64f6t631659e2083ea735@mail.gmail.com>

On Dec 15, 2007 9:46 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> ForkJoinPool.invoke should allow helping (non-blocking) joins when called
> from FJ tasks, but need not immediately invoke the task, but instead process
> tasks in pool submission order until completion.


I still think renaming FJTask.invoke() to forkJoin() would make this
framework slightly easier to learn and reduce the potential for confusion
with FJPool.invoke(task). This isn't critical with FJPool.invoke doing the
"right thing", but the two kinds of invoke we have now *are* still
different, and I see no reason for a reader to have a moment's indecision
about it, if it can be avoided.

As things stand, I get the misleading impression that FJTask, with the one
method name in common with FJExecutor, is trying to *be* an FJExecutor, and
I end up translating "task.invoke()" to "efficient form of task.fork();
task.join()" in my head just to make sure of the distinction. You could say
that basing names on such impressions is a poor way to go about design, but
I'm pretty sure my own hangups in learning this stuff are shared by others.
Don't we count? (Only to ten, Mudhead. :-))

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071215/dcf61946/attachment.html 

From dl at cs.oswego.edu  Sat Dec 15 11:35:50 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 15 Dec 2007 11:35:50 -0500
Subject: [concurrency-interest] ForkJoinExecutor.invoke
	and	ForkJoinTask.invoke
In-Reply-To: <63b4e4050712150739x43db64f6t631659e2083ea735@mail.gmail.com>
References: <47611FA3.9060507@optrak.co.uk>
	<47612643.20307@cs.oswego.edu>	<4763E8A9.8010707@cs.oswego.edu>
	<63b4e4050712150739x43db64f6t631659e2083ea735@mail.gmail.com>
Message-ID: <47640266.40804@cs.oswego.edu>

Tim Peierls wrote:
> 
> I still think renaming FJTask.invoke() to forkJoin() would make this 
> framework slightly easier to learn and reduce the potential for 
> confusion with FJPool.invoke(task). 

At first this name sounded too wrong to me -- shouldn't it be
forkAndJoin()? (precedent: atomic.getAndAdd()) Which itself
sounds odd.  But just forkJoin() is somehow now starting to sound
OK to me, and you are right that it may forestall
some confusion. Any other opinions?

-Doug



From David.Biesack at sas.com  Mon Dec 17 09:14:02 2007
From: David.Biesack at sas.com (David J. Biesack)
Date: Mon, 17 Dec 2007 09:14:02 -0500 (EST)
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 35,
	Issue 20
In-Reply-To: <mailman.1.1197738000.19604.concurrency-interest@altair.cs.oswego.edu>
	(concurrency-interest-request@cs.oswego.edu)
References: <mailman.1.1197738000.19604.concurrency-interest@altair.cs.oswego.edu>
Message-ID: <200712171414.lBHEE2cK025561@cs.oswego.edu>

> From: concurrency-interest-request at cs.oswego.edu
> Date: Sat, 15 Dec 2007 12:00:00 -0500
> Date: Sat, 15 Dec 2007 11:35:50 -0500
> From: Doug Lea <dl at cs.oswego.edu>
> Subject: Re: [concurrency-interest] ForkJoinExecutor.invoke   and
>       ForkJoinTask.invoke
> To: Tim Peierls <tim at peierls.net>
> Cc: Concurrency Interest <concurrency-interest at cs.oswego.edu>
> Message-ID: <47640266.40804 at cs.oswego.edu>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> Tim Peierls wrote:
> > 
> > I still think renaming FJTask.invoke() to forkJoin() would make this 
> > framework slightly easier to learn and reduce the potential for 
> > confusion with FJPool.invoke(task). 
> 
> At first this name sounded too wrong to me -- shouldn't it be
> forkAndJoin()? (precedent: atomic.getAndAdd()) Which itself
> sounds odd.  But just forkJoin() is somehow now starting to sound
> OK to me, and you are right that it may forestall
> some confusion. Any other opinions?

My humble opinion concurs; I prefer forkJoin() over invoke(), avoiding what I also think will be inevitable confusion.

And to really mix things up, I agree forkAndJoin sounds a little wrong. I think forkThenJoin would be slightly better than forkAndJoin because "then" implies sequentiality, whereas "and" can be interpreted as "concurrently". Other examples of "X Then Y" are org.apache.log4j.lf5.util.StreamUtils.copyThenClose() and org.apache.batik.swing.svg.JSVGComponent.stopThenRun()

Side note: In addition to getAndAdd, the name pattern "X And Y" has other precedent: invokeAndWait() in java.awt.EventQueue.invokeAndWait(Runnable) and javax.swing.SwingUtilities(Runnable). Unfortunately, the name invokeAndWait would be confusing here since it may imply "fork() followed by wait()" which is wrong (i.e. invokeAndWait() was poorly named). 

> -Doug

-- 
David J. Biesack     SAS Institute Inc.
(919) 531-7771       SAS Campus Drive
http://www.sas.com   Cary, NC 27513


From dl at cs.oswego.edu  Mon Dec 17 20:17:01 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 17 Dec 2007 20:17:01 -0500
Subject: [concurrency-interest] International Workshop on Multicore Software
	Engineering (IWMSE)
Message-ID: <47671F8D.3010503@cs.oswego.edu>


Academic/researcher types on this list, especially those in Europe,
might be interested in the following:


CALL FOR PAPERS - IWMSE 2008

International Workshop on Multicore Software Engineering (IWMSE)

May 11, 2008, Leipzig, Germany

http://www.multicore-systems.org/iwmse

co-located with the 30th International Conference on Software Engineering 
(ICSE), May 10-18, 2008
_________________________________________________

With the emergence of multicore computers, software engineers face the
challenge of parallelizing performance-critical applications of all sorts.
Compared to sequential applications, our repertoire of tools and methods for
cost-effectively developing reliable, parallel applications is spotty. The
purpose of this workshop is to bring together researchers and practitioners
with diverse backgrounds in order to advance the state of the art in
software engineering for multi/manycore parallel applications.

The workshop is aimed at making parallelism available to a wide range of
applications using systematic software engineering methodology. We seek a
broad variety of work that furthers the knowledge and understanding of the
software engineering and parallel systems communities as a whole, continues
a significant research dialog, or pushes the architectural boundaries of
multicore software. We solicit original, previously unpublished papers of
current or work-in-progress research. Specific topics of interest include,
but are not limited to:

   * Parallel patterns
   * Frameworks and libraries for multicore software
   * Parallel software architectures
   * Modeling techniques for multicore software
   * Software components and composition
   * Programming languages/models for multicore software
   * Compilers for parallelism
   * Testing and debugging parallel applications
   * Parallel algorithms and data structures
   * Software reengineering for parallelism
   * Transactional Memory
   * Autotuning
   * Operating system support, scheduling
   * Visualization tools
   * Development environments for multicore software
   * Process models for multicore software development
   * Experience reports from research or industrial projects

In addition, we also seek express tutorials (30 min duration) that bring
participants up to speed in particular topics, such as OpenMP, Transactional
Memory, etc. A tutorial proposal should be 2-3 pages long.


Important dates
===============

* Submission deadline: 24 January, 2008
* Notification deadline: 7 February, 2008
* Camera-ready version: 18 February, 2008


Submission
==========

Please consult the workshop Web site http://www.multicore-systems.org/iwmse
for details.


Workshop Proceedings
=====================

All accepted papers and express tutorials will be published in ICSE
companion workshop proceedings. As the ICSE paper proceedings, workshop
paper proceedings will be published in the ACM and IEEE Digital Libraries.


Workshop Organizers
===================

* Walter F. Tichy, University of Karlsruhe, Germany
* Victor Pankratius, University of Karlsruhe, Germany

Program Committee
=================

* Dennis Gannon (Indiana University, USA)
* Tim Harris (Microsoft Research, Cambridge, UK)
* Doug Lea (State University of New York at Oswego, USA)
* David Lowenthal (University of Georgia, USA)
* Timothy Mattson (Intel, USA)
* Victor Pankratius (University of Karlsruhe, Germany)
* Erhard Ploedereder (University of Stuttgart, Germany)
* Adam Porter (University of Maryland, USA)
* Peter Sanders (University of Karlsruhe, Germany)
* Walter Tichy (University of Karlsruhe, Germany)
* Lawrence Votta (Sun Microsystems, USA)



From moran at gigaspaces.com  Tue Dec 18 10:16:28 2007
From: moran at gigaspaces.com (Moran Avigdor)
Date: Tue, 18 Dec 2007 17:16:28 +0200
Subject: [concurrency-interest] ThreadPoolExecutor with corePoolSize as Zero
In-Reply-To: <mailman.2.1197910800.6626.concurrency-interest@altair.cs.oswego.edu>
Message-ID: <3623E06481E65B45866CB3AF32C4FA87A325C1@hercules.gspaces.com>

When initializing a thread pool with a core-pool-size of Zero I see that
no threads execute the runnable tasks.

Looking at this further it seems that only if the queue rejected the
offer, then a first thread will be created.

 

A ThreadPoolExecutor with a SynchronousQueue() will work - but a
LinkedBlockingQueue() will not.
Is this considered a known limitation? Must I initialize my pool with
corePoolSize as 1 with an unbounded queue?

 

ThreadPoolExecutor pool = new ThreadPoolExecutor(0, 10, 60L,
TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());

            

for (int i=0; i<10; ++i)

{

      pool.execute(new Runnable() {

            public void run() {

                  System.out.println("running");

            };

});

}

 

Thanks.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071218/6f86b468/attachment.html 

From gergg at cox.net  Sat Dec 15 10:40:52 2007
From: gergg at cox.net (Gregg Wonderly)
Date: Sat, 15 Dec 2007 09:40:52 -0600
Subject: [concurrency-interest] Swing translates InterruptedException
 to	Error
In-Reply-To: <C3A12306-72F2-46BF-8F6B-B796FBCE3E10@majorgames.de>
References: <2F734A03-3171-4D56-B22A-5E539E7191A4@majorgames.de>	<31f2a7bd0712141826u3c39adf6w82a4b9648e8977c9@mail.gmail.com>
	<C3A12306-72F2-46BF-8F6B-B796FBCE3E10@majorgames.de>
Message-ID: <4763F584.1070003@cox.net>

Darius Polonis wrote:
> Allright, in that case I'll just wrap my code and use  
> SwingUtilities.invokeLater(). But I am not sure the Swing folks have  
> to come to your conclusion, too, as the javadoc still states  
> AbstractDocument.insertString() would be thread safe...

They used to say that an unrealized component could be manipulated outside of 
the event thread.  This is now, no longer possible.  It is important to write 
any code that touches a swing component to use the event dispatch thread.

Gregg Wonderly


From gergg at cox.net  Sat Dec 15 10:40:57 2007
From: gergg at cox.net (Gregg Wonderly)
Date: Sat, 15 Dec 2007 09:40:57 -0600
Subject: [concurrency-interest] TheadLocal variable
	usage	inReentrantReadWriteLock
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGENFHJAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCGENFHJAA.dcholmes@optusnet.com.au>
Message-ID: <4763F589.2060909@cox.net>

David Holmes wrote:
> Hi Rob,
>  
> Yes this is a known issue in Java 6. I think there's been some 
> discussion on the list if you search the archive. Otherwise see:
>  
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6625723
>  
> The number of RRWL used in an application was expected to be relatively 
> small given that their benefits only become apparent in certain 
> use-cases. So the ThreadLocal overhead wasn't expected to be a 
> significant issue.
>  
> For Java 7 the best solution might be to make the read counts optional. 
> Other approaches are also being looked at.

Recently, I put together a class that uses PhantomReference, an active reference 
counter and a background ReferenceQueue polling thread to allow me to use GC 
reference based discards to free resources.  It's use involves creating a 
concrete implementation somewhere that will not create a reference chain in the 
used types.  For example you might define a class such as the following.

public class MyRefTracker extends ReferenceTracker<MyClass,ToBeFreedType> {
	public void released( ToBeFreedType freeMe ) {
		freeMe.shutdown();
	}
}

Then, you might have a class which has some reference based cleanup that is 
needed where objects of type MyClass are passed around, and have an implicit 
reference to a ToBeFreedType instance.  You would probably declare a static 
instance such as

	private MyRefTracker track = new MyRefTracker();

Then, in your classes activities, where instances of MyClass come into being, 
you'd use

	MyClass inst;
	ToBeFreedType free = inst.somethingToGetReference();
	track.trackReference( inst, free );

to start tracking the lifecycle of the instances of MyClass and relate those to 
the corresponding instances of ToBeFreedType.

For me, this finally allows the features of finalization to actually be achievable.

ReferenceTracker utilizes an atomic counter to decide when the ReferenceQueue 
polling thread should be active.  That thread is not daemon so that it can 
actually recover all of the released objects before the JVM exits.

My usage came about from the use of smart proxies in a Jini application which 
carry a Lease object that allows some resources that the server has reserved for 
each smart proxy instance to be freed.  Timeouts would allow the Lease to be 
eventually canceled correctly.  The LeaseRenewalManager will be faithfully 
renewing the Lease to the server, and when the final reference goes away, the 
Lease needs to be canceled.   This allows the smart proxy to be used as a normal 
object without lifecycle being strewn throughout the code with everyone tracking 
who they passed a reference to.

Gregg Wonderly


From Ben.Rowlands at morganstanley.com  Tue Dec 18 12:45:04 2007
From: Ben.Rowlands at morganstanley.com (Rowlands, Ben (IT))
Date: Tue, 18 Dec 2007 17:45:04 -0000
Subject: [concurrency-interest] ThreadPoolExecutor with corePoolSize as
	Zero
In-Reply-To: <3623E06481E65B45866CB3AF32C4FA87A325C1@hercules.gspaces.com>
References: <mailman.2.1197910800.6626.concurrency-interest@altair.cs.oswego.edu>
	<3623E06481E65B45866CB3AF32C4FA87A325C1@hercules.gspaces.com>
Message-ID: <6E88AC4207ED7245BA40032D5772AB640AEA7A6B@LNWEXMB25.msad.ms.com>

I also encountered this recently (some of the TPE configuration can be a
bit tricky). There are many posts on this is you Google around, for
example: 

  http://osdir.com/ml/java.jsr.166-concurrency/2006-12/msg00081.html

The javadocs are very clear as to when threads will be created - and it
depends on the current pool size relative to the core and max sizes.

Note, core threads are only created on demand so you could just create a
pool with > 0 core thread(s). An idle thread is unlikely to impact
performance significantly. In JDK/6 (or if using latest backport.j.u.c)
you can allow core threads to timeout so they won't stick around if
there is nothing to do:

  ThreadPoolExecutor#allowCoreThreadsToTimeout( true );

Interestingly, if you run your code using the j.u.c implementation
bundled with JDK/6 it *does* work - it appears to start a new thread
before the queue is full. I never managed to track down the change that
fixed this - does anyone know what changed to cause this to work?.
 
Ben Rowlands
Morgan Stanley | Technology
20 Cabot Square | Canary Wharf | Floor 06
London, E14 4QW
Phone: +44 20 7677-4075
Ben.Rowlands at morganstanley.com

 


________________________________

	From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Moran
Avigdor
	Sent: 18 December 2007 15:16
	To: concurrency-interest at cs.oswego.edu
	Subject: [concurrency-interest] ThreadPoolExecutor with
corePoolSize as Zero
	
	

	When initializing a thread pool with a core-pool-size of Zero I
see that no threads execute the runnable tasks.

	Looking at this further it seems that only if the queue rejected
the offer, then a first thread will be created.

	 

	A ThreadPoolExecutor with a SynchronousQueue() will work - but a
LinkedBlockingQueue() will not.
	Is this considered a known limitation? Must I initialize my pool
with corePoolSize as 1 with an unbounded queue?

	 

	ThreadPoolExecutor pool = new ThreadPoolExecutor(0, 10, 60L,
TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());

	            

	for (int i=0; i<10; ++i)

	{

	      pool.execute(new Runnable() {

	            public void run() {

	                  System.out.println("running");

	            };

	});

	}

	 

	Thanks.
--------------------------------------------------------

NOTICE: If received in error, please destroy and notify sender. Sender does not intend to waive confidentiality or privilege. Use of this email is prohibited when received in error.


From Martin.Buchholz at Sun.COM  Tue Dec 18 12:59:04 2007
From: Martin.Buchholz at Sun.COM (Martin Buchholz)
Date: Tue, 18 Dec 2007 09:59:04 -0800
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 35,
	Issue 22
In-Reply-To: <mailman.3.1197997200.10465.concurrency-interest@altair.cs.oswego.edu>
References: <mailman.3.1197997200.10465.concurrency-interest@altair.cs.oswego.edu>
Message-ID: <47680A68.1050606@sun.com>

Moran,
This bug was fixed in 6.0 build 93,
apparently by fix for:

6440728: ThreadPoolExecutor can fail to execute successfully submitted
tasks as specified
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6440728

Martin

> Message: 2
> Date: Tue, 18 Dec 2007 17:16:28 +0200
> From: "Moran Avigdor" <moran at gigaspaces.com>
> Subject: [concurrency-interest] ThreadPoolExecutor with corePoolSize
> 	as Zero
> To: <concurrency-interest at cs.oswego.edu>
> Message-ID:
> 	<3623E06481E65B45866CB3AF32C4FA87A325C1 at hercules.gspaces.com>
> Content-Type: text/plain; charset="us-ascii"
> 
> When initializing a thread pool with a core-pool-size of Zero I see that
> no threads execute the runnable tasks.
> 
> Looking at this further it seems that only if the queue rejected the
> offer, then a first thread will be created.
> 
>  
> 
> A ThreadPoolExecutor with a SynchronousQueue() will work - but a
> LinkedBlockingQueue() will not.
> Is this considered a known limitation? Must I initialize my pool with
> corePoolSize as 1 with an unbounded queue?
> 
>  
> 
> ThreadPoolExecutor pool = new ThreadPoolExecutor(0, 10, 60L,
> TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());
> 
>             
> 
> for (int i=0; i<10; ++i)
> 
> {
> 
>       pool.execute(new Runnable() {
> 
>             public void run() {
> 
>                   System.out.println("running");
> 
>             };
> 
> });
> 
> }
> 
>  
> 
> Thanks.
> 

From tackline at tackline.plus.com  Tue Dec 18 13:03:12 2007
From: tackline at tackline.plus.com (Thomas Hawtin)
Date: Tue, 18 Dec 2007 18:03:12 +0000
Subject: [concurrency-interest] Swing translates InterruptedException
 to	Error
In-Reply-To: <4763F584.1070003@cox.net>
References: <2F734A03-3171-4D56-B22A-5E539E7191A4@majorgames.de>	<31f2a7bd0712141826u3c39adf6w82a4b9648e8977c9@mail.gmail.com>	<C3A12306-72F2-46BF-8F6B-B796FBCE3E10@majorgames.de>
	<4763F584.1070003@cox.net>
Message-ID: <47680B60.3060305@tackline.plus.com>

Gregg Wonderly wrote:
> Darius Polonis wrote:
>> Allright, in that case I'll just wrap my code and use  
>> SwingUtilities.invokeLater(). But I am not sure the Swing folks have  
>> to come to your conclusion, too, as the javadoc still states  
>> AbstractDocument.insertString() would be thread safe...
> 
> They used to say that an unrealized component could be manipulated outside of 
> the event thread.  This is now, no longer possible.  It is important to write 
> any code that touches a swing component to use the event dispatch thread.

A number of methods explicitly state that they are thread-safe. For 
instance, JTextArea.append. But given the interface of Document or even 
AbstractDocument, how would you write such a thing correctly?

Tom Hawtin

From sberlin at gmail.com  Tue Dec 18 13:39:10 2007
From: sberlin at gmail.com (Sam Berlin)
Date: Tue, 18 Dec 2007 13:39:10 -0500
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 35,
	Issue 22
In-Reply-To: <47680A68.1050606@sun.com>
References: <mailman.3.1197997200.10465.concurrency-interest@altair.cs.oswego.edu>
	<47680A68.1050606@sun.com>
Message-ID: <19196d860712181039m38c10d61ud14332b40cc9943@mail.gmail.com>

Are changes to Java's built-in TPE mirrored (or first added to) the
public domain one at:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/
?

Sam

On 12/18/07, Martin Buchholz <Martin.Buchholz at sun.com> wrote:
> Moran,
> This bug was fixed in 6.0 build 93,
> apparently by fix for:
>
> 6440728: ThreadPoolExecutor can fail to execute successfully submitted
> tasks as specified
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6440728
>
> Martin
>
> > Message: 2
> > Date: Tue, 18 Dec 2007 17:16:28 +0200
> > From: "Moran Avigdor" <moran at gigaspaces.com>
> > Subject: [concurrency-interest] ThreadPoolExecutor with corePoolSize
> >       as Zero
> > To: <concurrency-interest at cs.oswego.edu>
> > Message-ID:
> >       <3623E06481E65B45866CB3AF32C4FA87A325C1 at hercules.gspaces.com>
> > Content-Type: text/plain; charset="us-ascii"
> >
> > When initializing a thread pool with a core-pool-size of Zero I see that
> > no threads execute the runnable tasks.
> >
> > Looking at this further it seems that only if the queue rejected the
> > offer, then a first thread will be created.
> >
> >
> >
> > A ThreadPoolExecutor with a SynchronousQueue() will work - but a
> > LinkedBlockingQueue() will not.
> > Is this considered a known limitation? Must I initialize my pool with
> > corePoolSize as 1 with an unbounded queue?
> >
> >
> >
> > ThreadPoolExecutor pool = new ThreadPoolExecutor(0, 10, 60L,
> > TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());
> >
> >
> >
> > for (int i=0; i<10; ++i)
> >
> > {
> >
> >       pool.execute(new Runnable() {
> >
> >             public void run() {
> >
> >                   System.out.println("running");
> >
> >             };
> >
> > });
> >
> > }
> >
> >
> >
> > Thanks.
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From Ben.Rowlands at morganstanley.com  Tue Dec 18 13:54:20 2007
From: Ben.Rowlands at morganstanley.com (Rowlands, Ben (IT))
Date: Tue, 18 Dec 2007 18:54:20 -0000
Subject: [concurrency-interest] Joining on multiple futures
	(ListenableFuture?)
Message-ID: <6E88AC4207ED7245BA40032D5772AB640AEA7A9E@LNWEXMB25.msad.ms.com>

Are there any utilities in j.u.c to help join on multiple futures? For
example,
 
 // returns when all futures complete or a future fails
 join( Future... futures, long timeout, TimeUnit unit )

A trivial implementation would loop through each future and call get().
This would work, and we could keep track of the time spent waiting on
each future to support a timeout. 

The main drawback of this technique is we only find out about a failed
future after we've called get() on all previous futures. We'd like to
learn about a failure as soon as possible to start recovery (if any
futures fail we repeat the unit of work, so the sooner we can detect a
failure the better). We cancel any pending futures before the repeat.

Our alternative implementation uses polling by calling get() with a
timeout of a few hundred ms and looping until all complete. This works
but isn't terribly elegant, and has caused some problems with poorly
implemented futures (the polling basically gets pushed down to the
resource the future wraps, often a read on a TCP socket within the
timeout).

In our use-case, the futures are backed by tcp sockets (they hold the
result of a tcp request). One solution might be support for
"ListenableFutures" that notify a listener when they complete (or fail
with an exception), allowing us to use a more efficient strategy to join
on the futures. Something like:

  interface ListenableFuture extends Future {
    // register a listener to be called when this future is 'ready' (has
a result or failed)
    // if the future is already complete the callback is fired
immediately
    void registerListener( FutureListener listener );
  }

  interface FutureListener {
    void ready( Future future );
  }

I'd guess there are other Futures that could support asynchronous
notification on completion including async file IO.

Are there any plans to support anything like this? Or have we missed
something obvious? I came across a discussion back in 2003 related to
this but it looks like the done() subclass hook that resulted is more
for implementers of futures than consumers of the future(s).

  http://osdir.com/ml/java.jsr.166-concurrency/2003-10/msg00001.html

Thanks,

Ben
--------------------------------------------------------

NOTICE: If received in error, please destroy and notify sender. Sender does not intend to waive confidentiality or privilege. Use of this email is prohibited when received in error.


From Martin.Buchholz at Sun.COM  Tue Dec 18 13:55:00 2007
From: Martin.Buchholz at Sun.COM (Martin Buchholz)
Date: Tue, 18 Dec 2007 10:55:00 -0800
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 35,
 Issue 22
In-Reply-To: <19196d860712181039m38c10d61ud14332b40cc9943@mail.gmail.com>
References: <mailman.3.1197997200.10465.concurrency-interest@altair.cs.oswego.edu>
	<47680A68.1050606@sun.com>
	<19196d860712181039m38c10d61ud14332b40cc9943@mail.gmail.com>
Message-ID: <47681784.3080305@sun.com>

The JDK's (GPL'ed) code is "downstream" from Doug's public domain
code.  Changes generally show up first in Doug's CVS.

Martin

Sam Berlin wrote:
> Are changes to Java's built-in TPE mirrored (or first added to) the
> public domain one at:
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/
> ?
> 
> Sam

From joe.bowbeer at gmail.com  Tue Dec 18 14:43:55 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 18 Dec 2007 11:43:55 -0800
Subject: [concurrency-interest] Joining on multiple futures
	(ListenableFuture?)
In-Reply-To: <6E88AC4207ED7245BA40032D5772AB640AEA7A9E@LNWEXMB25.msad.ms.com>
References: <6E88AC4207ED7245BA40032D5772AB640AEA7A9E@LNWEXMB25.msad.ms.com>
Message-ID: <31f2a7bd0712181143ib2a97a1qc5d21dd3a2c4696@mail.gmail.com>

On Dec 18, 2007 10:54 AM, Rowlands, Ben (IT)
<Ben.Rowlands at morganstanley.com> wrote:
> Are there any utilities in j.u.c to help join on multiple futures? For
> example,
>
>  // returns when all futures complete or a future fails
>  join( Future... futures, long timeout, TimeUnit unit )
>
> A trivial implementation would loop through each future and call get().
> This would work, and we could keep track of the time spent waiting on
> each future to support a timeout.
>
> The main drawback of this technique is we only find out about a failed
> future after we've called get() on all previous futures. We'd like to
> learn about a failure as soon as possible to start recovery (if any
> futures fail we repeat the unit of work, so the sooner we can detect a
> failure the better). We cancel any pending futures before the repeat.
>
> Our alternative implementation uses polling by calling get() with a
> timeout of a few hundred ms and looping until all complete. This works
> but isn't terribly elegant, and has caused some problems with poorly
> implemented futures (the polling basically gets pushed down to the
> resource the future wraps, often a read on a TCP socket within the
> timeout).
>
> In our use-case, the futures are backed by tcp sockets (they hold the
> result of a tcp request). One solution might be support for
> "ListenableFutures" that notify a listener when they complete (or fail
> with an exception), allowing us to use a more efficient strategy to join
> on the futures. Something like:
>
>   interface ListenableFuture extends Future {
>     // register a listener to be called when this future is 'ready' (has
> a result or failed)
>     // if the future is already complete the callback is fired
> immediately
>     void registerListener( FutureListener listener );
>   }
>
>   interface FutureListener {
>     void ready( Future future );
>   }
>
> I'd guess there are other Futures that could support asynchronous
> notification on completion including async file IO.
>
> Are there any plans to support anything like this? Or have we missed
> something obvious? I came across a discussion back in 2003 related to
> this but it looks like the done() subclass hook that resulted is more
> for implementers of futures than consumers of the future(s).
>
>   http://osdir.com/ml/java.jsr.166-concurrency/2003-10/msg00001.html
>
> Thanks,
>
> Ben

You didn't mention ExecutorCompletionService.  You might find what you
need there.

Concerning listeners, one can override the done() completion hook to
implement observable FutureTask subclass that sends events to
listeners.  The new SwingWorker does this
(https://swingworker.dev.java.net/), and the SwingWorker in this
article enqueues a runnable event:

http://java.sun.com/products/jfc/tsc/articles/threads/threads3.html#swing-worker

--Joe

From gregg at cytetech.com  Tue Dec 18 16:03:34 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 18 Dec 2007 15:03:34 -0600
Subject: [concurrency-interest] Swing translates InterruptedException to
 Error
In-Reply-To: <47680B60.3060305@tackline.plus.com>
References: <2F734A03-3171-4D56-B22A-5E539E7191A4@majorgames.de>
	<31f2a7bd0712141826u3c39adf6w82a4b9648e8977c9@mail.gmail.com>
	<C3A12306-72F2-46BF-8F6B-B796FBCE3E10@majorgames.de>
	<4763F584.1070003@cox.net> <47680B60.3060305@tackline.plus.com>
Message-ID: <476835A6.5030909@cytetech.com>

Thomas Hawtin wrote:
> Gregg Wonderly wrote:
>> Darius Polonis wrote:
>>> Allright, in that case I'll just wrap my code and use  
>>> SwingUtilities.invokeLater(). But I am not sure the Swing folks have  
>>> to come to your conclusion, too, as the javadoc still states  
>>> AbstractDocument.insertString() would be thread safe...
>> They used to say that an unrealized component could be manipulated outside of 
>> the event thread.  This is now, no longer possible.  It is important to write 
>> any code that touches a swing component to use the event dispatch thread.
> 
> A number of methods explicitly state that they are thread-safe. For 
> instance, JTextArea.append. But given the interface of Document or even 
> AbstractDocument, how would you write such a thing correctly?

My experience is that for any text component, JLabel included, append() or 
setText() et.al, will cause resize() to be called which will assert the 
lock-of-doom...  I really think that it is no longer safe to manipulate anything 
outside of the event thread based on what I've experienced in a multi threaded 
desktop UI which I wrote.  This UI gets new components from dynamic network 
discovery (using Jini) which is a good demonstration of random thread behavior.

Gregg Wonderly

From dl at cs.oswego.edu  Tue Dec 18 16:44:48 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 18 Dec 2007 16:44:48 -0500
Subject: [concurrency-interest] ForkJoinExecutor.invoke
	and	ForkJoinTask.invoke
In-Reply-To: <63b4e4050712150739x43db64f6t631659e2083ea735@mail.gmail.com>
References: <47611FA3.9060507@optrak.co.uk>
	<47612643.20307@cs.oswego.edu>	<4763E8A9.8010707@cs.oswego.edu>
	<63b4e4050712150739x43db64f6t631659e2083ea735@mail.gmail.com>
Message-ID: <47683F50.3000601@cs.oswego.edu>

Tim Peierls wrote:
> 
> I still think renaming FJTask.invoke() to forkJoin() would make this 
> framework slightly easier to learn and reduce the potential for 
> confusion with FJPool.invoke(task). 

Thanks, done:
   ForkJoinTask.invoke() is renamed ForkJoinTask.forkJoin()
and similarly,
   RecursiveAction.coInvoke(t1, t2) is now RecursiveAction.forkJoin(t1, t2)
and similarly for the array and list versions.

Sorry to those of you who have been using the framework;
most of you will probably need to edit and recompile.
But better now than after full release.

-Doug


From moran at gigaspaces.com  Wed Dec 19 04:35:58 2007
From: moran at gigaspaces.com (Moran Avigdor)
Date: Wed, 19 Dec 2007 11:35:58 +0200
Subject: [concurrency-interest] ThreadPoolExecutor with corePoolSize as
	Zero
In-Reply-To: <6E88AC4207ED7245BA40032D5772AB640AEA7A6B@LNWEXMB25.msad.ms.com>
Message-ID: <3623E06481E65B45866CB3AF32C4FA87A32723@hercules.gspaces.com>

Ben, Martin - many thanks.
 

-----Original Message-----
From: Rowlands, Ben (IT) [mailto:Ben.Rowlands at morganstanley.com] 
Sent: Tuesday, December 18, 2007 7:45 PM
To: Moran Avigdor; concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] ThreadPoolExecutor with corePoolSize
as Zero

I also encountered this recently (some of the TPE configuration can be a
bit tricky). There are many posts on this is you Google around, for
example: 

  http://osdir.com/ml/java.jsr.166-concurrency/2006-12/msg00081.html

The javadocs are very clear as to when threads will be created - and it
depends on the current pool size relative to the core and max sizes.

Note, core threads are only created on demand so you could just create a
pool with > 0 core thread(s). An idle thread is unlikely to impact
performance significantly. In JDK/6 (or if using latest backport.j.u.c)
you can allow core threads to timeout so they won't stick around if
there is nothing to do:

  ThreadPoolExecutor#allowCoreThreadsToTimeout( true );

Interestingly, if you run your code using the j.u.c implementation
bundled with JDK/6 it *does* work - it appears to start a new thread
before the queue is full. I never managed to track down the change that
fixed this - does anyone know what changed to cause this to work?.
 
Ben Rowlands
Morgan Stanley | Technology
20 Cabot Square | Canary Wharf | Floor 06
London, E14 4QW
Phone: +44 20 7677-4075
Ben.Rowlands at morganstanley.com

 


________________________________

	From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Moran
Avigdor
	Sent: 18 December 2007 15:16
	To: concurrency-interest at cs.oswego.edu
	Subject: [concurrency-interest] ThreadPoolExecutor with
corePoolSize as Zero
	
	

	When initializing a thread pool with a core-pool-size of Zero I
see that no threads execute the runnable tasks.

	Looking at this further it seems that only if the queue rejected
the offer, then a first thread will be created.

	 

	A ThreadPoolExecutor with a SynchronousQueue() will work - but a
LinkedBlockingQueue() will not.
	Is this considered a known limitation? Must I initialize my pool
with corePoolSize as 1 with an unbounded queue?

	 

	ThreadPoolExecutor pool = new ThreadPoolExecutor(0, 10, 60L,
TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());

	            

	for (int i=0; i<10; ++i)

	{

	      pool.execute(new Runnable() {

	            public void run() {

	                  System.out.println("running");

	            };

	});

	}

	 

	Thanks.
--------------------------------------------------------

NOTICE: If received in error, please destroy and notify sender. Sender
does not intend to waive confidentiality or privilege. Use of this email
is prohibited when received in error.


From kasper at kav.dk  Wed Dec 19 05:50:42 2007
From: kasper at kav.dk (Kasper Nielsen)
Date: Wed, 19 Dec 2007 11:50:42 +0100
Subject: [concurrency-interest] ParallelArray Extension
Message-ID: <4768F782.6030303@kav.dk>

Hi,

I've run into two scenarios where I haven't been able to find a 
satisfying solution with the current ParallelArray api.

1.
Selecting distinct elements. This comes up fairly often, for example, 
you want to find a set of currently open trades or all the groups some 
students belong to. The best solution I've been able to come with goes 
something along the lines of:

ParallelArray<Integer> pa = ...
final Map<Integer, Integer> m = new ConcurrentHashMap<Integer, Integer>();
w.apply(new Procedure<Integer>() {
     public void apply(Integer t) {
         if (!m.containsKey(t))
             m.put(t, t);
     }
});
Set<Integer> result = m.keySet();

2.

I have a data set that changes rarely but I need to make a lot of 
read-only calculations on the dataset. So internally I've encapsulated 
an instance of ParallelArray<Integer> with a ReadWriteLock and then 
exposing the wrapped methods:
public Integer sum() {
    readLock.lock();
    return pa.sum();
}
public size size() {
    readLock.lock();
    return pa.size();
}
public void set(int i, Integer x) {
    writeLock.lock();
    pa.set(i, x);
}
etc.

I don't want to expose the ReadWriteLock, because someday I might want 
to change the implementation. Now, say the client to calculate the 
average of all integers in the ParallelArray. Normally it would just be 
pa.sum() / pa.size(). However since pa.sum() and pa.size() are two 
separate methods, somebody might have changed the dataset in the mean 
time (I've not included the operations for changing the size). So I need 
someway to calculate the sum and size in one 'atomic' operation.

One solution I've thought is to add something resembling this method:
<U> U combineReduce(Combiner<T, U, U> c, Reducer<U> reducer);

The idea is that first individual threads work on the combiner. And when 
they are done, all the individual results are combined using the 
reducer. Here is the code for both problems:

1:
HashSet result = w.combineReduce(new Combiner<Integer, HashSet, HashSet>() {
     public HashSet combine(Integer t, HashSet u) {
         if (u == null) {
             u = new HashSet();
         }
         u.add(t);
         return u;
     }

}, new Reducer<HashSet>() {
     public HashSet combine(HashSet t, HashSet v) {
         t.addAll(v);
         return t;
     }
});

2:
static class Aggregate {
     public int count;
     public long sum;
}

Aggregate a = w.combineReduce(new Combiner<Integer, Aggregate, 
Aggregate>() {
     public Aggregate combine(Integer t, Aggregate u) {
         if (u == null) {
             u = new Aggregate();
         }
         u.count++;
         u.sum += t;
         return u;
     }

}, new Reducer<Aggregate>() {
     public Aggregate combine(Aggregate t, Aggregate v) {
         t.count += v.count;
         t.sum += v.sum;
         return t;
     }
});
System.out.println("Average : " + (a.sum / a.count));

Thoughts, or any other way to solve these problems?

cheers
   Kasper

From tim at peierls.net  Wed Dec 19 09:51:24 2007
From: tim at peierls.net (Tim Peierls)
Date: Wed, 19 Dec 2007 09:51:24 -0500
Subject: [concurrency-interest] ParallelArray Extension
In-Reply-To: <4768F782.6030303@kav.dk>
References: <4768F782.6030303@kav.dk>
Message-ID: <63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>

These sound like applications for map-reduce:

1. w.withMapping(elementsToSingletonSets).reduce(combineSets);
2. w.withMapping(elementsToAggregates).reduce(combineAggregates);

I'm don't know whether these will perform as well in practice as the
solutions you've come up with, but they seem essentially equivalent to your
proposed combineReduce.

Incidentally, wouldn't you want to use ParallelIntArray in your examples?

--tim

On Dec 19, 2007 5:50 AM, Kasper Nielsen <kasper at kav.dk> wrote:

> Hi,
>
> I've run into two scenarios where I haven't been able to find a
> satisfying solution with the current ParallelArray api.
>
> 1.
> Selecting distinct elements. This comes up fairly often, for example,
> you want to find a set of currently open trades or all the groups some
> students belong to. The best solution I've been able to come with goes
> something along the lines of:
>
> ParallelArray<Integer> pa = ...
> final Map<Integer, Integer> m = new ConcurrentHashMap<Integer, Integer>();
> w.apply(new Procedure<Integer>() {
>     public void apply(Integer t) {
>         if (!m.containsKey(t))
>             m.put(t, t);
>     }
> });
> Set<Integer> result = m.keySet();
>
> 2.
>
> I have a data set that changes rarely but I need to make a lot of
> read-only calculations on the dataset. So internally I've encapsulated
> an instance of ParallelArray<Integer> with a ReadWriteLock and then
> exposing the wrapped methods:
> public Integer sum() {
>    readLock.lock();
>    return pa.sum();
> }
> public size size() {
>    readLock.lock();
>    return pa.size();
> }
> public void set(int i, Integer x) {
>    writeLock.lock();
>    pa.set(i, x);
> }
> etc.
>
> I don't want to expose the ReadWriteLock, because someday I might want
> to change the implementation. Now, say the client to calculate the
> average of all integers in the ParallelArray. Normally it would just be
> pa.sum() / pa.size(). However since pa.sum() and pa.size() are two
> separate methods, somebody might have changed the dataset in the mean
> time (I've not included the operations for changing the size). So I need
> someway to calculate the sum and size in one 'atomic' operation.
>
> One solution I've thought is to add something resembling this method:
> <U> U combineReduce(Combiner<T, U, U> c, Reducer<U> reducer);
>
> The idea is that first individual threads work on the combiner. And when
> they are done, all the individual results are combined using the
> reducer. Here is the code for both problems:
>
> 1:
> HashSet result = w.combineReduce(new Combiner<Integer, HashSet, HashSet>()
> {
>     public HashSet combine(Integer t, HashSet u) {
>         if (u == null) {
>             u = new HashSet();
>         }
>         u.add(t);
>         return u;
>     }
>
> }, new Reducer<HashSet>() {
>     public HashSet combine(HashSet t, HashSet v) {
>         t.addAll(v);
>         return t;
>     }
> });
>
> 2:
> static class Aggregate {
>     public int count;
>     public long sum;
> }
>
> Aggregate a = w.combineReduce(new Combiner<Integer, Aggregate,
> Aggregate>() {
>     public Aggregate combine(Integer t, Aggregate u) {
>         if (u == null) {
>             u = new Aggregate();
>         }
>         u.count++;
>         u.sum += t;
>         return u;
>     }
>
> }, new Reducer<Aggregate>() {
>     public Aggregate combine(Aggregate t, Aggregate v) {
>         t.count += v.count;
>         t.sum += v.sum;
>         return t;
>     }
> });
> System.out.println("Average : " + (a.sum / a.count));
>
> Thoughts, or any other way to solve these problems?
>
> cheers
>   Kasper
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071219/bc2ae142/attachment.html 

From forax at univ-mlv.fr  Wed Dec 19 10:07:06 2007
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Wed, 19 Dec 2007 16:07:06 +0100
Subject: [concurrency-interest] some stupid questions
In-Reply-To: <47683F50.3000601@cs.oswego.edu>
References: <47611FA3.9060507@optrak.co.uk>	<47612643.20307@cs.oswego.edu>	<4763E8A9.8010707@cs.oswego.edu>	<63b4e4050712150739x43db64f6t631659e2083ea735@mail.gmail.com>
	<47683F50.3000601@cs.oswego.edu>
Message-ID: <4769339A.4090409@univ-mlv.fr>

First a remark,
in the doc of ForkJoinTask, you can see this sentence:
"The ForkJoinTask class is not directly subclassable outside of this 
package"
but the constructor is public ?

Else, why ||*ParallelArray 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20int,%20java.lang.Class%29> 
is not a List,
it provides get/set/size/iterator but if you want a list you need to 
create another object
using asList(). So basically, why he doesn't inherits from AbstractList 
(and implements RandomAccess) ?

I think the same idea can be applied to  *ParallelArray.WithMapping.
By the way why there is no get and set here ?, is it because these 
operations can't be done in constant time ?
Perhaps ParallelArray.WithMapping can inherit from SequentialAbstractList.

*I don't like the fact *|*ParallelArray 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20int,%20T%5B%5D%29>*(ForkJoinExecutor 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ForkJoinExecutor.html> executor, 
int size, T 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html>[] sourceToCopy)| 

perform a defensive copy and |*ParallelArray 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20T%5B%5D%29>*(ForkJoinExecutor 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ForkJoinExecutor.html> executor, 
T 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html>[] handoff)
don't.
|*
Perhaps The constructor
*|*ParallelArray 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20T%5B%5D%29>*(ForkJoinExecutor 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ForkJoinExecutor.html> executor, 
T 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html>[] handoff) 
should be protected
and a static method "wrap" introduced.

|I  think all  replace* method  should be renamed  to  fill*  |like in 
java.util.Arrays.
||And to be consistent, randomFill() should be fillRandom().

I wonder if ||*newArray 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.WithFilter.html#newArray%28java.lang.Class%29>*(Class 
<http://java.sun.com/javase/6/docs/api/java/lang/Class.html?is-external=true><? 
super T 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.WithFilter.html>> elementType 
can't be rewritten to
<U> ParallelArray<U> newArray(Class<U> elementType) in order to allow 
upcasting
if possible and a ClassCastException otherwise.

cheers,
R?mi



|


From dl at cs.oswego.edu  Wed Dec 19 10:10:30 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 19 Dec 2007 10:10:30 -0500
Subject: [concurrency-interest] ParallelArray Extension
In-Reply-To: <63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>
References: <4768F782.6030303@kav.dk>
	<63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>
Message-ID: <47693466.9050804@cs.oswego.edu>

Kasper: Thanks for trying things out! Reports like this are
extremely helpful! We need to know what people are doing with
these APIs and what problems they encounter.


Tim Peierls wrote:
> These sound like applications for map-reduce:
> 
> 1. w.withMapping(elementsToSingletonSets).reduce(combineSets);
> 2. w.withMapping(elementsToAggregates).reduce(combineAggregates);
> 

In which case, the question amounts to whether it is worth
special support to uniquify elements. Probably so, because
in this case we can internally exploit some parallelism
that is otherwise hard for people to do themselves.
This suggests adding:

  ParallelArray uniqueElements();    // kill duplicates
  ParallelArray uniqueConsecutiveElements(); //  assumes array already sorted
  ParallelArray uniqueElements(ParallelArray a); // merge non-dups
  ParallelArray uniqueConsecutiveElements(ParallelArray a); // assume both sorted

(And possibly some mutatuve (versus constructive) forms.)

The broader issue is that ParallelArray is list/array-like,
but with some map, set, and ordering support. Eventually we also want
full map/set-like, as well as sorted-map/set-like classes. These are
more challenging to implement in ways that give you as good
speed-ups as array/list style: You lose some of the advantages
of data contiguity and fast (implicit) split/merge. Also, map/set
style invites a more relational-database-like API. This is
not TOO different than ParallelArray though. Notice that
pa.withFilter(...).withMapping(...) is basically similar to
SQL select/project. But my guess based on what the PLINQ (C#)
folks are doing is that some people will want further extensions,
lead to all sorts of syntax/tool/etc issues that I'd rather avoid.

Anyway, ParallelMap and friends will probably eventually make it in.
But in the mean time, it is likely that using ParallelArray to
emulate set/map semantics will work reasonably well, especially
if we add a few methods to simplify emulation.

-Doug

From kasper at kav.dk  Wed Dec 19 10:13:09 2007
From: kasper at kav.dk (Kasper Nielsen)
Date: Wed, 19 Dec 2007 16:13:09 +0100
Subject: [concurrency-interest] ParallelArray Extension
In-Reply-To: <63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>
References: <4768F782.6030303@kav.dk>
	<63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>
Message-ID: <47693505.1060304@kav.dk>

Tim Peierls wrote:
> These sound like applications for map-reduce:
> 
> 1. w.withMapping(elementsToSingletonSets).reduce(combineSets);
> 2. w.withMapping(elementsToAggregates).reduce(combineAggregates);
> 
> I'm don't know whether these will perform as well in practice as the 
> solutions you've come up with, but they seem essentially equivalent to 
> your proposed combineReduce.

 From a functional standpoint this works fine, but it doesn't really 
perform. If I create a new object in withMapping. I would need to create 
a new HashSet/Aggregate for each element in ParallelArray.

> Incidentally, wouldn't you want to use ParallelIntArray in your examples?

I've just used Integer's to keep things simple in the examples.

- Kasper

> 
> --tim
> 
> On Dec 19, 2007 5:50 AM, Kasper Nielsen <kasper at kav.dk 
> <mailto:kasper at kav.dk> > wrote:
> 
>     Hi,
> 
>     I've run into two scenarios where I haven't been able to find a
>     satisfying solution with the current ParallelArray api.
> 
>     1.
>     Selecting distinct elements. This comes up fairly often, for example,
>     you want to find a set of currently open trades or all the groups some
>     students belong to. The best solution I've been able to come with goes
>     something along the lines of:
> 
>     ParallelArray<Integer> pa = ...
>     final Map<Integer, Integer> m = new ConcurrentHashMap<Integer,
>     Integer>();
>     w.apply(new Procedure<Integer>() {
>         public void apply(Integer t) {
>             if (!m.containsKey(t))
>                 m.put(t, t);
>         }
>     });
>     Set<Integer> result = m.keySet();
> 
>     2.
> 
>     I have a data set that changes rarely but I need to make a lot of
>     read-only calculations on the dataset. So internally I've encapsulated
>     an instance of ParallelArray<Integer> with a ReadWriteLock and then
>     exposing the wrapped methods:
>     public Integer sum() {
>        readLock.lock();
>        return pa.sum();
>     }
>     public size size() {
>         readLock.lock();
>        return pa.size();
>     }
>     public void set(int i, Integer x) {
>        writeLock.lock();
>        pa.set(i, x);
>     }
>     etc.
> 
>     I don't want to expose the ReadWriteLock, because someday I might want
>     to change the implementation. Now, say the client to calculate the
>     average of all integers in the ParallelArray. Normally it would just be
>     pa.sum() / pa.size(). However since pa.sum() and pa.size() are two
>     separate methods, somebody might have changed the dataset in the mean
>     time (I've not included the operations for changing the size). So I need
>     someway to calculate the sum and size in one 'atomic' operation.
> 
>     One solution I've thought is to add something resembling this method:
>     <U> U combineReduce(Combiner<T, U, U> c, Reducer<U> reducer);
> 
>     The idea is that first individual threads work on the combiner. And when
>     they are done, all the individual results are combined using the
>     reducer. Here is the code for both problems:
> 
>     1:
>     HashSet result = w.combineReduce(new Combiner<Integer, HashSet,
>     HashSet>() {
>         public HashSet combine(Integer t, HashSet u) {
>             if (u == null) {
>                 u = new HashSet();
>             }
>             u.add(t);
>             return u;
>         }
> 
>     }, new Reducer<HashSet>() {
>         public HashSet combine(HashSet t, HashSet v) {
>             t.addAll(v);
>             return t;
>         }
>     });
> 
>     2:
>     static class Aggregate {
>         public int count;
>         public long sum;
>     }
> 
>     Aggregate a = w.combineReduce(new Combiner<Integer, Aggregate,
>     Aggregate>() {
>         public Aggregate combine(Integer t, Aggregate u) {
>             if (u == null) {
>                 u = new Aggregate();
>             }
>             u.count++;
>             u.sum += t;
>             return u;
>         }
> 
>     }, new Reducer<Aggregate>() {
>         public Aggregate combine(Aggregate t, Aggregate v) {
>             t.count += v.count;
>             t.sum += v.sum;
>             return t;
>         }
>     });
>     System.out.println ("Average : " + (a.sum / a.count));
> 
>     Thoughts, or any other way to solve these problems?
> 
>     cheers
>       Kasper
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at altair.cs.oswego.edu
>     <mailto:Concurrency-interest at altair.cs.oswego.edu>
>     http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>     <http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> 
> 
> ------------------------------------------------------------------------
> 
> No virus found in this incoming message.
> Checked by AVG Free Edition. 
> Version: 7.5.503 / Virus Database: 269.17.1/1183 - Release Date: 13-12-2007 09:15


From dl at cs.oswego.edu  Wed Dec 19 11:02:59 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 19 Dec 2007 11:02:59 -0500
Subject: [concurrency-interest] some stupid questions
In-Reply-To: <4769339A.4090409@univ-mlv.fr>
References: <47611FA3.9060507@optrak.co.uk>	<47612643.20307@cs.oswego.edu>	<4763E8A9.8010707@cs.oswego.edu>	<63b4e4050712150739x43db64f6t631659e2083ea735@mail.gmail.com>	<47683F50.3000601@cs.oswego.edu>
	<4769339A.4090409@univ-mlv.fr>
Message-ID: <476940B3.1060906@cs.oswego.edu>

Thanks for the suggestions!

R?mi Forax wrote:
> First a remark,
> in the doc of ForkJoinTask, you can see this sentence:
> "The ForkJoinTask class is not directly subclassable outside of this 
> package"
> but the constructor is public ?
> 

An oversight. Thanks!

> Else, why ||*ParallelArray 
> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20int,%20java.lang.Class%29> 
> is not a List,
> it provides get/set/size/iterator but if you want a list you need to 
> create another object
> using asList(). So basically, why he doesn't inherits from AbstractList 
> (and implements RandomAccess) ?

It will be. See the discussion on this a few months ago.
To briefly recap, reformulating as List does better integrate
with other collections, but has a few non-obvious aspects --
mainly that parallelArray.add(x) and similar per-element operations
cannot themselves be threadsafe, which is a little surprising.
But the pluses outweigh the minuses, so the next version will
include all the corresponding adaptations to support List. I've sat
on this pending a couple of algorithmic issues I haven't decided
upon (for example whether/how to parallelize data movement for
element shifts), but ought to put these on hold and implement
correct but suboptimal ones to get this out.

> *I don't like the fact *|*ParallelArray 
> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20int,%20T%5B%5D%29>*(ForkJoinExecutor 
> 
> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html>[] sourceToCopy)| 
> 
> perform a defensive copy and |*ParallelArray 
> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20T%5B%5D%29>*(ForkJoinExecutor 
> [] handoff)
> don't.
> |*
> Perhaps The constructor
> *|*ParallelArray 
> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20T%5B%5D%29>*(ForkJoinExecutor 
>  should be protected
> and a static method "wrap" introduced.

Do you think this would be enough less error-prone to bother doing?
Bearing in mind that ParallelArrays will normally be large, and
that copying is a sequential bottleneck (parallelizing copying is
of limited value in practice), people will need to think about how
much copying they are doing.

> 
> |I  think all  replace* method  should be renamed  to  fill*  |like in 
> java.util.Arrays.
> ||And to be consistent, randomFill() should be fillRandom().
> 

We've at one point or another had this both ways. To me,
the replace prefix sounds a little better.

Other opinions are welcome though. Here's what they look like
with the different prefixes:


     void replaceWithCombination(ParallelArray<? extends T> other, 
Ops.Reducer<T> combiner);
     void replaceWithCombination(T[] other, Ops.Reducer<T> combiner);
     void replaceWithGeneratedValue(Ops.Generator<? extends T> generator);
     void replaceWithMappedIndex(Ops.MapperFromInt<? extends T> mapper);
     void replaceWithTransform(Ops.Mapper<? super T,? extends T> mapper);
     void replaceWithValue(T value);

     void fillWithCombination(ParallelArray<? extends T> other, Ops.Reducer<T> 
combiner);
     void fillWithCombination(T[] other, Ops.Reducer<T> combiner);
     void fillWithGeneratedValue(Ops.Generator<? extends T> generator);
     void fillWithMappedIndex(Ops.MapperFromInt<? extends T> mapper);
     void fillWithTransform(Ops.Mapper<? super T,? extends T> mapper);
     void fillWithValue(T value);



> I wonder if ||*newArray 
> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.WithFilter.html#newArray%28java.lang.Class%29>*(Class 
> <http://java.sun.com/javase/6/docs/api/java/lang/Class.html?is-external=true><? 
> super T 
> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.WithFilter.html>> elementType 
> can't be rewritten to
> <U> ParallelArray<U> newArray(Class<U> elementType) in order to allow 
> upcasting
> if possible and a ClassCastException otherwise.
> 

Thanks. You are right that it would be more consistent with other similar
APIs to support this overloaded version.


-Doug



From tim at peierls.net  Wed Dec 19 12:09:19 2007
From: tim at peierls.net (Tim Peierls)
Date: Wed, 19 Dec 2007 12:09:19 -0500
Subject: [concurrency-interest] ParallelArray Extension
In-Reply-To: <47693505.1060304@kav.dk>
References: <4768F782.6030303@kav.dk>
	<63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>
	<47693505.1060304@kav.dk>
Message-ID: <63b4e4050712190909m5f6835f0j24bb24a63a5a21e3@mail.gmail.com>

On Dec 19, 2007 10:13 AM, Kasper Nielsen <kasper at kav.dk> wrote:

> Tim Peierls wrote:
> > These sound like applications for map-reduce:
> >
> > 1. w.withMapping(elementsToSingletonSets).reduce(combineSets);
> > 2. w.withMapping(elementsToAggregates).reduce(combineAggregates);
> >
> > I'm don't know whether these will perform as well in practice as the
> > solutions you've come up with, but they seem essentially equivalent to
> > your proposed combineReduce.
>
>  From a functional standpoint this works fine, but it doesn't really
> perform. If I create a new object in withMapping. I would need to create
> a new HashSet/Aggregate for each element in ParallelArray.
>

Aren't you effectively doing the same thing in your combineReduce examples?

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071219/f8e56709/attachment.html 

From kasper at kav.dk  Wed Dec 19 12:31:22 2007
From: kasper at kav.dk (Kasper Nielsen)
Date: Wed, 19 Dec 2007 18:31:22 +0100
Subject: [concurrency-interest] ParallelArray Extension
In-Reply-To: <63b4e4050712190909m5f6835f0j24bb24a63a5a21e3@mail.gmail.com>
References: <4768F782.6030303@kav.dk>	
	<63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>	
	<47693505.1060304@kav.dk>
	<63b4e4050712190909m5f6835f0j24bb24a63a5a21e3@mail.gmail.com>
Message-ID: <4769556A.5090603@kav.dk>

Tim Peierls wrote:
> On Dec 19, 2007 10:13 AM, Kasper Nielsen <kasper at kav.dk 
> <mailto:kasper at kav.dk>> wrote:
> 
>     Tim Peierls wrote:
>      > These sound like applications for map-reduce:
>      >
>      > 1. w.withMapping(elementsToSingletonSets).reduce(combineSets);
>      > 2. w.withMapping(elementsToAggregates).reduce(combineAggregates);
>      >
>      > I'm don't know whether these will perform as well in practice as the
>      > solutions you've come up with, but they seem essentially
>     equivalent to
>      > your proposed combineReduce.
> 
>      From a functional standpoint this works fine, but it doesn't really
>     perform. If I create a new object in withMapping. I would need to create
>     a new HashSet/Aggregate for each element in ParallelArray.
> 
> 
> Aren't you effectively doing the same thing in your combineReduce examples?
> 
> --tim
> 

No, only one for each thread:
T1:
Aggregate a=combine(1, null); <- Create new Aggregate
           a=combine(2, a); <-returns a
           a=combine(3, a); <-returns a
....
T2:
Aggregate a=combine(8, null); <- Create new Aggregate
           a=combine(9, a); <-returns a
           a=combine(10, a); <-returns a
....
public Aggregate combine(Integer t, Aggregate u) {
          if (u == null) {
              u = new Aggregate();
          }
          u.count++;
          u.sum += t;
          return u;
}

- Kasper

From forax at univ-mlv.fr  Wed Dec 19 12:44:48 2007
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Wed, 19 Dec 2007 18:44:48 +0100
Subject: [concurrency-interest] some stupid questions
In-Reply-To: <476940B3.1060906@cs.oswego.edu>
References: <47611FA3.9060507@optrak.co.uk>	<47612643.20307@cs.oswego.edu>	<4763E8A9.8010707@cs.oswego.edu>	<63b4e4050712150739x43db64f6t631659e2083ea735@mail.gmail.com>	<47683F50.3000601@cs.oswego.edu>
	<4769339A.4090409@univ-mlv.fr> <476940B3.1060906@cs.oswego.edu>
Message-ID: <47695890.8000707@univ-mlv.fr>

Doug Lea a ?crit :
> Thanks for the suggestions!
>
> R?mi Forax wrote:
>> First a remark,
>> in the doc of ForkJoinTask, you can see this sentence:
>> "The ForkJoinTask class is not directly subclassable outside of this 
>> package"
>> but the constructor is public ?
>>
>
> An oversight. Thanks!
>
>> Else, why ||*ParallelArray 
>> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20int,%20java.lang.Class%29> 
>> is not a List,
>> it provides get/set/size/iterator but if you want a list you need to 
>> create another object
>> using asList(). So basically, why he doesn't inherits from 
>> AbstractList (and implements RandomAccess) ?
>
> It will be. See the discussion on this a few months ago.
sorry.
> To briefly recap, reformulating as List does better integrate
> with other collections, but has a few non-obvious aspects --
> mainly that parallelArray.add(x) and similar per-element operations
> cannot themselves be threadsafe, which is a little surprising.
Is the methods that are not thread safe are the ones that change the size of
the array ?
I see a ParallelArray as a List backed in an array, so get/set are ok
but not add/remove.

> But the pluses outweigh the minuses, so the next version will
> include all the corresponding adaptations to support List. I've sat
> on this pending a couple of algorithmic issues I haven't decided
> upon (for example whether/how to parallelize data movement for
> element shifts), but ought to put these on hold and implement
> correct but suboptimal ones to get this out.
>
>> *I don't like the fact *|*ParallelArray 
>> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20int,%20T%5B%5D%29>*(ForkJoinExecutor 
>>
>> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html>[] 
>> sourceToCopy)|
>> perform a defensive copy and |*ParallelArray 
>> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20T%5B%5D%29>*(ForkJoinExecutor 
>> [] handoff)
>> don't.
>> |*
>> Perhaps The constructor
>> *|*ParallelArray 
>> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20T%5B%5D%29>*(ForkJoinExecutor 
>>  should be protected
>> and a static method "wrap" introduced.
>
> Do you think this would be enough less error-prone to bother doing?
> Bearing in mind that ParallelArrays will normally be large, and
> that copying is a sequential bottleneck (parallelizing copying is
> of limited value in practice), people will need to think about how
> much copying they are doing.
ok, so perhaps the wrong constructor is the other one:
|*ParallelArray 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html#ParallelArray%28jsr166y.forkjoin.ForkJoinExecutor,%20int,%20T%5B%5D%29>*(ForkJoinExecutor 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ForkJoinExecutor.html> executor, 
int size, T 
<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html>[] sourceToCopy)|.

It's a matter of consistency, i think it's not a good idea to have a way 
to create a ParallelArray with a constructor
that do a defensive copy and another one that do nothing. It's error prone.

About a possiblestatic method, because it has a name are, in my opinion, 
a static method is easier to understand than a constructor
(in an other way, a static method is not great if you allow subclasses).
>
>>
>> |I  think all  replace* method  should be renamed  to  fill*  |like 
>> in java.util.Arrays.
>> ||And to be consistent, randomFill() should be fillRandom().
>>
>
> We've at one point or another had this both ways. To me,
> the replace prefix sounds a little better.
>
> Other opinions are welcome though. Here's what they look like
> with the different prefixes:
>
>
>     void replaceWithCombination(ParallelArray<? extends T> other, 
> Ops.Reducer<T> combiner);
>     void replaceWithCombination(T[] other, Ops.Reducer<T> combiner);
>     void replaceWithGeneratedValue(Ops.Generator<? extends T> generator);
>     void replaceWithMappedIndex(Ops.MapperFromInt<? extends T> mapper);
>     void replaceWithTransform(Ops.Mapper<? super T,? extends T> mapper);
>     void replaceWithValue(T value);
i don't like "replace" because in my opinion it implictly refer to one 
element
so perhaps replaceAllWith* is better.
>
>     void fillWithCombination(ParallelArray<? extends T> other, 
> Ops.Reducer<T> combiner);
>     void fillWithCombination(T[] other, Ops.Reducer<T> combiner);
>     void fillWithGeneratedValue(Ops.Generator<? extends T> generator);
>     void fillWithMappedIndex(Ops.MapperFromInt<? extends T> mapper);
>     void fillWithTransform(Ops.Mapper<? super T,? extends T> mapper);
>     void fillWithValue(T value);
>
>
>
>> I wonder if ||*newArray 
>> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.WithFilter.html#newArray%28java.lang.Class%29>*(Class 
>> <http://java.sun.com/javase/6/docs/api/java/lang/Class.html?is-external=true><? 
>> super T 
>> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.WithFilter.html>> 
>> elementType can't be rewritten to
>> <U> ParallelArray<U> newArray(Class<U> elementType) in order to allow 
>> upcasting
>> if possible and a ClassCastException otherwise.
>>
>
> Thanks. You are right that it would be more consistent with other similar
> APIs to support this overloaded version.
>
>
> -Doug
>
cheers,
R?mi

From kasper at kav.dk  Wed Dec 19 15:02:38 2007
From: kasper at kav.dk (Kasper Nielsen)
Date: Wed, 19 Dec 2007 21:02:38 +0100
Subject: [concurrency-interest] ParallelArray Extension
In-Reply-To: <47693466.9050804@cs.oswego.edu>
References: <4768F782.6030303@kav.dk>
	<63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>
	<47693466.9050804@cs.oswego.edu>
Message-ID: <476978DE.7050609@kav.dk>

Doug Lea wrote:
>  But my guess based on what the PLINQ (C#)
> folks are doing is that some people will want further extensions,
> lead to all sorts of syntax/tool/etc issues that I'd rather avoid.

I would prefer as many low level operations as possible in ParallelArray 
and friends, and then if it gets to crowded, move the high-level 
operations into utility classes. Here I'm thinking about mix, max, sum, 
etc...

For example,
public class ParallelOps {

public static <T> T max(ParallelArray<T> array) {
     return array.reduce(new RawMaxReducer<T>(), null);
}

public static <T> T max(ParallelArray.WithMapping<?, T> mapping) {
     return mapping.reduce(new RawMaxReducer<T>(), null);
}

public static int max(ParallelArray.WithIntMapping<?> mapping) {
     return mapping.reduce(NaturalIntMaxReducer.max, Integer.MIN_VALUE);
}

public static int max(ParallelIntArray array) {
     return array.reduce(NaturalIntMaxReducer.max, Integer.MIN_VALUE);
}
...min
...sum
...
}
Instead of call ParallelIntArray.max() you would use something along the 
lines of:
----
import static jsr166y.forkjoin.ParallelOps.max;

ParallelIntArray pia = new ParallelIntArray(new ForkJoinPool(), new 
int[] { 1, 2, 3 });
int max = max(pia);
----
distinct() and other operations could also be added to this class. The 
methods can always call some package-private methods if they can benefit 
from knowing the internal representation of ParallelArray.

finally, if this approach is taken. It might make sense to add these 
interfaces: Reducible, LongReducible, IntReducible, DoubleReducible
public interface Ops.LongReducible {
   long reduce(LongReducer reducer, long base);
}
In this way any datastructure could take advantage of the methods in 
ParallelOps.

- Kasper

From tim at peierls.net  Wed Dec 19 15:09:33 2007
From: tim at peierls.net (Tim Peierls)
Date: Wed, 19 Dec 2007 15:09:33 -0500
Subject: [concurrency-interest] ParallelArray Extension
In-Reply-To: <4769556A.5090603@kav.dk>
References: <4768F782.6030303@kav.dk>
	<63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>
	<47693505.1060304@kav.dk>
	<63b4e4050712190909m5f6835f0j24bb24a63a5a21e3@mail.gmail.com>
	<4769556A.5090603@kav.dk>
Message-ID: <63b4e4050712191209r1dc1addav40e984e26517a986@mail.gmail.com>

On Dec 19, 2007 12:31 PM, Kasper Nielsen <kasper at kav.dk> wrote:

> >     Tim Peierls wrote:
> >      > These sound like applications for map-reduce:
> >      > 1. w.withMapping(elementsToSingletonSets).reduce(combineSets);
> >      > 2. w.withMapping(elementsToAggregates).reduce(combineAggregates);
> >      > ...seem essentially equivalent to your proposed combineReduce.
> >
> >     From a functional standpoint this works fine, but it doesn't really
> >     perform. If I create a new object in withMapping. I would need to
> create
> >     a new HashSet/Aggregate for each element in ParallelArray.
> >
> > Aren't you effectively doing the same thing in your combineReduce
> examples?
>
> No, only one for each thread:
> T1:
> Aggregate a=combine(1, null); <- Create new Aggregate
>           a=combine(2, a); <-returns a
>           a=combine(3, a); <-returns a
> ....
> T2:
> Aggregate a=combine(8, null); <- Create new Aggregate
>           a=combine(9, a); <-returns a
>           a=combine(10, a); <-returns a
> ....
>

ParallelArray.combine works through a subclass of RecursiveAction, so that
there would be as many calls to combine(x, null) as there are leaves in the
recursive decomposition, more or less -- probably less since Doug does some
fancy things to improve on this. So I think the number of Aggregate
constructions in combineReduce would be only a constant factor (related to
parallelism level) less than in map-reduce. I don't think it would be
directly related to the number of threads available to the ForkJoinPool.

Aggregates, as you present them, are lightweight objects. It's quite
possible that the cost of creating one for each element is dominated by the
parallelism benefits when you have a lot of processors.

With the Sets, it isn't so simple. Instead of Set, I'd want to use a purely
functional data structure designed for efficient merging, such as found in
Chris Okasaki's book, *Purely Functional Data Structures*. But presumably
Doug can find a way to do this even more efficiently internally.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071219/caa73d5b/attachment.html 

From tim at peierls.net  Wed Dec 19 17:10:37 2007
From: tim at peierls.net (Tim Peierls)
Date: Wed, 19 Dec 2007 17:10:37 -0500
Subject: [concurrency-interest] ParallelArray Extension
In-Reply-To: <63b4e4050712191209r1dc1addav40e984e26517a986@mail.gmail.com>
References: <4768F782.6030303@kav.dk>
	<63b4e4050712190651x5f1f47bek63c9811bb120078c@mail.gmail.com>
	<47693505.1060304@kav.dk>
	<63b4e4050712190909m5f6835f0j24bb24a63a5a21e3@mail.gmail.com>
	<4769556A.5090603@kav.dk>
	<63b4e4050712191209r1dc1addav40e984e26517a986@mail.gmail.com>
Message-ID: <63b4e4050712191410m1fa8faa2vab8e57b394406c4b@mail.gmail.com>

On Dec 19, 2007 3:09 PM, Tim Peierls <tim at peierls.net> wrote:

> ...I think the number of Aggregate constructions in combineReduce would be
> only a constant factor (related to parallelism level) less than in
> map-reduce. I don't think it would be directly related to the number of
> threads available to the ForkJoinPool.


I'm wrong on both counts. The method that decides whether to split a
computation of size n or call it a leaf aims for # leaf tasks = 2 * the
parallelism level, and for ForkJoinPool the parallelism level is just the
pool size. It allows more tasks (dynamically) if there are few stealable
tasks, but I have no idea how often this would kick in for the two examples
in this thread.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071219/6db499be/attachment.html 

From dhanji at gmail.com  Thu Dec 20 03:07:07 2007
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Thu, 20 Dec 2007 18:07:07 +1000
Subject: [concurrency-interest] jcip annotations
Message-ID: <aa067ea10712200007y68f2fd83p2b2916e66dc9a7d4@mail.gmail.com>

Hi

I find the jcip annotations quite useful and like Kevin Bourrillion
said some time ago wish there was an expanded set for more cases (an
@ThreadConfined springs to mind).

However I find it odd that they have runtime retention policies. This
makes it awkward for me to package libraries that use the jcip
annotations in source code. It also does not seem to jive with the
following stated intent (from package-info):

"These annotations are relatively unintrusive and are beneficial to
both users and maintainers. Users can see immediately whether a class
is thread-safe, and maintainers can see immediately whether
thread-safety guarantees must be preserved. Annotations are also
useful to a third constituency: tools. Static code-analysis tools may
be able to verify that the code complies with the contract indicated
by the annotation, such as verifying that a class annotated with
@Immutable actually is immutable."

I believe source-level retention would be the correct choice (at best,
class-level). I cannot see any value in them being retained at runtime
and in fact a potential nuisance to reflection and debugging...  I
suppose it is trivial for me to recompile the annotations with a
reduced retention but I am curious as to the motivation behind
retaining them at runtime.

Thanks,

Dhanji.

From tim at peierls.net  Thu Dec 20 09:29:59 2007
From: tim at peierls.net (Tim Peierls)
Date: Thu, 20 Dec 2007 09:29:59 -0500
Subject: [concurrency-interest] jcip annotations
In-Reply-To: <aa067ea10712200007y68f2fd83p2b2916e66dc9a7d4@mail.gmail.com>
References: <aa067ea10712200007y68f2fd83p2b2916e66dc9a7d4@mail.gmail.com>
Message-ID: <63b4e4050712200629l5165ce6cwbd3b8c1d78dc9455@mail.gmail.com>

On Dec 20, 2007 3:07 AM, Dhanji R. Prasanna <dhanji at gmail.com> wrote:

> I believe source-level retention would be the correct choice (at best,
> class-level).


At least class level, so tools like FindBugs can do their thing.



> I cannot see any value in them being retained at runtime and in fact a
> potential nuisance to reflection and debugging...


I'm curious, how is the presence of a runtime annotation a nuisance?



> I suppose it is trivial for me to recompile the annotations with a reduced
> retention but I am curious as to the motivation behind retaining them at
> runtime.
>

FindBugs does static analysis of bytecode, but other tools could do dynamic
analysis of running code.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071220/def483c4/attachment.html 

From Darron_Shaffer at stercomm.com  Thu Dec 20 11:27:07 2007
From: Darron_Shaffer at stercomm.com (Shaffer, Darron)
Date: Thu, 20 Dec 2007 11:27:07 -0500
Subject: [concurrency-interest] jcip annotations
In-Reply-To: <63b4e4050712200629l5165ce6cwbd3b8c1d78dc9455@mail.gmail.com>
Message-ID: <FC30D8A2D3DEE64D93E8DA54A1DB349AEC00C9@IWDUBCORMSG007.sci.local>

The presence of a runtime annotation means one more jar file to ship
with a product.  And one more to run past legal if it's a commercial
product.

 

________________________________

From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Tim
Peierls
Sent: Thursday, December 20, 2007 8:30 AM
To: Dhanji R. Prasanna
Cc: concurrency-interest
Subject: Re: [concurrency-interest] jcip annotations

 

On Dec 20, 2007 3:07 AM, Dhanji R. Prasanna <dhanji at gmail.com> wrote:

	I believe source-level retention would be the correct choice (at
best, class-level). 


At least class level, so tools like FindBugs can do their thing.

 

	I cannot see any value in them being retained at runtime and in
fact a potential nuisance to reflection and debugging...  


I'm curious, how is the presence of a runtime annotation a nuisance?

 

	I suppose it is trivial for me to recompile the annotations with
a reduced retention but I am curious as to the motivation behind
retaining them at runtime.


FindBugs does static analysis of bytecode, but other tools could do
dynamic analysis of running code. 

--tim

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071220/ebad8618/attachment.html 

From tim at peierls.net  Fri Dec 21 09:38:55 2007
From: tim at peierls.net (Tim Peierls)
Date: Fri, 21 Dec 2007 09:38:55 -0500
Subject: [concurrency-interest] jcip annotations
In-Reply-To: <FC30D8A2D3DEE64D93E8DA54A1DB349AEC00C9@IWDUBCORMSG007.sci.local>
References: <63b4e4050712200629l5165ce6cwbd3b8c1d78dc9455@mail.gmail.com>
	<FC30D8A2D3DEE64D93E8DA54A1DB349AEC00C9@IWDUBCORMSG007.sci.local>
Message-ID: <63b4e4050712210638s7874570j820209d4d0284237@mail.gmail.com>

>
> [Dhanji] I cannot see any value in them being retained at runtime and in
> fact a potential nuisance to reflection and debugging...
>
>
> [Tim] I'm curious, how is the presence of a runtime annotation a nuisance?
>

On Dec 20, 2007 11:27 AM, Shaffer, Darron <Darron_Shaffer at stercomm.com>
wrote:

>  The presence of a runtime annotation means one more jar file to ship with
> a product.  And one more to run past legal if it's a commercial product.
>

OK, but those are production issues that can be addressed at packaging time.
I was curious what Dhanji meant by "potential nuisance to reflection and
debugging"?

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071221/a1888395/attachment.html 

From dhanji at gmail.com  Fri Dec 21 11:08:17 2007
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Sat, 22 Dec 2007 02:08:17 +1000
Subject: [concurrency-interest] jcip annotations
In-Reply-To: <63b4e4050712210638s7874570j820209d4d0284237@mail.gmail.com>
References: <63b4e4050712200629l5165ce6cwbd3b8c1d78dc9455@mail.gmail.com>
	<FC30D8A2D3DEE64D93E8DA54A1DB349AEC00C9@IWDUBCORMSG007.sci.local>
	<63b4e4050712210638s7874570j820209d4d0284237@mail.gmail.com>
Message-ID: <aa067ea10712210808j630577c7p810cb0cee8e8b4c5@mail.gmail.com>

On 12/22/07, Tim Peierls <tim at peierls.net> wrote:
>
>  [Dhanji] I cannot see any value in them being retained at runtime and in
> > fact a potential nuisance to reflection and debugging...
> >
> >
> > [Tim] I'm curious, how is the presence of a runtime annotation a
> > nuisance?
> >
>
> On Dec 20, 2007 11:27 AM, Shaffer, Darron <Darron_Shaffer at stercomm.com>
> wrote:
>
> >  The presence of a runtime annotation means one more jar file to ship
> > with a product.  And one more to run past legal if it's a commercial
> > product.
> >
>
> OK, but those are production issues that can be addressed at packaging
> time. I was curious what Dhanji meant by "potential nuisance to reflection
> and debugging"?
>


Nothing more than that they add distraction and clutter to an increasingly
cluttered field of runtime-annotations.


Dhanji.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071222/4ff3ac92/attachment.html 

From dl at cs.oswego.edu  Wed Dec 26 11:37:49 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 26 Dec 2007 11:37:49 -0500
Subject: [concurrency-interest] In-progress ParallelArray changes
Message-ID: <4772835D.6060202@cs.oswego.edu>

As punishment :-) to those of you who have tried ParallelArray
and related classes and offered suggestions, we're taking
people up on some of them, and doing a round of API updates.

Over the next few days or so, a bunch of changes will be
committed to ParallelArray. The javadocs will precede the
committed changes, so you can see what will be happening
before you can actually run these versions. Snapshots of
the the main javadoc for ParallelArray are at:
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/ParallelArray.html

Here's a summary:

* A full-featured List view accessed via asList: You can
   efficiently perform all List operations (add, remove, etc), on
   ParallelArrays without needing to copy to/from
   lists. This turns out to work more
   nicely in terms of both API and internal mechanics than
   would redefining ParallelArray to itself be a List.
* Static factory methods instead of constructors
* A static defaultExecutor method to make it easier to
   use the same global ForkJoinExecutor across ParallelArrays.
* Elimination of "Int" specializations. As discussed once
   before, having ParallelIntArray and WithIntPapping in addition to
   the Long versions was pretty marginal on performance grounds.
   Adding list views led to enough additional adaptation snags to
   tip the balance against it.
* Better support for set-like operations: allUniqueElements,
   WithFilter.removeAll, binarySearch on sorted arrays, and
   a few others.
* A few method renamings to make the above a bit more consistent.
* Corresponding changes to ParallelDoubleArray and ParallelLongArray
   (that will probably lag those of ParallelArray.)

As always, comments and suggestions would be very welcome.
Continued thanks to those of you who have been trying these out!

-Doug

From tkramer at hampshire.edu  Thu Dec 27 17:39:15 2007
From: tkramer at hampshire.edu (Trevor Kramer)
Date: Thu, 27 Dec 2007 14:39:15 -0800
Subject: [concurrency-interest] Exception handling with ParallelArray
Message-ID: <f9cec2830712271439v555c91e5o8b9d896dd6c3c59c@mail.gmail.com>

If I am using ParallelArray.apply and am doing something inside the
apply() that throws a checked exception (that I have to catch since
Ops.Procedure doesn't throw exceptions) is there a standard way to get
that exception out to the calling thread?

Thanks,

Trevor

From dl at cs.oswego.edu  Fri Dec 28 07:20:12 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 28 Dec 2007 07:20:12 -0500
Subject: [concurrency-interest] Exception handling with ParallelArray
In-Reply-To: <f9cec2830712271439v555c91e5o8b9d896dd6c3c59c@mail.gmail.com>
References: <f9cec2830712271439v555c91e5o8b9d896dd6c3c59c@mail.gmail.com>
Message-ID: <4774E9FC.7070808@cs.oswego.edu>

Trevor Kramer wrote:
> If I am using ParallelArray.apply and am doing something inside the
> apply() that throws a checked exception (that I have to catch since
> Ops.Procedure doesn't throw exceptions) is there a standard way to get
> that exception out to the calling thread?
> 

If you really need to do this (see below),
the (messy) recommended strategy is to tunnel it as
an unchecked exception. For example, if you make a call
to method f() that can throw some checked exception Ex,
you can declare:
   class TrappedEx extends RuntimeException {
      TrappedEx(Ex cause) { super(cause); }
   }
and then inside the Procedure do
   try {
      f();
   } catch(Ex ex) {
     throw new TrappedEx(ex);
   }
And the surround the ParallelArray.apply:
   try {
      pa.apply(theProcedure);
   } catch(TrappedEx ex) {
      // handle it
   }

There are a few reasons we force you to do this:

1. In practice, the majority of Checked Exceptions are defined
   for operations like IO that don't belong in forkjoin
   computations anyway.

2. While we internally perform the cleanup and cancellation
   across parallel threads executing the apply when any one of them
   encounters an exception, we can't supply
   any points at which you can locally handle the exception until
   they complete, at which point any encountered (unchecked) exception
   is rethrown. (Of course, if you can locally handle the
   exception inside the Procedure, you don't need any of this anyway.)

3. Until/unless better support for disjunctive exception types
   emerges, we can't nicely support methods that allow arbitrary
   combinations of exceptions on Procedures etc, except by using
   the Callable strategy which permits any exception. Which people
   have grown to dislike, because it forces you to place try/catch
   blocks around every call, even if you know that no handleable
   exceptions are thrown. (Which in turn leads people to use empty
   catches, which are much worse than not catching at all.) Given
   (1) and (2) above though, it seems unlikely that we'd include
   support for fancier exception types even if it becomes available.


-Doug



From neal at gafter.com  Fri Dec 28 08:50:03 2007
From: neal at gafter.com (Neal Gafter)
Date: Fri, 28 Dec 2007 05:50:03 -0800
Subject: [concurrency-interest] Exception handling with ParallelArray
In-Reply-To: <4774E9FC.7070808@cs.oswego.edu>
References: <f9cec2830712271439v555c91e5o8b9d896dd6c3c59c@mail.gmail.com>
	<4774E9FC.7070808@cs.oswego.edu>
Message-ID: <15e8b9d20712280550u2ab43fcetb2974ba40920431d@mail.gmail.com>

On Dec 28, 2007 4:20 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> 2. While we internally perform the cleanup and cancellation
>   across parallel threads executing the apply when any one of them
>   encounters an exception, we can't supply
>   any points at which you can locally handle the exception until
>   they complete, at which point any encountered (unchecked) exception
>   is rethrown. (Of course, if you can locally handle the
>   exception inside the Procedure, you don't need any of this anyway.)
>
> 3. Until/unless better support for disjunctive exception types
>   emerges, we can't nicely support methods that allow arbitrary
>   combinations of exceptions on Procedures etc, except by using
>   the Callable strategy which permits any exception. Which people
>   have grown to dislike, because it forces you to place try/catch
>   blocks around every call, even if you know that no handleable
>   exceptions are thrown. (Which in turn leads people to use empty
>   catches, which are much worse than not catching at all.) Given
>   (1) and (2) above though, it seems unlikely that we'd include
>   support for fancier exception types even if it becomes available.


I don't get it.  (2), above, seems like a reason to include fancier support
for exception tunneling (the generic exception type can be placed in the
throws clause of ParallelArray.apply), rather than a reason not to.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071228/2f12ba58/attachment.html 

From dl at cs.oswego.edu  Fri Dec 28 08:57:34 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 28 Dec 2007 08:57:34 -0500
Subject: [concurrency-interest] Exception handling with ParallelArray
In-Reply-To: <15e8b9d20712280550u2ab43fcetb2974ba40920431d@mail.gmail.com>
References: <f9cec2830712271439v555c91e5o8b9d896dd6c3c59c@mail.gmail.com>
	<4774E9FC.7070808@cs.oswego.edu>
	<15e8b9d20712280550u2ab43fcetb2974ba40920431d@mail.gmail.com>
Message-ID: <477500CE.7010601@cs.oswego.edu>

Neal Gafter wrote:
> I don't get it.  (2), above, seems like a reason to include fancier 
> support for exception tunneling (the generic exception type can be 
> placed in the throws clause of ParallelArray.apply), rather than a 
> reason not to.

Maybe so. I don't think I've seen a proposal that boils down
to this sort of selective tunneling without adding more declaration
overhead in those cases it isn't used though.

-Doug

From neal at gafter.com  Fri Dec 28 09:45:48 2007
From: neal at gafter.com (Neal Gafter)
Date: Fri, 28 Dec 2007 06:45:48 -0800
Subject: [concurrency-interest] Exception handling with ParallelArray
In-Reply-To: <477500CE.7010601@cs.oswego.edu>
References: <f9cec2830712271439v555c91e5o8b9d896dd6c3c59c@mail.gmail.com>
	<4774E9FC.7070808@cs.oswego.edu>
	<15e8b9d20712280550u2ab43fcetb2974ba40920431d@mail.gmail.com>
	<477500CE.7010601@cs.oswego.edu>
Message-ID: <15e8b9d20712280645q562c825bof1a3fc318c0f574a@mail.gmail.com>

On Dec 28, 2007 5:57 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> Neal Gafter wrote:
> > I don't get it.  (2), above, seems like a reason to include fancier
> > support for exception tunneling (the generic exception type can be
> > placed in the throws clause of ParallelArray.apply), rather than a
> > reason not to.
>
> Maybe so. I don't think I've seen a proposal that boils down
> to this sort of selective tunneling without adding more declaration
> overhead in those cases it isn't used though.


Again, I don't get it.  This comment sounds to me like it is conflating
declaration with use.  I agree that we want the uses not to be cluttered,
but adding a bit to the declaration doesn't seem harmful if it adds
something to the usabiity of the API.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071228/afbe5b67/attachment.html 

From neal at gafter.com  Fri Dec 28 10:05:43 2007
From: neal at gafter.com (Neal Gafter)
Date: Fri, 28 Dec 2007 07:05:43 -0800
Subject: [concurrency-interest] Exception handling with ParallelArray
In-Reply-To: <15e8b9d20712280645q562c825bof1a3fc318c0f574a@mail.gmail.com>
References: <f9cec2830712271439v555c91e5o8b9d896dd6c3c59c@mail.gmail.com>
	<4774E9FC.7070808@cs.oswego.edu>
	<15e8b9d20712280550u2ab43fcetb2974ba40920431d@mail.gmail.com>
	<477500CE.7010601@cs.oswego.edu>
	<15e8b9d20712280645q562c825bof1a3fc318c0f574a@mail.gmail.com>
Message-ID: <15e8b9d20712280705n526515a3q9a34995a31c1ba47@mail.gmail.com>

On Dec 28, 2007 6:45 AM, Neal Gafter <neal at gafter.com> wrote:

> On Dec 28, 2007 5:57 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>
> > Neal Gafter wrote:
> > > I don't get it.  (2), above, seems like a reason to include fancier
> > > support for exception tunneling (the generic exception type can be
> > > placed in the throws clause of ParallelArray.apply), rather than a
> > > reason not to.
> >
> > Maybe so. I don't think I've seen a proposal that boils down
> > to this sort of selective tunneling without adding more declaration
> > overhead in those cases it isn't used though.
>
>
> Again, I don't get it.  This comment sounds to me like it is conflating
> declaration with use.  I agree that we want the uses not to be cluttered,
> but adding a bit to the declaration doesn't seem harmful if it adds
> something to the usabiity of the API.
>

Just to be perfectly clear, using BGGA closures one can define apply this
way:

class ParallelArray<T> {
    <throws X> void apply({ T => void throws X } proc) throws X { ... }
}

or, using a nominal type

interface Proc<T, throws X> {
    void apply(T t) throws X;
}
class ParallelArray<T> {
    <throws X> void apply(Proc<? super String, ? extends X> proc) throws X {
... }
}

and use it (either version) this way:

ParallelArray<String> pa = ...;
try {
    pa.apply({ String s => doSomethingThatMightThrowSomeException(s); })
} catch (*SomeException* ex) {
    // handle the exception here
}

Admittedly, doing this with anonymous inner classes (or CICE) would be more
painful, as you have to repeatedly call out the thrown exception type, and
it gets worse if you attempt to nest such calls:

ParallelArray<String> pa = ...;
try {
    pa.apply(new Proc<String,*SomeException*>() {
        public void apply(String s) throws *SomeException* {
            doSomethingThatMightThrowSomeException(s);
        }
    });
} catch (*SomeException* ex) {
    // handle the exception here
}

Incidentally, this all works today in the BGGA prototype.

Regards,
Neal
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20071228/3b3f87c3/attachment.html 

From hanson.char at gmail.com  Sat Dec 29 17:40:09 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 29 Dec 2007 14:40:09 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
	problem ?
Message-ID: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>

(I refer to JDK 6+). It seems a scheduled event via

  ScheduledThreadPoolExecutor.schedule(Runnable command, long delay,
TimeUnit unit)

may occur before the target time.  For example, if an event is
scheduled to occur 1000ms from now.  The event could actually get
triggered after 990ms ie 10 ms less than the specified delay.

Is this a known issue ?  Shouldn't the event be delayed for at least
the specified delay time ?

Hanson Char

Original discussion:

    http://sourceforge.net/tracker/index.php?func=detail&aid=1860222&group_id=111957&atid=660863

From dl at cs.oswego.edu  Mon Dec 31 08:59:39 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 31 Dec 2007 08:59:39 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
 problem ?
In-Reply-To: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>
Message-ID: <4778F5CB.40209@cs.oswego.edu>

Hanson Char wrote:
> (I refer to JDK 6+). It seems a scheduled event via
> 
>   ScheduledThreadPoolExecutor.schedule(Runnable command, long delay,
> TimeUnit unit)
> 
> may occur before the target time.  For example, if an event is
> scheduled to occur 1000ms from now.  The event could actually get
> triggered after 990ms ie 10 ms less than the specified delay.
> 
> Is this a known issue ?  Shouldn't the event be delayed for at least
> the specified delay time ?
> 

I hope David Holmes also answers this, since he knows most about
platform-level timing issues. But I don't know of any problems
that affect ScheduledThreadPoolExecutor in this way. Internally,
methods recheck times to ensure that they do not fire too early.
(See DelayQueue.take and related methods). On some platforms
(including Windows), there may be some *apparent* differences
in times if you are monitoring times with System.currentTimeMillis()
versus System.nanoTime() -- these use different clocks that may become
out of sync. ScheduledThreadPoolExecutor uses nanoTime, which should in
general be more accurate.

-Doug

From hanson.char at gmail.com  Mon Dec 31 12:01:12 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 31 Dec 2007 09:01:12 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
	problem ?
In-Reply-To: <4778F5CB.40209@cs.oswego.edu>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>
	<4778F5CB.40209@cs.oswego.edu>
Message-ID: <ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>

This is really strange.  The problem occurred at least on Linux
platforms.  The one I am using is

  Linux version 2.4.21-37a8 (gcc version 3.2.3 20030502 (Red Hat Linux
3.2.3-47))

Hey John, you mentioned you had a unit test that exhibits the problem
intermittently.  Would you mind sharing it so we potentially can have
it fixed by the experts ?

Regards,
Hanson Char

On Dec 31, 2007 5:59 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> Hanson Char wrote:
> > (I refer to JDK 6+). It seems a scheduled event via
> >
> >   ScheduledThreadPoolExecutor.schedule(Runnable command, long delay,
> > TimeUnit unit)
> >
> > may occur before the target time.  For example, if an event is
> > scheduled to occur 1000ms from now.  The event could actually get
> > triggered after 990ms ie 10 ms less than the specified delay.
> >
> > Is this a known issue ?  Shouldn't the event be delayed for at least
> > the specified delay time ?
> >
>
> I hope David Holmes also answers this, since he knows most about
> platform-level timing issues. But I don't know of any problems
> that affect ScheduledThreadPoolExecutor in this way. Internally,
> methods recheck times to ensure that they do not fire too early.
> (See DelayQueue.take and related methods). On some platforms
> (including Windows), there may be some *apparent* differences
> in times if you are monitoring times with System.currentTimeMillis()
> versus System.nanoTime() -- these use different clocks that may become
> out of sync. ScheduledThreadPoolExecutor uses nanoTime, which should in
> general be more accurate.
>
> -Doug
>

From hanson.char at gmail.com  Mon Dec 31 14:10:49 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 31 Dec 2007 11:10:49 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
	problem ?
In-Reply-To: <ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>
	<4778F5CB.40209@cs.oswego.edu>
	<ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>
Message-ID: <ca53c8f80712311110t7bb030ddm63ff0b0a6f5f6db@mail.gmail.com>

Please find below is a simple test that consistently exhibits the
earlier-than-expected scheduling problem under java version "1.6.0_01"
on Windows XP.  So far it doesn't seem to have the problem when run on
Linux, though.

So does this mean the delay of ScheduledThreadPoolExecutor is not
guaranteed to be at least equal to the specified amount, but could be
earlier ?  It would be nice if this could be fixed to work on all
platforms.

Hanson Char

import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

public class TestScheduledThreadPoolExecutor {
	private static final long PERIOD = 1000;
	
	public void test() {
		final ScheduledExecutorService ses = Executors.newScheduledThreadPool(1);
		final long[] start = {System.currentTimeMillis()};
		final int[] count = {0};
		
		ses.schedule(new Runnable() {
			public void run() {
				long elapsed = System.currentTimeMillis() - start[0];
				System.out.println(elapsed);
				
				if (elapsed < PERIOD)
					System.err.println("Oops " + (PERIOD-elapsed));
				if (++count[0] < 30) {
					start[0] = System.currentTimeMillis();
					ses.schedule(this, PERIOD, TimeUnit.MILLISECONDS);
				}
				else
					System.exit(0);
			}
		}, PERIOD, TimeUnit.MILLISECONDS);
	}
	
	public static void main(String...args) {
		new TestScheduledThreadPoolExecutor().test();
	}
}

Sample output:

1001
1002
1001
1001
1002
1001
1002
1001
1002
991
Oops 9
1002
1001
1001
1002
1001
1002
1001
1002
1001
1001
1002
1001
1002
1001
1002
1001
Oops 8
992
1001
1001
1002

On Dec 31, 2007 9:01 AM, Hanson Char <hanson.char at gmail.com> wrote:
> This is really strange.  The problem occurred at least on Linux
> platforms.  The one I am using is
>
>   Linux version 2.4.21-37a8 (gcc version 3.2.3 20030502 (Red Hat Linux
> 3.2.3-47))
>
> Hey John, you mentioned you had a unit test that exhibits the problem
> intermittently.  Would you mind sharing it so we potentially can have
> it fixed by the experts ?
>
> Regards,
> Hanson Char
>
>
> On Dec 31, 2007 5:59 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> > Hanson Char wrote:
> > > (I refer to JDK 6+). It seems a scheduled event via
> > >
> > >   ScheduledThreadPoolExecutor.schedule(Runnable command, long delay,
> > > TimeUnit unit)
> > >
> > > may occur before the target time.  For example, if an event is
> > > scheduled to occur 1000ms from now.  The event could actually get
> > > triggered after 990ms ie 10 ms less than the specified delay.
> > >
> > > Is this a known issue ?  Shouldn't the event be delayed for at least
> > > the specified delay time ?
> > >
> >
> > I hope David Holmes also answers this, since he knows most about
> > platform-level timing issues. But I don't know of any problems
> > that affect ScheduledThreadPoolExecutor in this way. Internally,
> > methods recheck times to ensure that they do not fire too early.
> > (See DelayQueue.take and related methods). On some platforms
> > (including Windows), there may be some *apparent* differences
> > in times if you are monitoring times with System.currentTimeMillis()
> > versus System.nanoTime() -- these use different clocks that may become
> > out of sync. ScheduledThreadPoolExecutor uses nanoTime, which should in
> > general be more accurate.
> >
> > -Doug

From hanson.char at gmail.com  Mon Dec 31 14:19:39 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 31 Dec 2007 11:19:39 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
	problem ?
In-Reply-To: <ca53c8f80712311110t7bb030ddm63ff0b0a6f5f6db@mail.gmail.com>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>
	<4778F5CB.40209@cs.oswego.edu>
	<ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>
	<ca53c8f80712311110t7bb030ddm63ff0b0a6f5f6db@mail.gmail.com>
Message-ID: <ca53c8f80712311119ud46f3fet36e5f16728e18865@mail.gmail.com>

Interestingly when I changed the test to replace all uses of
milli-time with nano-time, the earlier-than-expected problem seems to
go away.  (Nano-test code below.)

The lesson/solution seems apparent.  Replace all use of
System.currentTimeMillis() with System.nanoTime().

Regards,
Hanson Char

import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

public class TestNanoScheduledThreadPoolExecutor {
	
	private static final long PERIOD = 1000*1000*1000;
	
	public void test() {
		final ScheduledExecutorService ses = Executors.newScheduledThreadPool(1);
		final long[] start = {System.nanoTime()};
		final int[] count = {0};
		
		ses.schedule(new Runnable() {
			public void run() {
				long elapsed = System.nanoTime() - start[0];
				System.out.println(elapsed);
				
				if (elapsed < PERIOD)
					System.err.println("Oops " + (PERIOD-elapsed));
				if (++count[0] < 30) {
					start[0] = System.nanoTime();
					ses.schedule(this, PERIOD, TimeUnit.NANOSECONDS);
				}
				else
					System.exit(0);
			}
		}, PERIOD, TimeUnit.NANOSECONDS);
	}
	
	public static void main(String...args) {
		new TestNanoScheduledThreadPoolExecutor().test();
	}
}


On Dec 31, 2007 11:10 AM, Hanson Char <hanson.char at gmail.com> wrote:
> Please find below is a simple test that consistently exhibits the
> earlier-than-expected scheduling problem under java version "1.6.0_01"
> on Windows XP.  So far it doesn't seem to have the problem when run on
> Linux, though.
>
> So does this mean the delay of ScheduledThreadPoolExecutor is not
> guaranteed to be at least equal to the specified amount, but could be
> earlier ?  It would be nice if this could be fixed to work on all
> platforms.
>
> Hanson Char
>
> import java.util.concurrent.Executors;
> import java.util.concurrent.ScheduledExecutorService;
> import java.util.concurrent.TimeUnit;
>
> public class TestScheduledThreadPoolExecutor {
>         private static final long PERIOD = 1000;
>
>         public void test() {
>                 final ScheduledExecutorService ses = Executors.newScheduledThreadPool(1);
>                 final long[] start = {System.currentTimeMillis()};
>                 final int[] count = {0};
>
>                 ses.schedule(new Runnable() {
>                         public void run() {
>                                 long elapsed = System.currentTimeMillis() - start[0];
>                                 System.out.println(elapsed);
>
>                                 if (elapsed < PERIOD)
>                                         System.err.println("Oops " + (PERIOD-elapsed));
>                                 if (++count[0] < 30) {
>                                         start[0] = System.currentTimeMillis();
>                                         ses.schedule(this, PERIOD, TimeUnit.MILLISECONDS);
>                                 }
>                                 else
>                                         System.exit(0);
>                         }
>                 }, PERIOD, TimeUnit.MILLISECONDS);
>         }
>
>         public static void main(String...args) {
>                 new TestScheduledThreadPoolExecutor().test();
>         }
> }
>
> Sample output:
>
> 1001
> 1002
> 1001
> 1001
> 1002
> 1001
> 1002
> 1001
> 1002
> 991
> Oops 9
> 1002
> 1001
> 1001
> 1002
> 1001
> 1002
> 1001
> 1002
> 1001
> 1001
> 1002
> 1001
> 1002
> 1001
> 1002
> 1001
> Oops 8
> 992
> 1001
> 1001
> 1002
>
> On Dec 31, 2007 9:01 AM, Hanson Char <hanson.char at gmail.com> wrote:
>
> > This is really strange.  The problem occurred at least on Linux
> > platforms.  The one I am using is
> >
> >   Linux version 2.4.21-37a8 (gcc version 3.2.3 20030502 (Red Hat Linux
> > 3.2.3-47))
> >
> > Hey John, you mentioned you had a unit test that exhibits the problem
> > intermittently.  Would you mind sharing it so we potentially can have
> > it fixed by the experts ?
> >
> > Regards,
> > Hanson Char
> >
> >
> > On Dec 31, 2007 5:59 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> > > Hanson Char wrote:
> > > > (I refer to JDK 6+). It seems a scheduled event via
> > > >
> > > >   ScheduledThreadPoolExecutor.schedule(Runnable command, long delay,
> > > > TimeUnit unit)
> > > >
> > > > may occur before the target time.  For example, if an event is
> > > > scheduled to occur 1000ms from now.  The event could actually get
> > > > triggered after 990ms ie 10 ms less than the specified delay.
> > > >
> > > > Is this a known issue ?  Shouldn't the event be delayed for at least
> > > > the specified delay time ?
> > > >
> > >
> > > I hope David Holmes also answers this, since he knows most about
> > > platform-level timing issues. But I don't know of any problems
> > > that affect ScheduledThreadPoolExecutor in this way. Internally,
> > > methods recheck times to ensure that they do not fire too early.
> > > (See DelayQueue.take and related methods). On some platforms
> > > (including Windows), there may be some *apparent* differences
> > > in times if you are monitoring times with System.currentTimeMillis()
> > > versus System.nanoTime() -- these use different clocks that may become
> > > out of sync. ScheduledThreadPoolExecutor uses nanoTime, which should in
> > > general be more accurate.
> > >
> > > -Doug
>

From dl at cs.oswego.edu  Mon Dec 31 14:28:07 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 31 Dec 2007 14:28:07 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
 problem ?
In-Reply-To: <ca53c8f80712311119ud46f3fet36e5f16728e18865@mail.gmail.com>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>	
	<4778F5CB.40209@cs.oswego.edu>	
	<ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>	
	<ca53c8f80712311110t7bb030ddm63ff0b0a6f5f6db@mail.gmail.com>
	<ca53c8f80712311119ud46f3fet36e5f16728e18865@mail.gmail.com>
Message-ID: <477942C7.9070409@cs.oswego.edu>

Hanson Char wrote:
> Interestingly when I changed the test to replace all uses of
> milli-time with nano-time, the earlier-than-expected problem seems to
> go away.  (Nano-test code below.)
> 
> The lesson/solution seems apparent.  Replace all use of
> System.currentTimeMillis() with System.nanoTime().


Yes, It is easily possible for nanoTime-based vs currentTimeMillis-based
duration estimates to differ once in a while;
for example, because the underlying currentTimeMillis
system clock is typically updated less frequently.
On average they balance out, but
if you need consistency across all readings, you should rely on nanoTime.


-Doug



From hanson.char at gmail.com  Mon Dec 31 14:34:51 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 31 Dec 2007 11:34:51 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
	problem ?
In-Reply-To: <477942C7.9070409@cs.oswego.edu>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>
	<4778F5CB.40209@cs.oswego.edu>
	<ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>
	<ca53c8f80712311110t7bb030ddm63ff0b0a6f5f6db@mail.gmail.com>
	<ca53c8f80712311119ud46f3fet36e5f16728e18865@mail.gmail.com>
	<477942C7.9070409@cs.oswego.edu>
Message-ID: <ca53c8f80712311134h338fbf2cj2be6b3a0373604df@mail.gmail.com>

Is it true however that System.nanoTime() incurs more overhead than
System.currentTimeMillis(), since it has a million time more accuracy
?

Hanson Char

On Dec 31, 2007 11:28 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> Yes, It is easily possible for nanoTime-based vs currentTimeMillis-based
> duration estimates to differ once in a while;
> for example, because the underlying currentTimeMillis
> system clock is typically updated less frequently.
> On average they balance out, but
> if you need consistency across all readings, you should rely on nanoTime.
>
>
> -Doug

From hanson.char at gmail.com  Mon Dec 31 14:38:42 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 31 Dec 2007 11:38:42 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
	problem ?
In-Reply-To: <ca53c8f80712311134h338fbf2cj2be6b3a0373604df@mail.gmail.com>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>
	<4778F5CB.40209@cs.oswego.edu>
	<ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>
	<ca53c8f80712311110t7bb030ddm63ff0b0a6f5f6db@mail.gmail.com>
	<ca53c8f80712311119ud46f3fet36e5f16728e18865@mail.gmail.com>
	<477942C7.9070409@cs.oswego.edu>
	<ca53c8f80712311134h338fbf2cj2be6b3a0373604df@mail.gmail.com>
Message-ID: <ca53c8f80712311138r6307ad23j87388f434e4595a5@mail.gmail.com>

Oops, not accuracy but precision, as stated in the System.nanoTime javadoc:

    "This method provides nanosecond precision, but not necessarily
nanosecond accuracy"

But still, I would imagine it would costs more to compute the
nano-time than milli-time.  I wonder how much more.

Hanson Char

On Dec 31, 2007 11:34 AM, Hanson Char <hanson.char at gmail.com> wrote:
> Is it true however that System.nanoTime() incurs more overhead than
> System.currentTimeMillis(), since it has a million time more accuracy
> ?
>
> Hanson Char
>
>
> On Dec 31, 2007 11:28 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> > Yes, It is easily possible for nanoTime-based vs currentTimeMillis-based
> > duration estimates to differ once in a while;
> > for example, because the underlying currentTimeMillis
> > system clock is typically updated less frequently.
> > On average they balance out, but
> > if you need consistency across all readings, you should rely on nanoTime.
> >
> >
> > -Doug
>

From hanson.char at gmail.com  Mon Dec 31 14:40:44 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 31 Dec 2007 11:40:44 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
	problem ?
In-Reply-To: <8b9ac3e80712311139s79f20a46h836a5a2126689fc5@mail.gmail.com>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>
	<4778F5CB.40209@cs.oswego.edu>
	<ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>
	<ca53c8f80712311110t7bb030ddm63ff0b0a6f5f6db@mail.gmail.com>
	<ca53c8f80712311119ud46f3fet36e5f16728e18865@mail.gmail.com>
	<477942C7.9070409@cs.oswego.edu>
	<ca53c8f80712311134h338fbf2cj2be6b3a0373604df@mail.gmail.com>
	<8b9ac3e80712311139s79f20a46h836a5a2126689fc5@mail.gmail.com>
Message-ID: <ca53c8f80712311140o7b01b707r3f2b4385941ce0ed@mail.gmail.com>

On Dec 31, 2007 11:39 AM, John Xiao <codesith at gmail.com> wrote:
> System.nanoTime() doesn't guarantee any nanosecond accuracy either.

True.

> It still could cause scheduler to execute before expected delay.

Proof ?

Hanson Char

From dl at cs.oswego.edu  Mon Dec 31 16:15:58 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 31 Dec 2007 16:15:58 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
 problem ?
In-Reply-To: <ca53c8f80712311138r6307ad23j87388f434e4595a5@mail.gmail.com>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>	
	<4778F5CB.40209@cs.oswego.edu>	
	<ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>	
	<ca53c8f80712311110t7bb030ddm63ff0b0a6f5f6db@mail.gmail.com>	
	<ca53c8f80712311119ud46f3fet36e5f16728e18865@mail.gmail.com>	
	<477942C7.9070409@cs.oswego.edu>	
	<ca53c8f80712311134h338fbf2cj2be6b3a0373604df@mail.gmail.com>
	<ca53c8f80712311138r6307ad23j87388f434e4595a5@mail.gmail.com>
Message-ID: <47795C0E.5080702@cs.oswego.edu>

Hanson Char wrote:
> Oops, not accuracy but precision, as stated in the System.nanoTime javadoc:
> 
>     "This method provides nanosecond precision, but not necessarily
> nanosecond accuracy"
> 
> But still, I would imagine it would costs more to compute the
> nano-time than milli-time.  

Generally, the opposite, but there are no guarantees.
On most platforms nanoTime is cheaper than currentTimeMillis,
on others about the same. But I don't think there
are any platforms on which the performance
difference is big enough in either direction to
cause anyone to choose one versus the other on performance
grounds alone.

-Doug

From hanson.char at gmail.com  Mon Dec 31 16:17:39 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 31 Dec 2007 13:17:39 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor schdeuling
	problem ?
In-Reply-To: <477942C7.9070409@cs.oswego.edu>
References: <ca53c8f80712291440x60cf4f22na81040f108198f7e@mail.gmail.com>
	<4778F5CB.40209@cs.oswego.edu>
	<ca53c8f80712310901q59c51ce8n54a1cb86c2c8ba7f@mail.gmail.com>
	<ca53c8f80712311110t7bb030ddm63ff0b0a6f5f6db@mail.gmail.com>
	<ca53c8f80712311119ud46f3fet36e5f16728e18865@mail.gmail.com>
	<477942C7.9070409@cs.oswego.edu>
Message-ID: <ca53c8f80712311317g5b416f91ue4a3f13daa31c76f@mail.gmail.com>

Note System.currentTimeMillis() returns the current time in
milliseconds, whereas System.nanoTime() does NOT.  Instead,
System.nanoTime() can only be used to measure elapsed time and is not
related to any other notion of system or wall-clock time.

This means, in the original case, changing the implementation to use
nano-time won't solve the problem.  There is simply no way to figure
out the current time in nano second precision.  The design of the
CronThreadPoolExecutor

  http://ha-jdbc.sourceforge.net/api/net/sf/hajdbc/util/concurrent/CronThreadPoolExecutor.html

is to try to figure out an absolute time in the future to execute by
calculating the difference of the future time from now.  But once such
scheduled task is executed, the then current time (in millisec) may
still be before the target future time (also in millisec), even though
the delay was accurate to the highest possible degree (in nanosec).
Hence the problem.

Should the task got executed by the scheduler happened earlier than
the target/absolute time, the proper solution would probably be to
skip the execution but immediately reschedule the task to execute at
the same target/absolute time again (by calculating the difference
between the then current time and the target time, both only available
in millisec).

Hanson Char

On Dec 31, 2007 11:28 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> Hanson Char wrote:
> > Interestingly when I changed the test to replace all uses of
> > milli-time with nano-time, the earlier-than-expected problem seems to
> > go away.  (Nano-test code below.)
> >
> > The lesson/solution seems apparent.  Replace all use of
> > System.currentTimeMillis() with System.nanoTime().
>
>
> Yes, It is easily possible for nanoTime-based vs currentTimeMillis-based
> duration estimates to differ once in a while;
> for example, because the underlying currentTimeMillis
> system clock is typically updated less frequently.
> On average they balance out, but
> if you need consistency across all readings, you should rely on nanoTime.
>
>
> -Doug
>
>
>

