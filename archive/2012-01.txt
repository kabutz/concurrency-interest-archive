From stanimir at riflexo.com  Tue Jan  3 09:02:02 2012
From: stanimir at riflexo.com (bestsss)
Date: Tue, 3 Jan 2012 06:02:02 -0800 (PST)
Subject: [concurrency-interest] Why not expose array entry
 atomic/volatile operations directly?
In-Reply-To: <CAE-f1xR2V0xJ6EJCjofXLC9zvZnpmkQDinPnuHaUjJcuaTg77g@mail.gmail.com>
References: <CAE-f1xR2V0xJ6EJCjofXLC9zvZnpmkQDinPnuHaUjJcuaTg77g@mail.gmail.com>
Message-ID: <33072035.post@talk.nabble.com>


What I do in such cases - just extend AtomicReferenceArray insteadof Object.
It's entirely portable and there is no extra overhead for the indirection.
Of course that exposes all extra methods, so it's applicable for internal
usage only.

All AtomicFieldUpdaters suffer from the extra checks which are not optimized
away, judging by the generated assembler. Using Unsafe directly would
require at least the bounds check which may not be optimized either just
like the AtomicReferenceArray. Skipping the bounds check and using unsafe
technically might yeild better performance than plain array (on loads
mostly).


Charles Oliver Nutter-4 wrote:
> 
> Maybe this has been beaten to death...feel free to tell me to shove
> off and find the relevant FAQ or thread.
> 
> I was experimenting last week with making Ruby objects' instance
> variables 100% volatile, mostly to see what sort of performance impact
> would result. In the process, I tried two approaches:
> 
> * Replacing the variable table (Object[]) with an AtomicReferenceArray.
> 
> This worked as expected, but I saw more overhead than I would have
> liked due to the additional object allocation and indirection through
> ARA.
> 
> * Hand-copy the Unsafe logic from ARA into the Ruby Object base class,
> avoiding the extra object and indirection.
> 
> This was still a larger perf hit than I'd want, but it did reduce the
> overhead and object cost.
> 
> What surprised me was that a plain old array could be magically
> treated as having volatile entries just by "cheating" and going
> directly to unsafe. So my question is this...why isn't there an
> AtomicReferenceArrayUpdater or similar "external" way to get
> volatile/atomic behavior against array entries?
> 
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 

-- 
View this message in context: http://old.nabble.com/Why-not-expose-array-entry-atomic-volatile-operations-directly--tp33006691p33072035.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From viktor.klang at gmail.com  Tue Jan  3 16:26:34 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 3 Jan 2012 22:26:34 +0100
Subject: [concurrency-interest] Why not expose array entry
 atomic/volatile operations directly?
In-Reply-To: <33072035.post@talk.nabble.com>
References: <CAE-f1xR2V0xJ6EJCjofXLC9zvZnpmkQDinPnuHaUjJcuaTg77g@mail.gmail.com>
	<33072035.post@talk.nabble.com>
Message-ID: <CANPzfU9EuqFt4g2VzCds-wLGavxH4g+aAsfN16W+JViVmW33PQ@mail.gmail.com>

Go Unsafe?

On Tue, Jan 3, 2012 at 3:02 PM, bestsss <stanimir at riflexo.com> wrote:

>
> What I do in such cases - just extend AtomicReferenceArray insteadof
> Object.
> It's entirely portable and there is no extra overhead for the indirection.
> Of course that exposes all extra methods, so it's applicable for internal
> usage only.
>
> All AtomicFieldUpdaters suffer from the extra checks which are not
> optimized
> away, judging by the generated assembler. Using Unsafe directly would
> require at least the bounds check which may not be optimized either just
> like the AtomicReferenceArray. Skipping the bounds check and using unsafe
> technically might yeild better performance than plain array (on loads
> mostly).
>
>
> Charles Oliver Nutter-4 wrote:
> >
> > Maybe this has been beaten to death...feel free to tell me to shove
> > off and find the relevant FAQ or thread.
> >
> > I was experimenting last week with making Ruby objects' instance
> > variables 100% volatile, mostly to see what sort of performance impact
> > would result. In the process, I tried two approaches:
> >
> > * Replacing the variable table (Object[]) with an AtomicReferenceArray.
> >
> > This worked as expected, but I saw more overhead than I would have
> > liked due to the additional object allocation and indirection through
> > ARA.
> >
> > * Hand-copy the Unsafe logic from ARA into the Ruby Object base class,
> > avoiding the extra object and indirection.
> >
> > This was still a larger perf hit than I'd want, but it did reduce the
> > overhead and object cost.
> >
> > What surprised me was that a plain old array could be magically
> > treated as having volatile entries just by "cheating" and going
> > directly to unsafe. So my question is this...why isn't there an
> > AtomicReferenceArrayUpdater or similar "external" way to get
> > volatile/atomic behavior against array entries?
> >
> > - Charlie
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
> --
> View this message in context:
> http://old.nabble.com/Why-not-expose-array-entry-atomic-volatile-operations-directly--tp33006691p33072035.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120103/8d15bd66/attachment.html>

From pugh at cs.umd.edu  Tue Jan  3 17:27:28 2012
From: pugh at cs.umd.edu (Bill Pugh)
Date: Tue, 3 Jan 2012 17:27:28 -0500
Subject: [concurrency-interest] Why not expose array entry
	atomic/volatile operations directly?
In-Reply-To: <CAE-f1xR2V0xJ6EJCjofXLC9zvZnpmkQDinPnuHaUjJcuaTg77g@mail.gmail.com>
References: <CAE-f1xR2V0xJ6EJCjofXLC9zvZnpmkQDinPnuHaUjJcuaTg77g@mail.gmail.com>
Message-ID: <F219544B-0580-46DF-BCCD-FF2014A819B3@cs.umd.edu>


On Dec 19, 2011, at 7:07 PM, Charles Oliver Nutter wrote:
> 
> 
> What surprised me was that a plain old array could be magically
> treated as having volatile entries just by "cheating" and going
> directly to unsafe. So my question is this...why isn't there an
> AtomicReferenceArrayUpdater or similar "external" way to get
> volatile/atomic behavior against array entries?
> 
> - Charlie


Among other reasons, I presume because there are _no_ semantics defined for what happens when you have a memory location that is sometimes accesses as a volatile memory location and sometimes accessed as a normal memory location.

Bill Pugh



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120103/e23b2e0d/attachment.html>

From peter.firmstone at zeus.net.au  Wed Jan  4 07:02:50 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Wed, 04 Jan 2012 22:02:50 +1000
Subject: [concurrency-interest] Reference Collections
Message-ID: <1325678569.21868.167.camel@bluto>

Hi,

Over at river.apache.org, we've got a reference collections library,
utilised for concurrent caches, it's straightforward clean and simple
code.  Easy to use, although Generics are a little verbose.  RC is a
class with a simple public api, containing static methods used to wrap
any of the java collections interfaces, allowing you to use any
reference type, to refer to objects contained in your collections.

All implementation is package private, sync / locking strategies are
left to the underlying collection, removal / cleaning is performed by
any thread that obtains a lock on a private ReferenceQueue (each
collection has it's own), which is guarded by a ReentrantLock.tryLock(),
threads that don't obtain the lock, continue without performing any
cleanup.  Do you think that all calling threads attempting to obtain the
tryLock(), (once only) will cause any performance issues?  Garbage
collection occurs concurrently with other operations.

Reference types available are Weak Identity, Weak, Soft Identity, Soft
and Strong.  It's relatively simple to add additional reference types
such as a timed reference, however this isn't implemented at present.

Here's a useage example, straight from our SecurityManager code:

<SNIP>
private final ConcurrentMap<AccessControlContext,
NavigableSet<Permission>> checked;
private final Comparator<Referrer<Permission>> permCompare;
</SNIP>

<SNIP-FROM-CONSTRUCTOR>
ConcurrentMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>> refmap 
	= new
ConcurrentHashMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>(100);

checked = RC.concurrentMap(refmap, Ref.SOFT, Ref.STRONG);
permCompare = RC.comparator(new PermissionComparator());

</SNIP-FROM-CONSTRUCTOR>

<SNIP-METHOD>
    @Override
    public void checkPermission(Permission perm, Object context) throws
SecurityException {
	if (!(context instanceof AccessControlContext)) throw new
SecurityException();
	if (perm == null ) throw new NullPointerException("Permission
Collection null");
        /* The next line speeds up permission checks related to this
SecurityManager. */
        if ( SMPrivilegedContext.equals(context) ||
SMConstructorContext.equals(context)) return; // prevents endless loop
in debug.
        AccessControlContext executionContext = (AccessControlContext)
context;
        // Checks if Permission has already been checked for this
context.
        NavigableSet<Permission> checkedPerms =
checked.get(executionContext);
        if (checkedPerms == null){
            /* A ConcurrentSkipListSet is used to avoid blocking during
             * removal operations that occur while the garbage collector
             * recovers softly reachable memory.  Since this happens
while
             * the jvm's under stress, it's important that permission
checks
             * continue to perform well.
             * 
             * Although I considered a multi read, single write Set, I
wanted
             * to avoid blocking under stress, caused as a result
             * of garbage collection.
             * 
             * The Reference Collection that encapsulates the
ConcurrentSkipListSet
             * uses a ReentrantLock.tryLock() guard the ReferenceQueue
used
             * to remove objects from the Set.  This allows other
threads to
             * proceed during object removal.  Only one thread is given
access
             * to the ReferenceQueue, the unlucky caller thread performs
garbage
             * removal from the Set before accessing the Set for its
original
             * purpose.
             */
            NavigableSet<Referrer<Permission>> internal = 
                    new
ConcurrentSkipListSet<Referrer<Permission>>(permCompare);
            checkedPerms = RC.navigableSet(internal, Ref.SOFT);
            NavigableSet<Permission> existed =
checked.putIfAbsent(executionContext, checkedPerms);
            if (existed != null) checkedPerms = existed;
        }
        if (checkedPerms.contains(perm)) return; // don't need to check
again.

</SNIP-METHOD>

Serialization is implemented so implementations can be replaced and
upgraded, serial form is a separate concern; see the Serialization
Builder pattern for details: http://wiki.apache.org/river/Serialization

This a copy of some implementation code:

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.river.impl.util;


import java.lang.ref.Reference;
import java.util.Random;
import java.util.concurrent.ConcurrentMap;
import java.util.logging.Level;
import java.util.logging.Logger;

/**
 * A referenced hash map, that encapsulates and utilises any
ConcurrentMap
 * implementation passed in at construction.
 * 
 * Based on any ConcurrentMap implementation, it doesn't accept null
keys or values.
 *
 * It is recommended although not mandatory to use identity based
References for keys,
 * unexpected results occur when relying on equal keys, if one key is no
longer 
 * strongly reachable and has been garbage collected and removed from
the 
 * Map.
 * 
 * 
 * 
 * If either a key or value, is no longer strongly reachable, their
mapping
 * will be queued for removal and garbage collection, in compliance with
 * the Reference implementation selected.
 *
 * @param <K> 
 * @param <V> 
 * @see Ref
 * @author Peter Firmstone.
 *
 * @since 2.3
 */
class ReferenceConcurrentMap<K, V> extends ReferenceMap<K, V> implements
ConcurrentMap<K, V> {

    // ConcurrentMap must be protected from null values?  It changes
it's behaviour, is that a problem?
    private final ConcurrentMap<Referrer<K>, Referrer<V>> map;
    
    ReferenceConcurrentMap(ConcurrentMap<Referrer<K>,Referrer<V>> map,
Ref key, Ref val){
        super (map, key, val);
        this.map = map;
    }
    
    ReferenceConcurrentMap(ConcurrentMap<Referrer<K>, Referrer<V>> map,
            ReferenceQueuingFactory<K, Referrer<K>> krqf,
ReferenceQueuingFactory<V, Referrer<V>> vrqf, Ref key, Ref val){
        super(map, krqf, vrqf, key, val);
        this.map = map;
    }
    
    public V putIfAbsent(K key, V value) {
        processQueue();  //may be a slight delay before atomic
putIfAbsent
        Referrer<K> k = wrapKey(key, true, false);
        Referrer<V> v = wrapVal(value, true, false);
        Referrer<V> val = map.putIfAbsent(k, v);
        while ( val != null ) {
            V existed = val.get();
            // We hold a strong reference to value, so 
            if ( existed == null ){
                // stale reference must be replaced, it has been garbage
collect but hasn't 
                // been removed, we must treat it like the entry doesn't
exist.
                if ( map.replace(k, val, v)){
                    // replace successful
                    return null; // Because officially there was no
record.
                } else {
                    // Another thread may have replaced it.
                    val = map.putIfAbsent(k, v);
                }
            } else {
                return existed;
            }
        }
        return null;
    }

    @SuppressWarnings("unchecked")
    public boolean remove(Object key, Object value) {
        processQueue();
        return map.remove(wrapKey((K) key, false, true), wrapVal((V)
value, false, true));
    }

    public boolean replace(K key, V oldValue, V newValue) {
        processQueue();
        return map.replace(wrapKey(key, false, true), wrapVal(oldValue,
false, true), wrapVal(newValue, true, false));
    }

    public V replace(K key, V value) {
        processQueue();
        Referrer<V> val = map.replace(wrapKey(key, false, true),
wrapVal(value, true, false));
        if ( val != null ) return val.get();
        return null;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.river.impl.util;

import java.io.InvalidObjectException;
import java.io.ObjectInputStream;
import java.io.ObjectStreamException;
import java.io.Serializable;
import java.io.WriteAbortedException;
import java.lang.ref.Reference;
import java.lang.ref.ReferenceQueue;
import java.util.AbstractCollection;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.Set;
import java.util.logging.Level;
import java.util.logging.Logger;

/**
 * A Collection of Reference Objects, the developer may chose any
Collection
 * implementation to store the References, which is passed in a runtime.
 * 
 * The underlying Collection implementation governs the specific
behaviour of this
 * Collection.
 * 
 * Synchronisation must be implemented by the underlying Collection and
cannot
 * be performed externally to this class.  The underlying Collection
must
 * also be mutable.  Objects will be removed automatically from the
underlying
 * Collection when they are eligible for garbage collection.
 * 
 * Weak, Weak Identity, Soft, Soft Identity or Strong references may be
used.
 * This Collection may be used as an Object pool cache or any other
purpose 
 * that requires unique memory handling.
 * 
 * For concurrent threads, it is recommended to encapsulate the
underlying
 * collection in a multi read, single write collection for scalability.
 * 
 * @see Ref
 * @see ConcurrentCollections#multiReadCollection(java.util.Collection) 
 * @author Peter Firmstone.
 */
class ReferenceCollection<T> extends AbstractCollection<T> 
                                implements Collection<T>, Serializable {
    private static final long serialVersionUID = 1L;
    private final Collection<Referrer<T>> col;
    private final ReferenceQueuingFactory<T, Referrer<T>> rqf;
    private final Ref type;
    
    ReferenceCollection(Collection<Referrer<T>> col, Ref type){
        this(col, new ReferenceProcessor<T>(col, type, type ==
Ref.STRONG ? null : new ReferenceQueue<T>()), type);
    }
    
    ReferenceCollection(Collection<Referrer<T>> col, 
            ReferenceQueuingFactory<T, Referrer<T>> rqf, Ref type){
        this.col = col;
        this.rqf = rqf;
        this.type = type;
    }
    
    void processQueue(){
        rqf.processQueue();
        }
    
    ReferenceQueuingFactory<T, Referrer<T>> getRQF(){
        return rqf;
    }
    
    Ref getRef(){
        return type;
    }
    
    Referrer<T> wrapObj(T t, boolean enqueue, boolean temporary){
        return rqf.referenced(t, enqueue, temporary);
    }
    
    public int size() {
        processQueue();
        return col.size();
    }

    public boolean isEmpty() {
        processQueue();
        return col.isEmpty();
    }

    public boolean contains(Object o) {
        processQueue();
        return col.contains(wrapObj((T) o, false, true));
    }
    
    /**
     * This Iterator may return null values if garbage collection
     * runs during iteration.
     * 
     * Always check for null values.
     * 
     * @return T - possibly null.
     */
    public Iterator<T> iterator() {
        processQueue();
        return new ReferenceIterator<T>(col.iterator());
    }

    public boolean add(T e) {
        processQueue();
        return col.add(wrapObj(e, true, false));
    }

    public boolean remove(Object o) {
        processQueue();
        return col.remove(wrapObj((T) o, false, true));
    }

 
    @SuppressWarnings("unchecked")
    public boolean containsAll(Collection<?> c) {
        processQueue();
        return col.containsAll(new CollectionWrapper<T>((Collection<T>)
c, getRQF(), false, true));
    }

    
    @SuppressWarnings("unchecked")
    public boolean addAll(Collection<? extends T> c) {
        processQueue();
        return col.addAll(new CollectionWrapper<T>((Collection<T>) c,
getRQF(), true, false));
    }

    public void clear() {
        col.clear();
    }
    
    /*
     * The next three methods are suitable implementations for
subclasses also.
     */
    public String toString(){
        return col.toString();
    }

    @Override
    public int hashCode() {
        if ( col instanceof List || col instanceof Set ){
            return col.hashCode();
        }
        return System.identityHashCode(this);
    }
    
    /**
     * Because equals and hashCode are not defined for collections, we 
     * cannot guarantee consistent behaviour by implementing equals and
     * hashCode.  A collection could be a list, set, queue or deque.
     * So a List != Queue and a Set != list. therefore equals for
collections is
     * not defined.
     * 
     * However since two collections may both also be Lists, while
abstracted
     * from the client two lists may still be equal.
     * @see Collection#equals(java.lang.Object) 
     */
    
    @Override
    public boolean equals(Object o){
        if ( o == this ) return true;
        if ( col instanceof List || col instanceof Set ){
            return col.equals(o);
        }
        return false;
    }
    
    final Object writeReplace() throws ObjectStreamException {
        try {
            // returns a Builder instead of this class.
            return SerializationOfReferenceCollection.create(getClass(),
col, type );
        } catch (InstantiationException ex) {
            throw new WriteAbortedException("Unable to create
serialization proxy", ex);
        } catch (IllegalAccessException ex) {
            throw new WriteAbortedException("Unable to create
serialization proxy", ex);
        }
    }
    
    private void readObject(ObjectInputStream stream) 
            throws InvalidObjectException{
        throw new InvalidObjectException("Builder required");
    }

}



From nathan.reynolds at oracle.com  Wed Jan  4 10:56:03 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 04 Jan 2012 08:56:03 -0700
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <1325678569.21868.167.camel@bluto>
References: <1325678569.21868.167.camel@bluto>
Message-ID: <4F047693.6080508@oracle.com>

>  Do you think that all calling threads attempting to obtain the tryLock(), (once only) will cause any performance issues?

tryLock() will ideally execute a CAS instruction to acquire the lock.  
Under no contention, this means that each access will execute a CAS.  
This is going to be a performance problem.  It will cost at least 10s of 
cycles.  If you're lucky, it won't be significant.  Under contention, 
the CAS instruction will stall the processor pipeline and become a 
bottleneck.  If the tryLock() is called heavily, the system won't be 
able to scale past 3-socket Nehalem system.

The JDK's ReferenceQueue.poll() simply reads a volatile variable.  If 
the queue is empty, then the impact will simply be reading the 
variable's value from cache or RAM.  If the value is in L1 cache, then 
this will only cost 3 cycles.  If the queue is not empty, then the code 
acquires a lock and does the poll operation.  You probably want to do 
likewise except instead of acquiring the lock, do a tryLock().  Thus, 
threads won't block on the clean up.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/4/2012 5:02 AM, Peter Firmstone wrote:
> Hi,
>
> Over at river.apache.org, we've got a reference collections library,
> utilised for concurrent caches, it's straightforward clean and simple
> code.  Easy to use, although Generics are a little verbose.  RC is a
> class with a simple public api, containing static methods used to wrap
> any of the java collections interfaces, allowing you to use any
> reference type, to refer to objects contained in your collections.
>
> All implementation is package private, sync / locking strategies are
> left to the underlying collection, removal / cleaning is performed by
> any thread that obtains a lock on a private ReferenceQueue (each
> collection has it's own), which is guarded by a ReentrantLock.tryLock(),
> threads that don't obtain the lock, continue without performing any
> cleanup.  Do you think that all calling threads attempting to obtain the
> tryLock(), (once only) will cause any performance issues?  Garbage
> collection occurs concurrently with other operations.
>
> Reference types available are Weak Identity, Weak, Soft Identity, Soft
> and Strong.  It's relatively simple to add additional reference types
> such as a timed reference, however this isn't implemented at present.
>
> Here's a useage example, straight from our SecurityManager code:
>
> <SNIP>
> private final ConcurrentMap<AccessControlContext,
> NavigableSet<Permission>>  checked;
> private final Comparator<Referrer<Permission>>  permCompare;
> </SNIP>
>
> <SNIP-FROM-CONSTRUCTOR>
> ConcurrentMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>  refmap
> 	= new
> ConcurrentHashMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>(100);
>
> checked = RC.concurrentMap(refmap, Ref.SOFT, Ref.STRONG);
> permCompare = RC.comparator(new PermissionComparator());
>
> </SNIP-FROM-CONSTRUCTOR>
>
> <SNIP-METHOD>
>      @Override
>      public void checkPermission(Permission perm, Object context) throws
> SecurityException {
> 	if (!(context instanceof AccessControlContext)) throw new
> SecurityException();
> 	if (perm == null ) throw new NullPointerException("Permission
> Collection null");
>          /* The next line speeds up permission checks related to this
> SecurityManager. */
>          if ( SMPrivilegedContext.equals(context) ||
> SMConstructorContext.equals(context)) return; // prevents endless loop
> in debug.
>          AccessControlContext executionContext = (AccessControlContext)
> context;
>          // Checks if Permission has already been checked for this
> context.
>          NavigableSet<Permission>  checkedPerms =
> checked.get(executionContext);
>          if (checkedPerms == null){
>              /* A ConcurrentSkipListSet is used to avoid blocking during
>               * removal operations that occur while the garbage collector
>               * recovers softly reachable memory.  Since this happens
> while
>               * the jvm's under stress, it's important that permission
> checks
>               * continue to perform well.
>               *
>               * Although I considered a multi read, single write Set, I
> wanted
>               * to avoid blocking under stress, caused as a result
>               * of garbage collection.
>               *
>               * The Reference Collection that encapsulates the
> ConcurrentSkipListSet
>               * uses a ReentrantLock.tryLock() guard the ReferenceQueue
> used
>               * to remove objects from the Set.  This allows other
> threads to
>               * proceed during object removal.  Only one thread is given
> access
>               * to the ReferenceQueue, the unlucky caller thread performs
> garbage
>               * removal from the Set before accessing the Set for its
> original
>               * purpose.
>               */
>              NavigableSet<Referrer<Permission>>  internal =
>                      new
> ConcurrentSkipListSet<Referrer<Permission>>(permCompare);
>              checkedPerms = RC.navigableSet(internal, Ref.SOFT);
>              NavigableSet<Permission>  existed =
> checked.putIfAbsent(executionContext, checkedPerms);
>              if (existed != null) checkedPerms = existed;
>          }
>          if (checkedPerms.contains(perm)) return; // don't need to check
> again.
>
> </SNIP-METHOD>
>
> Serialization is implemented so implementations can be replaced and
> upgraded, serial form is a separate concern; see the Serialization
> Builder pattern for details: http://wiki.apache.org/river/Serialization
>
> This a copy of some implementation code:
>
> /*
>   * Licensed to the Apache Software Foundation (ASF) under one
>   * or more contributor license agreements.  See the NOTICE file
>   * distributed with this work for additional information
>   * regarding copyright ownership. The ASF licenses this file
>   * to you under the Apache License, Version 2.0 (the
>   * "License"); you may not use this file except in compliance
>   * with the License. You may obtain a copy of the License at
>   *
>   *      http://www.apache.org/licenses/LICENSE-2.0
>   *
>   * Unless required by applicable law or agreed to in writing, software
>   * distributed under the License is distributed on an "AS IS" BASIS,
>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> implied.
>   * See the License for the specific language governing permissions and
>   * limitations under the License.
>   */
>
> package org.apache.river.impl.util;
>
>
> import java.lang.ref.Reference;
> import java.util.Random;
> import java.util.concurrent.ConcurrentMap;
> import java.util.logging.Level;
> import java.util.logging.Logger;
>
> /**
>   * A referenced hash map, that encapsulates and utilises any
> ConcurrentMap
>   * implementation passed in at construction.
>   *
>   * Based on any ConcurrentMap implementation, it doesn't accept null
> keys or values.
>   *
>   * It is recommended although not mandatory to use identity based
> References for keys,
>   * unexpected results occur when relying on equal keys, if one key is no
> longer
>   * strongly reachable and has been garbage collected and removed from
> the
>   * Map.
>   *
>   *
>   *
>   * If either a key or value, is no longer strongly reachable, their
> mapping
>   * will be queued for removal and garbage collection, in compliance with
>   * the Reference implementation selected.
>   *
>   * @param<K>
>   * @param<V>
>   * @see Ref
>   * @author Peter Firmstone.
>   *
>   * @since 2.3
>   */
> class ReferenceConcurrentMap<K, V>  extends ReferenceMap<K, V>  implements
> ConcurrentMap<K, V>  {
>
>      // ConcurrentMap must be protected from null values?  It changes
> it's behaviour, is that a problem?
>      private final ConcurrentMap<Referrer<K>, Referrer<V>>  map;
>
>      ReferenceConcurrentMap(ConcurrentMap<Referrer<K>,Referrer<V>>  map,
> Ref key, Ref val){
>          super (map, key, val);
>          this.map = map;
>      }
>
>      ReferenceConcurrentMap(ConcurrentMap<Referrer<K>, Referrer<V>>  map,
>              ReferenceQueuingFactory<K, Referrer<K>>  krqf,
> ReferenceQueuingFactory<V, Referrer<V>>  vrqf, Ref key, Ref val){
>          super(map, krqf, vrqf, key, val);
>          this.map = map;
>      }
>
>      public V putIfAbsent(K key, V value) {
>          processQueue();  //may be a slight delay before atomic
> putIfAbsent
>          Referrer<K>  k = wrapKey(key, true, false);
>          Referrer<V>  v = wrapVal(value, true, false);
>          Referrer<V>  val = map.putIfAbsent(k, v);
>          while ( val != null ) {
>              V existed = val.get();
>              // We hold a strong reference to value, so
>              if ( existed == null ){
>                  // stale reference must be replaced, it has been garbage
> collect but hasn't
>                  // been removed, we must treat it like the entry doesn't
> exist.
>                  if ( map.replace(k, val, v)){
>                      // replace successful
>                      return null; // Because officially there was no
> record.
>                  } else {
>                      // Another thread may have replaced it.
>                      val = map.putIfAbsent(k, v);
>                  }
>              } else {
>                  return existed;
>              }
>          }
>          return null;
>      }
>
>      @SuppressWarnings("unchecked")
>      public boolean remove(Object key, Object value) {
>          processQueue();
>          return map.remove(wrapKey((K) key, false, true), wrapVal((V)
> value, false, true));
>      }
>
>      public boolean replace(K key, V oldValue, V newValue) {
>          processQueue();
>          return map.replace(wrapKey(key, false, true), wrapVal(oldValue,
> false, true), wrapVal(newValue, true, false));
>      }
>
>      public V replace(K key, V value) {
>          processQueue();
>          Referrer<V>  val = map.replace(wrapKey(key, false, true),
> wrapVal(value, true, false));
>          if ( val != null ) return val.get();
>          return null;
>      }
> }
>
> /*
>   * Licensed to the Apache Software Foundation (ASF) under one
>   * or more contributor license agreements.  See the NOTICE file
>   * distributed with this work for additional information
>   * regarding copyright ownership. The ASF licenses this file
>   * to you under the Apache License, Version 2.0 (the
>   * "License"); you may not use this file except in compliance
>   * with the License. You may obtain a copy of the License at
>   *
>   *      http://www.apache.org/licenses/LICENSE-2.0
>   *
>   * Unless required by applicable law or agreed to in writing, software
>   * distributed under the License is distributed on an "AS IS" BASIS,
>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> implied.
>   * See the License for the specific language governing permissions and
>   * limitations under the License.
>   */
>
> package org.apache.river.impl.util;
>
> import java.io.InvalidObjectException;
> import java.io.ObjectInputStream;
> import java.io.ObjectStreamException;
> import java.io.Serializable;
> import java.io.WriteAbortedException;
> import java.lang.ref.Reference;
> import java.lang.ref.ReferenceQueue;
> import java.util.AbstractCollection;
> import java.util.ArrayList;
> import java.util.Collection;
> import java.util.Iterator;
> import java.util.List;
> import java.util.Set;
> import java.util.logging.Level;
> import java.util.logging.Logger;
>
> /**
>   * A Collection of Reference Objects, the developer may chose any
> Collection
>   * implementation to store the References, which is passed in a runtime.
>   *
>   * The underlying Collection implementation governs the specific
> behaviour of this
>   * Collection.
>   *
>   * Synchronisation must be implemented by the underlying Collection and
> cannot
>   * be performed externally to this class.  The underlying Collection
> must
>   * also be mutable.  Objects will be removed automatically from the
> underlying
>   * Collection when they are eligible for garbage collection.
>   *
>   * Weak, Weak Identity, Soft, Soft Identity or Strong references may be
> used.
>   * This Collection may be used as an Object pool cache or any other
> purpose
>   * that requires unique memory handling.
>   *
>   * For concurrent threads, it is recommended to encapsulate the
> underlying
>   * collection in a multi read, single write collection for scalability.
>   *
>   * @see Ref
>   * @see ConcurrentCollections#multiReadCollection(java.util.Collection)
>   * @author Peter Firmstone.
>   */
> class ReferenceCollection<T>  extends AbstractCollection<T>
>                                  implements Collection<T>, Serializable {
>      private static final long serialVersionUID = 1L;
>      private final Collection<Referrer<T>>  col;
>      private final ReferenceQueuingFactory<T, Referrer<T>>  rqf;
>      private final Ref type;
>
>      ReferenceCollection(Collection<Referrer<T>>  col, Ref type){
>          this(col, new ReferenceProcessor<T>(col, type, type ==
> Ref.STRONG ? null : new ReferenceQueue<T>()), type);
>      }
>
>      ReferenceCollection(Collection<Referrer<T>>  col,
>              ReferenceQueuingFactory<T, Referrer<T>>  rqf, Ref type){
>          this.col = col;
>          this.rqf = rqf;
>          this.type = type;
>      }
>
>      void processQueue(){
>          rqf.processQueue();
>          }
>
>      ReferenceQueuingFactory<T, Referrer<T>>  getRQF(){
>          return rqf;
>      }
>
>      Ref getRef(){
>          return type;
>      }
>
>      Referrer<T>  wrapObj(T t, boolean enqueue, boolean temporary){
>          return rqf.referenced(t, enqueue, temporary);
>      }
>
>      public int size() {
>          processQueue();
>          return col.size();
>      }
>
>      public boolean isEmpty() {
>          processQueue();
>          return col.isEmpty();
>      }
>
>      public boolean contains(Object o) {
>          processQueue();
>          return col.contains(wrapObj((T) o, false, true));
>      }
>
>      /**
>       * This Iterator may return null values if garbage collection
>       * runs during iteration.
>       *
>       * Always check for null values.
>       *
>       * @return T - possibly null.
>       */
>      public Iterator<T>  iterator() {
>          processQueue();
>          return new ReferenceIterator<T>(col.iterator());
>      }
>
>      public boolean add(T e) {
>          processQueue();
>          return col.add(wrapObj(e, true, false));
>      }
>
>      public boolean remove(Object o) {
>          processQueue();
>          return col.remove(wrapObj((T) o, false, true));
>      }
>
>
>      @SuppressWarnings("unchecked")
>      public boolean containsAll(Collection<?>  c) {
>          processQueue();
>          return col.containsAll(new CollectionWrapper<T>((Collection<T>)
> c, getRQF(), false, true));
>      }
>
>
>      @SuppressWarnings("unchecked")
>      public boolean addAll(Collection<? extends T>  c) {
>          processQueue();
>          return col.addAll(new CollectionWrapper<T>((Collection<T>) c,
> getRQF(), true, false));
>      }
>
>      public void clear() {
>          col.clear();
>      }
>
>      /*
>       * The next three methods are suitable implementations for
> subclasses also.
>       */
>      public String toString(){
>          return col.toString();
>      }
>
>      @Override
>      public int hashCode() {
>          if ( col instanceof List || col instanceof Set ){
>              return col.hashCode();
>          }
>          return System.identityHashCode(this);
>      }
>
>      /**
>       * Because equals and hashCode are not defined for collections, we
>       * cannot guarantee consistent behaviour by implementing equals and
>       * hashCode.  A collection could be a list, set, queue or deque.
>       * So a List != Queue and a Set != list. therefore equals for
> collections is
>       * not defined.
>       *
>       * However since two collections may both also be Lists, while
> abstracted
>       * from the client two lists may still be equal.
>       * @see Collection#equals(java.lang.Object)
>       */
>
>      @Override
>      public boolean equals(Object o){
>          if ( o == this ) return true;
>          if ( col instanceof List || col instanceof Set ){
>              return col.equals(o);
>          }
>          return false;
>      }
>
>      final Object writeReplace() throws ObjectStreamException {
>          try {
>              // returns a Builder instead of this class.
>              return SerializationOfReferenceCollection.create(getClass(),
> col, type );
>          } catch (InstantiationException ex) {
>              throw new WriteAbortedException("Unable to create
> serialization proxy", ex);
>          } catch (IllegalAccessException ex) {
>              throw new WriteAbortedException("Unable to create
> serialization proxy", ex);
>          }
>      }
>
>      private void readObject(ObjectInputStream stream)
>              throws InvalidObjectException{
>          throw new InvalidObjectException("Builder required");
>      }
>
> }
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/80f227b4/attachment-0001.html>

From david.lloyd at redhat.com  Wed Jan  4 11:11:42 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 04 Jan 2012 10:11:42 -0600
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <4F047693.6080508@oracle.com>
References: <1325678569.21868.167.camel@bluto> <4F047693.6080508@oracle.com>
Message-ID: <4F047A3E.7050109@redhat.com>

Are you saying that a failed CAS has greater impact than a volatile read 
+ external comparison?

On 01/04/2012 09:56 AM, Nathan Reynolds wrote:
>>  Do you think that all calling threads attempting to obtain the tryLock(), (once only) will cause any performance issues?
>
> tryLock() will ideally execute a CAS instruction to acquire the lock.
> Under no contention, this means that each access will execute a CAS.
> This is going to be a performance problem. It will cost at least 10s of
> cycles. If you're lucky, it won't be significant. Under contention, the
> CAS instruction will stall the processor pipeline and become a
> bottleneck. If the tryLock() is called heavily, the system won't be able
> to scale past 3-socket Nehalem system.
>
> The JDK's ReferenceQueue.poll() simply reads a volatile variable. If the
> queue is empty, then the impact will simply be reading the variable's
> value from cache or RAM. If the value is in L1 cache, then this will
> only cost 3 cycles. If the queue is not empty, then the code acquires a
> lock and does the poll operation. You probably want to do likewise
> except instead of acquiring the lock, do a tryLock(). Thus, threads
> won't block on the clean up.
>
> Nathan Reynolds
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
> Consulting Member of Technical Staff | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/4/2012 5:02 AM, Peter Firmstone wrote:
>> Hi,
>>
>> Over at river.apache.org, we've got a reference collections library,
>> utilised for concurrent caches, it's straightforward clean and simple
>> code.  Easy to use, although Generics are a little verbose.  RC is a
>> class with a simple public api, containing static methods used to wrap
>> any of the java collections interfaces, allowing you to use any
>> reference type, to refer to objects contained in your collections.
>>
>> All implementation is package private, sync / locking strategies are
>> left to the underlying collection, removal / cleaning is performed by
>> any thread that obtains a lock on a private ReferenceQueue (each
>> collection has it's own), which is guarded by a ReentrantLock.tryLock(),
>> threads that don't obtain the lock, continue without performing any
>> cleanup.  Do you think that all calling threads attempting to obtain the
>> tryLock(), (once only) will cause any performance issues?  Garbage
>> collection occurs concurrently with other operations.
>>
>> Reference types available are Weak Identity, Weak, Soft Identity, Soft
>> and Strong.  It's relatively simple to add additional reference types
>> such as a timed reference, however this isn't implemented at present.
>>
>> Here's a useage example, straight from our SecurityManager code:
>>
>> <SNIP>
>> private final ConcurrentMap<AccessControlContext,
>> NavigableSet<Permission>>  checked;
>> private final Comparator<Referrer<Permission>>  permCompare;
>> </SNIP>
>>
>> <SNIP-FROM-CONSTRUCTOR>
>> ConcurrentMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>  refmap
>> 	= new
>> ConcurrentHashMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>(100);
>>
>> checked = RC.concurrentMap(refmap, Ref.SOFT, Ref.STRONG);
>> permCompare = RC.comparator(new PermissionComparator());
>>
>> </SNIP-FROM-CONSTRUCTOR>
>>
>> <SNIP-METHOD>
>>      @Override
>>      public void checkPermission(Permission perm, Object context) throws
>> SecurityException {
>> 	if (!(context instanceof AccessControlContext)) throw new
>> SecurityException();
>> 	if (perm == null ) throw new NullPointerException("Permission
>> Collection null");
>>          /* The next line speeds up permission checks related to this
>> SecurityManager. */
>>          if ( SMPrivilegedContext.equals(context) ||
>> SMConstructorContext.equals(context)) return; // prevents endless loop
>> in debug.
>>          AccessControlContext executionContext = (AccessControlContext)
>> context;
>>          // Checks if Permission has already been checked for this
>> context.
>>          NavigableSet<Permission>  checkedPerms =
>> checked.get(executionContext);
>>          if (checkedPerms == null){
>>              /* A ConcurrentSkipListSet is used to avoid blocking during
>>               * removal operations that occur while the garbage collector
>>               * recovers softly reachable memory.  Since this happens
>> while
>>               * the jvm's under stress, it's important that permission
>> checks
>>               * continue to perform well.
>>               *
>>               * Although I considered a multi read, single write Set, I
>> wanted
>>               * to avoid blocking under stress, caused as a result
>>               * of garbage collection.
>>               *
>>               * The Reference Collection that encapsulates the
>> ConcurrentSkipListSet
>>               * uses a ReentrantLock.tryLock() guard the ReferenceQueue
>> used
>>               * to remove objects from the Set.  This allows other
>> threads to
>>               * proceed during object removal.  Only one thread is given
>> access
>>               * to the ReferenceQueue, the unlucky caller thread performs
>> garbage
>>               * removal from the Set before accessing the Set for its
>> original
>>               * purpose.
>>               */
>>              NavigableSet<Referrer<Permission>>  internal =
>>                      new
>> ConcurrentSkipListSet<Referrer<Permission>>(permCompare);
>>              checkedPerms = RC.navigableSet(internal, Ref.SOFT);
>>              NavigableSet<Permission>  existed =
>> checked.putIfAbsent(executionContext, checkedPerms);
>>              if (existed != null) checkedPerms = existed;
>>          }
>>          if (checkedPerms.contains(perm)) return; // don't need to check
>> again.
>>
>> </SNIP-METHOD>
>>
>> Serialization is implemented so implementations can be replaced and
>> upgraded, serial form is a separate concern; see the Serialization
>> Builder pattern for details:http://wiki.apache.org/river/Serialization
>>
>> This a copy of some implementation code:
>>
>> /*
>>   * Licensed to the Apache Software Foundation (ASF) under one
>>   * or more contributor license agreements.  See the NOTICE file
>>   * distributed with this work for additional information
>>   * regarding copyright ownership. The ASF licenses this file
>>   * to you under the Apache License, Version 2.0 (the
>>   * "License"); you may not use this file except in compliance
>>   * with the License. You may obtain a copy of the License at
>>   *
>>   *http://www.apache.org/licenses/LICENSE-2.0
>>   *
>>   * Unless required by applicable law or agreed to in writing, software
>>   * distributed under the License is distributed on an "AS IS" BASIS,
>>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>> implied.
>>   * See the License for the specific language governing permissions and
>>   * limitations under the License.
>>   */
>>
>> package org.apache.river.impl.util;
>>
>>
>> import java.lang.ref.Reference;
>> import java.util.Random;
>> import java.util.concurrent.ConcurrentMap;
>> import java.util.logging.Level;
>> import java.util.logging.Logger;
>>
>> /**
>>   * A referenced hash map, that encapsulates and utilises any
>> ConcurrentMap
>>   * implementation passed in at construction.
>>   *
>>   * Based on any ConcurrentMap implementation, it doesn't accept null
>> keys or values.
>>   *
>>   * It is recommended although not mandatory to use identity based
>> References for keys,
>>   * unexpected results occur when relying on equal keys, if one key is no
>> longer
>>   * strongly reachable and has been garbage collected and removed from
>> the
>>   * Map.
>>   *
>>   *
>>   *
>>   * If either a key or value, is no longer strongly reachable, their
>> mapping
>>   * will be queued for removal and garbage collection, in compliance with
>>   * the Reference implementation selected.
>>   *
>>   * @param<K>
>>   * @param<V>
>>   * @see Ref
>>   * @author Peter Firmstone.
>>   *
>>   * @since 2.3
>>   */
>> class ReferenceConcurrentMap<K, V>  extends ReferenceMap<K, V>  implements
>> ConcurrentMap<K, V>  {
>>
>>      // ConcurrentMap must be protected from null values?  It changes
>> it's behaviour, is that a problem?
>>      private final ConcurrentMap<Referrer<K>, Referrer<V>>  map;
>>
>>      ReferenceConcurrentMap(ConcurrentMap<Referrer<K>,Referrer<V>>  map,
>> Ref key, Ref val){
>>          super (map, key, val);
>>          this.map = map;
>>      }
>>
>>      ReferenceConcurrentMap(ConcurrentMap<Referrer<K>, Referrer<V>>  map,
>>              ReferenceQueuingFactory<K, Referrer<K>>  krqf,
>> ReferenceQueuingFactory<V, Referrer<V>>  vrqf, Ref key, Ref val){
>>          super(map, krqf, vrqf, key, val);
>>          this.map = map;
>>      }
>>
>>      public V putIfAbsent(K key, V value) {
>>          processQueue();  //may be a slight delay before atomic
>> putIfAbsent
>>          Referrer<K>  k = wrapKey(key, true, false);
>>          Referrer<V>  v = wrapVal(value, true, false);
>>          Referrer<V>  val = map.putIfAbsent(k, v);
>>          while ( val != null ) {
>>              V existed = val.get();
>>              // We hold a strong reference to value, so
>>              if ( existed == null ){
>>                  // stale reference must be replaced, it has been garbage
>> collect but hasn't
>>                  // been removed, we must treat it like the entry doesn't
>> exist.
>>                  if ( map.replace(k, val, v)){
>>                      // replace successful
>>                      return null; // Because officially there was no
>> record.
>>                  } else {
>>                      // Another thread may have replaced it.
>>                      val = map.putIfAbsent(k, v);
>>                  }
>>              } else {
>>                  return existed;
>>              }
>>          }
>>          return null;
>>      }
>>
>>      @SuppressWarnings("unchecked")
>>      public boolean remove(Object key, Object value) {
>>          processQueue();
>>          return map.remove(wrapKey((K) key, false, true), wrapVal((V)
>> value, false, true));
>>      }
>>
>>      public boolean replace(K key, V oldValue, V newValue) {
>>          processQueue();
>>          return map.replace(wrapKey(key, false, true), wrapVal(oldValue,
>> false, true), wrapVal(newValue, true, false));
>>      }
>>
>>      public V replace(K key, V value) {
>>          processQueue();
>>          Referrer<V>  val = map.replace(wrapKey(key, false, true),
>> wrapVal(value, true, false));
>>          if ( val != null ) return val.get();
>>          return null;
>>      }
>> }
>>
>> /*
>>   * Licensed to the Apache Software Foundation (ASF) under one
>>   * or more contributor license agreements.  See the NOTICE file
>>   * distributed with this work for additional information
>>   * regarding copyright ownership. The ASF licenses this file
>>   * to you under the Apache License, Version 2.0 (the
>>   * "License"); you may not use this file except in compliance
>>   * with the License. You may obtain a copy of the License at
>>   *
>>   *http://www.apache.org/licenses/LICENSE-2.0
>>   *
>>   * Unless required by applicable law or agreed to in writing, software
>>   * distributed under the License is distributed on an "AS IS" BASIS,
>>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>> implied.
>>   * See the License for the specific language governing permissions and
>>   * limitations under the License.
>>   */
>>
>> package org.apache.river.impl.util;
>>
>> import java.io.InvalidObjectException;
>> import java.io.ObjectInputStream;
>> import java.io.ObjectStreamException;
>> import java.io.Serializable;
>> import java.io.WriteAbortedException;
>> import java.lang.ref.Reference;
>> import java.lang.ref.ReferenceQueue;
>> import java.util.AbstractCollection;
>> import java.util.ArrayList;
>> import java.util.Collection;
>> import java.util.Iterator;
>> import java.util.List;
>> import java.util.Set;
>> import java.util.logging.Level;
>> import java.util.logging.Logger;
>>
>> /**
>>   * A Collection of Reference Objects, the developer may chose any
>> Collection
>>   * implementation to store the References, which is passed in a runtime.
>>   *
>>   * The underlying Collection implementation governs the specific
>> behaviour of this
>>   * Collection.
>>   *
>>   * Synchronisation must be implemented by the underlying Collection and
>> cannot
>>   * be performed externally to this class.  The underlying Collection
>> must
>>   * also be mutable.  Objects will be removed automatically from the
>> underlying
>>   * Collection when they are eligible for garbage collection.
>>   *
>>   * Weak, Weak Identity, Soft, Soft Identity or Strong references may be
>> used.
>>   * This Collection may be used as an Object pool cache or any other
>> purpose
>>   * that requires unique memory handling.
>>   *
>>   * For concurrent threads, it is recommended to encapsulate the
>> underlying
>>   * collection in a multi read, single write collection for scalability.
>>   *
>>   * @see Ref
>>   * @see ConcurrentCollections#multiReadCollection(java.util.Collection)
>>   * @author Peter Firmstone.
>>   */
>> class ReferenceCollection<T>  extends AbstractCollection<T>
>>                                  implements Collection<T>, Serializable {
>>      private static final long serialVersionUID = 1L;
>>      private final Collection<Referrer<T>>  col;
>>      private final ReferenceQueuingFactory<T, Referrer<T>>  rqf;
>>      private final Ref type;
>>
>>      ReferenceCollection(Collection<Referrer<T>>  col, Ref type){
>>          this(col, new ReferenceProcessor<T>(col, type, type ==
>> Ref.STRONG ? null : new ReferenceQueue<T>()), type);
>>      }
>>
>>      ReferenceCollection(Collection<Referrer<T>>  col,
>>              ReferenceQueuingFactory<T, Referrer<T>>  rqf, Ref type){
>>          this.col = col;
>>          this.rqf = rqf;
>>          this.type = type;
>>      }
>>
>>      void processQueue(){
>>          rqf.processQueue();
>>          }
>>
>>      ReferenceQueuingFactory<T, Referrer<T>>  getRQF(){
>>          return rqf;
>>      }
>>
>>      Ref getRef(){
>>          return type;
>>      }
>>
>>      Referrer<T>  wrapObj(T t, boolean enqueue, boolean temporary){
>>          return rqf.referenced(t, enqueue, temporary);
>>      }
>>
>>      public int size() {
>>          processQueue();
>>          return col.size();
>>      }
>>
>>      public boolean isEmpty() {
>>          processQueue();
>>          return col.isEmpty();
>>      }
>>
>>      public boolean contains(Object o) {
>>          processQueue();
>>          return col.contains(wrapObj((T) o, false, true));
>>      }
>>
>>      /**
>>       * This Iterator may return null values if garbage collection
>>       * runs during iteration.
>>       *
>>       * Always check for null values.
>>       *
>>       * @return T - possibly null.
>>       */
>>      public Iterator<T>  iterator() {
>>          processQueue();
>>          return new ReferenceIterator<T>(col.iterator());
>>      }
>>
>>      public boolean add(T e) {
>>          processQueue();
>>          return col.add(wrapObj(e, true, false));
>>      }
>>
>>      public boolean remove(Object o) {
>>          processQueue();
>>          return col.remove(wrapObj((T) o, false, true));
>>      }
>>
>>
>>      @SuppressWarnings("unchecked")
>>      public boolean containsAll(Collection<?>  c) {
>>          processQueue();
>>          return col.containsAll(new CollectionWrapper<T>((Collection<T>)
>> c, getRQF(), false, true));
>>      }
>>
>>
>>      @SuppressWarnings("unchecked")
>>      public boolean addAll(Collection<? extends T>  c) {
>>          processQueue();
>>          return col.addAll(new CollectionWrapper<T>((Collection<T>) c,
>> getRQF(), true, false));
>>      }
>>
>>      public void clear() {
>>          col.clear();
>>      }
>>
>>      /*
>>       * The next three methods are suitable implementations for
>> subclasses also.
>>       */
>>      public String toString(){
>>          return col.toString();
>>      }
>>
>>      @Override
>>      public int hashCode() {
>>          if ( col instanceof List || col instanceof Set ){
>>              return col.hashCode();
>>          }
>>          return System.identityHashCode(this);
>>      }
>>
>>      /**
>>       * Because equals and hashCode are not defined for collections, we
>>       * cannot guarantee consistent behaviour by implementing equals and
>>       * hashCode.  A collection could be a list, set, queue or deque.
>>       * So a List != Queue and a Set != list. therefore equals for
>> collections is
>>       * not defined.
>>       *
>>       * However since two collections may both also be Lists, while
>> abstracted
>>       * from the client two lists may still be equal.
>>       * @see Collection#equals(java.lang.Object)
>>       */
>>
>>      @Override
>>      public boolean equals(Object o){
>>          if ( o == this ) return true;
>>          if ( col instanceof List || col instanceof Set ){
>>              return col.equals(o);
>>          }
>>          return false;
>>      }
>>
>>      final Object writeReplace() throws ObjectStreamException {
>>          try {
>>              // returns a Builder instead of this class.
>>              return SerializationOfReferenceCollection.create(getClass(),
>> col, type );
>>          } catch (InstantiationException ex) {
>>              throw new WriteAbortedException("Unable to create
>> serialization proxy", ex);
>>          } catch (IllegalAccessException ex) {
>>              throw new WriteAbortedException("Unable to create
>> serialization proxy", ex);
>>          }
>>      }
>>
>>      private void readObject(ObjectInputStream stream)
>>              throws InvalidObjectException{
>>          throw new InvalidObjectException("Builder required");
>>      }
>>
>> }
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-- 
- DML

From vitalyd at gmail.com  Wed Jan  4 11:38:42 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 4 Jan 2012 11:38:42 -0500
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <4F047693.6080508@oracle.com>
References: <1325678569.21868.167.camel@bluto>
	<4F047693.6080508@oracle.com>
Message-ID: <CAHjP37Fkj0oibVNhsKstXChwwUe7gaVvWSLYPavY3-++YZcK8w@mail.gmail.com>

Uncontended CAS on modern cpus (e.g. nehalem) is fairly cheap.  It incurs
local latency in that case and involves draining the store buffer; if the
buffer is empty or near empty it's quite cheap.  See this discussion:
http://www.azulsystems.com/blog/cliff/2011-11-16-a-short-conversation-on-biased-locking
On Jan 4, 2012 10:59 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  > Do you think that all calling threads attempting to obtain the tryLock(), (once only) will cause any performance issues?
>
> tryLock() will ideally execute a CAS instruction to acquire the lock.
> Under no contention, this means that each access will execute a CAS.  This
> is going to be a performance problem.  It will cost at least 10s of
> cycles.  If you're lucky, it won't be significant.  Under contention, the
> CAS instruction will stall the processor pipeline and become a bottleneck.
> If the tryLock() is called heavily, the system won't be able to scale past
> 3-socket Nehalem system.
>
> The JDK's ReferenceQueue.poll() simply reads a volatile variable.  If the
> queue is empty, then the impact will simply be reading the variable's value
> from cache or RAM.  If the value is in L1 cache, then this will only cost 3
> cycles.  If the queue is not empty, then the code acquires a lock and does
> the poll operation.  You probably want to do likewise except instead of
> acquiring the lock, do a tryLock().  Thus, threads won't block on the clean
> up.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/4/2012 5:02 AM, Peter Firmstone wrote:
>
> Hi,
>
> Over at river.apache.org, we've got a reference collections library,
> utilised for concurrent caches, it's straightforward clean and simple
> code.  Easy to use, although Generics are a little verbose.  RC is a
> class with a simple public api, containing static methods used to wrap
> any of the java collections interfaces, allowing you to use any
> reference type, to refer to objects contained in your collections.
>
> All implementation is package private, sync / locking strategies are
> left to the underlying collection, removal / cleaning is performed by
> any thread that obtains a lock on a private ReferenceQueue (each
> collection has it's own), which is guarded by a ReentrantLock.tryLock(),
> threads that don't obtain the lock, continue without performing any
> cleanup.  Do you think that all calling threads attempting to obtain the
> tryLock(), (once only) will cause any performance issues?  Garbage
> collection occurs concurrently with other operations.
>
> Reference types available are Weak Identity, Weak, Soft Identity, Soft
> and Strong.  It's relatively simple to add additional reference types
> such as a timed reference, however this isn't implemented at present.
>
> Here's a useage example, straight from our SecurityManager code:
>
> <SNIP>
> private final ConcurrentMap<AccessControlContext,
> NavigableSet<Permission>> checked;
> private final Comparator<Referrer<Permission>> permCompare;
> </SNIP>
>
> <SNIP-FROM-CONSTRUCTOR>
> ConcurrentMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>> refmap
> 	= new
> ConcurrentHashMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>(100);
>
> checked = RC.concurrentMap(refmap, Ref.SOFT, Ref.STRONG);
> permCompare = RC.comparator(new PermissionComparator());
>
> </SNIP-FROM-CONSTRUCTOR>
>
> <SNIP-METHOD>
>     @Override
>     public void checkPermission(Permission perm, Object context) throws
> SecurityException {
> 	if (!(context instanceof AccessControlContext)) throw new
> SecurityException();
> 	if (perm == null ) throw new NullPointerException("Permission
> Collection null");
>         /* The next line speeds up permission checks related to this
> SecurityManager. */
>         if ( SMPrivilegedContext.equals(context) ||
> SMConstructorContext.equals(context)) return; // prevents endless loop
> in debug.
>         AccessControlContext executionContext = (AccessControlContext)
> context;
>         // Checks if Permission has already been checked for this
> context.
>         NavigableSet<Permission> checkedPerms =
> checked.get(executionContext);
>         if (checkedPerms == null){
>             /* A ConcurrentSkipListSet is used to avoid blocking during
>              * removal operations that occur while the garbage collector
>              * recovers softly reachable memory.  Since this happens
> while
>              * the jvm's under stress, it's important that permission
> checks
>              * continue to perform well.
>              *
>              * Although I considered a multi read, single write Set, I
> wanted
>              * to avoid blocking under stress, caused as a result
>              * of garbage collection.
>              *
>              * The Reference Collection that encapsulates the
> ConcurrentSkipListSet
>              * uses a ReentrantLock.tryLock() guard the ReferenceQueue
> used
>              * to remove objects from the Set.  This allows other
> threads to
>              * proceed during object removal.  Only one thread is given
> access
>              * to the ReferenceQueue, the unlucky caller thread performs
> garbage
>              * removal from the Set before accessing the Set for its
> original
>              * purpose.
>              */
>             NavigableSet<Referrer<Permission>> internal =
>                     new
> ConcurrentSkipListSet<Referrer<Permission>>(permCompare);
>             checkedPerms = RC.navigableSet(internal, Ref.SOFT);
>             NavigableSet<Permission> existed =
> checked.putIfAbsent(executionContext, checkedPerms);
>             if (existed != null) checkedPerms = existed;
>         }
>         if (checkedPerms.contains(perm)) return; // don't need to check
> again.
>
> </SNIP-METHOD>
>
> Serialization is implemented so implementations can be replaced and
> upgraded, serial form is a separate concern; see the Serialization
> Builder pattern for details: http://wiki.apache.org/river/Serialization
>
> This a copy of some implementation code:
>
> /*
>  * Licensed to the Apache Software Foundation (ASF) under one
>  * or more contributor license agreements.  See the NOTICE file
>  * distributed with this work for additional information
>  * regarding copyright ownership. The ASF licenses this file
>  * to you under the Apache License, Version 2.0 (the
>  * "License"); you may not use this file except in compliance
>  * with the License. You may obtain a copy of the License at
>  *
>  *      http://www.apache.org/licenses/LICENSE-2.0
>  *
>  * Unless required by applicable law or agreed to in writing, software
>  * distributed under the License is distributed on an "AS IS" BASIS,
>  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> implied.
>  * See the License for the specific language governing permissions and
>  * limitations under the License.
>  */
>
> package org.apache.river.impl.util;
>
>
> import java.lang.ref.Reference;
> import java.util.Random;
> import java.util.concurrent.ConcurrentMap;
> import java.util.logging.Level;
> import java.util.logging.Logger;
>
> /**
>  * A referenced hash map, that encapsulates and utilises any
> ConcurrentMap
>  * implementation passed in at construction.
>  *
>  * Based on any ConcurrentMap implementation, it doesn't accept null
> keys or values.
>  *
>  * It is recommended although not mandatory to use identity based
> References for keys,
>  * unexpected results occur when relying on equal keys, if one key is no
> longer
>  * strongly reachable and has been garbage collected and removed from
> the
>  * Map.
>  *
>  *
>  *
>  * If either a key or value, is no longer strongly reachable, their
> mapping
>  * will be queued for removal and garbage collection, in compliance with
>  * the Reference implementation selected.
>  *
>  * @param <K>
>  * @param <V>
>  * @see Ref
>  * @author Peter Firmstone.
>  *
>  * @since 2.3
>  */
> class ReferenceConcurrentMap<K, V> extends ReferenceMap<K, V> implements
> ConcurrentMap<K, V> {
>
>     // ConcurrentMap must be protected from null values?  It changes
> it's behaviour, is that a problem?
>     private final ConcurrentMap<Referrer<K>, Referrer<V>> map;
>
>     ReferenceConcurrentMap(ConcurrentMap<Referrer<K>,Referrer<V>> map,
> Ref key, Ref val){
>         super (map, key, val);
>         this.map = map;
>     }
>
>     ReferenceConcurrentMap(ConcurrentMap<Referrer<K>, Referrer<V>> map,
>             ReferenceQueuingFactory<K, Referrer<K>> krqf,
> ReferenceQueuingFactory<V, Referrer<V>> vrqf, Ref key, Ref val){
>         super(map, krqf, vrqf, key, val);
>         this.map = map;
>     }
>
>     public V putIfAbsent(K key, V value) {
>         processQueue();  //may be a slight delay before atomic
> putIfAbsent
>         Referrer<K> k = wrapKey(key, true, false);
>         Referrer<V> v = wrapVal(value, true, false);
>         Referrer<V> val = map.putIfAbsent(k, v);
>         while ( val != null ) {
>             V existed = val.get();
>             // We hold a strong reference to value, so
>             if ( existed == null ){
>                 // stale reference must be replaced, it has been garbage
> collect but hasn't
>                 // been removed, we must treat it like the entry doesn't
> exist.
>                 if ( map.replace(k, val, v)){
>                     // replace successful
>                     return null; // Because officially there was no
> record.
>                 } else {
>                     // Another thread may have replaced it.
>                     val = map.putIfAbsent(k, v);
>                 }
>             } else {
>                 return existed;
>             }
>         }
>         return null;
>     }
>
>     @SuppressWarnings("unchecked")
>     public boolean remove(Object key, Object value) {
>         processQueue();
>         return map.remove(wrapKey((K) key, false, true), wrapVal((V)
> value, false, true));
>     }
>
>     public boolean replace(K key, V oldValue, V newValue) {
>         processQueue();
>         return map.replace(wrapKey(key, false, true), wrapVal(oldValue,
> false, true), wrapVal(newValue, true, false));
>     }
>
>     public V replace(K key, V value) {
>         processQueue();
>         Referrer<V> val = map.replace(wrapKey(key, false, true),
> wrapVal(value, true, false));
>         if ( val != null ) return val.get();
>         return null;
>     }
> }
>
> /*
>  * Licensed to the Apache Software Foundation (ASF) under one
>  * or more contributor license agreements.  See the NOTICE file
>  * distributed with this work for additional information
>  * regarding copyright ownership. The ASF licenses this file
>  * to you under the Apache License, Version 2.0 (the
>  * "License"); you may not use this file except in compliance
>  * with the License. You may obtain a copy of the License at
>  *
>  *      http://www.apache.org/licenses/LICENSE-2.0
>  *
>  * Unless required by applicable law or agreed to in writing, software
>  * distributed under the License is distributed on an "AS IS" BASIS,
>  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> implied.
>  * See the License for the specific language governing permissions and
>  * limitations under the License.
>  */
>
> package org.apache.river.impl.util;
>
> import java.io.InvalidObjectException;
> import java.io.ObjectInputStream;
> import java.io.ObjectStreamException;
> import java.io.Serializable;
> import java.io.WriteAbortedException;
> import java.lang.ref.Reference;
> import java.lang.ref.ReferenceQueue;
> import java.util.AbstractCollection;
> import java.util.ArrayList;
> import java.util.Collection;
> import java.util.Iterator;
> import java.util.List;
> import java.util.Set;
> import java.util.logging.Level;
> import java.util.logging.Logger;
>
> /**
>  * A Collection of Reference Objects, the developer may chose any
> Collection
>  * implementation to store the References, which is passed in a runtime.
>  *
>  * The underlying Collection implementation governs the specific
> behaviour of this
>  * Collection.
>  *
>  * Synchronisation must be implemented by the underlying Collection and
> cannot
>  * be performed externally to this class.  The underlying Collection
> must
>  * also be mutable.  Objects will be removed automatically from the
> underlying
>  * Collection when they are eligible for garbage collection.
>  *
>  * Weak, Weak Identity, Soft, Soft Identity or Strong references may be
> used.
>  * This Collection may be used as an Object pool cache or any other
> purpose
>  * that requires unique memory handling.
>  *
>  * For concurrent threads, it is recommended to encapsulate the
> underlying
>  * collection in a multi read, single write collection for scalability.
>  *
>  * @see Ref
>  * @see ConcurrentCollections#multiReadCollection(java.util.Collection)
>  * @author Peter Firmstone.
>  */
> class ReferenceCollection<T> extends AbstractCollection<T>
>                                 implements Collection<T>, Serializable {
>     private static final long serialVersionUID = 1L;
>     private final Collection<Referrer<T>> col;
>     private final ReferenceQueuingFactory<T, Referrer<T>> rqf;
>     private final Ref type;
>
>     ReferenceCollection(Collection<Referrer<T>> col, Ref type){
>         this(col, new ReferenceProcessor<T>(col, type, type ==
> Ref.STRONG ? null : new ReferenceQueue<T>()), type);
>     }
>
>     ReferenceCollection(Collection<Referrer<T>> col,
>             ReferenceQueuingFactory<T, Referrer<T>> rqf, Ref type){
>         this.col = col;
>         this.rqf = rqf;
>         this.type = type;
>     }
>
>     void processQueue(){
>         rqf.processQueue();
>         }
>
>     ReferenceQueuingFactory<T, Referrer<T>> getRQF(){
>         return rqf;
>     }
>
>     Ref getRef(){
>         return type;
>     }
>
>     Referrer<T> wrapObj(T t, boolean enqueue, boolean temporary){
>         return rqf.referenced(t, enqueue, temporary);
>     }
>
>     public int size() {
>         processQueue();
>         return col.size();
>     }
>
>     public boolean isEmpty() {
>         processQueue();
>         return col.isEmpty();
>     }
>
>     public boolean contains(Object o) {
>         processQueue();
>         return col.contains(wrapObj((T) o, false, true));
>     }
>
>     /**
>      * This Iterator may return null values if garbage collection
>      * runs during iteration.
>      *
>      * Always check for null values.
>      *
>      * @return T - possibly null.
>      */
>     public Iterator<T> iterator() {
>         processQueue();
>         return new ReferenceIterator<T>(col.iterator());
>     }
>
>     public boolean add(T e) {
>         processQueue();
>         return col.add(wrapObj(e, true, false));
>     }
>
>     public boolean remove(Object o) {
>         processQueue();
>         return col.remove(wrapObj((T) o, false, true));
>     }
>
>
>     @SuppressWarnings("unchecked")
>     public boolean containsAll(Collection<?> c) {
>         processQueue();
>         return col.containsAll(new CollectionWrapper<T>((Collection<T>)
> c, getRQF(), false, true));
>     }
>
>
>     @SuppressWarnings("unchecked")
>     public boolean addAll(Collection<? extends T> c) {
>         processQueue();
>         return col.addAll(new CollectionWrapper<T>((Collection<T>) c,
> getRQF(), true, false));
>     }
>
>     public void clear() {
>         col.clear();
>     }
>
>     /*
>      * The next three methods are suitable implementations for
> subclasses also.
>      */
>     public String toString(){
>         return col.toString();
>     }
>
>     @Override
>     public int hashCode() {
>         if ( col instanceof List || col instanceof Set ){
>             return col.hashCode();
>         }
>         return System.identityHashCode(this);
>     }
>
>     /**
>      * Because equals and hashCode are not defined for collections, we
>      * cannot guarantee consistent behaviour by implementing equals and
>      * hashCode.  A collection could be a list, set, queue or deque.
>      * So a List != Queue and a Set != list. therefore equals for
> collections is
>      * not defined.
>      *
>      * However since two collections may both also be Lists, while
> abstracted
>      * from the client two lists may still be equal.
>      * @see Collection#equals(java.lang.Object)
>      */
>
>     @Override
>     public boolean equals(Object o){
>         if ( o == this ) return true;
>         if ( col instanceof List || col instanceof Set ){
>             return col.equals(o);
>         }
>         return false;
>     }
>
>     final Object writeReplace() throws ObjectStreamException {
>         try {
>             // returns a Builder instead of this class.
>             return SerializationOfReferenceCollection.create(getClass(),
> col, type );
>         } catch (InstantiationException ex) {
>             throw new WriteAbortedException("Unable to create
> serialization proxy", ex);
>         } catch (IllegalAccessException ex) {
>             throw new WriteAbortedException("Unable to create
> serialization proxy", ex);
>         }
>     }
>
>     private void readObject(ObjectInputStream stream)
>             throws InvalidObjectException{
>         throw new InvalidObjectException("Builder required");
>     }
>
> }
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/ecfe5aee/attachment-0001.html>

From nathan.reynolds at oracle.com  Wed Jan  4 11:59:07 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 04 Jan 2012 09:59:07 -0700
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <4F047A3E.7050109@redhat.com>
References: <1325678569.21868.167.camel@bluto> <4F047693.6080508@oracle.com>
	<4F047A3E.7050109@redhat.com>
Message-ID: <4F04855B.1050603@oracle.com>

Yes.  A CAS operation has to load the cache line, invalidate the cache 
line on all other processors, do the comparison and possibly write to 
the cache line.  A volatile read only has to load the cache line and 
perform the comparison in another instruction.  The volatile read + 
comparison saves time by avoiding the invalidation.

The queue will be empty most of the time.  Only right after GC will the 
queue have entries.  Those entries will be removed and the queue will 
become empty again.  The common case is for the queue to be empty.  So, 
if the queue is not empty, then the volatile read + comparison + CAS 
(tryLock) will take longer.  However, this longer path will save time 
for the common case.

When the queue is non-empty, the threads will execute tryLock() on each 
access.  This means while 1 thread is busy cleaning the queue, the other 
threads will tryLock() several times.  A better approach would be to do 
the following.  This code does the volatile read and comparison.  If the 
queue is not empty, then with 1 CAS instruction the queue is made empty 
and the thread gets the entire contents of the queue.  The thread can 
then process the queue and the rest of the threads will see an empty queue.

public Reference<? extends T> pollAll()
{
     Reference<? extends T> head;

     do
     {
         head = this.head;

         if (head == null)
             return(null);
     }
     while (!s_atomicHead.compareAndSet(this, head, null));

    return(head);
}

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/4/2012 9:11 AM, David M. Lloyd wrote:
> Are you saying that a failed CAS has greater impact than a volatile 
> read + external comparison?
>
> On 01/04/2012 09:56 AM, Nathan Reynolds wrote:
>>>  Do you think that all calling threads attempting to obtain the 
>>> tryLock(), (once only) will cause any performance issues?
>>
>> tryLock() will ideally execute a CAS instruction to acquire the lock.
>> Under no contention, this means that each access will execute a CAS.
>> This is going to be a performance problem. It will cost at least 10s of
>> cycles. If you're lucky, it won't be significant. Under contention, the
>> CAS instruction will stall the processor pipeline and become a
>> bottleneck. If the tryLock() is called heavily, the system won't be able
>> to scale past 3-socket Nehalem system.
>>
>> The JDK's ReferenceQueue.poll() simply reads a volatile variable. If the
>> queue is empty, then the impact will simply be reading the variable's
>> value from cache or RAM. If the value is in L1 cache, then this will
>> only cost 3 cycles. If the queue is not empty, then the code acquires a
>> lock and does the poll operation. You probably want to do likewise
>> except instead of acquiring the lock, do a tryLock(). Thus, threads
>> won't block on the clean up.
>>
>> Nathan Reynolds
>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>> Consulting Member of Technical Staff | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>> On 1/4/2012 5:02 AM, Peter Firmstone wrote:
>>> Hi,
>>>
>>> Over at river.apache.org, we've got a reference collections library,
>>> utilised for concurrent caches, it's straightforward clean and simple
>>> code.  Easy to use, although Generics are a little verbose.  RC is a
>>> class with a simple public api, containing static methods used to wrap
>>> any of the java collections interfaces, allowing you to use any
>>> reference type, to refer to objects contained in your collections.
>>>
>>> All implementation is package private, sync / locking strategies are
>>> left to the underlying collection, removal / cleaning is performed by
>>> any thread that obtains a lock on a private ReferenceQueue (each
>>> collection has it's own), which is guarded by a 
>>> ReentrantLock.tryLock(),
>>> threads that don't obtain the lock, continue without performing any
>>> cleanup.  Do you think that all calling threads attempting to obtain 
>>> the
>>> tryLock(), (once only) will cause any performance issues?  Garbage
>>> collection occurs concurrently with other operations.
>>>
>>> Reference types available are Weak Identity, Weak, Soft Identity, Soft
>>> and Strong.  It's relatively simple to add additional reference types
>>> such as a timed reference, however this isn't implemented at present.
>>>
>>> Here's a useage example, straight from our SecurityManager code:
>>>
>>> <SNIP>
>>> private final ConcurrentMap<AccessControlContext,
>>> NavigableSet<Permission>>  checked;
>>> private final Comparator<Referrer<Permission>>  permCompare;
>>> </SNIP>
>>>
>>> <SNIP-FROM-CONSTRUCTOR>
>>> ConcurrentMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>  
>>> refmap
>>>     = new
>>> ConcurrentHashMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>(100); 
>>>
>>>
>>> checked = RC.concurrentMap(refmap, Ref.SOFT, Ref.STRONG);
>>> permCompare = RC.comparator(new PermissionComparator());
>>>
>>> </SNIP-FROM-CONSTRUCTOR>
>>>
>>> <SNIP-METHOD>
>>>      @Override
>>>      public void checkPermission(Permission perm, Object context) 
>>> throws
>>> SecurityException {
>>>     if (!(context instanceof AccessControlContext)) throw new
>>> SecurityException();
>>>     if (perm == null ) throw new NullPointerException("Permission
>>> Collection null");
>>>          /* The next line speeds up permission checks related to this
>>> SecurityManager. */
>>>          if ( SMPrivilegedContext.equals(context) ||
>>> SMConstructorContext.equals(context)) return; // prevents endless loop
>>> in debug.
>>>          AccessControlContext executionContext = (AccessControlContext)
>>> context;
>>>          // Checks if Permission has already been checked for this
>>> context.
>>>          NavigableSet<Permission>  checkedPerms =
>>> checked.get(executionContext);
>>>          if (checkedPerms == null){
>>>              /* A ConcurrentSkipListSet is used to avoid blocking 
>>> during
>>>               * removal operations that occur while the garbage 
>>> collector
>>>               * recovers softly reachable memory.  Since this happens
>>> while
>>>               * the jvm's under stress, it's important that permission
>>> checks
>>>               * continue to perform well.
>>>               *
>>>               * Although I considered a multi read, single write Set, I
>>> wanted
>>>               * to avoid blocking under stress, caused as a result
>>>               * of garbage collection.
>>>               *
>>>               * The Reference Collection that encapsulates the
>>> ConcurrentSkipListSet
>>>               * uses a ReentrantLock.tryLock() guard the ReferenceQueue
>>> used
>>>               * to remove objects from the Set.  This allows other
>>> threads to
>>>               * proceed during object removal.  Only one thread is 
>>> given
>>> access
>>>               * to the ReferenceQueue, the unlucky caller thread 
>>> performs
>>> garbage
>>>               * removal from the Set before accessing the Set for its
>>> original
>>>               * purpose.
>>>               */
>>>              NavigableSet<Referrer<Permission>>  internal =
>>>                      new
>>> ConcurrentSkipListSet<Referrer<Permission>>(permCompare);
>>>              checkedPerms = RC.navigableSet(internal, Ref.SOFT);
>>>              NavigableSet<Permission>  existed =
>>> checked.putIfAbsent(executionContext, checkedPerms);
>>>              if (existed != null) checkedPerms = existed;
>>>          }
>>>          if (checkedPerms.contains(perm)) return; // don't need to 
>>> check
>>> again.
>>>
>>> </SNIP-METHOD>
>>>
>>> Serialization is implemented so implementations can be replaced and
>>> upgraded, serial form is a separate concern; see the Serialization
>>> Builder pattern for details:http://wiki.apache.org/river/Serialization
>>>
>>> This a copy of some implementation code:
>>>
>>> /*
>>>   * Licensed to the Apache Software Foundation (ASF) under one
>>>   * or more contributor license agreements.  See the NOTICE file
>>>   * distributed with this work for additional information
>>>   * regarding copyright ownership. The ASF licenses this file
>>>   * to you under the Apache License, Version 2.0 (the
>>>   * "License"); you may not use this file except in compliance
>>>   * with the License. You may obtain a copy of the License at
>>>   *
>>>   *http://www.apache.org/licenses/LICENSE-2.0
>>>   *
>>>   * Unless required by applicable law or agreed to in writing, software
>>>   * distributed under the License is distributed on an "AS IS" BASIS,
>>>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>>> implied.
>>>   * See the License for the specific language governing permissions and
>>>   * limitations under the License.
>>>   */
>>>
>>> package org.apache.river.impl.util;
>>>
>>>
>>> import java.lang.ref.Reference;
>>> import java.util.Random;
>>> import java.util.concurrent.ConcurrentMap;
>>> import java.util.logging.Level;
>>> import java.util.logging.Logger;
>>>
>>> /**
>>>   * A referenced hash map, that encapsulates and utilises any
>>> ConcurrentMap
>>>   * implementation passed in at construction.
>>>   *
>>>   * Based on any ConcurrentMap implementation, it doesn't accept null
>>> keys or values.
>>>   *
>>>   * It is recommended although not mandatory to use identity based
>>> References for keys,
>>>   * unexpected results occur when relying on equal keys, if one key 
>>> is no
>>> longer
>>>   * strongly reachable and has been garbage collected and removed from
>>> the
>>>   * Map.
>>>   *
>>>   *
>>>   *
>>>   * If either a key or value, is no longer strongly reachable, their
>>> mapping
>>>   * will be queued for removal and garbage collection, in compliance 
>>> with
>>>   * the Reference implementation selected.
>>>   *
>>>   * @param<K>
>>>   * @param<V>
>>>   * @see Ref
>>>   * @author Peter Firmstone.
>>>   *
>>>   * @since 2.3
>>>   */
>>> class ReferenceConcurrentMap<K, V>  extends ReferenceMap<K, V>  
>>> implements
>>> ConcurrentMap<K, V>  {
>>>
>>>      // ConcurrentMap must be protected from null values?  It changes
>>> it's behaviour, is that a problem?
>>>      private final ConcurrentMap<Referrer<K>, Referrer<V>>  map;
>>>
>>>      ReferenceConcurrentMap(ConcurrentMap<Referrer<K>,Referrer<V>>  
>>> map,
>>> Ref key, Ref val){
>>>          super (map, key, val);
>>>          this.map = map;
>>>      }
>>>
>>>      ReferenceConcurrentMap(ConcurrentMap<Referrer<K>, Referrer<V>>  
>>> map,
>>>              ReferenceQueuingFactory<K, Referrer<K>>  krqf,
>>> ReferenceQueuingFactory<V, Referrer<V>>  vrqf, Ref key, Ref val){
>>>          super(map, krqf, vrqf, key, val);
>>>          this.map = map;
>>>      }
>>>
>>>      public V putIfAbsent(K key, V value) {
>>>          processQueue();  //may be a slight delay before atomic
>>> putIfAbsent
>>>          Referrer<K>  k = wrapKey(key, true, false);
>>>          Referrer<V>  v = wrapVal(value, true, false);
>>>          Referrer<V>  val = map.putIfAbsent(k, v);
>>>          while ( val != null ) {
>>>              V existed = val.get();
>>>              // We hold a strong reference to value, so
>>>              if ( existed == null ){
>>>                  // stale reference must be replaced, it has been 
>>> garbage
>>> collect but hasn't
>>>                  // been removed, we must treat it like the entry 
>>> doesn't
>>> exist.
>>>                  if ( map.replace(k, val, v)){
>>>                      // replace successful
>>>                      return null; // Because officially there was no
>>> record.
>>>                  } else {
>>>                      // Another thread may have replaced it.
>>>                      val = map.putIfAbsent(k, v);
>>>                  }
>>>              } else {
>>>                  return existed;
>>>              }
>>>          }
>>>          return null;
>>>      }
>>>
>>>      @SuppressWarnings("unchecked")
>>>      public boolean remove(Object key, Object value) {
>>>          processQueue();
>>>          return map.remove(wrapKey((K) key, false, true), wrapVal((V)
>>> value, false, true));
>>>      }
>>>
>>>      public boolean replace(K key, V oldValue, V newValue) {
>>>          processQueue();
>>>          return map.replace(wrapKey(key, false, true), 
>>> wrapVal(oldValue,
>>> false, true), wrapVal(newValue, true, false));
>>>      }
>>>
>>>      public V replace(K key, V value) {
>>>          processQueue();
>>>          Referrer<V>  val = map.replace(wrapKey(key, false, true),
>>> wrapVal(value, true, false));
>>>          if ( val != null ) return val.get();
>>>          return null;
>>>      }
>>> }
>>>
>>> /*
>>>   * Licensed to the Apache Software Foundation (ASF) under one
>>>   * or more contributor license agreements.  See the NOTICE file
>>>   * distributed with this work for additional information
>>>   * regarding copyright ownership. The ASF licenses this file
>>>   * to you under the Apache License, Version 2.0 (the
>>>   * "License"); you may not use this file except in compliance
>>>   * with the License. You may obtain a copy of the License at
>>>   *
>>>   *http://www.apache.org/licenses/LICENSE-2.0
>>>   *
>>>   * Unless required by applicable law or agreed to in writing, software
>>>   * distributed under the License is distributed on an "AS IS" BASIS,
>>>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>>> implied.
>>>   * See the License for the specific language governing permissions and
>>>   * limitations under the License.
>>>   */
>>>
>>> package org.apache.river.impl.util;
>>>
>>> import java.io.InvalidObjectException;
>>> import java.io.ObjectInputStream;
>>> import java.io.ObjectStreamException;
>>> import java.io.Serializable;
>>> import java.io.WriteAbortedException;
>>> import java.lang.ref.Reference;
>>> import java.lang.ref.ReferenceQueue;
>>> import java.util.AbstractCollection;
>>> import java.util.ArrayList;
>>> import java.util.Collection;
>>> import java.util.Iterator;
>>> import java.util.List;
>>> import java.util.Set;
>>> import java.util.logging.Level;
>>> import java.util.logging.Logger;
>>>
>>> /**
>>>   * A Collection of Reference Objects, the developer may chose any
>>> Collection
>>>   * implementation to store the References, which is passed in a 
>>> runtime.
>>>   *
>>>   * The underlying Collection implementation governs the specific
>>> behaviour of this
>>>   * Collection.
>>>   *
>>>   * Synchronisation must be implemented by the underlying Collection 
>>> and
>>> cannot
>>>   * be performed externally to this class.  The underlying Collection
>>> must
>>>   * also be mutable.  Objects will be removed automatically from the
>>> underlying
>>>   * Collection when they are eligible for garbage collection.
>>>   *
>>>   * Weak, Weak Identity, Soft, Soft Identity or Strong references 
>>> may be
>>> used.
>>>   * This Collection may be used as an Object pool cache or any other
>>> purpose
>>>   * that requires unique memory handling.
>>>   *
>>>   * For concurrent threads, it is recommended to encapsulate the
>>> underlying
>>>   * collection in a multi read, single write collection for 
>>> scalability.
>>>   *
>>>   * @see Ref
>>>   * @see 
>>> ConcurrentCollections#multiReadCollection(java.util.Collection)
>>>   * @author Peter Firmstone.
>>>   */
>>> class ReferenceCollection<T>  extends AbstractCollection<T>
>>>                                  implements Collection<T>, 
>>> Serializable {
>>>      private static final long serialVersionUID = 1L;
>>>      private final Collection<Referrer<T>>  col;
>>>      private final ReferenceQueuingFactory<T, Referrer<T>>  rqf;
>>>      private final Ref type;
>>>
>>>      ReferenceCollection(Collection<Referrer<T>>  col, Ref type){
>>>          this(col, new ReferenceProcessor<T>(col, type, type ==
>>> Ref.STRONG ? null : new ReferenceQueue<T>()), type);
>>>      }
>>>
>>>      ReferenceCollection(Collection<Referrer<T>>  col,
>>>              ReferenceQueuingFactory<T, Referrer<T>>  rqf, Ref type){
>>>          this.col = col;
>>>          this.rqf = rqf;
>>>          this.type = type;
>>>      }
>>>
>>>      void processQueue(){
>>>          rqf.processQueue();
>>>          }
>>>
>>>      ReferenceQueuingFactory<T, Referrer<T>>  getRQF(){
>>>          return rqf;
>>>      }
>>>
>>>      Ref getRef(){
>>>          return type;
>>>      }
>>>
>>>      Referrer<T>  wrapObj(T t, boolean enqueue, boolean temporary){
>>>          return rqf.referenced(t, enqueue, temporary);
>>>      }
>>>
>>>      public int size() {
>>>          processQueue();
>>>          return col.size();
>>>      }
>>>
>>>      public boolean isEmpty() {
>>>          processQueue();
>>>          return col.isEmpty();
>>>      }
>>>
>>>      public boolean contains(Object o) {
>>>          processQueue();
>>>          return col.contains(wrapObj((T) o, false, true));
>>>      }
>>>
>>>      /**
>>>       * This Iterator may return null values if garbage collection
>>>       * runs during iteration.
>>>       *
>>>       * Always check for null values.
>>>       *
>>>       * @return T - possibly null.
>>>       */
>>>      public Iterator<T>  iterator() {
>>>          processQueue();
>>>          return new ReferenceIterator<T>(col.iterator());
>>>      }
>>>
>>>      public boolean add(T e) {
>>>          processQueue();
>>>          return col.add(wrapObj(e, true, false));
>>>      }
>>>
>>>      public boolean remove(Object o) {
>>>          processQueue();
>>>          return col.remove(wrapObj((T) o, false, true));
>>>      }
>>>
>>>
>>>      @SuppressWarnings("unchecked")
>>>      public boolean containsAll(Collection<?>  c) {
>>>          processQueue();
>>>          return col.containsAll(new 
>>> CollectionWrapper<T>((Collection<T>)
>>> c, getRQF(), false, true));
>>>      }
>>>
>>>
>>>      @SuppressWarnings("unchecked")
>>>      public boolean addAll(Collection<? extends T>  c) {
>>>          processQueue();
>>>          return col.addAll(new CollectionWrapper<T>((Collection<T>) c,
>>> getRQF(), true, false));
>>>      }
>>>
>>>      public void clear() {
>>>          col.clear();
>>>      }
>>>
>>>      /*
>>>       * The next three methods are suitable implementations for
>>> subclasses also.
>>>       */
>>>      public String toString(){
>>>          return col.toString();
>>>      }
>>>
>>>      @Override
>>>      public int hashCode() {
>>>          if ( col instanceof List || col instanceof Set ){
>>>              return col.hashCode();
>>>          }
>>>          return System.identityHashCode(this);
>>>      }
>>>
>>>      /**
>>>       * Because equals and hashCode are not defined for collections, we
>>>       * cannot guarantee consistent behaviour by implementing equals 
>>> and
>>>       * hashCode.  A collection could be a list, set, queue or deque.
>>>       * So a List != Queue and a Set != list. therefore equals for
>>> collections is
>>>       * not defined.
>>>       *
>>>       * However since two collections may both also be Lists, while
>>> abstracted
>>>       * from the client two lists may still be equal.
>>>       * @see Collection#equals(java.lang.Object)
>>>       */
>>>
>>>      @Override
>>>      public boolean equals(Object o){
>>>          if ( o == this ) return true;
>>>          if ( col instanceof List || col instanceof Set ){
>>>              return col.equals(o);
>>>          }
>>>          return false;
>>>      }
>>>
>>>      final Object writeReplace() throws ObjectStreamException {
>>>          try {
>>>              // returns a Builder instead of this class.
>>>              return 
>>> SerializationOfReferenceCollection.create(getClass(),
>>> col, type );
>>>          } catch (InstantiationException ex) {
>>>              throw new WriteAbortedException("Unable to create
>>> serialization proxy", ex);
>>>          } catch (IllegalAccessException ex) {
>>>              throw new WriteAbortedException("Unable to create
>>> serialization proxy", ex);
>>>          }
>>>      }
>>>
>>>      private void readObject(ObjectInputStream stream)
>>>              throws InvalidObjectException{
>>>          throw new InvalidObjectException("Builder required");
>>>      }
>>>
>>> }
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/0c665c85/attachment-0001.html>

From nathan.reynolds at oracle.com  Wed Jan  4 12:08:27 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 04 Jan 2012 10:08:27 -0700
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <CAHjP37Fkj0oibVNhsKstXChwwUe7gaVvWSLYPavY3-++YZcK8w@mail.gmail.com>
References: <1325678569.21868.167.camel@bluto> <4F047693.6080508@oracle.com>
	<CAHjP37Fkj0oibVNhsKstXChwwUe7gaVvWSLYPavY3-++YZcK8w@mail.gmail.com>
Message-ID: <4F04878B.9010105@oracle.com>

Yes, in terms of performance it is fairly cheap, but it will never be as 
cheap as a volatile read.  Current HotSpot and JRockit x86 
implementations turn volatile reads into normal loads from cache/RAM 
(not registers).  This is much cheaper than draining the store buffer 
with a fence.  Normal loads will bypass pending stores... unless there 
is a fence after a store (e.g. volatile store).

No, in terms of scalability.  An uncontended CAS in this use case 
requires sending invalidation messages to all of the other processors on 
the system.  This adds quite a bit of latency on an 8-socket system 
where the sockets are not fully connected to each other.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/4/2012 9:38 AM, Vitaly Davidovich wrote:
>
> Uncontended CAS on modern cpus (e.g. nehalem) is fairly cheap.  It 
> incurs local latency in that case and involves draining the store 
> buffer; if the buffer is empty or near empty it's quite cheap.  See 
> this discussion: 
> http://www.azulsystems.com/blog/cliff/2011-11-16-a-short-conversation-on-biased-locking
>
> On Jan 4, 2012 10:59 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com 
> <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     >  Do you think that all calling threads attempting to obtain the tryLock(), (once only) will cause any performance issues?
>
>     tryLock() will ideally execute a CAS instruction to acquire the
>     lock.  Under no contention, this means that each access will
>     execute a CAS.  This is going to be a performance problem.  It
>     will cost at least 10s of cycles.  If you're lucky, it won't be
>     significant.  Under contention, the CAS instruction will stall the
>     processor pipeline and become a bottleneck.  If the tryLock() is
>     called heavily, the system won't be able to scale past 3-socket
>     Nehalem system.
>
>     The JDK's ReferenceQueue.poll() simply reads a volatile variable. 
>     If the queue is empty, then the impact will simply be reading the
>     variable's value from cache or RAM.  If the value is in L1 cache,
>     then this will only cost 3 cycles.  If the queue is not empty,
>     then the code acquires a lock and does the poll operation.  You
>     probably want to do likewise except instead of acquiring the lock,
>     do a tryLock().  Thus, threads won't block on the clean up.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Consulting Member of Technical Staff | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
>     On 1/4/2012 5:02 AM, Peter Firmstone wrote:
>>     Hi,
>>
>>     Over atriver.apache.org  <http://river.apache.org>, we've got a reference collections library,
>>     utilised for concurrent caches, it's straightforward clean and simple
>>     code.  Easy to use, although Generics are a little verbose.  RC is a
>>     class with a simple public api, containing static methods used to wrap
>>     any of the java collections interfaces, allowing you to use any
>>     reference type, to refer to objects contained in your collections.
>>
>>     All implementation is package private, sync / locking strategies are
>>     left to the underlying collection, removal / cleaning is performed by
>>     any thread that obtains a lock on a private ReferenceQueue (each
>>     collection has it's own), which is guarded by a ReentrantLock.tryLock(),
>>     threads that don't obtain the lock, continue without performing any
>>     cleanup.  Do you think that all calling threads attempting to obtain the
>>     tryLock(), (once only) will cause any performance issues?  Garbage
>>     collection occurs concurrently with other operations.
>>
>>     Reference types available are Weak Identity, Weak, Soft Identity, Soft
>>     and Strong.  It's relatively simple to add additional reference types
>>     such as a timed reference, however this isn't implemented at present.
>>
>>     Here's a useage example, straight from our SecurityManager code:
>>
>>     <SNIP>
>>     private final ConcurrentMap<AccessControlContext,
>>     NavigableSet<Permission>>  checked;
>>     private final Comparator<Referrer<Permission>>  permCompare;
>>     </SNIP>
>>
>>     <SNIP-FROM-CONSTRUCTOR>
>>     ConcurrentMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>  refmap
>>     	= new
>>     ConcurrentHashMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>(100);
>>
>>     checked = RC.concurrentMap(refmap, Ref.SOFT, Ref.STRONG);
>>     permCompare = RC.comparator(new PermissionComparator());
>>
>>     </SNIP-FROM-CONSTRUCTOR>
>>
>>     <SNIP-METHOD>
>>          @Override
>>          public void checkPermission(Permission perm, Object context) throws
>>     SecurityException {
>>     	if (!(context instanceof AccessControlContext)) throw new
>>     SecurityException();
>>     	if (perm == null ) throw new NullPointerException("Permission
>>     Collection null");
>>              /* The next line speeds up permission checks related to this
>>     SecurityManager. */
>>              if ( SMPrivilegedContext.equals(context) ||
>>     SMConstructorContext.equals(context)) return; // prevents endless loop
>>     in debug.
>>              AccessControlContext executionContext = (AccessControlContext)
>>     context;
>>              // Checks if Permission has already been checked for this
>>     context.
>>              NavigableSet<Permission>  checkedPerms =
>>     checked.get(executionContext);
>>              if (checkedPerms == null){
>>                  /* A ConcurrentSkipListSet is used to avoid blocking during
>>                   * removal operations that occur while the garbage collector
>>                   * recovers softly reachable memory.  Since this happens
>>     while
>>                   * the jvm's under stress, it's important that permission
>>     checks
>>                   * continue to perform well.
>>                   *
>>                   * Although I considered a multi read, single write Set, I
>>     wanted
>>                   * to avoid blocking under stress, caused as a result
>>                   * of garbage collection.
>>                   *
>>                   * The Reference Collection that encapsulates the
>>     ConcurrentSkipListSet
>>                   * uses a ReentrantLock.tryLock() guard the ReferenceQueue
>>     used
>>                   * to remove objects from the Set.  This allows other
>>     threads to
>>                   * proceed during object removal.  Only one thread is given
>>     access
>>                   * to the ReferenceQueue, the unlucky caller thread performs
>>     garbage
>>                   * removal from the Set before accessing the Set for its
>>     original
>>                   * purpose.
>>                   */
>>                  NavigableSet<Referrer<Permission>>  internal =
>>                          new
>>     ConcurrentSkipListSet<Referrer<Permission>>(permCompare);
>>                  checkedPerms = RC.navigableSet(internal, Ref.SOFT);
>>                  NavigableSet<Permission>  existed =
>>     checked.putIfAbsent(executionContext, checkedPerms);
>>                  if (existed != null) checkedPerms = existed;
>>              }
>>              if (checkedPerms.contains(perm)) return; // don't need to check
>>     again.
>>
>>     </SNIP-METHOD>
>>
>>     Serialization is implemented so implementations can be replaced and
>>     upgraded, serial form is a separate concern; see the Serialization
>>     Builder pattern for details:http://wiki.apache.org/river/Serialization
>>
>>     This a copy of some implementation code:
>>
>>     /*
>>       * Licensed to the Apache Software Foundation (ASF) under one
>>       * or more contributor license agreements.  See the NOTICE file
>>       * distributed with this work for additional information
>>       * regarding copyright ownership. The ASF licenses this file
>>       * to you under the Apache License, Version 2.0 (the
>>       * "License"); you may not use this file except in compliance
>>       * with the License. You may obtain a copy of the License at
>>       *
>>       *http://www.apache.org/licenses/LICENSE-2.0
>>       *
>>       * Unless required by applicable law or agreed to in writing, software
>>       * distributed under the License is distributed on an "AS IS" BASIS,
>>       * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>>     implied.
>>       * See the License for the specific language governing permissions and
>>       * limitations under the License.
>>       */
>>
>>     package org.apache.river.impl.util;
>>
>>
>>     import java.lang.ref.Reference;
>>     import java.util.Random;
>>     import java.util.concurrent.ConcurrentMap;
>>     import java.util.logging.Level;
>>     import java.util.logging.Logger;
>>
>>     /**
>>       * A referenced hash map, that encapsulates and utilises any
>>     ConcurrentMap
>>       * implementation passed in at construction.
>>       *
>>       * Based on any ConcurrentMap implementation, it doesn't accept null
>>     keys or values.
>>       *
>>       * It is recommended although not mandatory to use identity based
>>     References for keys,
>>       * unexpected results occur when relying on equal keys, if one key is no
>>     longer
>>       * strongly reachable and has been garbage collected and removed from
>>     the
>>       * Map.
>>       *
>>       *
>>       *
>>       * If either a key or value, is no longer strongly reachable, their
>>     mapping
>>       * will be queued for removal and garbage collection, in compliance with
>>       * the Reference implementation selected.
>>       *
>>       * @param<K>
>>       * @param<V>
>>       * @see Ref
>>       * @author Peter Firmstone.
>>       *
>>       * @since 2.3
>>       */
>>     class ReferenceConcurrentMap<K, V>  extends ReferenceMap<K, V>  implements
>>     ConcurrentMap<K, V>  {
>>
>>          // ConcurrentMap must be protected from null values?  It changes
>>     it's behaviour, is that a problem?
>>          private final ConcurrentMap<Referrer<K>, Referrer<V>>  map;
>>
>>          ReferenceConcurrentMap(ConcurrentMap<Referrer<K>,Referrer<V>>  map,
>>     Ref key, Ref val){
>>              super (map, key, val);
>>              this.map = map;
>>          }
>>
>>          ReferenceConcurrentMap(ConcurrentMap<Referrer<K>, Referrer<V>>  map,
>>                  ReferenceQueuingFactory<K, Referrer<K>>  krqf,
>>     ReferenceQueuingFactory<V, Referrer<V>>  vrqf, Ref key, Ref val){
>>              super(map, krqf, vrqf, key, val);
>>              this.map = map;
>>          }
>>
>>          public V putIfAbsent(K key, V value) {
>>              processQueue();  //may be a slight delay before atomic
>>     putIfAbsent
>>              Referrer<K>  k = wrapKey(key, true, false);
>>              Referrer<V>  v = wrapVal(value, true, false);
>>              Referrer<V>  val = map.putIfAbsent(k, v);
>>              while ( val != null ) {
>>                  V existed = val.get();
>>                  // We hold a strong reference to value, so
>>                  if ( existed == null ){
>>                      // stale reference must be replaced, it has been garbage
>>     collect but hasn't
>>                      // been removed, we must treat it like the entry doesn't
>>     exist.
>>                      if ( map.replace(k, val, v)){
>>                          // replace successful
>>                          return null; // Because officially there was no
>>     record.
>>                      } else {
>>                          // Another thread may have replaced it.
>>                          val = map.putIfAbsent(k, v);
>>                      }
>>                  } else {
>>                      return existed;
>>                  }
>>              }
>>              return null;
>>          }
>>
>>          @SuppressWarnings("unchecked")
>>          public boolean remove(Object key, Object value) {
>>              processQueue();
>>              return map.remove(wrapKey((K) key, false, true), wrapVal((V)
>>     value, false, true));
>>          }
>>
>>          public boolean replace(K key, V oldValue, V newValue) {
>>              processQueue();
>>              return map.replace(wrapKey(key, false, true), wrapVal(oldValue,
>>     false, true), wrapVal(newValue, true, false));
>>          }
>>
>>          public V replace(K key, V value) {
>>              processQueue();
>>              Referrer<V>  val = map.replace(wrapKey(key, false, true),
>>     wrapVal(value, true, false));
>>              if ( val != null ) return val.get();
>>              return null;
>>          }
>>     }
>>
>>     /*
>>       * Licensed to the Apache Software Foundation (ASF) under one
>>       * or more contributor license agreements.  See the NOTICE file
>>       * distributed with this work for additional information
>>       * regarding copyright ownership. The ASF licenses this file
>>       * to you under the Apache License, Version 2.0 (the
>>       * "License"); you may not use this file except in compliance
>>       * with the License. You may obtain a copy of the License at
>>       *
>>       *http://www.apache.org/licenses/LICENSE-2.0
>>       *
>>       * Unless required by applicable law or agreed to in writing, software
>>       * distributed under the License is distributed on an "AS IS" BASIS,
>>       * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>>     implied.
>>       * See the License for the specific language governing permissions and
>>       * limitations under the License.
>>       */
>>
>>     package org.apache.river.impl.util;
>>
>>     import java.io.InvalidObjectException;
>>     import java.io.ObjectInputStream;
>>     import java.io.ObjectStreamException;
>>     import java.io.Serializable;
>>     import java.io.WriteAbortedException;
>>     import java.lang.ref.Reference;
>>     import java.lang.ref.ReferenceQueue;
>>     import java.util.AbstractCollection;
>>     import java.util.ArrayList;
>>     import java.util.Collection;
>>     import java.util.Iterator;
>>     import java.util.List;
>>     import java.util.Set;
>>     import java.util.logging.Level;
>>     import java.util.logging.Logger;
>>
>>     /**
>>       * A Collection of Reference Objects, the developer may chose any
>>     Collection
>>       * implementation to store the References, which is passed in a runtime.
>>       *
>>       * The underlying Collection implementation governs the specific
>>     behaviour of this
>>       * Collection.
>>       *
>>       * Synchronisation must be implemented by the underlying Collection and
>>     cannot
>>       * be performed externally to this class.  The underlying Collection
>>     must
>>       * also be mutable.  Objects will be removed automatically from the
>>     underlying
>>       * Collection when they are eligible for garbage collection.
>>       *
>>       * Weak, Weak Identity, Soft, Soft Identity or Strong references may be
>>     used.
>>       * This Collection may be used as an Object pool cache or any other
>>     purpose
>>       * that requires unique memory handling.
>>       *
>>       * For concurrent threads, it is recommended to encapsulate the
>>     underlying
>>       * collection in a multi read, single write collection for scalability.
>>       *
>>       * @see Ref
>>       * @see ConcurrentCollections#multiReadCollection(java.util.Collection)
>>       * @author Peter Firmstone.
>>       */
>>     class ReferenceCollection<T>  extends AbstractCollection<T>
>>                                      implements Collection<T>, Serializable {
>>          private static final long serialVersionUID = 1L;
>>          private final Collection<Referrer<T>>  col;
>>          private final ReferenceQueuingFactory<T, Referrer<T>>  rqf;
>>          private final Ref type;
>>
>>          ReferenceCollection(Collection<Referrer<T>>  col, Ref type){
>>              this(col, new ReferenceProcessor<T>(col, type, type ==
>>     Ref.STRONG ? null : new ReferenceQueue<T>()), type);
>>          }
>>
>>          ReferenceCollection(Collection<Referrer<T>>  col,
>>                  ReferenceQueuingFactory<T, Referrer<T>>  rqf, Ref type){
>>              this.col = col;
>>              this.rqf = rqf;
>>              this.type = type;
>>          }
>>
>>          void processQueue(){
>>              rqf.processQueue();
>>              }
>>
>>          ReferenceQueuingFactory<T, Referrer<T>>  getRQF(){
>>              return rqf;
>>          }
>>
>>          Ref getRef(){
>>              return type;
>>          }
>>
>>          Referrer<T>  wrapObj(T t, boolean enqueue, boolean temporary){
>>              return rqf.referenced(t, enqueue, temporary);
>>          }
>>
>>          public int size() {
>>              processQueue();
>>              return col.size();
>>          }
>>
>>          public boolean isEmpty() {
>>              processQueue();
>>              return col.isEmpty();
>>          }
>>
>>          public boolean contains(Object o) {
>>              processQueue();
>>              return col.contains(wrapObj((T) o, false, true));
>>          }
>>
>>          /**
>>           * This Iterator may return null values if garbage collection
>>           * runs during iteration.
>>           *
>>           * Always check for null values.
>>           *
>>           * @return T - possibly null.
>>           */
>>          public Iterator<T>  iterator() {
>>              processQueue();
>>              return new ReferenceIterator<T>(col.iterator());
>>          }
>>
>>          public boolean add(T e) {
>>              processQueue();
>>              return col.add(wrapObj(e, true, false));
>>          }
>>
>>          public boolean remove(Object o) {
>>              processQueue();
>>              return col.remove(wrapObj((T) o, false, true));
>>          }
>>
>>
>>          @SuppressWarnings("unchecked")
>>          public boolean containsAll(Collection<?>  c) {
>>              processQueue();
>>              return col.containsAll(new CollectionWrapper<T>((Collection<T>)
>>     c, getRQF(), false, true));
>>          }
>>
>>
>>          @SuppressWarnings("unchecked")
>>          public boolean addAll(Collection<? extends T>  c) {
>>              processQueue();
>>              return col.addAll(new CollectionWrapper<T>((Collection<T>) c,
>>     getRQF(), true, false));
>>          }
>>
>>          public void clear() {
>>              col.clear();
>>          }
>>
>>          /*
>>           * The next three methods are suitable implementations for
>>     subclasses also.
>>           */
>>          public String toString(){
>>              return col.toString();
>>          }
>>
>>          @Override
>>          public int hashCode() {
>>              if ( col instanceof List || col instanceof Set ){
>>                  return col.hashCode();
>>              }
>>              return System.identityHashCode(this);
>>          }
>>
>>          /**
>>           * Because equals and hashCode are not defined for collections, we
>>           * cannot guarantee consistent behaviour by implementing equals and
>>           * hashCode.  A collection could be a list, set, queue or deque.
>>           * So a List != Queue and a Set != list. therefore equals for
>>     collections is
>>           * not defined.
>>           *
>>           * However since two collections may both also be Lists, while
>>     abstracted
>>           * from the client two lists may still be equal.
>>           * @see Collection#equals(java.lang.Object)
>>           */
>>
>>          @Override
>>          public boolean equals(Object o){
>>              if ( o == this ) return true;
>>              if ( col instanceof List || col instanceof Set ){
>>                  return col.equals(o);
>>              }
>>              return false;
>>          }
>>
>>          final Object writeReplace() throws ObjectStreamException {
>>              try {
>>                  // returns a Builder instead of this class.
>>                  return SerializationOfReferenceCollection.create(getClass(),
>>     col, type );
>>              } catch (InstantiationException ex) {
>>                  throw new WriteAbortedException("Unable to create
>>     serialization proxy", ex);
>>              } catch (IllegalAccessException ex) {
>>                  throw new WriteAbortedException("Unable to create
>>     serialization proxy", ex);
>>              }
>>          }
>>
>>          private void readObject(ObjectInputStream stream)
>>                  throws InvalidObjectException{
>>              throw new InvalidObjectException("Builder required");
>>          }
>>
>>     }
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/8ec4cf9c/attachment-0001.html>

From david.lloyd at redhat.com  Wed Jan  4 12:23:58 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 04 Jan 2012 11:23:58 -0600
Subject: [concurrency-interest] CAS patterns (Re:  Reference Collections)
In-Reply-To: <4F04855B.1050603@oracle.com>
References: <1325678569.21868.167.camel@bluto> <4F047693.6080508@oracle.com>
	<4F047A3E.7050109@redhat.com> <4F04855B.1050603@oracle.com>
Message-ID: <4F048B2E.7000909@redhat.com>

Good to know.  I ask because I commonly use this sort of pattern with 
int/long field updaters:

int old, new;
do {
     old = fieldVal;
     if (old == threshold) { return; }
     new = old + diff;
} while (! updater.compareAndSet(this, old, new));
...act on new value...

which strikes me as being read/compare/read+write as well as subject to 
spurious failure under contention.  Sometimes, however, it is possible 
to do this instead:

int old = updater.getAndAdd(this, diff);
if (old + diff > threshold) {
    updater.getAndAdd(this, -diff);
    return;
}
...act on new value...

which is semantically the same (assuming you have a big enough "cushion" 
that the updated field can't wrap or whatever), but it's hard to say 
whether it's "better".

On 01/04/2012 10:59 AM, Nathan Reynolds wrote:
> Yes. A CAS operation has to load the cache line, invalidate the cache
> line on all other processors, do the comparison and possibly write to
> the cache line. A volatile read only has to load the cache line and
> perform the comparison in another instruction. The volatile read +
> comparison saves time by avoiding the invalidation.
>
> The queue will be empty most of the time. Only right after GC will the
> queue have entries. Those entries will be removed and the queue will
> become empty again. The common case is for the queue to be empty. So, if
> the queue is not empty, then the volatile read + comparison + CAS
> (tryLock) will take longer. However, this longer path will save time for
> the common case.
>
> When the queue is non-empty, the threads will execute tryLock() on each
> access. This means while 1 thread is busy cleaning the queue, the other
> threads will tryLock() several times. A better approach would be to do
> the following. This code does the volatile read and comparison. If the
> queue is not empty, then with 1 CAS instruction the queue is made empty
> and the thread gets the entire contents of the queue. The thread can
> then process the queue and the rest of the threads will see an empty queue.
>
> public Reference<? extends T> pollAll()
> {
> Reference<? extends T> head;
>
> do
> {
> head = this.head;
>
> if (head == null)
> return(null);
> }
> while (!s_atomicHead.compareAndSet(this, head, null));
>
> return(head);
> }
>
> Nathan Reynolds
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
> Consulting Member of Technical Staff | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/4/2012 9:11 AM, David M. Lloyd wrote:
>> Are you saying that a failed CAS has greater impact than a volatile
>> read + external comparison?
>>
>> On 01/04/2012 09:56 AM, Nathan Reynolds wrote:
>>>> Do you think that all calling threads attempting to obtain the
>>>> tryLock(), (once only) will cause any performance issues?
>>>
>>> tryLock() will ideally execute a CAS instruction to acquire the lock.
>>> Under no contention, this means that each access will execute a CAS.
>>> This is going to be a performance problem. It will cost at least 10s of
>>> cycles. If you're lucky, it won't be significant. Under contention, the
>>> CAS instruction will stall the processor pipeline and become a
>>> bottleneck. If the tryLock() is called heavily, the system won't be able
>>> to scale past 3-socket Nehalem system.
>>>
>>> The JDK's ReferenceQueue.poll() simply reads a volatile variable. If the
>>> queue is empty, then the impact will simply be reading the variable's
>>> value from cache or RAM. If the value is in L1 cache, then this will
>>> only cost 3 cycles. If the queue is not empty, then the code acquires a
>>> lock and does the poll operation. You probably want to do likewise
>>> except instead of acquiring the lock, do a tryLock(). Thus, threads
>>> won't block on the clean up.
>>>
>>> Nathan Reynolds
>>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>> Consulting Member of Technical Staff | 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>
>>> On 1/4/2012 5:02 AM, Peter Firmstone wrote:
>>>> Hi,
>>>>
>>>> Over at river.apache.org, we've got a reference collections library,
>>>> utilised for concurrent caches, it's straightforward clean and simple
>>>> code. Easy to use, although Generics are a little verbose. RC is a
>>>> class with a simple public api, containing static methods used to wrap
>>>> any of the java collections interfaces, allowing you to use any
>>>> reference type, to refer to objects contained in your collections.
>>>>
>>>> All implementation is package private, sync / locking strategies are
>>>> left to the underlying collection, removal / cleaning is performed by
>>>> any thread that obtains a lock on a private ReferenceQueue (each
>>>> collection has it's own), which is guarded by a
>>>> ReentrantLock.tryLock(),
>>>> threads that don't obtain the lock, continue without performing any
>>>> cleanup. Do you think that all calling threads attempting to obtain the
>>>> tryLock(), (once only) will cause any performance issues? Garbage
>>>> collection occurs concurrently with other operations.
>>>>
>>>> Reference types available are Weak Identity, Weak, Soft Identity, Soft
>>>> and Strong. It's relatively simple to add additional reference types
>>>> such as a timed reference, however this isn't implemented at present.
>>>>
>>>> Here's a useage example, straight from our SecurityManager code:
>>>>
>>>> <SNIP>
>>>> private final ConcurrentMap<AccessControlContext,
>>>> NavigableSet<Permission>> checked;
>>>> private final Comparator<Referrer<Permission>> permCompare;
>>>> </SNIP>
>>>>
>>>> <SNIP-FROM-CONSTRUCTOR>
>>>> ConcurrentMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>
>>>> refmap
>>>> = new
>>>> ConcurrentHashMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>(100);
>>>>
>>>>
>>>> checked = RC.concurrentMap(refmap, Ref.SOFT, Ref.STRONG);
>>>> permCompare = RC.comparator(new PermissionComparator());
>>>>
>>>> </SNIP-FROM-CONSTRUCTOR>
>>>>
>>>> <SNIP-METHOD>
>>>> @Override
>>>> public void checkPermission(Permission perm, Object context) throws
>>>> SecurityException {
>>>> if (!(context instanceof AccessControlContext)) throw new
>>>> SecurityException();
>>>> if (perm == null ) throw new NullPointerException("Permission
>>>> Collection null");
>>>> /* The next line speeds up permission checks related to this
>>>> SecurityManager. */
>>>> if ( SMPrivilegedContext.equals(context) ||
>>>> SMConstructorContext.equals(context)) return; // prevents endless loop
>>>> in debug.
>>>> AccessControlContext executionContext = (AccessControlContext)
>>>> context;
>>>> // Checks if Permission has already been checked for this
>>>> context.
>>>> NavigableSet<Permission> checkedPerms =
>>>> checked.get(executionContext);
>>>> if (checkedPerms == null){
>>>> /* A ConcurrentSkipListSet is used to avoid blocking during
>>>> * removal operations that occur while the garbage collector
>>>> * recovers softly reachable memory. Since this happens
>>>> while
>>>> * the jvm's under stress, it's important that permission
>>>> checks
>>>> * continue to perform well.
>>>> *
>>>> * Although I considered a multi read, single write Set, I
>>>> wanted
>>>> * to avoid blocking under stress, caused as a result
>>>> * of garbage collection.
>>>> *
>>>> * The Reference Collection that encapsulates the
>>>> ConcurrentSkipListSet
>>>> * uses a ReentrantLock.tryLock() guard the ReferenceQueue
>>>> used
>>>> * to remove objects from the Set. This allows other
>>>> threads to
>>>> * proceed during object removal. Only one thread is given
>>>> access
>>>> * to the ReferenceQueue, the unlucky caller thread performs
>>>> garbage
>>>> * removal from the Set before accessing the Set for its
>>>> original
>>>> * purpose.
>>>> */
>>>> NavigableSet<Referrer<Permission>> internal =
>>>> new
>>>> ConcurrentSkipListSet<Referrer<Permission>>(permCompare);
>>>> checkedPerms = RC.navigableSet(internal, Ref.SOFT);
>>>> NavigableSet<Permission> existed =
>>>> checked.putIfAbsent(executionContext, checkedPerms);
>>>> if (existed != null) checkedPerms = existed;
>>>> }
>>>> if (checkedPerms.contains(perm)) return; // don't need to check
>>>> again.
>>>>
>>>> </SNIP-METHOD>
>>>>
>>>> Serialization is implemented so implementations can be replaced and
>>>> upgraded, serial form is a separate concern; see the Serialization
>>>> Builder pattern for details:http://wiki.apache.org/river/Serialization
>>>>
>>>> This a copy of some implementation code:
>>>>
>>>> /*
>>>> * Licensed to the Apache Software Foundation (ASF) under one
>>>> * or more contributor license agreements. See the NOTICE file
>>>> * distributed with this work for additional information
>>>> * regarding copyright ownership. The ASF licenses this file
>>>> * to you under the Apache License, Version 2.0 (the
>>>> * "License"); you may not use this file except in compliance
>>>> * with the License. You may obtain a copy of the License at
>>>> *
>>>> *http://www.apache.org/licenses/LICENSE-2.0
>>>> *
>>>> * Unless required by applicable law or agreed to in writing, software
>>>> * distributed under the License is distributed on an "AS IS" BASIS,
>>>> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>>>> implied.
>>>> * See the License for the specific language governing permissions and
>>>> * limitations under the License.
>>>> */
>>>>
>>>> package org.apache.river.impl.util;
>>>>
>>>>
>>>> import java.lang.ref.Reference;
>>>> import java.util.Random;
>>>> import java.util.concurrent.ConcurrentMap;
>>>> import java.util.logging.Level;
>>>> import java.util.logging.Logger;
>>>>
>>>> /**
>>>> * A referenced hash map, that encapsulates and utilises any
>>>> ConcurrentMap
>>>> * implementation passed in at construction.
>>>> *
>>>> * Based on any ConcurrentMap implementation, it doesn't accept null
>>>> keys or values.
>>>> *
>>>> * It is recommended although not mandatory to use identity based
>>>> References for keys,
>>>> * unexpected results occur when relying on equal keys, if one key is no
>>>> longer
>>>> * strongly reachable and has been garbage collected and removed from
>>>> the
>>>> * Map.
>>>> *
>>>> *
>>>> *
>>>> * If either a key or value, is no longer strongly reachable, their
>>>> mapping
>>>> * will be queued for removal and garbage collection, in compliance with
>>>> * the Reference implementation selected.
>>>> *
>>>> * @param<K>
>>>> * @param<V>
>>>> * @see Ref
>>>> * @author Peter Firmstone.
>>>> *
>>>> * @since 2.3
>>>> */
>>>> class ReferenceConcurrentMap<K, V> extends ReferenceMap<K, V>
>>>> implements
>>>> ConcurrentMap<K, V> {
>>>>
>>>> // ConcurrentMap must be protected from null values? It changes
>>>> it's behaviour, is that a problem?
>>>> private final ConcurrentMap<Referrer<K>, Referrer<V>> map;
>>>>
>>>> ReferenceConcurrentMap(ConcurrentMap<Referrer<K>,Referrer<V>> map,
>>>> Ref key, Ref val){
>>>> super (map, key, val);
>>>> this.map = map;
>>>> }
>>>>
>>>> ReferenceConcurrentMap(ConcurrentMap<Referrer<K>, Referrer<V>> map,
>>>> ReferenceQueuingFactory<K, Referrer<K>> krqf,
>>>> ReferenceQueuingFactory<V, Referrer<V>> vrqf, Ref key, Ref val){
>>>> super(map, krqf, vrqf, key, val);
>>>> this.map = map;
>>>> }
>>>>
>>>> public V putIfAbsent(K key, V value) {
>>>> processQueue(); //may be a slight delay before atomic
>>>> putIfAbsent
>>>> Referrer<K> k = wrapKey(key, true, false);
>>>> Referrer<V> v = wrapVal(value, true, false);
>>>> Referrer<V> val = map.putIfAbsent(k, v);
>>>> while ( val != null ) {
>>>> V existed = val.get();
>>>> // We hold a strong reference to value, so
>>>> if ( existed == null ){
>>>> // stale reference must be replaced, it has been garbage
>>>> collect but hasn't
>>>> // been removed, we must treat it like the entry doesn't
>>>> exist.
>>>> if ( map.replace(k, val, v)){
>>>> // replace successful
>>>> return null; // Because officially there was no
>>>> record.
>>>> } else {
>>>> // Another thread may have replaced it.
>>>> val = map.putIfAbsent(k, v);
>>>> }
>>>> } else {
>>>> return existed;
>>>> }
>>>> }
>>>> return null;
>>>> }
>>>>
>>>> @SuppressWarnings("unchecked")
>>>> public boolean remove(Object key, Object value) {
>>>> processQueue();
>>>> return map.remove(wrapKey((K) key, false, true), wrapVal((V)
>>>> value, false, true));
>>>> }
>>>>
>>>> public boolean replace(K key, V oldValue, V newValue) {
>>>> processQueue();
>>>> return map.replace(wrapKey(key, false, true), wrapVal(oldValue,
>>>> false, true), wrapVal(newValue, true, false));
>>>> }
>>>>
>>>> public V replace(K key, V value) {
>>>> processQueue();
>>>> Referrer<V> val = map.replace(wrapKey(key, false, true),
>>>> wrapVal(value, true, false));
>>>> if ( val != null ) return val.get();
>>>> return null;
>>>> }
>>>> }
>>>>
>>>> /*
>>>> * Licensed to the Apache Software Foundation (ASF) under one
>>>> * or more contributor license agreements. See the NOTICE file
>>>> * distributed with this work for additional information
>>>> * regarding copyright ownership. The ASF licenses this file
>>>> * to you under the Apache License, Version 2.0 (the
>>>> * "License"); you may not use this file except in compliance
>>>> * with the License. You may obtain a copy of the License at
>>>> *
>>>> *http://www.apache.org/licenses/LICENSE-2.0
>>>> *
>>>> * Unless required by applicable law or agreed to in writing, software
>>>> * distributed under the License is distributed on an "AS IS" BASIS,
>>>> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>>>> implied.
>>>> * See the License for the specific language governing permissions and
>>>> * limitations under the License.
>>>> */
>>>>
>>>> package org.apache.river.impl.util;
>>>>
>>>> import java.io.InvalidObjectException;
>>>> import java.io.ObjectInputStream;
>>>> import java.io.ObjectStreamException;
>>>> import java.io.Serializable;
>>>> import java.io.WriteAbortedException;
>>>> import java.lang.ref.Reference;
>>>> import java.lang.ref.ReferenceQueue;
>>>> import java.util.AbstractCollection;
>>>> import java.util.ArrayList;
>>>> import java.util.Collection;
>>>> import java.util.Iterator;
>>>> import java.util.List;
>>>> import java.util.Set;
>>>> import java.util.logging.Level;
>>>> import java.util.logging.Logger;
>>>>
>>>> /**
>>>> * A Collection of Reference Objects, the developer may chose any
>>>> Collection
>>>> * implementation to store the References, which is passed in a runtime.
>>>> *
>>>> * The underlying Collection implementation governs the specific
>>>> behaviour of this
>>>> * Collection.
>>>> *
>>>> * Synchronisation must be implemented by the underlying Collection and
>>>> cannot
>>>> * be performed externally to this class. The underlying Collection
>>>> must
>>>> * also be mutable. Objects will be removed automatically from the
>>>> underlying
>>>> * Collection when they are eligible for garbage collection.
>>>> *
>>>> * Weak, Weak Identity, Soft, Soft Identity or Strong references may be
>>>> used.
>>>> * This Collection may be used as an Object pool cache or any other
>>>> purpose
>>>> * that requires unique memory handling.
>>>> *
>>>> * For concurrent threads, it is recommended to encapsulate the
>>>> underlying
>>>> * collection in a multi read, single write collection for scalability.
>>>> *
>>>> * @see Ref
>>>> * @see ConcurrentCollections#multiReadCollection(java.util.Collection)
>>>> * @author Peter Firmstone.
>>>> */
>>>> class ReferenceCollection<T> extends AbstractCollection<T>
>>>> implements Collection<T>, Serializable {
>>>> private static final long serialVersionUID = 1L;
>>>> private final Collection<Referrer<T>> col;
>>>> private final ReferenceQueuingFactory<T, Referrer<T>> rqf;
>>>> private final Ref type;
>>>>
>>>> ReferenceCollection(Collection<Referrer<T>> col, Ref type){
>>>> this(col, new ReferenceProcessor<T>(col, type, type ==
>>>> Ref.STRONG ? null : new ReferenceQueue<T>()), type);
>>>> }
>>>>
>>>> ReferenceCollection(Collection<Referrer<T>> col,
>>>> ReferenceQueuingFactory<T, Referrer<T>> rqf, Ref type){
>>>> this.col = col;
>>>> this.rqf = rqf;
>>>> this.type = type;
>>>> }
>>>>
>>>> void processQueue(){
>>>> rqf.processQueue();
>>>> }
>>>>
>>>> ReferenceQueuingFactory<T, Referrer<T>> getRQF(){
>>>> return rqf;
>>>> }
>>>>
>>>> Ref getRef(){
>>>> return type;
>>>> }
>>>>
>>>> Referrer<T> wrapObj(T t, boolean enqueue, boolean temporary){
>>>> return rqf.referenced(t, enqueue, temporary);
>>>> }
>>>>
>>>> public int size() {
>>>> processQueue();
>>>> return col.size();
>>>> }
>>>>
>>>> public boolean isEmpty() {
>>>> processQueue();
>>>> return col.isEmpty();
>>>> }
>>>>
>>>> public boolean contains(Object o) {
>>>> processQueue();
>>>> return col.contains(wrapObj((T) o, false, true));
>>>> }
>>>>
>>>> /**
>>>> * This Iterator may return null values if garbage collection
>>>> * runs during iteration.
>>>> *
>>>> * Always check for null values.
>>>> *
>>>> * @return T - possibly null.
>>>> */
>>>> public Iterator<T> iterator() {
>>>> processQueue();
>>>> return new ReferenceIterator<T>(col.iterator());
>>>> }
>>>>
>>>> public boolean add(T e) {
>>>> processQueue();
>>>> return col.add(wrapObj(e, true, false));
>>>> }
>>>>
>>>> public boolean remove(Object o) {
>>>> processQueue();
>>>> return col.remove(wrapObj((T) o, false, true));
>>>> }
>>>>
>>>>
>>>> @SuppressWarnings("unchecked")
>>>> public boolean containsAll(Collection<?> c) {
>>>> processQueue();
>>>> return col.containsAll(new CollectionWrapper<T>((Collection<T>)
>>>> c, getRQF(), false, true));
>>>> }
>>>>
>>>>
>>>> @SuppressWarnings("unchecked")
>>>> public boolean addAll(Collection<? extends T> c) {
>>>> processQueue();
>>>> return col.addAll(new CollectionWrapper<T>((Collection<T>) c,
>>>> getRQF(), true, false));
>>>> }
>>>>
>>>> public void clear() {
>>>> col.clear();
>>>> }
>>>>
>>>> /*
>>>> * The next three methods are suitable implementations for
>>>> subclasses also.
>>>> */
>>>> public String toString(){
>>>> return col.toString();
>>>> }
>>>>
>>>> @Override
>>>> public int hashCode() {
>>>> if ( col instanceof List || col instanceof Set ){
>>>> return col.hashCode();
>>>> }
>>>> return System.identityHashCode(this);
>>>> }
>>>>
>>>> /**
>>>> * Because equals and hashCode are not defined for collections, we
>>>> * cannot guarantee consistent behaviour by implementing equals and
>>>> * hashCode. A collection could be a list, set, queue or deque.
>>>> * So a List != Queue and a Set != list. therefore equals for
>>>> collections is
>>>> * not defined.
>>>> *
>>>> * However since two collections may both also be Lists, while
>>>> abstracted
>>>> * from the client two lists may still be equal.
>>>> * @see Collection#equals(java.lang.Object)
>>>> */
>>>>
>>>> @Override
>>>> public boolean equals(Object o){
>>>> if ( o == this ) return true;
>>>> if ( col instanceof List || col instanceof Set ){
>>>> return col.equals(o);
>>>> }
>>>> return false;
>>>> }
>>>>
>>>> final Object writeReplace() throws ObjectStreamException {
>>>> try {
>>>> // returns a Builder instead of this class.
>>>> return SerializationOfReferenceCollection.create(getClass(),
>>>> col, type );
>>>> } catch (InstantiationException ex) {
>>>> throw new WriteAbortedException("Unable to create
>>>> serialization proxy", ex);
>>>> } catch (IllegalAccessException ex) {
>>>> throw new WriteAbortedException("Unable to create
>>>> serialization proxy", ex);
>>>> }
>>>> }
>>>>
>>>> private void readObject(ObjectInputStream stream)
>>>> throws InvalidObjectException{
>>>> throw new InvalidObjectException("Builder required");
>>>> }
>>>>
>>>> }
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>


-- 
- DML

From nathan.reynolds at oracle.com  Wed Jan  4 12:56:00 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 04 Jan 2012 10:56:00 -0700
Subject: [concurrency-interest] CAS patterns (Re: Reference Collections)
In-Reply-To: <4F048B2E.7000909@redhat.com>
References: <1325678569.21868.167.camel@bluto> <4F047693.6080508@oracle.com>
	<4F047A3E.7050109@redhat.com> <4F04855B.1050603@oracle.com>
	<4F048B2E.7000909@redhat.com>
Message-ID: <4F0492B0.7020405@oracle.com>

Dave Dice has a blog entry on this.  One of the problems with the CAS 
loop is that it suffers from throughput problems.  The cache line is 
loaded and set into a shared state.  The CAS then has to invalidate the 
cache line on all the other processors.  I've pointed this out to Intel 
architects.  I don't know if they will do anything about it.  It would 
be nice if the processor could detect these kinds of loops and simply 
pass the cache line in the exclusive state from core to core.  This will 
improve throughput and reduce coherency traffic.  Another problem is 
that if there is significant contention, the branch predictor will 
predict to repeat the loop.  The thread that succeeds will have to clear 
its pipeline and stall.  If the code after the compareAndSet is the 
inside of a critical region, then the critical region is now several 
cycles longer.

The problem with the getAndAdd code is that it always takes a 
scalability hit from the getAndAdd() operation.  If the updater is at 
the threshold, then it will do 2 getAndAdd() operations making this code 
even more expensive.  Another problem with this code is what happens if 
a bunch of threads are between getAndAdd().  If updater is decreased 
because resources are available, then threads might not ever see the 
available resources.

I guess neither code is a clear winner.  It really depends upon how 
frequently the atomic instruction must be executed.  If the code is 
always operating at the threshold, then the CAS loop will be a winner.  
If the code is never operating near the threshold, then the getAndAdd() 
code will be the winner.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/4/2012 10:23 AM, David M. Lloyd wrote:
> Good to know.  I ask because I commonly use this sort of pattern with 
> int/long field updaters:
>
> int old, new;
> do {
>     old = fieldVal;
>     if (old == threshold) { return; }
>     new = old + diff;
> } while (! updater.compareAndSet(this, old, new));
> ...act on new value...
>
> which strikes me as being read/compare/read+write as well as subject 
> to spurious failure under contention.  Sometimes, however, it is 
> possible to do this instead:
>
> int old = updater.getAndAdd(this, diff);
> if (old + diff > threshold) {
>    updater.getAndAdd(this, -diff);
>    return;
> }
> ...act on new value...
>
> which is semantically the same (assuming you have a big enough 
> "cushion" that the updated field can't wrap or whatever), but it's 
> hard to say whether it's "better".
>
> On 01/04/2012 10:59 AM, Nathan Reynolds wrote:
>> Yes. A CAS operation has to load the cache line, invalidate the cache
>> line on all other processors, do the comparison and possibly write to
>> the cache line. A volatile read only has to load the cache line and
>> perform the comparison in another instruction. The volatile read +
>> comparison saves time by avoiding the invalidation.
>>
>> The queue will be empty most of the time. Only right after GC will the
>> queue have entries. Those entries will be removed and the queue will
>> become empty again. The common case is for the queue to be empty. So, if
>> the queue is not empty, then the volatile read + comparison + CAS
>> (tryLock) will take longer. However, this longer path will save time for
>> the common case.
>>
>> When the queue is non-empty, the threads will execute tryLock() on each
>> access. This means while 1 thread is busy cleaning the queue, the other
>> threads will tryLock() several times. A better approach would be to do
>> the following. This code does the volatile read and comparison. If the
>> queue is not empty, then with 1 CAS instruction the queue is made empty
>> and the thread gets the entire contents of the queue. The thread can
>> then process the queue and the rest of the threads will see an empty 
>> queue.
>>
>> public Reference<? extends T> pollAll()
>> {
>> Reference<? extends T> head;
>>
>> do
>> {
>> head = this.head;
>>
>> if (head == null)
>> return(null);
>> }
>> while (!s_atomicHead.compareAndSet(this, head, null));
>>
>> return(head);
>> }
>>
>> Nathan Reynolds
>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>> Consulting Member of Technical Staff | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>> On 1/4/2012 9:11 AM, David M. Lloyd wrote:
>>> Are you saying that a failed CAS has greater impact than a volatile
>>> read + external comparison?
>>>
>>> On 01/04/2012 09:56 AM, Nathan Reynolds wrote:
>>>>> Do you think that all calling threads attempting to obtain the
>>>>> tryLock(), (once only) will cause any performance issues?
>>>>
>>>> tryLock() will ideally execute a CAS instruction to acquire the lock.
>>>> Under no contention, this means that each access will execute a CAS.
>>>> This is going to be a performance problem. It will cost at least 
>>>> 10s of
>>>> cycles. If you're lucky, it won't be significant. Under contention, 
>>>> the
>>>> CAS instruction will stall the processor pipeline and become a
>>>> bottleneck. If the tryLock() is called heavily, the system won't be 
>>>> able
>>>> to scale past 3-socket Nehalem system.
>>>>
>>>> The JDK's ReferenceQueue.poll() simply reads a volatile variable. 
>>>> If the
>>>> queue is empty, then the impact will simply be reading the variable's
>>>> value from cache or RAM. If the value is in L1 cache, then this will
>>>> only cost 3 cycles. If the queue is not empty, then the code 
>>>> acquires a
>>>> lock and does the poll operation. You probably want to do likewise
>>>> except instead of acquiring the lock, do a tryLock(). Thus, threads
>>>> won't block on the clean up.
>>>>
>>>> Nathan Reynolds
>>>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>>> Consulting Member of Technical Staff | 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>
>>>> On 1/4/2012 5:02 AM, Peter Firmstone wrote:
>>>>> Hi,
>>>>>
>>>>> Over at river.apache.org, we've got a reference collections library,
>>>>> utilised for concurrent caches, it's straightforward clean and simple
>>>>> code. Easy to use, although Generics are a little verbose. RC is a
>>>>> class with a simple public api, containing static methods used to 
>>>>> wrap
>>>>> any of the java collections interfaces, allowing you to use any
>>>>> reference type, to refer to objects contained in your collections.
>>>>>
>>>>> All implementation is package private, sync / locking strategies are
>>>>> left to the underlying collection, removal / cleaning is performed by
>>>>> any thread that obtains a lock on a private ReferenceQueue (each
>>>>> collection has it's own), which is guarded by a
>>>>> ReentrantLock.tryLock(),
>>>>> threads that don't obtain the lock, continue without performing any
>>>>> cleanup. Do you think that all calling threads attempting to 
>>>>> obtain the
>>>>> tryLock(), (once only) will cause any performance issues? Garbage
>>>>> collection occurs concurrently with other operations.
>>>>>
>>>>> Reference types available are Weak Identity, Weak, Soft Identity, 
>>>>> Soft
>>>>> and Strong. It's relatively simple to add additional reference types
>>>>> such as a timed reference, however this isn't implemented at present.
>>>>>
>>>>> Here's a useage example, straight from our SecurityManager code:
>>>>>
>>>>> <SNIP>
>>>>> private final ConcurrentMap<AccessControlContext,
>>>>> NavigableSet<Permission>> checked;
>>>>> private final Comparator<Referrer<Permission>> permCompare;
>>>>> </SNIP>
>>>>>
>>>>> <SNIP-FROM-CONSTRUCTOR>
>>>>> ConcurrentMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>> 
>>>>>
>>>>> refmap
>>>>> = new
>>>>> ConcurrentHashMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>(100); 
>>>>>
>>>>>
>>>>>
>>>>> checked = RC.concurrentMap(refmap, Ref.SOFT, Ref.STRONG);
>>>>> permCompare = RC.comparator(new PermissionComparator());
>>>>>
>>>>> </SNIP-FROM-CONSTRUCTOR>
>>>>>
>>>>> <SNIP-METHOD>
>>>>> @Override
>>>>> public void checkPermission(Permission perm, Object context) throws
>>>>> SecurityException {
>>>>> if (!(context instanceof AccessControlContext)) throw new
>>>>> SecurityException();
>>>>> if (perm == null ) throw new NullPointerException("Permission
>>>>> Collection null");
>>>>> /* The next line speeds up permission checks related to this
>>>>> SecurityManager. */
>>>>> if ( SMPrivilegedContext.equals(context) ||
>>>>> SMConstructorContext.equals(context)) return; // prevents endless 
>>>>> loop
>>>>> in debug.
>>>>> AccessControlContext executionContext = (AccessControlContext)
>>>>> context;
>>>>> // Checks if Permission has already been checked for this
>>>>> context.
>>>>> NavigableSet<Permission> checkedPerms =
>>>>> checked.get(executionContext);
>>>>> if (checkedPerms == null){
>>>>> /* A ConcurrentSkipListSet is used to avoid blocking during
>>>>> * removal operations that occur while the garbage collector
>>>>> * recovers softly reachable memory. Since this happens
>>>>> while
>>>>> * the jvm's under stress, it's important that permission
>>>>> checks
>>>>> * continue to perform well.
>>>>> *
>>>>> * Although I considered a multi read, single write Set, I
>>>>> wanted
>>>>> * to avoid blocking under stress, caused as a result
>>>>> * of garbage collection.
>>>>> *
>>>>> * The Reference Collection that encapsulates the
>>>>> ConcurrentSkipListSet
>>>>> * uses a ReentrantLock.tryLock() guard the ReferenceQueue
>>>>> used
>>>>> * to remove objects from the Set. This allows other
>>>>> threads to
>>>>> * proceed during object removal. Only one thread is given
>>>>> access
>>>>> * to the ReferenceQueue, the unlucky caller thread performs
>>>>> garbage
>>>>> * removal from the Set before accessing the Set for its
>>>>> original
>>>>> * purpose.
>>>>> */
>>>>> NavigableSet<Referrer<Permission>> internal =
>>>>> new
>>>>> ConcurrentSkipListSet<Referrer<Permission>>(permCompare);
>>>>> checkedPerms = RC.navigableSet(internal, Ref.SOFT);
>>>>> NavigableSet<Permission> existed =
>>>>> checked.putIfAbsent(executionContext, checkedPerms);
>>>>> if (existed != null) checkedPerms = existed;
>>>>> }
>>>>> if (checkedPerms.contains(perm)) return; // don't need to check
>>>>> again.
>>>>>
>>>>> </SNIP-METHOD>
>>>>>
>>>>> Serialization is implemented so implementations can be replaced and
>>>>> upgraded, serial form is a separate concern; see the Serialization
>>>>> Builder pattern for 
>>>>> details:http://wiki.apache.org/river/Serialization
>>>>>
>>>>> This a copy of some implementation code:
>>>>>
>>>>> /*
>>>>> * Licensed to the Apache Software Foundation (ASF) under one
>>>>> * or more contributor license agreements. See the NOTICE file
>>>>> * distributed with this work for additional information
>>>>> * regarding copyright ownership. The ASF licenses this file
>>>>> * to you under the Apache License, Version 2.0 (the
>>>>> * "License"); you may not use this file except in compliance
>>>>> * with the License. You may obtain a copy of the License at
>>>>> *
>>>>> *http://www.apache.org/licenses/LICENSE-2.0
>>>>> *
>>>>> * Unless required by applicable law or agreed to in writing, software
>>>>> * distributed under the License is distributed on an "AS IS" BASIS,
>>>>> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>>>>> implied.
>>>>> * See the License for the specific language governing permissions and
>>>>> * limitations under the License.
>>>>> */
>>>>>
>>>>> package org.apache.river.impl.util;
>>>>>
>>>>>
>>>>> import java.lang.ref.Reference;
>>>>> import java.util.Random;
>>>>> import java.util.concurrent.ConcurrentMap;
>>>>> import java.util.logging.Level;
>>>>> import java.util.logging.Logger;
>>>>>
>>>>> /**
>>>>> * A referenced hash map, that encapsulates and utilises any
>>>>> ConcurrentMap
>>>>> * implementation passed in at construction.
>>>>> *
>>>>> * Based on any ConcurrentMap implementation, it doesn't accept null
>>>>> keys or values.
>>>>> *
>>>>> * It is recommended although not mandatory to use identity based
>>>>> References for keys,
>>>>> * unexpected results occur when relying on equal keys, if one key 
>>>>> is no
>>>>> longer
>>>>> * strongly reachable and has been garbage collected and removed from
>>>>> the
>>>>> * Map.
>>>>> *
>>>>> *
>>>>> *
>>>>> * If either a key or value, is no longer strongly reachable, their
>>>>> mapping
>>>>> * will be queued for removal and garbage collection, in compliance 
>>>>> with
>>>>> * the Reference implementation selected.
>>>>> *
>>>>> * @param<K>
>>>>> * @param<V>
>>>>> * @see Ref
>>>>> * @author Peter Firmstone.
>>>>> *
>>>>> * @since 2.3
>>>>> */
>>>>> class ReferenceConcurrentMap<K, V> extends ReferenceMap<K, V>
>>>>> implements
>>>>> ConcurrentMap<K, V> {
>>>>>
>>>>> // ConcurrentMap must be protected from null values? It changes
>>>>> it's behaviour, is that a problem?
>>>>> private final ConcurrentMap<Referrer<K>, Referrer<V>> map;
>>>>>
>>>>> ReferenceConcurrentMap(ConcurrentMap<Referrer<K>,Referrer<V>> map,
>>>>> Ref key, Ref val){
>>>>> super (map, key, val);
>>>>> this.map = map;
>>>>> }
>>>>>
>>>>> ReferenceConcurrentMap(ConcurrentMap<Referrer<K>, Referrer<V>> map,
>>>>> ReferenceQueuingFactory<K, Referrer<K>> krqf,
>>>>> ReferenceQueuingFactory<V, Referrer<V>> vrqf, Ref key, Ref val){
>>>>> super(map, krqf, vrqf, key, val);
>>>>> this.map = map;
>>>>> }
>>>>>
>>>>> public V putIfAbsent(K key, V value) {
>>>>> processQueue(); //may be a slight delay before atomic
>>>>> putIfAbsent
>>>>> Referrer<K> k = wrapKey(key, true, false);
>>>>> Referrer<V> v = wrapVal(value, true, false);
>>>>> Referrer<V> val = map.putIfAbsent(k, v);
>>>>> while ( val != null ) {
>>>>> V existed = val.get();
>>>>> // We hold a strong reference to value, so
>>>>> if ( existed == null ){
>>>>> // stale reference must be replaced, it has been garbage
>>>>> collect but hasn't
>>>>> // been removed, we must treat it like the entry doesn't
>>>>> exist.
>>>>> if ( map.replace(k, val, v)){
>>>>> // replace successful
>>>>> return null; // Because officially there was no
>>>>> record.
>>>>> } else {
>>>>> // Another thread may have replaced it.
>>>>> val = map.putIfAbsent(k, v);
>>>>> }
>>>>> } else {
>>>>> return existed;
>>>>> }
>>>>> }
>>>>> return null;
>>>>> }
>>>>>
>>>>> @SuppressWarnings("unchecked")
>>>>> public boolean remove(Object key, Object value) {
>>>>> processQueue();
>>>>> return map.remove(wrapKey((K) key, false, true), wrapVal((V)
>>>>> value, false, true));
>>>>> }
>>>>>
>>>>> public boolean replace(K key, V oldValue, V newValue) {
>>>>> processQueue();
>>>>> return map.replace(wrapKey(key, false, true), wrapVal(oldValue,
>>>>> false, true), wrapVal(newValue, true, false));
>>>>> }
>>>>>
>>>>> public V replace(K key, V value) {
>>>>> processQueue();
>>>>> Referrer<V> val = map.replace(wrapKey(key, false, true),
>>>>> wrapVal(value, true, false));
>>>>> if ( val != null ) return val.get();
>>>>> return null;
>>>>> }
>>>>> }
>>>>>
>>>>> /*
>>>>> * Licensed to the Apache Software Foundation (ASF) under one
>>>>> * or more contributor license agreements. See the NOTICE file
>>>>> * distributed with this work for additional information
>>>>> * regarding copyright ownership. The ASF licenses this file
>>>>> * to you under the Apache License, Version 2.0 (the
>>>>> * "License"); you may not use this file except in compliance
>>>>> * with the License. You may obtain a copy of the License at
>>>>> *
>>>>> *http://www.apache.org/licenses/LICENSE-2.0
>>>>> *
>>>>> * Unless required by applicable law or agreed to in writing, software
>>>>> * distributed under the License is distributed on an "AS IS" BASIS,
>>>>> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>>>>> implied.
>>>>> * See the License for the specific language governing permissions and
>>>>> * limitations under the License.
>>>>> */
>>>>>
>>>>> package org.apache.river.impl.util;
>>>>>
>>>>> import java.io.InvalidObjectException;
>>>>> import java.io.ObjectInputStream;
>>>>> import java.io.ObjectStreamException;
>>>>> import java.io.Serializable;
>>>>> import java.io.WriteAbortedException;
>>>>> import java.lang.ref.Reference;
>>>>> import java.lang.ref.ReferenceQueue;
>>>>> import java.util.AbstractCollection;
>>>>> import java.util.ArrayList;
>>>>> import java.util.Collection;
>>>>> import java.util.Iterator;
>>>>> import java.util.List;
>>>>> import java.util.Set;
>>>>> import java.util.logging.Level;
>>>>> import java.util.logging.Logger;
>>>>>
>>>>> /**
>>>>> * A Collection of Reference Objects, the developer may chose any
>>>>> Collection
>>>>> * implementation to store the References, which is passed in a 
>>>>> runtime.
>>>>> *
>>>>> * The underlying Collection implementation governs the specific
>>>>> behaviour of this
>>>>> * Collection.
>>>>> *
>>>>> * Synchronisation must be implemented by the underlying Collection 
>>>>> and
>>>>> cannot
>>>>> * be performed externally to this class. The underlying Collection
>>>>> must
>>>>> * also be mutable. Objects will be removed automatically from the
>>>>> underlying
>>>>> * Collection when they are eligible for garbage collection.
>>>>> *
>>>>> * Weak, Weak Identity, Soft, Soft Identity or Strong references 
>>>>> may be
>>>>> used.
>>>>> * This Collection may be used as an Object pool cache or any other
>>>>> purpose
>>>>> * that requires unique memory handling.
>>>>> *
>>>>> * For concurrent threads, it is recommended to encapsulate the
>>>>> underlying
>>>>> * collection in a multi read, single write collection for 
>>>>> scalability.
>>>>> *
>>>>> * @see Ref
>>>>> * @see 
>>>>> ConcurrentCollections#multiReadCollection(java.util.Collection)
>>>>> * @author Peter Firmstone.
>>>>> */
>>>>> class ReferenceCollection<T> extends AbstractCollection<T>
>>>>> implements Collection<T>, Serializable {
>>>>> private static final long serialVersionUID = 1L;
>>>>> private final Collection<Referrer<T>> col;
>>>>> private final ReferenceQueuingFactory<T, Referrer<T>> rqf;
>>>>> private final Ref type;
>>>>>
>>>>> ReferenceCollection(Collection<Referrer<T>> col, Ref type){
>>>>> this(col, new ReferenceProcessor<T>(col, type, type ==
>>>>> Ref.STRONG ? null : new ReferenceQueue<T>()), type);
>>>>> }
>>>>>
>>>>> ReferenceCollection(Collection<Referrer<T>> col,
>>>>> ReferenceQueuingFactory<T, Referrer<T>> rqf, Ref type){
>>>>> this.col = col;
>>>>> this.rqf = rqf;
>>>>> this.type = type;
>>>>> }
>>>>>
>>>>> void processQueue(){
>>>>> rqf.processQueue();
>>>>> }
>>>>>
>>>>> ReferenceQueuingFactory<T, Referrer<T>> getRQF(){
>>>>> return rqf;
>>>>> }
>>>>>
>>>>> Ref getRef(){
>>>>> return type;
>>>>> }
>>>>>
>>>>> Referrer<T> wrapObj(T t, boolean enqueue, boolean temporary){
>>>>> return rqf.referenced(t, enqueue, temporary);
>>>>> }
>>>>>
>>>>> public int size() {
>>>>> processQueue();
>>>>> return col.size();
>>>>> }
>>>>>
>>>>> public boolean isEmpty() {
>>>>> processQueue();
>>>>> return col.isEmpty();
>>>>> }
>>>>>
>>>>> public boolean contains(Object o) {
>>>>> processQueue();
>>>>> return col.contains(wrapObj((T) o, false, true));
>>>>> }
>>>>>
>>>>> /**
>>>>> * This Iterator may return null values if garbage collection
>>>>> * runs during iteration.
>>>>> *
>>>>> * Always check for null values.
>>>>> *
>>>>> * @return T - possibly null.
>>>>> */
>>>>> public Iterator<T> iterator() {
>>>>> processQueue();
>>>>> return new ReferenceIterator<T>(col.iterator());
>>>>> }
>>>>>
>>>>> public boolean add(T e) {
>>>>> processQueue();
>>>>> return col.add(wrapObj(e, true, false));
>>>>> }
>>>>>
>>>>> public boolean remove(Object o) {
>>>>> processQueue();
>>>>> return col.remove(wrapObj((T) o, false, true));
>>>>> }
>>>>>
>>>>>
>>>>> @SuppressWarnings("unchecked")
>>>>> public boolean containsAll(Collection<?> c) {
>>>>> processQueue();
>>>>> return col.containsAll(new CollectionWrapper<T>((Collection<T>)
>>>>> c, getRQF(), false, true));
>>>>> }
>>>>>
>>>>>
>>>>> @SuppressWarnings("unchecked")
>>>>> public boolean addAll(Collection<? extends T> c) {
>>>>> processQueue();
>>>>> return col.addAll(new CollectionWrapper<T>((Collection<T>) c,
>>>>> getRQF(), true, false));
>>>>> }
>>>>>
>>>>> public void clear() {
>>>>> col.clear();
>>>>> }
>>>>>
>>>>> /*
>>>>> * The next three methods are suitable implementations for
>>>>> subclasses also.
>>>>> */
>>>>> public String toString(){
>>>>> return col.toString();
>>>>> }
>>>>>
>>>>> @Override
>>>>> public int hashCode() {
>>>>> if ( col instanceof List || col instanceof Set ){
>>>>> return col.hashCode();
>>>>> }
>>>>> return System.identityHashCode(this);
>>>>> }
>>>>>
>>>>> /**
>>>>> * Because equals and hashCode are not defined for collections, we
>>>>> * cannot guarantee consistent behaviour by implementing equals and
>>>>> * hashCode. A collection could be a list, set, queue or deque.
>>>>> * So a List != Queue and a Set != list. therefore equals for
>>>>> collections is
>>>>> * not defined.
>>>>> *
>>>>> * However since two collections may both also be Lists, while
>>>>> abstracted
>>>>> * from the client two lists may still be equal.
>>>>> * @see Collection#equals(java.lang.Object)
>>>>> */
>>>>>
>>>>> @Override
>>>>> public boolean equals(Object o){
>>>>> if ( o == this ) return true;
>>>>> if ( col instanceof List || col instanceof Set ){
>>>>> return col.equals(o);
>>>>> }
>>>>> return false;
>>>>> }
>>>>>
>>>>> final Object writeReplace() throws ObjectStreamException {
>>>>> try {
>>>>> // returns a Builder instead of this class.
>>>>> return SerializationOfReferenceCollection.create(getClass(),
>>>>> col, type );
>>>>> } catch (InstantiationException ex) {
>>>>> throw new WriteAbortedException("Unable to create
>>>>> serialization proxy", ex);
>>>>> } catch (IllegalAccessException ex) {
>>>>> throw new WriteAbortedException("Unable to create
>>>>> serialization proxy", ex);
>>>>> }
>>>>> }
>>>>>
>>>>> private void readObject(ObjectInputStream stream)
>>>>> throws InvalidObjectException{
>>>>> throw new InvalidObjectException("Builder required");
>>>>> }
>>>>>
>>>>> }
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/7481d67a/attachment-0001.html>

From vitalyd at gmail.com  Wed Jan  4 13:32:17 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 4 Jan 2012 13:32:17 -0500
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <4F04878B.9010105@oracle.com>
References: <1325678569.21868.167.camel@bluto> <4F047693.6080508@oracle.com>
	<CAHjP37Fkj0oibVNhsKstXChwwUe7gaVvWSLYPavY3-++YZcK8w@mail.gmail.com>
	<4F04878B.9010105@oracle.com>
Message-ID: <CAHjP37GDXFrtZTJYJUahuyr0n9hJh_sCdYnu5quzBXYVTR0trQ@mail.gmail.com>

Volatile read will be cheaper but if other cores are constantly writing to
the shared memory that's being read then throughput will degrade -
basically even normal stores/loads of shared locations won't scale under
heavy writing.  My only point was that CAS doesn't have to be 10s of cycles
as you said (and it's getting cheaper with each new generation, except
sandy bridge seems to have gotten worse than nehalem) - that implied to me
that you were saying it's always very expensive, which isn't true;
apologies if I misunderstood you.

In this use case if it's possible to avoid writes then of course it'll be
faster than unconditional CAS.

Vitaly
On Jan 4, 2012 12:08 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  Yes, in terms of performance it is fairly cheap, but it will never be as
> cheap as a volatile read.  Current HotSpot and JRockit x86 implementations
> turn volatile reads into normal loads from cache/RAM (not registers).  This
> is much cheaper than draining the store buffer with a fence.  Normal loads
> will bypass pending stores... unless there is a fence after a store (e.g.
> volatile store).
>
> No, in terms of scalability.  An uncontended CAS in this use case requires
> sending invalidation messages to all of the other processors on the
> system.  This adds quite a bit of latency on an 8-socket system where the
> sockets are not fully connected to each other.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/4/2012 9:38 AM, Vitaly Davidovich wrote:
>
> Uncontended CAS on modern cpus (e.g. nehalem) is fairly cheap.  It incurs
> local latency in that case and involves draining the store buffer; if the
> buffer is empty or near empty it's quite cheap.  See this discussion:
> http://www.azulsystems.com/blog/cliff/2011-11-16-a-short-conversation-on-biased-locking
> On Jan 4, 2012 10:59 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>
>>  > Do you think that all calling threads attempting to obtain the tryLock(), (once only) will cause any performance issues?
>>
>> tryLock() will ideally execute a CAS instruction to acquire the lock.
>> Under no contention, this means that each access will execute a CAS.  This
>> is going to be a performance problem.  It will cost at least 10s of
>> cycles.  If you're lucky, it won't be significant.  Under contention, the
>> CAS instruction will stall the processor pipeline and become a bottleneck.
>> If the tryLock() is called heavily, the system won't be able to scale past
>> 3-socket Nehalem system.
>>
>> The JDK's ReferenceQueue.poll() simply reads a volatile variable.  If the
>> queue is empty, then the impact will simply be reading the variable's value
>> from cache or RAM.  If the value is in L1 cache, then this will only cost 3
>> cycles.  If the queue is not empty, then the code acquires a lock and does
>> the poll operation.  You probably want to do likewise except instead of
>> acquiring the lock, do a tryLock().  Thus, threads won't block on the clean
>> up.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>> On 1/4/2012 5:02 AM, Peter Firmstone wrote:
>>
>> Hi,
>>
>> Over at river.apache.org, we've got a reference collections library,
>> utilised for concurrent caches, it's straightforward clean and simple
>> code.  Easy to use, although Generics are a little verbose.  RC is a
>> class with a simple public api, containing static methods used to wrap
>> any of the java collections interfaces, allowing you to use any
>> reference type, to refer to objects contained in your collections.
>>
>> All implementation is package private, sync / locking strategies are
>> left to the underlying collection, removal / cleaning is performed by
>> any thread that obtains a lock on a private ReferenceQueue (each
>> collection has it's own), which is guarded by a ReentrantLock.tryLock(),
>> threads that don't obtain the lock, continue without performing any
>> cleanup.  Do you think that all calling threads attempting to obtain the
>> tryLock(), (once only) will cause any performance issues?  Garbage
>> collection occurs concurrently with other operations.
>>
>> Reference types available are Weak Identity, Weak, Soft Identity, Soft
>> and Strong.  It's relatively simple to add additional reference types
>> such as a timed reference, however this isn't implemented at present.
>>
>> Here's a useage example, straight from our SecurityManager code:
>>
>> <SNIP>
>> private final ConcurrentMap<AccessControlContext,
>> NavigableSet<Permission>> checked;
>> private final Comparator<Referrer<Permission>> permCompare;
>> </SNIP>
>>
>> <SNIP-FROM-CONSTRUCTOR>
>> ConcurrentMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>> refmap
>> 	= new
>> ConcurrentHashMap<Referrer<AccessControlContext>,Referrer<NavigableSet<Permission>>>(100);
>>
>> checked = RC.concurrentMap(refmap, Ref.SOFT, Ref.STRONG);
>> permCompare = RC.comparator(new PermissionComparator());
>>
>> </SNIP-FROM-CONSTRUCTOR>
>>
>> <SNIP-METHOD>
>>     @Override
>>     public void checkPermission(Permission perm, Object context) throws
>> SecurityException {
>> 	if (!(context instanceof AccessControlContext)) throw new
>> SecurityException();
>> 	if (perm == null ) throw new NullPointerException("Permission
>> Collection null");
>>         /* The next line speeds up permission checks related to this
>> SecurityManager. */
>>         if ( SMPrivilegedContext.equals(context) ||
>> SMConstructorContext.equals(context)) return; // prevents endless loop
>> in debug.
>>         AccessControlContext executionContext = (AccessControlContext)
>> context;
>>         // Checks if Permission has already been checked for this
>> context.
>>         NavigableSet<Permission> checkedPerms =
>> checked.get(executionContext);
>>         if (checkedPerms == null){
>>             /* A ConcurrentSkipListSet is used to avoid blocking during
>>              * removal operations that occur while the garbage collector
>>              * recovers softly reachable memory.  Since this happens
>> while
>>              * the jvm's under stress, it's important that permission
>> checks
>>              * continue to perform well.
>>              *
>>              * Although I considered a multi read, single write Set, I
>> wanted
>>              * to avoid blocking under stress, caused as a result
>>              * of garbage collection.
>>              *
>>              * The Reference Collection that encapsulates the
>> ConcurrentSkipListSet
>>              * uses a ReentrantLock.tryLock() guard the ReferenceQueue
>> used
>>              * to remove objects from the Set.  This allows other
>> threads to
>>              * proceed during object removal.  Only one thread is given
>> access
>>              * to the ReferenceQueue, the unlucky caller thread performs
>> garbage
>>              * removal from the Set before accessing the Set for its
>> original
>>              * purpose.
>>              */
>>             NavigableSet<Referrer<Permission>> internal =
>>                     new
>> ConcurrentSkipListSet<Referrer<Permission>>(permCompare);
>>             checkedPerms = RC.navigableSet(internal, Ref.SOFT);
>>             NavigableSet<Permission> existed =
>> checked.putIfAbsent(executionContext, checkedPerms);
>>             if (existed != null) checkedPerms = existed;
>>         }
>>         if (checkedPerms.contains(perm)) return; // don't need to check
>> again.
>>
>> </SNIP-METHOD>
>>
>> Serialization is implemented so implementations can be replaced and
>> upgraded, serial form is a separate concern; see the Serialization
>> Builder pattern for details: http://wiki.apache.org/river/Serialization
>>
>> This a copy of some implementation code:
>>
>> /*
>>  * Licensed to the Apache Software Foundation (ASF) under one
>>  * or more contributor license agreements.  See the NOTICE file
>>  * distributed with this work for additional information
>>  * regarding copyright ownership. The ASF licenses this file
>>  * to you under the Apache License, Version 2.0 (the
>>  * "License"); you may not use this file except in compliance
>>  * with the License. You may obtain a copy of the License at
>>  *
>>  *      http://www.apache.org/licenses/LICENSE-2.0
>>  *
>>  * Unless required by applicable law or agreed to in writing, software
>>  * distributed under the License is distributed on an "AS IS" BASIS,
>>  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>> implied.
>>  * See the License for the specific language governing permissions and
>>  * limitations under the License.
>>  */
>>
>> package org.apache.river.impl.util;
>>
>>
>> import java.lang.ref.Reference;
>> import java.util.Random;
>> import java.util.concurrent.ConcurrentMap;
>> import java.util.logging.Level;
>> import java.util.logging.Logger;
>>
>> /**
>>  * A referenced hash map, that encapsulates and utilises any
>> ConcurrentMap
>>  * implementation passed in at construction.
>>  *
>>  * Based on any ConcurrentMap implementation, it doesn't accept null
>> keys or values.
>>  *
>>  * It is recommended although not mandatory to use identity based
>> References for keys,
>>  * unexpected results occur when relying on equal keys, if one key is no
>> longer
>>  * strongly reachable and has been garbage collected and removed from
>> the
>>  * Map.
>>  *
>>  *
>>  *
>>  * If either a key or value, is no longer strongly reachable, their
>> mapping
>>  * will be queued for removal and garbage collection, in compliance with
>>  * the Reference implementation selected.
>>  *
>>  * @param <K>
>>  * @param <V>
>>  * @see Ref
>>  * @author Peter Firmstone.
>>  *
>>  * @since 2.3
>>  */
>> class ReferenceConcurrentMap<K, V> extends ReferenceMap<K, V> implements
>> ConcurrentMap<K, V> {
>>
>>     // ConcurrentMap must be protected from null values?  It changes
>> it's behaviour, is that a problem?
>>     private final ConcurrentMap<Referrer<K>, Referrer<V>> map;
>>
>>     ReferenceConcurrentMap(ConcurrentMap<Referrer<K>,Referrer<V>> map,
>> Ref key, Ref val){
>>         super (map, key, val);
>>         this.map = map;
>>     }
>>
>>     ReferenceConcurrentMap(ConcurrentMap<Referrer<K>, Referrer<V>> map,
>>             ReferenceQueuingFactory<K, Referrer<K>> krqf,
>> ReferenceQueuingFactory<V, Referrer<V>> vrqf, Ref key, Ref val){
>>         super(map, krqf, vrqf, key, val);
>>         this.map = map;
>>     }
>>
>>     public V putIfAbsent(K key, V value) {
>>         processQueue();  //may be a slight delay before atomic
>> putIfAbsent
>>         Referrer<K> k = wrapKey(key, true, false);
>>         Referrer<V> v = wrapVal(value, true, false);
>>         Referrer<V> val = map.putIfAbsent(k, v);
>>         while ( val != null ) {
>>             V existed = val.get();
>>             // We hold a strong reference to value, so
>>             if ( existed == null ){
>>                 // stale reference must be replaced, it has been garbage
>> collect but hasn't
>>                 // been removed, we must treat it like the entry doesn't
>> exist.
>>                 if ( map.replace(k, val, v)){
>>                     // replace successful
>>                     return null; // Because officially there was no
>> record.
>>                 } else {
>>                     // Another thread may have replaced it.
>>                     val = map.putIfAbsent(k, v);
>>                 }
>>             } else {
>>                 return existed;
>>             }
>>         }
>>         return null;
>>     }
>>
>>     @SuppressWarnings("unchecked")
>>     public boolean remove(Object key, Object value) {
>>         processQueue();
>>         return map.remove(wrapKey((K) key, false, true), wrapVal((V)
>> value, false, true));
>>     }
>>
>>     public boolean replace(K key, V oldValue, V newValue) {
>>         processQueue();
>>         return map.replace(wrapKey(key, false, true), wrapVal(oldValue,
>> false, true), wrapVal(newValue, true, false));
>>     }
>>
>>     public V replace(K key, V value) {
>>         processQueue();
>>         Referrer<V> val = map.replace(wrapKey(key, false, true),
>> wrapVal(value, true, false));
>>         if ( val != null ) return val.get();
>>         return null;
>>     }
>> }
>>
>> /*
>>  * Licensed to the Apache Software Foundation (ASF) under one
>>  * or more contributor license agreements.  See the NOTICE file
>>  * distributed with this work for additional information
>>  * regarding copyright ownership. The ASF licenses this file
>>  * to you under the Apache License, Version 2.0 (the
>>  * "License"); you may not use this file except in compliance
>>  * with the License. You may obtain a copy of the License at
>>  *
>>  *      http://www.apache.org/licenses/LICENSE-2.0
>>  *
>>  * Unless required by applicable law or agreed to in writing, software
>>  * distributed under the License is distributed on an "AS IS" BASIS,
>>  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>> implied.
>>  * See the License for the specific language governing permissions and
>>  * limitations under the License.
>>  */
>>
>> package org.apache.river.impl.util;
>>
>> import java.io.InvalidObjectException;
>> import java.io.ObjectInputStream;
>> import java.io.ObjectStreamException;
>> import java.io.Serializable;
>> import java.io.WriteAbortedException;
>> import java.lang.ref.Reference;
>> import java.lang.ref.ReferenceQueue;
>> import java.util.AbstractCollection;
>> import java.util.ArrayList;
>> import java.util.Collection;
>> import java.util.Iterator;
>> import java.util.List;
>> import java.util.Set;
>> import java.util.logging.Level;
>> import java.util.logging.Logger;
>>
>> /**
>>  * A Collection of Reference Objects, the developer may chose any
>> Collection
>>  * implementation to store the References, which is passed in a runtime.
>>  *
>>  * The underlying Collection implementation governs the specific
>> behaviour of this
>>  * Collection.
>>  *
>>  * Synchronisation must be implemented by the underlying Collection and
>> cannot
>>  * be performed externally to this class.  The underlying Collection
>> must
>>  * also be mutable.  Objects will be removed automatically from the
>> underlying
>>  * Collection when they are eligible for garbage collection.
>>  *
>>  * Weak, Weak Identity, Soft, Soft Identity or Strong references may be
>> used.
>>  * This Collection may be used as an Object pool cache or any other
>> purpose
>>  * that requires unique memory handling.
>>  *
>>  * For concurrent threads, it is recommended to encapsulate the
>> underlying
>>  * collection in a multi read, single write collection for scalability.
>>  *
>>  * @see Ref
>>  * @see ConcurrentCollections#multiReadCollection(java.util.Collection)
>>  * @author Peter Firmstone.
>>  */
>> class ReferenceCollection<T> extends AbstractCollection<T>
>>                                 implements Collection<T>, Serializable {
>>     private static final long serialVersionUID = 1L;
>>     private final Collection<Referrer<T>> col;
>>     private final ReferenceQueuingFactory<T, Referrer<T>> rqf;
>>     private final Ref type;
>>
>>     ReferenceCollection(Collection<Referrer<T>> col, Ref type){
>>         this(col, new ReferenceProcessor<T>(col, type, type ==
>> Ref.STRONG ? null : new ReferenceQueue<T>()), type);
>>     }
>>
>>     ReferenceCollection(Collection<Referrer<T>> col,
>>             ReferenceQueuingFactory<T, Referrer<T>> rqf, Ref type){
>>         this.col = col;
>>         this.rqf = rqf;
>>         this.type = type;
>>     }
>>
>>     void processQueue(){
>>         rqf.processQueue();
>>         }
>>
>>     ReferenceQueuingFactory<T, Referrer<T>> getRQF(){
>>         return rqf;
>>     }
>>
>>     Ref getRef(){
>>         return type;
>>     }
>>
>>     Referrer<T> wrapObj(T t, boolean enqueue, boolean temporary){
>>         return rqf.referenced(t, enqueue, temporary);
>>     }
>>
>>     public int size() {
>>         processQueue();
>>         return col.size();
>>     }
>>
>>     public boolean isEmpty() {
>>         processQueue();
>>         return col.isEmpty();
>>     }
>>
>>     public boolean contains(Object o) {
>>         processQueue();
>>         return col.contains(wrapObj((T) o, false, true));
>>     }
>>
>>     /**
>>      * This Iterator may return null values if garbage collection
>>      * runs during iteration.
>>      *
>>      * Always check for null values.
>>      *
>>      * @return T - possibly null.
>>      */
>>     public Iterator<T> iterator() {
>>         processQueue();
>>         return new ReferenceIterator<T>(col.iterator());
>>     }
>>
>>     public boolean add(T e) {
>>         processQueue();
>>         return col.add(wrapObj(e, true, false));
>>     }
>>
>>     public boolean remove(Object o) {
>>         processQueue();
>>         return col.remove(wrapObj((T) o, false, true));
>>     }
>>
>>
>>     @SuppressWarnings("unchecked")
>>     public boolean containsAll(Collection<?> c) {
>>         processQueue();
>>         return col.containsAll(new CollectionWrapper<T>((Collection<T>)
>> c, getRQF(), false, true));
>>     }
>>
>>
>>     @SuppressWarnings("unchecked")
>>     public boolean addAll(Collection<? extends T> c) {
>>         processQueue();
>>         return col.addAll(new CollectionWrapper<T>((Collection<T>) c,
>> getRQF(), true, false));
>>     }
>>
>>     public void clear() {
>>         col.clear();
>>     }
>>
>>     /*
>>      * The next three methods are suitable implementations for
>> subclasses also.
>>      */
>>     public String toString(){
>>         return col.toString();
>>     }
>>
>>     @Override
>>     public int hashCode() {
>>         if ( col instanceof List || col instanceof Set ){
>>             return col.hashCode();
>>         }
>>         return System.identityHashCode(this);
>>     }
>>
>>     /**
>>      * Because equals and hashCode are not defined for collections, we
>>      * cannot guarantee consistent behaviour by implementing equals and
>>      * hashCode.  A collection could be a list, set, queue or deque.
>>      * So a List != Queue and a Set != list. therefore equals for
>> collections is
>>      * not defined.
>>      *
>>      * However since two collections may both also be Lists, while
>> abstracted
>>      * from the client two lists may still be equal.
>>      * @see Collection#equals(java.lang.Object)
>>      */
>>
>>     @Override
>>     public boolean equals(Object o){
>>         if ( o == this ) return true;
>>         if ( col instanceof List || col instanceof Set ){
>>             return col.equals(o);
>>         }
>>         return false;
>>     }
>>
>>     final Object writeReplace() throws ObjectStreamException {
>>         try {
>>             // returns a Builder instead of this class.
>>             return SerializationOfReferenceCollection.create(getClass(),
>> col, type );
>>         } catch (InstantiationException ex) {
>>             throw new WriteAbortedException("Unable to create
>> serialization proxy", ex);
>>         } catch (IllegalAccessException ex) {
>>             throw new WriteAbortedException("Unable to create
>> serialization proxy", ex);
>>         }
>>     }
>>
>>     private void readObject(ObjectInputStream stream)
>>             throws InvalidObjectException{
>>         throw new InvalidObjectException("Builder required");
>>     }
>>
>> }
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/b0912aec/attachment-0001.html>

From heinz at javaspecialists.eu  Wed Jan  4 14:18:54 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 04 Jan 2012 21:18:54 +0200
Subject: [concurrency-interest] Funny ThreadLocalRandom Bug in Random :-)
In-Reply-To: <4F0492B0.7020405@oracle.com>
References: <1325678569.21868.167.camel@bluto>
	<4F047693.6080508@oracle.com>	<4F047A3E.7050109@redhat.com>
	<4F04855B.1050603@oracle.com>	<4F048B2E.7000909@redhat.com>
	<4F0492B0.7020405@oracle.com>
Message-ID: <4F04A61E.8090802@javaspecialists.eu>

What started as a quest to write a little Phaser demo, ended up as a 
chase through the Java 7 source code to find a fun bug.  Turns out that 
in the version of Java 7 that I am using (on the Mac OS X, unofficial 
build, build 1.7.0-ea-b221), there is a bug in Random.  In the 
constructor, it should have said:

    public Random(long seed) {
        this.seed = new AtomicLong();
        setSeed(initialScramble(seed));
    }

But instead, they just do:

    public Random(long seed) {
        this.seed = new AtomicLong(initialScramble(seed));
    }

As a result, the setSeed() method in the ThreadLocalRandom is never called!

In other words, the seed will always be zero and all ThreadLocalRandom 
instances will give back the same values.  Every time.

Here is the Phaser demo I was talking about.  It makes all the buttons 
change their background color for a random amount of time.  Here they 
have to create their own instance of Random, since ThreadLocalRandom 
does not work correctly.  Enjoy :-)



import javax.swing.*;
import java.awt.*;
import java.util.*;
import java.util.concurrent.*;

/**
 * @author Heinz Kabutz
 */
public class Blinker extends JFrame {
  private static final boolean USE_THREAD_LOCAL_RANDOM = false;

  public Blinker() {
    setLayout(new FlowLayout());
  }

  private void addButtons(int buttons, final int blinks) {
    final Phaser phaser = new Phaser(buttons) {
      protected boolean onAdvance(int phase, int registeredParties) {
        return phase >= blinks - 1 || registeredParties == 0;
      }
    };

    for (int i = 0; i < buttons; i++) {
      final JComponent comp = new JButton("Button " + i);
      comp.setOpaque(true);
      changeColor(comp, Color.WHITE);
      add(comp);
      new Thread() {
        public void run() {
          final Random rand =
              USE_THREAD_LOCAL_RANDOM ? ThreadLocalRandom.current() : 
new Random();
          try {
            int phases = 0;
            do {
              System.out.println(++phases);
              Color newColor = new Color(rand.nextInt());
              changeColor(comp, newColor);
              Thread.sleep(100 + rand.nextInt(3000));
              changeColor(comp, Color.WHITE);
              Toolkit.getDefaultToolkit().beep();
              Thread.sleep(2000);
              phaser.arriveAndAwaitAdvance();
            } while (!phaser.isTerminated());
          } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
          }
        }
      }.start();
      try {
        Thread.sleep(10);
      } catch (InterruptedException e) {
        e.printStackTrace();
      }
    }
  }

  private void changeColor(final JComponent comp, final Color color) {
    SwingUtilities.invokeLater(new Runnable() {
      public void run() {
        comp.setBackground(color);
        invalidate();
        repaint();
      }
    });
  }

  public static void main(String[] args) {
    SwingUtilities.invokeLater(new Runnable() {
      public void run() {
        Blinker blinker = new Blinker();
        blinker.addButtons(10, 3);
        blinker.pack();
        blinker.setVisible(true);
        blinker.setDefaultCloseOperation(EXIT_ON_CLOSE);
      }
    });
  }
}

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz






From dl at cs.oswego.edu  Wed Jan  4 14:24:06 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 04 Jan 2012 14:24:06 -0500
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <CAHjP37GDXFrtZTJYJUahuyr0n9hJh_sCdYnu5quzBXYVTR0trQ@mail.gmail.com>
References: <1325678569.21868.167.camel@bluto>
	<4F047693.6080508@oracle.com>	<CAHjP37Fkj0oibVNhsKstXChwwUe7gaVvWSLYPavY3-++YZcK8w@mail.gmail.com>	<4F04878B.9010105@oracle.com>
	<CAHjP37GDXFrtZTJYJUahuyr0n9hJh_sCdYnu5quzBXYVTR0trQ@mail.gmail.com>
Message-ID: <4F04A756.8050806@cs.oswego.edu>

On 01/04/12 13:32, Vitaly Davidovich wrote:
> Volatile read will be cheaper but if other cores are constantly writing to the
> shared memory that's being read then throughput will degrade - basically even
> normal stores/loads of shared locations won't scale under heavy writing.  My
> only point was that CAS doesn't have to be 10s of cycles as you said (and it's
> getting cheaper with each new generation, except sandy bridge seems to have
> gotten worse than nehalem)

This is my experience as well. A CAS that almost never fails is sometimes
even cheaper than a volatile write on i7-Nehalem. It is more
expensive on SandyBridge/NehalemEX and recent AMDs but still not worth
spending more than a couple of cycles trying to avoid. Nathan's advice to read
rather than CAS when possible is a good example of when it is worth
avoiding, but beyond that there are diminishing returns. Of course,
avoiding unnecessary writes of any form is always a good idea.

These days, memory contention (mainly false-sharing-style cache
contention, plus NUMA effects) is a far more serious performance
issue than CAS contention per se, especially on multisocketed machines.

I've been working on a set of improvements
to a bunch of j.u.c classes that address this. Stay tuned.
Currently, the only one committed to our CVS is a preliminary
version of overhauled Exchanger. (Exchangers are not commonly
used, but they provide an ideal setting for evaluating new
performance enhancement algorithms, since they are subject to
extreme contention, lock-free data-transfer, and blocking;
all of which are found in more commonly used classes).

-Doug


From mlists at juma.me.uk  Wed Jan  4 14:24:45 2012
From: mlists at juma.me.uk (Ismael Juma)
Date: Wed, 4 Jan 2012 19:24:45 +0000
Subject: [concurrency-interest] Funny ThreadLocalRandom Bug in Random :-)
In-Reply-To: <4F04A61E.8090802@javaspecialists.eu>
References: <1325678569.21868.167.camel@bluto> <4F047693.6080508@oracle.com>
	<4F047A3E.7050109@redhat.com> <4F04855B.1050603@oracle.com>
	<4F048B2E.7000909@redhat.com> <4F0492B0.7020405@oracle.com>
	<4F04A61E.8090802@javaspecialists.eu>
Message-ID: <CAD5tkZYAX9rPA84XDjeg5w01rGnGD7t9GgDOwH5jR2avs1RJzA@mail.gmail.com>

On Wed, Jan 4, 2012 at 7:18 PM, Dr Heinz M. Kabutz <heinz at javaspecialists.eu
> wrote:

> What started as a quest to write a little Phaser demo, ended up as a chase
> through the Java 7 source code to find a fun bug.  Turns out that in the
> version of Java 7 that I am using (on the Mac OS X, unofficial build, build
> 1.7.0-ea-b221), there is a bug in Random.  In the constructor, it should
> have said:
>
>   public Random(long seed) {
>       this.seed = new AtomicLong();
>       setSeed(initialScramble(seed))**;
>   }
>
> But instead, they just do:
>
>   public Random(long seed) {
>       this.seed = new AtomicLong(initialScramble(**seed));
>   }
>

In Java 7 update 2:

    public Random(long seed) {
        if (getClass() == Random.class)
            this.seed = new AtomicLong(initialScramble(seed));
        else {
            // subclass might have overriden setSeed
            this.seed = new AtomicLong();
            setSeed(seed);
        }
    }

Best,
Ismael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/2e48dcc1/attachment.html>

From martinrb at google.com  Wed Jan  4 14:25:58 2012
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 4 Jan 2012 11:25:58 -0800
Subject: [concurrency-interest] Funny ThreadLocalRandom Bug in Random :-)
In-Reply-To: <4F04A61E.8090802@javaspecialists.eu>
References: <1325678569.21868.167.camel@bluto> <4F047693.6080508@oracle.com>
	<4F047A3E.7050109@redhat.com> <4F04855B.1050603@oracle.com>
	<4F048B2E.7000909@redhat.com> <4F0492B0.7020405@oracle.com>
	<4F04A61E.8090802@javaspecialists.eu>
Message-ID: <CA+kOe09=Pfebn65kFhHQb+_bZXYQX5YR9OOsnamY+himZyU1fA@mail.gmail.com>

This known bug was fixed a while ago, and is included in openjdk7u2.

On Wed, Jan 4, 2012 at 11:18, Dr Heinz M. Kabutz
<heinz at javaspecialists.eu>wrote:

> What started as a quest to write a little Phaser demo, ended up as a chase
> through the Java 7 source code to find a fun bug.  Turns out that in the
> version of Java 7 that I am using (on the Mac OS X, unofficial build, build
> 1.7.0-ea-b221), there is a bug in Random.  In the constructor, it should
> have said:
>
>   public Random(long seed) {
>       this.seed = new AtomicLong();
>       setSeed(initialScramble(seed))**;
>   }
>
> But instead, they just do:
>
>   public Random(long seed) {
>       this.seed = new AtomicLong(initialScramble(**seed));
>   }
>
> As a result, the setSeed() method in the ThreadLocalRandom is never called!
>
> In other words, the seed will always be zero and all ThreadLocalRandom
> instances will give back the same values.  Every time.
>
> Here is the Phaser demo I was talking about.  It makes all the buttons
> change their background color for a random amount of time.  Here they have
> to create their own instance of Random, since ThreadLocalRandom does not
> work correctly.  Enjoy :-)
>
>
>
> import javax.swing.*;
> import java.awt.*;
> import java.util.*;
> import java.util.concurrent.*;
>
> /**
> * @author Heinz Kabutz
> */
> public class Blinker extends JFrame {
>  private static final boolean USE_THREAD_LOCAL_RANDOM = false;
>
>  public Blinker() {
>   setLayout(new FlowLayout());
>  }
>
>  private void addButtons(int buttons, final int blinks) {
>   final Phaser phaser = new Phaser(buttons) {
>     protected boolean onAdvance(int phase, int registeredParties) {
>       return phase >= blinks - 1 || registeredParties == 0;
>     }
>   };
>
>   for (int i = 0; i < buttons; i++) {
>     final JComponent comp = new JButton("Button " + i);
>     comp.setOpaque(true);
>     changeColor(comp, Color.WHITE);
>     add(comp);
>     new Thread() {
>       public void run() {
>         final Random rand =
>             USE_THREAD_LOCAL_RANDOM ? ThreadLocalRandom.current() : new
> Random();
>         try {
>           int phases = 0;
>           do {
>             System.out.println(++phases);
>             Color newColor = new Color(rand.nextInt());
>             changeColor(comp, newColor);
>             Thread.sleep(100 + rand.nextInt(3000));
>             changeColor(comp, Color.WHITE);
>             Toolkit.getDefaultToolkit().**beep();
>             Thread.sleep(2000);
>             phaser.arriveAndAwaitAdvance()**;
>           } while (!phaser.isTerminated());
>         } catch (InterruptedException e) {
>           Thread.currentThread().**interrupt();
>         }
>       }
>     }.start();
>     try {
>       Thread.sleep(10);
>     } catch (InterruptedException e) {
>       e.printStackTrace();
>     }
>   }
>  }
>
>  private void changeColor(final JComponent comp, final Color color) {
>   SwingUtilities.invokeLater(new Runnable() {
>     public void run() {
>       comp.setBackground(color);
>       invalidate();
>       repaint();
>     }
>   });
>  }
>
>  public static void main(String[] args) {
>   SwingUtilities.invokeLater(new Runnable() {
>     public void run() {
>       Blinker blinker = new Blinker();
>       blinker.addButtons(10, 3);
>       blinker.pack();
>       blinker.setVisible(true);
>       blinker.**setDefaultCloseOperation(EXIT_**ON_CLOSE);
>     }
>   });
>  }
> }
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professional
> http://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/fd140362/attachment.html>

From heinz at javaspecialists.eu  Wed Jan  4 14:28:35 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 4 Jan 2012 21:28:35 +0200
Subject: [concurrency-interest] Funny ThreadLocalRandom Bug in Random :-)
In-Reply-To: <CA+kOe09=Pfebn65kFhHQb+_bZXYQX5YR9OOsnamY+himZyU1fA@mail.gmail.com>
References: <1325678569.21868.167.camel@bluto> <4F047693.6080508@oracle.com>
	<4F047A3E.7050109@redhat.com> <4F04855B.1050603@oracle.com>
	<4F048B2E.7000909@redhat.com> <4F0492B0.7020405@oracle.com>
	<4F04A61E.8090802@javaspecialists.eu>
	<CA+kOe09=Pfebn65kFhHQb+_bZXYQX5YR9OOsnamY+himZyU1fA@mail.gmail.com>
Message-ID: <CACLL95qz9XMLKNMVaQf-Ab173Ap63JFU41tAmwVJf4eyGykdAQ@mail.gmail.com>

I expected as much, Martin.  The Phaser example is quite fun though if you
get a chance to run it.

On 4 January 2012 21:25, Martin Buchholz <martinrb at google.com> wrote:

> This known bug was fixed a while ago, and is included in openjdk7u2.
>
> On Wed, Jan 4, 2012 at 11:18, Dr Heinz M. Kabutz <heinz at javaspecialists.eu
> > wrote:
>
>> What started as a quest to write a little Phaser demo, ended up as a
>> chase through the Java 7 source code to find a fun bug.  Turns out that in
>> the version of Java 7 that I am using (on the Mac OS X, unofficial build,
>> build 1.7.0-ea-b221), there is a bug in Random.  In the constructor, it
>> should have said:
>>
>>   public Random(long seed) {
>>       this.seed = new AtomicLong();
>>       setSeed(initialScramble(seed))**;
>>   }
>>
>> But instead, they just do:
>>
>>   public Random(long seed) {
>>       this.seed = new AtomicLong(initialScramble(**seed));
>>   }
>>
>> As a result, the setSeed() method in the ThreadLocalRandom is never
>> called!
>>
>> In other words, the seed will always be zero and all ThreadLocalRandom
>> instances will give back the same values.  Every time.
>>
>> Here is the Phaser demo I was talking about.  It makes all the buttons
>> change their background color for a random amount of time.  Here they have
>> to create their own instance of Random, since ThreadLocalRandom does not
>> work correctly.  Enjoy :-)
>>
>>
>>
>> import javax.swing.*;
>> import java.awt.*;
>> import java.util.*;
>> import java.util.concurrent.*;
>>
>> /**
>> * @author Heinz Kabutz
>> */
>> public class Blinker extends JFrame {
>>  private static final boolean USE_THREAD_LOCAL_RANDOM = false;
>>
>>  public Blinker() {
>>   setLayout(new FlowLayout());
>>  }
>>
>>  private void addButtons(int buttons, final int blinks) {
>>   final Phaser phaser = new Phaser(buttons) {
>>     protected boolean onAdvance(int phase, int registeredParties) {
>>       return phase >= blinks - 1 || registeredParties == 0;
>>     }
>>   };
>>
>>   for (int i = 0; i < buttons; i++) {
>>     final JComponent comp = new JButton("Button " + i);
>>     comp.setOpaque(true);
>>     changeColor(comp, Color.WHITE);
>>     add(comp);
>>     new Thread() {
>>       public void run() {
>>         final Random rand =
>>             USE_THREAD_LOCAL_RANDOM ? ThreadLocalRandom.current() : new
>> Random();
>>         try {
>>           int phases = 0;
>>           do {
>>             System.out.println(++phases);
>>             Color newColor = new Color(rand.nextInt());
>>             changeColor(comp, newColor);
>>             Thread.sleep(100 + rand.nextInt(3000));
>>             changeColor(comp, Color.WHITE);
>>             Toolkit.getDefaultToolkit().**beep();
>>             Thread.sleep(2000);
>>             phaser.arriveAndAwaitAdvance()**;
>>           } while (!phaser.isTerminated());
>>         } catch (InterruptedException e) {
>>           Thread.currentThread().**interrupt();
>>         }
>>       }
>>     }.start();
>>     try {
>>       Thread.sleep(10);
>>     } catch (InterruptedException e) {
>>       e.printStackTrace();
>>     }
>>   }
>>  }
>>
>>  private void changeColor(final JComponent comp, final Color color) {
>>   SwingUtilities.invokeLater(new Runnable() {
>>     public void run() {
>>       comp.setBackground(color);
>>       invalidate();
>>       repaint();
>>     }
>>   });
>>  }
>>
>>  public static void main(String[] args) {
>>   SwingUtilities.invokeLater(new Runnable() {
>>     public void run() {
>>       Blinker blinker = new Blinker();
>>       blinker.addButtons(10, 3);
>>       blinker.pack();
>>       blinker.setVisible(true);
>>       blinker.**setDefaultCloseOperation(EXIT_**ON_CLOSE);
>>     }
>>   });
>>  }
>> }
>>
>> Regards
>>
>> Heinz
>> --
>> Dr Heinz M. Kabutz (PhD CompSci)
>> Author of "The Java(tm) Specialists' Newsletter"
>> Sun Java Champion
>> IEEE Certified Software Development Professional
>> http://www.javaspecialists.eu
>> Tel: +30 69 72 850 460
>> Skype: kabutz
>>
>>
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
>


-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/55975646/attachment-0001.html>

From peter.firmstone at zeus.net.au  Wed Jan  4 15:35:33 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Thu, 05 Jan 2012 06:35:33 +1000
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <mailman.596.1325705325.6569.concurrency-interest@cs.oswego.edu>
References: <mailman.596.1325705325.6569.concurrency-interest@cs.oswego.edu>
Message-ID: <1325709333.2319.35.camel@bluto>

Interesting ...So it sounds like each collection needs it's own private
gc thread waiting on ReferenceQueue.

ReferenceQueue then has a single consumer thread.  Other threads
accessing the collection no longer need to worry about cleaning dead
references, although they will of course receive the occassional null
value when they access a reference that has been garbage collected, but
not yet cleaned.  This is good in the case of putIfAbsent, where a null
return indicates absence.

Then garbage collection is just one more thread that accesses the
underlying collection, in this case to perform removals, so we should
see similar performance to the native underlying collection, enabling it
to take advantage of your future work.

In the implementation, actual References are only created for
insertions, for reads, a Referrer is created and discarded (never
shared) which has the same identity as the Reference.  References and
Referrers are invisible to the client caller, which just sees the
collection.

But threads themselves consume memory, hard to see all the use cases, so
it sounds like it needs to be a construction parameter, leaving the
choice up to the user-developer: If you want to scale, the Reference
Collection creates a garbage cleaning thread, if not you save the memory
and live with CAS.

Thank you all very much for the comments.

Regards,

Peter.



> 
> Message: 2
> Date: Wed, 04 Jan 2012 14:24:06 -0500
> From: Doug Lea <dl at cs.oswego.edu>
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Reference Collections
> Message-ID: <4F04A756.8050806 at cs.oswego.edu>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> On 01/04/12 13:32, Vitaly Davidovich wrote:
> > Volatile read will be cheaper but if other cores are constantly writing to the
> > shared memory that's being read then throughput will degrade - basically even
> > normal stores/loads of shared locations won't scale under heavy writing.  My
> > only point was that CAS doesn't have to be 10s of cycles as you said (and it's
> > getting cheaper with each new generation, except sandy bridge seems to have
> > gotten worse than nehalem)
> 
> This is my experience as well. A CAS that almost never fails is sometimes
> even cheaper than a volatile write on i7-Nehalem. It is more
> expensive on SandyBridge/NehalemEX and recent AMDs but still not worth
> spending more than a couple of cycles trying to avoid. Nathan's advice to read
> rather than CAS when possible is a good example of when it is worth
> avoiding, but beyond that there are diminishing returns. Of course,
> avoiding unnecessary writes of any form is always a good idea.
> 
> These days, memory contention (mainly false-sharing-style cache
> contention, plus NUMA effects) is a far more serious performance
> issue than CAS contention per se, especially on multisocketed machines.
> 
> I've been working on a set of improvements
> to a bunch of j.u.c classes that address this. Stay tuned.
> Currently, the only one committed to our CVS is a preliminary
> version of overhauled Exchanger. (Exchangers are not commonly
> used, but they provide an ideal setting for evaluating new
> performance enhancement algorithms, since they are subject to
> extreme contention, lock-free data-transfer, and blocking;
> all of which are found in more commonly used classes).
> 
> -Doug
> 



From nathan.reynolds at oracle.com  Wed Jan  4 15:54:44 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 04 Jan 2012 13:54:44 -0700
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <1325709333.2319.35.camel@bluto>
References: <mailman.596.1325705325.6569.concurrency-interest@cs.oswego.edu>
	<1325709333.2319.35.camel@bluto>
Message-ID: <4F04BC94.1090704@oracle.com>

I would be careful in having 1 thread do all of the cleaning.  JRockit 
has an option to allow for multiple finalization threads to run.  One 
finalizer thread couldn't finalize objects fast enough.

What about having 1 ReferenceQueue for all of the collections?  Each 
reference pulled from the queue holds the collection that it belongs 
to.  If the cleaning threads aren't able to get some sleep between GCs, 
then add another thread.  If the threads are sleeping too much, then 
reduce the number of threads.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/4/2012 1:35 PM, Peter Firmstone wrote:
> Interesting ...So it sounds like each collection needs it's own private
> gc thread waiting on ReferenceQueue.
>
> ReferenceQueue then has a single consumer thread.  Other threads
> accessing the collection no longer need to worry about cleaning dead
> references, although they will of course receive the occassional null
> value when they access a reference that has been garbage collected, but
> not yet cleaned.  This is good in the case of putIfAbsent, where a null
> return indicates absence.
>
> Then garbage collection is just one more thread that accesses the
> underlying collection, in this case to perform removals, so we should
> see similar performance to the native underlying collection, enabling it
> to take advantage of your future work.
>
> In the implementation, actual References are only created for
> insertions, for reads, a Referrer is created and discarded (never
> shared) which has the same identity as the Reference.  References and
> Referrers are invisible to the client caller, which just sees the
> collection.
>
> But threads themselves consume memory, hard to see all the use cases, so
> it sounds like it needs to be a construction parameter, leaving the
> choice up to the user-developer: If you want to scale, the Reference
> Collection creates a garbage cleaning thread, if not you save the memory
> and live with CAS.
>
> Thank you all very much for the comments.
>
> Regards,
>
> Peter.
>
>
>
>> Message: 2
>> Date: Wed, 04 Jan 2012 14:24:06 -0500
>> From: Doug Lea<dl at cs.oswego.edu>
>> To: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Reference Collections
>> Message-ID:<4F04A756.8050806 at cs.oswego.edu>
>> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>>
>> On 01/04/12 13:32, Vitaly Davidovich wrote:
>>> Volatile read will be cheaper but if other cores are constantly writing to the
>>> shared memory that's being read then throughput will degrade - basically even
>>> normal stores/loads of shared locations won't scale under heavy writing.  My
>>> only point was that CAS doesn't have to be 10s of cycles as you said (and it's
>>> getting cheaper with each new generation, except sandy bridge seems to have
>>> gotten worse than nehalem)
>> This is my experience as well. A CAS that almost never fails is sometimes
>> even cheaper than a volatile write on i7-Nehalem. It is more
>> expensive on SandyBridge/NehalemEX and recent AMDs but still not worth
>> spending more than a couple of cycles trying to avoid. Nathan's advice to read
>> rather than CAS when possible is a good example of when it is worth
>> avoiding, but beyond that there are diminishing returns. Of course,
>> avoiding unnecessary writes of any form is always a good idea.
>>
>> These days, memory contention (mainly false-sharing-style cache
>> contention, plus NUMA effects) is a far more serious performance
>> issue than CAS contention per se, especially on multisocketed machines.
>>
>> I've been working on a set of improvements
>> to a bunch of j.u.c classes that address this. Stay tuned.
>> Currently, the only one committed to our CVS is a preliminary
>> version of overhauled Exchanger. (Exchangers are not commonly
>> used, but they provide an ideal setting for evaluating new
>> performance enhancement algorithms, since they are subject to
>> extreme contention, lock-free data-transfer, and blocking;
>> all of which are found in more commonly used classes).
>>
>> -Doug
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120104/9d694069/attachment.html>

From peter.firmstone at zeus.net.au  Wed Jan  4 16:38:49 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Thu, 05 Jan 2012 07:38:49 +1000
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <4F04BC94.1090704@oracle.com>
References: <mailman.596.1325705325.6569.concurrency-interest@cs.oswego.edu>
	<1325709333.2319.35.camel@bluto>  <4F04BC94.1090704@oracle.com>
Message-ID: <1325713129.2319.61.camel@bluto>


On Thu, 2012-01-05 at 06:54, Nathan Reynolds wrote:
> I would be careful in having 1 thread do all of the cleaning.  JRockit
> has an option to allow for multiple finalization threads to run.  One
> finalizer thread couldn't finalize objects fast enough.

Ok that's interesting, one problem though, using more than on thread on
ReferenceQueue will create contention, so there would need to be one
thread that takes from ReferenceQueue, with its own double ended queue,
where other threads are created to steal off the reference queue if it
becomes overloaded.  This is assuming that the bottle neck is the
collection and not the ReferenceQueue.

Regards,

Peter.

This is the single gc thread implementation, in this case there's a
queue for each collection:

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.river.impl.util;

import java.lang.ref.Reference;
import java.lang.ref.ReferenceQueue;
import java.util.Collection;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;
import java.util.logging.Level;
import java.util.logging.Logger;

/**
 * ReferenceProcessor is responsible for creation and collection of
References 
 * on behalf of Reference Collection implementations.
 *
 * @param <T> 
 * @author peter
 */
class ReferenceProcessor<T> implements ReferenceQueuingFactory<T,
Referrer<T>> {
    
    private final Collection<Referrer<T>> col;
    private final ReferenceQueue<T> queue;
    private final Ref type;
    private final Lock queueLock;
    private final Thread cleaner;
    
    ReferenceProcessor(Collection<Referrer<T>> col, Ref type,
ReferenceQueue<T> queue, boolean gcThread){
        if (col == null || type == null ) throw new
NullPointerException("collection or reference type cannot be null");
        this.col = col;
        this.type = type;
        this.queue = type == Ref.STRONG ? null : queue;
        queueLock = new ReentrantLock();
        cleaner = (gcThread && this.queue != null)
                ? new Thread( new CleanerTask(this.col, this.queue),
                                                   
"ReferenceProcessorGC") 
                : null;
        if (cleaner != null){
            cleaner.setDaemon(true);
            cleaner.start();
        }
    }

    @Override
    public T pseudoReferent(Referrer<T> u) {
        throw new UnsupportedOperationException("Not supported.");
    }

    @Override
    public Referrer<T> referenced(T w, boolean enque, boolean temporary)
{
        if (w == null) return null;
        if (temporary) return ReferenceFactory.singleUseForLookup(w,
type);
        return ReferenceFactory.create(w, enque == true ? queue : null,
type);
    }

    @Override
    public void processQueue() {
        if (queue == null || cleaner != null) return;
        Object t = null;
        /*
         * The reason for using an explicit lock is if another thread is
         * removing the garbage, we don't want to prevent all other
threads
         * accessing the underlying collection, when it blocks on poll,
         * this means that some client threads will receive null values 
         * on occassion, but this is a small price to pay.  
         * Might have to employ the null object pattern.
         */
        if ( queueLock.tryLock()){
            try {
                while ( (t = queue.poll()) != null){
                    col.remove(t);
                }
            }finally{
                queueLock.unlock();
            }
        }
    }
    
    private class CleanerTask implements Runnable {
        
        private final Collection col;
        private final ReferenceQueue queue;
        
        private CleanerTask(Collection c, ReferenceQueue queue){
            col = c;
            this.queue = queue;
        }
        
        @Override
        public void run() {
            Thread thread = Thread.currentThread();
            Object t = null;
            try {
                // should never be null, remove blocks.
                while ( (t = queue.remove()) != null ){ 
                    col.remove(t);
                }
                if ( t == null) throw new NullPointerException("null
remove from ReferenceQueue");
            } catch (InterruptedException ex) {
                // Restore the interrupt.
                thread.interrupt();
                return;
            }
            
        }
    
    }
}




From peter.firmstone at zeus.net.au  Wed Jan  4 17:47:22 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Thu, 05 Jan 2012 08:47:22 +1000
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <4F04CA0C.3090000@oracle.com>
References: <mailman.596.1325705325.6569.concurrency-interest@cs.oswego.edu>
	<1325709333.2319.35.camel@bluto>  <4F04BC94.1090704@oracle.com>
	<1325713129.2319.61.camel@bluto>  <4F04CA0C.3090000@oracle.com>
Message-ID: <1325717242.2319.100.camel@bluto>

On Thu, 2012-01-05 at 07:52, Nathan Reynolds wrote:
> Would the fork-join framework with work steal be an ideal fit?

For ultimate scalability it would, although I'm unable to test for
scalability, only reason about it and ask questions of developers who do
have acces to big iron ;)  It might also make sense to use multiple
ReferenceQueue's for a collection under some circumstances.

I've implemented Reference Collections to fulfill a need that our
project has, I'm sure very few are aware of its existence, but since it
has such a compact public API that's very easy to extend to new
collection interfaces, it could potentially make a very good collections
based library for using references in your favoured collection
implementation.  It's also possible to make new types of refernces, eg:
timer based.  There's no serialized form lock in either, so evolution
isn't hampered by serialization and serialization can be easily
supported in a backward compatible evolutionary manner.

If there's enough interest, we could split Reference Collections out as
a separate library or subproject.

The code can be seen here:

http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolicy/src/org/apache/river/impl/util/

The unit tests here:

http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolicy/test/src/org/apache/river/impl/util/

Or:

svn checkout (replacing viewvc with repos/asf):

https://svn.apache.org/repos/asf/river/jtsk/skunk/peterConcurrentPolicy/src/org/apache/river/impl/util

This is the public API:

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.river.impl.util;

import java.lang.ref.Reference;
import java.lang.Comparable;
import java.util.Collection;
import java.util.Collections;
import java.util.Comparator;
import java.util.Deque;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.ListIterator;
import java.util.NavigableMap;
import java.util.NavigableSet;
import java.util.Queue;
import java.util.Set;
import java.util.SortedMap;
import java.util.SortedSet;
import java.util.concurrent.BlockingDeque;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.ConcurrentNavigableMap;

/**
 * <p>
 * This class contains a number of static methods for using and
abstracting
 * References in Collections.  Interfaces from the Java Collections
Framework
 * are supported.
 * </p><p>
 * Referents in these collections may implement {@link Comparable} or
 * a {@link Comparator} may be used for sorting.  When Comparator's are
utilised,
 * they must first be encapsulated {@link
RC#comparator(java.util.Comparator) },
 * before passing to a constructor for your preferred underlying
Collection
 * implementation.
 * </p><p>
 * {@link Comparable} is not supported for IDENTITY == referenced
Collections, 
 * in this case a Comparator must be used.  
 * </p><p>
 * All other references support {@link Comparable}, if the referent
Object
 * doesn't implement {@link Comparable}, then {@link
Reference#hashCode()} is used
 * for sorting.  If two referent Objects have identical hashCodes,
 * but are unequal and do not implement {@link Comparable}, their
references
 * will also have identical hashCodes, so only one of the referents can
 * be added to a {@link SortedSet} or {@link SortedMap}.  This can be
fixed by using a
 * {@link Comparator}.
 * </p><p>
 * For all intents and purposes these utilities behave the same as your
preferred
 * underlying {@link Collection} implementation, with the exception of 
 * {@link Reference} reachability.  An Object or Key,Value entry is
removed 
 * from a {@link Collection} or {@link Map}, upon becoming eligible for 
 * garbage collection.
 * </p><p>
 * Synchronisation must be implemented by your preferred {@link
Collection}
 * and cannot be performed externally to the returned {@link
Collection}.  
 * Your chosen underlying {@link Collection} must also be mutable.  
 * Objects will be removed automatically from underlying Collections
when 
 * they are eligible for garbage collection, this breaks external
synchronisation.
 * {@link
CollectionsConcurrent#multiReadCollection(java.util.Collection)} may
 * be useful for synchronising your chosen underlying {@link
Collection}, 
 * especially if Objects are not being garbage collected often and
writes
 * are minimal. 
 * </p><p>  
 * An Unmodifiable wrapper {@link
Collections#unmodifiableCollection(java.util.Collection)}
 * may be used externally to prevent additions to the underlying
Collections,
 * referents will still be removed as they become unreachable however.
 * </p><p>
 * Note that any Sub List, Sub Set or Sub Map obtained by any of the
Java
 * Collections Framework interfaces, must be views of the underlying
 * Collection, if the Collection uses defensive copies instead of views,
 * References could potentially remain in one copy after garbage
collection, 
 * causing null returns.  If using standard Java Collections Framework 
 * implementations, these problems don't occur as all Sub Lists, 
 * Sub Sets or Sub Maps are views only.
 * </p><p>
 * {@link Map#entrySet() } view instances returned preserve your chosen
reference
 * behaviour, they even support {@link Set#add(java.lang.Object)} or 
 * {@link Set#addAll(java.util.Collection)} methods, although you'll be
hard
 * pressed to find a standard java implementation that does.  If you
have a
 * Map with a Set of Entry's implementing add, the implementation will
need a 
 * Comparator, that compares Entry's only by their keys, to avoid
duplicating
 * keys, primarily because an Entry hashCode includes the both key and
value in its 
 * calculation. {@link Entry#hashCode() }
 * </p><p>
 * All other {@link Map#entrySet() } methods are fully implemented and
supported.
 * </p><p>
 * {@link Entry} view instances returned by these methods preserve
reference
 * behaviour, all methods are fully implemented and supported.
 * </p><p>
 * {@link Set} and it's sub interfaces {@link SortedSet} and 
 * {@link NavigableSet}, return views that preserve reference behaviour,
 * all methods are fully implemented and supported.
 * </p><p>
 * {@link Map} and it's sub interfaces {@link SortedMap}, {@link
NavigableMap},
 * {@link ConcurrentMap} and {@link ConcurrentNavigableMap} return
 * views that preserve reference behaviour, all methods are fully
implemented 
 * and supported.
 * </p><p>      
 * {@link List} returns views that preserve reference behaviour, all
methods are 
 * fully implemented and supported.
 * </p><p>
 * {@link Queue} and it's sub interfaces {@link Deque}, {@link
BlockingQueue} and
 * {@link BlockingDeque} return views that preserve reference behaviour,
 * all methods are fully implemented and supported.
 * </p><p>
 * {@link Iterator} and {@link ListIterator} views preserve reference
behaviour, all methods
 * are fully implemented and supported.
 * </p><p>
 * Serialisation is supported, provided it is also supported by
underlying
 * collections.  Collections are not defensively copied during
de-serialisation,
 * due in part to an inability of determining whether a Comparator is
 * used and in part, that if it is, it prevents Class.newInstance()
construction.
 * </p><p>
 * Note that when a collection is first de-serialised, it's contents are
 * strongly referenced, then changed to the correct reference type. 
This
 * will still occur, even if the Collection is immutable.
 * </p><p>
 * RC stands for Reference Collection and is abbreviated due to the
length of
 * generic parameter arguments typically required.
 * </p>
 * @see Ref
 * @see Referrer
 * @see Reference
 * @author Peter Firmstone.
 */
public class RC {
    private RC(){} // Non instantiable
    
    /**
     * When using a Comparator in SortedSet's and SortedMap's, the
Comparator
     * must be encapsulated using this method, to order the Set or Map 
     * by referents and not References.
     * 
     * @param <T>
     * @param comparator
     * @return
     */
    public static <T> Comparator<Referrer<T>> comparator(Comparator<?
super T> comparator){
        return new ReferenceComparator<T>(comparator);
    }
    
    /**
     * Wrap a Collection for holding references so it appears as a
Collection
     * containing referents.
     * 
     * @param <T>
     * @param internal
     * @param type
     * @return
     */
    public static <T> Collection<T> collection(Collection<Referrer<T>>
internal, Ref type){
        return new ReferenceCollection<T>(internal, type, false);
    }
    
//    /**
//     * The general idea here is, create a factory that produces a the
underlying
//     * reference collection, then it can be used again later to
defensively
//     * produce a new copy of the original collection after
de-serialisation.
//     * 
//     * @param <T>
//     * @param factory
//     * @param type
//     * @return
//     */
//    public static <T> Collection<T>
collection(CollectionFactory<Collection<Referrer<T>>> factory, Ref
type){
//        return new ReferenceCollection<T>(factory.create(), type);
//    }
            
    /**
     * Wrap a List for holding references so it appears as a List
     * containing referents.
     * 
     * @param <T>
     * @param internal
     * @param type
     * @return
     */
    public static <T> List<T> list(List<Referrer<T>> internal, Ref
type){
        return new ReferenceList<T>(internal, type);
    }   
    
    /** 
     * Wrap a Set for holding references so it appears as a Set
     * containing referents.
     * 
     * @param <T>
     * @param internal
     * @param type
     * @return
     */
    public static <T> Set<T> set(Set<Referrer<T>> internal, Ref type){
        return new ReferenceSet<T>(internal, type);
    }
    /**
     * Wrap a SortedSet for holding references so it appears as a
SortedSet
     * containing referents.
     * 
     * @para        m <T>
     * @param internal
     * @param type
     * @return
     */
    public static <T> SortedSet<T> sortedSet(
            SortedSet<Referrer<T>> internal, Ref type){
        return new ReferenceSortedSet<T>(internal, type);
    }
    /**
     * Wrap a NavigableSet for holding references so it appears as a
NavigableSet
     * containing referents.
     * 
     * @param <T>
     * @param internal
     * @param type
     * @return
     */
    public static <T> NavigableSet<T> navigableSet(
            NavigableSet<Referrer<T>> internal, Ref type){
        return new ReferenceNavigableSet<T>(internal, type);
    }
    /**
     * Wrap a Queue for holding references so it appears as a Queue
     * containing referents.
     * 
     * @param <T>
     * @param internal
     * @param type
     * @return
     */
    public static <T> Queue<T> queue(Queue<Referrer<T>> internal, Ref
type){
        return new ReferencedQueue<T>(internal, type);
    }
    /**
     * Wrap a Deque for holding references so it appears as a Deque
     * containing referents.
     * 
     * @param <T>
     * @param internal
     * @param type
     * @return
     */
    public static <T> Deque<T> deque(Deque<Referrer<T>> internal, Ref
type){
        return new ReferenceDeque<T>(internal, type);
    }
    /**
     * Wrap a BlockingQueue for holding references so it appears as a
BlockingQueue
     * containing referents.
     * 
     * @param <T>
     * @param internal
     * @param type
     * @return
     */
    public static <T> BlockingQueue<T> blockingQueue(
            BlockingQueue<Referrer<T>> internal, Ref type){
        return new ReferenceBlockingQueue<T>(internal, type);
    }
    /**
     * Wrap a BlockingDeque for holding references so it appears as a
BlockingDeque
     * containing referents.
     * 
     * @param <T>
     * @param internal
     * @param type
     * @return
     */
    public static <T> BlockingDeque<T> blockingDeque(
            BlockingDeque<Referrer<T>> internal, Ref type){
        return new ReferenceBlockingDeque<T>(internal, type);
    }
    /**
     * Wrap a Map for holding references so it appears as a Map
     * containing referents.
     * 
     * @param <K>
     * @param <V>
     * @param internal
     * @param key
     * @param value
     * @return
     */
    public static <K, V> Map<K, V> map(
            Map<Referrer<K>, Referrer<V>> internal, Ref key, Ref value){
        return new ReferenceMap<K, V>(internal, key, value);
    }
    /**
     * Wrap a SortedMap for holding references so it appears as a
SortedMap
     * containing referents.
     * 
     * @param <K>
     * @param <V>
     * @param internal
     * @param key
     * @param value
     * @return
     */
    public static <K, V> SortedMap<K, V> sortedMap(
            SortedMap<Referrer<K>, Referrer<V>> internal, Ref key, Ref
value){
        return new ReferenceSortedMap<K, V>(internal, key, value);
    }
    /**
     * Wrap a NavigableMap for holding Referrers so it appears as a
NavigableMap
     * containing referents.
     * 
     * @param <K>
     * @param <V>
     * @param internal
     * @param key
     * @param value
     * @return
     */
    public static <K, V> NavigableMap<K, V> navigableMap(
            NavigableMap<Referrer<K>, Referrer<V>> internal, Ref key,
Ref value){
        return new ReferenceNavigableMap<K, V>(internal, key, value);
    }
    /**
     * Wrap a ConcurrentMap for holding references so it appears as a
ConcurrentMap
     * containing referents.
     * 
     * @param <K> - key type.
     * @param <V> - value type.
     * @param internal - for holding references.
     * @param key - key reference type.
     * @param value - value reference type.
     * @return
     */
    public static <K, V> ConcurrentMap<K, V> concurrentMap(
            ConcurrentMap<Referrer<K>, Referrer<V>> internal, Ref key,
Ref value){
        return new ReferenceConcurrentMap<K, V>(internal, key, value);
    }
    
    /**
     * Wrap a ConcurrentNavigableMap for holding references so it
appears as a 
     * ConcurrentNavigableMap containing referents.
     * 
     * @param <K>
     * @param <V>
     * @param internal
     * @param key
     * @param value
     * @return
     */
    public static <K, V> ConcurrentNavigableMap<K, V>
concurrentNavigableMap(
            ConcurrentNavigableMap<Referrer<K>, Referrer<V>> internal,
Ref key, Ref value){
        return new ReferenceConcurrentNavigableMap<K, V>(internal, key,
value);
    }
}



From davidcholmes at aapt.net.au  Wed Jan  4 18:18:28 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 5 Jan 2012 09:18:28 +1000
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <1325717242.2319.100.camel@bluto>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEDCJCAA.davidcholmes@aapt.net.au>

Peter Firmstone writes:
> On Thu, 2012-01-05 at 07:52, Nathan Reynolds wrote:
> > Would the fork-join framework with work steal be an ideal fit?
>
> For ultimate scalability it would, although I'm unable to test for
> scalability, only reason about it and ask questions of developers who do
> have acces to big iron ;)  It might also make sense to use multiple
> ReferenceQueue's for a collection under some circumstances.

I don't see how FJ would be applicable here. What tasks are you forking and
joining?

Using a Thread pool for the cleanup seems like the most scalable approach,
but I think if you are beyond the point where a single cleanup thread
suffices then your design is already in trouble.

David
-----

> I've implemented Reference Collections to fulfill a need that our
> project has, I'm sure very few are aware of its existence, but since it
> has such a compact public API that's very easy to extend to new
> collection interfaces, it could potentially make a very good collections
> based library for using references in your favoured collection
> implementation.  It's also possible to make new types of refernces, eg:
> timer based.  There's no serialized form lock in either, so evolution
> isn't hampered by serialization and serialization can be easily
> supported in a backward compatible evolutionary manner.
>
> If there's enough interest, we could split Reference Collections out as
> a separate library or subproject.
>
> The code can be seen here:
>
> http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolic
> y/src/org/apache/river/impl/util/
>
> The unit tests here:
>
> http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolic
> y/test/src/org/apache/river/impl/util/
>
> Or:
>
> svn checkout (replacing viewvc with repos/asf):
>
> https://svn.apache.org/repos/asf/river/jtsk/skunk/peterConcurrentP
> olicy/src/org/apache/river/impl/util
>
> This is the public API:
>
> /*
>  * Licensed to the Apache Software Foundation (ASF) under one
>  * or more contributor license agreements.  See the NOTICE file
>  * distributed with this work for additional information
>  * regarding copyright ownership. The ASF licenses this file
>  * to you under the Apache License, Version 2.0 (the
>  * "License"); you may not use this file except in compliance
>  * with the License. You may obtain a copy of the License at
>  *
>  *      http://www.apache.org/licenses/LICENSE-2.0
>  *
>  * Unless required by applicable law or agreed to in writing, software
>  * distributed under the License is distributed on an "AS IS" BASIS,
>  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> implied.
>  * See the License for the specific language governing permissions and
>  * limitations under the License.
>  */
>
> package org.apache.river.impl.util;
>
> import java.lang.ref.Reference;
> import java.lang.Comparable;
> import java.util.Collection;
> import java.util.Collections;
> import java.util.Comparator;
> import java.util.Deque;
> import java.util.List;
> import java.util.Map;
> import java.util.Map.Entry;
> import java.util.ListIterator;
> import java.util.NavigableMap;
> import java.util.NavigableSet;
> import java.util.Queue;
> import java.util.Set;
> import java.util.SortedMap;
> import java.util.SortedSet;
> import java.util.concurrent.BlockingDeque;
> import java.util.concurrent.BlockingQueue;
> import java.util.concurrent.ConcurrentMap;
> import java.util.concurrent.ConcurrentNavigableMap;
>
> /**
>  * <p>
>  * This class contains a number of static methods for using and
> abstracting
>  * References in Collections.  Interfaces from the Java Collections
> Framework
>  * are supported.
>  * </p><p>
>  * Referents in these collections may implement {@link Comparable} or
>  * a {@link Comparator} may be used for sorting.  When Comparator's are
> utilised,
>  * they must first be encapsulated {@link
> RC#comparator(java.util.Comparator) },
>  * before passing to a constructor for your preferred underlying
> Collection
>  * implementation.
>  * </p><p>
>  * {@link Comparable} is not supported for IDENTITY == referenced
> Collections,
>  * in this case a Comparator must be used.
>  * </p><p>
>  * All other references support {@link Comparable}, if the referent
> Object
>  * doesn't implement {@link Comparable}, then {@link
> Reference#hashCode()} is used
>  * for sorting.  If two referent Objects have identical hashCodes,
>  * but are unequal and do not implement {@link Comparable}, their
> references
>  * will also have identical hashCodes, so only one of the referents can
>  * be added to a {@link SortedSet} or {@link SortedMap}.  This can be
> fixed by using a
>  * {@link Comparator}.
>  * </p><p>
>  * For all intents and purposes these utilities behave the same as your
> preferred
>  * underlying {@link Collection} implementation, with the exception of
>  * {@link Reference} reachability.  An Object or Key,Value entry is
> removed
>  * from a {@link Collection} or {@link Map}, upon becoming eligible for
>  * garbage collection.
>  * </p><p>
>  * Synchronisation must be implemented by your preferred {@link
> Collection}
>  * and cannot be performed externally to the returned {@link
> Collection}.
>  * Your chosen underlying {@link Collection} must also be mutable.
>  * Objects will be removed automatically from underlying Collections
> when
>  * they are eligible for garbage collection, this breaks external
> synchronisation.
>  * {@link
> CollectionsConcurrent#multiReadCollection(java.util.Collection)} may
>  * be useful for synchronising your chosen underlying {@link
> Collection},
>  * especially if Objects are not being garbage collected often and
> writes
>  * are minimal.
>  * </p><p>
>  * An Unmodifiable wrapper {@link
> Collections#unmodifiableCollection(java.util.Collection)}
>  * may be used externally to prevent additions to the underlying
> Collections,
>  * referents will still be removed as they become unreachable however.
>  * </p><p>
>  * Note that any Sub List, Sub Set or Sub Map obtained by any of the
> Java
>  * Collections Framework interfaces, must be views of the underlying
>  * Collection, if the Collection uses defensive copies instead of views,
>  * References could potentially remain in one copy after garbage
> collection,
>  * causing null returns.  If using standard Java Collections Framework
>  * implementations, these problems don't occur as all Sub Lists,
>  * Sub Sets or Sub Maps are views only.
>  * </p><p>
>  * {@link Map#entrySet() } view instances returned preserve your chosen
> reference
>  * behaviour, they even support {@link Set#add(java.lang.Object)} or
>  * {@link Set#addAll(java.util.Collection)} methods, although you'll be
> hard
>  * pressed to find a standard java implementation that does.  If you
> have a
>  * Map with a Set of Entry's implementing add, the implementation will
> need a
>  * Comparator, that compares Entry's only by their keys, to avoid
> duplicating
>  * keys, primarily because an Entry hashCode includes the both key and
> value in its
>  * calculation. {@link Entry#hashCode() }
>  * </p><p>
>  * All other {@link Map#entrySet() } methods are fully implemented and
> supported.
>  * </p><p>
>  * {@link Entry} view instances returned by these methods preserve
> reference
>  * behaviour, all methods are fully implemented and supported.
>  * </p><p>
>  * {@link Set} and it's sub interfaces {@link SortedSet} and
>  * {@link NavigableSet}, return views that preserve reference behaviour,
>  * all methods are fully implemented and supported.
>  * </p><p>
>  * {@link Map} and it's sub interfaces {@link SortedMap}, {@link
> NavigableMap},
>  * {@link ConcurrentMap} and {@link ConcurrentNavigableMap} return
>  * views that preserve reference behaviour, all methods are fully
> implemented
>  * and supported.
>  * </p><p>
>  * {@link List} returns views that preserve reference behaviour, all
> methods are
>  * fully implemented and supported.
>  * </p><p>
>  * {@link Queue} and it's sub interfaces {@link Deque}, {@link
> BlockingQueue} and
>  * {@link BlockingDeque} return views that preserve reference behaviour,
>  * all methods are fully implemented and supported.
>  * </p><p>
>  * {@link Iterator} and {@link ListIterator} views preserve reference
> behaviour, all methods
>  * are fully implemented and supported.
>  * </p><p>
>  * Serialisation is supported, provided it is also supported by
> underlying
>  * collections.  Collections are not defensively copied during
> de-serialisation,
>  * due in part to an inability of determining whether a Comparator is
>  * used and in part, that if it is, it prevents Class.newInstance()
> construction.
>  * </p><p>
>  * Note that when a collection is first de-serialised, it's contents are
>  * strongly referenced, then changed to the correct reference type.
> This
>  * will still occur, even if the Collection is immutable.
>  * </p><p>
>  * RC stands for Reference Collection and is abbreviated due to the
> length of
>  * generic parameter arguments typically required.
>  * </p>
>  * @see Ref
>  * @see Referrer
>  * @see Reference
>  * @author Peter Firmstone.
>  */
> public class RC {
>     private RC(){} // Non instantiable
>
>     /**
>      * When using a Comparator in SortedSet's and SortedMap's, the
> Comparator
>      * must be encapsulated using this method, to order the Set or Map
>      * by referents and not References.
>      *
>      * @param <T>
>      * @param comparator
>      * @return
>      */
>     public static <T> Comparator<Referrer<T>> comparator(Comparator<?
> super T> comparator){
>         return new ReferenceComparator<T>(comparator);
>     }
>
>     /**
>      * Wrap a Collection for holding references so it appears as a
> Collection
>      * containing referents.
>      *
>      * @param <T>
>      * @param internal
>      * @param type
>      * @return
>      */
>     public static <T> Collection<T> collection(Collection<Referrer<T>>
> internal, Ref type){
>         return new ReferenceCollection<T>(internal, type, false);
>     }
>
> //    /**
> //     * The general idea here is, create a factory that produces a the
> underlying
> //     * reference collection, then it can be used again later to
> defensively
> //     * produce a new copy of the original collection after
> de-serialisation.
> //     *
> //     * @param <T>
> //     * @param factory
> //     * @param type
> //     * @return
> //     */
> //    public static <T> Collection<T>
> collection(CollectionFactory<Collection<Referrer<T>>> factory, Ref
> type){
> //        return new ReferenceCollection<T>(factory.create(), type);
> //    }
>
>     /**
>      * Wrap a List for holding references so it appears as a List
>      * containing referents.
>      *
>      * @param <T>
>      * @param internal
>      * @param type
>      * @return
>      */
>     public static <T> List<T> list(List<Referrer<T>> internal, Ref
> type){
>         return new ReferenceList<T>(internal, type);
>     }
>
>     /**
>      * Wrap a Set for holding references so it appears as a Set
>      * containing referents.
>      *
>      * @param <T>
>      * @param internal
>      * @param type
>      * @return
>      */
>     public static <T> Set<T> set(Set<Referrer<T>> internal, Ref type){
>         return new ReferenceSet<T>(internal, type);
>     }
>     /**
>      * Wrap a SortedSet for holding references so it appears as a
> SortedSet
>      * containing referents.
>      *
>      * @para        m <T>
>      * @param internal
>      * @param type
>      * @return
>      */
>     public static <T> SortedSet<T> sortedSet(
>             SortedSet<Referrer<T>> internal, Ref type){
>         return new ReferenceSortedSet<T>(internal, type);
>     }
>     /**
>      * Wrap a NavigableSet for holding references so it appears as a
> NavigableSet
>      * containing referents.
>      *
>      * @param <T>
>      * @param internal
>      * @param type
>      * @return
>      */
>     public static <T> NavigableSet<T> navigableSet(
>             NavigableSet<Referrer<T>> internal, Ref type){
>         return new ReferenceNavigableSet<T>(internal, type);
>     }
>     /**
>      * Wrap a Queue for holding references so it appears as a Queue
>      * containing referents.
>      *
>      * @param <T>
>      * @param internal
>      * @param type
>      * @return
>      */
>     public static <T> Queue<T> queue(Queue<Referrer<T>> internal, Ref
> type){
>         return new ReferencedQueue<T>(internal, type);
>     }
>     /**
>      * Wrap a Deque for holding references so it appears as a Deque
>      * containing referents.
>      *
>      * @param <T>
>      * @param internal
>      * @param type
>      * @return
>      */
>     public static <T> Deque<T> deque(Deque<Referrer<T>> internal, Ref
> type){
>         return new ReferenceDeque<T>(internal, type);
>     }
>     /**
>      * Wrap a BlockingQueue for holding references so it appears as a
> BlockingQueue
>      * containing referents.
>      *
>      * @param <T>
>      * @param internal
>      * @param type
>      * @return
>      */
>     public static <T> BlockingQueue<T> blockingQueue(
>             BlockingQueue<Referrer<T>> internal, Ref type){
>         return new ReferenceBlockingQueue<T>(internal, type);
>     }
>     /**
>      * Wrap a BlockingDeque for holding references so it appears as a
> BlockingDeque
>      * containing referents.
>      *
>      * @param <T>
>      * @param internal
>      * @param type
>      * @return
>      */
>     public static <T> BlockingDeque<T> blockingDeque(
>             BlockingDeque<Referrer<T>> internal, Ref type){
>         return new ReferenceBlockingDeque<T>(internal, type);
>     }
>     /**
>      * Wrap a Map for holding references so it appears as a Map
>      * containing referents.
>      *
>      * @param <K>
>      * @param <V>
>      * @param internal
>      * @param key
>      * @param value
>      * @return
>      */
>     public static <K, V> Map<K, V> map(
>             Map<Referrer<K>, Referrer<V>> internal, Ref key, Ref value){
>         return new ReferenceMap<K, V>(internal, key, value);
>     }
>     /**
>      * Wrap a SortedMap for holding references so it appears as a
> SortedMap
>      * containing referents.
>      *
>      * @param <K>
>      * @param <V>
>      * @param internal
>      * @param key
>      * @param value
>      * @return
>      */
>     public static <K, V> SortedMap<K, V> sortedMap(
>             SortedMap<Referrer<K>, Referrer<V>> internal, Ref key, Ref
> value){
>         return new ReferenceSortedMap<K, V>(internal, key, value);
>     }
>     /**
>      * Wrap a NavigableMap for holding Referrers so it appears as a
> NavigableMap
>      * containing referents.
>      *
>      * @param <K>
>      * @param <V>
>      * @param internal
>      * @param key
>      * @param value
>      * @return
>      */
>     public static <K, V> NavigableMap<K, V> navigableMap(
>             NavigableMap<Referrer<K>, Referrer<V>> internal, Ref key,
> Ref value){
>         return new ReferenceNavigableMap<K, V>(internal, key, value);
>     }
>     /**
>      * Wrap a ConcurrentMap for holding references so it appears as a
> ConcurrentMap
>      * containing referents.
>      *
>      * @param <K> - key type.
>      * @param <V> - value type.
>      * @param internal - for holding references.
>      * @param key - key reference type.
>      * @param value - value reference type.
>      * @return
>      */
>     public static <K, V> ConcurrentMap<K, V> concurrentMap(
>             ConcurrentMap<Referrer<K>, Referrer<V>> internal, Ref key,
> Ref value){
>         return new ReferenceConcurrentMap<K, V>(internal, key, value);
>     }
>
>     /**
>      * Wrap a ConcurrentNavigableMap for holding references so it
> appears as a
>      * ConcurrentNavigableMap containing referents.
>      *
>      * @param <K>
>      * @param <V>
>      * @param internal
>      * @param key
>      * @param value
>      * @return
>      */
>     public static <K, V> ConcurrentNavigableMap<K, V>
> concurrentNavigableMap(
>             ConcurrentNavigableMap<Referrer<K>, Referrer<V>> internal,
> Ref key, Ref value){
>         return new ReferenceConcurrentNavigableMap<K, V>(internal, key,
> value);
>     }
> }
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From peter.firmstone at zeus.net.au  Wed Jan  4 20:42:06 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Thu, 05 Jan 2012 11:42:06 +1000
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEDCJCAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCEDCJCAA.davidcholmes@aapt.net.au>
Message-ID: <1325727726.2319.178.camel@bluto>

Thanks David,

...Hmm you're speaking from experience, had the privilege of meeting
briefly while you were at Sun in Brisbane.

Users will be given a choice of one dedicated cleanup thread (per
Collection) which waits on the ReferenceQueue OR CAS with
ReentrantLock.tryLock() to share cleanup between callers.

CAS for single or few threads and less memory consumption, the dedicated
cleanup thread for scalability.

Note: Maps will have up to two ReferenceQueue's and two dedicated
threads, if based on the existing design. EG: Weak Keys and Soft Values
or Weak Keys and Values, when there's a circular reference from the
value to the key.  Weak keys with strongly reference values will only
have one ReferenceQueue and one cleanup Thread.

Reference Collection's can be widely applied to any java collection
type, my current concern is scalability for large collections (including
concurrent maps) which I have no way of testing.

In the implementation, the only mutable state is in ReferenceQueue, the
underlying collection and Reference, everything else is final or uses
method variables.  Dummy References, used for reads, are created and
discarded and die young, so never enter main memory.  Hopefully the
majority of state should be in cache during execution.  My main concern
is the current design employed a ReentrantLock.tryLock() call to hand
maintenance to one of the calling threads, I was worried that this would
limit scalability.  The feedback so far is yes (info much appreciated).

If ReferenceQueue.remove() is only called by one thread, the calling
threads no longer need to perform cleanup, it's done by the maintenance
thread, which is blocked on remove, so doesn't consume cpu unless
required, although relative to the number of collections in use, will
consume more memory.

Nathan suggested more than one thread might be needed to perform
cleanup, due to experiences with jrocket's finalizer.  This situation
could occur briefly if a large number objects suddenly became reachable,
such as a large collection using soft references, when the jvm is low on
memory.  We were discussing work stealing, but I guess more concurrency
on a bottlenecking underlying collection won't help.

The symptom seen by calling threads is occasional null return values in
iterators.  This isn't a problem if the caller expects a null.  I'd
imagine the cleanup thread will catch up. ReferenceComparator
encapsulates existing Comparator's and guards against null.  This could
be a problem for existing code if the collection is used as a drop in
replacement.

My experience so far has been that null returns are easy to deal with in
new code.

What are your thoughts?

Regards,

Peter.

On Thu, 2012-01-05 at 09:18, David Holmes wrote:
> Peter Firmstone writes:
> > On Thu, 2012-01-05 at 07:52, Nathan Reynolds wrote:
> > > Would the fork-join framework with work steal be an ideal fit?
> >
> > For ultimate scalability it would, although I'm unable to test for
> > scalability, only reason about it and ask questions of developers who do
> > have acces to big iron ;)  It might also make sense to use multiple
> > ReferenceQueue's for a collection under some circumstances.
> 
> I don't see how FJ would be applicable here. What tasks are you forking and
> joining?
> 
> Using a Thread pool for the cleanup seems like the most scalable approach,
> but I think if you are beyond the point where a single cleanup thread
> suffices then your design is already in trouble.
> 
> David
> -----
> 
> > I've implemented Reference Collections to fulfill a need that our
> > project has, I'm sure very few are aware of its existence, but since it
> > has such a compact public API that's very easy to extend to new
> > collection interfaces, it could potentially make a very good collections
> > based library for using references in your favoured collection
> > implementation.  It's also possible to make new types of refernces, eg:
> > timer based.  There's no serialized form lock in either, so evolution
> > isn't hampered by serialization and serialization can be easily
> > supported in a backward compatible evolutionary manner.
> >
> > If there's enough interest, we could split Reference Collections out as
> > a separate library or subproject.
> >
> > The code can be seen here:
> >
> > http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolic
> > y/src/org/apache/river/impl/util/
> >
> > The unit tests here:
> >
> > http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolic
> > y/test/src/org/apache/river/impl/util/
> >
> > Or:
> >
> > svn checkout (replacing viewvc with repos/asf):
> >
> > https://svn.apache.org/repos/asf/river/jtsk/skunk/peterConcurrentP
> > olicy/src/org/apache/river/impl/util
> >
> > This is the public API:
> >
> > /*
> >  * Licensed to the Apache Software Foundation (ASF) under one
> >  * or more contributor license agreements.  See the NOTICE file
> >  * distributed with this work for additional information
> >  * regarding copyright ownership. The ASF licenses this file
> >  * to you under the Apache License, Version 2.0 (the
> >  * "License"); you may not use this file except in compliance
> >  * with the License. You may obtain a copy of the License at
> >  *
> >  *      http://www.apache.org/licenses/LICENSE-2.0
> >  *
> >  * Unless required by applicable law or agreed to in writing, software
> >  * distributed under the License is distributed on an "AS IS" BASIS,
> >  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> > implied.
> >  * See the License for the specific language governing permissions and
> >  * limitations under the License.
> >  */
> >
> > package org.apache.river.impl.util;
> >
> > import java.lang.ref.Reference;
> > import java.lang.Comparable;
> > import java.util.Collection;
> > import java.util.Collections;
> > import java.util.Comparator;
> > import java.util.Deque;
> > import java.util.List;
> > import java.util.Map;
> > import java.util.Map.Entry;
> > import java.util.ListIterator;
> > import java.util.NavigableMap;
> > import java.util.NavigableSet;
> > import java.util.Queue;
> > import java.util.Set;
> > import java.util.SortedMap;
> > import java.util.SortedSet;
> > import java.util.concurrent.BlockingDeque;
> > import java.util.concurrent.BlockingQueue;
> > import java.util.concurrent.ConcurrentMap;
> > import java.util.concurrent.ConcurrentNavigableMap;
> >
> > /**
> >  * <p>
> >  * This class contains a number of static methods for using and
> > abstracting
> >  * References in Collections.  Interfaces from the Java Collections
> > Framework
> >  * are supported.
> >  * </p><p>
> >  * Referents in these collections may implement {@link Comparable} or
> >  * a {@link Comparator} may be used for sorting.  When Comparator's are
> > utilised,
> >  * they must first be encapsulated {@link
> > RC#comparator(java.util.Comparator) },
> >  * before passing to a constructor for your preferred underlying
> > Collection
> >  * implementation.
> >  * </p><p>
> >  * {@link Comparable} is not supported for IDENTITY == referenced
> > Collections,
> >  * in this case a Comparator must be used.
> >  * </p><p>
> >  * All other references support {@link Comparable}, if the referent
> > Object
> >  * doesn't implement {@link Comparable}, then {@link
> > Reference#hashCode()} is used
> >  * for sorting.  If two referent Objects have identical hashCodes,
> >  * but are unequal and do not implement {@link Comparable}, their
> > references
> >  * will also have identical hashCodes, so only one of the referents can
> >  * be added to a {@link SortedSet} or {@link SortedMap}.  This can be
> > fixed by using a
> >  * {@link Comparator}.
> >  * </p><p>
> >  * For all intents and purposes these utilities behave the same as your
> > preferred
> >  * underlying {@link Collection} implementation, with the exception of
> >  * {@link Reference} reachability.  An Object or Key,Value entry is
> > removed
> >  * from a {@link Collection} or {@link Map}, upon becoming eligible for
> >  * garbage collection.
> >  * </p><p>
> >  * Synchronisation must be implemented by your preferred {@link
> > Collection}
> >  * and cannot be performed externally to the returned {@link
> > Collection}.
> >  * Your chosen underlying {@link Collection} must also be mutable.
> >  * Objects will be removed automatically from underlying Collections
> > when
> >  * they are eligible for garbage collection, this breaks external
> > synchronisation.
> >  * {@link
> > CollectionsConcurrent#multiReadCollection(java.util.Collection)} may
> >  * be useful for synchronising your chosen underlying {@link
> > Collection},
> >  * especially if Objects are not being garbage collected often and
> > writes
> >  * are minimal.
> >  * </p><p>
> >  * An Unmodifiable wrapper {@link
> > Collections#unmodifiableCollection(java.util.Collection)}
> >  * may be used externally to prevent additions to the underlying
> > Collections,
> >  * referents will still be removed as they become unreachable however.
> >  * </p><p>
> >  * Note that any Sub List, Sub Set or Sub Map obtained by any of the
> > Java
> >  * Collections Framework interfaces, must be views of the underlying
> >  * Collection, if the Collection uses defensive copies instead of views,
> >  * References could potentially remain in one copy after garbage
> > collection,
> >  * causing null returns.  If using standard Java Collections Framework
> >  * implementations, these problems don't occur as all Sub Lists,
> >  * Sub Sets or Sub Maps are views only.
> >  * </p><p>
> >  * {@link Map#entrySet() } view instances returned preserve your chosen
> > reference
> >  * behaviour, they even support {@link Set#add(java.lang.Object)} or
> >  * {@link Set#addAll(java.util.Collection)} methods, although you'll be
> > hard
> >  * pressed to find a standard java implementation that does.  If you
> > have a
> >  * Map with a Set of Entry's implementing add, the implementation will
> > need a
> >  * Comparator, that compares Entry's only by their keys, to avoid
> > duplicating
> >  * keys, primarily because an Entry hashCode includes the both key and
> > value in its
> >  * calculation. {@link Entry#hashCode() }
> >  * </p><p>
> >  * All other {@link Map#entrySet() } methods are fully implemented and
> > supported.
> >  * </p><p>
> >  * {@link Entry} view instances returned by these methods preserve
> > reference
> >  * behaviour, all methods are fully implemented and supported.
> >  * </p><p>
> >  * {@link Set} and it's sub interfaces {@link SortedSet} and
> >  * {@link NavigableSet}, return views that preserve reference behaviour,
> >  * all methods are fully implemented and supported.
> >  * </p><p>
> >  * {@link Map} and it's sub interfaces {@link SortedMap}, {@link
> > NavigableMap},
> >  * {@link ConcurrentMap} and {@link ConcurrentNavigableMap} return
> >  * views that preserve reference behaviour, all methods are fully
> > implemented
> >  * and supported.
> >  * </p><p>
> >  * {@link List} returns views that preserve reference behaviour, all
> > methods are
> >  * fully implemented and supported.
> >  * </p><p>
> >  * {@link Queue} and it's sub interfaces {@link Deque}, {@link
> > BlockingQueue} and
> >  * {@link BlockingDeque} return views that preserve reference behaviour,
> >  * all methods are fully implemented and supported.
> >  * </p><p>
> >  * {@link Iterator} and {@link ListIterator} views preserve reference
> > behaviour, all methods
> >  * are fully implemented and supported.
> >  * </p><p>
> >  * Serialisation is supported, provided it is also supported by
> > underlying
> >  * collections.  Collections are not defensively copied during
> > de-serialisation,
> >  * due in part to an inability of determining whether a Comparator is
> >  * used and in part, that if it is, it prevents Class.newInstance()
> > construction.
> >  * </p><p>
> >  * Note that when a collection is first de-serialised, it's contents are
> >  * strongly referenced, then changed to the correct reference type.
> > This
> >  * will still occur, even if the Collection is immutable.
> >  * </p><p>
> >  * RC stands for Reference Collection and is abbreviated due to the
> > length of
> >  * generic parameter arguments typically required.
> >  * </p>
> >  * @see Ref
> >  * @see Referrer
> >  * @see Reference
> >  * @author Peter Firmstone.
> >  */
> > public class RC {
> >     private RC(){} // Non instantiable
> >
> >     /**
> >      * When using a Comparator in SortedSet's and SortedMap's, the
> > Comparator
> >      * must be encapsulated using this method, to order the Set or Map
> >      * by referents and not References.
> >      *
> >      * @param <T>
> >      * @param comparator
> >      * @return
> >      */
> >     public static <T> Comparator<Referrer<T>> comparator(Comparator<?
> > super T> comparator){
> >         return new ReferenceComparator<T>(comparator);
> >     }
> >
> >     /**
> >      * Wrap a Collection for holding references so it appears as a
> > Collection
> >      * containing referents.
> >      *
> >      * @param <T>
> >      * @param internal
> >      * @param type
> >      * @return
> >      */
> >     public static <T> Collection<T> collection(Collection<Referrer<T>>
> > internal, Ref type){
> >         return new ReferenceCollection<T>(internal, type, false);
> >     }
> >
> > //    /**
> > //     * The general idea here is, create a factory that produces a the
> > underlying
> > //     * reference collection, then it can be used again later to
> > defensively
> > //     * produce a new copy of the original collection after
> > de-serialisation.
> > //     *
> > //     * @param <T>
> > //     * @param factory
> > //     * @param type
> > //     * @return
> > //     */
> > //    public static <T> Collection<T>
> > collection(CollectionFactory<Collection<Referrer<T>>> factory, Ref
> > type){
> > //        return new ReferenceCollection<T>(factory.create(), type);
> > //    }
> >
> >     /**
> >      * Wrap a List for holding references so it appears as a List
> >      * containing referents.
> >      *
> >      * @param <T>
> >      * @param internal
> >      * @param type
> >      * @return
> >      */
> >     public static <T> List<T> list(List<Referrer<T>> internal, Ref
> > type){
> >         return new ReferenceList<T>(internal, type);
> >     }
> >
> >     /**
> >      * Wrap a Set for holding references so it appears as a Set
> >      * containing referents.
> >      *
> >      * @param <T>
> >      * @param internal
> >      * @param type
> >      * @return
> >      */
> >     public static <T> Set<T> set(Set<Referrer<T>> internal, Ref type){
> >         return new ReferenceSet<T>(internal, type);
> >     }
> >     /**
> >      * Wrap a SortedSet for holding references so it appears as a
> > SortedSet
> >      * containing referents.
> >      *
> >      * @para        m <T>
> >      * @param internal
> >      * @param type
> >      * @return
> >      */
> >     public static <T> SortedSet<T> sortedSet(
> >             SortedSet<Referrer<T>> internal, Ref type){
> >         return new ReferenceSortedSet<T>(internal, type);
> >     }
> >     /**
> >      * Wrap a NavigableSet for holding references so it appears as a
> > NavigableSet
> >      * containing referents.
> >      *
> >      * @param <T>
> >      * @param internal
> >      * @param type
> >      * @return
> >      */
> >     public static <T> NavigableSet<T> navigableSet(
> >             NavigableSet<Referrer<T>> internal, Ref type){
> >         return new ReferenceNavigableSet<T>(internal, type);
> >     }
> >     /**
> >      * Wrap a Queue for holding references so it appears as a Queue
> >      * containing referents.
> >      *
> >      * @param <T>
> >      * @param internal
> >      * @param type
> >      * @return
> >      */
> >     public static <T> Queue<T> queue(Queue<Referrer<T>> internal, Ref
> > type){
> >         return new ReferencedQueue<T>(internal, type);
> >     }
> >     /**
> >      * Wrap a Deque for holding references so it appears as a Deque
> >      * containing referents.
> >      *
> >      * @param <T>
> >      * @param internal
> >      * @param type
> >      * @return
> >      */
> >     public static <T> Deque<T> deque(Deque<Referrer<T>> internal, Ref
> > type){
> >         return new ReferenceDeque<T>(internal, type);
> >     }
> >     /**
> >      * Wrap a BlockingQueue for holding references so it appears as a
> > BlockingQueue
> >      * containing referents.
> >      *
> >      * @param <T>
> >      * @param internal
> >      * @param type
> >      * @return
> >      */
> >     public static <T> BlockingQueue<T> blockingQueue(
> >             BlockingQueue<Referrer<T>> internal, Ref type){
> >         return new ReferenceBlockingQueue<T>(internal, type);
> >     }
> >     /**
> >      * Wrap a BlockingDeque for holding references so it appears as a
> > BlockingDeque
> >      * containing referents.
> >      *
> >      * @param <T>
> >      * @param internal
> >      * @param type
> >      * @return
> >      */
> >     public static <T> BlockingDeque<T> blockingDeque(
> >             BlockingDeque<Referrer<T>> internal, Ref type){
> >         return new ReferenceBlockingDeque<T>(internal, type);
> >     }
> >     /**
> >      * Wrap a Map for holding references so it appears as a Map
> >      * containing referents.
> >      *
> >      * @param <K>
> >      * @param <V>
> >      * @param internal
> >      * @param key
> >      * @param value
> >      * @return
> >      */
> >     public static <K, V> Map<K, V> map(
> >             Map<Referrer<K>, Referrer<V>> internal, Ref key, Ref value){
> >         return new ReferenceMap<K, V>(internal, key, value);
> >     }
> >     /**
> >      * Wrap a SortedMap for holding references so it appears as a
> > SortedMap
> >      * containing referents.
> >      *
> >      * @param <K>
> >      * @param <V>
> >      * @param internal
> >      * @param key
> >      * @param value
> >      * @return
> >      */
> >     public static <K, V> SortedMap<K, V> sortedMap(
> >             SortedMap<Referrer<K>, Referrer<V>> internal, Ref key, Ref
> > value){
> >         return new ReferenceSortedMap<K, V>(internal, key, value);
> >     }
> >     /**
> >      * Wrap a NavigableMap for holding Referrers so it appears as a
> > NavigableMap
> >      * containing referents.
> >      *
> >      * @param <K>
> >      * @param <V>
> >      * @param internal
> >      * @param key
> >      * @param value
> >      * @return
> >      */
> >     public static <K, V> NavigableMap<K, V> navigableMap(
> >             NavigableMap<Referrer<K>, Referrer<V>> internal, Ref key,
> > Ref value){
> >         return new ReferenceNavigableMap<K, V>(internal, key, value);
> >     }
> >     /**
> >      * Wrap a ConcurrentMap for holding references so it appears as a
> > ConcurrentMap
> >      * containing referents.
> >      *
> >      * @param <K> - key type.
> >      * @param <V> - value type.
> >      * @param internal - for holding references.
> >      * @param key - key reference type.
> >      * @param value - value reference type.
> >      * @return
> >      */
> >     public static <K, V> ConcurrentMap<K, V> concurrentMap(
> >             ConcurrentMap<Referrer<K>, Referrer<V>> internal, Ref key,
> > Ref value){
> >         return new ReferenceConcurrentMap<K, V>(internal, key, value);
> >     }
> >
> >     /**
> >      * Wrap a ConcurrentNavigableMap for holding references so it
> > appears as a
> >      * ConcurrentNavigableMap containing referents.
> >      *
> >      * @param <K>
> >      * @param <V>
> >      * @param internal
> >      * @param key
> >      * @param value
> >      * @return
> >      */
> >     public static <K, V> ConcurrentNavigableMap<K, V>
> > concurrentNavigableMap(
> >             ConcurrentNavigableMap<Referrer<K>, Referrer<V>> internal,
> > Ref key, Ref value){
> >         return new ReferenceConcurrentNavigableMap<K, V>(internal, key,
> > value);
> >     }
> > }
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> 


From peter.firmstone at zeus.net.au  Thu Jan  5 00:08:33 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Thu, 05 Jan 2012 15:08:33 +1000
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <1325727726.2319.178.camel@bluto>
References: <NFBBKALFDCPFIDBNKAPCCEDCJCAA.davidcholmes@aapt.net.au>
	<1325727726.2319.178.camel@bluto>
Message-ID: <1325740113.2319.277.camel@bluto>

I think there's a case for sharing a ReferenceQueue and cleanup thread
between collections too, as Nathan suggested, to economise on memory and
avoid creating too many threads.

The ReferenceQueuingFactory is an interface for encapsulating the
functionality, it'll take some refactoring to add option 2 below.

Providing users with three choices:

     1. Making calling threads responsible for reference removal, for
        minimal number of threads / load.
     2. Sharing a ReferenceQueuingFactory among collections.
     3. One ReferenceQueuingFactory per collection.

There is just one catch, to share a ReferenceQueuingFactory, the
referents must be of the same type.

Option 2, also exposes some more of the internal api, in order to share
a thread and queue, the ReferenceQueuingFactory would need to be passed
in at construction, in addition, the Reference would need to contain a
copy of the collection it will be removed from.

Alternatively how reliable (depending on the platform) is it to minimise
the stack size of of the garbage collection thread, say to 128k reducing
memory consumption?  The javadoc says it can be ignored completely.

Using the following Thread constructor option (copied from javadoc):

Thread
public Thread(ThreadGroup group,
              Runnable target,
              String name,
              long stackSize)
Allocates a new Thread object so that it has target as its run object,
has the specified name as its name, belongs to the thread group referred
to by group, and has the specified stack size.
This constructor is identical to Thread(ThreadGroup,Runnable,String)
with the exception of the fact that it allows the thread stack size to
be specified. The stack size is the approximate number of bytes of
address space that the virtual machine is to allocate for this thread's
stack. The effect of the stackSize parameter, if any, is highly platform
dependent.
On some platforms, specifying a higher value for the stackSize parameter
may allow a thread to achieve greater recursion depth before throwing a
StackOverflowError. Similarly, specifying a lower value may allow a
greater number of threads to exist concurrently without throwing an
OutOfMemoryError (or other internal error). The details of the
relationship between the value of the stackSize parameter and the
maximum recursion depth and concurrency level are platform-dependent. On
some platforms, the value of the stackSize parameter may have no effect
whatsoever.
The virtual machine is free to treat the stackSize parameter as a
suggestion. If the specified value is unreasonably low for the platform,
the virtual machine may instead use some platform-specific minimum
value; if the specified value is unreasonably high, the virtual machine
may instead use some platform-specific maximum. Likewise, the virtual
machine is free to round the specified value up or down as it sees fit
(or to ignore it completely).
Specifying a value of zero for the stackSize parameter will cause this
constructor to behave exactly like the Thread(ThreadGroup, Runnable,
String) constructor.
Due to the platform-dependent nature of the behavior of this
constructor, extreme care should be exercised in its use. The thread
stack size necessary to perform a given computation will likely vary
from one JRE implementation to another. In light of this variation,
careful tuning of the stack size parameter may be required, and the
tuning may need to be repeated for each JRE implementation on which an
application is to run.
Implementation note: Java platform implementers are encouraged to
document their implementation's behavior with respect to the stackSize
parameter.



From stanimir at riflexo.com  Thu Jan  5 06:13:42 2012
From: stanimir at riflexo.com (bestsss)
Date: Thu, 5 Jan 2012 03:13:42 -0800 (PST)
Subject: [concurrency-interest] Re ference Collections
In-Reply-To: <1325727726.2319.178.camel@bluto>
References: <1325678569.21868.167.camel@bluto>
	<1325709333.2319.35.camel@bluto> <4F04BC94.1090704@oracle.com>
	<1325713129.2319.61.camel@bluto> <1325717242.2319.100.camel@bluto>
	<NFBBKALFDCPFIDBNKAPCCEDCJCAA.davidcholmes@aapt.net.au>
	<1325727726.2319.178.camel@bluto>
Message-ID: <33084757.post@talk.nabble.com>


 I am not sure how differently weak/soft references are handled by the GC.
Yet creating tons of them might have adverse effects, esp if the real key is
GC'd before the wrapping references.  That will create a lot of work for the
ReferenceHandler thread.

Ideally you may wish to skip wrapping keys on get() but in that case the
default impl. of CHM not suffice. A possible mitigation of the problem via a
non-reference class (just extending Object) that has equals/hashCode the
same as the wrapping references.

My understanding of the ReferenceMap would be that it should not affect
normal operation if it's not expunged at all besides creating leaks. If
that's true you may wish just to check System.currentTimeMillis and expunge
once a (few) seconds.

Few other musings: 
* ReferenceQueue is actually a stack but still called queue, indeed it's
filled from another stack (ReferenceHandler) but still it acts as stack. 
Unfortunately, it doesn't have have drain() method that looks like a good
optimization acquiring the monitor just once and reducing the contention of
the fast path (head!=null). Imo, the code would be better off 1st draining
the queue in temporary collection, reducing the possible contention and then
performing the real expunge, increasing the odds to coarsen the monitor. The
queue also keeps internal size that's not published anywhere.

* SoftReferences are sort of hit and miss - very cheap to make memory caches
and quite unreliable under load or when keeping references to large native
objects like direct ByteBuffer or java.util.zip.Deflater. There is no
contract when their reclamation comes so the application may show quite
different behavior but just chagning the garbage collector.
 

Peter Firmstone-3 wrote:
> 
> ...
> CAS for single or few threads and less memory consumption, the dedicated
> cleanup thread for scalability.
> 
> Reference Collection's can be widely applied to any java collection
> type, my current concern is scalability for large collections (including
> concurrent maps) which I have no way of testing.
> 
> In the implementation, the only mutable state is in ReferenceQueue, the
> underlying collection and Reference, everything else is final or uses
> method variables.  Dummy References, used for reads, are created and
> discarded and die young, so never enter main memory.  Hopefully the
> majority of state should be in cache during execution.  My main concern
> is the current design employed a ReentrantLock.tryLock() call to hand
> maintenance to one of the calling threads, I was worried that this would
> limit scalability.  The feedback so far is yes (info much appreciated).
> ...
> The symptom seen by calling threads is occasional null return values in
> iterators.  This isn't a problem if the caller expects a null.  I'd
> imagine the cleanup thread will catch up. ReferenceComparator
> encapsulates existing Comparator's and guards against null.  This could
> be a problem for existing code if the collection is used as a drop in
> replacement.
> 
> 

-- 
View this message in context: http://old.nabble.com/Reference-Collections-tp33078668p33084757.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From peter.firmstone at zeus.net.au  Thu Jan  5 09:07:51 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Fri, 06 Jan 2012 00:07:51 +1000
Subject: [concurrency-interest] Reference Collections
Message-ID: <1325772471.6784.60.camel@bluto>

Thinking about it further, the best solution for cleaning reference
collections is using a static final ScheduledExecutor as part of the
ReferenceProcessor class.

That way if all the reference collections in use only require one
thread, then only one shared thread will be created.

Each Collection will still get its own ReferenceQueue, it keeps the code
simple and allows the use of multiple collections to scale, since
reference queues would be polled only when running as scheduled tasks,
there's no contention for the ReferenceQueue lock, instead the threads
increase if the load increases and there a plenty of queues to share
among the threads.

A disadvantage is the ReferenceProcessor needs a finalizer to cancel the
task in the scheduled executor when no longer required.

Example: ConcurrentMap<AccessControContext,Set<Permission>> checked,
using weak keys referring to ConcurrentSkipListSet's containing soft
references.

In this case there would be one ReferenceQueue for the keys and one for
each of the Set's.

The path for a calling thread that doesn't add then becomes, create
referrer object, perform get, remove, contains, then return, discarding
the referrer.  Reference object instances are only created for put,
putIfAbsent, add, etc - write methods, in this case the ReferenceQueue
is passed to the Reference constructor, the Reference added to the
collection then the method returns.

Do you think this strategy will scale?

Since Reference Collection's might still be used by client developers
for single threaded use, I'll probably keep the CAS tryLock() option, in
case the underlying collection in use doesn't support multi threading.

Thanks in advance,

Peter.


From viktor.klang at gmail.com  Thu Jan  5 09:45:49 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 5 Jan 2012 15:45:49 +0100
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <1325772471.6784.60.camel@bluto>
References: <1325772471.6784.60.camel@bluto>
Message-ID: <CANPzfU-pBrhdA7eJXja2wNKc3g8ZK2Q3+OVC+Sy2jOoKJCtHHA@mail.gmail.com>

There's no guarantees whatsoever that it will get enough slices to keep up
with the pressure.
On Jan 5, 2012 3:21 PM, "Peter Firmstone" <peter.firmstone at zeus.net.au>
wrote:

> Thinking about it further, the best solution for cleaning reference
> collections is using a static final ScheduledExecutor as part of the
> ReferenceProcessor class.
>
> That way if all the reference collections in use only require one
> thread, then only one shared thread will be created.
>
> Each Collection will still get its own ReferenceQueue, it keeps the code
> simple and allows the use of multiple collections to scale, since
> reference queues would be polled only when running as scheduled tasks,
> there's no contention for the ReferenceQueue lock, instead the threads
> increase if the load increases and there a plenty of queues to share
> among the threads.
>
> A disadvantage is the ReferenceProcessor needs a finalizer to cancel the
> task in the scheduled executor when no longer required.
>
> Example: ConcurrentMap<AccessControContext,Set<Permission>> checked,
> using weak keys referring to ConcurrentSkipListSet's containing soft
> references.
>
> In this case there would be one ReferenceQueue for the keys and one for
> each of the Set's.
>
> The path for a calling thread that doesn't add then becomes, create
> referrer object, perform get, remove, contains, then return, discarding
> the referrer.  Reference object instances are only created for put,
> putIfAbsent, add, etc - write methods, in this case the ReferenceQueue
> is passed to the Reference constructor, the Reference added to the
> collection then the method returns.
>
> Do you think this strategy will scale?
>
> Since Reference Collection's might still be used by client developers
> for single threaded use, I'll probably keep the CAS tryLock() option, in
> case the underlying collection in use doesn't support multi threading.
>
> Thanks in advance,
>
> Peter.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/9be2b458/attachment.html>

From nathan.reynolds at oracle.com  Thu Jan  5 11:19:34 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 05 Jan 2012 09:19:34 -0700
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <1325727726.2319.178.camel@bluto>
References: <NFBBKALFDCPFIDBNKAPCCEDCJCAA.davidcholmes@aapt.net.au>
	<1325727726.2319.178.camel@bluto>
Message-ID: <4F05CD96.8080108@oracle.com>

 >  The symptom seen by calling threads is occasional null return values 
in iterators.

You can eliminate this problem by caching the next value in hasNext().  
Here's an iterator which removes nulls from the source iterator.

public class NoNullIterator<T> implements Iterator<T>
{
    private final Iterator<T> m_source;
    private       T           m_next;

    public NoNullIterator(Iterator<T> source)
    {
       if (source == null)
          throw new NullPointerException();

       m_source = source;
    }

    public boolean hasNext()
    {
       T next;

       if (m_next != null)
          return(true);

       while (m_source.hasNext())
       {
          m_next = m_source.next();

          if (m_next != null)
             return(true);
       }

       return(false);
    }

    public T next()
    {
       T result;

       if (m_next == null)
          if (!hasNext())
             throw new IllegalStateException();

       result = m_next;
       m_next = null;

       return(result);
    }

    public void remove()
    {
       m_source.remove();
    }
}

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/4/2012 6:42 PM, Peter Firmstone wrote:
> Thanks David,
>
> ...Hmm you're speaking from experience, had the privilege of meeting
> briefly while you were at Sun in Brisbane.
>
> Users will be given a choice of one dedicated cleanup thread (per
> Collection) which waits on the ReferenceQueue OR CAS with
> ReentrantLock.tryLock() to share cleanup between callers.
>
> CAS for single or few threads and less memory consumption, the dedicated
> cleanup thread for scalability.
>
> Note: Maps will have up to two ReferenceQueue's and two dedicated
> threads, if based on the existing design. EG: Weak Keys and Soft Values
> or Weak Keys and Values, when there's a circular reference from the
> value to the key.  Weak keys with strongly reference values will only
> have one ReferenceQueue and one cleanup Thread.
>
> Reference Collection's can be widely applied to any java collection
> type, my current concern is scalability for large collections (including
> concurrent maps) which I have no way of testing.
>
> In the implementation, the only mutable state is in ReferenceQueue, the
> underlying collection and Reference, everything else is final or uses
> method variables.  Dummy References, used for reads, are created and
> discarded and die young, so never enter main memory.  Hopefully the
> majority of state should be in cache during execution.  My main concern
> is the current design employed a ReentrantLock.tryLock() call to hand
> maintenance to one of the calling threads, I was worried that this would
> limit scalability.  The feedback so far is yes (info much appreciated).
>
> If ReferenceQueue.remove() is only called by one thread, the calling
> threads no longer need to perform cleanup, it's done by the maintenance
> thread, which is blocked on remove, so doesn't consume cpu unless
> required, although relative to the number of collections in use, will
> consume more memory.
>
> Nathan suggested more than one thread might be needed to perform
> cleanup, due to experiences with jrocket's finalizer.  This situation
> could occur briefly if a large number objects suddenly became reachable,
> such as a large collection using soft references, when the jvm is low on
> memory.  We were discussing work stealing, but I guess more concurrency
> on a bottlenecking underlying collection won't help.
>
> The symptom seen by calling threads is occasional null return values in
> iterators.  This isn't a problem if the caller expects a null.  I'd
> imagine the cleanup thread will catch up. ReferenceComparator
> encapsulates existing Comparator's and guards against null.  This could
> be a problem for existing code if the collection is used as a drop in
> replacement.
>
> My experience so far has been that null returns are easy to deal with in
> new code.
>
> What are your thoughts?
>
> Regards,
>
> Peter.
>
> On Thu, 2012-01-05 at 09:18, David Holmes wrote:
>> Peter Firmstone writes:
>>> On Thu, 2012-01-05 at 07:52, Nathan Reynolds wrote:
>>>> Would the fork-join framework with work steal be an ideal fit?
>>> For ultimate scalability it would, although I'm unable to test for
>>> scalability, only reason about it and ask questions of developers who do
>>> have acces to big iron ;)  It might also make sense to use multiple
>>> ReferenceQueue's for a collection under some circumstances.
>> I don't see how FJ would be applicable here. What tasks are you forking and
>> joining?
>>
>> Using a Thread pool for the cleanup seems like the most scalable approach,
>> but I think if you are beyond the point where a single cleanup thread
>> suffices then your design is already in trouble.
>>
>> David
>> -----
>>
>>> I've implemented Reference Collections to fulfill a need that our
>>> project has, I'm sure very few are aware of its existence, but since it
>>> has such a compact public API that's very easy to extend to new
>>> collection interfaces, it could potentially make a very good collections
>>> based library for using references in your favoured collection
>>> implementation.  It's also possible to make new types of refernces, eg:
>>> timer based.  There's no serialized form lock in either, so evolution
>>> isn't hampered by serialization and serialization can be easily
>>> supported in a backward compatible evolutionary manner.
>>>
>>> If there's enough interest, we could split Reference Collections out as
>>> a separate library or subproject.
>>>
>>> The code can be seen here:
>>>
>>> http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolic
>>> y/src/org/apache/river/impl/util/
>>>
>>> The unit tests here:
>>>
>>> http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolic
>>> y/test/src/org/apache/river/impl/util/
>>>
>>> Or:
>>>
>>> svn checkout (replacing viewvc with repos/asf):
>>>
>>> https://svn.apache.org/repos/asf/river/jtsk/skunk/peterConcurrentP
>>> olicy/src/org/apache/river/impl/util
>>>
>>> This is the public API:
>>>
>>> /*
>>>   * Licensed to the Apache Software Foundation (ASF) under one
>>>   * or more contributor license agreements.  See the NOTICE file
>>>   * distributed with this work for additional information
>>>   * regarding copyright ownership. The ASF licenses this file
>>>   * to you under the Apache License, Version 2.0 (the
>>>   * "License"); you may not use this file except in compliance
>>>   * with the License. You may obtain a copy of the License at
>>>   *
>>>   *      http://www.apache.org/licenses/LICENSE-2.0
>>>   *
>>>   * Unless required by applicable law or agreed to in writing, software
>>>   * distributed under the License is distributed on an "AS IS" BASIS,
>>>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
>>> implied.
>>>   * See the License for the specific language governing permissions and
>>>   * limitations under the License.
>>>   */
>>>
>>> package org.apache.river.impl.util;
>>>
>>> import java.lang.ref.Reference;
>>> import java.lang.Comparable;
>>> import java.util.Collection;
>>> import java.util.Collections;
>>> import java.util.Comparator;
>>> import java.util.Deque;
>>> import java.util.List;
>>> import java.util.Map;
>>> import java.util.Map.Entry;
>>> import java.util.ListIterator;
>>> import java.util.NavigableMap;
>>> import java.util.NavigableSet;
>>> import java.util.Queue;
>>> import java.util.Set;
>>> import java.util.SortedMap;
>>> import java.util.SortedSet;
>>> import java.util.concurrent.BlockingDeque;
>>> import java.util.concurrent.BlockingQueue;
>>> import java.util.concurrent.ConcurrentMap;
>>> import java.util.concurrent.ConcurrentNavigableMap;
>>>
>>> /**
>>>   *<p>
>>>   * This class contains a number of static methods for using and
>>> abstracting
>>>   * References in Collections.  Interfaces from the Java Collections
>>> Framework
>>>   * are supported.
>>>   *</p><p>
>>>   * Referents in these collections may implement {@link Comparable} or
>>>   * a {@link Comparator} may be used for sorting.  When Comparator's are
>>> utilised,
>>>   * they must first be encapsulated {@link
>>> RC#comparator(java.util.Comparator) },
>>>   * before passing to a constructor for your preferred underlying
>>> Collection
>>>   * implementation.
>>>   *</p><p>
>>>   * {@link Comparable} is not supported for IDENTITY == referenced
>>> Collections,
>>>   * in this case a Comparator must be used.
>>>   *</p><p>
>>>   * All other references support {@link Comparable}, if the referent
>>> Object
>>>   * doesn't implement {@link Comparable}, then {@link
>>> Reference#hashCode()} is used
>>>   * for sorting.  If two referent Objects have identical hashCodes,
>>>   * but are unequal and do not implement {@link Comparable}, their
>>> references
>>>   * will also have identical hashCodes, so only one of the referents can
>>>   * be added to a {@link SortedSet} or {@link SortedMap}.  This can be
>>> fixed by using a
>>>   * {@link Comparator}.
>>>   *</p><p>
>>>   * For all intents and purposes these utilities behave the same as your
>>> preferred
>>>   * underlying {@link Collection} implementation, with the exception of
>>>   * {@link Reference} reachability.  An Object or Key,Value entry is
>>> removed
>>>   * from a {@link Collection} or {@link Map}, upon becoming eligible for
>>>   * garbage collection.
>>>   *</p><p>
>>>   * Synchronisation must be implemented by your preferred {@link
>>> Collection}
>>>   * and cannot be performed externally to the returned {@link
>>> Collection}.
>>>   * Your chosen underlying {@link Collection} must also be mutable.
>>>   * Objects will be removed automatically from underlying Collections
>>> when
>>>   * they are eligible for garbage collection, this breaks external
>>> synchronisation.
>>>   * {@link
>>> CollectionsConcurrent#multiReadCollection(java.util.Collection)} may
>>>   * be useful for synchronising your chosen underlying {@link
>>> Collection},
>>>   * especially if Objects are not being garbage collected often and
>>> writes
>>>   * are minimal.
>>>   *</p><p>
>>>   * An Unmodifiable wrapper {@link
>>> Collections#unmodifiableCollection(java.util.Collection)}
>>>   * may be used externally to prevent additions to the underlying
>>> Collections,
>>>   * referents will still be removed as they become unreachable however.
>>>   *</p><p>
>>>   * Note that any Sub List, Sub Set or Sub Map obtained by any of the
>>> Java
>>>   * Collections Framework interfaces, must be views of the underlying
>>>   * Collection, if the Collection uses defensive copies instead of views,
>>>   * References could potentially remain in one copy after garbage
>>> collection,
>>>   * causing null returns.  If using standard Java Collections Framework
>>>   * implementations, these problems don't occur as all Sub Lists,
>>>   * Sub Sets or Sub Maps are views only.
>>>   *</p><p>
>>>   * {@link Map#entrySet() } view instances returned preserve your chosen
>>> reference
>>>   * behaviour, they even support {@link Set#add(java.lang.Object)} or
>>>   * {@link Set#addAll(java.util.Collection)} methods, although you'll be
>>> hard
>>>   * pressed to find a standard java implementation that does.  If you
>>> have a
>>>   * Map with a Set of Entry's implementing add, the implementation will
>>> need a
>>>   * Comparator, that compares Entry's only by their keys, to avoid
>>> duplicating
>>>   * keys, primarily because an Entry hashCode includes the both key and
>>> value in its
>>>   * calculation. {@link Entry#hashCode() }
>>>   *</p><p>
>>>   * All other {@link Map#entrySet() } methods are fully implemented and
>>> supported.
>>>   *</p><p>
>>>   * {@link Entry} view instances returned by these methods preserve
>>> reference
>>>   * behaviour, all methods are fully implemented and supported.
>>>   *</p><p>
>>>   * {@link Set} and it's sub interfaces {@link SortedSet} and
>>>   * {@link NavigableSet}, return views that preserve reference behaviour,
>>>   * all methods are fully implemented and supported.
>>>   *</p><p>
>>>   * {@link Map} and it's sub interfaces {@link SortedMap}, {@link
>>> NavigableMap},
>>>   * {@link ConcurrentMap} and {@link ConcurrentNavigableMap} return
>>>   * views that preserve reference behaviour, all methods are fully
>>> implemented
>>>   * and supported.
>>>   *</p><p>
>>>   * {@link List} returns views that preserve reference behaviour, all
>>> methods are
>>>   * fully implemented and supported.
>>>   *</p><p>
>>>   * {@link Queue} and it's sub interfaces {@link Deque}, {@link
>>> BlockingQueue} and
>>>   * {@link BlockingDeque} return views that preserve reference behaviour,
>>>   * all methods are fully implemented and supported.
>>>   *</p><p>
>>>   * {@link Iterator} and {@link ListIterator} views preserve reference
>>> behaviour, all methods
>>>   * are fully implemented and supported.
>>>   *</p><p>
>>>   * Serialisation is supported, provided it is also supported by
>>> underlying
>>>   * collections.  Collections are not defensively copied during
>>> de-serialisation,
>>>   * due in part to an inability of determining whether a Comparator is
>>>   * used and in part, that if it is, it prevents Class.newInstance()
>>> construction.
>>>   *</p><p>
>>>   * Note that when a collection is first de-serialised, it's contents are
>>>   * strongly referenced, then changed to the correct reference type.
>>> This
>>>   * will still occur, even if the Collection is immutable.
>>>   *</p><p>
>>>   * RC stands for Reference Collection and is abbreviated due to the
>>> length of
>>>   * generic parameter arguments typically required.
>>>   *</p>
>>>   * @see Ref
>>>   * @see Referrer
>>>   * @see Reference
>>>   * @author Peter Firmstone.
>>>   */
>>> public class RC {
>>>      private RC(){} // Non instantiable
>>>
>>>      /**
>>>       * When using a Comparator in SortedSet's and SortedMap's, the
>>> Comparator
>>>       * must be encapsulated using this method, to order the Set or Map
>>>       * by referents and not References.
>>>       *
>>>       * @param<T>
>>>       * @param comparator
>>>       * @return
>>>       */
>>>      public static<T>  Comparator<Referrer<T>>  comparator(Comparator<?
>>> super T>  comparator){
>>>          return new ReferenceComparator<T>(comparator);
>>>      }
>>>
>>>      /**
>>>       * Wrap a Collection for holding references so it appears as a
>>> Collection
>>>       * containing referents.
>>>       *
>>>       * @param<T>
>>>       * @param internal
>>>       * @param type
>>>       * @return
>>>       */
>>>      public static<T>  Collection<T>  collection(Collection<Referrer<T>>
>>> internal, Ref type){
>>>          return new ReferenceCollection<T>(internal, type, false);
>>>      }
>>>
>>> //    /**
>>> //     * The general idea here is, create a factory that produces a the
>>> underlying
>>> //     * reference collection, then it can be used again later to
>>> defensively
>>> //     * produce a new copy of the original collection after
>>> de-serialisation.
>>> //     *
>>> //     * @param<T>
>>> //     * @param factory
>>> //     * @param type
>>> //     * @return
>>> //     */
>>> //    public static<T>  Collection<T>
>>> collection(CollectionFactory<Collection<Referrer<T>>>  factory, Ref
>>> type){
>>> //        return new ReferenceCollection<T>(factory.create(), type);
>>> //    }
>>>
>>>      /**
>>>       * Wrap a List for holding references so it appears as a List
>>>       * containing referents.
>>>       *
>>>       * @param<T>
>>>       * @param internal
>>>       * @param type
>>>       * @return
>>>       */
>>>      public static<T>  List<T>  list(List<Referrer<T>>  internal, Ref
>>> type){
>>>          return new ReferenceList<T>(internal, type);
>>>      }
>>>
>>>      /**
>>>       * Wrap a Set for holding references so it appears as a Set
>>>       * containing referents.
>>>       *
>>>       * @param<T>
>>>       * @param internal
>>>       * @param type
>>>       * @return
>>>       */
>>>      public static<T>  Set<T>  set(Set<Referrer<T>>  internal, Ref type){
>>>          return new ReferenceSet<T>(internal, type);
>>>      }
>>>      /**
>>>       * Wrap a SortedSet for holding references so it appears as a
>>> SortedSet
>>>       * containing referents.
>>>       *
>>>       * @para        m<T>
>>>       * @param internal
>>>       * @param type
>>>       * @return
>>>       */
>>>      public static<T>  SortedSet<T>  sortedSet(
>>>              SortedSet<Referrer<T>>  internal, Ref type){
>>>          return new ReferenceSortedSet<T>(internal, type);
>>>      }
>>>      /**
>>>       * Wrap a NavigableSet for holding references so it appears as a
>>> NavigableSet
>>>       * containing referents.
>>>       *
>>>       * @param<T>
>>>       * @param internal
>>>       * @param type
>>>       * @return
>>>       */
>>>      public static<T>  NavigableSet<T>  navigableSet(
>>>              NavigableSet<Referrer<T>>  internal, Ref type){
>>>          return new ReferenceNavigableSet<T>(internal, type);
>>>      }
>>>      /**
>>>       * Wrap a Queue for holding references so it appears as a Queue
>>>       * containing referents.
>>>       *
>>>       * @param<T>
>>>       * @param internal
>>>       * @param type
>>>       * @return
>>>       */
>>>      public static<T>  Queue<T>  queue(Queue<Referrer<T>>  internal, Ref
>>> type){
>>>          return new ReferencedQueue<T>(internal, type);
>>>      }
>>>      /**
>>>       * Wrap a Deque for holding references so it appears as a Deque
>>>       * containing referents.
>>>       *
>>>       * @param<T>
>>>       * @param internal
>>>       * @param type
>>>       * @return
>>>       */
>>>      public static<T>  Deque<T>  deque(Deque<Referrer<T>>  internal, Ref
>>> type){
>>>          return new ReferenceDeque<T>(internal, type);
>>>      }
>>>      /**
>>>       * Wrap a BlockingQueue for holding references so it appears as a
>>> BlockingQueue
>>>       * containing referents.
>>>       *
>>>       * @param<T>
>>>       * @param internal
>>>       * @param type
>>>       * @return
>>>       */
>>>      public static<T>  BlockingQueue<T>  blockingQueue(
>>>              BlockingQueue<Referrer<T>>  internal, Ref type){
>>>          return new ReferenceBlockingQueue<T>(internal, type);
>>>      }
>>>      /**
>>>       * Wrap a BlockingDeque for holding references so it appears as a
>>> BlockingDeque
>>>       * containing referents.
>>>       *
>>>       * @param<T>
>>>       * @param internal
>>>       * @param type
>>>       * @return
>>>       */
>>>      public static<T>  BlockingDeque<T>  blockingDeque(
>>>              BlockingDeque<Referrer<T>>  internal, Ref type){
>>>          return new ReferenceBlockingDeque<T>(internal, type);
>>>      }
>>>      /**
>>>       * Wrap a Map for holding references so it appears as a Map
>>>       * containing referents.
>>>       *
>>>       * @param<K>
>>>       * @param<V>
>>>       * @param internal
>>>       * @param key
>>>       * @param value
>>>       * @return
>>>       */
>>>      public static<K, V>  Map<K, V>  map(
>>>              Map<Referrer<K>, Referrer<V>>  internal, Ref key, Ref value){
>>>          return new ReferenceMap<K, V>(internal, key, value);
>>>      }
>>>      /**
>>>       * Wrap a SortedMap for holding references so it appears as a
>>> SortedMap
>>>       * containing referents.
>>>       *
>>>       * @param<K>
>>>       * @param<V>
>>>       * @param internal
>>>       * @param key
>>>       * @param value
>>>       * @return
>>>       */
>>>      public static<K, V>  SortedMap<K, V>  sortedMap(
>>>              SortedMap<Referrer<K>, Referrer<V>>  internal, Ref key, Ref
>>> value){
>>>          return new ReferenceSortedMap<K, V>(internal, key, value);
>>>      }
>>>      /**
>>>       * Wrap a NavigableMap for holding Referrers so it appears as a
>>> NavigableMap
>>>       * containing referents.
>>>       *
>>>       * @param<K>
>>>       * @param<V>
>>>       * @param internal
>>>       * @param key
>>>       * @param value
>>>       * @return
>>>       */
>>>      public static<K, V>  NavigableMap<K, V>  navigableMap(
>>>              NavigableMap<Referrer<K>, Referrer<V>>  internal, Ref key,
>>> Ref value){
>>>          return new ReferenceNavigableMap<K, V>(internal, key, value);
>>>      }
>>>      /**
>>>       * Wrap a ConcurrentMap for holding references so it appears as a
>>> ConcurrentMap
>>>       * containing referents.
>>>       *
>>>       * @param<K>  - key type.
>>>       * @param<V>  - value type.
>>>       * @param internal - for holding references.
>>>       * @param key - key reference type.
>>>       * @param value - value reference type.
>>>       * @return
>>>       */
>>>      public static<K, V>  ConcurrentMap<K, V>  concurrentMap(
>>>              ConcurrentMap<Referrer<K>, Referrer<V>>  internal, Ref key,
>>> Ref value){
>>>          return new ReferenceConcurrentMap<K, V>(internal, key, value);
>>>      }
>>>
>>>      /**
>>>       * Wrap a ConcurrentNavigableMap for holding references so it
>>> appears as a
>>>       * ConcurrentNavigableMap containing referents.
>>>       *
>>>       * @param<K>
>>>       * @param<V>
>>>       * @param internal
>>>       * @param key
>>>       * @param value
>>>       * @return
>>>       */
>>>      public static<K, V>  ConcurrentNavigableMap<K, V>
>>> concurrentNavigableMap(
>>>              ConcurrentNavigableMap<Referrer<K>, Referrer<V>>  internal,
>>> Ref key, Ref value){
>>>          return new ReferenceConcurrentNavigableMap<K, V>(internal, key,
>>> value);
>>>      }
>>> }
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/4b676c71/attachment-0001.html>

From nathan.reynolds at oracle.com  Thu Jan  5 11:29:18 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 05 Jan 2012 09:29:18 -0700
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <1325772471.6784.60.camel@bluto>
References: <1325772471.6784.60.camel@bluto>
Message-ID: <4F05CFDE.4050005@oracle.com>

One could get around the finalizer if the task keeps a WeakReference to 
the ReferenceQueue.  If WeakReference.get() returns null, then the task 
knows it needs to be cancelled.

Since Referrer objects can't escape, then JIT might be able to get rid 
of the object's allocator altogether.  I am not sure if current JIT can 
do this or if this is an enhancement request waiting to be implemented.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/5/2012 7:07 AM, Peter Firmstone wrote:
> Thinking about it further, the best solution for cleaning reference
> collections is using a static final ScheduledExecutor as part of the
> ReferenceProcessor class.
>
> That way if all the reference collections in use only require one
> thread, then only one shared thread will be created.
>
> Each Collection will still get its own ReferenceQueue, it keeps the code
> simple and allows the use of multiple collections to scale, since
> reference queues would be polled only when running as scheduled tasks,
> there's no contention for the ReferenceQueue lock, instead the threads
> increase if the load increases and there a plenty of queues to share
> among the threads.
>
> A disadvantage is the ReferenceProcessor needs a finalizer to cancel the
> task in the scheduled executor when no longer required.
>
> Example: ConcurrentMap<AccessControContext,Set<Permission>>  checked,
> using weak keys referring to ConcurrentSkipListSet's containing soft
> references.
>
> In this case there would be one ReferenceQueue for the keys and one for
> each of the Set's.
>
> The path for a calling thread that doesn't add then becomes, create
> referrer object, perform get, remove, contains, then return, discarding
> the referrer.  Reference object instances are only created for put,
> putIfAbsent, add, etc - write methods, in this case the ReferenceQueue
> is passed to the Reference constructor, the Reference added to the
> collection then the method returns.
>
> Do you think this strategy will scale?
>
> Since Reference Collection's might still be used by client developers
> for single threaded use, I'll probably keep the CAS tryLock() option, in
> case the underlying collection in use doesn't support multi threading.
>
> Thanks in advance,
>
> Peter.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/e19edefd/attachment.html>

From nathan.reynolds at oracle.com  Thu Jan  5 11:31:25 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 05 Jan 2012 09:31:25 -0700
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <CANPzfU-pBrhdA7eJXja2wNKc3g8ZK2Q3+OVC+Sy2jOoKJCtHHA@mail.gmail.com>
References: <1325772471.6784.60.camel@bluto>
	<CANPzfU-pBrhdA7eJXja2wNKc3g8ZK2Q3+OVC+Sy2jOoKJCtHHA@mail.gmail.com>
Message-ID: <4F05D05D.7060701@oracle.com>

I don't understand.  I thought the ScheduledExecutor was a great idea.  
If 1 thread can't keep up, then more threads will be created to process 
ReferenceQueues.  Oh, maybe I get your point.  A ReferenceQueue could be 
so busy that 1 thread can't keep up.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/5/2012 7:45 AM, ?iktor ?lang wrote:
>
> There's no guarantees whatsoever that it will get enough slices to 
> keep up with the pressure.
>
> On Jan 5, 2012 3:21 PM, "Peter Firmstone" <peter.firmstone at zeus.net.au 
> <mailto:peter.firmstone at zeus.net.au>> wrote:
>
>     Thinking about it further, the best solution for cleaning reference
>     collections is using a static final ScheduledExecutor as part of the
>     ReferenceProcessor class.
>
>     That way if all the reference collections in use only require one
>     thread, then only one shared thread will be created.
>
>     Each Collection will still get its own ReferenceQueue, it keeps
>     the code
>     simple and allows the use of multiple collections to scale, since
>     reference queues would be polled only when running as scheduled tasks,
>     there's no contention for the ReferenceQueue lock, instead the threads
>     increase if the load increases and there a plenty of queues to share
>     among the threads.
>
>     A disadvantage is the ReferenceProcessor needs a finalizer to
>     cancel the
>     task in the scheduled executor when no longer required.
>
>     Example: ConcurrentMap<AccessControContext,Set<Permission>> checked,
>     using weak keys referring to ConcurrentSkipListSet's containing soft
>     references.
>
>     In this case there would be one ReferenceQueue for the keys and
>     one for
>     each of the Set's.
>
>     The path for a calling thread that doesn't add then becomes, create
>     referrer object, perform get, remove, contains, then return,
>     discarding
>     the referrer.  Reference object instances are only created for put,
>     putIfAbsent, add, etc - write methods, in this case the ReferenceQueue
>     is passed to the Reference constructor, the Reference added to the
>     collection then the method returns.
>
>     Do you think this strategy will scale?
>
>     Since Reference Collection's might still be used by client developers
>     for single threaded use, I'll probably keep the CAS tryLock()
>     option, in
>     case the underlying collection in use doesn't support multi threading.
>
>     Thanks in advance,
>
>     Peter.
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/b546aa4b/attachment.html>

From viktor.klang at gmail.com  Thu Jan  5 11:38:22 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 5 Jan 2012 17:38:22 +0100
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <4F05D05D.7060701@oracle.com>
References: <1325772471.6784.60.camel@bluto>
	<CANPzfU-pBrhdA7eJXja2wNKc3g8ZK2Q3+OVC+Sy2jOoKJCtHHA@mail.gmail.com>
	<4F05D05D.7060701@oracle.com>
Message-ID: <CANPzfU-W2Q0VRNZeKCUp9FmRdxTQuvpvJiJzSnEWRBPkAwgdaQ@mail.gmail.com>

2012/1/5 Nathan Reynolds <nathan.reynolds at oracle.com>

>  I don't understand.  I thought the ScheduledExecutor was a great idea.
> If 1 thread can't keep up, then more threads will be created to process
> ReferenceQueues.  Oh, maybe I get your point.  A ReferenceQueue could be so
> busy that 1 thread can't keep up.
>

Are we effectively talking about making a ReferenceQueue a BlockQueue and
use it as the task-queue for an ExecutorService?




>
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/5/2012 7:45 AM, ?iktor ?lang wrote:
>
> There's no guarantees whatsoever that it will get enough slices to keep up
> with the pressure.
> On Jan 5, 2012 3:21 PM, "Peter Firmstone" <peter.firmstone at zeus.net.au>
> wrote:
>
>> Thinking about it further, the best solution for cleaning reference
>> collections is using a static final ScheduledExecutor as part of the
>> ReferenceProcessor class.
>>
>> That way if all the reference collections in use only require one
>> thread, then only one shared thread will be created.
>>
>> Each Collection will still get its own ReferenceQueue, it keeps the code
>> simple and allows the use of multiple collections to scale, since
>> reference queues would be polled only when running as scheduled tasks,
>> there's no contention for the ReferenceQueue lock, instead the threads
>> increase if the load increases and there a plenty of queues to share
>> among the threads.
>>
>> A disadvantage is the ReferenceProcessor needs a finalizer to cancel the
>> task in the scheduled executor when no longer required.
>>
>> Example: ConcurrentMap<AccessControContext,Set<Permission>> checked,
>> using weak keys referring to ConcurrentSkipListSet's containing soft
>> references.
>>
>> In this case there would be one ReferenceQueue for the keys and one for
>> each of the Set's.
>>
>> The path for a calling thread that doesn't add then becomes, create
>> referrer object, perform get, remove, contains, then return, discarding
>> the referrer.  Reference object instances are only created for put,
>> putIfAbsent, add, etc - write methods, in this case the ReferenceQueue
>> is passed to the Reference constructor, the Reference added to the
>> collection then the method returns.
>>
>> Do you think this strategy will scale?
>>
>> Since Reference Collection's might still be used by client developers
>> for single threaded use, I'll probably keep the CAS tryLock() option, in
>> case the underlying collection in use doesn't support multi threading.
>>
>> Thanks in advance,
>>
>> Peter.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/6aed4f2e/attachment-0001.html>

From nathan.reynolds at oracle.com  Thu Jan  5 11:44:42 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 05 Jan 2012 09:44:42 -0700
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <CANPzfU-W2Q0VRNZeKCUp9FmRdxTQuvpvJiJzSnEWRBPkAwgdaQ@mail.gmail.com>
References: <1325772471.6784.60.camel@bluto>
	<CANPzfU-pBrhdA7eJXja2wNKc3g8ZK2Q3+OVC+Sy2jOoKJCtHHA@mail.gmail.com>
	<4F05D05D.7060701@oracle.com>
	<CANPzfU-W2Q0VRNZeKCUp9FmRdxTQuvpvJiJzSnEWRBPkAwgdaQ@mail.gmail.com>
Message-ID: <4F05D37A.9000407@oracle.com>

I was thinking that when a ReferenceQueue is created (per collection) a 
fixed-rate task is added to the ScheduledExecutor.  When executed, the 
task will call poll() on the ReferenceQueue and process references until 
poll() returns null.  At this point, the task ends and waits to be 
executed again due to the fixed-rate parameter.

To extremely minimize threads, the tasks could be scheduled so that 
their execution causes the least number of threads are required.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/5/2012 9:38 AM, ?iktor ?lang wrote:
>
>
> 2012/1/5 Nathan Reynolds <nathan.reynolds at oracle.com 
> <mailto:nathan.reynolds at oracle.com>>
>
>     I don't understand.  I thought the ScheduledExecutor was a great
>     idea.  If 1 thread can't keep up, then more threads will be
>     created to process ReferenceQueues.  Oh, maybe I get your point. 
>     A ReferenceQueue could be so busy that 1 thread can't keep up.
>
>
> Are we effectively talking about making a ReferenceQueue a BlockQueue 
> and use it as the task-queue for an ExecutorService?
>
>
>
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Consulting Member of Technical Staff | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
>     On 1/5/2012 7:45 AM, ?iktor ?lang wrote:
>>
>>     There's no guarantees whatsoever that it will get enough slices
>>     to keep up with the pressure.
>>
>>     On Jan 5, 2012 3:21 PM, "Peter Firmstone"
>>     <peter.firmstone at zeus.net.au
>>     <mailto:peter.firmstone at zeus.net.au>> wrote:
>>
>>         Thinking about it further, the best solution for cleaning
>>         reference
>>         collections is using a static final ScheduledExecutor as part
>>         of the
>>         ReferenceProcessor class.
>>
>>         That way if all the reference collections in use only require one
>>         thread, then only one shared thread will be created.
>>
>>         Each Collection will still get its own ReferenceQueue, it
>>         keeps the code
>>         simple and allows the use of multiple collections to scale, since
>>         reference queues would be polled only when running as
>>         scheduled tasks,
>>         there's no contention for the ReferenceQueue lock, instead
>>         the threads
>>         increase if the load increases and there a plenty of queues
>>         to share
>>         among the threads.
>>
>>         A disadvantage is the ReferenceProcessor needs a finalizer to
>>         cancel the
>>         task in the scheduled executor when no longer required.
>>
>>         Example: ConcurrentMap<AccessControContext,Set<Permission>>
>>         checked,
>>         using weak keys referring to ConcurrentSkipListSet's
>>         containing soft
>>         references.
>>
>>         In this case there would be one ReferenceQueue for the keys
>>         and one for
>>         each of the Set's.
>>
>>         The path for a calling thread that doesn't add then becomes,
>>         create
>>         referrer object, perform get, remove, contains, then return,
>>         discarding
>>         the referrer.  Reference object instances are only created
>>         for put,
>>         putIfAbsent, add, etc - write methods, in this case the
>>         ReferenceQueue
>>         is passed to the Reference constructor, the Reference added
>>         to the
>>         collection then the method returns.
>>
>>         Do you think this strategy will scale?
>>
>>         Since Reference Collection's might still be used by client
>>         developers
>>         for single threaded use, I'll probably keep the CAS tryLock()
>>         option, in
>>         case the underlying collection in use doesn't support multi
>>         threading.
>>
>>         Thanks in advance,
>>
>>         Peter.
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> Viktor Klang
>
> Akka Tech Lead
> Typesafe <http://www.typesafe.com/>- Enterprise-Grade Scala from the 
> Experts
>
> Twitter: @viktorklang
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/1310e961/attachment.html>

From viktor.klang at gmail.com  Thu Jan  5 11:57:32 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 5 Jan 2012 17:57:32 +0100
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <4F05D37A.9000407@oracle.com>
References: <1325772471.6784.60.camel@bluto>
	<CANPzfU-pBrhdA7eJXja2wNKc3g8ZK2Q3+OVC+Sy2jOoKJCtHHA@mail.gmail.com>
	<4F05D05D.7060701@oracle.com>
	<CANPzfU-W2Q0VRNZeKCUp9FmRdxTQuvpvJiJzSnEWRBPkAwgdaQ@mail.gmail.com>
	<4F05D37A.9000407@oracle.com>
Message-ID: <CANPzfU_zDG+Fqw2JRda=uWp63B1nEcJR+64MqJiR8wx_H77uGA@mail.gmail.com>

2012/1/5 Nathan Reynolds <nathan.reynolds at oracle.com>

>  I was thinking that when a ReferenceQueue is created (per collection) a
> fixed-rate task is added to the ScheduledExecutor.  When executed, the task
> will call poll() on the ReferenceQueue and process references until poll()
> returns null.  At this point, the task ends and waits to be executed again
> due to the fixed-rate parameter.
>

Could definitely work, but as stated previously, if one thread cannot keep
up.


>
> To extremely minimize threads, the tasks could be scheduled so that their
> execution causes the least number of threads are required.
>

Yeah, and you can employ core pool timeouts to avoid creating threads if
the feature isn't used.

Cheers,
?


>
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/5/2012 9:38 AM, ?iktor ?lang wrote:
>
>
>
> 2012/1/5 Nathan Reynolds <nathan.reynolds at oracle.com>
>
>>  I don't understand.  I thought the ScheduledExecutor was a great idea.
>> If 1 thread can't keep up, then more threads will be created to process
>> ReferenceQueues.  Oh, maybe I get your point.  A ReferenceQueue could be so
>> busy that 1 thread can't keep up.
>>
>
>  Are we effectively talking about making a ReferenceQueue a BlockQueue
> and use it as the task-queue for an ExecutorService?
>
>
>
>
>>
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>>   On 1/5/2012 7:45 AM, ?iktor ?lang wrote:
>>
>> There's no guarantees whatsoever that it will get enough slices to keep
>> up with the pressure.
>> On Jan 5, 2012 3:21 PM, "Peter Firmstone" <peter.firmstone at zeus.net.au>
>> wrote:
>>
>>> Thinking about it further, the best solution for cleaning reference
>>> collections is using a static final ScheduledExecutor as part of the
>>> ReferenceProcessor class.
>>>
>>> That way if all the reference collections in use only require one
>>> thread, then only one shared thread will be created.
>>>
>>> Each Collection will still get its own ReferenceQueue, it keeps the code
>>> simple and allows the use of multiple collections to scale, since
>>> reference queues would be polled only when running as scheduled tasks,
>>> there's no contention for the ReferenceQueue lock, instead the threads
>>> increase if the load increases and there a plenty of queues to share
>>> among the threads.
>>>
>>> A disadvantage is the ReferenceProcessor needs a finalizer to cancel the
>>> task in the scheduled executor when no longer required.
>>>
>>> Example: ConcurrentMap<AccessControContext,Set<Permission>> checked,
>>> using weak keys referring to ConcurrentSkipListSet's containing soft
>>> references.
>>>
>>> In this case there would be one ReferenceQueue for the keys and one for
>>> each of the Set's.
>>>
>>> The path for a calling thread that doesn't add then becomes, create
>>> referrer object, perform get, remove, contains, then return, discarding
>>> the referrer.  Reference object instances are only created for put,
>>> putIfAbsent, add, etc - write methods, in this case the ReferenceQueue
>>> is passed to the Reference constructor, the Reference added to the
>>> collection then the method returns.
>>>
>>> Do you think this strategy will scale?
>>>
>>> Since Reference Collection's might still be used by client developers
>>> for single threaded use, I'll probably keep the CAS tryLock() option, in
>>> case the underlying collection in use doesn't support multi threading.
>>>
>>> Thanks in advance,
>>>
>>> Peter.
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>  --
> Viktor Klang
>
> Akka Tech Lead
> Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
> Experts
>
> Twitter: @viktorklang
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/34ac0c15/attachment-0001.html>

From peter.firmstone at zeus.net.au  Thu Jan  5 18:17:53 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Fri, 06 Jan 2012 09:17:53 +1000
Subject: [concurrency-interest] Reference Collections
Message-ID: <1325805472.10496.6.camel@bluto>

Just thought I'd better clarify how reads are performed, the Referrer
interface exists to avoid creating unnecessary references, it defines
the equals and hashCode contracts all reference implementations must
follow.

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.river.impl.util;

import java.lang.ref.Reference;

/**
 * An interface for References used in collections, it defines the
equals
 * and hashCode contracts as well as methods identical to Reference.
 * 
 * A client may wish to implement this interface to replace a standard
Referrer
 * during serialisation with custom implementations to deconstruct and
 * reconstruct non serialisable objects, or to perform integrity checks.
 * 
 * This must be implemented in a Collection provided by the client.
 * 
 * After de-serialisation is complete, the client Referrer will be
replaced
 * with a standard Referrer.
 * 
 * @see Reference
 * @see Ref
 * @param <T> 
 * @author Peter Firmstone
 */
public interface Referrer<T> {
    
    /**
     * @see Reference#get()
     */
    public T get() ;
    /**
     * @see Reference#clear()
     */
    public void clear();
    /**
     * @see Reference#isEnqueued() 
     * @return true if enqueued.
     */
    public boolean isEnqueued();
    /**
     * @see Reference#enqueue() 
     * @return 
     */
    public boolean enqueue();
    
    /**
     * Equals is calculated on IDENTITY or equality.
     * 
     * IDENTITY calculation:
     * 
     * if (this == o) return true;
     * if (!(o instanceof Referrer)) return false;
     * Object k1 = get();
     * Object k2 = ((Referrer) o).get();
     * if ( k1 != null && k1 == k2 ) return true;
     * return ( k1 == null && k2 == null && hashCode() == o.hashCode());
     * 
     * Equality calculation:
     * 
     * if (this == o)  return true; // Same reference.
     * if (!(o instanceof Referrer))  return false;
     * Object k1 = get();
     * Object k2 = ((Referrer) o).get();
     * if ( k1 != null && k1.equals(k2)) return true;
     * return ( k1 == null && k2 == null && hashCode() == o.hashCode());
     *
     * @see Ref
     * @param o
     * @return 
     */
    public boolean equals(Object o);
    
    /**
     * Standard hashCode calculation for IDENTITY based references,
where k
     * is the referent.
     * 
     * int hash = 7;
     * hash = 29 * hash + System.identityHashCode(k);
     * hash = 29 * hash + k.getClass().hashCode();
     * 
     * For non IDENTITY references, the hashCode returned is the
referent's 
     * hashCode, after the reference has been cleared, it reverts to the
     * IDENTITY based value.  The IDENTITY hashCode should be calculated
     * at construction time.
     * 
     * @return 
     */
    public int hashCode();
}

Here's an implementation to avoid creating an unnecessary reference:

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.river.impl.util;

/**
 *
 * @author peter
 */
class TempIdentityReferrer<T> implements Referrer<T> {
    
    private final T t;
    
    TempIdentityReferrer(T t){
        if ( t == null ) throw new NullPointerException("Null
prohibited");
        this.t = t;
    }

    @Override
    public T get() {
        return t;
    }

    @Override
    public void clear() {
        throw new UnsupportedOperationException("Not supported.");
    }

    @Override
    public boolean isEnqueued() {
        return false;
    }

    @Override
    public boolean enqueue() {
        return false;
    }
    
    @Override
    public boolean equals(Object o) {
        if (this == o) {
            return true;
        } else if (!(o instanceof Referrer)) {
            return false;
        }
        Object t2 = ((Referrer) o).get();
        return( t == t2 );
    }
    
    public int hashCode(){
        int hash = 7;
        hash = 29 * hash + System.identityHashCode(t);
        hash = 29 * hash + t.getClass().hashCode();
        return hash;
    }
    
}


This is an implementation for a permanent Reference:

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.river.impl.util;

import java.io.InvalidObjectException;
import java.io.ObjectInputStream;
import java.io.Serializable;
import java.lang.ref.ReferenceQueue;
import java.lang.ref.SoftReference;

/**
 * Implementation as per Ref.SOFT_IDENTITY
 * 
 * @see Ref#SOFT_IDENTITY
 * @author Peter Firmstone.
 */
class SoftIdentityReferenceKey<T> extends SoftReference<T> implements
Referrer<T>, Serializable{
    private static final long serialVersionUID = 1L;
    private final int hash;

    SoftIdentityReferenceKey(T k, ReferenceQueue<? super T> q) {
        super(k,q);
        int hash = 7;
        hash = 29 * hash + System.identityHashCode(k);
        hash = 29 * hash + k.getClass().hashCode();
        this.hash = hash;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (!(o instanceof Referrer)) return false;
        Object k1 = get();
        Object k2 = ((Referrer) o).get();
        if ( k1 != null && k1 == k2 ) return true;
        return ( k1 == null && k2 == null && hashCode() ==
o.hashCode());
    }

    @Override
    public int hashCode() {
        return hash;
    }
    
    @Override
    public String toString(){
        Object s = get();
        if (s != null) return s.toString();
        return super.toString();
    }
    
    private Object writeReplace() {
        // returns a Builder instead of this class.
        return ReferenceSerializationFactory.create(get());
    }
    
    private void readObject(ObjectInputStream stream) 
            throws InvalidObjectException{
        throw new InvalidObjectException("Factory required");
    }

}



From peter.firmstone at zeus.net.au  Thu Jan  5 18:34:45 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Fri, 06 Jan 2012 09:34:45 +1000
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <4F05CD96.8080108@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCCEDCJCAA.davidcholmes@aapt.net.au>
	<1325727726.2319.178.camel@bluto>  <4F05CD96.8080108@oracle.com>
Message-ID: <1325806484.10496.9.camel@bluto>

Thanks Nathan, can I use it under an Apache 2 license?

Peter.

On Fri, 2012-01-06 at 02:19, Nathan Reynolds wrote:
> >  The symptom seen by calling threads is occasional null return
> values in iterators. 
> 
> You can eliminate this problem by caching the next value in
> hasNext().  Here's an iterator which removes nulls from the source
> iterator.
> 
> public class NoNullIterator<T> implements Iterator<T>
> {
>    private final Iterator<T> m_source;
>    private       T           m_next;
> 
>    public NoNullIterator(Iterator<T> source)
>    {
>       if (source == null)
>          throw new NullPointerException();
> 
>       m_source = source;
>    }
> 
>    public boolean hasNext()
>    {
>       T next;
> 
>       if (m_next != null)
>          return(true);
> 
>       while (m_source.hasNext())
>       {
>          m_next = m_source.next();
>          
>          if (m_next != null)
>             return(true);
>       }
> 
>       return(false);
>    }
> 
>    public T next()
>    {
>       T result;
> 
>       if (m_next == null)
>          if (!hasNext())
>             throw new IllegalStateException();
> 
>       result = m_next;
>       m_next = null;
> 
>       return(result);
>    }
> 
>    public void remove()
>    {
>       m_source.remove();
>    }
> }
> 
> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
> Oracle PSR Engineering | Server Technology
> 
> On 1/4/2012 6:42 PM, Peter Firmstone wrote: 
> > Thanks David,
> > 
> > ...Hmm you're speaking from experience, had the privilege of meeting
> > briefly while you were at Sun in Brisbane.
> > 
> > Users will be given a choice of one dedicated cleanup thread (per
> > Collection) which waits on the ReferenceQueue OR CAS with
> > ReentrantLock.tryLock() to share cleanup between callers.
> > 
> > CAS for single or few threads and less memory consumption, the dedicated
> > cleanup thread for scalability.
> > 
> > Note: Maps will have up to two ReferenceQueue's and two dedicated
> > threads, if based on the existing design. EG: Weak Keys and Soft Values
> > or Weak Keys and Values, when there's a circular reference from the
> > value to the key.  Weak keys with strongly reference values will only
> > have one ReferenceQueue and one cleanup Thread.
> > 
> > Reference Collection's can be widely applied to any java collection
> > type, my current concern is scalability for large collections (including
> > concurrent maps) which I have no way of testing.
> > 
> > In the implementation, the only mutable state is in ReferenceQueue, the
> > underlying collection and Reference, everything else is final or uses
> > method variables.  Dummy References, used for reads, are created and
> > discarded and die young, so never enter main memory.  Hopefully the
> > majority of state should be in cache during execution.  My main concern
> > is the current design employed a ReentrantLock.tryLock() call to hand
> > maintenance to one of the calling threads, I was worried that this would
> > limit scalability.  The feedback so far is yes (info much appreciated).
> > 
> > If ReferenceQueue.remove() is only called by one thread, the calling
> > threads no longer need to perform cleanup, it's done by the maintenance
> > thread, which is blocked on remove, so doesn't consume cpu unless
> > required, although relative to the number of collections in use, will
> > consume more memory.
> > 
> > Nathan suggested more than one thread might be needed to perform
> > cleanup, due to experiences with jrocket's finalizer.  This situation
> > could occur briefly if a large number objects suddenly became reachable,
> > such as a large collection using soft references, when the jvm is low on
> > memory.  We were discussing work stealing, but I guess more concurrency
> > on a bottlenecking underlying collection won't help.
> > 
> > The symptom seen by calling threads is occasional null return values in
> > iterators.  This isn't a problem if the caller expects a null.  I'd
> > imagine the cleanup thread will catch up. ReferenceComparator
> > encapsulates existing Comparator's and guards against null.  This could
> > be a problem for existing code if the collection is used as a drop in
> > replacement.
> > 
> > My experience so far has been that null returns are easy to deal with in
> > new code.
> > 
> > What are your thoughts?
> > 
> > Regards,
> > 
> > Peter.
> > 
> > On Thu, 2012-01-05 at 09:18, David Holmes wrote:
> > > Peter Firmstone writes:
> > > > On Thu, 2012-01-05 at 07:52, Nathan Reynolds wrote:
> > > > > Would the fork-join framework with work steal be an ideal fit?
> > > > For ultimate scalability it would, although I'm unable to test for
> > > > scalability, only reason about it and ask questions of developers who do
> > > > have acces to big iron ;)  It might also make sense to use multiple
> > > > ReferenceQueue's for a collection under some circumstances.
> > > I don't see how FJ would be applicable here. What tasks are you forking and
> > > joining?
> > > 
> > > Using a Thread pool for the cleanup seems like the most scalable approach,
> > > but I think if you are beyond the point where a single cleanup thread
> > > suffices then your design is already in trouble.
> > > 
> > > David
> > > -----
> > > 
> > > > I've implemented Reference Collections to fulfill a need that our
> > > > project has, I'm sure very few are aware of its existence, but since it
> > > > has such a compact public API that's very easy to extend to new
> > > > collection interfaces, it could potentially make a very good collections
> > > > based library for using references in your favoured collection
> > > > implementation.  It's also possible to make new types of refernces, eg:
> > > > timer based.  There's no serialized form lock in either, so evolution
> > > > isn't hampered by serialization and serialization can be easily
> > > > supported in a backward compatible evolutionary manner.
> > > > 
> > > > If there's enough interest, we could split Reference Collections out as
> > > > a separate library or subproject.
> > > > 
> > > > The code can be seen here:
> > > > 
> > > > http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolic
> > > > y/src/org/apache/river/impl/util/
> > > > 
> > > > The unit tests here:
> > > > 
> > > > http://svn.apache.org/viewvc/river/jtsk/skunk/peterConcurrentPolic
> > > > y/test/src/org/apache/river/impl/util/
> > > > 
> > > > Or:
> > > > 
> > > > svn checkout (replacing viewvc with repos/asf):
> > > > 
> > > > https://svn.apache.org/repos/asf/river/jtsk/skunk/peterConcurrentP
> > > > olicy/src/org/apache/river/impl/util
> > > > 
> > > > This is the public API:
> > > > 
> > > > /*
> > > >  * Licensed to the Apache Software Foundation (ASF) under one
> > > >  * or more contributor license agreements.  See the NOTICE file
> > > >  * distributed with this work for additional information
> > > >  * regarding copyright ownership. The ASF licenses this file
> > > >  * to you under the Apache License, Version 2.0 (the
> > > >  * "License"); you may not use this file except in compliance
> > > >  * with the License. You may obtain a copy of the License at
> > > >  *
> > > >  *      http://www.apache.org/licenses/LICENSE-2.0
> > > >  *
> > > >  * Unless required by applicable law or agreed to in writing, software
> > > >  * distributed under the License is distributed on an "AS IS" BASIS,
> > > >  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> > > > implied.
> > > >  * See the License for the specific language governing permissions and
> > > >  * limitations under the License.
> > > >  */
> > > > 
> > > > package org.apache.river.impl.util;
> > > > 
> > > > import java.lang.ref.Reference;
> > > > import java.lang.Comparable;
> > > > import java.util.Collection;
> > > > import java.util.Collections;
> > > > import java.util.Comparator;
> > > > import java.util.Deque;
> > > > import java.util.List;
> > > > import java.util.Map;
> > > > import java.util.Map.Entry;
> > > > import java.util.ListIterator;
> > > > import java.util.NavigableMap;
> > > > import java.util.NavigableSet;
> > > > import java.util.Queue;
> > > > import java.util.Set;
> > > > import java.util.SortedMap;
> > > > import java.util.SortedSet;
> > > > import java.util.concurrent.BlockingDeque;
> > > > import java.util.concurrent.BlockingQueue;
> > > > import java.util.concurrent.ConcurrentMap;
> > > > import java.util.concurrent.ConcurrentNavigableMap;
> > > > 
> > > > /**
> > > >  * <p>
> > > >  * This class contains a number of static methods for using and
> > > > abstracting
> > > >  * References in Collections.  Interfaces from the Java Collections
> > > > Framework
> > > >  * are supported.
> > > >  * </p><p>
> > > >  * Referents in these collections may implement {@link Comparable} or
> > > >  * a {@link Comparator} may be used for sorting.  When Comparator's are
> > > > utilised,
> > > >  * they must first be encapsulated {@link
> > > > RC#comparator(java.util.Comparator) },
> > > >  * before passing to a constructor for your preferred underlying
> > > > Collection
> > > >  * implementation.
> > > >  * </p><p>
> > > >  * {@link Comparable} is not supported for IDENTITY == referenced
> > > > Collections,
> > > >  * in this case a Comparator must be used.
> > > >  * </p><p>
> > > >  * All other references support {@link Comparable}, if the referent
> > > > Object
> > > >  * doesn't implement {@link Comparable}, then {@link
> > > > Reference#hashCode()} is used
> > > >  * for sorting.  If two referent Objects have identical hashCodes,
> > > >  * but are unequal and do not implement {@link Comparable}, their
> > > > references
> > > >  * will also have identical hashCodes, so only one of the referents can
> > > >  * be added to a {@link SortedSet} or {@link SortedMap}.  This can be
> > > > fixed by using a
> > > >  * {@link Comparator}.
> > > >  * </p><p>
> > > >  * For all intents and purposes these utilities behave the same as your
> > > > preferred
> > > >  * underlying {@link Collection} implementation, with the exception of
> > > >  * {@link Reference} reachability.  An Object or Key,Value entry is
> > > > removed
> > > >  * from a {@link Collection} or {@link Map}, upon becoming eligible for
> > > >  * garbage collection.
> > > >  * </p><p>
> > > >  * Synchronisation must be implemented by your preferred {@link
> > > > Collection}
> > > >  * and cannot be performed externally to the returned {@link
> > > > Collection}.
> > > >  * Your chosen underlying {@link Collection} must also be mutable.
> > > >  * Objects will be removed automatically from underlying Collections
> > > > when
> > > >  * they are eligible for garbage collection, this breaks external
> > > > synchronisation.
> > > >  * {@link
> > > > CollectionsConcurrent#multiReadCollection(java.util.Collection)} may
> > > >  * be useful for synchronising your chosen underlying {@link
> > > > Collection},
> > > >  * especially if Objects are not being garbage collected often and
> > > > writes
> > > >  * are minimal.
> > > >  * </p><p>
> > > >  * An Unmodifiable wrapper {@link
> > > > Collections#unmodifiableCollection(java.util.Collection)}
> > > >  * may be used externally to prevent additions to the underlying
> > > > Collections,
> > > >  * referents will still be removed as they become unreachable however.
> > > >  * </p><p>
> > > >  * Note that any Sub List, Sub Set or Sub Map obtained by any of the
> > > > Java
> > > >  * Collections Framework interfaces, must be views of the underlying
> > > >  * Collection, if the Collection uses defensive copies instead of views,
> > > >  * References could potentially remain in one copy after garbage
> > > > collection,
> > > >  * causing null returns.  If using standard Java Collections Framework
> > > >  * implementations, these problems don't occur as all Sub Lists,
> > > >  * Sub Sets or Sub Maps are views only.
> > > >  * </p><p>
> > > >  * {@link Map#entrySet() } view instances returned preserve your chosen
> > > > reference
> > > >  * behaviour, they even support {@link Set#add(java.lang.Object)} or
> > > >  * {@link Set#addAll(java.util.Collection)} methods, although you'll be
> > > > hard
> > > >  * pressed to find a standard java implementation that does.  If you
> > > > have a
> > > >  * Map with a Set of Entry's implementing add, the implementation will
> > > > need a
> > > >  * Comparator, that compares Entry's only by their keys, to avoid
> > > > duplicating
> > > >  * keys, primarily because an Entry hashCode includes the both key and
> > > > value in its
> > > >  * calculation. {@link Entry#hashCode() }
> > > >  * </p><p>
> > > >  * All other {@link Map#entrySet() } methods are fully implemented and
> > > > supported.
> > > >  * </p><p>
> > > >  * {@link Entry} view instances returned by these methods preserve
> > > > reference
> > > >  * behaviour, all methods are fully implemented and supported.
> > > >  * </p><p>
> > > >  * {@link Set} and it's sub interfaces {@link SortedSet} and
> > > >  * {@link NavigableSet}, return views that preserve reference behaviour,
> > > >  * all methods are fully implemented and supported.
> > > >  * </p><p>
> > > >  * {@link Map} and it's sub interfaces {@link SortedMap}, {@link
> > > > NavigableMap},
> > > >  * {@link ConcurrentMap} and {@link ConcurrentNavigableMap} return
> > > >  * views that preserve reference behaviour, all methods are fully
> > > > implemented
> > > >  * and supported.
> > > >  * </p><p>
> > > >  * {@link List} returns views that preserve reference behaviour, all
> > > > methods are
> > > >  * fully implemented and supported.
> > > >  * </p><p>
> > > >  * {@link Queue} and it's sub interfaces {@link Deque}, {@link
> > > > BlockingQueue} and
> > > >  * {@link BlockingDeque} return views that preserve reference behaviour,
> > > >  * all methods are fully implemented and supported.
> > > >  * </p><p>
> > > >  * {@link Iterator} and {@link ListIterator} views preserve reference
> > > > behaviour, all methods
> > > >  * are fully implemented and supported.
> > > >  * </p><p>
> > > >  * Serialisation is supported, provided it is also supported by
> > > > underlying
> > > >  * collections.  Collections are not defensively copied during
> > > > de-serialisation,
> > > >  * due in part to an inability of determining whether a Comparator is
> > > >  * used and in part, that if it is, it prevents Class.newInstance()
> > > > construction.
> > > >  * </p><p>
> > > >  * Note that when a collection is first de-serialised, it's contents are
> > > >  * strongly referenced, then changed to the correct reference type.
> > > > This
> > > >  * will still occur, even if the Collection is immutable.
> > > >  * </p><p>
> > > >  * RC stands for Reference Collection and is abbreviated due to the
> > > > length of
> > > >  * generic parameter arguments typically required.
> > > >  * </p>
> > > >  * @see Ref
> > > >  * @see Referrer
> > > >  * @see Reference
> > > >  * @author Peter Firmstone.
> > > >  */
> > > > public class RC {
> > > >     private RC(){} // Non instantiable
> > > > 
> > > >     /**
> > > >      * When using a Comparator in SortedSet's and SortedMap's, the
> > > > Comparator
> > > >      * must be encapsulated using this method, to order the Set or Map
> > > >      * by referents and not References.
> > > >      *
> > > >      * @param <T>
> > > >      * @param comparator
> > > >      * @return
> > > >      */
> > > >     public static <T> Comparator<Referrer<T>> comparator(Comparator<?
> > > > super T> comparator){
> > > >         return new ReferenceComparator<T>(comparator);
> > > >     }
> > > > 
> > > >     /**
> > > >      * Wrap a Collection for holding references so it appears as a
> > > > Collection
> > > >      * containing referents.
> > > >      *
> > > >      * @param <T>
> > > >      * @param internal
> > > >      * @param type
> > > >      * @return
> > > >      */
> > > >     public static <T> Collection<T> collection(Collection<Referrer<T>>
> > > > internal, Ref type){
> > > >         return new ReferenceCollection<T>(internal, type, false);
> > > >     }
> > > > 
> > > > //    /**
> > > > //     * The general idea here is, create a factory that produces a the
> > > > underlying
> > > > //     * reference collection, then it can be used again later to
> > > > defensively
> > > > //     * produce a new copy of the original collection after
> > > > de-serialisation.
> > > > //     *
> > > > //     * @param <T>
> > > > //     * @param factory
> > > > //     * @param type
> > > > //     * @return
> > > > //     */
> > > > //    public static <T> Collection<T>
> > > > collection(CollectionFactory<Collection<Referrer<T>>> factory, Ref
> > > > type){
> > > > //        return new ReferenceCollection<T>(factory.create(), type);
> > > > //    }
> > > > 
> > > >     /**
> > > >      * Wrap a List for holding references so it appears as a List
> > > >      * containing referents.
> > > >      *
> > > >      * @param <T>
> > > >      * @param internal
> > > >      * @param type
> > > >      * @return
> > > >      */
> > > >     public static <T> List<T> list(List<Referrer<T>> internal, Ref
> > > > type){
> > > >         return new ReferenceList<T>(internal, type);
> > > >     }
> > > > 
> > > >     /**
> > > >      * Wrap a Set for holding references so it appears as a Set
> > > >      * containing referents.
> > > >      *
> > > >      * @param <T>
> > > >      * @param internal
> > > >      * @param type
> > > >      * @return
> > > >      */
> > > >     public static <T> Set<T> set(Set<Referrer<T>> internal, Ref type){
> > > >         return new ReferenceSet<T>(internal, type);
> > > >     }
> > > >     /**
> > > >      * Wrap a SortedSet for holding references so it appears as a
> > > > SortedSet
> > > >      * containing referents.
> > > >      *
> > > >      * @para        m <T>
> > > >      * @param internal
> > > >      * @param type
> > > >      * @return
> > > >      */
> > > >     public static <T> SortedSet<T> sortedSet(
> > > >             SortedSet<Referrer<T>> internal, Ref type){
> > > >         return new ReferenceSortedSet<T>(internal, type);
> > > >     }
> > > >     /**
> > > >      * Wrap a NavigableSet for holding references so it appears as a
> > > > NavigableSet
> > > >      * containing referents.
> > > >      *
> > > >      * @param <T>
> > > >      * @param internal
> > > >      * @param type
> > > >      * @return
> > > >      */
> > > >     public static <T> NavigableSet<T> navigableSet(
> > > >             NavigableSet<Referrer<T>> internal, Ref type){
> > > >         return new ReferenceNavigableSet<T>(internal, type);
> > > >     }
> > > >     /**
> > > >      * Wrap a Queue for holding references so it appears as a Queue
> > > >      * containing referents.
> > > >      *
> > > >      * @param <T>
> > > >      * @param internal
> > > >      * @param type
> > > >      * @return
> > > >      */
> > > >     public static <T> Queue<T> queue(Queue<Referrer<T>> internal, Ref
> > > > type){
> > > >         return new ReferencedQueue<T>(internal, type);
> > > >     }
> > > >     /**
> > > >      * Wrap a Deque for holding references so it appears as a Deque
> > > >      * containing referents.
> > > >      *
> > > >      * @param <T>
> > > >      * @param internal
> > > >      * @param type
> > > >      * @return
> > > >      */
> > > >     public static <T> Deque<T> deque(Deque<Referrer<T>> internal, Ref
> > > > type){
> > > >         return new ReferenceDeque<T>(internal, type);
> > > >     }
> > > >     /**
> > > >      * Wrap a BlockingQueue for holding references so it appears as a
> > > > BlockingQueue
> > > >      * containing referents.
> > > >      *
> > > >      * @param <T>
> > > >      * @param internal
> > > >      * @param type
> > > >      * @return
> > > >      */
> > > >     public static <T> BlockingQueue<T> blockingQueue(
> > > >             BlockingQueue<Referrer<T>> internal, Ref type){
> > > >         return new ReferenceBlockingQueue<T>(internal, type);
> > > >     }
> > > >     /**
> > > >      * Wrap a BlockingDeque for holding references so it appears as a
> > > > BlockingDeque
> > > >      * containing referents.
> > > >      *
> > > >      * @param <T>
> > > >      * @param internal
> > > >      * @param type
> > > >      * @return
> > > >      */
> > > >     public static <T> BlockingDeque<T> blockingDeque(
> > > >             BlockingDeque<Referrer<T>> internal, Ref type){
> > > >         return new ReferenceBlockingDeque<T>(internal, type);
> > > >     }
> > > >     /**
> > > >      * Wrap a Map for holding references so it appears as a Map
> > > >      * containing referents.
> > > >      *
> > > >      * @param <K>
> > > >      * @param <V>
> > > >      * @param internal
> > > >      * @param key
> > > >      * @param value
> > > >      * @return
> > > >      */
> > > >     public static <K, V> Map<K, V> map(
> > > >             Map<Referrer<K>, Referrer<V>> internal, Ref key, Ref value){
> > > >         return new ReferenceMap<K, V>(internal, key, value);
> > > >     }
> > > >     /**
> > > >      * Wrap a SortedMap for holding references so it appears as a
> > > > SortedMap
> > > >      * containing referents.
> > > >      *
> > > >      * @param <K>
> > > >      * @param <V>
> > > >      * @param internal
> > > >      * @param key
> > > >      * @param value
> > > >      * @return
> > > >      */
> > > >     public static <K, V> SortedMap<K, V> sortedMap(
> > > >             SortedMap<Referrer<K>, Referrer<V>> internal, Ref key, Ref
> > > > value){
> > > >         return new ReferenceSortedMap<K, V>(internal, key, value);
> > > >     }
> > > >     /**
> > > >      * Wrap a NavigableMap for holding Referrers so it appears as a
> > > > NavigableMap
> > > >      * containing referents.
> > > >      *
> > > >      * @param <K>
> > > >      * @param <V>
> > > >      * @param internal
> > > >      * @param key
> > > >      * @param value
> > > >      * @return
> > > >      */
> > > >     public static <K, V> NavigableMap<K, V> navigableMap(
> > > >             NavigableMap<Referrer<K>, Referrer<V>> internal, Ref key,
> > > > Ref value){
> > > >         return new ReferenceNavigableMap<K, V>(internal, key, value);
> > > >     }
> > > >     /**
> > > >      * Wrap a ConcurrentMap for holding references so it appears as a
> > > > ConcurrentMap
> > > >      * containing referents.
> > > >      *
> > > >      * @param <K> - key type.
> > > >      * @param <V> - value type.
> > > >      * @param internal - for holding references.
> > > >      * @param key - key reference type.
> > > >      * @param value - value reference type.
> > > >      * @return
> > > >      */
> > > >     public static <K, V> ConcurrentMap<K, V> concurrentMap(
> > > >             ConcurrentMap<Referrer<K>, Referrer<V>> internal, Ref key,
> > > > Ref value){
> > > >         return new ReferenceConcurrentMap<K, V>(internal, key, value);
> > > >     }
> > > > 
> > > >     /**
> > > >      * Wrap a ConcurrentNavigableMap for holding references so it
> > > > appears as a
> > > >      * ConcurrentNavigableMap containing referents.
> > > >      *
> > > >      * @param <K>
> > > >      * @param <V>
> > > >      * @param internal
> > > >      * @param key
> > > >      * @param value
> > > >      * @return
> > > >      */
> > > >     public static <K, V> ConcurrentNavigableMap<K, V>
> > > > concurrentNavigableMap(
> > > >             ConcurrentNavigableMap<Referrer<K>, Referrer<V>> internal,
> > > > Ref key, Ref value){
> > > >         return new ReferenceConcurrentNavigableMap<K, V>(internal, key,
> > > > value);
> > > >     }
> > > > }
> > > > 
> > > > 
> > > > _______________________________________________
> > > > Concurrency-interest mailing list
> > > > Concurrency-interest at cs.oswego.edu
> > > > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > > > 
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathan.reynolds at oracle.com  Thu Jan  5 18:45:38 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 05 Jan 2012 16:45:38 -0700
Subject: [concurrency-interest] Reference Collections
In-Reply-To: <1325805472.10496.6.camel@bluto>
References: <1325805472.10496.6.camel@bluto>
Message-ID: <4F063622.3000401@oracle.com>

Does using a referrer instead of a reference make any difference?  As 
long as the reference is garbage by the time GC starts then GC won't 
have to deal with it.  It won't have to track that there is a soft/weak 
link to an object because the Reference is garbage.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/5/2012 4:17 PM, Peter Firmstone wrote:
> Just thought I'd better clarify how reads are performed, the Referrer
> interface exists to avoid creating unnecessary references, it defines
> the equals and hashCode contracts all reference implementations must
> follow.
>
> /*
>   * Licensed to the Apache Software Foundation (ASF) under one
>   * or more contributor license agreements.  See the NOTICE file
>   * distributed with this work for additional information
>   * regarding copyright ownership. The ASF licenses this file
>   * to you under the Apache License, Version 2.0 (the
>   * "License"); you may not use this file except in compliance
>   * with the License. You may obtain a copy of the License at
>   *
>   *      http://www.apache.org/licenses/LICENSE-2.0
>   *
>   * Unless required by applicable law or agreed to in writing, software
>   * distributed under the License is distributed on an "AS IS" BASIS,
>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> implied.
>   * See the License for the specific language governing permissions and
>   * limitations under the License.
>   */
>
> package org.apache.river.impl.util;
>
> import java.lang.ref.Reference;
>
> /**
>   * An interface for References used in collections, it defines the
> equals
>   * and hashCode contracts as well as methods identical to Reference.
>   *
>   * A client may wish to implement this interface to replace a standard
> Referrer
>   * during serialisation with custom implementations to deconstruct and
>   * reconstruct non serialisable objects, or to perform integrity checks.
>   *
>   * This must be implemented in a Collection provided by the client.
>   *
>   * After de-serialisation is complete, the client Referrer will be
> replaced
>   * with a standard Referrer.
>   *
>   * @see Reference
>   * @see Ref
>   * @param<T>
>   * @author Peter Firmstone
>   */
> public interface Referrer<T>  {
>
>      /**
>       * @see Reference#get()
>       */
>      public T get() ;
>      /**
>       * @see Reference#clear()
>       */
>      public void clear();
>      /**
>       * @see Reference#isEnqueued()
>       * @return true if enqueued.
>       */
>      public boolean isEnqueued();
>      /**
>       * @see Reference#enqueue()
>       * @return
>       */
>      public boolean enqueue();
>
>      /**
>       * Equals is calculated on IDENTITY or equality.
>       *
>       * IDENTITY calculation:
>       *
>       * if (this == o) return true;
>       * if (!(o instanceof Referrer)) return false;
>       * Object k1 = get();
>       * Object k2 = ((Referrer) o).get();
>       * if ( k1 != null&&  k1 == k2 ) return true;
>       * return ( k1 == null&&  k2 == null&&  hashCode() == o.hashCode());
>       *
>       * Equality calculation:
>       *
>       * if (this == o)  return true; // Same reference.
>       * if (!(o instanceof Referrer))  return false;
>       * Object k1 = get();
>       * Object k2 = ((Referrer) o).get();
>       * if ( k1 != null&&  k1.equals(k2)) return true;
>       * return ( k1 == null&&  k2 == null&&  hashCode() == o.hashCode());
>       *
>       * @see Ref
>       * @param o
>       * @return
>       */
>      public boolean equals(Object o);
>
>      /**
>       * Standard hashCode calculation for IDENTITY based references,
> where k
>       * is the referent.
>       *
>       * int hash = 7;
>       * hash = 29 * hash + System.identityHashCode(k);
>       * hash = 29 * hash + k.getClass().hashCode();
>       *
>       * For non IDENTITY references, the hashCode returned is the
> referent's
>       * hashCode, after the reference has been cleared, it reverts to the
>       * IDENTITY based value.  The IDENTITY hashCode should be calculated
>       * at construction time.
>       *
>       * @return
>       */
>      public int hashCode();
> }
>
> Here's an implementation to avoid creating an unnecessary reference:
>
> /*
>   * Licensed to the Apache Software Foundation (ASF) under one
>   * or more contributor license agreements.  See the NOTICE file
>   * distributed with this work for additional information
>   * regarding copyright ownership. The ASF licenses this file
>   * to you under the Apache License, Version 2.0 (the
>   * "License"); you may not use this file except in compliance
>   * with the License. You may obtain a copy of the License at
>   *
>   *      http://www.apache.org/licenses/LICENSE-2.0
>   *
>   * Unless required by applicable law or agreed to in writing, software
>   * distributed under the License is distributed on an "AS IS" BASIS,
>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> implied.
>   * See the License for the specific language governing permissions and
>   * limitations under the License.
>   */
>
> package org.apache.river.impl.util;
>
> /**
>   *
>   * @author peter
>   */
> class TempIdentityReferrer<T>  implements Referrer<T>  {
>
>      private final T t;
>
>      TempIdentityReferrer(T t){
>          if ( t == null ) throw new NullPointerException("Null
> prohibited");
>          this.t = t;
>      }
>
>      @Override
>      public T get() {
>          return t;
>      }
>
>      @Override
>      public void clear() {
>          throw new UnsupportedOperationException("Not supported.");
>      }
>
>      @Override
>      public boolean isEnqueued() {
>          return false;
>      }
>
>      @Override
>      public boolean enqueue() {
>          return false;
>      }
>
>      @Override
>      public boolean equals(Object o) {
>          if (this == o) {
>              return true;
>          } else if (!(o instanceof Referrer)) {
>              return false;
>          }
>          Object t2 = ((Referrer) o).get();
>          return( t == t2 );
>      }
>
>      public int hashCode(){
>          int hash = 7;
>          hash = 29 * hash + System.identityHashCode(t);
>          hash = 29 * hash + t.getClass().hashCode();
>          return hash;
>      }
>
> }
>
>
> This is an implementation for a permanent Reference:
>
> /*
>   * Licensed to the Apache Software Foundation (ASF) under one
>   * or more contributor license agreements.  See the NOTICE file
>   * distributed with this work for additional information
>   * regarding copyright ownership. The ASF licenses this file
>   * to you under the Apache License, Version 2.0 (the
>   * "License"); you may not use this file except in compliance
>   * with the License. You may obtain a copy of the License at
>   *
>   *      http://www.apache.org/licenses/LICENSE-2.0
>   *
>   * Unless required by applicable law or agreed to in writing, software
>   * distributed under the License is distributed on an "AS IS" BASIS,
>   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> implied.
>   * See the License for the specific language governing permissions and
>   * limitations under the License.
>   */
>
> package org.apache.river.impl.util;
>
> import java.io.InvalidObjectException;
> import java.io.ObjectInputStream;
> import java.io.Serializable;
> import java.lang.ref.ReferenceQueue;
> import java.lang.ref.SoftReference;
>
> /**
>   * Implementation as per Ref.SOFT_IDENTITY
>   *
>   * @see Ref#SOFT_IDENTITY
>   * @author Peter Firmstone.
>   */
> class SoftIdentityReferenceKey<T>  extends SoftReference<T>  implements
> Referrer<T>, Serializable{
>      private static final long serialVersionUID = 1L;
>      private final int hash;
>
>      SoftIdentityReferenceKey(T k, ReferenceQueue<? super T>  q) {
>          super(k,q);
>          int hash = 7;
>          hash = 29 * hash + System.identityHashCode(k);
>          hash = 29 * hash + k.getClass().hashCode();
>          this.hash = hash;
>      }
>
>      @Override
>      public boolean equals(Object o) {
>          if (this == o) return true;
>          if (!(o instanceof Referrer)) return false;
>          Object k1 = get();
>          Object k2 = ((Referrer) o).get();
>          if ( k1 != null&&  k1 == k2 ) return true;
>          return ( k1 == null&&  k2 == null&&  hashCode() ==
> o.hashCode());
>      }
>
>      @Override
>      public int hashCode() {
>          return hash;
>      }
>
>      @Override
>      public String toString(){
>          Object s = get();
>          if (s != null) return s.toString();
>          return super.toString();
>      }
>
>      private Object writeReplace() {
>          // returns a Builder instead of this class.
>          return ReferenceSerializationFactory.create(get());
>      }
>
>      private void readObject(ObjectInputStream stream)
>              throws InvalidObjectException{
>          throw new InvalidObjectException("Factory required");
>      }
>
> }
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/d91b6582/attachment.html>

From howard.lovatt at gmail.com  Thu Jan  5 19:52:45 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Fri, 6 Jan 2012 11:52:45 +1100
Subject: [concurrency-interest] Nested synchronized
Message-ID: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>

Hi,

I have seen something I think is a JVM bug but would like to check my
understanding before reporting a problem. The following program normally
hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
6 or 7, 4 core processor. The problem is that there are synchronized
methods, isSetA1 and isSetA2 (near end of listing below), that call another
synchronized method, conditionallySumArguments (at end of listing below).
The second synchronized is unnecessary since the method is always called
within an already synchronized method and if the second synchronized is
removed the program works as expected. However I think an extra
synchronized should be redundant, not a problem?

package nestedsynchronizedproblem;

import java.util.concurrent.Callable;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;

import static java.lang.System.*;

/**
 * Test of nested synchronized. Mimics calling a parallel sum method.
 *
 * @author  Howard Lovatt
 */
public class NestedSynchronizedProblem {
  private static final int loops = 10 * 1000 * 1000; // This needs to be
large for hanging!

  public static void main( final String... notUsed ) throws
InterruptedException {
    final ParrallelSumMethod sum = new ParrallelSumMethod();
    final Callable<Void> setA1 = new Callable<Void>() {
      @Override public Void call() throws Exception {
        for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
        return null;
      }
    };
    final Callable<Void> setA2 = new Callable<Void>() {
      @Override public Void call() throws Exception {
        for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
        return null;
      }
    };
    final ExecutorService pool = Executors.newCachedThreadPool();
    pool.submit( setA1 );
    pool.submit( setA2 );
    pool.shutdown();
    final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
    out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed to
terminate") );
    pool.shutdownNow();
  }
}


final class ParrallelSumMethod {
  private long sum = 0;
  private Long a1 = null;
  private Long a2 = null;

  public void setA1( final long a1Arg ) throws InterruptedException {
    for ( ;; ) {
      if ( isSetA1( a1Arg ) ) { return; }
      checkForInterrupt();
    }
  }

  public void setA2( final long a2Arg ) throws InterruptedException {
    for ( ;; ) {
      if ( isSetA2( a2Arg ) ) { return; }
      checkForInterrupt();
    }
  }

  public Long getSum() { return sum; }

  private static void checkForInterrupt() throws InterruptedException {
    if ( Thread.interrupted() ) { throw new InterruptedException(); }
  }

  private synchronized boolean isSetA1( final long a1Arg ) {
    if ( a1 == null ) {
      a1 = a1Arg;
      conditionallySumArguments();
      return true;
    }
    return false;
  }

  private synchronized boolean isSetA2( final long a2Arg ) {
    if ( a2 == null ) {
      a2 = a2Arg;
      conditionallySumArguments();
      return true;
    }
    return false;
  }

  private synchronized void conditionallySumArguments() { // Works if not
synchronized!!!
    if ( ( a1 == null ) || ( a2 == null ) ) { return; }
    sum += a1 + a2;
    a1 = a2 = null;
  }
}


Thanks in advance for any comments,

  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/225253d1/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Jan  5 20:00:06 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 6 Jan 2012 11:00:06 +1000
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>

That's not nested synchronization as you are using two different objects. It
is a classic deadlock:

- sync method on Obj A calls sync method on Obj B
- sync method on Obj B calls sync method on Objj A

Thread 1 does the call to ObjA
Thread 2 does the call to Obj B

David
------
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Howard
Lovatt
  Sent: Friday, 6 January 2012 10:53 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Nested synchronized


  Hi,


  I have seen something I think is a JVM bug but would like to check my
understanding before reporting a problem. The following program normally
hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java 6
or 7, 4 core processor. The problem is that there are synchronized methods,
isSetA1 and isSetA2 (near end of listing below), that call another
synchronized method, conditionallySumArguments (at end of listing below).
The second synchronized is unnecessary since the method is always called
within an already synchronized method and if the second synchronized is
removed the program works as expected. However I think an extra synchronized
should be redundant, not a problem?


    package nestedsynchronizedproblem;


    import java.util.concurrent.Callable;
    import java.util.concurrent.ExecutorService;
    import java.util.concurrent.Executors;
    import java.util.concurrent.TimeUnit;


    import static java.lang.System.*;


    /**
     * Test of nested synchronized. Mimics calling a parallel sum method.
     *
     * @author  Howard Lovatt
     */
    public class NestedSynchronizedProblem {
      private static final int loops = 10 * 1000 * 1000; // This needs to be
large for hanging!


      public static void main( final String... notUsed ) throws
InterruptedException {
        final ParrallelSumMethod sum = new ParrallelSumMethod();
        final Callable<Void> setA1 = new Callable<Void>() {
          @Override public Void call() throws Exception {
            for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
            return null;
          }
        };
        final Callable<Void> setA2 = new Callable<Void>() {
          @Override public Void call() throws Exception {
            for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
            return null;
          }
        };
        final ExecutorService pool = Executors.newCachedThreadPool();
        pool.submit( setA1 );
        pool.submit( setA2 );
        pool.shutdown();
        final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
        out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed to
terminate") );
        pool.shutdownNow();
      }
    }




    final class ParrallelSumMethod {
      private long sum = 0;
      private Long a1 = null;
      private Long a2 = null;


      public void setA1( final long a1Arg ) throws InterruptedException {
        for ( ;; ) {
          if ( isSetA1( a1Arg ) ) { return; }
          checkForInterrupt();
        }
      }


      public void setA2( final long a2Arg ) throws InterruptedException {
        for ( ;; ) {
          if ( isSetA2( a2Arg ) ) { return; }
          checkForInterrupt();
        }
      }


      public Long getSum() { return sum; }


      private static void checkForInterrupt() throws InterruptedException {
        if ( Thread.interrupted() ) { throw new InterruptedException(); }
      }


      private synchronized boolean isSetA1( final long a1Arg ) {
        if ( a1 == null ) {
          a1 = a1Arg;
          conditionallySumArguments();
          return true;
        }
        return false;
      }


      private synchronized boolean isSetA2( final long a2Arg ) {
        if ( a2 == null ) {
          a2 = a2Arg;
          conditionallySumArguments();
          return true;
        }
        return false;
      }


      private synchronized void conditionallySumArguments() { // Works if
not synchronized!!!
        if ( ( a1 == null ) || ( a2 == null ) ) { return; }
        sum += a1 + a2;
        a1 = a2 = null;
      }
    }


  Thanks in advance for any comments,

    -- Howard.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/2528b38f/attachment.html>

From howard.lovatt at gmail.com  Thu Jan  5 20:12:52 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Fri, 6 Jan 2012 12:12:52 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
Message-ID: <CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>

Hi David,

There is only one sum object shared between the two threads (1st line of
main) and hence all synchronization is on the same object. Therefore I
think the code should work (even though the second synchronization is
redundant). As a double check on my understanding I just added a specific
mutex object to the code and synchronized on that and got the same result.

Have I understood you comment correctly?

Thanks,

 -- Howard.

On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au> wrote:

> **
> That's not nested synchronization as you are using two different objects.
> It is a classic deadlock:
>
> - sync method on Obj A calls sync method on Obj B
> - sync method on Obj B calls sync method on Objj A
>
> Thread 1 does the call to ObjA
> Thread 2 does the call to Obj B
>
> David
> ------
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard Lovatt
> *Sent:* Friday, 6 January 2012 10:53 AM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Nested synchronized
>
> Hi,
>
> I have seen something I think is a JVM bug but would like to check my
> understanding before reporting a problem. The following program normally
> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
> 6 or 7, 4 core processor. The problem is that there are synchronized
> methods, isSetA1 and isSetA2 (near end of listing below), that call another
> synchronized method, conditionallySumArguments (at end of listing below).
> The second synchronized is unnecessary since the method is always called
> within an already synchronized method and if the second synchronized is
> removed the program works as expected. However I think an extra
> synchronized should be redundant, not a problem?
>
>  package nestedsynchronizedproblem;
>
>  import java.util.concurrent.Callable;
>  import java.util.concurrent.ExecutorService;
>  import java.util.concurrent.Executors;
>  import java.util.concurrent.TimeUnit;
>
>  import static java.lang.System.*;
>
>  /**
>   * Test of nested synchronized. Mimics calling a parallel sum method.
>   *
>   * @author  Howard Lovatt
>   */
>  public class NestedSynchronizedProblem {
>    private static final int loops = 10 * 1000 * 1000; // This needs to be
> large for hanging!
>
>    public static void main( final String... notUsed ) throws
> InterruptedException {
>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>      final Callable<Void> setA1 = new Callable<Void>() {
>        @Override public Void call() throws Exception {
>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>          return null;
>        }
>      };
>      final Callable<Void> setA2 = new Callable<Void>() {
>        @Override public Void call() throws Exception {
>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>          return null;
>        }
>      };
>      final ExecutorService pool = Executors.newCachedThreadPool();
>      pool.submit( setA1 );
>      pool.submit( setA2 );
>      pool.shutdown();
>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed to
> terminate") );
>      pool.shutdownNow();
>    }
>  }
>
>
>  final class ParrallelSumMethod {
>    private long sum = 0;
>    private Long a1 = null;
>    private Long a2 = null;
>
>    public void setA1( final long a1Arg ) throws InterruptedException {
>      for ( ;; ) {
>        if ( isSetA1( a1Arg ) ) { return; }
>        checkForInterrupt();
>      }
>    }
>
>    public void setA2( final long a2Arg ) throws InterruptedException {
>      for ( ;; ) {
>        if ( isSetA2( a2Arg ) ) { return; }
>        checkForInterrupt();
>      }
>    }
>
>    public Long getSum() { return sum; }
>
>    private static void checkForInterrupt() throws InterruptedException {
>      if ( Thread.interrupted() ) { throw new InterruptedException(); }
>    }
>
>    private synchronized boolean isSetA1( final long a1Arg ) {
>      if ( a1 == null ) {
>        a1 = a1Arg;
>        conditionallySumArguments();
>        return true;
>      }
>      return false;
>    }
>
>    private synchronized boolean isSetA2( final long a2Arg ) {
>      if ( a2 == null ) {
>        a2 = a2Arg;
>        conditionallySumArguments();
>        return true;
>      }
>      return false;
>    }
>
>    private synchronized void conditionallySumArguments() { // Works if
> not synchronized!!!
>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>      sum += a1 + a2;
>      a1 = a2 = null;
>    }
>  }
>
>
> Thanks in advance for any comments,
>
>   -- Howard.
>
>


-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/9f960da6/attachment-0001.html>

From justin at krasama.com  Thu Jan  5 20:12:57 2012
From: justin at krasama.com (Justin T. Sampson)
Date: Thu, 5 Jan 2012 17:12:57 -0800
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
Message-ID: <CAAshuuFv9G3eBER_k3KC3AhD+WPY7YdvN6C6swauXgkcTVTSHw@mail.gmail.com>

That was my first thought, but there's only one instance
of ParrallelSumMethod, which is the class with the synchronized methods,
and therefore only one lock being acquired.

On Thu, Jan 5, 2012 at 5:00 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> That's not nested synchronization as you are using two different objects.
> It is a classic deadlock:
>
> - sync method on Obj A calls sync method on Obj B
> - sync method on Obj B calls sync method on Objj A
>
> Thread 1 does the call to ObjA
> Thread 2 does the call to Obj B
>
> David
> ------
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard Lovatt
> *Sent:* Friday, 6 January 2012 10:53 AM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Nested synchronized
>
> Hi,
>
> I have seen something I think is a JVM bug but would like to check my
> understanding before reporting a problem. The following program normally
> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
> 6 or 7, 4 core processor. The problem is that there are synchronized
> methods, isSetA1 and isSetA2 (near end of listing below), that call another
> synchronized method, conditionallySumArguments (at end of listing below).
> The second synchronized is unnecessary since the method is always called
> within an already synchronized method and if the second synchronized is
> removed the program works as expected. However I think an extra
> synchronized should be redundant, not a problem?
>
>  package nestedsynchronizedproblem;
>
>  import java.util.concurrent.Callable;
>  import java.util.concurrent.ExecutorService;
>  import java.util.concurrent.Executors;
>  import java.util.concurrent.TimeUnit;
>
>  import static java.lang.System.*;
>
>  /**
>   * Test of nested synchronized. Mimics calling a parallel sum method.
>   *
>   * @author  Howard Lovatt
>   */
>  public class NestedSynchronizedProblem {
>    private static final int loops = 10 * 1000 * 1000; // This needs to be
> large for hanging!
>
>    public static void main( final String... notUsed ) throws
> InterruptedException {
>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>      final Callable<Void> setA1 = new Callable<Void>() {
>        @Override public Void call() throws Exception {
>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>          return null;
>        }
>      };
>      final Callable<Void> setA2 = new Callable<Void>() {
>        @Override public Void call() throws Exception {
>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>          return null;
>        }
>      };
>      final ExecutorService pool = Executors.newCachedThreadPool();
>      pool.submit( setA1 );
>      pool.submit( setA2 );
>      pool.shutdown();
>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed to
> terminate") );
>      pool.shutdownNow();
>    }
>  }
>
>
>  final class ParrallelSumMethod {
>    private long sum = 0;
>    private Long a1 = null;
>    private Long a2 = null;
>
>    public void setA1( final long a1Arg ) throws InterruptedException {
>      for ( ;; ) {
>        if ( isSetA1( a1Arg ) ) { return; }
>        checkForInterrupt();
>      }
>    }
>
>    public void setA2( final long a2Arg ) throws InterruptedException {
>      for ( ;; ) {
>        if ( isSetA2( a2Arg ) ) { return; }
>        checkForInterrupt();
>      }
>    }
>
>    public Long getSum() { return sum; }
>
>    private static void checkForInterrupt() throws InterruptedException {
>      if ( Thread.interrupted() ) { throw new InterruptedException(); }
>    }
>
>    private synchronized boolean isSetA1( final long a1Arg ) {
>      if ( a1 == null ) {
>        a1 = a1Arg;
>        conditionallySumArguments();
>        return true;
>      }
>      return false;
>    }
>
>    private synchronized boolean isSetA2( final long a2Arg ) {
>      if ( a2 == null ) {
>        a2 = a2Arg;
>        conditionallySumArguments();
>        return true;
>      }
>      return false;
>    }
>
>    private synchronized void conditionallySumArguments() { // Works if
> not synchronized!!!
>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>      sum += a1 + a2;
>      a1 = a2 = null;
>    }
>  }
>
>
> Thanks in advance for any comments,
>
>   -- Howard.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/88b8517d/attachment.html>

From vitalyd at gmail.com  Thu Jan  5 20:18:06 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 5 Jan 2012 20:18:06 -0500
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
Message-ID: <CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>

Howard,

Have you captured call stacks when it hangs?

Vitaly
On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:

> Hi David,
>
> There is only one sum object shared between the two threads (1st line of
> main) and hence all synchronization is on the same object. Therefore I
> think the code should work (even though the second synchronization is
> redundant). As a double check on my understanding I just added a specific
> mutex object to the code and synchronized on that and got the same result.
>
> Have I understood you comment correctly?
>
> Thanks,
>
>  -- Howard.
>
> On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au> wrote:
>
>> **
>> That's not nested synchronization as you are using two different objects.
>> It is a classic deadlock:
>>
>> - sync method on Obj A calls sync method on Obj B
>> - sync method on Obj B calls sync method on Objj A
>>
>> Thread 1 does the call to ObjA
>> Thread 2 does the call to Obj B
>>
>> David
>> ------
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard Lovatt
>> *Sent:* Friday, 6 January 2012 10:53 AM
>> *To:* concurrency-interest at cs.oswego.edu
>> *Subject:* [concurrency-interest] Nested synchronized
>>
>> Hi,
>>
>> I have seen something I think is a JVM bug but would like to check my
>> understanding before reporting a problem. The following program normally
>> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
>> 6 or 7, 4 core processor. The problem is that there are synchronized
>> methods, isSetA1 and isSetA2 (near end of listing below), that call another
>> synchronized method, conditionallySumArguments (at end of listing below).
>> The second synchronized is unnecessary since the method is always called
>> within an already synchronized method and if the second synchronized is
>> removed the program works as expected. However I think an extra
>> synchronized should be redundant, not a problem?
>>
>>  package nestedsynchronizedproblem;
>>
>>  import java.util.concurrent.Callable;
>>  import java.util.concurrent.ExecutorService;
>>  import java.util.concurrent.Executors;
>>  import java.util.concurrent.TimeUnit;
>>
>>  import static java.lang.System.*;
>>
>>  /**
>>   * Test of nested synchronized. Mimics calling a parallel sum method.
>>   *
>>   * @author  Howard Lovatt
>>   */
>>  public class NestedSynchronizedProblem {
>>    private static final int loops = 10 * 1000 * 1000; // This needs to
>> be large for hanging!
>>
>>    public static void main( final String... notUsed ) throws
>> InterruptedException {
>>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>>      final Callable<Void> setA1 = new Callable<Void>() {
>>        @Override public Void call() throws Exception {
>>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>>          return null;
>>        }
>>      };
>>      final Callable<Void> setA2 = new Callable<Void>() {
>>        @Override public Void call() throws Exception {
>>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>>          return null;
>>        }
>>      };
>>      final ExecutorService pool = Executors.newCachedThreadPool();
>>      pool.submit( setA1 );
>>      pool.submit( setA2 );
>>      pool.shutdown();
>>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed to
>> terminate") );
>>      pool.shutdownNow();
>>    }
>>  }
>>
>>
>>  final class ParrallelSumMethod {
>>    private long sum = 0;
>>    private Long a1 = null;
>>    private Long a2 = null;
>>
>>    public void setA1( final long a1Arg ) throws InterruptedException {
>>      for ( ;; ) {
>>        if ( isSetA1( a1Arg ) ) { return; }
>>        checkForInterrupt();
>>      }
>>    }
>>
>>    public void setA2( final long a2Arg ) throws InterruptedException {
>>      for ( ;; ) {
>>        if ( isSetA2( a2Arg ) ) { return; }
>>        checkForInterrupt();
>>      }
>>    }
>>
>>    public Long getSum() { return sum; }
>>
>>    private static void checkForInterrupt() throws InterruptedException {
>>      if ( Thread.interrupted() ) { throw new InterruptedException(); }
>>    }
>>
>>    private synchronized boolean isSetA1( final long a1Arg ) {
>>      if ( a1 == null ) {
>>        a1 = a1Arg;
>>        conditionallySumArguments();
>>        return true;
>>      }
>>      return false;
>>    }
>>
>>    private synchronized boolean isSetA2( final long a2Arg ) {
>>      if ( a2 == null ) {
>>        a2 = a2Arg;
>>        conditionallySumArguments();
>>        return true;
>>      }
>>      return false;
>>    }
>>
>>    private synchronized void conditionallySumArguments() { // Works if
>> not synchronized!!!
>>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>>      sum += a1 + a2;
>>      a1 = a2 = null;
>>    }
>>  }
>>
>>
>> Thanks in advance for any comments,
>>
>>   -- Howard.
>>
>>
>
>
> --
>   -- Howard.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/0a82f1dc/attachment-0001.html>

From howard.lovatt at gmail.com  Thu Jan  5 20:27:55 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Fri, 6 Jan 2012 12:27:55 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
Message-ID: <CACR_FB5STPMUyO4F1=9S2wfrz_46jyZDW4Bp2H=nMGdgZVL3-Q@mail.gmail.com>

Hi Vitaly,

If I increase the awaitTermination to 2 minutes I can run the program in
the debugger, however it works then! Is there another away, without using
the debugger, of generating a stack trace for a hanging program?

Thanks,

 -- Howard.

On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Howard,
>
> Have you captured call stacks when it hangs?
>
> Vitaly
> On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>
>> Hi David,
>>
>> There is only one sum object shared between the two threads (1st line of
>> main) and hence all synchronization is on the same object. Therefore I
>> think the code should work (even though the second synchronization is
>> redundant). As a double check on my understanding I just added a specific
>> mutex object to the code and synchronized on that and got the same result.
>>
>> Have I understood you comment correctly?
>>
>> Thanks,
>>
>>  -- Howard.
>>
>> On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au> wrote:
>>
>>> **
>>> That's not nested synchronization as you are using two different
>>> objects. It is a classic deadlock:
>>>
>>> - sync method on Obj A calls sync method on Obj B
>>> - sync method on Obj B calls sync method on Objj A
>>>
>>> Thread 1 does the call to ObjA
>>> Thread 2 does the call to Obj B
>>>
>>> David
>>> ------
>>>
>>> -----Original Message-----
>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard Lovatt
>>> *Sent:* Friday, 6 January 2012 10:53 AM
>>> *To:* concurrency-interest at cs.oswego.edu
>>> *Subject:* [concurrency-interest] Nested synchronized
>>>
>>> Hi,
>>>
>>> I have seen something I think is a JVM bug but would like to check my
>>> understanding before reporting a problem. The following program normally
>>> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
>>> 6 or 7, 4 core processor. The problem is that there are synchronized
>>> methods, isSetA1 and isSetA2 (near end of listing below), that call another
>>> synchronized method, conditionallySumArguments (at end of listing below).
>>> The second synchronized is unnecessary since the method is always called
>>> within an already synchronized method and if the second synchronized is
>>> removed the program works as expected. However I think an extra
>>> synchronized should be redundant, not a problem?
>>>
>>>  package nestedsynchronizedproblem;
>>>
>>>  import java.util.concurrent.Callable;
>>>  import java.util.concurrent.ExecutorService;
>>>  import java.util.concurrent.Executors;
>>>  import java.util.concurrent.TimeUnit;
>>>
>>>  import static java.lang.System.*;
>>>
>>>  /**
>>>   * Test of nested synchronized. Mimics calling a parallel sum method.
>>>   *
>>>   * @author  Howard Lovatt
>>>   */
>>>  public class NestedSynchronizedProblem {
>>>    private static final int loops = 10 * 1000 * 1000; // This needs to
>>> be large for hanging!
>>>
>>>    public static void main( final String... notUsed ) throws
>>> InterruptedException {
>>>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>>>      final Callable<Void> setA1 = new Callable<Void>() {
>>>        @Override public Void call() throws Exception {
>>>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>>>          return null;
>>>        }
>>>      };
>>>      final Callable<Void> setA2 = new Callable<Void>() {
>>>        @Override public Void call() throws Exception {
>>>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>>>          return null;
>>>        }
>>>      };
>>>      final ExecutorService pool = Executors.newCachedThreadPool();
>>>      pool.submit( setA1 );
>>>      pool.submit( setA2 );
>>>      pool.shutdown();
>>>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>>>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed to
>>> terminate") );
>>>      pool.shutdownNow();
>>>    }
>>>  }
>>>
>>>
>>>  final class ParrallelSumMethod {
>>>    private long sum = 0;
>>>    private Long a1 = null;
>>>    private Long a2 = null;
>>>
>>>    public void setA1( final long a1Arg ) throws InterruptedException {
>>>      for ( ;; ) {
>>>        if ( isSetA1( a1Arg ) ) { return; }
>>>        checkForInterrupt();
>>>      }
>>>    }
>>>
>>>    public void setA2( final long a2Arg ) throws InterruptedException {
>>>      for ( ;; ) {
>>>        if ( isSetA2( a2Arg ) ) { return; }
>>>        checkForInterrupt();
>>>      }
>>>    }
>>>
>>>    public Long getSum() { return sum; }
>>>
>>>    private static void checkForInterrupt() throws InterruptedException {
>>>      if ( Thread.interrupted() ) { throw new InterruptedException(); }
>>>    }
>>>
>>>    private synchronized boolean isSetA1( final long a1Arg ) {
>>>      if ( a1 == null ) {
>>>        a1 = a1Arg;
>>>        conditionallySumArguments();
>>>        return true;
>>>      }
>>>      return false;
>>>    }
>>>
>>>    private synchronized boolean isSetA2( final long a2Arg ) {
>>>      if ( a2 == null ) {
>>>        a2 = a2Arg;
>>>        conditionallySumArguments();
>>>        return true;
>>>      }
>>>      return false;
>>>    }
>>>
>>>    private synchronized void conditionallySumArguments() { // Works if
>>> not synchronized!!!
>>>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>>>      sum += a1 + a2;
>>>      a1 = a2 = null;
>>>    }
>>>  }
>>>
>>>
>>> Thanks in advance for any comments,
>>>
>>>   -- Howard.
>>>
>>>
>>
>>
>> --
>>   -- Howard.
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>


-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/668fbd96/attachment.html>

From vitalyd at gmail.com  Thu Jan  5 20:44:22 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 5 Jan 2012 20:44:22 -0500
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB5STPMUyO4F1=9S2wfrz_46jyZDW4Bp2H=nMGdgZVL3-Q@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
	<CACR_FB5STPMUyO4F1=9S2wfrz_46jyZDW4Bp2H=nMGdgZVL3-Q@mail.gmail.com>
Message-ID: <CAHjP37EETfOkUmv1gRwWSedVz=LM3Yze2Ucp94FGCLfFe0D4qw@mail.gmail.com>

a bit crude but if its somewhat easy to repro try increasing the await
period to something like 10 mins and then jstack it a few times and see
what it shows.

Also can you repro in interpreter (i.e. -Xint)?
On Jan 5, 2012 8:27 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:

> Hi Vitaly,
>
> If I increase the awaitTermination to 2 minutes I can run the program in
> the debugger, however it works then! Is there another away, without using
> the debugger, of generating a stack trace for a hanging program?
>
> Thanks,
>
>  -- Howard.
>
> On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
>> Howard,
>>
>> Have you captured call stacks when it hangs?
>>
>> Vitaly
>> On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>>
>>> Hi David,
>>>
>>> There is only one sum object shared between the two threads (1st line of
>>> main) and hence all synchronization is on the same object. Therefore I
>>> think the code should work (even though the second synchronization is
>>> redundant). As a double check on my understanding I just added a specific
>>> mutex object to the code and synchronized on that and got the same result.
>>>
>>> Have I understood you comment correctly?
>>>
>>> Thanks,
>>>
>>>  -- Howard.
>>>
>>> On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au> wrote:
>>>
>>>> **
>>>> That's not nested synchronization as you are using two different
>>>> objects. It is a classic deadlock:
>>>>
>>>> - sync method on Obj A calls sync method on Obj B
>>>> - sync method on Obj B calls sync method on Objj A
>>>>
>>>> Thread 1 does the call to ObjA
>>>> Thread 2 does the call to Obj B
>>>>
>>>> David
>>>> ------
>>>>
>>>> -----Original Message-----
>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard Lovatt
>>>> *Sent:* Friday, 6 January 2012 10:53 AM
>>>> *To:* concurrency-interest at cs.oswego.edu
>>>> *Subject:* [concurrency-interest] Nested synchronized
>>>>
>>>> Hi,
>>>>
>>>> I have seen something I think is a JVM bug but would like to check my
>>>> understanding before reporting a problem. The following program normally
>>>> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
>>>> 6 or 7, 4 core processor. The problem is that there are synchronized
>>>> methods, isSetA1 and isSetA2 (near end of listing below), that call another
>>>> synchronized method, conditionallySumArguments (at end of listing below).
>>>> The second synchronized is unnecessary since the method is always called
>>>> within an already synchronized method and if the second synchronized is
>>>> removed the program works as expected. However I think an extra
>>>> synchronized should be redundant, not a problem?
>>>>
>>>>  package nestedsynchronizedproblem;
>>>>
>>>>  import java.util.concurrent.Callable;
>>>>  import java.util.concurrent.ExecutorService;
>>>>  import java.util.concurrent.Executors;
>>>>  import java.util.concurrent.TimeUnit;
>>>>
>>>>  import static java.lang.System.*;
>>>>
>>>>  /**
>>>>   * Test of nested synchronized. Mimics calling a parallel sum method.
>>>>   *
>>>>   * @author  Howard Lovatt
>>>>   */
>>>>  public class NestedSynchronizedProblem {
>>>>    private static final int loops = 10 * 1000 * 1000; // This needs to
>>>> be large for hanging!
>>>>
>>>>    public static void main( final String... notUsed ) throws
>>>> InterruptedException {
>>>>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>>>>      final Callable<Void> setA1 = new Callable<Void>() {
>>>>        @Override public Void call() throws Exception {
>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>>>>          return null;
>>>>        }
>>>>      };
>>>>      final Callable<Void> setA2 = new Callable<Void>() {
>>>>        @Override public Void call() throws Exception {
>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>>>>          return null;
>>>>        }
>>>>      };
>>>>      final ExecutorService pool = Executors.newCachedThreadPool();
>>>>      pool.submit( setA1 );
>>>>      pool.submit( setA2 );
>>>>      pool.shutdown();
>>>>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>>>>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed
>>>> to terminate") );
>>>>      pool.shutdownNow();
>>>>    }
>>>>  }
>>>>
>>>>
>>>>  final class ParrallelSumMethod {
>>>>    private long sum = 0;
>>>>    private Long a1 = null;
>>>>    private Long a2 = null;
>>>>
>>>>    public void setA1( final long a1Arg ) throws InterruptedException {
>>>>      for ( ;; ) {
>>>>        if ( isSetA1( a1Arg ) ) { return; }
>>>>        checkForInterrupt();
>>>>      }
>>>>    }
>>>>
>>>>    public void setA2( final long a2Arg ) throws InterruptedException {
>>>>      for ( ;; ) {
>>>>        if ( isSetA2( a2Arg ) ) { return; }
>>>>        checkForInterrupt();
>>>>      }
>>>>    }
>>>>
>>>>    public Long getSum() { return sum; }
>>>>
>>>>    private static void checkForInterrupt() throws InterruptedException
>>>> {
>>>>      if ( Thread.interrupted() ) { throw new InterruptedException(); }
>>>>    }
>>>>
>>>>    private synchronized boolean isSetA1( final long a1Arg ) {
>>>>      if ( a1 == null ) {
>>>>        a1 = a1Arg;
>>>>        conditionallySumArguments();
>>>>        return true;
>>>>      }
>>>>      return false;
>>>>    }
>>>>
>>>>    private synchronized boolean isSetA2( final long a2Arg ) {
>>>>      if ( a2 == null ) {
>>>>        a2 = a2Arg;
>>>>        conditionallySumArguments();
>>>>        return true;
>>>>      }
>>>>      return false;
>>>>    }
>>>>
>>>>    private synchronized void conditionallySumArguments() { // Works if
>>>> not synchronized!!!
>>>>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>>>>      sum += a1 + a2;
>>>>      a1 = a2 = null;
>>>>    }
>>>>  }
>>>>
>>>>
>>>> Thanks in advance for any comments,
>>>>
>>>>   -- Howard.
>>>>
>>>>
>>>
>>>
>>> --
>>>   -- Howard.
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
>
> --
>   -- Howard.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/2432cc4a/attachment-0001.html>

From howard.lovatt at gmail.com  Thu Jan  5 20:44:52 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Fri, 6 Jan 2012 12:44:52 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
Message-ID: <CACR_FB4oyNvgDuD5jWcqb0ARKPqLHd+SWMnKr_Pn9r21HO9PWg@mail.gmail.com>

Hi Vitaly,

If I add the following JVM options, -ea -XX:+UseBiasedLocking
-XX:-UseSpinning -XX:+UseTLAB -XX:+UseThreadPriorities, then the program
runs. Indicating to me that it is a JVM problem.

 -- Howard.

On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Howard,
>
> Have you captured call stacks when it hangs?
>
> Vitaly
> On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>
>> Hi David,
>>
>> There is only one sum object shared between the two threads (1st line of
>> main) and hence all synchronization is on the same object. Therefore I
>> think the code should work (even though the second synchronization is
>> redundant). As a double check on my understanding I just added a specific
>> mutex object to the code and synchronized on that and got the same result.
>>
>> Have I understood you comment correctly?
>>
>> Thanks,
>>
>>  -- Howard.
>>
>> On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au> wrote:
>>
>>> **
>>> That's not nested synchronization as you are using two different
>>> objects. It is a classic deadlock:
>>>
>>> - sync method on Obj A calls sync method on Obj B
>>> - sync method on Obj B calls sync method on Objj A
>>>
>>> Thread 1 does the call to ObjA
>>> Thread 2 does the call to Obj B
>>>
>>> David
>>> ------
>>>
>>> -----Original Message-----
>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard Lovatt
>>> *Sent:* Friday, 6 January 2012 10:53 AM
>>> *To:* concurrency-interest at cs.oswego.edu
>>> *Subject:* [concurrency-interest] Nested synchronized
>>>
>>> Hi,
>>>
>>> I have seen something I think is a JVM bug but would like to check my
>>> understanding before reporting a problem. The following program normally
>>> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
>>> 6 or 7, 4 core processor. The problem is that there are synchronized
>>> methods, isSetA1 and isSetA2 (near end of listing below), that call another
>>> synchronized method, conditionallySumArguments (at end of listing below).
>>> The second synchronized is unnecessary since the method is always called
>>> within an already synchronized method and if the second synchronized is
>>> removed the program works as expected. However I think an extra
>>> synchronized should be redundant, not a problem?
>>>
>>>  package nestedsynchronizedproblem;
>>>
>>>  import java.util.concurrent.Callable;
>>>  import java.util.concurrent.ExecutorService;
>>>  import java.util.concurrent.Executors;
>>>  import java.util.concurrent.TimeUnit;
>>>
>>>  import static java.lang.System.*;
>>>
>>>  /**
>>>   * Test of nested synchronized. Mimics calling a parallel sum method.
>>>   *
>>>   * @author  Howard Lovatt
>>>   */
>>>  public class NestedSynchronizedProblem {
>>>    private static final int loops = 10 * 1000 * 1000; // This needs to
>>> be large for hanging!
>>>
>>>    public static void main( final String... notUsed ) throws
>>> InterruptedException {
>>>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>>>      final Callable<Void> setA1 = new Callable<Void>() {
>>>        @Override public Void call() throws Exception {
>>>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>>>          return null;
>>>        }
>>>      };
>>>      final Callable<Void> setA2 = new Callable<Void>() {
>>>        @Override public Void call() throws Exception {
>>>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>>>          return null;
>>>        }
>>>      };
>>>      final ExecutorService pool = Executors.newCachedThreadPool();
>>>      pool.submit( setA1 );
>>>      pool.submit( setA2 );
>>>      pool.shutdown();
>>>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>>>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed to
>>> terminate") );
>>>      pool.shutdownNow();
>>>    }
>>>  }
>>>
>>>
>>>  final class ParrallelSumMethod {
>>>    private long sum = 0;
>>>    private Long a1 = null;
>>>    private Long a2 = null;
>>>
>>>    public void setA1( final long a1Arg ) throws InterruptedException {
>>>      for ( ;; ) {
>>>        if ( isSetA1( a1Arg ) ) { return; }
>>>        checkForInterrupt();
>>>      }
>>>    }
>>>
>>>    public void setA2( final long a2Arg ) throws InterruptedException {
>>>      for ( ;; ) {
>>>        if ( isSetA2( a2Arg ) ) { return; }
>>>        checkForInterrupt();
>>>      }
>>>    }
>>>
>>>    public Long getSum() { return sum; }
>>>
>>>    private static void checkForInterrupt() throws InterruptedException {
>>>      if ( Thread.interrupted() ) { throw new InterruptedException(); }
>>>    }
>>>
>>>    private synchronized boolean isSetA1( final long a1Arg ) {
>>>      if ( a1 == null ) {
>>>        a1 = a1Arg;
>>>        conditionallySumArguments();
>>>        return true;
>>>      }
>>>      return false;
>>>    }
>>>
>>>    private synchronized boolean isSetA2( final long a2Arg ) {
>>>      if ( a2 == null ) {
>>>        a2 = a2Arg;
>>>        conditionallySumArguments();
>>>        return true;
>>>      }
>>>      return false;
>>>    }
>>>
>>>    private synchronized void conditionallySumArguments() { // Works if
>>> not synchronized!!!
>>>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>>>      sum += a1 + a2;
>>>      a1 = a2 = null;
>>>    }
>>>  }
>>>
>>>
>>> Thanks in advance for any comments,
>>>
>>>   -- Howard.
>>>
>>>
>>
>>
>> --
>>   -- Howard.
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>


-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/82c73c23/attachment.html>

From vitalyd at gmail.com  Thu Jan  5 20:55:08 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 5 Jan 2012 20:55:08 -0500
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB4oyNvgDuD5jWcqb0ARKPqLHd+SWMnKr_Pn9r21HO9PWg@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
	<CACR_FB4oyNvgDuD5jWcqb0ARKPqLHd+SWMnKr_Pn9r21HO9PWg@mail.gmail.com>
Message-ID: <CAHjP37HgMx=i0aDAF-65SKazXZOZ7qa-n_AEnXdXzGk4FmbJBQ@mail.gmail.com>

Possible but would be good to have call stacks.  Also can you try using a
reentrant lock instead of synchronized?
On Jan 5, 2012 8:44 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:

> Hi Vitaly,
>
> If I add the following JVM options, -ea -XX:+UseBiasedLocking
> -XX:-UseSpinning -XX:+UseTLAB -XX:+UseThreadPriorities, then the program
> runs. Indicating to me that it is a JVM problem.
>
>  -- Howard.
>
> On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
>> Howard,
>>
>> Have you captured call stacks when it hangs?
>>
>> Vitaly
>> On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>>
>>> Hi David,
>>>
>>> There is only one sum object shared between the two threads (1st line of
>>> main) and hence all synchronization is on the same object. Therefore I
>>> think the code should work (even though the second synchronization is
>>> redundant). As a double check on my understanding I just added a specific
>>> mutex object to the code and synchronized on that and got the same result.
>>>
>>> Have I understood you comment correctly?
>>>
>>> Thanks,
>>>
>>>  -- Howard.
>>>
>>> On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au> wrote:
>>>
>>>> **
>>>> That's not nested synchronization as you are using two different
>>>> objects. It is a classic deadlock:
>>>>
>>>> - sync method on Obj A calls sync method on Obj B
>>>> - sync method on Obj B calls sync method on Objj A
>>>>
>>>> Thread 1 does the call to ObjA
>>>> Thread 2 does the call to Obj B
>>>>
>>>> David
>>>> ------
>>>>
>>>> -----Original Message-----
>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard Lovatt
>>>> *Sent:* Friday, 6 January 2012 10:53 AM
>>>> *To:* concurrency-interest at cs.oswego.edu
>>>> *Subject:* [concurrency-interest] Nested synchronized
>>>>
>>>> Hi,
>>>>
>>>> I have seen something I think is a JVM bug but would like to check my
>>>> understanding before reporting a problem. The following program normally
>>>> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
>>>> 6 or 7, 4 core processor. The problem is that there are synchronized
>>>> methods, isSetA1 and isSetA2 (near end of listing below), that call another
>>>> synchronized method, conditionallySumArguments (at end of listing below).
>>>> The second synchronized is unnecessary since the method is always called
>>>> within an already synchronized method and if the second synchronized is
>>>> removed the program works as expected. However I think an extra
>>>> synchronized should be redundant, not a problem?
>>>>
>>>>  package nestedsynchronizedproblem;
>>>>
>>>>  import java.util.concurrent.Callable;
>>>>  import java.util.concurrent.ExecutorService;
>>>>  import java.util.concurrent.Executors;
>>>>  import java.util.concurrent.TimeUnit;
>>>>
>>>>  import static java.lang.System.*;
>>>>
>>>>  /**
>>>>   * Test of nested synchronized. Mimics calling a parallel sum method.
>>>>   *
>>>>   * @author  Howard Lovatt
>>>>   */
>>>>  public class NestedSynchronizedProblem {
>>>>    private static final int loops = 10 * 1000 * 1000; // This needs to
>>>> be large for hanging!
>>>>
>>>>    public static void main( final String... notUsed ) throws
>>>> InterruptedException {
>>>>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>>>>      final Callable<Void> setA1 = new Callable<Void>() {
>>>>        @Override public Void call() throws Exception {
>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>>>>          return null;
>>>>        }
>>>>      };
>>>>      final Callable<Void> setA2 = new Callable<Void>() {
>>>>        @Override public Void call() throws Exception {
>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>>>>          return null;
>>>>        }
>>>>      };
>>>>      final ExecutorService pool = Executors.newCachedThreadPool();
>>>>      pool.submit( setA1 );
>>>>      pool.submit( setA2 );
>>>>      pool.shutdown();
>>>>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>>>>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed
>>>> to terminate") );
>>>>      pool.shutdownNow();
>>>>    }
>>>>  }
>>>>
>>>>
>>>>  final class ParrallelSumMethod {
>>>>    private long sum = 0;
>>>>    private Long a1 = null;
>>>>    private Long a2 = null;
>>>>
>>>>    public void setA1( final long a1Arg ) throws InterruptedException {
>>>>      for ( ;; ) {
>>>>        if ( isSetA1( a1Arg ) ) { return; }
>>>>        checkForInterrupt();
>>>>      }
>>>>    }
>>>>
>>>>    public void setA2( final long a2Arg ) throws InterruptedException {
>>>>      for ( ;; ) {
>>>>        if ( isSetA2( a2Arg ) ) { return; }
>>>>        checkForInterrupt();
>>>>      }
>>>>    }
>>>>
>>>>    public Long getSum() { return sum; }
>>>>
>>>>    private static void checkForInterrupt() throws InterruptedException
>>>> {
>>>>      if ( Thread.interrupted() ) { throw new InterruptedException(); }
>>>>    }
>>>>
>>>>    private synchronized boolean isSetA1( final long a1Arg ) {
>>>>      if ( a1 == null ) {
>>>>        a1 = a1Arg;
>>>>        conditionallySumArguments();
>>>>        return true;
>>>>      }
>>>>      return false;
>>>>    }
>>>>
>>>>    private synchronized boolean isSetA2( final long a2Arg ) {
>>>>      if ( a2 == null ) {
>>>>        a2 = a2Arg;
>>>>        conditionallySumArguments();
>>>>        return true;
>>>>      }
>>>>      return false;
>>>>    }
>>>>
>>>>    private synchronized void conditionallySumArguments() { // Works if
>>>> not synchronized!!!
>>>>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>>>>      sum += a1 + a2;
>>>>      a1 = a2 = null;
>>>>    }
>>>>  }
>>>>
>>>>
>>>> Thanks in advance for any comments,
>>>>
>>>>   -- Howard.
>>>>
>>>>
>>>
>>>
>>> --
>>>   -- Howard.
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
>
> --
>   -- Howard.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/c775b7b2/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Jan  5 20:59:17 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 6 Jan 2012 11:59:17 +1000
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEDIJCAA.davidcholmes@aapt.net.au>

Sorry - too many occurrences of setA1/setA2 and I misread it.

You can get a Java stacktrace using ctrl-/ when it hangs. But you will need
native stack info I think, so you need to use "jstack -m" if it is supported
on the Mac.

Seems very likely to be a VM bug, but as this is the existing Apple port I
know nothing about the internals. Even the flags you later list may have
different default values and semantics.

David

 -----Original Message-----
From: Howard Lovatt [mailto:howard.lovatt at gmail.com]
Sent: Friday, 6 January 2012 11:13 AM
To: dholmes at ieee.org
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Nested synchronized


  Hi David,


  There is only one sum object shared between the two threads (1st line of
main) and hence all synchronization is on the same object. Therefore I think
the code should work (even though the second synchronization is redundant).
As a double check on my understanding I just added a specific mutex object
to the code and synchronized on that and got the same result.


  Have I understood you comment correctly?


  Thanks,


   -- Howard.


  On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au> wrote:

    That's not nested synchronization as you are using two different
objects. It is a classic deadlock:

    - sync method on Obj A calls sync method on Obj B
    - sync method on Obj B calls sync method on Objj A

    Thread 1 does the call to ObjA
    Thread 2 does the call to Obj B

    David
    ------
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Howard
Lovatt
      Sent: Friday, 6 January 2012 10:53 AM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] Nested synchronized


      Hi,


      I have seen something I think is a JVM bug but would like to check my
understanding before reporting a problem. The following program normally
hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java 6
or 7, 4 core processor. The problem is that there are synchronized methods,
isSetA1 and isSetA2 (near end of listing below), that call another
synchronized method, conditionallySumArguments (at end of listing below).
The second synchronized is unnecessary since the method is always called
within an already synchronized method and if the second synchronized is
removed the program works as expected. However I think an extra synchronized
should be redundant, not a problem?


        package nestedsynchronizedproblem;


        import java.util.concurrent.Callable;
        import java.util.concurrent.ExecutorService;
        import java.util.concurrent.Executors;
        import java.util.concurrent.TimeUnit;


        import static java.lang.System.*;


        /**
         * Test of nested synchronized. Mimics calling a parallel sum
method.
         *
         * @author  Howard Lovatt
         */
        public class NestedSynchronizedProblem {
          private static final int loops = 10 * 1000 * 1000; // This needs
to be large for hanging!


          public static void main( final String... notUsed ) throws
InterruptedException {
            final ParrallelSumMethod sum = new ParrallelSumMethod();
            final Callable<Void> setA1 = new Callable<Void>() {
              @Override public Void call() throws Exception {
                for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
                return null;
              }
            };
            final Callable<Void> setA2 = new Callable<Void>() {
              @Override public Void call() throws Exception {
                for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
                return null;
              }
            };
            final ExecutorService pool = Executors.newCachedThreadPool();
            pool.submit( setA1 );
            pool.submit( setA2 );
            pool.shutdown();
            final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
            out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed
to terminate") );
            pool.shutdownNow();
          }
        }




        final class ParrallelSumMethod {
          private long sum = 0;
          private Long a1 = null;
          private Long a2 = null;


          public void setA1( final long a1Arg ) throws InterruptedException
{
            for ( ;; ) {
              if ( isSetA1( a1Arg ) ) { return; }
              checkForInterrupt();
            }
          }


          public void setA2( final long a2Arg ) throws InterruptedException
{
            for ( ;; ) {
              if ( isSetA2( a2Arg ) ) { return; }
              checkForInterrupt();
            }
          }


          public Long getSum() { return sum; }


          private static void checkForInterrupt() throws
InterruptedException {
            if ( Thread.interrupted() ) { throw new
InterruptedException(); }
          }


          private synchronized boolean isSetA1( final long a1Arg ) {
            if ( a1 == null ) {
              a1 = a1Arg;
              conditionallySumArguments();
              return true;
            }
            return false;
          }


          private synchronized boolean isSetA2( final long a2Arg ) {
            if ( a2 == null ) {
              a2 = a2Arg;
              conditionallySumArguments();
              return true;
            }
            return false;
          }


          private synchronized void conditionallySumArguments() { // Works
if not synchronized!!!
            if ( ( a1 == null ) || ( a2 == null ) ) { return; }
            sum += a1 + a2;
            a1 = a2 = null;
          }
        }


      Thanks in advance for any comments,

        -- Howard.







  --
    -- Howard.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/1a1f26ef/attachment.html>

From vitalyd at gmail.com  Thu Jan  5 21:01:49 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 5 Jan 2012 21:01:49 -0500
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CAHjP37HgMx=i0aDAF-65SKazXZOZ7qa-n_AEnXdXzGk4FmbJBQ@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
	<CACR_FB4oyNvgDuD5jWcqb0ARKPqLHd+SWMnKr_Pn9r21HO9PWg@mail.gmail.com>
	<CAHjP37HgMx=i0aDAF-65SKazXZOZ7qa-n_AEnXdXzGk4FmbJBQ@mail.gmail.com>
Message-ID: <CAHjP37GPkjG_KXLdhA0VeVKGkDAGC+C8gMNmj3eK8r8ohZFYAg@mail.gmail.com>

Sorry left another bit out.  Since the suspicion is the nesting try
-XX:-EliminateLocks, IIRC.  Get rid of the other flags too so we're
controlling one thing at a time.
On Jan 5, 2012 8:55 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> Possible but would be good to have call stacks.  Also can you try using a
> reentrant lock instead of synchronized?
> On Jan 5, 2012 8:44 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>
>> Hi Vitaly,
>>
>> If I add the following JVM options, -ea -XX:+UseBiasedLocking
>> -XX:-UseSpinning -XX:+UseTLAB -XX:+UseThreadPriorities, then the program
>> runs. Indicating to me that it is a JVM problem.
>>
>>  -- Howard.
>>
>> On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>>> Howard,
>>>
>>> Have you captured call stacks when it hangs?
>>>
>>> Vitaly
>>> On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>>>
>>>> Hi David,
>>>>
>>>> There is only one sum object shared between the two threads (1st line
>>>> of main) and hence all synchronization is on the same object. Therefore I
>>>> think the code should work (even though the second synchronization is
>>>> redundant). As a double check on my understanding I just added a specific
>>>> mutex object to the code and synchronized on that and got the same result.
>>>>
>>>> Have I understood you comment correctly?
>>>>
>>>> Thanks,
>>>>
>>>>  -- Howard.
>>>>
>>>> On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au> wrote:
>>>>
>>>>> **
>>>>> That's not nested synchronization as you are using two different
>>>>> objects. It is a classic deadlock:
>>>>>
>>>>> - sync method on Obj A calls sync method on Obj B
>>>>> - sync method on Obj B calls sync method on Objj A
>>>>>
>>>>> Thread 1 does the call to ObjA
>>>>> Thread 2 does the call to Obj B
>>>>>
>>>>> David
>>>>> ------
>>>>>
>>>>> -----Original Message-----
>>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard
>>>>> Lovatt
>>>>> *Sent:* Friday, 6 January 2012 10:53 AM
>>>>> *To:* concurrency-interest at cs.oswego.edu
>>>>> *Subject:* [concurrency-interest] Nested synchronized
>>>>>
>>>>> Hi,
>>>>>
>>>>> I have seen something I think is a JVM bug but would like to check my
>>>>> understanding before reporting a problem. The following program normally
>>>>> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
>>>>> 6 or 7, 4 core processor. The problem is that there are synchronized
>>>>> methods, isSetA1 and isSetA2 (near end of listing below), that call another
>>>>> synchronized method, conditionallySumArguments (at end of listing below).
>>>>> The second synchronized is unnecessary since the method is always called
>>>>> within an already synchronized method and if the second synchronized is
>>>>> removed the program works as expected. However I think an extra
>>>>> synchronized should be redundant, not a problem?
>>>>>
>>>>>  package nestedsynchronizedproblem;
>>>>>
>>>>>  import java.util.concurrent.Callable;
>>>>>  import java.util.concurrent.ExecutorService;
>>>>>  import java.util.concurrent.Executors;
>>>>>  import java.util.concurrent.TimeUnit;
>>>>>
>>>>>  import static java.lang.System.*;
>>>>>
>>>>>  /**
>>>>>   * Test of nested synchronized. Mimics calling a parallel sum method.
>>>>>   *
>>>>>   * @author  Howard Lovatt
>>>>>   */
>>>>>  public class NestedSynchronizedProblem {
>>>>>    private static final int loops = 10 * 1000 * 1000; // This needs
>>>>> to be large for hanging!
>>>>>
>>>>>    public static void main( final String... notUsed ) throws
>>>>> InterruptedException {
>>>>>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>>>>>      final Callable<Void> setA1 = new Callable<Void>() {
>>>>>        @Override public Void call() throws Exception {
>>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>>>>>          return null;
>>>>>        }
>>>>>      };
>>>>>      final Callable<Void> setA2 = new Callable<Void>() {
>>>>>        @Override public Void call() throws Exception {
>>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>>>>>          return null;
>>>>>        }
>>>>>      };
>>>>>      final ExecutorService pool = Executors.newCachedThreadPool();
>>>>>      pool.submit( setA1 );
>>>>>      pool.submit( setA2 );
>>>>>      pool.shutdown();
>>>>>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>>>>>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed
>>>>> to terminate") );
>>>>>      pool.shutdownNow();
>>>>>    }
>>>>>  }
>>>>>
>>>>>
>>>>>  final class ParrallelSumMethod {
>>>>>    private long sum = 0;
>>>>>    private Long a1 = null;
>>>>>    private Long a2 = null;
>>>>>
>>>>>    public void setA1( final long a1Arg ) throws InterruptedException {
>>>>>      for ( ;; ) {
>>>>>        if ( isSetA1( a1Arg ) ) { return; }
>>>>>        checkForInterrupt();
>>>>>      }
>>>>>    }
>>>>>
>>>>>    public void setA2( final long a2Arg ) throws InterruptedException {
>>>>>      for ( ;; ) {
>>>>>        if ( isSetA2( a2Arg ) ) { return; }
>>>>>        checkForInterrupt();
>>>>>      }
>>>>>    }
>>>>>
>>>>>    public Long getSum() { return sum; }
>>>>>
>>>>>    private static void checkForInterrupt() throws
>>>>> InterruptedException {
>>>>>      if ( Thread.interrupted() ) { throw new InterruptedException(); }
>>>>>    }
>>>>>
>>>>>    private synchronized boolean isSetA1( final long a1Arg ) {
>>>>>      if ( a1 == null ) {
>>>>>        a1 = a1Arg;
>>>>>        conditionallySumArguments();
>>>>>        return true;
>>>>>      }
>>>>>      return false;
>>>>>    }
>>>>>
>>>>>    private synchronized boolean isSetA2( final long a2Arg ) {
>>>>>      if ( a2 == null ) {
>>>>>        a2 = a2Arg;
>>>>>        conditionallySumArguments();
>>>>>        return true;
>>>>>      }
>>>>>      return false;
>>>>>    }
>>>>>
>>>>>    private synchronized void conditionallySumArguments() { // Works
>>>>> if not synchronized!!!
>>>>>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>>>>>      sum += a1 + a2;
>>>>>      a1 = a2 = null;
>>>>>    }
>>>>>  }
>>>>>
>>>>>
>>>>> Thanks in advance for any comments,
>>>>>
>>>>>   -- Howard.
>>>>>
>>>>>
>>>>
>>>>
>>>> --
>>>>   -- Howard.
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
>>
>> --
>>   -- Howard.
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120105/31d10f81/attachment-0001.html>

From howard.lovatt at gmail.com  Thu Jan  5 21:05:18 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Fri, 6 Jan 2012 13:05:18 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CAHjP37EETfOkUmv1gRwWSedVz=LM3Yze2Ucp94FGCLfFe0D4qw@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
	<CACR_FB5STPMUyO4F1=9S2wfrz_46jyZDW4Bp2H=nMGdgZVL3-Q@mail.gmail.com>
	<CAHjP37EETfOkUmv1gRwWSedVz=LM3Yze2Ucp94FGCLfFe0D4qw@mail.gmail.com>
Message-ID: <CACR_FB5e1kUWSzfEGgFMzCbjAVofg-z6rqxZuUf223iKoTaLag@mail.gmail.com>

Hi Vitaly,

jstack didn't work.

gar-ln:~ lov080$ ps
  PID TTY           TIME CMD
62924 ttys000    0:00.02 -bash
62955 ttys000    0:08.15 /usr/bin/java
nestedsynchronizedproblem.NestedSynchron
62967 ttys001    0:00.01 -bash
gar-ln:~ lov080$ jstack -l -F 62955
Attaching to process ID 62955, please wait...
attach: task_for_pid(62955) failed (5)
Error attaching to process: Error attaching to process, or no such process

Good sugestion though.

Thanks,

 -- Howard.

On 6 January 2012 12:44, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> a bit crude but if its somewhat easy to repro try increasing the await
> period to something like 10 mins and then jstack it a few times and see
> what it shows.
>
> Also can you repro in interpreter (i.e. -Xint)?
> On Jan 5, 2012 8:27 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>
>> Hi Vitaly,
>>
>> If I increase the awaitTermination to 2 minutes I can run the program in
>> the debugger, however it works then! Is there another away, without using
>> the debugger, of generating a stack trace for a hanging program?
>>
>> Thanks,
>>
>>  -- Howard.
>>
>> On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>>> Howard,
>>>
>>> Have you captured call stacks when it hangs?
>>>
>>> Vitaly
>>> On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>>>
>>>> Hi David,
>>>>
>>>> There is only one sum object shared between the two threads (1st line
>>>> of main) and hence all synchronization is on the same object. Therefore I
>>>> think the code should work (even though the second synchronization is
>>>> redundant). As a double check on my understanding I just added a specific
>>>> mutex object to the code and synchronized on that and got the same result.
>>>>
>>>> Have I understood you comment correctly?
>>>>
>>>> Thanks,
>>>>
>>>>  -- Howard.
>>>>
>>>> On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au> wrote:
>>>>
>>>>> **
>>>>> That's not nested synchronization as you are using two different
>>>>> objects. It is a classic deadlock:
>>>>>
>>>>> - sync method on Obj A calls sync method on Obj B
>>>>> - sync method on Obj B calls sync method on Objj A
>>>>>
>>>>> Thread 1 does the call to ObjA
>>>>> Thread 2 does the call to Obj B
>>>>>
>>>>> David
>>>>> ------
>>>>>
>>>>> -----Original Message-----
>>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard
>>>>> Lovatt
>>>>> *Sent:* Friday, 6 January 2012 10:53 AM
>>>>> *To:* concurrency-interest at cs.oswego.edu
>>>>> *Subject:* [concurrency-interest] Nested synchronized
>>>>>
>>>>> Hi,
>>>>>
>>>>> I have seen something I think is a JVM bug but would like to check my
>>>>> understanding before reporting a problem. The following program normally
>>>>> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
>>>>> 6 or 7, 4 core processor. The problem is that there are synchronized
>>>>> methods, isSetA1 and isSetA2 (near end of listing below), that call another
>>>>> synchronized method, conditionallySumArguments (at end of listing below).
>>>>> The second synchronized is unnecessary since the method is always called
>>>>> within an already synchronized method and if the second synchronized is
>>>>> removed the program works as expected. However I think an extra
>>>>> synchronized should be redundant, not a problem?
>>>>>
>>>>>  package nestedsynchronizedproblem;
>>>>>
>>>>>  import java.util.concurrent.Callable;
>>>>>  import java.util.concurrent.ExecutorService;
>>>>>  import java.util.concurrent.Executors;
>>>>>  import java.util.concurrent.TimeUnit;
>>>>>
>>>>>  import static java.lang.System.*;
>>>>>
>>>>>  /**
>>>>>   * Test of nested synchronized. Mimics calling a parallel sum method.
>>>>>   *
>>>>>   * @author  Howard Lovatt
>>>>>   */
>>>>>  public class NestedSynchronizedProblem {
>>>>>    private static final int loops = 10 * 1000 * 1000; // This needs
>>>>> to be large for hanging!
>>>>>
>>>>>    public static void main( final String... notUsed ) throws
>>>>> InterruptedException {
>>>>>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>>>>>      final Callable<Void> setA1 = new Callable<Void>() {
>>>>>        @Override public Void call() throws Exception {
>>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>>>>>          return null;
>>>>>        }
>>>>>      };
>>>>>      final Callable<Void> setA2 = new Callable<Void>() {
>>>>>        @Override public Void call() throws Exception {
>>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>>>>>          return null;
>>>>>        }
>>>>>      };
>>>>>      final ExecutorService pool = Executors.newCachedThreadPool();
>>>>>      pool.submit( setA1 );
>>>>>      pool.submit( setA2 );
>>>>>      pool.shutdown();
>>>>>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>>>>>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed
>>>>> to terminate") );
>>>>>      pool.shutdownNow();
>>>>>    }
>>>>>  }
>>>>>
>>>>>
>>>>>  final class ParrallelSumMethod {
>>>>>    private long sum = 0;
>>>>>    private Long a1 = null;
>>>>>    private Long a2 = null;
>>>>>
>>>>>    public void setA1( final long a1Arg ) throws InterruptedException {
>>>>>      for ( ;; ) {
>>>>>        if ( isSetA1( a1Arg ) ) { return; }
>>>>>        checkForInterrupt();
>>>>>      }
>>>>>    }
>>>>>
>>>>>    public void setA2( final long a2Arg ) throws InterruptedException {
>>>>>      for ( ;; ) {
>>>>>        if ( isSetA2( a2Arg ) ) { return; }
>>>>>        checkForInterrupt();
>>>>>      }
>>>>>    }
>>>>>
>>>>>    public Long getSum() { return sum; }
>>>>>
>>>>>    private static void checkForInterrupt() throws
>>>>> InterruptedException {
>>>>>      if ( Thread.interrupted() ) { throw new InterruptedException(); }
>>>>>    }
>>>>>
>>>>>    private synchronized boolean isSetA1( final long a1Arg ) {
>>>>>      if ( a1 == null ) {
>>>>>        a1 = a1Arg;
>>>>>        conditionallySumArguments();
>>>>>        return true;
>>>>>      }
>>>>>      return false;
>>>>>    }
>>>>>
>>>>>    private synchronized boolean isSetA2( final long a2Arg ) {
>>>>>      if ( a2 == null ) {
>>>>>        a2 = a2Arg;
>>>>>        conditionallySumArguments();
>>>>>        return true;
>>>>>      }
>>>>>      return false;
>>>>>    }
>>>>>
>>>>>    private synchronized void conditionallySumArguments() { // Works
>>>>> if not synchronized!!!
>>>>>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>>>>>      sum += a1 + a2;
>>>>>      a1 = a2 = null;
>>>>>    }
>>>>>  }
>>>>>
>>>>>
>>>>> Thanks in advance for any comments,
>>>>>
>>>>>   -- Howard.
>>>>>
>>>>>
>>>>
>>>>
>>>> --
>>>>   -- Howard.
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
>>
>> --
>>   -- Howard.
>>
>>


-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/b37c1848/attachment.html>

From howard.lovatt at gmail.com  Thu Jan  5 21:27:47 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Fri, 6 Jan 2012 13:27:47 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CAHjP37GPkjG_KXLdhA0VeVKGkDAGC+C8gMNmj3eK8r8ohZFYAg@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
	<CACR_FB4oyNvgDuD5jWcqb0ARKPqLHd+SWMnKr_Pn9r21HO9PWg@mail.gmail.com>
	<CAHjP37HgMx=i0aDAF-65SKazXZOZ7qa-n_AEnXdXzGk4FmbJBQ@mail.gmail.com>
	<CAHjP37GPkjG_KXLdhA0VeVKGkDAGC+C8gMNmj3eK8r8ohZFYAg@mail.gmail.com>
Message-ID: <CACR_FB6JPz3vJNN0rkRB14qYsUHHM_sTRspMR_-s++xDFJDUTg@mail.gmail.com>

Hi,

No luck with -XX:-EliminateLocks, still hangs. Also I can't get a stack
dump from kill. Tried kill -1 to kill -9, no luck. Will try reentrant locks
and report.

Thanks for all the suggestions,

 -- Howard.

On 6 January 2012 13:01, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Sorry left another bit out.  Since the suspicion is the nesting try
> -XX:-EliminateLocks, IIRC.  Get rid of the other flags too so we're
> controlling one thing at a time.
> On Jan 5, 2012 8:55 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>
>> Possible but would be good to have call stacks.  Also can you try using a
>> reentrant lock instead of synchronized?
>> On Jan 5, 2012 8:44 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>>
>>> Hi Vitaly,
>>>
>>> If I add the following JVM options, -ea -XX:+UseBiasedLocking
>>> -XX:-UseSpinning -XX:+UseTLAB -XX:+UseThreadPriorities, then the program
>>> runs. Indicating to me that it is a JVM problem.
>>>
>>>  -- Howard.
>>>
>>> On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>>
>>>> Howard,
>>>>
>>>> Have you captured call stacks when it hangs?
>>>>
>>>> Vitaly
>>>> On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com>
>>>> wrote:
>>>>
>>>>> Hi David,
>>>>>
>>>>> There is only one sum object shared between the two threads (1st line
>>>>> of main) and hence all synchronization is on the same object. Therefore I
>>>>> think the code should work (even though the second synchronization is
>>>>> redundant). As a double check on my understanding I just added a specific
>>>>> mutex object to the code and synchronized on that and got the same result.
>>>>>
>>>>> Have I understood you comment correctly?
>>>>>
>>>>> Thanks,
>>>>>
>>>>>  -- Howard.
>>>>>
>>>>> On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au>wrote:
>>>>>
>>>>>> **
>>>>>> That's not nested synchronization as you are using two different
>>>>>> objects. It is a classic deadlock:
>>>>>>
>>>>>> - sync method on Obj A calls sync method on Obj B
>>>>>> - sync method on Obj B calls sync method on Objj A
>>>>>>
>>>>>> Thread 1 does the call to ObjA
>>>>>> Thread 2 does the call to Obj B
>>>>>>
>>>>>> David
>>>>>> ------
>>>>>>
>>>>>> -----Original Message-----
>>>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard
>>>>>> Lovatt
>>>>>> *Sent:* Friday, 6 January 2012 10:53 AM
>>>>>> *To:* concurrency-interest at cs.oswego.edu
>>>>>> *Subject:* [concurrency-interest] Nested synchronized
>>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> I have seen something I think is a JVM bug but would like to check my
>>>>>> understanding before reporting a problem. The following program normally
>>>>>> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
>>>>>> 6 or 7, 4 core processor. The problem is that there are synchronized
>>>>>> methods, isSetA1 and isSetA2 (near end of listing below), that call another
>>>>>> synchronized method, conditionallySumArguments (at end of listing below).
>>>>>> The second synchronized is unnecessary since the method is always called
>>>>>> within an already synchronized method and if the second synchronized is
>>>>>> removed the program works as expected. However I think an extra
>>>>>> synchronized should be redundant, not a problem?
>>>>>>
>>>>>>  package nestedsynchronizedproblem;
>>>>>>
>>>>>>  import java.util.concurrent.Callable;
>>>>>>  import java.util.concurrent.ExecutorService;
>>>>>>  import java.util.concurrent.Executors;
>>>>>>  import java.util.concurrent.TimeUnit;
>>>>>>
>>>>>>  import static java.lang.System.*;
>>>>>>
>>>>>>  /**
>>>>>>   * Test of nested synchronized. Mimics calling a parallel sum
>>>>>> method.
>>>>>>   *
>>>>>>   * @author  Howard Lovatt
>>>>>>   */
>>>>>>  public class NestedSynchronizedProblem {
>>>>>>    private static final int loops = 10 * 1000 * 1000; // This needs
>>>>>> to be large for hanging!
>>>>>>
>>>>>>    public static void main( final String... notUsed ) throws
>>>>>> InterruptedException {
>>>>>>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>>>>>>      final Callable<Void> setA1 = new Callable<Void>() {
>>>>>>        @Override public Void call() throws Exception {
>>>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>>>>>>          return null;
>>>>>>        }
>>>>>>      };
>>>>>>      final Callable<Void> setA2 = new Callable<Void>() {
>>>>>>        @Override public Void call() throws Exception {
>>>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>>>>>>          return null;
>>>>>>        }
>>>>>>      };
>>>>>>      final ExecutorService pool = Executors.newCachedThreadPool();
>>>>>>      pool.submit( setA1 );
>>>>>>      pool.submit( setA2 );
>>>>>>      pool.shutdown();
>>>>>>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES );
>>>>>>      out.println( sum.getSum() + (ok ? ", terminated ok" : ", failed
>>>>>> to terminate") );
>>>>>>      pool.shutdownNow();
>>>>>>    }
>>>>>>  }
>>>>>>
>>>>>>
>>>>>>  final class ParrallelSumMethod {
>>>>>>    private long sum = 0;
>>>>>>    private Long a1 = null;
>>>>>>    private Long a2 = null;
>>>>>>
>>>>>>    public void setA1( final long a1Arg ) throws InterruptedException
>>>>>> {
>>>>>>      for ( ;; ) {
>>>>>>        if ( isSetA1( a1Arg ) ) { return; }
>>>>>>        checkForInterrupt();
>>>>>>      }
>>>>>>    }
>>>>>>
>>>>>>    public void setA2( final long a2Arg ) throws InterruptedException
>>>>>> {
>>>>>>      for ( ;; ) {
>>>>>>        if ( isSetA2( a2Arg ) ) { return; }
>>>>>>        checkForInterrupt();
>>>>>>      }
>>>>>>    }
>>>>>>
>>>>>>    public Long getSum() { return sum; }
>>>>>>
>>>>>>    private static void checkForInterrupt() throws
>>>>>> InterruptedException {
>>>>>>      if ( Thread.interrupted() ) { throw new InterruptedException();
>>>>>> }
>>>>>>    }
>>>>>>
>>>>>>    private synchronized boolean isSetA1( final long a1Arg ) {
>>>>>>      if ( a1 == null ) {
>>>>>>        a1 = a1Arg;
>>>>>>        conditionallySumArguments();
>>>>>>        return true;
>>>>>>      }
>>>>>>      return false;
>>>>>>    }
>>>>>>
>>>>>>    private synchronized boolean isSetA2( final long a2Arg ) {
>>>>>>      if ( a2 == null ) {
>>>>>>        a2 = a2Arg;
>>>>>>        conditionallySumArguments();
>>>>>>        return true;
>>>>>>      }
>>>>>>      return false;
>>>>>>    }
>>>>>>
>>>>>>    private synchronized void conditionallySumArguments() { // Works
>>>>>> if not synchronized!!!
>>>>>>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>>>>>>      sum += a1 + a2;
>>>>>>      a1 = a2 = null;
>>>>>>    }
>>>>>>  }
>>>>>>
>>>>>>
>>>>>> Thanks in advance for any comments,
>>>>>>
>>>>>>   -- Howard.
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>> --
>>>>>   -- Howard.
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>
>>>
>>> --
>>>   -- Howard.
>>>
>>>


-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/c4a92c8d/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Jan  5 21:33:48 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 6 Jan 2012 12:33:48 +1000
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB6JPz3vJNN0rkRB14qYsUHHM_sTRspMR_-s++xDFJDUTg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEDKJCAA.davidcholmes@aapt.net.au>

If you have a hang from which you can't get a stack dump it generally
indicates that the VM can't reach a safepoint. At that point you need a
native tools (eg pstack on Solaris) or a debugger to help you see the
stacks.

The inability to reach a safepoint indicates to me that there is a livelock,
probably involving spinning on locks (hence why -UseSpinning can avoid the
problem). Can you confirm that CPU is being consumed during the "hang"?

Anyway it's a VM bug.

David
  -----Original Message-----
  From: Howard Lovatt [mailto:howard.lovatt at gmail.com]
  Sent: Friday, 6 January 2012 12:28 PM
  To: Vitaly Davidovich
  Cc: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Nested synchronized


  Hi,


  No luck with -XX:-EliminateLocks, still hangs. Also I can't get a stack
dump from kill. Tried kill -1 to kill -9, no luck. Will try reentrant locks
and report.


  Thanks for all the suggestions,


   -- Howard.


  On 6 January 2012 13:01, Vitaly Davidovich <vitalyd at gmail.com> wrote:

    Sorry left another bit out.  Since the suspicion is the nesting
try -XX:-EliminateLocks, IIRC.  Get rid of the other flags too so we're
controlling one thing at a time.

    On Jan 5, 2012 8:55 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

      Possible but would be good to have call stacks.  Also can you try
using a reentrant lock instead of synchronized?

      On Jan 5, 2012 8:44 PM, "Howard Lovatt" <howard.lovatt at gmail.com>
wrote:

        Hi Vitaly,


        If I add the following JVM
options, -ea -XX:+UseBiasedLocking -XX:-UseSpinning -XX:+UseTLAB -XX:+UseThr
eadPriorities, then the program runs. Indicating to me that it is a JVM
problem.


         -- Howard.


        On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

          Howard,

          Have you captured call stacks when it hangs?

          Vitaly

          On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com>
wrote:

            Hi David,


            There is only one sum object shared between the two threads (1st
line of main) and hence all synchronization is on the same object. Therefore
I think the code should work (even though the second synchronization is
redundant). As a double check on my understanding I just added a specific
mutex object to the code and synchronized on that and got the same result.


            Have I understood you comment correctly?


            Thanks,


             -- Howard.


            On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au>
wrote:

              That's not nested synchronization as you are using two
different objects. It is a classic deadlock:

              - sync method on Obj A calls sync method on Obj B
              - sync method on Obj B calls sync method on Objj A

              Thread 1 does the call to ObjA
              Thread 2 does the call to Obj B

              David
              ------
                -----Original Message-----
                From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Howard
Lovatt
                Sent: Friday, 6 January 2012 10:53 AM
                To: concurrency-interest at cs.oswego.edu
                Subject: [concurrency-interest] Nested synchronized


                Hi,


                I have seen something I think is a JVM bug but would like to
check my understanding before reporting a problem. The following program
normally hangs, i.e. the problem is intermittent, on my computer, MacBook
Pro, Java 6 or 7, 4 core processor. The problem is that there are
synchronized methods, isSetA1 and isSetA2 (near end of listing below), that
call another synchronized method, conditionallySumArguments (at end of
listing below). The second synchronized is unnecessary since the method is
always called within an already synchronized method and if the second
synchronized is removed the program works as expected. However I think an
extra synchronized should be redundant, not a problem?


                  package nestedsynchronizedproblem;


                  import java.util.concurrent.Callable;
                  import java.util.concurrent.ExecutorService;
                  import java.util.concurrent.Executors;
                  import java.util.concurrent.TimeUnit;


                  import static java.lang.System.*;


                  /**
                   * Test of nested synchronized. Mimics calling a parallel
sum method.
                   *
                   * @author  Howard Lovatt
                   */
                  public class NestedSynchronizedProblem {
                    private static final int loops = 10 * 1000 * 1000; //
This needs to be large for hanging!


                    public static void main( final String... notUsed )
throws InterruptedException {
                      final ParrallelSumMethod sum = new
ParrallelSumMethod();
                      final Callable<Void> setA1 = new Callable<Void>() {
                        @Override public Void call() throws Exception {
                          for ( int l = 0; l < loops; l++ ) { sum.setA1(
l ); }
                          return null;
                        }
                      };
                      final Callable<Void> setA2 = new Callable<Void>() {
                        @Override public Void call() throws Exception {
                          for ( int l = 0; l < loops; l++ ) { sum.setA2(
l ); }
                          return null;
                        }
                      };
                      final ExecutorService pool =
Executors.newCachedThreadPool();
                      pool.submit( setA1 );
                      pool.submit( setA2 );
                      pool.shutdown();
                      final boolean ok = pool.awaitTermination( 1,
TimeUnit.MINUTES );
                      out.println( sum.getSum() + (ok ? ", terminated ok" :
", failed to terminate") );
                      pool.shutdownNow();
                    }
                  }




                  final class ParrallelSumMethod {
                    private long sum = 0;
                    private Long a1 = null;
                    private Long a2 = null;


                    public void setA1( final long a1Arg ) throws
InterruptedException {
                      for ( ;; ) {
                        if ( isSetA1( a1Arg ) ) { return; }
                        checkForInterrupt();
                      }
                    }


                    public void setA2( final long a2Arg ) throws
InterruptedException {
                      for ( ;; ) {
                        if ( isSetA2( a2Arg ) ) { return; }
                        checkForInterrupt();
                      }
                    }


                    public Long getSum() { return sum; }


                    private static void checkForInterrupt() throws
InterruptedException {
                      if ( Thread.interrupted() ) { throw new
InterruptedException(); }
                    }


                    private synchronized boolean isSetA1( final long a1Arg )
{
                      if ( a1 == null ) {
                        a1 = a1Arg;
                        conditionallySumArguments();
                        return true;
                      }
                      return false;
                    }


                    private synchronized boolean isSetA2( final long a2Arg )
{
                      if ( a2 == null ) {
                        a2 = a2Arg;
                        conditionallySumArguments();
                        return true;
                      }
                      return false;
                    }


                    private synchronized void conditionallySumArguments()
{ // Works if not synchronized!!!
                      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
                      sum += a1 + a2;
                      a1 = a2 = null;
                    }
                  }


                Thanks in advance for any comments,

                  -- Howard.







            --
              -- Howard.




            _______________________________________________
            Concurrency-interest mailing list
            Concurrency-interest at cs.oswego.edu
            http://cs.oswego.edu/mailman/listinfo/concurrency-interest







        --
          -- Howard.







  --
    -- Howard.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/500052b8/attachment-0001.html>

From howard.lovatt at gmail.com  Thu Jan  5 22:42:49 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Fri, 6 Jan 2012 14:42:49 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEDKJCAA.davidcholmes@aapt.net.au>
References: <CACR_FB6JPz3vJNN0rkRB14qYsUHHM_sTRspMR_-s++xDFJDUTg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEDKJCAA.davidcholmes@aapt.net.au>
Message-ID: <CACR_FB6rjJ339pLaZ1R+PjGCB88=EpHotuHUqQJz=WcP_2keAA@mail.gmail.com>

Hi,

Finally got a stack trace, but from native tools so I am not sure how much
use it is. When hung it consumes negligible CPU. Stack trace below.

Sampling process 64408 for 3 seconds with 1 millisecond of run time between
samples

Sampling completed, processing symbols...

Analysis of sampling java (pid 64408) every 1 millisecond

Process:         java [64408]

Path:
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/bin/java

Load Address:    0x104fae000

Identifier:      java

Version:         1.0 (1.0)

Code Type:       X86-64 (Native)

Parent Process:  bash [62924]


Date/Time:       2012-01-06 14:38:26.917 +1100

OS Version:      Mac OS X 10.7.2 (11C74)

Report Version:  7


Call graph:

    2274 Thread_7277650   DispatchQueue_1: com.apple.main-thread  (serial)

    + 2274 ???  (in java)  load address 0x104fae000 + 0x1a98  [0x104fafa98]

    +   2274 ???  (in java)  load address 0x104fae000 + 0x429a
[0x104fb229a]

    +     2274 ???  (in java)  load address 0x104fae000 + 0x4843
[0x104fb2843]

    +       2274 CFRunLoopRunSpecific  (in CoreFoundation) + 230
[0x7fff8d6c0ae6]

    +         2274 __CFRunLoopRun  (in CoreFoundation) + 1204
[0x7fff8d6c12d4]

    +           2274 __CFRunLoopServiceMachPort  (in CoreFoundation) + 188
[0x7fff8d6b8b6c]

    +             2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +               2274 mach_msg_trap  (in libsystem_kernel.dylib) + 10
[0x7fff8990767a]

    2274 Thread_7277657

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in java)  load address 0x104fae000 + 0x3240
[0x104fb1240]

    +       2274 ???  (in java)  load address 0x104fae000 + 0x2cf3
[0x104fb0cf3]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xcea1d  [0x10510fa1d]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0xceb75  [0x10510fb75]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xab53e  [0x1050ec53e]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xab77a  [0x1050ec77a]

    +                 2274 ???  (in <unknown binary>)  [0x1059f8438]

    +                   2274 ???  (in <unknown binary>)  [0x1059fdf5c]

    +                     2274 ???  (in <unknown binary>)  [0x1059fde03]

    +                       2274 ???  (in <unknown binary>)  [0x1059fd85a]

    +                         2274 ???  (in <unknown binary>)  [0x1059fd85a]

    +                           2274 ???  (in <unknown binary>)
[0x105a08d6e]

    +                             2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x230068  [0x105271068]

    +                               2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x2302ad  [0x1052712ad]

    +                                 2274 ???  (in libclient64.dylib)
load address 0x105041000 + 0x13fb5  [0x105054fb5]

    +                                   2274 mach_msg  (in
libsystem_kernel.dylib) + 73  [0x7fff89906d71]

    +                                     2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]

    2274 Thread_7277658: Java: Exception Handler Thread

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x14ca3  [0x105055ca3]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x14de9  [0x105055de9]

    +           2274 mach_msg_server  (in libsystem_kernel.dylib) + 473
[0x7fff89907274]

    +             2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +               2274 mach_msg_trap  (in libsystem_kernel.dylib) + 10
[0x7fff8990767a]

    2274 Thread_7277659: Java: Gang worker#0 (Parallel GC Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x2c5c8  [0x10506d5c8]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277660: Java: Gang worker#1 (Parallel GC Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x2c5c8  [0x10506d5c8]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277661: Java: Gang worker#2 (Parallel GC Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x2c5c8  [0x10506d5c8]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277662: Java: Gang worker#3 (Parallel GC Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x2c5c8  [0x10506d5c8]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277663: Java: Gang worker#4 (Parallel GC Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x2c5c8  [0x10506d5c8]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277664: Java: Gang worker#5 (Parallel GC Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x2c5c8  [0x10506d5c8]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277665: Java: Gang worker#6 (Parallel GC Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x2c5c8  [0x10506d5c8]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277666: Java: Gang worker#7 (Parallel GC Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x2c5c8  [0x10506d5c8]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277667: Java: Gang worker#0 (Parallel CMS Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x477e35  [0x1054b8e35]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277668: Java: Gang worker#1 (Parallel CMS Threads)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x477e35  [0x1054b8e35]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1391d  [0x10505491d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                   2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                     2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277669: Java: Concurrent Mark-Sweep GC Thread

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x329e9  [0x1050739e9]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x1087cf  [0x1051497cf]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1088e5  [0x1051498e5]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x137c3  [0x1050547c3]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1391d  [0x10505491d]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x32b6e  [0x105073b6e]

    +                   2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13fb5  [0x105054fb5]

    +                     2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                       2274 mach_msg_trap  (in libsystem_kernel.dylib)
+ 10  [0x7fff8990767a]

    2274 Thread_7277670: Java: VM Thread

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xaa307  [0x1050eb307]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xaa6f4  [0x1050eb6f4]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0xd30da  [0x1051140da]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1b5976  [0x1051f6976]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13fb5  [0x105054fb5]

    +                 2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +                   2274 mach_msg_trap  (in libsystem_kernel.dylib) +
10  [0x7fff8990767a]

    2274 Thread_7277671: Java: Reference Handler

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7355  [0x1050f8355]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7540  [0x1050f8540]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0xb769e  [0x1050f869e]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xb76fe  [0x1050f86fe]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xb7808  [0x1050f8808]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xab77a  [0x1050ec77a]

    +                   2274 ???  (in <unknown binary>)  [0x1059f8438]

    +                     2274 ???  (in <unknown binary>)  [0x1059fd85a]

    +                       2274 ???  (in <unknown binary>)  [0x1059fd85a]

    +                         2274 ???  (in <unknown binary>)  [0x105a08d6e]

    +                           2274 JVM_MonitorWait  (in
libjvmlinkage.dylib) + 59  [0x10573fb0b]

    +                             2274 JVM_MonitorWait  (in
libclient64.dylib) + 154  [0x1050f8cbc]

    +                               2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0xb8050  [0x1050f9050]

    +                                 2274 ???  (in libclient64.dylib)
load address 0x105041000 + 0xb8bc3  [0x1050f9bc3]

    +                                   2274 ???  (in libclient64.dylib)
load address 0x105041000 + 0x13de3  [0x105054de3]

    +                                     2274 ???  (in libclient64.dylib)
load address 0x105041000 + 0x13f23  [0x105054f23]

    +                                       2274 mach_msg  (in
libsystem_kernel.dylib) + 73  [0x7fff89906d71]

    +                                         2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]

    2274 Thread_7277672: Java: Finalizer

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7355  [0x1050f8355]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7540  [0x1050f8540]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0xb769e  [0x1050f869e]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xb76fe  [0x1050f86fe]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xb7808  [0x1050f8808]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xab77a  [0x1050ec77a]

    +                   2274 ???  (in <unknown binary>)  [0x1059f8438]

    +                     2274 ???  (in <unknown binary>)  [0x1059fd9b3]

    +                       2274 ???  (in <unknown binary>)  [0x1059fd9b3]

    +                         2274 ???  (in <unknown binary>)  [0x1059fd85a]

    +                           2274 ???  (in <unknown binary>)
[0x105a08d6e]

    +                             2274 JVM_MonitorWait  (in
libjvmlinkage.dylib) + 59  [0x10573fb0b]

    +                               2274 JVM_MonitorWait  (in
libclient64.dylib) + 154  [0x1050f8cbc]

    +                                 2274 ???  (in libclient64.dylib)
load address 0x105041000 + 0xb8050  [0x1050f9050]

    +                                   2274 ???  (in libclient64.dylib)
load address 0x105041000 + 0xb8bc3  [0x1050f9bc3]

    +                                     2274 ???  (in libclient64.dylib)
load address 0x105041000 + 0x13de3  [0x105054de3]

    +                                       2274 ???  (in
libclient64.dylib)  load address 0x105041000 + 0x13f23  [0x105054f23]

    +                                         2274 mach_msg  (in
libsystem_kernel.dylib) + 73  [0x7fff89906d71]

    +                                           2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]

    2274 Thread_7277673   DispatchQueue_2: com.apple.libdispatch-manager
(serial)

    + 2274 _dispatch_mgr_thread  (in libdispatch.dylib) + 54
[0x7fff88c1f14e]

    +   2274 _dispatch_mgr_invoke  (in libdispatch.dylib) + 923
[0x7fff88c205be]

    +     2274 kevent  (in libsystem_kernel.dylib) + 10  [0x7fff899097e6]

    2274 Thread_7277677: Java: Surrogate Locker Thread (Concurrent GC)

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7355  [0x1050f8355]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7540  [0x1050f8540]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0xc7fcd  [0x105108fcd]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1372a  [0x10505472a]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1391d  [0x10505491d]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +                   2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                     2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                       2274 mach_msg  (in libsystem_kernel.dylib) +
73  [0x7fff89906d71]

    +                         2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]

    2274 Thread_7277678: Java: Signal Dispatcher

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7355  [0x1050f8355]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7540  [0x1050f8540]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0xc8296  [0x105109296]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xc880f  [0x10510980f]

    +               2274 semaphore_wait_trap  (in libsystem_kernel.dylib) +
10  [0x7fff899076b6]

    2274 Thread_7277679: Java: C2 CompilerThread0

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7355  [0x1050f8355]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7540  [0x1050f8540]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0xc967f  [0x10510a67f]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xcf07d  [0x10511007d]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1372a  [0x10505472a]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1391d  [0x10505491d]

    +                   2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +                     2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                       2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                         2274 mach_msg  (in libsystem_kernel.dylib) +
73  [0x7fff89906d71]

    +                           2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]

    2274 Thread_7277680: Java: C2 CompilerThread1

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7355  [0x1050f8355]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7540  [0x1050f8540]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0xc967f  [0x10510a67f]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xcf07d  [0x10511007d]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1372a  [0x10505472a]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1391d  [0x10505491d]

    +                   2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +                     2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                       2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                         2274 mach_msg  (in libsystem_kernel.dylib) +
73  [0x7fff89906d71]

    +                           2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]

    2274 Thread_7277681: Java: Low Memory Detector

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7355  [0x1050f8355]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7540  [0x1050f8540]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0xcb080  [0x10510c080]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x137c3  [0x1050547c3]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1391d  [0x10505491d]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +                   2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                     2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                       2274 mach_msg  (in libsystem_kernel.dylib) +
73  [0x7fff89906d71]

    +                         2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]

    2274 Thread_7277682: Java: VM Periodic Task Thread

    + 2274 thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

    +   2274 _pthread_start  (in libsystem_c.dylib) + 335  [0x7fff88b138bf]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

    +       2273 ???  (in libclient64.dylib)  load address 0x105041000 +
0xcd1b2  [0x10510e1b2]

    +       ! 2273 ???  (in libclient64.dylib)  load address 0x105041000 +
0x32b6e  [0x105073b6e]

    +       !   2273 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x13fb5  [0x105054fb5]

    +       !     2273 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +       !       2273 mach_msg_trap  (in libsystem_kernel.dylib) + 10
[0x7fff8990767a]

    +       1 ???  (in libclient64.dylib)  load address 0x105041000 +
0xcd209  [0x10510e209]

    +         1 ???  (in libclient64.dylib)  load address 0x105041000 +
0xd1ec1  [0x105112ec1]

    +           1 ???  (in libclient64.dylib)  load address 0x105041000 +
0xd20dc  [0x1051130dc]

    +             1 ???  (in libclient64.dylib)  load address 0x105041000 +
0x2f258  [0x105070258]

    +               1 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x30c61  [0x105071c61]

    +                 1 ???  (in libclient64.dylib)  load address
0x105041000 + 0x30c90  [0x105071c90]

    +                   1 ???  (in libclient64.dylib)  load address
0x105041000 + 0x30ce8  [0x105071ce8]

    2274 Thread_7277683: Java: pool-1-thread-1

    + 2274 ???  (in <unknown binary>)  [0x105a66a78]

    +   2274 ???  (in <unknown binary>)  [0x105a5fa48]

    +     2274 JVM_IsInterrupted  (in libjvmlinkage.dylib) + 59
[0x10574078b]

    +       2274 JVM_IsInterrupted  (in libclient64.dylib) + 149
[0x1051cf69d]

    +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x7c198  [0x1050bd198]

    +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1b5c4d  [0x1051f6c4d]

    +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x12b0f  [0x105053b0f]

    +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x12bcd  [0x105053bcd]

    +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

    +                   2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

    +                     2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

    +                       2274 mach_msg  (in libsystem_kernel.dylib) +
73  [0x7fff89906d71]

    +                         2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]

    2274 Thread_7277684: Java: pool-1-thread-2

    + 2274 ???  (in <unknown binary>)  [0x105a65287]

    +   2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x1e3d25  [0x105224d25]

    +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xd12cd  [0x1051122cd]

    +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x14125  [0x105055125]

    +         2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

    +           2274 mach_msg_trap  (in libsystem_kernel.dylib) + 10
[0x7fff8990767a]

    2274 Thread_7277753   DispatchQueue_4: com.apple.root.low-priority
(concurrent)

      2274 start_wqthread  (in libsystem_c.dylib) + 13  [0x7fff88b16b85]

        2274 _pthread_wqthread  (in libsystem_c.dylib) + 316
[0x7fff88b153da]

          2274 _dispatch_worker_thread2  (in libdispatch.dylib) + 198
[0x7fff88c1f760]

            2274 _dispatch_queue_invoke  (in libdispatch.dylib) + 71
[0x7fff88c1ff77]

              2274 _dispatch_source_invoke  (in libdispatch.dylib) + 635
[0x7fff88c232b6]

                2274 _dispatch_after_timer_callback  (in libdispatch.dylib)
+ 16  [0x7fff88c20c07]

                  2274 _dispatch_call_block_and_release  (in
libdispatch.dylib) + 18  [0x7fff88c1e8ba]

                    2274 ???  (in libjava.jnilib)  load address 0x1058a3000
+ 0xda1c  [0x1058b0a1c]

                      2274 JNFPerformEnvBlock  (in JavaNativeFoundation) +
27  [0x10589414f]

                        2274 JNFObtainEnv  (in JavaNativeFoundation) + 171
[0x105893f5a]

                          2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1a0f41  [0x1051e1f41]

                            2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xcf404  [0x105110404]

                              2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x12b0f  [0x105053b0f]

                                2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x12bcd  [0x105053bcd]

                                  2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x13d42  [0x105054d42]

                                    2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x13de3  [0x105054de3]

                                      2274 ???  (in libclient64.dylib)
load address 0x105041000 + 0x13f23  [0x105054f23]

                                        2274 mach_msg  (in
libsystem_kernel.dylib) + 73  [0x7fff89906d71]

                                          2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]


Total number in stack (recursive counted multiple, when >=5):

        25       mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

        25       mach_msg_trap  (in libsystem_kernel.dylib) + 0
[0x7fff89907670]

        22       _pthread_start  (in libsystem_c.dylib) + 335
[0x7fff88b138bf]

        22       thread_start  (in libsystem_c.dylib) + 13  [0x7fff88b16b75]

        21       ???  (in libclient64.dylib)  load address 0x105041000 +
0x13884  [0x105054884]

        18       ???  (in libclient64.dylib)  load address 0x105041000 +
0x13de3  [0x105054de3]

        18       ???  (in libclient64.dylib)  load address 0x105041000 +
0x13f23  [0x105054f23]

        16       ???  (in libclient64.dylib)  load address 0x105041000 +
0x13d42  [0x105054d42]

        15       ???  (in libclient64.dylib)  load address 0x105041000 +
0x1391d  [0x10505491d]

        12       ???  (in libclient64.dylib)  load address 0x105041000 +
0x137c3  [0x1050547c3]

        8       ???  (in libclient64.dylib)  load address 0x105041000 +
0x2c5c8  [0x10506d5c8]

        7       ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7355  [0x1050f8355]

        7       ???  (in libclient64.dylib)  load address 0x105041000 +
0xb7540  [0x1050f8540]

        5       ???  (in <unknown binary>)  [0x1059fd85a]


Sort by top of stack, same collapsed (when >= 5):

        mach_msg_trap  (in libsystem_kernel.dylib)        56849

        kevent  (in libsystem_kernel.dylib)        2274

        semaphore_wait_trap  (in libsystem_kernel.dylib)        2274


Binary Images:

       0x104fae000 -        0x104fb5fff  java (1.0 - 1.0)
<4AC9C9AB-E0C3-3116-ADBE-2A99D1505D7B>
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/bin/java

       0x104fba000 -        0x104fc0fff  libjli.jnilib (??? - ???)
<2927BE65-1D10-3C92-B5B1-B5E070A84C31>
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libjli.jnilib

       0x104fcb000 -        0x104fd0fff  com.apple.JavaVM (14.1.0 - 14.1.0)
<83C8C2AB-E99D-39FF-80B4-90A7DEB1DAFB>
/System/Library/Frameworks/JavaVM.framework/Versions/A/JavaVM

       0x104fd9000 -        0x104fdefff  JavaLaunching (??? - ???)
<6322E021-91C3-36A3-AEED-AD4FAB888E4A>
/System/Library/PrivateFrameworks/JavaLaunching.framework/Versions/A/JavaLaunching

       0x105041000 -        0x10566fff7  libclient64.dylib (??? - ???)
<D3325E4E-CACF-39AE-8668-8F8E5A8F5B2D>
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/bundle/Libraries/libclient64.dylib

       0x10573f000 -        0x105746fff  libjvmlinkage.dylib (??? - ???)
<4E1B7F6A-050F-3F83-B333-4C0B5456022B>
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libjvmlinkage.dylib

       0x105880000 -        0x105888ff7  libverify.dylib (1.0 - 1.0)
<BEE0B2CE-E7BA-3830-BD53-D1AC2E127C38>
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libverify.dylib

       0x10588e000 -        0x105898fff  JavaNativeFoundation (??? - ???)
<3D79FBBE-054D-3209-A8D0-BF5D3D733A91>
/System/Library/Frameworks/JavaVM.framework/Versions/A/Frameworks/JavaNativeFoundation.framework/Versions/A/JavaNativeFoundation

       0x1058a3000 -        0x1058c3ff7  libjava.jnilib (??? - ???)
<F16EC2DC-1801-3D12-BB50-2D88A62FC0BA>
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libjava.jnilib

       0x1059e4000 -        0x1059f1ff7  libzip.jnilib (??? - ???)
<457C25EB-8F18-35CF-878C-82B4D6369300>
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libzip.jnilib

    0x7fff64bae000 -     0x7fff64be2ac7  dyld (195.5 - ???)
<B372EB7D-DCD8-30CE-9342-E06CADD7CACA> /usr/lib/dyld

    0x7fff80e87000 -     0x7fff80e88fff  libsystem_sandbox.dylib (??? -
???) <DC97E52F-C577-3A8A-A2F6-431AE3D40C40>
/usr/lib/system/libsystem_sandbox.dylib

    0x7fff80ec8000 -     0x7fff81ac9ff7  com.apple.AppKit (6.7.2 - 1138.23)
<5CD2C850-4F52-3BA2-BA11-3107DFD2D23C>
/System/Library/Frameworks/AppKit.framework/Versions/C/AppKit

    0x7fff82950000 -     0x7fff82950fff  com.apple.Accelerate.vecLib (3.7 -
vecLib 3.7) <C06A140F-6114-3B8B-B080-E509303145B8>
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib

    0x7fff82951000 -     0x7fff8295aff7  libsystem_notify.dylib (80.1.0 -
compatibility 1.0.0) <A4D651E3-D1C6-3934-AD49-7A104FD14596>
/usr/lib/system/libsystem_notify.dylib

    0x7fff8295b000 -     0x7fff829e0ff7  com.apple.Heimdal (2.1 - 2.0)
<C92E327E-CB5F-3C9B-92B0-F1680095C8A3>
/System/Library/PrivateFrameworks/Heimdal.framework/Versions/A/Heimdal

    0x7fff829e1000 -     0x7fff82a21ff7  libcups.2.dylib (2.9.0 -
compatibility 2.0.0) <B7173CA4-CE16-3BAB-8D83-185FCEFA15F5>
/usr/lib/libcups.2.dylib

    0x7fff82a22000 -     0x7fff82aa5fef  com.apple.Metadata (10.7.0 -
627.20) <E00156B0-663A-35EF-A307-A2CEB00F1845>
/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Metadata

    0x7fff82baa000 -     0x7fff82c71ff7  com.apple.ColorSync (4.7.0 -
4.7.0) <F325A9D7-7203-36B7-8C1C-B6A4D5CC73A8>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ColorSync.framework/Versions/A/ColorSync

    0x7fff82c72000 -     0x7fff82e8cfef  com.apple.CoreData (104 - 358.12)
<33B1FA75-7970-3751-9DCC-FF809D3E1FA2>
/System/Library/Frameworks/CoreData.framework/Versions/A/CoreData

    0x7fff82fbb000 -     0x7fff82fc3fff  libsystem_dnssd.dylib (??? - ???)
<998E3778-7B43-301C-9053-12045AB8544D> /usr/lib/system/libsystem_dnssd.dylib

    0x7fff82fd8000 -     0x7fff82fddfff  libcache.dylib (47.0.0 -
compatibility 1.0.0) <1571C3AB-BCB2-38CD-B3B2-C5FC3F927C6A>
/usr/lib/system/libcache.dylib

    0x7fff830db000 -     0x7fff831b9fff  com.apple.ImageIO.framework (3.1.1
- 3.1.1) <13E549F8-5BD6-3BAE-8C33-1D0BD269C081>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/ImageIO

    0x7fff8397c000 -     0x7fff83a1eff7  com.apple.securityfoundation (5.0
- 55005) <2814D17E-E6BB-30A2-A62E-2D481AF514F2>
/System/Library/Frameworks/SecurityFoundation.framework/Versions/A/SecurityFoundation

    0x7fff83a3d000 -     0x7fff83a61fff  com.apple.Kerberos (1.0 - 1)
<1F826BCE-DA8F-381D-9C4C-A36AA0EA1CB9>
/System/Library/Frameworks/Kerberos.framework/Versions/A/Kerberos

    0x7fff83a62000 -     0x7fff83aa1fff  com.apple.AE (527.7 - 527.7)
<B82F7ABC-AC8B-3507-B029-969DD5CA813D>
/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE

    0x7fff83c3b000 -     0x7fff83c46fff  com.apple.CommonAuth (2.1 - 2.0)
<BFDD0A8D-4BEA-39EC-98B3-2E083D7B1ABD>
/System/Library/PrivateFrameworks/CommonAuth.framework/Versions/A/CommonAuth

    0x7fff83c47000 -     0x7fff83cb5fff  com.apple.CoreSymbolication (2.1 -
66) <7CF9EF4A-262A-3009-8D42-A76F5614E372>
/System/Library/PrivateFrameworks/CoreSymbolication.framework/Versions/A/CoreSymbolication

    0x7fff83e04000 -     0x7fff83e9eff7  com.apple.SearchKit (1.4.0 -
1.4.0) <4E70C394-773E-3A4B-A93C-59A88ABA9509>
/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit

    0x7fff83e9f000 -     0x7fff84005fff  com.apple.CFNetwork (520.2.5 -
520.2.5) <406712D9-3F0C-3763-B4EB-868D01F1F042>
/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CFNetwork.framework/Versions/A/CFNetwork

    0x7fff8402e000 -     0x7fff84044fff  libGL.dylib (??? - ???)
<6A473BF9-4D35-34C6-9F8B-86B68091A9AF>
/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGL.dylib

    0x7fff84045000 -     0x7fff8409cfff  libTIFF.dylib (??? - ???)
<FF0D9A24-6956-3F03-81EA-3EEAD22C9DB8>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libTIFF.dylib

    0x7fff8409d000 -     0x7fff840a2fff  libcompiler_rt.dylib (6.0.0 -
compatibility 1.0.0) <98ECD5F6-E85C-32A5-98CD-8911230CB66A>
/usr/lib/system/libcompiler_rt.dylib

    0x7fff840a5000 -     0x7fff840a6fff  libdnsinfo.dylib (395.7.0 -
compatibility 1.0.0) <37FEFE78-BCB5-37EC-8E99-747469BCA4C7>
/usr/lib/system/libdnsinfo.dylib

    0x7fff840b3000 -     0x7fff840b3fff  com.apple.vecLib (3.7 - vecLib
3.7) <9A58105C-B36E-35B5-812C-4ED693F2618F>
/System/Library/Frameworks/vecLib.framework/Versions/A/vecLib

    0x7fff84142000 -     0x7fff84609fff  FaceCoreLight (1.4.7 -
compatibility 1.0.0) <E9D2A69C-6E81-358C-A162-510969F91490>
/System/Library/PrivateFrameworks/FaceCoreLight.framework/Versions/A/FaceCoreLight

    0x7fff84610000 -     0x7fff846f1fff  com.apple.CoreServices.OSServices
(478.29 - 478.29) <B487110E-C942-33A8-A494-3BDEDB88B1CD>
/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices

    0x7fff846f2000 -     0x7fff846f6fff  libmathCommon.A.dylib (2026.0.0 -
compatibility 1.0.0) <FF83AFF7-42B2-306E-90AF-D539C51A4542>
/usr/lib/system/libmathCommon.A.dylib

    0x7fff847f5000 -     0x7fff847f7fff  com.apple.TrustEvaluationAgent
(2.0 - 1) <1F31CAFF-C1C6-33D3-94E9-11B721761DDF>
/System/Library/PrivateFrameworks/TrustEvaluationAgent.framework/Versions/A/TrustEvaluationAgent

    0x7fff847f8000 -     0x7fff847f8fff  com.apple.audio.units.AudioUnit
(1.7.1 - 1.7.1) <04C10813-CCE5-3333-8C72-E8E35E417B3B>
/System/Library/Frameworks/AudioUnit.framework/Versions/A/AudioUnit

    0x7fff84886000 -     0x7fff84901ff7
com.apple.print.framework.PrintCore (7.1 - 366.1)
<3F140DEB-9F87-3672-97CC-F983752581AC>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/PrintCore.framework/Versions/A/PrintCore

    0x7fff84a85000 -     0x7fff84b87ff7  libxml2.2.dylib (10.3.0 -
compatibility 10.0.0) <22F1D1B6-1761-3687-9EFD-036EA15FB2E4>
/usr/lib/libxml2.2.dylib

    0x7fff84b88000 -     0x7fff84b8dfff  com.apple.OpenDirectory (10.7 -
146) <91A87249-6A2F-3F89-A8DE-0E95C0B54A3A>
/System/Library/Frameworks/OpenDirectory.framework/Versions/A/OpenDirectory

    0x7fff84b8e000 -     0x7fff84ba0ff7  libbsm.0.dylib (??? - ???)
<349BB16F-75FA-363F-8D98-7A9C3FA90A0D> /usr/lib/libbsm.0.dylib

    0x7fff84bdd000 -     0x7fff84c38ff7  com.apple.HIServices (1.10 - ???)
<BAB8B422-7047-3D2D-8E0A-13FCF153E4E7>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/HIServices.framework/Versions/A/HIServices

    0x7fff84cd0000 -     0x7fff84cd1fff  liblangid.dylib (??? - ???)
<CACBE3C3-2F7B-3EED-B50E-EDB73F473B77> /usr/lib/liblangid.dylib

    0x7fff84cd2000 -     0x7fff84cfbfff  libJPEG.dylib (??? - ???)
<64D079F9-256A-323B-A837-84628B172F21>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libJPEG.dylib

    0x7fff84d5d000 -     0x7fff84d62fff  libGIF.dylib (??? - ???)
<393E2DB5-9479-39A6-A75A-B5F20B852532>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libGIF.dylib

    0x7fff850bc000 -     0x7fff850bcfff  com.apple.Cocoa (6.6 - ???)
<7EC4D759-B2A6-3A99-AC75-809FED1500C6>
/System/Library/Frameworks/Cocoa.framework/Versions/A/Cocoa

    0x7fff850bd000 -     0x7fff850cbfff  com.apple.NetAuth (3.1 - 3.1)
<FE7EC4D7-5632-3B8D-9094-A0AC8D60EDEE>
/System/Library/PrivateFrameworks/NetAuth.framework/Versions/A/NetAuth

    0x7fff8512e000 -     0x7fff8512fff7  libremovefile.dylib (21.0.0 -
compatibility 1.0.0) <001E87FF-97DF-328D-B22F-16E3ACEF8864>
/usr/lib/system/libremovefile.dylib

    0x7fff85133000 -     0x7fff851a6fff  libstdc++.6.dylib (52.0.0 -
compatibility 7.0.0) <6BDD43E4-A4B1-379E-9ED5-8C713653DFF2>
/usr/lib/libstdc++.6.dylib

    0x7fff851a7000 -     0x7fff851d7ff7  com.apple.DictionaryServices
(1.2.1 - 158.2) <3FC86118-7553-38F7-8916-B329D2E94476>
/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/DictionaryServices.framework/Versions/A/DictionaryServices

    0x7fff8597c000 -     0x7fff85c98ff7  com.apple.CoreServices.CarbonCore
(960.18 - 960.18) <6020C3FB-6125-3EAE-A55D-1E77E38BEDEA>
/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore

    0x7fff85c99000 -     0x7fff85cc0ff7  com.apple.PerformanceAnalysis
(1.10 - 10) <DD87C994-66D6-330A-BAF9-AB86BE125A62>
/System/Library/PrivateFrameworks/PerformanceAnalysis.framework/Versions/A/PerformanceAnalysis

    0x7fff85d36000 -     0x7fff85d3bff7  libsystem_network.dylib (??? -
???) <5DE7024E-1D2D-34A2-80F4-08326331A75B>
/usr/lib/system/libsystem_network.dylib

    0x7fff85dc7000 -     0x7fff85dd5ff7  libkxld.dylib (??? - ???)
<B1BD4862-9D3F-3EEF-895C-A8E2E53684B6> /usr/lib/system/libkxld.dylib

    0x7fff85dd6000 -     0x7fff85dd8fff  libquarantine.dylib (36.0.0 -
compatibility 1.0.0) <4C3BFBC7-E592-3939-B376-1C2E2D7C5389>
/usr/lib/system/libquarantine.dylib

    0x7fff85dd9000 -     0x7fff85debff7  libz.1.dylib (1.2.5 -
compatibility 1.0.0) <30CBEF15-4978-3DED-8629-7109880A19D4>
/usr/lib/libz.1.dylib

    0x7fff85dec000 -     0x7fff85e4efff  com.apple.coreui (1.2.1 - 164.1)
<F7972630-F696-3FC5-9FCF-A6E1C8771078>
/System/Library/PrivateFrameworks/CoreUI.framework/Versions/A/CoreUI

    0x7fff85e4f000 -     0x7fff85e66fff  com.apple.CFOpenDirectory (10.7 -
146) <E71AE4A2-F72B-35F2-9043-9F45CF75F11A>
/System/Library/Frameworks/OpenDirectory.framework/Versions/A/Frameworks/CFOpenDirectory.framework/Versions/A/CFOpenDirectory

    0x7fff85e67000 -     0x7fff85e6eff7  com.apple.CommerceCore (1.0 - 17)
<3894FE48-EDCE-30E9-9796-E2F959D92704>
/System/Library/PrivateFrameworks/CommerceKit.framework/Versions/A/Frameworks/CommerceCore.framework/Versions/A/CommerceCore

    0x7fff860eb000 -     0x7fff860effff  libdyld.dylib (195.5.0 -
compatibility 1.0.0) <380C3F44-0CA7-3514-8080-46D1C9DF4FCD>
/usr/lib/system/libdyld.dylib

    0x7fff86336000 -     0x7fff86363fe7  libSystem.B.dylib (159.1.0 -
compatibility 1.0.0) <095FDD3C-3961-3865-A59B-A5B0A4B8B923>
/usr/lib/libSystem.B.dylib

    0x7fff863b8000 -     0x7fff863e5ff7  com.apple.opencl (1.50.63 -
1.50.63) <DB335C5C-3ABD-38C8-B6A5-8436EE1484D3>
/System/Library/Frameworks/OpenCL.framework/Versions/A/OpenCL

    0x7fff86930000 -     0x7fff86936ff7  libunwind.dylib (30.0.0 -
compatibility 1.0.0) <1E9C6C8C-CBE8-3F4B-A5B5-E03E3AB53231>
/usr/lib/system/libunwind.dylib

    0x7fff86937000 -     0x7fff8698bff7  com.apple.ScalableUserInterface
(1.0 - 1) <33563775-C662-313D-B7FA-3D575A9F3D41>
/System/Library/Frameworks/QuartzCore.framework/Versions/A/Frameworks/ScalableUserInterface.framework/Versions/A/ScalableUserInterface

    0x7fff86cf4000 -     0x7fff86d69ff7  libc++.1.dylib (19.0.0 -
compatibility 1.0.0) <C0EFFF1B-0FEB-3F99-BE54-506B35B555A9>
/usr/lib/libc++.1.dylib

    0x7fff86dee000 -     0x7fff86e03fff
com.apple.speech.synthesis.framework (4.0.74 - 4.0.74)
<C061ECBB-7061-3A43-8A18-90633F943295>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/SpeechSynthesis.framework/Versions/A/SpeechSynthesis

    0x7fff86e40000 -     0x7fff86e81fff  com.apple.QD (3.12 - ???)
<983D6E1E-B8BD-3260-A960-13727351D867>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/QD.framework/Versions/A/QD

    0x7fff8707e000 -     0x7fff870a4ff7  com.apple.framework.familycontrols
(3.0 - 300) <41A6DFC2-EAF5-390A-83A1-C8832528705C>
/System/Library/PrivateFrameworks/FamilyControls.framework/Versions/A/FamilyControls

    0x7fff870a5000 -     0x7fff870a5fff  com.apple.CoreServices (53 - 53)
<043C8026-8EDD-3241-B090-F589E24062EF>
/System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices

    0x7fff870fb000 -     0x7fff87528fff  libLAPACK.dylib (??? - ???)
<4F2E1055-2207-340B-BB45-E4F16171EE0D>
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib

    0x7fff87529000 -     0x7fff87533ff7  liblaunch.dylib (392.35.0 -
compatibility 1.0.0) <8F8BB206-CECA-33A5-A105-4A01C3ED5D23>
/usr/lib/system/liblaunch.dylib

    0x7fff8808a000 -     0x7fff880b2ff7  com.apple.CoreVideo (1.7 - 70.1)
<98F917B2-FB53-3EA3-B548-7E97B38309A7>
/System/Library/Frameworks/CoreVideo.framework/Versions/A/CoreVideo

    0x7fff880b3000 -     0x7fff880b6fff  libRadiance.dylib (??? - ???)
<CD89D70D-F177-3BAE-8A26-644EA7D5E28E>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libRadiance.dylib

    0x7fff886e8000 -     0x7fff886effff  com.apple.NetFS (4.0 - 4.0)
<433EEE54-E383-3505-9154-45B909FD3AF0>
/System/Library/Frameworks/NetFS.framework/Versions/A/NetFS

    0x7fff886f0000 -     0x7fff88a09ff7  com.apple.Foundation (6.7.1 -
833.20) <D922F590-FDA6-3D89-A271-FD35E2290624>
/System/Library/Frameworks/Foundation.framework/Versions/C/Foundation

    0x7fff88a79000 -     0x7fff88a96ff7  libxpc.dylib (77.17.0 -
compatibility 1.0.0) <72A16104-2F23-3C22-B474-1953F06F9376>
/usr/lib/system/libxpc.dylib

    0x7fff88a9c000 -     0x7fff88ab0ff7  com.apple.LangAnalysis (1.7.0 -
1.7.0) <04C31EF0-912A-3004-A08F-CEC27030E0B2>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/LangAnalysis.framework/Versions/A/LangAnalysis

    0x7fff88ac5000 -     0x7fff88ba2fef  libsystem_c.dylib (763.12.0 -
compatibility 1.0.0) <FF69F06E-0904-3C08-A5EF-536FAFFFDC22>
/usr/lib/system/libsystem_c.dylib

    0x7fff88ba3000 -     0x7fff88bf1fff  libauto.dylib (??? - ???)
<D8AC8458-DDD0-3939-8B96-B6CED81613EF> /usr/lib/libauto.dylib

    0x7fff88bf2000 -     0x7fff88c09fff
com.apple.MultitouchSupport.framework (220.62.1 - 220.62.1)
<F21C79C0-4B5A-3645-81A6-74F8EFA900CE>
/System/Library/PrivateFrameworks/MultitouchSupport.framework/Versions/A/MultitouchSupport

    0x7fff88c1d000 -     0x7fff88c2bfff  libdispatch.dylib (187.7.0 -
compatibility 1.0.0) <712AAEAC-AD90-37F7-B71F-293FF8AE8723>
/usr/lib/system/libdispatch.dylib

    0x7fff88c2c000 -     0x7fff88c77ff7  com.apple.SystemConfiguration
(1.11.1 - 1.11) <F832FE21-5509-37C6-B1F1-48928F31BE45>
/System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration

    0x7fff88c7c000 -     0x7fff88d1bfff  com.apple.LaunchServices (480.21 -
480.21) <6BFADEA9-5BC1-3B53-A013-488EB7F1AB57>
/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices

    0x7fff88d1c000 -     0x7fff88d1cfff  com.apple.ApplicationServices (41
- 41) <89B6AD5B-5C75-3E83-8C2B-AA7F4C55E400>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/ApplicationServices

    0x7fff88d1d000 -     0x7fff88d41ff7  com.apple.RemoteViewServices (1.2
- 39) <862849C8-84C1-32A1-B87E-B29E74778C9F>
/System/Library/PrivateFrameworks/RemoteViewServices.framework/Versions/A/RemoteViewServices

    0x7fff88d42000 -     0x7fff88d7dff7  libsystem_info.dylib (??? - ???)
<9C8C2DCB-96DB-3471-9DCE-ADCC26BE2DD4> /usr/lib/system/libsystem_info.dylib

    0x7fff88dbf000 -     0x7fff88e2ffff  com.apple.datadetectorscore (3.0 -
179.4) <2A822A13-94B3-3A43-8724-98FDF698BB12>
/System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/DataDetectorsCore

    0x7fff88e99000 -     0x7fff88edbff7  libcommonCrypto.dylib (55010.0.0 -
compatibility 1.0.0) <BB770C22-8C57-365A-8716-4A3C36AE7BFB>
/usr/lib/system/libcommonCrypto.dylib

    0x7fff88f2c000 -     0x7fff89085fff
com.apple.audio.toolbox.AudioToolbox (1.7.1 - 1.7.1)
<4877267E-F736-3019-85D3-40A32A042A80>
/System/Library/Frameworks/AudioToolbox.framework/Versions/A/AudioToolbox

    0x7fff89086000 -     0x7fff89193fff  libJP2.dylib (??? - ???)
<6052C973-9354-35CB-AAB9-31D00D8786F9>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libJP2.dylib

    0x7fff89194000 -     0x7fff89297fff  libsqlite3.dylib (9.6.0 -
compatibility 9.0.0) <7F60B0FF-4946-3639-89AB-B540D318B249>
/usr/lib/libsqlite3.dylib

    0x7fff8929c000 -     0x7fff8929dfff  libunc.dylib (24.0.0 -
compatibility 1.0.0) <337960EE-0A85-3DD0-A760-7134CF4C0AFF>
/usr/lib/system/libunc.dylib

    0x7fff8929e000 -     0x7fff892a4fff  com.apple.DiskArbitration (2.4.1 -
2.4.1) <CEA34337-63DE-302E-81AA-10D717E1F699>
/System/Library/Frameworks/DiskArbitration.framework/Versions/A/DiskArbitration

    0x7fff892ad000 -     0x7fff89351fff  com.apple.ink.framework (1.3.2 -
110) <C8840EA4-AE7B-360C-A191-D36B5F10B6B5>
/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Ink.framework/Versions/A/Ink

    0x7fff8937c000 -     0x7fff8937ffff  libCoreVMClient.dylib (??? - ???)
<E034C772-4263-3F48-B083-25A758DD6228>
/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreVMClient.dylib

    0x7fff89380000 -     0x7fff89485ff7  libFontParser.dylib (??? - ???)
<B9A53808-C97E-3293-9C33-1EA9D4E83EC8>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontParser.dylib

    0x7fff89712000 -     0x7fff897c5fff  com.apple.CoreText (220.11.0 -
???) <4EA8E2DF-542D-38D5-ADB9-C0DAA73F898B>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreText.framework/Versions/A/CoreText

    0x7fff898f2000 -     0x7fff89912fff  libsystem_kernel.dylib (1699.24.8
- compatibility 1.0.0) <C56819BB-3779-3726-B610-4CF7B3ABB6F9>
/usr/lib/system/libsystem_kernel.dylib

    0x7fff8993b000 -     0x7fff89b3dfff  libicucore.A.dylib (46.1.0 -
compatibility 1.0.0) <38CD6ED3-C8E4-3CCD-89AC-9C3198803101>
/usr/lib/libicucore.A.dylib

    0x7fff89b3e000 -     0x7fff89b5bfff  libPng.dylib (??? - ???)
<3C70A94C-9442-3E11-AF51-C1B0EF81680E>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/Versions/A/Resources/libPng.dylib

    0x7fff89b68000 -     0x7fff89bccfff  com.apple.Symbolication (1.2 -
83.1) <A7E088DE-BC16-3C24-A0D0-30EEBA221659>
/System/Library/PrivateFrameworks/Symbolication.framework/Versions/A/Symbolication

    0x7fff89f72000 -     0x7fff89f74fff  libCVMSPluginSupport.dylib (??? -
???) <61D89F3C-C64D-3733-819F-8AAAE4E2E993>
/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCVMSPluginSupport.dylib

    0x7fff8a07c000 -     0x7fff8a0e6fff  com.apple.framework.IOKit (2.0 -
???) <87D55F1D-CDB5-3D13-A5F9-98EA4E22F8EE>
/System/Library/Frameworks/IOKit.framework/Versions/A/IOKit

    0x7fff8a0e7000 -     0x7fff8a0f2ff7  libc++abi.dylib (14.0.0 -
compatibility 1.0.0) <8FF3D766-D678-36F6-84AC-423C878E6D14>
/usr/lib/libc++abi.dylib

    0x7fff8a0f3000 -     0x7fff8a153fff  libvDSP.dylib (325.4.0 -
compatibility 1.0.0) <3A7521E6-5510-3FA7-AB65-79693A7A5839>
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib

    0x7fff8a154000 -     0x7fff8a167ff7  libCRFSuite.dylib (??? - ???)
<0B76941F-218E-30C8-B6DE-E15919F8DBEB> /usr/lib/libCRFSuite.dylib

    0x7fff8a168000 -     0x7fff8a24cdef  libobjc.A.dylib (228.0.0 -
compatibility 1.0.0) <C5F2392D-B481-3A9D-91BE-3D039FFF4DEC>
/usr/lib/libobjc.A.dylib

    0x7fff8a24d000 -     0x7fff8a287fef  com.apple.DebugSymbols (2.1 - 85)
<F45985E2-D1D0-3F47-861E-47904837B76F>
/System/Library/PrivateFrameworks/DebugSymbols.framework/Versions/A/DebugSymbols

    0x7fff8a288000 -     0x7fff8a289ff7  libsystem_blocks.dylib (53.0.0 -
compatibility 1.0.0) <8BCA214A-8992-34B2-A8B9-B74DEACA1869>
/usr/lib/system/libsystem_blocks.dylib

    0x7fff8a994000 -     0x7fff8a9fcff7  com.apple.audio.CoreAudio (4.0.1 -
4.0.1) <7966E3BE-376B-371A-A21D-9BD763C0BAE7>
/System/Library/Frameworks/CoreAudio.framework/Versions/A/CoreAudio

    0x7fff8a9fd000 -     0x7fff8aa08ff7
com.apple.speech.recognition.framework (4.0.19 - 4.0.19)
<48607E6E-8612-3267-9184-E948B1863B32>
/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SpeechRecognition.framework/Versions/A/SpeechRecognition

    0x7fff8add7000 -     0x7fff8aeefff7  com.apple.DesktopServices (1.6.1 -
1.6.1) <4418EAA6-7163-3A77-ABD3-F8289796C81A>
/System/Library/PrivateFrameworks/DesktopServicesPriv.framework/Versions/A/DesktopServicesPriv

    0x7fff8aef0000 -     0x7fff8af23ff7  com.apple.GSS (2.1 - 2.0)
<9A2C9736-DA10-367A-B376-2C7A584E6C7A>
/System/Library/Frameworks/GSS.framework/Versions/A/GSS

    0x7fff8af24000 -     0x7fff8af24fff  libkeymgr.dylib (23.0.0 -
compatibility 1.0.0) <61EFED6A-A407-301E-B454-CD18314F0075>
/usr/lib/system/libkeymgr.dylib

    0x7fff8af25000 -     0x7fff8af64ff7  libGLImage.dylib (??? - ???)
<2D1D8488-EC5F-3229-B983-CFDE0BB37586>
/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLImage.dylib

    0x7fff8af65000 -     0x7fff8afb8fff  libFontRegistry.dylib (??? - ???)
<57FBD85F-41A6-3DB9-B5F4-FCC6B260F1AD>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontRegistry.dylib

    0x7fff8afb9000 -     0x7fff8afb9fff  com.apple.Accelerate (1.7 -
Accelerate 1.7) <82DDF6F5-FBC3-323D-B71D-CF7ABC5CF568>
/System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate

    0x7fff8afbc000 -     0x7fff8afc2fff  IOSurface (??? - ???)
<03F95CAC-569C-3573-B3D7-2D211B8BDC56>
/System/Library/Frameworks/IOSurface.framework/Versions/A/IOSurface

    0x7fff8afd1000 -     0x7fff8afd7fff  libGFXShared.dylib (??? - ???)
<343AE6C0-EB02-333C-8D35-DF6093B92758>
/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGFXShared.dylib

    0x7fff8b0b5000 -     0x7fff8b38dff7  com.apple.security (7.0 - 55010)
<93713FF4-FE86-3B4C-8150-5FCC7F3320C8>
/System/Library/Frameworks/Security.framework/Versions/A/Security

    0x7fff8b456000 -     0x7fff8b562fff  libcrypto.0.9.8.dylib (44.0.0 -
compatibility 0.9.8) <3A8E1F89-5E26-3C8B-B538-81F5D61DBF8A>
/usr/lib/libcrypto.0.9.8.dylib

    0x7fff8b564000 -     0x7fff8b573ff7  com.apple.opengl (1.7.5 - 1.7.5)
<2945F1A6-910C-3596-9988-5701B04BD821>
/System/Library/Frameworks/OpenGL.framework/Versions/A/OpenGL

    0x7fff8b574000 -     0x7fff8b579fff  libpam.2.dylib (3.0.0 -
compatibility 3.0.0) <D952F17B-200A-3A23-B9B2-7C1F7AC19189>
/usr/lib/libpam.2.dylib

    0x7fff8b5c7000 -     0x7fff8bcda587  com.apple.CoreGraphics (1.600.0 -
???) <A9F2451E-6F60-350E-A6E5-539669B53074>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/CoreGraphics.framework/Versions/A/CoreGraphics

    0x7fff8bcdb000 -     0x7fff8bffffff  com.apple.HIToolbox (1.8 - ???)
<A3BE7C59-52E6-3A7F-9B30-24B7DD3E95F2>
/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/HIToolbox

    0x7fff8c4a8000 -     0x7fff8c4faff7  libGLU.dylib (??? - ???)
<3C9153A0-8499-3DC0-AAA4-9FA6E488BE13>
/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLU.dylib

    0x7fff8c4fb000 -     0x7fff8c517ff7  com.apple.GenerationalStorage (1.0
- 125) <31F60175-E38D-3C63-8D95-32CFE7062BCB>
/System/Library/PrivateFrameworks/GenerationalStorage.framework/Versions/A/GenerationalStorage

    0x7fff8c518000 -     0x7fff8c78bfff  com.apple.CoreImage (7.82 - 1.0.1)
<282801B6-5D80-3E2C-88A4-00FE29906D5A>
/System/Library/Frameworks/QuartzCore.framework/Versions/A/Frameworks/CoreImage.framework/Versions/A/CoreImage

    0x7fff8c78c000 -     0x7fff8c92bfff  com.apple.QuartzCore (1.7 - 270.0)
<E8FC9AA4-A5CB-384B-AD29-7190A1387D3E>
/System/Library/Frameworks/QuartzCore.framework/Versions/A/QuartzCore

    0x7fff8c92c000 -     0x7fff8c9c2ff7  libvMisc.dylib (325.4.0 -
compatibility 1.0.0) <642D8D54-F9F5-3FBB-A96C-EEFE94C6278B>
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib

    0x7fff8c9c3000 -     0x7fff8c9c9fff  libmacho.dylib (800.0.0 -
compatibility 1.0.0) <165514D7-1BFA-38EF-A151-676DCD21FB64>
/usr/lib/system/libmacho.dylib

    0x7fff8c9f4000 -     0x7fff8ca78ff7  com.apple.ApplicationServices.ATS
(317.5.0 - ???) <FE629F2D-6BC0-3A58-9844-D8B9A6808A00>
/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/ATS

    0x7fff8ca95000 -     0x7fff8ca96fff  libDiagnosticMessagesClient.dylib
(??? - ???) <3DCF577B-F126-302B-BCE2-4DB9A95B8598>
/usr/lib/libDiagnosticMessagesClient.dylib

    0x7fff8ca97000 -     0x7fff8cac2ff7  libxslt.1.dylib (3.24.0 -
compatibility 3.0.0) <4DB5ED11-004B-36B5-AE5F-2AB714754241>
/usr/lib/libxslt.1.dylib

    0x7fff8cace000 -     0x7fff8d0b2fff  libBLAS.dylib (??? - ???)
<C34F6D88-187F-33DC-8A68-C0C9D1FA36DF>
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib

    0x7fff8d0b3000 -     0x7fff8d0d2fff  libresolv.9.dylib (46.0.0 -
compatibility 1.0.0) <33263568-E6F3-359C-A4FA-66AD1300F7D4>
/usr/lib/libresolv.9.dylib

    0x7fff8d185000 -     0x7fff8d27afff  libiconv.2.dylib (7.0.0 -
compatibility 7.0.0) <5C40E880-0706-378F-B864-3C2BD922D926>
/usr/lib/libiconv.2.dylib

    0x7fff8d27b000 -     0x7fff8d3b4fef  com.apple.vImage (5.1 - 5.1)
<EB634387-CD15-3246-AC28-5FB368ACCEA2>
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage

    0x7fff8d688000 -     0x7fff8d85cfff  com.apple.CoreFoundation (6.7.1 -
635.15) <FE4A86C2-3599-3CF8-AD1A-822F1FEA820F>
/System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation

    0x7fff8d85d000 -     0x7fff8d864fff  libcopyfile.dylib (85.1.0 -
compatibility 1.0.0) <0AB51EE2-E914-358C-AC19-47BC024BDAE7>
/usr/lib/system/libcopyfile.dylib

Sample analysis of process 64408 written to file /dev/stdout

Cheers,

 -- Howard.

On 6 January 2012 13:33, David Holmes <davidcholmes at aapt.net.au> wrote:

> **
> If you have a hang from which you can't get a stack dump it generally
> indicates that the VM can't reach a safepoint. At that point you need a
> native tools (eg pstack on Solaris) or a debugger to help you see the
> stacks.
>
> The inability to reach a safepoint indicates to me that there is a
> livelock, probably involving spinning on locks (hence why -UseSpinning can
> avoid the problem). Can you confirm that CPU is being consumed during the
> "hang"?
>
> Anyway it's a VM bug.
>
> David
>
> -----Original Message-----
> *From:* Howard Lovatt [mailto:howard.lovatt at gmail.com]
> *Sent:* Friday, 6 January 2012 12:28 PM
> *To:* Vitaly Davidovich
> *Cc:* dholmes at ieee.org; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Nested synchronized
>
> Hi,
>
> No luck with -XX:-EliminateLocks, still hangs. Also I can't get a stack
> dump from kill. Tried kill -1 to kill -9, no luck. Will try reentrant locks
> and report.
>
> Thanks for all the suggestions,
>
>  -- Howard.
>
> On 6 January 2012 13:01, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
>> Sorry left another bit out.  Since the suspicion is the nesting try
>> -XX:-EliminateLocks, IIRC.  Get rid of the other flags too so we're
>> controlling one thing at a time.
>>  On Jan 5, 2012 8:55 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>>
>>> Possible but would be good to have call stacks.  Also can you try using
>>> a reentrant lock instead of synchronized?
>>> On Jan 5, 2012 8:44 PM, "Howard Lovatt" <howard.lovatt at gmail.com> wrote:
>>>
>>>> Hi Vitaly,
>>>>
>>>> If I add the following JVM options, -ea -XX:+UseBiasedLocking
>>>> -XX:-UseSpinning -XX:+UseTLAB -XX:+UseThreadPriorities, then the program
>>>> runs. Indicating to me that it is a JVM problem.
>>>>
>>>>  -- Howard.
>>>>
>>>> On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>>>
>>>>> Howard,
>>>>>
>>>>> Have you captured call stacks when it hangs?
>>>>>
>>>>> Vitaly
>>>>>  On Jan 5, 2012 8:14 PM, "Howard Lovatt" <howard.lovatt at gmail.com>
>>>>> wrote:
>>>>>
>>>>>>  Hi David,
>>>>>>
>>>>>> There is only one sum object shared between the two threads (1st line
>>>>>> of main) and hence all synchronization is on the same object. Therefore I
>>>>>> think the code should work (even though the second synchronization is
>>>>>> redundant). As a double check on my understanding I just added a specific
>>>>>> mutex object to the code and synchronized on that and got the same result.
>>>>>>
>>>>>> Have I understood you comment correctly?
>>>>>>
>>>>>> Thanks,
>>>>>>
>>>>>>  -- Howard.
>>>>>>
>>>>>> On 6 January 2012 12:00, David Holmes <davidcholmes at aapt.net.au>wrote:
>>>>>>
>>>>>>> **
>>>>>>> That's not nested synchronization as you are using two different
>>>>>>> objects. It is a classic deadlock:
>>>>>>>
>>>>>>> - sync method on Obj A calls sync method on Obj B
>>>>>>> - sync method on Obj B calls sync method on Objj A
>>>>>>>
>>>>>>> Thread 1 does the call to ObjA
>>>>>>> Thread 2 does the call to Obj B
>>>>>>>
>>>>>>> David
>>>>>>> ------
>>>>>>>
>>>>>>> -----Original Message-----
>>>>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Howard
>>>>>>> Lovatt
>>>>>>> *Sent:* Friday, 6 January 2012 10:53 AM
>>>>>>> *To:* concurrency-interest at cs.oswego.edu
>>>>>>> *Subject:* [concurrency-interest] Nested synchronized
>>>>>>>
>>>>>>> Hi,
>>>>>>>
>>>>>>> I have seen something I think is a JVM bug but would like to check
>>>>>>> my understanding before reporting a problem. The following program normally
>>>>>>> hangs, i.e. the problem is intermittent, on my computer, MacBook Pro, Java
>>>>>>> 6 or 7, 4 core processor. The problem is that there are synchronized
>>>>>>> methods, isSetA1 and isSetA2 (near end of listing below), that call another
>>>>>>> synchronized method, conditionallySumArguments (at end of listing below).
>>>>>>> The second synchronized is unnecessary since the method is always called
>>>>>>> within an already synchronized method and if the second synchronized is
>>>>>>> removed the program works as expected. However I think an extra
>>>>>>> synchronized should be redundant, not a problem?
>>>>>>>
>>>>>>>  package nestedsynchronizedproblem;
>>>>>>>
>>>>>>>  import java.util.concurrent.Callable;
>>>>>>>  import java.util.concurrent.ExecutorService;
>>>>>>>  import java.util.concurrent.Executors;
>>>>>>>  import java.util.concurrent.TimeUnit;
>>>>>>>
>>>>>>>  import static java.lang.System.*;
>>>>>>>
>>>>>>>  /**
>>>>>>>   * Test of nested synchronized. Mimics calling a parallel sum
>>>>>>> method.
>>>>>>>   *
>>>>>>>   * @author  Howard Lovatt
>>>>>>>   */
>>>>>>>  public class NestedSynchronizedProblem {
>>>>>>>    private static final int loops = 10 * 1000 * 1000; // This needs
>>>>>>> to be large for hanging!
>>>>>>>
>>>>>>>    public static void main( final String... notUsed ) throws
>>>>>>> InterruptedException {
>>>>>>>      final ParrallelSumMethod sum = new ParrallelSumMethod();
>>>>>>>      final Callable<Void> setA1 = new Callable<Void>() {
>>>>>>>        @Override public Void call() throws Exception {
>>>>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA1( l ); }
>>>>>>>          return null;
>>>>>>>        }
>>>>>>>      };
>>>>>>>      final Callable<Void> setA2 = new Callable<Void>() {
>>>>>>>        @Override public Void call() throws Exception {
>>>>>>>          for ( int l = 0; l < loops; l++ ) { sum.setA2( l ); }
>>>>>>>          return null;
>>>>>>>        }
>>>>>>>      };
>>>>>>>      final ExecutorService pool = Executors.newCachedThreadPool();
>>>>>>>      pool.submit( setA1 );
>>>>>>>      pool.submit( setA2 );
>>>>>>>      pool.shutdown();
>>>>>>>      final boolean ok = pool.awaitTermination( 1, TimeUnit.MINUTES
>>>>>>> );
>>>>>>>      out.println( sum.getSum() + (ok ? ", terminated ok" : ",
>>>>>>> failed to terminate") );
>>>>>>>      pool.shutdownNow();
>>>>>>>    }
>>>>>>>  }
>>>>>>>
>>>>>>>
>>>>>>>  final class ParrallelSumMethod {
>>>>>>>    private long sum = 0;
>>>>>>>    private Long a1 = null;
>>>>>>>    private Long a2 = null;
>>>>>>>
>>>>>>>    public void setA1( final long a1Arg ) throws
>>>>>>> InterruptedException {
>>>>>>>      for ( ;; ) {
>>>>>>>        if ( isSetA1( a1Arg ) ) { return; }
>>>>>>>        checkForInterrupt();
>>>>>>>      }
>>>>>>>    }
>>>>>>>
>>>>>>>    public void setA2( final long a2Arg ) throws
>>>>>>> InterruptedException {
>>>>>>>      for ( ;; ) {
>>>>>>>        if ( isSetA2( a2Arg ) ) { return; }
>>>>>>>        checkForInterrupt();
>>>>>>>      }
>>>>>>>    }
>>>>>>>
>>>>>>>    public Long getSum() { return sum; }
>>>>>>>
>>>>>>>    private static void checkForInterrupt() throws
>>>>>>> InterruptedException {
>>>>>>>      if ( Thread.interrupted() ) { throw new
>>>>>>> InterruptedException(); }
>>>>>>>    }
>>>>>>>
>>>>>>>    private synchronized boolean isSetA1( final long a1Arg ) {
>>>>>>>      if ( a1 == null ) {
>>>>>>>        a1 = a1Arg;
>>>>>>>        conditionallySumArguments();
>>>>>>>        return true;
>>>>>>>      }
>>>>>>>      return false;
>>>>>>>    }
>>>>>>>
>>>>>>>    private synchronized boolean isSetA2( final long a2Arg ) {
>>>>>>>      if ( a2 == null ) {
>>>>>>>        a2 = a2Arg;
>>>>>>>        conditionallySumArguments();
>>>>>>>        return true;
>>>>>>>      }
>>>>>>>      return false;
>>>>>>>    }
>>>>>>>
>>>>>>>    private synchronized void conditionallySumArguments() { // Works
>>>>>>> if not synchronized!!!
>>>>>>>      if ( ( a1 == null ) || ( a2 == null ) ) { return; }
>>>>>>>      sum += a1 + a2;
>>>>>>>      a1 = a2 = null;
>>>>>>>    }
>>>>>>>  }
>>>>>>>
>>>>>>>
>>>>>>> Thanks in advance for any comments,
>>>>>>>
>>>>>>>   -- Howard.
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>>   -- Howard.
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>
>>>>
>>>> --
>>>>   -- Howard.
>>>>
>>>>
>
>
> --
>   -- Howard.
>
>


-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/5b973093/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Jan  5 23:05:22 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 6 Jan 2012 14:05:22 +1000
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB6rjJ339pLaZ1R+PjGCB88=EpHotuHUqQJz=WcP_2keAA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEDMJCAA.davidcholmes@aapt.net.au>

Yeah not much help without symbolic information. It's the two pool threads
that are of interest.

David

 -----Original Message-----
From: Howard Lovatt [mailto:howard.lovatt at gmail.com]
Sent: Friday, 6 January 2012 1:43 PM
To: dholmes at ieee.org
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Nested synchronized


  Hi,


  Finally got a stack trace, but from native tools so I am not sure how much
use it is. When hung it consumes negligible CPU. Stack trace below.


      2274 Thread_7277683: Java: pool-1-thread-1

      + 2274 ???  (in <unknown binary>)  [0x105a66a78]

      +   2274 ???  (in <unknown binary>)  [0x105a5fa48]

      +     2274 JVM_IsInterrupted  (in libjvmlinkage.dylib) + 59
[0x10574078b]

      +       2274 JVM_IsInterrupted  (in libclient64.dylib) + 149
[0x1051cf69d]

      +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x7c198  [0x1050bd198]

      +           2274 ???  (in libclient64.dylib)  load address 0x105041000
+ 0x1b5c4d  [0x1051f6c4d]

      +             2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x12b0f  [0x105053b0f]

      +               2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x12bcd  [0x105053bcd]

      +                 2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13d42  [0x105054d42]

      +                   2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13de3  [0x105054de3]

      +                     2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x13f23  [0x105054f23]

      +                       2274 mach_msg  (in libsystem_kernel.dylib) +
73  [0x7fff89906d71]

      +                         2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]

      2274 Thread_7277684: Java: pool-1-thread-2

      + 2274 ???  (in <unknown binary>)  [0x105a65287]

      +   2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x1e3d25  [0x105224d25]

      +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0xd12cd  [0x1051122cd]

      +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
0x14125  [0x105055125]

      +         2274 mach_msg  (in libsystem_kernel.dylib) + 73
[0x7fff89906d71]

      +           2274 mach_msg_trap  (in libsystem_kernel.dylib) + 10
[0x7fff8990767a]

      2274 Thread_7277753   DispatchQueue_4: com.apple.root.low-priority
(concurrent)

        2274 start_wqthread  (in libsystem_c.dylib) + 13  [0x7fff88b16b85]

          2274 _pthread_wqthread  (in libsystem_c.dylib) + 316
[0x7fff88b153da]

            2274 _dispatch_worker_thread2  (in libdispatch.dylib) + 198
[0x7fff88c1f760]

              2274 _dispatch_queue_invoke  (in libdispatch.dylib) + 71
[0x7fff88c1ff77]

                2274 _dispatch_source_invoke  (in libdispatch.dylib) + 635
[0x7fff88c232b6]

                  2274 _dispatch_after_timer_callback  (in
libdispatch.dylib) + 16  [0x7fff88c20c07]

                    2274 _dispatch_call_block_and_release  (in
libdispatch.dylib) + 18  [0x7fff88c1e8ba]

                      2274 ???  (in libjava.jnilib)  load address
0x1058a3000 + 0xda1c  [0x1058b0a1c]

                        2274 JNFPerformEnvBlock  (in JavaNativeFoundation) +
27  [0x10589414f]

                          2274 JNFObtainEnv  (in JavaNativeFoundation) + 171
[0x105893f5a]

                            2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0x1a0f41  [0x1051e1f41]

                              2274 ???  (in libclient64.dylib)  load address
0x105041000 + 0xcf404  [0x105110404]

                                2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x12b0f  [0x105053b0f]

                                  2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x12bcd  [0x105053bcd]

                                    2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x13d42  [0x105054d42]

                                      2274 ???  (in libclient64.dylib)  load
address 0x105041000 + 0x13de3  [0x105054de3]

                                        2274 ???  (in libclient64.dylib)
load address 0x105041000 + 0x13f23  [0x105054f23]

                                          2274 mach_msg  (in
libsystem_kernel.dylib) + 73  [0x7fff89906d71]

                                            2274 mach_msg_trap  (in
libsystem_kernel.dylib) + 10  [0x7fff8990767a]





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/ad05ff0e/attachment.html>

From peter.firmstone at zeus.net.au  Thu Jan  5 23:09:55 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Fri, 06 Jan 2012 14:09:55 +1000
Subject: [concurrency-interest] Reference Collections
Message-ID: <1325822995.10496.167.camel@bluto>

Ok, based on the comments so far:

Reasoning about the possibility that one thread may not be able to keep
up with a ReferenceQueue, bestsss mentioned that ReferenceQueue is a
stack, not a queue, because it blocks while the garbage collector thread
holds it's lock, the cleaning thread won't be able to poll(), but I
suspect the jvm only does this a scheduled intervals.

Oracle might decide to improve ReferenceQueue in future, to use a lock
free queue.

If the underlying collection is causing a backlog then only way to
increase concurrency is to change the underlying collection, that's up
to the user.

If ReferenceQueue is causing a concurrency issue, then we could
potentially have multiple queue's for the collection, but I think we
need to see that happen before increasing complexity.

Anyone have access to some big iron?

It's probably also worth noting that the performance of asynchronous
garbage cleaning, only indirectly affects performance of client threads
using the collection.  If it's blocked, it doesn't consume cpu, it
should eventually succeed, at the risk of increased memory consumption.

Here's the current implementation based on everyone's input:

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.river.impl.util;

import java.lang.ref.PhantomReference;
import java.lang.ref.Reference;
import java.lang.ref.ReferenceQueue;
import java.util.Collection;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ScheduledFuture;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

/**
 * ReferenceProcessor is responsible for creation and collection of
References 
 * on behalf of Reference Collection implementations.
 *
 * @param <T> 
 * @author peter
 */
class ReferenceProcessor<T> implements ReferenceQueuingFactory<T,
Referrer<T>> {
    
    private final static ScheduledExecutorService garbageCleaner =
            Executors.newScheduledThreadPool(1);
    // Map to register newly created object references.
    private final static Map<Reference,ScheduledFuture> finalizerTasks =
            new ConcurrentHashMap<Reference,ScheduledFuture>();
    // Finalizer queue to advise cancellation of ScheduledFuture's, 
    // when their ReferenceProcessor has been collected.
    private final static ReferenceQueue<Reference> phantomQueue = 
            new ReferenceQueue<Reference>();
    static {
        // Finizer Task to cancel unneeded tasks.
        garbageCleaner.scheduleAtFixedRate(
                new FinalizerTask(phantomQueue, finalizerTasks), 
                5L, 5L, TimeUnit.MINUTES
                );
    }
    
    private final Collection<Referrer<T>> col;
    private final ReferenceQueue<T> queue;
    private final Ref type;
    private final Lock queueLock;
    private final boolean gcThreads;
    private volatile boolean started = false;
    
    ReferenceProcessor(Collection<Referrer<T>> col, Ref type,
ReferenceQueue<T> queue, boolean gcThreads){
        if (col == null || type == null ) throw new
NullPointerException("collection or reference type cannot be null");
        this.col = col;
        this.type = type;
        this.queue = type == Ref.STRONG ? null : queue;
        this.gcThreads = gcThreads;
        queueLock = new ReentrantLock();
    }
    
    /**
     * Register with executor service and finaliser for cleanup.
     */
    public void start(){
       if (started) return; // Start once only.
       synchronized (this){
           started = true;
       }
       ScheduledFuture task;
       task = (gcThreads && queue != null)
                ? garbageCleaner.scheduleAtFixedRate(new
CleanerTask(col, queue), 10L, 10L, TimeUnit.SECONDS) 
                : null;
       if ( task != null ){
           // Register with finaliser.
            @SuppressWarnings("unchecked")
           Reference r = new PhantomReference(this, phantomQueue);
           finalizerTasks.put(r, task);
       }
    }

    @Override
    public T pseudoReferent(Referrer<T> u) {
        throw new UnsupportedOperationException("Not supported.");
    }

    @Override
    public Referrer<T> referenced(T w, boolean enque, boolean temporary)
{
        if (w == null) return null;
        if (temporary) return ReferenceFactory.singleUseForLookup(w,
type);
        return ReferenceFactory.create(w, enque == true ? queue : null,
type);
    }

    @Override
    public void processQueue() {
        if (queue == null || gcThreads) return;
        Object t = null;
        /*
         * The reason for using an explicit lock is if another thread is
         * removing the garbage, we don't want to prevent all other
threads
         * accessing the underlying collection, when it blocks on poll,
         * this means that some client threads will receive null values 
         * on occassion, but this is a small price to pay.  
         * Might have to employ the null object pattern.
         */
        if ( queueLock.tryLock()){
            try {
                while ( (t = queue.poll()) != null){
                    col.remove(t);
                }
            }finally{
                queueLock.unlock();
            }
        }
    }
    
    private static class CleanerTask implements Runnable {
        
        private final Collection col;
        private final ReferenceQueue queue;
        
        private CleanerTask(Collection c, ReferenceQueue queue){
            col = c;
            this.queue = queue;
        }
        
        @Override
        public void run() {
            Object t;
            while ( (t = queue.poll()) != null ){ 
                col.remove(t);
            }
        }
    
    }
    
    private static class FinalizerTask implements Runnable {
        
        private final ReferenceQueue phantomQueue;
        private final Map<Reference,ScheduledFuture> finalizerTasks ;
        
        private FinalizerTask(ReferenceQueue queue, 
                Map<Reference,ScheduledFuture> tasks){
            phantomQueue = queue;
            finalizerTasks = tasks;
        }

        @Override
        public void run() {
            Reference p;
            while ( (p = phantomQueue.poll()) != null){
                ScheduledFuture sf = finalizerTasks.remove(p);
                if (sf !=null) sf.cancel(true);
            }
        }
        
    }
    
}



From howard.lovatt at gmail.com  Thu Jan  5 23:21:36 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Fri, 6 Jan 2012 15:21:36 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEDMJCAA.davidcholmes@aapt.net.au>
References: <CACR_FB6rjJ339pLaZ1R+PjGCB88=EpHotuHUqQJz=WcP_2keAA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDMJCAA.davidcholmes@aapt.net.au>
Message-ID: <CACR_FB5vbXUTMXU3NdCzcSio3k3hY1K9Z+KyFW2t5WwxcYLrsA@mail.gmail.com>

I did recode with ReentrantLock and it works just fine.

 -- Howard.

On 6 January 2012 15:05, David Holmes <davidcholmes at aapt.net.au> wrote:

> **
> Yeah not much help without symbolic information. It's the two pool threads
> that are of interest.
>
> David
>
>  -----Original Message-----
> *From:* Howard Lovatt [mailto:howard.lovatt at gmail.com]
> *Sent:* Friday, 6 January 2012 1:43 PM
> *To:* dholmes at ieee.org
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Nested synchronized
>
> Hi,
>
> Finally got a stack trace, but from native tools so I am not sure how much
> use it is. When hung it consumes negligible CPU. Stack trace below.
>
>      2274 Thread_7277683: Java: pool-1-thread-1
>
>     + 2274 ???  (in <unknown binary>)  [0x105a66a78]
>
>     +   2274 ???  (in <unknown binary>)  [0x105a5fa48]
>
>     +     2274 JVM_IsInterrupted  (in libjvmlinkage.dylib) + 59
> [0x10574078b]
>
>     +       2274 JVM_IsInterrupted  (in libclient64.dylib) + 149
> [0x1051cf69d]
>
>     +         2274 ???  (in libclient64.dylib)  load address 0x105041000 +
> 0x7c198  [0x1050bd198]
>
>     +           2274 ???  (in libclient64.dylib)  load address 0x105041000
> + 0x1b5c4d  [0x1051f6c4d]
>
>     +             2274 ???  (in libclient64.dylib)  load address
> 0x105041000 + 0x12b0f  [0x105053b0f]
>
>     +               2274 ???  (in libclient64.dylib)  load address
> 0x105041000 + 0x12bcd  [0x105053bcd]
>
>     +                 2274 ???  (in libclient64.dylib)  load address
> 0x105041000 + 0x13d42  [0x105054d42]
>
>     +                   2274 ???  (in libclient64.dylib)  load address
> 0x105041000 + 0x13de3  [0x105054de3]
>
>     +                     2274 ???  (in libclient64.dylib)  load address
> 0x105041000 + 0x13f23  [0x105054f23]
>
>     +                       2274 mach_msg  (in libsystem_kernel.dylib) +
> 73  [0x7fff89906d71]
>
>     +                         2274 mach_msg_trap  (in
> libsystem_kernel.dylib) + 10  [0x7fff8990767a]
>
>     2274 Thread_7277684: Java: pool-1-thread-2
>
>     + 2274 ???  (in <unknown binary>)  [0x105a65287]
>
>     +   2274 ???  (in libclient64.dylib)  load address 0x105041000 +
> 0x1e3d25  [0x105224d25]
>
>     +     2274 ???  (in libclient64.dylib)  load address 0x105041000 +
> 0xd12cd  [0x1051122cd]
>
>     +       2274 ???  (in libclient64.dylib)  load address 0x105041000 +
> 0x14125  [0x105055125]
>
>     +         2274 mach_msg  (in libsystem_kernel.dylib) + 73
> [0x7fff89906d71]
>
>     +           2274 mach_msg_trap  (in libsystem_kernel.dylib) + 10
> [0x7fff8990767a]
>
>     2274 Thread_7277753   DispatchQueue_4: com.apple.root.low-priority
> (concurrent)
>
>       2274 start_wqthread  (in libsystem_c.dylib) + 13  [0x7fff88b16b85]
>
>         2274 _pthread_wqthread  (in libsystem_c.dylib) + 316
> [0x7fff88b153da]
>
>           2274 _dispatch_worker_thread2  (in libdispatch.dylib) + 198
> [0x7fff88c1f760]
>
>             2274 _dispatch_queue_invoke  (in libdispatch.dylib) + 71
> [0x7fff88c1ff77]
>
>               2274 _dispatch_source_invoke  (in libdispatch.dylib) + 635
> [0x7fff88c232b6]
>
>                 2274 _dispatch_after_timer_callback  (in
> libdispatch.dylib) + 16  [0x7fff88c20c07]
>
>                   2274 _dispatch_call_block_and_release  (in
> libdispatch.dylib) + 18  [0x7fff88c1e8ba]
>
>                     2274 ???  (in libjava.jnilib)  load address
> 0x1058a3000 + 0xda1c  [0x1058b0a1c]
>
>                       2274 JNFPerformEnvBlock  (in JavaNativeFoundation) +
> 27  [0x10589414f]
>
>                         2274 JNFObtainEnv  (in JavaNativeFoundation) +
> 171  [0x105893f5a]
>
>                           2274 ???  (in libclient64.dylib)  load address
> 0x105041000 + 0x1a0f41  [0x1051e1f41]
>
>                             2274 ???  (in libclient64.dylib)  load address
> 0x105041000 + 0xcf404  [0x105110404]
>
>                               2274 ???  (in libclient64.dylib)  load
> address 0x105041000 + 0x12b0f  [0x105053b0f]
>
>                                 2274 ???  (in libclient64.dylib)  load
> address 0x105041000 + 0x12bcd  [0x105053bcd]
>
>                                   2274 ???  (in libclient64.dylib)  load
> address 0x105041000 + 0x13d42  [0x105054d42]
>
>                                     2274 ???  (in libclient64.dylib)  load
> address 0x105041000 + 0x13de3  [0x105054de3]
>
>                                       2274 ???  (in libclient64.dylib)
> load address 0x105041000 + 0x13f23  [0x105054f23]
>
>                                         2274 mach_msg  (in
> libsystem_kernel.dylib) + 73  [0x7fff89906d71]
>
>                                           2274 mach_msg_trap  (in
> libsystem_kernel.dylib) + 10  [0x7fff8990767a]
>
>
>
>
>


-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/ce6c0ec6/attachment-0001.html>

From gergg at cox.net  Thu Jan  5 23:51:37 2012
From: gergg at cox.net (Gregg Wonderly)
Date: Thu, 5 Jan 2012 22:51:37 -0600
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB5vbXUTMXU3NdCzcSio3k3hY1K9Z+KyFW2t5WwxcYLrsA@mail.gmail.com>
References: <CACR_FB6rjJ339pLaZ1R+PjGCB88=EpHotuHUqQJz=WcP_2keAA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDMJCAA.davidcholmes@aapt.net.au>
	<CACR_FB5vbXUTMXU3NdCzcSio3k3hY1K9Z+KyFW2t5WwxcYLrsA@mail.gmail.com>
Message-ID: <6896E468-4B9B-4BF1-8656-27302AA14A9B@cox.net>

On my mac, it seems to complete successfully.  I've included a stack dump just FYI.

Gregg Wonderly

gwmac:~ gregg$ javac -d . Prob.java
gwmac:~ gregg$ java my.Prob
^\
2012-01-05 22:48:09
Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed mode):

"pool-1-thread-2" prio=5 tid=7ffd42969800 nid=0x10c132000 runnable [10c131000]
   java.lang.Thread.State: RUNNABLE
	at my.ParrallelSumMethod.conditionallySumArguments(Prob.java:87)
	- locked <7f44e1400> (a my.ParrallelSumMethod)
	at my.ParrallelSumMethod.isSetA2(Prob.java:80)
	- locked <7f44e1400> (a my.ParrallelSumMethod)
	at my.ParrallelSumMethod.setA2(Prob.java:57)
	at my.Prob$2.call(Prob.java:28)
	at my.Prob$2.call(Prob.java:26)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)

"pool-1-thread-1" prio=5 tid=7ffd4287f000 nid=0x10c02f000 runnable [10c02e000]
   java.lang.Thread.State: RUNNABLE
	at java.lang.Thread.isInterrupted(Native Method)
	at java.lang.Thread.interrupted(Thread.java:934)
	at my.ParrallelSumMethod.checkForInterrupt(Prob.java:65)
	at my.ParrallelSumMethod.setA1(Prob.java:51)
	at my.Prob$1.call(Prob.java:22)
	at my.Prob$1.call(Prob.java:20)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)

"Low Memory Detector" daemon prio=5 tid=7ffd4409f000 nid=0x10bc3d000 runnable [00000000]
   java.lang.Thread.State: RUNNABLE

"C2 CompilerThread1" daemon prio=9 tid=7ffd4409e800 nid=0x10bb3a000 waiting on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"C2 CompilerThread0" daemon prio=9 tid=7ffd4409d800 nid=0x10ba37000 waiting on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"Signal Dispatcher" daemon prio=9 tid=7ffd4409d000 nid=0x10b934000 waiting on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7ffd4409c000 nid=0x10b831000 waiting on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"Finalizer" daemon prio=8 tid=7ffd44095800 nid=0x10b54f000 in Object.wait() [10b54e000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
	- locked <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

"Reference Handler" daemon prio=10 tid=7ffd44094800 nid=0x10b44c000 in Object.wait() [10b44b000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7f44e1ff8> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:485)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
	- locked <7f44e1ff8> (a java.lang.ref.Reference$Lock)

"main" prio=5 tid=7ffd44000800 nid=0x104aee000 waiting on condition [104aed000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7f44e2070> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
	at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1253)
	at my.Prob.main(Prob.java:36)

"VM Thread" prio=9 tid=7ffd42802800 nid=0x10b349000 runnable 

"Gang worker#0 (Parallel GC Threads)" prio=9 tid=7ffd44002000 nid=0x107e27000 runnable 

"Gang worker#1 (Parallel GC Threads)" prio=9 tid=7ffd44002800 nid=0x107f2a000 runnable 

"Concurrent Mark-Sweep GC Thread" prio=9 tid=7ffd4404d000 nid=0x10afef000 runnable 
"VM Periodic Task Thread" prio=10 tid=7ffd440a1000 nid=0x10bd40000 waiting on condition 

"Exception Catcher Thread" prio=10 tid=7ffd44001800 nid=0x104c50000 runnable 
JNI global references: 914

Heap
 par new generation   total 19136K, used 5150K [7f3000000, 7f44c0000, 7f44c0000)
  eden space 17024K,  30% used [7f3000000, 7f35078b8, 7f40a0000)
  from space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
  to   space 2112K,   0% used [7f42b0000, 7f42b0000, 7f44c0000)
 concurrent mark-sweep generation total 63872K, used 379K [7f44c0000, 7f8320000, 7fae00000)
 concurrent-mark-sweep perm gen total 21248K, used 4811K [7fae00000, 7fc2c0000, 800000000)

99999990000000, terminated ok
gwmac:~ gregg$ java -version
java version "1.6.0_29"
Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)
gwmac:~ gregg$ 

From heinz at javaspecialists.eu  Fri Jan  6 01:46:56 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Fri, 6 Jan 2012 08:46:56 +0200
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <6896E468-4B9B-4BF1-8656-27302AA14A9B@cox.net>
References: <CACR_FB6rjJ339pLaZ1R+PjGCB88=EpHotuHUqQJz=WcP_2keAA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDMJCAA.davidcholmes@aapt.net.au>
	<CACR_FB5vbXUTMXU3NdCzcSio3k3hY1K9Z+KyFW2t5WwxcYLrsA@mail.gmail.com>
	<6896E468-4B9B-4BF1-8656-27302AA14A9B@cox.net>
Message-ID: <CACLL95p39Esgej1uQTz_-9g1Xc+Zffn1dFMtNfBrtF+sdX5fsA@mail.gmail.com>

We found a similar problem, but it only occurs with the latest i7
chipsets on the Mac.  We think it might be a Mac OS problem.  It would
be interesting to try booting with Linux (bootcamp?) and then to
verify that this is a problem with the JVM on Mac rather than the
hardware.  Could you try that, Howard?

Heinz

On 06/01/2012, Gregg Wonderly <gergg at cox.net> wrote:
> On my mac, it seems to complete successfully.  I've included a stack dump
> just FYI.
>
> Gregg Wonderly
>
> gwmac:~ gregg$ javac -d . Prob.java
> gwmac:~ gregg$ java my.Prob
> ^\
> 2012-01-05 22:48:09
> Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed
> mode):
>
> "pool-1-thread-2" prio=5 tid=7ffd42969800 nid=0x10c132000 runnable
> [10c131000]
>    java.lang.Thread.State: RUNNABLE
> 	at my.ParrallelSumMethod.conditionallySumArguments(Prob.java:87)
> 	- locked <7f44e1400> (a my.ParrallelSumMethod)
> 	at my.ParrallelSumMethod.isSetA2(Prob.java:80)
> 	- locked <7f44e1400> (a my.ParrallelSumMethod)
> 	at my.ParrallelSumMethod.setA2(Prob.java:57)
> 	at my.Prob$2.call(Prob.java:28)
> 	at my.Prob$2.call(Prob.java:26)
> 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
> 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
> 	at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> 	at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> 	at java.lang.Thread.run(Thread.java:680)
>
> "pool-1-thread-1" prio=5 tid=7ffd4287f000 nid=0x10c02f000 runnable
> [10c02e000]
>    java.lang.Thread.State: RUNNABLE
> 	at java.lang.Thread.isInterrupted(Native Method)
> 	at java.lang.Thread.interrupted(Thread.java:934)
> 	at my.ParrallelSumMethod.checkForInterrupt(Prob.java:65)
> 	at my.ParrallelSumMethod.setA1(Prob.java:51)
> 	at my.Prob$1.call(Prob.java:22)
> 	at my.Prob$1.call(Prob.java:20)
> 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
> 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
> 	at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> 	at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> 	at java.lang.Thread.run(Thread.java:680)
>
> "Low Memory Detector" daemon prio=5 tid=7ffd4409f000 nid=0x10bc3d000
> runnable [00000000]
>    java.lang.Thread.State: RUNNABLE
>
> "C2 CompilerThread1" daemon prio=9 tid=7ffd4409e800 nid=0x10bb3a000 waiting
> on condition [00000000]
>    java.lang.Thread.State: RUNNABLE
>
> "C2 CompilerThread0" daemon prio=9 tid=7ffd4409d800 nid=0x10ba37000 waiting
> on condition [00000000]
>    java.lang.Thread.State: RUNNABLE
>
> "Signal Dispatcher" daemon prio=9 tid=7ffd4409d000 nid=0x10b934000 waiting
> on condition [00000000]
>    java.lang.Thread.State: RUNNABLE
>
> "Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7ffd4409c000
> nid=0x10b831000 waiting on condition [00000000]
>    java.lang.Thread.State: RUNNABLE
>
> "Finalizer" daemon prio=8 tid=7ffd44095800 nid=0x10b54f000 in Object.wait()
> [10b54e000]
>    java.lang.Thread.State: WAITING (on object monitor)
> 	at java.lang.Object.wait(Native Method)
> 	- waiting on <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
> 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
> 	- locked <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
> 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
> 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
>
> "Reference Handler" daemon prio=10 tid=7ffd44094800 nid=0x10b44c000 in
> Object.wait() [10b44b000]
>    java.lang.Thread.State: WAITING (on object monitor)
> 	at java.lang.Object.wait(Native Method)
> 	- waiting on <7f44e1ff8> (a java.lang.ref.Reference$Lock)
> 	at java.lang.Object.wait(Object.java:485)
> 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
> 	- locked <7f44e1ff8> (a java.lang.ref.Reference$Lock)
>
> "main" prio=5 tid=7ffd44000800 nid=0x104aee000 waiting on condition
> [104aed000]
>    java.lang.Thread.State: TIMED_WAITING (parking)
> 	at sun.misc.Unsafe.park(Native Method)
> 	- parking to wait for  <7f44e2070> (a
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
> 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
> 	at
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
> 	at
> java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1253)
> 	at my.Prob.main(Prob.java:36)
>
> "VM Thread" prio=9 tid=7ffd42802800 nid=0x10b349000 runnable
>
> "Gang worker#0 (Parallel GC Threads)" prio=9 tid=7ffd44002000
> nid=0x107e27000 runnable
>
> "Gang worker#1 (Parallel GC Threads)" prio=9 tid=7ffd44002800
> nid=0x107f2a000 runnable
>
> "Concurrent Mark-Sweep GC Thread" prio=9 tid=7ffd4404d000 nid=0x10afef000
> runnable
> "VM Periodic Task Thread" prio=10 tid=7ffd440a1000 nid=0x10bd40000 waiting
> on condition
>
> "Exception Catcher Thread" prio=10 tid=7ffd44001800 nid=0x104c50000 runnable
> JNI global references: 914
>
> Heap
>  par new generation   total 19136K, used 5150K [7f3000000, 7f44c0000,
> 7f44c0000)
>   eden space 17024K,  30% used [7f3000000, 7f35078b8, 7f40a0000)
>   from space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
>   to   space 2112K,   0% used [7f42b0000, 7f42b0000, 7f44c0000)
>  concurrent mark-sweep generation total 63872K, used 379K [7f44c0000,
> 7f8320000, 7fae00000)
>  concurrent-mark-sweep perm gen total 21248K, used 4811K [7fae00000,
> 7fc2c0000, 800000000)
>
> 99999990000000, terminated ok
> gwmac:~ gregg$ java -version
> java version "1.6.0_29"
> Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
> Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)
> gwmac:~ gregg$
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz

From heinz at javaspecialists.eu  Fri Jan  6 01:48:58 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Fri, 6 Jan 2012 08:48:58 +0200
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACLL95p39Esgej1uQTz_-9g1Xc+Zffn1dFMtNfBrtF+sdX5fsA@mail.gmail.com>
References: <CACR_FB6rjJ339pLaZ1R+PjGCB88=EpHotuHUqQJz=WcP_2keAA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDMJCAA.davidcholmes@aapt.net.au>
	<CACR_FB5vbXUTMXU3NdCzcSio3k3hY1K9Z+KyFW2t5WwxcYLrsA@mail.gmail.com>
	<6896E468-4B9B-4BF1-8656-27302AA14A9B@cox.net>
	<CACLL95p39Esgej1uQTz_-9g1Xc+Zffn1dFMtNfBrtF+sdX5fsA@mail.gmail.com>
Message-ID: <CACLL95pa7wdAasqiXXyTVs_yHmfVV5biSJDeoGVNf5w6ANdMFQ@mail.gmail.com>

Oh just to add.  When this happens, the JVM freezes completely.  You
cannot get a stack dump or attach with any monitoring tools.  You can
only kill it with -9.

On 06/01/2012, Dr Heinz M. Kabutz <heinz at javaspecialists.eu> wrote:
> We found a similar problem, but it only occurs with the latest i7
> chipsets on the Mac.  We think it might be a Mac OS problem.  It would
> be interesting to try booting with Linux (bootcamp?) and then to
> verify that this is a problem with the JVM on Mac rather than the
> hardware.  Could you try that, Howard?
>
> Heinz
>
> On 06/01/2012, Gregg Wonderly <gergg at cox.net> wrote:
>> On my mac, it seems to complete successfully.  I've included a stack dump
>> just FYI.
>>
>> Gregg Wonderly
>>
>> gwmac:~ gregg$ javac -d . Prob.java
>> gwmac:~ gregg$ java my.Prob
>> ^\
>> 2012-01-05 22:48:09
>> Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed
>> mode):
>>
>> "pool-1-thread-2" prio=5 tid=7ffd42969800 nid=0x10c132000 runnable
>> [10c131000]
>>    java.lang.Thread.State: RUNNABLE
>> 	at my.ParrallelSumMethod.conditionallySumArguments(Prob.java:87)
>> 	- locked <7f44e1400> (a my.ParrallelSumMethod)
>> 	at my.ParrallelSumMethod.isSetA2(Prob.java:80)
>> 	- locked <7f44e1400> (a my.ParrallelSumMethod)
>> 	at my.ParrallelSumMethod.setA2(Prob.java:57)
>> 	at my.Prob$2.call(Prob.java:28)
>> 	at my.Prob$2.call(Prob.java:26)
>> 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>> 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>> 	at
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>> 	at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>> 	at java.lang.Thread.run(Thread.java:680)
>>
>> "pool-1-thread-1" prio=5 tid=7ffd4287f000 nid=0x10c02f000 runnable
>> [10c02e000]
>>    java.lang.Thread.State: RUNNABLE
>> 	at java.lang.Thread.isInterrupted(Native Method)
>> 	at java.lang.Thread.interrupted(Thread.java:934)
>> 	at my.ParrallelSumMethod.checkForInterrupt(Prob.java:65)
>> 	at my.ParrallelSumMethod.setA1(Prob.java:51)
>> 	at my.Prob$1.call(Prob.java:22)
>> 	at my.Prob$1.call(Prob.java:20)
>> 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>> 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>> 	at
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>> 	at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>> 	at java.lang.Thread.run(Thread.java:680)
>>
>> "Low Memory Detector" daemon prio=5 tid=7ffd4409f000 nid=0x10bc3d000
>> runnable [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "C2 CompilerThread1" daemon prio=9 tid=7ffd4409e800 nid=0x10bb3a000
>> waiting
>> on condition [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "C2 CompilerThread0" daemon prio=9 tid=7ffd4409d800 nid=0x10ba37000
>> waiting
>> on condition [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "Signal Dispatcher" daemon prio=9 tid=7ffd4409d000 nid=0x10b934000
>> waiting
>> on condition [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7ffd4409c000
>> nid=0x10b831000 waiting on condition [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "Finalizer" daemon prio=8 tid=7ffd44095800 nid=0x10b54f000 in
>> Object.wait()
>> [10b54e000]
>>    java.lang.Thread.State: WAITING (on object monitor)
>> 	at java.lang.Object.wait(Native Method)
>> 	- waiting on <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
>> 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
>> 	- locked <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
>> 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
>> 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
>>
>> "Reference Handler" daemon prio=10 tid=7ffd44094800 nid=0x10b44c000 in
>> Object.wait() [10b44b000]
>>    java.lang.Thread.State: WAITING (on object monitor)
>> 	at java.lang.Object.wait(Native Method)
>> 	- waiting on <7f44e1ff8> (a java.lang.ref.Reference$Lock)
>> 	at java.lang.Object.wait(Object.java:485)
>> 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
>> 	- locked <7f44e1ff8> (a java.lang.ref.Reference$Lock)
>>
>> "main" prio=5 tid=7ffd44000800 nid=0x104aee000 waiting on condition
>> [104aed000]
>>    java.lang.Thread.State: TIMED_WAITING (parking)
>> 	at sun.misc.Unsafe.park(Native Method)
>> 	- parking to wait for  <7f44e2070> (a
>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
>> 	at
>> java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
>> 	at
>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
>> 	at
>> java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1253)
>> 	at my.Prob.main(Prob.java:36)
>>
>> "VM Thread" prio=9 tid=7ffd42802800 nid=0x10b349000 runnable
>>
>> "Gang worker#0 (Parallel GC Threads)" prio=9 tid=7ffd44002000
>> nid=0x107e27000 runnable
>>
>> "Gang worker#1 (Parallel GC Threads)" prio=9 tid=7ffd44002800
>> nid=0x107f2a000 runnable
>>
>> "Concurrent Mark-Sweep GC Thread" prio=9 tid=7ffd4404d000 nid=0x10afef000
>> runnable
>> "VM Periodic Task Thread" prio=10 tid=7ffd440a1000 nid=0x10bd40000
>> waiting
>> on condition
>>
>> "Exception Catcher Thread" prio=10 tid=7ffd44001800 nid=0x104c50000
>> runnable
>> JNI global references: 914
>>
>> Heap
>>  par new generation   total 19136K, used 5150K [7f3000000, 7f44c0000,
>> 7f44c0000)
>>   eden space 17024K,  30% used [7f3000000, 7f35078b8, 7f40a0000)
>>   from space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
>>   to   space 2112K,   0% used [7f42b0000, 7f42b0000, 7f44c0000)
>>  concurrent mark-sweep generation total 63872K, used 379K [7f44c0000,
>> 7f8320000, 7fae00000)
>>  concurrent-mark-sweep perm gen total 21248K, used 4811K [7fae00000,
>> 7fc2c0000, 800000000)
>>
>> 99999990000000, terminated ok
>> gwmac:~ gregg$ java -version
>> java version "1.6.0_29"
>> Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
>> Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)
>> gwmac:~ gregg$
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professional
> http://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>


-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz

From nathan.reynolds at oracle.com  Fri Jan  6 11:34:16 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 06 Jan 2012 09:34:16 -0700
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB5e1kUWSzfEGgFMzCbjAVofg-z6rqxZuUf223iKoTaLag@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
	<CACR_FB5STPMUyO4F1=9S2wfrz_46jyZDW4Bp2H=nMGdgZVL3-Q@mail.gmail.com>
	<CAHjP37EETfOkUmv1gRwWSedVz=LM3Yze2Ucp94FGCLfFe0D4qw@mail.gmail.com>
	<CACR_FB5e1kUWSzfEGgFMzCbjAVofg-z6rqxZuUf223iKoTaLag@mail.gmail.com>
Message-ID: <4F072288.9030406@oracle.com>

Try connecting with JConsole before the hang and then dump call stacks 
after the hang via JConsole.

Also, please send the output of java -version so that all can match JVM 
versions.  If you could provide the download link, that would help.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/5/2012 7:05 PM, Howard Lovatt wrote:
> Hi Vitaly,
>
> jstack didn't work.
>
> gar-ln:~ lov080$ ps
>   PID TTY           TIME CMD
> 62924 ttys000    0:00.02 -bash
> 62955 ttys000    0:08.15 /usr/bin/java 
> nestedsynchronizedproblem.NestedSynchron
> 62967 ttys001    0:00.01 -bash
> gar-ln:~ lov080$ jstack -l -F 62955
> Attaching to process ID 62955, please wait...
> attach: task_for_pid(62955) failed (5)
> Error attaching to process: Error attaching to process, or no such process
>
> Good sugestion though.
>
> Thanks,
>
>  -- Howard.
>
> On 6 January 2012 12:44, Vitaly Davidovich <vitalyd at gmail.com 
> <mailto:vitalyd at gmail.com>> wrote:
>
>     a bit crude but if its somewhat easy to repro try increasing the
>     await period to something like 10 mins and then jstack it a few
>     times and see what it shows.
>
>     Also can you repro in interpreter (i.e. -Xint)?
>
>     On Jan 5, 2012 8:27 PM, "Howard Lovatt" <howard.lovatt at gmail.com
>     <mailto:howard.lovatt at gmail.com>> wrote:
>
>         Hi Vitaly,
>
>         If I increase the awaitTermination to 2 minutes I can run the
>         program in the debugger, however it works then! Is there
>         another away, without using the debugger, of generating a
>         stack trace for a hanging program?
>
>         Thanks,
>
>          -- Howard.
>
>         On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com
>         <mailto:vitalyd at gmail.com>> wrote:
>
>             Howard,
>
>             Have you captured call stacks when it hangs?
>
>             Vitaly
>
>             On Jan 5, 2012 8:14 PM, "Howard Lovatt"
>             <howard.lovatt at gmail.com <mailto:howard.lovatt at gmail.com>>
>             wrote:
>
>                 Hi David,
>
>                 There is only one sum object shared between the two
>                 threads (1st line of main) and hence all
>                 synchronization is on the same object. Therefore I
>                 think the code should work (even though the second
>                 synchronization is redundant). As a double check on my
>                 understanding I just added a specific mutex object to
>                 the code and synchronized on that and got the same result.
>
>                 Have I understood you comment correctly?
>
>                 Thanks,
>
>                  -- Howard.
>
>                 On 6 January 2012 12:00, David Holmes
>                 <davidcholmes at aapt.net.au
>                 <mailto:davidcholmes at aapt.net.au>> wrote:
>
>                     That's not nested synchronization as you are using
>                     two different objects. It is a classic deadlock:
>                     - sync method on Obj A calls sync method on Obj B
>                     - sync method on Obj B calls sync method on Objj A
>                     Thread 1 does the call to ObjA
>                     Thread 2 does the call to Obj B
>                     David
>                     ------
>
>                         -----Original Message-----
>                         *From:*
>                         concurrency-interest-bounces at cs.oswego.edu
>                         <mailto:concurrency-interest-bounces at cs.oswego.edu>
>                         [mailto:concurrency-interest-bounces at cs.oswego.edu
>                         <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On
>                         Behalf Of *Howard Lovatt
>                         *Sent:* Friday, 6 January 2012 10:53 AM
>                         *To:* concurrency-interest at cs.oswego.edu
>                         <mailto:concurrency-interest at cs.oswego.edu>
>                         *Subject:* [concurrency-interest] Nested
>                         synchronized
>
>                         Hi,
>
>                         I have seen something I think is a JVM bug but
>                         would like to check my understanding before
>                         reporting a problem. The following program
>                         normally hangs, i.e. the problem is
>                         intermittent, on my computer, MacBook Pro,
>                         Java 6 or 7, 4 core processor. The problem is
>                         that there are synchronized methods, isSetA1
>                         and isSetA2 (near end of listing below), that
>                         call another synchronized method,
>                         conditionallySumArguments (at end of listing
>                         below). The second synchronized
>                         is unnecessary since the method is always
>                         called within an already synchronized method
>                         and if the second synchronized is removed the
>                         program works as expected. However I think an
>                         extra synchronized should be redundant, not a
>                         problem?
>
>                             package nestedsynchronizedproblem;
>
>                             import java.util.concurrent.Callable;
>                             import java.util.concurrent.ExecutorService;
>                             import java.util.concurrent.Executors;
>                             import java.util.concurrent.TimeUnit;
>
>                             import static java.lang.System.*;
>
>                             /**
>                              * Test of nested synchronized. Mimics
>                             calling a parallel sum method.
>                              *
>                              * @author  Howard Lovatt
>                              */
>                             public class NestedSynchronizedProblem {
>                               private static final int loops = 10 *
>                             1000 * 1000; // This needs to be large for
>                             hanging!
>
>                               public static void main( final String...
>                             notUsed ) throws InterruptedException {
>                                 final ParrallelSumMethod sum = new
>                             ParrallelSumMethod();
>                                 final Callable<Void> setA1 = new
>                             Callable<Void>() {
>                                   @Override public Void call() throws
>                             Exception {
>                                     for ( int l = 0; l < loops; l++ )
>                             { sum.setA1( l ); }
>                                     return null;
>                                   }
>                                 };
>                                 final Callable<Void> setA2 = new
>                             Callable<Void>() {
>                                   @Override public Void call() throws
>                             Exception {
>                                     for ( int l = 0; l < loops; l++ )
>                             { sum.setA2( l ); }
>                                     return null;
>                                   }
>                                 };
>                                 final ExecutorService pool =
>                             Executors.newCachedThreadPool();
>                                 pool.submit( setA1 );
>                                 pool.submit( setA2 );
>                                 pool.shutdown();
>                                 final boolean ok =
>                             pool.awaitTermination( 1, TimeUnit.MINUTES );
>                                 out.println( sum.getSum() + (ok ? ",
>                             terminated ok" : ", failed to terminate") );
>                                 pool.shutdownNow();
>                               }
>                             }
>
>
>                             final class ParrallelSumMethod {
>                               private long sum = 0;
>                               private Long a1 = null;
>                               private Long a2 = null;
>
>                               public void setA1( final long a1Arg )
>                             throws InterruptedException {
>                                 for ( ;; ) {
>                                   if ( isSetA1( a1Arg ) ) { return; }
>                                   checkForInterrupt();
>                                 }
>                               }
>
>                               public void setA2( final long a2Arg )
>                             throws InterruptedException {
>                                 for ( ;; ) {
>                                   if ( isSetA2( a2Arg ) ) { return; }
>                                   checkForInterrupt();
>                                 }
>                               }
>
>                               public Long getSum() { return sum; }
>
>                               private static void checkForInterrupt()
>                             throws InterruptedException {
>                                 if ( Thread.interrupted() ) { throw
>                             new InterruptedException(); }
>                               }
>
>                               private synchronized boolean isSetA1(
>                             final long a1Arg ) {
>                                 if ( a1 == null ) {
>                                   a1 = a1Arg;
>                                   conditionallySumArguments();
>                                   return true;
>                                 }
>                                 return false;
>                               }
>
>                               private synchronized boolean isSetA2(
>                             final long a2Arg ) {
>                                 if ( a2 == null ) {
>                                   a2 = a2Arg;
>                                   conditionallySumArguments();
>                                   return true;
>                                 }
>                                 return false;
>                               }
>
>                               private synchronized void
>                             conditionallySumArguments() { // Works if
>                             not synchronized!!!
>                                 if ( ( a1 == null ) || ( a2 == null )
>                             ) { return; }
>                                 sum += a1 + a2;
>                                 a1 = a2 = null;
>                               }
>                             }
>
>
>                         Thanks in advance for any comments,
>
>                           -- Howard.
>
>
>
>
>                 -- 
>                   -- Howard.
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>         -- 
>           -- Howard.
>
>
>
>
> -- 
>   -- Howard.
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/17674435/attachment-0001.html>

From heinz at javaspecialists.eu  Fri Jan  6 12:17:07 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Fri, 6 Jan 2012 19:17:07 +0200
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <4F072288.9030406@oracle.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
	<CACR_FB5STPMUyO4F1=9S2wfrz_46jyZDW4Bp2H=nMGdgZVL3-Q@mail.gmail.com>
	<CAHjP37EETfOkUmv1gRwWSedVz=LM3Yze2Ucp94FGCLfFe0D4qw@mail.gmail.com>
	<CACR_FB5e1kUWSzfEGgFMzCbjAVofg-z6rqxZuUf223iKoTaLag@mail.gmail.com>
	<4F072288.9030406@oracle.com>
Message-ID: <CACLL95qd49NwKtQANrYn+RT7WFbtmCc1bLGdtZSEGGK8G_hGgw@mail.gmail.com>

Hi Nathan,

I would bet my bottom dollar that when the system hangs, it won't
respond to JConsole anymore either.  This is very similar to another
test that we wrote.

Incidentally, I discovered a condition that puts the JVM in an
unbreakable hard spin.  I reported this at least a year ago.  However,
it seems to still not be fixed.  I don't want to report it to publicly
as it would put any modern Java system at risk.  I think all JVM's
after 1.6.0_16 are affected.  It is easy to reproduce on Windows,
Linux and Mac. Anybody here with the clout to get this one fixed?
Please contact me directly.

Heinz

On 06/01/2012, Nathan Reynolds <nathan.reynolds at oracle.com> wrote:
> Try connecting with JConsole before the hang and then dump call stacks
> after the hang via JConsole.
>
> Also, please send the output of java -version so that all can match JVM
> versions.  If you could provide the download link, that would help.
>
> Nathan Reynolds
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
> Consulting Member of Technical Staff | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/5/2012 7:05 PM, Howard Lovatt wrote:
>> Hi Vitaly,
>>
>> jstack didn't work.
>>
>> gar-ln:~ lov080$ ps
>>   PID TTY           TIME CMD
>> 62924 ttys000    0:00.02 -bash
>> 62955 ttys000    0:08.15 /usr/bin/java
>> nestedsynchronizedproblem.NestedSynchron
>> 62967 ttys001    0:00.01 -bash
>> gar-ln:~ lov080$ jstack -l -F 62955
>> Attaching to process ID 62955, please wait...
>> attach: task_for_pid(62955) failed (5)
>> Error attaching to process: Error attaching to process, or no such process
>>
>> Good sugestion though.
>>
>> Thanks,
>>
>>  -- Howard.
>>
>> On 6 January 2012 12:44, Vitaly Davidovich <vitalyd at gmail.com
>> <mailto:vitalyd at gmail.com>> wrote:
>>
>>     a bit crude but if its somewhat easy to repro try increasing the
>>     await period to something like 10 mins and then jstack it a few
>>     times and see what it shows.
>>
>>     Also can you repro in interpreter (i.e. -Xint)?
>>
>>     On Jan 5, 2012 8:27 PM, "Howard Lovatt" <howard.lovatt at gmail.com
>>     <mailto:howard.lovatt at gmail.com>> wrote:
>>
>>         Hi Vitaly,
>>
>>         If I increase the awaitTermination to 2 minutes I can run the
>>         program in the debugger, however it works then! Is there
>>         another away, without using the debugger, of generating a
>>         stack trace for a hanging program?
>>
>>         Thanks,
>>
>>          -- Howard.
>>
>>         On 6 January 2012 12:18, Vitaly Davidovich <vitalyd at gmail.com
>>         <mailto:vitalyd at gmail.com>> wrote:
>>
>>             Howard,
>>
>>             Have you captured call stacks when it hangs?
>>
>>             Vitaly
>>
>>             On Jan 5, 2012 8:14 PM, "Howard Lovatt"
>>             <howard.lovatt at gmail.com <mailto:howard.lovatt at gmail.com>>
>>             wrote:
>>
>>                 Hi David,
>>
>>                 There is only one sum object shared between the two
>>                 threads (1st line of main) and hence all
>>                 synchronization is on the same object. Therefore I
>>                 think the code should work (even though the second
>>                 synchronization is redundant). As a double check on my
>>                 understanding I just added a specific mutex object to
>>                 the code and synchronized on that and got the same result.
>>
>>                 Have I understood you comment correctly?
>>
>>                 Thanks,
>>
>>                  -- Howard.
>>
>>                 On 6 January 2012 12:00, David Holmes
>>                 <davidcholmes at aapt.net.au
>>                 <mailto:davidcholmes at aapt.net.au>> wrote:
>>
>>                     That's not nested synchronization as you are using
>>                     two different objects. It is a classic deadlock:
>>                     - sync method on Obj A calls sync method on Obj B
>>                     - sync method on Obj B calls sync method on Objj A
>>                     Thread 1 does the call to ObjA
>>                     Thread 2 does the call to Obj B
>>                     David
>>                     ------
>>
>>                         -----Original Message-----
>>                         *From:*
>>                         concurrency-interest-bounces at cs.oswego.edu
>>
>> <mailto:concurrency-interest-bounces at cs.oswego.edu>
>>                         [mailto:concurrency-interest-bounces at cs.oswego.edu
>>
>> <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On
>>                         Behalf Of *Howard Lovatt
>>                         *Sent:* Friday, 6 January 2012 10:53 AM
>>                         *To:* concurrency-interest at cs.oswego.edu
>>                         <mailto:concurrency-interest at cs.oswego.edu>
>>                         *Subject:* [concurrency-interest] Nested
>>                         synchronized
>>
>>                         Hi,
>>
>>                         I have seen something I think is a JVM bug but
>>                         would like to check my understanding before
>>                         reporting a problem. The following program
>>                         normally hangs, i.e. the problem is
>>                         intermittent, on my computer, MacBook Pro,
>>                         Java 6 or 7, 4 core processor. The problem is
>>                         that there are synchronized methods, isSetA1
>>                         and isSetA2 (near end of listing below), that
>>                         call another synchronized method,
>>                         conditionallySumArguments (at end of listing
>>                         below). The second synchronized
>>                         is unnecessary since the method is always
>>                         called within an already synchronized method
>>                         and if the second synchronized is removed the
>>                         program works as expected. However I think an
>>                         extra synchronized should be redundant, not a
>>                         problem?
>>
>>                             package nestedsynchronizedproblem;
>>
>>                             import java.util.concurrent.Callable;
>>                             import java.util.concurrent.ExecutorService;
>>                             import java.util.concurrent.Executors;
>>                             import java.util.concurrent.TimeUnit;
>>
>>                             import static java.lang.System.*;
>>
>>                             /**
>>                              * Test of nested synchronized. Mimics
>>                             calling a parallel sum method.
>>                              *
>>                              * @author  Howard Lovatt
>>                              */
>>                             public class NestedSynchronizedProblem {
>>                               private static final int loops = 10 *
>>                             1000 * 1000; // This needs to be large for
>>                             hanging!
>>
>>                               public static void main( final String...
>>                             notUsed ) throws InterruptedException {
>>                                 final ParrallelSumMethod sum = new
>>                             ParrallelSumMethod();
>>                                 final Callable<Void> setA1 = new
>>                             Callable<Void>() {
>>                                   @Override public Void call() throws
>>                             Exception {
>>                                     for ( int l = 0; l < loops; l++ )
>>                             { sum.setA1( l ); }
>>                                     return null;
>>                                   }
>>                                 };
>>                                 final Callable<Void> setA2 = new
>>                             Callable<Void>() {
>>                                   @Override public Void call() throws
>>                             Exception {
>>                                     for ( int l = 0; l < loops; l++ )
>>                             { sum.setA2( l ); }
>>                                     return null;
>>                                   }
>>                                 };
>>                                 final ExecutorService pool =
>>                             Executors.newCachedThreadPool();
>>                                 pool.submit( setA1 );
>>                                 pool.submit( setA2 );
>>                                 pool.shutdown();
>>                                 final boolean ok =
>>                             pool.awaitTermination( 1, TimeUnit.MINUTES );
>>                                 out.println( sum.getSum() + (ok ? ",
>>                             terminated ok" : ", failed to terminate") );
>>                                 pool.shutdownNow();
>>                               }
>>                             }
>>
>>
>>                             final class ParrallelSumMethod {
>>                               private long sum = 0;
>>                               private Long a1 = null;
>>                               private Long a2 = null;
>>
>>                               public void setA1( final long a1Arg )
>>                             throws InterruptedException {
>>                                 for ( ;; ) {
>>                                   if ( isSetA1( a1Arg ) ) { return; }
>>                                   checkForInterrupt();
>>                                 }
>>                               }
>>
>>                               public void setA2( final long a2Arg )
>>                             throws InterruptedException {
>>                                 for ( ;; ) {
>>                                   if ( isSetA2( a2Arg ) ) { return; }
>>                                   checkForInterrupt();
>>                                 }
>>                               }
>>
>>                               public Long getSum() { return sum; }
>>
>>                               private static void checkForInterrupt()
>>                             throws InterruptedException {
>>                                 if ( Thread.interrupted() ) { throw
>>                             new InterruptedException(); }
>>                               }
>>
>>                               private synchronized boolean isSetA1(
>>                             final long a1Arg ) {
>>                                 if ( a1 == null ) {
>>                                   a1 = a1Arg;
>>                                   conditionallySumArguments();
>>                                   return true;
>>                                 }
>>                                 return false;
>>                               }
>>
>>                               private synchronized boolean isSetA2(
>>                             final long a2Arg ) {
>>                                 if ( a2 == null ) {
>>                                   a2 = a2Arg;
>>                                   conditionallySumArguments();
>>                                   return true;
>>                                 }
>>                                 return false;
>>                               }
>>
>>                               private synchronized void
>>                             conditionallySumArguments() { // Works if
>>                             not synchronized!!!
>>                                 if ( ( a1 == null ) || ( a2 == null )
>>                             ) { return; }
>>                                 sum += a1 + a2;
>>                                 a1 = a2 = null;
>>                               }
>>                             }
>>
>>
>>                         Thanks in advance for any comments,
>>
>>                           -- Howard.
>>
>>
>>
>>
>>                 --
>>                   -- Howard.
>>
>>
>>                 _______________________________________________
>>                 Concurrency-interest mailing list
>>                 Concurrency-interest at cs.oswego.edu
>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>         --
>>           -- Howard.
>>
>>
>>
>>
>> --
>>   -- Howard.
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz

From nathan.reynolds at oracle.com  Fri Jan  6 12:39:08 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 06 Jan 2012 10:39:08 -0700
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACLL95qd49NwKtQANrYn+RT7WFbtmCc1bLGdtZSEGGK8G_hGgw@mail.gmail.com>
References: <CACR_FB5AY+PV8eL7c10gk7Hh9ggMvt22hKDvgFc3BGFq03EmuA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDHJCAA.davidcholmes@aapt.net.au>
	<CACR_FB4gAzhxK_jUqO0SwHUft2DdbzPX=a4rdznK_auisdeO5A@mail.gmail.com>
	<CAHjP37E+W2ZYS1K8pu1COasPWXz6jDxAgZnGNVQPtEQwx2RqeA@mail.gmail.com>
	<CACR_FB5STPMUyO4F1=9S2wfrz_46jyZDW4Bp2H=nMGdgZVL3-Q@mail.gmail.com>
	<CAHjP37EETfOkUmv1gRwWSedVz=LM3Yze2Ucp94FGCLfFe0D4qw@mail.gmail.com>
	<CACR_FB5e1kUWSzfEGgFMzCbjAVofg-z6rqxZuUf223iKoTaLag@mail.gmail.com>
	<4F072288.9030406@oracle.com>
	<CACLL95qd49NwKtQANrYn+RT7WFbtmCc1bLGdtZSEGGK8G_hGgw@mail.gmail.com>
Message-ID: <4F0731BC.6050908@oracle.com>

 > when the system hangs, it won't respond to JConsole anymore either.

Agreed.  I thought it was worth trying.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/6/2012 10:17 AM, Dr Heinz M. Kabutz wrote:
> Hi Nathan,
>
> I would bet my bottom dollar that when the system hangs, it won't
> respond to JConsole anymore either.  This is very similar to another
> test that we wrote.
>
> Incidentally, I discovered a condition that puts the JVM in an
> unbreakable hard spin.  I reported this at least a year ago.  However,
> it seems to still not be fixed.  I don't want to report it to publicly
> as it would put any modern Java system at risk.  I think all JVM's
> after 1.6.0_16 are affected.  It is easy to reproduce on Windows,
> Linux and Mac. Anybody here with the clout to get this one fixed?
> Please contact me directly.
>
> Heinz
>
> On 06/01/2012, Nathan Reynolds<nathan.reynolds at oracle.com>  wrote:
>> Try connecting with JConsole before the hang and then dump call stacks
>> after the hang via JConsole.
>>
>> Also, please send the output of java -version so that all can match JVM
>> versions.  If you could provide the download link, that would help.
>>
>> Nathan Reynolds
>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>  |
>> Consulting Member of Technical Staff | 602.333.9091
>> Oracle PSR Engineering<http://psr.us.oracle.com/>  | Server Technology
>>
>> On 1/5/2012 7:05 PM, Howard Lovatt wrote:
>>> Hi Vitaly,
>>>
>>> jstack didn't work.
>>>
>>> gar-ln:~ lov080$ ps
>>>    PID TTY           TIME CMD
>>> 62924 ttys000    0:00.02 -bash
>>> 62955 ttys000    0:08.15 /usr/bin/java
>>> nestedsynchronizedproblem.NestedSynchron
>>> 62967 ttys001    0:00.01 -bash
>>> gar-ln:~ lov080$ jstack -l -F 62955
>>> Attaching to process ID 62955, please wait...
>>> attach: task_for_pid(62955) failed (5)
>>> Error attaching to process: Error attaching to process, or no such process
>>>
>>> Good sugestion though.
>>>
>>> Thanks,
>>>
>>>   -- Howard.
>>>
>>> On 6 January 2012 12:44, Vitaly Davidovich<vitalyd at gmail.com
>>> <mailto:vitalyd at gmail.com>>  wrote:
>>>
>>>      a bit crude but if its somewhat easy to repro try increasing the
>>>      await period to something like 10 mins and then jstack it a few
>>>      times and see what it shows.
>>>
>>>      Also can you repro in interpreter (i.e. -Xint)?
>>>
>>>      On Jan 5, 2012 8:27 PM, "Howard Lovatt"<howard.lovatt at gmail.com
>>>      <mailto:howard.lovatt at gmail.com>>  wrote:
>>>
>>>          Hi Vitaly,
>>>
>>>          If I increase the awaitTermination to 2 minutes I can run the
>>>          program in the debugger, however it works then! Is there
>>>          another away, without using the debugger, of generating a
>>>          stack trace for a hanging program?
>>>
>>>          Thanks,
>>>
>>>           -- Howard.
>>>
>>>          On 6 January 2012 12:18, Vitaly Davidovich<vitalyd at gmail.com
>>>          <mailto:vitalyd at gmail.com>>  wrote:
>>>
>>>              Howard,
>>>
>>>              Have you captured call stacks when it hangs?
>>>
>>>              Vitaly
>>>
>>>              On Jan 5, 2012 8:14 PM, "Howard Lovatt"
>>>              <howard.lovatt at gmail.com<mailto:howard.lovatt at gmail.com>>
>>>              wrote:
>>>
>>>                  Hi David,
>>>
>>>                  There is only one sum object shared between the two
>>>                  threads (1st line of main) and hence all
>>>                  synchronization is on the same object. Therefore I
>>>                  think the code should work (even though the second
>>>                  synchronization is redundant). As a double check on my
>>>                  understanding I just added a specific mutex object to
>>>                  the code and synchronized on that and got the same result.
>>>
>>>                  Have I understood you comment correctly?
>>>
>>>                  Thanks,
>>>
>>>                   -- Howard.
>>>
>>>                  On 6 January 2012 12:00, David Holmes
>>>                  <davidcholmes at aapt.net.au
>>>                  <mailto:davidcholmes at aapt.net.au>>  wrote:
>>>
>>>                      That's not nested synchronization as you are using
>>>                      two different objects. It is a classic deadlock:
>>>                      - sync method on Obj A calls sync method on Obj B
>>>                      - sync method on Obj B calls sync method on Objj A
>>>                      Thread 1 does the call to ObjA
>>>                      Thread 2 does the call to Obj B
>>>                      David
>>>                      ------
>>>
>>>                          -----Original Message-----
>>>                          *From:*
>>>                          concurrency-interest-bounces at cs.oswego.edu
>>>
>>> <mailto:concurrency-interest-bounces at cs.oswego.edu>
>>>                          [mailto:concurrency-interest-bounces at cs.oswego.edu
>>>
>>> <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On
>>>                          Behalf Of *Howard Lovatt
>>>                          *Sent:* Friday, 6 January 2012 10:53 AM
>>>                          *To:* concurrency-interest at cs.oswego.edu
>>>                          <mailto:concurrency-interest at cs.oswego.edu>
>>>                          *Subject:* [concurrency-interest] Nested
>>>                          synchronized
>>>
>>>                          Hi,
>>>
>>>                          I have seen something I think is a JVM bug but
>>>                          would like to check my understanding before
>>>                          reporting a problem. The following program
>>>                          normally hangs, i.e. the problem is
>>>                          intermittent, on my computer, MacBook Pro,
>>>                          Java 6 or 7, 4 core processor. The problem is
>>>                          that there are synchronized methods, isSetA1
>>>                          and isSetA2 (near end of listing below), that
>>>                          call another synchronized method,
>>>                          conditionallySumArguments (at end of listing
>>>                          below). The second synchronized
>>>                          is unnecessary since the method is always
>>>                          called within an already synchronized method
>>>                          and if the second synchronized is removed the
>>>                          program works as expected. However I think an
>>>                          extra synchronized should be redundant, not a
>>>                          problem?
>>>
>>>                              package nestedsynchronizedproblem;
>>>
>>>                              import java.util.concurrent.Callable;
>>>                              import java.util.concurrent.ExecutorService;
>>>                              import java.util.concurrent.Executors;
>>>                              import java.util.concurrent.TimeUnit;
>>>
>>>                              import static java.lang.System.*;
>>>
>>>                              /**
>>>                               * Test of nested synchronized. Mimics
>>>                              calling a parallel sum method.
>>>                               *
>>>                               * @author  Howard Lovatt
>>>                               */
>>>                              public class NestedSynchronizedProblem {
>>>                                private static final int loops = 10 *
>>>                              1000 * 1000; // This needs to be large for
>>>                              hanging!
>>>
>>>                                public static void main( final String...
>>>                              notUsed ) throws InterruptedException {
>>>                                  final ParrallelSumMethod sum = new
>>>                              ParrallelSumMethod();
>>>                                  final Callable<Void>  setA1 = new
>>>                              Callable<Void>() {
>>>                                    @Override public Void call() throws
>>>                              Exception {
>>>                                      for ( int l = 0; l<  loops; l++ )
>>>                              { sum.setA1( l ); }
>>>                                      return null;
>>>                                    }
>>>                                  };
>>>                                  final Callable<Void>  setA2 = new
>>>                              Callable<Void>() {
>>>                                    @Override public Void call() throws
>>>                              Exception {
>>>                                      for ( int l = 0; l<  loops; l++ )
>>>                              { sum.setA2( l ); }
>>>                                      return null;
>>>                                    }
>>>                                  };
>>>                                  final ExecutorService pool =
>>>                              Executors.newCachedThreadPool();
>>>                                  pool.submit( setA1 );
>>>                                  pool.submit( setA2 );
>>>                                  pool.shutdown();
>>>                                  final boolean ok =
>>>                              pool.awaitTermination( 1, TimeUnit.MINUTES );
>>>                                  out.println( sum.getSum() + (ok ? ",
>>>                              terminated ok" : ", failed to terminate") );
>>>                                  pool.shutdownNow();
>>>                                }
>>>                              }
>>>
>>>
>>>                              final class ParrallelSumMethod {
>>>                                private long sum = 0;
>>>                                private Long a1 = null;
>>>                                private Long a2 = null;
>>>
>>>                                public void setA1( final long a1Arg )
>>>                              throws InterruptedException {
>>>                                  for ( ;; ) {
>>>                                    if ( isSetA1( a1Arg ) ) { return; }
>>>                                    checkForInterrupt();
>>>                                  }
>>>                                }
>>>
>>>                                public void setA2( final long a2Arg )
>>>                              throws InterruptedException {
>>>                                  for ( ;; ) {
>>>                                    if ( isSetA2( a2Arg ) ) { return; }
>>>                                    checkForInterrupt();
>>>                                  }
>>>                                }
>>>
>>>                                public Long getSum() { return sum; }
>>>
>>>                                private static void checkForInterrupt()
>>>                              throws InterruptedException {
>>>                                  if ( Thread.interrupted() ) { throw
>>>                              new InterruptedException(); }
>>>                                }
>>>
>>>                                private synchronized boolean isSetA1(
>>>                              final long a1Arg ) {
>>>                                  if ( a1 == null ) {
>>>                                    a1 = a1Arg;
>>>                                    conditionallySumArguments();
>>>                                    return true;
>>>                                  }
>>>                                  return false;
>>>                                }
>>>
>>>                                private synchronized boolean isSetA2(
>>>                              final long a2Arg ) {
>>>                                  if ( a2 == null ) {
>>>                                    a2 = a2Arg;
>>>                                    conditionallySumArguments();
>>>                                    return true;
>>>                                  }
>>>                                  return false;
>>>                                }
>>>
>>>                                private synchronized void
>>>                              conditionallySumArguments() { // Works if
>>>                              not synchronized!!!
>>>                                  if ( ( a1 == null ) || ( a2 == null )
>>>                              ) { return; }
>>>                                  sum += a1 + a2;
>>>                                  a1 = a2 = null;
>>>                                }
>>>                              }
>>>
>>>
>>>                          Thanks in advance for any comments,
>>>
>>>                            -- Howard.
>>>
>>>
>>>
>>>
>>>                  --
>>>                    -- Howard.
>>>
>>>
>>>                  _______________________________________________
>>>                  Concurrency-interest mailing list
>>>                  Concurrency-interest at cs.oswego.edu
>>>                  <mailto:Concurrency-interest at cs.oswego.edu>
>>>                  http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>          --
>>>            -- Howard.
>>>
>>>
>>>
>>>
>>> --
>>>    -- Howard.
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120106/9f23d2e8/attachment-0001.html>

From howard.lovatt at gmail.com  Fri Jan  6 21:49:03 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Sat, 7 Jan 2012 13:49:03 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <6896E468-4B9B-4BF1-8656-27302AA14A9B@cox.net>
References: <CACR_FB6rjJ339pLaZ1R+PjGCB88=EpHotuHUqQJz=WcP_2keAA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDMJCAA.davidcholmes@aapt.net.au>
	<CACR_FB5vbXUTMXU3NdCzcSio3k3hY1K9Z+KyFW2t5WwxcYLrsA@mail.gmail.com>
	<6896E468-4B9B-4BF1-8656-27302AA14A9B@cox.net>
Message-ID: <CACR_FB671C+T8BmLysvUMHPTnQAikXB=0rnQ3mz714tYmJsu_w@mail.gmail.com>

Hi Gregg,

Weird I have the same Java version, but for me it starts working (high CPU
usage) and then hangs (negligible CPU usage). Whilst it is running I can
get a stack trace, see below. But once it has hung I can't get a stack
trace and I have to kill it.

Could it be a hardware variation; I have a 4-core i7 processor?

  Model Name: MacBook Pro

  Model Identifier: MacBookPro8,2

  Processor Name: Intel Core i7

  Processor Speed: 2.3 GHz

  Number of Processors: 1

  Total Number of Cores: 4

  L2 Cache (per Core): 256 KB

  L3 Cache: 8 MB

Could it be a software variation; I have Lion?

  System Version: Mac OS X 10.7.2 (11C74)

  Kernel Version: Darwin 11.2.0

  Boot Volume: sunzero-ln

  Boot Mode: Normal

  Computer Name: sunzero-ln

  User Name: Howard Lovatt (lov080)

  Secure Virtual Memory: Enabled

  64-bit Kernel and Extensions: Yes

  Time since boot: 29 days 4:35


Thanks for running the code.

 -- Howard

===============================================

gar-ln:src lov080$ java nestedsynchronizedproblem.NestedSynchronizedProblem
^\2012-01-06 17:17:34
Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed
mode):

"pool-1-thread-2" prio=5 tid=7fb8fb291800 nid=0x113dfd000 runnable
[113dfc000]
   java.lang.Thread.State: RUNNABLE
at java.lang.Thread.isInterrupted(Native Method)
 at java.lang.Thread.interrupted(Thread.java:934)
at
nestedsynchronizedproblem.ParrallelSumMethod.checkForInterrupt(NestedSynchronizedProblem.java:68)
 at
nestedsynchronizedproblem.ParrallelSumMethod.setA2(NestedSynchronizedProblem.java:61)
at
nestedsynchronizedproblem.NestedSynchronizedProblem$2.call(NestedSynchronizedProblem.java:31)
 at
nestedsynchronizedproblem.NestedSynchronizedProblem$2.call(NestedSynchronizedProblem.java:29)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
 at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
 at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:680)

"pool-1-thread-1" prio=5 tid=7fb8fb290800 nid=0x113cfa000 waiting for
monitor entry [113cf9000]
   java.lang.Thread.State: BLOCKED (on object monitor)
at
nestedsynchronizedproblem.ParrallelSumMethod.isSetA1(NestedSynchronizedProblem.java:72)
 - locked <7f42b20a0> (a nestedsynchronizedproblem.ParrallelSumMethod)
at
nestedsynchronizedproblem.ParrallelSumMethod.setA1(NestedSynchronizedProblem.java:53)
 at
nestedsynchronizedproblem.NestedSynchronizedProblem$1.call(NestedSynchronizedProblem.java:25)
at
nestedsynchronizedproblem.NestedSynchronizedProblem$1.call(NestedSynchronizedProblem.java:23)
 at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
 at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
 at java.lang.Thread.run(Thread.java:680)

"Low Memory Detector" daemon prio=5 tid=7fb8fb15a000 nid=0x11395d000
runnable [00000000]
   java.lang.Thread.State: RUNNABLE

"C2 CompilerThread1" daemon prio=9 tid=7fb8fb159800 nid=0x11385a000 waiting
on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"C2 CompilerThread0" daemon prio=9 tid=7fb8fb158800 nid=0x113757000 waiting
on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"Signal Dispatcher" daemon prio=9 tid=7fb8fb158000 nid=0x113654000 waiting
on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7fb8fb157000
nid=0x113551000 waiting on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"Finalizer" daemon prio=8 tid=7fb8fb13f000 nid=0x11328d000 in Object.wait()
[11328c000]
   java.lang.Thread.State: WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
 - waiting on <7f42b42a0> (a java.lang.ref.ReferenceQueue$Lock)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
 - locked <7f42b42a0> (a java.lang.ref.ReferenceQueue$Lock)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
 at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

"Reference Handler" daemon prio=10 tid=7fb8fb13e800 nid=0x11318a000 in
Object.wait() [113189000]
   java.lang.Thread.State: WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
- waiting on <7f42b0100> (a java.lang.ref.Reference$Lock)
 at java.lang.Object.wait(Object.java:485)
at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
 - locked <7f42b0100> (a java.lang.ref.Reference$Lock)

"main" prio=5 tid=7fb8fb001000 nid=0x10b414000 waiting on condition
[10b413000]
   java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <7f42bc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
 at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
 at
java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1253)
at
nestedsynchronizedproblem.NestedSynchronizedProblem.main(NestedSynchronizedProblem.java:39)

"VM Thread" prio=9 tid=7fb8fb13a000 nid=0x113087000 runnable

"Gang worker#0 (Parallel GC Threads)" prio=9 tid=7fb8fb002800
nid=0x10e74d000 runnable

"Gang worker#1 (Parallel GC Threads)" prio=9 tid=7fb8fb003000
nid=0x10e850000 runnable

"Gang worker#2 (Parallel GC Threads)" prio=9 tid=7fb8fb003800
nid=0x10e953000 runnable

"Gang worker#3 (Parallel GC Threads)" prio=9 tid=7fb8fb004000
nid=0x10ea56000 runnable

"Gang worker#4 (Parallel GC Threads)" prio=9 tid=7fb8fb005000
nid=0x10eb59000 runnable

"Gang worker#5 (Parallel GC Threads)" prio=9 tid=7fb8fb005800
nid=0x10ec5c000 runnable

"Gang worker#6 (Parallel GC Threads)" prio=9 tid=7fb8fb006000
nid=0x10ed5f000 runnable

"Gang worker#7 (Parallel GC Threads)" prio=9 tid=7fb8fb006800
nid=0x10ee62000 runnable

"Concurrent Mark-Sweep GC Thread" prio=9 tid=7fb8fb0e4000 nid=0x112d2d000
runnable
"Gang worker#0 (Parallel CMS Threads)" prio=9 tid=7fb8fb0e3000
nid=0x112327000 runnable

"Gang worker#1 (Parallel CMS Threads)" prio=9 tid=7fb8fb0e3800
nid=0x11242a000 runnable

"VM Periodic Task Thread" prio=10 tid=7fb8fb16b800 nid=0x113a60000 waiting
on condition

"Exception Catcher Thread" prio=10 tid=7fb8fb001800 nid=0x10b576000
runnable
JNI global references: 906

Heap
 par new generation   total 19136K, used 1211K [7f3000000, 7f44c0000,
7f44c0000)
  eden space 17024K,   4% used [7f3000000, 7f30d16f8, 7f40a0000)
  from space 2112K,  17% used [7f42b0000, 7f430d720, 7f44c0000)
  to   space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
 concurrent mark-sweep generation total 63872K, used 0K [7f44c0000,
7f8320000, 7fae00000)
 concurrent-mark-sweep perm gen total 21248K, used 4769K [7fae00000,
7fc2c0000, 800000000)

Killed: 9


On 6 January 2012 15:51, Gregg Wonderly <gergg at cox.net> wrote:

> On my mac, it seems to complete successfully.  I've included a stack dump
> just FYI.
>
> Gregg Wonderly
>
> gwmac:~ gregg$ javac -d . Prob.java
> gwmac:~ gregg$ java my.Prob
> ^\
> 2012-01-05 22:48:09
> Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed
> mode):
>
> "pool-1-thread-2" prio=5 tid=7ffd42969800 nid=0x10c132000 runnable
> [10c131000]
>   java.lang.Thread.State: RUNNABLE
>        at my.ParrallelSumMethod.conditionallySumArguments(Prob.java:87)
>        - locked <7f44e1400> (a my.ParrallelSumMethod)
>        at my.ParrallelSumMethod.isSetA2(Prob.java:80)
>        - locked <7f44e1400> (a my.ParrallelSumMethod)
>        at my.ParrallelSumMethod.setA2(Prob.java:57)
>        at my.Prob$2.call(Prob.java:28)
>        at my.Prob$2.call(Prob.java:26)
>        at
> java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>        at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>        at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>        at java.lang.Thread.run(Thread.java:680)
>
> "pool-1-thread-1" prio=5 tid=7ffd4287f000 nid=0x10c02f000 runnable
> [10c02e000]
>   java.lang.Thread.State: RUNNABLE
>        at java.lang.Thread.isInterrupted(Native Method)
>        at java.lang.Thread.interrupted(Thread.java:934)
>        at my.ParrallelSumMethod.checkForInterrupt(Prob.java:65)
>        at my.ParrallelSumMethod.setA1(Prob.java:51)
>        at my.Prob$1.call(Prob.java:22)
>        at my.Prob$1.call(Prob.java:20)
>        at
> java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>        at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>        at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>        at java.lang.Thread.run(Thread.java:680)
>
> "Low Memory Detector" daemon prio=5 tid=7ffd4409f000 nid=0x10bc3d000
> runnable [00000000]
>   java.lang.Thread.State: RUNNABLE
>
> "C2 CompilerThread1" daemon prio=9 tid=7ffd4409e800 nid=0x10bb3a000
> waiting on condition [00000000]
>   java.lang.Thread.State: RUNNABLE
>
> "C2 CompilerThread0" daemon prio=9 tid=7ffd4409d800 nid=0x10ba37000
> waiting on condition [00000000]
>   java.lang.Thread.State: RUNNABLE
>
> "Signal Dispatcher" daemon prio=9 tid=7ffd4409d000 nid=0x10b934000 waiting
> on condition [00000000]
>   java.lang.Thread.State: RUNNABLE
>
> "Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7ffd4409c000
> nid=0x10b831000 waiting on condition [00000000]
>   java.lang.Thread.State: RUNNABLE
>
> "Finalizer" daemon prio=8 tid=7ffd44095800 nid=0x10b54f000 in
> Object.wait() [10b54e000]
>   java.lang.Thread.State: WAITING (on object monitor)
>        at java.lang.Object.wait(Native Method)
>        - waiting on <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
>        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
>        - locked <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
>        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
>        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
>
> "Reference Handler" daemon prio=10 tid=7ffd44094800 nid=0x10b44c000 in
> Object.wait() [10b44b000]
>   java.lang.Thread.State: WAITING (on object monitor)
>        at java.lang.Object.wait(Native Method)
>        - waiting on <7f44e1ff8> (a java.lang.ref.Reference$Lock)
>        at java.lang.Object.wait(Object.java:485)
>        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
>        - locked <7f44e1ff8> (a java.lang.ref.Reference$Lock)
>
> "main" prio=5 tid=7ffd44000800 nid=0x104aee000 waiting on condition
> [104aed000]
>   java.lang.Thread.State: TIMED_WAITING (parking)
>        at sun.misc.Unsafe.park(Native Method)
>        - parking to wait for  <7f44e2070> (a
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
>        at
> java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
>        at
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
>        at
> java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1253)
>        at my.Prob.main(Prob.java:36)
>
> "VM Thread" prio=9 tid=7ffd42802800 nid=0x10b349000 runnable
>
> "Gang worker#0 (Parallel GC Threads)" prio=9 tid=7ffd44002000
> nid=0x107e27000 runnable
>
> "Gang worker#1 (Parallel GC Threads)" prio=9 tid=7ffd44002800
> nid=0x107f2a000 runnable
>
> "Concurrent Mark-Sweep GC Thread" prio=9 tid=7ffd4404d000 nid=0x10afef000
> runnable
> "VM Periodic Task Thread" prio=10 tid=7ffd440a1000 nid=0x10bd40000 waiting
> on condition
>
> "Exception Catcher Thread" prio=10 tid=7ffd44001800 nid=0x104c50000
> runnable
> JNI global references: 914
>
> Heap
>  par new generation   total 19136K, used 5150K [7f3000000, 7f44c0000,
> 7f44c0000)
>  eden space 17024K,  30% used [7f3000000, 7f35078b8, 7f40a0000)
>  from space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
>  to   space 2112K,   0% used [7f42b0000, 7f42b0000, 7f44c0000)
>  concurrent mark-sweep generation total 63872K, used 379K [7f44c0000,
> 7f8320000, 7fae00000)
>  concurrent-mark-sweep perm gen total 21248K, used 4811K [7fae00000,
> 7fc2c0000, 800000000)
>
> 99999990000000, terminated ok
> gwmac:~ gregg$ java -version
> java version "1.6.0_29"
> Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
> Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)
> gwmac:~ gregg$




-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120107/c184e73f/attachment-0001.html>

From heinz at javaspecialists.eu  Sat Jan  7 04:44:26 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Sat, 07 Jan 2012 11:44:26 +0200
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB671C+T8BmLysvUMHPTnQAikXB=0rnQ3mz714tYmJsu_w@mail.gmail.com>
References: <CACR_FB6rjJ339pLaZ1R+PjGCB88=EpHotuHUqQJz=WcP_2keAA@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCGEDMJCAA.davidcholmes@aapt.net.au>	<CACR_FB5vbXUTMXU3NdCzcSio3k3hY1K9Z+KyFW2t5WwxcYLrsA@mail.gmail.com>	<6896E468-4B9B-4BF1-8656-27302AA14A9B@cox.net>
	<CACR_FB671C+T8BmLysvUMHPTnQAikXB=0rnQ3mz714tYmJsu_w@mail.gmail.com>
Message-ID: <4F0813FA.2060903@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120107/4e109f65/attachment-0001.html>

From stanimir at riflexo.com  Sat Jan  7 06:19:47 2012
From: stanimir at riflexo.com (bestsss)
Date: Sat, 7 Jan 2012 03:19:47 -0800 (PST)
Subject: [concurrency-interest] Re ference Collections
In-Reply-To: <1325822995.10496.167.camel@bluto>
References: <1325678569.21868.167.camel@bluto>
	<1325822995.10496.167.camel@bluto>
Message-ID: <33097991.post@talk.nabble.com>




Peter Firmstone-3 wrote:
> 
> Ok, based on the comments so far:
> 
> Reasoning about the possibility that one thread may not be able to keep
> up with a ReferenceQueue, bestsss mentioned that ReferenceQueue is a
> stack, not a queue, because it blocks while the garbage collector thread
> holds it's lock, the cleaning thread won't be able to poll(), but I
> suspect the jvm only does this a scheduled intervals.
> 
It's not a GC thread but the dedicated ReferenceHandler which gets the
enqueued references and adds them to the appropriate queues (stacks), also
executes sun.misc.Cleaner directly. It employs wait/notify and notify() is
called by the JVM itself.


Peter Firmstone-3 wrote:
> 
>     private final static ScheduledExecutorService garbageCleaner =
>             Executors.newScheduledThreadPool(1);
>     // Map to register newly created object references.
>     private final static Map<Reference,ScheduledFuture> finalizerTasks =
>             new ConcurrentHashMap<Reference,ScheduledFuture>();
>     // Finalizer queue to advise cancellation of ScheduledFuture's, 
>     // when their ReferenceProcessor has been collected.
>     private final static ReferenceQueue<Reference> phantomQueue = 
>             new ReferenceQueue<Reference>();
>     static {
>         // Finizer Task to cancel unneeded tasks.
>         garbageCleaner.scheduleAtFixedRate(
>                 new FinalizerTask(phantomQueue, finalizerTasks), 
>                 5L, 5L, TimeUnit.MINUTES
>                 );
>     }
>     
> 

I'd encourage you to pay extra attention at creating the "static" threads,
they leak resources badly - mostly thread group, context ClassLoader and
java.security.AccessControlContext (that contains reference to a classloader
too) and it has caused me truly a lot of grief. The problem occurs in
managed environments where redeploys are normal practice in long (months++)
running processes. The spawned thread will inherit and keep forever the
context class loader of the calling thread. It's absolutely random which
thread will initialize the class first. Use thread a ThreadFactory and set
the conext ClassLoader to ReferenceProcessor.class.getClassLoader() (or just
null) and try to put in thread in the "system" (or main) thread group if
there is no System.getSecurityManager(). To prevent the leaking of ACL can
do smth like AccessController.doPrivileged(...). Also you may want to
increase the thread priority but thread prir. are often ignored anyways.
Least, name the thread properly :)

JDK, itself, has problems leaking ClassLoaders (KeepAliveCache for example)
and quite a lot of other libraries suffer from the same issue, the open
sources ones are relatively easy to fix but some require heavy hacking via
reflection.
-- 
View this message in context: http://old.nabble.com/Reference-Collections-tp33078668p33097991.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From peter.firmstone at zeus.net.au  Sun Jan  8 06:01:23 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Sun, 08 Jan 2012 21:01:23 +1000
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 84,
	Issue 30
In-Reply-To: <mailman.1.1325955600.22431.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1325955600.22431.concurrency-interest@cs.oswego.edu>
Message-ID: <1326020483.20976.27.camel@bluto>

Wow thanks, those gold nuggets will save many headaches, much
appreciated.

Regards,

Peter.


On Sun, 2012-01-08 at 03:00, concurrency-interest-request at cs.oswego.edu
wrote:
> Send Concurrency-interest mailing list submissions to
> 	concurrency-interest at cs.oswego.edu
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
> 	concurrency-interest-request at cs.oswego.edu
> 
> You can reach the person managing the list at
> 	concurrency-interest-owner at cs.oswego.edu
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
> 
> 
> Today's Topics:
> 
>    1. Re: Re ference Collections (bestsss)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Sat, 7 Jan 2012 03:19:47 -0800 (PST)
> From: bestsss <stanimir at riflexo.com>
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Re ference Collections
> Message-ID: <33097991.post at talk.nabble.com>
> Content-Type: text/plain; charset=us-ascii
> 
> 
> 
> 
> Peter Firmstone-3 wrote:
> > 
> > Ok, based on the comments so far:
> > 
> > Reasoning about the possibility that one thread may not be able to keep
> > up with a ReferenceQueue, bestsss mentioned that ReferenceQueue is a
> > stack, not a queue, because it blocks while the garbage collector thread
> > holds it's lock, the cleaning thread won't be able to poll(), but I
> > suspect the jvm only does this a scheduled intervals.
> > 
> It's not a GC thread but the dedicated ReferenceHandler which gets the
> enqueued references and adds them to the appropriate queues (stacks), also
> executes sun.misc.Cleaner directly. It employs wait/notify and notify() is
> called by the JVM itself.
> 
> 
> Peter Firmstone-3 wrote:
> > 
> >     private final static ScheduledExecutorService garbageCleaner =
> >             Executors.newScheduledThreadPool(1);
> >     // Map to register newly created object references.
> >     private final static Map<Reference,ScheduledFuture> finalizerTasks =
> >             new ConcurrentHashMap<Reference,ScheduledFuture>();
> >     // Finalizer queue to advise cancellation of ScheduledFuture's, 
> >     // when their ReferenceProcessor has been collected.
> >     private final static ReferenceQueue<Reference> phantomQueue = 
> >             new ReferenceQueue<Reference>();
> >     static {
> >         // Finizer Task to cancel unneeded tasks.
> >         garbageCleaner.scheduleAtFixedRate(
> >                 new FinalizerTask(phantomQueue, finalizerTasks), 
> >                 5L, 5L, TimeUnit.MINUTES
> >                 );
> >     }
> >     
> > 
> 
> I'd encourage you to pay extra attention at creating the "static" threads,
> they leak resources badly - mostly thread group, context ClassLoader and
> java.security.AccessControlContext (that contains reference to a classloader
> too) and it has caused me truly a lot of grief. The problem occurs in
> managed environments where redeploys are normal practice in long (months++)
> running processes. The spawned thread will inherit and keep forever the
> context class loader of the calling thread. It's absolutely random which
> thread will initialize the class first. Use thread a ThreadFactory and set
> the conext ClassLoader to ReferenceProcessor.class.getClassLoader() (or just
> null) and try to put in thread in the "system" (or main) thread group if
> there is no System.getSecurityManager(). To prevent the leaking of ACL can
> do smth like AccessController.doPrivileged(...). Also you may want to
> increase the thread priority but thread prir. are often ignored anyways.
> Least, name the thread properly :)
> 
> JDK, itself, has problems leaking ClassLoaders (KeepAliveCache for example)
> and quite a lot of other libraries suffer from the same issue, the open
> sources ones are relatively easy to fix but some require heavy hacking via
> reflection.
> -- 
> View this message in context: http://old.nabble.com/Reference-Collections-tp33078668p33097991.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> End of Concurrency-interest Digest, Vol 84, Issue 30
> ****************************************************


From howard.lovatt at gmail.com  Sun Jan  8 06:14:21 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Sun, 8 Jan 2012 22:14:21 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <4F0813FA.2060903@javaspecialists.eu>
References: <CACR_FB6rjJ339pLaZ1R+PjGCB88=EpHotuHUqQJz=WcP_2keAA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDMJCAA.davidcholmes@aapt.net.au>
	<CACR_FB5vbXUTMXU3NdCzcSio3k3hY1K9Z+KyFW2t5WwxcYLrsA@mail.gmail.com>
	<6896E468-4B9B-4BF1-8656-27302AA14A9B@cox.net>
	<CACR_FB671C+T8BmLysvUMHPTnQAikXB=0rnQ3mz714tYmJsu_w@mail.gmail.com>
	<4F0813FA.2060903@javaspecialists.eu>
Message-ID: <CACR_FB6Rh+xg9M0JYexL21j1q5O-4ZJanWwhYGe5DEvMnFAhSQ@mail.gmail.com>

Hi All,

Thanks for everyones suggestions.

Here is a brief summary of my latest experiments with this bug.

   1. I cannot get JConsole to connect
   2. Both Heinz's code and my code run fine on a core 2 duo Mac I have
   (same Java version, same OS - see below)
   3. Both Heinz's code and my code run fine on my MacBook
   running Stephen Bannasch's MLVM, i.e. same machine. JVM version is:

gar-ln:dist lov080$ ~/Downloads/MLVM/1.7.0_2011_05_02/Home/bin/java -version
openjdk version "1.7.0-internal"
OpenJDK Runtime Environment (build
1.7.0-internal-stephen_2011_05_02_10_53-b00)
OpenJDK 64-Bit Server VM (build 21.0-b07, mixed mode)

     4. Both Heinz's code and my code run fine on my MacBook, i.e. same
machine, running Windows 7 (via parallels). JVM version is:

Z:\Dropbox\Personal\Java\examples\NestedSynchronizedProblem\dist>java
-version
java version "1.7.0_02"
Java(TM) SE Runtime Environment (build 1.7.0_02-b13)
Java HotSpot(TM) 64-Bit Server VM (build 22.0-b10, mixed mode)


The only combination for which I can test that fails, for both Heinz's code
and my code, is Mac OS 10.7.2 running:

gar-ln:src lov080$ java -version
java version "1.6.0_29"
Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)



Is there somewhere to log this bug formally, since I think it is over to
Oracle and/or Apple on this one now?

Thanks again,

 -- Howard.

On 7 January 2012 20:44, Dr Heinz M. Kabutz <heinz at javaspecialists.eu>wrote:

> **
> Yes, we also found this problem to be hardware related.  Here is a class
> sent to me by Jim McClure that also causes a JVM deadlock on certain modern
> i7 chips (not the older ones) on Mac OS X:
>
>  public class Foo {
>
>  private int counter;
>
>  public static void main(String[] args) throws Exception {
>  new Foo().run();
>  }
>
>  public void run() throws Exception {
>
>  Thread[] threads = new Thread[2];
>
>  for (int i = 0; i < threads.length; i++) {
>  threads[i] = new Thread() {
>  public void run() {
>
>  System.out.println("Thread started " + Thread.currentThread().getName());
>  System.out.flush();
>
>  for (int i = 0; i < 100000; i++) {
>  increment();
>  }
>
>  System.out.println("Thread done " + Thread.currentThread().getName());
>  System.out.flush();
>  }
>  };
>  }
>
>  for (Thread t : threads) {
>  t.start();
>  }
>
>  System.out.println("Started");
>  System.out.flush();
>
>  System.out.println("Awaiting complete...");
>  System.out.flush();
>
>  for (Thread t : threads) {
>  t.join();
>  }
>
>  System.out.println("Completed...");
>  System.out.flush();
>  }
>
>  private synchronized void increment() {
>
>  counter++;
>
>  if (counter % 10000 == 0) {
>  System.out.printf("%s: %d\n", Thread.currentThread().getName(), counter);
>  System.out.flush();
>  }
>  }
> }
>
>  Output:
>
>  Thread started Thread-1
> Thread started Thread-2
> Started
> Awaiting complete...
> Thread-2: 10000
> Thread-2: 20000
> Thread-1: 30000
> Thread-2: 40000
> Thread-1: 50000
> Thread-2: 60000
> Thread-2: 70000
> Thread-2: 80000
> Thread-1: 90000
> Thread-1: 100000
> Thread-1: 110000
> Thread-1: 120000
> Thread-1: 130000
> Thread-1: 140000
> Thread done Thread-1
>
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professional
> http://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
>
> On 1/7/12 4:49 AM, Howard Lovatt wrote:
>
> Hi Gregg,
>
>  Weird I have the same Java version, but for me it starts working (high
> CPU usage) and then hangs (negligible CPU usage). Whilst it is running I
> can get a stack trace, see below. But once it has hung I can't get a stack
> trace and I have to kill it.
>
>  Could it be a hardware variation; I have a 4-core i7 processor?
>
>    Model Name: MacBook Pro
>
>   Model Identifier: MacBookPro8,2
>
>   Processor Name: Intel Core i7
>
>   Processor Speed: 2.3 GHz
>
>   Number of Processors: 1
>
>   Total Number of Cores: 4
>
>   L2 Cache (per Core): 256 KB
>
>   L3 Cache: 8 MB
>
>  Could it be a software variation; I have Lion?
>
>    System Version: Mac OS X 10.7.2 (11C74)
>
>   Kernel Version: Darwin 11.2.0
>
>   Boot Volume: sunzero-ln
>
>   Boot Mode: Normal
>
>   Computer Name: sunzero-ln
>
>   User Name: Howard Lovatt (lov080)
>
>   Secure Virtual Memory: Enabled
>
>   64-bit Kernel and Extensions: Yes
>
>   Time since boot: 29 days 4:35
>
>
>  Thanks for running the code.
>
>  -- Howard
>
> ===============================================
>
>  gar-ln:src lov080$ java
> nestedsynchronizedproblem.NestedSynchronizedProblem
> ^\2012-01-06 17:17:34
> Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed
> mode):
>
>  "pool-1-thread-2" prio=5 tid=7fb8fb291800 nid=0x113dfd000 runnable
> [113dfc000]
>    java.lang.Thread.State: RUNNABLE
>  at java.lang.Thread.isInterrupted(Native Method)
>  at java.lang.Thread.interrupted(Thread.java:934)
>  at
> nestedsynchronizedproblem.ParrallelSumMethod.checkForInterrupt(NestedSynchronizedProblem.java:68)
>  at
> nestedsynchronizedproblem.ParrallelSumMethod.setA2(NestedSynchronizedProblem.java:61)
>  at
> nestedsynchronizedproblem.NestedSynchronizedProblem$2.call(NestedSynchronizedProblem.java:31)
>  at
> nestedsynchronizedproblem.NestedSynchronizedProblem$2.call(NestedSynchronizedProblem.java:29)
>  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>  at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>  at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>  at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>  at java.lang.Thread.run(Thread.java:680)
>
>  "pool-1-thread-1" prio=5 tid=7fb8fb290800 nid=0x113cfa000 waiting for
> monitor entry [113cf9000]
>    java.lang.Thread.State: BLOCKED (on object monitor)
>  at
> nestedsynchronizedproblem.ParrallelSumMethod.isSetA1(NestedSynchronizedProblem.java:72)
>  - locked <7f42b20a0> (a nestedsynchronizedproblem.ParrallelSumMethod)
>  at
> nestedsynchronizedproblem.ParrallelSumMethod.setA1(NestedSynchronizedProblem.java:53)
>  at
> nestedsynchronizedproblem.NestedSynchronizedProblem$1.call(NestedSynchronizedProblem.java:25)
>  at
> nestedsynchronizedproblem.NestedSynchronizedProblem$1.call(NestedSynchronizedProblem.java:23)
>  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>  at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>  at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>  at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>  at java.lang.Thread.run(Thread.java:680)
>
>  "Low Memory Detector" daemon prio=5 tid=7fb8fb15a000 nid=0x11395d000
> runnable [00000000]
>    java.lang.Thread.State: RUNNABLE
>
>  "C2 CompilerThread1" daemon prio=9 tid=7fb8fb159800 nid=0x11385a000
> waiting on condition [00000000]
>    java.lang.Thread.State: RUNNABLE
>
>  "C2 CompilerThread0" daemon prio=9 tid=7fb8fb158800 nid=0x113757000
> waiting on condition [00000000]
>    java.lang.Thread.State: RUNNABLE
>
>  "Signal Dispatcher" daemon prio=9 tid=7fb8fb158000 nid=0x113654000
> waiting on condition [00000000]
>    java.lang.Thread.State: RUNNABLE
>
>  "Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7fb8fb157000
> nid=0x113551000 waiting on condition [00000000]
>    java.lang.Thread.State: RUNNABLE
>
>  "Finalizer" daemon prio=8 tid=7fb8fb13f000 nid=0x11328d000 in
> Object.wait() [11328c000]
>    java.lang.Thread.State: WAITING (on object monitor)
>  at java.lang.Object.wait(Native Method)
>  - waiting on <7f42b42a0> (a java.lang.ref.ReferenceQueue$Lock)
>  at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
>  - locked <7f42b42a0> (a java.lang.ref.ReferenceQueue$Lock)
>  at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
>  at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
>
>  "Reference Handler" daemon prio=10 tid=7fb8fb13e800 nid=0x11318a000 in
> Object.wait() [113189000]
>    java.lang.Thread.State: WAITING (on object monitor)
>  at java.lang.Object.wait(Native Method)
>  - waiting on <7f42b0100> (a java.lang.ref.Reference$Lock)
>  at java.lang.Object.wait(Object.java:485)
>  at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
>  - locked <7f42b0100> (a java.lang.ref.Reference$Lock)
>
>  "main" prio=5 tid=7fb8fb001000 nid=0x10b414000 waiting on condition
> [10b413000]
>    java.lang.Thread.State: TIMED_WAITING (parking)
>  at sun.misc.Unsafe.park(Native Method)
>  - parking to wait for  <7f42bc010> (a
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
>  at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
>  at
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
>  at
> java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1253)
>  at
> nestedsynchronizedproblem.NestedSynchronizedProblem.main(NestedSynchronizedProblem.java:39)
>
>  "VM Thread" prio=9 tid=7fb8fb13a000 nid=0x113087000 runnable
>
>  "Gang worker#0 (Parallel GC Threads)" prio=9 tid=7fb8fb002800
> nid=0x10e74d000 runnable
>
>  "Gang worker#1 (Parallel GC Threads)" prio=9 tid=7fb8fb003000
> nid=0x10e850000 runnable
>
>  "Gang worker#2 (Parallel GC Threads)" prio=9 tid=7fb8fb003800
> nid=0x10e953000 runnable
>
>  "Gang worker#3 (Parallel GC Threads)" prio=9 tid=7fb8fb004000
> nid=0x10ea56000 runnable
>
>  "Gang worker#4 (Parallel GC Threads)" prio=9 tid=7fb8fb005000
> nid=0x10eb59000 runnable
>
>  "Gang worker#5 (Parallel GC Threads)" prio=9 tid=7fb8fb005800
> nid=0x10ec5c000 runnable
>
>  "Gang worker#6 (Parallel GC Threads)" prio=9 tid=7fb8fb006000
> nid=0x10ed5f000 runnable
>
>  "Gang worker#7 (Parallel GC Threads)" prio=9 tid=7fb8fb006800
> nid=0x10ee62000 runnable
>
>  "Concurrent Mark-Sweep GC Thread" prio=9 tid=7fb8fb0e4000
> nid=0x112d2d000 runnable
> "Gang worker#0 (Parallel CMS Threads)" prio=9 tid=7fb8fb0e3000
> nid=0x112327000 runnable
>
>  "Gang worker#1 (Parallel CMS Threads)" prio=9 tid=7fb8fb0e3800
> nid=0x11242a000 runnable
>
>  "VM Periodic Task Thread" prio=10 tid=7fb8fb16b800 nid=0x113a60000
> waiting on condition
>
>  "Exception Catcher Thread" prio=10 tid=7fb8fb001800 nid=0x10b576000
> runnable
> JNI global references: 906
>
>  Heap
>  par new generation   total 19136K, used 1211K [7f3000000, 7f44c0000,
> 7f44c0000)
>   eden space 17024K,   4% used [7f3000000, 7f30d16f8, 7f40a0000)
>   from space 2112K,  17% used [7f42b0000, 7f430d720, 7f44c0000)
>   to   space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
>  concurrent mark-sweep generation total 63872K, used 0K [7f44c0000,
> 7f8320000, 7fae00000)
>  concurrent-mark-sweep perm gen total 21248K, used 4769K [7fae00000,
> 7fc2c0000, 800000000)
>
>  Killed: 9
>
>
> On 6 January 2012 15:51, Gregg Wonderly <gergg at cox.net> wrote:
>
>> On my mac, it seems to complete successfully.  I've included a stack dump
>> just FYI.
>>
>> Gregg Wonderly
>>
>> gwmac:~ gregg$ javac -d . Prob.java
>> gwmac:~ gregg$ java my.Prob
>> ^\
>> 2012-01-05 22:48:09
>> Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed
>> mode):
>>
>> "pool-1-thread-2" prio=5 tid=7ffd42969800 nid=0x10c132000 runnable
>> [10c131000]
>>   java.lang.Thread.State: RUNNABLE
>>        at my.ParrallelSumMethod.conditionallySumArguments(Prob.java:87)
>>        - locked <7f44e1400> (a my.ParrallelSumMethod)
>>        at my.ParrallelSumMethod.isSetA2(Prob.java:80)
>>        - locked <7f44e1400> (a my.ParrallelSumMethod)
>>        at my.ParrallelSumMethod.setA2(Prob.java:57)
>>        at my.Prob$2.call(Prob.java:28)
>>        at my.Prob$2.call(Prob.java:26)
>>        at
>> java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>>        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>>        at
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>>        at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>>        at java.lang.Thread.run(Thread.java:680)
>>
>> "pool-1-thread-1" prio=5 tid=7ffd4287f000 nid=0x10c02f000 runnable
>> [10c02e000]
>>   java.lang.Thread.State: RUNNABLE
>>        at java.lang.Thread.isInterrupted(Native Method)
>>        at java.lang.Thread.interrupted(Thread.java:934)
>>        at my.ParrallelSumMethod.checkForInterrupt(Prob.java:65)
>>        at my.ParrallelSumMethod.setA1(Prob.java:51)
>>        at my.Prob$1.call(Prob.java:22)
>>        at my.Prob$1.call(Prob.java:20)
>>        at
>> java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>>        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>>        at
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>>        at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>>        at java.lang.Thread.run(Thread.java:680)
>>
>> "Low Memory Detector" daemon prio=5 tid=7ffd4409f000 nid=0x10bc3d000
>> runnable [00000000]
>>   java.lang.Thread.State: RUNNABLE
>>
>> "C2 CompilerThread1" daemon prio=9 tid=7ffd4409e800 nid=0x10bb3a000
>> waiting on condition [00000000]
>>   java.lang.Thread.State: RUNNABLE
>>
>> "C2 CompilerThread0" daemon prio=9 tid=7ffd4409d800 nid=0x10ba37000
>> waiting on condition [00000000]
>>   java.lang.Thread.State: RUNNABLE
>>
>> "Signal Dispatcher" daemon prio=9 tid=7ffd4409d000 nid=0x10b934000
>> waiting on condition [00000000]
>>   java.lang.Thread.State: RUNNABLE
>>
>> "Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7ffd4409c000
>> nid=0x10b831000 waiting on condition [00000000]
>>   java.lang.Thread.State: RUNNABLE
>>
>> "Finalizer" daemon prio=8 tid=7ffd44095800 nid=0x10b54f000 in
>> Object.wait() [10b54e000]
>>   java.lang.Thread.State: WAITING (on object monitor)
>>        at java.lang.Object.wait(Native Method)
>>        - waiting on <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
>>        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
>>        - locked <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
>>        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
>>        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
>>
>> "Reference Handler" daemon prio=10 tid=7ffd44094800 nid=0x10b44c000 in
>> Object.wait() [10b44b000]
>>   java.lang.Thread.State: WAITING (on object monitor)
>>        at java.lang.Object.wait(Native Method)
>>        - waiting on <7f44e1ff8> (a java.lang.ref.Reference$Lock)
>>        at java.lang.Object.wait(Object.java:485)
>>        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
>>        - locked <7f44e1ff8> (a java.lang.ref.Reference$Lock)
>>
>> "main" prio=5 tid=7ffd44000800 nid=0x104aee000 waiting on condition
>> [104aed000]
>>   java.lang.Thread.State: TIMED_WAITING (parking)
>>        at sun.misc.Unsafe.park(Native Method)
>>        - parking to wait for  <7f44e2070> (a
>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
>>        at
>> java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
>>        at
>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
>>        at
>> java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1253)
>>        at my.Prob.main(Prob.java:36)
>>
>> "VM Thread" prio=9 tid=7ffd42802800 nid=0x10b349000 runnable
>>
>> "Gang worker#0 (Parallel GC Threads)" prio=9 tid=7ffd44002000
>> nid=0x107e27000 runnable
>>
>> "Gang worker#1 (Parallel GC Threads)" prio=9 tid=7ffd44002800
>> nid=0x107f2a000 runnable
>>
>> "Concurrent Mark-Sweep GC Thread" prio=9 tid=7ffd4404d000 nid=0x10afef000
>> runnable
>> "VM Periodic Task Thread" prio=10 tid=7ffd440a1000 nid=0x10bd40000
>> waiting on condition
>>
>> "Exception Catcher Thread" prio=10 tid=7ffd44001800 nid=0x104c50000
>> runnable
>> JNI global references: 914
>>
>> Heap
>>  par new generation   total 19136K, used 5150K [7f3000000, 7f44c0000,
>> 7f44c0000)
>>  eden space 17024K,  30% used [7f3000000, 7f35078b8, 7f40a0000)
>>  from space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
>>  to   space 2112K,   0% used [7f42b0000, 7f42b0000, 7f44c0000)
>>  concurrent mark-sweep generation total 63872K, used 379K [7f44c0000,
>> 7f8320000, 7fae00000)
>>  concurrent-mark-sweep perm gen total 21248K, used 4811K [7fae00000,
>> 7fc2c0000, 800000000)
>>
>> 99999990000000, terminated ok
>> gwmac:~ gregg$ java -version
>> java version "1.6.0_29"
>> Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
>> Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)
>> gwmac:~ gregg$
>
>
>
>
>  --
>   -- Howard.
>
>  ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120108/d05796f9/attachment-0001.html>

From peter.firmstone at zeus.net.au  Sun Jan  8 06:40:13 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Sun, 08 Jan 2012 21:40:13 +1000
Subject: [concurrency-interest] Non Blocking java.security.Policy -
	synchronized method is	superclass.
Message-ID: <1326022813.20976.30.camel@bluto>

Appended is a new java.security.Policy implementation, it fully supports
the existing java policy syntax and accepts alternate PolicyParser's. 

All state is immutable, except for 2 volatile references, referents
replaced, not mutated, when the policy is updated.  One referent is an
array containing PermissionGrant's (interface for immutable object
representing a grant statement in a policy), the second a
PermissionCollection containing the Policy Permissions.  The array is
never mutated after creation, a reference to the array is copied before
accessing the array or any array methods. 

The policy creates PermissionCollection's on demand for checking,
Permission's are ordered using a PermissionComparator to ensure that for
example, wildcard SocketPermission's are checked first, to avoid
unnecessary DNS lookups.  Only the permission being checked and any
UnresolvedPermission's are added to the PermissionCollection, limiting
the size of the objects created. 

In existing policy implementations PermissionCollection's perform
blocking operations. 

Also, after parsing policy files, PermissionGrant implementations avoid
the need to open files or network connections to confirm URL's, eg
CodeSource.implies is not called, but instead reimplemented using URI. 

Will this scale?  There but one smell: 

ProtectionDomain uses a synchronized method Policy.getPolicyNoCheck(),
but this only retrieves a reference on 99% of occasions. 

For every permission check, the stack access control context is
retrieved, every ProtectionDomain on the stack must be checked,
ProtectionDomain's must call getPolicyNoCheck() to call
Policy.implies(ProtectionDomain domain, Permission permission). 

To make this worse, I've got a SecurityManager that divides the
ProtectionDomain.implies() calls into tasks and submits them to an
executor (if there are 4 or more PD's in a context).  The
SecurityManager is also non blocking, at least it will be when I use the
new ConcurrentHashMap for the checked permission cache (avoids repeated
security checks), for now the cache is implemented using the existing
ConcurrentHashMap, but is mostly read in any case.  (P.S. This is the
cache I'm using the Reference Collection's for.) 

How much can this one synchronized method spoil scalability? 

Cheers & thanks in advance, 

Peter. 



/* 
*  Licensed to the Apache Software Foundation (ASF) under one or more 
*  contributor license agreements.  See the NOTICE file distributed with
*  this work for additional information regarding copyright ownership. 
*  The ASF licenses this file to You under the Apache License, Version
2.0 
*  (the "License"); you may not use this file except in compliance with 
*  the License.  You may obtain a copy of the License at 
* 
*     http://www.apache.org/licenses/LICENSE-2.0
* 
*  Unless required by applicable law or agreed to in writing, software 
*  distributed under the License is distributed on an "AS IS" BASIS, 
*  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied. 
*  See the License for the specific language governing permissions and 
*  limitations under the License. 
*/ 

/** 
 * Default Policy implementation taken from Apache Harmony, refactored
for 
 * concurrency. 
 * 
 * @author Alexey V. Varlamov 
 * @author Peter Firmstone 
 * @version $Revision$ 
 */ 

package net.jini.security.policy; 

import java.io.File; 
import java.net.URL; 
import java.security.AccessController; 
import java.security.AllPermission; 
import java.security.CodeSource; 
import java.security.Guard; 
import java.security.Permission; 
import java.security.PermissionCollection; 
import java.security.Permissions; 
import java.security.Policy; 
import java.security.PrivilegedActionException; 
import java.security.PrivilegedExceptionAction; 
import java.security.ProtectionDomain; 
import java.security.SecurityPermission; 
import java.security.UnresolvedPermission; 
import java.util.ArrayList; 
import java.util.Collection; 
import java.util.Enumeration; 
import java.util.Iterator; 
import java.util.List; 
import java.util.NavigableSet; 
import java.util.Properties; 
import java.util.TreeSet; 
import net.jini.security.PermissionComparator; 
import org.apache.river.api.security.PermissionGrant; 
import org.apache.river.impl.security.policy.util.DefaultPolicyParser; 
import org.apache.river.impl.security.policy.util.PolicyParser; 
import org.apache.river.impl.security.policy.util.PolicyUtils; 


/** 
* Concurrent Policy implementation based on policy configuration files, 
* it is intended to provide concurrent implies() for greatly improved 
* throughput at the expense of single threaded performance. 
* 
* Set the following system properties to use this Policy instead of the 
* built in Java sun.security.provider.PolicyFile: 
* * net.jini.security.policy.PolicyFileProvider.basePolicyClass = 
* org.apache.river.security.concurrent.ConcurrentPolicyFile 
* 
* 
* This 
* implementation recognizes text files, consisting of clauses with the 
* following syntax: 
* 
* <pre> 
* keystore &quot;some_keystore_url&quot; [, &quot;keystore_type&quot;]; 
* </pre> 
<pre> 
* grant [SignedBy &quot;signer_names&quot;] [, CodeBase &quot;URL&quot;]
*  [, Principal [principal_class_name] &quot;principal_name&quot;] 
*  [, Principal [principal_class_name] &quot;principal_name&quot;] ... {
*  permission permission_class_name [ &quot;target_name&quot; ] [,
&quot;action&quot;] 
*  [, SignedBy &quot;signer_names&quot;]; 
*  permission ... 
*  }; 
* * </pre> 
* 
* The <i>keystore </i> clause specifies reference to a keystore, which
is a 
* database of private keys and their associated digital certificates.
The 
* keystore is used to look up the certificates of signers specified in
the 
* <i>grant </i> entries of the file. The policy file can contain any
number of 
* <i>keystore </i> entries which can appear at any ordinal position.
However, 
* only the first successfully loaded keystore is used, others are
ignored. The 
* keystore must be specified if some grant clause refers to a
certificate's 
* alias. <br> 
* The <i>grant </i> clause associates a CodeSource (consisting of an URL
and a 
* set of certificates) of some executable code with a set of Permissions
which 
* should be granted to the code. So, the CodeSource is defined by values
of 
* <i>CodeBase </i> and <i>SignedBy </i> fields. The <i>CodeBase </i>
value must 
* be in URL format, while <i>SignedBy </i> value is a (comma-separated
list of) 
* alias(es) to keystore certificates. These fields can be omitted to
denote any 
* codebase and any signers (including case of unsigned code),
respectively. 
* <br> 
* Also, the code may be required to be executed on behalf of some
Principals 
* (in other words, code's ProtectionDomain must have the array of
Principals 
* associated) in order to possess the Permissions. This fact is
indicated by 
* specifying one or more <i>Principal </i> fields in the <i>grant </i>
clause. 
* Each Principal is specified as class/name pair; name and class can be
either 
* concrete value or wildcard <i>* </i>. As a special case, the class
value may 
* be omitted and then the name is treated as an alias to X.509
Certificate, and 
* the Principal is assumed to be javax.security.auth.x500.X500Principal
with a 
* name of subject's distinguished name from the certificate. <br> 
* The order between the <i>CodeBase </i>, <i>SignedBy </i>, and
<i>Principal 
* </i> fields does not matter. The policy file can contain any number of
grant 
* clauses. <br> 
* Each <i>grant </i> clause must contain one or more <i>permission </i>
entry. 
* The permission entry consist of a fully qualified class name along
with 
* optional <i>name </i>, <i>actions </i> and <i>signedby </i> values.
Name and 
* actions are arguments to the corresponding constructor of the
permission 
* class. SignedBy value represents the keystore alias(es) to
certificate(s) 
* used to sign the permission class. That is, this permission entry is 
* effective (i.e., access control permission will be granted based on
this 
* entry) only if the bytecode implementation of permission class is
verified to 
* be correctly signed by the said alias(es). <br> 
* <br> 
* The policy content may be parameterized via property expansion.
Namely, 
* expressions like <i>${key} </i> are replaced by values of
corresponding 
* system properties. Also, the special <i>slash </i> key (i.e. ${/}) is 
* supported, it is a shortcut to &quot;file.separator&quot; key.
Property 
* expansion is performed anywhere a double quoted string is allowed in
the 
* policy file. However, this feature is controlled by security
properties and 
* should be turned on by setting &quot;policy.expandProperties&quot;
property 
* to <i>true </i>. <br> 
* If property expansion fails (due to a missing key), a corresponding
entry is 
* ignored. For fields of <i>keystore </i> and <i>grant </i> clauses, the
whole 
* clause is ignored, and for <i>permission </i> entry, only that entry
is 
* ignored. <br> 
* <br> 
* The policy also supports generalized expansion in permissions names,
of 
* expressions like <i>${{protocol:data}} </i>. Currently the following 
* protocols supported: 
* <dl> 
* <dt>self 
* <dd>Denotes substitution to a principal information of the parental
Grant 
* entry. Replaced by a space-separated list of resolved Principals
(including 
* wildcarded), each formatted as <i>class &quot;name&quot; </i>. If
parental 
* Grant entry has no Principals, the permission is ignored. 
* <dt>alias: <i>name </i> 
* <dd>Denotes substitution of a KeyStore alias. Namely, if a KeyStore
has an 
* X.509 certificate associated with the specified name, then replaced by
* <i>javax.security.auth.x500.X500Principal &quot; <i>DN </i>&quot; </i>
* string, where <i>DN </i> is a certificate's subject distinguished
name. 
* </dl> 
* <br> 
* 
*/ 

public class ConcurrentPolicyFile extends Policy implements
ConcurrentPolicy { 

   /** 
    * System property for dynamically added policy location. 
    */ 
   private static final String JAVA_SECURITY_POLICY =
"java.security.policy"; //$NON-NLS-1$ 

   /** 
    * Prefix for numbered Policy locations specified in
security.properties. 
    */ 
   private static final String POLICY_URL_PREFIX = "policy.url.";
//$NON-NLS-1$ 
     // Reference must be defensively copied before access, once
published, never mutated. 
   private volatile PermissionGrant [] grantArray; 
     // A specific parser for a particular policy file format. 
   private final PolicyParser parser; 
     private static final Guard guard = new
SecurityPermission("getPolicy"); 
     private final ProtectionDomain myDomain; 
     // reference must be defensively copied before access, once
published, never mutated. 
   private volatile PermissionCollection myPermissions; 
     /** 
    * Default constructor, equivalent to 
    * <code>ConcurrentPolicyFile(new DefaultPolicyParser())</code>. 
    */ 
   public ConcurrentPolicyFile() throws PolicyInitializationException { 
       this(new DefaultPolicyParser()); 
   } 

   /** 
    * Extension constructor for plugging-in a custom parser. 
    * @param dpr 
    */ 
   protected ConcurrentPolicyFile(PolicyParser dpr) throws
PolicyInitializationException { 
       guard.checkGuard(null); 
       parser = dpr; 
       myDomain = this.getClass().getProtectionDomain(); 
       /* 
        * The bootstrap policy makes implies decisions until this
constructor 
        * has returned.  We don't need to lock. 
        */ 
       try { 
           // Bug 4911907, do we need to do anything more? 
           // The permissions for this domain must be retrieved before 
           // construction is complete and this policy takes over. 
           initialize(); // Instantiates myPermissions. 
       } catch (SecurityException e){ 
           throw e; 
       } catch (Exception e){ 
           throw new PolicyInitializationException("PolicyInitialization
failed", e); 
       } 
   } 
     private PermissionCollection convert(NavigableSet<Permission>
permissions){ 
       PermissionCollection pc = new Permissions(); 
       // The descending iterator is for SocketPermission. 
       Iterator<Permission> it = permissions.descendingIterator(); 
       while (it.hasNext()) { 
           pc.add(it.next()); 
       } 
       return pc; 
   } 

   /** 
    * Returns collection of permissions allowed for the domain 
    * according to the policy. The evaluated characteristics of the 
    * domain are it's codesource and principals; they are assumed 
    * to be <code>null</code> if the domain is <code>null</code>. 
    * 
    * Each PermissionCollection returned is a unique instance. 
    * 
    * @param pd ProtectionDomain 
    * @see ProtectionDomain 
    */ 
   @Override 
   public PermissionCollection getPermissions(ProtectionDomain pd) { 
       NavigableSet<Permission> perms = new TreeSet<Permission>(new
PermissionComparator()); 
       PermissionGrant [] grantRefCopy = grantArray; 
       int l = grantRefCopy.length; 
       for ( int j =0; j < l; j++ ){ 
           PermissionGrant ge = grantRefCopy[j]; 
           if (ge.implies(pd)){ 
               if (ge.isPrivileged()){// Don't stuff around finish early
if you can. 
                   PermissionCollection pc = new Permissions(); 
                   pc.add(new AllPermission()); 
                   return pc; 
               } 
               Collection<Permission> c = ge.getPermissions(); 
               Iterator<Permission> i = c.iterator(); 
               while (i.hasNext()){ 
                   Permission p = i.next(); 
                   perms.add(p); 
               } 
           } 
       } 
       // Don't forget to merge the static Permissions. 
       PermissionCollection staticPC = null; 
       if (pd != null) { 
           staticPC = pd.getPermissions(); 
           if (staticPC != null){ 
               Enumeration<Permission> e = staticPC.elements(); 
               while (e.hasMoreElements()){ 
                   Permission p = e.nextElement(); 
                   if (p instanceof AllPermission) { 
                       PermissionCollection pc = new Permissions(); 
                       pc.add(p); 
                       return pc; 
                   } 
                   perms.add(p); 
               } 
           } 
       } 
       return convert(perms); 
   } 

   /** 
    * Returns collection of permissions allowed for the codesource 
    * according to the policy. 
    * The evaluation assumes that current principals are undefined. 
    * 
    * This returns a java.security.Permissions collection, which allows 
    * ProtectionDomain to optimise for the AllPermission case, which
avoids 
    * unnecessarily consulting the policy. 
    * 
    * If constructed with the four argument constructor,
ProtectionDomain.implies 
    * first consults the Policy, then it's own internal Permissions
collection, 
    * unless it has AllPermission, in which case it returns true without
    * consulting the policy. 
    * 
    * @param cs CodeSource 
    * @see CodeSource 
    */ 
   @Override 
   public PermissionCollection getPermissions(CodeSource cs) { 
       if (cs == null) throw new NullPointerException("CodeSource cannot
be null"); 
       NavigableSet<Permission> perms = new TreeSet<Permission>(new
PermissionComparator()); 
       // for ProtectionDomain AllPermission optimisation. 
       PermissionGrant [] grantRefCopy = grantArray; 
       int l = grantRefCopy.length; 
       for ( int j =0; j < l; j++ ){ 
           PermissionGrant ge = grantRefCopy[j]; 
           if (ge.implies(cs, null)){ // No Principal's 
               if (ge.isPrivileged()){// Don't stuff around finish early
if you can. 
                   PermissionCollection pc = new Permissions(); 
                   pc.add(new AllPermission()); 
                   return pc; 
               } 
               Collection<Permission> c = ge.getPermissions(); 
               Iterator<Permission> i = c.iterator(); 
               while (i.hasNext()){ 
                   Permission p = i.next(); 
                   perms.add(p); 
               } 
           } 
       } 
       return convert(perms); 
   } 
     @Override 
   public boolean implies(ProtectionDomain domain, Permission
permission) { 
       if (permission == null) throw new
NullPointerException("permission not allowed to be null"); 
       if (domain == myDomain) { 
           PermissionCollection pc = myPermissions; 
           return pc.implies(permission); 
       } 
       Class klass = permission.getClass(); 
       // Need to have a list of Permission's we can sort if permission
is SocketPermission. 
       NavigableSet<Permission> perms = new TreeSet<Permission>(new
PermissionComparator()); 
       PermissionGrant [] grantRefCopy = grantArray; 
       int l = grantRefCopy.length; 
       for ( int j =0; j < l; j++ ){ 
           PermissionGrant ge = grantRefCopy[j]; 
           if (ge.implies(domain)){ 
               if (ge.isPrivileged()) return true; // Don't stuff around
finish early if you can. 
               Collection<Permission> c = ge.getPermissions(); 
               Iterator<Permission> i = c.iterator(); 
               while (i.hasNext()){ 
                   Permission p = i.next(); 
                   // Don't make it larger than necessary. 
                   if (klass.isInstance(permission) || permission
instanceof UnresolvedPermission){ 
                       perms.add(p); 
                   } 
               } 
           } 
       } 
       // Don't forget to merge the static Permissions. 
       PermissionCollection staticPC = null; 
       if (domain != null) { 
           staticPC =domain.getPermissions(); 
           if (staticPC != null){ 
               Enumeration<Permission> e = staticPC.elements(); 
               while (e.hasMoreElements()){ 
                   Permission p = e.nextElement(); 
                   // return early if possible. 
                   if (p instanceof AllPermission ) return true; 
                   // Don't make it larger than necessary, but don't
worry about duplicates either. 
                   if (klass.isInstance(permission) || permission
instanceof UnresolvedPermission){ 
                       perms.add(p); 
                   } 
               } 
           } 
       } 
       return convert(perms).implies(permission); 
   } 

   /** 
    * Gets fresh list of locations and tries to load all of them in
sequence; 
    * failed loads are ignored. After processing all locations, old
policy 
    * settings are discarded and new ones come into force. <br> 
    * 
    * @see PolicyUtils#getPolicyURLs(Properties, String, String) 
    */ 
   @Override 
   public void refresh() { 
       try { 
           initialize(); 
       } catch (Exception ex) { 
           System.err.println(ex); 
       } 
   } 
     private void initialize() throws Exception{ 
       try { 
           Collection<PermissionGrant> fresh =
AccessController.doPrivileged( 
               new
PrivilegedExceptionAction<Collection<PermissionGrant>>(){ 
                   public Collection<PermissionGrant> run() throws
SecurityException { 
                       Collection<PermissionGrant> fresh = new
ArrayList<PermissionGrant>(120); 
                       Properties system = System.getProperties(); 
                       system.setProperty("/", File.separator);
//$NON-NLS-1$ 
                       URL[] policyLocations =
PolicyUtils.getPolicyURLs(system, 
                                                        
JAVA_SECURITY_POLICY, 
                                                        
POLICY_URL_PREFIX); 
                       int l = policyLocations.length; 
                       for (int i = 0; i < l; i++) { 
                           //TODO debug log 
//                                System.err.println("Parsing policy
file: " + policyLocations[i]); 
                           try { 
                               Collection<PermissionGrant> pc = null; 
                               pc = parser.parse(policyLocations[i],
system); 
                               fresh.addAll(pc); 
                           } catch (Exception e){ 
                               // It's best to let a SecurityException
bubble up 
                               // in case there is a problem with our
policy configuration 
                               // or implementation. 
                               if ( e instanceof SecurityException ) { 
                                   e.printStackTrace(System.out); 
                                   throw (SecurityException) e; 
                               } 
                               // ignore. 
                           } 
                       } 
                       return fresh; 
                   } 
               } 
           ); 
           // Volatile reference, publish after mutation complete. 
           grantArray = fresh.toArray(new
PermissionGrant[fresh.size()]); 
           myPermissions = getPermissions(myDomain); 
       }catch (PrivilegedActionException e){ 
           Throwable t = e.getCause(); 
           if ( t instanceof Exception ) throw (Exception) t; 
           throw e; 
       } 
   } 

   public boolean isConcurrent() { 
       return true; 
   } 

   public PermissionGrant[] getPermissionGrants() { 
       PermissionGrant [] grants = grantArray; // copy volatile
reference target. 
       return grants.clone(); 
   } 
     public PermissionGrant[] getPermissionGrants(ProtectionDomain pd) {
       PermissionGrant [] grants = grantArray; // copy volatile
reference target. 
       int l = grants.length; 
       List<PermissionGrant> applicable = new
ArrayList<PermissionGrant>(l); // Always too large, never too small. 
       for (int i =0; i < l; i++){ 
           if (grants[i].implies(pd)){ 
               applicable.add(grants[i]); 
           } 
       } 
       return applicable.toArray(new
PermissionGrant[applicable.size()]); 
   } 

} 


From davidcholmes at aapt.net.au  Sun Jan  8 17:30:08 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 9 Jan 2012 08:30:08 +1000
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <CACR_FB6Rh+xg9M0JYexL21j1q5O-4ZJanWwhYGe5DEvMnFAhSQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEEJJCAA.davidcholmes@aapt.net.au>

Howard,

There is a Mac OSX port  project for the OpenJDK

http://openjdk.java.net/projects/macosx-port/
https://wikis.oracle.com/display/OpenJDK/Mac+OS+X+Port+Project+Status

it is in the process of being integrated with the OpenJDK 7u main
development stream. The mercurial forest is at:

http://hg.openjdk.java.net/jdk7u/jdk7u-osx/

You might try with the latest JDK7 version if you can, to see if the bug
still reproduces. If so then bugreport.sun.com is the place to file bugs
against the OpenJDK 7 version:

http://bugreport.sun.com/bugreport/

I have no idea how bugs in the Apple 6uX versions are to be handled.

You could also ask on the port mailing list as per the first link above.

David
  -----Original Message-----
  From: Howard Lovatt [mailto:howard.lovatt at gmail.com]
  Sent: Sunday, 8 January 2012 9:14 PM
  To: Dr Heinz M. Kabutz; nathan.reynolds at oracle.com; Gregg Wonderly;
concurrency-interest at cs.oswego.edu; dholmes at ieee.org; Jim McClure
  Subject: Re: [concurrency-interest] Nested synchronized


  Hi All,


  Thanks for everyones suggestions.


  Here is a brief summary of my latest experiments with this bug.
    1.. I cannot get JConsole to connect
    2.. Both Heinz's code and my code run fine on a core 2 duo Mac I have
(same Java version, same OS - see below)
    3.. Both Heinz's code and my code run fine on my MacBook running Stephen
Bannasch's MLVM, i.e. same machine. JVM version is:
    gar-ln:dist lov080$
~/Downloads/MLVM/1.7.0_2011_05_02/Home/bin/java -version
    openjdk version "1.7.0-internal"
    OpenJDK Runtime Environment (build
1.7.0-internal-stephen_2011_05_02_10_53-b00)
    OpenJDK 64-Bit Server VM (build 21.0-b07, mixed mode)


       4. Both Heinz's code and my code run fine on my MacBook, i.e. same
machine, running Windows 7 (via parallels). JVM version is:


    Z:\Dropbox\Personal\Java\examples\NestedSynchronizedProblem\dist>java -v
ersion
    java version "1.7.0_02"
    Java(TM) SE Runtime Environment (build 1.7.0_02-b13)
    Java HotSpot(TM) 64-Bit Server VM (build 22.0-b10, mixed mode)


  The only combination for which I can test that fails, for both Heinz's
code and my code, is Mac OS 10.7.2 running:


    gar-ln:src lov080$ java -version
    java version "1.6.0_29"
    Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
    Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)




  Is there somewhere to log this bug formally, since I think it is over to
Oracle and/or Apple on this one now?


  Thanks again,


   -- Howard.


  On 7 January 2012 20:44, Dr Heinz M. Kabutz <heinz at javaspecialists.eu>
wrote:

    Yes, we also found this problem to be hardware related.  Here is a class
sent to me by Jim McClure that also causes a JVM deadlock on certain modern
i7 chips (not the older ones) on Mac OS X:


    public class Foo {


    private int counter;


    public static void main(String[] args) throws Exception {
    new Foo().run();
    }


    public void run() throws Exception {


    Thread[] threads = new Thread[2];


    for (int i = 0; i < threads.length; i++) {
    threads[i] = new Thread() {
    public void run() {


    System.out.println("Thread started " +
Thread.currentThread().getName());
    System.out.flush();


    for (int i = 0; i < 100000; i++) {
    increment();
    }


    System.out.println("Thread done " + Thread.currentThread().getName());
    System.out.flush();
    }
    };
    }


    for (Thread t : threads) {
    t.start();
    }


    System.out.println("Started");
    System.out.flush();


    System.out.println("Awaiting complete...");
    System.out.flush();


    for (Thread t : threads) {
    t.join();
    }


    System.out.println("Completed...");
    System.out.flush();
    }


    private synchronized void increment() {


    counter++;


    if (counter % 10000 == 0) {
    System.out.printf("%s: %d\n", Thread.currentThread().getName(),
counter);
    System.out.flush();
    }
    }
    }


    Output:


    Thread started Thread-1
    Thread started Thread-2
    Started
    Awaiting complete...
    Thread-2: 10000
    Thread-2: 20000
    Thread-1: 30000
    Thread-2: 40000
    Thread-1: 50000
    Thread-2: 60000
    Thread-2: 70000
    Thread-2: 80000
    Thread-1: 90000
    Thread-1: 100000
    Thread-1: 110000
    Thread-1: 120000
    Thread-1: 130000
    Thread-1: 140000
    Thread done Thread-1




Regards

Heinz
--
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz


    On 1/7/12 4:49 AM, Howard Lovatt wrote:
      Hi Gregg,


      Weird I have the same Java version, but for me it starts working (high
CPU usage) and then hangs (negligible CPU usage). Whilst it is running I can
get a stack trace, see below. But once it has hung I can't get a stack trace
and I have to kill it.


      Could it be a hardware variation; I have a 4-core i7 processor?


        Model Name: MacBook Pro

        Model Identifier: MacBookPro8,2

        Processor Name: Intel Core i7

        Processor Speed: 2.3 GHz

        Number of Processors: 1

        Total Number of Cores: 4

        L2 Cache (per Core): 256 KB

        L3 Cache: 8 MB



      Could it be a software variation; I have Lion?


        System Version: Mac OS X 10.7.2 (11C74)

        Kernel Version: Darwin 11.2.0

        Boot Volume: sunzero-ln

        Boot Mode: Normal

        Computer Name: sunzero-ln

        User Name: Howard Lovatt (lov080)

        Secure Virtual Memory: Enabled

        64-bit Kernel and Extensions: Yes

        Time since boot: 29 days 4:35




      Thanks for running the code.

       -- Howard

      ===============================================



      gar-ln:src lov080$ java
nestedsynchronizedproblem.NestedSynchronizedProblem
      ^\2012-01-06 17:17:34
      Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed
mode):


      "pool-1-thread-2" prio=5 tid=7fb8fb291800 nid=0x113dfd000 runnable
[113dfc000]
         java.lang.Thread.State: RUNNABLE
      at java.lang.Thread.isInterrupted(Native Method)
      at java.lang.Thread.interrupted(Thread.java:934)
      at
nestedsynchronizedproblem.ParrallelSumMethod.checkForInterrupt(NestedSynchro
nizedProblem.java:68)
      at
nestedsynchronizedproblem.ParrallelSumMethod.setA2(NestedSynchronizedProblem
.java:61)
      at
nestedsynchronizedproblem.NestedSynchronizedProblem$2.call(NestedSynchronize
dProblem.java:31)
      at
nestedsynchronizedproblem.NestedSynchronizedProblem$2.call(NestedSynchronize
dProblem.java:29)
      at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
      at java.util.concurrent.FutureTask.run(FutureTask.java:138)
      at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.ja
va:886)
      at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:9
08)
      at java.lang.Thread.run(Thread.java:680)


      "pool-1-thread-1" prio=5 tid=7fb8fb290800 nid=0x113cfa000 waiting for
monitor entry [113cf9000]
         java.lang.Thread.State: BLOCKED (on object monitor)
      at
nestedsynchronizedproblem.ParrallelSumMethod.isSetA1(NestedSynchronizedProbl
em.java:72)
      - locked <7f42b20a0> (a nestedsynchronizedproblem.ParrallelSumMethod)
      at
nestedsynchronizedproblem.ParrallelSumMethod.setA1(NestedSynchronizedProblem
.java:53)
      at
nestedsynchronizedproblem.NestedSynchronizedProblem$1.call(NestedSynchronize
dProblem.java:25)
      at
nestedsynchronizedproblem.NestedSynchronizedProblem$1.call(NestedSynchronize
dProblem.java:23)
      at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
      at java.util.concurrent.FutureTask.run(FutureTask.java:138)
      at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.ja
va:886)
      at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:9
08)
      at java.lang.Thread.run(Thread.java:680)


      "Low Memory Detector" daemon prio=5 tid=7fb8fb15a000 nid=0x11395d000
runnable [00000000]
         java.lang.Thread.State: RUNNABLE


      "C2 CompilerThread1" daemon prio=9 tid=7fb8fb159800 nid=0x11385a000
waiting on condition [00000000]
         java.lang.Thread.State: RUNNABLE


      "C2 CompilerThread0" daemon prio=9 tid=7fb8fb158800 nid=0x113757000
waiting on condition [00000000]
         java.lang.Thread.State: RUNNABLE


      "Signal Dispatcher" daemon prio=9 tid=7fb8fb158000 nid=0x113654000
waiting on condition [00000000]
         java.lang.Thread.State: RUNNABLE


      "Surrogate Locker Thread (Concurrent GC)" daemon prio=5
tid=7fb8fb157000 nid=0x113551000 waiting on condition [00000000]
         java.lang.Thread.State: RUNNABLE


      "Finalizer" daemon prio=8 tid=7fb8fb13f000 nid=0x11328d000 in
Object.wait() [11328c000]
         java.lang.Thread.State: WAITING (on object monitor)
      at java.lang.Object.wait(Native Method)
      - waiting on <7f42b42a0> (a java.lang.ref.ReferenceQueue$Lock)
      at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
      - locked <7f42b42a0> (a java.lang.ref.ReferenceQueue$Lock)
      at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
      at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)


      "Reference Handler" daemon prio=10 tid=7fb8fb13e800 nid=0x11318a000 in
Object.wait() [113189000]
         java.lang.Thread.State: WAITING (on object monitor)
      at java.lang.Object.wait(Native Method)
      - waiting on <7f42b0100> (a java.lang.ref.Reference$Lock)
      at java.lang.Object.wait(Object.java:485)
      at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
      - locked <7f42b0100> (a java.lang.ref.Reference$Lock)


      "main" prio=5 tid=7fb8fb001000 nid=0x10b414000 waiting on condition
[10b413000]
         java.lang.Thread.State: TIMED_WAITING (parking)
      at sun.misc.Unsafe.park(Native Method)
      - parking to wait for  <7f42bc010> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
      at
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
      at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitN
anos(AbstractQueuedSynchronizer.java:2025)
      at
java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.
java:1253)
      at
nestedsynchronizedproblem.NestedSynchronizedProblem.main(NestedSynchronizedP
roblem.java:39)


      "VM Thread" prio=9 tid=7fb8fb13a000 nid=0x113087000 runnable


      "Gang worker#0 (Parallel GC Threads)" prio=9 tid=7fb8fb002800
nid=0x10e74d000 runnable


      "Gang worker#1 (Parallel GC Threads)" prio=9 tid=7fb8fb003000
nid=0x10e850000 runnable


      "Gang worker#2 (Parallel GC Threads)" prio=9 tid=7fb8fb003800
nid=0x10e953000 runnable


      "Gang worker#3 (Parallel GC Threads)" prio=9 tid=7fb8fb004000
nid=0x10ea56000 runnable


      "Gang worker#4 (Parallel GC Threads)" prio=9 tid=7fb8fb005000
nid=0x10eb59000 runnable


      "Gang worker#5 (Parallel GC Threads)" prio=9 tid=7fb8fb005800
nid=0x10ec5c000 runnable


      "Gang worker#6 (Parallel GC Threads)" prio=9 tid=7fb8fb006000
nid=0x10ed5f000 runnable


      "Gang worker#7 (Parallel GC Threads)" prio=9 tid=7fb8fb006800
nid=0x10ee62000 runnable


      "Concurrent Mark-Sweep GC Thread" prio=9 tid=7fb8fb0e4000
nid=0x112d2d000 runnable
      "Gang worker#0 (Parallel CMS Threads)" prio=9 tid=7fb8fb0e3000
nid=0x112327000 runnable


      "Gang worker#1 (Parallel CMS Threads)" prio=9 tid=7fb8fb0e3800
nid=0x11242a000 runnable


      "VM Periodic Task Thread" prio=10 tid=7fb8fb16b800 nid=0x113a60000
waiting on condition


      "Exception Catcher Thread" prio=10 tid=7fb8fb001800 nid=0x10b576000
runnable
      JNI global references: 906


      Heap
       par new generation   total 19136K, used 1211K [7f3000000, 7f44c0000,
7f44c0000)
        eden space 17024K,   4% used [7f3000000, 7f30d16f8, 7f40a0000)
        from space 2112K,  17% used [7f42b0000, 7f430d720, 7f44c0000)
        to   space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
       concurrent mark-sweep generation total 63872K, used 0K [7f44c0000,
7f8320000, 7fae00000)
       concurrent-mark-sweep perm gen total 21248K, used 4769K [7fae00000,
7fc2c0000, 800000000)


      Killed: 9




      On 6 January 2012 15:51, Gregg Wonderly <gergg at cox.net> wrote:

        On my mac, it seems to complete successfully.  I've included a stack
dump just FYI.

        Gregg Wonderly

        gwmac:~ gregg$ javac -d . Prob.java
        gwmac:~ gregg$ java my.Prob
        ^\
        2012-01-05 22:48:09
        Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402
mixed mode):

        "pool-1-thread-2" prio=5 tid=7ffd42969800 nid=0x10c132000 runnable
[10c131000]
          java.lang.Thread.State: RUNNABLE
               at
my.ParrallelSumMethod.conditionallySumArguments(Prob.java:87)
               - locked <7f44e1400> (a my.ParrallelSumMethod)
               at my.ParrallelSumMethod.isSetA2(Prob.java:80)
               - locked <7f44e1400> (a my.ParrallelSumMethod)
               at my.ParrallelSumMethod.setA2(Prob.java:57)
               at my.Prob$2.call(Prob.java:28)
               at my.Prob$2.call(Prob.java:26)
               at
java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
               at java.util.concurrent.FutureTask.run(FutureTask.java:138)
               at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.ja
va:886)
               at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:9
08)
               at java.lang.Thread.run(Thread.java:680)

        "pool-1-thread-1" prio=5 tid=7ffd4287f000 nid=0x10c02f000 runnable
[10c02e000]
          java.lang.Thread.State: RUNNABLE
               at java.lang.Thread.isInterrupted(Native Method)
               at java.lang.Thread.interrupted(Thread.java:934)
               at my.ParrallelSumMethod.checkForInterrupt(Prob.java:65)
               at my.ParrallelSumMethod.setA1(Prob.java:51)
               at my.Prob$1.call(Prob.java:22)
               at my.Prob$1.call(Prob.java:20)
               at
java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
               at java.util.concurrent.FutureTask.run(FutureTask.java:138)
               at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.ja
va:886)
               at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:9
08)
               at java.lang.Thread.run(Thread.java:680)

        "Low Memory Detector" daemon prio=5 tid=7ffd4409f000 nid=0x10bc3d000
runnable [00000000]
          java.lang.Thread.State: RUNNABLE

        "C2 CompilerThread1" daemon prio=9 tid=7ffd4409e800 nid=0x10bb3a000
waiting on condition [00000000]
          java.lang.Thread.State: RUNNABLE

        "C2 CompilerThread0" daemon prio=9 tid=7ffd4409d800 nid=0x10ba37000
waiting on condition [00000000]
          java.lang.Thread.State: RUNNABLE

        "Signal Dispatcher" daemon prio=9 tid=7ffd4409d000 nid=0x10b934000
waiting on condition [00000000]
          java.lang.Thread.State: RUNNABLE

        "Surrogate Locker Thread (Concurrent GC)" daemon prio=5
tid=7ffd4409c000 nid=0x10b831000 waiting on condition [00000000]
          java.lang.Thread.State: RUNNABLE

        "Finalizer" daemon prio=8 tid=7ffd44095800 nid=0x10b54f000 in
Object.wait() [10b54e000]
          java.lang.Thread.State: WAITING (on object monitor)
               at java.lang.Object.wait(Native Method)
               - waiting on <7f44e10c0> (a
java.lang.ref.ReferenceQueue$Lock)
               at
java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
               - locked <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
               at
java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
               at
java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

        "Reference Handler" daemon prio=10 tid=7ffd44094800 nid=0x10b44c000
in Object.wait() [10b44b000]
          java.lang.Thread.State: WAITING (on object monitor)
               at java.lang.Object.wait(Native Method)
               - waiting on <7f44e1ff8> (a java.lang.ref.Reference$Lock)
               at java.lang.Object.wait(Object.java:485)
               at
java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
               - locked <7f44e1ff8> (a java.lang.ref.Reference$Lock)

        "main" prio=5 tid=7ffd44000800 nid=0x104aee000 waiting on condition
[104aed000]
          java.lang.Thread.State: TIMED_WAITING (parking)
               at sun.misc.Unsafe.park(Native Method)
               - parking to wait for  <7f44e2070> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
               at
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
               at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitN
anos(AbstractQueuedSynchronizer.java:2025)
               at
java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.
java:1253)
               at my.Prob.main(Prob.java:36)

        "VM Thread" prio=9 tid=7ffd42802800 nid=0x10b349000 runnable

        "Gang worker#0 (Parallel GC Threads)" prio=9 tid=7ffd44002000
nid=0x107e27000 runnable

        "Gang worker#1 (Parallel GC Threads)" prio=9 tid=7ffd44002800
nid=0x107f2a000 runnable

        "Concurrent Mark-Sweep GC Thread" prio=9 tid=7ffd4404d000
nid=0x10afef000 runnable
        "VM Periodic Task Thread" prio=10 tid=7ffd440a1000 nid=0x10bd40000
waiting on condition

        "Exception Catcher Thread" prio=10 tid=7ffd44001800 nid=0x104c50000
runnable
        JNI global references: 914

        Heap
         par new generation   total 19136K, used 5150K [7f3000000,
7f44c0000, 7f44c0000)
         eden space 17024K,  30% used [7f3000000, 7f35078b8, 7f40a0000)
         from space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
         to   space 2112K,   0% used [7f42b0000, 7f42b0000, 7f44c0000)
         concurrent mark-sweep generation total 63872K, used 379K
[7f44c0000, 7f8320000, 7fae00000)
         concurrent-mark-sweep perm gen total 21248K, used 4811K [7fae00000,
7fc2c0000, 800000000)

        99999990000000, terminated ok
        gwmac:~ gregg$ java -version
        java version "1.6.0_29"
        Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
        Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)
        gwmac:~ gregg$





      --
        -- Howard.


--------------------------------------------------------------------------
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





  --
    -- Howard.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120109/06d5a51d/attachment-0001.html>

From howard.lovatt at gmail.com  Sun Jan  8 19:09:29 2012
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Mon, 9 Jan 2012 11:09:29 +1100
Subject: [concurrency-interest] Nested synchronized
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEEJJCAA.davidcholmes@aapt.net.au>
References: <CACR_FB6Rh+xg9M0JYexL21j1q5O-4ZJanWwhYGe5DEvMnFAhSQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOEEJJCAA.davidcholmes@aapt.net.au>
Message-ID: <CACR_FB6LHLHo0+rx_3rkkjkbpNyLsm+UO=WecXGrdNVYHcmMqw@mail.gmail.com>

David,

I have tested on JDK7 from:

http://jdk7.java.net/macportpreview/

And JDK8 from:

http://www.concord.org/~sbannasch/mlvm/

No problems on either with either Heinz's code or my own.

Would still be nice to fix JDK6 on Mac since 7, none developer release, is
about 1 year away.

Thanks for the suggestion,

 -- Howard.

On 9 January 2012 09:30, David Holmes <davidcholmes at aapt.net.au> wrote:

> **
> Howard,
>
> There is a Mac OSX port  project for the OpenJDK
>
> http://openjdk.java.net/projects/macosx-port/
> https://wikis.oracle.com/display/OpenJDK/Mac+OS+X+Port+Project+Status
>
> it is in the process of being integrated with the OpenJDK 7u main
> development stream. The mercurial forest is at:
>
> http://hg.openjdk.java.net/jdk7u/jdk7u-osx/
>
> You might try with the latest JDK7 version if you can, to see if the bug
> still reproduces. If so then bugreport.sun.com is the place to file bugs
> against the OpenJDK 7 version:
>
> http://bugreport.sun.com/bugreport/
>
> I have no idea how bugs in the Apple 6uX versions are to be handled.
>
> You could also ask on the port mailing list as per the first link above.
>
> David
>
> -----Original Message-----
> *From:* Howard Lovatt [mailto:howard.lovatt at gmail.com]
> *Sent:* Sunday, 8 January 2012 9:14 PM
> *To:* Dr Heinz M. Kabutz; nathan.reynolds at oracle.com; Gregg Wonderly;
> concurrency-interest at cs.oswego.edu; dholmes at ieee.org; Jim McClure
> *Subject:* Re: [concurrency-interest] Nested synchronized
>
> Hi All,
>
> Thanks for everyones suggestions.
>
> Here is a brief summary of my latest experiments with this bug.
>
>    1. I cannot get JConsole to connect
>    2. Both Heinz's code and my code run fine on a core 2 duo Mac I have
>    (same Java version, same OS - see below)
>    3. Both Heinz's code and my code run fine on my MacBook
>    running Stephen Bannasch's MLVM, i.e. same machine. JVM version is:
>
>   gar-ln:dist lov080$ ~/Downloads/MLVM/1.7.0_2011_05_02/Home/bin/java
> -version
> openjdk version "1.7.0-internal"
> OpenJDK Runtime Environment (build
> 1.7.0-internal-stephen_2011_05_02_10_53-b00)
> OpenJDK 64-Bit Server VM (build 21.0-b07, mixed mode)
>
>      4. Both Heinz's code and my code run fine on my MacBook, i.e. same
> machine, running Windows 7 (via parallels). JVM version is:
>
>  Z:\Dropbox\Personal\Java\examples\NestedSynchronizedProblem\dist>java
> -version
>  java version "1.7.0_02"
>  Java(TM) SE Runtime Environment (build 1.7.0_02-b13)
>  Java HotSpot(TM) 64-Bit Server VM (build 22.0-b10, mixed mode)
>
>
> The only combination for which I can test that fails, for both Heinz's
> code and my code, is Mac OS 10.7.2 running:
>
>  gar-ln:src lov080$ java -version
>  java version "1.6.0_29"
>  Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
>  Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)
>
>
>
> Is there somewhere to log this bug formally, since I think it is over to
> Oracle and/or Apple on this one now?
>
> Thanks again,
>
>  -- Howard.
>
> On 7 January 2012 20:44, Dr Heinz M. Kabutz <heinz at javaspecialists.eu>wrote:
>
>> **
>> Yes, we also found this problem to be hardware related.  Here is a class
>> sent to me by Jim McClure that also causes a JVM deadlock on certain modern
>> i7 chips (not the older ones) on Mac OS X:
>>
>>  public class Foo {
>>
>> private int counter;
>>
>> public static void main(String[] args) throws Exception {
>> new Foo().run();
>> }
>>
>> public void run() throws Exception {
>>
>> Thread[] threads = new Thread[2];
>>
>> for (int i = 0; i < threads.length; i++) {
>> threads[i] = new Thread() {
>> public void run() {
>>
>> System.out.println("Thread started " + Thread.currentThread().getName());
>> System.out.flush();
>>
>> for (int i = 0; i < 100000; i++) {
>> increment();
>> }
>>
>> System.out.println("Thread done " + Thread.currentThread().getName());
>> System.out.flush();
>> }
>> };
>> }
>>
>> for (Thread t : threads) {
>> t.start();
>> }
>>
>> System.out.println("Started");
>> System.out.flush();
>>
>> System.out.println("Awaiting complete...");
>> System.out.flush();
>>
>> for (Thread t : threads) {
>> t.join();
>> }
>>
>> System.out.println("Completed...");
>> System.out.flush();
>> }
>>
>> private synchronized void increment() {
>>
>> counter++;
>>
>> if (counter % 10000 == 0) {
>> System.out.printf("%s: %d\n", Thread.currentThread().getName(), counter);
>> System.out.flush();
>> }
>> }
>> }
>>
>> Output:
>>
>>  Thread started Thread-1
>> Thread started Thread-2
>> Started
>> Awaiting complete...
>> Thread-2: 10000
>> Thread-2: 20000
>> Thread-1: 30000
>> Thread-2: 40000
>> Thread-1: 50000
>> Thread-2: 60000
>> Thread-2: 70000
>> Thread-2: 80000
>> Thread-1: 90000
>> Thread-1: 100000
>> Thread-1: 110000
>> Thread-1: 120000
>> Thread-1: 130000
>> Thread-1: 140000
>> Thread done Thread-1
>>
>>
>> Regards
>>
>> Heinz
>> --
>> Dr Heinz M. Kabutz (PhD CompSci)
>> Author of "The Java(tm) Specialists' Newsletter"
>> Sun Java Champion
>> IEEE Certified Software Development Professional
>> http://www.javaspecialists.eu
>> Tel: +30 69 72 850 460
>> Skype: kabutz
>>
>>
>>
>> On 1/7/12 4:49 AM, Howard Lovatt wrote:
>>
>>  Hi Gregg,
>>
>> Weird I have the same Java version, but for me it starts working (high
>> CPU usage) and then hangs (negligible CPU usage). Whilst it is running I
>> can get a stack trace, see below. But once it has hung I can't get a stack
>> trace and I have to kill it.
>>
>> Could it be a hardware variation; I have a 4-core i7 processor?
>>
>>    Model Name: MacBook Pro
>>
>>   Model Identifier: MacBookPro8,2
>>
>>   Processor Name: Intel Core i7
>>
>>   Processor Speed: 2.3 GHz
>>
>>   Number of Processors: 1
>>
>>   Total Number of Cores: 4
>>
>>   L2 Cache (per Core): 256 KB
>>
>>   L3 Cache: 8 MB
>>
>> Could it be a software variation; I have Lion?
>>
>>    System Version: Mac OS X 10.7.2 (11C74)
>>
>>   Kernel Version: Darwin 11.2.0
>>
>>   Boot Volume: sunzero-ln
>>
>>   Boot Mode: Normal
>>
>>   Computer Name: sunzero-ln
>>
>>   User Name: Howard Lovatt (lov080)
>>
>>   Secure Virtual Memory: Enabled
>>
>>   64-bit Kernel and Extensions: Yes
>>
>>   Time since boot: 29 days 4:35
>>
>>
>> Thanks for running the code.
>>
>>  -- Howard
>>
>> ===============================================
>>
>>  gar-ln:src lov080$ java
>> nestedsynchronizedproblem.NestedSynchronizedProblem
>> ^\2012-01-06 17:17:34
>> Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed
>> mode):
>>
>> "pool-1-thread-2" prio=5 tid=7fb8fb291800 nid=0x113dfd000 runnable
>> [113dfc000]
>>    java.lang.Thread.State: RUNNABLE
>> at java.lang.Thread.isInterrupted(Native Method)
>> at java.lang.Thread.interrupted(Thread.java:934)
>> at
>> nestedsynchronizedproblem.ParrallelSumMethod.checkForInterrupt(NestedSynchronizedProblem.java:68)
>> at
>> nestedsynchronizedproblem.ParrallelSumMethod.setA2(NestedSynchronizedProblem.java:61)
>> at
>> nestedsynchronizedproblem.NestedSynchronizedProblem$2.call(NestedSynchronizedProblem.java:31)
>> at
>> nestedsynchronizedproblem.NestedSynchronizedProblem$2.call(NestedSynchronizedProblem.java:29)
>> at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>> at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>> at
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>> at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>> at java.lang.Thread.run(Thread.java:680)
>>
>> "pool-1-thread-1" prio=5 tid=7fb8fb290800 nid=0x113cfa000 waiting for
>> monitor entry [113cf9000]
>>    java.lang.Thread.State: BLOCKED (on object monitor)
>> at
>> nestedsynchronizedproblem.ParrallelSumMethod.isSetA1(NestedSynchronizedProblem.java:72)
>> - locked <7f42b20a0> (a nestedsynchronizedproblem.ParrallelSumMethod)
>> at
>> nestedsynchronizedproblem.ParrallelSumMethod.setA1(NestedSynchronizedProblem.java:53)
>> at
>> nestedsynchronizedproblem.NestedSynchronizedProblem$1.call(NestedSynchronizedProblem.java:25)
>> at
>> nestedsynchronizedproblem.NestedSynchronizedProblem$1.call(NestedSynchronizedProblem.java:23)
>> at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>> at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>> at
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>> at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>> at java.lang.Thread.run(Thread.java:680)
>>
>> "Low Memory Detector" daemon prio=5 tid=7fb8fb15a000 nid=0x11395d000
>> runnable [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "C2 CompilerThread1" daemon prio=9 tid=7fb8fb159800 nid=0x11385a000
>> waiting on condition [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "C2 CompilerThread0" daemon prio=9 tid=7fb8fb158800 nid=0x113757000
>> waiting on condition [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "Signal Dispatcher" daemon prio=9 tid=7fb8fb158000 nid=0x113654000
>> waiting on condition [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7fb8fb157000
>> nid=0x113551000 waiting on condition [00000000]
>>    java.lang.Thread.State: RUNNABLE
>>
>> "Finalizer" daemon prio=8 tid=7fb8fb13f000 nid=0x11328d000 in
>> Object.wait() [11328c000]
>>    java.lang.Thread.State: WAITING (on object monitor)
>> at java.lang.Object.wait(Native Method)
>> - waiting on <7f42b42a0> (a java.lang.ref.ReferenceQueue$Lock)
>> at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
>> - locked <7f42b42a0> (a java.lang.ref.ReferenceQueue$Lock)
>> at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
>> at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
>>
>> "Reference Handler" daemon prio=10 tid=7fb8fb13e800 nid=0x11318a000 in
>> Object.wait() [113189000]
>>    java.lang.Thread.State: WAITING (on object monitor)
>> at java.lang.Object.wait(Native Method)
>> - waiting on <7f42b0100> (a java.lang.ref.Reference$Lock)
>> at java.lang.Object.wait(Object.java:485)
>> at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
>> - locked <7f42b0100> (a java.lang.ref.Reference$Lock)
>>
>> "main" prio=5 tid=7fb8fb001000 nid=0x10b414000 waiting on condition
>> [10b413000]
>>    java.lang.Thread.State: TIMED_WAITING (parking)
>> at sun.misc.Unsafe.park(Native Method)
>> - parking to wait for  <7f42bc010> (a
>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
>> at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
>> at
>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
>> at
>> java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1253)
>> at
>> nestedsynchronizedproblem.NestedSynchronizedProblem.main(NestedSynchronizedProblem.java:39)
>>
>> "VM Thread" prio=9 tid=7fb8fb13a000 nid=0x113087000 runnable
>>
>> "Gang worker#0 (Parallel GC Threads)" prio=9 tid=7fb8fb002800
>> nid=0x10e74d000 runnable
>>
>> "Gang worker#1 (Parallel GC Threads)" prio=9 tid=7fb8fb003000
>> nid=0x10e850000 runnable
>>
>> "Gang worker#2 (Parallel GC Threads)" prio=9 tid=7fb8fb003800
>> nid=0x10e953000 runnable
>>
>> "Gang worker#3 (Parallel GC Threads)" prio=9 tid=7fb8fb004000
>> nid=0x10ea56000 runnable
>>
>> "Gang worker#4 (Parallel GC Threads)" prio=9 tid=7fb8fb005000
>> nid=0x10eb59000 runnable
>>
>> "Gang worker#5 (Parallel GC Threads)" prio=9 tid=7fb8fb005800
>> nid=0x10ec5c000 runnable
>>
>> "Gang worker#6 (Parallel GC Threads)" prio=9 tid=7fb8fb006000
>> nid=0x10ed5f000 runnable
>>
>> "Gang worker#7 (Parallel GC Threads)" prio=9 tid=7fb8fb006800
>> nid=0x10ee62000 runnable
>>
>> "Concurrent Mark-Sweep GC Thread" prio=9 tid=7fb8fb0e4000 nid=0x112d2d000
>> runnable
>> "Gang worker#0 (Parallel CMS Threads)" prio=9 tid=7fb8fb0e3000
>> nid=0x112327000 runnable
>>
>> "Gang worker#1 (Parallel CMS Threads)" prio=9 tid=7fb8fb0e3800
>> nid=0x11242a000 runnable
>>
>> "VM Periodic Task Thread" prio=10 tid=7fb8fb16b800 nid=0x113a60000
>> waiting on condition
>>
>> "Exception Catcher Thread" prio=10 tid=7fb8fb001800 nid=0x10b576000
>> runnable
>> JNI global references: 906
>>
>> Heap
>>  par new generation   total 19136K, used 1211K [7f3000000, 7f44c0000,
>> 7f44c0000)
>>   eden space 17024K,   4% used [7f3000000, 7f30d16f8, 7f40a0000)
>>   from space 2112K,  17% used [7f42b0000, 7f430d720, 7f44c0000)
>>   to   space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
>>  concurrent mark-sweep generation total 63872K, used 0K [7f44c0000,
>> 7f8320000, 7fae00000)
>>  concurrent-mark-sweep perm gen total 21248K, used 4769K [7fae00000,
>> 7fc2c0000, 800000000)
>>
>> Killed: 9
>>
>>
>> On 6 January 2012 15:51, Gregg Wonderly <gergg at cox.net> wrote:
>>
>>> On my mac, it seems to complete successfully.  I've included a stack
>>> dump just FYI.
>>>
>>> Gregg Wonderly
>>>
>>> gwmac:~ gregg$ javac -d . Prob.java
>>> gwmac:~ gregg$ java my.Prob
>>> ^\
>>> 2012-01-05 22:48:09
>>> Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.4-b02-402 mixed
>>> mode):
>>>
>>> "pool-1-thread-2" prio=5 tid=7ffd42969800 nid=0x10c132000 runnable
>>> [10c131000]
>>>   java.lang.Thread.State: RUNNABLE
>>>        at my.ParrallelSumMethod.conditionallySumArguments(Prob.java:87)
>>>        - locked <7f44e1400> (a my.ParrallelSumMethod)
>>>        at my.ParrallelSumMethod.isSetA2(Prob.java:80)
>>>        - locked <7f44e1400> (a my.ParrallelSumMethod)
>>>        at my.ParrallelSumMethod.setA2(Prob.java:57)
>>>        at my.Prob$2.call(Prob.java:28)
>>>        at my.Prob$2.call(Prob.java:26)
>>>        at
>>> java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>>>        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>>>        at
>>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>>>        at
>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>>>        at java.lang.Thread.run(Thread.java:680)
>>>
>>> "pool-1-thread-1" prio=5 tid=7ffd4287f000 nid=0x10c02f000 runnable
>>> [10c02e000]
>>>   java.lang.Thread.State: RUNNABLE
>>>        at java.lang.Thread.isInterrupted(Native Method)
>>>        at java.lang.Thread.interrupted(Thread.java:934)
>>>        at my.ParrallelSumMethod.checkForInterrupt(Prob.java:65)
>>>        at my.ParrallelSumMethod.setA1(Prob.java:51)
>>>        at my.Prob$1.call(Prob.java:22)
>>>        at my.Prob$1.call(Prob.java:20)
>>>        at
>>> java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
>>>        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
>>>        at
>>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>>>        at
>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>>>        at java.lang.Thread.run(Thread.java:680)
>>>
>>> "Low Memory Detector" daemon prio=5 tid=7ffd4409f000 nid=0x10bc3d000
>>> runnable [00000000]
>>>   java.lang.Thread.State: RUNNABLE
>>>
>>> "C2 CompilerThread1" daemon prio=9 tid=7ffd4409e800 nid=0x10bb3a000
>>> waiting on condition [00000000]
>>>   java.lang.Thread.State: RUNNABLE
>>>
>>> "C2 CompilerThread0" daemon prio=9 tid=7ffd4409d800 nid=0x10ba37000
>>> waiting on condition [00000000]
>>>   java.lang.Thread.State: RUNNABLE
>>>
>>> "Signal Dispatcher" daemon prio=9 tid=7ffd4409d000 nid=0x10b934000
>>> waiting on condition [00000000]
>>>   java.lang.Thread.State: RUNNABLE
>>>
>>> "Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7ffd4409c000
>>> nid=0x10b831000 waiting on condition [00000000]
>>>   java.lang.Thread.State: RUNNABLE
>>>
>>> "Finalizer" daemon prio=8 tid=7ffd44095800 nid=0x10b54f000 in
>>> Object.wait() [10b54e000]
>>>   java.lang.Thread.State: WAITING (on object monitor)
>>>        at java.lang.Object.wait(Native Method)
>>>        - waiting on <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
>>>        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
>>>        - locked <7f44e10c0> (a java.lang.ref.ReferenceQueue$Lock)
>>>        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
>>>        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
>>>
>>> "Reference Handler" daemon prio=10 tid=7ffd44094800 nid=0x10b44c000 in
>>> Object.wait() [10b44b000]
>>>   java.lang.Thread.State: WAITING (on object monitor)
>>>        at java.lang.Object.wait(Native Method)
>>>        - waiting on <7f44e1ff8> (a java.lang.ref.Reference$Lock)
>>>        at java.lang.Object.wait(Object.java:485)
>>>        at
>>> java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
>>>        - locked <7f44e1ff8> (a java.lang.ref.Reference$Lock)
>>>
>>> "main" prio=5 tid=7ffd44000800 nid=0x104aee000 waiting on condition
>>> [104aed000]
>>>   java.lang.Thread.State: TIMED_WAITING (parking)
>>>        at sun.misc.Unsafe.park(Native Method)
>>>        - parking to wait for  <7f44e2070> (a
>>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
>>>        at
>>> java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
>>>        at
>>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
>>>        at
>>> java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1253)
>>>        at my.Prob.main(Prob.java:36)
>>>
>>> "VM Thread" prio=9 tid=7ffd42802800 nid=0x10b349000 runnable
>>>
>>> "Gang worker#0 (Parallel GC Threads)" prio=9 tid=7ffd44002000
>>> nid=0x107e27000 runnable
>>>
>>> "Gang worker#1 (Parallel GC Threads)" prio=9 tid=7ffd44002800
>>> nid=0x107f2a000 runnable
>>>
>>> "Concurrent Mark-Sweep GC Thread" prio=9 tid=7ffd4404d000
>>> nid=0x10afef000 runnable
>>> "VM Periodic Task Thread" prio=10 tid=7ffd440a1000 nid=0x10bd40000
>>> waiting on condition
>>>
>>> "Exception Catcher Thread" prio=10 tid=7ffd44001800 nid=0x104c50000
>>> runnable
>>> JNI global references: 914
>>>
>>> Heap
>>>  par new generation   total 19136K, used 5150K [7f3000000, 7f44c0000,
>>> 7f44c0000)
>>>  eden space 17024K,  30% used [7f3000000, 7f35078b8, 7f40a0000)
>>>  from space 2112K,   0% used [7f40a0000, 7f40a0000, 7f42b0000)
>>>  to   space 2112K,   0% used [7f42b0000, 7f42b0000, 7f44c0000)
>>>  concurrent mark-sweep generation total 63872K, used 379K [7f44c0000,
>>> 7f8320000, 7fae00000)
>>>  concurrent-mark-sweep perm gen total 21248K, used 4811K [7fae00000,
>>> 7fc2c0000, 800000000)
>>>
>>> 99999990000000, terminated ok
>>> gwmac:~ gregg$ java -version
>>> java version "1.6.0_29"
>>> Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
>>> Java HotSpot(TM) 64-Bit Server VM (build 20.4-b02-402, mixed mode)
>>> gwmac:~ gregg$
>>
>>
>>
>>
>> --
>>   -- Howard.
>>
>> ------------------------------
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
>   -- Howard.
>
>


-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120109/284f20b4/attachment-0001.html>

From nathan.reynolds at oracle.com  Mon Jan  9 12:22:30 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 09 Jan 2012 10:22:30 -0700
Subject: [concurrency-interest] Non Blocking java.security.Policy -
 synchronized method is	superclass.
In-Reply-To: <1326022813.20976.30.camel@bluto>
References: <1326022813.20976.30.camel@bluto>
Message-ID: <4F0B2256.4010802@oracle.com>

 > How much can this one synchronized method spoil scalability?

Depends upon the workload.  Some workloads will never hit the method and 
hence will never have a scalability problem.  Other workloads will hit 
the method but not heavily enough to be a concern.  A contrived workload 
which launches a bunch of threads and simply call this method in a tight 
loop will not scale.  So, the question really is: Is there a real 
workload out there that won't scale due to this method?

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/8/2012 4:40 AM, Peter Firmstone wrote:
> Appended is a new java.security.Policy implementation, it fully supports
> the existing java policy syntax and accepts alternate PolicyParser's.
>
> All state is immutable, except for 2 volatile references, referents
> replaced, not mutated, when the policy is updated.  One referent is an
> array containing PermissionGrant's (interface for immutable object
> representing a grant statement in a policy), the second a
> PermissionCollection containing the Policy Permissions.  The array is
> never mutated after creation, a reference to the array is copied before
> accessing the array or any array methods.
>
> The policy creates PermissionCollection's on demand for checking,
> Permission's are ordered using a PermissionComparator to ensure that for
> example, wildcard SocketPermission's are checked first, to avoid
> unnecessary DNS lookups.  Only the permission being checked and any
> UnresolvedPermission's are added to the PermissionCollection, limiting
> the size of the objects created.
>
> In existing policy implementations PermissionCollection's perform
> blocking operations.
>
> Also, after parsing policy files, PermissionGrant implementations avoid
> the need to open files or network connections to confirm URL's, eg
> CodeSource.implies is not called, but instead reimplemented using URI.
>
> Will this scale?  There but one smell:
>
> ProtectionDomain uses a synchronized method Policy.getPolicyNoCheck(),
> but this only retrieves a reference on 99% of occasions.
>
> For every permission check, the stack access control context is
> retrieved, every ProtectionDomain on the stack must be checked,
> ProtectionDomain's must call getPolicyNoCheck() to call
> Policy.implies(ProtectionDomain domain, Permission permission).
>
> To make this worse, I've got a SecurityManager that divides the
> ProtectionDomain.implies() calls into tasks and submits them to an
> executor (if there are 4 or more PD's in a context).  The
> SecurityManager is also non blocking, at least it will be when I use the
> new ConcurrentHashMap for the checked permission cache (avoids repeated
> security checks), for now the cache is implemented using the existing
> ConcurrentHashMap, but is mostly read in any case.  (P.S. This is the
> cache I'm using the Reference Collection's for.)
>
> How much can this one synchronized method spoil scalability?
>
> Cheers&  thanks in advance,
>
> Peter.
>
>
>
> /*
> *  Licensed to the Apache Software Foundation (ASF) under one or more
> *  contributor license agreements.  See the NOTICE file distributed with
> *  this work for additional information regarding copyright ownership.
> *  The ASF licenses this file to You under the Apache License, Version
> 2.0
> *  (the "License"); you may not use this file except in compliance with
> *  the License.  You may obtain a copy of the License at
> *
> *     http://www.apache.org/licenses/LICENSE-2.0
> *
> *  Unless required by applicable law or agreed to in writing, software
> *  distributed under the License is distributed on an "AS IS" BASIS,
> *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> implied.
> *  See the License for the specific language governing permissions and
> *  limitations under the License.
> */
>
> /**
>   * Default Policy implementation taken from Apache Harmony, refactored
> for
>   * concurrency.
>   *
>   * @author Alexey V. Varlamov
>   * @author Peter Firmstone
>   * @version $Revision$
>   */
>
> package net.jini.security.policy;
>
> import java.io.File;
> import java.net.URL;
> import java.security.AccessController;
> import java.security.AllPermission;
> import java.security.CodeSource;
> import java.security.Guard;
> import java.security.Permission;
> import java.security.PermissionCollection;
> import java.security.Permissions;
> import java.security.Policy;
> import java.security.PrivilegedActionException;
> import java.security.PrivilegedExceptionAction;
> import java.security.ProtectionDomain;
> import java.security.SecurityPermission;
> import java.security.UnresolvedPermission;
> import java.util.ArrayList;
> import java.util.Collection;
> import java.util.Enumeration;
> import java.util.Iterator;
> import java.util.List;
> import java.util.NavigableSet;
> import java.util.Properties;
> import java.util.TreeSet;
> import net.jini.security.PermissionComparator;
> import org.apache.river.api.security.PermissionGrant;
> import org.apache.river.impl.security.policy.util.DefaultPolicyParser;
> import org.apache.river.impl.security.policy.util.PolicyParser;
> import org.apache.river.impl.security.policy.util.PolicyUtils;
>
>
> /**
> * Concurrent Policy implementation based on policy configuration files,
> * it is intended to provide concurrent implies() for greatly improved
> * throughput at the expense of single threaded performance.
> *
> * Set the following system properties to use this Policy instead of the
> * built in Java sun.security.provider.PolicyFile:
> * * net.jini.security.policy.PolicyFileProvider.basePolicyClass =
> * org.apache.river.security.concurrent.ConcurrentPolicyFile
> *
> *
> * This
> * implementation recognizes text files, consisting of clauses with the
> * following syntax:
> *
> *<pre>
> * keystore&quot;some_keystore_url&quot; [,&quot;keystore_type&quot;];
> *</pre>
> <pre>
> * grant [SignedBy&quot;signer_names&quot;] [, CodeBase&quot;URL&quot;]
> *  [, Principal [principal_class_name]&quot;principal_name&quot;]
> *  [, Principal [principal_class_name]&quot;principal_name&quot;] ... {
> *  permission permission_class_name [&quot;target_name&quot; ] [,
> &quot;action&quot;]
> *  [, SignedBy&quot;signer_names&quot;];
> *  permission ...
> *  };
> * *</pre>
> *
> * The<i>keystore</i>  clause specifies reference to a keystore, which
> is a
> * database of private keys and their associated digital certificates.
> The
> * keystore is used to look up the certificates of signers specified in
> the
> *<i>grant</i>  entries of the file. The policy file can contain any
> number of
> *<i>keystore</i>  entries which can appear at any ordinal position.
> However,
> * only the first successfully loaded keystore is used, others are
> ignored. The
> * keystore must be specified if some grant clause refers to a
> certificate's
> * alias.<br>
> * The<i>grant</i>  clause associates a CodeSource (consisting of an URL
> and a
> * set of certificates) of some executable code with a set of Permissions
> which
> * should be granted to the code. So, the CodeSource is defined by values
> of
> *<i>CodeBase</i>  and<i>SignedBy</i>  fields. The<i>CodeBase</i>
> value must
> * be in URL format, while<i>SignedBy</i>  value is a (comma-separated
> list of)
> * alias(es) to keystore certificates. These fields can be omitted to
> denote any
> * codebase and any signers (including case of unsigned code),
> respectively.
> *<br>
> * Also, the code may be required to be executed on behalf of some
> Principals
> * (in other words, code's ProtectionDomain must have the array of
> Principals
> * associated) in order to possess the Permissions. This fact is
> indicated by
> * specifying one or more<i>Principal</i>  fields in the<i>grant</i>
> clause.
> * Each Principal is specified as class/name pair; name and class can be
> either
> * concrete value or wildcard<i>*</i>. As a special case, the class
> value may
> * be omitted and then the name is treated as an alias to X.509
> Certificate, and
> * the Principal is assumed to be javax.security.auth.x500.X500Principal
> with a
> * name of subject's distinguished name from the certificate.<br>
> * The order between the<i>CodeBase</i>,<i>SignedBy</i>, and
> <i>Principal
> *</i>  fields does not matter. The policy file can contain any number of
> grant
> * clauses.<br>
> * Each<i>grant</i>  clause must contain one or more<i>permission</i>
> entry.
> * The permission entry consist of a fully qualified class name along
> with
> * optional<i>name</i>,<i>actions</i>  and<i>signedby</i>  values.
> Name and
> * actions are arguments to the corresponding constructor of the
> permission
> * class. SignedBy value represents the keystore alias(es) to
> certificate(s)
> * used to sign the permission class. That is, this permission entry is
> * effective (i.e., access control permission will be granted based on
> this
> * entry) only if the bytecode implementation of permission class is
> verified to
> * be correctly signed by the said alias(es).<br>
> *<br>
> * The policy content may be parameterized via property expansion.
> Namely,
> * expressions like<i>${key}</i>  are replaced by values of
> corresponding
> * system properties. Also, the special<i>slash</i>  key (i.e. ${/}) is
> * supported, it is a shortcut to&quot;file.separator&quot; key.
> Property
> * expansion is performed anywhere a double quoted string is allowed in
> the
> * policy file. However, this feature is controlled by security
> properties and
> * should be turned on by setting&quot;policy.expandProperties&quot;
> property
> * to<i>true</i>.<br>
> * If property expansion fails (due to a missing key), a corresponding
> entry is
> * ignored. For fields of<i>keystore</i>  and<i>grant</i>  clauses, the
> whole
> * clause is ignored, and for<i>permission</i>  entry, only that entry
> is
> * ignored.<br>
> *<br>
> * The policy also supports generalized expansion in permissions names,
> of
> * expressions like<i>${{protocol:data}}</i>. Currently the following
> * protocols supported:
> *<dl>
> *<dt>self
> *<dd>Denotes substitution to a principal information of the parental
> Grant
> * entry. Replaced by a space-separated list of resolved Principals
> (including
> * wildcarded), each formatted as<i>class&quot;name&quot;</i>. If
> parental
> * Grant entry has no Principals, the permission is ignored.
> *<dt>alias:<i>name</i>
> *<dd>Denotes substitution of a KeyStore alias. Namely, if a KeyStore
> has an
> * X.509 certificate associated with the specified name, then replaced by
> *<i>javax.security.auth.x500.X500Principal&quot;<i>DN</i>&quot;</i>
> * string, where<i>DN</i>  is a certificate's subject distinguished
> name.
> *</dl>
> *<br>
> *
> */
>
> public class ConcurrentPolicyFile extends Policy implements
> ConcurrentPolicy {
>
>     /**
>      * System property for dynamically added policy location.
>      */
>     private static final String JAVA_SECURITY_POLICY =
> "java.security.policy"; //$NON-NLS-1$
>
>     /**
>      * Prefix for numbered Policy locations specified in
> security.properties.
>      */
>     private static final String POLICY_URL_PREFIX = "policy.url.";
> //$NON-NLS-1$
>       // Reference must be defensively copied before access, once
> published, never mutated.
>     private volatile PermissionGrant [] grantArray;
>       // A specific parser for a particular policy file format.
>     private final PolicyParser parser;
>       private static final Guard guard = new
> SecurityPermission("getPolicy");
>       private final ProtectionDomain myDomain;
>       // reference must be defensively copied before access, once
> published, never mutated.
>     private volatile PermissionCollection myPermissions;
>       /**
>      * Default constructor, equivalent to
>      *<code>ConcurrentPolicyFile(new DefaultPolicyParser())</code>.
>      */
>     public ConcurrentPolicyFile() throws PolicyInitializationException {
>         this(new DefaultPolicyParser());
>     }
>
>     /**
>      * Extension constructor for plugging-in a custom parser.
>      * @param dpr
>      */
>     protected ConcurrentPolicyFile(PolicyParser dpr) throws
> PolicyInitializationException {
>         guard.checkGuard(null);
>         parser = dpr;
>         myDomain = this.getClass().getProtectionDomain();
>         /*
>          * The bootstrap policy makes implies decisions until this
> constructor
>          * has returned.  We don't need to lock.
>          */
>         try {
>             // Bug 4911907, do we need to do anything more?
>             // The permissions for this domain must be retrieved before
>             // construction is complete and this policy takes over.
>             initialize(); // Instantiates myPermissions.
>         } catch (SecurityException e){
>             throw e;
>         } catch (Exception e){
>             throw new PolicyInitializationException("PolicyInitialization
> failed", e);
>         }
>     }
>       private PermissionCollection convert(NavigableSet<Permission>
> permissions){
>         PermissionCollection pc = new Permissions();
>         // The descending iterator is for SocketPermission.
>         Iterator<Permission>  it = permissions.descendingIterator();
>         while (it.hasNext()) {
>             pc.add(it.next());
>         }
>         return pc;
>     }
>
>     /**
>      * Returns collection of permissions allowed for the domain
>      * according to the policy. The evaluated characteristics of the
>      * domain are it's codesource and principals; they are assumed
>      * to be<code>null</code>  if the domain is<code>null</code>.
>      *
>      * Each PermissionCollection returned is a unique instance.
>      *
>      * @param pd ProtectionDomain
>      * @see ProtectionDomain
>      */
>     @Override
>     public PermissionCollection getPermissions(ProtectionDomain pd) {
>         NavigableSet<Permission>  perms = new TreeSet<Permission>(new
> PermissionComparator());
>         PermissionGrant [] grantRefCopy = grantArray;
>         int l = grantRefCopy.length;
>         for ( int j =0; j<  l; j++ ){
>             PermissionGrant ge = grantRefCopy[j];
>             if (ge.implies(pd)){
>                 if (ge.isPrivileged()){// Don't stuff around finish early
> if you can.
>                     PermissionCollection pc = new Permissions();
>                     pc.add(new AllPermission());
>                     return pc;
>                 }
>                 Collection<Permission>  c = ge.getPermissions();
>                 Iterator<Permission>  i = c.iterator();
>                 while (i.hasNext()){
>                     Permission p = i.next();
>                     perms.add(p);
>                 }
>             }
>         }
>         // Don't forget to merge the static Permissions.
>         PermissionCollection staticPC = null;
>         if (pd != null) {
>             staticPC = pd.getPermissions();
>             if (staticPC != null){
>                 Enumeration<Permission>  e = staticPC.elements();
>                 while (e.hasMoreElements()){
>                     Permission p = e.nextElement();
>                     if (p instanceof AllPermission) {
>                         PermissionCollection pc = new Permissions();
>                         pc.add(p);
>                         return pc;
>                     }
>                     perms.add(p);
>                 }
>             }
>         }
>         return convert(perms);
>     }
>
>     /**
>      * Returns collection of permissions allowed for the codesource
>      * according to the policy.
>      * The evaluation assumes that current principals are undefined.
>      *
>      * This returns a java.security.Permissions collection, which allows
>      * ProtectionDomain to optimise for the AllPermission case, which
> avoids
>      * unnecessarily consulting the policy.
>      *
>      * If constructed with the four argument constructor,
> ProtectionDomain.implies
>      * first consults the Policy, then it's own internal Permissions
> collection,
>      * unless it has AllPermission, in which case it returns true without
>      * consulting the policy.
>      *
>      * @param cs CodeSource
>      * @see CodeSource
>      */
>     @Override
>     public PermissionCollection getPermissions(CodeSource cs) {
>         if (cs == null) throw new NullPointerException("CodeSource cannot
> be null");
>         NavigableSet<Permission>  perms = new TreeSet<Permission>(new
> PermissionComparator());
>         // for ProtectionDomain AllPermission optimisation.
>         PermissionGrant [] grantRefCopy = grantArray;
>         int l = grantRefCopy.length;
>         for ( int j =0; j<  l; j++ ){
>             PermissionGrant ge = grantRefCopy[j];
>             if (ge.implies(cs, null)){ // No Principal's
>                 if (ge.isPrivileged()){// Don't stuff around finish early
> if you can.
>                     PermissionCollection pc = new Permissions();
>                     pc.add(new AllPermission());
>                     return pc;
>                 }
>                 Collection<Permission>  c = ge.getPermissions();
>                 Iterator<Permission>  i = c.iterator();
>                 while (i.hasNext()){
>                     Permission p = i.next();
>                     perms.add(p);
>                 }
>             }
>         }
>         return convert(perms);
>     }
>       @Override
>     public boolean implies(ProtectionDomain domain, Permission
> permission) {
>         if (permission == null) throw new
> NullPointerException("permission not allowed to be null");
>         if (domain == myDomain) {
>             PermissionCollection pc = myPermissions;
>             return pc.implies(permission);
>         }
>         Class klass = permission.getClass();
>         // Need to have a list of Permission's we can sort if permission
> is SocketPermission.
>         NavigableSet<Permission>  perms = new TreeSet<Permission>(new
> PermissionComparator());
>         PermissionGrant [] grantRefCopy = grantArray;
>         int l = grantRefCopy.length;
>         for ( int j =0; j<  l; j++ ){
>             PermissionGrant ge = grantRefCopy[j];
>             if (ge.implies(domain)){
>                 if (ge.isPrivileged()) return true; // Don't stuff around
> finish early if you can.
>                 Collection<Permission>  c = ge.getPermissions();
>                 Iterator<Permission>  i = c.iterator();
>                 while (i.hasNext()){
>                     Permission p = i.next();
>                     // Don't make it larger than necessary.
>                     if (klass.isInstance(permission) || permission
> instanceof UnresolvedPermission){
>                         perms.add(p);
>                     }
>                 }
>             }
>         }
>         // Don't forget to merge the static Permissions.
>         PermissionCollection staticPC = null;
>         if (domain != null) {
>             staticPC =domain.getPermissions();
>             if (staticPC != null){
>                 Enumeration<Permission>  e = staticPC.elements();
>                 while (e.hasMoreElements()){
>                     Permission p = e.nextElement();
>                     // return early if possible.
>                     if (p instanceof AllPermission ) return true;
>                     // Don't make it larger than necessary, but don't
> worry about duplicates either.
>                     if (klass.isInstance(permission) || permission
> instanceof UnresolvedPermission){
>                         perms.add(p);
>                     }
>                 }
>             }
>         }
>         return convert(perms).implies(permission);
>     }
>
>     /**
>      * Gets fresh list of locations and tries to load all of them in
> sequence;
>      * failed loads are ignored. After processing all locations, old
> policy
>      * settings are discarded and new ones come into force.<br>
>      *
>      * @see PolicyUtils#getPolicyURLs(Properties, String, String)
>      */
>     @Override
>     public void refresh() {
>         try {
>             initialize();
>         } catch (Exception ex) {
>             System.err.println(ex);
>         }
>     }
>       private void initialize() throws Exception{
>         try {
>             Collection<PermissionGrant>  fresh =
> AccessController.doPrivileged(
>                 new
> PrivilegedExceptionAction<Collection<PermissionGrant>>(){
>                     public Collection<PermissionGrant>  run() throws
> SecurityException {
>                         Collection<PermissionGrant>  fresh = new
> ArrayList<PermissionGrant>(120);
>                         Properties system = System.getProperties();
>                         system.setProperty("/", File.separator);
> //$NON-NLS-1$
>                         URL[] policyLocations =
> PolicyUtils.getPolicyURLs(system,
>
> JAVA_SECURITY_POLICY,
>
> POLICY_URL_PREFIX);
>                         int l = policyLocations.length;
>                         for (int i = 0; i<  l; i++) {
>                             //TODO debug log
> //                                System.err.println("Parsing policy
> file: " + policyLocations[i]);
>                             try {
>                                 Collection<PermissionGrant>  pc = null;
>                                 pc = parser.parse(policyLocations[i],
> system);
>                                 fresh.addAll(pc);
>                             } catch (Exception e){
>                                 // It's best to let a SecurityException
> bubble up
>                                 // in case there is a problem with our
> policy configuration
>                                 // or implementation.
>                                 if ( e instanceof SecurityException ) {
>                                     e.printStackTrace(System.out);
>                                     throw (SecurityException) e;
>                                 }
>                                 // ignore.
>                             }
>                         }
>                         return fresh;
>                     }
>                 }
>             );
>             // Volatile reference, publish after mutation complete.
>             grantArray = fresh.toArray(new
> PermissionGrant[fresh.size()]);
>             myPermissions = getPermissions(myDomain);
>         }catch (PrivilegedActionException e){
>             Throwable t = e.getCause();
>             if ( t instanceof Exception ) throw (Exception) t;
>             throw e;
>         }
>     }
>
>     public boolean isConcurrent() {
>         return true;
>     }
>
>     public PermissionGrant[] getPermissionGrants() {
>         PermissionGrant [] grants = grantArray; // copy volatile
> reference target.
>         return grants.clone();
>     }
>       public PermissionGrant[] getPermissionGrants(ProtectionDomain pd) {
>         PermissionGrant [] grants = grantArray; // copy volatile
> reference target.
>         int l = grants.length;
>         List<PermissionGrant>  applicable = new
> ArrayList<PermissionGrant>(l); // Always too large, never too small.
>         for (int i =0; i<  l; i++){
>             if (grants[i].implies(pd)){
>                 applicable.add(grants[i]);
>             }
>         }
>         return applicable.toArray(new
> PermissionGrant[applicable.size()]);
>     }
>
> }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120109/ddeebd9a/attachment-0001.html>

From peter.firmstone at zeus.net.au  Mon Jan  9 17:55:30 2012
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Tue, 10 Jan 2012 08:55:30 +1000
Subject: [concurrency-interest] Non Blocking java.security.Policy
	-	synchronized method is	superclass.
In-Reply-To: <4F0B2256.4010802@oracle.com>
References: <1326022813.20976.30.camel@bluto>  <4F0B2256.4010802@oracle.com>
Message-ID: <1326149729.20976.44.camel@bluto>

It's not causing any noticeable problems for me on 4 cpu's, looks like
it caused enough contention for someone to report it though, it's fixed
in Java 8.

We have a new SecurityManager that checks ProtectionDomain's in the
AccessControlContext in parallel, rather than in series, if there are
ten ProtectionDomain's on the stack, that would increase the number of
threads accessing the lock by a factor of ten.

So the potentials certainly there.

-Peter.

7093090 
*Votes* 0 
*Synopsis* Reduce synchronization in
java.security.Policy.getPolicyNoCheck * 
Category* java:classes_security 
*Reported Against* *Release Fixed* 8(b15) * 
State* 10-Fix Delivered, bug *Priority:* 2-High *Related Bugs* *Submit
Date* 20-SEP-2011 *Description*

java.security.Policy.getPolicyNoCheck() is synchronized which causes
some thread contention. 
Posted Date : 2011-09-20 23:44:03.0 

*Work Around*

N/A 

*Evaluation*

The fix involved adding an initialized flag to indicate when the
system-wide 
policy has been initialized and storing both the flag and the Policy
object in 
an AtomicReference. Then, I also used the double-check locking idiom to
avoid 
locking the Policy class when the Policy had already been initialized. 

Changeset: http://hg.openjdk.java.net/jdk8/tl/jdk/rev/1945abeb82a0
Posted Date : 2011-11-22 15:15:14.0 
On Tue, 2012-01-10 at 03:22, Nathan Reynolds wrote:
> > How much can this one synchronized method spoil scalability? 
> 
> Depends upon the workload.  Some workloads will never hit the method
> and hence will never have a scalability problem.  Other workloads will
> hit the method but not heavily enough to be a concern.  A contrived
> workload which launches a bunch of threads and simply call this method
> in a tight loop will not scale.  So, the question really is: Is there
> a real workload out there that won't scale due to this method?
> 
> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
> Oracle PSR Engineering | Server Technology
> 
> On 1/8/2012 4:40 AM, Peter Firmstone wrote: 
> > Appended is a new java.security.Policy implementation, it fully supports
> > the existing java policy syntax and accepts alternate PolicyParser's. 
> > 
> > All state is immutable, except for 2 volatile references, referents
> > replaced, not mutated, when the policy is updated.  One referent is an
> > array containing PermissionGrant's (interface for immutable object
> > representing a grant statement in a policy), the second a
> > PermissionCollection containing the Policy Permissions.  The array is
> > never mutated after creation, a reference to the array is copied before
> > accessing the array or any array methods. 
> > 
> > The policy creates PermissionCollection's on demand for checking,
> > Permission's are ordered using a PermissionComparator to ensure that for
> > example, wildcard SocketPermission's are checked first, to avoid
> > unnecessary DNS lookups.  Only the permission being checked and any
> > UnresolvedPermission's are added to the PermissionCollection, limiting
> > the size of the objects created. 
> > 
> > In existing policy implementations PermissionCollection's perform
> > blocking operations. 
> > 
> > Also, after parsing policy files, PermissionGrant implementations avoid
> > the need to open files or network connections to confirm URL's, eg
> > CodeSource.implies is not called, but instead reimplemented using URI. 
> > 
> > Will this scale?  There but one smell: 
> > 
> > ProtectionDomain uses a synchronized method Policy.getPolicyNoCheck(),
> > but this only retrieves a reference on 99% of occasions. 
> > 
> > For every permission check, the stack access control context is
> > retrieved, every ProtectionDomain on the stack must be checked,
> > ProtectionDomain's must call getPolicyNoCheck() to call
> > Policy.implies(ProtectionDomain domain, Permission permission). 
> > 
> > To make this worse, I've got a SecurityManager that divides the
> > ProtectionDomain.implies() calls into tasks and submits them to an
> > executor (if there are 4 or more PD's in a context).  The
> > SecurityManager is also non blocking, at least it will be when I use the
> > new ConcurrentHashMap for the checked permission cache (avoids repeated
> > security checks), for now the cache is implemented using the existing
> > ConcurrentHashMap, but is mostly read in any case.  (P.S. This is the
> > cache I'm using the Reference Collection's for.) 
> > 
> > How much can this one synchronized method spoil scalability? 
> > 
> > Cheers & thanks in advance, 
> > 
> > Peter. 
> > 
> > 
> > 
> > /* 
> > *  Licensed to the Apache Software Foundation (ASF) under one or more 
> > *  contributor license agreements.  See the NOTICE file distributed with
> > *  this work for additional information regarding copyright ownership. 
> > *  The ASF licenses this file to You under the Apache License, Version
> > 2.0 
> > *  (the "License"); you may not use this file except in compliance with 
> > *  the License.  You may obtain a copy of the License at 
> > * 
> > *     http://www.apache.org/licenses/LICENSE-2.0
> > * 
> > *  Unless required by applicable law or agreed to in writing, software 
> > *  distributed under the License is distributed on an "AS IS" BASIS, 
> > *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
> > implied. 
> > *  See the License for the specific language governing permissions and 
> > *  limitations under the License. 
> > */ 
> > 
> > /** 
> >  * Default Policy implementation taken from Apache Harmony, refactored
> > for 
> >  * concurrency. 
> >  * 
> >  * @author Alexey V. Varlamov 
> >  * @author Peter Firmstone 
> >  * @version $Revision$ 
> >  */ 
> > 
> > package net.jini.security.policy; 
> > 
> > import java.io.File; 
> > import java.net.URL; 
> > import java.security.AccessController; 
> > import java.security.AllPermission; 
> > import java.security.CodeSource; 
> > import java.security.Guard; 
> > import java.security.Permission; 
> > import java.security.PermissionCollection; 
> > import java.security.Permissions; 
> > import java.security.Policy; 
> > import java.security.PrivilegedActionException; 
> > import java.security.PrivilegedExceptionAction; 
> > import java.security.ProtectionDomain; 
> > import java.security.SecurityPermission; 
> > import java.security.UnresolvedPermission; 
> > import java.util.ArrayList; 
> > import java.util.Collection; 
> > import java.util.Enumeration; 
> > import java.util.Iterator; 
> > import java.util.List; 
> > import java.util.NavigableSet; 
> > import java.util.Properties; 
> > import java.util.TreeSet; 
> > import net.jini.security.PermissionComparator; 
> > import org.apache.river.api.security.PermissionGrant; 
> > import org.apache.river.impl.security.policy.util.DefaultPolicyParser; 
> > import org.apache.river.impl.security.policy.util.PolicyParser; 
> > import org.apache.river.impl.security.policy.util.PolicyUtils; 
> > 
> > 
> > /** 
> > * Concurrent Policy implementation based on policy configuration files, 
> > * it is intended to provide concurrent implies() for greatly improved 
> > * throughput at the expense of single threaded performance. 
> > * 
> > * Set the following system properties to use this Policy instead of the 
> > * built in Java sun.security.provider.PolicyFile: 
> > * * net.jini.security.policy.PolicyFileProvider.basePolicyClass = 
> > * org.apache.river.security.concurrent.ConcurrentPolicyFile 
> > * 
> > * 
> > * This 
> > * implementation recognizes text files, consisting of clauses with the 
> > * following syntax: 
> > * 
> > * <pre> 
> > * keystore &quot;some_keystore_url&quot; [, &quot;keystore_type&quot;]; 
> > * </pre> 
> > <pre> 
> > * grant [SignedBy &quot;signer_names&quot;] [, CodeBase &quot;URL&quot;]
> > *  [, Principal [principal_class_name] &quot;principal_name&quot;] 
> > *  [, Principal [principal_class_name] &quot;principal_name&quot;] ... {
> > *  permission permission_class_name [ &quot;target_name&quot; ] [,
> > &quot;action&quot;] 
> > *  [, SignedBy &quot;signer_names&quot;]; 
> > *  permission ... 
> > *  }; 
> > * * </pre> 
> > * 
> > * The <i>keystore </i> clause specifies reference to a keystore, which
> > is a 
> > * database of private keys and their associated digital certificates.
> > The 
> > * keystore is used to look up the certificates of signers specified in
> > the 
> > * <i>grant </i> entries of the file. The policy file can contain any
> > number of 
> > * <i>keystore </i> entries which can appear at any ordinal position.
> > However, 
> > * only the first successfully loaded keystore is used, others are
> > ignored. The 
> > * keystore must be specified if some grant clause refers to a
> > certificate's 
> > * alias. <br> 
> > * The <i>grant </i> clause associates a CodeSource (consisting of an URL
> > and a 
> > * set of certificates) of some executable code with a set of Permissions
> > which 
> > * should be granted to the code. So, the CodeSource is defined by values
> > of 
> > * <i>CodeBase </i> and <i>SignedBy </i> fields. The <i>CodeBase </i>
> > value must 
> > * be in URL format, while <i>SignedBy </i> value is a (comma-separated
> > list of) 
> > * alias(es) to keystore certificates. These fields can be omitted to
> > denote any 
> > * codebase and any signers (including case of unsigned code),
> > respectively. 
> > * <br> 
> > * Also, the code may be required to be executed on behalf of some
> > Principals 
> > * (in other words, code's ProtectionDomain must have the array of
> > Principals 
> > * associated) in order to possess the Permissions. This fact is
> > indicated by 
> > * specifying one or more <i>Principal </i> fields in the <i>grant </i>
> > clause. 
> > * Each Principal is specified as class/name pair; name and class can be
> > either 
> > * concrete value or wildcard <i>* </i>. As a special case, the class
> > value may 
> > * be omitted and then the name is treated as an alias to X.509
> > Certificate, and 
> > * the Principal is assumed to be javax.security.auth.x500.X500Principal
> > with a 
> > * name of subject's distinguished name from the certificate. <br> 
> > * The order between the <i>CodeBase </i>, <i>SignedBy </i>, and
> > <i>Principal 
> > * </i> fields does not matter. The policy file can contain any number of
> > grant 
> > * clauses. <br> 
> > * Each <i>grant </i> clause must contain one or more <i>permission </i>
> > entry. 
> > * The permission entry consist of a fully qualified class name along
> > with 
> > * optional <i>name </i>, <i>actions </i> and <i>signedby </i> values.
> > Name and 
> > * actions are arguments to the corresponding constructor of the
> > permission 
> > * class. SignedBy value represents the keystore alias(es) to
> > certificate(s) 
> > * used to sign the permission class. That is, this permission entry is 
> > * effective (i.e., access control permission will be granted based on
> > this 
> > * entry) only if the bytecode implementation of permission class is
> > verified to 
> > * be correctly signed by the said alias(es). <br> 
> > * <br> 
> > * The policy content may be parameterized via property expansion.
> > Namely, 
> > * expressions like <i>${key} </i> are replaced by values of
> > corresponding 
> > * system properties. Also, the special <i>slash </i> key (i.e. ${/}) is 
> > * supported, it is a shortcut to &quot;file.separator&quot; key.
> > Property 
> > * expansion is performed anywhere a double quoted string is allowed in
> > the 
> > * policy file. However, this feature is controlled by security
> > properties and 
> > * should be turned on by setting &quot;policy.expandProperties&quot;
> > property 
> > * to <i>true </i>. <br> 
> > * If property expansion fails (due to a missing key), a corresponding
> > entry is 
> > * ignored. For fields of <i>keystore </i> and <i>grant </i> clauses, the
> > whole 
> > * clause is ignored, and for <i>permission </i> entry, only that entry
> > is 
> > * ignored. <br> 
> > * <br> 
> > * The policy also supports generalized expansion in permissions names,
> > of 
> > * expressions like <i>${{protocol:data}} </i>. Currently the following 
> > * protocols supported: 
> > * <dl> 
> > * <dt>self 
> > * <dd>Denotes substitution to a principal information of the parental
> > Grant 
> > * entry. Replaced by a space-separated list of resolved Principals
> > (including 
> > * wildcarded), each formatted as <i>class &quot;name&quot; </i>. If
> > parental 
> > * Grant entry has no Principals, the permission is ignored. 
> > * <dt>alias: <i>name </i> 
> > * <dd>Denotes substitution of a KeyStore alias. Namely, if a KeyStore
> > has an 
> > * X.509 certificate associated with the specified name, then replaced by
> > * <i>javax.security.auth.x500.X500Principal &quot; <i>DN </i>&quot; </i>
> > * string, where <i>DN </i> is a certificate's subject distinguished
> > name. 
> > * </dl> 
> > * <br> 
> > * 
> > */ 
> > 
> > public class ConcurrentPolicyFile extends Policy implements
> > ConcurrentPolicy { 
> > 
> >    /** 
> >     * System property for dynamically added policy location. 
> >     */ 
> >    private static final String JAVA_SECURITY_POLICY =
> > "java.security.policy"; //$NON-NLS-1$ 
> > 
> >    /** 
> >     * Prefix for numbered Policy locations specified in
> > security.properties. 
> >     */ 
> >    private static final String POLICY_URL_PREFIX = "policy.url.";
> > //$NON-NLS-1$ 
> >      // Reference must be defensively copied before access, once
> > published, never mutated. 
> >    private volatile PermissionGrant [] grantArray; 
> >      // A specific parser for a particular policy file format. 
> >    private final PolicyParser parser; 
> >      private static final Guard guard = new
> > SecurityPermission("getPolicy"); 
> >      private final ProtectionDomain myDomain; 
> >      // reference must be defensively copied before access, once
> > published, never mutated. 
> >    private volatile PermissionCollection myPermissions; 
> >      /** 
> >     * Default constructor, equivalent to 
> >     * <code>ConcurrentPolicyFile(new DefaultPolicyParser())</code>. 
> >     */ 
> >    public ConcurrentPolicyFile() throws PolicyInitializationException { 
> >        this(new DefaultPolicyParser()); 
> >    } 
> > 
> >    /** 
> >     * Extension constructor for plugging-in a custom parser. 
> >     * @param dpr 
> >     */ 
> >    protected ConcurrentPolicyFile(PolicyParser dpr) throws
> > PolicyInitializationException { 
> >        guard.checkGuard(null); 
> >        parser = dpr; 
> >        myDomain = this.getClass().getProtectionDomain(); 
> >        /* 
> >         * The bootstrap policy makes implies decisions until this
> > constructor 
> >         * has returned.  We don't need to lock. 
> >         */ 
> >        try { 
> >            // Bug 4911907, do we need to do anything more? 
> >            // The permissions for this domain must be retrieved before 
> >            // construction is complete and this policy takes over. 
> >            initialize(); // Instantiates myPermissions. 
> >        } catch (SecurityException e){ 
> >            throw e; 
> >        } catch (Exception e){ 
> >            throw new PolicyInitializationException("PolicyInitialization
> > failed", e); 
> >        } 
> >    } 
> >      private PermissionCollection convert(NavigableSet<Permission>
> > permissions){ 
> >        PermissionCollection pc = new Permissions(); 
> >        // The descending iterator is for SocketPermission. 
> >        Iterator<Permission> it = permissions.descendingIterator(); 
> >        while (it.hasNext()) { 
> >            pc.add(it.next()); 
> >        } 
> >        return pc; 
> >    } 
> > 
> >    /** 
> >     * Returns collection of permissions allowed for the domain 
> >     * according to the policy. The evaluated characteristics of the 
> >     * domain are it's codesource and principals; they are assumed 
> >     * to be <code>null</code> if the domain is <code>null</code>. 
> >     * 
> >     * Each PermissionCollection returned is a unique instance. 
> >     * 
> >     * @param pd ProtectionDomain 
> >     * @see ProtectionDomain 
> >     */ 
> >    @Override 
> >    public PermissionCollection getPermissions(ProtectionDomain pd) { 
> >        NavigableSet<Permission> perms = new TreeSet<Permission>(new
> > PermissionComparator()); 
> >        PermissionGrant [] grantRefCopy = grantArray; 
> >        int l = grantRefCopy.length; 
> >        for ( int j =0; j < l; j++ ){ 
> >            PermissionGrant ge = grantRefCopy[j]; 
> >            if (ge.implies(pd)){ 
> >                if (ge.isPrivileged()){// Don't stuff around finish early
> > if you can. 
> >                    PermissionCollection pc = new Permissions(); 
> >                    pc.add(new AllPermission()); 
> >                    return pc; 
> >                } 
> >                Collection<Permission> c = ge.getPermissions(); 
> >                Iterator<Permission> i = c.iterator(); 
> >                while (i.hasNext()){ 
> >                    Permission p = i.next(); 
> >                    perms.add(p); 
> >                } 
> >            } 
> >        } 
> >        // Don't forget to merge the static Permissions. 
> >        PermissionCollection staticPC = null; 
> >        if (pd != null) { 
> >            staticPC = pd.getPermissions(); 
> >            if (staticPC != null){ 
> >                Enumeration<Permission> e = staticPC.elements(); 
> >                while (e.hasMoreElements()){ 
> >                    Permission p = e.nextElement(); 
> >                    if (p instanceof AllPermission) { 
> >                        PermissionCollection pc = new Permissions(); 
> >                        pc.add(p); 
> >                        return pc; 
> >                    } 
> >                    perms.add(p); 
> >                } 
> >            } 
> >        } 
> >        return convert(perms); 
> >    } 
> > 
> >    /** 
> >     * Returns collection of permissions allowed for the codesource 
> >     * according to the policy. 
> >     * The evaluation assumes that current principals are undefined. 
> >     * 
> >     * This returns a java.security.Permissions collection, which allows 
> >     * ProtectionDomain to optimise for the AllPermission case, which
> > avoids 
> >     * unnecessarily consulting the policy. 
> >     * 
> >     * If constructed with the four argument constructor,
> > ProtectionDomain.implies 
> >     * first consults the Policy, then it's own internal Permissions
> > collection, 
> >     * unless it has AllPermission, in which case it returns true without
> >     * consulting the policy. 
> >     * 
> >     * @param cs CodeSource 
> >     * @see CodeSource 
> >     */ 
> >    @Override 
> >    public PermissionCollection getPermissions(CodeSource cs) { 
> >        if (cs == null) throw new NullPointerException("CodeSource cannot
> > be null"); 
> >        NavigableSet<Permission> perms = new TreeSet<Permission>(new
> > PermissionComparator()); 
> >        // for ProtectionDomain AllPermission optimisation. 
> >        PermissionGrant [] grantRefCopy = grantArray; 
> >        int l = grantRefCopy.length; 
> >        for ( int j =0; j < l; j++ ){ 
> >            PermissionGrant ge = grantRefCopy[j]; 
> >            if (ge.implies(cs, null)){ // No Principal's 
> >                if (ge.isPrivileged()){// Don't stuff around finish early
> > if you can. 
> >                    PermissionCollection pc = new Permissions(); 
> >                    pc.add(new AllPermission()); 
> >                    return pc; 
> >                } 
> >                Collection<Permission> c = ge.getPermissions(); 
> >                Iterator<Permission> i = c.iterator(); 
> >                while (i.hasNext()){ 
> >                    Permission p = i.next(); 
> >                    perms.add(p); 
> >                } 
> >            } 
> >        } 
> >        return convert(perms); 
> >    } 
> >      @Override 
> >    public boolean implies(ProtectionDomain domain, Permission
> > permission) { 
> >        if (permission == null) throw new
> > NullPointerException("permission not allowed to be null"); 
> >        if (domain == myDomain) { 
> >            PermissionCollection pc = myPermissions; 
> >            return pc.implies(permission); 
> >        } 
> >        Class klass = permission.getClass(); 
> >        // Need to have a list of Permission's we can sort if permission
> > is SocketPermission. 
> >        NavigableSet<Permission> perms = new TreeSet<Permission>(new
> > PermissionComparator()); 
> >        PermissionGrant [] grantRefCopy = grantArray; 
> >        int l = grantRefCopy.length; 
> >        for ( int j =0; j < l; j++ ){ 
> >            PermissionGrant ge = grantRefCopy[j]; 
> >            if (ge.implies(domain)){ 
> >                if (ge.isPrivileged()) return true; // Don't stuff around
> > finish early if you can. 
> >                Collection<Permission> c = ge.getPermissions(); 
> >                Iterator<Permission> i = c.iterator(); 
> >                while (i.hasNext()){ 
> >                    Permission p = i.next(); 
> >                    // Don't make it larger than necessary. 
> >                    if (klass.isInstance(permission) || permission
> > instanceof UnresolvedPermission){ 
> >                        perms.add(p); 
> >                    } 
> >                } 
> >            } 
> >        } 
> >        // Don't forget to merge the static Permissions. 
> >        PermissionCollection staticPC = null; 
> >        if (domain != null) { 
> >            staticPC =domain.getPermissions(); 
> >            if (staticPC != null){ 
> >                Enumeration<Permission> e = staticPC.elements(); 
> >                while (e.hasMoreElements()){ 
> >                    Permission p = e.nextElement(); 
> >                    // return early if possible. 
> >                    if (p instanceof AllPermission ) return true; 
> >                    // Don't make it larger than necessary, but don't
> > worry about duplicates either. 
> >                    if (klass.isInstance(permission) || permission
> > instanceof UnresolvedPermission){ 
> >                        perms.add(p); 
> >                    } 
> >                } 
> >            } 
> >        } 
> >        return convert(perms).implies(permission); 
> >    } 
> > 
> >    /** 
> >     * Gets fresh list of locations and tries to load all of them in
> > sequence; 
> >     * failed loads are ignored. After processing all locations, old
> > policy 
> >     * settings are discarded and new ones come into force. <br> 
> >     * 
> >     * @see PolicyUtils#getPolicyURLs(Properties, String, String) 
> >     */ 
> >    @Override 
> >    public void refresh() { 
> >        try { 
> >            initialize(); 
> >        } catch (Exception ex) { 
> >            System.err.println(ex); 
> >        } 
> >    } 
> >      private void initialize() throws Exception{ 
> >        try { 
> >            Collection<PermissionGrant> fresh =
> > AccessController.doPrivileged( 
> >                new
> > PrivilegedExceptionAction<Collection<PermissionGrant>>(){ 
> >                    public Collection<PermissionGrant> run() throws
> > SecurityException { 
> >                        Collection<PermissionGrant> fresh = new
> > ArrayList<PermissionGrant>(120); 
> >                        Properties system = System.getProperties(); 
> >                        system.setProperty("/", File.separator);
> > //$NON-NLS-1$ 
> >                        URL[] policyLocations =
> > PolicyUtils.getPolicyURLs(system, 
> >                                                         
> > JAVA_SECURITY_POLICY, 
> >                                                         
> > POLICY_URL_PREFIX); 
> >                        int l = policyLocations.length; 
> >                        for (int i = 0; i < l; i++) { 
> >                            //TODO debug log 
> > //                                System.err.println("Parsing policy
> > file: " + policyLocations[i]); 
> >                            try { 
> >                                Collection<PermissionGrant> pc = null; 
> >                                pc = parser.parse(policyLocations[i],
> > system); 
> >                                fresh.addAll(pc); 
> >                            } catch (Exception e){ 
> >                                // It's best to let a SecurityException
> > bubble up 
> >                                // in case there is a problem with our
> > policy configuration 
> >                                // or implementation. 
> >                                if ( e instanceof SecurityException ) { 
> >                                    e.printStackTrace(System.out); 
> >                                    throw (SecurityException) e; 
> >                                } 
> >                                // ignore. 
> >                            } 
> >                        } 
> >                        return fresh; 
> >                    } 
> >                } 
> >            ); 
> >            // Volatile reference, publish after mutation complete. 
> >            grantArray = fresh.toArray(new
> > PermissionGrant[fresh.size()]); 
> >            myPermissions = getPermissions(myDomain); 
> >        }catch (PrivilegedActionException e){ 
> >            Throwable t = e.getCause(); 
> >            if ( t instanceof Exception ) throw (Exception) t; 
> >            throw e; 
> >        } 
> >    } 
> > 
> >    public boolean isConcurrent() { 
> >        return true; 
> >    } 
> > 
> >    public PermissionGrant[] getPermissionGrants() { 
> >        PermissionGrant [] grants = grantArray; // copy volatile
> > reference target. 
> >        return grants.clone(); 
> >    } 
> >      public PermissionGrant[] getPermissionGrants(ProtectionDomain pd) {
> >        PermissionGrant [] grants = grantArray; // copy volatile
> > reference target. 
> >        int l = grants.length; 
> >        List<PermissionGrant> applicable = new
> > ArrayList<PermissionGrant>(l); // Always too large, never too small. 
> >        for (int i =0; i < l; i++){ 
> >            if (grants[i].implies(pd)){ 
> >                applicable.add(grants[i]); 
> >            } 
> >        } 
> >        return applicable.toArray(new
> > PermissionGrant[applicable.size()]); 
> >    } 
> > 
> > } 
> > 
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From radhakrishnan.mohan at gmail.com  Tue Jan 10 06:39:12 2012
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Tue, 10 Jan 2012 17:09:12 +0530
Subject: [concurrency-interest] tsc register
Message-ID: <CAOoXFP9_3FE9P2epvfygL=bPQRWvio5xWSGNTX=MMHhBLiUG1g@mail.gmail.com>

Hi,

One more question from the novice and for the novice.

I see these points in Dr. click's PPT. Can I know why ? I ask this
here because it seems to
involve multiple cores. Maybe the jvm forums are better suited for this.
Does this mean that we get wrong time values if threads run on
different cores ?

But cannot use, e.g. X86's "tsc" register
? Value not coherent across CPUs
? Not consistent, e.g. slow ticking in low-power mode
? Monotonic per CPU ? but not per-thread

Thanks,
Mohan


From heinz at javaspecialists.eu  Tue Jan 10 07:03:17 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 10 Jan 2012 14:03:17 +0200
Subject: [concurrency-interest] tsc register
In-Reply-To: <CAOoXFP9_3FE9P2epvfygL=bPQRWvio5xWSGNTX=MMHhBLiUG1g@mail.gmail.com>
References: <CAOoXFP9_3FE9P2epvfygL=bPQRWvio5xWSGNTX=MMHhBLiUG1g@mail.gmail.com>
Message-ID: <CACLL95qKOQ74Gz1FJAGYfpTD327QLwaU8uhm5LZKs2z6y3-9Qw@mail.gmail.com>

Only if you use System.nanoTime().  Time difference might even be
negative if the thread is swapped between different cores.

On 10/01/2012, Mohan Radhakrishnan <radhakrishnan.mohan at gmail.com> wrote:
> Hi,
>
> One more question from the novice and for the novice.
>
> I see these points in Dr. click's PPT. Can I know why ? I ask this
> here because it seems to
> involve multiple cores. Maybe the jvm forums are better suited for this.
> Does this mean that we get wrong time values if threads run on
> different cores ?
>
> But cannot use, e.g. X86's "tsc" register
> ? Value not coherent across CPUs
> ? Not consistent, e.g. slow ticking in low-power mode
> ? Monotonic per CPU ? but not per-thread
>
> Thanks,
> Mohan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz


From nathan.reynolds at oracle.com  Tue Jan 10 11:20:35 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 10 Jan 2012 09:20:35 -0700
Subject: [concurrency-interest] tsc register
In-Reply-To: <CACLL95qKOQ74Gz1FJAGYfpTD327QLwaU8uhm5LZKs2z6y3-9Qw@mail.gmail.com>
References: <CAOoXFP9_3FE9P2epvfygL=bPQRWvio5xWSGNTX=MMHhBLiUG1g@mail.gmail.com>
	<CACLL95qKOQ74Gz1FJAGYfpTD327QLwaU8uhm5LZKs2z6y3-9Qw@mail.gmail.com>
Message-ID: <4F0C6553.8070201@oracle.com>

The tsc register on older processors did not increment at the same 
rate.  If a core slept or slowed down then the tsc register would stop 
or slow down its increments.  More modern processors guarantee that tsc 
register increments at a fixed frequency.  If you are working on Linux, 
cpuinfo (?) could report the const_tsc flag.  This means that the 
processor and OS recognize that this feature is on the processor.

The tsc register is not synchronized across sockets.  This is something 
Oracle has asked Intel to enhance many times.  It is a very difficult 
problem to solve.  However, more modern Linux kernels will (?) 
synchronize the tsc register at startup so that it is impossible to read 
the tsc register on two different cores and see that the 2^(n)^(d) value 
is smaller.  This does not mean that the tsc register is synchronized.  
It only means that two threads running on different cores will hopefully 
never see the tsc "move backwards".

There is no guarantee that once the tsc register is synchronized across 
sockets that it will remain so.  Some processors are hot swappable.  The 
newly added processor is not going to have the correct tsc register 
value.  Furthermore, the OS is free to reset the tsc value at any time.

If I understand correctly, the HotSpot JVM will guarantee that 
System.nanoTime() never moves backwards.  It reads the tsc register with 
each call (?).  It the compares the read value with the last read 
value.  If the read value is < the last read value, then the last read 
value is returned.  If the read value is > the last read value, then the 
last read value is updated and the read value is returned.  Updating the 
last read value requires a CAS.  This CAS can lead to scalability 
bottlenecks if System.nanoTime() is called too frequently.  I am not 
sure if a better algorithm has been devised to fix this CAS contention.  
I kind of remember it being talked about.

I think the JVMs will default to more stable clock sources with worse 
resolution for nanoTime() if tsc is not behaving well.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
> Only if you use System.nanoTime().  Time difference might even be
> negative if the thread is swapped between different cores.
>
> On 10/01/2012, Mohan Radhakrishnan<radhakrishnan.mohan at gmail.com>  wrote:
>> Hi,
>>
>> One more question from the novice and for the novice.
>>
>> I see these points in Dr. click's PPT. Can I know why ? I ask this
>> here because it seems to
>> involve multiple cores. Maybe the jvm forums are better suited for this.
>> Does this mean that we get wrong time values if threads run on
>> different cores ?
>>
>> But cannot use, e.g. X86's "tsc" register
>> ? Value not coherent across CPUs
>> ? Not consistent, e.g. slow ticking in low-power mode
>> ? Monotonic per CPU ? but not per-thread
>>
>> Thanks,
>> Mohan
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120110/2fabc09c/attachment.html>

From vitalyd at gmail.com  Tue Jan 10 11:27:55 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 10 Jan 2012 11:27:55 -0500
Subject: [concurrency-interest] tsc register
In-Reply-To: <4F0C6553.8070201@oracle.com>
References: <CAOoXFP9_3FE9P2epvfygL=bPQRWvio5xWSGNTX=MMHhBLiUG1g@mail.gmail.com>
	<CACLL95qKOQ74Gz1FJAGYfpTD327QLwaU8uhm5LZKs2z6y3-9Qw@mail.gmail.com>
	<4F0C6553.8070201@oracle.com>
Message-ID: <CAHjP37Ffq+=RC-RzfWJ8BrU4PzzThZs3ZH=u4CzU0TzKHwHiew@mail.gmail.com>

I thought JVM (hotspot at least) uses the os monotonic clock source (if
present) rather than reading tsc directly and then doing its own
adjustments?
On Jan 10, 2012 11:24 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  The tsc register on older processors did not increment at the same rate.
> If a core slept or slowed down then the tsc register would stop or slow
> down its increments.  More modern processors guarantee that tsc register
> increments at a fixed frequency.  If you are working on Linux, cpuinfo (?)
> could report the const_tsc flag.  This means that the processor and OS
> recognize that this feature is on the processor.
>
> The tsc register is not synchronized across sockets.  This is something
> Oracle has asked Intel to enhance many times.  It is a very difficult
> problem to solve.  However, more modern Linux kernels will (?) synchronize
> the tsc register at startup so that it is impossible to read the tsc
> register on two different cores and see that the 2?? value is smaller.
> This does not mean that the tsc register is synchronized.  It only means
> that two threads running on different cores will hopefully never see the
> tsc "move backwards".
>
> There is no guarantee that once the tsc register is synchronized across
> sockets that it will remain so.  Some processors are hot swappable.  The
> newly added processor is not going to have the correct tsc register value.
> Furthermore, the OS is free to reset the tsc value at any time.
>
> If I understand correctly, the HotSpot JVM will guarantee that
> System.nanoTime() never moves backwards.  It reads the tsc register with
> each call (?).  It the compares the read value with the last read value.
> If the read value is < the last read value, then the last read value is
> returned.  If the read value is > the last read value, then the last read
> value is updated and the read value is returned.  Updating the last read
> value requires a CAS.  This CAS can lead to scalability bottlenecks if
> System.nanoTime() is called too frequently.  I am not sure if a better
> algorithm has been devised to fix this CAS contention.  I kind of remember
> it being talked about.
>
> I think the JVMs will default to more stable clock sources with worse
> resolution for nanoTime() if tsc is not behaving well.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
>
> Only if you use System.nanoTime().  Time difference might even be
> negative if the thread is swapped between different cores.
>
> On 10/01/2012, Mohan Radhakrishnan <radhakrishnan.mohan at gmail.com> <radhakrishnan.mohan at gmail.com> wrote:
>
>  Hi,
>
> One more question from the novice and for the novice.
>
> I see these points in Dr. click's PPT. Can I know why ? I ask this
> here because it seems to
> involve multiple cores. Maybe the jvm forums are better suited for this.
> Does this mean that we get wrong time values if threads run on
> different cores ?
>
> But cannot use, e.g. X86's "tsc" register
> ? Value not coherent across CPUs
> ? Not consistent, e.g. slow ticking in low-power mode
> ? Monotonic per CPU ? but not per-thread
>
> Thanks,
> Mohan
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120110/fe27b96e/attachment.html>

From nathan.reynolds at oracle.com  Tue Jan 10 11:34:18 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 10 Jan 2012 09:34:18 -0700
Subject: [concurrency-interest] tsc register
In-Reply-To: <CAHjP37Ffq+=RC-RzfWJ8BrU4PzzThZs3ZH=u4CzU0TzKHwHiew@mail.gmail.com>
References: <CAOoXFP9_3FE9P2epvfygL=bPQRWvio5xWSGNTX=MMHhBLiUG1g@mail.gmail.com>
	<CACLL95qKOQ74Gz1FJAGYfpTD327QLwaU8uhm5LZKs2z6y3-9Qw@mail.gmail.com>
	<4F0C6553.8070201@oracle.com>
	<CAHjP37Ffq+=RC-RzfWJ8BrU4PzzThZs3ZH=u4CzU0TzKHwHiew@mail.gmail.com>
Message-ID: <4F0C688A.8040209@oracle.com>

You could be right.  I am just going on what I hear and memory.  Both of 
which are faulty.

I was under the impression that the HotSpot JVM used tsc directly if it 
determines that it is stable and works well.  If not, then it defaults 
down to the OS clock source.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/10/2012 9:27 AM, Vitaly Davidovich wrote:
>
> I thought JVM (hotspot at least) uses the os monotonic clock source 
> (if present) rather than reading tsc directly and then doing its own 
> adjustments?
>
> On Jan 10, 2012 11:24 AM, "Nathan Reynolds" 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     The tsc register on older processors did not increment at the same
>     rate.  If a core slept or slowed down then the tsc register would
>     stop or slow down its increments.  More modern processors
>     guarantee that tsc register increments at a fixed frequency.  If
>     you are working on Linux, cpuinfo (?) could report the const_tsc
>     flag.  This means that the processor and OS recognize that this
>     feature is on the processor.
>
>     The tsc register is not synchronized across sockets.  This is
>     something Oracle has asked Intel to enhance many times.  It is a
>     very difficult problem to solve.  However, more modern Linux
>     kernels will (?) synchronize the tsc register at startup so that
>     it is impossible to read the tsc register on two different cores
>     and see that the 2?? value is smaller.  This does not mean that
>     the tsc register is synchronized.  It only means that two threads
>     running on different cores will hopefully never see the tsc "move
>     backwards".
>
>     There is no guarantee that once the tsc register is synchronized
>     across sockets that it will remain so.  Some processors are hot
>     swappable.  The newly added processor is not going to have the
>     correct tsc register value.  Furthermore, the OS is free to reset
>     the tsc value at any time.
>
>     If I understand correctly, the HotSpot JVM will guarantee that
>     System.nanoTime() never moves backwards.  It reads the tsc
>     register with each call (?).  It the compares the read value with
>     the last read value.  If the read value is < the last read value,
>     then the last read value is returned.  If the read value is > the
>     last read value, then the last read value is updated and the read
>     value is returned.  Updating the last read value requires a CAS. 
>     This CAS can lead to scalability bottlenecks if System.nanoTime()
>     is called too frequently.  I am not sure if a better algorithm has
>     been devised to fix this CAS contention.  I kind of remember it
>     being talked about.
>
>     I think the JVMs will default to more stable clock sources with
>     worse resolution for nanoTime() if tsc is not behaving well.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Consulting Member of Technical Staff | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
>     On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
>>     Only if you use System.nanoTime().  Time difference might even be
>>     negative if the thread is swapped between different cores.
>>
>>     On 10/01/2012, Mohan Radhakrishnan<radhakrishnan.mohan at gmail.com>  <mailto:radhakrishnan.mohan at gmail.com>  wrote:
>>>     Hi,
>>>
>>>     One more question from the novice and for the novice.
>>>
>>>     I see these points in Dr. click's PPT. Can I know why ? I ask this
>>>     here because it seems to
>>>     involve multiple cores. Maybe the jvm forums are better suited for this.
>>>     Does this mean that we get wrong time values if threads run on
>>>     different cores ?
>>>
>>>     But cannot use, e.g. X86's "tsc" register
>>>     ? Value not coherent across CPUs
>>>     ? Not consistent, e.g. slow ticking in low-power mode
>>>     ? Monotonic per CPU ? but not per-thread
>>>
>>>     Thanks,
>>>     Mohan
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120110/8a30f81d/attachment-0001.html>

From vitalyd at gmail.com  Tue Jan 10 11:49:56 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 10 Jan 2012 11:49:56 -0500
Subject: [concurrency-interest] tsc register
In-Reply-To: <4F0C688A.8040209@oracle.com>
References: <CAOoXFP9_3FE9P2epvfygL=bPQRWvio5xWSGNTX=MMHhBLiUG1g@mail.gmail.com>
	<CACLL95qKOQ74Gz1FJAGYfpTD327QLwaU8uhm5LZKs2z6y3-9Qw@mail.gmail.com>
	<4F0C6553.8070201@oracle.com>
	<CAHjP37Ffq+=RC-RzfWJ8BrU4PzzThZs3ZH=u4CzU0TzKHwHiew@mail.gmail.com>
	<4F0C688A.8040209@oracle.com>
Message-ID: <CAHjP37EEoaXo-sf-5S=ztfuChD_0H7jXm_+-oq9e70kkUgbQWA@mail.gmail.com>

from openjdk os_Linux.cpp os::javaTimeNanos seems to either use Linux
monotonic clock, if available, or falls back on gettimeofday() with scaling
otherwise.

I think Cliff Click mentions that System.currentTimeMillis was optimized to
avoid a syscall each time by instead reading some cached value that's
updated in the background (or something like that) but I'm not sure if
that's just in Azul or hotspot as well.
On Jan 10, 2012 11:34 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  You could be right.  I am just going on what I hear and memory.  Both of
> which are faulty.
>
> I was under the impression that the HotSpot JVM used tsc directly if it
> determines that it is stable and works well.  If not, then it defaults down
> to the OS clock source.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/10/2012 9:27 AM, Vitaly Davidovich wrote:
>
> I thought JVM (hotspot at least) uses the os monotonic clock source (if
> present) rather than reading tsc directly and then doing its own
> adjustments?
> On Jan 10, 2012 11:24 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>
>>  The tsc register on older processors did not increment at the same
>> rate.  If a core slept or slowed down then the tsc register would stop or
>> slow down its increments.  More modern processors guarantee that tsc
>> register increments at a fixed frequency.  If you are working on Linux,
>> cpuinfo (?) could report the const_tsc flag.  This means that the processor
>> and OS recognize that this feature is on the processor.
>>
>> The tsc register is not synchronized across sockets.  This is something
>> Oracle has asked Intel to enhance many times.  It is a very difficult
>> problem to solve.  However, more modern Linux kernels will (?) synchronize
>> the tsc register at startup so that it is impossible to read the tsc
>> register on two different cores and see that the 2?? value is smaller.
>> This does not mean that the tsc register is synchronized.  It only means
>> that two threads running on different cores will hopefully never see the
>> tsc "move backwards".
>>
>> There is no guarantee that once the tsc register is synchronized across
>> sockets that it will remain so.  Some processors are hot swappable.  The
>> newly added processor is not going to have the correct tsc register value.
>> Furthermore, the OS is free to reset the tsc value at any time.
>>
>> If I understand correctly, the HotSpot JVM will guarantee that
>> System.nanoTime() never moves backwards.  It reads the tsc register with
>> each call (?).  It the compares the read value with the last read value.
>> If the read value is < the last read value, then the last read value is
>> returned.  If the read value is > the last read value, then the last read
>> value is updated and the read value is returned.  Updating the last read
>> value requires a CAS.  This CAS can lead to scalability bottlenecks if
>> System.nanoTime() is called too frequently.  I am not sure if a better
>> algorithm has been devised to fix this CAS contention.  I kind of remember
>> it being talked about.
>>
>> I think the JVMs will default to more stable clock sources with worse
>> resolution for nanoTime() if tsc is not behaving well.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>> On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
>>
>> Only if you use System.nanoTime().  Time difference might even be
>> negative if the thread is swapped between different cores.
>>
>> On 10/01/2012, Mohan Radhakrishnan <radhakrishnan.mohan at gmail.com> <radhakrishnan.mohan at gmail.com> wrote:
>>
>>  Hi,
>>
>> One more question from the novice and for the novice.
>>
>> I see these points in Dr. click's PPT. Can I know why ? I ask this
>> here because it seems to
>> involve multiple cores. Maybe the jvm forums are better suited for this.
>> Does this mean that we get wrong time values if threads run on
>> different cores ?
>>
>> But cannot use, e.g. X86's "tsc" register
>> ? Value not coherent across CPUs
>> ? Not consistent, e.g. slow ticking in low-power mode
>> ? Monotonic per CPU ? but not per-thread
>>
>> Thanks,
>> Mohan
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120110/768ee5fd/attachment.html>

From david.dice at gmail.com  Tue Jan 10 13:14:32 2012
From: david.dice at gmail.com (David Dice)
Date: Tue, 10 Jan 2012 13:14:32 -0500
Subject: [concurrency-interest] tsc register
Message-ID: <CANbRUciRw4ZD2xt7tfcMhWqPczD=xpdoXKtvoYHOUJU7qet-mQ@mail.gmail.com>

The basic rule for nanoTime() is that the JVM must provide globally
consistent non-retrograde (monotonically non-decreasing) time.   If a
thread calls nanoTime() twice, the 2nd return value must be >= the 1st.
And time needs to be causal in the sense that if some thread A calls
nanoTime() and stores the value in a volatile shared variable, and some
other thread B observes that stored value -- an HBO edge -- and then itself
calls nanoTime(), then the value returned by B's call should be >= the
value B observed in memory.

Using SPARC/Solaris as an example, we implemented nanoTime() in HotSpot via
calls to gethrtime(), which is documented to return monotonically
non-decreasing values.   Gethrtime is implemented as a fast-trap into the
kernel, and computes the return value via the STICK or TICK registers,
which are vaguely similar to the TSC.  The source is cycle-accurate and the
trap costs about 60 cycles.   Unfortunately gethrtime() admits retrograde
time in practice in the 2-thread case above -- I won't go into the reasons
but they're related to clock domains in HW.   The maximum skew between
processors was constrained, however, so to implement nanoTime() we needed
to track the maximum observed gethrtime() value and return the maximum of
that value and the value returned from nanoTime().   Unfortunately that
means we're updating the maximum value frequently -- based on the
nanoTime() call rate -- which then gives us a coherence hotspot that
impedes scaling on big systems.    (One way to reduce the update rate is to
mask off a few low order bits returned from gethrtime(), trading off some
quantization and effective resolution for a reduced update rate when we
have frequent calls to nanoTime()).  The key point is that if the system
provides poor clock sources then the JVM must compensate accordingly.

As others have noted, tsc is messy because of skew, core clock rate
differences, thermal capping, onlining and offlining of processors, etc.
But it's often OK for timing code paths & single-threaded performance
analysis if you're willing to tolerate the issues.

Over the years lots of thought has gone into using tsc/tscp in user-space
in the JVM to implement nanoTime().   Generally, I don' think it's a viable
path unless the kernel is willing and able to provide some additional
constraints on maximum skew or if the kernel could publish per-processor
tsc deltas and you're willing to make the JVM more intimate with that
particular kernel.

Dave

http://blogs.oracle.com/dave/

p.s., if you must really want to use rdtsc, then rdtscp is preferred if
available.

p.s., as a throw-away experiment I wrote a kernel driver that created a
kernel page that could be mmap()ed RO into user-space.  The page held
lbolt, ticking at 100hz, and a "delta" value that expressed the difference
between lbolt and absolute time.   The values were protected by a seqlock
so that readers could ensure the values were mutually coherent.  Thus, you
could use this page as a replacement for gethrtime (relative time) and
gettimeofday (absolute time).  It didn't quite have the resolution of
gethrtime(), but it was potentially sufficient for the needs of the JVM.
(Recall that nanoTime is already 10 msecs granular on various HotSpot
reference platforms)  The problem is that the kernel needed to update the
time on the clock tick, which didn't work well with the new tickless
kernels -- they help the system get to lower power states.   The trick to
get the best of both worlds was to withdraw permissions on the page when
the page hadn't been accessed recently.   If you subsequently accessed the
page we'd trap, make the page RW, update the time, and restart the periodic
tick timer for a few seconds.   The time is guaranteed monotonic, so
there's no need to update that hot "max time" variable.   Perhaps even
better, it's possible for the JIT to directly inline fetches to the magic
page to query time via a DirectBuffer, avoiding control flow back into
native code,    If you don't have this type of kernel support you can
instead have each JVM create a thread that updates a process-private page
every 10 msecs, and quiesce the thread if there have been no recent time
queries.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120110/5f4218f5/attachment.html>

From davidcholmes at aapt.net.au  Tue Jan 10 17:14:34 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 11 Jan 2012 08:14:34 +1000
Subject: [concurrency-interest] tsc register
In-Reply-To: <CAHjP37Ffq+=RC-RzfWJ8BrU4PzzThZs3ZH=u4CzU0TzKHwHiew@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>

Correct. Hotspot uses/relies-on the high-resolution monotonic time source of the OS, else falls back to plain time-of-day. It never uses the TSC directly.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly Davidovich
  Sent: Wednesday, 11 January 2012 2:28 AM
  To: Nathan Reynolds
  Cc: Concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] tsc register


  I thought JVM (hotspot at least) uses the os monotonic clock source (if present) rather than reading tsc directly and then doing its own adjustments? 

  On Jan 10, 2012 11:24 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com> wrote:

    The tsc register on older processors did not increment at the same rate.  If a core slept or slowed down then the tsc register would stop or slow down its increments.  More modern processors guarantee that tsc register increments at a fixed frequency.  If you are working on Linux, cpuinfo (?) could report the const_tsc flag.  This means that the processor and OS recognize that this feature is on the processor.

    The tsc register is not synchronized across sockets.  This is something Oracle has asked Intel to enhance many times.  It is a very difficult problem to solve.  However, more modern Linux kernels will (?) synchronize the tsc register at startup so that it is impossible to read the tsc register on two different cores and see that the 2?? value is smaller.  This does not mean that the tsc register is synchronized.  It only means that two threads running on different cores will hopefully never see the tsc "move backwards".

    There is no guarantee that once the tsc register is synchronized across sockets that it will remain so.  Some processors are hot swappable.  The newly added processor is not going to have the correct tsc register value.  Furthermore, the OS is free to reset the tsc value at any time.

    If I understand correctly, the HotSpot JVM will guarantee that System.nanoTime() never moves backwards.  It reads the tsc register with each call (?).  It the compares the read value with the last read value.  If the read value is < the last read value, then the last read value is returned.  If the read value is > the last read value, then the last read value is updated and the read value is returned.  Updating the last read value requires a CAS.  This CAS can lead to scalability bottlenecks if System.nanoTime() is called too frequently.  I am not sure if a better algorithm has been devised to fix this CAS contention.  I kind of remember it being talked about.

    I think the JVMs will default to more stable clock sources with worse resolution for nanoTime() if tsc is not behaving well.


    Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
    Oracle PSR Engineering | Server Technology


    On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote: 
Only if you use System.nanoTime().  Time difference might even be
negative if the thread is swapped between different cores.

On 10/01/2012, Mohan Radhakrishnan <radhakrishnan.mohan at gmail.com> wrote:
Hi,

One more question from the novice and for the novice.

I see these points in Dr. click's PPT. Can I know why ? I ask this
here because it seems to
involve multiple cores. Maybe the jvm forums are better suited for this.
Does this mean that we get wrong time values if threads run on
different cores ?

But cannot use, e.g. X86's "tsc" register
? Value not coherent across CPUs
? Not consistent, e.g. slow ticking in low-power mode
? Monotonic per CPU ? but not per-thread

Thanks,
Mohan

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120111/52f73d7c/attachment.html>

From davidcholmes at aapt.net.au  Tue Jan 10 17:16:47 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 11 Jan 2012 08:16:47 +1000
Subject: [concurrency-interest] tsc register
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEFBJCAA.davidcholmes@aapt.net.au>

And with guards against a buggy OS not giving correct monotonic time.

David
  -----Original Message-----
  From: David Holmes [mailto:davidcholmes at aapt.net.au]
  Sent: Wednesday, 11 January 2012 8:15 AM
  To: Vitaly Davidovich; Nathan Reynolds
  Cc: Concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] tsc register


  Correct. Hotspot uses/relies-on the high-resolution monotonic time source of the OS, else falls back to plain time-of-day. It never uses the TSC directly.

  David Holmes
    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly Davidovich
    Sent: Wednesday, 11 January 2012 2:28 AM
    To: Nathan Reynolds
    Cc: Concurrency-interest at cs.oswego.edu
    Subject: Re: [concurrency-interest] tsc register


    I thought JVM (hotspot at least) uses the os monotonic clock source (if present) rather than reading tsc directly and then doing its own adjustments? 

    On Jan 10, 2012 11:24 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com> wrote:

      The tsc register on older processors did not increment at the same rate.  If a core slept or slowed down then the tsc register would stop or slow down its increments.  More modern processors guarantee that tsc register increments at a fixed frequency.  If you are working on Linux, cpuinfo (?) could report the const_tsc flag.  This means that the processor and OS recognize that this feature is on the processor.

      The tsc register is not synchronized across sockets.  This is something Oracle has asked Intel to enhance many times.  It is a very difficult problem to solve.  However, more modern Linux kernels will (?) synchronize the tsc register at startup so that it is impossible to read the tsc register on two different cores and see that the 2?? value is smaller.  This does not mean that the tsc register is synchronized.  It only means that two threads running on different cores will hopefully never see the tsc "move backwards".

      There is no guarantee that once the tsc register is synchronized across sockets that it will remain so.  Some processors are hot swappable.  The newly added processor is not going to have the correct tsc register value.  Furthermore, the OS is free to reset the tsc value at any time.

      If I understand correctly, the HotSpot JVM will guarantee that System.nanoTime() never moves backwards.  It reads the tsc register with each call (?).  It the compares the read value with the last read value.  If the read value is < the last read value, then the last read value is returned.  If the read value is > the last read value, then the last read value is updated and the read value is returned.  Updating the last read value requires a CAS.  This CAS can lead to scalability bottlenecks if System.nanoTime() is called too frequently.  I am not sure if a better algorithm has been devised to fix this CAS contention.  I kind of remember it being talked about.

      I think the JVMs will default to more stable clock sources with worse resolution for nanoTime() if tsc is not behaving well.


      Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
      Oracle PSR Engineering | Server Technology


      On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote: 
Only if you use System.nanoTime().  Time difference might even be
negative if the thread is swapped between different cores.

On 10/01/2012, Mohan Radhakrishnan <radhakrishnan.mohan at gmail.com> wrote:
Hi,

One more question from the novice and for the novice.

I see these points in Dr. click's PPT. Can I know why ? I ask this
here because it seems to
involve multiple cores. Maybe the jvm forums are better suited for this.
Does this mean that we get wrong time values if threads run on
different cores ?

But cannot use, e.g. X86's "tsc" register
? Value not coherent across CPUs
? Not consistent, e.g. slow ticking in low-power mode
? Monotonic per CPU ? but not per-thread

Thanks,
Mohan

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120111/2fd73143/attachment-0001.html>

From nathan.reynolds at oracle.com  Tue Jan 10 17:32:42 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 10 Jan 2012 15:32:42 -0700
Subject: [concurrency-interest] tsc register
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEFBJCAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCIEFBJCAA.davidcholmes@aapt.net.au>
Message-ID: <4F0CBC8A.7010205@oracle.com>

Hmm... maybe I am thinking of JRockit that uses tsc.  I could be very 
wrong though.  Maybe I am thinking of HotSpot Sparc implementation.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/10/2012 3:16 PM, David Holmes wrote:
> And with guards against a buggy OS not giving correct monotonic time.
> David
>
>     -----Original Message-----
>     *From:* David Holmes [mailto:davidcholmes at aapt.net.au]
>     *Sent:* Wednesday, 11 January 2012 8:15 AM
>     *To:* Vitaly Davidovich; Nathan Reynolds
>     *Cc:* Concurrency-interest at cs.oswego.edu
>     *Subject:* RE: [concurrency-interest] tsc register
>
>     Correct. Hotspot uses/relies-on the high-resolution monotonic time
>     source of the OS, else falls back to plain time-of-day. It never
>     uses the TSC directly.
>     David Holmes
>
>         -----Original Message-----
>         *From:* concurrency-interest-bounces at cs.oswego.edu
>         [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf
>         Of *Vitaly Davidovich
>         *Sent:* Wednesday, 11 January 2012 2:28 AM
>         *To:* Nathan Reynolds
>         *Cc:* Concurrency-interest at cs.oswego.edu
>         *Subject:* Re: [concurrency-interest] tsc register
>
>         I thought JVM (hotspot at least) uses the os monotonic clock
>         source (if present) rather than reading tsc directly and then
>         doing its own adjustments?
>
>         On Jan 10, 2012 11:24 AM, "Nathan Reynolds"
>         <nathan.reynolds at oracle.com
>         <mailto:nathan.reynolds at oracle.com>> wrote:
>
>             The tsc register on older processors did not increment at
>             the same rate.  If a core slept or slowed down then the
>             tsc register would stop or slow down its increments.  More
>             modern processors guarantee that tsc register increments
>             at a fixed frequency.  If you are working on Linux,
>             cpuinfo (?) could report the const_tsc flag.  This means
>             that the processor and OS recognize that this feature is
>             on the processor.
>
>             The tsc register is not synchronized across sockets.  This
>             is something Oracle has asked Intel to enhance many
>             times.  It is a very difficult problem to solve.  However,
>             more modern Linux kernels will (?) synchronize the tsc
>             register at startup so that it is impossible to read the
>             tsc register on two different cores and see that the 2??
>             value is smaller.  This does not mean that the tsc
>             register is synchronized.  It only means that two threads
>             running on different cores will hopefully never see the
>             tsc "move backwards".
>
>             There is no guarantee that once the tsc register is
>             synchronized across sockets that it will remain so.  Some
>             processors are hot swappable.  The newly added processor
>             is not going to have the correct tsc register value. 
>             Furthermore, the OS is free to reset the tsc value at any
>             time.
>
>             If I understand correctly, the HotSpot JVM will guarantee
>             that System.nanoTime() never moves backwards.  It reads
>             the tsc register with each call (?).  It the compares the
>             read value with the last read value.  If the read value is
>             < the last read value, then the last read value is
>             returned.  If the read value is > the last read value,
>             then the last read value is updated and the read value is
>             returned.  Updating the last read value requires a CAS. 
>             This CAS can lead to scalability bottlenecks if
>             System.nanoTime() is called too frequently.  I am not sure
>             if a better algorithm has been devised to fix this CAS
>             contention.  I kind of remember it being talked about.
>
>             I think the JVMs will default to more stable clock sources
>             with worse resolution for nanoTime() if tsc is not
>             behaving well.
>
>             Nathan Reynolds
>             <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>             | Consulting Member of Technical Staff | 602.333.9091
>             <tel:602.333.9091>
>             Oracle PSR Engineering <http://psr.us.oracle.com/> |
>             Server Technology
>
>             On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
>>             Only if you use System.nanoTime().  Time difference might even be
>>             negative if the thread is swapped between different cores.
>>
>>             On 10/01/2012, Mohan Radhakrishnan<radhakrishnan.mohan at gmail.com>  <mailto:radhakrishnan.mohan at gmail.com>  wrote:
>>>             Hi,
>>>
>>>             One more question from the novice and for the novice.
>>>
>>>             I see these points in Dr. click's PPT. Can I know why ? I ask this
>>>             here because it seems to
>>>             involve multiple cores. Maybe the jvm forums are better suited for this.
>>>             Does this mean that we get wrong time values if threads run on
>>>             different cores ?
>>>
>>>             But cannot use, e.g. X86's "tsc" register
>>>             ? Value not coherent across CPUs
>>>             ? Not consistent, e.g. slow ticking in low-power mode
>>>             ? Monotonic per CPU ? but not per-thread
>>>
>>>             Thanks,
>>>             Mohan
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120110/ff52953d/attachment.html>

From heinz at javaspecialists.eu  Tue Jan 10 17:33:01 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 11 Jan 2012 00:33:01 +0200
Subject: [concurrency-interest] tsc register
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>
Message-ID: <4F0CBC9D.7010906@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120111/9a0f6728/attachment.html>

From vitalyd at gmail.com  Tue Jan 10 17:39:06 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 10 Jan 2012 17:39:06 -0500
Subject: [concurrency-interest] tsc register
In-Reply-To: <4F0CBC9D.7010906@javaspecialists.eu>
References: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>
	<4F0CBC9D.7010906@javaspecialists.eu>
Message-ID: <CAHjP37EXrebQcxB6vDMLfxQpBen_oaxOZUTeRpMPViR2VwR0sw@mail.gmail.com>

That's right, there were bugs reported against it, e.g.
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6458294 and the
discussion there for when nanoTime can break its contract.
On Jan 10, 2012 5:33 PM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
wrote:

> **
> What is interesting is that we have had reports of System.nanoTime()
> sometimes counting backwards.  Has nanoTime() always been monotonic?  If
> so, I need to follow up on the claims.  I've heard it from two sources, but
> it might just be hearsay.
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
>
> On 1/11/12 12:14 AM, David Holmes wrote:
>
> Correct. Hotspot uses/relies-on the high-resolution monotonic time source
> of the OS, else falls back to plain time-of-day. It never uses the TSC
> directly.
>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [
> mailto:concurrency-interest-bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
> ]*On Behalf Of *Vitaly Davidovich
> *Sent:* Wednesday, 11 January 2012 2:28 AM
> *To:* Nathan Reynolds
> *Cc:* Concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] tsc register
>
>  I thought JVM (hotspot at least) uses the os monotonic clock source (if
> present) rather than reading tsc directly and then doing its own
> adjustments?
> On Jan 10, 2012 11:24 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>
>> The tsc register on older processors did not increment at the same rate.
>> If a core slept or slowed down then the tsc register would stop or slow
>> down its increments.  More modern processors guarantee that tsc register
>> increments at a fixed frequency.  If you are working on Linux, cpuinfo (?)
>> could report the const_tsc flag.  This means that the processor and OS
>> recognize that this feature is on the processor.
>>
>> The tsc register is not synchronized across sockets.  This is something
>> Oracle has asked Intel to enhance many times.  It is a very difficult
>> problem to solve.  However, more modern Linux kernels will (?) synchronize
>> the tsc register at startup so that it is impossible to read the tsc
>> register on two different cores and see that the 2?? value is smaller.
>> This does not mean that the tsc register is synchronized.  It only means
>> that two threads running on different cores will hopefully never see the
>> tsc "move backwards".
>>
>> There is no guarantee that once the tsc register is synchronized across
>> sockets that it will remain so.  Some processors are hot swappable.  The
>> newly added processor is not going to have the correct tsc register value.
>> Furthermore, the OS is free to reset the tsc value at any time.
>>
>> If I understand correctly, the HotSpot JVM will guarantee that
>> System.nanoTime() never moves backwards.  It reads the tsc register with
>> each call (?).  It the compares the read value with the last read value.
>> If the read value is < the last read value, then the last read value is
>> returned.  If the read value is > the last read value, then the last read
>> value is updated and the read value is returned.  Updating the last read
>> value requires a CAS.  This CAS can lead to scalability bottlenecks if
>> System.nanoTime() is called too frequently.  I am not sure if a better
>> algorithm has been devised to fix this CAS contention.  I kind of remember
>> it being talked about.
>>
>> I think the JVMs will default to more stable clock sources with worse
>> resolution for nanoTime() if tsc is not behaving well.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>> On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
>>
>> Only if you use System.nanoTime().  Time difference might even be
>> negative if the thread is swapped between different cores.
>>
>> On 10/01/2012, Mohan Radhakrishnan <radhakrishnan.mohan at gmail.com> <radhakrishnan.mohan at gmail.com> wrote:
>>
>>
>>  Hi,
>>
>> One more question from the novice and for the novice.
>>
>> I see these points in Dr. click's PPT. Can I know why ? I ask this
>> here because it seems to
>> involve multiple cores. Maybe the jvm forums are better suited for this.
>> Does this mean that we get wrong time values if threads run on
>> different cores ?
>>
>> But cannot use, e.g. X86's "tsc" register
>> ? Value not coherent across CPUs
>> ? Not consistent, e.g. slow ticking in low-power mode
>> ? Monotonic per CPU ? but not per-thread
>>
>> Thanks,
>> Mohan
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>   ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120110/71b67326/attachment-0001.html>

From r.spilker at topdesk.com  Wed Jan 11 08:02:16 2012
From: r.spilker at topdesk.com (Roel Spilker)
Date: Wed, 11 Jan 2012 14:02:16 +0100
Subject: [concurrency-interest] tsc register
In-Reply-To: <4F0CBC9D.7010906@javaspecialists.eu>
References: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>
	<4F0CBC9D.7010906@javaspecialists.eu>
Message-ID: <4F0D8858.6080807@topdesk.com>

Hi Heinz,

I have another datapoint for you.

We've also observed "counting backwards" in our application end of 2009. 
Unfortunately, at the time and under deadline pressure, we just removed 
the usage and did not record any system information. I'm pretty sure 
about the following:
- An at the time recent Java 1.6
- Most likely an Oracle (well, at the time Sun) JDK running -server
- Most likely 64 bit JVM
- Probably running on Windows 7, 64 bit
- Intel processor, multi-core, possibly with hyperthreading enabled

We did notice that this behavior occurred when between two calls we 
accessed the database. Our assumption at the time was that the blocking 
thread was rescheduled to a different core, resulting in inconsistent 
numbers.

Roel


On 10-1-2012 23:33, Dr Heinz M. Kabutz wrote:
> What is interesting is that we have had reports of System.nanoTime() 
> sometimes counting backwards.  Has nanoTime() always been monotonic?  
> If so, I need to follow up on the claims.  I've heard it from two 
> sources, but it might just be hearsay.
> Regards
>
> Heinz
> -- 
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professional
> http://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
> On 1/11/12 12:14 AM, David Holmes wrote:
>> Correct. Hotspot uses/relies-on the high-resolution monotonic time 
>> source of the OS, else falls back to plain time-of-day. It never uses 
>> the TSC directly.
>> David Holmes
>>
>>     -----Original Message-----
>>     *From:* concurrency-interest-bounces at cs.oswego.edu
>>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>>     *Vitaly Davidovich
>>     *Sent:* Wednesday, 11 January 2012 2:28 AM
>>     *To:* Nathan Reynolds
>>     *Cc:* Concurrency-interest at cs.oswego.edu
>>     *Subject:* Re: [concurrency-interest] tsc register
>>
>>     I thought JVM (hotspot at least) uses the os monotonic clock
>>     source (if present) rather than reading tsc directly and then
>>     doing its own adjustments?
>>
>>     On Jan 10, 2012 11:24 AM, "Nathan Reynolds"
>>     <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>>
>>     wrote:
>>
>>         The tsc register on older processors did not increment at the
>>         same rate.  If a core slept or slowed down then the tsc
>>         register would stop or slow down its increments.  More modern
>>         processors guarantee that tsc register increments at a fixed
>>         frequency.  If you are working on Linux, cpuinfo (?) could
>>         report the const_tsc flag.  This means that the processor and
>>         OS recognize that this feature is on the processor.
>>
>>         The tsc register is not synchronized across sockets.  This is
>>         something Oracle has asked Intel to enhance many times.  It
>>         is a very difficult problem to solve.  However, more modern
>>         Linux kernels will (?) synchronize the tsc register at
>>         startup so that it is impossible to read the tsc register on
>>         two different cores and see that the 2^(n)^(d) value is
>>         smaller.  This does not mean that the tsc register is
>>         synchronized.  It only means that two threads running on
>>         different cores will hopefully never see the tsc "move
>>         backwards".
>>
>>         There is no guarantee that once the tsc register is
>>         synchronized across sockets that it will remain so.  Some
>>         processors are hot swappable.  The newly added processor is
>>         not going to have the correct tsc register value. 
>>         Furthermore, the OS is free to reset the tsc value at any time.
>>
>>         If I understand correctly, the HotSpot JVM will guarantee
>>         that System.nanoTime() never moves backwards.  It reads the
>>         tsc register with each call (?).  It the compares the read
>>         value with the last read value.  If the read value is < the
>>         last read value, then the last read value is returned.  If
>>         the read value is > the last read value, then the last read
>>         value is updated and the read value is returned.  Updating
>>         the last read value requires a CAS.  This CAS can lead to
>>         scalability bottlenecks if System.nanoTime() is called too
>>         frequently.  I am not sure if a better algorithm has been
>>         devised to fix this CAS contention.  I kind of remember it
>>         being talked about.
>>
>>         I think the JVMs will default to more stable clock sources
>>         with worse resolution for nanoTime() if tsc is not behaving well.
>>
>>         Nathan Reynolds
>>         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>         Consulting Member of Technical Staff | 602.333.9091
>>         <tel:602.333.9091>
>>         Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>         Technology
>>
>>         On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
>>>         Only if you use System.nanoTime().  Time difference might even be
>>>         negative if the thread is swapped between different cores.
>>>
>>>         On 10/01/2012, Mohan Radhakrishnan<radhakrishnan.mohan at gmail.com>  <mailto:radhakrishnan.mohan at gmail.com>  wrote:
>>>                  
>>>>         Hi,
>>>>
>>>>         One more question from the novice and for the novice.
>>>>
>>>>         I see these points in Dr. click's PPT. Can I know why ? I ask this
>>>>         here because it seems to
>>>>         involve multiple cores. Maybe the jvm forums are better suited for this.
>>>>         Does this mean that we get wrong time values if threads run on
>>>>         different cores ?
>>>>
>>>>         But cannot use, e.g. X86's "tsc" register
>>>>         ? Value not coherent across CPUs
>>>>         ? Not consistent, e.g. slow ticking in low-power mode
>>>>         ? Monotonic per CPU ? but not per-thread
>>>>
>>>>         Thanks,
>>>>         Mohan
>>>>
>>>>         _______________________________________________
>>>>         Concurrency-interest mailing list
>>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>                    
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>    
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120111/be7bf9ab/attachment.html>

From adrian.tarau at gmail.com  Wed Jan 11 17:30:18 2012
From: adrian.tarau at gmail.com (Adrian Tarau)
Date: Wed, 11 Jan 2012 17:30:18 -0500
Subject: [concurrency-interest] PriorityBlockingDeque
Message-ID: <4F0E0D7A.2070202@gmail.com>

It would be really great if a PriorityBlockingDeque will be available in 
the JDK. Any recomandations for an external library?

Thanks,
Adrian Tarau.

From heinz at javaspecialists.eu  Wed Jan 11 17:32:40 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 12 Jan 2012 00:32:40 +0200
Subject: [concurrency-interest] LinkedTransferQueue in ForkJoinPool
In-Reply-To: <4F0D8858.6080807@topdesk.com>
References: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>
	<4F0CBC9D.7010906@javaspecialists.eu>
	<4F0D8858.6080807@topdesk.com>
Message-ID: <4F0E0E08.7050709@javaspecialists.eu>

On my Mac 1.7.0 build 223, the ForkJoinPool still contains a 
LinkedTransferQueue<ForkJoinTask<?>>.

On Linux and Windows, 1.7.0_02 and 1.7.0_01 instead use an array of 
ForkJoinTask.

I had a look on the concurrency interest mailing archives and could not 
find any discussion as to why this was changed.  Do any of you know the 
story behind this?  I have my suspicions, but would prefer to hear it 
from the guys who really know :-)

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz


From viktor.klang at gmail.com  Wed Jan 11 17:43:35 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 11 Jan 2012 23:43:35 +0100
Subject: [concurrency-interest] PriorityBlockingDeque
In-Reply-To: <4F0E0D7A.2070202@gmail.com>
References: <4F0E0D7A.2070202@gmail.com>
Message-ID: <CANPzfU8v+-Q+1Xj3XyYa+GkaGZKb-XUtQunxyiEevYg2TKhPTw@mail.gmail.com>

Not to mention a PriorityBlockingQueue that preserves insertion order for
entries of the same priority ;-)

On Wed, Jan 11, 2012 at 11:30 PM, Adrian Tarau <adrian.tarau at gmail.com>wrote:

> It would be really great if a PriorityBlockingDeque will be available in
> the JDK. Any recomandations for an external library?
>
> Thanks,
> Adrian Tarau.
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120111/a81aeb68/attachment.html>

From heinz at javaspecialists.eu  Wed Jan 11 17:49:27 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 12 Jan 2012 00:49:27 +0200
Subject: [concurrency-interest] PriorityBlockingDeque
In-Reply-To: <CANPzfU8v+-Q+1Xj3XyYa+GkaGZKb-XUtQunxyiEevYg2TKhPTw@mail.gmail.com>
References: <4F0E0D7A.2070202@gmail.com>
	<CANPzfU8v+-Q+1Xj3XyYa+GkaGZKb-XUtQunxyiEevYg2TKhPTw@mail.gmail.com>
Message-ID: <4F0E11F7.1020504@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120112/6f154609/attachment.html>

From viktor.klang at gmail.com  Wed Jan 11 17:59:34 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 11 Jan 2012 23:59:34 +0100
Subject: [concurrency-interest] PriorityBlockingDeque
In-Reply-To: <4F0E11F7.1020504@javaspecialists.eu>
References: <4F0E0D7A.2070202@gmail.com>
	<CANPzfU8v+-Q+1Xj3XyYa+GkaGZKb-XUtQunxyiEevYg2TKhPTw@mail.gmail.com>
	<4F0E11F7.1020504@javaspecialists.eu>
Message-ID: <CANPzfU9bpBicD_q9a366bkPmD2vNCswELoL6RVB6BhDtWzzrtQ@mail.gmail.com>

2012/1/11 Dr Heinz M. Kabutz <heinz at javaspecialists.eu>

> **
> That one is fairly easily worked around.  Just wrap your submission and
> remember the insertion order, then use that in your comparison.
>

You mean adding yet another allocation + memory indirection + add the
overhead of either having to use: AtomicLong (another memory indirection),
a @volatile field + an updater (a bit better but not optimal, or a
@volatile field and the good old sun.misc.Unsafe (my best friend in the
world)? Not only that, all insertions will also be more expensive since it
has to factor the order into the comparison as well.

I think a proper data structure for this would be the way to go, but my
need for such a structure isn't that bad that I can sacrifice time on
researching it.

But yes, for those who are willing to pay, there's generally someone that's
willing to sell :-)

Cheers,
?


> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
>
> On 1/12/12 12:43 AM, ?iktor ?lang wrote:
>
> Not to mention a PriorityBlockingQueue that preserves insertion order for
> entries of the same priority ;-)
>
> On Wed, Jan 11, 2012 at 11:30 PM, Adrian Tarau <adrian.tarau at gmail.com>wrote:
>
>> It would be really great if a PriorityBlockingDeque will be available in
>> the JDK. Any recomandations for an external library?
>>
>> Thanks,
>> Adrian Tarau.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
>  --
> Viktor Klang
>
> Akka Tech Lead
> Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
> Experts
>
> Twitter: @viktorklang
>
> ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120111/ee0c8097/attachment.html>

From heinz at javaspecialists.eu  Wed Jan 11 18:20:56 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 12 Jan 2012 01:20:56 +0200
Subject: [concurrency-interest] PriorityBlockingDeque
In-Reply-To: <4F0E0D7A.2070202@gmail.com>
References: <4F0E0D7A.2070202@gmail.com>
Message-ID: <4F0E1958.8050109@javaspecialists.eu>

What  would really like to see is a PriorityBlockingQueue that can 
upgrade priorities over time.  Living in Greece, we have a lot of 
priority queues.  If your cousin works in the department that does 
electricity connections, you go to the front of the queue.  If you are a 
nobody (like me), you get left at the back of the queue and if enough 
people with cousin's arrive, you never get serviced.  In real life you 
can start shouting and that increases your priority, but in the 
PriorityBlockingQueue, you can get starvation of low-priority jobs that 
then never get done.

I don't think increasing the priority of a job in the queue would work 
in the current implementation.

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz 



On 1/12/12 12:30 AM, Adrian Tarau wrote:
> It would be really great if a PriorityBlockingDeque will be available 
> in the JDK. Any recomandations for an external library?
>
> Thanks,
> Adrian Tarau.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From cheremin at gmail.com  Wed Jan 11 18:32:13 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Thu, 12 Jan 2012 02:32:13 +0300
Subject: [concurrency-interest] PriorityBlockingDeque
In-Reply-To: <4F0E1958.8050109@javaspecialists.eu>
References: <4F0E0D7A.2070202@gmail.com> <4F0E1958.8050109@javaspecialists.eu>
Message-ID: <CAOwENiL_J_WzxOJ4j4vZNSH9DWMk_fmrRczEx0-TbcKfo0KhLg@mail.gmail.com>

I think, PriorityBlockingQueue with ability for changing priorities is
rather exotic case, to be part of JDK. It seems more like good chance
for third party lib...

Offtop: You have really good live, in Greece. Here, in Russia,
shouting does not help against cousins. Only shooting. May be. Didn't
try yet... :)

2012/1/12 Dr Heinz M. Kabutz <heinz at javaspecialists.eu>:
> What ?would really like to see is a PriorityBlockingQueue that can upgrade
> priorities over time. ?Living in Greece, we have a lot of priority queues.
> ?If your cousin works in the department that does electricity connections,
> you go to the front of the queue. ?If you are a nobody (like me), you get
> left at the back of the queue and if enough people with cousin's arrive, you
> never get serviced. ?In real life you can start shouting and that increases
> your priority, but in the PriorityBlockingQueue, you can get starvation of
> low-priority jobs that then never get done.
>
> I don't think increasing the priority of a job in the queue would work in
> the current implementation.
>
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professional
> http://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
> On 1/12/12 12:30 AM, Adrian Tarau wrote:
>>
>> It would be really great if a PriorityBlockingDeque will be available in
>> the JDK. Any recomandations for an external library?
>>
>> Thanks,
>> Adrian Tarau.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Wed Jan 11 23:58:30 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 12 Jan 2012 14:58:30 +1000
Subject: [concurrency-interest] tsc register
In-Reply-To: <4F0CBC9D.7010906@javaspecialists.eu>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEFIJCAA.davidcholmes@aapt.net.au>

Hotspot uses the available OS high-resolution nominally monotonic time source if it exists, else it falls back to a time-of-day source (which is not monotonic). It should be very rare (ie only really old systems) to not have a monotonic timesource available.

Solaris had a number of bugs in this area (because unlike the other OSes that dropped use of the TSC due to its instability, Solaris decided to force it to be stable and synchronized - and occasionally they failed) and so a guard was added to ensure it was actually monotonic.

On Windows if the TSC is being used without using the external utilities/drivers to sync it then QueryPerformanceCounter can be non-monotonic. Similarly on Linux if you set your clocksource to be TSC instead of HPET (and the TSC is not synchronized) then CLOCK_MONOTONIC can also exhibit non-monotonic behaviour.

See bug 6458294 for some info. Sadly, back in November 2006 I reported that we would add the guard logic on all platforms, but it never happened.

All-in-all clocks/counters/timers are a general mess.

David

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M. Kabutz
Sent: Wednesday, 11 January 2012 8:33 AM
To: dholmes at ieee.org
Cc: Concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] tsc register


  What is interesting is that we have had reports of System.nanoTime() sometimes counting backwards.  Has nanoTime() always been monotonic?  If so, I need to follow up on the claims.  I've heard it from two sources, but it might just be hearsay.

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz 


  On 1/11/12 12:14 AM, David Holmes wrote: 
    Correct. Hotspot uses/relies-on the high-resolution monotonic time source of the OS, else falls back to plain time-of-day. It never uses the TSC directly.

    David Holmes
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly Davidovich
      Sent: Wednesday, 11 January 2012 2:28 AM
      To: Nathan Reynolds
      Cc: Concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] tsc register


      I thought JVM (hotspot at least) uses the os monotonic clock source (if present) rather than reading tsc directly and then doing its own adjustments? 

      On Jan 10, 2012 11:24 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com> wrote:

        The tsc register on older processors did not increment at the same rate.  If a core slept or slowed down then the tsc register would stop or slow down its increments.  More modern processors guarantee that tsc register increments at a fixed frequency.  If you are working on Linux, cpuinfo (?) could report the const_tsc flag.  This means that the processor and OS recognize that this feature is on the processor.

        The tsc register is not synchronized across sockets.  This is something Oracle has asked Intel to enhance many times.  It is a very difficult problem to solve.  However, more modern Linux kernels will (?) synchronize the tsc register at startup so that it is impossible to read the tsc register on two different cores and see that the 2?? value is smaller.  This does not mean that the tsc register is synchronized.  It only means that two threads running on different cores will hopefully never see the tsc "move backwards".

        There is no guarantee that once the tsc register is synchronized across sockets that it will remain so.  Some processors are hot swappable.  The newly added processor is not going to have the correct tsc register value.  Furthermore, the OS is free to reset the tsc value at any time.

        If I understand correctly, the HotSpot JVM will guarantee that System.nanoTime() never moves backwards.  It reads the tsc register with each call (?).  It the compares the read value with the last read value.  If the read value is < the last read value, then the last read value is returned.  If the read value is > the last read value, then the last read value is updated and the read value is returned.  Updating the last read value requires a CAS.  This CAS can lead to scalability bottlenecks if System.nanoTime() is called too frequently.  I am not sure if a better algorithm has been devised to fix this CAS contention.  I kind of remember it being talked about.

        I think the JVMs will default to more stable clock sources with worse resolution for nanoTime() if tsc is not behaving well.


        Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
        Oracle PSR Engineering | Server Technology


        On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote: 
Only if you use System.nanoTime().  Time difference might even be
negative if the thread is swapped between different cores.

On 10/01/2012, Mohan Radhakrishnan <radhakrishnan.mohan at gmail.com> wrote:
        Hi,

One more question from the novice and for the novice.

I see these points in Dr. click's PPT. Can I know why ? I ask this
here because it seems to
involve multiple cores. Maybe the jvm forums are better suited for this.
Does this mean that we get wrong time values if threads run on
different cores ?

But cannot use, e.g. X86's "tsc" register
? Value not coherent across CPUs
? Not consistent, e.g. slow ticking in low-power mode
? Monotonic per CPU ? but not per-thread

Thanks,
Mohan

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

          
        _______________________________________________
        Concurrency-interest mailing list
        Concurrency-interest at cs.oswego.edu
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest


----------------------------------------------------------------------------
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120112/4716b994/attachment.html>

From heinz at javaspecialists.eu  Thu Jan 12 03:04:24 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 12 Jan 2012 10:04:24 +0200
Subject: [concurrency-interest] LinkedTransferQueue in ForkJoinPool
In-Reply-To: <CACR_FB7wOHQyw+eT_BXd-YHYve4p5v_CFLw2Cz7x3mkmeR4vaQ@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>	<4F0CBC9D.7010906@javaspecialists.eu>	<4F0D8858.6080807@topdesk.com>	<4F0E0E08.7050709@javaspecialists.eu>
	<CACR_FB7wOHQyw+eT_BXd-YHYve4p5v_CFLw2Cz7x3mkmeR4vaQ@mail.gmail.com>
Message-ID: <4F0E9408.7070406@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120112/cf53a1f9/attachment.html>

From davidcholmes at aapt.net.au  Thu Jan 12 03:10:16 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 12 Jan 2012 18:10:16 +1000
Subject: [concurrency-interest] LinkedTransferQueue in ForkJoinPool
In-Reply-To: <4F0E9408.7070406@javaspecialists.eu>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEFLJCAA.davidcholmes@aapt.net.au>

I suspect it was simply performance. The change went in around the same time
a bunch of work was done to control the creation of threads.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M.
Kabutz
  Sent: Thursday, 12 January 2012 6:04 PM
  To: Howard Lovatt
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] LinkedTransferQueue in ForkJoinPool


  Ah, sorry, I was looking at a much older version of the JDK 7.  You're
right - in 223 it is also using an array.

  However, the question remains - why was it changed from
LinkedTransferQueue to an array?

Regards

Heinz
--
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz


  On 1/12/12 1:22 AM, Howard Lovatt wrote:
    Hi Heinz,


    I just had a quick look at the source for my Mac b223 and it uses an
array accessed via UNSAFE.


    I am probably looking in the wrong place - I looked in the source for
ForkJoinPool.java - where should I look?


    Cheers,


     -- Howard.


    On 12 January 2012 09:32, Dr Heinz M. Kabutz <heinz at javaspecialists.eu>
wrote:

      On my Mac 1.7.0 build 223, the ForkJoinPool still contains a
LinkedTransferQueue<ForkJoinTask<?>>.

      On Linux and Windows, 1.7.0_02 and 1.7.0_01 instead use an array of
ForkJoinTask.

      I had a look on the concurrency interest mailing archives and could
not find any discussion as to why this was changed.  Do any of you know the
story behind this?  I have my suspicions, but would prefer to hear it from
the guys who really know :-)

      Regards

      Heinz
      --
      Dr Heinz M. Kabutz (PhD CompSci)
      Author of "The Java(tm) Specialists' Newsletter"
      Sun Java Champion
      IEEE Certified Software Development Professional
      http://www.javaspecialists.eu
      Tel: +30 69 72 850 460
      Skype: kabutz

      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest






    --
      -- Howard.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120112/9ec45c95/attachment-0001.html>

From radhakrishnan.mohan at gmail.com  Thu Jan 12 07:19:57 2012
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Thu, 12 Jan 2012 17:49:57 +0530
Subject: [concurrency-interest] tsc register
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEFIJCAA.davidcholmes@aapt.net.au>
References: <4F0CBC9D.7010906@javaspecialists.eu>
	<NFBBKALFDCPFIDBNKAPCEEFIJCAA.davidcholmes@aapt.net.au>
Message-ID: <CAOoXFP_GUb4E-ScDqayy+dEEZHOZv=rEnByaWRtXZK7qYvFRqA@mail.gmail.com>

This CR http://cr.openjdk.java.net/~johnc/7117303/webrev.0/ is quite recent.

Mohan

On Thu, Jan 12, 2012 at 10:28 AM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Hotspot uses the available OS high-resolution nominally monotonic time
> source if it exists, else it falls back to a time-of-day source (which is
> not monotonic). It should be very rare (ie only really old systems) to not
> have a monotonic timesource available.
>
> Solaris had a number of bugs in this area (because unlike the other OSes
> that dropped use of the TSC due to its instability, Solaris decided to force
> it to be stable and synchronized - and occasionally they failed) and so a
> guard was added to ensure it was actually monotonic.
>
> On Windows if the TSC is being used without using the external
> utilities/drivers to sync it then QueryPerformanceCounter can be
> non-monotonic. Similarly on Linux if you set your clocksource to be TSC
> instead of HPET (and the TSC is not synchronized) then CLOCK_MONOTONIC can
> also exhibit non-monotonic behaviour.
>
> See bug 6458294 for some info. Sadly, back in November 2006 I reported that
> we would add the guard logic on all platforms, but it never happened.
>
> All-in-all clocks/counters/timers are a general mess.
>
> David
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M.
> Kabutz
> Sent: Wednesday, 11 January 2012 8:33 AM
> To: dholmes at ieee.org
> Cc: Concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] tsc register
>
> What is interesting is that we have had reports of System.nanoTime()
> sometimes counting backwards.? Has nanoTime() always been monotonic?? If so,
> I need to follow up on the claims.? I've heard it from two sources, but it
> might just be hearsay.
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professional
> http://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
>
> On 1/11/12 12:14 AM, David Holmes wrote:
>
> Correct. Hotspot uses/relies-on the high-resolution monotonic time source of
> the OS, else falls back to plain time-of-day. It never uses the TSC
> directly.
>
> David Holmes
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
> Davidovich
> Sent: Wednesday, 11 January 2012 2:28 AM
> To: Nathan Reynolds
> Cc: Concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] tsc register
>
> I thought JVM (hotspot at least) uses the os monotonic clock source (if
> present) rather than reading tsc directly and then doing its own
> adjustments?
>
> On Jan 10, 2012 11:24 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>>
>> The tsc register on older processors did not increment at the same rate.
>> If a core slept or slowed down then the tsc register would stop or slow down
>> its increments.? More modern processors guarantee that tsc register
>> increments at a fixed frequency.? If you are working on Linux, cpuinfo (?)
>> could report the const_tsc flag.? This means that the processor and OS
>> recognize that this feature is on the processor.
>>
>> The tsc register is not synchronized across sockets.? This is something
>> Oracle has asked Intel to enhance many times.? It is a very difficult
>> problem to solve.? However, more modern Linux kernels will (?) synchronize
>> the tsc register at startup so that it is impossible to read the tsc
>> register on two different cores and see that the 2?? value is smaller.? This
>> does not mean that the tsc register is synchronized.? It only means that two
>> threads running on different cores will hopefully never see the tsc "move
>> backwards".
>>
>> There is no guarantee that once the tsc register is synchronized across
>> sockets that it will remain so.? Some processors are hot swappable.? The
>> newly added processor is not going to have the correct tsc register value.
>> Furthermore, the OS is free to reset the tsc value at any time.
>>
>> If I understand correctly, the HotSpot JVM will guarantee that
>> System.nanoTime() never moves backwards.? It reads the tsc register with
>> each call (?).? It the compares the read value with the last read value.? If
>> the read value is < the last read value, then the last read value is
>> returned.? If the read value is > the last read value, then the last read
>> value is updated and the read value is returned.? Updating the last read
>> value requires a CAS.? This CAS can lead to scalability bottlenecks if
>> System.nanoTime() is called too frequently.? I am not sure if a better
>> algorithm has been devised to fix this CAS contention.? I kind of remember
>> it being talked about.
>>
>> I think the JVMs will default to more stable clock sources with worse
>> resolution for nanoTime() if tsc is not behaving well.
>>
>> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
>> Oracle PSR Engineering | Server Technology
>>
>> On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
>>
>> Only if you use System.nanoTime().  Time difference might even be
>> negative if the thread is swapped between different cores.
>>
>> On 10/01/2012, Mohan Radhakrishnan <radhakrishnan.mohan at gmail.com> wrote:
>>
>>
>> Hi,
>>
>> One more question from the novice and for the novice.
>>
>> I see these points in Dr. click's PPT. Can I know why ? I ask this
>> here because it seems to
>> involve multiple cores. Maybe the jvm forums are better suited for this.
>> Does this mean that we get wrong time values if threads run on
>> different cores ?
>>
>> But cannot use, e.g. X86's "tsc" register
>> ? Value not coherent across CPUs
>> ? Not consistent, e.g. slow ticking in low-power mode
>> ? Monotonic per CPU ? but not per-thread
>>
>> Thanks,
>> Mohan
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> ________________________________
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From dl at cs.oswego.edu  Thu Jan 12 07:27:47 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 12 Jan 2012 07:27:47 -0500
Subject: [concurrency-interest] LinkedTransferQueue in ForkJoinPool
In-Reply-To: <4F0E0E08.7050709@javaspecialists.eu>
References: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>	<4F0CBC9D.7010906@javaspecialists.eu>	<4F0D8858.6080807@topdesk.com>
	<4F0E0E08.7050709@javaspecialists.eu>
Message-ID: <4F0ED1C3.6080806@cs.oswego.edu>

On 01/11/12 17:32, Dr Heinz M. Kabutz wrote:
> On my Mac 1.7.0 build 223, the ForkJoinPool still contains a
> LinkedTransferQueue<ForkJoinTask<?>>.

Sheesh. That was more than a year out of date even for JDK7. I hope
that OpenJDK for OSX starts putting out regular releases soon.

The handling of pool submissions has always been a difficult
engineering issue -- in classic ForkJoin use, one would expect
relatively few external submissions that recursively decompose,
which favors a low-overhead single array-based queue. However,
it turns out that a lot of people are using them as "ForkPools"
or plain ExecutorServices rather than ForkJoinPools, with tons
of small non-recursive submissions. This often works OK as is,
but can become highly contended (plus, is hostile to IO-bound tasks).
So I'm now in the midst of replacing with a distributed
random-multi-lane queue, and reworking  stealing and scheduling
to improve throughput in these and other cases.

I'll post more about these upcoming changes as they get closer
(hopefully soon). In the mean time, if anyone is using
ForkJoinPools in this way and is feeling brave or curious,
I've been putting snapshots of jsr166.jar
at http://gee.cs.oswego.edu/dl/wwwtmp/jsr166.jar
Placing this in -Xbootclasspath/p:jsr166.jar (plus possibly
adding to CLASSPATH) replaces the j.u.c versions.


-Doug

From heinz at javaspecialists.eu  Thu Jan 12 07:49:10 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 12 Jan 2012 14:49:10 +0200
Subject: [concurrency-interest] LinkedTransferQueue in ForkJoinPool
In-Reply-To: <4F0ED1C3.6080806@cs.oswego.edu>
References: <NFBBKALFDCPFIDBNKAPCEEFBJCAA.davidcholmes@aapt.net.au>	<4F0CBC9D.7010906@javaspecialists.eu>	<4F0D8858.6080807@topdesk.com>	<4F0E0E08.7050709@javaspecialists.eu>
	<4F0ED1C3.6080806@cs.oswego.edu>
Message-ID: <4F0ED6C6.9060803@javaspecialists.eu>

On 1/12/12 2:27 PM, Doug Lea wrote:
> On 01/11/12 17:32, Dr Heinz M. Kabutz wrote:
>> On my Mac 1.7.0 build 223, the ForkJoinPool still contains a
>> LinkedTransferQueue<ForkJoinTask<?>>.
>
> Sheesh. That was more than a year out of date even for JDK7. I hope
> that OpenJDK for OSX starts putting out regular releases soon.
Yes, I had my IDE pointing at the Java 7 sources from Jul 2011.  My 
mistake.  However, we're not at 1.7.0_02 yet on the Mac.  The bug in the 
Random constructor is still there, causing ThreadLocalRandom to always 
be seeded to zero.  But I believe we'll be on the same release soon on 
the Mac.
>
> The handling of pool submissions has always been a difficult
> engineering issue -- in classic ForkJoin use, one would expect
> relatively few external submissions that recursively decompose,
> which favors a low-overhead single array-based queue. However,
> it turns out that a lot of people are using them as "ForkPools"
> or plain ExecutorServices rather than ForkJoinPools, with tons
> of small non-recursive submissions. This often works OK as is,
> but can become highly contended (plus, is hostile to IO-bound tasks).
> So I'm now in the midst of replacing with a distributed
> random-multi-lane queue, and reworking  stealing and scheduling
> to improve throughput in these and other cases.
>
> I'll post more about these upcoming changes as they get closer
> (hopefully soon). In the mean time, if anyone is using
> ForkJoinPools in this way and is feeling brave or curious,
> I've been putting snapshots of jsr166.jar
> at http://gee.cs.oswego.edu/dl/wwwtmp/jsr166.jar
> Placing this in -Xbootclasspath/p:jsr166.jar (plus possibly
> adding to CLASSPATH) replaces the j.u.c versions.
Very interesting, thanks :-)

From dl at cs.oswego.edu  Thu Jan 12 09:37:43 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 12 Jan 2012 09:37:43 -0500
Subject: [concurrency-interest] Adding ForkJoinPool.setPoolSizeLimit
Message-ID: <4F0EF037.1030707@cs.oswego.edu>


A series of early prereleases of ForkJoinPool included some
configuration methods/options similar to those of ThreadPoolExecutor.
But we found that setting them almost never did anything
sensible, so they were removed. The main reason is that
the pool must be able to create "continuation" threads to
avoid starvation when tasks become blocked on joins;
any dynamic configuration changes that interfere with this
can lead to either starvation or runaway growth, and there's
pretty much nothing users can do about it until it is too late.

However, there are some legitimate use cases for supporting
some advisory dynamic resource control. So I'm contemplating
adding the following. Opinions are welcome from those of you who
have come up against a situation where this kind of control,
or something else, would be necessary or helpful.



     /**
      * Sets the limit for the number of worker threads created when
      * new tasks or subtasks become available to execute. This value
      * may be either less than or greater than the parallelism level
      * (but at least 1 and at most an implementation-defined capacity
      * serving as the default value). Setting this value need not
      * immediately decrease or increase the pool size, but will
      * gradually take effect as new threads are needed or existing
      * threads complete tasks. Setting to a value too low to
      * accommodate the minimum required parallelism of a set of
      * dependent tasks may cause starvation -- all processing may
      * stall indefinitely. But assuming this constraint is met, using
      * setPoolSizeLimit may be preferable to constructing a
      * ForkJoinPool with non-default parallelism value.
      *
      * @param limit the new target limit
      * @throws IllegalArgumentException if limit is less than one
      * or greater than this implementation's maximum capacity.
      */
     public void setPoolSizeLimit(int limit) { ...


From viktor.klang at gmail.com  Thu Jan 12 09:57:21 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 12 Jan 2012 15:57:21 +0100
Subject: [concurrency-interest] Adding ForkJoinPool.setPoolSizeLimit
In-Reply-To: <4F0EF037.1030707@cs.oswego.edu>
References: <4F0EF037.1030707@cs.oswego.edu>
Message-ID: <CANPzfU-CNFpeoFWp=UTUZ_6uUwCQy73eNwdyqa8QN3FcK0S6oA@mail.gmail.com>

Hey Doug,

a couple of quick questions:

1) Should there be any possible backpressure for the FJP?
2) What's the current behavior for submissions during shutdown?

Cheers,
?

On Thu, Jan 12, 2012 at 3:37 PM, Doug Lea <dl at cs.oswego.edu> wrote:

>
> A series of early prereleases of ForkJoinPool included some
> configuration methods/options similar to those of ThreadPoolExecutor.
> But we found that setting them almost never did anything
> sensible, so they were removed. The main reason is that
> the pool must be able to create "continuation" threads to
> avoid starvation when tasks become blocked on joins;
> any dynamic configuration changes that interfere with this
> can lead to either starvation or runaway growth, and there's
> pretty much nothing users can do about it until it is too late.
>
> However, there are some legitimate use cases for supporting
> some advisory dynamic resource control. So I'm contemplating
> adding the following. Opinions are welcome from those of you who
> have come up against a situation where this kind of control,
> or something else, would be necessary or helpful.
>
>
>
>    /**
>     * Sets the limit for the number of worker threads created when
>     * new tasks or subtasks become available to execute. This value
>     * may be either less than or greater than the parallelism level
>     * (but at least 1 and at most an implementation-defined capacity
>     * serving as the default value). Setting this value need not
>     * immediately decrease or increase the pool size, but will
>     * gradually take effect as new threads are needed or existing
>     * threads complete tasks. Setting to a value too low to
>     * accommodate the minimum required parallelism of a set of
>     * dependent tasks may cause starvation -- all processing may
>     * stall indefinitely. But assuming this constraint is met, using
>     * setPoolSizeLimit may be preferable to constructing a
>     * ForkJoinPool with non-default parallelism value.
>     *
>     * @param limit the new target limit
>     * @throws IllegalArgumentException if limit is less than one
>     * or greater than this implementation's maximum capacity.
>     */
>    public void setPoolSizeLimit(int limit) { ...
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120112/5c3b56a2/attachment.html>

From dl at cs.oswego.edu  Thu Jan 12 15:08:24 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 12 Jan 2012 15:08:24 -0500
Subject: [concurrency-interest] Adding ForkJoinPool.setPoolSizeLimit
In-Reply-To: <CANPzfU-CNFpeoFWp=UTUZ_6uUwCQy73eNwdyqa8QN3FcK0S6oA@mail.gmail.com>
References: <4F0EF037.1030707@cs.oswego.edu>
	<CANPzfU-CNFpeoFWp=UTUZ_6uUwCQy73eNwdyqa8QN3FcK0S6oA@mail.gmail.com>
Message-ID: <4F0F3DB8.3010504@cs.oswego.edu>

As I was just reminded, one benefit of FJP currently
NOT having dynamic configuration methods is that
different parts of a system all using the same FJP
cannot hurt each others usages by changing settings
in a way that only work well for one part. This
benefit seems worth preserving.

Which makes an alternative (once suggested by Viktor
and others) that doesn't expose control more attractive:

There already is a way to limit pool size: Use a
ForkJoinPool.ForkJoinWorkerThreadFactory that returns
null when it doesn't want to create additional
threads due to resource constraints. FJP copes
with null returns from ForkJoinWorkerThreadFactory.newThread,
and does so reasonably cheaply (which can/will be
made a bit cheaper still). Because the pool
is provided as an argument to newThread, the
factory can check the current size and state when
doing so.

For management and monitoring purposes this option
would be nicer if ForkJoinWorkerThreadFactory had
a second method
   void terminated(ForkJoinWorkerThread w)
that is invoked when a worker thread is terminated.
Only full termination seems possible -- we cannot reuse
these threads unless we change specs about
UncaughtExceptionHandlers, etc.
And anyway, creation/termination cycles tend to be long
enough in FJ that the extra bookkeeping necessary to reuse vs
recreate would rarely pay for itself.

Because ForkJoinWorkerThreadFactory is a nested
interface, doing this would be reasonably straightforward.
We could define:

public static interface ManagedForkJoinWorkerThreadFactory
   extends ForkJoinWorkerThreadFactory {
   void terminated(ForkJoinWorkerThread w);
}

And then add a few instanceof checks internally
to invoke terminated() if the provided factory
supports it.

If we take this option, then we should further consider
any other methods to add to this subinterface while we are
at it. Suggestions welcome.

Note though that even without a termination callback,
the existing mechanics above can minimally suffice (although
in a messier way), because of the opportunity for status probes
inside method newThread -- the main limitation is that these
queries can tell you only how many threads there are, but not
which threads are still alive.

Some replies:


On 01/12/12 09:57, ?iktor ?lang wrote:

> 1) Should there be any possible backpressure for the FJP?

Do you have something in mind beyond the things I mentioned above?

> 2) What's the current behavior for submissions during shutdown?

The same as ThreadPoolExecutor. Upon shutdown, no new
submissions are accepted but existing tasks are processed
until the pool is quiescent, at which point it terminates.

-Doug




From viktor.klang at gmail.com  Thu Jan 12 15:45:26 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 12 Jan 2012 21:45:26 +0100
Subject: [concurrency-interest] Adding ForkJoinPool.setPoolSizeLimit
In-Reply-To: <4F0F3DB8.3010504@cs.oswego.edu>
References: <4F0EF037.1030707@cs.oswego.edu>
	<CANPzfU-CNFpeoFWp=UTUZ_6uUwCQy73eNwdyqa8QN3FcK0S6oA@mail.gmail.com>
	<4F0F3DB8.3010504@cs.oswego.edu>
Message-ID: <CANPzfU8-L+o6ESJoVyEpHBKD2Ctfbo0nk524tcCP+fzkjU3-TQ@mail.gmail.com>

2012/1/12 Doug Lea <dl at cs.oswego.edu>

> As I was just reminded, one benefit of FJP currently
> NOT having dynamic configuration methods is that
> different parts of a system all using the same FJP
> cannot hurt each others usages by changing settings
> in a way that only work well for one part. This
> benefit seems worth preserving.
>
> Which makes an alternative (once suggested by Viktor
> and others) that doesn't expose control more attractive:
>

Yeah, this is very nice.


>
> There already is a way to limit pool size: Use a
> ForkJoinPool.**ForkJoinWorkerThreadFactory that returns
> null when it doesn't want to create additional
> threads due to resource constraints. FJP copes
> with null returns from ForkJoinWorkerThreadFactory.**newThread,
> and does so reasonably cheaply (which can/will be
> made a bit cheaper still). Because the pool
> is provided as an argument to newThread, the
> factory can check the current size and state when
> doing so.
>

Excellent tip.


>
> For management and monitoring purposes this option
> would be nicer if ForkJoinWorkerThreadFactory had
> a second method
>  void terminated(**ForkJoinWorkerThread w)
> that is invoked when a worker thread is terminated.
>

Sounds like Pool-style behavior, cool.


> Only full termination seems possible -- we cannot reuse
> these threads unless we change specs about
> UncaughtExceptionHandlers, etc.
> And anyway, creation/termination cycles tend to be long
> enough in FJ that the extra bookkeeping necessary to reuse vs
> recreate would rarely pay for itself.
>
> Because ForkJoinWorkerThreadFactory is a nested
> interface, doing this would be reasonably straightforward.
> We could define:
>
> public static interface ManagedForkJoinWorkerThreadFac**tory
>  extends ForkJoinWorkerThreadFactory {
>  void terminated(**ForkJoinWorkerThread w);
> }
>
> And then add a few instanceof checks internally
> to invoke terminated() if the provided factory
> supports it.
>
> If we take this option, then we should further consider
> any other methods to add to this subinterface while we are
> at it. Suggestions welcome.
>
> Note though that even without a termination callback,
> the existing mechanics above can minimally suffice (although
> in a messier way), because of the opportunity for status probes
> inside method newThread -- the main limitation is that these
> queries can tell you only how many threads there are, but not
> which threads are still alive.
>
> Some replies:
>
>
>
> On 01/12/12 09:57, ?iktor ?lang wrote:
>
>  1) Should there be any possible backpressure for the FJP?
>>
>
> Do you have something in mind beyond the things I mentioned above?


Nevermind, I've already come up with a way to solve this on a higher level
:-)


>
>
>  2) What's the current behavior for submissions during shutdown?
>>
>
> The same as ThreadPoolExecutor. Upon shutdown, no new
> submissions are accepted but existing tasks are processed
> until the pool is quiescent, at which point it terminates.


We're talking RejectedExecutionException? If so, then that's completely
fine with me.

Cheers,
?


>
> -Doug
>
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120112/eec21a69/attachment.html>

From davidcholmes at aapt.net.au  Thu Jan 12 16:39:26 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 13 Jan 2012 07:39:26 +1000
Subject: [concurrency-interest] tsc register
In-Reply-To: <CAOoXFP_GUb4E-ScDqayy+dEEZHOZv=rEnByaWRtXZK7qYvFRqA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEFOJCAA.davidcholmes@aapt.net.au>

Mohan Radhakrishnan writes:
> This CR http://cr.openjdk.java.net/~johnc/7117303/webrev.0/ is 
> quite recent.

Yes but I don't see the relevance. As it says there the VM code was assuming a monotonic time source but was using os::javaTimeMillis which is most definitely not monotonic.

David
-----
 
> Mohan
> 
> On Thu, Jan 12, 2012 at 10:28 AM, David Holmes 
> <davidcholmes at aapt.net.au> wrote:
> > Hotspot uses the available OS high-resolution nominally monotonic time
> > source if it exists, else it falls back to a time-of-day source 
> (which is
> > not monotonic). It should be very rare (ie only really old 
> systems) to not
> > have a monotonic timesource available.
> >
> > Solaris had a number of bugs in this area (because unlike the other OSes
> > that dropped use of the TSC due to its instability, Solaris 
> decided to force
> > it to be stable and synchronized - and occasionally they 
> failed) and so a
> > guard was added to ensure it was actually monotonic.
> >
> > On Windows if the TSC is being used without using the external
> > utilities/drivers to sync it then QueryPerformanceCounter can be
> > non-monotonic. Similarly on Linux if you set your clocksource to be TSC
> > instead of HPET (and the TSC is not synchronized) then 
> CLOCK_MONOTONIC can
> > also exhibit non-monotonic behaviour.
> >
> > See bug 6458294 for some info. Sadly, back in November 2006 I 
> reported that
> > we would add the guard logic on all platforms, but it never happened.
> >
> > All-in-all clocks/counters/timers are a general mess.
> >
> > David
> >
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of 
> Dr Heinz M.
> > Kabutz
> > Sent: Wednesday, 11 January 2012 8:33 AM
> > To: dholmes at ieee.org
> > Cc: Concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] tsc register
> >
> > What is interesting is that we have had reports of System.nanoTime()
> > sometimes counting backwards.  Has nanoTime() always been 
> monotonic?  If so,
> > I need to follow up on the claims.  I've heard it from two 
> sources, but it
> > might just be hearsay.
> >
> > Regards
> >
> > Heinz
> > --
> > Dr Heinz M. Kabutz (PhD CompSci)
> > Author of "The Java(tm) Specialists' Newsletter"
> > Sun Java Champion
> > IEEE Certified Software Development Professional
> > http://www.javaspecialists.eu
> > Tel: +30 69 72 850 460
> > Skype: kabutz
> >
> >
> >
> > On 1/11/12 12:14 AM, David Holmes wrote:
> >
> > Correct. Hotspot uses/relies-on the high-resolution monotonic 
> time source of
> > the OS, else falls back to plain time-of-day. It never uses the TSC
> > directly.
> >
> > David Holmes
> >
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
> > Davidovich
> > Sent: Wednesday, 11 January 2012 2:28 AM
> > To: Nathan Reynolds
> > Cc: Concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] tsc register
> >
> > I thought JVM (hotspot at least) uses the os monotonic clock source (if
> > present) rather than reading tsc directly and then doing its own
> > adjustments?
> >
> > On Jan 10, 2012 11:24 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> > wrote:
> >>
> >> The tsc register on older processors did not increment at the 
> same rate.
> >> If a core slept or slowed down then the tsc register would 
> stop or slow down
> >> its increments.  More modern processors guarantee that tsc register
> >> increments at a fixed frequency.  If you are working on Linux, 
> cpuinfo (?)
> >> could report the const_tsc flag.  This means that the processor and OS
> >> recognize that this feature is on the processor.
> >>
> >> The tsc register is not synchronized across sockets.  This is something
> >> Oracle has asked Intel to enhance many times.  It is a very difficult
> >> problem to solve.  However, more modern Linux kernels will (?) 
> synchronize
> >> the tsc register at startup so that it is impossible to read the tsc
> >> register on two different cores and see that the 2?? value is 
> smaller.  This
> >> does not mean that the tsc register is synchronized.  It only 
> means that two
> >> threads running on different cores will hopefully never see 
> the tsc "move
> >> backwards".
> >>
> >> There is no guarantee that once the tsc register is synchronized across
> >> sockets that it will remain so.  Some processors are hot 
> swappable.  The
> >> newly added processor is not going to have the correct tsc 
> register value.
> >> Furthermore, the OS is free to reset the tsc value at any time.
> >>
> >> If I understand correctly, the HotSpot JVM will guarantee that
> >> System.nanoTime() never moves backwards.  It reads the tsc 
> register with
> >> each call (?).  It the compares the read value with the last 
> read value.  If
> >> the read value is < the last read value, then the last read value is
> >> returned.  If the read value is > the last read value, then 
> the last read
> >> value is updated and the read value is returned.  Updating the 
> last read
> >> value requires a CAS.  This CAS can lead to scalability bottlenecks if
> >> System.nanoTime() is called too frequently.  I am not sure if a better
> >> algorithm has been devised to fix this CAS contention.  I kind 
> of remember
> >> it being talked about.
> >>
> >> I think the JVMs will default to more stable clock sources with worse
> >> resolution for nanoTime() if tsc is not behaving well.
> >>
> >> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
> >> Oracle PSR Engineering | Server Technology
> >>
> >> On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
> >>
> >> Only if you use System.nanoTime().  Time difference might even be
> >> negative if the thread is swapped between different cores.
> >>
> >> On 10/01/2012, Mohan Radhakrishnan 
> <radhakrishnan.mohan at gmail.com> wrote:
> >>
> >>
> >> Hi,
> >>
> >> One more question from the novice and for the novice.
> >>
> >> I see these points in Dr. click's PPT. Can I know why ? I ask this
> >> here because it seems to
> >> involve multiple cores. Maybe the jvm forums are better suited 
> for this.
> >> Does this mean that we get wrong time values if threads run on
> >> different cores ?
> >>
> >> But cannot use, e.g. X86's "tsc" register
> >> ? Value not coherent across CPUs
> >> ? Not consistent, e.g. slow ticking in low-power mode
> >> ? Monotonic per CPU ? but not per-thread
> >>
> >> Thanks,
> >> Mohan
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> > ________________________________
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From radhakrishnan.mohan at gmail.com  Thu Jan 12 23:58:00 2012
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Fri, 13 Jan 2012 10:28:00 +0530
Subject: [concurrency-interest] tsc register
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEFOJCAA.davidcholmes@aapt.net.au>
References: <CAOoXFP_GUb4E-ScDqayy+dEEZHOZv=rEnByaWRtXZK7qYvFRqA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEFOJCAA.davidcholmes@aapt.net.au>
Message-ID: <CAOoXFP-j0UDvXh9Yh8hSHAGu+m109VqN7oTiTyCUnyKuF7aY3Q@mail.gmail.com>

I meant that since this CR is recent even newer VM's could be affected
? I think if I were to set the thread affinity using JNI and
continuosly run java threads then that is the test case. Am I right ?

I am mostly interested in tyring to understand and explain a test to
others. We do run some heavy batch schedulers in a critical banking
application on multiple cores using Jdk 5 on HP-UX. The problem is
that we haven't seen any problem. Sometimes when these types of
financial applications fail we assume something and restart them.

As was pointed out such timer failures are extremely rare on multiple
cores even if they happen and newer hardware is better. That is what I
have understood :-)

Thanks,
Mohan

On Fri, Jan 13, 2012 at 3:09 AM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Mohan Radhakrishnan writes:
>> This CR http://cr.openjdk.java.net/~johnc/7117303/webrev.0/ is
>> quite recent.
>
> Yes but I don't see the relevance. As it says there the VM code was assuming a monotonic time source but was using os::javaTimeMillis which is most definitely not monotonic.
>
> David
> -----
>
>> Mohan
>>
>> On Thu, Jan 12, 2012 at 10:28 AM, David Holmes
>> <davidcholmes at aapt.net.au> wrote:
>> > Hotspot uses the available OS high-resolution nominally monotonic time
>> > source if it exists, else it falls back to a time-of-day source
>> (which is
>> > not monotonic). It should be very rare (ie only really old
>> systems) to not
>> > have a monotonic timesource available.
>> >
>> > Solaris had a number of bugs in this area (because unlike the other OSes
>> > that dropped use of the TSC due to its instability, Solaris
>> decided to force
>> > it to be stable and synchronized - and occasionally they
>> failed) and so a
>> > guard was added to ensure it was actually monotonic.
>> >
>> > On Windows if the TSC is being used without using the external
>> > utilities/drivers to sync it then QueryPerformanceCounter can be
>> > non-monotonic. Similarly on Linux if you set your clocksource to be TSC
>> > instead of HPET (and the TSC is not synchronized) then
>> CLOCK_MONOTONIC can
>> > also exhibit non-monotonic behaviour.
>> >
>> > See bug 6458294 for some info. Sadly, back in November 2006 I
>> reported that
>> > we would add the guard logic on all platforms, but it never happened.
>> >
>> > All-in-all clocks/counters/timers are a general mess.
>> >
>> > David
>> >
>> > -----Original Message-----
>> > From: concurrency-interest-bounces at cs.oswego.edu
>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> Dr Heinz M.
>> > Kabutz
>> > Sent: Wednesday, 11 January 2012 8:33 AM
>> > To: dholmes at ieee.org
>> > Cc: Concurrency-interest at cs.oswego.edu
>> > Subject: Re: [concurrency-interest] tsc register
>> >
>> > What is interesting is that we have had reports of System.nanoTime()
>> > sometimes counting backwards. ?Has nanoTime() always been
>> monotonic? ?If so,
>> > I need to follow up on the claims. ?I've heard it from two
>> sources, but it
>> > might just be hearsay.
>> >
>> > Regards
>> >
>> > Heinz
>> > --
>> > Dr Heinz M. Kabutz (PhD CompSci)
>> > Author of "The Java(tm) Specialists' Newsletter"
>> > Sun Java Champion
>> > IEEE Certified Software Development Professional
>> > http://www.javaspecialists.eu
>> > Tel: +30 69 72 850 460
>> > Skype: kabutz
>> >
>> >
>> >
>> > On 1/11/12 12:14 AM, David Holmes wrote:
>> >
>> > Correct. Hotspot uses/relies-on the high-resolution monotonic
>> time source of
>> > the OS, else falls back to plain time-of-day. It never uses the TSC
>> > directly.
>> >
>> > David Holmes
>> >
>> > -----Original Message-----
>> > From: concurrency-interest-bounces at cs.oswego.edu
>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
>> > Davidovich
>> > Sent: Wednesday, 11 January 2012 2:28 AM
>> > To: Nathan Reynolds
>> > Cc: Concurrency-interest at cs.oswego.edu
>> > Subject: Re: [concurrency-interest] tsc register
>> >
>> > I thought JVM (hotspot at least) uses the os monotonic clock source (if
>> > present) rather than reading tsc directly and then doing its own
>> > adjustments?
>> >
>> > On Jan 10, 2012 11:24 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>> > wrote:
>> >>
>> >> The tsc register on older processors did not increment at the
>> same rate.
>> >> If a core slept or slowed down then the tsc register would
>> stop or slow down
>> >> its increments. ?More modern processors guarantee that tsc register
>> >> increments at a fixed frequency. ?If you are working on Linux,
>> cpuinfo (?)
>> >> could report the const_tsc flag. ?This means that the processor and OS
>> >> recognize that this feature is on the processor.
>> >>
>> >> The tsc register is not synchronized across sockets. ?This is something
>> >> Oracle has asked Intel to enhance many times. ?It is a very difficult
>> >> problem to solve. ?However, more modern Linux kernels will (?)
>> synchronize
>> >> the tsc register at startup so that it is impossible to read the tsc
>> >> register on two different cores and see that the 2?? value is
>> smaller. ?This
>> >> does not mean that the tsc register is synchronized. ?It only
>> means that two
>> >> threads running on different cores will hopefully never see
>> the tsc "move
>> >> backwards".
>> >>
>> >> There is no guarantee that once the tsc register is synchronized across
>> >> sockets that it will remain so. ?Some processors are hot
>> swappable. ?The
>> >> newly added processor is not going to have the correct tsc
>> register value.
>> >> Furthermore, the OS is free to reset the tsc value at any time.
>> >>
>> >> If I understand correctly, the HotSpot JVM will guarantee that
>> >> System.nanoTime() never moves backwards. ?It reads the tsc
>> register with
>> >> each call (?). ?It the compares the read value with the last
>> read value. ?If
>> >> the read value is < the last read value, then the last read value is
>> >> returned. ?If the read value is > the last read value, then
>> the last read
>> >> value is updated and the read value is returned. ?Updating the
>> last read
>> >> value requires a CAS. ?This CAS can lead to scalability bottlenecks if
>> >> System.nanoTime() is called too frequently. ?I am not sure if a better
>> >> algorithm has been devised to fix this CAS contention. ?I kind
>> of remember
>> >> it being talked about.
>> >>
>> >> I think the JVMs will default to more stable clock sources with worse
>> >> resolution for nanoTime() if tsc is not behaving well.
>> >>
>> >> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
>> >> Oracle PSR Engineering | Server Technology
>> >>
>> >> On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
>> >>
>> >> Only if you use System.nanoTime(). ?Time difference might even be
>> >> negative if the thread is swapped between different cores.
>> >>
>> >> On 10/01/2012, Mohan Radhakrishnan
>> <radhakrishnan.mohan at gmail.com> wrote:
>> >>
>> >>
>> >> Hi,
>> >>
>> >> One more question from the novice and for the novice.
>> >>
>> >> I see these points in Dr. click's PPT. Can I know why ? I ask this
>> >> here because it seems to
>> >> involve multiple cores. Maybe the jvm forums are better suited
>> for this.
>> >> Does this mean that we get wrong time values if threads run on
>> >> different cores ?
>> >>
>> >> But cannot use, e.g. X86's "tsc" register
>> >> ? Value not coherent across CPUs
>> >> ? Not consistent, e.g. slow ticking in low-power mode
>> >> ? Monotonic per CPU ? but not per-thread
>> >>
>> >> Thanks,
>> >> Mohan
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> >>
>> >>
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> > ________________________________
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>


From davidcholmes at aapt.net.au  Fri Jan 13 00:13:08 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 13 Jan 2012 15:13:08 +1000
Subject: [concurrency-interest] tsc register
In-Reply-To: <CAOoXFP-j0UDvXh9Yh8hSHAGu+m109VqN7oTiTyCUnyKuF7aY3Q@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEHCJCAA.davidcholmes@aapt.net.au>

Mohan Radhakrishnan writes:
> I meant that since this CR is recent even newer VM's could be affected

That CR has nothing to do with the TSC.

> ? I think if I were to set the thread affinity using JNI and
> continuosly run java threads then that is the test case. Am I right ?

If you bind a thread to a given core (using JNI) and that thread reads the TSC then it should always be reading the same TSC. But remember there are two issues with using the basic TSC: synchronization across processors and stability of the frequency of updates. Thread affinity only addresses one part.
 
> I am mostly interested in tyring to understand and explain a test to
> others. We do run some heavy batch schedulers in a critical banking
> application on multiple cores using Jdk 5 on HP-UX. The problem is
> that we haven't seen any problem. Sometimes when these types of
> financial applications fail we assume something and restart them.

???
 
> As was pointed out such timer failures are extremely rare on multiple
> cores even if they happen and newer hardware is better. That is what I
> have understood :-)

Unsynchronized TSC is not at all extremely rare - quite the contrary, if you have a multi-core/multi-processor system that does not support the latest synchronized TSC hardware, or else does not run special TSC-synchronizing software, then you have unsynchronized TSC.

David Holmes
------------

> Thanks,
> Mohan
> 
> On Fri, Jan 13, 2012 at 3:09 AM, David Holmes 
> <davidcholmes at aapt.net.au> wrote:
> > Mohan Radhakrishnan writes:
> >> This CR http://cr.openjdk.java.net/~johnc/7117303/webrev.0/ is
> >> quite recent.
> >
> > Yes but I don't see the relevance. As it says there the VM code 
> was assuming a monotonic time source but was using 
> os::javaTimeMillis which is most definitely not monotonic.
> >
> > David
> > -----
> >
> >> Mohan
> >>
> >> On Thu, Jan 12, 2012 at 10:28 AM, David Holmes
> >> <davidcholmes at aapt.net.au> wrote:
> >> > Hotspot uses the available OS high-resolution nominally 
> monotonic time
> >> > source if it exists, else it falls back to a time-of-day source
> >> (which is
> >> > not monotonic). It should be very rare (ie only really old
> >> systems) to not
> >> > have a monotonic timesource available.
> >> >
> >> > Solaris had a number of bugs in this area (because unlike 
> the other OSes
> >> > that dropped use of the TSC due to its instability, Solaris
> >> decided to force
> >> > it to be stable and synchronized - and occasionally they
> >> failed) and so a
> >> > guard was added to ensure it was actually monotonic.
> >> >
> >> > On Windows if the TSC is being used without using the external
> >> > utilities/drivers to sync it then QueryPerformanceCounter can be
> >> > non-monotonic. Similarly on Linux if you set your 
> clocksource to be TSC
> >> > instead of HPET (and the TSC is not synchronized) then
> >> CLOCK_MONOTONIC can
> >> > also exhibit non-monotonic behaviour.
> >> >
> >> > See bug 6458294 for some info. Sadly, back in November 2006 I
> >> reported that
> >> > we would add the guard logic on all platforms, but it never happened.
> >> >
> >> > All-in-all clocks/counters/timers are a general mess.
> >> >
> >> > David
> >> >
> >> > -----Original Message-----
> >> > From: concurrency-interest-bounces at cs.oswego.edu
> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >> Dr Heinz M.
> >> > Kabutz
> >> > Sent: Wednesday, 11 January 2012 8:33 AM
> >> > To: dholmes at ieee.org
> >> > Cc: Concurrency-interest at cs.oswego.edu
> >> > Subject: Re: [concurrency-interest] tsc register
> >> >
> >> > What is interesting is that we have had reports of System.nanoTime()
> >> > sometimes counting backwards.  Has nanoTime() always been
> >> monotonic?  If so,
> >> > I need to follow up on the claims.  I've heard it from two
> >> sources, but it
> >> > might just be hearsay.
> >> >
> >> > Regards
> >> >
> >> > Heinz
> >> > --
> >> > Dr Heinz M. Kabutz (PhD CompSci)
> >> > Author of "The Java(tm) Specialists' Newsletter"
> >> > Sun Java Champion
> >> > IEEE Certified Software Development Professional
> >> > http://www.javaspecialists.eu
> >> > Tel: +30 69 72 850 460
> >> > Skype: kabutz
> >> >
> >> >
> >> >
> >> > On 1/11/12 12:14 AM, David Holmes wrote:
> >> >
> >> > Correct. Hotspot uses/relies-on the high-resolution monotonic
> >> time source of
> >> > the OS, else falls back to plain time-of-day. It never uses the TSC
> >> > directly.
> >> >
> >> > David Holmes
> >> >
> >> > -----Original Message-----
> >> > From: concurrency-interest-bounces at cs.oswego.edu
> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf 
> Of Vitaly
> >> > Davidovich
> >> > Sent: Wednesday, 11 January 2012 2:28 AM
> >> > To: Nathan Reynolds
> >> > Cc: Concurrency-interest at cs.oswego.edu
> >> > Subject: Re: [concurrency-interest] tsc register
> >> >
> >> > I thought JVM (hotspot at least) uses the os monotonic clock 
> source (if
> >> > present) rather than reading tsc directly and then doing its own
> >> > adjustments?
> >> >
> >> > On Jan 10, 2012 11:24 AM, "Nathan Reynolds" 
> <nathan.reynolds at oracle.com>
> >> > wrote:
> >> >>
> >> >> The tsc register on older processors did not increment at the
> >> same rate.
> >> >> If a core slept or slowed down then the tsc register would
> >> stop or slow down
> >> >> its increments.  More modern processors guarantee that tsc register
> >> >> increments at a fixed frequency.  If you are working on Linux,
> >> cpuinfo (?)
> >> >> could report the const_tsc flag.  This means that the 
> processor and OS
> >> >> recognize that this feature is on the processor.
> >> >>
> >> >> The tsc register is not synchronized across sockets.  This 
> is something
> >> >> Oracle has asked Intel to enhance many times.  It is a very 
> difficult
> >> >> problem to solve.  However, more modern Linux kernels will (?)
> >> synchronize
> >> >> the tsc register at startup so that it is impossible to read the tsc
> >> >> register on two different cores and see that the 2?? value is
> >> smaller.  This
> >> >> does not mean that the tsc register is synchronized.  It only
> >> means that two
> >> >> threads running on different cores will hopefully never see
> >> the tsc "move
> >> >> backwards".
> >> >>
> >> >> There is no guarantee that once the tsc register is 
> synchronized across
> >> >> sockets that it will remain so.  Some processors are hot
> >> swappable.  The
> >> >> newly added processor is not going to have the correct tsc
> >> register value.
> >> >> Furthermore, the OS is free to reset the tsc value at any time.
> >> >>
> >> >> If I understand correctly, the HotSpot JVM will guarantee that
> >> >> System.nanoTime() never moves backwards.  It reads the tsc
> >> register with
> >> >> each call (?).  It the compares the read value with the last
> >> read value.  If
> >> >> the read value is < the last read value, then the last read value is
> >> >> returned.  If the read value is > the last read value, then
> >> the last read
> >> >> value is updated and the read value is returned.  Updating the
> >> last read
> >> >> value requires a CAS.  This CAS can lead to scalability 
> bottlenecks if
> >> >> System.nanoTime() is called too frequently.  I am not sure 
> if a better
> >> >> algorithm has been devised to fix this CAS contention.  I kind
> >> of remember
> >> >> it being talked about.
> >> >>
> >> >> I think the JVMs will default to more stable clock sources 
> with worse
> >> >> resolution for nanoTime() if tsc is not behaving well.
> >> >>
> >> >> Nathan Reynolds | Consulting Member of Technical Staff | 
> 602.333.9091
> >> >> Oracle PSR Engineering | Server Technology
> >> >>
> >> >> On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
> >> >>
> >> >> Only if you use System.nanoTime().  Time difference might even be
> >> >> negative if the thread is swapped between different cores.
> >> >>
> >> >> On 10/01/2012, Mohan Radhakrishnan
> >> <radhakrishnan.mohan at gmail.com> wrote:
> >> >>
> >> >>
> >> >> Hi,
> >> >>
> >> >> One more question from the novice and for the novice.
> >> >>
> >> >> I see these points in Dr. click's PPT. Can I know why ? I ask this
> >> >> here because it seems to
> >> >> involve multiple cores. Maybe the jvm forums are better suited
> >> for this.
> >> >> Does this mean that we get wrong time values if threads run on
> >> >> different cores ?
> >> >>
> >> >> But cannot use, e.g. X86's "tsc" register
> >> >> ? Value not coherent across CPUs
> >> >> ? Not consistent, e.g. slow ticking in low-power mode
> >> >> ? Monotonic per CPU ? but not per-thread
> >> >>
> >> >> Thanks,
> >> >> Mohan
> >> >>
> >> >> _______________________________________________
> >> >> Concurrency-interest mailing list
> >> >> Concurrency-interest at cs.oswego.edu
> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >>
> >> >>
> >> >>
> >> >>
> >> >> _______________________________________________
> >> >> Concurrency-interest mailing list
> >> >> Concurrency-interest at cs.oswego.edu
> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >>
> >> > ________________________________
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> >
> >> >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From davidcholmes at aapt.net.au  Fri Jan 13 00:17:44 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 13 Jan 2012 15:17:44 +1000
Subject: [concurrency-interest] tsc register
In-Reply-To: <CAOoXFP-j0UDvXh9Yh8hSHAGu+m109VqN7oTiTyCUnyKuF7aY3Q@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHCJCAA.davidcholmes@aapt.net.au>

If you want to check TSC stability check out this test program by Ingo Molnar:

http://people.redhat.com/mingo/time-warp-test/time-warp-test.c

Set TEST_CLOCK to 1 to also test the behaviour of CLOCK_MONOTONIC. Try changing the clocksource and see how that affects CLOCK_MONOTONIC stability.

David
-----

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mohan
> Radhakrishnan
> Sent: Friday, 13 January 2012 2:58 PM
> To: Concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] tsc register
> 
> 
> I meant that since this CR is recent even newer VM's could be affected
> ? I think if I were to set the thread affinity using JNI and
> continuosly run java threads then that is the test case. Am I right ?
> 
> I am mostly interested in tyring to understand and explain a test to
> others. We do run some heavy batch schedulers in a critical banking
> application on multiple cores using Jdk 5 on HP-UX. The problem is
> that we haven't seen any problem. Sometimes when these types of
> financial applications fail we assume something and restart them.
> 
> As was pointed out such timer failures are extremely rare on multiple
> cores even if they happen and newer hardware is better. That is what I
> have understood :-)
> 
> Thanks,
> Mohan
> 
> On Fri, Jan 13, 2012 at 3:09 AM, David Holmes 
> <davidcholmes at aapt.net.au> wrote:
> > Mohan Radhakrishnan writes:
> >> This CR http://cr.openjdk.java.net/~johnc/7117303/webrev.0/ is
> >> quite recent.
> >
> > Yes but I don't see the relevance. As it says there the VM code 
> was assuming a monotonic time source but was using 
> os::javaTimeMillis which is most definitely not monotonic.
> >
> > David
> > -----
> >
> >> Mohan
> >>
> >> On Thu, Jan 12, 2012 at 10:28 AM, David Holmes
> >> <davidcholmes at aapt.net.au> wrote:
> >> > Hotspot uses the available OS high-resolution nominally 
> monotonic time
> >> > source if it exists, else it falls back to a time-of-day source
> >> (which is
> >> > not monotonic). It should be very rare (ie only really old
> >> systems) to not
> >> > have a monotonic timesource available.
> >> >
> >> > Solaris had a number of bugs in this area (because unlike 
> the other OSes
> >> > that dropped use of the TSC due to its instability, Solaris
> >> decided to force
> >> > it to be stable and synchronized - and occasionally they
> >> failed) and so a
> >> > guard was added to ensure it was actually monotonic.
> >> >
> >> > On Windows if the TSC is being used without using the external
> >> > utilities/drivers to sync it then QueryPerformanceCounter can be
> >> > non-monotonic. Similarly on Linux if you set your 
> clocksource to be TSC
> >> > instead of HPET (and the TSC is not synchronized) then
> >> CLOCK_MONOTONIC can
> >> > also exhibit non-monotonic behaviour.
> >> >
> >> > See bug 6458294 for some info. Sadly, back in November 2006 I
> >> reported that
> >> > we would add the guard logic on all platforms, but it never happened.
> >> >
> >> > All-in-all clocks/counters/timers are a general mess.
> >> >
> >> > David
> >> >
> >> > -----Original Message-----
> >> > From: concurrency-interest-bounces at cs.oswego.edu
> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >> Dr Heinz M.
> >> > Kabutz
> >> > Sent: Wednesday, 11 January 2012 8:33 AM
> >> > To: dholmes at ieee.org
> >> > Cc: Concurrency-interest at cs.oswego.edu
> >> > Subject: Re: [concurrency-interest] tsc register
> >> >
> >> > What is interesting is that we have had reports of System.nanoTime()
> >> > sometimes counting backwards.  Has nanoTime() always been
> >> monotonic?  If so,
> >> > I need to follow up on the claims.  I've heard it from two
> >> sources, but it
> >> > might just be hearsay.
> >> >
> >> > Regards
> >> >
> >> > Heinz
> >> > --
> >> > Dr Heinz M. Kabutz (PhD CompSci)
> >> > Author of "The Java(tm) Specialists' Newsletter"
> >> > Sun Java Champion
> >> > IEEE Certified Software Development Professional
> >> > http://www.javaspecialists.eu
> >> > Tel: +30 69 72 850 460
> >> > Skype: kabutz
> >> >
> >> >
> >> >
> >> > On 1/11/12 12:14 AM, David Holmes wrote:
> >> >
> >> > Correct. Hotspot uses/relies-on the high-resolution monotonic
> >> time source of
> >> > the OS, else falls back to plain time-of-day. It never uses the TSC
> >> > directly.
> >> >
> >> > David Holmes
> >> >
> >> > -----Original Message-----
> >> > From: concurrency-interest-bounces at cs.oswego.edu
> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf 
> Of Vitaly
> >> > Davidovich
> >> > Sent: Wednesday, 11 January 2012 2:28 AM
> >> > To: Nathan Reynolds
> >> > Cc: Concurrency-interest at cs.oswego.edu
> >> > Subject: Re: [concurrency-interest] tsc register
> >> >
> >> > I thought JVM (hotspot at least) uses the os monotonic clock 
> source (if
> >> > present) rather than reading tsc directly and then doing its own
> >> > adjustments?
> >> >
> >> > On Jan 10, 2012 11:24 AM, "Nathan Reynolds" 
> <nathan.reynolds at oracle.com>
> >> > wrote:
> >> >>
> >> >> The tsc register on older processors did not increment at the
> >> same rate.
> >> >> If a core slept or slowed down then the tsc register would
> >> stop or slow down
> >> >> its increments.  More modern processors guarantee that tsc register
> >> >> increments at a fixed frequency.  If you are working on Linux,
> >> cpuinfo (?)
> >> >> could report the const_tsc flag.  This means that the 
> processor and OS
> >> >> recognize that this feature is on the processor.
> >> >>
> >> >> The tsc register is not synchronized across sockets.  This 
> is something
> >> >> Oracle has asked Intel to enhance many times.  It is a very 
> difficult
> >> >> problem to solve.  However, more modern Linux kernels will (?)
> >> synchronize
> >> >> the tsc register at startup so that it is impossible to read the tsc
> >> >> register on two different cores and see that the 2?? value is
> >> smaller.  This
> >> >> does not mean that the tsc register is synchronized.  It only
> >> means that two
> >> >> threads running on different cores will hopefully never see
> >> the tsc "move
> >> >> backwards".
> >> >>
> >> >> There is no guarantee that once the tsc register is 
> synchronized across
> >> >> sockets that it will remain so.  Some processors are hot
> >> swappable.  The
> >> >> newly added processor is not going to have the correct tsc
> >> register value.
> >> >> Furthermore, the OS is free to reset the tsc value at any time.
> >> >>
> >> >> If I understand correctly, the HotSpot JVM will guarantee that
> >> >> System.nanoTime() never moves backwards.  It reads the tsc
> >> register with
> >> >> each call (?).  It the compares the read value with the last
> >> read value.  If
> >> >> the read value is < the last read value, then the last read value is
> >> >> returned.  If the read value is > the last read value, then
> >> the last read
> >> >> value is updated and the read value is returned.  Updating the
> >> last read
> >> >> value requires a CAS.  This CAS can lead to scalability 
> bottlenecks if
> >> >> System.nanoTime() is called too frequently.  I am not sure 
> if a better
> >> >> algorithm has been devised to fix this CAS contention.  I kind
> >> of remember
> >> >> it being talked about.
> >> >>
> >> >> I think the JVMs will default to more stable clock sources 
> with worse
> >> >> resolution for nanoTime() if tsc is not behaving well.
> >> >>
> >> >> Nathan Reynolds | Consulting Member of Technical Staff | 
> 602.333.9091
> >> >> Oracle PSR Engineering | Server Technology
> >> >>
> >> >> On 1/10/2012 5:03 AM, Dr Heinz M. Kabutz wrote:
> >> >>
> >> >> Only if you use System.nanoTime().  Time difference might even be
> >> >> negative if the thread is swapped between different cores.
> >> >>
> >> >> On 10/01/2012, Mohan Radhakrishnan
> >> <radhakrishnan.mohan at gmail.com> wrote:
> >> >>
> >> >>
> >> >> Hi,
> >> >>
> >> >> One more question from the novice and for the novice.
> >> >>
> >> >> I see these points in Dr. click's PPT. Can I know why ? I ask this
> >> >> here because it seems to
> >> >> involve multiple cores. Maybe the jvm forums are better suited
> >> for this.
> >> >> Does this mean that we get wrong time values if threads run on
> >> >> different cores ?
> >> >>
> >> >> But cannot use, e.g. X86's "tsc" register
> >> >> ? Value not coherent across CPUs
> >> >> ? Not consistent, e.g. slow ticking in low-power mode
> >> >> ? Monotonic per CPU ? but not per-thread
> >> >>
> >> >> Thanks,
> >> >> Mohan
> >> >>
> >> >> _______________________________________________
> >> >> Concurrency-interest mailing list
> >> >> Concurrency-interest at cs.oswego.edu
> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >>
> >> >>
> >> >>
> >> >>
> >> >> _______________________________________________
> >> >> Concurrency-interest mailing list
> >> >> Concurrency-interest at cs.oswego.edu
> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >>
> >> > ________________________________
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> >
> >> >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From rco at quartetfs.com  Fri Jan 13 05:06:59 2012
From: rco at quartetfs.com (Romain Colle)
Date: Fri, 13 Jan 2012 11:06:59 +0100
Subject: [concurrency-interest] Use-cases and issues with the ForkJoinPool
Message-ID: <CAJp3eRCqV3ykLMhgppeR-f2t2sxLoGHwQGc_1dnGB9erZSmitA@mail.gmail.com>

Hi Doug & everyone,

With the current improvements being done to the ForkJoinPool,
I wanted to share some of the "stories" and concerns we had
lately with it.

Our company uses the ForkJoinPool a lot in our flagship product,
a main-memory aggregation engine.
To be fair, this is the only kind of thread pool that we actually
use for the most important parts.
Our typical usage is that when a new request comes in, we wrap it
up in a task, submit it in the pool and wait for it to be done.
Therefore, most of our code runs within the pool and heavily uses
its fork/join abilities since our software primarily relies on
massively parallel computations.

We had two main issues that we faced with this approach.

Fork/Join out of order
======================
A request submitted by a user is essentially a query that is being
split into multiple subqueries. These subqueries can themselves be
split, so on and so forth ...
The additional twist here is that some of these subqueries need to
wait for the result of some other subqueries before they can run
(we are guaranteed to be cycle-free). Therefore, they simply join
the associated ForkJoinTask and we rely on the work-stealing
capabilities of the pool to use the waiting thread to participate
in the joined task's computation.

This joined task can basically be found in 3 different places:
 1) This is the last task forked by the current thread.
    In this case the current thread simply deques and executes
    it.
 2) It has been stolen by another thread that was looking for work
    and is currently executing somewhere else. The current thread
    will try to find where and help.
 3) It is in the current thread's queue, but not on top. In this
    case, the current thread cannot do anything and will have to
    wait for another thread in the pool to steal this task, execute
    it and signal it.
    It will also mark itself as non-active, but this will only
    trigger the creation of a new "compensation" thread if NO threads
    are active anymore, which is not a usual case in a steady state.

As you might have guessed, our problem is case #3.
We find this to happen a lot, especially under heavy load with several
computation-heavy requests having to be served concurrently.
This, of course, decreases the computation concurrency a lot, reverting
to a monothreaded computation at times on a 16 cores machine (which kind
of defeats the whole purpose of multithreading!).

We managed to solve the problem in most cases by adding a planning step
to our queries and creating a DAG of dependencies, then by forking and
joining the tasks in the "correct" order (a.k.a. invokeAll).

However, there are still some places and cases where we cannot do that
and have to take the performance hit.

Deadlocks
=========
This problem happens because of the previous use-case (out of order
fork/join) and the fact that, inevitably, some of our forked tasks need to
take locks to execute.
The reason why this is is that, as mentioned before, everything runs in
the pool, including callbacks to our users' code. And more often than not,
it is "synchronized" or makes use of locks.

A simple case where we can see a deadlock is the following.
Consider a ForkJoinPool of size 2, two ForkJoinTasks T1 and T2 and a lock L.
The tasks' compute method does the following:
 - lock L
 - fork 2 subtasks Ta and Tb (in that order)
 - join Ta
 - join Tb
 - unlock L

If T1 and T2 are both running in one of the pool's worker, only one of them
can get the lock L. The other thread will be blocked waiting for L, but will
still be considered active by the FJPool (since this is not a "managed"
block).
The thread that got the lock will fork Ta and Tb, then join Ta and find
itself
in case #3 described in the section above.
It will therefore wait for another thread in the pool to execute Ta for it,
but
none are available, leading to a deadlock.
No compensation threads will be created since there is one thread
considered
active in the pool (even though it is actually waiting for a lock).

This is a very simple example, but it illustrates the problem we are facing.

In this case, we solved the issue by implementing an execJoin() method on
some
special kind of tasks: execJoin() executes the task if this is the first
time
this method is called on this task, or joins it otherwise.
This requires an extra field (I didn't want to hijack that ForkJoinTask's
"status" field) and a CAS.

The more general issue in both cases is that a worker thread can only
execute a
task that is at the top of its queue on a join().
What is the reason why localHelpJoinTask() cannot find a task anywhere in
the
local queue, CAS its slot to null in the queue and execute it? From what I
saw
null slots are well-handled throughout the code.

Anyway, thanks a lot for all the pool maintenance and improvements, this is
a
great piece of software that we are very thankful for!

Regards,

-- 
Romain Colle
Senior R&D Engineer
QuartetFS
2 rue Jean Lantier, 75001 Paris, France
http://www.quartetfs.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120113/b340cb9e/attachment-0001.html>

From david.lloyd at redhat.com  Fri Jan 13 10:13:01 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Fri, 13 Jan 2012 09:13:01 -0600
Subject: [concurrency-interest] tsc register
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEHCJCAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCAEHCJCAA.davidcholmes@aapt.net.au>
Message-ID: <4F1049FD.6080608@redhat.com>

On 01/12/2012 11:13 PM, David Holmes wrote:
> Mohan Radhakrishnan writes:
>> I meant that since this CR is recent even newer VM's could be affected
>
> That CR has nothing to do with the TSC.
>
>> ? I think if I were to set the thread affinity using JNI and
>> continuosly run java threads then that is the test case. Am I right ?
>
> If you bind a thread to a given core (using JNI) and that thread reads the TSC then it should always be reading the same TSC. But remember there are two issues with using the basic TSC: synchronization across processors and stability of the frequency of updates. Thread affinity only addresses one part.

Synchronizing TSC across processors seems like a non-requirement from 
our perspective.  As long as the TSC value is saved and restored on 
context switch, each thread will see a monotonic increase, right?  And 
really that's about as much as you ought to expect out of nanoTime().

--
- DML

From vitalyd at gmail.com  Fri Jan 13 10:22:01 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 13 Jan 2012 10:22:01 -0500
Subject: [concurrency-interest] tsc register
In-Reply-To: <4F1049FD.6080608@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCAEHCJCAA.davidcholmes@aapt.net.au>
	<4F1049FD.6080608@redhat.com>
Message-ID: <CAHjP37EVKou64NDVb7qF282YSR4L-e2sxqjCBaONPLssHsZXNQ@mail.gmail.com>

David,

I don't think tsc is saved and restored as it's not associated with a user
or kernel context.  Its not like we save and restore wall clock time :).
On Jan 13, 2012 10:15 AM, "David M. Lloyd" <david.lloyd at redhat.com> wrote:

> On 01/12/2012 11:13 PM, David Holmes wrote:
>
>> Mohan Radhakrishnan writes:
>>
>>> I meant that since this CR is recent even newer VM's could be affected
>>>
>>
>> That CR has nothing to do with the TSC.
>>
>>  ? I think if I were to set the thread affinity using JNI and
>>> continuosly run java threads then that is the test case. Am I right ?
>>>
>>
>> If you bind a thread to a given core (using JNI) and that thread reads
>> the TSC then it should always be reading the same TSC. But remember there
>> are two issues with using the basic TSC: synchronization across processors
>> and stability of the frequency of updates. Thread affinity only addresses
>> one part.
>>
>
> Synchronizing TSC across processors seems like a non-requirement from our
> perspective.  As long as the TSC value is saved and restored on context
> switch, each thread will see a monotonic increase, right?  And really
> that's about as much as you ought to expect out of nanoTime().
>
> --
> - DML
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120113/464ebb2e/attachment.html>

From dl at cs.oswego.edu  Fri Jan 13 11:37:01 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 13 Jan 2012 11:37:01 -0500
Subject: [concurrency-interest] Use-cases and issues with the
	ForkJoinPool
In-Reply-To: <CAJp3eRCqV3ykLMhgppeR-f2t2sxLoGHwQGc_1dnGB9erZSmitA@mail.gmail.com>
References: <CAJp3eRCqV3ykLMhgppeR-f2t2sxLoGHwQGc_1dnGB9erZSmitA@mail.gmail.com>
Message-ID: <4F105DAD.8010602@cs.oswego.edu>

On 01/13/12 05:06, Romain Colle wrote:
> Hi Doug & everyone,
>
> With the current improvements being done to the ForkJoinPool,
> I wanted to share some of the "stories" and concerns we had
> lately with it.

Thanks! This is the perfect time for everyone to post experiences
and suggestions about FJ. The most surprising thing we've found is that
most of the heaviest uses don't fall into classic parallel
computation categories, but instead are the results of people finding
that the basic decentralized task framework (work-stealing etc)
is so much faster than alternatives on larger systems that they use
it for all sorts of other things.
We'd like to better support these other things.

Some questions and answers, out of order...

 > Consider a ForkJoinPool of size 2, two ForkJoinTasks T1 and T2 and a lock L.
 > The tasks' compute method does the following:
 >   - lock L
 >   - fork 2 subtasks Ta and Tb (in that order)
 >   - join Ta
 >   - join Tb
 >   - unlock L
 >

 > What is the reason why localHelpJoinTask() cannot find a task anywhere in the
 > local queue, CAS its slot to null in the queue and execute it? From what I saw
 > null slots are well-handled throughout the code.

The ultimate reason is that if someone says ta.join(); tb.join(); we
are not in general allowed to reorder them. However, when
the joins are dependent (as in your lock example), callers
don't really mean that we must join in that order.
Unfortunately, we have no way of letting them tell us this in
the case of joins. We do for invoke, so in the above example, the best
usage would be:
   Lock L
   invokeAll(ta, tb);
One possibility is to add method
   joinAll(ForkJoinTask<?>... tasks)

The main trouble is that people would need to learn to use it.
Already we see too many cases of people naively doing:
   ta.fork(); tb.fork(); ta.join(); tb.join();
and then seeing it "work" but with a huge blowup of compensation
threads and crummy performance when it is recursive.

One way to address this is to use a "claim" bit on
every task (somewhat similar to your execJoin workaround).
This would cost one more atomic operation on every task
execution but would eliminate a set of misusage problems
(among other things it would allow your suggested
localHelpJoinTask change).

I've contemplated this many times over the years, without
being able to convince myself that it was worthwhile
to penalize all the well-structured FJ usages for the
sake of improving robustness in the face of more
questionable usages. But it is worth some exploration to see
if this can be done with small enough impact to adopt.


> Fork/Join out of order
> ======================
> A request submitted by a user is essentially a query that is being
> split into multiple subqueries. These subqueries can themselves be
> split, so on and so forth ...
> The additional twist here is that some of these subqueries need to
> wait for the result of some other subqueries before they can run
> (we are guaranteed to be cycle-free). Therefore, they simply join
> the associated ForkJoinTask and we rely on the work-stealing
> capabilities of the pool to use the waiting thread to participate
> in the joined task's computation.

To clarify: The user is a non-FJ thread and the queries
say q1, q2, are submitted using submit (not invoke), and then joined?
If so, you are right that this can be a problem (in any execution
framework) because the pool doesn't know that q1 must be performed
before q2. (Calling p.invoke(q1); p.invoke(q2) would ensure this.)
Under upcoming more relaxed submission queues, out of order processing
will become even more common.


>   3) It is in the current thread's queue, but not on top. In this
>      case, the current thread cannot do anything and will have to
>      wait for another thread in the pool to steal this task, execute
>      it and signal it.
>      It will also mark itself as non-active, but this will only
>      trigger the creation of a new "compensation" thread if NO threads
>      are active anymore, which is not a usual case in a steady state.
>
> As you might have guessed, our problem is case #3.
> We find this to happen a lot, especially under heavy load with several
> computation-heavy requests having to be served concurrently.
> This, of course, decreases the computation concurrency a lot, reverting
> to a monothreaded computation at times on a 16 cores machine (which kind
> of defeats the whole purpose of multithreading!).

Yes. Some might recall that we had in earlier versions a
"maintainParallelism" method that was meant for exactly this
situation. It was pulled for the same reason as other dynamic
adjustment methods -- nothing sensible happens if you dynamically
change settings while running. Suggestions about somehow supporting
this again would be welcome.

-Doug


From davidcholmes at aapt.net.au  Fri Jan 13 16:24:23 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 14 Jan 2012 07:24:23 +1000
Subject: [concurrency-interest] tsc register
In-Reply-To: <CAHjP37EVKou64NDVb7qF282YSR4L-e2sxqjCBaONPLssHsZXNQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEHEJCAA.davidcholmes@aapt.net.au>

Exactly. The TSC is not part of thread-state. There is no saving and
restoring. Plus if you did that it would no longer be a global counter.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
  Sent: Saturday, 14 January 2012 1:22 AM
  To: David M. Lloyd
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] tsc register


  David,

  I don't think tsc is saved and restored as it's not associated with a user
or kernel context.  Its not like we save and restore wall clock time :).

  On Jan 13, 2012 10:15 AM, "David M. Lloyd" <david.lloyd at redhat.com> wrote:

    On 01/12/2012 11:13 PM, David Holmes wrote:

      Mohan Radhakrishnan writes:

        I meant that since this CR is recent even newer VM's could be
affected


      That CR has nothing to do with the TSC.


        ? I think if I were to set the thread affinity using JNI and
        continuosly run java threads then that is the test case. Am I right
?


      If you bind a thread to a given core (using JNI) and that thread reads
the TSC then it should always be reading the same TSC. But remember there
are two issues with using the basic TSC: synchronization across processors
and stability of the frequency of updates. Thread affinity only addresses
one part.


    Synchronizing TSC across processors seems like a non-requirement from
our perspective.  As long as the TSC value is saved and restored on context
switch, each thread will see a monotonic increase, right?  And really that's
about as much as you ought to expect out of nanoTime().

    --
    - DML
    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120114/8c7ac46e/attachment.html>

From blair at orcaware.com  Fri Jan 13 23:37:10 2012
From: blair at orcaware.com (Blair Zajac)
Date: Fri, 13 Jan 2012 20:37:10 -0800
Subject: [concurrency-interest] tsc register
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEHCJCAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEHCJCAA.davidcholmes@aapt.net.au>
Message-ID: <C7D5B52A-8D8B-476F-8E8E-F000C0086AF9@orcaware.com>


On Jan 12, 2012, at 9:17 PM, David Holmes wrote:

> If you want to check TSC stability check out this test program by Ingo Molnar:
> 
> http://people.redhat.com/mingo/time-warp-test/time-warp-test.c

I cannot get this to compile on a 64-bit Ubuntu 11.10 system.  The first error is fixed by passing 0755 to open():

$ gcc -Wall -O2 -o time-warp-test time-warp-test.c -lrt
In file included from /usr/include/fcntl.h:252:0,
                 from time-warp-test.c:24:
In function ?open?,
    inlined from ?setup_shared_var? at time-warp-test.c:127:5:
/usr/include/x86_64-linux-gnu/bits/fcntl2.h:51:24: error: call to ?__open_missing_mode? declared with attribute error: open with O_CREAT in second argument needs 3 arguments

The second error I don't know how to get around, since I don't know assembler:

$ gcc -Wall -O2 -o time-warp-test time-warp-test.c -lrt
time-warp-test.c: Assembler messages:
time-warp-test.c:169: Error: incorrect register `%rax' used with `l' suffix
time-warp-test.c:169: Error: incorrect register `%rax' used with `l' suffix

This is the code, but changing the '#if 0' to '#if 1' gets a similar error.

static inline void unlock(unsigned long *flag)
{
#if 0
        __asm__ __volatile__(
                "lock; btrl $0,%0\n"
                             : "=g"(*flag) :: "memory");
        __asm__ __volatile__("rep; nop");
#else
        __asm__ __volatile__("movl $0,%0; rep; nop" : "=g"(*flag) :: "memory");
#endif
}

Blair



From vitalyd at gmail.com  Sat Jan 14 00:31:38 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 14 Jan 2012 00:31:38 -0500
Subject: [concurrency-interest] tsc register
In-Reply-To: <C7D5B52A-8D8B-476F-8E8E-F000C0086AF9@orcaware.com>
References: <NFBBKALFDCPFIDBNKAPCGEHCJCAA.davidcholmes@aapt.net.au>
	<C7D5B52A-8D8B-476F-8E8E-F000C0086AF9@orcaware.com>
Message-ID: <CAHjP37GvAXjQdT4ejMuFq3X=4wR7RSxWnKUjdHVy3WGh90XqTA@mail.gmail.com>

For your 2nd issue try replacing "movl" with "movq".
On Jan 13, 2012 11:39 PM, "Blair Zajac" <blair at orcaware.com> wrote:

>
> On Jan 12, 2012, at 9:17 PM, David Holmes wrote:
>
> > If you want to check TSC stability check out this test program by Ingo
> Molnar:
> >
> > http://people.redhat.com/mingo/time-warp-test/time-warp-test.c
>
> I cannot get this to compile on a 64-bit Ubuntu 11.10 system.  The first
> error is fixed by passing 0755 to open():
>
> $ gcc -Wall -O2 -o time-warp-test time-warp-test.c -lrt
> In file included from /usr/include/fcntl.h:252:0,
>                 from time-warp-test.c:24:
> In function ?open?,
>    inlined from ?setup_shared_var? at time-warp-test.c:127:5:
> /usr/include/x86_64-linux-gnu/bits/fcntl2.h:51:24: error: call to
> ?__open_missing_mode? declared with attribute error: open with O_CREAT in
> second argument needs 3 arguments
>
> The second error I don't know how to get around, since I don't know
> assembler:
>
> $ gcc -Wall -O2 -o time-warp-test time-warp-test.c -lrt
> time-warp-test.c: Assembler messages:
> time-warp-test.c:169: Error: incorrect register `%rax' used with `l' suffix
> time-warp-test.c:169: Error: incorrect register `%rax' used with `l' suffix
>
> This is the code, but changing the '#if 0' to '#if 1' gets a similar error.
>
> static inline void unlock(unsigned long *flag)
> {
> #if 0
>        __asm__ __volatile__(
>                "lock; btrl $0,%0\n"
>                             : "=g"(*flag) :: "memory");
>        __asm__ __volatile__("rep; nop");
> #else
>        __asm__ __volatile__("movl $0,%0; rep; nop" : "=g"(*flag) ::
> "memory");
> #endif
> }
>
> Blair
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120114/042361f7/attachment.html>

From blair at orcaware.com  Sat Jan 14 01:03:48 2012
From: blair at orcaware.com (Blair Zajac)
Date: Fri, 13 Jan 2012 22:03:48 -0800
Subject: [concurrency-interest] tsc register
In-Reply-To: <CAHjP37GvAXjQdT4ejMuFq3X=4wR7RSxWnKUjdHVy3WGh90XqTA@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCGEHCJCAA.davidcholmes@aapt.net.au>	<C7D5B52A-8D8B-476F-8E8E-F000C0086AF9@orcaware.com>
	<CAHjP37GvAXjQdT4ejMuFq3X=4wR7RSxWnKUjdHVy3WGh90XqTA@mail.gmail.com>
Message-ID: <4F111AC4.6020407@orcaware.com>

On 1/13/12 9:31 PM, Vitaly Davidovich wrote:
> For your 2nd issue try replacing "movl" with "movq".

Thanks, that worked,

Blair

From csgzlong at iastate.edu  Mon Jan 16 09:59:34 2012
From: csgzlong at iastate.edu (Yuheng Long)
Date: Mon, 16 Jan 2012 08:59:34 -0600
Subject: [concurrency-interest] Problem getting any speed up in using the
 withIndexedMapping method in the ParallelArray class
Message-ID: <CAHYWNzAPqEHD_FpiBqV208jmgP3dQ7SSgfr_Ne-fLReF9+QWcA@mail.gmail.com>

To whom it concerns,
          I tried to parallelize a for loop these days which iterates the
elements (Integer) of an array and applies a pure hash function on each of
the elements
and substitute the original elements with the results.
          So I used the withIndexedMapping method in the ParallelArray
class to parallelize the code.
          I try both my 4 cores and my 24 cores machines. I did not get any
speedup.
          The code snippet is listed below. Would you please tell me what I
did wrong or explain why I did not get any speedup in the implementation at
all?
          I would like to send out the complete code if necessary. Thank
you.

Hash h = new Hash();
ForkJoinPool fjp = new ForkJoinPool();
ParallelArray<Integer> pa = ParallelArray.createUsingHandoff(elementData,
fjp);
pa.replaceWithMappedIndex(h);

public class Hash implements Ops.IntAndObjectToObject<Integer, Integer> {
   public Integer op (int index, Integer o) {
        int key = o;
        /* key = the hash computation code */
        return key;
   }
}

Thank you very much,
Sincerely,
Yuheng
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120116/13701078/attachment.html>

From rco at quartetfs.com  Mon Jan 16 10:48:54 2012
From: rco at quartetfs.com (Romain Colle)
Date: Mon, 16 Jan 2012 16:48:54 +0100
Subject: [concurrency-interest] Use-cases and issues with the
	ForkJoinPool
In-Reply-To: <4F105DAD.8010602@cs.oswego.edu>
References: <CAJp3eRCqV3ykLMhgppeR-f2t2sxLoGHwQGc_1dnGB9erZSmitA@mail.gmail.com>
	<4F105DAD.8010602@cs.oswego.edu>
Message-ID: <CAJp3eRBAwVHW+kkjmu8ph3s7+GxWpRne0iuzocbC-AB53FptAQ@mail.gmail.com>

Hi Doug,

Thanks a lot for the answers!
Please find some comments below.

The ultimate reason is that if someone says ta.join(); tb.join(); we
> are not in general allowed to reorder them. [...]

Already we see too many cases of people naively doing:
>  ta.fork(); tb.fork(); ta.join(); tb.join();
> and then seeing it "work" but with a huge blowup of compensation
> threads and crummy performance when it is recursive.
>

The need for us here is not to re-order joins.
As you said, the join order should be preserved as it may actually mean
something.
On the other hand, when I write "ta.fork(); tb.fork()", I expect them
to potentially run in parallel or in any order, and my code should be
prepared for that.

Would there be something illegal about the following
handling of this case?
 1) ta is forked and put in the local queue
 2) tb is forked and put in the local queue
 3) ta.join() is called. It is found to still be in the local queue,
    so we just CAS its slot to null and execute it ourselves.
 4) tb.join() is called. We are in the usual case where we can just
    deque and execute it.

This does not require a claim bit and there are no risks of any
task being executed more than once.
There is an extra cost of getting ta from the local queue
 (especially if the queue is quite large), but the "well-structured"
FJ usage won't be affected by it.
Moreover, the larger the queue, the more likely the task is to be
in it.

I guess the more general question I had was: why does a joining thread
goes into idle wait (pool.tryAwaitJoin()) if it has a non-empty queue?
I'd like to at least see it "steal" from anywhere in its own queue,
and even go for helpJoinTask().

If none of this is possible, I still like the idea of the claim bit,
even though it adds some CAS overhead (is it really that much in some
cases btw?).
We could imagine a flag for this in the pool, like the locallyFifo one
if the cost turns out to be too much.


To clarify: The user is a non-FJ thread and the queries
> say q1, q2, are submitted using submit (not invoke), and then joined?
> If so, you are right that this can be a problem (in any execution
> framework) because the pool doesn't know that q1 must be performed
> before q2. (Calling p.invoke(q1); p.invoke(q2) would ensure this.)
> Under upcoming more relaxed submission queues, out of order processing
> will become even more common.


This is not really this. Let's use an almost real use-case.

A non-FJ thread submits a query Q to retrieve the values of the measures
M1 and M2, and that query is wrapped into a task T and submitted.
T is split into 2 sub-tasks: T1 that computes the value of M1 and T2 that
computes the value of M2.
M1 can be computed by itself, so T1 just computes its value when it gets
a chance to do so.
M2, however, turns to to be the sum of M1 and M3. We therefore create a
task T3 for computing M3 and run it, but we also need to join T1 and wait
for its result to get the value of M1.
Hence the out-of-order join.

What we do now to avoid this issue is that we first compute the query plan
in a single-threaded pass.
In this case, we can see that we can first compute M1 and M3 in parallel,
then compute M2 once this is done and finally return the result.

We initially thought we could avoid that single-threaded planning step and
simply rely on the work-stealing feature to have joining threads participate
in the global computation.
And we'd still like to be able to do it, especially if this only requires a
single CAS per task :-)

-- 
Romain Colle
Senior R&D Engineer
QuartetFS
2 rue Jean Lantier, 75001 Paris, France
http://www.quartetfs.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120116/d5c2120c/attachment.html>

From cheremin at gmail.com  Mon Jan 16 11:33:28 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Mon, 16 Jan 2012 20:33:28 +0400
Subject: [concurrency-interest] synchronized constructors
In-Reply-To: <5F6252CB-94B7-43AD-8842-D06450D4EE6E@rkuhn.info>
References: <CAC2Zdp3_KFga6hyMzKtDww4gNGgc4jO1Uy-vNjaEVkwEnLZQig@mail.gmail.com>
	<CACuKZqFpxVutmgrk6pPaU97MLGG+iFMAtu0F5w0M6W9oVD1p1w@mail.gmail.com>
	<5AE501BD-18AB-41C2-8F5D-6D995CD2B83B@rkuhn.info>
	<9319F360221C65428EA819A4E8DC34ED037966AAAF@OPMBOX21UK.options-it.com>
	<08F07B8B-F044-4160-B69A-F7CE7D07C57E@rkuhn.info>
	<CAOwENiKeAStTDxocoC1M_aBo__MQxV3+P-9-FAvTr3eQYsF-aQ@mail.gmail.com>
	<AD0C68FA-5044-4B28-BFC3-4544BB815D46@rkuhn.info>
	<CAOwENi+1s7JLd5F4v=T3xBdXGk_dPVd9_rgx6e82iybL5tSzXg@mail.gmail.com>
	<DEC34F1E-7D37-4ACD-803E-BDAC4FF5F540@rkuhn.info>
	<CAOwENi+FJTfi-LPO3cW-ntpU_hvi6J8-r-T_OhdfsPweDx8VVA@mail.gmail.com>
	<CAC2Zdp1XRbnmoe=CWAUJN_JRiy0FuzLbm8fyPG5hvpSGf3YqMQ@mail.gmail.com>
	<CACuKZqG8HvZ5ZB1ZrLuj_e3byOsVGcT=oqmtcWeUtpBB4Uheqw@mail.gmail.com>
	<CAC2Zdp0oZyy4Y4wVi8_z=pSBThzCzRmv4-crh+Y-uZCQKn8cKQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20D59D8@G4W3299.americas.hpqcorp.net>
	<5F6252CB-94B7-43AD-8842-D06450D4EE6E@rkuhn.info>
Message-ID: <CAOwENiK91vr3112tuBDjhkXV53WwzRotdXSPp8R3RKRE5ygWbg@mail.gmail.com>

2011/12/17 Roland Kuhn <rk at rkuhn.info>:
> You might ask, why have a non-final field without proper synchronization, so
> let me add that the background to this question is that in my case the
> construction of the object needs to proceed in two phases because of other
> inter-dependencies, which requires certain fields to be written to after the
> constructor has finished; they will be written to only exactly once, and
> this happens before ?untrusted? client code obtains the reference (in
> program order).

May be it's too late for this -- but you can still use final fields,
but update them via reflection. If you do not publish object reference
_before_ all reflection-based modifications finished -- you still have
all final fields guarantee. From you description it seems like it is
your case.


> Thanks in advance,
>
> Roland
>
> On Dec 16, 2011, at 22:49 , Boehm, Hans wrote:
>
> Just to be clear: Safe publication for final fields requires that you do not
> make a pointer to the object available to other threads (publish it) before
> the constructor finishes.? ?Safe publication? as used below is a slightly
> stronger property; you also should not communicate the pointer
> post-construction to another thread via a data race.? Avoiding them both is
> great advice, but the second one may be difficult to avoid if you need to
> give a reference to your object to code you don?t trust.? Malicious code
> could always pass the object to a third thread through a race, hoping that
> the third thread, which has no happens-before relationship to the other two,
> would find your object in an inconsistent state.? My intuition is that this
> is a rare case, but one that does occur.? When it does occur, the odds of an
> actual security hole are probably small, but so are the odds of proving
> security properties without addressing the issue.
>
> Hans
>
> From:?concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]?On Behalf Of?Yuval
> Shavit
> Sent:?Friday, December 16, 2011 11:59 AM
> To:?Zhong Yu
> Cc:?concurrency-interest at cs.oswego.edu
> Subject:?Re: [concurrency-interest] synchronized constructors
>
> On Fri, Dec 16, 2011 at 2:10 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>
> On Fri, Dec 16, 2011 at 8:38 AM, Yuval Shavit <yshavit at akiban.com> wrote:
>> Several people have made the claim that you could see partially
>> initialized
>> state even if the constructor were synchronized, and I don't see how this
>> could be. Yes, you could assign MyPoint myUnsafeRef before MyPoint()
>> finishes -- but if you ever tried to *use* myUnsafeRef, you would run into
>> a
>> synchronized block which would then ensure that the constructor
>> happened-before whatever method you're writing. Seems to me you should see
>> the fully-thread-safe, not-partially-initialized object at that point.
>
> If the reference is unsafely published, another thread can get the
> reference early; it then calls an instance method which may obtain the
> lock before the creation thread can obtain the lock for the
> constructor. Therefore the other thread can observe the blank state.
> As Ruslan corrected me, no partial state can be observed though.
>
> Ah yes, I hadn't thought of that.
>
> So now, in order to have my class be over-achievingly thread safe, I need to
> replace my synchronized methods with a latch that waits for construction to
> finish and *then* a synchronized (this) {...}.?But I probably decide that
> this is really going far out of my way to support a bad usage pattern, so I
> throw up my arms and say "you'll see either totally blank or
> post-initialization state, but not partial initialization." ?But that means
> I have to guard against uninitialized state in each method, so I probably
> throw up my arms again and say "just publish the darn object safely!" ?And
> then the JLS people come to me and say, "well if that's your requirement,
> why do you want a synchronized constructor?" ?And suddenly the whole thing
> makes sense.
>
> This has been an interesting discussion for me! Thanks everyone. :)
>
> -Yuval
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> --
> [scala-debate on 2009/10/2]
> Viktor Klang: When will the days of numerical overflow be gone?
> Ricky Clarkson: One second after 03:14:07 UTC on Tuesday, 19 January 2038
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From viktor.klang at gmail.com  Mon Jan 16 11:42:35 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 16 Jan 2012 17:42:35 +0100
Subject: [concurrency-interest] synchronized constructors
In-Reply-To: <CAOwENiK91vr3112tuBDjhkXV53WwzRotdXSPp8R3RKRE5ygWbg@mail.gmail.com>
References: <CAC2Zdp3_KFga6hyMzKtDww4gNGgc4jO1Uy-vNjaEVkwEnLZQig@mail.gmail.com>
	<CACuKZqFpxVutmgrk6pPaU97MLGG+iFMAtu0F5w0M6W9oVD1p1w@mail.gmail.com>
	<5AE501BD-18AB-41C2-8F5D-6D995CD2B83B@rkuhn.info>
	<9319F360221C65428EA819A4E8DC34ED037966AAAF@OPMBOX21UK.options-it.com>
	<08F07B8B-F044-4160-B69A-F7CE7D07C57E@rkuhn.info>
	<CAOwENiKeAStTDxocoC1M_aBo__MQxV3+P-9-FAvTr3eQYsF-aQ@mail.gmail.com>
	<AD0C68FA-5044-4B28-BFC3-4544BB815D46@rkuhn.info>
	<CAOwENi+1s7JLd5F4v=T3xBdXGk_dPVd9_rgx6e82iybL5tSzXg@mail.gmail.com>
	<DEC34F1E-7D37-4ACD-803E-BDAC4FF5F540@rkuhn.info>
	<CAOwENi+FJTfi-LPO3cW-ntpU_hvi6J8-r-T_OhdfsPweDx8VVA@mail.gmail.com>
	<CAC2Zdp1XRbnmoe=CWAUJN_JRiy0FuzLbm8fyPG5hvpSGf3YqMQ@mail.gmail.com>
	<CACuKZqG8HvZ5ZB1ZrLuj_e3byOsVGcT=oqmtcWeUtpBB4Uheqw@mail.gmail.com>
	<CAC2Zdp0oZyy4Y4wVi8_z=pSBThzCzRmv4-crh+Y-uZCQKn8cKQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20D59D8@G4W3299.americas.hpqcorp.net>
	<5F6252CB-94B7-43AD-8842-D06450D4EE6E@rkuhn.info>
	<CAOwENiK91vr3112tuBDjhkXV53WwzRotdXSPp8R3RKRE5ygWbg@mail.gmail.com>
Message-ID: <CANPzfU8rxNBL5LfvZscDJV0foOYpcyYSaqMxS0cHq7adVMmabQ@mail.gmail.com>

On Mon, Jan 16, 2012 at 5:33 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:

> 2011/12/17 Roland Kuhn <rk at rkuhn.info>:
> > You might ask, why have a non-final field without proper
> synchronization, so
> > let me add that the background to this question is that in my case the
> > construction of the object needs to proceed in two phases because of
> other
> > inter-dependencies, which requires certain fields to be written to after
> the
> > constructor has finished; they will be written to only exactly once, and
> > this happens before ?untrusted? client code obtains the reference (in
> > program order).
>
> May be it's too late for this -- but you can still use final fields,
> but update them via reflection. If you do not publish object reference
> _before_ all reflection-based modifications finished -- you still have
> all final fields guarantee. From you description it seems like it is
> your case.
>
>
That's interesting, can you point me to the spec?

Cheers,
?



>
> > Thanks in advance,
> >
> > Roland
> >
> > On Dec 16, 2011, at 22:49 , Boehm, Hans wrote:
> >
> > Just to be clear: Safe publication for final fields requires that you do
> not
> > make a pointer to the object available to other threads (publish it)
> before
> > the constructor finishes.  ?Safe publication? as used below is a slightly
> > stronger property; you also should not communicate the pointer
> > post-construction to another thread via a data race.  Avoiding them both
> is
> > great advice, but the second one may be difficult to avoid if you need to
> > give a reference to your object to code you don?t trust.  Malicious code
> > could always pass the object to a third thread through a race, hoping
> that
> > the third thread, which has no happens-before relationship to the other
> two,
> > would find your object in an inconsistent state.  My intuition is that
> this
> > is a rare case, but one that does occur.  When it does occur, the odds
> of an
> > actual security hole are probably small, but so are the odds of proving
> > security properties without addressing the issue.
> >
> > Hans
> >
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Yuval
> > Shavit
> > Sent: Friday, December 16, 2011 11:59 AM
> > To: Zhong Yu
> > Cc: concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] synchronized constructors
> >
> > On Fri, Dec 16, 2011 at 2:10 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
> >
> > On Fri, Dec 16, 2011 at 8:38 AM, Yuval Shavit <yshavit at akiban.com>
> wrote:
> >> Several people have made the claim that you could see partially
> >> initialized
> >> state even if the constructor were synchronized, and I don't see how
> this
> >> could be. Yes, you could assign MyPoint myUnsafeRef before MyPoint()
> >> finishes -- but if you ever tried to *use* myUnsafeRef, you would run
> into
> >> a
> >> synchronized block which would then ensure that the constructor
> >> happened-before whatever method you're writing. Seems to me you should
> see
> >> the fully-thread-safe, not-partially-initialized object at that point.
> >
> > If the reference is unsafely published, another thread can get the
> > reference early; it then calls an instance method which may obtain the
> > lock before the creation thread can obtain the lock for the
> > constructor. Therefore the other thread can observe the blank state.
> > As Ruslan corrected me, no partial state can be observed though.
> >
> > Ah yes, I hadn't thought of that.
> >
> > So now, in order to have my class be over-achievingly thread safe, I
> need to
> > replace my synchronized methods with a latch that waits for construction
> to
> > finish and *then* a synchronized (this) {...}. But I probably decide that
> > this is really going far out of my way to support a bad usage pattern,
> so I
> > throw up my arms and say "you'll see either totally blank or
> > post-initialization state, but not partial initialization."  But that
> means
> > I have to guard against uninitialized state in each method, so I probably
> > throw up my arms again and say "just publish the darn object safely!"
>  And
> > then the JLS people come to me and say, "well if that's your requirement,
> > why do you want a synchronized constructor?"  And suddenly the whole
> thing
> > makes sense.
> >
> > This has been an interesting discussion for me! Thanks everyone. :)
> >
> > -Yuval
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> > --
> > [scala-debate on 2009/10/2]
> > Viktor Klang: When will the days of numerical overflow be gone?
> > Ricky Clarkson: One second after 03:14:07 UTC on Tuesday, 19 January 2038
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120116/ef6c1b9a/attachment-0001.html>

From dl at cs.oswego.edu  Mon Jan 16 11:45:46 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 16 Jan 2012 11:45:46 -0500
Subject: [concurrency-interest] Use-cases and issues with the
	ForkJoinPool
In-Reply-To: <CAJp3eRBAwVHW+kkjmu8ph3s7+GxWpRne0iuzocbC-AB53FptAQ@mail.gmail.com>
References: <CAJp3eRCqV3ykLMhgppeR-f2t2sxLoGHwQGc_1dnGB9erZSmitA@mail.gmail.com>
	<4F105DAD.8010602@cs.oswego.edu>
	<CAJp3eRBAwVHW+kkjmu8ph3s7+GxWpRne0iuzocbC-AB53FptAQ@mail.gmail.com>
Message-ID: <4F14543A.6090406@cs.oswego.edu>

On 01/16/12 10:48, Romain Colle wrote:
> I guess the more general question I had was: why does a joining thread
> goes into idle wait (pool.tryAwaitJoin()) if it has a non-empty queue?
> I'd like to at least see it "steal" from anywhere in its own queue,
> and even go for helpJoinTask().

Yet more deja vu. There was initially a FJT method "helpJoin"
that did exactly this, but had a disclaimer that your task
graph must be a dag, and that only the callers of forks
ever perform joins. This is fact the most common case, but
people were concerned that the restrictions were hard
to understand (and unenforceable). We should have at least
documented how to recreate it from the methods that
we do support. The following should suffice for most purposes:

     public V helpJoin() {
         ForkJoinTask<?> t;
         while (!isDone() && (t = pollLocalTask()) != null)
             t.quietlyInvoke();
         return join();
     }

-Doug


From cheremin at gmail.com  Mon Jan 16 11:53:33 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Mon, 16 Jan 2012 20:53:33 +0400
Subject: [concurrency-interest] synchronized constructors
In-Reply-To: <CANPzfU8rxNBL5LfvZscDJV0foOYpcyYSaqMxS0cHq7adVMmabQ@mail.gmail.com>
References: <CAC2Zdp3_KFga6hyMzKtDww4gNGgc4jO1Uy-vNjaEVkwEnLZQig@mail.gmail.com>
	<CACuKZqFpxVutmgrk6pPaU97MLGG+iFMAtu0F5w0M6W9oVD1p1w@mail.gmail.com>
	<5AE501BD-18AB-41C2-8F5D-6D995CD2B83B@rkuhn.info>
	<9319F360221C65428EA819A4E8DC34ED037966AAAF@OPMBOX21UK.options-it.com>
	<08F07B8B-F044-4160-B69A-F7CE7D07C57E@rkuhn.info>
	<CAOwENiKeAStTDxocoC1M_aBo__MQxV3+P-9-FAvTr3eQYsF-aQ@mail.gmail.com>
	<AD0C68FA-5044-4B28-BFC3-4544BB815D46@rkuhn.info>
	<CAOwENi+1s7JLd5F4v=T3xBdXGk_dPVd9_rgx6e82iybL5tSzXg@mail.gmail.com>
	<DEC34F1E-7D37-4ACD-803E-BDAC4FF5F540@rkuhn.info>
	<CAOwENi+FJTfi-LPO3cW-ntpU_hvi6J8-r-T_OhdfsPweDx8VVA@mail.gmail.com>
	<CAC2Zdp1XRbnmoe=CWAUJN_JRiy0FuzLbm8fyPG5hvpSGf3YqMQ@mail.gmail.com>
	<CACuKZqG8HvZ5ZB1ZrLuj_e3byOsVGcT=oqmtcWeUtpBB4Uheqw@mail.gmail.com>
	<CAC2Zdp0oZyy4Y4wVi8_z=pSBThzCzRmv4-crh+Y-uZCQKn8cKQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20D59D8@G4W3299.americas.hpqcorp.net>
	<5F6252CB-94B7-43AD-8842-D06450D4EE6E@rkuhn.info>
	<CAOwENiK91vr3112tuBDjhkXV53WwzRotdXSPp8R3RKRE5ygWbg@mail.gmail.com>
	<CANPzfU8rxNBL5LfvZscDJV0foOYpcyYSaqMxS0cHq7adVMmabQ@mail.gmail.com>
Message-ID: <CAOwENiKq4me3dy+oTfpnJOwbei+MQf5t8+ZbMKMYScwkrHtsUQ@mail.gmail.com>

JSL 17.5.3, http://java.sun.com/docs/books/jls/third_edition/html/memory.html#60903

"In some cases, such as deserialization, the system will need to
change the final fields of an object after construction. Final fields
can be changed via reflection and other implementation dependent
means. The only pattern in which this has reasonable semantics is one
in which an object is constructed and then the final fields of the
object are updated. The object should not be made visible to other
threads, nor should the final fields be read, until all updates to the
final fields of the object are complete. Freezes of a final field
occur both at the end of the constructor in which the final field is
set, and immediately after each modification of a final field via
reflection or other special mechanism."

So it you do not publish reference, and do not read field which will
be modified -- you'll have "freeze" action after reflection-based
update of final field, which is the same as in the end of constructor.
It will look like you have long-long constructor, which finished just
here.

Just read carefully subsequent warnings in JLS, about setting final
field to compile time constant, which can be inlined by compiler.  It
seems like you should set initial f-field value to something, passed
from constructor -- to prevent inlining.

16 ?????? 2012??. 20:42 ???????????? ?iktor ?lang
<viktor.klang at gmail.com> ???????:
>
>
> On Mon, Jan 16, 2012 at 5:33 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:
>>
>> 2011/12/17 Roland Kuhn <rk at rkuhn.info>:
>> > You might ask, why have a non-final field without proper
>> > synchronization, so
>> > let me add that the background to this question is that in my case the
>> > construction of the object needs to proceed in two phases because of
>> > other
>> > inter-dependencies, which requires certain fields to be written to after
>> > the
>> > constructor has finished; they will be written to only exactly once, and
>> > this happens before ?untrusted? client code obtains the reference (in
>> > program order).
>>
>> May be it's too late for this -- but you can still use final fields,
>> but update them via reflection. If you do not publish object reference
>> _before_ all reflection-based modifications finished -- you still have
>> all final fields guarantee. From you description it seems like it is
>> your case.
>>
>
> That's interesting, can you point me to the spec?
>
> Cheers,
> ?
>
>
>>
>>
>> > Thanks in advance,
>> >
>> > Roland
>> >
>> > On Dec 16, 2011, at 22:49 , Boehm, Hans wrote:
>> >
>> > Just to be clear: Safe publication for final fields requires that you do
>> > not
>> > make a pointer to the object available to other threads (publish it)
>> > before
>> > the constructor finishes.? ?Safe publication? as used below is a
>> > slightly
>> > stronger property; you also should not communicate the pointer
>> > post-construction to another thread via a data race.? Avoiding them both
>> > is
>> > great advice, but the second one may be difficult to avoid if you need
>> > to
>> > give a reference to your object to code you don?t trust.? Malicious code
>> > could always pass the object to a third thread through a race, hoping
>> > that
>> > the third thread, which has no happens-before relationship to the other
>> > two,
>> > would find your object in an inconsistent state.? My intuition is that
>> > this
>> > is a rare case, but one that does occur.? When it does occur, the odds
>> > of an
>> > actual security hole are probably small, but so are the odds of proving
>> > security properties without addressing the issue.
>> >
>> > Hans
>> >
>> > From:?concurrency-interest-bounces at cs.oswego.edu
>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]?On Behalf Of?Yuval
>> > Shavit
>> > Sent:?Friday, December 16, 2011 11:59 AM
>> > To:?Zhong Yu
>> > Cc:?concurrency-interest at cs.oswego.edu
>> > Subject:?Re: [concurrency-interest] synchronized constructors
>> >
>> > On Fri, Dec 16, 2011 at 2:10 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>> >
>> > On Fri, Dec 16, 2011 at 8:38 AM, Yuval Shavit <yshavit at akiban.com>
>> > wrote:
>> >> Several people have made the claim that you could see partially
>> >> initialized
>> >> state even if the constructor were synchronized, and I don't see how
>> >> this
>> >> could be. Yes, you could assign MyPoint myUnsafeRef before MyPoint()
>> >> finishes -- but if you ever tried to *use* myUnsafeRef, you would run
>> >> into
>> >> a
>> >> synchronized block which would then ensure that the constructor
>> >> happened-before whatever method you're writing. Seems to me you should
>> >> see
>> >> the fully-thread-safe, not-partially-initialized object at that point.
>> >
>> > If the reference is unsafely published, another thread can get the
>> > reference early; it then calls an instance method which may obtain the
>> > lock before the creation thread can obtain the lock for the
>> > constructor. Therefore the other thread can observe the blank state.
>> > As Ruslan corrected me, no partial state can be observed though.
>> >
>> > Ah yes, I hadn't thought of that.
>> >
>> > So now, in order to have my class be over-achievingly thread safe, I
>> > need to
>> > replace my synchronized methods with a latch that waits for construction
>> > to
>> > finish and *then* a synchronized (this) {...}.?But I probably decide
>> > that
>> > this is really going far out of my way to support a bad usage pattern,
>> > so I
>> > throw up my arms and say "you'll see either totally blank or
>> > post-initialization state, but not partial initialization." ?But that
>> > means
>> > I have to guard against uninitialized state in each method, so I
>> > probably
>> > throw up my arms again and say "just publish the darn object safely!"
>> > ?And
>> > then the JLS people come to me and say, "well if that's your
>> > requirement,
>> > why do you want a synchronized constructor?" ?And suddenly the whole
>> > thing
>> > makes sense.
>> >
>> > This has been an interesting discussion for me! Thanks everyone. :)
>> >
>> > -Yuval
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> > --
>> > [scala-debate on 2009/10/2]
>> > Viktor Klang: When will the days of numerical overflow be gone?
>> > Ricky Clarkson: One second after 03:14:07 UTC on Tuesday, 19 January
>> > 2038
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> --
> Viktor Klang
>
> Akka Tech Lead
> Typesafe?- The software stack for applications that scale
>
> Twitter: @viktorklang
>


From viktor.klang at gmail.com  Mon Jan 16 11:55:05 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 16 Jan 2012 17:55:05 +0100
Subject: [concurrency-interest] synchronized constructors
In-Reply-To: <CAOwENiKq4me3dy+oTfpnJOwbei+MQf5t8+ZbMKMYScwkrHtsUQ@mail.gmail.com>
References: <CAC2Zdp3_KFga6hyMzKtDww4gNGgc4jO1Uy-vNjaEVkwEnLZQig@mail.gmail.com>
	<CACuKZqFpxVutmgrk6pPaU97MLGG+iFMAtu0F5w0M6W9oVD1p1w@mail.gmail.com>
	<5AE501BD-18AB-41C2-8F5D-6D995CD2B83B@rkuhn.info>
	<9319F360221C65428EA819A4E8DC34ED037966AAAF@OPMBOX21UK.options-it.com>
	<08F07B8B-F044-4160-B69A-F7CE7D07C57E@rkuhn.info>
	<CAOwENiKeAStTDxocoC1M_aBo__MQxV3+P-9-FAvTr3eQYsF-aQ@mail.gmail.com>
	<AD0C68FA-5044-4B28-BFC3-4544BB815D46@rkuhn.info>
	<CAOwENi+1s7JLd5F4v=T3xBdXGk_dPVd9_rgx6e82iybL5tSzXg@mail.gmail.com>
	<DEC34F1E-7D37-4ACD-803E-BDAC4FF5F540@rkuhn.info>
	<CAOwENi+FJTfi-LPO3cW-ntpU_hvi6J8-r-T_OhdfsPweDx8VVA@mail.gmail.com>
	<CAC2Zdp1XRbnmoe=CWAUJN_JRiy0FuzLbm8fyPG5hvpSGf3YqMQ@mail.gmail.com>
	<CACuKZqG8HvZ5ZB1ZrLuj_e3byOsVGcT=oqmtcWeUtpBB4Uheqw@mail.gmail.com>
	<CAC2Zdp0oZyy4Y4wVi8_z=pSBThzCzRmv4-crh+Y-uZCQKn8cKQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20D59D8@G4W3299.americas.hpqcorp.net>
	<5F6252CB-94B7-43AD-8842-D06450D4EE6E@rkuhn.info>
	<CAOwENiK91vr3112tuBDjhkXV53WwzRotdXSPp8R3RKRE5ygWbg@mail.gmail.com>
	<CANPzfU8rxNBL5LfvZscDJV0foOYpcyYSaqMxS0cHq7adVMmabQ@mail.gmail.com>
	<CAOwENiKq4me3dy+oTfpnJOwbei+MQf5t8+ZbMKMYScwkrHtsUQ@mail.gmail.com>
Message-ID: <CANPzfU9-3TCtdnbkq_mJ4SkP0tkvuKmRYynjudEe7QqrNkWt9w@mail.gmail.com>

2012/1/16 Ruslan Cheremin <cheremin at gmail.com>

> JSL 17.5.3,
> http://java.sun.com/docs/books/jls/third_edition/html/memory.html#60903
>
> "In some cases, such as deserialization, the system will need to
> change the final fields of an object after construction. Final fields
> can be changed via reflection and other implementation dependent
> means. The only pattern in which this has reasonable semantics is one
> in which an object is constructed and then the final fields of the
> object are updated. The object should not be made visible to other
> threads, nor should the final fields be read, until all updates to the
> final fields of the object are complete. Freezes of a final field
> occur both at the end of the constructor in which the final field is
> set, and immediately after each modification of a final field via
> reflection or other special mechanism."
>
> So it you do not publish reference, and do not read field which will
> be modified -- you'll have "freeze" action after reflection-based
> update of final field, which is the same as in the end of constructor.
> It will look like you have long-long constructor, which finished just
> here.
>
> Just read carefully subsequent warnings in JLS, about setting final
> field to compile time constant, which can be inlined by compiler.  It
> seems like you should set initial f-field value to something, passed
> from constructor -- to prevent inlining.
>

Now I remember that section, that's a great find. You just, as you say,
need to be careful of constant inlining.


>
> 16 ?????? 2012 ?. 20:42 ???????????? ?iktor ?lang
> <viktor.klang at gmail.com> ???????:
> >
> >
> > On Mon, Jan 16, 2012 at 5:33 PM, Ruslan Cheremin <cheremin at gmail.com>
> wrote:
> >>
> >> 2011/12/17 Roland Kuhn <rk at rkuhn.info>:
> >> > You might ask, why have a non-final field without proper
> >> > synchronization, so
> >> > let me add that the background to this question is that in my case the
> >> > construction of the object needs to proceed in two phases because of
> >> > other
> >> > inter-dependencies, which requires certain fields to be written to
> after
> >> > the
> >> > constructor has finished; they will be written to only exactly once,
> and
> >> > this happens before ?untrusted? client code obtains the reference (in
> >> > program order).
> >>
> >> May be it's too late for this -- but you can still use final fields,
> >> but update them via reflection. If you do not publish object reference
> >> _before_ all reflection-based modifications finished -- you still have
> >> all final fields guarantee. From you description it seems like it is
> >> your case.
> >>
> >
> > That's interesting, can you point me to the spec?
> >
> > Cheers,
> > ?
> >
> >
> >>
> >>
> >> > Thanks in advance,
> >> >
> >> > Roland
> >> >
> >> > On Dec 16, 2011, at 22:49 , Boehm, Hans wrote:
> >> >
> >> > Just to be clear: Safe publication for final fields requires that you
> do
> >> > not
> >> > make a pointer to the object available to other threads (publish it)
> >> > before
> >> > the constructor finishes.  ?Safe publication? as used below is a
> >> > slightly
> >> > stronger property; you also should not communicate the pointer
> >> > post-construction to another thread via a data race.  Avoiding them
> both
> >> > is
> >> > great advice, but the second one may be difficult to avoid if you need
> >> > to
> >> > give a reference to your object to code you don?t trust.  Malicious
> code
> >> > could always pass the object to a third thread through a race, hoping
> >> > that
> >> > the third thread, which has no happens-before relationship to the
> other
> >> > two,
> >> > would find your object in an inconsistent state.  My intuition is that
> >> > this
> >> > is a rare case, but one that does occur.  When it does occur, the odds
> >> > of an
> >> > actual security hole are probably small, but so are the odds of
> proving
> >> > security properties without addressing the issue.
> >> >
> >> > Hans
> >> >
> >> > From: concurrency-interest-bounces at cs.oswego.edu
> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> Of Yuval
> >> > Shavit
> >> > Sent: Friday, December 16, 2011 11:59 AM
> >> > To: Zhong Yu
> >> > Cc: concurrency-interest at cs.oswego.edu
> >> > Subject: Re: [concurrency-interest] synchronized constructors
> >> >
> >> > On Fri, Dec 16, 2011 at 2:10 PM, Zhong Yu <zhong.j.yu at gmail.com>
> wrote:
> >> >
> >> > On Fri, Dec 16, 2011 at 8:38 AM, Yuval Shavit <yshavit at akiban.com>
> >> > wrote:
> >> >> Several people have made the claim that you could see partially
> >> >> initialized
> >> >> state even if the constructor were synchronized, and I don't see how
> >> >> this
> >> >> could be. Yes, you could assign MyPoint myUnsafeRef before MyPoint()
> >> >> finishes -- but if you ever tried to *use* myUnsafeRef, you would run
> >> >> into
> >> >> a
> >> >> synchronized block which would then ensure that the constructor
> >> >> happened-before whatever method you're writing. Seems to me you
> should
> >> >> see
> >> >> the fully-thread-safe, not-partially-initialized object at that
> point.
> >> >
> >> > If the reference is unsafely published, another thread can get the
> >> > reference early; it then calls an instance method which may obtain the
> >> > lock before the creation thread can obtain the lock for the
> >> > constructor. Therefore the other thread can observe the blank state.
> >> > As Ruslan corrected me, no partial state can be observed though.
> >> >
> >> > Ah yes, I hadn't thought of that.
> >> >
> >> > So now, in order to have my class be over-achievingly thread safe, I
> >> > need to
> >> > replace my synchronized methods with a latch that waits for
> construction
> >> > to
> >> > finish and *then* a synchronized (this) {...}. But I probably decide
> >> > that
> >> > this is really going far out of my way to support a bad usage pattern,
> >> > so I
> >> > throw up my arms and say "you'll see either totally blank or
> >> > post-initialization state, but not partial initialization."  But that
> >> > means
> >> > I have to guard against uninitialized state in each method, so I
> >> > probably
> >> > throw up my arms again and say "just publish the darn object safely!"
> >> >  And
> >> > then the JLS people come to me and say, "well if that's your
> >> > requirement,
> >> > why do you want a synchronized constructor?"  And suddenly the whole
> >> > thing
> >> > makes sense.
> >> >
> >> > This has been an interesting discussion for me! Thanks everyone. :)
> >> >
> >> > -Yuval
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> >
> >> > --
> >> > [scala-debate on 2009/10/2]
> >> > Viktor Klang: When will the days of numerical overflow be gone?
> >> > Ricky Clarkson: One second after 03:14:07 UTC on Tuesday, 19 January
> >> > 2038
> >> >
> >> >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
> >
> > --
> > Viktor Klang
> >
> > Akka Tech Lead
> > Typesafe - The software stack for applications that scale
> >
> > Twitter: @viktorklang
> >
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120116/d788e138/attachment-0001.html>

From vitalyd at gmail.com  Mon Jan 16 14:00:04 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 16 Jan 2012 14:00:04 -0500
Subject: [concurrency-interest] Problem getting any speed up in using
 the withIndexedMapping method in the ParallelArray class
In-Reply-To: <CAHYWNzAPqEHD_FpiBqV208jmgP3dQ7SSgfr_Ne-fLReF9+QWcA@mail.gmail.com>
References: <CAHYWNzAPqEHD_FpiBqV208jmgP3dQ7SSgfr_Ne-fLReF9+QWcA@mail.gmail.com>
Message-ID: <CAHjP37GxkcPmLBeeg_Bs6W=wJp9j9pS-HZxfiTDq4jWu28o5kw@mail.gmail.com>

Hashing integers in a linearly-traversed array is going to be pretty quick
even on a single core (unless your hash function does something expensive);
for parallel to give you benefit, you may have to throw more work at the
problem.  How large is the int array that you're scanning?

On Mon, Jan 16, 2012 at 9:59 AM, Yuheng Long <csgzlong at iastate.edu> wrote:

> To whom it concerns,
>           I tried to parallelize a for loop these days which iterates the
> elements (Integer) of an array and applies a pure hash function on each of
> the elements
> and substitute the original elements with the results.
>           So I used the withIndexedMapping method in the ParallelArray
> class to parallelize the code.
>           I try both my 4 cores and my 24 cores machines. I did not get
> any speedup.
>           The code snippet is listed below. Would you please tell me what
> I did wrong or explain why I did not get any speedup in the implementation
> at all?
>           I would like to send out the complete code if necessary. Thank
> you.
>
> Hash h = new Hash();
> ForkJoinPool fjp = new ForkJoinPool();
> ParallelArray<Integer> pa = ParallelArray.createUsingHandoff(elementData,
> fjp);
> pa.replaceWithMappedIndex(h);
>
> public class Hash implements Ops.IntAndObjectToObject<Integer, Integer> {
>    public Integer op (int index, Integer o) {
>         int key = o;
>         /* key = the hash computation code */
>         return key;
>    }
> }
>
> Thank you very much,
> Sincerely,
> Yuheng
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120116/5115ef29/attachment.html>

From rk at rkuhn.info  Mon Jan 16 14:04:03 2012
From: rk at rkuhn.info (Roland Kuhn)
Date: Mon, 16 Jan 2012 20:04:03 +0100
Subject: [concurrency-interest] synchronized constructors
In-Reply-To: <CAOwENiKq4me3dy+oTfpnJOwbei+MQf5t8+ZbMKMYScwkrHtsUQ@mail.gmail.com>
References: <CAC2Zdp3_KFga6hyMzKtDww4gNGgc4jO1Uy-vNjaEVkwEnLZQig@mail.gmail.com>
	<CACuKZqFpxVutmgrk6pPaU97MLGG+iFMAtu0F5w0M6W9oVD1p1w@mail.gmail.com>
	<5AE501BD-18AB-41C2-8F5D-6D995CD2B83B@rkuhn.info>
	<9319F360221C65428EA819A4E8DC34ED037966AAAF@OPMBOX21UK.options-it.com>
	<08F07B8B-F044-4160-B69A-F7CE7D07C57E@rkuhn.info>
	<CAOwENiKeAStTDxocoC1M_aBo__MQxV3+P-9-FAvTr3eQYsF-aQ@mail.gmail.com>
	<AD0C68FA-5044-4B28-BFC3-4544BB815D46@rkuhn.info>
	<CAOwENi+1s7JLd5F4v=T3xBdXGk_dPVd9_rgx6e82iybL5tSzXg@mail.gmail.com>
	<DEC34F1E-7D37-4ACD-803E-BDAC4FF5F540@rkuhn.info>
	<CAOwENi+FJTfi-LPO3cW-ntpU_hvi6J8-r-T_OhdfsPweDx8VVA@mail.gmail.com>
	<CAC2Zdp1XRbnmoe=CWAUJN_JRiy0FuzLbm8fyPG5hvpSGf3YqMQ@mail.gmail.com>
	<CACuKZqG8HvZ5ZB1ZrLuj_e3byOsVGcT=oqmtcWeUtpBB4Uheqw@mail.gmail.com>
	<CAC2Zdp0oZyy4Y4wVi8_z=pSBThzCzRmv4-crh+Y-uZCQKn8cKQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20D59D8@G4W3299.americas.hpqcorp.net>
	<5F6252CB-94B7-43AD-8842-D06450D4EE6E@rkuhn.info>
	<CAOwENiK91vr3112tuBDjhkXV53WwzRotd
	XSPp8R3RKRE5ygWbg@mail.gmail.com>
	<CANPzfU8rxNBL5LfvZscDJV0foOYpcyYSaqMxS0cHq7adVMmabQ@mail.gmail.com>
	<CAOwENiKq4me3dy+oTfpnJOwbei+MQf5t8+ZbMKMYScwkrHtsUQ@mail.gmail.com>
Message-ID: <7E596631-56F7-4045-80C3-918588109AF0@rkuhn.info>

On 16 jan 2012, at 17:53, Ruslan Cheremin <cheremin at gmail.com> wrote:

> JSL 17.5.3, http://java.sun.com/docs/books/jls/third_edition/html/memory.html#60903
> 
> "In some cases, such as deserialization, the system will need to
> change the final fields of an object after construction. Final fields
> can be changed via reflection and other implementation dependent
> means. The only pattern in which this has reasonable semantics is one
> in which an object is constructed and then the final fields of the
> object are updated. The object should not be made visible to other
> threads, nor should the final fields be read, until all updates to the
> final fields of the object are complete. Freezes of a final field
> occur both at the end of the constructor in which the final field is
> set, and immediately after each modification of a final field via
> reflection or other special mechanism."
> 
> So it you do not publish reference,

This is precisely the weak spot: if publication happens by data race, the JMM does not guarantee anything (as I was told in this thread). It might work in practice on current processors because the memory barriers supported are coarse grained, but that may change in the future, breaking this code. 

> and do not read field which will
> be modified -- you'll have "freeze" action after reflection-based
> update of final field, which is the same as in the end of constructor.
> It will look like you have long-long constructor, which finished just
> here.
> 
> Just read carefully subsequent warnings in JLS, about setting final
> field to compile time constant, which can be inlined by compiler.  It
> seems like you should set initial f-field value to something, passed
> from constructor -- to prevent inlining.
> 
Yes, good idea, but see above. 

Regards,

Roland

> 16 ?????? 2012 ?. 20:42 ???????????? ?iktor ?lang
> <viktor.klang at gmail.com> ???????:
>> 
>> 
>> On Mon, Jan 16, 2012 at 5:33 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:
>>> 
>>> 2011/12/17 Roland Kuhn <rk at rkuhn.info>:
>>>> You might ask, why have a non-final field without proper
>>>> synchronization, so
>>>> let me add that the background to this question is that in my case the
>>>> construction of the object needs to proceed in two phases because of
>>>> other
>>>> inter-dependencies, which requires certain fields to be written to after
>>>> the
>>>> constructor has finished; they will be written to only exactly once, and
>>>> this happens before ?untrusted? client code obtains the reference (in
>>>> program order).
>>> 
>>> May be it's too late for this -- but you can still use final fields,
>>> but update them via reflection. If you do not publish object reference
>>> _before_ all reflection-based modifications finished -- you still have
>>> all final fields guarantee. From you description it seems like it is
>>> your case.
>>> 
>> 
>> That's interesting, can you point me to the spec?
>> 
>> Cheers,
>> ?
>> 
>> 
>>> 
>>> 
>>>> Thanks in advance,
>>>> 
>>>> Roland
>>>> 
>>>> On Dec 16, 2011, at 22:49 , Boehm, Hans wrote:
>>>> 
>>>> Just to be clear: Safe publication for final fields requires that you do
>>>> not
>>>> make a pointer to the object available to other threads (publish it)
>>>> before
>>>> the constructor finishes.  ?Safe publication? as used below is a
>>>> slightly
>>>> stronger property; you also should not communicate the pointer
>>>> post-construction to another thread via a data race.  Avoiding them both
>>>> is
>>>> great advice, but the second one may be difficult to avoid if you need
>>>> to
>>>> give a reference to your object to code you don?t trust.  Malicious code
>>>> could always pass the object to a third thread through a race, hoping
>>>> that
>>>> the third thread, which has no happens-before relationship to the other
>>>> two,
>>>> would find your object in an inconsistent state.  My intuition is that
>>>> this
>>>> is a rare case, but one that does occur.  When it does occur, the odds
>>>> of an
>>>> actual security hole are probably small, but so are the odds of proving
>>>> security properties without addressing the issue.
>>>> 
>>>> Hans
>>>> 
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Yuval
>>>> Shavit
>>>> Sent: Friday, December 16, 2011 11:59 AM
>>>> To: Zhong Yu
>>>> Cc: concurrency-interest at cs.oswego.edu
>>>> Subject: Re: [concurrency-interest] synchronized constructors
>>>> 
>>>> On Fri, Dec 16, 2011 at 2:10 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>>>> 
>>>> On Fri, Dec 16, 2011 at 8:38 AM, Yuval Shavit <yshavit at akiban.com>
>>>> wrote:
>>>>> Several people have made the claim that you could see partially
>>>>> initialized
>>>>> state even if the constructor were synchronized, and I don't see how
>>>>> this
>>>>> could be. Yes, you could assign MyPoint myUnsafeRef before MyPoint()
>>>>> finishes -- but if you ever tried to *use* myUnsafeRef, you would run
>>>>> into
>>>>> a
>>>>> synchronized block which would then ensure that the constructor
>>>>> happened-before whatever method you're writing. Seems to me you should
>>>>> see
>>>>> the fully-thread-safe, not-partially-initialized object at that point.
>>>> 
>>>> If the reference is unsafely published, another thread can get the
>>>> reference early; it then calls an instance method which may obtain the
>>>> lock before the creation thread can obtain the lock for the
>>>> constructor. Therefore the other thread can observe the blank state.
>>>> As Ruslan corrected me, no partial state can be observed though.
>>>> 
>>>> Ah yes, I hadn't thought of that.
>>>> 
>>>> So now, in order to have my class be over-achievingly thread safe, I
>>>> need to
>>>> replace my synchronized methods with a latch that waits for construction
>>>> to
>>>> finish and *then* a synchronized (this) {...}. But I probably decide
>>>> that
>>>> this is really going far out of my way to support a bad usage pattern,
>>>> so I
>>>> throw up my arms and say "you'll see either totally blank or
>>>> post-initialization state, but not partial initialization."  But that
>>>> means
>>>> I have to guard against uninitialized state in each method, so I
>>>> probably
>>>> throw up my arms again and say "just publish the darn object safely!"
>>>>  And
>>>> then the JLS people come to me and say, "well if that's your
>>>> requirement,
>>>> why do you want a synchronized constructor?"  And suddenly the whole
>>>> thing
>>>> makes sense.
>>>> 
>>>> This has been an interesting discussion for me! Thanks everyone. :)
>>>> 
>>>> -Yuval
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> 
>>>> 
>>>> --
>>>> [scala-debate on 2009/10/2]
>>>> Viktor Klang: When will the days of numerical overflow be gone?
>>>> Ricky Clarkson: One second after 03:14:07 UTC on Tuesday, 19 January
>>>> 2038
>>>> 
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> 
>> 
>> --
>> Viktor Klang
>> 
>> Akka Tech Lead
>> Typesafe - The software stack for applications that scale
>> 
>> Twitter: @viktorklang
>> 


From erkin.kanlioglu at gmail.com  Mon Jan 16 14:38:07 2012
From: erkin.kanlioglu at gmail.com (erkin.kanlioglu at gmail.com)
Date: Mon, 16 Jan 2012 19:38:07 +0000
Subject: [concurrency-interest] Problem getting any speed up in using
	the withIndexedMapping method in the ParallelArray class
In-Reply-To: <CAHjP37GxkcPmLBeeg_Bs6W=wJp9j9pS-HZxfiTDq4jWu28o5kw@mail.gmail.com>
References: <CAHYWNzAPqEHD_FpiBqV208jmgP3dQ7SSgfr_Ne-fLReF9+QWcA@mail.gmail.com>
	<CAHjP37GxkcPmLBeeg_Bs6W=wJp9j9pS-HZxfiTDq4jWu28o5kw@mail.gmail.com>
Message-ID: <DDC54BAC-6031-4487-894C-AF6E0303CE89@gmail.com>

Is there any guideline(rough) how compute dependent an operation should be to get benefit from ParallelArray ?

Creating ParallelArray<int[]> (int[2^21][16]) vs ParallelArray<Integer> for pure a function like hashing gave 10x boost in my local for array size 2^25.



On 16 Jan 2012, at 19:00, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Hashing integers in a linearly-traversed array is going to be pretty quick even on a single core (unless your hash function does something expensive); for parallel to give you benefit, you may have to throw more work at the problem.  How large is the int array that you're scanning?
> 
> On Mon, Jan 16, 2012 at 9:59 AM, Yuheng Long <csgzlong at iastate.edu> wrote:
> To whom it concerns,
>           I tried to parallelize a for loop these days which iterates the elements (Integer) of an array and applies a pure hash function on each of the elements
> and substitute the original elements with the results.
>           So I used the withIndexedMapping method in the ParallelArray class to parallelize the code.
>           I try both my 4 cores and my 24 cores machines. I did not get any speedup.
>           The code snippet is listed below. Would you please tell me what I did wrong or explain why I did not get any speedup in the implementation at all?
>           I would like to send out the complete code if necessary. Thank you.
> 
> Hash h = new Hash();
> ForkJoinPool fjp = new ForkJoinPool();
> ParallelArray<Integer> pa = ParallelArray.createUsingHandoff(elementData, fjp);
> pa.replaceWithMappedIndex(h);               
> 
> public class Hash implements Ops.IntAndObjectToObject<Integer, Integer> {
>    public Integer op (int index, Integer o) {
>         int key = o;
>         /* key = the hash computation code */
>         return key;
>    }
> }
> 
> Thank you very much,
> Sincerely,
> Yuheng
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
> 
> -- 
> Vitaly
> 617-548-7007 (mobile)
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120116/007f3fa1/attachment.html>

From csgzlong at iastate.edu  Mon Jan 16 14:50:34 2012
From: csgzlong at iastate.edu (Yuheng Long)
Date: Mon, 16 Jan 2012 13:50:34 -0600
Subject: [concurrency-interest] Problem getting any speed up in using
 the withIndexedMapping method in the ParallelArray class
In-Reply-To: <CAHjP37GxkcPmLBeeg_Bs6W=wJp9j9pS-HZxfiTDq4jWu28o5kw@mail.gmail.com>
References: <CAHYWNzAPqEHD_FpiBqV208jmgP3dQ7SSgfr_Ne-fLReF9+QWcA@mail.gmail.com>
	<CAHjP37GxkcPmLBeeg_Bs6W=wJp9j9pS-HZxfiTDq4jWu28o5kw@mail.gmail.com>
Message-ID: <CAHYWNzBfUqxhsukwspm4vRmjUUsyGthFmOTFRkgwPcrGUqx7MA@mail.gmail.com>

Hi Vitaly,

Well, I agree it may not be that expensive, but if it this operation/logic
in the system and it is done many many time,
perhaps, you still want to parallelize it, do not you?

> for parallel to give you benefit, you may have to throw more work at the
problem
What do you mean by "you may have to throw more work at the problem".
Would you elaborate, please? What are the problems here?

I tried 10000000 elements in my machines, which takes about 4 seconds.
According to my experience, benchmarks/operations that take more than a
second,
it becomes easier to parallelize (I mean simple parallelization code does
show relatively good speedups.)

Thank you very much,
Sincerely,
Yuheng


On Mon, Jan 16, 2012 at 1:00 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Hashing integers in a linearly-traversed array is going to be pretty quick
> even on a single core (unless your hash function does something expensive);
> for parallel to give you benefit, you may have to throw more work at the
> problem.  How large is the int array that you're scanning?
>
> On Mon, Jan 16, 2012 at 9:59 AM, Yuheng Long <csgzlong at iastate.edu> wrote:
>
>> To whom it concerns,
>>           I tried to parallelize a for loop these days which iterates the
>> elements (Integer) of an array and applies a pure hash function on each of
>> the elements
>> and substitute the original elements with the results.
>>           So I used the withIndexedMapping method in the ParallelArray
>> class to parallelize the code.
>>           I try both my 4 cores and my 24 cores machines. I did not get
>> any speedup.
>>           The code snippet is listed below. Would you please tell me what
>> I did wrong or explain why I did not get any speedup in the implementation
>> at all?
>>           I would like to send out the complete code if necessary. Thank
>> you.
>>
>> Hash h = new Hash();
>> ForkJoinPool fjp = new ForkJoinPool();
>> ParallelArray<Integer> pa = ParallelArray.createUsingHandoff(elementData,
>> fjp);
>> pa.replaceWithMappedIndex(h);
>>
>> public class Hash implements Ops.IntAndObjectToObject<Integer, Integer> {
>>    public Integer op (int index, Integer o) {
>>         int key = o;
>>         /* key = the hash computation code */
>>         return key;
>>    }
>> }
>>
>> Thank you very much,
>> Sincerely,
>> Yuheng
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Vitaly
> 617-548-7007 (mobile)
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120116/23f2b4fe/attachment-0001.html>

From csgzlong at iastate.edu  Mon Jan 16 14:55:27 2012
From: csgzlong at iastate.edu (Yuheng Long)
Date: Mon, 16 Jan 2012 13:55:27 -0600
Subject: [concurrency-interest] Problem getting any speed up in using
 the withIndexedMapping method in the ParallelArray class
In-Reply-To: <CAHYWNzBfUqxhsukwspm4vRmjUUsyGthFmOTFRkgwPcrGUqx7MA@mail.gmail.com>
References: <CAHYWNzAPqEHD_FpiBqV208jmgP3dQ7SSgfr_Ne-fLReF9+QWcA@mail.gmail.com>
	<CAHjP37GxkcPmLBeeg_Bs6W=wJp9j9pS-HZxfiTDq4jWu28o5kw@mail.gmail.com>
	<CAHYWNzBfUqxhsukwspm4vRmjUUsyGthFmOTFRkgwPcrGUqx7MA@mail.gmail.com>
Message-ID: <CAHYWNzDKWz=-SzzPTB9G0PhzRZ13_OzHnz9HA1iQ7cf51BiS9g@mail.gmail.com>

> I tried 10000000 elements in my machines, which takes about 4 seconds.
I increase the size, which takes about 20 seconds (I assume/hope 20 seconds
is expensive enough),
but still, using the withIndexedMapping, I got no benefits.

Thank you.

On Mon, Jan 16, 2012 at 1:50 PM, Yuheng Long <csgzlong at iastate.edu> wrote:

> Hi Vitaly,
>
> Well, I agree it may not be that expensive, but if it this operation/logic
> in the system and it is done many many time,
> perhaps, you still want to parallelize it, do not you?
>
>
> > for parallel to give you benefit, you may have to throw more work at the
> problem
> What do you mean by "you may have to throw more work at the problem".
> Would you elaborate, please? What are the problems here?
>
> I tried 10000000 elements in my machines, which takes about 4 seconds.
> According to my experience, benchmarks/operations that take more than a
> second,
> it becomes easier to parallelize (I mean simple parallelization code does
> show relatively good speedups.)
>
>
> Thank you very much,
> Sincerely,
> Yuheng
>
>
> On Mon, Jan 16, 2012 at 1:00 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> Hashing integers in a linearly-traversed array is going to be pretty
>> quick even on a single core (unless your hash function does something
>> expensive); for parallel to give you benefit, you may have to throw more
>> work at the problem.  How large is the int array that you're scanning?
>>
>> On Mon, Jan 16, 2012 at 9:59 AM, Yuheng Long <csgzlong at iastate.edu>wrote:
>>
>>> To whom it concerns,
>>>           I tried to parallelize a for loop these days which iterates
>>> the elements (Integer) of an array and applies a pure hash function on each
>>> of the elements
>>> and substitute the original elements with the results.
>>>           So I used the withIndexedMapping method in the ParallelArray
>>> class to parallelize the code.
>>>           I try both my 4 cores and my 24 cores machines. I did not get
>>> any speedup.
>>>           The code snippet is listed below. Would you please tell me
>>> what I did wrong or explain why I did not get any speedup in the
>>> implementation at all?
>>>           I would like to send out the complete code if necessary. Thank
>>> you.
>>>
>>> Hash h = new Hash();
>>> ForkJoinPool fjp = new ForkJoinPool();
>>> ParallelArray<Integer> pa =
>>> ParallelArray.createUsingHandoff(elementData, fjp);
>>> pa.replaceWithMappedIndex(h);
>>>
>>> public class Hash implements Ops.IntAndObjectToObject<Integer, Integer> {
>>>    public Integer op (int index, Integer o) {
>>>         int key = o;
>>>         /* key = the hash computation code */
>>>         return key;
>>>    }
>>> }
>>>
>>> Thank you very much,
>>> Sincerely,
>>> Yuheng
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> --
>> Vitaly
>> 617-548-7007 (mobile)
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120116/967b5867/attachment.html>

From vitalyd at gmail.com  Mon Jan 16 15:11:03 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 16 Jan 2012 15:11:03 -0500
Subject: [concurrency-interest] Problem getting any speed up in using
 the withIndexedMapping method in the ParallelArray class
In-Reply-To: <CAHYWNzDKWz=-SzzPTB9G0PhzRZ13_OzHnz9HA1iQ7cf51BiS9g@mail.gmail.com>
References: <CAHYWNzAPqEHD_FpiBqV208jmgP3dQ7SSgfr_Ne-fLReF9+QWcA@mail.gmail.com>
	<CAHjP37GxkcPmLBeeg_Bs6W=wJp9j9pS-HZxfiTDq4jWu28o5kw@mail.gmail.com>
	<CAHYWNzBfUqxhsukwspm4vRmjUUsyGthFmOTFRkgwPcrGUqx7MA@mail.gmail.com>
	<CAHYWNzDKWz=-SzzPTB9G0PhzRZ13_OzHnz9HA1iQ7cf51BiS9g@mail.gmail.com>
Message-ID: <CAHjP37GyKB2u=eLCT0FBQdoTf-fmFdt9r_HYJmaQaNBkrDbJ3g@mail.gmail.com>

If your hash function is trivial, then this test is probably stressing the
memory interconnect more than anything else; all the cores are going to
compete for bus bandwidth to service memory fetches.

Can you try making your hash function more expensive compute wise? The
problem needs to become compute bound rather than memory bound ...
 On Jan 16, 2012 2:55 PM, "Yuheng Long" <csgzlong at iastate.edu> wrote:

> > I tried 10000000 elements in my machines, which takes about 4 seconds.
> I increase the size, which takes about 20 seconds (I assume/hope 20
> seconds
> is expensive enough),
> but still, using the withIndexedMapping, I got no benefits.
>
> Thank you.
>
> On Mon, Jan 16, 2012 at 1:50 PM, Yuheng Long <csgzlong at iastate.edu> wrote:
>
>> Hi Vitaly,
>>
>> Well, I agree it may not be that expensive, but if it this
>> operation/logic in the system and it is done many many time,
>> perhaps, you still want to parallelize it, do not you?
>>
>>
>> > for parallel to give you benefit, you may have to throw more work at
>> the problem
>>  What do you mean by "you may have to throw more work at the problem".
>> Would you elaborate, please? What are the problems here?
>>
>> I tried 10000000 elements in my machines, which takes about 4 seconds.
>> According to my experience, benchmarks/operations that take more than a
>> second,
>> it becomes easier to parallelize (I mean simple parallelization code does
>> show relatively good speedups.)
>>
>>
>> Thank you very much,
>> Sincerely,
>> Yuheng
>>
>>
>> On Mon, Jan 16, 2012 at 1:00 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>
>>> Hashing integers in a linearly-traversed array is going to be pretty
>>> quick even on a single core (unless your hash function does something
>>> expensive); for parallel to give you benefit, you may have to throw more
>>> work at the problem.  How large is the int array that you're scanning?
>>>
>>> On Mon, Jan 16, 2012 at 9:59 AM, Yuheng Long <csgzlong at iastate.edu>wrote:
>>>
>>>> To whom it concerns,
>>>>           I tried to parallelize a for loop these days which iterates
>>>> the elements (Integer) of an array and applies a pure hash function on each
>>>> of the elements
>>>> and substitute the original elements with the results.
>>>>           So I used the withIndexedMapping method in the ParallelArray
>>>> class to parallelize the code.
>>>>           I try both my 4 cores and my 24 cores machines. I did not get
>>>> any speedup.
>>>>           The code snippet is listed below. Would you please tell me
>>>> what I did wrong or explain why I did not get any speedup in the
>>>> implementation at all?
>>>>           I would like to send out the complete code if necessary.
>>>> Thank you.
>>>>
>>>> Hash h = new Hash();
>>>> ForkJoinPool fjp = new ForkJoinPool();
>>>> ParallelArray<Integer> pa =
>>>> ParallelArray.createUsingHandoff(elementData, fjp);
>>>> pa.replaceWithMappedIndex(h);
>>>>
>>>> public class Hash implements Ops.IntAndObjectToObject<Integer, Integer>
>>>> {
>>>>    public Integer op (int index, Integer o) {
>>>>         int key = o;
>>>>         /* key = the hash computation code */
>>>>         return key;
>>>>    }
>>>> }
>>>>
>>>> Thank you very much,
>>>> Sincerely,
>>>> Yuheng
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> --
>>> Vitaly
>>> 617-548-7007 (mobile)
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120116/4e809b60/attachment.html>

From cheremin at gmail.com  Tue Jan 17 06:39:28 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 17 Jan 2012 15:39:28 +0400
Subject: [concurrency-interest] padding in Exchanger
Message-ID: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>

My question is about cache-line padding in j.u.c.Exchanger.Slot class:

private static final class Slot extends AtomicReference<Object> {
        // Improve likelihood of isolation on <= 64 byte cache lines
        long q0, q1, q2, q3, q4, q5, q6, q7, q8, q9, qa, qb, qc, qd, qe;
}

Comments telling about <=64 byte cache line padding, but it seems for
me what 15 longs*8 = 120 bytes (+8 bytes on object header, I suppose =
even 128). Why to have 128 bytes padding for 64 byte cache line? It
seems for me, what if I rely on one-side padding, I need only 7 longs
(+header).

----
Cheremin Ruslan

From dl at cs.oswego.edu  Tue Jan 17 06:52:15 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Jan 2012 06:52:15 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
Message-ID: <4F1560EF.7050206@cs.oswego.edu>

On 01/17/12 06:39, Ruslan Cheremin wrote:
> My question is about cache-line padding in j.u.c.Exchanger.Slot class:
>
> private static final class Slot extends AtomicReference<Object>  {
>          // Improve likelihood of isolation on<= 64 byte cache lines
>          long q0, q1, q2, q3, q4, q5, q6, q7, q8, q9, qa, qb, qc, qd, qe;
> }
>
> Comments telling about<=64 byte cache line padding, but it seems for
> me what 15 longs*8 = 120 bytes (+8 bytes on object header, I suppose =
> even 128). Why to have 128 bytes padding for 64 byte cache line? It
> seems for me, what if I rely on one-side padding, I need only 7 longs
> (+header).

The short answer is that JVMs do not guarantee alignment.
For a longer answer, see the internals of the new Striped64 class at
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/Striped64.java?view=log

and/or the overhauled Exchanger class at
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/Exchanger.java?view=log

In general, especially on multisocket multicores these days,
you need to waste a lot of space to overcome crippling memory
contention effects that can easily slow down an application
by a factor of 4; sometimes more. Were trying to isolate
the cases where doing so is worthwhile inside libraries,
so that users don't need to think about it very often, so
long as they use the provided components.

-Doug

From cheremin at gmail.com  Tue Jan 17 07:28:00 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 17 Jan 2012 16:28:00 +0400
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F1560EF.7050206@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
Message-ID: <CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>

Thank you for your comment. I've just read both links, but still do
not find answer.

Let me clarify my question. I understand the reasons for cache line
padding in general. I understand what it is better to have both-side
padding like it is in Striped64.Cell
static final class Cell {
        volatile long p0, p1, p2, p3, p4, p5, p6;
        volatile long value;
        volatile long q0, q1, q2, q3, q4, q5, q6;

     ...other methods...

}

since it is close to bulletproof version (assuming field layout
algorithm in JVM is stable, and does not reorder same-type adjusted
fields -- which is true for now, AFAIK). But Exchanger.Slot in my JDK,
end even Exchanger.Node in version you've refered to used one-side
padding. And it is not clear for me how, on system with 64 bytes cache
line, one-side padding 128 bytes long can be better then one side
padding 64 bytes long. Even taking in account possible field
reordering by JVM. From my point of view one side padding is not
bulletproof anyway...


> The short answer is that JVMs do not guarantee alignment.
> For a longer answer, see the internals of the new Striped64 class at
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/Striped64.java?view=log
>
> and/or the overhauled Exchanger class at
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/Exchanger.java?view=log
>
> In general, especially on multisocket multicores these days,
> you need to waste a lot of space to overcome crippling memory
> contention effects that can easily slow down an application
> by a factor of 4; sometimes more. Were trying to isolate
> the cases where doing so is worthwhile inside libraries,
> so that users don't need to think about it very often, so
> long as they use the provided components.
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From viktor.klang at gmail.com  Tue Jan 17 07:33:33 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 17 Jan 2012 13:33:33 +0100
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F1560EF.7050206@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
Message-ID: <CANPzfU_zVEn_d8HAd2he7T0dMpQvHtwPWTCbwVd5p2cMUiQ-cA@mail.gmail.com>

On Tue, Jan 17, 2012 at 12:52 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/17/12 06:39, Ruslan Cheremin wrote:
>
>> My question is about cache-line padding in j.u.c.Exchanger.Slot class:
>>
>> private static final class Slot extends AtomicReference<Object>  {
>>         // Improve likelihood of isolation on<= 64 byte cache lines
>>         long q0, q1, q2, q3, q4, q5, q6, q7, q8, q9, qa, qb, qc, qd, qe;
>> }
>>
>> Comments telling about<=64 byte cache line padding, but it seems for
>> me what 15 longs*8 = 120 bytes (+8 bytes on object header, I suppose =
>> even 128). Why to have 128 bytes padding for 64 byte cache line? It
>> seems for me, what if I rely on one-side padding, I need only 7 longs
>> (+header).
>>
>
> The short answer is that JVMs do not guarantee alignment.
> For a longer answer, see the internals of the new Striped64 class at
> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**
> jsr166e/Striped64.java?view=**log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/Striped64.java?view=log>
>
> and/or the overhauled Exchanger class at
> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**
> main/java/util/concurrent/**Exchanger.java?view=log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/Exchanger.java?view=log>
>
> In general, especially on multisocket multicores these days,
> you need to waste a lot of space to overcome crippling memory
> contention effects that can easily slow down an application
> by a factor of 4; sometimes more. Were trying to isolate
> the cases where doing so is worthwhile inside libraries,
> so that users don't need to think about it very often, so
> long as they use the provided components.
>

Also, one needs to think about compressedOops

Cheers,
?


>
> -Doug
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/c3022316/attachment.html>

From dl at cs.oswego.edu  Tue Jan 17 07:40:04 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Jan 2012 07:40:04 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
Message-ID: <4F156C24.6020403@cs.oswego.edu>

On 01/17/12 07:28, Ruslan Cheremin wrote:

> . From my point of view one side padding is not
> bulletproof anyway...

Yes. As a practical matter though, until an @Contended attribute
or something like it is supported across JVMS (see list archives for
discussion), you cannot arrange reliable two-sided padding
for objects with mixed field types (ints, longs, refs that may be
either 32 or 64 bits, etc), so one-sided is the best you can do.

-Doug

From cheremin at gmail.com  Tue Jan 17 07:47:58 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 17 Jan 2012 16:47:58 +0400
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F156C24.6020403@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
Message-ID: <CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>

Yes, I understand. I do not understand why -- in current conditions --
128 bytes padding is better then 64 bytes one. Both are not
bulletproof, and 64 bytes seems to be enough for arch with 64 cache
line...

Memory is cheap, but cache memory is still expensive.

2012/1/17 Doug Lea <dl at cs.oswego.edu>:
> On 01/17/12 07:28, Ruslan Cheremin wrote:
>
>> . From my point of view one side padding is not
>> bulletproof anyway...
>
>
> Yes. As a practical matter though, until an @Contended attribute
> or something like it is supported across JVMS (see list archives for
> discussion), you cannot arrange reliable two-sided padding
> for objects with mixed field types (ints, longs, refs that may be
> either 32 or 64 bits, etc), so one-sided is the best you can do.
>
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From dl at cs.oswego.edu  Tue Jan 17 07:56:34 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Jan 2012 07:56:34 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>	<4F1560EF.7050206@cs.oswego.edu>	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>
Message-ID: <4F157002.8010003@cs.oswego.edu>

On 01/17/12 07:47, Ruslan Cheremin wrote:
> Yes, I understand. I do not understand why -- in current conditions --
> 128 bytes padding is better then 64 bytes one. Both are not
> bulletproof, and 64 bytes seems to be enough for arch with 64 cache
> line...

Several common processors (including Intel i7s) are normally run in
128byte cache line mode. There are also a few less common processors
such as recent POWER that normally run with even larger cache lines,
but until we get better JVM support, the best we can do is target
the most common cases.

-Doug


From cheremin at gmail.com  Tue Jan 17 07:59:00 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 17 Jan 2012 16:59:00 +0400
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F157002.8010003@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>
	<4F157002.8010003@cs.oswego.edu>
Message-ID: <CAOwENiJsObxgQiSWuCvEfj4MQS+js9tUtBn=USEVpxiThR3mbQ@mail.gmail.com>

Oh, so easy... It was comments in code which confuses me -- since it
explicitly refer to this padding as targeted to 64 bytes cache lines
arch. So I was thinking "may be I miss something important here?"

Thank you for clarification!

2012/1/17 Doug Lea <dl at cs.oswego.edu>:
> On 01/17/12 07:47, Ruslan Cheremin wrote:
>>
>> Yes, I understand. I do not understand why -- in current conditions --
>> 128 bytes padding is better then 64 bytes one. Both are not
>> bulletproof, and 64 bytes seems to be enough for arch with 64 cache
>> line...
>
>
> Several common processors (including Intel i7s) are normally run in
> 128byte cache line mode. There are also a few less common processors
> such as recent POWER that normally run with even larger cache lines,
> but until we get better JVM support, the best we can do is target
> the most common cases.
>
> -Doug
>

From viktor.klang at gmail.com  Tue Jan 17 08:01:49 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 17 Jan 2012 14:01:49 +0100
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F157002.8010003@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>
	<4F157002.8010003@cs.oswego.edu>
Message-ID: <CANPzfU_8HMByq0C--1jm7AKcTP_P3K=Scuvv0fr6nSJm5TzxsA@mail.gmail.com>

On Tue, Jan 17, 2012 at 1:56 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/17/12 07:47, Ruslan Cheremin wrote:
>
>> Yes, I understand. I do not understand why -- in current conditions --
>> 128 bytes padding is better then 64 bytes one. Both are not
>> bulletproof, and 64 bytes seems to be enough for arch with 64 cache
>> line...
>>
>
> Several common processors (including Intel i7s) are normally run in
> 128byte cache line mode. There are also a few less common processors
> such as recent POWER that normally run with even larger cache lines,
> but until we get better JVM support, the best we can do is target
> the most common cases.


What is the outlook for getting the contended annotation in the java spec?

Cheers,
?


>
> -Doug
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/5512e83a/attachment.html>

From dl at cs.oswego.edu  Tue Jan 17 08:18:19 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Jan 2012 08:18:19 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CANPzfU_8HMByq0C--1jm7AKcTP_P3K=Scuvv0fr6nSJm5TzxsA@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>	<4F1560EF.7050206@cs.oswego.edu>	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>	<4F156C24.6020403@cs.oswego.edu>	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>	<4F157002.8010003@cs.oswego.edu>
	<CANPzfU_8HMByq0C--1jm7AKcTP_P3K=Scuvv0fr6nSJm5TzxsA@mail.gmail.com>
Message-ID: <4F15751B.1030707@cs.oswego.edu>

On 01/17/12 08:01, ?iktor ?lang wrote:
> What is the outlook for getting the contended annotation in the java spec?
>

I have expected an OpenJDK Enhancement Proposal (JEP -- see
http://openjdk.java.net/jeps/0) on this to appear any day now
for about four months. People at Oracle reading this list
should look into what it holding it up :-)

-Doug




From viktor.klang at gmail.com  Tue Jan 17 08:35:29 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 17 Jan 2012 14:35:29 +0100
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F15751B.1030707@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>
	<4F157002.8010003@cs.oswego.edu>
	<CANPzfU_8HMByq0C--1jm7AKcTP_P3K=Scuvv0fr6nSJm5TzxsA@mail.gmail.com>
	<4F15751B.1030707@cs.oswego.edu>
Message-ID: <CANPzfU_zWb+_siZOMgdhgW4yZ2WpOdVcTM4Evrmg=o6UeVjxoA@mail.gmail.com>

On Tue, Jan 17, 2012 at 2:18 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/17/12 08:01, ?iktor ?lang wrote:
>
>> What is the outlook for getting the contended annotation in the java spec?
>>
>>
> I have expected an OpenJDK Enhancement Proposal (JEP -- see
> http://openjdk.java.net/jeps/0**) on this to appear any day now
> for about four months. People at Oracle reading this list
> should look into what it holding it up :-)


I'll wait until end of January, then I'll occupy something ;-)


>
>
> -Doug
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/e3d4aaf2/attachment-0001.html>

From vitalyd at gmail.com  Tue Jan 17 09:33:13 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 09:33:13 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F15751B.1030707@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>
	<4F157002.8010003@cs.oswego.edu>
	<CANPzfU_8HMByq0C--1jm7AKcTP_P3K=Scuvv0fr6nSJm5TzxsA@mail.gmail.com>
	<4F15751B.1030707@cs.oswego.edu>
Message-ID: <CAHjP37Fq5WvDq+4ZwnXAkmrQeM-0E16zE_zbx9sm122mgnd4tQ@mail.gmail.com>

Doug,

For what it's worth, +1 on the annotation proposal.

Curious - which i7 arch has 128 byte cache line? I know Netburst used to
have 128 as a blocked pair of two 64 byte lines, but didn't see anything on
the newer chips.

Thanks
On Jan 17, 2012 8:24 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:

> On 01/17/12 08:01, ?iktor ?lang wrote:
>
>> What is the outlook for getting the contended annotation in the java spec?
>>
>>
> I have expected an OpenJDK Enhancement Proposal (JEP -- see
> http://openjdk.java.net/jeps/0**) on this to appear any day now
> for about four months. People at Oracle reading this list
> should look into what it holding it up :-)
>
> -Doug
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/3b3db911/attachment.html>

From radhakrishnan.mohan at gmail.com  Tue Jan 17 09:40:05 2012
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Tue, 17 Jan 2012 20:10:05 +0530
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F1560EF.7050206@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
Message-ID: <CAOoXFP-8CNpp3+NHNcWfpnJMYthkiqFPSs_ssEJ9Y_VN-7R7ow@mail.gmail.com>

The more the padding the less the contention in cache. Is this the
general idea apart from the calculation ?

Thanks,
Mohan

On Tue, Jan 17, 2012 at 5:22 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 01/17/12 06:39, Ruslan Cheremin wrote:
>>
>> My question is about cache-line padding in j.u.c.Exchanger.Slot class:
>>
>> private static final class Slot extends AtomicReference<Object> ?{
>> ? ? ? ? // Improve likelihood of isolation on<= 64 byte cache lines
>> ? ? ? ? long q0, q1, q2, q3, q4, q5, q6, q7, q8, q9, qa, qb, qc, qd, qe;
>> }
>>
>> Comments telling about<=64 byte cache line padding, but it seems for
>> me what 15 longs*8 = 120 bytes (+8 bytes on object header, I suppose =
>> even 128). Why to have 128 bytes padding for 64 byte cache line? It
>> seems for me, what if I rely on one-side padding, I need only 7 longs
>> (+header).
>
>
> The short answer is that JVMs do not guarantee alignment.
> For a longer answer, see the internals of the new Striped64 class at
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/Striped64.java?view=log
>
> and/or the overhauled Exchanger class at
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/Exchanger.java?view=log
>
> In general, especially on multisocket multicores these days,
> you need to waste a lot of space to overcome crippling memory
> contention effects that can easily slow down an application
> by a factor of 4; sometimes more. Were trying to isolate
> the cases where doing so is worthwhile inside libraries,
> so that users don't need to think about it very often, so
> long as they use the provided components.
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Tue Jan 17 09:49:02 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 09:49:02 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAOoXFP-8CNpp3+NHNcWfpnJMYthkiqFPSs_ssEJ9Y_VN-7R7ow@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOoXFP-8CNpp3+NHNcWfpnJMYthkiqFPSs_ssEJ9Y_VN-7R7ow@mail.gmail.com>
Message-ID: <CAHjP37G5FZHdEzQ44xrbphz81BrHZxnLbTQe+zn2YVkEoNppYw@mail.gmail.com>

It's not really a matter of more is better - you just need sufficient
padding to avoid two unrelated memory locations written by two different
cores being on the same cache line or else it'll cause excessive coherency
traffic.   Google "false sharing" for more info.
On Jan 17, 2012 9:42 AM, "Mohan Radhakrishnan" <
radhakrishnan.mohan at gmail.com> wrote:

> The more the padding the less the contention in cache. Is this the
> general idea apart from the calculation ?
>
> Thanks,
> Mohan
>
> On Tue, Jan 17, 2012 at 5:22 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> > On 01/17/12 06:39, Ruslan Cheremin wrote:
> >>
> >> My question is about cache-line padding in j.u.c.Exchanger.Slot class:
> >>
> >> private static final class Slot extends AtomicReference<Object>  {
> >>         // Improve likelihood of isolation on<= 64 byte cache lines
> >>         long q0, q1, q2, q3, q4, q5, q6, q7, q8, q9, qa, qb, qc, qd, qe;
> >> }
> >>
> >> Comments telling about<=64 byte cache line padding, but it seems for
> >> me what 15 longs*8 = 120 bytes (+8 bytes on object header, I suppose =
> >> even 128). Why to have 128 bytes padding for 64 byte cache line? It
> >> seems for me, what if I rely on one-side padding, I need only 7 longs
> >> (+header).
> >
> >
> > The short answer is that JVMs do not guarantee alignment.
> > For a longer answer, see the internals of the new Striped64 class at
> >
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/Striped64.java?view=log
> >
> > and/or the overhauled Exchanger class at
> >
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/Exchanger.java?view=log
> >
> > In general, especially on multisocket multicores these days,
> > you need to waste a lot of space to overcome crippling memory
> > contention effects that can easily slow down an application
> > by a factor of 4; sometimes more. Were trying to isolate
> > the cases where doing so is worthwhile inside libraries,
> > so that users don't need to think about it very often, so
> > long as they use the provided components.
> >
> > -Doug
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/5adfda21/attachment.html>

From heinz at javaspecialists.eu  Tue Jan 17 10:03:33 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 17 Jan 2012 17:03:33 +0200
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
Message-ID: <4F158DC5.6030203@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/25f078b5/attachment.html>

From vitalyd at gmail.com  Tue Jan 17 10:11:58 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 10:11:58 -0500
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
In-Reply-To: <4F158DC5.6030203@javaspecialists.eu>
References: <4F158DC5.6030203@javaspecialists.eu>
Message-ID: <CAHjP37HrB6AeRsiUrK-+=BH+kLOGDauw4o-n8dm6KjUm41GW0w@mail.gmail.com>

Perhaps because you don't know upfront how many threads will actually be
needed to service the workload and thus don't want to spin them up eagerly
(for perf and efficiency reasons) but rather adjust as work comes in? That
is, what if just 1 thread is enough but pool is configured to allow for
more ...

Sent from my phone
On Jan 17, 2012 10:06 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
wrote:

> **
> A quick historical question.  What was the thinking behind constructing
> the threads in the ThreadPoolExecutor lazily?
>
> In the JavaDocs of ThreadPoolExecutor:
>
> *On-demand construction*
>     By default, even core threads are initially created and started only
> when new tasks arrive, but this can be overridden dynamically using method
> {@link ThreadPoolExecutor#prestartCoreThread} or {@link
> ThreadPoolExecutor#prestartAllCoreThreads}. You probably want to prestart
> threads if you construct the pool with a non-empty queue.
>
> I can think of two possible answers:
>
> Correctness: The author of this class did not want the ThreadPoolExecutor
> instance to escape into the threads before it had completed being
> constructed.  We should avoid letting "this" escape during construction.
>
> Performance: It might be better to throttle the construction of new
> threads.  Say someone creates a new fixed thread pool with 2000 threads,
> instead of waiting for them all to be created, we let each task submission
> create a thread until we reach the core size.
>
> Or perhaps it was neither of these two answers.  Maybe it was just easier
> to write it this way, so that one class could be used for both the cached
> and the fixed thread pool.
>
> Would love to hear the reasons for this :-)
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/9213f1b2/attachment.html>

From heinz at javaspecialists.eu  Tue Jan 17 10:15:47 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 17 Jan 2012 17:15:47 +0200
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
In-Reply-To: <CAHjP37HrB6AeRsiUrK-+=BH+kLOGDauw4o-n8dm6KjUm41GW0w@mail.gmail.com>
References: <4F158DC5.6030203@javaspecialists.eu>
	<CAHjP37HrB6AeRsiUrK-+=BH+kLOGDauw4o-n8dm6KjUm41GW0w@mail.gmail.com>
Message-ID: <4F1590A3.4000000@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/fe7aa681/attachment-0001.html>

From dl at cs.oswego.edu  Tue Jan 17 10:17:00 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Jan 2012 10:17:00 -0500
Subject: [concurrency-interest] On-demand Construction of Threads
	in	ThreadPoolExecutor
In-Reply-To: <4F158DC5.6030203@javaspecialists.eu>
References: <4F158DC5.6030203@javaspecialists.eu>
Message-ID: <4F1590EC.7030709@cs.oswego.edu>

On 01/17/12 10:03, Dr Heinz M. Kabutz wrote:
> A quick historical question. What was the thinking behind constructing the
> threads in the ThreadPoolExecutor lazily?

I don't remember ALL the reasons, so you are free to play
deconstructionist and make some up :-)

-Doug


From dl at cs.oswego.edu  Tue Jan 17 10:22:30 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Jan 2012 10:22:30 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAHjP37Fq5WvDq+4ZwnXAkmrQeM-0E16zE_zbx9sm122mgnd4tQ@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>	<4F1560EF.7050206@cs.oswego.edu>	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>	<4F156C24.6020403@cs.oswego.edu>	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>	<4F157002.8010003@cs.oswego.edu>	<CANPzfU_8HMByq0C--1jm7AKcTP_P3K=Scuvv0fr6nSJm5TzxsA@mail.gmail.com>	<4F15751B.1030707@cs.oswego.edu>
	<CAHjP37Fq5WvDq+4ZwnXAkmrQeM-0E16zE_zbx9sm122mgnd4tQ@mail.gmail.com>
Message-ID: <4F159236.5090006@cs.oswego.edu>

On 01/17/12 09:33, Vitaly Davidovich wrote:
> Curious - which i7 arch has 128 byte cache line? I know Netburst used to have
> 128 as a blocked pair of two 64 byte lines, but didn't see anything on the newer
> chips.

It seems that even the ones without cacheline fusing still do
adjacent-cacheline prefetching, with similar effect.

-Doug


From vitalyd at gmail.com  Tue Jan 17 10:24:12 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 10:24:12 -0500
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
In-Reply-To: <4F1590A3.4000000@javaspecialists.eu>
References: <4F158DC5.6030203@javaspecialists.eu>
	<CAHjP37HrB6AeRsiUrK-+=BH+kLOGDauw4o-n8dm6KjUm41GW0w@mail.gmail.com>
	<4F1590A3.4000000@javaspecialists.eu>
Message-ID: <CAHjP37Hnq+0wpEWZQBhSpCWPkYdiXfYcTpYY9x_p7csaNkOu4A@mail.gmail.com>

I don't have source code in front of me right now but it shouldn't add
threads if there are spare ones unless min or core size was specified,
IIRC.  If one thread can keep up with task submission rate why create more?
For cases where you know you'll have a high rate that outpaces consumption
and don't want to take the hit of waiting for threads to spin up, you call
the pre start method. At least that's what makes sense to me :)

Sent from my phone
On Jan 17, 2012 10:15 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
wrote:

> **
> I doubt it.  Threads are created as tasks are submitted, even if there are
> threads available.  But maybe you are right :-)
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
>
> On 1/17/12 5:11 PM, Vitaly Davidovich wrote:
>
> Perhaps because you don't know upfront how many threads will actually be
> needed to service the workload and thus don't want to spin them up eagerly
> (for perf and efficiency reasons) but rather adjust as work comes in? That
> is, what if just 1 thread is enough but pool is configured to allow for
> more ...
>
> Sent from my phone
> On Jan 17, 2012 10:06 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
> wrote:
>
>> A quick historical question.  What was the thinking behind constructing
>> the threads in the ThreadPoolExecutor lazily?
>>
>> In the JavaDocs of ThreadPoolExecutor:
>>
>> *On-demand construction*
>>     By default, even core threads are initially created and started only
>> when new tasks arrive, but this can be overridden dynamically using method
>> {@link ThreadPoolExecutor#prestartCoreThread} or {@link
>> ThreadPoolExecutor#prestartAllCoreThreads}. You probably want to prestart
>> threads if you construct the pool with a non-empty queue.
>>
>> I can think of two possible answers:
>>
>> Correctness: The author of this class did not want the ThreadPoolExecutor
>> instance to escape into the threads before it had completed being
>> constructed.  We should avoid letting "this" escape during construction.
>>
>> Performance: It might be better to throttle the construction of new
>> threads.  Say someone creates a new fixed thread pool with 2000 threads,
>> instead of waiting for them all to be created, we let each task submission
>> create a thread until we reach the core size.
>>
>> Or perhaps it was neither of these two answers.  Maybe it was just easier
>> to write it this way, so that one class could be used for both the cached
>> and the fixed thread pool.
>>
>> Would love to hear the reasons for this :-)
>>
>> Regards
>>
>> Heinz
>> --
>> Dr Heinz M. Kabutz (PhD CompSci)
>> Author of "The Java(tm) Specialists' Newsletter"
>> Sun Java Champion
>> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
>> Tel: +30 69 72 850 460
>> Skype: kabutz
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/384eeb07/attachment.html>

From vitalyd at gmail.com  Tue Jan 17 10:29:08 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 10:29:08 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F159236.5090006@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>
	<4F157002.8010003@cs.oswego.edu>
	<CANPzfU_8HMByq0C--1jm7AKcTP_P3K=Scuvv0fr6nSJm5TzxsA@mail.gmail.com>
	<4F15751B.1030707@cs.oswego.edu>
	<CAHjP37Fq5WvDq+4ZwnXAkmrQeM-0E16zE_zbx9sm122mgnd4tQ@mail.gmail.com>
	<4F159236.5090006@cs.oswego.edu>
Message-ID: <CAHjP37H78dUebQCKPggrj_sF6vuU7gY3iDRU0N3r3TK5NKggGw@mail.gmail.com>

But does prefetching actually cause sharing issues? Fusing them makes sense
why it would but prefetch seems like it'd be orthogonal to this.  Perhaps
I'm wrong though ...

Thanks

Sent from my phone
On Jan 17, 2012 10:23 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:

> On 01/17/12 09:33, Vitaly Davidovich wrote:
>
>> Curious - which i7 arch has 128 byte cache line? I know Netburst used to
>> have
>> 128 as a blocked pair of two 64 byte lines, but didn't see anything on
>> the newer
>> chips.
>>
>
> It seems that even the ones without cacheline fusing still do
> adjacent-cacheline prefetching, with similar effect.
>
> -Doug
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/b035f7a0/attachment.html>

From david.dice at gmail.com  Tue Jan 17 10:29:30 2012
From: david.dice at gmail.com (David Dice)
Date: Tue, 17 Jan 2012 10:29:30 -0500
Subject: [concurrency-interest] padding in Exchanger
Message-ID: <CANbRUcj00a16V_U2atociv5=aQjxqu7m6tdAeLUfW1sXXSLQHw@mail.gmail.com>

>
>
> For what it's worth, +1 on the annotation proposal.
>
> Curious - which i7 arch has 128 byte cache line? I know Netburst used to
> have 128 as a blocked pair of two 64 byte lines, but didn't see anything on
> the newer chips.
>
>
The line size and coherence unit is 64 bytes but adjacent sector prefetch
when enabled (sometimes a BIOS setting), can make the coherence unit
effectively 128 bytes, which is why I suggested extra padding to Doug.
 ASP is usually a good bet for single-threaded code, but occasionally it
can cause destructive interference, and it can be a challenge for MP code.


I think Intel officially recommends 128 bytes in their current optimization
manuals.

Dave
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/97eb595c/attachment.html>

From heinz at javaspecialists.eu  Tue Jan 17 10:40:27 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 17 Jan 2012 17:40:27 +0200
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
In-Reply-To: <CAHjP37Hnq+0wpEWZQBhSpCWPkYdiXfYcTpYY9x_p7csaNkOu4A@mail.gmail.com>
References: <4F158DC5.6030203@javaspecialists.eu>	<CAHjP37HrB6AeRsiUrK-+=BH+kLOGDauw4o-n8dm6KjUm41GW0w@mail.gmail.com>	<4F1590A3.4000000@javaspecialists.eu>
	<CAHjP37Hnq+0wpEWZQBhSpCWPkYdiXfYcTpYY9x_p7csaNkOu4A@mail.gmail.com>
Message-ID: <4F15966B.7000207@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/b59e3656/attachment-0001.html>

From vitalyd at gmail.com  Tue Jan 17 10:41:55 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 10:41:55 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CANbRUcj00a16V_U2atociv5=aQjxqu7m6tdAeLUfW1sXXSLQHw@mail.gmail.com>
References: <CANbRUcj00a16V_U2atociv5=aQjxqu7m6tdAeLUfW1sXXSLQHw@mail.gmail.com>
Message-ID: <CAHjP37FhnRU_ja1bmK61pPiTJqHcTQMdGCp2zb62LNT_7BJWHg@mail.gmail.com>

Thanks Dave, good to know.

Sent from my phone
On Jan 17, 2012 10:35 AM, "David Dice" <david.dice at gmail.com> wrote:

>
>> For what it's worth, +1 on the annotation proposal.
>>
>> Curious - which i7 arch has 128 byte cache line? I know Netburst used to
>> have 128 as a blocked pair of two 64 byte lines, but didn't see anything
>> on
>> the newer chips.
>>
>>
> The line size and coherence unit is 64 bytes but adjacent sector prefetch
> when enabled (sometimes a BIOS setting), can make the coherence unit
> effectively 128 bytes, which is why I suggested extra padding to Doug.
>  ASP is usually a good bet for single-threaded code, but occasionally it
> can cause destructive interference, and it can be a challenge for MP code.
>
>
> I think Intel officially recommends 128 bytes in their current
> optimization manuals.
>
> Dave
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/8b987a80/attachment.html>

From mikeb01 at gmail.com  Tue Jan 17 10:45:50 2012
From: mikeb01 at gmail.com (Michael Barker)
Date: Tue, 17 Jan 2012 15:45:50 +0000
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CALwNKeTEnWdvD_Usqe2Y-DgRcpo0-wKpVUfTLNTTTiUtPZ9ToQ@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>
	<4F157002.8010003@cs.oswego.edu>
	<CANPzfU_8HMByq0C--1jm7AKcTP_P3K=Scuvv0fr6nSJm5TzxsA@mail.gmail.com>
	<4F15751B.1030707@cs.oswego.edu>
	<CAHjP37Fq5WvDq+4ZwnXAkmrQeM-0E16zE_zbx9sm122mgnd4tQ@mail.gmail.com>
	<4F159236.5090006@cs.oswego.edu>
	<CAHjP37H78dUebQCKPggrj_sF6vuU7gY3iDRU0N3r3TK5NKggGw@mail.gmail.com>
	<CALwNKeTEnWdvD_Usqe2Y-DgRcpo0-wKpVUfTLNTTTiUtPZ9ToQ@mail.gmail.com>
Message-ID: <CALwNKeQtrLNZsNPTOdT10zRe+ODnxJ57PbgZNhw88VL0+LA3AA@mail.gmail.com>

> But does prefetching actually cause sharing issues? Fusing them makes sense
> why it would but prefetch seems like it'd be orthogonal to this.? Perhaps
> I'm wrong though ...

I was about to ask the same question, my understanding of false
sharing is that it is triggered by writes to the same cache line by
multiple cores resulting in excessive RFO traffic. ?I.e. ownership of
the contented cache line moves frequently between cores. ?If caches
lines are fetched together but written back independently would it
still be a problem?

Mike.


From vitalyd at gmail.com  Tue Jan 17 10:51:18 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 10:51:18 -0500
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
In-Reply-To: <4F15966B.7000207@javaspecialists.eu>
References: <4F158DC5.6030203@javaspecialists.eu>
	<CAHjP37HrB6AeRsiUrK-+=BH+kLOGDauw4o-n8dm6KjUm41GW0w@mail.gmail.com>
	<4F1590A3.4000000@javaspecialists.eu>
	<CAHjP37Hnq+0wpEWZQBhSpCWPkYdiXfYcTpYY9x_p7csaNkOu4A@mail.gmail.com>
	<4F15966B.7000207@javaspecialists.eu>
Message-ID: <CAHjP37E-DgeNNDMrhR2Di=kbpYuSv4x8os4F0msLfR2V9dC3JA@mail.gmail.com>

That's because you created a fixed size pool; as I mentioned about core
size before, the pool will prefer to add threads in this case to come up to
your core size.  Try with Executor.newCachedThreadPool() instead.

Sent from my phone
On Jan 17, 2012 10:40 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
wrote:

> **
> Here's a quick demo:
>
> import java.util.concurrent.*;
>
> public class ThreadPoolOnDemandConstruction {
>   public static void main(String[] args) throws ExecutionException,
> InterruptedException {
>     ThreadPoolExecutor pool = (ThreadPoolExecutor)
> Executors.newFixedThreadPool(5);
>     System.out.println("initial pool size: " + pool.getPoolSize());
>
>     for(int i=0; i<10; i++) {
>       Future<Void> future = pool.submit(new Callable<Void>() {
>         public Void call() throws Exception {
>           return null;
>         }
>       });
>       future.get();
>       System.out.println("Task " + i + " done");
>       System.out.println("pool size: " + pool.getPoolSize());
>     }
>
>     pool.shutdown();
>   }
> }
>
>
> Output is:
>
> initial pool size: 0
> Task 0 done
> pool size: 1
> Task 1 done
> pool size: 2
> Task 2 done
> pool size: 3
> Task 3 done
> pool size: 4
> Task 4 done
> pool size: 5
> Task 5 done
> pool size: 5
> Task 6 done
> pool size: 5
> Task 7 done
> pool size: 5
> Task 8 done
> pool size: 5
> Task 9 done
> pool size: 5
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
>
> On 1/17/12 5:24 PM, Vitaly Davidovich wrote:
>
> I don't have source code in front of me right now but it shouldn't add
> threads if there are spare ones unless min or core size was specified,
> IIRC.  If one thread can keep up with task submission rate why create more?
> For cases where you know you'll have a high rate that outpaces consumption
> and don't want to take the hit of waiting for threads to spin up, you call
> the pre start method. At least that's what makes sense to me :)
>
> Sent from my phone
> On Jan 17, 2012 10:15 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
> wrote:
>
>>  I doubt it.  Threads are created as tasks are submitted, even if there
>> are threads available.  But maybe you are right :-)
>>
>> Regards
>>
>> Heinz
>> --
>> Dr Heinz M. Kabutz (PhD CompSci)
>> Author of "The Java(tm) Specialists' Newsletter"
>> Sun Java Champion
>> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
>> Tel: +30 69 72 850 460
>> Skype: kabutz
>>
>>
>>
>>
>> On 1/17/12 5:11 PM, Vitaly Davidovich wrote:
>>
>> Perhaps because you don't know upfront how many threads will actually be
>> needed to service the workload and thus don't want to spin them up eagerly
>> (for perf and efficiency reasons) but rather adjust as work comes in? That
>> is, what if just 1 thread is enough but pool is configured to allow for
>> more ...
>>
>> Sent from my phone
>> On Jan 17, 2012 10:06 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
>> wrote:
>>
>>> A quick historical question.  What was the thinking behind constructing
>>> the threads in the ThreadPoolExecutor lazily?
>>>
>>> In the JavaDocs of ThreadPoolExecutor:
>>>
>>> *On-demand construction*
>>>     By default, even core threads are initially created and started only
>>> when new tasks arrive, but this can be overridden dynamically using method
>>> {@link ThreadPoolExecutor#prestartCoreThread} or {@link
>>> ThreadPoolExecutor#prestartAllCoreThreads}. You probably want to prestart
>>> threads if you construct the pool with a non-empty queue.
>>>
>>> I can think of two possible answers:
>>>
>>> Correctness: The author of this class did not want the
>>> ThreadPoolExecutor instance to escape into the threads before it had
>>> completed being constructed.  We should avoid letting "this" escape during
>>> construction.
>>>
>>> Performance: It might be better to throttle the construction of new
>>> threads.  Say someone creates a new fixed thread pool with 2000 threads,
>>> instead of waiting for them all to be created, we let each task submission
>>> create a thread until we reach the core size.
>>>
>>> Or perhaps it was neither of these two answers.  Maybe it was just
>>> easier to write it this way, so that one class could be used for both the
>>> cached and the fixed thread pool.
>>>
>>> Would love to hear the reasons for this :-)
>>>
>>> Regards
>>>
>>> Heinz
>>> --
>>> Dr Heinz M. Kabutz (PhD CompSci)
>>> Author of "The Java(tm) Specialists' Newsletter"
>>> Sun Java Champion
>>> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
>>> Tel: +30 69 72 850 460
>>> Skype: kabutz
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/d4d987ee/attachment.html>

From dl at cs.oswego.edu  Tue Jan 17 10:52:10 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Jan 2012 10:52:10 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CALwNKeQtrLNZsNPTOdT10zRe+ODnxJ57PbgZNhw88VL0+LA3AA@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>	<4F1560EF.7050206@cs.oswego.edu>	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>	<4F156C24.6020403@cs.oswego.edu>	<CAOwENiLnu2fDTgP=17m1SZ4JpXn2Sg-RZNXrn7vSLcty1st=AQ@mail.gmail.com>	<4F157002.8010003@cs.oswego.edu>	<CANPzfU_8HMByq0C--1jm7AKcTP_P3K=Scuvv0fr6nSJm5TzxsA@mail.gmail.com>	<4F15751B.1030707@cs.oswego.edu>	<CAHjP37Fq5WvDq+4ZwnXAkmrQeM-0E16zE_zbx9sm122mgnd4tQ@mail.gmail.com>	<4F159236.5090006@cs.oswego.edu>	<CAHjP37H78dUebQCKPggrj_sF6vuU7gY3iDRU0N3r3TK5NKggGw@mail.gmail.com>	<CALwNKeTEnWdvD_Usqe2Y-DgRcpo0-wKpVUfTLNTTTiUtPZ9ToQ@mail.gmail.com>
	<CALwNKeQtrLNZsNPTOdT10zRe+ODnxJ57PbgZNhw88VL0+LA3AA@mail.gmail.com>
Message-ID: <4F15992A.9090602@cs.oswego.edu>

On 01/17/12 10:45, Michael Barker wrote:
> I was about to ask the same question, my understanding of false
> sharing is that it is triggered by writes to the same cache line by
> multiple cores resulting in excessive RFO traffic.  I.e. ownership of
> the contented cache line moves frequently between cores.  If caches
> lines are fetched together but written back independently would it
> still be a problem?

Empirically, yes. If you are curious about your own machines,
one way to check this out is to vary the "ASHIFT" constant in the
new version of Exchanger (which you'd then need to compile/build).
The default is 7 (=> (1 << 7) == 128)  and then run some of
our perf tests like ExchangeLoops.

-Doug


From cheremin at gmail.com  Tue Jan 17 10:53:24 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 17 Jan 2012 19:53:24 +0400
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F156C24.6020403@cs.oswego.edu>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
Message-ID: <CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>

> Yes. As a practical matter though, until an @Contended attribute
> or something like it is supported across JVMS (see list archives for
> discussion), you cannot arrange reliable two-sided padding
> for objects with mixed field types (ints, longs, refs that may be
> either 32 or 64 bits, etc), so one-sided is the best you can do.

By the way -- I was not thinking about @Contended as "make padding for
me". It seems for me like padding is only dirty hack, since nothing
better available. If I would control memory allocation (like JVM does)
I just can allocate @Contended objects on 64 (128... etc) bytes
boundary. I do not have to "pad" them -- nor both, nor one side. And I
suppose @Contended implementation to do exactly this -- "use special
allocator for objects of that type, which allocate them on cache line
boundary"

Am I wrong here?


>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From joe.bowbeer at gmail.com  Tue Jan 17 10:57:03 2012
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 17 Jan 2012 07:57:03 -0800
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
In-Reply-To: <4F158DC5.6030203@javaspecialists.eu>
References: <4F158DC5.6030203@javaspecialists.eu>
Message-ID: <CAHzJPEo3vOYYE_5mfB3UyfU-cD+ftyG4YMeEeeaOFHCFx7cT7Q@mail.gmail.com>

In short, because that's the way it has always been done :-)  See the old
dl.util.concurrent classes for more history.

As a benefit, lazy construction enables implementations like
newCachedThreadPool(), which are on-demand thread pools.

Note that all of these j.u.c. Executors are created by various parameters
of the ThreadPoolExecutor constructor.  Executors, which hides
ThreadPoolExecutor behind factory methods was added fairly late in the
j.u.c. process.

If your question is about starting threads in a constructor, then I'd
answer that none of the EG members consider that proper design, and (as
FindBugs will point out) is particularly problematic for classes intended
to be subclassed.

There are many attributes of ThreadPoolExecutor that are meant to be
configurable before the first thread is started.  (If TPE were redesigned
from scratch, perhaps a Builder pattern would be employed now?)

The fundamental question remains as to whether TPE should be more on-demand
or less on-demand.  In my perspective, it started out entirely on-demand
but has been inching toward less to satisfy different users and uses.

Joe

On Tue, Jan 17, 2012 at 7:03 AM, Dr Heinz M. Kabutz wrote:

> **
> A quick historical question.  What was the thinking behind constructing
> the threads in the ThreadPoolExecutor lazily?
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/c682c0c7/attachment-0001.html>

From heinz at javaspecialists.eu  Tue Jan 17 11:00:26 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 17 Jan 2012 18:00:26 +0200
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
In-Reply-To: <CAHjP37E-DgeNNDMrhR2Di=kbpYuSv4x8os4F0msLfR2V9dC3JA@mail.gmail.com>
References: <4F158DC5.6030203@javaspecialists.eu>	<CAHjP37HrB6AeRsiUrK-+=BH+kLOGDauw4o-n8dm6KjUm41GW0w@mail.gmail.com>	<4F1590A3.4000000@javaspecialists.eu>	<CAHjP37Hnq+0wpEWZQBhSpCWPkYdiXfYcTpYY9x_p7csaNkOu4A@mail.gmail.com>	<4F15966B.7000207@javaspecialists.eu>
	<CAHjP37E-DgeNNDMrhR2Di=kbpYuSv4x8os4F0msLfR2V9dC3JA@mail.gmail.com>
Message-ID: <4F159B1A.7060809@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/222225cc/attachment.html>

From vitalyd at gmail.com  Tue Jan 17 11:13:36 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 11:13:36 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
Message-ID: <CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>

I think it's semantics - if you sometimes allocate with 64/128 byte
alignment then if your object is smaller than 64/128 the rest of the space
is effectively padding.  Or are you saying you want an @Alignment
annotation instead so it's more general? What other uses of custom
alignment do you envision? Java is too high-level  and the underlying
hardware/platform too abstracted away for a general purpose custom
alignment hint, IMHO.

Sent from my phone
On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

> > Yes. As a practical matter though, until an @Contended attribute
> > or something like it is supported across JVMS (see list archives for
> > discussion), you cannot arrange reliable two-sided padding
> > for objects with mixed field types (ints, longs, refs that may be
> > either 32 or 64 bits, etc), so one-sided is the best you can do.
>
> By the way -- I was not thinking about @Contended as "make padding for
> me". It seems for me like padding is only dirty hack, since nothing
> better available. If I would control memory allocation (like JVM does)
> I just can allocate @Contended objects on 64 (128... etc) bytes
> boundary. I do not have to "pad" them -- nor both, nor one side. And I
> suppose @Contended implementation to do exactly this -- "use special
> allocator for objects of that type, which allocate them on cache line
> boundary"
>
> Am I wrong here?
>
>
> >
> > -Doug
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/d945f6ae/attachment.html>

From cheremin at gmail.com  Tue Jan 17 11:27:33 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 17 Jan 2012 20:27:33 +0400
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
Message-ID: <CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>

2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
> I think it's semantics - if you sometimes allocate with 64/128 byte
> alignment then if your object is smaller than 64/128 the rest of the space
> is effectively padding.

Agree. But in case of alignment you lose sense of "one-side" or "two
side" padding -- you do not need "two side padding", you just make
sure object align on cache line boundary.

Actually I was asked is my understanding of how @Contended supposed to
be implemented is right?

>Or are you saying you want an @Alignment annotation
> instead so it's more general? What other uses of custom alignment do you
> envision? Java is too high-level? and the underlying hardware/platform too
> abstracted away for a general purpose custom alignment hint, IMHO.

No, I do not want such ugly thing to happen with java! It's enough C
for such things...


> Sent from my phone
>
> On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>>
>> > Yes. As a practical matter though, until an @Contended attribute
>> > or something like it is supported across JVMS (see list archives for
>> > discussion), you cannot arrange reliable two-sided padding
>> > for objects with mixed field types (ints, longs, refs that may be
>> > either 32 or 64 bits, etc), so one-sided is the best you can do.
>>
>> By the way -- I was not thinking about @Contended as "make padding for
>> me". It seems for me like padding is only dirty hack, since nothing
>> better available. If I would control memory allocation (like JVM does)
>> I just can allocate @Contended objects on 64 (128... etc) bytes
>> boundary. I do not have to "pad" them -- nor both, nor one side. And I
>> suppose @Contended implementation to do exactly this -- "use special
>> allocator for objects of that type, which allocate them on cache line
>> boundary"
>>
>> Am I wrong here?
>>
>>
>> >
>> > -Doug
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Tue Jan 17 11:35:03 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 11:35:03 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
Message-ID: <CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>

OK I see what you mean now.  I imagine @Contended will be used with fields
rather than classes so when the JVM lays out an instance of the class I
assume it will do two-sided padding on the contended field if required or
if natural layout is such that prior fields already fill up a cache line
then only one sided is needed.

Sent from my phone
On Jan 17, 2012 11:27 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

> 2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
> > I think it's semantics - if you sometimes allocate with 64/128 byte
> > alignment then if your object is smaller than 64/128 the rest of the
> space
> > is effectively padding.
>
> Agree. But in case of alignment you lose sense of "one-side" or "two
> side" padding -- you do not need "two side padding", you just make
> sure object align on cache line boundary.
>
> Actually I was asked is my understanding of how @Contended supposed to
> be implemented is right?
>
> >Or are you saying you want an @Alignment annotation
> > instead so it's more general? What other uses of custom alignment do you
> > envision? Java is too high-level  and the underlying hardware/platform
> too
> > abstracted away for a general purpose custom alignment hint, IMHO.
>
> No, I do not want such ugly thing to happen with java! It's enough C
> for such things...
>
>
> > Sent from my phone
> >
> > On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
> >>
> >> > Yes. As a practical matter though, until an @Contended attribute
> >> > or something like it is supported across JVMS (see list archives for
> >> > discussion), you cannot arrange reliable two-sided padding
> >> > for objects with mixed field types (ints, longs, refs that may be
> >> > either 32 or 64 bits, etc), so one-sided is the best you can do.
> >>
> >> By the way -- I was not thinking about @Contended as "make padding for
> >> me". It seems for me like padding is only dirty hack, since nothing
> >> better available. If I would control memory allocation (like JVM does)
> >> I just can allocate @Contended objects on 64 (128... etc) bytes
> >> boundary. I do not have to "pad" them -- nor both, nor one side. And I
> >> suppose @Contended implementation to do exactly this -- "use special
> >> allocator for objects of that type, which allocate them on cache line
> >> boundary"
> >>
> >> Am I wrong here?
> >>
> >>
> >> >
> >> > -Doug
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/9455901c/attachment-0001.html>

From vitalyd at gmail.com  Tue Jan 17 11:38:55 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 11:38:55 -0500
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
In-Reply-To: <4F159B1A.7060809@javaspecialists.eu>
References: <4F158DC5.6030203@javaspecialists.eu>
	<CAHjP37HrB6AeRsiUrK-+=BH+kLOGDauw4o-n8dm6KjUm41GW0w@mail.gmail.com>
	<4F1590A3.4000000@javaspecialists.eu>
	<CAHjP37Hnq+0wpEWZQBhSpCWPkYdiXfYcTpYY9x_p7csaNkOu4A@mail.gmail.com>
	<4F15966B.7000207@javaspecialists.eu>
	<CAHjP37E-DgeNNDMrhR2Di=kbpYuSv4x8os4F0msLfR2V9dC3JA@mail.gmail.com>
	<4F159B1A.7060809@javaspecialists.eu>
Message-ID: <CAHjP37HY-BmSebH35x7z_8uSCD=1ZAct8a8keMp298gCyQGORA@mail.gmail.com>

Yes that's right but if that one thread is sufficient you get efficiency
benefits.  I'm not sure if you're saying you still disagree or not though :)

Joe brings up other good points as well, such as starting threads in ctor
and ability to configure the pool after construction (although most tweaks
can be done after threads are running).

Sent from my phone
On Jan 17, 2012 11:00 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
wrote:

> **
> Ah, but the Cached Thread Pool always has a core size of zero.  By default
> the threads expire after a minute of not being used.
>
> import java.util.concurrent.*;
>
> public class CachedThreadPoolTest {
>   public static void main(String[] args) throws ExecutionException,
> InterruptedException {
>     ThreadPoolExecutor pool = (ThreadPoolExecutor)
> Executors.newCachedThreadPool();
>
>     System.out.println("initial pool size: " + pool.getPoolSize());
>     System.out.println("core pool size: " + pool.getCorePoolSize());
>
>     for(int i=0; i<10; i++) {
>       Future<Void> future = pool.submit(new Callable<Void>() {
>         public Void call() throws Exception {
>           return null;
>         }
>       });
>       future.get();
>       System.out.println("Task " + i + " done");
>       System.out.println("pool size: " + pool.getPoolSize());
>     }
>
>     pool.shutdown();
>   }
> }
>
> Output:
>
> initial pool size: 0
> core pool size: 0
> Task 0 done
> pool size: 1
> Task 1 done
> pool size: 1
> Task 2 done
> pool size: 1
> Task 3 done
> pool size: 1
> Task 4 done
> pool size: 1
> Task 5 done
> pool size: 1
> Task 6 done
> pool size: 1
> Task 7 done
> pool size: 1
> Task 8 done
> pool size: 1
> Task 9 done
> pool size: 1
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
> Tel: +30 69 72 850 460
> Skype: kabutz
>
>
>
> On 1/17/12 5:51 PM, Vitaly Davidovich wrote:
>
> That's because you created a fixed size pool; as I mentioned about core
> size before, the pool will prefer to add threads in this case to come up to
> your core size.  Try with Executor.newCachedThreadPool() instead.
>
> Sent from my phone
> On Jan 17, 2012 10:40 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
> wrote:
>
>>  Here's a quick demo:
>>
>> import java.util.concurrent.*;
>>
>> public class ThreadPoolOnDemandConstruction {
>>   public static void main(String[] args) throws ExecutionException,
>> InterruptedException {
>>     ThreadPoolExecutor pool = (ThreadPoolExecutor)
>> Executors.newFixedThreadPool(5);
>>     System.out.println("initial pool size: " + pool.getPoolSize());
>>
>>     for(int i=0; i<10; i++) {
>>       Future<Void> future = pool.submit(new Callable<Void>() {
>>         public Void call() throws Exception {
>>           return null;
>>         }
>>       });
>>       future.get();
>>       System.out.println("Task " + i + " done");
>>       System.out.println("pool size: " + pool.getPoolSize());
>>     }
>>
>>     pool.shutdown();
>>   }
>> }
>>
>>
>> Output is:
>>
>> initial pool size: 0
>> Task 0 done
>> pool size: 1
>> Task 1 done
>> pool size: 2
>> Task 2 done
>> pool size: 3
>> Task 3 done
>> pool size: 4
>> Task 4 done
>> pool size: 5
>> Task 5 done
>> pool size: 5
>> Task 6 done
>> pool size: 5
>> Task 7 done
>> pool size: 5
>> Task 8 done
>> pool size: 5
>> Task 9 done
>> pool size: 5
>>
>> Regards
>>
>> Heinz
>> --
>> Dr Heinz M. Kabutz (PhD CompSci)
>> Author of "The Java(tm) Specialists' Newsletter"
>> Sun Java Champion
>> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
>> Tel: +30 69 72 850 460
>> Skype: kabutz
>>
>>
>>
>>
>> On 1/17/12 5:24 PM, Vitaly Davidovich wrote:
>>
>> I don't have source code in front of me right now but it shouldn't add
>> threads if there are spare ones unless min or core size was specified,
>> IIRC.  If one thread can keep up with task submission rate why create more?
>> For cases where you know you'll have a high rate that outpaces consumption
>> and don't want to take the hit of waiting for threads to spin up, you call
>> the pre start method. At least that's what makes sense to me :)
>>
>> Sent from my phone
>> On Jan 17, 2012 10:15 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
>> wrote:
>>
>>> I doubt it.  Threads are created as tasks are submitted, even if there
>>> are threads available.  But maybe you are right :-)
>>>
>>> Regards
>>>
>>> Heinz
>>> --
>>> Dr Heinz M. Kabutz (PhD CompSci)
>>> Author of "The Java(tm) Specialists' Newsletter"
>>> Sun Java Champion
>>> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
>>> Tel: +30 69 72 850 460
>>> Skype: kabutz
>>>
>>>
>>>
>>>
>>> On 1/17/12 5:11 PM, Vitaly Davidovich wrote:
>>>
>>> Perhaps because you don't know upfront how many threads will actually be
>>> needed to service the workload and thus don't want to spin them up eagerly
>>> (for perf and efficiency reasons) but rather adjust as work comes in? That
>>> is, what if just 1 thread is enough but pool is configured to allow for
>>> more ...
>>>
>>> Sent from my phone
>>> On Jan 17, 2012 10:06 AM, "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
>>> wrote:
>>>
>>>> A quick historical question.  What was the thinking behind constructing
>>>> the threads in the ThreadPoolExecutor lazily?
>>>>
>>>> In the JavaDocs of ThreadPoolExecutor:
>>>>
>>>> *On-demand construction*
>>>>     By default, even core threads are initially created and started
>>>> only when new tasks arrive, but this can be overridden dynamically using
>>>> method {@link ThreadPoolExecutor#prestartCoreThread} or {@link
>>>> ThreadPoolExecutor#prestartAllCoreThreads}. You probably want to prestart
>>>> threads if you construct the pool with a non-empty queue.
>>>>
>>>> I can think of two possible answers:
>>>>
>>>> Correctness: The author of this class did not want the
>>>> ThreadPoolExecutor instance to escape into the threads before it had
>>>> completed being constructed.  We should avoid letting "this" escape during
>>>> construction.
>>>>
>>>> Performance: It might be better to throttle the construction of new
>>>> threads.  Say someone creates a new fixed thread pool with 2000 threads,
>>>> instead of waiting for them all to be created, we let each task submission
>>>> create a thread until we reach the core size.
>>>>
>>>> Or perhaps it was neither of these two answers.  Maybe it was just
>>>> easier to write it this way, so that one class could be used for both the
>>>> cached and the fixed thread pool.
>>>>
>>>> Would love to hear the reasons for this :-)
>>>>
>>>> Regards
>>>>
>>>> Heinz
>>>> --
>>>> Dr Heinz M. Kabutz (PhD CompSci)
>>>> Author of "The Java(tm) Specialists' Newsletter"
>>>> Sun Java Champion
>>>> IEEE Certified Software Development Professionalhttp://www.javaspecialists.eu
>>>> Tel: +30 69 72 850 460
>>>> Skype: kabutz
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/b3931899/attachment.html>

From heinz at javaspecialists.eu  Tue Jan 17 11:43:32 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 17 Jan 2012 18:43:32 +0200
Subject: [concurrency-interest] On-demand Construction of Threads in
	ThreadPoolExecutor
In-Reply-To: <CAHjP37HY-BmSebH35x7z_8uSCD=1ZAct8a8keMp298gCyQGORA@mail.gmail.com>
References: <4F158DC5.6030203@javaspecialists.eu>	<CAHjP37HrB6AeRsiUrK-+=BH+kLOGDauw4o-n8dm6KjUm41GW0w@mail.gmail.com>	<4F1590A3.4000000@javaspecialists.eu>	<CAHjP37Hnq+0wpEWZQBhSpCWPkYdiXfYcTpYY9x_p7csaNkOu4A@mail.gmail.com>	<4F15966B.7000207@javaspecialists.eu>	<CAHjP37E-DgeNNDMrhR2Di=kbpYuSv4x8os4F0msLfR2V9dC3JA@mail.gmail.com>	<4F159B1A.7060809@javaspecialists.eu>
	<CAHjP37HY-BmSebH35x7z_8uSCD=1ZAct8a8keMp298gCyQGORA@mail.gmail.com>
Message-ID: <4F15A534.30709@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/0ccae086/attachment-0001.html>

From raphfrk at gmail.com  Tue Jan 17 12:12:22 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Tue, 17 Jan 2012 17:12:22 +0000
Subject: [concurrency-interest] Volatile happens before question
Message-ID: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>

I was wondering if there was a happens before guarantee for reads of
volatiles relative to later writes.  I think maybe not, but it isn't
clear.

For example, assuming there were 2 threads with the following code:

** Thread 1 **

int b = 0;
volatile boolean a = false;
...
...
a = true;
b = 1;

** Thread 2 **

int bStore = b;
if (!a) {
? System.out.println("The value of bStore is " + bStore);
}

Are these reasonable statements:

"bStore = b" happens-before "if(!a)"
"if(!a)" happens-before "a = true;" [Assuming that (!a) evaluates to true]
"a = true;" happens-before "b=1;"

Thus, the program will either print "The value of bStore is 0" or not
print anything.


From nathan.reynolds at oracle.com  Tue Jan 17 12:22:23 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 17 Jan 2012 10:22:23 -0700
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
Message-ID: <4F15AE4F.7010700@oracle.com>

It would be nice if the processor could effectively tell the JVM that 
false sharing is happening.  It would be nice if the JVM could respond 
by moving objects within the heap or fields with the class to avoid 
false sharing.  Thus, we don't have to pad or worry about placing 
@Contended or other attributes into the class.

Intel was looking into to optimizing for true and false sharing.  They 
had a prototype that worked but required restarting the JVM.  Oracle was 
looking into dynamically relayout fields in objects.  I haven't heard 
anything from either group for a while...  I haven't asked either.  *IF* 
a solution becomes available, then it will be a while.  This is a very 
difficult thing to do.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>
> OK I see what you mean now.  I imagine @Contended will be used with 
> fields rather than classes so when the JVM lays out an instance of the 
> class I assume it will do two-sided padding on the contended field if 
> required or if natural layout is such that prior fields already fill 
> up a cache line then only one sided is needed.
>
> Sent from my phone
>
> On Jan 17, 2012 11:27 AM, "Ruslan Cheremin" <cheremin at gmail.com 
> <mailto:cheremin at gmail.com>> wrote:
>
>     2012/1/17 Vitaly Davidovich <vitalyd at gmail.com
>     <mailto:vitalyd at gmail.com>>:
>     > I think it's semantics - if you sometimes allocate with 64/128 byte
>     > alignment then if your object is smaller than 64/128 the rest of
>     the space
>     > is effectively padding.
>
>     Agree. But in case of alignment you lose sense of "one-side" or "two
>     side" padding -- you do not need "two side padding", you just make
>     sure object align on cache line boundary.
>
>     Actually I was asked is my understanding of how @Contended supposed to
>     be implemented is right?
>
>     >Or are you saying you want an @Alignment annotation
>     > instead so it's more general? What other uses of custom
>     alignment do you
>     > envision? Java is too high-level  and the underlying
>     hardware/platform too
>     > abstracted away for a general purpose custom alignment hint, IMHO.
>
>     No, I do not want such ugly thing to happen with java! It's enough C
>     for such things...
>
>
>     > Sent from my phone
>     >
>     > On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com
>     <mailto:cheremin at gmail.com>> wrote:
>     >>
>     >> > Yes. As a practical matter though, until an @Contended attribute
>     >> > or something like it is supported across JVMS (see list
>     archives for
>     >> > discussion), you cannot arrange reliable two-sided padding
>     >> > for objects with mixed field types (ints, longs, refs that may be
>     >> > either 32 or 64 bits, etc), so one-sided is the best you can do.
>     >>
>     >> By the way -- I was not thinking about @Contended as "make
>     padding for
>     >> me". It seems for me like padding is only dirty hack, since nothing
>     >> better available. If I would control memory allocation (like
>     JVM does)
>     >> I just can allocate @Contended objects on 64 (128... etc) bytes
>     >> boundary. I do not have to "pad" them -- nor both, nor one
>     side. And I
>     >> suppose @Contended implementation to do exactly this -- "use
>     special
>     >> allocator for objects of that type, which allocate them on
>     cache line
>     >> boundary"
>     >>
>     >> Am I wrong here?
>     >>
>     >>
>     >> >
>     >> > -Doug
>     >> > _______________________________________________
>     >> > Concurrency-interest mailing list
>     >> > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     >> _______________________________________________
>     >> Concurrency-interest mailing list
>     >> Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/4c209fca/attachment.html>

From cheremin at gmail.com  Tue Jan 17 12:29:04 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 17 Jan 2012 21:29:04 +0400
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F15AE4F.7010700@oracle.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
Message-ID: <CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>

You made my day. Few months ago I was dreaming (in my blog) about
complexity of false sharing prevention with padding. And come to two
options, one better another. First one was @PreventFalseSharing
annotation, next was atomatically contention detection and relocation
of contended objects by JIT. Readers of my blog soon pointed me to
@Contended annotation. And now you telling the second -- the best --
option is also being explored!

Just want to ask -- if all good things will be done by JIT -- what I
will be paid for?

2012/1/17 Nathan Reynolds <nathan.reynolds at oracle.com>:
> It would be nice if the processor could effectively tell the JVM that false
> sharing is happening.? It would be nice if the JVM could respond by moving
> objects within the heap or fields with the class to avoid false sharing.
> Thus, we don't have to pad or worry about placing @Contended or other
> attributes into the class.
>
> Intel was looking into to optimizing for true and false sharing.? They had a
> prototype that worked but required restarting the JVM.? Oracle was looking
> into dynamically relayout fields in objects.? I haven't heard anything from
> either group for a while...? I haven't asked either.? *IF* a solution
> becomes available, then it will be a while.? This is a very difficult thing
> to do.
>
> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
> Oracle PSR Engineering | Server Technology
>
> On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>
> OK I see what you mean now.? I imagine @Contended will be used with fields
> rather than classes so when the JVM lays out an instance of the class I
> assume it will do two-sided padding on the contended field if required or if
> natural layout is such that prior fields already fill up a cache line then
> only one sided is needed.
>
> Sent from my phone
>
> On Jan 17, 2012 11:27 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>>
>> 2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
>> > I think it's semantics - if you sometimes allocate with 64/128 byte
>> > alignment then if your object is smaller than 64/128 the rest of the
>> > space
>> > is effectively padding.
>>
>> Agree. But in case of alignment you lose sense of "one-side" or "two
>> side" padding -- you do not need "two side padding", you just make
>> sure object align on cache line boundary.
>>
>> Actually I was asked is my understanding of how @Contended supposed to
>> be implemented is right?
>>
>> >Or are you saying you want an @Alignment annotation
>> > instead so it's more general? What other uses of custom alignment do you
>> > envision? Java is too high-level? and the underlying hardware/platform
>> > too
>> > abstracted away for a general purpose custom alignment hint, IMHO.
>>
>> No, I do not want such ugly thing to happen with java! It's enough C
>> for such things...
>>
>>
>> > Sent from my phone
>> >
>> > On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>> >>
>> >> > Yes. As a practical matter though, until an @Contended attribute
>> >> > or something like it is supported across JVMS (see list archives for
>> >> > discussion), you cannot arrange reliable two-sided padding
>> >> > for objects with mixed field types (ints, longs, refs that may be
>> >> > either 32 or 64 bits, etc), so one-sided is the best you can do.
>> >>
>> >> By the way -- I was not thinking about @Contended as "make padding for
>> >> me". It seems for me like padding is only dirty hack, since nothing
>> >> better available. If I would control memory allocation (like JVM does)
>> >> I just can allocate @Contended objects on 64 (128... etc) bytes
>> >> boundary. I do not have to "pad" them -- nor both, nor one side. And I
>> >> suppose @Contended implementation to do exactly this -- "use special
>> >> allocator for objects of that type, which allocate them on cache line
>> >> boundary"
>> >>
>> >> Am I wrong here?
>> >>
>> >>
>> >> >
>> >> > -Doug
>> >> > _______________________________________________
>> >> > Concurrency-interest mailing list
>> >> > Concurrency-interest at cs.oswego.edu
>> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathan.reynolds at oracle.com  Tue Jan 17 12:44:39 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 17 Jan 2012 10:44:39 -0700
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
Message-ID: <4F15B387.1090706@oracle.com>

Assuming that the JVM can optimize for true and false sharing (and that 
is a big assumption at the moment), then you can focus your time on 
writing useful code.  Furthermore, optimizing for true and false sharing 
will never be able to fix actual data contention.  We still need clever 
ways of sharing data without bottlenecks.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
> You made my day. Few months ago I was dreaming (in my blog) about
> complexity of false sharing prevention with padding. And come to two
> options, one better another. First one was @PreventFalseSharing
> annotation, next was atomatically contention detection and relocation
> of contended objects by JIT. Readers of my blog soon pointed me to
> @Contended annotation. And now you telling the second -- the best --
> option is also being explored!
>
> Just want to ask -- if all good things will be done by JIT -- what I
> will be paid for?
>
> 2012/1/17 Nathan Reynolds<nathan.reynolds at oracle.com>:
>> It would be nice if the processor could effectively tell the JVM that false
>> sharing is happening.  It would be nice if the JVM could respond by moving
>> objects within the heap or fields with the class to avoid false sharing.
>> Thus, we don't have to pad or worry about placing @Contended or other
>> attributes into the class.
>>
>> Intel was looking into to optimizing for true and false sharing.  They had a
>> prototype that worked but required restarting the JVM.  Oracle was looking
>> into dynamically relayout fields in objects.  I haven't heard anything from
>> either group for a while...  I haven't asked either.  *IF* a solution
>> becomes available, then it will be a while.  This is a very difficult thing
>> to do.
>>
>> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
>> Oracle PSR Engineering | Server Technology
>>
>> On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>>
>> OK I see what you mean now.  I imagine @Contended will be used with fields
>> rather than classes so when the JVM lays out an instance of the class I
>> assume it will do two-sided padding on the contended field if required or if
>> natural layout is such that prior fields already fill up a cache line then
>> only one sided is needed.
>>
>> Sent from my phone
>>
>> On Jan 17, 2012 11:27 AM, "Ruslan Cheremin"<cheremin at gmail.com>  wrote:
>>> 2012/1/17 Vitaly Davidovich<vitalyd at gmail.com>:
>>>> I think it's semantics - if you sometimes allocate with 64/128 byte
>>>> alignment then if your object is smaller than 64/128 the rest of the
>>>> space
>>>> is effectively padding.
>>> Agree. But in case of alignment you lose sense of "one-side" or "two
>>> side" padding -- you do not need "two side padding", you just make
>>> sure object align on cache line boundary.
>>>
>>> Actually I was asked is my understanding of how @Contended supposed to
>>> be implemented is right?
>>>
>>>> Or are you saying you want an @Alignment annotation
>>>> instead so it's more general? What other uses of custom alignment do you
>>>> envision? Java is too high-level  and the underlying hardware/platform
>>>> too
>>>> abstracted away for a general purpose custom alignment hint, IMHO.
>>> No, I do not want such ugly thing to happen with java! It's enough C
>>> for such things...
>>>
>>>
>>>> Sent from my phone
>>>>
>>>> On Jan 17, 2012 10:56 AM, "Ruslan Cheremin"<cheremin at gmail.com>  wrote:
>>>>>> Yes. As a practical matter though, until an @Contended attribute
>>>>>> or something like it is supported across JVMS (see list archives for
>>>>>> discussion), you cannot arrange reliable two-sided padding
>>>>>> for objects with mixed field types (ints, longs, refs that may be
>>>>>> either 32 or 64 bits, etc), so one-sided is the best you can do.
>>>>> By the way -- I was not thinking about @Contended as "make padding for
>>>>> me". It seems for me like padding is only dirty hack, since nothing
>>>>> better available. If I would control memory allocation (like JVM does)
>>>>> I just can allocate @Contended objects on 64 (128... etc) bytes
>>>>> boundary. I do not have to "pad" them -- nor both, nor one side. And I
>>>>> suppose @Contended implementation to do exactly this -- "use special
>>>>> allocator for objects of that type, which allocate them on cache line
>>>>> boundary"
>>>>>
>>>>> Am I wrong here?
>>>>>
>>>>>
>>>>>> -Doug
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/2b604821/attachment-0001.html>

From vitalyd at gmail.com  Tue Jan 17 12:56:52 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 12:56:52 -0500
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>
References: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>
Message-ID: <CAHjP37FwVqm0Ad1qq5j8FTP-qqtRhEFs7edDKuxhS6gUZKZs=w@mail.gmail.com>

b=1 is allowed to move before a=true so you can read a=false and see b as 1.

Sent from my phone
On Jan 17, 2012 12:14 PM, "Raph Frank" <raphfrk at gmail.com> wrote:

> I was wondering if there was a happens before guarantee for reads of
> volatiles relative to later writes.  I think maybe not, but it isn't
> clear.
>
> For example, assuming there were 2 threads with the following code:
>
> ** Thread 1 **
>
> int b = 0;
> volatile boolean a = false;
> ...
> ...
> a = true;
> b = 1;
>
> ** Thread 2 **
>
> int bStore = b;
> if (!a) {
>   System.out.println("The value of bStore is " + bStore);
> }
>
> Are these reasonable statements:
>
> "bStore = b" happens-before "if(!a)"
> "if(!a)" happens-before "a = true;" [Assuming that (!a) evaluates to true]
> "a = true;" happens-before "b=1;"
>
> Thus, the program will either print "The value of bStore is 0" or not
> print anything.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/9ffeb437/attachment.html>

From vitalyd at gmail.com  Tue Jan 17 13:03:29 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 13:03:29 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F15B387.1090706@oracle.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
Message-ID: <CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>

Doing dynamic re-layout and contention adjustment seems nice in thought but
how practical is that? I can't see how that would be cheap enough where
it's worth the cost. What if the app goes through phases of contention?
Initially high but then no sharing - would the bloated object layout be
worth it at that point? Seems like this is a place where explicit developer
instructions is better than heuristics with potentially expensive
consequences.

Sent from my phone
On Jan 17, 2012 12:44 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  Assuming that the JVM can optimize for true and false sharing (and that
> is a big assumption at the moment), then you can focus your time on writing
> useful code.  Furthermore, optimizing for true and false sharing will never
> be able to fix actual data contention.  We still need clever ways of
> sharing data without bottlenecks.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
>
> You made my day. Few months ago I was dreaming (in my blog) about
> complexity of false sharing prevention with padding. And come to two
> options, one better another. First one was @PreventFalseSharing
> annotation, next was atomatically contention detection and relocation
> of contended objects by JIT. Readers of my blog soon pointed me to
> @Contended annotation. And now you telling the second -- the best --
> option is also being explored!
>
> Just want to ask -- if all good things will be done by JIT -- what I
> will be paid for?
>
> 2012/1/17 Nathan Reynolds <nathan.reynolds at oracle.com> <nathan.reynolds at oracle.com>:
>
>  It would be nice if the processor could effectively tell the JVM that false
> sharing is happening.  It would be nice if the JVM could respond by moving
> objects within the heap or fields with the class to avoid false sharing.
> Thus, we don't have to pad or worry about placing @Contended or other
> attributes into the class.
>
> Intel was looking into to optimizing for true and false sharing.  They had a
> prototype that worked but required restarting the JVM.  Oracle was looking
> into dynamically relayout fields in objects.  I haven't heard anything from
> either group for a while...  I haven't asked either.  *IF* a solution
> becomes available, then it will be a while.  This is a very difficult thing
> to do.
>
> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
> Oracle PSR Engineering | Server Technology
>
> On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>
> OK I see what you mean now.  I imagine @Contended will be used with fields
> rather than classes so when the JVM lays out an instance of the class I
> assume it will do two-sided padding on the contended field if required or if
> natural layout is such that prior fields already fill up a cache line then
> only one sided is needed.
>
> Sent from my phone
>
> On Jan 17, 2012 11:27 AM, "Ruslan Cheremin" <cheremin at gmail.com> <cheremin at gmail.com> wrote:
>
>
> 2012/1/17 Vitaly Davidovich <vitalyd at gmail.com> <vitalyd at gmail.com>:
>
>  I think it's semantics - if you sometimes allocate with 64/128 byte
> alignment then if your object is smaller than 64/128 the rest of the
> space
> is effectively padding.
>
>
> Agree. But in case of alignment you lose sense of "one-side" or "two
> side" padding -- you do not need "two side padding", you just make
> sure object align on cache line boundary.
>
> Actually I was asked is my understanding of how @Contended supposed to
> be implemented is right?
>
>
>  Or are you saying you want an @Alignment annotation
> instead so it's more general? What other uses of custom alignment do you
> envision? Java is too high-level  and the underlying hardware/platform
> too
> abstracted away for a general purpose custom alignment hint, IMHO.
>
>
> No, I do not want such ugly thing to happen with java! It's enough C
> for such things...
>
>
>
>  Sent from my phone
>
> On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com> <cheremin at gmail.com> wrote:
>
>   Yes. As a practical matter though, until an @Contended attribute
> or something like it is supported across JVMS (see list archives for
> discussion), you cannot arrange reliable two-sided padding
> for objects with mixed field types (ints, longs, refs that may be
> either 32 or 64 bits, etc), so one-sided is the best you can do.
>
>
> By the way -- I was not thinking about @Contended as "make padding for
> me". It seems for me like padding is only dirty hack, since nothing
> better available. If I would control memory allocation (like JVM does)
> I just can allocate @Contended objects on 64 (128... etc) bytes
> boundary. I do not have to "pad" them -- nor both, nor one side. And I
> suppose @Contended implementation to do exactly this -- "use special
> allocator for objects of that type, which allocate them on cache line
> boundary"
>
> Am I wrong here?
>
>
>
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/35286d86/attachment.html>

From cheremin at gmail.com  Tue Jan 17 13:06:07 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 17 Jan 2012 22:06:07 +0400
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
	<CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
Message-ID: <CAOwENi+erPy9vy4AvN9MQNjMzBftNspqLFCDRwbHuK0tOrqwmQ@mail.gmail.com>

I'm sure the same things was said by people against GC few decades ago :)

2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
> Doing dynamic re-layout and contention adjustment seems nice in thought but
> how practical is that? I can't see how that would be cheap enough where it's
> worth the cost. What if the app goes through phases of contention? Initially
> high but then no sharing - would the bloated object layout be worth it at
> that point? Seems like this is a place where explicit developer instructions
> is better than heuristics with potentially expensive consequences.
>
> Sent from my phone
>
> On Jan 17, 2012 12:44 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>>
>> Assuming that the JVM can optimize for true and false sharing (and that is
>> a big assumption at the moment), then you can focus your time on writing
>> useful code.? Furthermore, optimizing for true and false sharing will never
>> be able to fix actual data contention.? We still need clever ways of sharing
>> data without bottlenecks.
>>
>> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
>> Oracle PSR Engineering | Server Technology
>>
>> On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
>>
>> You made my day. Few months ago I was dreaming (in my blog) about
>> complexity of false sharing prevention with padding. And come to two
>> options, one better another. First one was @PreventFalseSharing
>> annotation, next was atomatically contention detection and relocation
>> of contended objects by JIT. Readers of my blog soon pointed me to
>> @Contended annotation. And now you telling the second -- the best --
>> option is also being explored!
>>
>> Just want to ask -- if all good things will be done by JIT -- what I
>> will be paid for?
>>
>> 2012/1/17 Nathan Reynolds <nathan.reynolds at oracle.com>:
>>
>> It would be nice if the processor could effectively tell the JVM that
>> false
>> sharing is happening.? It would be nice if the JVM could respond by moving
>> objects within the heap or fields with the class to avoid false sharing.
>> Thus, we don't have to pad or worry about placing @Contended or other
>> attributes into the class.
>>
>> Intel was looking into to optimizing for true and false sharing.? They had
>> a
>> prototype that worked but required restarting the JVM.? Oracle was looking
>> into dynamically relayout fields in objects.? I haven't heard anything
>> from
>> either group for a while...? I haven't asked either.? *IF* a solution
>> becomes available, then it will be a while.? This is a very difficult
>> thing
>> to do.
>>
>> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
>> Oracle PSR Engineering | Server Technology
>>
>> On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>>
>> OK I see what you mean now.? I imagine @Contended will be used with fields
>> rather than classes so when the JVM lays out an instance of the class I
>> assume it will do two-sided padding on the contended field if required or
>> if
>> natural layout is such that prior fields already fill up a cache line then
>> only one sided is needed.
>>
>> Sent from my phone
>>
>> On Jan 17, 2012 11:27 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>>
>> 2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
>>
>> I think it's semantics - if you sometimes allocate with 64/128 byte
>> alignment then if your object is smaller than 64/128 the rest of the
>> space
>> is effectively padding.
>>
>> Agree. But in case of alignment you lose sense of "one-side" or "two
>> side" padding -- you do not need "two side padding", you just make
>> sure object align on cache line boundary.
>>
>> Actually I was asked is my understanding of how @Contended supposed to
>> be implemented is right?
>>
>> Or are you saying you want an @Alignment annotation
>> instead so it's more general? What other uses of custom alignment do you
>> envision? Java is too high-level? and the underlying hardware/platform
>> too
>> abstracted away for a general purpose custom alignment hint, IMHO.
>>
>> No, I do not want such ugly thing to happen with java! It's enough C
>> for such things...
>>
>>
>> Sent from my phone
>>
>> On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>>
>> Yes. As a practical matter though, until an @Contended attribute
>> or something like it is supported across JVMS (see list archives for
>> discussion), you cannot arrange reliable two-sided padding
>> for objects with mixed field types (ints, longs, refs that may be
>> either 32 or 64 bits, etc), so one-sided is the best you can do.
>>
>> By the way -- I was not thinking about @Contended as "make padding for
>> me". It seems for me like padding is only dirty hack, since nothing
>> better available. If I would control memory allocation (like JVM does)
>> I just can allocate @Contended objects on 64 (128... etc) bytes
>> boundary. I do not have to "pad" them -- nor both, nor one side. And I
>> suppose @Contended implementation to do exactly this -- "use special
>> allocator for objects of that type, which allocate them on cache line
>> boundary"
>>
>> Am I wrong here?
>>
>>
>> -Doug
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Tue Jan 17 13:12:47 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 13:12:47 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAOwENi+erPy9vy4AvN9MQNjMzBftNspqLFCDRwbHuK0tOrqwmQ@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
	<CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
	<CAOwENi+erPy9vy4AvN9MQNjMzBftNspqLFCDRwbHuK0tOrqwmQ@mail.gmail.com>
Message-ID: <CAHjP37HUxtUf6Axq6EK-=TCoQgfp_dEMGb8No9bYS1KMKvvQeg@mail.gmail.com>

I'd love to be proven wrong though :)

Sent from my phone
On Jan 17, 2012 1:06 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

> I'm sure the same things was said by people against GC few decades ago :)
>
> 2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
> > Doing dynamic re-layout and contention adjustment seems nice in thought
> but
> > how practical is that? I can't see how that would be cheap enough where
> it's
> > worth the cost. What if the app goes through phases of contention?
> Initially
> > high but then no sharing - would the bloated object layout be worth it at
> > that point? Seems like this is a place where explicit developer
> instructions
> > is better than heuristics with potentially expensive consequences.
> >
> > Sent from my phone
> >
> > On Jan 17, 2012 12:44 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> > wrote:
> >>
> >> Assuming that the JVM can optimize for true and false sharing (and that
> is
> >> a big assumption at the moment), then you can focus your time on writing
> >> useful code.  Furthermore, optimizing for true and false sharing will
> never
> >> be able to fix actual data contention.  We still need clever ways of
> sharing
> >> data without bottlenecks.
> >>
> >> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
> >> Oracle PSR Engineering | Server Technology
> >>
> >> On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
> >>
> >> You made my day. Few months ago I was dreaming (in my blog) about
> >> complexity of false sharing prevention with padding. And come to two
> >> options, one better another. First one was @PreventFalseSharing
> >> annotation, next was atomatically contention detection and relocation
> >> of contended objects by JIT. Readers of my blog soon pointed me to
> >> @Contended annotation. And now you telling the second -- the best --
> >> option is also being explored!
> >>
> >> Just want to ask -- if all good things will be done by JIT -- what I
> >> will be paid for?
> >>
> >> 2012/1/17 Nathan Reynolds <nathan.reynolds at oracle.com>:
> >>
> >> It would be nice if the processor could effectively tell the JVM that
> >> false
> >> sharing is happening.  It would be nice if the JVM could respond by
> moving
> >> objects within the heap or fields with the class to avoid false sharing.
> >> Thus, we don't have to pad or worry about placing @Contended or other
> >> attributes into the class.
> >>
> >> Intel was looking into to optimizing for true and false sharing.  They
> had
> >> a
> >> prototype that worked but required restarting the JVM.  Oracle was
> looking
> >> into dynamically relayout fields in objects.  I haven't heard anything
> >> from
> >> either group for a while...  I haven't asked either.  *IF* a solution
> >> becomes available, then it will be a while.  This is a very difficult
> >> thing
> >> to do.
> >>
> >> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
> >> Oracle PSR Engineering | Server Technology
> >>
> >> On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
> >>
> >> OK I see what you mean now.  I imagine @Contended will be used with
> fields
> >> rather than classes so when the JVM lays out an instance of the class I
> >> assume it will do two-sided padding on the contended field if required
> or
> >> if
> >> natural layout is such that prior fields already fill up a cache line
> then
> >> only one sided is needed.
> >>
> >> Sent from my phone
> >>
> >> On Jan 17, 2012 11:27 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
> >>
> >> 2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
> >>
> >> I think it's semantics - if you sometimes allocate with 64/128 byte
> >> alignment then if your object is smaller than 64/128 the rest of the
> >> space
> >> is effectively padding.
> >>
> >> Agree. But in case of alignment you lose sense of "one-side" or "two
> >> side" padding -- you do not need "two side padding", you just make
> >> sure object align on cache line boundary.
> >>
> >> Actually I was asked is my understanding of how @Contended supposed to
> >> be implemented is right?
> >>
> >> Or are you saying you want an @Alignment annotation
> >> instead so it's more general? What other uses of custom alignment do you
> >> envision? Java is too high-level  and the underlying hardware/platform
> >> too
> >> abstracted away for a general purpose custom alignment hint, IMHO.
> >>
> >> No, I do not want such ugly thing to happen with java! It's enough C
> >> for such things...
> >>
> >>
> >> Sent from my phone
> >>
> >> On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
> >>
> >> Yes. As a practical matter though, until an @Contended attribute
> >> or something like it is supported across JVMS (see list archives for
> >> discussion), you cannot arrange reliable two-sided padding
> >> for objects with mixed field types (ints, longs, refs that may be
> >> either 32 or 64 bits, etc), so one-sided is the best you can do.
> >>
> >> By the way -- I was not thinking about @Contended as "make padding for
> >> me". It seems for me like padding is only dirty hack, since nothing
> >> better available. If I would control memory allocation (like JVM does)
> >> I just can allocate @Contended objects on 64 (128... etc) bytes
> >> boundary. I do not have to "pad" them -- nor both, nor one side. And I
> >> suppose @Contended implementation to do exactly this -- "use special
> >> allocator for objects of that type, which allocate them on cache line
> >> boundary"
> >>
> >> Am I wrong here?
> >>
> >>
> >> -Doug
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/67b2fcd2/attachment-0001.html>

From cheremin at gmail.com  Tue Jan 17 13:18:47 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 17 Jan 2012 22:18:47 +0400
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAHjP37HUxtUf6Axq6EK-=TCoQgfp_dEMGb8No9bYS1KMKvvQeg@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
	<CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
	<CAOwENi+erPy9vy4AvN9MQNjMzBftNspqLFCDRwbHuK0tOrqwmQ@mail.gmail.com>
	<CAHjP37HUxtUf6Axq6EK-=TCoQgfp_dEMGb8No9bYS1KMKvvQeg@mail.gmail.com>
Message-ID: <CAOwENiKKN9S7E3-jkZf0PNnv7x=qhKAVss06sOSZh+J0TEaHgA@mail.gmail.com>

Just wait "a few decades" :)

2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
> I'd love to be proven wrong though :)
>
> Sent from my phone
>
> On Jan 17, 2012 1:06 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>>
>> I'm sure the same things was said by people against GC few decades ago :)
>>
>> 2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
>> > Doing dynamic re-layout and contention adjustment seems nice in thought
>> > but
>> > how practical is that? I can't see how that would be cheap enough where
>> > it's
>> > worth the cost. What if the app goes through phases of contention?
>> > Initially
>> > high but then no sharing - would the bloated object layout be worth it
>> > at
>> > that point? Seems like this is a place where explicit developer
>> > instructions
>> > is better than heuristics with potentially expensive consequences.
>> >
>> > Sent from my phone
>> >
>> > On Jan 17, 2012 12:44 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>> > wrote:
>> >>
>> >> Assuming that the JVM can optimize for true and false sharing (and that
>> >> is
>> >> a big assumption at the moment), then you can focus your time on
>> >> writing
>> >> useful code.? Furthermore, optimizing for true and false sharing will
>> >> never
>> >> be able to fix actual data contention.? We still need clever ways of
>> >> sharing
>> >> data without bottlenecks.
>> >>
>> >> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
>> >> Oracle PSR Engineering | Server Technology
>> >>
>> >> On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
>> >>
>> >> You made my day. Few months ago I was dreaming (in my blog) about
>> >> complexity of false sharing prevention with padding. And come to two
>> >> options, one better another. First one was @PreventFalseSharing
>> >> annotation, next was atomatically contention detection and relocation
>> >> of contended objects by JIT. Readers of my blog soon pointed me to
>> >> @Contended annotation. And now you telling the second -- the best --
>> >> option is also being explored!
>> >>
>> >> Just want to ask -- if all good things will be done by JIT -- what I
>> >> will be paid for?
>> >>
>> >> 2012/1/17 Nathan Reynolds <nathan.reynolds at oracle.com>:
>> >>
>> >> It would be nice if the processor could effectively tell the JVM that
>> >> false
>> >> sharing is happening.? It would be nice if the JVM could respond by
>> >> moving
>> >> objects within the heap or fields with the class to avoid false
>> >> sharing.
>> >> Thus, we don't have to pad or worry about placing @Contended or other
>> >> attributes into the class.
>> >>
>> >> Intel was looking into to optimizing for true and false sharing.? They
>> >> had
>> >> a
>> >> prototype that worked but required restarting the JVM.? Oracle was
>> >> looking
>> >> into dynamically relayout fields in objects.? I haven't heard anything
>> >> from
>> >> either group for a while...? I haven't asked either.? *IF* a solution
>> >> becomes available, then it will be a while.? This is a very difficult
>> >> thing
>> >> to do.
>> >>
>> >> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
>> >> Oracle PSR Engineering | Server Technology
>> >>
>> >> On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>> >>
>> >> OK I see what you mean now.? I imagine @Contended will be used with
>> >> fields
>> >> rather than classes so when the JVM lays out an instance of the class I
>> >> assume it will do two-sided padding on the contended field if required
>> >> or
>> >> if
>> >> natural layout is such that prior fields already fill up a cache line
>> >> then
>> >> only one sided is needed.
>> >>
>> >> Sent from my phone
>> >>
>> >> On Jan 17, 2012 11:27 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>> >>
>> >> 2012/1/17 Vitaly Davidovich <vitalyd at gmail.com>:
>> >>
>> >> I think it's semantics - if you sometimes allocate with 64/128 byte
>> >> alignment then if your object is smaller than 64/128 the rest of the
>> >> space
>> >> is effectively padding.
>> >>
>> >> Agree. But in case of alignment you lose sense of "one-side" or "two
>> >> side" padding -- you do not need "two side padding", you just make
>> >> sure object align on cache line boundary.
>> >>
>> >> Actually I was asked is my understanding of how @Contended supposed to
>> >> be implemented is right?
>> >>
>> >> Or are you saying you want an @Alignment annotation
>> >> instead so it's more general? What other uses of custom alignment do
>> >> you
>> >> envision? Java is too high-level? and the underlying hardware/platform
>> >> too
>> >> abstracted away for a general purpose custom alignment hint, IMHO.
>> >>
>> >> No, I do not want such ugly thing to happen with java! It's enough C
>> >> for such things...
>> >>
>> >>
>> >> Sent from my phone
>> >>
>> >> On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>> >>
>> >> Yes. As a practical matter though, until an @Contended attribute
>> >> or something like it is supported across JVMS (see list archives for
>> >> discussion), you cannot arrange reliable two-sided padding
>> >> for objects with mixed field types (ints, longs, refs that may be
>> >> either 32 or 64 bits, etc), so one-sided is the best you can do.
>> >>
>> >> By the way -- I was not thinking about @Contended as "make padding for
>> >> me". It seems for me like padding is only dirty hack, since nothing
>> >> better available. If I would control memory allocation (like JVM does)
>> >> I just can allocate @Contended objects on 64 (128... etc) bytes
>> >> boundary. I do not have to "pad" them -- nor both, nor one side. And I
>> >> suppose @Contended implementation to do exactly this -- "use special
>> >> allocator for objects of that type, which allocate them on cache line
>> >> boundary"
>> >>
>> >> Am I wrong here?
>> >>
>> >>
>> >> -Doug
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathan.reynolds at oracle.com  Tue Jan 17 13:33:44 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 17 Jan 2012 11:33:44 -0700
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
	<CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
Message-ID: <4F15BF08.5060909@oracle.com>

If an application is contending on a cache line, then the processors 
will look 100% busy but very little work will get done.  Fixing the 
contention will almost always get huge gains.

Let's say the contention is due to 2 objects being next to each other.  
The solution is to separate them by moving 1 object to another place in 
the heap.  Moving an object in the heap is fairly cheap.  GC does it a lot!

Let's say the contention is due to 2 fields being next to each other.  
The solution is to separate them by rearranging the fields.  This is 
probably expensive.  Once that is done, the contention will go away but 
the memory will increase.  Very few locks actually contend.  Even fewer 
fields contend on cache lines.  So, the memory impact should be fairly 
small and probably insignificant.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/17/2012 11:03 AM, Vitaly Davidovich wrote:
>
> Doing dynamic re-layout and contention adjustment seems nice in 
> thought but how practical is that? I can't see how that would be cheap 
> enough where it's worth the cost. What if the app goes through phases 
> of contention? Initially high but then no sharing - would the bloated 
> object layout be worth it at that point? Seems like this is a place 
> where explicit developer instructions is better than heuristics with 
> potentially expensive consequences.
>
> Sent from my phone
>
> On Jan 17, 2012 12:44 PM, "Nathan Reynolds" 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     Assuming that the JVM can optimize for true and false sharing (and
>     that is a big assumption at the moment), then you can focus your
>     time on writing useful code.  Furthermore, optimizing for true and
>     false sharing will never be able to fix actual data contention. 
>     We still need clever ways of sharing data without bottlenecks.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Consulting Member of Technical Staff | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
>     On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
>>     You made my day. Few months ago I was dreaming (in my blog) about
>>     complexity of false sharing prevention with padding. And come to two
>>     options, one better another. First one was @PreventFalseSharing
>>     annotation, next was atomatically contention detection and relocation
>>     of contended objects by JIT. Readers of my blog soon pointed me to
>>     @Contended annotation. And now you telling the second -- the best --
>>     option is also being explored!
>>
>>     Just want to ask -- if all good things will be done by JIT -- what I
>>     will be paid for?
>>
>>     2012/1/17 Nathan Reynolds<nathan.reynolds at oracle.com>  <mailto:nathan.reynolds at oracle.com>:
>>>     It would be nice if the processor could effectively tell the JVM that false
>>>     sharing is happening.  It would be nice if the JVM could respond by moving
>>>     objects within the heap or fields with the class to avoid false sharing.
>>>     Thus, we don't have to pad or worry about placing @Contended or other
>>>     attributes into the class.
>>>
>>>     Intel was looking into to optimizing for true and false sharing.  They had a
>>>     prototype that worked but required restarting the JVM.  Oracle was looking
>>>     into dynamically relayout fields in objects.  I haven't heard anything from
>>>     either group for a while...  I haven't asked either.  *IF* a solution
>>>     becomes available, then it will be a while.  This is a very difficult thing
>>>     to do.
>>>
>>>     Nathan Reynolds | Consulting Member of Technical Staff |602.333.9091  <tel:602.333.9091>
>>>     Oracle PSR Engineering | Server Technology
>>>
>>>     On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>>>
>>>     OK I see what you mean now.  I imagine @Contended will be used with fields
>>>     rather than classes so when the JVM lays out an instance of the class I
>>>     assume it will do two-sided padding on the contended field if required or if
>>>     natural layout is such that prior fields already fill up a cache line then
>>>     only one sided is needed.
>>>
>>>     Sent from my phone
>>>
>>>     On Jan 17, 2012 11:27 AM, "Ruslan Cheremin"<cheremin at gmail.com>  <mailto:cheremin at gmail.com>  wrote:
>>>>     2012/1/17 Vitaly Davidovich<vitalyd at gmail.com>  <mailto:vitalyd at gmail.com>:
>>>>>     I think it's semantics - if you sometimes allocate with 64/128 byte
>>>>>     alignment then if your object is smaller than 64/128 the rest of the
>>>>>     space
>>>>>     is effectively padding.
>>>>     Agree. But in case of alignment you lose sense of "one-side" or "two
>>>>     side" padding -- you do not need "two side padding", you just make
>>>>     sure object align on cache line boundary.
>>>>
>>>>     Actually I was asked is my understanding of how @Contended supposed to
>>>>     be implemented is right?
>>>>
>>>>>     Or are you saying you want an @Alignment annotation
>>>>>     instead so it's more general? What other uses of custom alignment do you
>>>>>     envision? Java is too high-level  and the underlying hardware/platform
>>>>>     too
>>>>>     abstracted away for a general purpose custom alignment hint, IMHO.
>>>>     No, I do not want such ugly thing to happen with java! It's enough C
>>>>     for such things...
>>>>
>>>>
>>>>>     Sent from my phone
>>>>>
>>>>>     On Jan 17, 2012 10:56 AM, "Ruslan Cheremin"<cheremin at gmail.com>  <mailto:cheremin at gmail.com>  wrote:
>>>>>>>     Yes. As a practical matter though, until an @Contended attribute
>>>>>>>     or something like it is supported across JVMS (see list archives for
>>>>>>>     discussion), you cannot arrange reliable two-sided padding
>>>>>>>     for objects with mixed field types (ints, longs, refs that may be
>>>>>>>     either 32 or 64 bits, etc), so one-sided is the best you can do.
>>>>>>     By the way -- I was not thinking about @Contended as "make padding for
>>>>>>     me". It seems for me like padding is only dirty hack, since nothing
>>>>>>     better available. If I would control memory allocation (like JVM does)
>>>>>>     I just can allocate @Contended objects on 64 (128... etc) bytes
>>>>>>     boundary. I do not have to "pad" them -- nor both, nor one side. And I
>>>>>>     suppose @Contended implementation to do exactly this -- "use special
>>>>>>     allocator for objects of that type, which allocate them on cache line
>>>>>>     boundary"
>>>>>>
>>>>>>     Am I wrong here?
>>>>>>
>>>>>>
>>>>>>>     -Doug
>>>>>>>     _______________________________________________
>>>>>>>     Concurrency-interest mailing list
>>>>>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>     _______________________________________________
>>>>>>     Concurrency-interest mailing list
>>>>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/098b9703/attachment.html>

From vitalyd at gmail.com  Tue Jan 17 14:02:30 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 14:02:30 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F15BF08.5060909@oracle.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
	<CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
	<4F15BF08.5060909@oracle.com>
Message-ID: <CAHjP37Hg0r=3JP2-cVYerXG1o-En7Mhc-KjO_0jbgcXa7LXdSg@mail.gmail.com>

Yes the expensive op would be field contention requiring re-layout and
probably object movement.  How would the processor even tell the JVM about
this? Read some register? How does the cpu report this? As a percentage?
How does it notify you about multiple memory addresses being contended? How
do you know which field in the cache line is actually the culprit? Would
this register be per CPU? When does the VM poll this? Etc as you say this
is rare so I think the cost/benefit of this vs developer annotation isn't
in favor of it.  But it's not for me to implement :).

Sent from my phone
On Jan 17, 2012 1:33 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  If an application is contending on a cache line, then the processors will
> look 100% busy but very little work will get done.  Fixing the contention
> will almost always get huge gains.
>
> Let's say the contention is due to 2 objects being next to each other.
> The solution is to separate them by moving 1 object to another place in the
> heap.  Moving an object in the heap is fairly cheap.  GC does it a lot!
>
> Let's say the contention is due to 2 fields being next to each other.  The
> solution is to separate them by rearranging the fields.  This is probably
> expensive.  Once that is done, the contention will go away but the memory
> will increase.  Very few locks actually contend.  Even fewer fields contend
> on cache lines.  So, the memory impact should be fairly small and probably
> insignificant.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/17/2012 11:03 AM, Vitaly Davidovich wrote:
>
> Doing dynamic re-layout and contention adjustment seems nice in thought
> but how practical is that? I can't see how that would be cheap enough where
> it's worth the cost. What if the app goes through phases of contention?
> Initially high but then no sharing - would the bloated object layout be
> worth it at that point? Seems like this is a place where explicit developer
> instructions is better than heuristics with potentially expensive
> consequences.
>
> Sent from my phone
> On Jan 17, 2012 12:44 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>
>>  Assuming that the JVM can optimize for true and false sharing (and that
>> is a big assumption at the moment), then you can focus your time on writing
>> useful code.  Furthermore, optimizing for true and false sharing will never
>> be able to fix actual data contention.  We still need clever ways of
>> sharing data without bottlenecks.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>> On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
>>
>> You made my day. Few months ago I was dreaming (in my blog) about
>> complexity of false sharing prevention with padding. And come to two
>> options, one better another. First one was @PreventFalseSharing
>> annotation, next was atomatically contention detection and relocation
>> of contended objects by JIT. Readers of my blog soon pointed me to
>> @Contended annotation. And now you telling the second -- the best --
>> option is also being explored!
>>
>> Just want to ask -- if all good things will be done by JIT -- what I
>> will be paid for?
>>
>> 2012/1/17 Nathan Reynolds <nathan.reynolds at oracle.com> <nathan.reynolds at oracle.com>:
>>
>>  It would be nice if the processor could effectively tell the JVM that false
>> sharing is happening.  It would be nice if the JVM could respond by moving
>> objects within the heap or fields with the class to avoid false sharing.
>> Thus, we don't have to pad or worry about placing @Contended or other
>> attributes into the class.
>>
>> Intel was looking into to optimizing for true and false sharing.  They had a
>> prototype that worked but required restarting the JVM.  Oracle was looking
>> into dynamically relayout fields in objects.  I haven't heard anything from
>> either group for a while...  I haven't asked either.  *IF* a solution
>> becomes available, then it will be a while.  This is a very difficult thing
>> to do.
>>
>> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
>> Oracle PSR Engineering | Server Technology
>>
>> On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>>
>> OK I see what you mean now.  I imagine @Contended will be used with fields
>> rather than classes so when the JVM lays out an instance of the class I
>> assume it will do two-sided padding on the contended field if required or if
>> natural layout is such that prior fields already fill up a cache line then
>> only one sided is needed.
>>
>> Sent from my phone
>>
>> On Jan 17, 2012 11:27 AM, "Ruslan Cheremin" <cheremin at gmail.com> <cheremin at gmail.com> wrote:
>>
>>  2012/1/17 Vitaly Davidovich <vitalyd at gmail.com> <vitalyd at gmail.com>:
>>
>>  I think it's semantics - if you sometimes allocate with 64/128 byte
>> alignment then if your object is smaller than 64/128 the rest of the
>> space
>> is effectively padding.
>>
>>  Agree. But in case of alignment you lose sense of "one-side" or "two
>> side" padding -- you do not need "two side padding", you just make
>> sure object align on cache line boundary.
>>
>> Actually I was asked is my understanding of how @Contended supposed to
>> be implemented is right?
>>
>>
>>  Or are you saying you want an @Alignment annotation
>> instead so it's more general? What other uses of custom alignment do you
>> envision? Java is too high-level  and the underlying hardware/platform
>> too
>> abstracted away for a general purpose custom alignment hint, IMHO.
>>
>>  No, I do not want such ugly thing to happen with java! It's enough C
>> for such things...
>>
>>
>>
>>  Sent from my phone
>>
>> On Jan 17, 2012 10:56 AM, "Ruslan Cheremin" <cheremin at gmail.com> <cheremin at gmail.com> wrote:
>>
>>  Yes. As a practical matter though, until an @Contended attribute
>> or something like it is supported across JVMS (see list archives for
>> discussion), you cannot arrange reliable two-sided padding
>> for objects with mixed field types (ints, longs, refs that may be
>> either 32 or 64 bits, etc), so one-sided is the best you can do.
>>
>>  By the way -- I was not thinking about @Contended as "make padding for
>> me". It seems for me like padding is only dirty hack, since nothing
>> better available. If I would control memory allocation (like JVM does)
>> I just can allocate @Contended objects on 64 (128... etc) bytes
>> boundary. I do not have to "pad" them -- nor both, nor one side. And I
>> suppose @Contended implementation to do exactly this -- "use special
>> allocator for objects of that type, which allocate them on cache line
>> boundary"
>>
>> Am I wrong here?
>>
>>
>>
>>  -Doug
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>  _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/a6e88500/attachment-0001.html>

From dl at cs.oswego.edu  Tue Jan 17 14:11:26 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Jan 2012 14:11:26 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F15AE4F.7010700@oracle.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
Message-ID: <4F15C7DE.9040808@cs.oswego.edu>

Two footnotes:

1. Everyone using OpenJDK7+ / OracleJDK7+ on a multisocket x86
(Intel or AMD, also usually Sparcs) should always use the switch
   -XX:+UseCondCardMark
unless using -XX:+UseG1GC (that also includes a form of it.)
Dave Dice's blog from last year explains why in more detail:
   http://blogs.oracle.com/dave/entry/false_sharing_induced_by_card
In short, the marking done on reference writes can itself induce
serious contention problems, and no reasonable amount of padding helps,
since each byte of the cardmark table can stand for 512 addresses,
and they are all held side by side on the same cacheline.

On 01/17/12 12:22, Nathan Reynolds wrote:
> It would be nice if the processor could effectively tell the JVM that false
> sharing is happening. It would be nice if the JVM could respond by moving
> objects within the heap

While I'm skeptical of this happening any time soon, it is worth noting
that this is what Striped64 and new Exchanger do in the case
of CAS contention -- they move thread-local indices to less contended
spots. We still have to waste a fair amount of space to do this though
since we don't have feedback on plain memory contention; just CAS
contention.

-Doug


From nathan.reynolds at oracle.com  Tue Jan 17 14:12:54 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 17 Jan 2012 12:12:54 -0700
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAHjP37Hg0r=3JP2-cVYerXG1o-En7Mhc-KjO_0jbgcXa7LXdSg@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
	<CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
	<4F15BF08.5060909@oracle.com>
	<CAHjP37Hg0r=3JP2-cVYerXG1o-En7Mhc-KjO_0jbgcXa7LXdSg@mail.gmail.com>
Message-ID: <4F15C836.1060503@oracle.com>

 > How would the processor even tell the JVM about this? Read some 
register? How does the cpu report this? As a percentage?

Intel's x86 processors have hardware performance counters.  I can't 
remember which counter to use.  I think the counter captures this 
information: last level cache miss when the cache line was modified by 
another core.  The counter tells us the data address of the miss and 
some other key information... which I can't remember at the moment.

 > How does it notify you about multiple memory addresses being contended?

The JVM would have to poll the counter periodically and see what is 
there.  Since the counter is changing often, the JVM will see multiple 
addresses show up.

 > Would this register be per CPU?

If the correct performance counter is the one I am thinking of, then 
this performance counter is per processor socket.

 > When does the VM poll this?

I am not sure.  I would guess just on a timer.  The JVM could poll it 
right after a profiled method executes.

 > Etc as you say this is rare so I think the cost/benefit of this vs 
developer annotation isn't in favor of it.

True and false sharing happens a lot.  Its just that it happens on very 
few objects.  The JVM has to fix only those few objects and the whole 
program benefits greatly.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/17/2012 12:02 PM, Vitaly Davidovich wrote:
>
> Yes the expensive op would be field contention requiring re-layout and 
> probably object movement.  How would the processor even tell the JVM 
> about this? Read some register? How does the cpu report this? As a 
> percentage? How does it notify you about multiple memory addresses 
> being contended? How do you know which field in the cache line is 
> actually the culprit? Would this register be per CPU? When does the VM 
> poll this? Etc as you say this is rare so I think the cost/benefit of 
> this vs developer annotation isn't in favor of it.  But it's not for 
> me to implement :).
>
> Sent from my phone
>
> On Jan 17, 2012 1:33 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com 
> <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     If an application is contending on a cache line, then the
>     processors will look 100% busy but very little work will get
>     done.  Fixing the contention will almost always get huge gains.
>
>     Let's say the contention is due to 2 objects being next to each
>     other.  The solution is to separate them by moving 1 object to
>     another place in the heap.  Moving an object in the heap is fairly
>     cheap.  GC does it a lot!
>
>     Let's say the contention is due to 2 fields being next to each
>     other.  The solution is to separate them by rearranging the
>     fields.  This is probably expensive.  Once that is done, the
>     contention will go away but the memory will increase.  Very few
>     locks actually contend.  Even fewer fields contend on cache
>     lines.  So, the memory impact should be fairly small and probably
>     insignificant.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Consulting Member of Technical Staff | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
>     On 1/17/2012 11:03 AM, Vitaly Davidovich wrote:
>>
>>     Doing dynamic re-layout and contention adjustment seems nice in
>>     thought but how practical is that? I can't see how that would be
>>     cheap enough where it's worth the cost. What if the app goes
>>     through phases of contention? Initially high but then no sharing
>>     - would the bloated object layout be worth it at that point?
>>     Seems like this is a place where explicit developer instructions
>>     is better than heuristics with potentially expensive consequences.
>>
>>     Sent from my phone
>>
>>     On Jan 17, 2012 12:44 PM, "Nathan Reynolds"
>>     <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>>
>>     wrote:
>>
>>         Assuming that the JVM can optimize for true and false sharing
>>         (and that is a big assumption at the moment), then you can
>>         focus your time on writing useful code.  Furthermore,
>>         optimizing for true and false sharing will never be able to
>>         fix actual data contention.  We still need clever ways of
>>         sharing data without bottlenecks.
>>
>>         Nathan Reynolds
>>         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>         Consulting Member of Technical Staff | 602.333.9091
>>         <tel:602.333.9091>
>>         Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>         Technology
>>
>>         On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
>>>         You made my day. Few months ago I was dreaming (in my blog) about
>>>         complexity of false sharing prevention with padding. And come to two
>>>         options, one better another. First one was @PreventFalseSharing
>>>         annotation, next was atomatically contention detection and relocation
>>>         of contended objects by JIT. Readers of my blog soon pointed me to
>>>         @Contended annotation. And now you telling the second -- the best --
>>>         option is also being explored!
>>>
>>>         Just want to ask -- if all good things will be done by JIT -- what I
>>>         will be paid for?
>>>
>>>         2012/1/17 Nathan Reynolds<nathan.reynolds at oracle.com>  <mailto:nathan.reynolds at oracle.com>:
>>>>         It would be nice if the processor could effectively tell the JVM that false
>>>>         sharing is happening.  It would be nice if the JVM could respond by moving
>>>>         objects within the heap or fields with the class to avoid false sharing.
>>>>         Thus, we don't have to pad or worry about placing @Contended or other
>>>>         attributes into the class.
>>>>
>>>>         Intel was looking into to optimizing for true and false sharing.  They had a
>>>>         prototype that worked but required restarting the JVM.  Oracle was looking
>>>>         into dynamically relayout fields in objects.  I haven't heard anything from
>>>>         either group for a while...  I haven't asked either.  *IF* a solution
>>>>         becomes available, then it will be a while.  This is a very difficult thing
>>>>         to do.
>>>>
>>>>         Nathan Reynolds | Consulting Member of Technical Staff |602.333.9091  <tel:602.333.9091>
>>>>         Oracle PSR Engineering | Server Technology
>>>>
>>>>         On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>>>>
>>>>         OK I see what you mean now.  I imagine @Contended will be used with fields
>>>>         rather than classes so when the JVM lays out an instance of the class I
>>>>         assume it will do two-sided padding on the contended field if required or if
>>>>         natural layout is such that prior fields already fill up a cache line then
>>>>         only one sided is needed.
>>>>
>>>>         Sent from my phone
>>>>
>>>>         On Jan 17, 2012 11:27 AM, "Ruslan Cheremin"<cheremin at gmail.com>  <mailto:cheremin at gmail.com>  wrote:
>>>>>         2012/1/17 Vitaly Davidovich<vitalyd at gmail.com>  <mailto:vitalyd at gmail.com>:
>>>>>>         I think it's semantics - if you sometimes allocate with 64/128 byte
>>>>>>         alignment then if your object is smaller than 64/128 the rest of the
>>>>>>         space
>>>>>>         is effectively padding.
>>>>>         Agree. But in case of alignment you lose sense of "one-side" or "two
>>>>>         side" padding -- you do not need "two side padding", you just make
>>>>>         sure object align on cache line boundary.
>>>>>
>>>>>         Actually I was asked is my understanding of how @Contended supposed to
>>>>>         be implemented is right?
>>>>>
>>>>>>         Or are you saying you want an @Alignment annotation
>>>>>>         instead so it's more general? What other uses of custom alignment do you
>>>>>>         envision? Java is too high-level  and the underlying hardware/platform
>>>>>>         too
>>>>>>         abstracted away for a general purpose custom alignment hint, IMHO.
>>>>>         No, I do not want such ugly thing to happen with java! It's enough C
>>>>>         for such things...
>>>>>
>>>>>
>>>>>>         Sent from my phone
>>>>>>
>>>>>>         On Jan 17, 2012 10:56 AM, "Ruslan Cheremin"<cheremin at gmail.com>  <mailto:cheremin at gmail.com>  wrote:
>>>>>>>>         Yes. As a practical matter though, until an @Contended attribute
>>>>>>>>         or something like it is supported across JVMS (see list archives for
>>>>>>>>         discussion), you cannot arrange reliable two-sided padding
>>>>>>>>         for objects with mixed field types (ints, longs, refs that may be
>>>>>>>>         either 32 or 64 bits, etc), so one-sided is the best you can do.
>>>>>>>         By the way -- I was not thinking about @Contended as "make padding for
>>>>>>>         me". It seems for me like padding is only dirty hack, since nothing
>>>>>>>         better available. If I would control memory allocation (like JVM does)
>>>>>>>         I just can allocate @Contended objects on 64 (128... etc) bytes
>>>>>>>         boundary. I do not have to "pad" them -- nor both, nor one side. And I
>>>>>>>         suppose @Contended implementation to do exactly this -- "use special
>>>>>>>         allocator for objects of that type, which allocate them on cache line
>>>>>>>         boundary"
>>>>>>>
>>>>>>>         Am I wrong here?
>>>>>>>
>>>>>>>
>>>>>>>>         -Doug
>>>>>>>>         _______________________________________________
>>>>>>>>         Concurrency-interest mailing list
>>>>>>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>         _______________________________________________
>>>>>>>         Concurrency-interest mailing list
>>>>>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>         _______________________________________________
>>>>         Concurrency-interest mailing list
>>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/4bc17896/attachment-0001.html>

From brian at briangoetz.com  Tue Jan 17 14:36:54 2012
From: brian at briangoetz.com (Brian Goetz)
Date: Tue, 17 Jan 2012 14:36:54 -0500
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F15BF08.5060909@oracle.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
	<CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
	<4F15BF08.5060909@oracle.com>
Message-ID: <4F15CDD6.4040006@briangoetz.com>

Having the VM automagically figure this out would be great, but it seems 
a cheaper / faster-to-market solution to give developers the ability to 
detect false sharing during development and testing and on the basis of 
such observations give padding hints (say, with annotations) that the VM 
could use or ignore as it saw fit.  False sharing will happen with a 
relatively few concurrent building blocks (like, say, Exchanger) and 
maintainers of such classes are probably (willing to be) on the lookout 
for it anyway.

On 1/17/2012 1:33 PM, Nathan Reynolds wrote:
> If an application is contending on a cache line, then the processors
> will look 100% busy but very little work will get done. Fixing the
> contention will almost always get huge gains.
>
> Let's say the contention is due to 2 objects being next to each other.
> The solution is to separate them by moving 1 object to another place in
> the heap. Moving an object in the heap is fairly cheap. GC does it a lot!
>
> Let's say the contention is due to 2 fields being next to each other.
> The solution is to separate them by rearranging the fields. This is
> probably expensive. Once that is done, the contention will go away but
> the memory will increase. Very few locks actually contend. Even fewer
> fields contend on cache lines. So, the memory impact should be fairly
> small and probably insignificant.
>
> Nathan Reynolds
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
> Consulting Member of Technical Staff | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/17/2012 11:03 AM, Vitaly Davidovich wrote:
>>
>> Doing dynamic re-layout and contention adjustment seems nice in
>> thought but how practical is that? I can't see how that would be cheap
>> enough where it's worth the cost. What if the app goes through phases
>> of contention? Initially high but then no sharing - would the bloated
>> object layout be worth it at that point? Seems like this is a place
>> where explicit developer instructions is better than heuristics with
>> potentially expensive consequences.
>>
>> Sent from my phone
>>
>> On Jan 17, 2012 12:44 PM, "Nathan Reynolds"
>> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>>
>>     Assuming that the JVM can optimize for true and false sharing (and
>>     that is a big assumption at the moment), then you can focus your
>>     time on writing useful code. Furthermore, optimizing for true and
>>     false sharing will never be able to fix actual data contention. We
>>     still need clever ways of sharing data without bottlenecks.
>>
>>     Nathan Reynolds
>>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>     Consulting Member of Technical Staff | 602.333.9091 <tel:602.333.9091>
>>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>>     On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
>>>     You made my day. Few months ago I was dreaming (in my blog) about
>>>     complexity of false sharing prevention with padding. And come to two
>>>     options, one better another. First one was @PreventFalseSharing
>>>     annotation, next was atomatically contention detection and relocation
>>>     of contended objects by JIT. Readers of my blog soon pointed me to
>>>     @Contended annotation. And now you telling the second -- the best --
>>>     option is also being explored!
>>>
>>>     Just want to ask -- if all good things will be done by JIT -- what I
>>>     will be paid for?
>>>
>>>     2012/1/17 Nathan Reynolds<nathan.reynolds at oracle.com>  <mailto:nathan.reynolds at oracle.com>:
>>>>     It would be nice if the processor could effectively tell the JVM that false
>>>>     sharing is happening.    It would be nice if the JVM could respond by moving
>>>>     objects within the heap or fields with the class to avoid false sharing.
>>>>     Thus, we don't have to pad or worry about placing @Contended or other
>>>>     attributes into the class.
>>>>
>>>>     Intel was looking into to optimizing for true and false sharing.    They had a
>>>>     prototype that worked but required restarting the JVM.    Oracle was looking
>>>>     into dynamically relayout fields in objects.    I haven't heard anything from
>>>>     either group for a while...    I haven't asked either.    *IF* a solution
>>>>     becomes available, then it will be a while.    This is a very difficult thing
>>>>     to do.
>>>>
>>>>     Nathan Reynolds | Consulting Member of Technical Staff |602.333.9091  <tel:602.333.9091>
>>>>     Oracle PSR Engineering | Server Technology
>>>>
>>>>     On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>>>>
>>>>     OK I see what you mean now.    I imagine @Contended will be used with fields
>>>>     rather than classes so when the JVM lays out an instance of the class I
>>>>     assume it will do two-sided padding on the contended field if required or if
>>>>     natural layout is such that prior fields already fill up a cache line then
>>>>     only one sided is needed.
>>>>
>>>>     Sent from my phone
>>>>
>>>>     On Jan 17, 2012 11:27 AM, "Ruslan Cheremin"<cheremin at gmail.com>  <mailto:cheremin at gmail.com>  wrote:
>>>>>     2012/1/17 Vitaly Davidovich<vitalyd at gmail.com>  <mailto:vitalyd at gmail.com>:
>>>>>>     I think it's semantics - if you sometimes allocate with 64/128 byte
>>>>>>     alignment then if your object is smaller than 64/128 the rest of the
>>>>>>     space
>>>>>>     is effectively padding.
>>>>>     Agree. But in case of alignment you lose sense of "one-side" or "two
>>>>>     side" padding -- you do not need "two side padding", you just make
>>>>>     sure object align on cache line boundary.
>>>>>
>>>>>     Actually I was asked is my understanding of how @Contended supposed to
>>>>>     be implemented is right?
>>>>>
>>>>>>     Or are you saying you want an @Alignment annotation
>>>>>>     instead so it's more general? What other uses of custom alignment do you
>>>>>>     envision? Java is too high-level    and the underlying hardware/platform
>>>>>>     too
>>>>>>     abstracted away for a general purpose custom alignment hint, IMHO.
>>>>>     No, I do not want such ugly thing to happen with java! It's enough C
>>>>>     for such things...
>>>>>
>>>>>
>>>>>>     Sent from my phone
>>>>>>
>>>>>>     On Jan 17, 2012 10:56 AM, "Ruslan Cheremin"<cheremin at gmail.com>  <mailto:cheremin at gmail.com>  wrote:
>>>>>>>>     Yes. As a practical matter though, until an @Contended attribute
>>>>>>>>     or something like it is supported across JVMS (see list archives for
>>>>>>>>     discussion), you cannot arrange reliable two-sided padding
>>>>>>>>     for objects with mixed field types (ints, longs, refs that may be
>>>>>>>>     either 32 or 64 bits, etc), so one-sided is the best you can do.
>>>>>>>     By the way -- I was not thinking about @Contended as "make padding for
>>>>>>>     me". It seems for me like padding is only dirty hack, since nothing
>>>>>>>     better available. If I would control memory allocation (like JVM does)
>>>>>>>     I just can allocate @Contended objects on 64 (128... etc) bytes
>>>>>>>     boundary. I do not have to "pad" them -- nor both, nor one side. And I
>>>>>>>     suppose @Contended implementation to do exactly this -- "use special
>>>>>>>     allocator for objects of that type, which allocate them on cache line
>>>>>>>     boundary"
>>>>>>>
>>>>>>>     Am I wrong here?
>>>>>>>
>>>>>>>
>>>>>>>>     -Doug
>>>>>>>>     _______________________________________________
>>>>>>>>     Concurrency-interest mailing list
>>>>>>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>     _______________________________________________
>>>>>>>     Concurrency-interest mailing list
>>>>>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>     _______________________________________________
>>>>     Concurrency-interest mailing list
>>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From davidcholmes at aapt.net.au  Tue Jan 17 17:30:48 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 18 Jan 2012 08:30:48 +1000
Subject: [concurrency-interest] On-demand Construction of Threads
	inThreadPoolExecutor
In-Reply-To: <4F15A534.30709@javaspecialists.eu>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEIDJCAA.davidcholmes@aapt.net.au>

Because it amortizes the construction cost to create threads lazily. If you
want eager you can request it.

It is still an open RFE (6452337) to prefer to re-use an existing idle
Thread rather than create a new one when under core size.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M.
Kabutz
  Sent: Wednesday, 18 January 2012 2:44 AM
  To: Vitaly Davidovich
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] On-demand Construction of Threads
inThreadPoolExecutor


  I like Joe's point and it was something I also considered.  This was a
nice trade-off to have a single class that could handle both fixed and
cached thread pools.

  My question, however, was specifically about fixed thread pools and why
they did not just allocate the desired number of threads up front when the
thread pool was constructed.

Regards

Heinz
--
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz


  On 1/17/12 6:38 PM, Vitaly Davidovich wrote:
    Yes that's right but if that one thread is sufficient you get efficiency
benefits.  I'm not sure if you're saying you still disagree or not though :)

    Joe brings up other good points as well, such as starting threads in
ctor and ability to configure the pool after construction (although most
tweaks can be done after threads are running).

    Sent from my phone

    On Jan 17, 2012 11:00 AM, "Dr Heinz M. Kabutz"
<heinz at javaspecialists.eu> wrote:

      Ah, but the Cached Thread Pool always has a core size of zero.  By
default the threads expire after a minute of not being used.

      import java.util.concurrent.*;

      public class CachedThreadPoolTest {
        public static void main(String[] args) throws ExecutionException,
InterruptedException {
          ThreadPoolExecutor pool = (ThreadPoolExecutor)
Executors.newCachedThreadPool();

          System.out.println("initial pool size: " + pool.getPoolSize());
          System.out.println("core pool size: " + pool.getCorePoolSize());

          for(int i=0; i<10; i++) {
            Future<Void> future = pool.submit(new Callable<Void>() {
              public Void call() throws Exception {
                return null;
              }
            });
            future.get();
            System.out.println("Task " + i + " done");
            System.out.println("pool size: " + pool.getPoolSize());
          }

          pool.shutdown();
        }
      }

      Output:

      initial pool size: 0
      core pool size: 0
      Task 0 done
      pool size: 1
      Task 1 done
      pool size: 1
      Task 2 done
      pool size: 1
      Task 3 done
      pool size: 1
      Task 4 done
      pool size: 1
      Task 5 done
      pool size: 1
      Task 6 done
      pool size: 1
      Task 7 done
      pool size: 1
      Task 8 done
      pool size: 1
      Task 9 done
      pool size: 1


Regards

Heinz
--
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz


      On 1/17/12 5:51 PM, Vitaly Davidovich wrote:
        That's because you created a fixed size pool; as I mentioned about
core size before, the pool will prefer to add threads in this case to come
up to your core size.  Try with Executor.newCachedThreadPool() instead.

        Sent from my phone

        On Jan 17, 2012 10:40 AM, "Dr Heinz M. Kabutz"
<heinz at javaspecialists.eu> wrote:

          Here's a quick demo:

          import java.util.concurrent.*;

          public class ThreadPoolOnDemandConstruction {
            public static void main(String[] args) throws
ExecutionException, InterruptedException {
              ThreadPoolExecutor pool = (ThreadPoolExecutor)
Executors.newFixedThreadPool(5);
              System.out.println("initial pool size: " +
pool.getPoolSize());

              for(int i=0; i<10; i++) {
                Future<Void> future = pool.submit(new Callable<Void>() {
                  public Void call() throws Exception {
                    return null;
                  }
                });
                future.get();
                System.out.println("Task " + i + " done");
                System.out.println("pool size: " + pool.getPoolSize());
              }

              pool.shutdown();
            }
          }


          Output is:

          initial pool size: 0
          Task 0 done
          pool size: 1
          Task 1 done
          pool size: 2
          Task 2 done
          pool size: 3
          Task 3 done
          pool size: 4
          Task 4 done
          pool size: 5
          Task 5 done
          pool size: 5
          Task 6 done
          pool size: 5
          Task 7 done
          pool size: 5
          Task 8 done
          pool size: 5
          Task 9 done
          pool size: 5


Regards

Heinz
--
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz


          On 1/17/12 5:24 PM, Vitaly Davidovich wrote:
            I don't have source code in front of me right now but it
shouldn't add threads if there are spare ones unless min or core size was
specified, IIRC.  If one thread can keep up with task submission rate why
create more? For cases where you know you'll have a high rate that outpaces
consumption and don't want to take the hit of waiting for threads to spin
up, you call the pre start method. At least that's what makes sense to me :)

            Sent from my phone

            On Jan 17, 2012 10:15 AM, "Dr Heinz M. Kabutz"
<heinz at javaspecialists.eu> wrote:

              I doubt it.  Threads are created as tasks are submitted, even
if there are threads available.  But maybe you are right :-)

Regards

Heinz
--
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz


              On 1/17/12 5:11 PM, Vitaly Davidovich wrote:
                Perhaps because you don't know upfront how many threads will
actually be needed to service the workload and thus don't want to spin them
up eagerly (for perf and efficiency reasons) but rather adjust as work comes
in? That is, what if just 1 thread is enough but pool is configured to allow
for more ...

                Sent from my phone

                On Jan 17, 2012 10:06 AM, "Dr Heinz M. Kabutz"
<heinz at javaspecialists.eu> wrote:

                  A quick historical question.  What was the thinking behind
constructing the threads in the ThreadPoolExecutor lazily?

                  In the JavaDocs of ThreadPoolExecutor:

                  On-demand construction
                      By default, even core threads are initially created
and started only when new tasks arrive, but this can be overridden
dynamically using method {@link ThreadPoolExecutor#prestartCoreThread} or
{@link ThreadPoolExecutor#prestartAllCoreThreads}. You probably want to
prestart threads if you construct the pool with a non-empty queue.

                  I can think of two possible answers:

                  Correctness: The author of this class did not want the
ThreadPoolExecutor instance to escape into the threads before it had
completed being constructed.  We should avoid letting "this" escape during
construction.

                  Performance: It might be better to throttle the
construction of new threads.  Say someone creates a new fixed thread pool
with 2000 threads, instead of waiting for them all to be created, we let
each task submission create a thread until we reach the core size.

                  Or perhaps it was neither of these two answers.  Maybe it
was just easier to write it this way, so that one class could be used for
both the cached and the fixed thread pool.

                  Would love to hear the reasons for this :-)

Regards

Heinz
--
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz

                  _______________________________________________
                  Concurrency-interest mailing list
                  Concurrency-interest at cs.oswego.edu
                  http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120118/495cf02e/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Jan 17 17:50:14 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 18 Jan 2012 08:50:14 +1000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEIEJCAA.davidcholmes@aapt.net.au>

Raph Frank writes:
> I was wondering if there was a happens before guarantee for reads of
> volatiles relative to later writes.  I think maybe not, but it isn't
> clear.

The specification defines all the happens-before orderings that exist - see
JLS 17.4.5 for the definitions of happens-before, and 17.4.4 for the defined
synchronization actions. In terms of your question a volatile write
happens-before subsequent reads of the same variable. There is no
happens-before ordering between a volatile read and a subsequent volatile
write.

> For example, assuming there were 2 threads with the following code:
>
> ** Thread 1 **
>
> int b = 0;
> volatile boolean a = false;
> ...
> ...
> a = true;
> b = 1;
>
> ** Thread 2 **
>
> int bStore = b;
> if (!a) {
> ? System.out.println("The value of bStore is " + bStore);
> }
>
> Are these reasonable statements:
>
> "bStore = b" happens-before "if(!a)"

Yes - these are actions in the same thread and so happens-before order
follows program order

> "if(!a)" happens-before "a = true;" [Assuming that (!a) evaluates to true]

No. You need to be careful about "happens-before" vs. "occurred before". The
former is a clearly defined property in the memory-model. The latter is the
actual occurrence at execution time. If !a evaluates to true then it is
certainly the case that the read of 'a' occurred prior to the write a=true.
But there is no happens-before edge between the two actions.

> "a = true;" happens-before "b=1;"

Yes - these are actions in the same thread and so happens-before order
follow program order

> Thus, the program will either print "The value of bStore is 0" or not
> print anything.

No. You lost the transitive relationship of happens-before with your second
statement. Your code can be reordered and executed as follows:

    Thread 1                    Thread 2
                                if (!a) // sees false
    a = true
    b = 1;
                                bStore =b;
                                print   // prints 1

Or as Vitaly states, the assignments in Thread 1 could appear reordered when
seen from Thread 2

Cheers,
David Holmes



From nader at aeinehchi.com  Tue Jan 17 18:11:42 2012
From: nader at aeinehchi.com (Nader Aeinehchi)
Date: Wed, 18 Jan 2012 00:11:42 +0100
Subject: [concurrency-interest] Question about re-ordering
Message-ID: <4F16002E.8090405@aeinehchi.com>

Hi

In the following, a listener is notified by some configurator.  Question 
is whether a re-ordering may be enforced by compiler on the operations 
on "stateMap"?  Is it safe to assume that stateMap.clear() is always run 
before stateMap.put within the same thread?

     public void notify(ConfigurationEvent event) {
         if (event.getType() == AbstractFileConfiguration.EVENT_RELOAD) {
             logger.warn("log something");

             stateMap.clear();

             List<SubnodeConfiguration> configurations = 
xmlConfiguration.configurationsAt("*");
             for (SubnodeConfiguration subnodeConfiguration : 
configurations) {
                 
stateMap.put(subnodeConfiguration.getString("@configurationId"), new 
State(subnodeConfiguration));
             }
         }
     }

Thanks

From hans.boehm at hp.com  Tue Jan 17 18:13:15 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 17 Jan 2012 23:13:15 +0000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIEJCAA.davidcholmes@aapt.net.au>
References: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEIEJCAA.davidcholmes@aapt.net.au>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD22E52977C@G9W0339.americas.hpqcorp.net>

Another way to look at this is that compiler reordering that is only observable by programs with data races is generally allowed.  The program below clearly has an inherent data race on b.  If you make b volatile, the data race, and the problem, go away.

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> interest-bounces at cs.oswego.edu] On Behalf Of David Holmes
> Sent: Tuesday, January 17, 2012 2:50 PM
> To: Raph Frank
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Volatile happens before question
> 
> Raph Frank writes:
> > I was wondering if there was a happens before guarantee for reads of
> > volatiles relative to later writes.  I think maybe not, but it isn't
> > clear.
> 
> The specification defines all the happens-before orderings that exist -
> see
> JLS 17.4.5 for the definitions of happens-before, and 17.4.4 for the
> defined
> synchronization actions. In terms of your question a volatile write
> happens-before subsequent reads of the same variable. There is no
> happens-before ordering between a volatile read and a subsequent
> volatile
> write.
> 
> > For example, assuming there were 2 threads with the following code:
> >
> > ** Thread 1 **
> >
> > int b = 0;
> > volatile boolean a = false;
> > ...
> > ...
> > a = true;
> > b = 1;
> >
> > ** Thread 2 **
> >
> > int bStore = b;
> > if (!a) {
> > ? System.out.println("The value of bStore is " + bStore);
> > }
> >
> > Are these reasonable statements:
> >
> > "bStore = b" happens-before "if(!a)"
> 
> Yes - these are actions in the same thread and so happens-before order
> follows program order
> 
> > "if(!a)" happens-before "a = true;" [Assuming that (!a) evaluates to
> true]
> 
> No. You need to be careful about "happens-before" vs. "occurred
> before". The
> former is a clearly defined property in the memory-model. The latter is
> the
> actual occurrence at execution time. If !a evaluates to true then it is
> certainly the case that the read of 'a' occurred prior to the write
> a=true.
> But there is no happens-before edge between the two actions.
> 
> > "a = true;" happens-before "b=1;"
> 
> Yes - these are actions in the same thread and so happens-before order
> follow program order
> 
> > Thus, the program will either print "The value of bStore is 0" or not
> > print anything.
> 
> No. You lost the transitive relationship of happens-before with your
> second
> statement. Your code can be reordered and executed as follows:
> 
>     Thread 1                    Thread 2
>                                 if (!a) // sees false
>     a = true
>     b = 1;
>                                 bStore =b;
>                                 print   // prints 1
> 
> Or as Vitaly states, the assignments in Thread 1 could appear reordered
> when
> seen from Thread 2
> 
> Cheers,
> David Holmes
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Tue Jan 17 18:25:31 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 18:25:31 -0500
Subject: [concurrency-interest] Question about re-ordering
In-Reply-To: <4F16002E.8090405@aeinehchi.com>
References: <4F16002E.8090405@aeinehchi.com>
Message-ID: <CAHjP37H9LCZSeWk5Vi=WEwTP9RUEDb1+-8fvXVj=VnMA-iKn3g@mail.gmail.com>

Yes clear() runs before put() in same thread - reordering would utterly
break semantics, this is unrelated to concurrency.

Sent from my phone
On Jan 17, 2012 6:13 PM, "Nader Aeinehchi" <nader at aeinehchi.com> wrote:

> Hi
>
> In the following, a listener is notified by some configurator.  Question
> is whether a re-ordering may be enforced by compiler on the operations on
> "stateMap"?  Is it safe to assume that stateMap.clear() is always run
> before stateMap.put within the same thread?
>
>    public void notify(ConfigurationEvent event) {
>        if (event.getType() == AbstractFileConfiguration.**EVENT_RELOAD) {
>            logger.warn("log something");
>
>            stateMap.clear();
>
>            List<SubnodeConfiguration> configurations = xmlConfiguration.**
> configurationsAt("*");
>            for (SubnodeConfiguration subnodeConfiguration :
> configurations) {
>                stateMap.put(**subnodeConfiguration.**getString("@configurationId"),
> new State(subnodeConfiguration));
>            }
>        }
>    }
>
> Thanks
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/9fb49cf3/attachment.html>

From davidcholmes at aapt.net.au  Tue Jan 17 18:26:15 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 18 Jan 2012 09:26:15 +1000
Subject: [concurrency-interest] Question about re-ordering
In-Reply-To: <4F16002E.8090405@aeinehchi.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEIFJCAA.davidcholmes@aapt.net.au>

>From a theoretical perspective actions within a thread must appear to occur
in program order, but any reordering is allowed that would not be visible by
the thread. In practice without some aggressive inlining, and only if
intervening functions are trivially small, it is not likely that any
instructions from clear() would be reordered with any from put(). A compiler
is not going to reorder two method calls unless it can ascertain it is
correct to do so - which in general it can not do.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nader
> Aeinehchi
> Sent: Wednesday, 18 January 2012 9:12 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Question about re-ordering
>
>
> Hi
>
> In the following, a listener is notified by some configurator.  Question
> is whether a re-ordering may be enforced by compiler on the operations
> on "stateMap"?  Is it safe to assume that stateMap.clear() is always run
> before stateMap.put within the same thread?
>
>      public void notify(ConfigurationEvent event) {
>          if (event.getType() == AbstractFileConfiguration.EVENT_RELOAD) {
>              logger.warn("log something");
>
>              stateMap.clear();
>
>              List<SubnodeConfiguration> configurations =
> xmlConfiguration.configurationsAt("*");
>              for (SubnodeConfiguration subnodeConfiguration :
> configurations) {
>
> stateMap.put(subnodeConfiguration.getString("@configurationId"), new
> State(subnodeConfiguration));
>              }
>          }
>      }
>
> Thanks
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From vitalyd at gmail.com  Tue Jan 17 18:31:58 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 18:31:58 -0500
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIEJCAA.davidcholmes@aapt.net.au>
References: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEIEJCAA.davidcholmes@aapt.net.au>
Message-ID: <CAHjP37GY2a1fgsOjX2SecFYe2aDyqSGzV2rY4Yz2d7P8uROxPA@mail.gmail.com>

No. You lost the transitive relationship of happens-before with your second
statement. Your code can be reordered and executed as follows:

Thread 1 Thread 2 if (!a) // sees false a = true b = 1; bStore =b; print //
prints 1

This actually isn't valid because in original code nothing would print if a
is false - it doesn't enter the if block.  However, compiler can simply
move b=1 before a =true as that's permissible and maybe it'll do that for
some register allocation purposes, as an example.

Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/26063ca2/attachment.html>

From joe.bowbeer at gmail.com  Tue Jan 17 18:35:11 2012
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 17 Jan 2012 15:35:11 -0800
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <4F15CDD6.4040006@briangoetz.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
	<CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
	<4F15BF08.5060909@oracle.com> <4F15CDD6.4040006@briangoetz.com>
Message-ID: <CAHzJPEoF5cZXJaRoBpZkr=OCStezL6QA-CzEN=s-iQcDozCzEA@mail.gmail.com>

I like the generic annotation idea.

Note that optimizer/obfuscator/compactor tools that are commonly used in
some tool chains (e.g., proguard w/Android), will need to be instructed to
preserve these annotations.

On Tue, Jan 17, 2012 at 11:36 AM, Brian Goetz wrote:

> Having the VM automagically figure this out would be great, but it seems a
> cheaper / faster-to-market solution to give developers the ability to
> detect false sharing during development and testing and on the basis of
> such observations give padding hints (say, with annotations) that the VM
> could use or ignore as it saw fit.  False sharing will happen with a
> relatively few concurrent building blocks (like, say, Exchanger) and
> maintainers of such classes are probably (willing to be) on the lookout for
> it anyway.
>
>
> On 1/17/2012 1:33 PM, Nathan Reynolds wrote:
>
>> If an application is contending on a cache line, then the processors
>> will look 100% busy but very little work will get done. Fixing the
>> contention will almost always get huge gains.
>>
>> Let's say the contention is due to 2 objects being next to each other.
>> The solution is to separate them by moving 1 object to another place in
>> the heap. Moving an object in the heap is fairly cheap. GC does it a lot!
>>
>> Let's say the contention is due to 2 fields being next to each other.
>> The solution is to separate them by rearranging the fields. This is
>> probably expensive. Once that is done, the contention will go away but
>> the memory will increase. Very few locks actually contend. Even fewer
>> fields contend on cache lines. So, the memory impact should be fairly
>> small and probably insignificant.
>>
>> Nathan Reynolds
>> <http://psr.us.oracle.com/**wiki/index.php/User:Nathan_**Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>>
>> |
>>
>> Consulting Member of Technical Staff | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>>
>> On 1/17/2012 11:03 AM, Vitaly Davidovich wrote:
>>
>>>
>>> Doing dynamic re-layout and contention adjustment seems nice in
>>> thought but how practical is that? I can't see how that would be cheap
>>> enough where it's worth the cost. What if the app goes through phases
>>> of contention? Initially high but then no sharing - would the bloated
>>> object layout be worth it at that point? Seems like this is a place
>>> where explicit developer instructions is better than heuristics with
>>> potentially expensive consequences.
>>>
>>> Sent from my phone
>>>
>>> On Jan 17, 2012 12:44 PM, "Nathan Reynolds"
>>> <nathan.reynolds at oracle.com <mailto:nathan.reynolds@**oracle.com<nathan.reynolds at oracle.com>>>
>>> wrote:
>>>
>>>    Assuming that the JVM can optimize for true and false sharing (and
>>>    that is a big assumption at the moment), then you can focus your
>>>    time on writing useful code. Furthermore, optimizing for true and
>>>    false sharing will never be able to fix actual data contention. We
>>>    still need clever ways of sharing data without bottlenecks.
>>>
>>>    Nathan Reynolds
>>>    <http://psr.us.oracle.com/**wiki/index.php/User:Nathan_**Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>>
>>> |
>>>    Consulting Member of Technical Staff | 602.333.9091 <tel:602.333.9091
>>> >
>>>    Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>> Technology
>>>
>>>
>>>    On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
>>>
>>>>    You made my day. Few months ago I was dreaming (in my blog) about
>>>>    complexity of false sharing prevention with padding. And come to two
>>>>    options, one better another. First one was @PreventFalseSharing
>>>>    annotation, next was atomatically contention detection and relocation
>>>>    of contended objects by JIT. Readers of my blog soon pointed me to
>>>>    @Contended annotation. And now you telling the second -- the best --
>>>>    option is also being explored!
>>>>
>>>>    Just want to ask -- if all good things will be done by JIT -- what I
>>>>    will be paid for?
>>>>
>>>>    2012/1/17 Nathan Reynolds<nathan.reynolds@**oracle.com<nathan.reynolds at oracle.com>>
>>>>  <mailto:nathan.reynolds@**oracle.com <nathan.reynolds at oracle.com>>:
>>>>
>>>>>    It would be nice if the processor could effectively tell the JVM
>>>>> that false
>>>>>    sharing is happening.    It would be nice if the JVM could respond
>>>>> by moving
>>>>>    objects within the heap or fields with the class to avoid false
>>>>> sharing.
>>>>>    Thus, we don't have to pad or worry about placing @Contended or
>>>>> other
>>>>>    attributes into the class.
>>>>>
>>>>>    Intel was looking into to optimizing for true and false sharing.
>>>>>  They had a
>>>>>    prototype that worked but required restarting the JVM.    Oracle
>>>>> was looking
>>>>>    into dynamically relayout fields in objects.    I haven't heard
>>>>> anything from
>>>>>    either group for a while...    I haven't asked either.    *IF* a
>>>>> solution
>>>>>    becomes available, then it will be a while.    This is a very
>>>>> difficult thing
>>>>>    to do.
>>>>>
>>>>>    Nathan Reynolds | Consulting Member of Technical Staff |
>>>>> 602.333.9091  <tel:602.333.9091>
>>>>>
>>>>>    Oracle PSR Engineering | Server Technology
>>>>>
>>>>>    On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>>>>>
>>>>>    OK I see what you mean now.    I imagine @Contended will be used
>>>>> with fields
>>>>>    rather than classes so when the JVM lays out an instance of the
>>>>> class I
>>>>>    assume it will do two-sided padding on the contended field if
>>>>> required or if
>>>>>    natural layout is such that prior fields already fill up a cache
>>>>> line then
>>>>>    only one sided is needed.
>>>>>
>>>>>    Sent from my phone
>>>>>
>>>>>    On Jan 17, 2012 11:27 AM, "Ruslan Cheremin"<cheremin at gmail.com>
>>>>>  <mailto:cheremin at gmail.com>  wrote:
>>>>>
>>>>>>    2012/1/17 Vitaly Davidovich<vitalyd at gmail.com>  <mailto:
>>>>>> vitalyd at gmail.com>:
>>>>>>
>>>>>>     I think it's semantics - if you sometimes allocate with 64/128
>>>>>>> byte
>>>>>>>    alignment then if your object is smaller than 64/128 the rest of
>>>>>>> the
>>>>>>>    space
>>>>>>>    is effectively padding.
>>>>>>>
>>>>>>    Agree. But in case of alignment you lose sense of "one-side" or
>>>>>> "two
>>>>>>    side" padding -- you do not need "two side padding", you just make
>>>>>>    sure object align on cache line boundary.
>>>>>>
>>>>>>    Actually I was asked is my understanding of how @Contended
>>>>>> supposed to
>>>>>>    be implemented is right?
>>>>>>
>>>>>>     Or are you saying you want an @Alignment annotation
>>>>>>>    instead so it's more general? What other uses of custom alignment
>>>>>>> do you
>>>>>>>    envision? Java is too high-level    and the underlying
>>>>>>> hardware/platform
>>>>>>>    too
>>>>>>>    abstracted away for a general purpose custom alignment hint, IMHO.
>>>>>>>
>>>>>>    No, I do not want such ugly thing to happen with java! It's enough
>>>>>> C
>>>>>>    for such things...
>>>>>>
>>>>>>
>>>>>>     Sent from my phone
>>>>>>>
>>>>>>>    On Jan 17, 2012 10:56 AM, "Ruslan Cheremin"<cheremin at gmail.com>
>>>>>>>  <mailto:cheremin at gmail.com>  wrote:
>>>>>>>
>>>>>>>>    Yes. As a practical matter though, until an @Contended attribute
>>>>>>>>>    or something like it is supported across JVMS (see list
>>>>>>>>> archives for
>>>>>>>>>    discussion), you cannot arrange reliable two-sided padding
>>>>>>>>>    for objects with mixed field types (ints, longs, refs that may
>>>>>>>>> be
>>>>>>>>>    either 32 or 64 bits, etc), so one-sided is the best you can do.
>>>>>>>>>
>>>>>>>>    By the way -- I was not thinking about @Contended as "make
>>>>>>>> padding for
>>>>>>>>    me". It seems for me like padding is only dirty hack, since
>>>>>>>> nothing
>>>>>>>>    better available. If I would control memory allocation (like JVM
>>>>>>>> does)
>>>>>>>>    I just can allocate @Contended objects on 64 (128... etc) bytes
>>>>>>>>    boundary. I do not have to "pad" them -- nor both, nor one side.
>>>>>>>> And I
>>>>>>>>    suppose @Contended implementation to do exactly this -- "use
>>>>>>>> special
>>>>>>>>    allocator for objects of that type, which allocate them on cache
>>>>>>>> line
>>>>>>>>    boundary"
>>>>>>>>
>>>>>>>>    Am I wrong here?
>>>>>>>>
>>>>>>>>
>>>>>>>>     -Doug
>>>>>>>>>
>>>>>>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/d94626dc/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Jan 17 18:37:13 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 18 Jan 2012 09:37:13 +1000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <CAHjP37GY2a1fgsOjX2SecFYe2aDyqSGzV2rY4Yz2d7P8uROxPA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEIFJCAA.davidcholmes@aapt.net.au>

It may be a silly thing to do but I don't think there is anything to stop
the compiler from reordering as if the code were written:

if (!a) {
  bStore = b;
  print ..
} else {
 bStore = b;
}

David
  -----Original Message-----
  From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
  Sent: Wednesday, 18 January 2012 9:32 AM
  To: dholmes at ieee.org
  Cc: Raph Frank; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Volatile happens before question


  No. You lost the transitive relationship of happens-before with your
second statement. Your code can be reordered and executed as follows:

  Thread 1 Thread 2 if (!a) // sees false a = true b = 1; bStore =b; print
// prints 1

  This actually isn't valid because in original code nothing would print if
a is false - it doesn't enter the if block.  However, compiler can simply
move b=1 before a =true as that's permissible and maybe it'll do that for
some register allocation purposes, as an example.

  Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120118/c5fe4159/attachment.html>

From vitalyd at gmail.com  Tue Jan 17 18:45:45 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 18:45:45 -0500
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEIFJCAA.davidcholmes@aapt.net.au>
References: <CAHjP37GY2a1fgsOjX2SecFYe2aDyqSGzV2rY4Yz2d7P8uROxPA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEIFJCAA.davidcholmes@aapt.net.au>
Message-ID: <CAHjP37Hp9oYkC5QfQ4ouPDs+Mi-ELjkaSbzCQgs-TtjGaQV8fA@mail.gmail.com>

That's true, you're right - I didn't think of that as it does seem silly
but is allowed.

Cheers

Sent from my phone
On Jan 17, 2012 6:36 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

> **
> It may be a silly thing to do but I don't think there is anything to stop
> the compiler from reordering as if the code were written:
>
> if (!a) {
>   bStore = b;
>   print ..
> } else {
>  bStore = b;
> }
>
> David
>
> -----Original Message-----
> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
> *Sent:* Wednesday, 18 January 2012 9:32 AM
> *To:* dholmes at ieee.org
> *Cc:* Raph Frank; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Volatile happens before question
>
> No. You lost the transitive relationship of happens-before with your
> second statement. Your code can be reordered and executed as follows:
>
> Thread 1 Thread 2 if (!a) // sees false a = true b = 1; bStore =b; print
> // prints 1
>
> This actually isn't valid because in original code nothing would print if
> a is false - it doesn't enter the if block.  However, compiler can simply
> move b=1 before a =true as that's permissible and maybe it'll do that for
> some register allocation purposes, as an example.
>
> Sent from my phone
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/3768154e/attachment.html>

From yshavit at akiban.com  Tue Jan 17 22:42:52 2012
From: yshavit at akiban.com (Yuval Shavit)
Date: Tue, 17 Jan 2012 22:42:52 -0500
Subject: [concurrency-interest] Question about re-ordering
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEIFJCAA.davidcholmes@aapt.net.au>
References: <4F16002E.8090405@aeinehchi.com>
	<NFBBKALFDCPFIDBNKAPCAEIFJCAA.davidcholmes@aapt.net.au>
Message-ID: <CAC2Zdp0RmsWhqpkFn_AY8=tJ_dvs8KyjcijFc3yLaSB1t5S+HA@mail.gmail.com>

This isn't concurrency related, but since we're talking about reordering,
I've sometimes wondered what in the spec prevents the compiler from turning
this:

    long start = System.nanoTime();
    someMethod();
    long end = System.nanoTime();
    long time = end - start;

into the much less useful:

    long start = System.nanoTime();
    long end = System.nanoTime();
    someMethod();
    long time = end - start;

or even:

    long end = System.nanoTime();
    long start = System.nanoTime();
    someMethod();
    long time = end - start;

Obviously it's important that the actions not be reordered, but what
prevents it? More generally, is there any definition in the JLS of what
sorts of actions can and can't be reordered within a thread? I've looked a
few times and have never found anything. Is there some sort of reordering
fence at IO?

On Tue, Jan 17, 2012 at 6:26 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> From a theoretical perspective actions within a thread must appear to occur
> in program order, but any reordering is allowed that would not be visible
> by
> the thread. In practice without some aggressive inlining, and only if
> intervening functions are trivially small, it is not likely that any
> instructions from clear() would be reordered with any from put(). A
> compiler
> is not going to reorder two method calls unless it can ascertain it is
> correct to do so - which in general it can not do.
>
> David Holmes
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nader
> > Aeinehchi
> > Sent: Wednesday, 18 January 2012 9:12 AM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: [concurrency-interest] Question about re-ordering
> >
> >
> > Hi
> >
> > In the following, a listener is notified by some configurator.  Question
> > is whether a re-ordering may be enforced by compiler on the operations
> > on "stateMap"?  Is it safe to assume that stateMap.clear() is always run
> > before stateMap.put within the same thread?
> >
> >      public void notify(ConfigurationEvent event) {
> >          if (event.getType() == AbstractFileConfiguration.EVENT_RELOAD) {
> >              logger.warn("log something");
> >
> >              stateMap.clear();
> >
> >              List<SubnodeConfiguration> configurations =
> > xmlConfiguration.configurationsAt("*");
> >              for (SubnodeConfiguration subnodeConfiguration :
> > configurations) {
> >
> > stateMap.put(subnodeConfiguration.getString("@configurationId"), new
> > State(subnodeConfiguration));
> >              }
> >          }
> >      }
> >
> > Thanks
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/2790bec3/attachment.html>

From vitalyd at gmail.com  Tue Jan 17 23:17:18 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 23:17:18 -0500
Subject: [concurrency-interest] Question about re-ordering
In-Reply-To: <CAC2Zdp0RmsWhqpkFn_AY8=tJ_dvs8KyjcijFc3yLaSB1t5S+HA@mail.gmail.com>
References: <4F16002E.8090405@aeinehchi.com>
	<NFBBKALFDCPFIDBNKAPCAEIFJCAA.davidcholmes@aapt.net.au>
	<CAC2Zdp0RmsWhqpkFn_AY8=tJ_dvs8KyjcijFc3yLaSB1t5S+HA@mail.gmail.com>
Message-ID: <CAHjP37FuSfmx=A31hJEsx5fSw52zSioeSbZE5MT1jHNeRQy5-Q@mail.gmail.com>

Hi Yuval,

As David mentioned, in general compiler is free to reorder instructions
such that within same thread you cannot observe the difference (code
behaves in the same manner as if it left it alone, i.e. sequential
execution as-written); compiler would have to "prove" that it's the case,
however, in order to do that.  In your example, changing that sequence
changes sequential behavior, so it cannot do that reordering.

Vitaly

On Tue, Jan 17, 2012 at 10:42 PM, Yuval Shavit <yshavit at akiban.com> wrote:

> This isn't concurrency related, but since we're talking about reordering,
> I've sometimes wondered what in the spec prevents the compiler from turning
> this:
>
>     long start = System.nanoTime();
>     someMethod();
>     long end = System.nanoTime();
>     long time = end - start;
>
> into the much less useful:
>
>     long start = System.nanoTime();
>     long end = System.nanoTime();
>     someMethod();
>     long time = end - start;
>
> or even:
>
>     long end = System.nanoTime();
>     long start = System.nanoTime();
>     someMethod();
>     long time = end - start;
>
> Obviously it's important that the actions not be reordered, but what
> prevents it? More generally, is there any definition in the JLS of what
> sorts of actions can and can't be reordered within a thread? I've looked a
> few times and have never found anything. Is there some sort of reordering
> fence at IO?
>
> On Tue, Jan 17, 2012 at 6:26 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>> From a theoretical perspective actions within a thread must appear to
>> occur
>> in program order, but any reordering is allowed that would not be visible
>> by
>> the thread. In practice without some aggressive inlining, and only if
>> intervening functions are trivially small, it is not likely that any
>> instructions from clear() would be reordered with any from put(). A
>> compiler
>> is not going to reorder two method calls unless it can ascertain it is
>> correct to do so - which in general it can not do.
>>
>> David Holmes
>>
>> > -----Original Message-----
>> > From: concurrency-interest-bounces at cs.oswego.edu
>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nader
>> > Aeinehchi
>> > Sent: Wednesday, 18 January 2012 9:12 AM
>> > To: concurrency-interest at cs.oswego.edu
>> > Subject: [concurrency-interest] Question about re-ordering
>> >
>> >
>> > Hi
>> >
>> > In the following, a listener is notified by some configurator.  Question
>> > is whether a re-ordering may be enforced by compiler on the operations
>> > on "stateMap"?  Is it safe to assume that stateMap.clear() is always run
>> > before stateMap.put within the same thread?
>> >
>> >      public void notify(ConfigurationEvent event) {
>> >          if (event.getType() == AbstractFileConfiguration.EVENT_RELOAD)
>> {
>> >              logger.warn("log something");
>> >
>> >              stateMap.clear();
>> >
>> >              List<SubnodeConfiguration> configurations =
>> > xmlConfiguration.configurationsAt("*");
>> >              for (SubnodeConfiguration subnodeConfiguration :
>> > configurations) {
>> >
>> > stateMap.put(subnodeConfiguration.getString("@configurationId"), new
>> > State(subnodeConfiguration));
>> >              }
>> >          }
>> >      }
>> >
>> > Thanks
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/0632afd5/attachment-0001.html>

From vitalyd at gmail.com  Tue Jan 17 23:19:42 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 23:19:42 -0500
Subject: [concurrency-interest] Question about re-ordering
In-Reply-To: <CAHjP37FuSfmx=A31hJEsx5fSw52zSioeSbZE5MT1jHNeRQy5-Q@mail.gmail.com>
References: <4F16002E.8090405@aeinehchi.com>
	<NFBBKALFDCPFIDBNKAPCAEIFJCAA.davidcholmes@aapt.net.au>
	<CAC2Zdp0RmsWhqpkFn_AY8=tJ_dvs8KyjcijFc3yLaSB1t5S+HA@mail.gmail.com>
	<CAHjP37FuSfmx=A31hJEsx5fSw52zSioeSbZE5MT1jHNeRQy5-Q@mail.gmail.com>
Message-ID: <CAHjP37HEPF06DkOdhHZA7boxY=ApCj4oVztk1zLQkz1V8AbUuQ@mail.gmail.com>

Also, forgot to mention -- an optimizing compiler is an implementation
detail.  You can have a JVM that does no optimizations -- code executes as
you wrote it -- so I doubt (might be wrong though) that something like JLS
would talk about compiler optimizations.

Regards

On Tue, Jan 17, 2012 at 11:17 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Hi Yuval,
>
> As David mentioned, in general compiler is free to reorder instructions
> such that within same thread you cannot observe the difference (code
> behaves in the same manner as if it left it alone, i.e. sequential
> execution as-written); compiler would have to "prove" that it's the case,
> however, in order to do that.  In your example, changing that sequence
> changes sequential behavior, so it cannot do that reordering.
>
> Vitaly
>
>
> On Tue, Jan 17, 2012 at 10:42 PM, Yuval Shavit <yshavit at akiban.com> wrote:
>
>> This isn't concurrency related, but since we're talking about reordering,
>> I've sometimes wondered what in the spec prevents the compiler from turning
>> this:
>>
>>     long start = System.nanoTime();
>>     someMethod();
>>     long end = System.nanoTime();
>>     long time = end - start;
>>
>> into the much less useful:
>>
>>     long start = System.nanoTime();
>>     long end = System.nanoTime();
>>     someMethod();
>>     long time = end - start;
>>
>> or even:
>>
>>     long end = System.nanoTime();
>>     long start = System.nanoTime();
>>     someMethod();
>>     long time = end - start;
>>
>> Obviously it's important that the actions not be reordered, but what
>> prevents it? More generally, is there any definition in the JLS of what
>> sorts of actions can and can't be reordered within a thread? I've looked a
>> few times and have never found anything. Is there some sort of reordering
>> fence at IO?
>>
>> On Tue, Jan 17, 2012 at 6:26 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>>
>>> From a theoretical perspective actions within a thread must appear to
>>> occur
>>> in program order, but any reordering is allowed that would not be
>>> visible by
>>> the thread. In practice without some aggressive inlining, and only if
>>> intervening functions are trivially small, it is not likely that any
>>> instructions from clear() would be reordered with any from put(). A
>>> compiler
>>> is not going to reorder two method calls unless it can ascertain it is
>>> correct to do so - which in general it can not do.
>>>
>>> David Holmes
>>>
>>> > -----Original Message-----
>>> > From: concurrency-interest-bounces at cs.oswego.edu
>>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nader
>>> > Aeinehchi
>>> > Sent: Wednesday, 18 January 2012 9:12 AM
>>> > To: concurrency-interest at cs.oswego.edu
>>> > Subject: [concurrency-interest] Question about re-ordering
>>> >
>>> >
>>> > Hi
>>> >
>>> > In the following, a listener is notified by some configurator.
>>>  Question
>>> > is whether a re-ordering may be enforced by compiler on the operations
>>> > on "stateMap"?  Is it safe to assume that stateMap.clear() is always
>>> run
>>> > before stateMap.put within the same thread?
>>> >
>>> >      public void notify(ConfigurationEvent event) {
>>> >          if (event.getType() ==
>>> AbstractFileConfiguration.EVENT_RELOAD) {
>>> >              logger.warn("log something");
>>> >
>>> >              stateMap.clear();
>>> >
>>> >              List<SubnodeConfiguration> configurations =
>>> > xmlConfiguration.configurationsAt("*");
>>> >              for (SubnodeConfiguration subnodeConfiguration :
>>> > configurations) {
>>> >
>>> > stateMap.put(subnodeConfiguration.getString("@configurationId"), new
>>> > State(subnodeConfiguration));
>>> >              }
>>> >          }
>>> >      }
>>> >
>>> > Thanks
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Vitaly
> 617-548-7007 (mobile)
>



-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/2dcc39bd/attachment.html>

From vitalyd at gmail.com  Tue Jan 17 23:33:13 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Jan 2012 23:33:13 -0500
Subject: [concurrency-interest] Question about re-ordering
In-Reply-To: <CAHjP37HEPF06DkOdhHZA7boxY=ApCj4oVztk1zLQkz1V8AbUuQ@mail.gmail.com>
References: <4F16002E.8090405@aeinehchi.com>
	<NFBBKALFDCPFIDBNKAPCAEIFJCAA.davidcholmes@aapt.net.au>
	<CAC2Zdp0RmsWhqpkFn_AY8=tJ_dvs8KyjcijFc3yLaSB1t5S+HA@mail.gmail.com>
	<CAHjP37FuSfmx=A31hJEsx5fSw52zSioeSbZE5MT1jHNeRQy5-Q@mail.gmail.com>
	<CAHjP37HEPF06DkOdhHZA7boxY=ApCj4oVztk1zLQkz1V8AbUuQ@mail.gmail.com>
Message-ID: <CAHjP37GQMOKa-TGUJuSEsFfbf3M29dw2ja5zc_a5rynD5hhdaQ@mail.gmail.com>

And I just realized that maybe your question here is specifically about
System.nanoTime() as at the surface one may think that calling someMethod
is independent of the two nanoTime() calls and thus can be reordered.  If
we imagine that System.nanoTime() just read some non-volatile static
variable, then it certainly can change things around and inline the read of
the static variable, then inline someMethod, see that they're independent,
and shift things around.  In this case, however, nanoTime() is a VM call,
which I have a feeling serves as a compiler barrier in general (David can
probably confirm/deny that).  In addition, compiler has to inline the
method in order to see its guts before it knows that it can safely reorder
instructions.  If there's no inlining, I don't think it will move things
around as obviously it cannot "prove" that it's safe (e.g. that call may
have side-effects that it doesn't know about).

On Tue, Jan 17, 2012 at 11:19 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Also, forgot to mention -- an optimizing compiler is an implementation
> detail.  You can have a JVM that does no optimizations -- code executes as
> you wrote it -- so I doubt (might be wrong though) that something like JLS
> would talk about compiler optimizations.
>
> Regards
>
>
> On Tue, Jan 17, 2012 at 11:17 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> Hi Yuval,
>>
>> As David mentioned, in general compiler is free to reorder instructions
>> such that within same thread you cannot observe the difference (code
>> behaves in the same manner as if it left it alone, i.e. sequential
>> execution as-written); compiler would have to "prove" that it's the case,
>> however, in order to do that.  In your example, changing that sequence
>> changes sequential behavior, so it cannot do that reordering.
>>
>> Vitaly
>>
>>
>> On Tue, Jan 17, 2012 at 10:42 PM, Yuval Shavit <yshavit at akiban.com>wrote:
>>
>>> This isn't concurrency related, but since we're talking about
>>> reordering, I've sometimes wondered what in the spec prevents the compiler
>>> from turning this:
>>>
>>>     long start = System.nanoTime();
>>>     someMethod();
>>>     long end = System.nanoTime();
>>>     long time = end - start;
>>>
>>> into the much less useful:
>>>
>>>     long start = System.nanoTime();
>>>     long end = System.nanoTime();
>>>     someMethod();
>>>     long time = end - start;
>>>
>>> or even:
>>>
>>>     long end = System.nanoTime();
>>>     long start = System.nanoTime();
>>>     someMethod();
>>>     long time = end - start;
>>>
>>> Obviously it's important that the actions not be reordered, but what
>>> prevents it? More generally, is there any definition in the JLS of what
>>> sorts of actions can and can't be reordered within a thread? I've looked a
>>> few times and have never found anything. Is there some sort of reordering
>>> fence at IO?
>>>
>>> On Tue, Jan 17, 2012 at 6:26 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>>>
>>>> From a theoretical perspective actions within a thread must appear to
>>>> occur
>>>> in program order, but any reordering is allowed that would not be
>>>> visible by
>>>> the thread. In practice without some aggressive inlining, and only if
>>>> intervening functions are trivially small, it is not likely that any
>>>> instructions from clear() would be reordered with any from put(). A
>>>> compiler
>>>> is not going to reorder two method calls unless it can ascertain it is
>>>> correct to do so - which in general it can not do.
>>>>
>>>> David Holmes
>>>>
>>>> > -----Original Message-----
>>>> > From: concurrency-interest-bounces at cs.oswego.edu
>>>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nader
>>>> > Aeinehchi
>>>> > Sent: Wednesday, 18 January 2012 9:12 AM
>>>> > To: concurrency-interest at cs.oswego.edu
>>>> > Subject: [concurrency-interest] Question about re-ordering
>>>> >
>>>> >
>>>> > Hi
>>>> >
>>>> > In the following, a listener is notified by some configurator.
>>>>  Question
>>>> > is whether a re-ordering may be enforced by compiler on the operations
>>>> > on "stateMap"?  Is it safe to assume that stateMap.clear() is always
>>>> run
>>>> > before stateMap.put within the same thread?
>>>> >
>>>> >      public void notify(ConfigurationEvent event) {
>>>> >          if (event.getType() ==
>>>> AbstractFileConfiguration.EVENT_RELOAD) {
>>>> >              logger.warn("log something");
>>>> >
>>>> >              stateMap.clear();
>>>> >
>>>> >              List<SubnodeConfiguration> configurations =
>>>> > xmlConfiguration.configurationsAt("*");
>>>> >              for (SubnodeConfiguration subnodeConfiguration :
>>>> > configurations) {
>>>> >
>>>> > stateMap.put(subnodeConfiguration.getString("@configurationId"), new
>>>> > State(subnodeConfiguration));
>>>> >              }
>>>> >          }
>>>> >      }
>>>> >
>>>> > Thanks
>>>> > _______________________________________________
>>>> > Concurrency-interest mailing list
>>>> > Concurrency-interest at cs.oswego.edu
>>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> >
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> --
>> Vitaly
>> 617-548-7007 (mobile)
>>
>
>
>
> --
> Vitaly
> 617-548-7007 (mobile)
>



-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/2686abaf/attachment-0001.html>

From joe.bowbeer at gmail.com  Wed Jan 18 00:03:33 2012
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 17 Jan 2012 21:03:33 -0800
Subject: [concurrency-interest] Question about re-ordering
In-Reply-To: <CAHjP37HEPF06DkOdhHZA7boxY=ApCj4oVztk1zLQkz1V8AbUuQ@mail.gmail.com>
References: <4F16002E.8090405@aeinehchi.com>
	<NFBBKALFDCPFIDBNKAPCAEIFJCAA.davidcholmes@aapt.net.au>
	<CAC2Zdp0RmsWhqpkFn_AY8=tJ_dvs8KyjcijFc3yLaSB1t5S+HA@mail.gmail.com>
	<CAHjP37FuSfmx=A31hJEsx5fSw52zSioeSbZE5MT1jHNeRQy5-Q@mail.gmail.com>
	<CAHjP37HEPF06DkOdhHZA7boxY=ApCj4oVztk1zLQkz1V8AbUuQ@mail.gmail.com>
Message-ID: <CAHzJPEr2rat8-2XP0=UMV44xwYGzn9QB=Yjag-F5bvNwo+=0Yg@mail.gmail.com>

The JLS does mention compiler (and microprocessor) optimizations:

http://java.sun.com/docs/books/jls/third_edition/html/memory.html

The JLS also says that the execution of data race free programs is
sequentially consistent, which rules out any observable reordering of
statements in the body of a method.

Joe

On Tue, Jan 17, 2012 at 8:19 PM, Vitaly Davidovich wrote:

> Also, forgot to mention -- an optimizing compiler is an implementation
> detail.  You can have a JVM that does no optimizations -- code executes as
> you wrote it -- so I doubt (might be wrong though) that something like JLS
> would talk about compiler optimizations.
>
> Regards
>
>
> On Tue, Jan 17, 2012 at 11:17 PM, Vitaly Davidovich wrote:
>
>> Hi Yuval,
>>
>> As David mentioned, in general compiler is free to reorder instructions
>> such that within same thread you cannot observe the difference (code
>> behaves in the same manner as if it left it alone, i.e. sequential
>> execution as-written); compiler would have to "prove" that it's the case,
>> however, in order to do that.  In your example, changing that sequence
>> changes sequential behavior, so it cannot do that reordering.
>>
>> Vitaly
>>
>>
>> On Tue, Jan 17, 2012 at 10:42 PM, Yuval Shavit wrote:
>>
>>> This isn't concurrency related, but since we're talking about
>>> reordering, I've sometimes wondered what in the spec prevents the compiler
>>> from turning this:
>>>
>>>     long start = System.nanoTime();
>>>     someMethod();
>>>     long end = System.nanoTime();
>>>     long time = end - start;
>>>
>>> into the much less useful:
>>>
>>>     long start = System.nanoTime();
>>>     long end = System.nanoTime();
>>>     someMethod();
>>>     long time = end - start;
>>>
>>> or even:
>>>
>>>     long end = System.nanoTime();
>>>     long start = System.nanoTime();
>>>     someMethod();
>>>     long time = end - start;
>>>
>>> Obviously it's important that the actions not be reordered, but what
>>> prevents it? More generally, is there any definition in the JLS of what
>>> sorts of actions can and can't be reordered within a thread? I've looked a
>>> few times and have never found anything. Is there some sort of reordering
>>> fence at IO?
>>>
>>> On Tue, Jan 17, 2012 at 6:26 PM, David Holmes wrote:
>>>
>>>> From a theoretical perspective actions within a thread must appear to
>>>> occur
>>>> in program order, but any reordering is allowed that would not be
>>>> visible by
>>>> the thread. In practice without some aggressive inlining, and only if
>>>> intervening functions are trivially small, it is not likely that any
>>>> instructions from clear() would be reordered with any from put(). A
>>>> compiler
>>>> is not going to reorder two method calls unless it can ascertain it is
>>>> correct to do so - which in general it can not do.
>>>>
>>>> David Holmes
>>>>
>>>> > -----Original Message-----
>>>> > From: concurrency-interest-bounces at cs.oswego.edu
>>>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nader
>>>> > Aeinehchi
>>>> > Sent: Wednesday, 18 January 2012 9:12 AM
>>>> > To: concurrency-interest at cs.oswego.edu
>>>> > Subject: [concurrency-interest] Question about re-ordering
>>>> >
>>>> >
>>>> > Hi
>>>> >
>>>> > In the following, a listener is notified by some configurator.
>>>>  Question
>>>> > is whether a re-ordering may be enforced by compiler on the operations
>>>> > on "stateMap"?  Is it safe to assume that stateMap.clear() is always
>>>> run
>>>> > before stateMap.put within the same thread?
>>>> >
>>>> >      public void notify(ConfigurationEvent event) {
>>>> >          if (event.getType() ==
>>>> AbstractFileConfiguration.EVENT_RELOAD) {
>>>> >              logger.warn("log something");
>>>> >
>>>> >              stateMap.clear();
>>>> >
>>>> >              List<SubnodeConfiguration> configurations =
>>>> > xmlConfiguration.configurationsAt("*");
>>>> >              for (SubnodeConfiguration subnodeConfiguration :
>>>> > configurations) {
>>>> >
>>>> > stateMap.put(subnodeConfiguration.getString("@configurationId"), new
>>>> > State(subnodeConfiguration));
>>>> >              }
>>>> >          }
>>>> >      }
>>>> >
>>>> > Thanks
>>>>
>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120117/e3a68755/attachment.html>

From radhakrishnan.mohan at gmail.com  Wed Jan 18 01:48:47 2012
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Wed, 18 Jan 2012 12:18:47 +0530
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAHjP37G5FZHdEzQ44xrbphz81BrHZxnLbTQe+zn2YVkEoNppYw@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOoXFP-8CNpp3+NHNcWfpnJMYthkiqFPSs_ssEJ9Y_VN-7R7ow@mail.gmail.com>
	<CAHjP37G5FZHdEzQ44xrbphz81BrHZxnLbTQe+zn2YVkEoNppYw@mail.gmail.com>
Message-ID: <CAOoXFP-oVRpCYw6c2ixRb0CbRLr9ctnjSO4r0t+fg1d54UeHzA@mail.gmail.com>

I did come across 'False Sharing' by reading other threads and tried
Oracle Studio analyzer but its
project lead pointed out that its identification of false sharing is
targeted for Sparc systems.
Java support also did not seem to be there.

I also read that this studio uses HW counters to show addresses and
cache line contention.


Again by reading other threads :-) I understand JIT also already uses
HW counters. If annotations are introduced then concurrency might
become harder to understand. It is like Ruslan's GC example.That is my
humble thought.

Mohan

From cheremin at gmail.com  Wed Jan 18 04:23:49 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Wed, 18 Jan 2012 12:23:49 +0300
Subject: [concurrency-interest] padding in Exchanger
In-Reply-To: <CAHzJPEoF5cZXJaRoBpZkr=OCStezL6QA-CzEN=s-iQcDozCzEA@mail.gmail.com>
References: <CAOwENi+AxC=YNWyOW5Ohz18WeHdrN3Q0WK1BXVJxceb7=RSR=g@mail.gmail.com>
	<4F1560EF.7050206@cs.oswego.edu>
	<CAOwENiJjMGQ-KUuET3GYqxH4vCMnwRRKZtbJ_Ws4Suu0__ZWaw@mail.gmail.com>
	<4F156C24.6020403@cs.oswego.edu>
	<CAOwENi+0CJizmVwnmuwTS2h724UKGNTSDNpSZuoitCvJFhycHA@mail.gmail.com>
	<CAHjP37H_Lfj3VMM72hgg52-1e6dpRCvxcezF+47VETfBDDLUnw@mail.gmail.com>
	<CAOwENiKmkaOd_Sn1_Ktf2m96+BnO9+nPqocodzeWV4RfqawNiA@mail.gmail.com>
	<CAHjP37EmGehGyb0P7jzNwNPojSxBbQxT1p9RyzcDQU_=_O86Sg@mail.gmail.com>
	<4F15AE4F.7010700@oracle.com>
	<CAOwENiJnQnO9QLKq-idEOcu0RQtAv4dOpE4rSu4gHutqQ0GuXA@mail.gmail.com>
	<4F15B387.1090706@oracle.com>
	<CAHjP37EMO=eKrJmjrb09WMGxrHkQ9fykdDOMoqiU=G4hKQtDOA@mail.gmail.com>
	<4F15BF08.5060909@oracle.com> <4F15CDD6.4040006@briangoetz.com>
	<CAHzJPEoF5cZXJaRoBpZkr=OCStezL6QA-CzEN=s-iQcDozCzEA@mail.gmail.com>
Message-ID: <CAOwENiJ0QZ4CG-NQBfWwW8okK-V_+0RcLOWM64TvmZNkevAbuQ@mail.gmail.com>

I also like this idea -- because even if it support by JVM will be
delayed, it is not a hard task to make simple annotation processor,
which will implement @Contended via padding. And -- in case of run
time enchancer -- it is even possible to make some native call to find
out actual cache line size on current arch.

Such solution, although not ideal one, at least stop programmers to
invent it's own padding methods.

2012/1/18 Joe Bowbeer <joe.bowbeer at gmail.com>:
> I like the generic annotation idea.
>
> Note that optimizer/obfuscator/compactor tools that are commonly used in
> some tool chains (e.g., proguard w/Android), will need to be instructed to
> preserve these annotations.
>
> On Tue, Jan 17, 2012 at 11:36 AM, Brian Goetz wrote:
>>
>> Having the VM automagically figure this out would be great, but it seems a
>> cheaper / faster-to-market solution to give developers the ability to detect
>> false sharing during development and testing and on the basis of such
>> observations give padding hints (say, with annotations) that the VM could
>> use or ignore as it saw fit. ?False sharing will happen with a relatively
>> few concurrent building blocks (like, say, Exchanger) and maintainers of
>> such classes are probably (willing to be) on the lookout for it anyway.
>>
>>
>> On 1/17/2012 1:33 PM, Nathan Reynolds wrote:
>>>
>>> If an application is contending on a cache line, then the processors
>>> will look 100% busy but very little work will get done. Fixing the
>>> contention will almost always get huge gains.
>>>
>>> Let's say the contention is due to 2 objects being next to each other.
>>> The solution is to separate them by moving 1 object to another place in
>>> the heap. Moving an object in the heap is fairly cheap. GC does it a lot!
>>>
>>> Let's say the contention is due to 2 fields being next to each other.
>>> The solution is to separate them by rearranging the fields. This is
>>> probably expensive. Once that is done, the contention will go away but
>>> the memory will increase. Very few locks actually contend. Even fewer
>>> fields contend on cache lines. So, the memory impact should be fairly
>>> small and probably insignificant.
>>>
>>> Nathan Reynolds
>>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>>
>>> Consulting Member of Technical Staff | 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>
>>>
>>> On 1/17/2012 11:03 AM, Vitaly Davidovich wrote:
>>>>
>>>>
>>>> Doing dynamic re-layout and contention adjustment seems nice in
>>>> thought but how practical is that? I can't see how that would be cheap
>>>> enough where it's worth the cost. What if the app goes through phases
>>>> of contention? Initially high but then no sharing - would the bloated
>>>> object layout be worth it at that point? Seems like this is a place
>>>> where explicit developer instructions is better than heuristics with
>>>> potentially expensive consequences.
>>>>
>>>> Sent from my phone
>>>>
>>>> On Jan 17, 2012 12:44 PM, "Nathan Reynolds"
>>>> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>
>>>> ? ?Assuming that the JVM can optimize for true and false sharing (and
>>>> ? ?that is a big assumption at the moment), then you can focus your
>>>> ? ?time on writing useful code. Furthermore, optimizing for true and
>>>> ? ?false sharing will never be able to fix actual data contention. We
>>>> ? ?still need clever ways of sharing data without bottlenecks.
>>>>
>>>> ? ?Nathan Reynolds
>>>> ? ?<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>>> ? ?Consulting Member of Technical Staff | 602.333.9091
>>>> <tel:602.333.9091>
>>>> ? ?Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>>> Technology
>>>>
>>>>
>>>> ? ?On 1/17/2012 10:29 AM, Ruslan Cheremin wrote:
>>>>>
>>>>> ? ?You made my day. Few months ago I was dreaming (in my blog) about
>>>>> ? ?complexity of false sharing prevention with padding. And come to two
>>>>> ? ?options, one better another. First one was @PreventFalseSharing
>>>>> ? ?annotation, next was atomatically contention detection and
>>>>> relocation
>>>>> ? ?of contended objects by JIT. Readers of my blog soon pointed me to
>>>>> ? ?@Contended annotation. And now you telling the second -- the best --
>>>>> ? ?option is also being explored!
>>>>>
>>>>> ? ?Just want to ask -- if all good things will be done by JIT -- what I
>>>>> ? ?will be paid for?
>>>>>
>>>>> ? ?2012/1/17 Nathan Reynolds<nathan.reynolds at oracle.com>
>>>>> ?<mailto:nathan.reynolds at oracle.com>:
>>>>>>
>>>>>> ? ?It would be nice if the processor could effectively tell the JVM
>>>>>> that false
>>>>>> ? ?sharing is happening. ? ?It would be nice if the JVM could respond
>>>>>> by moving
>>>>>> ? ?objects within the heap or fields with the class to avoid false
>>>>>> sharing.
>>>>>> ? ?Thus, we don't have to pad or worry about placing @Contended or
>>>>>> other
>>>>>> ? ?attributes into the class.
>>>>>>
>>>>>> ? ?Intel was looking into to optimizing for true and false sharing.
>>>>>> ?They had a
>>>>>> ? ?prototype that worked but required restarting the JVM. ? ?Oracle
>>>>>> was looking
>>>>>> ? ?into dynamically relayout fields in objects. ? ?I haven't heard
>>>>>> anything from
>>>>>> ? ?either group for a while... ? ?I haven't asked either. ? ?*IF* a
>>>>>> solution
>>>>>> ? ?becomes available, then it will be a while. ? ?This is a very
>>>>>> difficult thing
>>>>>> ? ?to do.
>>>>>>
>>>>>> ? ?Nathan Reynolds | Consulting Member of Technical Staff
>>>>>> |602.333.9091 ?<tel:602.333.9091>
>>>>>>
>>>>>> ? ?Oracle PSR Engineering | Server Technology
>>>>>>
>>>>>> ? ?On 1/17/2012 9:35 AM, Vitaly Davidovich wrote:
>>>>>>
>>>>>> ? ?OK I see what you mean now. ? ?I imagine @Contended will be used
>>>>>> with fields
>>>>>> ? ?rather than classes so when the JVM lays out an instance of the
>>>>>> class I
>>>>>> ? ?assume it will do two-sided padding on the contended field if
>>>>>> required or if
>>>>>> ? ?natural layout is such that prior fields already fill up a cache
>>>>>> line then
>>>>>> ? ?only one sided is needed.
>>>>>>
>>>>>> ? ?Sent from my phone
>>>>>>
>>>>>> ? ?On Jan 17, 2012 11:27 AM, "Ruslan Cheremin"<cheremin at gmail.com>
>>>>>> ?<mailto:cheremin at gmail.com> ?wrote:
>>>>>>>
>>>>>>> ? ?2012/1/17 Vitaly Davidovich<vitalyd at gmail.com>
>>>>>>> ?<mailto:vitalyd at gmail.com>:
>>>>>>>
>>>>>>>> ? ?I think it's semantics - if you sometimes allocate with 64/128
>>>>>>>> byte
>>>>>>>> ? ?alignment then if your object is smaller than 64/128 the rest of
>>>>>>>> the
>>>>>>>> ? ?space
>>>>>>>> ? ?is effectively padding.
>>>>>>>
>>>>>>> ? ?Agree. But in case of alignment you lose sense of "one-side" or
>>>>>>> "two
>>>>>>> ? ?side" padding -- you do not need "two side padding", you just make
>>>>>>> ? ?sure object align on cache line boundary.
>>>>>>>
>>>>>>> ? ?Actually I was asked is my understanding of how @Contended
>>>>>>> supposed to
>>>>>>> ? ?be implemented is right?
>>>>>>>
>>>>>>>> ? ?Or are you saying you want an @Alignment annotation
>>>>>>>> ? ?instead so it's more general? What other uses of custom alignment
>>>>>>>> do you
>>>>>>>> ? ?envision? Java is too high-level ? ?and the underlying
>>>>>>>> hardware/platform
>>>>>>>> ? ?too
>>>>>>>> ? ?abstracted away for a general purpose custom alignment hint,
>>>>>>>> IMHO.
>>>>>>>
>>>>>>> ? ?No, I do not want such ugly thing to happen with java! It's enough
>>>>>>> C
>>>>>>> ? ?for such things...
>>>>>>>
>>>>>>>
>>>>>>>> ? ?Sent from my phone
>>>>>>>>
>>>>>>>> ? ?On Jan 17, 2012 10:56 AM, "Ruslan Cheremin"<cheremin at gmail.com>
>>>>>>>> ?<mailto:cheremin at gmail.com> ?wrote:
>>>>>>>>>>
>>>>>>>>>> ? ?Yes. As a practical matter though, until an @Contended
>>>>>>>>>> attribute
>>>>>>>>>> ? ?or something like it is supported across JVMS (see list
>>>>>>>>>> archives for
>>>>>>>>>> ? ?discussion), you cannot arrange reliable two-sided padding
>>>>>>>>>> ? ?for objects with mixed field types (ints, longs, refs that may
>>>>>>>>>> be
>>>>>>>>>> ? ?either 32 or 64 bits, etc), so one-sided is the best you can
>>>>>>>>>> do.
>>>>>>>>>
>>>>>>>>> ? ?By the way -- I was not thinking about @Contended as "make
>>>>>>>>> padding for
>>>>>>>>> ? ?me". It seems for me like padding is only dirty hack, since
>>>>>>>>> nothing
>>>>>>>>> ? ?better available. If I would control memory allocation (like JVM
>>>>>>>>> does)
>>>>>>>>> ? ?I just can allocate @Contended objects on 64 (128... etc) bytes
>>>>>>>>> ? ?boundary. I do not have to "pad" them -- nor both, nor one side.
>>>>>>>>> And I
>>>>>>>>> ? ?suppose @Contended implementation to do exactly this -- "use
>>>>>>>>> special
>>>>>>>>> ? ?allocator for objects of that type, which allocate them on cache
>>>>>>>>> line
>>>>>>>>> ? ?boundary"
>>>>>>>>>
>>>>>>>>> ? ?Am I wrong here?
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>> ? ?-Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From raphfrk at gmail.com  Wed Jan 18 05:42:19 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Wed, 18 Jan 2012 10:42:19 +0000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <CAHjP37Hp9oYkC5QfQ4ouPDs+Mi-ELjkaSbzCQgs-TtjGaQV8fA@mail.gmail.com>
References: <CAHjP37GY2a1fgsOjX2SecFYe2aDyqSGzV2rY4Yz2d7P8uROxPA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEIFJCAA.davidcholmes@aapt.net.au>
	<CAHjP37Hp9oYkC5QfQ4ouPDs+Mi-ELjkaSbzCQgs-TtjGaQV8fA@mail.gmail.com>
Message-ID: <CAN1xFdrRVEQxXBfnEqn0kMrTqZi9urZs1aRVYvQ3-BuhrSDT=g@mail.gmail.com>

Thanks all, for the info.

So, for a cache, if there is a write to a volatile variable, the only
requirement is that you flush the volatile to main memory last.

So, if the core writes

A) writes before volatile write
B) write to volatile
C) writes after volatile write

Then the cache can be flushed in the order A and then C and then B?
So, a core could wait before writing a volatile to main memory, until
a second write to that cache slot forces a flush (and writing a second
value to the volatile itself doesn't force a flush)?

Similarly for a read, reading from a volatile doesn't actually require
invaliding the read cache.  However, if the volatile is not in the
cache, then the read cache must be flushed before reading the volatile
from main memory?

On Tue, Jan 17, 2012 at 11:45 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> That's true, you're right - I didn't think of that as it does seem silly but
> is allowed.

From khilan at doc.ic.ac.uk  Wed Jan 18 06:05:47 2012
From: khilan at doc.ic.ac.uk (Khilan Gudka)
Date: Wed, 18 Jan 2012 11:05:47 +0000
Subject: [concurrency-interest] Question about re-ordering
In-Reply-To: <CAHzJPEr2rat8-2XP0=UMV44xwYGzn9QB=Yjag-F5bvNwo+=0Yg@mail.gmail.com>
References: <4F16002E.8090405@aeinehchi.com>
	<NFBBKALFDCPFIDBNKAPCAEIFJCAA.davidcholmes@aapt.net.au>
	<CAC2Zdp0RmsWhqpkFn_AY8=tJ_dvs8KyjcijFc3yLaSB1t5S+HA@mail.gmail.com>
	<CAHjP37FuSfmx=A31hJEsx5fSw52zSioeSbZE5MT1jHNeRQy5-Q@mail.gmail.com>
	<CAHjP37HEPF06DkOdhHZA7boxY=ApCj4oVztk1zLQkz1V8AbUuQ@mail.gmail.com>
	<CAHzJPEr2rat8-2XP0=UMV44xwYGzn9QB=Yjag-F5bvNwo+=0Yg@mail.gmail.com>
Message-ID: <CAEtTSAZx2Ac393oLjntFdhzyJjW2yYCVzBk3yUYu6Q+SzGeMzQ@mail.gmail.com>

Hi

So is this true for all native method calls? That they will never be
re-ordered?

--
Khilan Gudka
PhD Student
Department of Computing
Imperial College London
http://www.doc.ic.ac.uk/~khilan/



On 18 January 2012 05:03, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> The JLS does mention compiler (and microprocessor) optimizations:
>
> http://java.sun.com/docs/books/jls/third_edition/html/memory.html
>
> The JLS also says that the execution of data race free programs is
> sequentially consistent, which rules out any observable reordering of
> statements in the body of a method.
>
> Joe
>
> On Tue, Jan 17, 2012 at 8:19 PM, Vitaly Davidovich wrote:
>
>> Also, forgot to mention -- an optimizing compiler is an implementation
>> detail.  You can have a JVM that does no optimizations -- code executes as
>> you wrote it -- so I doubt (might be wrong though) that something like JLS
>> would talk about compiler optimizations.
>>
>> Regards
>>
>>
>> On Tue, Jan 17, 2012 at 11:17 PM, Vitaly Davidovich wrote:
>>
>>>  Hi Yuval,
>>>
>>> As David mentioned, in general compiler is free to reorder instructions
>>> such that within same thread you cannot observe the difference (code
>>> behaves in the same manner as if it left it alone, i.e. sequential
>>> execution as-written); compiler would have to "prove" that it's the case,
>>> however, in order to do that.  In your example, changing that sequence
>>> changes sequential behavior, so it cannot do that reordering.
>>>
>>> Vitaly
>>>
>>>
>>> On Tue, Jan 17, 2012 at 10:42 PM, Yuval Shavit wrote:
>>>
>>>> This isn't concurrency related, but since we're talking about
>>>> reordering, I've sometimes wondered what in the spec prevents the compiler
>>>> from turning this:
>>>>
>>>>     long start = System.nanoTime();
>>>>     someMethod();
>>>>     long end = System.nanoTime();
>>>>     long time = end - start;
>>>>
>>>> into the much less useful:
>>>>
>>>>     long start = System.nanoTime();
>>>>     long end = System.nanoTime();
>>>>     someMethod();
>>>>     long time = end - start;
>>>>
>>>> or even:
>>>>
>>>>     long end = System.nanoTime();
>>>>     long start = System.nanoTime();
>>>>     someMethod();
>>>>     long time = end - start;
>>>>
>>>> Obviously it's important that the actions not be reordered, but what
>>>> prevents it? More generally, is there any definition in the JLS of what
>>>> sorts of actions can and can't be reordered within a thread? I've looked a
>>>> few times and have never found anything. Is there some sort of reordering
>>>> fence at IO?
>>>>
>>>> On Tue, Jan 17, 2012 at 6:26 PM, David Holmes wrote:
>>>>
>>>> From a theoretical perspective actions within a thread must appear to
>>>>> occur
>>>>> in program order, but any reordering is allowed that would not be
>>>>> visible by
>>>>> the thread. In practice without some aggressive inlining, and only if
>>>>> intervening functions are trivially small, it is not likely that any
>>>>> instructions from clear() would be reordered with any from put(). A
>>>>> compiler
>>>>> is not going to reorder two method calls unless it can ascertain it is
>>>>> correct to do so - which in general it can not do.
>>>>>
>>>>> David Holmes
>>>>>
>>>>> > -----Original Message-----
>>>>> > From: concurrency-interest-bounces at cs.oswego.edu
>>>>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>>> Nader
>>>>> > Aeinehchi
>>>>> > Sent: Wednesday, 18 January 2012 9:12 AM
>>>>> > To: concurrency-interest at cs.oswego.edu
>>>>> > Subject: [concurrency-interest] Question about re-ordering
>>>>> >
>>>>> >
>>>>> > Hi
>>>>> >
>>>>> > In the following, a listener is notified by some configurator.
>>>>>  Question
>>>>> > is whether a re-ordering may be enforced by compiler on the
>>>>> operations
>>>>> > on "stateMap"?  Is it safe to assume that stateMap.clear() is always
>>>>> run
>>>>> > before stateMap.put within the same thread?
>>>>> >
>>>>> >      public void notify(ConfigurationEvent event) {
>>>>> >          if (event.getType() ==
>>>>> AbstractFileConfiguration.EVENT_RELOAD) {
>>>>> >              logger.warn("log something");
>>>>> >
>>>>> >              stateMap.clear();
>>>>> >
>>>>> >              List<SubnodeConfiguration> configurations =
>>>>> > xmlConfiguration.configurationsAt("*");
>>>>> >              for (SubnodeConfiguration subnodeConfiguration :
>>>>> > configurations) {
>>>>> >
>>>>> > stateMap.put(subnodeConfiguration.getString("@configurationId"), new
>>>>> > State(subnodeConfiguration));
>>>>> >              }
>>>>> >          }
>>>>> >      }
>>>>> >
>>>>> > Thanks
>>>>>
>>>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120118/8b42d30e/attachment.html>

From raphfrk at gmail.com  Wed Jan 18 07:10:02 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Wed, 18 Jan 2012 12:10:02 +0000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <CAN1xFdrRVEQxXBfnEqn0kMrTqZi9urZs1aRVYvQ3-BuhrSDT=g@mail.gmail.com>
References: <CAHjP37GY2a1fgsOjX2SecFYe2aDyqSGzV2rY4Yz2d7P8uROxPA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEIFJCAA.davidcholmes@aapt.net.au>
	<CAHjP37Hp9oYkC5QfQ4ouPDs+Mi-ELjkaSbzCQgs-TtjGaQV8fA@mail.gmail.com>
	<CAN1xFdrRVEQxXBfnEqn0kMrTqZi9urZs1aRVYvQ3-BuhrSDT=g@mail.gmail.com>
Message-ID: <CAN1xFdprFn__-HpTL1GKjqbXfX4duJH-7FDcbjjYfVsS+BY=6A@mail.gmail.com>

Another question, would this work properly as a lock?

public class OptimisticLockInt {

    private int x;
    private AtomicInteger counter = new AtomicInteger(0);
    private static final int UNSTABLE = 1;

    public void set(int x)
        int oldCount = counter.getAndSet(UNSTABLE);
        this.x = x;
        counter.set(oldCount + 2);
    }

    public int get() {
        while (true)
            int oldCount = counter.getAndSet();
            int tempX = x;
            if (counter.get() == oldCount) {
                return tempX;
            }
        }
    }
}

I guess the get() method can be reordered to the following?

    public int get() {
        while (true)
            int oldCount = counter.getAndSet();
            if (counter.get() == oldCount) {
                int tempX = x;
                return tempX;
            } else {
                int tempX = x;
            }
        }
    }

If so, what is the best way to do optimistic locking?

From zhong.j.yu at gmail.com  Wed Jan 18 07:20:56 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 18 Jan 2012 06:20:56 -0600
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD22E52977C@G9W0339.americas.hpqcorp.net>
References: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEIEJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52977C@G9W0339.americas.hpqcorp.net>
Message-ID: <CACuKZqGaVgSaqCH7QHOtJ=RV1s7d-8av-wAjUO30vVPWzdcy9w@mail.gmail.com>

On Tue, Jan 17, 2012 at 5:13 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
> Another way to look at this is that compiler reordering that is only observable by programs with data races is generally allowed. ?The program below clearly has an inherent data race on b. ?If you make b volatile, the data race, and the problem, go away.

Not by the definitions in the book.

Volatile r/w can form a data race, if there is neither hb(r,w) nor
hb(w,r), which is possible when r is before w in synchronization
order.

P.S. Reading JLS,  the wording of "allowed to observe" (17.4.5) is
very misleading; it is defined only in term of happens-before order,
meaning a volatile read can be "allowed to observe" a later volatile
write.

Zhong Yu


From vitalyd at gmail.com  Wed Jan 18 08:03:14 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 18 Jan 2012 08:03:14 -0500
Subject: [concurrency-interest] Question about re-ordering
In-Reply-To: <CAHzJPEr2rat8-2XP0=UMV44xwYGzn9QB=Yjag-F5bvNwo+=0Yg@mail.gmail.com>
References: <4F16002E.8090405@aeinehchi.com>
	<NFBBKALFDCPFIDBNKAPCAEIFJCAA.davidcholmes@aapt.net.au>
	<CAC2Zdp0RmsWhqpkFn_AY8=tJ_dvs8KyjcijFc3yLaSB1t5S+HA@mail.gmail.com>
	<CAHjP37FuSfmx=A31hJEsx5fSw52zSioeSbZE5MT1jHNeRQy5-Q@mail.gmail.com>
	<CAHjP37HEPF06DkOdhHZA7boxY=ApCj4oVztk1zLQkz1V8AbUuQ@mail.gmail.com>
	<CAHzJPEr2rat8-2XP0=UMV44xwYGzn9QB=Yjag-F5bvNwo+=0Yg@mail.gmail.com>
Message-ID: <CAHjP37GBACdc50rToabBQQhT4iFKuiUSBXwfugQbCq-KQJ-T8A@mail.gmail.com>

Hi Joe,

Yes but I should've been clearer - I was referring to traditional compiler
optimizations within a single thread, not memory model/concurrency.

Regards

Sent from my phone
On Jan 18, 2012 12:05 AM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:

> The JLS does mention compiler (and microprocessor) optimizations:
>
> http://java.sun.com/docs/books/jls/third_edition/html/memory.html
>
> The JLS also says that the execution of data race free programs is
> sequentially consistent, which rules out any observable reordering of
> statements in the body of a method.
>
> Joe
>
> On Tue, Jan 17, 2012 at 8:19 PM, Vitaly Davidovich wrote:
>
>> Also, forgot to mention -- an optimizing compiler is an implementation
>> detail.  You can have a JVM that does no optimizations -- code executes as
>> you wrote it -- so I doubt (might be wrong though) that something like JLS
>> would talk about compiler optimizations.
>>
>> Regards
>>
>>
>> On Tue, Jan 17, 2012 at 11:17 PM, Vitaly Davidovich wrote:
>>
>>> Hi Yuval,
>>>
>>> As David mentioned, in general compiler is free to reorder instructions
>>> such that within same thread you cannot observe the difference (code
>>> behaves in the same manner as if it left it alone, i.e. sequential
>>> execution as-written); compiler would have to "prove" that it's the case,
>>> however, in order to do that.  In your example, changing that sequence
>>> changes sequential behavior, so it cannot do that reordering.
>>>
>>> Vitaly
>>>
>>>
>>> On Tue, Jan 17, 2012 at 10:42 PM, Yuval Shavit wrote:
>>>
>>>> This isn't concurrency related, but since we're talking about
>>>> reordering, I've sometimes wondered what in the spec prevents the compiler
>>>> from turning this:
>>>>
>>>>     long start = System.nanoTime();
>>>>     someMethod();
>>>>     long end = System.nanoTime();
>>>>     long time = end - start;
>>>>
>>>> into the much less useful:
>>>>
>>>>     long start = System.nanoTime();
>>>>     long end = System.nanoTime();
>>>>     someMethod();
>>>>     long time = end - start;
>>>>
>>>> or even:
>>>>
>>>>     long end = System.nanoTime();
>>>>     long start = System.nanoTime();
>>>>     someMethod();
>>>>     long time = end - start;
>>>>
>>>> Obviously it's important that the actions not be reordered, but what
>>>> prevents it? More generally, is there any definition in the JLS of what
>>>> sorts of actions can and can't be reordered within a thread? I've looked a
>>>> few times and have never found anything. Is there some sort of reordering
>>>> fence at IO?
>>>>
>>>> On Tue, Jan 17, 2012 at 6:26 PM, David Holmes wrote:
>>>>
>>>>> From a theoretical perspective actions within a thread must appear to
>>>>> occur
>>>>> in program order, but any reordering is allowed that would not be
>>>>> visible by
>>>>> the thread. In practice without some aggressive inlining, and only if
>>>>> intervening functions are trivially small, it is not likely that any
>>>>> instructions from clear() would be reordered with any from put(). A
>>>>> compiler
>>>>> is not going to reorder two method calls unless it can ascertain it is
>>>>> correct to do so - which in general it can not do.
>>>>>
>>>>> David Holmes
>>>>>
>>>>> > -----Original Message-----
>>>>> > From: concurrency-interest-bounces at cs.oswego.edu
>>>>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>>> Nader
>>>>> > Aeinehchi
>>>>> > Sent: Wednesday, 18 January 2012 9:12 AM
>>>>> > To: concurrency-interest at cs.oswego.edu
>>>>> > Subject: [concurrency-interest] Question about re-ordering
>>>>> >
>>>>> >
>>>>> > Hi
>>>>> >
>>>>> > In the following, a listener is notified by some configurator.
>>>>>  Question
>>>>> > is whether a re-ordering may be enforced by compiler on the
>>>>> operations
>>>>> > on "stateMap"?  Is it safe to assume that stateMap.clear() is always
>>>>> run
>>>>> > before stateMap.put within the same thread?
>>>>> >
>>>>> >      public void notify(ConfigurationEvent event) {
>>>>> >          if (event.getType() ==
>>>>> AbstractFileConfiguration.EVENT_RELOAD) {
>>>>> >              logger.warn("log something");
>>>>> >
>>>>> >              stateMap.clear();
>>>>> >
>>>>> >              List<SubnodeConfiguration> configurations =
>>>>> > xmlConfiguration.configurationsAt("*");
>>>>> >              for (SubnodeConfiguration subnodeConfiguration :
>>>>> > configurations) {
>>>>> >
>>>>> > stateMap.put(subnodeConfiguration.getString("@configurationId"), new
>>>>> > State(subnodeConfiguration));
>>>>> >              }
>>>>> >          }
>>>>> >      }
>>>>> >
>>>>> > Thanks
>>>>>
>>>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120118/4a6139e9/attachment-0001.html>

From khilan at doc.ic.ac.uk  Wed Jan 18 08:05:30 2012
From: khilan at doc.ic.ac.uk (Khilan Gudka)
Date: Wed, 18 Jan 2012 13:05:30 +0000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <CACuKZqGaVgSaqCH7QHOtJ=RV1s7d-8av-wAjUO30vVPWzdcy9w@mail.gmail.com>
References: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEIEJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52977C@G9W0339.americas.hpqcorp.net>
	<CACuKZqGaVgSaqCH7QHOtJ=RV1s7d-8av-wAjUO30vVPWzdcy9w@mail.gmail.com>
Message-ID: <CAEtTSAZJyRNmqL7s6Dr1690i5+THMyZ3JUZfd8te37B0iaCGiQ@mail.gmail.com>

Hi

I may have misunderstood the intentions here but according to the JCIP
book, if you have a single field in a thread-shared class then it's
recommended to use AtomicX types for that field rather than having a
separate lock field to protect it.

What you appear to be doing is versioning. When you perform a set, you
increase the current version and when you get, you check that the
version hasn't changed. I thought that writes to a volatile always
happened-before a read, so if the version number was updated you would
definitely see it. Hence, if set was called between the two calls to
counter.get(), you would get different values and then loop.

With regards to reordering, in JCIP 3.1.4, it says:

"When a field is declared volatile, the compiler and runtime are put
on notice that this variable are shared and that operations on it
should not be reordered with other memory operations. Given that
counter.get() reads a volatile value, this ensures that operations are
not reordered with these calls? Or am I misunderstanding something?


--
Khilan Gudka
PhD Student
Department of Computing
Imperial College London
http://www.doc.ic.ac.uk/~khilan/



On 18 January 2012 12:20, Zhong Yu <zhong.j.yu at gmail.com> wrote:
> On Tue, Jan 17, 2012 at 5:13 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
>> Another way to look at this is that compiler reordering that is only observable by programs with data races is generally allowed. ?The program below clearly has an inherent data race on b. ?If you make b volatile, the data race, and the problem, go away.
>
> Not by the definitions in the book.
>
> Volatile r/w can form a data race, if there is neither hb(r,w) nor
> hb(w,r), which is possible when r is before w in synchronization
> order.
>
> P.S. Reading JLS, ?the wording of "allowed to observe" (17.4.5) is
> very misleading; it is defined only in term of happens-before order,
> meaning a volatile read can be "allowed to observe" a later volatile
> write.
>
> Zhong Yu
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nader at aeinehchi.com  Wed Jan 18 08:20:40 2012
From: nader at aeinehchi.com (Nader Aeinehchi)
Date: Wed, 18 Jan 2012 14:20:40 +0100
Subject: [concurrency-interest] The semantics of reentrant lock
Message-ID: <4F16C728.3040204@aeinehchi.com>

Hi

Forgive me for my ignorance, but I cannot be 100% sure from reading 
JavaDoc documentation whether java.util.concurrency.ReentrantLock has 
the same semantics as the intrinsic lock.  Is it so that lock.unlock() 
does not have any effect once method2 is called from method1?

class Foo{

private final Lock lock = new ReentrantLock();

public void method1(){
     lock.lock();
     try{
         //calls method2
         method2();
     }finally{
         lock.unlock();
     }
}

public void method2(){
     lock.lock();
     try{
         //do something
     }finally{
         lock.unlock();
     }
}

}

Thanks.

From viktor.klang at gmail.com  Wed Jan 18 08:30:28 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 18 Jan 2012 14:30:28 +0100
Subject: [concurrency-interest] The semantics of reentrant lock
In-Reply-To: <4F16C728.3040204@aeinehchi.com>
References: <4F16C728.3040204@aeinehchi.com>
Message-ID: <CANPzfU9MNmC2h1eGLLzxgpgXUa9J9UZ2+VxQ4B=vC1GOf6+OCA@mail.gmail.com>

Docs are a wonderful thing:
http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/locks/ReentrantLock.html#unlock%28%29

On Wed, Jan 18, 2012 at 2:20 PM, Nader Aeinehchi <nader at aeinehchi.com>wrote:

> Hi
>
> Forgive me for my ignorance, but I cannot be 100% sure from reading
> JavaDoc documentation whether java.util.concurrency.**ReentrantLock has
> the same semantics as the intrinsic lock.  Is it so that lock.unlock() does
> not have any effect once method2 is called from method1?
>
> class Foo{
>
> private final Lock lock = new ReentrantLock();
>
> public void method1(){
>    lock.lock();
>    try{
>        //calls method2
>        method2();
>    }finally{
>        lock.unlock();
>    }
> }
>
> public void method2(){
>    lock.lock();
>    try{
>        //do something
>    }finally{
>        lock.unlock();
>    }
> }
>
> }
>
> Thanks.
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120118/505468ab/attachment.html>

From heinz at javaspecialists.eu  Wed Jan 18 08:52:06 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 18 Jan 2012 15:52:06 +0200
Subject: [concurrency-interest] The semantics of reentrant lock
In-Reply-To: <CANPzfU9MNmC2h1eGLLzxgpgXUa9J9UZ2+VxQ4B=vC1GOf6+OCA@mail.gmail.com>
References: <4F16C728.3040204@aeinehchi.com>
	<CANPzfU9MNmC2h1eGLLzxgpgXUa9J9UZ2+VxQ4B=vC1GOf6+OCA@mail.gmail.com>
Message-ID: <4F16CE86.6030703@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120118/1ed4c55e/attachment.html>

From m.jevring at iontrading.com  Wed Jan 18 09:16:24 2012
From: m.jevring at iontrading.com (Markus Jevring)
Date: Wed, 18 Jan 2012 15:16:24 +0100
Subject: [concurrency-interest] Lock-free implementation of min including
	assignment
Message-ID: <4F16D438.8070008@iontrading.com>

I need to find a lock-free algorithm that will do min (or max) plus 
assignment atomically (to the extend that atomicity is possible without 
corresponding cpu instructions). I've been looking at j.u.c.AtomicLong 
for inspiration, and I've come up with this:

     /**
      * Atomically checks if {@code potentialReplacement} is less than 
{@code value}, and if so,
      * sets {@code value} to {@code potentialReplacement}. This is 
modelled after how most methods
      * inside {@link AtomicLong} are implemented.
      *
      * @param potentialReplacement the potential replacement
      * @param value the current value
      * @return the smallest value
      */
     private long min(long potentialReplacement, AtomicLong value) {
         for (;;) {
             long current = value.get();
             if (potentialReplacement < current) {
                 if (value.compareAndSet(current, potentialReplacement)) {
                     return potentialReplacement;
                 }
             } else {
                 if (value.compareAndSet(current, current)) {
                     return current;
                 }
             }
         }
     }

Will this work? I'm counting on the .compareAndSet() to be my sanity 
check in the loop. I mean, the call returns the correct value, and the 
assignment also works, but will it work in the face under concurrent 
access? Tests show that it seems to work when hit concurrently, and it 
reasonably looks like it should.

If it doesn't, what are the conditions under which it might fail?

If it does, I'm surprised it's not plastered all over the internet. 
Granted, it's taken me a significant amount of time to need such 
functionality, so perhaps it's rare.

Markus

From m.jevring at iontrading.com  Wed Jan 18 10:27:22 2012
From: m.jevring at iontrading.com (Markus Jevring)
Date: Wed, 18 Jan 2012 16:27:22 +0100
Subject: [concurrency-interest] Lock-free implementation of min
 including assignment
In-Reply-To: <C568AB36-17F9-4405-A27A-B3711CF268A8@oracle.com>
References: <4F16D438.8070008@iontrading.com>
	<C568AB36-17F9-4405-A27A-B3711CF268A8@oracle.com>
Message-ID: <4F16E4DA.2010308@iontrading.com>

I do it to ensure that the value hasn't changed while we were doing 
non-atomic things like "<".
It fills the same function as the other update, namely that the whole 
operation can only succeed if nothing else has changed the state of the 
AtomicLong at the same time.

Markus

On 18/01/2012 16:09, Victor Luchangco wrote:
> On Jan 18, 2012, at 9:16 AM, Markus Jevring wrote:
>
>> I need to find a lock-free algorithm that will do min (or max) plus assignment atomically (to the extend that atomicity is possible without corresponding cpu instructions). I've been looking at j.u.c.AtomicLong for inspiration, and I've come up with this:
>>
>>     /**
>>      * Atomically checks if {@code potentialReplacement} is less than {@code value}, and if so,
>>      * sets {@code value} to {@code potentialReplacement}. This is modelled after how most methods
>>      * inside {@link AtomicLong} are implemented.
>>      *
>>      * @param potentialReplacement the potential replacement
>>      * @param value the current value
>>      * @return the smallest value
>>      */
>>     private long min(long potentialReplacement, AtomicLong value) {
>>         for (;;) {
>>             long current = value.get();
>>             if (potentialReplacement<  current) {
>>                 if (value.compareAndSet(current, potentialReplacement)) {
>>                     return potentialReplacement;
>>                 }
>>             } else {
>>                 if (value.compareAndSet(current, current)) {
>>                     return current;
>>                 }
>>             }
>>         }
>>     }
>>
>> Will this work? I'm counting on the .compareAndSet() to be my sanity check in the loop. I mean, the call returns the correct value, and the assignment also works, but will it work in the face under concurrent access? Tests show that it seems to work when hit concurrently, and it reasonably looks like it should.
> Why do the compareAndSet when current<= potentialReplacement?  Since you aren't changing the value, you can pretend that you wrote it exactly when you read it, so you can just return immediately.
>
> - Victor
>
>
>> If it doesn't, what are the conditions under which it might fail?
>>
>> If it does, I'm surprised it's not plastered all over the internet. Granted, it's taken me a significant amount of time to need such functionality, so perhaps it's rare.
>>
>> Markus
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From vitalyd at gmail.com  Wed Jan 18 10:53:43 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 18 Jan 2012 10:53:43 -0500
Subject: [concurrency-interest] Lock-free implementation of min
	including assignment
In-Reply-To: <4F16E4DA.2010308@iontrading.com>
References: <4F16D438.8070008@iontrading.com>
	<C568AB36-17F9-4405-A27A-B3711CF268A8@oracle.com>
	<4F16E4DA.2010308@iontrading.com>
Message-ID: <CAHjP37Fg6vsdHBOY6V21ZNiHOogJCHiJ1uenoG+iECOcLw9wKg@mail.gmail.com>

Hi Markus,

I think Viktor's point is that even if nothing has changed when you do the
compareAndSet in the else clause, by the time you return the value it might
be stale so there doesn't seem to be much value in doing the CAS.

Sent from my phone
On Jan 18, 2012 10:29 AM, "Markus Jevring" <m.jevring at iontrading.com> wrote:

> I do it to ensure that the value hasn't changed while we were doing
> non-atomic things like "<".
> It fills the same function as the other update, namely that the whole
> operation can only succeed if nothing else has changed the state of the
> AtomicLong at the same time.
>
> Markus
>
> On 18/01/2012 16:09, Victor Luchangco wrote:
>
>> On Jan 18, 2012, at 9:16 AM, Markus Jevring wrote:
>>
>>  I need to find a lock-free algorithm that will do min (or max) plus
>>> assignment atomically (to the extend that atomicity is possible without
>>> corresponding cpu instructions). I've been looking at j.u.c.AtomicLong for
>>> inspiration, and I've come up with this:
>>>
>>>    /**
>>>     * Atomically checks if {@code potentialReplacement} is less than
>>> {@code value}, and if so,
>>>     * sets {@code value} to {@code potentialReplacement}. This is
>>> modelled after how most methods
>>>     * inside {@link AtomicLong} are implemented.
>>>     *
>>>     * @param potentialReplacement the potential replacement
>>>     * @param value the current value
>>>     * @return the smallest value
>>>     */
>>>    private long min(long potentialReplacement, AtomicLong value) {
>>>        for (;;) {
>>>            long current = value.get();
>>>            if (potentialReplacement<  current) {
>>>                if (value.compareAndSet(current, potentialReplacement)) {
>>>                    return potentialReplacement;
>>>                }
>>>            } else {
>>>                if (value.compareAndSet(current, current)) {
>>>                    return current;
>>>                }
>>>            }
>>>        }
>>>    }
>>>
>>> Will this work? I'm counting on the .compareAndSet() to be my sanity
>>> check in the loop. I mean, the call returns the correct value, and the
>>> assignment also works, but will it work in the face under concurrent
>>> access? Tests show that it seems to work when hit concurrently, and it
>>> reasonably looks like it should.
>>>
>> Why do the compareAndSet when current<= potentialReplacement?  Since you
>> aren't changing the value, you can pretend that you wrote it exactly when
>> you read it, so you can just return immediately.
>>
>> - Victor
>>
>>
>>  If it doesn't, what are the conditions under which it might fail?
>>>
>>> If it does, I'm surprised it's not plastered all over the internet.
>>> Granted, it's taken me a significant amount of time to need such
>>> functionality, so perhaps it's rare.
>>>
>>> Markus
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120118/7d09c17b/attachment-0001.html>

From yshavit at akiban.com  Wed Jan 18 10:58:14 2012
From: yshavit at akiban.com (Yuval Shavit)
Date: Wed, 18 Jan 2012 10:58:14 -0500
Subject: [concurrency-interest] Lock-free implementation of min
	including assignment
In-Reply-To: <4F16E4DA.2010308@iontrading.com>
References: <4F16D438.8070008@iontrading.com>
	<C568AB36-17F9-4405-A27A-B3711CF268A8@oracle.com>
	<4F16E4DA.2010308@iontrading.com>
Message-ID: <CAC2Zdp2yyXHpGL08kitsfH6MC5Cp_MQvxY7v7tHRgNFE6txhvw@mail.gmail.com>

Seems to me you can just return current in the else. Sure, there's a race
condition in that someone else may have put in a smaller value between the
value.get() and the return, but that race condition is inherent to this
kind of code, since someone could also put in a smaller value between
value.compareAndSet(current,current) and return current -- or between
return current and when the value is used at the call site.

I think the only thing you can guarantee with this method, with respect to
the return value, is that it will be <= potentialReplacement, and that at
some point it was the minimum value of the AtomicLong.

On Wed, Jan 18, 2012 at 10:27 AM, Markus Jevring
<m.jevring at iontrading.com>wrote:

> I do it to ensure that the value hasn't changed while we were doing
> non-atomic things like "<".
> It fills the same function as the other update, namely that the whole
> operation can only succeed if nothing else has changed the state of the
> AtomicLong at the same time.
>
> Markus
>
> On 18/01/2012 16:09, Victor Luchangco wrote:
>
>> On Jan 18, 2012, at 9:16 AM, Markus Jevring wrote:
>>
>>  I need to find a lock-free algorithm that will do min (or max) plus
>>> assignment atomically (to the extend that atomicity is possible without
>>> corresponding cpu instructions). I've been looking at j.u.c.AtomicLong for
>>> inspiration, and I've come up with this:
>>>
>>>    /**
>>>     * Atomically checks if {@code potentialReplacement} is less than
>>> {@code value}, and if so,
>>>     * sets {@code value} to {@code potentialReplacement}. This is
>>> modelled after how most methods
>>>     * inside {@link AtomicLong} are implemented.
>>>     *
>>>     * @param potentialReplacement the potential replacement
>>>     * @param value the current value
>>>     * @return the smallest value
>>>     */
>>>    private long min(long potentialReplacement, AtomicLong value) {
>>>        for (;;) {
>>>            long current = value.get();
>>>            if (potentialReplacement<  current) {
>>>                if (value.compareAndSet(current, potentialReplacement)) {
>>>                    return potentialReplacement;
>>>                }
>>>            } else {
>>>                if (value.compareAndSet(current, current)) {
>>>                    return current;
>>>                }
>>>            }
>>>        }
>>>    }
>>>
>>> Will this work? I'm counting on the .compareAndSet() to be my sanity
>>> check in the loop. I mean, the call returns the correct value, and the
>>> assignment also works, but will it work in the face under concurrent
>>> access? Tests show that it seems to work when hit concurrently, and it
>>> reasonably looks like it should.
>>>
>> Why do the compareAndSet when current<= potentialReplacement?  Since you
>> aren't changing the value, you can pretend that you wrote it exactly when
>> you read it, so you can just return immediately.
>>
>> - Victor
>>
>>
>>
>>  If it doesn't, what are the conditions under which it might fail?
>>>
>>> If it does, I'm surprised it's not plastered all over the internet.
>>> Granted, it's taken me a significant amount of time to need such
>>> functionality, so perhaps it's rare.
>>>
>>> Markus
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120118/4d14a208/attachment.html>

From hans.boehm at hp.com  Wed Jan 18 12:36:21 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 18 Jan 2012 17:36:21 +0000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <CACuKZqGaVgSaqCH7QHOtJ=RV1s7d-8av-wAjUO30vVPWzdcy9w@mail.gmail.com>
References: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEIEJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52977C@G9W0339.americas.hpqcorp.net>
	<CACuKZqGaVgSaqCH7QHOtJ=RV1s7d-8av-wAjUO30vVPWzdcy9w@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD22E529BC2@G9W0339.americas.hpqcorp.net>

> From: Zhong Yu [mailto:zhong.j.yu at gmail.com]
> On Tue, Jan 17, 2012 at 5:13 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
> > Another way to look at this is that compiler reordering that is only
> observable by programs with data races is generally allowed. ?The
> program below clearly has an inherent data race on b. ?If you make b
> volatile, the data race, and the problem, go away.
> 
> Not by the definitions in the book.
> 
> Volatile r/w can form a data race, if there is neither hb(r,w) nor
> hb(w,r), which is possible when r is before w in synchronization
> order.
Interesting observation.  But I think that's clearly yet another bug in the specification.  It's called a DATA race because it involves data, not synchronization (e.g. volatile) operations.  If this were correct, essentially every program containing volatiles would have data races.

This part of the spec really needs an overhaul.  A technical reason for that is that we know that there are serious fundamental and unsolved problems with the treatment of data races (as they should be defined), and it's unclear we can make enough progress without resolving those.

> 
> P.S. Reading JLS,  the wording of "allowed to observe" (17.4.5) is
> very misleading; it is defined only in term of happens-before order,
> meaning a volatile read can be "allowed to observe" a later volatile
> write.
I don't think so.  If it did, the write would happen before the read, which would lead to a contradiction.

Hans

> 
> Zhong Yu


From zhong.j.yu at gmail.com  Wed Jan 18 13:16:09 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 18 Jan 2012 12:16:09 -0600
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD22E529BC2@G9W0339.americas.hpqcorp.net>
References: <CAN1xFdqRuhgd-nbTCkEEMJ7_qNHAsem4ZSKNZpKJa=U8BaTxOg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEIEJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52977C@G9W0339.americas.hpqcorp.net>
	<CACuKZqGaVgSaqCH7QHOtJ=RV1s7d-8av-wAjUO30vVPWzdcy9w@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E529BC2@G9W0339.americas.hpqcorp.net>
Message-ID: <CACuKZqF6+eZEYoCW7WrLqN6KUa4RpwcP1BqTQb-FV5phLY_zJg@mail.gmail.com>

On Wed, Jan 18, 2012 at 11:36 AM, Boehm, Hans <hans.boehm at hp.com> wrote:
>> From: Zhong Yu [mailto:zhong.j.yu at gmail.com]
>> On Tue, Jan 17, 2012 at 5:13 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
>> > Another way to look at this is that compiler reordering that is only
>> observable by programs with data races is generally allowed. ?The
>> program below clearly has an inherent data race on b. ?If you make b
>> volatile, the data race, and the problem, go away.
>>
>> Not by the definitions in the book.
>>
>> Volatile r/w can form a data race, if there is neither hb(r,w) nor
>> hb(w,r), which is possible when r is before w in synchronization
>> order.
> Interesting observation. ?But I think that's clearly yet another bug in the specification. ?It's called a DATA race because it involves data, not synchronization (e.g. volatile) operations. ?If this were correct, essentially every program containing volatiles would have data races.

So basically almost all Java programs are not "correctly
synchronized", that's funny.

> This part of the spec really needs an overhaul. ?A technical reason for that is that we know that there are serious fundamental and unsolved problems with the treatment of data races (as they should be defined), and it's unclear we can make enough progress without resolving those.
>
>>
>> P.S. Reading JLS, ?the wording of "allowed to observe" (17.4.5) is
>> very misleading; it is defined only in term of happens-before order,
>> meaning a volatile read can be "allowed to observe" a later volatile
>> write.
> I don't think so. ?If it did, the write would happen before the read, which would lead to a contradiction.

It shouldn't be allowed to observe, in the plain English sense,
because it's barred by synchronization-order consistency
requirement(17.4.7).

In JLSv3, the phrase "allowed to observe" was italic; still, it was a
badly chosen term. In JLS/SE7 it's not even italic, making it worse.

Zhong Yu


From hans.boehm at hp.com  Wed Jan 18 13:21:05 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 18 Jan 2012 18:21:05 +0000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <CAN1xFdprFn__-HpTL1GKjqbXfX4duJH-7FDcbjjYfVsS+BY=6A@mail.gmail.com>
References: <CAHjP37GY2a1fgsOjX2SecFYe2aDyqSGzV2rY4Yz2d7P8uROxPA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEIFJCAA.davidcholmes@aapt.net.au>
	<CAHjP37Hp9oYkC5QfQ4ouPDs+Mi-ELjkaSbzCQgs-TtjGaQV8fA@mail.gmail.com>
	<CAN1xFdrRVEQxXBfnEqn0kMrTqZi9urZs1aRVYvQ3-BuhrSDT=g@mail.gmail.com>
	<CAN1xFdprFn__-HpTL1GKjqbXfX4duJH-7FDcbjjYfVsS+BY=6A@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD22E529C54@G9W0339.americas.hpqcorp.net>

> From: Raph Frank
> 
> Another question, would this work properly as a lock?
> 
> public class OptimisticLockInt {
> 
>     private int x;
>     private AtomicInteger counter = new AtomicInteger(0);
>     private static final int UNSTABLE = 1;
> 
>     public void set(int x)
>         int oldCount = counter.getAndSet(UNSTABLE);
>         this.x = x;
>         counter.set(oldCount + 2);
>     }
> 
>     public int get() {
>         while (true)
>             int oldCount = counter.getAndSet();
You meant counter.get() here?

>             int tempX = x;
>             if (counter.get() == oldCount) {
>                 return tempX;
>             }
>         }
>     }
> }
> 
> I guess the get() method can be reordered to the following?
> 
>     public int get() {
>         while (true)
>             int oldCount = counter.getAndSet();
>             if (counter.get() == oldCount) {
>                 int tempX = x;
>                 return tempX;
>             } else {
>                 int tempX = x;
>             }
>         }
>     }
> 
Yes.  This is unfortunately incorrect.  This is essentially the Linux seqlock problem.

The obvious fix is to eliminate the data race by making x volatile.  This isn't as ridiculous as it sounds, since x is inherently involved in a race.  You can't safely replace the access to x by arbitrary read-only data accesses anyway, since you may, in general, read inconsistent values, and your code has to be very carefully written to avoid crashes as a result.  Even if x is an int which is guaranteed to be read in one piece, reading it twice may yield a later, followed by an earlier value.

I believe a very delicate and brittle alternative may be to replace the trailing get() in the reader with a getAndAdd(0).  This relies on data race semantics, and we don't really know what those are.  And it will break with a data race detector, which wwe should probably all be using much more regularly.  But if the code inside the read-only critical section is written very carefully, and if the implementation is more-or-less consistent with the original intent of the Java memory model, I believe it should work.

Hans


From hans.boehm at hp.com  Wed Jan 18 13:33:12 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 18 Jan 2012 18:33:12 +0000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <CAN1xFdrRVEQxXBfnEqn0kMrTqZi9urZs1aRVYvQ3-BuhrSDT=g@mail.gmail.com>
References: <CAHjP37GY2a1fgsOjX2SecFYe2aDyqSGzV2rY4Yz2d7P8uROxPA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEIFJCAA.davidcholmes@aapt.net.au>
	<CAHjP37Hp9oYkC5QfQ4ouPDs+Mi-ELjkaSbzCQgs-TtjGaQV8fA@mail.gmail.com>
	<CAN1xFdrRVEQxXBfnEqn0kMrTqZi9urZs1aRVYvQ3-BuhrSDT=g@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD22E529C79@G9W0339.americas.hpqcorp.net>

I think that almost expresses the right intuition, though it's not very close to real hardware.  In addition to your constraints, a volatile write also has to ensure that it itself becomes visible before a subsequent volatile read by the same thread.

My understanding is that for most real hardware, at least X86 hardware, the cache coherency protocol essentially ensures that once a write is visible to any cache, it is visible to all.  No cache flushes are required for volatile accesses.  Memory fences are typically required primarily to flush (or order writes from) store buffers holding data that hasn't yet made it to the cache.  (On X86, the fence is needed ONLY to order a volatile store and a later volatile load, i.e. the case you missed below.)

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> interest-bounces at cs.oswego.edu] On Behalf Of Raph Frank
> Sent: Wednesday, January 18, 2012 2:42 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Volatile happens before question
> 
> Thanks all, for the info.
> 
> So, for a cache, if there is a write to a volatile variable, the only
> requirement is that you flush the volatile to main memory last.
> 
> So, if the core writes
> 
> A) writes before volatile write
> B) write to volatile
> C) writes after volatile write
> 
> Then the cache can be flushed in the order A and then C and then B?
> So, a core could wait before writing a volatile to main memory, until
> a second write to that cache slot forces a flush (and writing a second
> value to the volatile itself doesn't force a flush)?
> 
> Similarly for a read, reading from a volatile doesn't actually require
> invaliding the read cache.  However, if the volatile is not in the
> cache, then the read cache must be flushed before reading the volatile
> from main memory?
> 
> On Tue, Jan 17, 2012 at 11:45 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
> > That's true, you're right - I didn't think of that as it does seem
> silly but
> > is allowed.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From martinrb at google.com  Wed Jan 18 15:23:55 2012
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 18 Jan 2012 12:23:55 -0800
Subject: [concurrency-interest] Lock-free implementation of min
	including assignment
In-Reply-To: <4F16D438.8070008@iontrading.com>
References: <4F16D438.8070008@iontrading.com>
Message-ID: <CA+kOe09joD8FwP-c+1gFx+nR4gnrOWz9dnYfhHTsYzqDEG=rqw@mail.gmail.com>

On Wed, Jan 18, 2012 at 06:16, Markus Jevring <m.jevring at iontrading.com>wrote:

> If it does, I'm surprised it's not plastered all over the internet.


We've written something here:
http://g.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/package-info.java?view=co

 * <p>It is straightforward to define new utility functions that, like
 * {@code getAndIncrement}, apply a function to a value atomically.
 * For example, given some transformation
 * <pre> {@code long transform(long input)}</pre>
 *
 * write your utility method as follows:
 *  <pre> {@code
 * long getAndTransform(AtomicLong var) {
 *   while (true) {
 *     long current = var.get();
 *     long next = transform(current);
 *     if (var.compareAndSet(current, next))
 *         return current;
 *   }
 * }}</pre>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120118/b27fd8bd/attachment.html>

From raphfrk at gmail.com  Wed Jan 18 19:40:35 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Thu, 19 Jan 2012 00:40:35 +0000
Subject: [concurrency-interest] Volatile happens before question
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD22E529C79@G9W0339.americas.hpqcorp.net>
References: <CAHjP37GY2a1fgsOjX2SecFYe2aDyqSGzV2rY4Yz2d7P8uROxPA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEIFJCAA.davidcholmes@aapt.net.au>
	<CAHjP37Hp9oYkC5QfQ4ouPDs+Mi-ELjkaSbzCQgs-TtjGaQV8fA@mail.gmail.com>
	<CAN1xFdrRVEQxXBfnEqn0kMrTqZi9urZs1aRVYvQ3-BuhrSDT=g@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E529C79@G9W0339.americas.hpqcorp.net>
Message-ID: <CAN1xFdpfRyWu1VuSp8FbnjJiAhHq-q85pT+ybAZ=DyDa3+Rrxg@mail.gmail.com>

Would this class work?

The principle is to add a getAndAdd(0x80000000) call instead of a
.get() for the tryReadLock method and then ignore the MSB for most
purposes.

The 2 lock method return an int which is passed to the matching unlock
method.  The static isLocked(int) method can be used to see if the
lock was successful.  It just checks if the sequence number is even.

readUnlock returns true if the lock wasn't write locked while the read
lock was being held.

Write locks follows the sequence of

sequence -> UNSTABLE -> (sequence + 2)

- MSB ignored
- even sequences are considered stable
- UNSTABLE = 1

public class OptimisticLock {

  private final int UNSTABLE = 1;

  private final AtomicInteger seq = new AtomicInteger(0);

  public final int tryReadLock() {
    return seq.get() & 0x7FFFFFFF;
  }

  public final boolean readUnlock(int seq) {
    return (this.seq.getandAdd(0x80000000) & 0x7FFFFFFF) == seq;
  }

  public final int tryWriteLock() {
    return seq.getAndSet(UNSTABLE) & 0x7FFFFFFF;
  }

  public final void writeUnlock(int seq) {
    seq.set(seq + 2);
  }

  public static public boolean isLocked(int seq) {
    return (seq & 0x00000001) == 0;
  }

}

The read-lock sequence is

R1) seq.get() (volatile R)
R2) read from non-volatile objects
R3) seq.getAndAdd() (volatile R/W)

The write-lock sequence is thus

W1) seq.getAndSet() -> volatile(R/W)
W2) write to non-volatile objects
W3) seq.set -> volatile(W)

Steps 1 and 3 for both are "synchronization actions", and so are ordered.

R1 -> check if stable (R)
R3 -> check is changed (R/W)

W1 -> check if write lock (R/W)
W3 -> sets counter to stable (W)

There are 6 possible sync sequences:

R1 -> R3 -> W1 -> W3

W1 is a read and R3 is a write, so R2 happens before W2, so read success

R1 -> W1 -> R3 -> W3

R3 is a read and W1 is a write, so R3 happens after W1, so the read
unlock will fail

R1 -> W1 -> W3 -> R3

R3 is a read and W3 is a write, so R3 happens after W3, so the read
unlock will fail

W1 -> W3 -> R1 -> R3

R1 is a read and W3 is a write, so W2 happens before R2, so read success

W1 -> R1 -> W3 -> R3

R1 is a read and W1 is a write, so W1 happens before R1, so the read
lock will fail (R3 won't happen)

W1 -> R1 -> R3 -> W3

R1 is a read and W1 is a write, so W1 happens before R1, so the read
lock will fail (R3 won't happen)

From m.jevring at iontrading.com  Thu Jan 19 02:36:50 2012
From: m.jevring at iontrading.com (Markus Jevring)
Date: Thu, 19 Jan 2012 08:36:50 +0100
Subject: [concurrency-interest] Lock-free implementation of min
 including assignment
In-Reply-To: <CAHjP37Fg6vsdHBOY6V21ZNiHOogJCHiJ1uenoG+iECOcLw9wKg@mail.gmail.com>
References: <4F16D438.8070008@iontrading.com>
	<C568AB36-17F9-4405-A27A-B3711CF268A8@oracle.com>
	<4F16E4DA.2010308@iontrading.com>
	<CAHjP37Fg6vsdHBOY6V21ZNiHOogJCHiJ1uenoG+iECOcLw9wKg@mail.gmail.com>
Message-ID: <4F17C812.6080006@iontrading.com>

That may very well be true. Since I'm holding the smallest value in 
'value' constantly anyway, this is less of an issue, though.

On 18/01/2012 16:53, Vitaly Davidovich wrote:
>
> Hi Markus,
>
> I think Viktor's point is that even if nothing has changed when you do 
> the compareAndSet in the else clause, by the time you return the value 
> it might be stale so there doesn't seem to be much value in doing the CAS.
>
> Sent from my phone
>
> On Jan 18, 2012 10:29 AM, "Markus Jevring" <m.jevring at iontrading.com 
> <mailto:m.jevring at iontrading.com>> wrote:
>
>     I do it to ensure that the value hasn't changed while we were
>     doing non-atomic things like "<".
>     It fills the same function as the other update, namely that the
>     whole operation can only succeed if nothing else has changed the
>     state of the AtomicLong at the same time.
>
>     Markus
>
>     On 18/01/2012 16:09, Victor Luchangco wrote:
>
>         On Jan 18, 2012, at 9:16 AM, Markus Jevring wrote:
>
>             I need to find a lock-free algorithm that will do min (or
>             max) plus assignment atomically (to the extend that
>             atomicity is possible without corresponding cpu
>             instructions). I've been looking at j.u.c.AtomicLong for
>             inspiration, and I've come up with this:
>
>                /**
>                 * Atomically checks if {@code potentialReplacement} is
>             less than {@code value}, and if so,
>                 * sets {@code value} to {@code potentialReplacement}.
>             This is modelled after how most methods
>                 * inside {@link AtomicLong} are implemented.
>                 *
>                 * @param potentialReplacement the potential replacement
>                 * @param value the current value
>                 * @return the smallest value
>                 */
>                private long min(long potentialReplacement, AtomicLong
>             value) {
>                    for (;;) {
>                        long current = value.get();
>                        if (potentialReplacement<  current) {
>                            if (value.compareAndSet(current,
>             potentialReplacement)) {
>                                return potentialReplacement;
>                            }
>                        } else {
>                            if (value.compareAndSet(current, current)) {
>                                return current;
>                            }
>                        }
>                    }
>                }
>
>             Will this work? I'm counting on the .compareAndSet() to be
>             my sanity check in the loop. I mean, the call returns the
>             correct value, and the assignment also works, but will it
>             work in the face under concurrent access? Tests show that
>             it seems to work when hit concurrently, and it reasonably
>             looks like it should.
>
>         Why do the compareAndSet when current<= potentialReplacement?
>          Since you aren't changing the value, you can pretend that you
>         wrote it exactly when you read it, so you can just return
>         immediately.
>
>         - Victor
>
>
>             If it doesn't, what are the conditions under which it
>             might fail?
>
>             If it does, I'm surprised it's not plastered all over the
>             internet. Granted, it's taken me a significant amount of
>             time to need such functionality, so perhaps it's rare.
>
>             Markus
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/23b9ddcd/attachment-0001.html>

From m.jevring at iontrading.com  Thu Jan 19 02:38:22 2012
From: m.jevring at iontrading.com (Markus Jevring)
Date: Thu, 19 Jan 2012 08:38:22 +0100
Subject: [concurrency-interest] Lock-free implementation of min
 including assignment
In-Reply-To: <CAC2Zdp2yyXHpGL08kitsfH6MC5Cp_MQvxY7v7tHRgNFE6txhvw@mail.gmail.com>
References: <4F16D438.8070008@iontrading.com>
	<C568AB36-17F9-4405-A27A-B3711CF268A8@oracle.com>
	<4F16E4DA.2010308@iontrading.com>
	<CAC2Zdp2yyXHpGL08kitsfH6MC5Cp_MQvxY7v7tHRgNFE6txhvw@mail.gmail.com>
Message-ID: <4F17C86E.2080402@iontrading.com>

What I need is for 'value' to always contain the smallest value. I'm 
less concerned with the method returning the smallest value. Ideally it 
should return the smallest value at the time of invocation, but I could 
technically make this void and simply return, considering I'm keeping 
the smallest value in 'value' anyway.

On 18/01/2012 16:58, Yuval Shavit wrote:
> Seems to me you can just return current in the else. Sure, there's a 
> race condition in that someone else may have put in a smaller value 
> between the value.get() and the return, but that race condition is 
> inherent to this kind of code, since someone could also put in a 
> smaller value between value.compareAndSet(current,current) and return 
> current -- or between return current and when the value is used at the 
> call site.
>
> I think the only thing you can guarantee with this method, with 
> respect to the return value, is that it will be <= 
> potentialReplacement, and that at some point it was the minimum value 
> of the AtomicLong.
>
> On Wed, Jan 18, 2012 at 10:27 AM, Markus Jevring 
> <m.jevring at iontrading.com <mailto:m.jevring at iontrading.com>> wrote:
>
>     I do it to ensure that the value hasn't changed while we were
>     doing non-atomic things like "<".
>     It fills the same function as the other update, namely that the
>     whole operation can only succeed if nothing else has changed the
>     state of the AtomicLong at the same time.
>
>     Markus
>
>     On 18/01/2012 16:09, Victor Luchangco wrote:
>
>         On Jan 18, 2012, at 9:16 AM, Markus Jevring wrote:
>
>             I need to find a lock-free algorithm that will do min (or
>             max) plus assignment atomically (to the extend that
>             atomicity is possible without corresponding cpu
>             instructions). I've been looking at j.u.c.AtomicLong for
>             inspiration, and I've come up with this:
>
>                /**
>                 * Atomically checks if {@code potentialReplacement} is
>             less than {@code value}, and if so,
>                 * sets {@code value} to {@code potentialReplacement}.
>             This is modelled after how most methods
>                 * inside {@link AtomicLong} are implemented.
>                 *
>                 * @param potentialReplacement the potential replacement
>                 * @param value the current value
>                 * @return the smallest value
>                 */
>                private long min(long potentialReplacement, AtomicLong
>             value) {
>                    for (;;) {
>                        long current = value.get();
>                        if (potentialReplacement<  current) {
>                            if (value.compareAndSet(current,
>             potentialReplacement)) {
>                                return potentialReplacement;
>                            }
>                        } else {
>                            if (value.compareAndSet(current, current)) {
>                                return current;
>                            }
>                        }
>                    }
>                }
>
>             Will this work? I'm counting on the .compareAndSet() to be
>             my sanity check in the loop. I mean, the call returns the
>             correct value, and the assignment also works, but will it
>             work in the face under concurrent access? Tests show that
>             it seems to work when hit concurrently, and it reasonably
>             looks like it should.
>
>         Why do the compareAndSet when current<= potentialReplacement?
>          Since you aren't changing the value, you can pretend that you
>         wrote it exactly when you read it, so you can just return
>         immediately.
>
>         - Victor
>
>
>
>             If it doesn't, what are the conditions under which it
>             might fail?
>
>             If it does, I'm surprised it's not plastered all over the
>             internet. Granted, it's taken me a significant amount of
>             time to need such functionality, so perhaps it's rare.
>
>             Markus
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/2dac1792/attachment.html>

From m.jevring at iontrading.com  Thu Jan 19 02:48:21 2012
From: m.jevring at iontrading.com (Markus Jevring)
Date: Thu, 19 Jan 2012 08:48:21 +0100
Subject: [concurrency-interest] Lock-free implementation of min
 including assignment
In-Reply-To: <CA+kOe09joD8FwP-c+1gFx+nR4gnrOWz9dnYfhHTsYzqDEG=rqw@mail.gmail.com>
References: <4F16D438.8070008@iontrading.com>
	<CA+kOe09joD8FwP-c+1gFx+nR4gnrOWz9dnYfhHTsYzqDEG=rqw@mail.gmail.com>
Message-ID: <4F17CAC5.5040409@iontrading.com>

So, by paraphrasing what you have there, if I wanted something like 
getAndMin(AtomicLong var, long l),
I would define transform() as: "return Math.min(current, l)" to get the 
desired effect, right?
In essence, I'd be able to skip the cas operation in my else-case?

Similarly, minAndGet(AtomicLong var, long l) would simply return "next" 
instead of "current", right?

As other people have pointed out, both 'current' and 'min', as it were, 
may have become obsolete by the time the function returns, but as long 
as we keep the smallest value in 'var', we should be fine, right?

Markus

On 18/01/2012 21:23, Martin Buchholz wrote:
>
>
> On Wed, Jan 18, 2012 at 06:16, Markus Jevring 
> <m.jevring at iontrading.com <mailto:m.jevring at iontrading.com>> wrote:
>
>     If it does, I'm surprised it's not plastered all over the internet.
>
>
> We've written something here:
> http://g.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/package-info.java?view=co
>
>  * <p>It is straightforward to define new utility functions that, like
>  * {@code getAndIncrement}, apply a function to a value atomically.
>  * For example, given some transformation
>  * <pre> {@code long transform(long input)}</pre>
>  *
>  * write your utility method as follows:
>  * <pre> {@code
>  * long getAndTransform(AtomicLong var) {
>  *   while (true) {
>  *     long current = var.get();
>  *     long next = transform(current);
>  *     if (var.compareAndSet(current, next))
>  *         return current;
>  *   }
>  * }}</pre>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/fb3d3c10/attachment.html>

From radhakrishnan.mohan at gmail.com  Thu Jan 19 03:51:46 2012
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Thu, 19 Jan 2012 14:21:46 +0530
Subject: [concurrency-interest] Benchmarking
Message-ID: <CAOoXFP8hY0C7CC7Z-iwrE2DOtjNez+cM6mrjH7AdFoYQi5sGLA@mail.gmail.com>

Hi,
        What kind of benchmarking suites can I use to test the
performance of multi-threaded applications assuming we don't use j.u.c
yet ? Java Grande ? The idea is to switch to j.u.c but with some
benchmarking proof that a home-grown naive implementation of threads
don't scale. Multiple cores are.


Thanks,
Mohan

From dawid.weiss at gmail.com  Thu Jan 19 04:02:25 2012
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Thu, 19 Jan 2012 10:02:25 +0100
Subject: [concurrency-interest] Benchmarking
In-Reply-To: <CAOoXFP8hY0C7CC7Z-iwrE2DOtjNez+cM6mrjH7AdFoYQi5sGLA@mail.gmail.com>
References: <CAOoXFP8hY0C7CC7Z-iwrE2DOtjNez+cM6mrjH7AdFoYQi5sGLA@mail.gmail.com>
Message-ID: <CAM21Rt9rRWaqykZHrn0HKqM6JnsLbDsLBWDd2rz68Yc+pX8dEg@mail.gmail.com>

You can substitute your implementation and j.u.c.-backed
implementation, re-run a number of times on different hardware
configurations (whatever you plan to deploy on) and calculate simple
statistics (average, min, max, variance)? There are tools to help you
with some of that, but it all depends on the size of your benchmark
(some of these may not be suitable for long-running components):

1) Google Caliper: http://code.google.com/p/caliper/
2) JUnit Benchmarks: http://labs.carrotsearch.com/junit-benchmarks.html
3) /usr/bin/time...

Dawid

On Thu, Jan 19, 2012 at 9:51 AM, Mohan Radhakrishnan
<radhakrishnan.mohan at gmail.com> wrote:
> Hi,
> ? ? ? ?What kind of benchmarking suites can I use to test the
> performance of multi-threaded applications assuming we don't use j.u.c
> yet ? Java Grande ? The idea is to switch to j.u.c but with some
> benchmarking proof that a home-grown naive implementation of threads
> don't scale. Multiple cores are.
>
>
> Thanks,
> Mohan
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From martinrb at google.com  Thu Jan 19 08:36:07 2012
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 19 Jan 2012 05:36:07 -0800
Subject: [concurrency-interest] Lock-free implementation of min
	including assignment
In-Reply-To: <4F17CAC5.5040409@iontrading.com>
References: <4F16D438.8070008@iontrading.com>
	<CA+kOe09joD8FwP-c+1gFx+nR4gnrOWz9dnYfhHTsYzqDEG=rqw@mail.gmail.com>
	<4F17CAC5.5040409@iontrading.com>
Message-ID: <CA+kOe08BrQRTV6kDXFqhNkSotR9=b0eq5+WMJZEApVppNqpoPw@mail.gmail.com>

On Wed, Jan 18, 2012 at 23:48, Markus Jevring <m.jevring at iontrading.com>wrote:

>  So, by paraphrasing what you have there, if I wanted something like
> getAndMin(AtomicLong var, long l),
> I would define transform() as: "return Math.min(current, l)" to get the
> desired effect, right?
>

Right.


> In essence, I'd be able to skip the cas operation in my else-case?
>
>
Right. Skipping the CAS when current == next is just an optimization, but
an important one for transformations like min or max where current == next
is the common case.


> Similarly, minAndGet(AtomicLong var, long l) would simply return "next"
> instead of "current", right?
>
>
Right.


> As other people have pointed out, both 'current' and 'min', as it were,
> may have become obsolete by the time the function returns, but as long as
> we keep the smallest value in 'var', we should be fine, right?
>
>
Right.


> Markus
>
>
> On 18/01/2012 21:23, Martin Buchholz wrote:
>
>
>
> On Wed, Jan 18, 2012 at 06:16, Markus Jevring <m.jevring at iontrading.com>wrote:
>
>> If it does, I'm surprised it's not plastered all over the internet.
>
>
>  We've written something here:
>
> http://g.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/package-info.java?view=co
>
>   * <p>It is straightforward to define new utility functions that, like
>  * {@code getAndIncrement}, apply a function to a value atomically.
>  * For example, given some transformation
>  * <pre> {@code long transform(long input)}</pre>
>  *
>  * write your utility method as follows:
>  *  <pre> {@code
>  * long getAndTransform(AtomicLong var) {
>  *   while (true) {
>  *     long current = var.get();
>  *     long next = transform(current);
>  *     if (var.compareAndSet(current, next))
>  *         return current;
>  *   }
>  * }}</pre>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/77c7b68c/attachment-0001.html>

From raphfrk at gmail.com  Thu Jan 19 12:53:18 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Thu, 19 Jan 2012 17:53:18 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic variables
Message-ID: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>

In my thread "Volatile happens before question", one of the issues
with the .get() method is that it only counts as a volatile read.
This means that if you read the old value of the variable, it doesn't
establish any happens-before relationship.

A .hardGet() would be a get that also counts as a volatile write.

This would mean that if you .hardGet() on an atomic variable, it would
happen-before the write that over-writes the value that was read (at
least assuming that write was a combined read/write operation).

I assume that atomicInteger.getAndAdd(0) is equivalent?

Can atomicInteger.getAndAdd(0) be optimized away, or would it be
guaranteed to count as both a volatile read and write?

So, if a sequence counter is incremented at the start and end of all
write operation, then something like this would allow a read attempt.

final SomeType value = new SomeType();

boolean tryWrite(int value) {
  synchronized(this) {
    sequence.incrementAndGet();
    this.value.setInt(value);
    sequence.incrementAndGet();
  }
}

int tryRead() {
  int sequence = counter.get();

  int value = this.value.getInt();

  if (isEven(sequence) && sequence == counter.hardGet()) {
    return value;
  } else {
    return -1;
   }
}

(This assumes that calling .get() on SomeType won't explode if the
data is written to at the same time)

From vitalyd at gmail.com  Thu Jan 19 13:07:41 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 19 Jan 2012 13:07:41 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
Message-ID: <CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>

A volatile read gives you current value in memory (coherent cache).  This
volatile read happens before any subsequent operations (read or write),
volatile or not.  Why is this not good enough for you?

Sent from my phone
On Jan 19, 2012 12:55 PM, "Raph Frank" <raphfrk at gmail.com> wrote:

> In my thread "Volatile happens before question", one of the issues
> with the .get() method is that it only counts as a volatile read.
> This means that if you read the old value of the variable, it doesn't
> establish any happens-before relationship.
>
> A .hardGet() would be a get that also counts as a volatile write.
>
> This would mean that if you .hardGet() on an atomic variable, it would
> happen-before the write that over-writes the value that was read (at
> least assuming that write was a combined read/write operation).
>
> I assume that atomicInteger.getAndAdd(0) is equivalent?
>
> Can atomicInteger.getAndAdd(0) be optimized away, or would it be
> guaranteed to count as both a volatile read and write?
>
> So, if a sequence counter is incremented at the start and end of all
> write operation, then something like this would allow a read attempt.
>
> final SomeType value = new SomeType();
>
> boolean tryWrite(int value) {
>  synchronized(this) {
>    sequence.incrementAndGet();
>    this.value.setInt(value);
>    sequence.incrementAndGet();
>  }
> }
>
> int tryRead() {
>  int sequence = counter.get();
>
>  int value = this.value.getInt();
>
>  if (isEven(sequence) && sequence == counter.hardGet()) {
>    return value;
>  } else {
>    return -1;
>   }
> }
>
> (This assumes that calling .get() on SomeType won't explode if the
> data is written to at the same time)
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/2f108aab/attachment.html>

From raphfrk at gmail.com  Thu Jan 19 13:13:41 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Thu, 19 Jan 2012 18:13:41 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
Message-ID: <CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>

It doesn't have any happens before relationship with a later write.

It is for optimistic read locks base on a sequence counter.  If the
counter hasn't changed, then there is no guarantee that that
happens-before the counter was incremented.

From vitalyd at gmail.com  Thu Jan 19 13:30:51 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 19 Jan 2012 13:30:51 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
Message-ID: <CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>

Volatile read does not allow subsequent operations to reorder with it - the
load must retire before subsequent reads/writes.  Check Doug Lea's compiler
cookbook if you're skeptical.

I don't see what reordering you're concerned about - can you give a simple
example?

Sent from my phone
On Jan 19, 2012 1:15 PM, "Raph Frank" <raphfrk at gmail.com> wrote:

> It doesn't have any happens before relationship with a later write.
>
> It is for optimistic read locks base on a sequence counter.  If the
> counter hasn't changed, then there is no guarantee that that
> happens-before the counter was incremented.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/6ef70b74/attachment.html>

From raphfrk at gmail.com  Thu Jan 19 15:10:01 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Thu, 19 Jan 2012 20:10:01 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
Message-ID: <CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>

I gave more detail in the thread "Volatile happens before question".

It is for an optimistic read situation

The write is:

W1: sequenceCounter.increment()
W2: <modify the non-volatile memory>
W3: sequenceCounter.increment()

The increment makes the counter an odd number and the 2nd one sets it
back to even.

The read is:

R1: sequenceCounter.get()
R2: <read from non-volatile memory>
R3: sequenceCounter.get()

If the 2 reads give the same value (no sequence update) and it is even
(wasn't unstable), then R2 is assumed to have read correctly.

Otherwise, the read is considered to have failed.

------

However, it doesn't technically work.

R1, R3, W1 and W3 are changes to volatile variables (an AtomicInteger)
so they have to be sequenced somehow.

The danger sequence is this one:

R1 (volatile read)
R3 (volatile read)
W1 (volatile read/write)
W3 (volatile read/write)

Since R3 happens before W1, the 2nd read is even (still at the initial
value of 0), and therefore the read is considered to have read stable
data.

The happens before relationships are

R1 happens-before R2 happens-before R3
W1 happens-before W2 happens-before W3

However, since W1 is a write and R3 is a read, there is no happens
before relationship.

This means that R2 and W2 have no happens-before relationship and
effectively happen concurrently and there is a data race.

------

The code could be changed so that it works by using

The read is:

R1: sequenceCounter.get()
R2: <read from non-volatile memory>
R3: sequenceCounter.getAndAdd(0) [*]

[*] This assumes that adding zero still causes the write to be
considered to have happened

------

The sequence is this one:

R1 (volatile read)
R3 (volatile read/write)
W1 (volatile read/write)
W3 (volatile read/write)

Since R3 is now a write and W1 is a read, R3 happens before W1 and
therefore R2 happens before W2, so they execute in sequence.

.hardGet() would be equivalent to .getAndAdd(0), but would work for
the non-numeric Atomic types.

From cheremin at gmail.com  Thu Jan 19 15:28:18 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Fri, 20 Jan 2012 00:28:18 +0400
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
Message-ID: <CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>

Actually you still can implement your "hard get" (which is actually
seems like LoadLoadBarrier + vload) with non-numeric types as
atomicRef.compareAndSet(current, current) -- it is exactly the code to
which getAndAdd(0) will be actually transformed (AFAIK).



2012/1/20 Raph Frank <raphfrk at gmail.com>:
> I gave more detail in the thread "Volatile happens before question".
>
> It is for an optimistic read situation
>
> The write is:
>
> W1: sequenceCounter.increment()
> W2: <modify the non-volatile memory>
> W3: sequenceCounter.increment()
>
> The increment makes the counter an odd number and the 2nd one sets it
> back to even.
>
> The read is:
>
> R1: sequenceCounter.get()
> R2: <read from non-volatile memory>
> R3: sequenceCounter.get()
>
> If the 2 reads give the same value (no sequence update) and it is even
> (wasn't unstable), then R2 is assumed to have read correctly.
>
> Otherwise, the read is considered to have failed.
>
> ------
>
> However, it doesn't technically work.
>
> R1, R3, W1 and W3 are changes to volatile variables (an AtomicInteger)
> so they have to be sequenced somehow.
>
> The danger sequence is this one:
>
> R1 (volatile read)
> R3 (volatile read)
> W1 (volatile read/write)
> W3 (volatile read/write)
>
> Since R3 happens before W1, the 2nd read is even (still at the initial
> value of 0), and therefore the read is considered to have read stable
> data.
>
> The happens before relationships are
>
> R1 happens-before R2 happens-before R3
> W1 happens-before W2 happens-before W3
>
> However, since W1 is a write and R3 is a read, there is no happens
> before relationship.
>
> This means that R2 and W2 have no happens-before relationship and
> effectively happen concurrently and there is a data race.
>
> ------
>
> The code could be changed so that it works by using
>
> The read is:
>
> R1: sequenceCounter.get()
> R2: <read from non-volatile memory>
> R3: sequenceCounter.getAndAdd(0) [*]
>
> [*] This assumes that adding zero still causes the write to be
> considered to have happened
>
> ------
>
> The sequence is this one:
>
> R1 (volatile read)
> R3 (volatile read/write)
> W1 (volatile read/write)
> W3 (volatile read/write)
>
> Since R3 is now a write and W1 is a read, R3 happens before W1 and
> therefore R2 happens before W2, so they execute in sequence.
>
> .hardGet() would be equivalent to .getAndAdd(0), but would work for
> the non-numeric Atomic types.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From raphfrk at gmail.com  Thu Jan 19 15:31:05 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Thu, 19 Jan 2012 20:31:05 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
	<CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
Message-ID: <CAN1xFdrH6dZUiwh3oo9e2CedqkA9PsCSj3WymZEYjuBcztdrAg@mail.gmail.com>

On Thu, Jan 19, 2012 at 8:28 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:
> Actually you still can implement your "hard get" (which is actually
> seems like LoadLoadBarrier + vload) with non-numeric types as
> atomicRef.compareAndSet(current, current) -- it is exactly the code to
> which getAndAdd(0) will be actually transformed (AFAIK).

Thanks, can "current" be any non-null reference?

From raphfrk at gmail.com  Thu Jan 19 15:44:33 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Thu, 19 Jan 2012 20:44:33 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
	<CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
Message-ID: <CAN1xFdq5q3_JsyQF0JRoT5YssHcqj9io4xqfST9440fcu7fRKw@mail.gmail.com>

On Thu, Jan 19, 2012 at 8:28 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:
> Actually you still can implement your "hard get" (which is actually
> seems like LoadLoadBarrier + vload) with non-numeric types as
> atomicRef.compareAndSet(current, current) -- it is exactly the code to
> which getAndAdd(0) will be actually transformed (AFAIK).

Also, just to confirm, getAndAdd(0) will count as a write for working
out thread syncing ?

From cheremin at gmail.com  Thu Jan 19 15:48:04 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Fri, 20 Jan 2012 00:48:04 +0400
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAN1xFdrH6dZUiwh3oo9e2CedqkA9PsCSj3WymZEYjuBcztdrAg@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
	<CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
	<CAN1xFdrH6dZUiwh3oo9e2CedqkA9PsCSj3WymZEYjuBcztdrAg@mail.gmail.com>
Message-ID: <CAOwENiK7Khfby4WxWnzwrti-PH2zNyR17ZAqSBa4A-UFYg7EdQ@mail.gmail.com>

current == value you've just read by .get() few lines ago.

2012/1/20 Raph Frank <raphfrk at gmail.com>:
> On Thu, Jan 19, 2012 at 8:28 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:
>> Actually you still can implement your "hard get" (which is actually
>> seems like LoadLoadBarrier + vload) with non-numeric types as
>> atomicRef.compareAndSet(current, current) -- it is exactly the code to
>> which getAndAdd(0) will be actually transformed (AFAIK).
>
> Thanks, can "current" be any non-null reference?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From raphfrk at gmail.com  Thu Jan 19 15:55:52 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Thu, 19 Jan 2012 20:55:52 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAOwENiK7Khfby4WxWnzwrti-PH2zNyR17ZAqSBa4A-UFYg7EdQ@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
	<CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
	<CAN1xFdrH6dZUiwh3oo9e2CedqkA9PsCSj3WymZEYjuBcztdrAg@mail.gmail.com>
	<CAOwENiK7Khfby4WxWnzwrti-PH2zNyR17ZAqSBa4A-UFYg7EdQ@mail.gmail.com>
Message-ID: <CAN1xFdrvwQSigz4FD=6f3ik3g9p+RQP0P_jz4p0YCYY-s97E7Q@mail.gmail.com>

On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:
> current == value you've just read by .get() few lines ago.

Ahh, right.

For references, would .compareAndSet(null, null), also add in the syncing?

Does a compare and set that fails to update count as a write, or just a read?

From cheremin at gmail.com  Thu Jan 19 16:04:07 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Fri, 20 Jan 2012 01:04:07 +0400
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAN1xFdrvwQSigz4FD=6f3ik3g9p+RQP0P_jz4p0YCYY-s97E7Q@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
	<CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
	<CAN1xFdrH6dZUiwh3oo9e2CedqkA9PsCSj3WymZEYjuBcztdrAg@mail.gmail.com>
	<CAOwENiK7Khfby4WxWnzwrti-PH2zNyR17ZAqSBa4A-UFYg7EdQ@mail.gmail.com>
	<CAN1xFdrvwQSigz4FD=6f3ik3g9p+RQP0P_jz4p0YCYY-s97E7Q@mail.gmail.com>
Message-ID: <CAOwENiLVKC8BLWZQu-OWEmaHui1QNnNHTuy2EwAZEW2mxLo2gQ@mail.gmail.com>

I think, it depends on what you name "write". Failed CAS will be "like
write" in sense of cache coherence traffic -- at least AFAIK -- it
will request cache line to be in M state (read-for-update), so if
cache line was in some other core's cache -- it will be invalidated.
But failed CAS does not really update cache line value, so it seems
like writeback to main memory not needed. I do not know, does current
CPUs actually optimize this writeback.

2012/1/20 Raph Frank <raphfrk at gmail.com>:
> On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:
>> current == value you've just read by .get() few lines ago.
>
> Ahh, right.
>
> For references, would .compareAndSet(null, null), also add in the syncing?
>
> Does a compare and set that fails to update count as a write, or just a read?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From cheremin at gmail.com  Thu Jan 19 16:25:34 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Fri, 20 Jan 2012 01:25:34 +0400
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAN1xFdq563UoVMYTZth0g2s9XkuCjSdNoM+W-_v2Ggjzf-WZaw@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
	<CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
	<CAN1xFdrH6dZUiwh3oo9e2CedqkA9PsCSj3WymZEYjuBcztdrAg@mail.gmail.com>
	<CAOwENiK7Khfby4WxWnzwrti-PH2zNyR17ZAqSBa4A-UFYg7EdQ@mail.gmail.com>
	<CAN1xFdrvwQSigz4FD=6f3ik3g9p+RQP0P_jz4p0YCYY-s97E7Q@mail.gmail.com>
	<CAOwENiLVKC8BLWZQu-OWEmaHui1QNnNHTuy2EwAZEW2mxLo2gQ@mail.gmail.com>
	<CAN1xFdq563UoVMYTZth0g2s9XkuCjSdNoM+W-_v2Ggjzf-WZaw@mail.gmail.com>
Message-ID: <CAOwENiLscu3nq9Rs0gWMpb7iBSFuXW-O4iohE4RwFvSDx7ak+A@mail.gmail.com>

It depends on what a.increment (getAndIncrement, actually) returns. If
it is the same value, as you got by getAndAdd(0) in second thread --
yes, it HB(getAndAdd(0), a.getAndIncrement()).

In your example of optimistic lock, if you would chang second .get()
in optimistic read to .getAndAdd(0) -- it seems for me that your lock
will be ok.

2012/1/20 Raph Frank <raphfrk at gmail.com>:
> On Thu, Jan 19, 2012 at 9:04 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:
>> I do not know, does current
>> CPUs actually optimize this writeback.
>
> Well, I am just wondering what the model says.
>
> does .getAndAdd(0) count as a write for the purposes of creating a
> happens-before chain?
>
> So, if the sync order was
>
> AtomicInteger a = new AtomicInteger(0);
> int nonVolatile = 0;
>
> Thread 1 ? ? ? ? <-----------> ? ? ? ? Thread 2
>
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? int b = nonVolatile;
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? int aTemp = a.addAndSet(0);
>
> a.increment();
> nonVolatile = 1;
>
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? System.out.println("Atemp = " + aTemp);
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? System.out.println("b = " + b);
>
> System.out.println("a is " + a.get());
>
>
> Is a happens-before relationship established between b = nonVolatile
> and nonVolatile = 1?


From raphfrk at gmail.com  Thu Jan 19 16:40:09 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Thu, 19 Jan 2012 21:40:09 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAOwENiLscu3nq9Rs0gWMpb7iBSFuXW-O4iohE4RwFvSDx7ak+A@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
	<CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
	<CAN1xFdrH6dZUiwh3oo9e2CedqkA9PsCSj3WymZEYjuBcztdrAg@mail.gmail.com>
	<CAOwENiK7Khfby4WxWnzwrti-PH2zNyR17ZAqSBa4A-UFYg7EdQ@mail.gmail.com>
	<CAN1xFdrvwQSigz4FD=6f3ik3g9p+RQP0P_jz4p0YCYY-s97E7Q@mail.gmail.com>
	<CAOwENiLVKC8BLWZQu-OWEmaHui1QNnNHTuy2EwAZEW2mxLo2gQ@mail.gmail.com>
	<CAN1xFdq563UoVMYTZth0g2s9XkuCjSdNoM+W-_v2Ggjzf-WZaw@mail.gmail.com>
	<CAOwENiLscu3nq9Rs0gWMpb7iBSFuXW-O4iohE4RwFvSDx7ak+A@mail.gmail.com>
Message-ID: <CAN1xFdqiv+bsq9qtQn_UmFXpzB1W38XM-FavDpHCqgNCXGQVfw@mail.gmail.com>

On Thu, Jan 19, 2012 at 9:25 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:
> In your example of optimistic lock, if you would chang second .get()
> in optimistic read to .getAndAdd(0) -- it seems for me that your lock
> will be ok.

Yeah that is what I was hoping for.

From vitalyd at gmail.com  Thu Jan 19 16:42:13 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 19 Jan 2012 16:42:13 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAOwENiLVKC8BLWZQu-OWEmaHui1QNnNHTuy2EwAZEW2mxLo2gQ@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
	<CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
	<CAN1xFdrH6dZUiwh3oo9e2CedqkA9PsCSj3WymZEYjuBcztdrAg@mail.gmail.com>
	<CAOwENiK7Khfby4WxWnzwrti-PH2zNyR17ZAqSBa4A-UFYg7EdQ@mail.gmail.com>
	<CAN1xFdrvwQSigz4FD=6f3ik3g9p+RQP0P_jz4p0YCYY-s97E7Q@mail.gmail.com>
	<CAOwENiLVKC8BLWZQu-OWEmaHui1QNnNHTuy2EwAZEW2mxLo2gQ@mail.gmail.com>
Message-ID: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>

Failed cas won't write anything to cache (afterall you didn't modify
anything).  However, it does achieve same memory fencing/ordering as a
successful cas.  On x86/64 that's because the cmpxchg instruction is
prefixed with LOCK, which by itself makes the instruction serializing
irrespective of whether the cmpxchg succeeds.

Also, small addendum - processor doesn't always issue a RFO (request for
ownership) before writing - if the cache line is in exclusive state in the
writing processor, it doesn't need to do that.

Also Doug Lea has a version of seqlock in his CVS repo for jsr166e - you
can take a look at it for details.  I'll tell you that there is no funny
business there with dummy cas operations - he trusts volatile reads :).

Sent from my phone
On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

> I think, it depends on what you name "write". Failed CAS will be "like
> write" in sense of cache coherence traffic -- at least AFAIK -- it
> will request cache line to be in M state (read-for-update), so if
> cache line was in some other core's cache -- it will be invalidated.
> But failed CAS does not really update cache line value, so it seems
> like writeback to main memory not needed. I do not know, does current
> CPUs actually optimize this writeback.
>
> 2012/1/20 Raph Frank <raphfrk at gmail.com>:
> > On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com>
> wrote:
> >> current == value you've just read by .get() few lines ago.
> >
> > Ahh, right.
> >
> > For references, would .compareAndSet(null, null), also add in the
> syncing?
> >
> > Does a compare and set that fails to update count as a write, or just a
> read?
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/e2b0bf33/attachment.html>

From raphfrk at gmail.com  Thu Jan 19 19:11:06 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Fri, 20 Jan 2012 00:11:06 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for atomic
	variables
In-Reply-To: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
References: <CAN1xFdpj8bLnxHJOASq8vL+Y8RJDAULk-dwcxLR9fVDaCNtPbg@mail.gmail.com>
	<CAHjP37EQL9AMTwKW-KYoToQjBUAnFHORNRB-uhUG3NGTiNEDPQ@mail.gmail.com>
	<CAN1xFdpYwcwxSoZB7R6zj4=eJe4rQywJW0qeMbv8KDTDsfxNcw@mail.gmail.com>
	<CAHjP37E5j=_7EO+ZgpWaWz3afJmANBOqLXCJrNbd=7mDzoqZyQ@mail.gmail.com>
	<CAN1xFdrB74u-kUJAZcsAp0MuijR4hPUH_gR=FCBrnGF3BJ1AqA@mail.gmail.com>
	<CAOwENiK4u30seazUdkitKEt+L8+GjGb6mXZHYgq_w+OWKLDXGQ@mail.gmail.com>
	<CAN1xFdrH6dZUiwh3oo9e2CedqkA9PsCSj3WymZEYjuBcztdrAg@mail.gmail.com>
	<CAOwENiK7Khfby4WxWnzwrti-PH2zNyR17ZAqSBa4A-UFYg7EdQ@mail.gmail.com>
	<CAN1xFdrvwQSigz4FD=6f3ik3g9p+RQP0P_jz4p0YCYY-s97E7Q@mail.gmail.com>
	<CAOwENiLVKC8BLWZQu-OWEmaHui1QNnNHTuy2EwAZEW2mxLo2gQ@mail.gmail.com>
	<CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
Message-ID: <CAN1xFdoC=WvB_4reBULK0beg5A4iYigWqH1W+KrJ9kwrsvv+DA@mail.gmail.com>

On Thu, Jan 19, 2012 at 9:42 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>  I'll tell you that there is no funny
> business there with dummy cas operations - he trusts volatile reads :).

If they are trustworthy, then that's great.  I would just like to
understand why.

From davidcholmes at aapt.net.au  Thu Jan 19 19:20:08 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 20 Jan 2012 10:20:08 +1000
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>

Vitaly,

Doug's SequenceLock requires that the data being read is volatile.

David
-------
* <p> Methods {@code awaitAvailability} and {@code getSequence} can
 * be used together to define (partially) optimistic read-only methods
 * that are usually more efficient than ReadWriteLocks when they
 * apply.  These methods should in general be structured as loops that
 * await lock availability, then read {@code volatile} fields into
 * local variables (and may further read other values derived from
 * these, for example the {@code length} of a {@code volatile} array),
 * and retry if the sequence number changed while doing so.-----Original
Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
  Sent: Friday, 20 January 2012 7:42 AM
  To: Ruslan Cheremin
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Suggestion: .hardGet() for
atomicvariables


  Failed cas won't write anything to cache (afterall you didn't modify
anything).  However, it does achieve same memory fencing/ordering as a
successful cas.  On x86/64 that's because the cmpxchg instruction is
prefixed with LOCK, which by itself makes the instruction serializing
irrespective of whether the cmpxchg succeeds.

  Also, small addendum - processor doesn't always issue a RFO (request for
ownership) before writing - if the cache line is in exclusive state in the
writing processor, it doesn't need to do that.

  Also Doug Lea has a version of seqlock in his CVS repo for jsr166e - you
can take a look at it for details.  I'll tell you that there is no funny
business there with dummy cas operations - he trusts volatile reads :).

  Sent from my phone

  On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

    I think, it depends on what you name "write". Failed CAS will be "like
    write" in sense of cache coherence traffic -- at least AFAIK -- it
    will request cache line to be in M state (read-for-update), so if
    cache line was in some other core's cache -- it will be invalidated.
    But failed CAS does not really update cache line value, so it seems
    like writeback to main memory not needed. I do not know, does current
    CPUs actually optimize this writeback.

    2012/1/20 Raph Frank <raphfrk at gmail.com>:
    > On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com>
wrote:
    >> current == value you've just read by .get() few lines ago.
    >
    > Ahh, right.
    >
    > For references, would .compareAndSet(null, null), also add in the
syncing?
    >
    > Does a compare and set that fails to update count as a write, or just
a read?
    > _______________________________________________
    > Concurrency-interest mailing list
    > Concurrency-interest at cs.oswego.edu
    > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120120/6b9c2885/attachment-0001.html>

From hans.boehm at hp.com  Thu Jan 19 19:58:42 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 20 Jan 2012 00:58:42 +0000
Subject: [concurrency-interest] Suggestion: .hardGet()
	for	atomicvariables
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>

Yes.   It does require volatile, as it should.

Getting back to the earlier topic, I'm not at all convinced that a failed CAS behaves like a volatile write according to the Java memory model.  There is no write that can synchronize with anything.  It probably does hold on X86.

By my reading, getAndAdd(0) is required to write, and hence does the right thing.  If an implementation optimizes it to a volatile load, it's wrong.  If it uses a CAS loop, that's fine, since the last one should succeed.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David Holmes
Sent: Thursday, January 19, 2012 4:20 PM
To: Vitaly Davidovich; Ruslan Cheremin
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Suggestion: .hardGet() for atomicvariables

Vitaly,

Doug's SequenceLock requires that the data being read is volatile.

David
-------

* <p> Methods {@code awaitAvailability} and {@code getSequence} can

 * be used together to define (partially) optimistic read-only methods

 * that are usually more efficient than ReadWriteLocks when they

 * apply.  These methods should in general be structured as loops that

 * await lock availability, then read {@code volatile} fields into

 * local variables (and may further read other values derived from

 * these, for example the {@code length} of a {@code volatile} array),

 * and retry if the sequence number changed while doing so.
-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly Davidovich
Sent: Friday, 20 January 2012 7:42 AM
To: Ruslan Cheremin
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Suggestion: .hardGet() for atomicvariables

Failed cas won't write anything to cache (afterall you didn't modify anything).  However, it does achieve same memory fencing/ordering as a successful cas.  On x86/64 that's because the cmpxchg instruction is prefixed with LOCK, which by itself makes the instruction serializing irrespective of whether the cmpxchg succeeds.

Also, small addendum - processor doesn't always issue a RFO (request for ownership) before writing - if the cache line is in exclusive state in the writing processor, it doesn't need to do that.

Also Doug Lea has a version of seqlock in his CVS repo for jsr166e - you can take a look at it for details.  I'll tell you that there is no funny business there with dummy cas operations - he trusts volatile reads :).

Sent from my phone
On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com<mailto:cheremin at gmail.com>> wrote:
I think, it depends on what you name "write". Failed CAS will be "like
write" in sense of cache coherence traffic -- at least AFAIK -- it
will request cache line to be in M state (read-for-update), so if
cache line was in some other core's cache -- it will be invalidated.
But failed CAS does not really update cache line value, so it seems
like writeback to main memory not needed. I do not know, does current
CPUs actually optimize this writeback.

2012/1/20 Raph Frank <raphfrk at gmail.com<mailto:raphfrk at gmail.com>>:
> On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com<mailto:cheremin at gmail.com>> wrote:
>> current == value you've just read by .get() few lines ago.
>
> Ahh, right.
>
> For references, would .compareAndSet(null, null), also add in the syncing?
>
> Does a compare and set that fails to update count as a write, or just a read?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120120/c5d9b96c/attachment.html>

From vitalyd at gmail.com  Thu Jan 19 20:15:38 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 19 Jan 2012 20:15:38 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
Message-ID: <CAHjP37HdvFtMdEjg3g8jqiABD6OtTYN2AoabjxtGpdmmkpPadg@mail.gmail.com>

OK maybe I'm looking at this too x86-centric but under which architectures
would spinning on a seqlock not be enough? That is, why does the data you
read inside the loop need to be volatile, meaning you can't assume that the
spinning CAS won't provide the barrier? Intuitively, even though the CAS
loop reads unrelated memory to the body of the loop, I think it's
reasonable to assume that it provides an acquire fence, at least for a
successful CAS.  Granted that's an assumption but I'm curious under what
circumstances that wouldn't be the right thing to do.

Honestly, I'm really starting to think that the Fences API that Doug
proposed is the right way to go, similar to C++11x atomics - this would
allow explicit intent rather than mentally reconciling the JMM with actual
hardware.

Vitaly

Sent from my phone
On Jan 19, 2012 8:00 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

>  Yes.   It does require volatile, as it should.****
>
> ** **
>
> Getting back to the earlier topic, I?m not at all convinced that a failed
> CAS behaves like a volatile write according to the Java memory model.
> There is no write that can synchronize with anything.  It probably does
> hold on X86.****
>
> ** **
>
> By my reading, getAndAdd(0) is required to write, and hence does the right
> thing.  If an implementation optimizes it to a volatile load, it?s wrong.
> If it uses a CAS loop, that?s fine, since the last one should succeed.****
>
> ** **
>
> Hans****
>
> ** **
>
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *David Holmes
> *Sent:* Thursday, January 19, 2012 4:20 PM
> *To:* Vitaly Davidovich; Ruslan Cheremin
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Suggestion: .hardGet() for
> atomicvariables****
>
> ** **
>
> Vitaly,****
>
>  ****
>
> Doug's SequenceLock requires that the data being read is volatile.****
>
>  ****
>
> David****
>
> -------****
>
> * <p> Methods {@code awaitAvailability} and {@code getSequence} can****
>
>  * be used together to define (partially) optimistic read-only methods****
>
>  * that are usually more efficient than ReadWriteLocks when they****
>
>  * apply.  These methods should in general be structured as loops that****
>
>  * await lock availability, then read {@code volatile} fields into****
>
>  * local variables (and may further read other values derived from****
>
>  * these, for example the {@code length} of a {@code volatile} array),****
>
>  * and retry if the sequence number changed while doing so.****
>
>  -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Vitaly
> Davidovich
> *Sent:* Friday, 20 January 2012 7:42 AM
> *To:* Ruslan Cheremin
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Suggestion: .hardGet() for
> atomicvariables****
>
> Failed cas won't write anything to cache (afterall you didn't modify
> anything).  However, it does achieve same memory fencing/ordering as a
> successful cas.  On x86/64 that's because the cmpxchg instruction is
> prefixed with LOCK, which by itself makes the instruction serializing
> irrespective of whether the cmpxchg succeeds.****
>
> Also, small addendum - processor doesn't always issue a RFO (request for
> ownership) before writing - if the cache line is in exclusive state in the
> writing processor, it doesn't need to do that.****
>
> Also Doug Lea has a version of seqlock in his CVS repo for jsr166e - you
> can take a look at it for details.  I'll tell you that there is no funny
> business there with dummy cas operations - he trusts volatile reads :).***
> *
>
> Sent from my phone****
>
> On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:****
>
> I think, it depends on what you name "write". Failed CAS will be "like
> write" in sense of cache coherence traffic -- at least AFAIK -- it
> will request cache line to be in M state (read-for-update), so if
> cache line was in some other core's cache -- it will be invalidated.
> But failed CAS does not really update cache line value, so it seems
> like writeback to main memory not needed. I do not know, does current
> CPUs actually optimize this writeback.
>
> 2012/1/20 Raph Frank <raphfrk at gmail.com>:
> > On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com>
> wrote:
> >> current == value you've just read by .get() few lines ago.
> >
> > Ahh, right.
> >
> > For references, would .compareAndSet(null, null), also add in the
> syncing?
> >
> > Does a compare and set that fails to update count as a write, or just a
> read?
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/3b045eff/attachment-0001.html>

From vitalyd at gmail.com  Thu Jan 19 20:22:38 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 19 Jan 2012 20:22:38 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <CAHjP37HdvFtMdEjg3g8jqiABD6OtTYN2AoabjxtGpdmmkpPadg@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAHjP37HdvFtMdEjg3g8jqiABD6OtTYN2AoabjxtGpdmmkpPadg@mail.gmail.com>
Message-ID: <CAHjP37EBtRNDXOqjKpNsxeZ3FWHDh7xZd0uy=LPXv8Q2kwEjPw@mail.gmail.com>

Also, having to do getAndAdd(0) to simulate a fence is further evidence
that a Fences API would be useful.  Doing that dummy CAS is a performance
penalty for no good reason otherwise, it seems.

Sent from my phone
On Jan 19, 2012 8:15 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> OK maybe I'm looking at this too x86-centric but under which architectures
> would spinning on a seqlock not be enough? That is, why does the data you
> read inside the loop need to be volatile, meaning you can't assume that the
> spinning CAS won't provide the barrier? Intuitively, even though the CAS
> loop reads unrelated memory to the body of the loop, I think it's
> reasonable to assume that it provides an acquire fence, at least for a
> successful CAS.  Granted that's an assumption but I'm curious under what
> circumstances that wouldn't be the right thing to do.
>
> Honestly, I'm really starting to think that the Fences API that Doug
> proposed is the right way to go, similar to C++11x atomics - this would
> allow explicit intent rather than mentally reconciling the JMM with actual
> hardware.
>
> Vitaly
>
> Sent from my phone
> On Jan 19, 2012 8:00 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:
>
>>  Yes.   It does require volatile, as it should.****
>>
>> ** **
>>
>> Getting back to the earlier topic, I?m not at all convinced that a failed
>> CAS behaves like a volatile write according to the Java memory model.
>> There is no write that can synchronize with anything.  It probably does
>> hold on X86.****
>>
>> ** **
>>
>> By my reading, getAndAdd(0) is required to write, and hence does the
>> right thing.  If an implementation optimizes it to a volatile load, it?s
>> wrong.  If it uses a CAS loop, that?s fine, since the last one should
>> succeed.****
>>
>> ** **
>>
>> Hans****
>>
>> ** **
>>
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *David Holmes
>> *Sent:* Thursday, January 19, 2012 4:20 PM
>> *To:* Vitaly Davidovich; Ruslan Cheremin
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Suggestion: .hardGet() for
>> atomicvariables****
>>
>> ** **
>>
>> Vitaly,****
>>
>>  ****
>>
>> Doug's SequenceLock requires that the data being read is volatile.****
>>
>>  ****
>>
>> David****
>>
>> -------****
>>
>> * <p> Methods {@code awaitAvailability} and {@code getSequence} can****
>>
>>  * be used together to define (partially) optimistic read-only methods****
>>
>>  * that are usually more efficient than ReadWriteLocks when they****
>>
>>  * apply.  These methods should in general be structured as loops that****
>>
>>  * await lock availability, then read {@code volatile} fields into****
>>
>>  * local variables (and may further read other values derived from****
>>
>>  * these, for example the {@code length} of a {@code volatile} array),****
>>
>>  * and retry if the sequence number changed while doing so.****
>>
>>  -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Vitaly
>> Davidovich
>> *Sent:* Friday, 20 January 2012 7:42 AM
>> *To:* Ruslan Cheremin
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Suggestion: .hardGet() for
>> atomicvariables****
>>
>> Failed cas won't write anything to cache (afterall you didn't modify
>> anything).  However, it does achieve same memory fencing/ordering as a
>> successful cas.  On x86/64 that's because the cmpxchg instruction is
>> prefixed with LOCK, which by itself makes the instruction serializing
>> irrespective of whether the cmpxchg succeeds.****
>>
>> Also, small addendum - processor doesn't always issue a RFO (request for
>> ownership) before writing - if the cache line is in exclusive state in the
>> writing processor, it doesn't need to do that.****
>>
>> Also Doug Lea has a version of seqlock in his CVS repo for jsr166e - you
>> can take a look at it for details.  I'll tell you that there is no funny
>> business there with dummy cas operations - he trusts volatile reads :).**
>> **
>>
>> Sent from my phone****
>>
>> On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:***
>> *
>>
>> I think, it depends on what you name "write". Failed CAS will be "like
>> write" in sense of cache coherence traffic -- at least AFAIK -- it
>> will request cache line to be in M state (read-for-update), so if
>> cache line was in some other core's cache -- it will be invalidated.
>> But failed CAS does not really update cache line value, so it seems
>> like writeback to main memory not needed. I do not know, does current
>> CPUs actually optimize this writeback.
>>
>> 2012/1/20 Raph Frank <raphfrk at gmail.com>:
>> > On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com>
>> wrote:
>> >> current == value you've just read by .get() few lines ago.
>> >
>> > Ahh, right.
>> >
>> > For references, would .compareAndSet(null, null), also add in the
>> syncing?
>> >
>> > Does a compare and set that fails to update count as a write, or just a
>> read?
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/58d8f215/attachment.html>

From davidcholmes at aapt.net.au  Thu Jan 19 20:41:35 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 20 Jan 2012 11:41:35 +1000
Subject: [concurrency-interest] Suggestion: .hardGet() foratomicvariables
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEJEJCAA.davidcholmes@aapt.net.au>

Hans,

A CAS is  a two-way "barrier" as per the j.u.c package javadoc:

compareAndSet and all other read-and-update operations such as
getAndIncrement have the memory effects of both reading and writing volatile
variables

This is not dependent on the CAS succeeding - which may seem a little
surprising but means the VM must insert any additional explicit fence
instructions around the native CAS, eg StoreLoad; cas; LoadStore|LoadLoad.

There should probably be in implementation note that a delta of zero can not
be used to optimize away the effect of the volatile write.

David
  -----Original Message-----
  From: Boehm, Hans [mailto:hans.boehm at hp.com]
  Sent: Friday, 20 January 2012 10:59 AM
  To: dholmes at ieee.org; Vitaly Davidovich; Ruslan Cheremin
  Cc: concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] Suggestion: .hardGet()
foratomicvariables


  Yes.   It does require volatile, as it should.



  Getting back to the earlier topic, I'm not at all convinced that a failed
CAS behaves like a volatile write according to the Java memory model.  There
is no write that can synchronize with anything.  It probably does hold on
X86.



  By my reading, getAndAdd(0) is required to write, and hence does the right
thing.  If an implementation optimizes it to a volatile load, it's wrong.
If it uses a CAS loop, that's fine, since the last one should succeed.



  Hans



  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David
Holmes
  Sent: Thursday, January 19, 2012 4:20 PM
  To: Vitaly Davidovich; Ruslan Cheremin
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Suggestion: .hardGet() for
atomicvariables



  Vitaly,



  Doug's SequenceLock requires that the data being read is volatile.



  David

  -------

* <p> Methods {@code awaitAvailability} and {@code getSequence} can * be
used together to define (partially) optimistic read-only methods * that are
usually more efficient than ReadWriteLocks when they * apply.  These methods
should in general be structured as loops that * await lock availability,
then read {@code volatile} fields into * local variables (and may further
read other values derived from * these, for example the {@code length} of a
{@code volatile} array), * and retry if the sequence number changed while
doing so.-----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
    Sent: Friday, 20 January 2012 7:42 AM
    To: Ruslan Cheremin
    Cc: concurrency-interest at cs.oswego.edu
    Subject: Re: [concurrency-interest] Suggestion: .hardGet() for
atomicvariables

    Failed cas won't write anything to cache (afterall you didn't modify
anything).  However, it does achieve same memory fencing/ordering as a
successful cas.  On x86/64 that's because the cmpxchg instruction is
prefixed with LOCK, which by itself makes the instruction serializing
irrespective of whether the cmpxchg succeeds.

    Also, small addendum - processor doesn't always issue a RFO (request for
ownership) before writing - if the cache line is in exclusive state in the
writing processor, it doesn't need to do that.

    Also Doug Lea has a version of seqlock in his CVS repo for jsr166e - you
can take a look at it for details.  I'll tell you that there is no funny
business there with dummy cas operations - he trusts volatile reads :).

    Sent from my phone

    On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

    I think, it depends on what you name "write". Failed CAS will be "like
    write" in sense of cache coherence traffic -- at least AFAIK -- it
    will request cache line to be in M state (read-for-update), so if
    cache line was in some other core's cache -- it will be invalidated.
    But failed CAS does not really update cache line value, so it seems
    like writeback to main memory not needed. I do not know, does current
    CPUs actually optimize this writeback.

    2012/1/20 Raph Frank <raphfrk at gmail.com>:
    > On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com>
wrote:
    >> current == value you've just read by .get() few lines ago.
    >
    > Ahh, right.
    >
    > For references, would .compareAndSet(null, null), also add in the
syncing?
    >
    > Does a compare and set that fails to update count as a write, or just
a read?
    > _______________________________________________
    > Concurrency-interest mailing list
    > Concurrency-interest at cs.oswego.edu
    > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120120/27d6f391/attachment-0001.html>

From hans.boehm at hp.com  Thu Jan 19 21:06:34 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 20 Jan 2012 02:06:34 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for
 atomicvariables
In-Reply-To: <CAHjP37EBtRNDXOqjKpNsxeZ3FWHDh7xZd0uy=LPXv8Q2kwEjPw@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAHjP37HdvFtMdEjg3g8jqiABD6OtTYN2AoabjxtGpdmmkpPadg@mail.gmail.com>
	<CAHjP37EBtRNDXOqjKpNsxeZ3FWHDh7xZd0uy=LPXv8Q2kwEjPw@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD22E52A7EB@G9W0339.americas.hpqcorp.net>

I'm not sure we're understanding each other quite correctly.  The problem is with read-only uses of seq_lock, using awaitAvailability and getSequence in Doug's interface.  The problem is that this effectively performs

Read sequence number (volatile read)
Read data (ordinary read for the questionable case; the problem goes away if we make it volatile, avoiding data races)
Read sequence number (volatile read)

This doesn't work (with the ordinary read in the middle) because, by roach motel rules, the middle read can be moved to the end, i.e. outside the critical section.  Thus we need to replace the final read by something else that disallows the roach motel movement.  Either getAndAdd or a CAS loop works.  A single failing CAS (probably, officially) does not.

I'm not sure that any of this bothers me.  This (i.e. without the volatile middle read) needs to remain at most an obscure hack because of the weird and unavoidable restrictions as to what you can put inside the "read-only critical section".  And GetAndAdd(0) seems to have exactly the right semantics to make the obscure and dangerous hack work.  Real fences are quite hard to define correctly, as we've seen in C++.  The fact that anybody who really wants to implement this obscure and dangerous hack already has an equally obscure construct to do it with seems almost satisfying, actually.

None of which denies that there are other problems in this area that need solving.

Hans

From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
Sent: Thursday, January 19, 2012 5:23 PM
To: Boehm, Hans
Cc: Ruslan Cheremin; dholmes at ieee.org; concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] Suggestion: .hardGet() for atomicvariables


Also, having to do getAndAdd(0) to simulate a fence is further evidence that a Fences API would be useful.  Doing that dummy CAS is a performance penalty for no good reason otherwise, it seems.

Sent from my phone
On Jan 19, 2012 8:15 PM, "Vitaly Davidovich" <vitalyd at gmail.com<mailto:vitalyd at gmail.com>> wrote:

OK maybe I'm looking at this too x86-centric but under which architectures would spinning on a seqlock not be enough? That is, why does the data you read inside the loop need to be volatile, meaning you can't assume that the spinning CAS won't provide the barrier? Intuitively, even though the CAS loop reads unrelated memory to the body of the loop, I think it's reasonable to assume that it provides an acquire fence, at least for a successful CAS.  Granted that's an assumption but I'm curious under what circumstances that wouldn't be the right thing to do.

Honestly, I'm really starting to think that the Fences API that Doug proposed is the right way to go, similar to C++11x atomics - this would allow explicit intent rather than mentally reconciling the JMM with actual hardware.

Vitaly

Sent from my phone
On Jan 19, 2012 8:00 PM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:
Yes.   It does require volatile, as it should.

Getting back to the earlier topic, I'm not at all convinced that a failed CAS behaves like a volatile write according to the Java memory model.  There is no write that can synchronize with anything.  It probably does hold on X86.

By my reading, getAndAdd(0) is required to write, and hence does the right thing.  If an implementation optimizes it to a volatile load, it's wrong.  If it uses a CAS loop, that's fine, since the last one should succeed.

Hans

From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu>] On Behalf Of David Holmes
Sent: Thursday, January 19, 2012 4:20 PM
To: Vitaly Davidovich; Ruslan Cheremin
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Suggestion: .hardGet() for atomicvariables

Vitaly,

Doug's SequenceLock requires that the data being read is volatile.

David
-------

* <p> Methods {@code awaitAvailability} and {@code getSequence} can

 * be used together to define (partially) optimistic read-only methods

 * that are usually more efficient than ReadWriteLocks when they

 * apply.  These methods should in general be structured as loops that

 * await lock availability, then read {@code volatile} fields into

 * local variables (and may further read other values derived from

 * these, for example the {@code length} of a {@code volatile} array),

 * and retry if the sequence number changed while doing so.
-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu>]On Behalf Of Vitaly Davidovich
Sent: Friday, 20 January 2012 7:42 AM
To: Ruslan Cheremin
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Suggestion: .hardGet() for atomicvariables

Failed cas won't write anything to cache (afterall you didn't modify anything).  However, it does achieve same memory fencing/ordering as a successful cas.  On x86/64 that's because the cmpxchg instruction is prefixed with LOCK, which by itself makes the instruction serializing irrespective of whether the cmpxchg succeeds.

Also, small addendum - processor doesn't always issue a RFO (request for ownership) before writing - if the cache line is in exclusive state in the writing processor, it doesn't need to do that.

Also Doug Lea has a version of seqlock in his CVS repo for jsr166e - you can take a look at it for details.  I'll tell you that there is no funny business there with dummy cas operations - he trusts volatile reads :).

Sent from my phone
On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com<mailto:cheremin at gmail.com>> wrote:
I think, it depends on what you name "write". Failed CAS will be "like
write" in sense of cache coherence traffic -- at least AFAIK -- it
will request cache line to be in M state (read-for-update), so if
cache line was in some other core's cache -- it will be invalidated.
But failed CAS does not really update cache line value, so it seems
like writeback to main memory not needed. I do not know, does current
CPUs actually optimize this writeback.

2012/1/20 Raph Frank <raphfrk at gmail.com<mailto:raphfrk at gmail.com>>:
> On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com<mailto:cheremin at gmail.com>> wrote:
>> current == value you've just read by .get() few lines ago.
>
> Ahh, right.
>
> For references, would .compareAndSet(null, null), also add in the syncing?
>
> Does a compare and set that fails to update count as a write, or just a read?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120120/7a8e8bff/attachment.html>

From vitalyd at gmail.com  Thu Jan 19 22:49:45 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 19 Jan 2012 22:49:45 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD22E52A7EB@G9W0339.americas.hpqcorp.net>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAHjP37HdvFtMdEjg3g8jqiABD6OtTYN2AoabjxtGpdmmkpPadg@mail.gmail.com>
	<CAHjP37EBtRNDXOqjKpNsxeZ3FWHDh7xZd0uy=LPXv8Q2kwEjPw@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A7EB@G9W0339.americas.hpqcorp.net>
Message-ID: <CAHjP37Hy_N4jjrHN8t-CRSLiWD2Xdqmcjo=sQaR=B=+gyt+pew@mail.gmail.com>

Hans,

I agree on your point and example of a volatile read not being sufficient
and allowing for movement of non volatile read after the 2nd volatile
load.  However, my comments were geared towards the CAS loop that was
mentioned.  What architectures provide different ordering for successful vs
unsuccessful CAS operations? I'm legitimately interested as I can't see a
hardware vendor that would pick such an option to implement but I'd love to
hear of one.

Additionally, David mentioned that CAS is specified as providing the
ordering without mentioning failure case so it seems like it should work
with the CAS loop we drew from Doug's javadoc.

Sent from my phone
On Jan 19, 2012 9:08 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

>  I?m not sure we?re understanding each other quite correctly.  The
> problem is with read-only uses of seq_lock, using awaitAvailability and
> getSequence in Doug?s interface.  The problem is that this effectively
> performs****
>
> ** **
>
> Read sequence number (volatile read)****
>
> Read data (ordinary read for the questionable case; the problem goes away
> if we make it volatile, avoiding data races)****
>
> Read sequence number (volatile read)****
>
> ** **
>
> This doesn?t work (with the ordinary read in the middle) because, by roach
> motel rules, the middle read can be moved to the end, i.e. outside the
> critical section.  Thus we need to replace the final read by something else
> that disallows the roach motel movement.  Either getAndAdd or a CAS loop
> works.  A single failing CAS (probably, officially) does not.****
>
> ** **
>
> I?m not sure that any of this bothers me.  This (i.e. without the volatile
> middle read) needs to remain at most an obscure hack because of the weird
> and unavoidable restrictions as to what you can put inside the ?read-only
> critical section?.  And GetAndAdd(0) seems to have exactly the right
> semantics to make the obscure and dangerous hack work.  Real fences are
> quite hard to define correctly, as we?ve seen in C++.  The fact that
> anybody who really wants to implement this obscure and dangerous hack
> already has an equally obscure construct to do it with seems almost
> satisfying, actually.****
>
> ** **
>
> None of which denies that there are other problems in this area that need
> solving.****
>
> ** **
>
> Hans****
>
> ** **
>
> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
> *Sent:* Thursday, January 19, 2012 5:23 PM
> *To:* Boehm, Hans
> *Cc:* Ruslan Cheremin; dholmes at ieee.org;
> concurrency-interest at cs.oswego.edu
> *Subject:* RE: [concurrency-interest] Suggestion: .hardGet() for
> atomicvariables****
>
> ** **
>
> Also, having to do getAndAdd(0) to simulate a fence is further evidence
> that a Fences API would be useful.  Doing that dummy CAS is a performance
> penalty for no good reason otherwise, it seems.****
>
> Sent from my phone****
>
> On Jan 19, 2012 8:15 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:***
> *
>
> OK maybe I'm looking at this too x86-centric but under which architectures
> would spinning on a seqlock not be enough? That is, why does the data you
> read inside the loop need to be volatile, meaning you can't assume that the
> spinning CAS won't provide the barrier? Intuitively, even though the CAS
> loop reads unrelated memory to the body of the loop, I think it's
> reasonable to assume that it provides an acquire fence, at least for a
> successful CAS.  Granted that's an assumption but I'm curious under what
> circumstances that wouldn't be the right thing to do.****
>
> Honestly, I'm really starting to think that the Fences API that Doug
> proposed is the right way to go, similar to C++11x atomics - this would
> allow explicit intent rather than mentally reconciling the JMM with actual
> hardware.****
>
> Vitaly****
>
> Sent from my phone****
>
> On Jan 19, 2012 8:00 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:****
>
> Yes.   It does require volatile, as it should.****
>
>  ****
>
> Getting back to the earlier topic, I?m not at all convinced that a failed
> CAS behaves like a volatile write according to the Java memory model.
> There is no write that can synchronize with anything.  It probably does
> hold on X86.****
>
>  ****
>
> By my reading, getAndAdd(0) is required to write, and hence does the right
> thing.  If an implementation optimizes it to a volatile load, it?s wrong.
> If it uses a CAS loop, that?s fine, since the last one should succeed.****
>
>  ****
>
> Hans****
>
>  ****
>
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *David Holmes
> *Sent:* Thursday, January 19, 2012 4:20 PM
> *To:* Vitaly Davidovich; Ruslan Cheremin
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Suggestion: .hardGet() for
> atomicvariables****
>
>  ****
>
> Vitaly,****
>
>  ****
>
> Doug's SequenceLock requires that the data being read is volatile.****
>
>  ****
>
> David****
>
> -------****
>
> * <p> Methods {@code awaitAvailability} and {@code getSequence} can****
>
>  * be used together to define (partially) optimistic read-only methods****
>
>  * that are usually more efficient than ReadWriteLocks when they****
>
>  * apply.  These methods should in general be structured as loops that****
>
>  * await lock availability, then read {@code volatile} fields into****
>
>  * local variables (and may further read other values derived from****
>
>  * these, for example the {@code length} of a {@code volatile} array),****
>
>  * and retry if the sequence number changed while doing so.****
>
>  -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Vitaly
> Davidovich
> *Sent:* Friday, 20 January 2012 7:42 AM
> *To:* Ruslan Cheremin
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Suggestion: .hardGet() for
> atomicvariables****
>
> Failed cas won't write anything to cache (afterall you didn't modify
> anything).  However, it does achieve same memory fencing/ordering as a
> successful cas.  On x86/64 that's because the cmpxchg instruction is
> prefixed with LOCK, which by itself makes the instruction serializing
> irrespective of whether the cmpxchg succeeds.****
>
> Also, small addendum - processor doesn't always issue a RFO (request for
> ownership) before writing - if the cache line is in exclusive state in the
> writing processor, it doesn't need to do that.****
>
> Also Doug Lea has a version of seqlock in his CVS repo for jsr166e - you
> can take a look at it for details.  I'll tell you that there is no funny
> business there with dummy cas operations - he trusts volatile reads :).***
> *
>
> Sent from my phone****
>
> On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:****
>
> I think, it depends on what you name "write". Failed CAS will be "like
> write" in sense of cache coherence traffic -- at least AFAIK -- it
> will request cache line to be in M state (read-for-update), so if
> cache line was in some other core's cache -- it will be invalidated.
> But failed CAS does not really update cache line value, so it seems
> like writeback to main memory not needed. I do not know, does current
> CPUs actually optimize this writeback.
>
> 2012/1/20 Raph Frank <raphfrk at gmail.com>:
> > On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com>
> wrote:
> >> current == value you've just read by .get() few lines ago.
> >
> > Ahh, right.
> >
> > For references, would .compareAndSet(null, null), also add in the
> syncing?
> >
> > Does a compare and set that fails to update count as a write, or just a
> read?
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120119/dd2b2455/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Jan 19 23:20:05 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 20 Jan 2012 14:20:05 +1000
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <CAHjP37Hy_N4jjrHN8t-CRSLiWD2Xdqmcjo=sQaR=B=+gyt+pew@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEJFJCAA.davidcholmes@aapt.net.au>

In general there need not be any specific ordering constraints relating to
the "CAS", hence the j.u.c requirement that they act as volatile read+write.
x86 with its "lock: cmpxchg" is probably the minority case here. SPARC under
TSO is similar to x86 in this regard. But load-linked/store-conditional
based primitives, such as on ARM and PPC, don't impose special ordering
constraints between the ll/sc actions and other loads and stores - hence the
need for memory synchronization instructions (like DMB on ARM) to insert
necessary "fences".

See the JSR-133 Cookbook and the table showing whether atomic instructions
include barriers:

http://g.oswego.edu/dl/jmm/cookbook.html

David
  -----Original Message-----
  From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
  Sent: Friday, 20 January 2012 1:50 PM
  To: Boehm, Hans
  Cc: concurrency-interest at cs.oswego.edu; Ruslan Cheremin; dholmes at ieee.org
  Subject: RE: [concurrency-interest] Suggestion: .hardGet() for
atomicvariables


  Hans,

  I agree on your point and example of a volatile read not being sufficient
and allowing for movement of non volatile read after the 2nd volatile load.
However, my comments were geared towards the CAS loop that was mentioned.
What architectures provide different ordering for successful vs unsuccessful
CAS operations? I'm legitimately interested as I can't see a hardware vendor
that would pick such an option to implement but I'd love to hear of one.

  Additionally, David mentioned that CAS is specified as providing the
ordering without mentioning failure case so it seems like it should work
with the CAS loop we drew from Doug's javadoc.

  Sent from my phone

  On Jan 19, 2012 9:08 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

    I?m not sure we?re understanding each other quite correctly.  The
problem is with read-only uses of seq_lock, using awaitAvailability and
getSequence in Doug?s interface.  The problem is that this effectively
performs



    Read sequence number (volatile read)

    Read data (ordinary read for the questionable case; the problem goes
away if we make it volatile, avoiding data races)

    Read sequence number (volatile read)



    This doesn?t work (with the ordinary read in the middle) because, by
roach motel rules, the middle read can be moved to the end, i.e. outside the
critical section.  Thus we need to replace the final read by something else
that disallows the roach motel movement.  Either getAndAdd or a CAS loop
works.  A single failing CAS (probably, officially) does not.



    I?m not sure that any of this bothers me.  This (i.e. without the
volatile middle read) needs to remain at most an obscure hack because of the
weird and unavoidable restrictions as to what you can put inside the
?read-only critical section?.  And GetAndAdd(0) seems to have exactly the
right semantics to make the obscure and dangerous hack work.  Real fences
are quite hard to define correctly, as we?ve seen in C++.  The fact that
anybody who really wants to implement this obscure and dangerous hack
already has an equally obscure construct to do it with seems almost
satisfying, actually.



    None of which denies that there are other problems in this area that
need solving.



    Hans



    From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
    Sent: Thursday, January 19, 2012 5:23 PM
    To: Boehm, Hans
    Cc: Ruslan Cheremin; dholmes at ieee.org;
concurrency-interest at cs.oswego.edu
    Subject: RE: [concurrency-interest] Suggestion: .hardGet() for
atomicvariables



    Also, having to do getAndAdd(0) to simulate a fence is further evidence
that a Fences API would be useful.  Doing that dummy CAS is a performance
penalty for no good reason otherwise, it seems.

    Sent from my phone

    On Jan 19, 2012 8:15 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

    OK maybe I'm looking at this too x86-centric but under which
architectures would spinning on a seqlock not be enough? That is, why does
the data you read inside the loop need to be volatile, meaning you can't
assume that the spinning CAS won't provide the barrier? Intuitively, even
though the CAS loop reads unrelated memory to the body of the loop, I think
it's reasonable to assume that it provides an acquire fence, at least for a
successful CAS.  Granted that's an assumption but I'm curious under what
circumstances that wouldn't be the right thing to do.

    Honestly, I'm really starting to think that the Fences API that Doug
proposed is the right way to go, similar to C++11x atomics - this would
allow explicit intent rather than mentally reconciling the JMM with actual
hardware.

    Vitaly

    Sent from my phone

    On Jan 19, 2012 8:00 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

    Yes.   It does require volatile, as it should.



    Getting back to the earlier topic, I?m not at all convinced that a
failed CAS behaves like a volatile write according to the Java memory model.
There is no write that can synchronize with anything.  It probably does hold
on X86.



    By my reading, getAndAdd(0) is required to write, and hence does the
right thing.  If an implementation optimizes it to a volatile load, it?s
wrong.  If it uses a CAS loop, that?s fine, since the last one should
succeed.



    Hans



    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David
Holmes
    Sent: Thursday, January 19, 2012 4:20 PM
    To: Vitaly Davidovich; Ruslan Cheremin
    Cc: concurrency-interest at cs.oswego.edu
    Subject: Re: [concurrency-interest] Suggestion: .hardGet() for
atomicvariables



    Vitaly,



    Doug's SequenceLock requires that the data being read is volatile.



    David

    -------

* <p> Methods {@code awaitAvailability} and {@code getSequence} can * be
used together to define (partially) optimistic read-only methods * that are
usually more efficient than ReadWriteLocks when they * apply.  These methods
should in general be structured as loops that * await lock availability,
then read {@code volatile} fields into * local variables (and may further
read other values derived from * these, for example the {@code length} of a
{@code volatile} array), * and retry if the sequence number changed while
doing so.-----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
      Sent: Friday, 20 January 2012 7:42 AM
      To: Ruslan Cheremin
      Cc: concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Suggestion: .hardGet() for
atomicvariables

      Failed cas won't write anything to cache (afterall you didn't modify
anything).  However, it does achieve same memory fencing/ordering as a
successful cas.  On x86/64 that's because the cmpxchg instruction is
prefixed with LOCK, which by itself makes the instruction serializing
irrespective of whether the cmpxchg succeeds.

      Also, small addendum - processor doesn't always issue a RFO (request
for ownership) before writing - if the cache line is in exclusive state in
the writing processor, it doesn't need to do that.

      Also Doug Lea has a version of seqlock in his CVS repo for jsr166e -
you can take a look at it for details.  I'll tell you that there is no funny
business there with dummy cas operations - he trusts volatile reads :).

      Sent from my phone

      On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

      I think, it depends on what you name "write". Failed CAS will be "like
      write" in sense of cache coherence traffic -- at least AFAIK -- it
      will request cache line to be in M state (read-for-update), so if
      cache line was in some other core's cache -- it will be invalidated.
      But failed CAS does not really update cache line value, so it seems
      like writeback to main memory not needed. I do not know, does current
      CPUs actually optimize this writeback.

      2012/1/20 Raph Frank <raphfrk at gmail.com>:
      > On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin
<cheremin at gmail.com> wrote:
      >> current == value you've just read by .get() few lines ago.
      >
      > Ahh, right.
      >
      > For references, would .compareAndSet(null, null), also add in the
syncing?
      >
      > Does a compare and set that fails to update count as a write, or
just a read?
      > _______________________________________________
      > Concurrency-interest mailing list
      > Concurrency-interest at cs.oswego.edu
      > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120120/246c5342/attachment-0001.html>

From m.jevring at iontrading.com  Fri Jan 20 03:48:37 2012
From: m.jevring at iontrading.com (Markus Jevring)
Date: Fri, 20 Jan 2012 09:48:37 +0100
Subject: [concurrency-interest] Re : Lock-free implementation of min
 including assignment
In-Reply-To: <1327012263.80179.YahooMailNeo@web132105.mail.ird.yahoo.com>
References: <4F16D438.8070008@iontrading.com>
	<1327012263.80179.YahooMailNeo@web132105.mail.ird.yahoo.com>
Message-ID: <4F192A65.20704@iontrading.com>

These seem like they would be equivalent to what I posted after having 
applied the optimizations Martin mentioned.
Given that, I would say we're converging on a correct solution =)

On 19/01/2012 23:31, Jeff Hain wrote:
> Hello.
>
> I recently posted some code in this list, containing such (correct I 
> think!) implementations.
>
> Now that I read them again, I realize my Javadoc is misleading, since 
> it says there is (always)
> an atomic set done, while it is only done if needed.
>
> Here they are:
>
>     /**
>      * Atomically sets the specified atomic long with min(atomic long 
> value, specified value),
>      * and returns the new atomic long value (which might have not 
> changed).
>      */
>     private static long setMinAndGet(AtomicLong atomic, long value) {
>         long tmpLastReturned;
>         do {
>             tmpLastReturned = atomic.get();
>             if (tmpLastReturned <= value) {
>                 return tmpLastReturned;
>             }
>             // Here, value < tmpLastReturned,
>             // so we will try to set it as new value.
>         } while (!atomic.compareAndSet(tmpLastReturned, value));
>         return value;
>     }
>
>     /**
>      * Atomically sets the specified atomic long with max(atomic long 
> value, specified value),
>      * and returns the new atomic long value (which might have not 
> changed).
>      */
>     private static long setMaxAndGet(AtomicLong atomic, long value) {
>         long tmpLastReturned;
>         do {
>             tmpLastReturned = atomic.get();
>             if (tmpLastReturned >= value) {
>                 return tmpLastReturned;
>             }
>             // Here, value > tmpLastReturned,
>             // so we will try to set it as new value.
>         } while (!atomic.compareAndSet(tmpLastReturned, value));
>         return value;
>     }
>
> -Jeff
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120120/2a3e7685/attachment.html>

From cheremin at gmail.com  Fri Jan 20 06:10:08 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Fri, 20 Jan 2012 15:10:08 +0400
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
Message-ID: <CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>

2012/1/20 Boehm, Hans <hans.boehm at hp.com>:
> Yes.? ?It does require volatile, as it should.
> Getting back to the earlier topic, I?m not at all convinced that a failed
> CAS behaves like a volatile write according to the Java memory model.? There
> is no write that can synchronize with anything.? It probably does hold on
> X86.


But, back to the topic, _failed_ CAS in this case means "optimistic
approach failed" anyway. Look, your "optimistic read" will be:

long seq = sequence.get();
...some reading...
if( !sequence.CAS(seq, seq) ){
   //...approach failed -> retry
}

So, from my point of view, if CAS failed -- we shouldn't actually care
about it's ordering semantic (although it was interesting to know --
thanks, David -- what ordering does not depend on success/fail). If
CAS succeeded -- it does required guarantee anyway. Am I wrong
somewhere here?


> By my reading, getAndAdd(0) is required to write, and hence does the right
> thing.? If an implementation optimizes it to a volatile load, it?s wrong.
> If it uses a CAS loop, that?s fine, since the last one should succeed.
>
>
>
> Hans
>
>
>
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David
> Holmes
> Sent: Thursday, January 19, 2012 4:20 PM
> To: Vitaly Davidovich; Ruslan Cheremin
>
>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Suggestion: .hardGet() for
> atomicvariables
>
>
>
> Vitaly,
>
>
>
> Doug's SequenceLock requires that the data being read is volatile.
>
>
>
> David
>
> -------
>
> * <p> Methods {@code awaitAvailability} and {@code getSequence} can
>
>  * be used together to define (partially) optimistic read-only methods
>
>  * that are usually more efficient than ReadWriteLocks when they
>
>  * apply.? These methods should in general be structured as loops that
>
>  * await lock availability, then read {@code volatile} fields into
>
>  * local variables (and may further read other values derived from
>
>  * these, for example the {@code length} of a {@code volatile} array),
>
>  * and retry if the sequence number changed while doing so.
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
> Davidovich
> Sent: Friday, 20 January 2012 7:42 AM
> To: Ruslan Cheremin
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Suggestion: .hardGet() for
> atomicvariables
>
> Failed cas won't write anything to cache (afterall you didn't modify
> anything).? However, it does achieve same memory fencing/ordering as a
> successful cas.? On x86/64 that's because the cmpxchg instruction is
> prefixed with LOCK, which by itself makes the instruction serializing
> irrespective of whether the cmpxchg succeeds.
>
> Also, small addendum - processor doesn't always issue a RFO (request for
> ownership) before writing - if the cache line is in exclusive state in the
> writing processor, it doesn't need to do that.
>
> Also Doug Lea has a version of seqlock in his CVS repo for jsr166e - you can
> take a look at it for details.? I'll tell you that there is no funny
> business there with dummy cas operations - he trusts volatile reads :).
>
> Sent from my phone
>
> On Jan 19, 2012 4:05 PM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>
> I think, it depends on what you name "write". Failed CAS will be "like
> write" in sense of cache coherence traffic -- at least AFAIK -- it
> will request cache line to be in M state (read-for-update), so if
> cache line was in some other core's cache -- it will be invalidated.
> But failed CAS does not really update cache line value, so it seems
> like writeback to main memory not needed. I do not know, does current
> CPUs actually optimize this writeback.
>
> 2012/1/20 Raph Frank <raphfrk at gmail.com>:
>> On Thu, Jan 19, 2012 at 8:48 PM, Ruslan Cheremin <cheremin at gmail.com>
>> wrote:
>>> current == value you've just read by .get() few lines ago.
>>
>> Ahh, right.
>>
>> For references, would .compareAndSet(null, null), also add in the syncing?
>>
>> Does a compare and set that fails to update count as a write, or just a
>> read?
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From raphfrk at gmail.com  Fri Jan 20 06:56:12 2012
From: raphfrk at gmail.com (Raph Frank)
Date: Fri, 20 Jan 2012 11:56:12 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
Message-ID: <CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>

Thanks for the info.

On Fri, Jan 20, 2012 at 11:10 AM, Ruslan Cheremin <cheremin at gmail.com> wrote:
> long seq = sequence.get();
> ...some reading...
> if( !sequence.CAS(seq, seq) ){
> ? //...approach failed -> retry
> }
>
> So, from my point of view, if CAS failed -- we shouldn't actually care
> about it's ordering semantic (although it was interesting to know --
> thanks, David -- what ordering does not depend on success/fail). If
> CAS succeeded -- it does required guarantee anyway. Am I wrong
> somewhere here?

Ahh right, that is better than

if (sequence.getAndAdd(0) != seq) {
  <retry>
}

Anyway, thanks all for the info.


From hans.boehm at hp.com  Fri Jan 20 20:27:02 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sat, 21 Jan 2012 01:27:02 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for
 atomicvariables
In-Reply-To: <CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>

> From: Raph Frank [mailto:raphfrk at gmail.com]
> 
> Thanks for the info.
> 
> On Fri, Jan 20, 2012 at 11:10 AM, Ruslan Cheremin <cheremin at gmail.com>
> wrote:
> > long seq = sequence.get();
> > ...some reading...
> > if( !sequence.CAS(seq, seq) ){
> > ? //...approach failed -> retry
> > }
> >
> > So, from my point of view, if CAS failed -- we shouldn't actually
> care
> > about it's ordering semantic (although it was interesting to know --
> > thanks, David -- what ordering does not depend on success/fail). If
> > CAS succeeded -- it does required guarantee anyway. Am I wrong
> > somewhere here?
> 
> Ahh right, that is better than
> 
> if (sequence.getAndAdd(0) != seq) {
>   <retry>
> }
> 
> Anyway, thanks all for the info.

Thanks for the corrections and clarifications.  It does look like we should essentially view CAS as always performing a volatile store, possibly of the original value.

It seems to me that the trade-off between CAS and getAndAdd here is highly implementation dependent.  Clearly if getAndAdd(0) is implemented in terms of CAS(x,x), CAS is faster.  I suspect either could in theory be optimized to a plain old MOV + compiler constraints on x86.

Hans

 



From vitalyd at gmail.com  Fri Jan 20 22:45:43 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 20 Jan 2012 22:45:43 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
Message-ID: <CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>

MOV + compiler barrier wouldn't be the right transformation on x86 because
that wouldn't be the same thing as a volatile write (I.e. store-load
fence), it would be analogous to just store-store.  I think a cas(x, x)
instead of getAndAdd(0) would work and makes sense, although I wonder if
compilers care about such special cases - the cas will already possibly
incur a perf penalty so the extra add instruction is probably insignificant
to optimize.

Vitaly

Sent from my phone
On Jan 20, 2012 8:37 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

> > From: Raph Frank [mailto:raphfrk at gmail.com]
> >
> > Thanks for the info.
> >
> > On Fri, Jan 20, 2012 at 11:10 AM, Ruslan Cheremin <cheremin at gmail.com>
> > wrote:
> > > long seq = sequence.get();
> > > ...some reading...
> > > if( !sequence.CAS(seq, seq) ){
> > >   //...approach failed -> retry
> > > }
> > >
> > > So, from my point of view, if CAS failed -- we shouldn't actually
> > care
> > > about it's ordering semantic (although it was interesting to know --
> > > thanks, David -- what ordering does not depend on success/fail). If
> > > CAS succeeded -- it does required guarantee anyway. Am I wrong
> > > somewhere here?
> >
> > Ahh right, that is better than
> >
> > if (sequence.getAndAdd(0) != seq) {
> >   <retry>
> > }
> >
> > Anyway, thanks all for the info.
>
> Thanks for the corrections and clarifications.  It does look like we
> should essentially view CAS as always performing a volatile store, possibly
> of the original value.
>
> It seems to me that the trade-off between CAS and getAndAdd here is highly
> implementation dependent.  Clearly if getAndAdd(0) is implemented in terms
> of CAS(x,x), CAS is faster.  I suspect either could in theory be optimized
> to a plain old MOV + compiler constraints on x86.
>
> Hans
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120120/1eee4163/attachment.html>

From hans.boehm at hp.com  Sat Jan 21 00:38:58 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sat, 21 Jan 2012 05:38:58 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for
 atomicvariables
In-Reply-To: <CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
	<CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD232742E11@G4W3302.americas.hpqcorp.net>

I'm not sure.  Is there any legitimate Java code that could tell the difference?  An x86 load (MOV) can only be reordered with prior stores.   But I don't immediately see how  an observer thread could tell whether

x = ...
r1 = z.getAndAdd(0);

is reordered.  This would require more thought.  But I agree that it's not too likely that any compiler would try to specially optimize getAndAdd(0).

Hans

From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
Sent: Friday, January 20, 2012 7:46 PM
To: Boehm, Hans
Cc: concurrency-interest at cs.oswego.edu; Ruslan Cheremin; Raph Frank; dholmes at ieee.org
Subject: Re: [concurrency-interest] Suggestion: .hardGet() for atomicvariables


MOV + compiler barrier wouldn't be the right transformation on x86 because that wouldn't be the same thing as a volatile write (I.e. store-load fence), it would be analogous to just store-store.  I think a cas(x, x) instead of getAndAdd(0) would work and makes sense, although I wonder if compilers care about such special cases - the cas will already possibly incur a perf penalty so the extra add instruction is probably insignificant to optimize.

Vitaly

Sent from my phone
On Jan 20, 2012 8:37 PM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:
> From: Raph Frank [mailto:raphfrk at gmail.com<mailto:raphfrk at gmail.com>]
>
> Thanks for the info.
>
> On Fri, Jan 20, 2012 at 11:10 AM, Ruslan Cheremin <cheremin at gmail.com<mailto:cheremin at gmail.com>>
> wrote:
> > long seq = sequence.get();
> > ...some reading...
> > if( !sequence.CAS(seq, seq) ){
> >   //...approach failed -> retry
> > }
> >
> > So, from my point of view, if CAS failed -- we shouldn't actually
> care
> > about it's ordering semantic (although it was interesting to know --
> > thanks, David -- what ordering does not depend on success/fail). If
> > CAS succeeded -- it does required guarantee anyway. Am I wrong
> > somewhere here?
>
> Ahh right, that is better than
>
> if (sequence.getAndAdd(0) != seq) {
>   <retry>
> }
>
> Anyway, thanks all for the info.

Thanks for the corrections and clarifications.  It does look like we should essentially view CAS as always performing a volatile store, possibly of the original value.

It seems to me that the trade-off between CAS and getAndAdd here is highly implementation dependent.  Clearly if getAndAdd(0) is implemented in terms of CAS(x,x), CAS is faster.  I suspect either could in theory be optimized to a plain old MOV + compiler constraints on x86.

Hans




_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120121/b682d91c/attachment.html>

From vitalyd at gmail.com  Sat Jan 21 01:15:18 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 21 Jan 2012 01:15:18 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD232742E11@G4W3302.americas.hpqcorp.net>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
	<CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742E11@G4W3302.americas.hpqcorp.net>
Message-ID: <CAHjP37EsL0R375a9P4PdOZBkzrvPBoqUQK0ua85VJj=0G3rCSQ@mail.gmail.com>

Ruslan's example, if I understood correctly, looks like this:

long val;
do
{
   val = seq.get();
   // do some stuff ...
}  while (!seq.CAS(val, val));

If we take into account that a CAS provides the same barrier whether it
fails or succeeds and let's say the CAS fails, then it's basically:
- volatile write semantics via the CAS (even though it failed)
- volatile load of seq again via seq.get()

In between those two you typically need a StoreLoad (assuming a failing CAS
still provides that barrier, as documented).  I guess the compiler could
omit the StoreLoad on the failing branch since it knows that it didn't
actually store anything and so there's no StoreLoad hazard -- is that what
you meant perhaps? If so, that seems ok, in theory at least.

On Sat, Jan 21, 2012 at 12:38 AM, Boehm, Hans <hans.boehm at hp.com> wrote:

>  I?m not sure.  Is there any legitimate Java code that could tell the
> difference?  An x86 load (MOV) can only be reordered with prior stores.
>  But I don?t immediately see how  an observer thread could tell whether***
> *
>
> ** **
>
> x = ?****
>
> r1 = z.getAndAdd(0);****
>
> ** **
>
> is reordered.  This would require more thought.  But I agree that it?s not
> too likely that any compiler would try to specially optimize getAndAdd(0).
> ****
>
> ** **
>
> Hans****
>
> ** **
>
> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
> *Sent:* Friday, January 20, 2012 7:46 PM
> *To:* Boehm, Hans
> *Cc:* concurrency-interest at cs.oswego.edu; Ruslan Cheremin; Raph Frank;
> dholmes at ieee.org
>
> *Subject:* Re: [concurrency-interest] Suggestion: .hardGet() for
> atomicvariables****
>
>  ** **
>
> MOV + compiler barrier wouldn't be the right transformation on x86 because
> that wouldn't be the same thing as a volatile write (I.e. store-load
> fence), it would be analogous to just store-store.  I think a cas(x, x)
> instead of getAndAdd(0) would work and makes sense, although I wonder if
> compilers care about such special cases - the cas will already possibly
> incur a perf penalty so the extra add instruction is probably insignificant
> to optimize.****
>
> Vitaly****
>
> Sent from my phone****
>
> On Jan 20, 2012 8:37 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:****
>
> > From: Raph Frank [mailto:raphfrk at gmail.com]
> >
> > Thanks for the info.
> >
> > On Fri, Jan 20, 2012 at 11:10 AM, Ruslan Cheremin <cheremin at gmail.com>
> > wrote:
> > > long seq = sequence.get();
> > > ...some reading...
> > > if( !sequence.CAS(seq, seq) ){
> > >   //...approach failed -> retry
> > > }
> > >
> > > So, from my point of view, if CAS failed -- we shouldn't actually
> > care
> > > about it's ordering semantic (although it was interesting to know --
> > > thanks, David -- what ordering does not depend on success/fail). If
> > > CAS succeeded -- it does required guarantee anyway. Am I wrong
> > > somewhere here?
> >
> > Ahh right, that is better than
> >
> > if (sequence.getAndAdd(0) != seq) {
> >   <retry>
> > }
> >
> > Anyway, thanks all for the info.
>
> Thanks for the corrections and clarifications.  It does look like we
> should essentially view CAS as always performing a volatile store, possibly
> of the original value.
>
> It seems to me that the trade-off between CAS and getAndAdd here is highly
> implementation dependent.  Clearly if getAndAdd(0) is implemented in terms
> of CAS(x,x), CAS is faster.  I suspect either could in theory be optimized
> to a plain old MOV + compiler constraints on x86.
>
> Hans
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>



-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120121/a56eab89/attachment-0001.html>

From vitalyd at gmail.com  Sat Jan 21 01:24:07 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 21 Jan 2012 01:24:07 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <CAHjP37EsL0R375a9P4PdOZBkzrvPBoqUQK0ua85VJj=0G3rCSQ@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
	<CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742E11@G4W3302.americas.hpqcorp.net>
	<CAHjP37EsL0R375a9P4PdOZBkzrvPBoqUQK0ua85VJj=0G3rCSQ@mail.gmail.com>
Message-ID: <CAHjP37EAePXDhW8ZiW7ueiAZhjQ-nekPSB+poJR1=6Aa+ikt+A@mail.gmail.com>

Actually, I think if the "// do some stuff" inside the loop performs any
writes to shared memory, then even the failing CAS will need to provide a
StoreLoad so that another thread spinning on this loop will observe those
writes since we're saying that a failing CAS has volatile-write semantics.
 I don't think we're considering any writes inside the loop in this example
though, so maybe it's moot.

Thanks

On Sat, Jan 21, 2012 at 1:15 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Ruslan's example, if I understood correctly, looks like this:
>
> long val;
> do
> {
>    val = seq.get();
>    // do some stuff ...
> }  while (!seq.CAS(val, val));
>
> If we take into account that a CAS provides the same barrier whether it
> fails or succeeds and let's say the CAS fails, then it's basically:
> - volatile write semantics via the CAS (even though it failed)
> - volatile load of seq again via seq.get()
>
> In between those two you typically need a StoreLoad (assuming a failing
> CAS still provides that barrier, as documented).  I guess the compiler
> could omit the StoreLoad on the failing branch since it knows that it
> didn't actually store anything and so there's no StoreLoad hazard -- is
> that what you meant perhaps? If so, that seems ok, in theory at least.
>
> On Sat, Jan 21, 2012 at 12:38 AM, Boehm, Hans <hans.boehm at hp.com> wrote:
>
>>  I?m not sure.  Is there any legitimate Java code that could tell the
>> difference?  An x86 load (MOV) can only be reordered with prior stores.
>>  But I don?t immediately see how  an observer thread could tell whether**
>> **
>>
>> ** **
>>
>> x = ?****
>>
>> r1 = z.getAndAdd(0);****
>>
>> ** **
>>
>> is reordered.  This would require more thought.  But I agree that it?s
>> not too likely that any compiler would try to specially optimize
>> getAndAdd(0).****
>>
>> ** **
>>
>> Hans****
>>
>> ** **
>>
>> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
>> *Sent:* Friday, January 20, 2012 7:46 PM
>> *To:* Boehm, Hans
>> *Cc:* concurrency-interest at cs.oswego.edu; Ruslan Cheremin; Raph Frank;
>> dholmes at ieee.org
>>
>> *Subject:* Re: [concurrency-interest] Suggestion: .hardGet() for
>> atomicvariables****
>>
>>  ** **
>>
>> MOV + compiler barrier wouldn't be the right transformation on x86
>> because that wouldn't be the same thing as a volatile write (I.e.
>> store-load fence), it would be analogous to just store-store.  I think a
>> cas(x, x) instead of getAndAdd(0) would work and makes sense, although I
>> wonder if compilers care about such special cases - the cas will already
>> possibly incur a perf penalty so the extra add instruction is probably
>> insignificant to optimize.****
>>
>> Vitaly****
>>
>> Sent from my phone****
>>
>> On Jan 20, 2012 8:37 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:****
>>
>> > From: Raph Frank [mailto:raphfrk at gmail.com]
>> >
>> > Thanks for the info.
>> >
>> > On Fri, Jan 20, 2012 at 11:10 AM, Ruslan Cheremin <cheremin at gmail.com>
>> > wrote:
>> > > long seq = sequence.get();
>> > > ...some reading...
>> > > if( !sequence.CAS(seq, seq) ){
>> > >   //...approach failed -> retry
>> > > }
>> > >
>> > > So, from my point of view, if CAS failed -- we shouldn't actually
>> > care
>> > > about it's ordering semantic (although it was interesting to know --
>> > > thanks, David -- what ordering does not depend on success/fail). If
>> > > CAS succeeded -- it does required guarantee anyway. Am I wrong
>> > > somewhere here?
>> >
>> > Ahh right, that is better than
>> >
>> > if (sequence.getAndAdd(0) != seq) {
>> >   <retry>
>> > }
>> >
>> > Anyway, thanks all for the info.
>>
>> Thanks for the corrections and clarifications.  It does look like we
>> should essentially view CAS as always performing a volatile store, possibly
>> of the original value.
>>
>> It seems to me that the trade-off between CAS and getAndAdd here is
>> highly implementation dependent.  Clearly if getAndAdd(0) is implemented in
>> terms of CAS(x,x), CAS is faster.  I suspect either could in theory be
>> optimized to a plain old MOV + compiler constraints on x86.
>>
>> Hans
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>>
>
>
>
> --
> Vitaly
> 617-548-7007 (mobile)
>



-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120121/b3f9e33d/attachment.html>

From edsen at libero.it  Mon Jan 23 03:31:10 2012
From: edsen at libero.it (edsen at libero.it)
Date: Mon, 23 Jan 2012 09:31:10 +0100 (CET)
Subject: [concurrency-interest] Help
Message-ID: <8387875.16441641327307470071.JavaMail.defaultUser@defaultHost>

Hello,
 
I need orders my files from list directory, that is changing continously.
My files must be fetched by order-date (oldest from newest...) and I check the age of files from file naming convention. The files are:XXXXX_yymmdd.
Below you can find the code that get the following input parameters: 
 
- this.directory: directory of file list.
- fileRegexp: regular expression in order to retrieve the date.
- datetimeGroup: group where the date is present in the file naming convention.
- timestampFromFileNameFormat: date format convention.
 


// get the list of files currently in the directory



File[] files = 

this.dirListByDate(this.directory,fileRegexp,datetimeGroup,timestampFromFileNameFormat);
 
 






























protected  File[] dirListByDate(final File folder, final Pattern fileRegexp, final int datetimeGroup, final String timestampFromFileNameFormat) throws IOException {



 
if  (!folder.isDirectory() || !folder.exists()) {


throw new
 IOException(folder.getName() +  " : Not a folder or not exist");
}
 
File files[] = this.list(folder, false); 
// don't include subfolder
Arrays.sort(files, newComparator<Object>() {


public int compare(final Object o1, final Object o2) {
 


final String s1 = ((File) o1).getName();


final String s2 = ((File) o2).getName();



final  Matcher m = fileRegexp.matcher(s1);


final  Matcher n = fileRegexp.matcher(s2);



 
if  (m.matches() &amp;&amp; n.matches()) {


final String date1 = m.group(datetimeGroup);


final String date2 = n.group(datetimeGroup);



final long dateAndTime1 = Strings.setDateTimeFromFileName(date1, timestampFromFileNameFormat);


final long dateAndTime2 = Strings.setDateTimeFromFileName(date2, timestampFromFileNameFormat);


 
return dateAndTime1 < dateAndTime2 ? -1 : dateAndTime1 > dateAndTime2 ? 1 : 0;
}

else
{


log.error("Files don't match...File1->" + s1 + " File2 -> " + s2);


alarm.sendAlarm(AlarmInterface.FILE_NAMING_CONVENTION_FAILED,"File Naming Convention incorrect: "
+
s1 + 

" with size " + s1.getBytes() + " of directory " + folder + s2 + 

" with size " + s2.getBytes() + " of directory "+ folder);


return 0;
}
}
});


return files;
 
 














protected File[] list(File folder, boolean includeSubFolder) {


 
if  (!folder.isDirectory()) {


return null;
}
 
File files[] = folder.listFiles();
List<File> list = new ArrayList<File>();


 
for (File file : files) {


 
if  (file.isDirectory()) {


if (includeSubFolder) {
list.add(file); 
  }
} 

else
 {
list.add(file);
}
}
 


if(list.isEmpty()) {


return list.toArray(newFile[] {});
}


return list.toArray(new File[] {});
}
 
What happens? Happens that every thing works for 3-4 traffic days, but sometimes it processes newest files instead of oldest.
I don't understand what can be wrong??
Can you help me in order to fix it?
Can you suggest me a solution?
 
Thanks in advance.
Kind regards.
 
 
 
 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120123/2fdeccc8/attachment.html>

From m.jevring at iontrading.com  Mon Jan 23 06:33:35 2012
From: m.jevring at iontrading.com (Markus Jevring)
Date: Mon, 23 Jan 2012 12:33:35 +0100
Subject: [concurrency-interest] Lock-free implementation of min
 including assignment
In-Reply-To: <CA+kOe08BrQRTV6kDXFqhNkSotR9=b0eq5+WMJZEApVppNqpoPw@mail.gmail.com>
References: <4F16D438.8070008@iontrading.com>
	<CA+kOe09joD8FwP-c+1gFx+nR4gnrOWz9dnYfhHTsYzqDEG=rqw@mail.gmail.com>
	<4F17CAC5.5040409@iontrading.com>
	<CA+kOe08BrQRTV6kDXFqhNkSotR9=b0eq5+WMJZEApVppNqpoPw@mail.gmail.com>
Message-ID: <4F1D458F.8010000@iontrading.com>

Martin,

I actually see now that both your and my implementation are identical.
While mine has two different calls to CAS, yours has one, except it's 
essentially the same, as transform basically takes care of the 
if-statement that, in my code, differentiated between the two cases.

Just thought I should clarify this, in case someone stumbles across my 
code, and thinks it's fine to simply remove the else-CAS.

Markus

On 19/01/2012 14:36, Martin Buchholz wrote:
>
>
> On Wed, Jan 18, 2012 at 23:48, Markus Jevring 
> <m.jevring at iontrading.com <mailto:m.jevring at iontrading.com>> wrote:
>
>     So, by paraphrasing what you have there, if I wanted something
>     like getAndMin(AtomicLong var, long l),
>     I would define transform() as: "return Math.min(current, l)" to
>     get the desired effect, right?
>
>
> Right.
>
>     In essence, I'd be able to skip the cas operation in my else-case?
>
>
> Right. Skipping the CAS when current == next is just an optimization, 
> but an important one for transformations like min or max where current 
> == next is the common case.
>
>     Similarly, minAndGet(AtomicLong var, long l) would simply return
>     "next" instead of "current", right?
>
>
> Right.
>
>     As other people have pointed out, both 'current' and 'min', as it
>     were, may have become obsolete by the time the function returns,
>     but as long as we keep the smallest value in 'var', we should be
>     fine, right?
>
>
> Right.
>
>     Markus
>
>
>     On 18/01/2012 21:23, Martin Buchholz wrote:
>>
>>
>>     On Wed, Jan 18, 2012 at 06:16, Markus Jevring
>>     <m.jevring at iontrading.com <mailto:m.jevring at iontrading.com>> wrote:
>>
>>         If it does, I'm surprised it's not plastered all over the
>>         internet.
>>
>>
>>     We've written something here:
>>     http://g.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/package-info.java?view=co
>>
>>      * <p>It is straightforward to define new utility functions that,
>>     like
>>      * {@code getAndIncrement}, apply a function to a value atomically.
>>      * For example, given some transformation
>>      * <pre> {@code long transform(long input)}</pre>
>>      *
>>      * write your utility method as follows:
>>      * <pre> {@code
>>      * long getAndTransform(AtomicLong var) {
>>      *   while (true) {
>>      *     long current = var.get();
>>      *     long next = transform(current);
>>      *     if (var.compareAndSet(current, next))
>>      *         return current;
>>      *   }
>>      * }}</pre>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120123/e39bb30c/attachment-0001.html>

From nathan.reynolds at oracle.com  Mon Jan 23 10:55:55 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 23 Jan 2012 08:55:55 -0700
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
	<CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
Message-ID: <4F1D830B.70808@oracle.com>

I realize that this discussion has been theoretical.  I would like to 
point out one small gotcha.  Ideally, getAndAdd(0) would compile down to 
x86's "lock xadd" instruction.  But, HotSpot doesn't do that at the 
moment.  The current implementation is as what you see in the JDK Java 
code.  I have included it below.  So, compareAndSet(x, x) would actually 
perform better than getAndAdd(0).  This is because getAndAdd(0) executes 
a few more instructions and it could loop a few times under contention.

   public int getAndAdd(int delta)
   {
     while (true)
     {
       int i = get();
       int j = i + delta;
       if (compareAndSet(i, j))
         return i;
     }
   }

There is an enhancement request to make getAndAdd a compiler intrinsic 
on x86.  The method would be replaced with "lock xadd".  However, this 
won't help for processors that don't have an atomic add instruction 
(e.g. Sparc).  I have asked the Sparc team for an atomic add 
instruction.  I am not sure if it will be done.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/20/2012 8:45 PM, Vitaly Davidovich wrote:
>
> MOV + compiler barrier wouldn't be the right transformation on x86 
> because that wouldn't be the same thing as a volatile write (I.e. 
> store-load fence), it would be analogous to just store-store.  I think 
> a cas(x, x) instead of getAndAdd(0) would work and makes sense, 
> although I wonder if compilers care about such special cases - the cas 
> will already possibly incur a perf penalty so the extra add 
> instruction is probably insignificant to optimize.
>
> Vitaly
>
> Sent from my phone
>
> On Jan 20, 2012 8:37 PM, "Boehm, Hans" <hans.boehm at hp.com 
> <mailto:hans.boehm at hp.com>> wrote:
>
>     > From: Raph Frank [mailto:raphfrk at gmail.com
>     <mailto:raphfrk at gmail.com>]
>     >
>     > Thanks for the info.
>     >
>     > On Fri, Jan 20, 2012 at 11:10 AM, Ruslan Cheremin
>     <cheremin at gmail.com <mailto:cheremin at gmail.com>>
>     > wrote:
>     > > long seq = sequence.get();
>     > > ...some reading...
>     > > if( !sequence.CAS(seq, seq) ){
>     > >   //...approach failed -> retry
>     > > }
>     > >
>     > > So, from my point of view, if CAS failed -- we shouldn't actually
>     > care
>     > > about it's ordering semantic (although it was interesting to
>     know --
>     > > thanks, David -- what ordering does not depend on
>     success/fail). If
>     > > CAS succeeded -- it does required guarantee anyway. Am I wrong
>     > > somewhere here?
>     >
>     > Ahh right, that is better than
>     >
>     > if (sequence.getAndAdd(0) != seq) {
>     > <retry>
>     > }
>     >
>     > Anyway, thanks all for the info.
>
>     Thanks for the corrections and clarifications.  It does look like
>     we should essentially view CAS as always performing a volatile
>     store, possibly of the original value.
>
>     It seems to me that the trade-off between CAS and getAndAdd here
>     is highly implementation dependent.  Clearly if getAndAdd(0) is
>     implemented in terms of CAS(x,x), CAS is faster.  I suspect either
>     could in theory be optimized to a plain old MOV + compiler
>     constraints on x86.
>
>     Hans
>
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120123/62024ddc/attachment.html>

From mlists at juma.me.uk  Mon Jan 23 11:09:09 2012
From: mlists at juma.me.uk (Ismael Juma)
Date: Mon, 23 Jan 2012 16:09:09 +0000
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <4F1D830B.70808@oracle.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
	<CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
	<4F1D830B.70808@oracle.com>
Message-ID: <CAD5tkZaBwbyYEm_S1AHMi=do3eC0NVXxfSStqJNsLAg94yY3yQ@mail.gmail.com>

Hi Nathan,

On Mon, Jan 23, 2012 at 3:55 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

> There is an enhancement request to make getAndAdd a compiler intrinsic on
> x86.  The method would be replaced with "lock xadd".
>

This has been discussed a few times. Do you have any idea if this will be
implemented in the near future?

Best,
Ismael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120123/00a6a88f/attachment.html>

From cheremin at gmail.com  Mon Jan 23 11:19:17 2012
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Mon, 23 Jan 2012 20:19:17 +0400
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <CAD5tkZaBwbyYEm_S1AHMi=do3eC0NVXxfSStqJNsLAg94yY3yQ@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
	<CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
	<4F1D830B.70808@oracle.com>
	<CAD5tkZaBwbyYEm_S1AHMi=do3eC0NVXxfSStqJNsLAg94yY3yQ@mail.gmail.com>
Message-ID: <CAOwENiKd--eJjOAeta_hwy0dpgFdMb4HjU9R6N8-7tnUg2ZgxA@mail.gmail.com>

I can remember some blog post (by Dave Dice, if my memory is correct),
in which this improvement was discussed. And benchmarks, presented
where, shown rather little improvement -- like 15%, or something about
it. Do you have another values for performance improvement?

2012/1/23 Ismael Juma <mlists at juma.me.uk>:
> Hi Nathan,
>
> On Mon, Jan 23, 2012 at 3:55 PM, Nathan Reynolds
> <nathan.reynolds at oracle.com> wrote:
>>
>> There is an enhancement request to make getAndAdd a compiler intrinsic on
>> x86.? The method would be replaced with "lock xadd".
>
>
> This has been discussed a few times. Do you have any idea if this will be
> implemented in the near future?
>
> Best,
> Ismael
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From vitalyd at gmail.com  Mon Jan 23 12:10:05 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 23 Jan 2012 12:10:05 -0500
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <CAOwENiKd--eJjOAeta_hwy0dpgFdMb4HjU9R6N8-7tnUg2ZgxA@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
	<CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
	<4F1D830B.70808@oracle.com>
	<CAD5tkZaBwbyYEm_S1AHMi=do3eC0NVXxfSStqJNsLAg94yY3yQ@mail.gmail.com>
	<CAOwENiKd--eJjOAeta_hwy0dpgFdMb4HjU9R6N8-7tnUg2ZgxA@mail.gmail.com>
Message-ID: <CAHjP37FvveSpjntGnsJb-k+s3oMHnaFpGeUYDoCmzG0Z1c+52A@mail.gmail.com>

http://mechanical-sympathy.blogspot.com/2011/09/adventures-with-atomiclong.html

Sent from my phone
On Jan 23, 2012 11:21 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

> I can remember some blog post (by Dave Dice, if my memory is correct),
> in which this improvement was discussed. And benchmarks, presented
> where, shown rather little improvement -- like 15%, or something about
> it. Do you have another values for performance improvement?
>
> 2012/1/23 Ismael Juma <mlists at juma.me.uk>:
> > Hi Nathan,
> >
> > On Mon, Jan 23, 2012 at 3:55 PM, Nathan Reynolds
> > <nathan.reynolds at oracle.com> wrote:
> >>
> >> There is an enhancement request to make getAndAdd a compiler intrinsic
> on
> >> x86.  The method would be replaced with "lock xadd".
> >
> >
> > This has been discussed a few times. Do you have any idea if this will be
> > implemented in the near future?
> >
> > Best,
> > Ismael
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120123/9f26bee4/attachment.html>

From nathan.reynolds at oracle.com  Mon Jan 23 14:36:40 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 23 Jan 2012 12:36:40 -0700
Subject: [concurrency-interest] Suggestion: .hardGet() for
	atomicvariables
In-Reply-To: <CAD5tkZaBwbyYEm_S1AHMi=do3eC0NVXxfSStqJNsLAg94yY3yQ@mail.gmail.com>
References: <CAHjP37HgNru9wTBymBSDYjTvWBGsgbDkxNv7Cc7Og7F+XEPacw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEJDJCAA.davidcholmes@aapt.net.au>
	<A3E67C2071F49C4CBC4F17E6D77CDDD22E52A736@G9W0339.americas.hpqcorp.net>
	<CAOwENiKp2dYAuotuK6OUPhe8SCdQMBr8smS7waeGc4VTxuSGpQ@mail.gmail.com>
	<CAN1xFdruXAVYbq_jifcymDkN2gkHa7ERGTDjYGqF7whHkn0mXQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD232742CE0@G4W3302.americas.hpqcorp.net>
	<CAHjP37EJepsfSyA1Benp=Z91gFgRzi0bSd_=i8ATupx4Hrw2ww@mail.gmail.com>
	<4F1D830B.70808@oracle.com>
	<CAD5tkZaBwbyYEm_S1AHMi=do3eC0NVXxfSStqJNsLAg94yY3yQ@mail.gmail.com>
Message-ID: <4F1DB6C8.90105@oracle.com>

No clue.  I hope it is.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/23/2012 9:09 AM, Ismael Juma wrote:
> Hi Nathan,
>
> On Mon, Jan 23, 2012 at 3:55 PM, Nathan Reynolds 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     There is an enhancement request to make getAndAdd a compiler
>     intrinsic on x86.  The method would be replaced with "lock xadd".
>
>
> This has been discussed a few times. Do you have any idea if this will 
> be implemented in the near future?
>
> Best,
> Ismael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120123/71dd9794/attachment.html>

From heinz at javaspecialists.eu  Mon Jan 23 15:11:39 2012
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Mon, 23 Jan 2012 22:11:39 +0200
Subject: [concurrency-interest] Help
In-Reply-To: <8387875.16441641327307470071.JavaMail.defaultUser@defaultHost>
References: <8387875.16441641327307470071.JavaMail.defaultUser@defaultHost>
Message-ID: <4F1DBEFB.2070108@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120123/590b0462/attachment-0001.html>

From davidcholmes at aapt.net.au  Mon Jan 23 17:40:18 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 24 Jan 2012 08:40:18 +1000
Subject: [concurrency-interest] Help
In-Reply-To: <4F1DBEFB.2070108@javaspecialists.eu>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEKBJCAA.davidcholmes@aapt.net.au>

This is off-topic for this list.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M. Kabutz
  Sent: Tuesday, 24 January 2012 6:12 AM
  To: edsen at libero.it
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Help


  Does "Strings.setDateTimeFromFileName" by any chance use java.util.SimpleDateFormat stored in a static field?

  Otherwise, could you please send us some working code that we can run in order to see the effect you are talking about?

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz 


  On 1/23/12 10:31 AM, edsen at libero.it wrote: 
    Hello,



    I need orders my files from list directory, that is changing continously.

    My files must be fetched by order-date (oldest from newest...) and I check the age of files from file naming convention. The files are:XXXXX_yymmdd.

    Below you can find the code that get the following input parameters: 



    - this.directory: directory of file list.

    - fileRegexp: regular expression in order to retrieve the date.

    - datetimeGroup: group where the date is present in the file naming convention.

    - timestampFromFileNameFormat: date format convention.




    // get the list of files currently in the directory



    File[] files = 

    this.dirListByDate(this.directory,fileRegexp,datetimeGroup,timestampFromFileNameFormat);








    protected  File[] dirListByDate(final File folder, final Pattern fileRegexp, final int datetimeGroup, final String timestampFromFileNameFormat) throws IOException {



    if  (!folder.isDirectory() || !folder.exists()) {

    throw new

    IOException(folder.getName() +  " : Not a folder or not exist");

    }



    File files[] = this.list(folder, false); 

    // don't include subfolder

    Arrays.sort(files, newComparator<Object>() {

    public int compare(final Object o1, final Object o2) {



    final String s1 = ((File) o1).getName();

    final String s2 = ((File) o2).getName();

    final  Matcher m = fileRegexp.matcher(s1);

    final  Matcher n = fileRegexp.matcher(s2);



    if  (m.matches() && n.matches()) {

    final String date1 = m.group(datetimeGroup);

    final String date2 = n.group(datetimeGroup);

    final long dateAndTime1 = Strings.setDateTimeFromFileName(date1, timestampFromFileNameFormat);

    final long dateAndTime2 = Strings.setDateTimeFromFileName(date2, timestampFromFileNameFormat);



    return dateAndTime1 < dateAndTime2 ? -1 : dateAndTime1 > dateAndTime2 ? 1 : 0;

    }

    else

    {

    log.error("Files don't match...File1->" + s1 + " File2 -> " + s2);

    alarm.sendAlarm(AlarmInterface.FILE_NAMING_CONVENTION_FAILED,"File Naming Convention incorrect: "

    +

    s1 + 

    " with size " + s1.getBytes() + " of directory " + folder + s2 + 

    " with size " + s2.getBytes() + " of directory "+ folder);

    return 0;

    }

    }

    });

    return files;








    protected File[] list(File folder, boolean includeSubFolder) {



    if  (!folder.isDirectory()) {

    return null;

    }



    File files[] = folder.listFiles();

    List<File> list = new ArrayList<File>();



    for (File file : files) {



    if  (file.isDirectory()) {

    if (includeSubFolder) {

    list.add(file); 

      }

    } 

    else

    {

    list.add(file);

    }

    }



    if(list.isEmpty()) {

    return list.toArray(newFile[] {});

    }

    return list.toArray(new File[] {});

    }



    What happens? Happens that every thing works for 3-4 traffic days, but sometimes it processes newest files instead of oldest.

    I don't understand what can be wrong??

    Can you help me in order to fix it?

    Can you suggest me a solution?



    Thanks in advance.

    Kind regards.









----------------------------------------------------------------------------
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120124/28e96883/attachment.html>

From edsen at libero.it  Tue Jan 24 03:15:41 2012
From: edsen at libero.it (edsen at libero.it)
Date: Tue, 24 Jan 2012 09:15:41 +0100 (CET)
Subject: [concurrency-interest] R: RE:  Help
Message-ID: <18576968.15694301327392941533.JavaMail.defaultUser@defaultHost>


Hi all,

 

sorry I forgot to put the code for that function.

 

In the class is defined:


public static SimpleDateFormat sdfInput = (SimpleDateFormat) SimpleDateFormat.getInstance();
protected static long timestampFromFileName = -1;

 
The function is:
public static long setDateTimeFromFileName(final String datetime,final String dataFormat) {


try {timestampFromFileName = convertTimestampFromStringToLong(datetime,dataFormat);
      } catch (Exception e) {
        timestampFromFileName = -1;
     }
       return timestampFromFileName;
}
 





public static long convertTimestampFromStringToLong(String timestamp,String format) throws Exception {
sdfInput.applyPattern(format);


sdfInput.setTimeZone(TimeZone.getTimeZone("GMT"));


return sdfInput.parse(timestamp).getTime();
}


 
Thanks for your attention.
 
 

 
----Messaggio originale----
Da: davidcholmes at aapt.net.au
Data: 23-gen-2012 23.40
A: "Dr Heinz M. Kabutz"<heinz at javaspecialists.eu>, <edsen at libero.it>
Cc: <concurrency-interest at cs.oswego.edu>
Ogg: RE: [concurrency-interest] Help

? 
This is off-topic for this list.
 
David

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M. Kabutz
Sent: Tuesday, 24 January 2012 6:12 AM
To: edsen at libero.it
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Help

Does "Strings.setDateTimeFromFileName" by any chance use java.util.SimpleDateFormat stored in a static field?

Otherwise, could you please send us some working code that we can run in order to see the effect you are talking about?
Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 72 850 460
Skype: kabutz 


On 1/23/12 10:31 AM, edsen at libero.it wrote: 

Hello,
 
I need orders my files from list directory, that is changing continously.
My files must be fetched by order-date (oldest from newest...) and I check the age of files from file naming convention. The files are:XXXXX_yymmdd.
Below you can find the code that get the following input parameters: 
 
- this.directory: directory of file list.
- fileRegexp: regular expression in order to retrieve the date.
- datetimeGroup: group where the date is present in the file naming convention.
- timestampFromFileNameFormat: date format convention.
 


// get the list of files currently in the directory

 



File[] files = 

this.dirListByDate(this.directory,fileRegexp,datetimeGroup,timestampFromFileNameFormat);
 
 
 



protected  File[] dirListByDate(final File folder, final Pattern fileRegexp, final int datetimeGroup, final String timestampFromFileNameFormat) throws IOException {

 
if  (!folder.isDirectory() || !folder.exists()) {

throw new
IOException(folder.getName() +  " : Not a folder or not exist");
}
 
File files[] = this.list(folder, false); 
// don't include subfolder
Arrays.sort(files, newComparator<Object>() {

public int compare(final Object o1, final Object o2) {
 

final String s1 = ((File) o1).getName();

final String s2 = ((File) o2).getName();

final  Matcher m = fileRegexp.matcher(s1);

final  Matcher n = fileRegexp.matcher(s2);

 
if  (m.matches() &amp;&amp; n.matches()) {

final String date1 = m.group(datetimeGroup);

final String date2 = n.group(datetimeGroup);

final long dateAndTime1 = Strings.setDateTimeFromFileName(date1, timestampFromFileNameFormat);

final long dateAndTime2 = Strings.setDateTimeFromFileName(date2, timestampFromFileNameFormat);
 
return dateAndTime1 < dateAndTime2 ? -1 : dateAndTime1 > dateAndTime2 ? 1 : 0;
}
else
{

log.error("Files don't match...File1->" + s1 + " File2 -> " + s2);

alarm.sendAlarm(AlarmInterface.FILE_NAMING_CONVENTION_FAILED,"File Naming Convention incorrect: "
+
s1 + 

" with size " + s1.getBytes() + " of directory " + folder + s2 + 

" with size " + s2.getBytes() + " of directory "+ folder);

return 0;
}
}
});

return files;
 
 
 



protected File[] list(File folder, boolean includeSubFolder) {

 
if  (!folder.isDirectory()) {

return null;
}
 
File files[] = folder.listFiles();
List<File> list = new ArrayList<File>();

 
for (File file : files) {

 
if  (file.isDirectory()) {

if (includeSubFolder) {
list.add(file); 
  }
} 

else
{
list.add(file);
}
}
 

if(list.isEmpty()) {

return list.toArray(newFile[] {});
}

return list.toArray(new File[] {});
}
 
What happens? Happens that every thing works for 3-4 traffic days, but sometimes it processes newest files instead of oldest.
I don't understand what can be wrong??
Can you help me in order to fix it?
Can you suggest me a solution?
 
Thanks in advance.
Kind regards.
 
 
 
 
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
  



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120124/cd4e605e/attachment-0001.html>

From martinrb at google.com  Tue Jan 24 16:20:21 2012
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 24 Jan 2012 13:20:21 -0800
Subject: [concurrency-interest] Lock-free implementation of min
	including assignment
In-Reply-To: <4F1D458F.8010000@iontrading.com>
References: <4F16D438.8070008@iontrading.com>
	<CA+kOe09joD8FwP-c+1gFx+nR4gnrOWz9dnYfhHTsYzqDEG=rqw@mail.gmail.com>
	<4F17CAC5.5040409@iontrading.com>
	<CA+kOe08BrQRTV6kDXFqhNkSotR9=b0eq5+WMJZEApVppNqpoPw@mail.gmail.com>
	<4F1D458F.8010000@iontrading.com>
Message-ID: <CA+kOe0_tnjYgmxU2av7LjGduinLJ9oX2V7wxXBNifMaq-8bwUw@mail.gmail.com>

Suggestions for further documentation improvement are welcome.

If and when lambdas arrive in jdk-land, we can consider adding methods like

long transformAndGet(lambda(long): long)

On Mon, Jan 23, 2012 at 03:33, Markus Jevring <m.jevring at iontrading.com>wrote:

>  Martin,
>
> I actually see now that both your and my implementation are identical.
> While mine has two different calls to CAS, yours has one, except it's
> essentially the same, as transform basically takes care of the if-statement
> that, in my code, differentiated between the two cases.
>
> Just thought I should clarify this, in case someone stumbles across my
> code, and thinks it's fine to simply remove the else-CAS.
>
> Markus
>
>
> On 19/01/2012 14:36, Martin Buchholz wrote:
>
>
>
> On Wed, Jan 18, 2012 at 23:48, Markus Jevring <m.jevring at iontrading.com>wrote:
>
>>  So, by paraphrasing what you have there, if I wanted something like
>> getAndMin(AtomicLong var, long l),
>> I would define transform() as: "return Math.min(current, l)" to get the
>> desired effect, right?
>>
>
>  Right.
>
>
>>  In essence, I'd be able to skip the cas operation in my else-case?
>>
>>
>  Right. Skipping the CAS when current == next is just an optimization,
> but an important one for transformations like min or max where current ==
> next is the common case.
>
>
>>  Similarly, minAndGet(AtomicLong var, long l) would simply return "next"
>> instead of "current", right?
>>
>>
>  Right.
>
>
>>  As other people have pointed out, both 'current' and 'min', as it were,
>> may have become obsolete by the time the function returns, but as long as
>> we keep the smallest value in 'var', we should be fine, right?
>>
>>
>  Right.
>
>
>>  Markus
>>
>>
>> On 18/01/2012 21:23, Martin Buchholz wrote:
>>
>>
>>
>> On Wed, Jan 18, 2012 at 06:16, Markus Jevring <m.jevring at iontrading.com>wrote:
>>
>>> If it does, I'm surprised it's not plastered all over the internet.
>>
>>
>>  We've written something here:
>>
>> http://g.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/package-info.java?view=co
>>
>>   * <p>It is straightforward to define new utility functions that, like
>>  * {@code getAndIncrement}, apply a function to a value atomically.
>>  * For example, given some transformation
>>  * <pre> {@code long transform(long input)}</pre>
>>  *
>>  * write your utility method as follows:
>>  *  <pre> {@code
>>  * long getAndTransform(AtomicLong var) {
>>  *   while (true) {
>>  *     long current = var.get();
>>  *     long next = transform(current);
>>  *     if (var.compareAndSet(current, next))
>>  *         return current;
>>  *   }
>>  * }}</pre>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120124/b24d60fa/attachment.html>

From aleksandar.prokopec at gmail.com  Wed Jan 25 14:48:49 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Wed, 25 Jan 2012 20:48:49 +0100
Subject: [concurrency-interest] Array allocation and access on the JVM
Message-ID: <4F205CA1.3010801@gmail.com>

Hello,

I've recently posted the following question about memory contention on 
StackOverflow:

http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention

Basically, I have several threads defined like this:

final class Worker extends Thread {
     Foo[] array = new Foo[1024];
     int sz;

     public Worker(int _sz) {
         sz = _sz;
     }

     public void run() {
         //Foo[] arr = new Foo[1024];
         Foo[] arr = array;
         loop(arr);
     }

     public void loop(Foo[] arr) {
         int i = 0;
         int pos = 512;
         Foo v = new Foo();
         while (i < sz) {
             if (i % 2 == 0) {
                 arr[pos] = v;
                 pos += 1;
             } else {
                 pos -= 1;
                 v = arr[pos];
             }
             i++;
         }
     }
}

The program takes 2 parameters - `size` and `par`. `size` is the size of 
the workload, and `par` is the number of threads to distribute the 
workload to.
What each thread does is repetitively write and read an array location 
512, in total `size`/ `par` times. Each thread reads its own array 
(locations being written to should be at least 1024*4 bytes away from 
each other).

I've measured the speedup on an 2x 4-core Intel Xeon  server and 
observed the following speedups for 1, 2, 4 and 8 processors (7 
repetitions each):

 >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
 >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
 >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
 >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]

Apparently, this is due to memory contention.
What I've observed is that if I comment the line `Foo[] arr = array` and 
uncomment the line above it to allocate the array within the thread, I get:

 >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
 >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
 >>> All running times: [578, 508, 589, 571, 617, 643, 645]
 >>> All running times: [330, 299, 300, 322, 331, 324, 575]

My guess is that in the second case the arrays get allocated in thread 
local allocation buffers, so the array positions are much further from 
each other.
Still, even in the first case processors should be writing to separate 
cache lines, so why does the first example scale so badly?

Can anyone explain these numbers? Why does this memory contention happen?

Thank you,
Aleksandar Prokopec



------------------------------------------------------------------------------

// The complete runnable program:


import java.util.ArrayList;

class MultiStackJavaExperiment {

     final class Foo {
         int x = 0;
     }

     final class Worker extends Thread {
         Foo[] array = new Foo[1024];
         int sz;

         public Worker(int _sz) {
             sz = _sz;
         }

         public void run() {
             Foo[] arr = new Foo[1024];
             //Foo[] arr = array;
             loop(arr);
         }

         public void loop(Foo[] arr) {
             int i = 0;
             int pos = 512;
             Foo v = new Foo();
             while (i < sz) {
                 if (i % 2 == 0) {
                     arr[pos] = v;
                     pos += 1;
                 } else {
                     pos -= 1;
                     v = arr[pos];
                 }
                 i++;
             }
         }
     }

     public static void main(String[] args) {
         (new MultiStackJavaExperiment()).mainMethod(args);
     }

     int size = Integer.parseInt(System.getProperty("size"));
     int par = Integer.parseInt(System.getProperty("par"));

     public void mainMethod(String[] args) {
         int times = 0;
         if (args.length == 0) times = 1;
         else times = Integer.parseInt(args[0]);
         ArrayList < Long > measurements = new ArrayList < Long > ();

         for (int i = 0; i < times; i++) {
             long start = System.currentTimeMillis();
             run();
             long end = System.currentTimeMillis();

             long time = (end - start);
             System.out.println(i + ") Running time: " + time + " ms");
             measurements.add(time);
         }

         System.out.println(">>>");
         System.out.println(">>> All running times: " + measurements);
         System.out.println(">>>");
     }

     public void run() {
         int sz = size / par;
         ArrayList < Thread > threads = new ArrayList < Thread > ();

         for (int i = 0; i < par; i++) {
             threads.add(new Worker(sz));
             threads.get(i).start();
         }
         for (int i = 0; i < par; i++) {
             try {
                 threads.get(i).join();
             } catch (Exception e) {}
         }
     }

}
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120125/0d185c48/attachment.html>

From vitalyd at gmail.com  Wed Jan 25 17:41:48 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 25 Jan 2012 17:41:48 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <4F205CA1.3010801@gmail.com>
References: <4F205CA1.3010801@gmail.com>
Message-ID: <CAHjP37H+Cpu17Kyasx1yEP1AAjv57QzNKhNKD-pvtCZXsd6_bA@mail.gmail.com>

Perhaps the JVM prefetches additional cache lines which then need to be
invalidated by coherence once other cores start writing to them.  In the
example where one thread allocates all of the arrays upfront, they'll be
contiguous.  Once that memory is written by other cores it may be that even
though a given core is writing to its own cache line, it may have other
cache lines in its cache due to prefetch, which will be invalidated by some
other core that's writing to that line.  Multiply this by all your cores
and perhaps you get inadvertent sharing.

With TLAB allocation memory is spread out more, as you suggest, so it
avoids this type of problem.   If you increase the array size as you did
then perhaps the distance between the memory is beyond prefetch boundary
and you get scaling (though with memory waste); the larger size may even
bypass TLAB altogether, but it doesn't matter due to distance.

Just a guess though ... you can also try asking the GC group for insight.

Sent from my phone
On Jan 25, 2012 2:51 PM, "Aleksandar Prokopec" <
aleksandar.prokopec at gmail.com> wrote:

>  Hello,
>
> I've recently posted the following question about memory contention on
> StackOverflow:
>
>
> http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention
>
> Basically, I have several threads defined like this:
>
> final class Worker extends Thread {
>     Foo[] array = new Foo[1024];
>     int sz;
>
>     public Worker(int _sz) {
>         sz = _sz;
>     }
>
>     public void run() {
>         //Foo[] arr = new Foo[1024];
>         Foo[] arr = array;
>         loop(arr);
>     }
>
>     public void loop(Foo[] arr) {
>         int i = 0;
>         int pos = 512;
>         Foo v = new Foo();
>         while (i < sz) {
>             if (i % 2 == 0) {
>                 arr[pos] = v;
>                 pos += 1;
>             } else {
>                 pos -= 1;
>                 v = arr[pos];
>             }
>             i++;
>         }
>     }
> }
>
> The program takes 2 parameters - `size` and `par`. `size` is the size of
> the workload, and `par` is the number of threads to distribute the workload
> to.
> What each thread does is repetitively write and read an array location
> 512, in total `size`/ `par` times. Each thread reads its own array
> (locations being written to should be at least 1024*4 bytes away from each
> other).
>
> I've measured the speedup on an 2x 4-core Intel Xeon  server and observed
> the following speedups for 1, 2, 4 and 8 processors (7 repetitions each):
>
> >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
> >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
> >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
> >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
>
> Apparently, this is due to memory contention.
> What I've observed is that if I comment the line `Foo[] arr = array` and
> uncomment the line above it to allocate the array within the thread, I get:
>
> >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
> >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
> >>> All running times: [578, 508, 589, 571, 617, 643, 645]
> >>> All running times: [330, 299, 300, 322, 331, 324, 575]
>
> My guess is that in the second case the arrays get allocated in thread
> local allocation buffers, so the array positions are much further from each
> other.
> Still, even in the first case processors should be writing to separate
> cache lines, so why does the first example scale so badly?
>
> Can anyone explain these numbers? Why does this memory contention happen?
>
> Thank you,
> Aleksandar Prokopec
>
>
>
>
> ------------------------------------------------------------------------------
>
> // The complete runnable program:
>
>
> import java.util.ArrayList;
>
> class MultiStackJavaExperiment {
>
>     final class Foo {
>         int x = 0;
>     }
>
>     final class Worker extends Thread {
>         Foo[] array = new Foo[1024];
>         int sz;
>
>         public Worker(int _sz) {
>             sz = _sz;
>         }
>
>         public void run() {
>             Foo[] arr = new Foo[1024];
>             //Foo[] arr = array;
>             loop(arr);
>         }
>
>         public void loop(Foo[] arr) {
>             int i = 0;
>             int pos = 512;
>             Foo v = new Foo();
>             while (i < sz) {
>                 if (i % 2 == 0) {
>                     arr[pos] = v;
>                     pos += 1;
>                 } else {
>                     pos -= 1;
>                     v = arr[pos];
>                 }
>                 i++;
>             }
>         }
>     }
>
>     public static void main(String[] args) {
>         (new MultiStackJavaExperiment()).mainMethod(args);
>     }
>
>     int size = Integer.parseInt(System.getProperty("size"));
>     int par = Integer.parseInt(System.getProperty("par"));
>
>     public void mainMethod(String[] args) {
>         int times = 0;
>         if (args.length == 0) times = 1;
>         else times = Integer.parseInt(args[0]);
>         ArrayList < Long > measurements = new ArrayList < Long > ();
>
>         for (int i = 0; i < times; i++) {
>             long start = System.currentTimeMillis();
>             run();
>             long end = System.currentTimeMillis();
>
>             long time = (end - start);
>             System.out.println(i + ") Running time: " + time + " ms");
>             measurements.add(time);
>         }
>
>         System.out.println(">>>");
>         System.out.println(">>> All running times: " + measurements);
>         System.out.println(">>>");
>     }
>
>     public void run() {
>         int sz = size / par;
>         ArrayList < Thread > threads = new ArrayList < Thread > ();
>
>         for (int i = 0; i < par; i++) {
>             threads.add(new Worker(sz));
>             threads.get(i).start();
>         }
>         for (int i = 0; i < par; i++) {
>             try {
>                 threads.get(i).join();
>             } catch (Exception e) {}
>         }
>     }
>
> }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120125/b0a4cdae/attachment-0001.html>

From dmitry.zaslavsky at gmail.com  Wed Jan 25 18:46:58 2012
From: dmitry.zaslavsky at gmail.com (Dmitry Zaslavsky)
Date: Wed, 25 Jan 2012 18:46:58 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <4F205CA1.3010801@gmail.com>
References: <4F205CA1.3010801@gmail.com>
Message-ID: <06DDDC87-348A-446B-8969-FB8EEB1CD5AF@gmail.com>

This is a complete guess, but in C++ on NUMA system in Linux with default allocation strategy, you get into classical pattern of allocating memory local to the the node starting the threads and the other threads will pay perf penalty. 

While in c++ its a know "feature", I don't know if it is your case in java.

Do you have vtune to check?

Sent from mobile device

On Jan 25, 2012, at 2:48 PM, Aleksandar Prokopec <aleksandar.prokopec at gmail.com> wrote:

> Hello,
> 
> I've recently posted the following question about memory contention on StackOverflow:
> 
> http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention
> 
> Basically, I have several threads defined like this:
> 
> final class Worker extends Thread {
>     Foo[] array = new Foo[1024];
>     int sz;
> 
>     public Worker(int _sz) {
>         sz = _sz;
>     }
> 
>     public void run() {
>         //Foo[] arr = new Foo[1024];
>         Foo[] arr = array;
>         loop(arr);
>     }
> 
>     public void loop(Foo[] arr) {
>         int i = 0;
>         int pos = 512;
>         Foo v = new Foo();
>         while (i < sz) {
>             if (i % 2 == 0) {
>                 arr[pos] = v;
>                 pos += 1;
>             } else {
>                 pos -= 1;
>                 v = arr[pos];
>             }
>             i++;
>         }
>     }
> }
> 
> The program takes 2 parameters - `size` and `par`. `size` is the size of the workload, and `par` is the number of threads to distribute the workload to.
> What each thread does is repetitively write and read an array location 512, in total `size`/ `par` times. Each thread reads its own array (locations being written to should be at least 1024*4 bytes away from each other).
> 
> I've measured the speedup on an 2x 4-core Intel Xeon  server and observed the following speedups for 1, 2, 4 and 8 processors (7 repetitions each):
> 
> >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
> >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
> >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
> >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
> 
> Apparently, this is due to memory contention.
> What I've observed is that if I comment the line `Foo[] arr = array` and uncomment the line above it to allocate the array within the thread, I get:
> 
> >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
> >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
> >>> All running times: [578, 508, 589, 571, 617, 643, 645]
> >>> All running times: [330, 299, 300, 322, 331, 324, 575]
> 
> My guess is that in the second case the arrays get allocated in thread local allocation buffers, so the array positions are much further from each other.
> Still, even in the first case processors should be writing to separate cache lines, so why does the first example scale so badly?
> 
> Can anyone explain these numbers? Why does this memory contention happen?
> 
> Thank you,
> Aleksandar Prokopec
> 
> 
> 
> ------------------------------------------------------------------------------
> 
> // The complete runnable program:
> 
> 
> import java.util.ArrayList;
> 
> class MultiStackJavaExperiment {
> 
>     final class Foo {
>         int x = 0;
>     }
> 
>     final class Worker extends Thread {
>         Foo[] array = new Foo[1024];
>         int sz;
> 
>         public Worker(int _sz) {
>             sz = _sz;
>         }
> 
>         public void run() {
>             Foo[] arr = new Foo[1024];
>             //Foo[] arr = array;
>             loop(arr);
>         }
> 
>         public void loop(Foo[] arr) {
>             int i = 0;
>             int pos = 512;
>             Foo v = new Foo();
>             while (i < sz) {
>                 if (i % 2 == 0) {
>                     arr[pos] = v;
>                     pos += 1;
>                 } else {
>                     pos -= 1;
>                     v = arr[pos];
>                 }
>                 i++;
>             }
>         }
>     }
> 
>     public static void main(String[] args) {
>         (new MultiStackJavaExperiment()).mainMethod(args);
>     }
> 
>     int size = Integer.parseInt(System.getProperty("size"));
>     int par = Integer.parseInt(System.getProperty("par"));
> 
>     public void mainMethod(String[] args) {
>         int times = 0;
>         if (args.length == 0) times = 1;
>         else times = Integer.parseInt(args[0]);
>         ArrayList < Long > measurements = new ArrayList < Long > ();
> 
>         for (int i = 0; i < times; i++) {
>             long start = System.currentTimeMillis();
>             run();
>             long end = System.currentTimeMillis();
> 
>             long time = (end - start);
>             System.out.println(i + ") Running time: " + time + " ms");
>             measurements.add(time);
>         }
> 
>         System.out.println(">>>");
>         System.out.println(">>> All running times: " + measurements);
>         System.out.println(">>>");
>     }
> 
>     public void run() {
>         int sz = size / par;
>         ArrayList < Thread > threads = new ArrayList < Thread > ();
> 
>         for (int i = 0; i < par; i++) {
>             threads.add(new Worker(sz));
>             threads.get(i).start();
>         }
>         for (int i = 0; i < par; i++) {
>             try {
>                 threads.get(i).join();
>             } catch (Exception e) {}
>         }
>     }
> 
> }
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120125/09baa3fc/attachment.html>

From dl at cs.oswego.edu  Wed Jan 25 18:55:38 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 25 Jan 2012 18:55:38 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <4F205CA1.3010801@gmail.com>
References: <4F205CA1.3010801@gmail.com>
Message-ID: <4F20967A.6090703@cs.oswego.edu>

On 01/25/12 14:48, Aleksandar Prokopec wrote:
> Hello,
>
> I've recently posted the following question about memory contention on
> StackOverflow:


I asked Aleksandar offlist to check the effects of -XX:+UseCondCardMark
which as always these days was the main problem. Really, everyone
should always use this switch on hotspot on multiprocessors.
(or -XX:+UseG1GC, although it is often a little worse on throughput
but better-behaved for large heaps).

As I mentioned in a post from a few weeks ago, see Dive Dice's blog
http://blogs.oracle.com/dave/entry/false_sharing_induced_by_card


-Doug





>
> http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention
>
> Basically, I have several threads defined like this:
>
> final class Worker extends Thread {
> Foo[] array = new Foo[1024];
> int sz;
>
> public Worker(int _sz) {
> sz = _sz;
> }
>
> public void run() {
> //Foo[] arr = new Foo[1024];
> Foo[] arr = array;
> loop(arr);
> }
>
> public void loop(Foo[] arr) {
> int i = 0;
> int pos = 512;
> Foo v = new Foo();
> while (i < sz) {
> if (i % 2 == 0) {
> arr[pos] = v;
> pos += 1;
> } else {
> pos -= 1;
> v = arr[pos];
> }
> i++;
> }
> }
> }
>
> The program takes 2 parameters - `size` and `par`. `size` is the size of the
> workload, and `par` is the number of threads to distribute the workload to.
> What each thread does is repetitively write and read an array location 512, in
> total `size`/ `par` times. Each thread reads its own array (locations being
> written to should be at least 1024*4 bytes away from each other).
>
> I've measured the speedup on an 2x 4-core Intel Xeon server and observed the
> following speedups for 1, 2, 4 and 8 processors (7 repetitions each):
>
>  >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
>  >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
>  >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
>  >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
>
> Apparently, this is due to memory contention.
> What I've observed is that if I comment the line `Foo[] arr = array` and
> uncomment the line above it to allocate the array within the thread, I get:
>
>  >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
>  >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
>  >>> All running times: [578, 508, 589, 571, 617, 643, 645]
>  >>> All running times: [330, 299, 300, 322, 331, 324, 575]
>
> My guess is that in the second case the arrays get allocated in thread local
> allocation buffers, so the array positions are much further from each other.
> Still, even in the first case processors should be writing to separate cache
> lines, so why does the first example scale so badly?
>
> Can anyone explain these numbers? Why does this memory contention happen?
>
> Thank you,
> Aleksandar Prokopec
>
>
>
> ------------------------------------------------------------------------------
>
> // The complete runnable program:
>
>
> import java.util.ArrayList;
>
> class MultiStackJavaExperiment {
>
> final class Foo {
> int x = 0;
> }
>
> final class Worker extends Thread {
> Foo[] array = new Foo[1024];
> int sz;
>
> public Worker(int _sz) {
> sz = _sz;
> }
>
> public void run() {
> Foo[] arr = new Foo[1024];
> //Foo[] arr = array;
> loop(arr);
> }
>
> public void loop(Foo[] arr) {
> int i = 0;
> int pos = 512;
> Foo v = new Foo();
> while (i < sz) {
> if (i % 2 == 0) {
> arr[pos] = v;
> pos += 1;
> } else {
> pos -= 1;
> v = arr[pos];
> }
> i++;
> }
> }
> }
>
> public static void main(String[] args) {
> (new MultiStackJavaExperiment()).mainMethod(args);
> }
>
> int size = Integer.parseInt(System.getProperty("size"));
> int par = Integer.parseInt(System.getProperty("par"));
>
> public void mainMethod(String[] args) {
> int times = 0;
> if (args.length == 0) times = 1;
> else times = Integer.parseInt(args[0]);
> ArrayList < Long > measurements = new ArrayList < Long > ();
>
> for (int i = 0; i < times; i++) {
> long start = System.currentTimeMillis();
> run();
> long end = System.currentTimeMillis();
>
> long time = (end - start);
> System.out.println(i + ") Running time: " + time + " ms");
> measurements.add(time);
> }
>
> System.out.println(">>>");
> System.out.println(">>> All running times: " + measurements);
> System.out.println(">>>");
> }
>
> public void run() {
> int sz = size / par;
> ArrayList < Thread > threads = new ArrayList < Thread > ();
>
> for (int i = 0; i < par; i++) {
> threads.add(new Worker(sz));
> threads.get(i).start();
> }
> for (int i = 0; i < par; i++) {
> try {
> threads.get(i).join();
> } catch (Exception e) {}
> }
> }
>
> }
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Wed Jan 25 19:13:32 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 25 Jan 2012 19:13:32 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <4F20967A.6090703@cs.oswego.edu>
References: <4F205CA1.3010801@gmail.com>
	<4F20967A.6090703@cs.oswego.edu>
Message-ID: <CAHjP37FA7dzr4Aybr+X18p+ukjYW-co-Vezsn07ikxXUUymSmQ@mail.gmail.com>

I must say that's a really good hypothesis and would perfectly explain the
observations by Aleksandar- nice one Doug! :)

TLAB sizes are adjusted ergonomically (if not set via flag) as I understand
it, so even though it doesn't really make sense to have them smaller than
32k, they're masking away this issue.

Sent from my phone
On Jan 25, 2012 6:57 PM, "Doug Lea" <dl at cs.oswego.edu> wrote:

> On 01/25/12 14:48, Aleksandar Prokopec wrote:
>
>> Hello,
>>
>> I've recently posted the following question about memory contention on
>> StackOverflow:
>>
>
>
> I asked Aleksandar offlist to check the effects of -XX:+UseCondCardMark
> which as always these days was the main problem. Really, everyone
> should always use this switch on hotspot on multiprocessors.
> (or -XX:+UseG1GC, although it is often a little worse on throughput
> but better-behaved for large heaps).
>
> As I mentioned in a post from a few weeks ago, see Dive Dice's blog
> http://blogs.oracle.com/dave/**entry/false_sharing_induced_**by_card<http://blogs.oracle.com/dave/entry/false_sharing_induced_by_card>
>
>
> -Doug
>
>
>
>
>
>
>> http://stackoverflow.com/**questions/8942396/array-**
>> allocation-and-access-on-the-**java-virtual-machine-and-**
>> memory-contention<http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention>
>>
>> Basically, I have several threads defined like this:
>>
>> final class Worker extends Thread {
>> Foo[] array = new Foo[1024];
>> int sz;
>>
>> public Worker(int _sz) {
>> sz = _sz;
>> }
>>
>> public void run() {
>> //Foo[] arr = new Foo[1024];
>> Foo[] arr = array;
>> loop(arr);
>> }
>>
>> public void loop(Foo[] arr) {
>> int i = 0;
>> int pos = 512;
>> Foo v = new Foo();
>> while (i < sz) {
>> if (i % 2 == 0) {
>> arr[pos] = v;
>> pos += 1;
>> } else {
>> pos -= 1;
>> v = arr[pos];
>> }
>> i++;
>> }
>> }
>> }
>>
>> The program takes 2 parameters - `size` and `par`. `size` is the size of
>> the
>> workload, and `par` is the number of threads to distribute the workload
>> to.
>> What each thread does is repetitively write and read an array location
>> 512, in
>> total `size`/ `par` times. Each thread reads its own array (locations
>> being
>> written to should be at least 1024*4 bytes away from each other).
>>
>> I've measured the speedup on an 2x 4-core Intel Xeon server and observed
>> the
>> following speedups for 1, 2, 4 and 8 processors (7 repetitions each):
>>
>>  >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
>>  >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
>>  >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
>>  >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
>>
>> Apparently, this is due to memory contention.
>> What I've observed is that if I comment the line `Foo[] arr = array` and
>> uncomment the line above it to allocate the array within the thread, I
>> get:
>>
>>  >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
>>  >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
>>  >>> All running times: [578, 508, 589, 571, 617, 643, 645]
>>  >>> All running times: [330, 299, 300, 322, 331, 324, 575]
>>
>> My guess is that in the second case the arrays get allocated in thread
>> local
>> allocation buffers, so the array positions are much further from each
>> other.
>> Still, even in the first case processors should be writing to separate
>> cache
>> lines, so why does the first example scale so badly?
>>
>> Can anyone explain these numbers? Why does this memory contention happen?
>>
>> Thank you,
>> Aleksandar Prokopec
>>
>>
>>
>> ------------------------------**------------------------------**
>> ------------------
>>
>> // The complete runnable program:
>>
>>
>> import java.util.ArrayList;
>>
>> class MultiStackJavaExperiment {
>>
>> final class Foo {
>> int x = 0;
>> }
>>
>> final class Worker extends Thread {
>> Foo[] array = new Foo[1024];
>> int sz;
>>
>> public Worker(int _sz) {
>> sz = _sz;
>> }
>>
>> public void run() {
>> Foo[] arr = new Foo[1024];
>> //Foo[] arr = array;
>> loop(arr);
>> }
>>
>> public void loop(Foo[] arr) {
>> int i = 0;
>> int pos = 512;
>> Foo v = new Foo();
>> while (i < sz) {
>> if (i % 2 == 0) {
>> arr[pos] = v;
>> pos += 1;
>> } else {
>> pos -= 1;
>> v = arr[pos];
>> }
>> i++;
>> }
>> }
>> }
>>
>> public static void main(String[] args) {
>> (new MultiStackJavaExperiment()).**mainMethod(args);
>> }
>>
>> int size = Integer.parseInt(System.**getProperty("size"));
>> int par = Integer.parseInt(System.**getProperty("par"));
>>
>> public void mainMethod(String[] args) {
>> int times = 0;
>> if (args.length == 0) times = 1;
>> else times = Integer.parseInt(args[0]);
>> ArrayList < Long > measurements = new ArrayList < Long > ();
>>
>> for (int i = 0; i < times; i++) {
>> long start = System.currentTimeMillis();
>> run();
>> long end = System.currentTimeMillis();
>>
>> long time = (end - start);
>> System.out.println(i + ") Running time: " + time + " ms");
>> measurements.add(time);
>> }
>>
>> System.out.println(">>>");
>> System.out.println(">>> All running times: " + measurements);
>> System.out.println(">>>");
>> }
>>
>> public void run() {
>> int sz = size / par;
>> ArrayList < Thread > threads = new ArrayList < Thread > ();
>>
>> for (int i = 0; i < par; i++) {
>> threads.add(new Worker(sz));
>> threads.get(i).start();
>> }
>> for (int i = 0; i < par; i++) {
>> try {
>> threads.get(i).join();
>> } catch (Exception e) {}
>> }
>> }
>>
>> }
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120125/1ddfbb1f/attachment-0001.html>

From vitalyd at gmail.com  Wed Jan 25 19:22:48 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 25 Jan 2012 19:22:48 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <CAHjP37FA7dzr4Aybr+X18p+ukjYW-co-Vezsn07ikxXUUymSmQ@mail.gmail.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CAHjP37FA7dzr4Aybr+X18p+ukjYW-co-Vezsn07ikxXUUymSmQ@mail.gmail.com>
Message-ID: <CAHjP37EBq=rnu7Xb6yrgpYEnFZ0JGGarxctJjsWcenHTSwF76w@mail.gmail.com>

Slightly off topic but does the VM even need to mark cards when the
references are both in young gen? I thought they were needed for cases
where a tenured object references a young gen object; otherwise it seems
like scanning from the roots is sufficient? I assume the cost of figuring
out whether it's a cross generation write is too prohibitive to do for
every pointer store and it just always marks.

Sent from my phone
On Jan 25, 2012 7:13 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> I must say that's a really good hypothesis and would perfectly explain the
> observations by Aleksandar- nice one Doug! :)
>
> TLAB sizes are adjusted ergonomically (if not set via flag) as I
> understand it, so even though it doesn't really make sense to have them
> smaller than 32k, they're masking away this issue.
>
> Sent from my phone
> On Jan 25, 2012 6:57 PM, "Doug Lea" <dl at cs.oswego.edu> wrote:
>
>> On 01/25/12 14:48, Aleksandar Prokopec wrote:
>>
>>> Hello,
>>>
>>> I've recently posted the following question about memory contention on
>>> StackOverflow:
>>>
>>
>>
>> I asked Aleksandar offlist to check the effects of -XX:+UseCondCardMark
>> which as always these days was the main problem. Really, everyone
>> should always use this switch on hotspot on multiprocessors.
>> (or -XX:+UseG1GC, although it is often a little worse on throughput
>> but better-behaved for large heaps).
>>
>> As I mentioned in a post from a few weeks ago, see Dive Dice's blog
>> http://blogs.oracle.com/dave/**entry/false_sharing_induced_**by_card<http://blogs.oracle.com/dave/entry/false_sharing_induced_by_card>
>>
>>
>> -Doug
>>
>>
>>
>>
>>
>>
>>> http://stackoverflow.com/**questions/8942396/array-**
>>> allocation-and-access-on-the-**java-virtual-machine-and-**
>>> memory-contention<http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention>
>>>
>>> Basically, I have several threads defined like this:
>>>
>>> final class Worker extends Thread {
>>> Foo[] array = new Foo[1024];
>>> int sz;
>>>
>>> public Worker(int _sz) {
>>> sz = _sz;
>>> }
>>>
>>> public void run() {
>>> //Foo[] arr = new Foo[1024];
>>> Foo[] arr = array;
>>> loop(arr);
>>> }
>>>
>>> public void loop(Foo[] arr) {
>>> int i = 0;
>>> int pos = 512;
>>> Foo v = new Foo();
>>> while (i < sz) {
>>> if (i % 2 == 0) {
>>> arr[pos] = v;
>>> pos += 1;
>>> } else {
>>> pos -= 1;
>>> v = arr[pos];
>>> }
>>> i++;
>>> }
>>> }
>>> }
>>>
>>> The program takes 2 parameters - `size` and `par`. `size` is the size of
>>> the
>>> workload, and `par` is the number of threads to distribute the workload
>>> to.
>>> What each thread does is repetitively write and read an array location
>>> 512, in
>>> total `size`/ `par` times. Each thread reads its own array (locations
>>> being
>>> written to should be at least 1024*4 bytes away from each other).
>>>
>>> I've measured the speedup on an 2x 4-core Intel Xeon server and observed
>>> the
>>> following speedups for 1, 2, 4 and 8 processors (7 repetitions each):
>>>
>>>  >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
>>>  >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
>>>  >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
>>>  >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
>>>
>>> Apparently, this is due to memory contention.
>>> What I've observed is that if I comment the line `Foo[] arr = array` and
>>> uncomment the line above it to allocate the array within the thread, I
>>> get:
>>>
>>>  >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
>>>  >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
>>>  >>> All running times: [578, 508, 589, 571, 617, 643, 645]
>>>  >>> All running times: [330, 299, 300, 322, 331, 324, 575]
>>>
>>> My guess is that in the second case the arrays get allocated in thread
>>> local
>>> allocation buffers, so the array positions are much further from each
>>> other.
>>> Still, even in the first case processors should be writing to separate
>>> cache
>>> lines, so why does the first example scale so badly?
>>>
>>> Can anyone explain these numbers? Why does this memory contention happen?
>>>
>>> Thank you,
>>> Aleksandar Prokopec
>>>
>>>
>>>
>>> ------------------------------**------------------------------**
>>> ------------------
>>>
>>> // The complete runnable program:
>>>
>>>
>>> import java.util.ArrayList;
>>>
>>> class MultiStackJavaExperiment {
>>>
>>> final class Foo {
>>> int x = 0;
>>> }
>>>
>>> final class Worker extends Thread {
>>> Foo[] array = new Foo[1024];
>>> int sz;
>>>
>>> public Worker(int _sz) {
>>> sz = _sz;
>>> }
>>>
>>> public void run() {
>>> Foo[] arr = new Foo[1024];
>>> //Foo[] arr = array;
>>> loop(arr);
>>> }
>>>
>>> public void loop(Foo[] arr) {
>>> int i = 0;
>>> int pos = 512;
>>> Foo v = new Foo();
>>> while (i < sz) {
>>> if (i % 2 == 0) {
>>> arr[pos] = v;
>>> pos += 1;
>>> } else {
>>> pos -= 1;
>>> v = arr[pos];
>>> }
>>> i++;
>>> }
>>> }
>>> }
>>>
>>> public static void main(String[] args) {
>>> (new MultiStackJavaExperiment()).**mainMethod(args);
>>> }
>>>
>>> int size = Integer.parseInt(System.**getProperty("size"));
>>> int par = Integer.parseInt(System.**getProperty("par"));
>>>
>>> public void mainMethod(String[] args) {
>>> int times = 0;
>>> if (args.length == 0) times = 1;
>>> else times = Integer.parseInt(args[0]);
>>> ArrayList < Long > measurements = new ArrayList < Long > ();
>>>
>>> for (int i = 0; i < times; i++) {
>>> long start = System.currentTimeMillis();
>>> run();
>>> long end = System.currentTimeMillis();
>>>
>>> long time = (end - start);
>>> System.out.println(i + ") Running time: " + time + " ms");
>>> measurements.add(time);
>>> }
>>>
>>> System.out.println(">>>");
>>> System.out.println(">>> All running times: " + measurements);
>>> System.out.println(">>>");
>>> }
>>>
>>> public void run() {
>>> int sz = size / par;
>>> ArrayList < Thread > threads = new ArrayList < Thread > ();
>>>
>>> for (int i = 0; i < par; i++) {
>>> threads.add(new Worker(sz));
>>> threads.get(i).start();
>>> }
>>> for (int i = 0; i < par; i++) {
>>> try {
>>> threads.get(i).join();
>>> } catch (Exception e) {}
>>> }
>>> }
>>>
>>> }
>>>
>>>
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120125/8daa1c08/attachment.html>

From dl at cs.oswego.edu  Wed Jan 25 20:06:18 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 25 Jan 2012 20:06:18 -0500
Subject: [concurrency-interest] ForkJoin updates
Message-ID: <4F20A70A.7030204@cs.oswego.edu>

As promised for a while now, some updates to ForkJoin
are available from the usual places linked from
http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
-- both the java.util.concurrent and jsr166y versions.
I suppose these are targeted for JDK8, but there is no
reason to place these updates only in the jsr166e package.
There are a few very minor further planned improvements, but
it should be ready to use.

Highlights:

1. Substantially better throughput when lots of clients
submit lots of tasks. (I've measured up to 60X speedups
on microbenchmarks). The idea is to treat external submitters
in a similar way as workers -- using randomized queuing and
stealing. (This required a big internal refactoring to
disassociate work queues and workers.) This also greatly
improves throughput when all tasks are async and submitted
to the pool rather than forked, which becomes a reasonable
way to structure actor frameworks, as well as many plain
services that you might otherwise use ThreadPoolExecutor for.

These improvements also lead to a less hostile stance about
submitting possibly-blocking tasks. An added parag in
the ForkJoinTask documentation provides some guidance
(basically: we like them if they are small (even if numerous)
and don't have dependencies).
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/ForkJoinTask.html

2. More cases are handled that allow threads to help others rather
than generating compensation threads. Including most cases of
naive backward joins. (I'm not sure whether it is a bug or a
feature that some such cases are now only about twice as
slow as structuring joins correctly).

3. One small API addition: Explicit support for task marking.
It was cruel to tell people that they could use FJ for things
like graph traversal but not have a simple way to mark tasks
so they won't be revisited while processing a graph (among a few
other common use cases). Because they weren't supported initially,
marking methods need crummy names that won't conflict with
existing usages: markForkJoinTask and isMarkedForkJoinTask.

4. Better tolerance for GC/allocation stalls: It's not uncommon
for a "lead" task to stall producing subtasks because of GC, causing
others to give and block, requiring expensive unblocking when it
finally resumes. A new slower ramp-down scheme reduces performance
impact. (Although still, the best guidance is to remember Amdahl's
law, and minimize the sequential overhead needed to produce a task).

5. Other minor changes that give a few percent improvement in
common FJ task processing. On the other hand, this version is
even more prone to GC cardmark contention. So if using hotspot
on a multiprocessor (or even >4core multicore) you absolutely
must run in -XX:UseCondCardMark or -XX:+UseG1GC. (Also, it is
better behaved with biased locking disabled -XX:-UseBiasedLocking).

As always, suggestions and comments based on usage experience
would be very welcome.

-Doug



From aleksandar.prokopec at gmail.com  Thu Jan 26 07:32:37 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Thu, 26 Jan 2012 13:32:37 +0100
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <06DDDC87-348A-446B-8969-FB8EEB1CD5AF@gmail.com>
References: <4F205CA1.3010801@gmail.com>
	<06DDDC87-348A-446B-8969-FB8EEB1CD5AF@gmail.com>
Message-ID: <4F2147E5.60500@gmail.com>

As Doug explained, it turns out this was due to the "-XX:+UseCondCardMark".

I imagine that the only way to fight against this issue in the absence 
of this flag on JVMs prior to 1.7 is to allocate larger memory blocks so 
that the affected card table bytes don't end up in the same cache-line.

Thanks,
Alex


On 1/26/12 12:46 AM, Dmitry Zaslavsky wrote:
> This is a complete guess, but in C++ on NUMA system in Linux with 
> default allocation strategy, you get into classical pattern of 
> allocating memory local to the the node starting the threads and the 
> other threads will pay perf penalty.
>
> While in c++ its a know "feature", I don't know if it is your case in 
> java.
>
> Do you have vtune to check?
>
> Sent from mobile device
>
> On Jan 25, 2012, at 2:48 PM, Aleksandar Prokopec 
> <aleksandar.prokopec at gmail.com <mailto:aleksandar.prokopec at gmail.com>> 
> wrote:
>
>> Hello,
>>
>> I've recently posted the following question about memory contention 
>> on StackOverflow:
>>
>> http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention
>>
>> Basically, I have several threads defined like this:
>>
>> final class Worker extends Thread {
>>     Foo[] array = new Foo[1024];
>>     int sz;
>>
>>     public Worker(int _sz) {
>>         sz = _sz;
>>     }
>>
>>     public void run() {
>>         //Foo[] arr = new Foo[1024];
>>         Foo[] arr = array;
>>         loop(arr);
>>     }
>>
>>     public void loop(Foo[] arr) {
>>         int i = 0;
>>         int pos = 512;
>>         Foo v = new Foo();
>>         while (i < sz) {
>>             if (i % 2 == 0) {
>>                 arr[pos] = v;
>>                 pos += 1;
>>             } else {
>>                 pos -= 1;
>>                 v = arr[pos];
>>             }
>>             i++;
>>         }
>>     }
>> }
>>
>> The program takes 2 parameters - `size` and `par`. `size` is the size 
>> of the workload, and `par` is the number of threads to distribute the 
>> workload to.
>> What each thread does is repetitively write and read an array 
>> location 512, in total `size`/ `par` times. Each thread reads its own 
>> array (locations being written to should be at least 1024*4 bytes 
>> away from each other).
>>
>> I've measured the speedup on an 2x 4-core Intel Xeon  server and 
>> observed the following speedups for 1, 2, 4 and 8 processors (7 
>> repetitions each):
>>
>> >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
>> >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
>> >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
>> >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
>>
>> Apparently, this is due to memory contention.
>> What I've observed is that if I comment the line `Foo[] arr = array` 
>> and uncomment the line above it to allocate the array within the 
>> thread, I get:
>>
>> >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
>> >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
>> >>> All running times: [578, 508, 589, 571, 617, 643, 645]
>> >>> All running times: [330, 299, 300, 322, 331, 324, 575]
>>
>> My guess is that in the second case the arrays get allocated in 
>> thread local allocation buffers, so the array positions are much 
>> further from each other.
>> Still, even in the first case processors should be writing to 
>> separate cache lines, so why does the first example scale so badly?
>>
>> Can anyone explain these numbers? Why does this memory contention happen?
>>
>> Thank you,
>> Aleksandar Prokopec
>>
>>
>>
>> ------------------------------------------------------------------------------
>>
>> // The complete runnable program:
>>
>>
>> import java.util.ArrayList;
>>
>> class MultiStackJavaExperiment {
>>
>>     final class Foo {
>>         int x = 0;
>>     }
>>
>>     final class Worker extends Thread {
>>         Foo[] array = new Foo[1024];
>>         int sz;
>>
>>         public Worker(int _sz) {
>>             sz = _sz;
>>         }
>>
>>         public void run() {
>>             Foo[] arr = new Foo[1024];
>>             //Foo[] arr = array;
>>             loop(arr);
>>         }
>>
>>         public void loop(Foo[] arr) {
>>             int i = 0;
>>             int pos = 512;
>>             Foo v = new Foo();
>>             while (i < sz) {
>>                 if (i % 2 == 0) {
>>                     arr[pos] = v;
>>                     pos += 1;
>>                 } else {
>>                     pos -= 1;
>>                     v = arr[pos];
>>                 }
>>                 i++;
>>             }
>>         }
>>     }
>>
>>     public static void main(String[] args) {
>>         (new MultiStackJavaExperiment()).mainMethod(args);
>>     }
>>
>>     int size = Integer.parseInt(System.getProperty("size"));
>>     int par = Integer.parseInt(System.getProperty("par"));
>>
>>     public void mainMethod(String[] args) {
>>         int times = 0;
>>         if (args.length == 0) times = 1;
>>         else times = Integer.parseInt(args[0]);
>>         ArrayList < Long > measurements = new ArrayList < Long > ();
>>
>>         for (int i = 0; i < times; i++) {
>>             long start = System.currentTimeMillis();
>>             run();
>>             long end = System.currentTimeMillis();
>>
>>             long time = (end - start);
>>             System.out.println(i + ") Running time: " + time + " ms");
>>             measurements.add(time);
>>         }
>>
>>         System.out.println(">>>");
>>         System.out.println(">>> All running times: " + measurements);
>>         System.out.println(">>>");
>>     }
>>
>>     public void run() {
>>         int sz = size / par;
>>         ArrayList < Thread > threads = new ArrayList < Thread > ();
>>
>>         for (int i = 0; i < par; i++) {
>>             threads.add(new Worker(sz));
>>             threads.get(i).start();
>>         }
>>         for (int i = 0; i < par; i++) {
>>             try {
>>                 threads.get(i).join();
>>             } catch (Exception e) {}
>>         }
>>     }
>>
>> }
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120126/3b6f28f7/attachment-0001.html>

From aleksandar.prokopec at gmail.com  Thu Jan 26 07:46:43 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Thu, 26 Jan 2012 13:46:43 +0100
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <mailman.806.1327581174.6569.concurrency-interest@cs.oswego.edu>
References: <mailman.806.1327581174.6569.concurrency-interest@cs.oswego.edu>
Message-ID: <4F214B33.5070908@gmail.com>

If I understood correctly, this marking is not due to tracking if there 
exists a reference from the tenured to the young generation in the card.

It's an optimization in the young collector which allows completely 
skipping the scan of some cards during GC if there have been no writes 
in them.

Cheers,
Aleksandar


On 1/26/12 1:32 PM, concurrency-interest-request at cs.oswego.edu wrote:
> Message: 1
> Date: Wed, 25 Jan 2012 19:22:48 -0500
> From: Vitaly Davidovich<vitalyd at gmail.com>
> To: Doug Lea<dl at cs.oswego.edu>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Array allocation and access on the
> 	JVM
> Message-ID:
> 	<CAHjP37EBq=rnu7Xb6yrgpYEnFZ0JGGarxctJjsWcenHTSwF76w at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Slightly off topic but does the VM even need to mark cards when the
> references are both in young gen? I thought they were needed for cases
> where a tenured object references a young gen object; otherwise it seems
> like scanning from the roots is sufficient? I assume the cost of figuring
> out whether it's a cross generation write is too prohibitive to do for
> every pointer store and it just always marks.
>
> Sent from my phone
> On Jan 25, 2012 7:13 PM, "Vitaly Davidovich"<vitalyd at gmail.com>  wrote:
>
>


From dl at cs.oswego.edu  Thu Jan 26 09:35:24 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 26 Jan 2012 09:35:24 -0500
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <4F20A70A.7030204@cs.oswego.edu>
References: <4F20A70A.7030204@cs.oswego.edu>
Message-ID: <4F2164AC.4040008@cs.oswego.edu>


A few further followups of list discussions that led
to some of the updates:

* The ForkJoinWorkerThreadFactory interface is unchanged.
Even though it might be more convenient for some to
create a subinterface with a callback when a worker
thread terminates, there's not enough motivation to
do so considering that these threads cannot be reused.
As I mentioned in posts about this, it is OK for
Factories to limit thread creation by returning null
when they don't want to create more. Although if
they do so when there are fewer threads than the
parallelism level, the will be frequently asked to
provide one, which may end up slowing things down a lot.

* I also resisted the temptation to re-introduce a
"maintainParallelism" method to force creation of
additional workers when some but not all tasks are blocked.
I do not know how to support this in a way that
is not prone to unwanted positive feedback loops that
will tend to create unexpectedly high numbers of threads.
It is by far better for users to use the ManagedBlocker
API to issue advisories that we can internally use
to throttle threads, and further improve how we do so.

* The new version is much more likely than the previous
one to process submissions out of order. Nothing was
ever guaranteed about this but it is possible that
some users mistook the previously low probability
of out-of-order processing for "never". Also, the
previous version had a different bypass for
self-submissions, which can also impact processing
order, but is still within spec.

* I had written...
> One way to address this is to use a "claim" bit on
> every task (somewhat similar to your execJoin workaround).

This is one possible use of new task-marking methods,
People who need to claim tasks because of lack of static
dag structure can do so without adding to overhead in other
usages.

* A few of the status/inspection
methods are a bit slower and may require traversals
through multiple queues/workers. In particular,
getActiveThreadCount() remains O(1) and atomic, but
getRunningThreadCount(), that estimates the number of
threads that are not somehow blocked, now probes
the thread state of each worker, which is slower
but typically more accurate than previous version
because it relies on JVM Thread.State, that knows
that threads are blocked even when the pool does not.

-Doug

From viktor.klang at gmail.com  Thu Jan 26 09:41:35 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 26 Jan 2012 15:41:35 +0100
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <4F2164AC.4040008@cs.oswego.edu>
References: <4F20A70A.7030204@cs.oswego.edu> <4F2164AC.4040008@cs.oswego.edu>
Message-ID: <CANPzfU8FNv4LcEzj4jr=sERyJArsDyE9rF6OLzQ+qCK7JHD++A@mail.gmail.com>

Doug,

I owe you a crate of beer and a lot of thanks.

Cheers,
?

On Thu, Jan 26, 2012 at 3:35 PM, Doug Lea <dl at cs.oswego.edu> wrote:

>
> A few further followups of list discussions that led
> to some of the updates:
>
> * The ForkJoinWorkerThreadFactory interface is unchanged.
> Even though it might be more convenient for some to
> create a subinterface with a callback when a worker
> thread terminates, there's not enough motivation to
> do so considering that these threads cannot be reused.
> As I mentioned in posts about this, it is OK for
> Factories to limit thread creation by returning null
> when they don't want to create more. Although if
> they do so when there are fewer threads than the
> parallelism level, the will be frequently asked to
> provide one, which may end up slowing things down a lot.
>
> * I also resisted the temptation to re-introduce a
> "maintainParallelism" method to force creation of
> additional workers when some but not all tasks are blocked.
> I do not know how to support this in a way that
> is not prone to unwanted positive feedback loops that
> will tend to create unexpectedly high numbers of threads.
> It is by far better for users to use the ManagedBlocker
> API to issue advisories that we can internally use
> to throttle threads, and further improve how we do so.
>
> * The new version is much more likely than the previous
> one to process submissions out of order. Nothing was
> ever guaranteed about this but it is possible that
> some users mistook the previously low probability
> of out-of-order processing for "never". Also, the
> previous version had a different bypass for
> self-submissions, which can also impact processing
> order, but is still within spec.
>
> * I had written...
>
>> One way to address this is to use a "claim" bit on
>> every task (somewhat similar to your execJoin workaround).
>>
>
> This is one possible use of new task-marking methods,
> People who need to claim tasks because of lack of static
> dag structure can do so without adding to overhead in other
> usages.
>
> * A few of the status/inspection
> methods are a bit slower and may require traversals
> through multiple queues/workers. In particular,
> getActiveThreadCount() remains O(1) and atomic, but
> getRunningThreadCount(), that estimates the number of
> threads that are not somehow blocked, now probes
> the thread state of each worker, which is slower
> but typically more accurate than previous version
> because it relies on JVM Thread.State, that knows
> that threads are blocked even when the pool does not.
>
>
> -Doug
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120126/b409359b/attachment.html>

From hanson.char at gmail.com  Fri Jan 27 00:36:44 2012
From: hanson.char at gmail.com (Hanson Char)
Date: Thu, 26 Jan 2012 21:36:44 -0800
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <4F20967A.6090703@cs.oswego.edu>
References: <4F205CA1.3010801@gmail.com>
	<4F20967A.6090703@cs.oswego.edu>
Message-ID: <CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>

Is there plan to have -XX:+UseCondCardMark included as the default config
in Java 7 ?  Or can a VM auto detect the need for such configuration and
self config upon start up ?

Thanks,
Hanson

On Wed, Jan 25, 2012 at 3:55 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/25/12 14:48, Aleksandar Prokopec wrote:
>
>> Hello,
>>
>> I've recently posted the following question about memory contention on
>> StackOverflow:
>>
>
>
> I asked Aleksandar offlist to check the effects of -XX:+UseCondCardMark
> which as always these days was the main problem. Really, everyone
> should always use this switch on hotspot on multiprocessors.
> (or -XX:+UseG1GC, although it is often a little worse on throughput
> but better-behaved for large heaps).
>
> As I mentioned in a post from a few weeks ago, see Dive Dice's blog
> http://blogs.oracle.com/dave/**entry/false_sharing_induced_**by_card<http://blogs.oracle.com/dave/entry/false_sharing_induced_by_card>
>
>
> -Doug
>
>
>
>
>
>
>> http://stackoverflow.com/**questions/8942396/array-**
>> allocation-and-access-on-the-**java-virtual-machine-and-**
>> memory-contention<http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention>
>>
>> Basically, I have several threads defined like this:
>>
>> final class Worker extends Thread {
>> Foo[] array = new Foo[1024];
>> int sz;
>>
>> public Worker(int _sz) {
>> sz = _sz;
>> }
>>
>> public void run() {
>> //Foo[] arr = new Foo[1024];
>> Foo[] arr = array;
>> loop(arr);
>> }
>>
>> public void loop(Foo[] arr) {
>> int i = 0;
>> int pos = 512;
>> Foo v = new Foo();
>> while (i < sz) {
>> if (i % 2 == 0) {
>> arr[pos] = v;
>> pos += 1;
>> } else {
>> pos -= 1;
>> v = arr[pos];
>> }
>> i++;
>> }
>> }
>> }
>>
>> The program takes 2 parameters - `size` and `par`. `size` is the size of
>> the
>> workload, and `par` is the number of threads to distribute the workload
>> to.
>> What each thread does is repetitively write and read an array location
>> 512, in
>> total `size`/ `par` times. Each thread reads its own array (locations
>> being
>> written to should be at least 1024*4 bytes away from each other).
>>
>> I've measured the speedup on an 2x 4-core Intel Xeon server and observed
>> the
>> following speedups for 1, 2, 4 and 8 processors (7 repetitions each):
>>
>>  >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
>>  >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
>>  >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
>>  >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
>>
>> Apparently, this is due to memory contention.
>> What I've observed is that if I comment the line `Foo[] arr = array` and
>> uncomment the line above it to allocate the array within the thread, I
>> get:
>>
>>  >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
>>  >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
>>  >>> All running times: [578, 508, 589, 571, 617, 643, 645]
>>  >>> All running times: [330, 299, 300, 322, 331, 324, 575]
>>
>> My guess is that in the second case the arrays get allocated in thread
>> local
>> allocation buffers, so the array positions are much further from each
>> other.
>> Still, even in the first case processors should be writing to separate
>> cache
>> lines, so why does the first example scale so badly?
>>
>> Can anyone explain these numbers? Why does this memory contention happen?
>>
>> Thank you,
>> Aleksandar Prokopec
>>
>>
>>
>> ------------------------------**------------------------------**
>> ------------------
>>
>> // The complete runnable program:
>>
>>
>> import java.util.ArrayList;
>>
>> class MultiStackJavaExperiment {
>>
>> final class Foo {
>> int x = 0;
>> }
>>
>> final class Worker extends Thread {
>> Foo[] array = new Foo[1024];
>> int sz;
>>
>> public Worker(int _sz) {
>> sz = _sz;
>> }
>>
>> public void run() {
>> Foo[] arr = new Foo[1024];
>> //Foo[] arr = array;
>> loop(arr);
>> }
>>
>> public void loop(Foo[] arr) {
>> int i = 0;
>> int pos = 512;
>> Foo v = new Foo();
>> while (i < sz) {
>> if (i % 2 == 0) {
>> arr[pos] = v;
>> pos += 1;
>> } else {
>> pos -= 1;
>> v = arr[pos];
>> }
>> i++;
>> }
>> }
>> }
>>
>> public static void main(String[] args) {
>> (new MultiStackJavaExperiment()).**mainMethod(args);
>> }
>>
>> int size = Integer.parseInt(System.**getProperty("size"));
>> int par = Integer.parseInt(System.**getProperty("par"));
>>
>> public void mainMethod(String[] args) {
>> int times = 0;
>> if (args.length == 0) times = 1;
>> else times = Integer.parseInt(args[0]);
>> ArrayList < Long > measurements = new ArrayList < Long > ();
>>
>> for (int i = 0; i < times; i++) {
>> long start = System.currentTimeMillis();
>> run();
>> long end = System.currentTimeMillis();
>>
>> long time = (end - start);
>> System.out.println(i + ") Running time: " + time + " ms");
>> measurements.add(time);
>> }
>>
>> System.out.println(">>>");
>> System.out.println(">>> All running times: " + measurements);
>> System.out.println(">>>");
>> }
>>
>> public void run() {
>> int sz = size / par;
>> ArrayList < Thread > threads = new ArrayList < Thread > ();
>>
>> for (int i = 0; i < par; i++) {
>> threads.add(new Worker(sz));
>> threads.get(i).start();
>> }
>> for (int i = 0; i < par; i++) {
>> try {
>> threads.get(i).join();
>> } catch (Exception e) {}
>> }
>> }
>>
>> }
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120126/49cc9951/attachment.html>

From nathan.reynolds at oracle.com  Fri Jan 27 11:29:17 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 27 Jan 2012 09:29:17 -0700
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
Message-ID: <4F22D0DD.6020501@oracle.com>

Defaulting -XX:+UseCondCardMark would require a lot of testing to create 
a heuristic to guess when it should default on or off.  Not all 
workloads hit contention on the card table.  Making the card table 
concurrent for these workloads would slow them down.  If the heuristic 
sub-optimal, then we would be in a worse situation than we are today.

In order for the JVM to detect contention, it would need to profile each 
access or have a hardware counter to declare contention.  Timing each 
access and tracking statistics would be expensive.  Hardware counters 
exist but they are too expensive to access because of kernel round 
trips.  So, the current solutions are more expensive than the actual 
problem.

We have asked hardware vendors to supply user-mode access to hardware 
counters for true and false sharing.  The same hardware counters can be 
used for this problem.  If we get the user-mode hardware counters, then 
this might be automatize.

Another difficulty would be in making this flag dynamically changeable 
_without_ incurring a performance penalty.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/26/2012 10:36 PM, Hanson Char wrote:
> Is there plan to have -XX:+UseCondCardMark included as the default 
> config in Java 7 ?  Or can a VM auto detect the need for such 
> configuration and self config upon start up ?
>
> Thanks,
> Hanson
>
> On Wed, Jan 25, 2012 at 3:55 PM, Doug Lea <dl at cs.oswego.edu 
> <mailto:dl at cs.oswego.edu>> wrote:
>
>     On 01/25/12 14:48, Aleksandar Prokopec wrote:
>
>         Hello,
>
>         I've recently posted the following question about memory
>         contention on
>         StackOverflow:
>
>
>
>     I asked Aleksandar offlist to check the effects of
>     -XX:+UseCondCardMark
>     which as always these days was the main problem. Really, everyone
>     should always use this switch on hotspot on multiprocessors.
>     (or -XX:+UseG1GC, although it is often a little worse on throughput
>     but better-behaved for large heaps).
>
>     As I mentioned in a post from a few weeks ago, see Dive Dice's blog
>     http://blogs.oracle.com/dave/entry/false_sharing_induced_by_card
>
>
>     -Doug
>
>
>
>
>
>
>         http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention
>
>         Basically, I have several threads defined like this:
>
>         final class Worker extends Thread {
>         Foo[] array = new Foo[1024];
>         int sz;
>
>         public Worker(int _sz) {
>         sz = _sz;
>         }
>
>         public void run() {
>         //Foo[] arr = new Foo[1024];
>         Foo[] arr = array;
>         loop(arr);
>         }
>
>         public void loop(Foo[] arr) {
>         int i = 0;
>         int pos = 512;
>         Foo v = new Foo();
>         while (i < sz) {
>         if (i % 2 == 0) {
>         arr[pos] = v;
>         pos += 1;
>         } else {
>         pos -= 1;
>         v = arr[pos];
>         }
>         i++;
>         }
>         }
>         }
>
>         The program takes 2 parameters - `size` and `par`. `size` is
>         the size of the
>         workload, and `par` is the number of threads to distribute the
>         workload to.
>         What each thread does is repetitively write and read an array
>         location 512, in
>         total `size`/ `par` times. Each thread reads its own array
>         (locations being
>         written to should be at least 1024*4 bytes away from each other).
>
>         I've measured the speedup on an 2x 4-core Intel Xeon server
>         and observed the
>         following speedups for 1, 2, 4 and 8 processors (7 repetitions
>         each):
>
>         >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
>         >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
>         >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
>         >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
>
>         Apparently, this is due to memory contention.
>         What I've observed is that if I comment the line `Foo[] arr =
>         array` and
>         uncomment the line above it to allocate the array within the
>         thread, I get:
>
>         >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
>         >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
>         >>> All running times: [578, 508, 589, 571, 617, 643, 645]
>         >>> All running times: [330, 299, 300, 322, 331, 324, 575]
>
>         My guess is that in the second case the arrays get allocated
>         in thread local
>         allocation buffers, so the array positions are much further
>         from each other.
>         Still, even in the first case processors should be writing to
>         separate cache
>         lines, so why does the first example scale so badly?
>
>         Can anyone explain these numbers? Why does this memory
>         contention happen?
>
>         Thank you,
>         Aleksandar Prokopec
>
>
>
>         ------------------------------------------------------------------------------
>
>         // The complete runnable program:
>
>
>         import java.util.ArrayList;
>
>         class MultiStackJavaExperiment {
>
>         final class Foo {
>         int x = 0;
>         }
>
>         final class Worker extends Thread {
>         Foo[] array = new Foo[1024];
>         int sz;
>
>         public Worker(int _sz) {
>         sz = _sz;
>         }
>
>         public void run() {
>         Foo[] arr = new Foo[1024];
>         //Foo[] arr = array;
>         loop(arr);
>         }
>
>         public void loop(Foo[] arr) {
>         int i = 0;
>         int pos = 512;
>         Foo v = new Foo();
>         while (i < sz) {
>         if (i % 2 == 0) {
>         arr[pos] = v;
>         pos += 1;
>         } else {
>         pos -= 1;
>         v = arr[pos];
>         }
>         i++;
>         }
>         }
>         }
>
>         public static void main(String[] args) {
>         (new MultiStackJavaExperiment()).mainMethod(args);
>         }
>
>         int size = Integer.parseInt(System.getProperty("size"));
>         int par = Integer.parseInt(System.getProperty("par"));
>
>         public void mainMethod(String[] args) {
>         int times = 0;
>         if (args.length == 0) times = 1;
>         else times = Integer.parseInt(args[0]);
>         ArrayList < Long > measurements = new ArrayList < Long > ();
>
>         for (int i = 0; i < times; i++) {
>         long start = System.currentTimeMillis();
>         run();
>         long end = System.currentTimeMillis();
>
>         long time = (end - start);
>         System.out.println(i + ") Running time: " + time + " ms");
>         measurements.add(time);
>         }
>
>         System.out.println(">>>");
>         System.out.println(">>> All running times: " + measurements);
>         System.out.println(">>>");
>         }
>
>         public void run() {
>         int sz = size / par;
>         ArrayList < Thread > threads = new ArrayList < Thread > ();
>
>         for (int i = 0; i < par; i++) {
>         threads.add(new Worker(sz));
>         threads.get(i).start();
>         }
>         for (int i = 0; i < par; i++) {
>         try {
>         threads.get(i).join();
>         } catch (Exception e) {}
>         }
>         }
>
>         }
>
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120127/37c20d50/attachment-0001.html>

From mlists at juma.me.uk  Fri Jan 27 11:44:51 2012
From: mlists at juma.me.uk (Ismael Juma)
Date: Fri, 27 Jan 2012 16:44:51 +0000
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <4F22D0DD.6020501@oracle.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
	<4F22D0DD.6020501@oracle.com>
Message-ID: <CAD5tkZbaRG5j0_SBmT+g1xPOjYBkmsMXp0U3RkcLEdA_Hctp0A@mail.gmail.com>

On Fri, Jan 27, 2012 at 4:29 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

> Making the card table concurrent for these workloads would slow them down.
>

Are there measurements for the slowdown experienced in these cases?

Best,
Ismael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120127/613752ef/attachment.html>

From vitalyd at gmail.com  Fri Jan 27 12:01:48 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 27 Jan 2012 12:01:48 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <4F22D0DD.6020501@oracle.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
	<4F22D0DD.6020501@oracle.com>
Message-ID: <CAHjP37E2rCRwLoNvEyx6mB=_w1KAEGqxqEVG=R_hPJjn-xEi0A@mail.gmail.com>

I thought UseCondMark just adds a branch to test whether the mark is
already dirty instead of writing unconditionally - are you saying the
branch would cause noticeable perf degradation?

Vitaly

Sent from my phone
On Jan 27, 2012 11:38 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  Defaulting -XX:+UseCondCardMark would require a lot of testing to create
> a heuristic to guess when it should default on or off.  Not all workloads
> hit contention on the card table.  Making the card table concurrent for
> these workloads would slow them down.  If the heuristic sub-optimal, then
> we would be in a worse situation than we are today.
>
> In order for the JVM to detect contention, it would need to profile each
> access or have a hardware counter to declare contention.  Timing each
> access and tracking statistics would be expensive.  Hardware counters exist
> but they are too expensive to access because of kernel round trips.  So,
> the current solutions are more expensive than the actual problem.
>
> We have asked hardware vendors to supply user-mode access to hardware
> counters for true and false sharing.  The same hardware counters can be
> used for this problem.  If we get the user-mode hardware counters, then
> this might be automatize.
>
> Another difficulty would be in making this flag dynamically changeable *
> without* incurring a performance penalty.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
> On 1/26/2012 10:36 PM, Hanson Char wrote:
>
> Is there plan to have -XX:+UseCondCardMark included as the default config
> in Java 7 ?  Or can a VM auto detect the need for such configuration and
> self config upon start up ?
>
>  Thanks,
> Hanson
>
> On Wed, Jan 25, 2012 at 3:55 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 01/25/12 14:48, Aleksandar Prokopec wrote:
>>
>>> Hello,
>>>
>>> I've recently posted the following question about memory contention on
>>> StackOverflow:
>>>
>>
>>
>>  I asked Aleksandar offlist to check the effects of -XX:+UseCondCardMark
>> which as always these days was the main problem. Really, everyone
>> should always use this switch on hotspot on multiprocessors.
>> (or -XX:+UseG1GC, although it is often a little worse on throughput
>> but better-behaved for large heaps).
>>
>> As I mentioned in a post from a few weeks ago, see Dive Dice's blog
>> http://blogs.oracle.com/dave/entry/false_sharing_induced_by_card
>>
>>
>> -Doug
>>
>>
>>
>>
>>
>>
>>>
>>> http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention
>>>
>>> Basically, I have several threads defined like this:
>>>
>>> final class Worker extends Thread {
>>> Foo[] array = new Foo[1024];
>>> int sz;
>>>
>>> public Worker(int _sz) {
>>> sz = _sz;
>>> }
>>>
>>> public void run() {
>>> //Foo[] arr = new Foo[1024];
>>> Foo[] arr = array;
>>> loop(arr);
>>> }
>>>
>>> public void loop(Foo[] arr) {
>>> int i = 0;
>>> int pos = 512;
>>> Foo v = new Foo();
>>> while (i < sz) {
>>> if (i % 2 == 0) {
>>> arr[pos] = v;
>>> pos += 1;
>>> } else {
>>> pos -= 1;
>>> v = arr[pos];
>>> }
>>> i++;
>>> }
>>> }
>>> }
>>>
>>> The program takes 2 parameters - `size` and `par`. `size` is the size of
>>> the
>>> workload, and `par` is the number of threads to distribute the workload
>>> to.
>>> What each thread does is repetitively write and read an array location
>>> 512, in
>>> total `size`/ `par` times. Each thread reads its own array (locations
>>> being
>>> written to should be at least 1024*4 bytes away from each other).
>>>
>>> I've measured the speedup on an 2x 4-core Intel Xeon server and observed
>>> the
>>> following speedups for 1, 2, 4 and 8 processors (7 repetitions each):
>>>
>>>  >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
>>>  >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
>>>  >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
>>>  >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
>>>
>>> Apparently, this is due to memory contention.
>>> What I've observed is that if I comment the line `Foo[] arr = array` and
>>> uncomment the line above it to allocate the array within the thread, I
>>> get:
>>>
>>>  >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
>>>  >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
>>>  >>> All running times: [578, 508, 589, 571, 617, 643, 645]
>>>  >>> All running times: [330, 299, 300, 322, 331, 324, 575]
>>>
>>> My guess is that in the second case the arrays get allocated in thread
>>> local
>>> allocation buffers, so the array positions are much further from each
>>> other.
>>> Still, even in the first case processors should be writing to separate
>>> cache
>>> lines, so why does the first example scale so badly?
>>>
>>> Can anyone explain these numbers? Why does this memory contention happen?
>>>
>>> Thank you,
>>> Aleksandar Prokopec
>>>
>>>
>>>
>>>
>>> ------------------------------------------------------------------------------
>>>
>>> // The complete runnable program:
>>>
>>>
>>> import java.util.ArrayList;
>>>
>>> class MultiStackJavaExperiment {
>>>
>>> final class Foo {
>>> int x = 0;
>>> }
>>>
>>> final class Worker extends Thread {
>>> Foo[] array = new Foo[1024];
>>> int sz;
>>>
>>> public Worker(int _sz) {
>>> sz = _sz;
>>> }
>>>
>>> public void run() {
>>> Foo[] arr = new Foo[1024];
>>> //Foo[] arr = array;
>>> loop(arr);
>>> }
>>>
>>> public void loop(Foo[] arr) {
>>> int i = 0;
>>> int pos = 512;
>>> Foo v = new Foo();
>>> while (i < sz) {
>>> if (i % 2 == 0) {
>>> arr[pos] = v;
>>> pos += 1;
>>> } else {
>>> pos -= 1;
>>> v = arr[pos];
>>> }
>>> i++;
>>> }
>>> }
>>> }
>>>
>>> public static void main(String[] args) {
>>> (new MultiStackJavaExperiment()).mainMethod(args);
>>> }
>>>
>>> int size = Integer.parseInt(System.getProperty("size"));
>>> int par = Integer.parseInt(System.getProperty("par"));
>>>
>>> public void mainMethod(String[] args) {
>>> int times = 0;
>>> if (args.length == 0) times = 1;
>>> else times = Integer.parseInt(args[0]);
>>> ArrayList < Long > measurements = new ArrayList < Long > ();
>>>
>>> for (int i = 0; i < times; i++) {
>>> long start = System.currentTimeMillis();
>>> run();
>>> long end = System.currentTimeMillis();
>>>
>>> long time = (end - start);
>>> System.out.println(i + ") Running time: " + time + " ms");
>>> measurements.add(time);
>>> }
>>>
>>> System.out.println(">>>");
>>> System.out.println(">>> All running times: " + measurements);
>>> System.out.println(">>>");
>>> }
>>>
>>> public void run() {
>>> int sz = size / par;
>>> ArrayList < Thread > threads = new ArrayList < Thread > ();
>>>
>>> for (int i = 0; i < par; i++) {
>>> threads.add(new Worker(sz));
>>> threads.get(i).start();
>>> }
>>> for (int i = 0; i < par; i++) {
>>> try {
>>> threads.get(i).join();
>>> } catch (Exception e) {}
>>> }
>>> }
>>>
>>> }
>>>
>>>
>>>
>>>   _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120127/243e91b9/attachment.html>

From nathan.reynolds at oracle.com  Fri Jan 27 12:06:33 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 27 Jan 2012 10:06:33 -0700
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <CAD5tkZbaRG5j0_SBmT+g1xPOjYBkmsMXp0U3RkcLEdA_Hctp0A@mail.gmail.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
	<4F22D0DD.6020501@oracle.com>
	<CAD5tkZbaRG5j0_SBmT+g1xPOjYBkmsMXp0U3RkcLEdA_Hctp0A@mail.gmail.com>
Message-ID: <4F22D999.5030704@oracle.com>

I am not sure.  I think the concurrent version requires a branch to 
first check if the card is marked and if not then set it.  Branches are 
very expensive on some platforms.  I think the non-concurrent version 
simply sets the card every time.

The contention comes from every core trying to put the cache line in the 
exclusive state so that the write can happen.  On uni-core uni-socket 
machines there is no point to adding the branch.  On uni-socket machines 
there may not be a point to adding the branch.  That would require 
testing.  On multi-socket multi-core machines, adding the branch reduces 
contention but decreases performance.  If there is no contention to 
reduce, then we just get decreased performance.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 1/27/2012 9:44 AM, Ismael Juma wrote:
> On Fri, Jan 27, 2012 at 4:29 PM, Nathan Reynolds 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     Making the card table concurrent for these workloads would slow
>     them down.
>
>
> Are there measurements for the slowdown experienced in these cases?
>
> Best,
> Ismael
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120127/e34cb5c6/attachment-0001.html>

From david.dice at gmail.com  Fri Jan 27 12:10:20 2012
From: david.dice at gmail.com (David Dice)
Date: Fri, 27 Jan 2012 12:10:20 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
Message-ID: <CANbRUcj2R9=06EFAV2UY=u6UE=5eJDraCp_gwiT6yHvZUUumgg@mail.gmail.com>

Date: Fri, 27 Jan 2012 09:29:17 -0700
> From: Nathan Reynolds <nathan.reynolds at oracle.com>
> To: Hanson Char <hanson.char at gmail.com>
> Cc: Doug Lea <dl at cs.oswego.edu>, concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Array allocation and access on the
>        JVM
> Message-ID: <4F22D0DD.6020501 at oracle.com>
> Content-Type: text/plain; charset="iso-8859-1"; Format="flowed"
>
> Defaulting -XX:+UseCondCardMark would require a lot of testing to create
> a heuristic to guess when it should default on or off.  Not all
> workloads hit contention on the card table.  Making the card table
> concurrent for these workloads would slow them down.  If the heuristic
> sub-optimal, then we would be in a worse situation than we are today.
>
> In order for the JVM to detect contention, it would need to profile each
> access or have a hardware counter to declare contention.  Timing each
> access and tracking statistics would be expensive.  Hardware counters
> exist but they are too expensive to access because of kernel round
> trips.  So, the current solutions are more expensive than the actual
> problem.
>
> We have asked hardware vendors to supply user-mode access to hardware
> counters for true and false sharing.  The same hardware counters can be
> used for this problem.  If we get the user-mode hardware counters, then
> this might be automatize.
>
> Another difficulty would be in making this flag dynamically changeable
> _without_ incurring a performance penalty.
>
>
The decision is platform-specific as well as load-specific.   Using CCM
won't be profitable on a single-core T2+, for instance, where false sharing
is particularly cheap.   Similarly, if you have the system partitioned into
small-diameter domains then it's more apt to be unprofitable.

Currently, the flag exists as a diagnostic tool to help us identify how
wide-spread the issue is, as well as the magnitude of the performance drop.


Also, the current CCM idiom could use improvement before we try to make
decisions.     It could certainly be made faster.   Some branch-free forms
have a slightly longer path but appear to perform better.

Regards
Dave

blogs.sun.com/dave

p.s., if we were to have good hardware profiling (say, like AMD's LWP that
disintermediated the OS) then we might be able to sample and adapt by
individual card marking site, instead of trying to turn the feature on/off
globally.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120127/746d410d/attachment.html>

From dl at cs.oswego.edu  Fri Jan 27 12:12:47 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 27 Jan 2012 12:12:47 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <CAD5tkZbaRG5j0_SBmT+g1xPOjYBkmsMXp0U3RkcLEdA_Hctp0A@mail.gmail.com>
References: <4F205CA1.3010801@gmail.com>
	<4F20967A.6090703@cs.oswego.edu>	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>	<4F22D0DD.6020501@oracle.com>
	<CAD5tkZbaRG5j0_SBmT+g1xPOjYBkmsMXp0U3RkcLEdA_Hctp0A@mail.gmail.com>
Message-ID: <4F22DB0F.5090100@cs.oswego.edu>

On 01/27/12 11:44, Ismael Juma wrote:
> On Fri, Jan 27, 2012 at 4:29 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     Making the card table concurrent for these workloads would slow them down.
>
>
> Are there measurements for the slowdown experienced in these cases?
>

I see a 0-4% slowdown (depending on exact processor/platform)
in non-concurrent programs on multisocket (multiprocesssor)
machines when -XX:+UseCondCardMark is used.

But I routinely see complete failure to speed up (so, a slowdown as
large as the number of processors you have) in parallel programs
when -XX:+UseCondCardMark is not used.

The obvious heuristic is to turn it on by default if on
a multisocket machine. But Oracle moves exceedingly slowly
in deciding to do such things, so for the foreseable future,
just tell everyone you know to set it.

While I'm at it: -XX:+UseCondCardMark is basically a bandaid
until GC folks find a way to efficiently support per-thread
card tables or other alternatives that don't have intrinsic
false-sharing problems.


-Doug

From dl at cs.oswego.edu  Fri Jan 27 12:29:01 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 27 Jan 2012 12:29:01 -0500
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <4F20A70A.7030204@cs.oswego.edu>
References: <4F20A70A.7030204@cs.oswego.edu>
Message-ID: <4F22DEDD.5070706@cs.oswego.edu>

On 01/25/12 20:06, Doug Lea wrote:
> As promised for a while now, some updates to ForkJoin
> are available from the usual places linked from
> http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
> -- both the java.util.concurrent and jsr166y versions.
> I suppose these are targeted for JDK8, but there is no
> reason to place these updates only in the jsr166e package.
> There are a few very minor further planned improvements,

Most of which are now in place, including a bugfix (some
previous code that wasn't correctly adapted but now is),
so if you have been helping to try this out, please get
an update.

-Doug



From mlists at juma.me.uk  Fri Jan 27 12:41:48 2012
From: mlists at juma.me.uk (Ismael Juma)
Date: Fri, 27 Jan 2012 17:41:48 +0000
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <4F22DB0F.5090100@cs.oswego.edu>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
	<4F22D0DD.6020501@oracle.com>
	<CAD5tkZbaRG5j0_SBmT+g1xPOjYBkmsMXp0U3RkcLEdA_Hctp0A@mail.gmail.com>
	<4F22DB0F.5090100@cs.oswego.edu>
Message-ID: <CAD5tkZafHb+4qvLMCHfxo+tjOpGQ6YaMohYVW06OVQC81VOhjA@mail.gmail.com>

On Fri, Jan 27, 2012 at 5:12 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> I see a 0-4% slowdown (depending on exact processor/platform)
> in non-concurrent programs on multisocket (multiprocesssor)
> machines when -XX:+UseCondCardMark is used.


> But I routinely see complete failure to speed up (so, a slowdown as
> large as the number of processors you have) in parallel programs
> when -XX:+UseCondCardMark is not used.
>
> The obvious heuristic is to turn it on by default if on
> a multisocket machine.


Thanks for the information Doug. Seems obvious indeed.

Best,
Ismael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120127/4937d386/attachment.html>

From aleksandar.prokopec at gmail.com  Fri Jan 27 13:06:45 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Fri, 27 Jan 2012 19:06:45 +0100
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <CAHjP37E2rCRwLoNvEyx6mB=_w1KAEGqxqEVG=R_hPJjn-xEi0A@mail.gmail.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
	<4F22D0DD.6020501@oracle.com>
	<CAHjP37E2rCRwLoNvEyx6mB=_w1KAEGqxqEVG=R_hPJjn-xEi0A@mail.gmail.com>
Message-ID: <b18bae16-8ac0-482b-bf13-2a7c94e2daae@email.android.com>

On one thread the particular benchmark i had went from 1200ms to 1400ms with the flag on.
-- 
Aleksandar Prokopec
LAMP, IC, EPFL

Sent from my Android phone with K-9 Mail. Please excuse my brevity.

Vitaly Davidovich <vitalyd at gmail.com> wrote:

I thought UseCondMark just adds a branch to test whether the mark is already dirty instead of writing unconditionally - are you saying the branch would cause noticeable perf degradation?

Vitaly

Sent from my phone

On Jan 27, 2012 11:38 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com> wrote:

Defaulting -XX:+UseCondCardMark would require a lot of testing to create a heuristic to guess when it should default on or off.  Not all workloads hit contention on the card table.  Making the card table concurrent for these workloads would slow them down.  If the heuristic sub-optimal, then we would be in a worse situation than we are today.

In order for the JVM to detect contention, it would need to profile each access or have a hardware counter to declare contention.  Timing each access and tracking statistics would be expensive.  Hardware counters exist but they are too expensive to access because of kernel round trips.  So, the current solutions are more expensive than the actual problem.

We have asked hardware vendors to supply user-mode access to hardware counters for true and false sharing.  The same hardware counters can be used for this problem.  If we get the user-mode hardware counters, then this might be automatize.

Another difficulty would be in making this flag dynamically changeable without incurring a performance penalty.

Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering | Server Technology


On 1/26/2012 10:36 PM, Hanson Char wrote: 

Is there plan to have -XX:+UseCondCardMark included as the default config in Java 7 ?  Or can a VM auto detect the need for such configuration and self config upon start up ? 


Thanks,

Hanson

On Wed, Jan 25, 2012 at 3:55 PM, Doug Lea <dl at cs.oswego.edu> wrote:

On 01/25/12 14:48, Aleksandar Prokopec wrote:

Hello,

I've recently posted the following question about memory contention on
StackOverflow:



I asked Aleksandar offlist to check the effects of -XX:+UseCondCardMark
which as always these days was the main problem. Really, everyone
should always use this switch on hotspot on multiprocessors.
(or -XX:+UseG1GC, although it is often a little worse on throughput
but better-behaved for large heaps).

As I mentioned in a post from a few weeks ago, see Dive Dice's blog
http://blogs.oracle.com/dave/entry/false_sharing_induced_by_card


-Doug






http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention

Basically, I have several threads defined like this:

final class Worker extends Thread {
Foo[] array = new Foo[1024];
int sz;

public Worker(int _sz) {
sz = _sz;
}

public void run() {
//Foo[] arr = new Foo[1024];
Foo[] arr = array;
loop(arr);
}

public void loop(Foo[] arr) {
int i = 0;
int pos = 512;
Foo v = new Foo();
while (i < sz) {
if (i % 2 == 0) {
arr[pos] = v;
pos += 1;
} else {
pos -= 1;
v = arr[pos];
}
i++;
}
}
}

The program takes 2 parameters - `size` and `par`. `size` is the size of the
workload, and `par` is the number of threads to distribute the workload to.
What each thread does is repetitively write and read an array location 512, in
total `size`/ `par` times. Each thread reads its own array (locations being
written to should be at least 1024*4 bytes away from each other).

I've measured the speedup on an 2x 4-core Intel Xeon server and observed the
following speedups for 1, 2, 4 and 8 processors (7 repetitions each):

 >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
 >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
 >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
 >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]

Apparently, this is due to memory contention.
What I've observed is that if I comment the line `Foo[] arr = array` and
uncomment the line above it to allocate the array within the thread, I get:

 >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
 >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
 >>> All running times: [578, 508, 589, 571, 617, 643, 645]
 >>> All running times: [330, 299, 300, 322, 331, 324, 575]

My guess is that in the second case the arrays get allocated in thread local
allocation buffers, so the array positions are much further from each other.
Still, even in the first case processors should be writing to separate cache
lines, so why does the first example scale so badly?

Can anyone explain these numbers? Why does this memory contention happen?

Thank you,
Aleksandar Prokopec



------------------------------------------------------------------------------

// The complete runnable program:


import java.util.ArrayList;

class MultiStackJavaExperiment {

final class Foo {
int x = 0;
}

final class Worker extends Thread {
Foo[] array = new Foo[1024];
int sz;

public Worker(int _sz) {
sz = _sz;
}

public void run() {
Foo[] arr = new Foo[1024];
//Foo[] arr = array;
loop(arr);
}

public void loop(Foo[] arr) {
int i = 0;
int pos = 512;
Foo v = new Foo();
while (i < sz) {
if (i % 2 == 0) {
arr[pos] = v;
pos += 1;
} else {
pos -= 1;
v = arr[pos];
}
i++;
}
}
}

public static void main(String[] args) {
(new MultiStackJavaExperiment()).mainMethod(args);
}

int size = Integer.parseInt(System.getProperty("size"));
int par = Integer.parseInt(System.getProperty("par"));

public void mainMethod(String[] args) {
int times = 0;
if (args.length == 0) times = 1;
else times = Integer.parseInt(args[0]);
ArrayList < Long > measurements = new ArrayList < Long > ();

for (int i = 0; i < times; i++) {
long start = System.currentTimeMillis();
run();
long end = System.currentTimeMillis();

long time = (end - start);
System.out.println(i + ") Running time: " + time + " ms");
measurements.add(time);
}

System.out.println(">>>");
System.out.println(">>> All running times: " + measurements);
System.out.println(">>>");
}

public void run() {
int sz = size / par;
ArrayList < Thread > threads = new ArrayList < Thread > ();

for (int i = 0; i < par; i++) {
threads.add(new Worker(sz));
threads.get(i).start();
}
for (int i = 0; i < par; i++) {
try {
threads.get(i).join();
} catch (Exception e) {}
}
}

}



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




_______________________________________________ Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120127/8d21db1e/attachment-0001.html>

From vitalyd at gmail.com  Fri Jan 27 13:24:30 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 27 Jan 2012 13:24:30 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <b18bae16-8ac0-482b-bf13-2a7c94e2daae@email.android.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
	<4F22D0DD.6020501@oracle.com>
	<CAHjP37E2rCRwLoNvEyx6mB=_w1KAEGqxqEVG=R_hPJjn-xEi0A@mail.gmail.com>
	<b18bae16-8ac0-482b-bf13-2a7c94e2daae@email.android.com>
Message-ID: <CAHjP37EyhzY8zAJ+=+uUMc99=_hvUu16BqU4QBEn=7BS+odfCg@mail.gmail.com>

On the same 2x4 Xeon server you mentioned earlier? Given that you said
there were no GCs I'd expect the cards to stay dirty since you're writing
to same location and CPU should predict the branch perfectly (cost of a
predicted branch is just a few cycles).  Interesting though, thanks for
sharing.

Sent from my phone
On Jan 27, 2012 1:07 PM, "Aleksandar Prokopec" <
aleksandar.prokopec at gmail.com> wrote:

> ** On one thread the particular benchmark i had went from 1200ms to
> 1400ms with the flag on.
> --
> Aleksandar Prokopec
> LAMP, IC, EPFL
>
> Sent from my Android phone with K-9 Mail. Please excuse my brevity.
>
> Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>> I thought UseCondMark just adds a branch to test whether the mark is
>> already dirty instead of writing unconditionally - are you saying the
>> branch would cause noticeable perf degradation?
>>
>> Vitaly
>>
>> Sent from my phone
>> On Jan 27, 2012 11:38 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>> wrote:
>>
>>>  Defaulting -XX:+UseCondCardMark would require a lot of testing to
>>> create a heuristic to guess when it should default on or off.  Not all
>>> workloads hit contention on the card table.  Making the card table
>>> concurrent for these workloads would slow them down.  If the heuristic
>>> sub-optimal, then we would be in a worse situation than we are today.
>>>
>>> In order for the JVM to detect contention, it would need to profile each
>>> access or have a hardware counter to declare contention.  Timing each
>>> access and tracking statistics would be expensive.  Hardware counters exist
>>> but they are too expensive to access because of kernel round trips.  So,
>>> the current solutions are more expensive than the actual problem.
>>>
>>> We have asked hardware vendors to supply user-mode access to hardware
>>> counters for true and false sharing.  The same hardware counters can be
>>> used for this problem.  If we get the user-mode hardware counters, then
>>> this might be automatize.
>>>
>>> Another difficulty would be in making this flag dynamically changeable *
>>> without* incurring a performance penalty.
>>>
>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Consulting Member of Technical Staff |
>>> 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>
>>> On 1/26/2012 10:36 PM, Hanson Char wrote:
>>>
>>> Is there plan to have -XX:+UseCondCardMark included as the default
>>> config in Java 7 ?  Or can a VM auto detect the need for such configuration
>>> and self config upon start up ?
>>>
>>>  Thanks,
>>> Hanson
>>>
>>> On Wed, Jan 25, 2012 at 3:55 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>
>>>> On 01/25/12 14:48, Aleksandar Prokopec wrote:
>>>>
>>>>> Hello,
>>>>>
>>>>> I've recently posted the following question about memory contention on
>>>>> StackOverflow:
>>>>>
>>>>
>>>>
>>>>  I asked Aleksandar offlist to check the effects of -XX:+UseCondCardMark
>>>> which as always these days was the main problem. Really, everyone
>>>> should always use this switch on hotspot on multiprocessors.
>>>> (or -XX:+UseG1GC, although it is often a little worse on throughput
>>>> but better-behaved for large heaps).
>>>>
>>>> As I mentioned in a post from a few weeks ago, see Dive Dice's blog
>>>> http://blogs.oracle.com/dave/entry/false_sharing_induced_by_card
>>>>
>>>>
>>>> -Doug
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>>
>>>>> http://stackoverflow.com/questions/8942396/array-allocation-and-access-on-the-java-virtual-machine-and-memory-contention
>>>>>
>>>>> Basically, I have several threads defined like this:
>>>>>
>>>>> final class Worker extends Thread {
>>>>> Foo[] array = new Foo[1024];
>>>>> int sz;
>>>>>
>>>>> public Worker(int _sz) {
>>>>> sz = _sz;
>>>>> }
>>>>>
>>>>> public void run() {
>>>>> //Foo[] arr = new Foo[1024];
>>>>> Foo[] arr = array;
>>>>> loop(arr);
>>>>> }
>>>>>
>>>>> public void loop(Foo[] arr) {
>>>>> int i = 0;
>>>>> int pos = 512;
>>>>> Foo v = new Foo();
>>>>> while (i < sz) {
>>>>> if (i % 2 == 0) {
>>>>> arr[pos] = v;
>>>>> pos += 1;
>>>>> } else {
>>>>> pos -= 1;
>>>>> v = arr[pos];
>>>>> }
>>>>> i++;
>>>>> }
>>>>> }
>>>>> }
>>>>>
>>>>> The program takes 2 parameters - `size` and `par`. `size` is the size
>>>>> of the
>>>>> workload, and `par` is the number of threads to distribute the
>>>>> workload to.
>>>>> What each thread does is repetitively write and read an array location
>>>>> 512, in
>>>>> total `size`/ `par` times. Each thread reads its own array (locations
>>>>> being
>>>>> written to should be at least 1024*4 bytes away from each other).
>>>>>
>>>>> I've measured the speedup on an 2x 4-core Intel Xeon server and
>>>>> observed the
>>>>> following speedups for 1, 2, 4 and 8 processors (7 repetitions each):
>>>>>
>>>>>  >>> All running times: [2149, 2227, 1974, 1948, 1803, 2283, 1878]
>>>>>  >>> All running times: [1140, 1124, 2022, 1141, 2028, 2004, 2136]
>>>>>  >>> All running times: [867, 1022, 1457, 1342, 1436, 966, 1531]
>>>>>  >>> All running times: [915, 864, 1245, 1243, 948, 790, 1007]
>>>>>
>>>>> Apparently, this is due to memory contention.
>>>>> What I've observed is that if I comment the line `Foo[] arr = array`
>>>>> and
>>>>> uncomment the line above it to allocate the array within the thread, I
>>>>> get:
>>>>>
>>>>>  >>> All running times: [2053, 1966, 2089, 1937, 2046, 1909, 2011]
>>>>>  >>> All running times: [1048, 1178, 1100, 1194, 1367, 1271, 1207]
>>>>>  >>> All running times: [578, 508, 589, 571, 617, 643, 645]
>>>>>  >>> All running times: [330, 299, 300, 322, 331, 324, 575]
>>>>>
>>>>> My guess is that in the second case the arrays get allocated in thread
>>>>> local
>>>>> allocation buffers, so the array positions are much further from each
>>>>> other.
>>>>> Still, even in the first case processors should be writing to separate
>>>>> cache
>>>>> lines, so why does the first example scale so badly?
>>>>>
>>>>> Can anyone explain these numbers? Why does this memory contention
>>>>> happen?
>>>>>
>>>>> Thank you,
>>>>> Aleksandar Prokopec
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> ------------------------------------------------------------------------------
>>>>>
>>>>> // The complete runnable program:
>>>>>
>>>>>
>>>>> import java.util.ArrayList;
>>>>>
>>>>> class MultiStackJavaExperiment {
>>>>>
>>>>> final class Foo {
>>>>> int x = 0;
>>>>> }
>>>>>
>>>>> final class Worker extends Thread {
>>>>> Foo[] array = new Foo[1024];
>>>>> int sz;
>>>>>
>>>>> public Worker(int _sz) {
>>>>> sz = _sz;
>>>>> }
>>>>>
>>>>> public void run() {
>>>>> Foo[] arr = new Foo[1024];
>>>>> //Foo[] arr = array;
>>>>> loop(arr);
>>>>> }
>>>>>
>>>>> public void loop(Foo[] arr) {
>>>>> int i = 0;
>>>>> int pos = 512;
>>>>> Foo v = new Foo();
>>>>> while (i < sz) {
>>>>> if (i % 2 == 0) {
>>>>> arr[pos] = v;
>>>>> pos += 1;
>>>>> } else {
>>>>> pos -= 1;
>>>>> v = arr[pos];
>>>>> }
>>>>> i++;
>>>>> }
>>>>> }
>>>>> }
>>>>>
>>>>> public static void main(String[] args) {
>>>>> (new MultiStackJavaExperiment()).mainMethod(args);
>>>>> }
>>>>>
>>>>> int size = Integer.parseInt(System.getProperty("size"));
>>>>> int par = Integer.parseInt(System.getProperty("par"));
>>>>>
>>>>> public void mainMethod(String[] args) {
>>>>> int times = 0;
>>>>> if (args.length == 0) times = 1;
>>>>> else times = Integer.parseInt(args[0]);
>>>>> ArrayList < Long > measurements = new ArrayList < Long > ();
>>>>>
>>>>> for (int i = 0; i < times; i++) {
>>>>> long start = System.currentTimeMillis();
>>>>> run();
>>>>> long end = System.currentTimeMillis();
>>>>>
>>>>> long time = (end - start);
>>>>> System.out.println(i + ") Running time: " + time + " ms");
>>>>> measurements.add(time);
>>>>> }
>>>>>
>>>>> System.out.println(">>>");
>>>>> System.out.println(">>> All running times: " + measurements);
>>>>> System.out.println(">>>");
>>>>> }
>>>>>
>>>>> public void run() {
>>>>> int sz = size / par;
>>>>> ArrayList < Thread > threads = new ArrayList < Thread > ();
>>>>>
>>>>> for (int i = 0; i < par; i++) {
>>>>> threads.add(new Worker(sz));
>>>>> threads.get(i).start();
>>>>> }
>>>>> for (int i = 0; i < par; i++) {
>>>>> try {
>>>>> threads.get(i).join();
>>>>> } catch (Exception e) {}
>>>>> }
>>>>> }
>>>>>
>>>>> }
>>>>>
>>>>>
>>>>>
>>>>>   _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120127/bc3ffde7/attachment.html>

From Johannes.Lichtenberger at uni-konstanz.de  Sun Jan 29 16:40:44 2012
From: Johannes.Lichtenberger at uni-konstanz.de (Johannes.Lichtenberger)
Date: Sun, 29 Jan 2012 22:40:44 +0100
Subject: [concurrency-interest] tree-to-tree differences / getting
 descendant-count and modification-count for each node
Message-ID: <4F25BCDC.1090408@uni-konstanz.de>

Hello,

I'm not sure if that's the right place to ask, as it seems, the topics
discussed are much more "low level", about the forkJoin-architecture
itself for instance.

I'm currently computing tree-to-tree differences in a top-down
(preorder-traversal) approach and want to get the number of descendants
and the number of modifications for each node (the modifications denote
how many modifications have been done in the whole subtree).

The differences between each two nodes are fired to observers, that is
if a node has been INSERTED, DELETED, UPDATED or is the SAME (and
optionally MOVEDFROM and MOVEDTO in a postprocessing step).

Now I'm going to create an agglomeration, that is want to represent the
diffs in a radial SunburstView and visualize the differences.

I somehow thought about one year ago, that I'm saving the diffs in a
List and traverse the new revision of the tree and change the
transaction to the old revision whenever a DELETE- or MOVEDTO-event /
diff occured. I'm computing the number of descendants and modifications
per node in parallel with an ExecutorService for each node, but maybe I
should also split the task of computing the descendants and
modifications for each node (the trees might become very large) and
essentially it's an O(n^2) operation which can be parallelized. It's
working but it's very complex and slow for about 150_000 nodes, and I
think I would like to deal with much more nodes, even though they are
pruned... that is subtrees which have the same node-ID and the same hash
value are skipped by the diff-algorithm and emitted as SAME_HASH.

I just thought about saving the diffs in a tree-structure itself and
adjusting each ancestor descendant-count and modification-count whenever
a new child is added. That would probably be much easier and reduce the
runtime to O(n*k/2) whereas k is the height of the tree and n the number
of diffs.

But maybe depending on the number of cores it would scale well with my
current approach, as the descendant-count and modification-count is
calculated in parallel to the traversal of the new tree-revision and the
creation of the Sunburst items (but well, the traversal might be much
faster and therefore the results from the BlockingQueue (descendant- and
modification-count) might come too late nontheless). Haven't tested it
yet as I'm currently only able to test on my notebook with a Core 2 Duo,
but I'm just a little bit depressed, as I think saving the diffs in a
tree-structure itself would have been much better and afterwards
traverse the tree and create the Sunburst items for the visualization
thereof. Don't know why I haven't thought about it in the first place,
but I don't know of a ready-made tree-collection -- though I know the
guys from Google Guava are working on it.

Do you think it makes sense to improve the computation of the
modifications and descendants for each node itself (parallelize the tree
traversal)? Or should I refactor the whole code-base to use a
tree-structure in the first place instead of a List which can afterwards
be traversed, whereas the descendant-count and modification-count is
adjusted for each new diff I get from the diff-algorithm (that is for
each ancestor add + 1 to the descendant-count and if it's a modification
(INSERTED, UPDATED, DELETED) add + 1 to the modification-count.

kind regards,
Johannes

From aleksandar.prokopec at gmail.com  Mon Jan 30 12:24:42 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Mon, 30 Jan 2012 18:24:42 +0100
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <CAHjP37EyhzY8zAJ+=+uUMc99=_hvUu16BqU4QBEn=7BS+odfCg@mail.gmail.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
	<4F22D0DD.6020501@oracle.com>
	<CAHjP37E2rCRwLoNvEyx6mB=_w1KAEGqxqEVG=R_hPJjn-xEi0A@mail.gmail.com>
	<b18bae16-8ac0-482b-bf13-2a7c94e2daae@email.android.com>
	<CAHjP37EyhzY8zAJ+=+uUMc99=_hvUu16BqU4QBEn=7BS+odfCg@mail.gmail.com>
Message-ID: <4F26D25A.9030701@gmail.com>

Here are the results without the UseCondCardMark flag:

Executing: java -Xmx512m -Xms512m -server -Dsize=500000000 -Dpar=1 
-XX:CompileCommand=print,*Worker.run 
org.scalapool.bench.MultiStackJavaExperiment 7
CompilerOracle: print *Worker.run
Java HotSpot(TM) 64-Bit Server VM warning: printing of assembly code is 
enabled; turning on DebugNonSafepoints to gain additional output
0) Running time: 1930 ms
1) Running time: 1876 ms
2) Running time: 1220 ms
3) Running time: 1220 ms
4) Running time: 1219 ms
5) Running time: 1220 ms
6) Running time: 1220 ms
 >>>
 >>> All running times: [1930, 1876, 1220, 1220, 1219, 1220, 1220]
 >>>


With the UseCondCardMark flag:


Executing: java -Xmx512m -Xms512m -server -Dsize=500000000 -Dpar=1 
-XX:+UseCondCardMark -XX:CompileCommand=print,*Worker.run 
org.scalapool.bench.MultiStackJavaExperiment 7
CompilerOracle: print *Worker.run
Java HotSpot(TM) 64-Bit Server VM warning: printing of assembly code is 
enabled; turning on DebugNonSafepoints to gain additional output
0) Running time: 1325 ms
1) Running time: 1318 ms
2) Running time: 1407 ms
3) Running time: 1408 ms
4) Running time: 1407 ms
5) Running time: 1407 ms
6) Running time: 1407 ms
 >>>
 >>> All running times: [1325, 1318, 1407, 1408, 1407, 1407, 1407]
 >>>


Cheers,
Alex


On 1/27/12 7:24 PM, Vitaly Davidovich wrote:
>
> On the same 2x4 Xeon server you mentioned earlier? Given that you said 
> there were no GCs I'd expect the cards to stay dirty since you're 
> writing to same location and CPU should predict the branch perfectly 
> (cost of a predicted branch is just a few cycles).  Interesting 
> though, thanks for sharing.
>
> Sent from my phone
>


From vitalyd at gmail.com  Mon Jan 30 12:54:00 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 30 Jan 2012 12:54:00 -0500
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <4F26D25A.9030701@gmail.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
	<4F22D0DD.6020501@oracle.com>
	<CAHjP37E2rCRwLoNvEyx6mB=_w1KAEGqxqEVG=R_hPJjn-xEi0A@mail.gmail.com>
	<b18bae16-8ac0-482b-bf13-2a7c94e2daae@email.android.com>
	<CAHjP37EyhzY8zAJ+=+uUMc99=_hvUu16BqU4QBEn=7BS+odfCg@mail.gmail.com>
	<4F26D25A.9030701@gmail.com>
Message-ID: <CAHjP37EqDKHj3WoD2_dsCD7X6+qKtA3CMitJ_Zpka9b8HunCTw@mail.gmail.com>

Thanks Alex.  I wonder why there's jitter in the run times, especially in
the one using conditional marking; I'd expect initial runs to be higher
than later ones, but your results show otherwise.  Is there anything else
using the cpu on this server besides your tests?

Also, a 15% increase in time seems too high for my intuition based on this
case (no gc, card stays dirty after first mark, should be perfectly
predicted thereafter), but I think only cpu counter profiling would shed
some light.

Sent from my phone
On Jan 30, 2012 12:24 PM, "Aleksandar Prokopec" <
aleksandar.prokopec at gmail.com> wrote:

> Here are the results without the UseCondCardMark flag:
>
> Executing: java -Xmx512m -Xms512m -server -Dsize=500000000 -Dpar=1
> -XX:CompileCommand=print,***Worker.run org.scalapool.bench.**MultiStackJavaExperiment
> 7
> CompilerOracle: print *Worker.run
> Java HotSpot(TM) 64-Bit Server VM warning: printing of assembly code is
> enabled; turning on DebugNonSafepoints to gain additional output
> 0) Running time: 1930 ms
> 1) Running time: 1876 ms
> 2) Running time: 1220 ms
> 3) Running time: 1220 ms
> 4) Running time: 1219 ms
> 5) Running time: 1220 ms
> 6) Running time: 1220 ms
> >>>
> >>> All running times: [1930, 1876, 1220, 1220, 1219, 1220, 1220]
> >>>
>
>
> With the UseCondCardMark flag:
>
>
> Executing: java -Xmx512m -Xms512m -server -Dsize=500000000 -Dpar=1
> -XX:+UseCondCardMark -XX:CompileCommand=print,***Worker.run
> org.scalapool.bench.**MultiStackJavaExperiment 7
> CompilerOracle: print *Worker.run
> Java HotSpot(TM) 64-Bit Server VM warning: printing of assembly code is
> enabled; turning on DebugNonSafepoints to gain additional output
> 0) Running time: 1325 ms
> 1) Running time: 1318 ms
> 2) Running time: 1407 ms
> 3) Running time: 1408 ms
> 4) Running time: 1407 ms
> 5) Running time: 1407 ms
> 6) Running time: 1407 ms
> >>>
> >>> All running times: [1325, 1318, 1407, 1408, 1407, 1407, 1407]
> >>>
>
>
> Cheers,
> Alex
>
>
> On 1/27/12 7:24 PM, Vitaly Davidovich wrote:
>
>>
>> On the same 2x4 Xeon server you mentioned earlier? Given that you said
>> there were no GCs I'd expect the cards to stay dirty since you're writing
>> to same location and CPU should predict the branch perfectly (cost of a
>> predicted branch is just a few cycles).  Interesting though, thanks for
>> sharing.
>>
>> Sent from my phone
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120130/22f980eb/attachment.html>

From aleksandar.prokopec at gmail.com  Mon Jan 30 13:06:40 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Mon, 30 Jan 2012 19:06:40 +0100
Subject: [concurrency-interest] Array allocation and access on the JVM
In-Reply-To: <CAHjP37EqDKHj3WoD2_dsCD7X6+qKtA3CMitJ_Zpka9b8HunCTw@mail.gmail.com>
References: <4F205CA1.3010801@gmail.com> <4F20967A.6090703@cs.oswego.edu>
	<CABWgujaEt4PVQ78w_CDkG-XMr5VFTheL7ZyXPfEeFSP1sJVujA@mail.gmail.com>
	<4F22D0DD.6020501@oracle.com>
	<CAHjP37E2rCRwLoNvEyx6mB=_w1KAEGqxqEVG=R_hPJjn-xEi0A@mail.gmail.com>
	<b18bae16-8ac0-482b-bf13-2a7c94e2daae@email.android.com>
	<CAHjP37EyhzY8zAJ+=+uUMc99=_hvUu16BqU4QBEn=7BS+odfCg@mail.gmail.com>
	<4F26D25A.9030701@gmail.com>
	<CAHjP37EqDKHj3WoD2_dsCD7X6+qKtA3CMitJ_Zpka9b8HunCTw@mail.gmail.com>
Message-ID: <89c73bd5-05a6-4436-b3de-1121dec8d57a@email.android.com>

You're welcome. Well, aside from top and some webservers, i don't see anything taking more than 1% cpu time and even that only occasionally.

But it is a bit odd - as i am able to reproduce the first two faster iterations every time. Perhaps there is a second jit optimization run which actually deoptimizes something.
-- 
Aleksandar Prokopec
LAMP, IC, EPFL

Sent from my Android phone with K-9 Mail. Please excuse my brevity.

Vitaly Davidovich <vitalyd at gmail.com> wrote:

Thanks Alex.  I wonder why there's jitter in the run times, especially in the one using conditional marking; I'd expect initial runs to be higher than later ones, but your results show otherwise.  Is there anything else using the cpu on this server besides your tests?

Also, a 15% increase in time seems too high for my intuition based on this case (no gc, card stays dirty after first mark, should be perfectly predicted thereafter), but I think only cpu counter profiling would shed some light.

Sent from my phone

On Jan 30, 2012 12:24 PM, "Aleksandar Prokopec" <aleksandar.prokopec at gmail.com> wrote:

Here are the results without the UseCondCardMark flag:

Executing: java -Xmx512m -Xms512m -server -Dsize=500000000 -Dpar=1 -XX:CompileCommand=print,*Worker.run org.scalapool.bench.MultiStackJavaExperiment 7
CompilerOracle: print *Worker.run
Java HotSpot(TM) 64-Bit Server VM warning: printing of assembly code is enabled; turning on DebugNonSafepoints to gain additional output
0) Running time: 1930 ms
1) Running time: 1876 ms
2) Running time: 1220 ms
3) Running time: 1220 ms
4) Running time: 1219 ms
5) Running time: 1220 ms
6) Running time: 1220 ms
>>>
>>> All running times: [1930, 1876, 1220, 1220, 1219, 1220, 1220]
>>>


With the UseCondCardMark flag:


Executing: java -Xmx512m -Xms512m -server -Dsize=500000000 -Dpar=1 -XX:+UseCondCardMark -XX:CompileCommand=print,*Worker.run org.scalapool.bench.MultiStackJavaExperiment 7
CompilerOracle: print *Worker.run
Java HotSpot(TM) 64-Bit Server VM warning: printing of assembly code is enabled; turning on DebugNonSafepoints to gain additional output
0) Running time: 1325 ms
1) Running time: 1318 ms
2) Running time: 1407 ms
3) Running time: 1408 ms
4) Running time: 1407 ms
5) Running time: 1407 ms
6) Running time: 1407 ms
>>>
>>> All running times: [1325, 1318, 1407, 1408, 1407, 1407, 1407]
>>>


Cheers,
Alex


On 1/27/12 7:24 PM, Vitaly Davidovich wrote:


On the same 2x4 Xeon server you mentioned earlier? Given that you said there were no GCs I'd expect the cards to stay dirty since you're writing to same location and CPU should predict the branch perfectly (cost of a predicted branch is just a few cycles).  Interesting though, thanks for sharing.

Sent from my phone


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120130/1b380f7d/attachment.html>

From viktor.klang at gmail.com  Tue Jan 31 10:23:13 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 31 Jan 2012 16:23:13 +0100
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <4F22DEDD.5070706@cs.oswego.edu>
References: <4F20A70A.7030204@cs.oswego.edu> <4F22DEDD.5070706@cs.oswego.edu>
Message-ID: <CANPzfU9K_+ydV_dewM1hYhgeCEP4=9y-1p67SvDRCtStxb5LXA@mail.gmail.com>

Hey Doug,

a surprising behavior is that AdaptedRunnable doesn't escalate uncaught
exception to the WorkerThreads UncaughtExceptionHandler,
I'm trying to align the behavior with ThreadPoolExecutor.

What the strategy to handle potentially fatal errors?

Cheers,
?

On Fri, Jan 27, 2012 at 6:29 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/25/12 20:06, Doug Lea wrote:
>
>> As promised for a while now, some updates to ForkJoin
>> are available from the usual places linked from
>> http://gee.cs.oswego.edu/dl/**concurrency-interest/index.**html<http://gee.cs.oswego.edu/dl/concurrency-interest/index.html>
>> -- both the java.util.concurrent and jsr166y versions.
>> I suppose these are targeted for JDK8, but there is no
>> reason to place these updates only in the jsr166e package.
>> There are a few very minor further planned improvements,
>>
>
> Most of which are now in place, including a bugfix (some
> previous code that wasn't correctly adapted but now is),
> so if you have been helping to try this out, please get
> an update.
>
>
> -Doug
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120131/511ac3a3/attachment.html>

From dl at cs.oswego.edu  Tue Jan 31 11:47:38 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 31 Jan 2012 11:47:38 -0500
Subject: [concurrency-interest] ForkJoin updates
In-Reply-To: <CANPzfU9K_+ydV_dewM1hYhgeCEP4=9y-1p67SvDRCtStxb5LXA@mail.gmail.com>
References: <4F20A70A.7030204@cs.oswego.edu>	<4F22DEDD.5070706@cs.oswego.edu>
	<CANPzfU9K_+ydV_dewM1hYhgeCEP4=9y-1p67SvDRCtStxb5LXA@mail.gmail.com>
Message-ID: <4F281B2A.9040204@cs.oswego.edu>

On 01/31/12 10:23, ?iktor ?lang wrote:
> a surprising behavior is that AdaptedRunnable doesn't escalate uncaught
> exception to the WorkerThreads UncaughtExceptionHandler,
> I'm trying to align the behavior with ThreadPoolExecutor.
>
> What the strategy to handle potentially fatal errors?

All exceptions are associated with the tasks encountering them.
The only exceptions propagated to UncaughtExceptionHandlers
are resource exhaustion, internal errors, and setup errors mode by
people extending class ForkJoinWorkerThread.

If you'd like certain kinds of exceptions to instead trigger
some sort of coordinated error response, you could create
your own variant of AdaptedRunnable that probes via getException.

While I'm at it: the current CVS commit almost completes this
round of updates. (I still plan a few non-essential changes
to fit internals together better under new submission scheme
etc but these won't happen immediately.)

-Doug








