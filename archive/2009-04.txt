From rgomes1997 at yahoo.co.uk  Thu Apr  2 20:49:29 2009
From: rgomes1997 at yahoo.co.uk (Richard Gomes)
Date: Thu, 2 Apr 2009 17:49:29 -0700 (PDT)
Subject: [concurrency-interest] About CyclicAction
In-Reply-To: <491ACF26.9090700@cs.oswego.edu>
References: <611a1ad40811112338v6ab27df8i5192b620bfe0bab0@mail.gmail.com>
	<491ACF26.9090700@cs.oswego.edu>
Message-ID: <22860006.post@talk.nabble.com>


Hi List

I've recently joined and I have interest on this subject, specially if
OpenCL is involved.
Please let me know if you guy know something about it.

Thanks a lot



Doug Lea wrote:
> 
> Milla Sun wrote:
>> I downloaded the current jar files and found that the "CyclicAction" and 
>> "TaskBarrier" not exist....
>> How to get them?
> 
> These classes are being held hostage; sorry.
> I've been working on extensions to the basic framework
> that enable support for a wider range of task synchronization,
> including better support for looping tasks. These classes
> will re-emerge when that is more stable.
> 
> For people wondering about upcoming changes: The main extension
> is to allow tasks to block, although only in cooperative/controlled
> ways, in which case the pool may create spare workers etc to maintain
> the desired parallelism level when necessary. Doing this
> enables much simpler mixing of different styles of parallelism.
> (For example, nested loops, transactional tasks, arbitrary DAGs.)
> It will also lead to some other API changes and extensions.
> While these are basically functional, they are still in
> some flux. I haven't committed them mainly because I don't
> want early users to suffer through a bunch of likely API
> changes while they stabilize.
> 
>>  
>> And , another question:
>> If I want to implement and generate SPMD code, does the fork-join 
>> framework provide any mechanism?
>> 
> 
> Hooking this to GPU/CUDA/Cell etc backends is in principal possible
> but a lot of work, that I don't think anyone has taken on yet.
> 
> -Doug
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


-----
Richard Gomes
http://www.jquantlib.org/index.php/User:RichardGomes
-- 
View this message in context: http://www.nabble.com/About-CyclicAction-tp20455608p22860006.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From mr.tom.strickland at gmail.com  Mon Apr  6 11:09:42 2009
From: mr.tom.strickland at gmail.com (tom strickland)
Date: Mon, 6 Apr 2009 16:09:42 +0100
Subject: [concurrency-interest] transferring queued threads
Message-ID: <635c64210904060809s1b952ee9m4899c9742fb27dd2@mail.gmail.com>

I have an interesting locking scenario and I'd appreciate some help getting
my head around it. I'm sorry if I've made it a little abstract, but
hopefully this will come across clearly.


I have some entities (A, B, C, D, E...) that can be associated in pairs.
When 2 entities are associated, only one thread may process them at a time:
if A and B are associated and a thread has come in for A, then any threads
coming in for B will be blocked until A's thread has completed. This seems
to imply a simple lock, shared by A and B, that can be acquired and
released.

The problem comes when I want to dissociate A and B in order to associate B
with C. Once B is associated with C, triggers coming in for B should queue
on a lock shared between B and C, and triggers coming in for A should use a
separate lock (perhaps the original lock).

How might this be achieved?
My initial thoughts are that I should use wait... notify and a stored value
that tells a thread which lock it should be waiting for. At any time, a
running thread can look at this value to see which lock it should acquire.
If a lock has threads queued up for A and B and we need to split B's threads
out and queue them on a different lock:

1. create a new lock for B
2. acquire the new lock (in order to prevent races)
3. change the lock-id field for B to point to the new lock
4. wake the waiting threads
5. A woken thread will check whether it has been woken because:
   a. it has acquired the lock - nothing to do here, the lock is acquired,
proceed as normal.
   b. the lock that it should be waiting for has changed - wait on the new
lock
6. do stuff with B and C
7. release the lock acquired in step 2


Does this make sense? I have more questions, mainly about how to implement
and encapsulate this, but I'd like to start by making sure that I've
understood and explained the problem clearly.


Thanks,

Tom
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090406/25910efe/attachment.html>

From karmazilla at gmail.com  Mon Apr  6 11:46:01 2009
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Mon, 6 Apr 2009 17:46:01 +0200
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <635c64210904060809s1b952ee9m4899c9742fb27dd2@mail.gmail.com>
References: <635c64210904060809s1b952ee9m4899c9742fb27dd2@mail.gmail.com>
Message-ID: <90622e530904060846p30c5806cte7db1e0dbd4c1b7e@mail.gmail.com>

Can't you synchronize on the instances individually in the order of
their System.identityHashCode?

On Mon, Apr 6, 2009 at 5:09 PM, tom strickland
<mr.tom.strickland at gmail.com> wrote:
> I have an interesting locking scenario and I'd appreciate some help getting
> my head around it. I'm sorry if I've made it a little abstract, but
> hopefully this will come across clearly.
>
>
> I have some entities (A, B, C, D, E...) that can be associated in pairs.
> When 2 entities are associated, only one thread may process them at a time:
> if A and B are associated and a thread has come in for A, then any threads
> coming in for B will be blocked until A's thread has completed. This seems
> to imply a simple lock, shared by A and B, that can be acquired and
> released.
>
> The problem comes when I want to dissociate A and B in order to associate B
> with C. Once B is associated with C, triggers coming in for B should queue
> on a lock shared between B and C, and triggers coming in for A should use a
> separate lock (perhaps the original lock).
>
> How might this be achieved?
> My initial thoughts are that I should use wait... notify and a stored value
> that tells a thread which lock it should be waiting for. At any time, a
> running thread can look at this value to see which lock it should acquire.
> If a lock has threads queued up for A and B and we need to split B's threads
> out and queue them on a different lock:
>
> 1. create a new lock for B
> 2. acquire the new lock (in order to prevent races)
> 3. change the lock-id field for B to point to the new lock
> 4. wake the waiting threads
> 5. A woken thread will check whether it has been woken because:
> ?? a. it has acquired the lock - nothing to do here, the lock is acquired,
> proceed as normal.
> ?? b. the lock that it should be waiting for has changed - wait on the new
> lock
> 6. do stuff with B and C
> 7. release the lock acquired in step 2
>
>
> Does this make sense? I have more questions, mainly about how to implement
> and encapsulate this, but I'd like to start by making sure that I've
> understood and explained the problem clearly.
>
>
> Thanks,
>
> Tom
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.


From mr.tom.strickland at gmail.com  Mon Apr  6 11:55:17 2009
From: mr.tom.strickland at gmail.com (tom strickland)
Date: Mon, 6 Apr 2009 16:55:17 +0100
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <90622e530904060846p30c5806cte7db1e0dbd4c1b7e@mail.gmail.com>
References: <635c64210904060809s1b952ee9m4899c9742fb27dd2@mail.gmail.com>
	<90622e530904060846p30c5806cte7db1e0dbd4c1b7e@mail.gmail.com>
Message-ID: <635c64210904060855m1db6bab8wa2f5661eee810b90@mail.gmail.com>

Instances of what? The entities? It seems to me that the problem is that
[what I need to be queuing on] changes over time. I can't queue on
individual entities: when A and B are associated, then I would like to queue
triggers for A and B on the same lock... until they are not associated any
more.
Of course, I might have misunderstood your point!

Tom

2009/4/6 Christian Vest Hansen <karmazilla at gmail.com>

> Can't you synchronize on the instances individually in the order of
> their System.identityHashCode?
>

On Mon, Apr 6, 2009 at 5:09 PM, tom strickland
> <mr.tom.strickland at gmail.com> wrote:
> > I have an interesting locking scenario and I'd appreciate some help
> getting
> > my head around it. I'm sorry if I've made it a little abstract, but
> > hopefully this will come across clearly.
> >
> >
> > I have some entities (A, B, C, D, E...) that can be associated in pairs.
> > When 2 entities are associated, only one thread may process them at a
> time:
> > if A and B are associated and a thread has come in for A, then any
> threads
> > coming in for B will be blocked until A's thread has completed. This
> seems
> > to imply a simple lock, shared by A and B, that can be acquired and
> > released.
> >
> > The problem comes when I want to dissociate A and B in order to associate
> B
> > with C. Once B is associated with C, triggers coming in for B should
> queue
> > on a lock shared between B and C, and triggers coming in for A should use
> a
> > separate lock (perhaps the original lock).
> >
> > How might this be achieved?
> > My initial thoughts are that I should use wait... notify and a stored
> value
> > that tells a thread which lock it should be waiting for. At any time, a
> > running thread can look at this value to see which lock it should
> acquire.
> > If a lock has threads queued up for A and B and we need to split B's
> threads
> > out and queue them on a different lock:
> >
> > 1. create a new lock for B
> > 2. acquire the new lock (in order to prevent races)
> > 3. change the lock-id field for B to point to the new lock
> > 4. wake the waiting threads
> > 5. A woken thread will check whether it has been woken because:
> >    a. it has acquired the lock - nothing to do here, the lock is
> acquired,
> > proceed as normal.
> >    b. the lock that it should be waiting for has changed - wait on the
> new
> > lock
> > 6. do stuff with B and C
> > 7. release the lock acquired in step 2
> >
> >
> > Does this make sense? I have more questions, mainly about how to
> implement
> > and encapsulate this, but I'd like to start by making sure that I've
> > understood and explained the problem clearly.
> >
> >
> > Thanks,
> >
> > Tom
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
>
>
> --
> Venlig hilsen / Kind regards,
> Christian Vest Hansen.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090406/8e5a2350/attachment.html>

From karmazilla at gmail.com  Mon Apr  6 15:00:07 2009
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Mon, 6 Apr 2009 21:00:07 +0200
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <635c64210904060855m1db6bab8wa2f5661eee810b90@mail.gmail.com>
References: <635c64210904060809s1b952ee9m4899c9742fb27dd2@mail.gmail.com>
	<90622e530904060846p30c5806cte7db1e0dbd4c1b7e@mail.gmail.com>
	<635c64210904060855m1db6bab8wa2f5661eee810b90@mail.gmail.com>
Message-ID: <90622e530904061200p2d06123pe04d16dc1bc0d4c3@mail.gmail.com>

Have a lock for each entity; L(A), L(B), L(C) etc. such that each
entity at any given point in time has exactly one lock. These locks
have a universal and immutable ordering to them (like
System.identityHashCode, for instance).

Now, whenever you want to assoc A and B, you start by acquirering L(A)
and L(B) in accord to their universal order. When another thread comes
to assoc B and C, then the first thread will either 1) have gotten
L(B) and so this thread must wait, or 2) get L(B) and then L(C),
causing the first thread to wait on L(B) until this thread is finished
with B and C.

This universal order will prevent lock-ordering dead-locks, while
allowing any thread to reach for any combination of locks.

On Mon, Apr 6, 2009 at 5:55 PM, tom strickland
<mr.tom.strickland at gmail.com> wrote:
> Instances of what? The entities? It seems to me that the problem is that
> [what I need to be queuing on] changes over time. I can't queue on
> individual entities: when A and B are associated, then I would like to queue
> triggers for A and B on the same lock... until they are not associated any
> more.
> Of course, I might have misunderstood your point!
>
> Tom
>
> 2009/4/6 Christian Vest Hansen <karmazilla at gmail.com>
>>
>> Can't you synchronize on the instances individually in the order of
>> their System.identityHashCode?
>
>> On Mon, Apr 6, 2009 at 5:09 PM, tom strickland
>> <mr.tom.strickland at gmail.com> wrote:
>> > I have an interesting locking scenario and I'd appreciate some help
>> > getting
>> > my head around it. I'm sorry if I've made it a little abstract, but
>> > hopefully this will come across clearly.
>> >
>> >
>> > I have some entities (A, B, C, D, E...) that can be associated in pairs.
>> > When 2 entities are associated, only one thread may process them at a
>> > time:
>> > if A and B are associated and a thread has come in for A, then any
>> > threads
>> > coming in for B will be blocked until A's thread has completed. This
>> > seems
>> > to imply a simple lock, shared by A and B, that can be acquired and
>> > released.
>> >
>> > The problem comes when I want to dissociate A and B in order to
>> > associate B
>> > with C. Once B is associated with C, triggers coming in for B should
>> > queue
>> > on a lock shared between B and C, and triggers coming in for A should
>> > use a
>> > separate lock (perhaps the original lock).
>> >
>> > How might this be achieved?
>> > My initial thoughts are that I should use wait... notify and a stored
>> > value
>> > that tells a thread which lock it should be waiting for. At any time, a
>> > running thread can look at this value to see which lock it should
>> > acquire.
>> > If a lock has threads queued up for A and B and we need to split B's
>> > threads
>> > out and queue them on a different lock:
>> >
>> > 1. create a new lock for B
>> > 2. acquire the new lock (in order to prevent races)
>> > 3. change the lock-id field for B to point to the new lock
>> > 4. wake the waiting threads
>> > 5. A woken thread will check whether it has been woken because:
>> > ?? a. it has acquired the lock - nothing to do here, the lock is
>> > acquired,
>> > proceed as normal.
>> > ?? b. the lock that it should be waiting for has changed - wait on the
>> > new
>> > lock
>> > 6. do stuff with B and C
>> > 7. release the lock acquired in step 2
>> >
>> >
>> > Does this make sense? I have more questions, mainly about how to
>> > implement
>> > and encapsulate this, but I'd like to start by making sure that I've
>> > understood and explained the problem clearly.
>> >
>> >
>> > Thanks,
>> >
>> > Tom
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>>
>>
>>
>> --
>> Venlig hilsen / Kind regards,
>> Christian Vest Hansen.
>
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.


From brett.bernstein at gmail.com  Tue Apr  7 00:06:49 2009
From: brett.bernstein at gmail.com (Brett Bernstein)
Date: Tue, 7 Apr 2009 00:06:49 -0400
Subject: [concurrency-interest] Multi consumer queue
In-Reply-To: <49A1DC4F.8060403@cs.oswego.edu>
References: <11F3540B1E724C328BDC4CA1BB02D7FB@BrettPC>	<151764.23601.qm@web38802.mail.mud.yahoo.com>
	<A3586230ECE64F548D75AA22E6230AA2@BrettPC>
	<49A1DC4F.8060403@cs.oswego.edu>
Message-ID: <9ED11E2F60E44D2B9DE330B75C8CDD8D@BrettPC>

I could be entirely wrong on this, but to me it seems wasteful that I could 
have a program with 2 threads and 1 lock, do a bunch of locking and 
releasing over the course of the program, and as a result allocate millions 
of Nodes in the addWaiter method of AbstractQueuedSynchronizer.  It feels 
like there could be a much more efficient implementation here.  For example, 
if I wanted to build a low latency network application that never garbage 
collected it becomes much harder when the best locking tools allocate 
aggresively.
-Brett


----- Original Message ----- 
From: "Doug Lea" <dl at cs.oswego.edu>
To: "Brett Bernstein" <brett.bernstein at gmail.com>
Cc: <concurrency-interest at cs.oswego.edu>
Sent: Sunday, February 22, 2009 7:14 PM
Subject: Re: [concurrency-interest] Multi consumer queue


> Brett Bernstein wrote:
>> As a side note, I have a bit of a qualm with ReentrantLock.  Specifically 
>> the fact that waiting on the lock requires new to be called, which in 
>> some apps/situations I try to avoid.
>
> I assume you mean internal queue nodes constructed when threads block
> waiting for locks? Of course the same thing happens inside builtin locks,
> but uses the JVM's internal memory management, which in most JVMs does
> not have the benefit of sitting on a high-performance garbage collector.
>
> There are a bunch of situations in (concurrent) programming where
> allocating memory is a bad idea, but waiting for locks is not
> often among them. But if you would like to trade off more time
> spinning rather than  allocating and blocking, you can precede
> calls to lock() with some number of calls to tryLock.
>
> -Doug
> 


From davidcholmes at aapt.net.au  Tue Apr  7 01:03:53 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 7 Apr 2009 15:03:53 +1000
Subject: [concurrency-interest] Multi consumer queue
In-Reply-To: <9ED11E2F60E44D2B9DE330B75C8CDD8D@BrettPC>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEKOIAAA.davidcholmes@aapt.net.au>

Brett Bernstein writes:
> I could be entirely wrong on this, but to me it seems wasteful
> that I could have a program with 2 threads and 1 lock, do a bunch
> of locking and releasing over the course of the program, and as a result
> allocate millions of Nodes in the addWaiter method of
AbstractQueuedSynchronizer.
> It feels like there could be a much more efficient implementation here.
> For example, if I wanted to build a low latency network application that
never garbage
> collected it becomes much harder when the best locking tools allocate
aggresively.

For a particular application scenario a more efficient implementation is
nearly always possible. But in general, allocation is dirt cheap (99% of the
time from the thread-local allocation buffer) and short-lived objects are
easily reclaimed by the GC from the young generation. And you only get
millions of allocations if you get millions of contentions on the lock -
which is likely to be more of a problem with regards to performance and
latency than the allocation itself.

One alternative would be to have each Thread have it's own pre-defined
Node - given that a Thread can only block on one synchronizer at a time.
However this also has implications for the queuing algorithms as it means
that dequeuing has to be precise (so the Node can't end up on more than one
queue), where presently some dequeuing (for timeouts/cancellation) is done
lazily - and so could impact performance of individual operations. A
practical constraint on this approach is that Thread is not part of j.u.c so
(until Java 7 modules arrive) AQS would have to use some hackery to get
access to the Thread's Node.

Object pooling for Nodes could also be used but the cost of CASing a Node
out of the freelist might be more than the cost of a TLAB allocation; and
CASing it back into the freelist is much more expensive than simply dropping
the Node as garbage - of course you don't get GC running (just for Nodes)
but perhaps GCing a million garbage Nodes is cheaper than a million freelist
adds? And of course freelists have other management issues that can impact
the worst-case cost.

Cheers,
David Holmes
>
>
> ----- Original Message -----
> From: "Doug Lea" <dl at cs.oswego.edu>
> To: "Brett Bernstein" <brett.bernstein at gmail.com>
> Cc: <concurrency-interest at cs.oswego.edu>
> Sent: Sunday, February 22, 2009 7:14 PM
> Subject: Re: [concurrency-interest] Multi consumer queue
>
>
> > Brett Bernstein wrote:
> >> As a side note, I have a bit of a qualm with ReentrantLock.
> Specifically
> >> the fact that waiting on the lock requires new to be called, which in
> >> some apps/situations I try to avoid.
> >
> > I assume you mean internal queue nodes constructed when threads block
> > waiting for locks? Of course the same thing happens inside
> builtin locks,
> > but uses the JVM's internal memory management, which in most JVMs does
> > not have the benefit of sitting on a high-performance garbage collector.
> >
> > There are a bunch of situations in (concurrent) programming where
> > allocating memory is a bad idea, but waiting for locks is not
> > often among them. But if you would like to trade off more time
> > spinning rather than  allocating and blocking, you can precede
> > calls to lock() with some number of calls to tryLock.
> >
> > -Doug
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From peter.kovacs.1.0rc at gmail.com  Tue Apr  7 03:04:50 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Tue, 7 Apr 2009 09:04:50 +0200
Subject: [concurrency-interest] Canceling Futures - Callable implementations
Message-ID: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>

Hi,

How will a Callable implementation know that the corresponding Future
has been canceled?

One mechanism I am aware of is thread interruption, but practice shows
that relying on this mechanism is highly unsafe. For interruption
checks to work safely, everyone up the call stack would have to
observe the related protocol, which is rarely the case.

Is there anything else in place for this purpose? If there is not,
wouldn't it be reasonable to provide a "useful" default implementation
of Callable -- along the lines of FutureTask being an implementation
of Future?

Thanks
Peter

From joe.bowbeer at gmail.com  Tue Apr  7 13:10:49 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 7 Apr 2009 10:10:49 -0700
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
Message-ID: <31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>

Peter,

Callable was designed to be useful stand-alone, like Runnable.  Because of
this, Callable has no more knowledge of its Future (if associated) than
Runnable has of its Thread, and therein lies the rub when one is trying to
write a Callable that knows the state of its Future.

A better "Callable" for the purpose of detecting cancellation might have
been:

  public interface MyCallable<V> {
      V call(Future task) throws Exception;
  }

Then you could write "callables" of the form:

  MyCallable<Integer> mc = new MyCallable<Integer>() {
      public Integer call(Future task) {
          while (!task.isCancelled()) {
              Thread.yield(); // Please!!
          }
          return 0;
      }
  };

Below is a Task class that uses these instead:

class MyTask<V> implements Future<V>, Runnable {

    protected final FutureTask<V> task;

    public MyTask(final MyCallable<V> mc) {
        task = new FutureTask<V>(new Callable<V>() {
            public V call() throws Exception {
                return mc.call(task);
            }
        });
    }

    /* Runnable implementation. */

    public void run() {
        task.run();
    }

    /* Future implementation. */

    public boolean cancel(boolean mayInterruptIfRunning) {
        return task.cancel(mayInterruptIfRunning);
    }

    public boolean isCancelled() {
        return task.isCancelled();
    }

    public boolean isDone() {
        return task.isDone();
    }

    public V get() throws InterruptedException, ExecutionException {
        return task.get();
    }

    public V get(long timeout, TimeUnit unit)
        throws InterruptedException, ExecutionException, TimeoutException {
        return task.get(timeout, unit);
    }
}

This internal FutureTask construction is also found in SwingWorker, by the
way.

Joe

2009/4/7 P?ter Kov?cs

> Hi,
>
> How will a Callable implementation know that the corresponding Future
> has been canceled?
>
> One mechanism I am aware of is thread interruption, but practice shows
> that relying on this mechanism is highly unsafe. For interruption
> checks to work safely, everyone up the call stack would have to
> observe the related protocol, which is rarely the case.
>
> Is there anything else in place for this purpose? If there is not,
> wouldn't it be reasonable to provide a "useful" default implementation
> of Callable -- along the lines of FutureTask being an implementation
> of Future?
>
> Thanks
> Peter
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090407/775df810/attachment.html>

From peter.kovacs.1.0rc at gmail.com  Tue Apr  7 16:45:12 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Tue, 7 Apr 2009 22:45:12 +0200
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
Message-ID: <fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>

Joe,

Thank you for your reply.

I like your suggestion for a 'better "Callable"'. Couldn't it be
included in the API with the ExecutorService et al. supporting it?
Implementing against it would be, in a number of cases, much more
straightforward than with the current apparatus.

Thanks
Peter

On Tue, Apr 7, 2009 at 7:10 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> Peter,
>
> Callable was designed to be useful stand-alone, like Runnable.? Because of
> this, Callable has no more knowledge of its Future (if associated) than
> Runnable has of its Thread, and therein lies the rub when one is trying to
> write a Callable that knows the state of its Future.
>
> A better "Callable" for the purpose of detecting cancellation might have
> been:
>
> ? public interface MyCallable<V> {
> ????? V call(Future task) throws Exception;
> ? }
>
> Then you could write "callables" of the form:
>
> ? MyCallable<Integer> mc = new MyCallable<Integer>() {
> ????? public Integer call(Future task) {
> ????????? while (!task.isCancelled()) {
> ????????????? Thread.yield(); // Please!!
> ????????? }
> ????????? return 0;
> ????? }
> ? };
>
> Below is a Task class that uses these instead:
>
> class MyTask<V> implements Future<V>, Runnable {
>
> ??? protected final FutureTask<V> task;
>
> ??? public MyTask(final MyCallable<V> mc) {
> ??????? task = new FutureTask<V>(new Callable<V>() {
> ??????????? public V call() throws Exception {
> ??????????????? return mc.call(task);
> ??????????? }
> ??????? });
> ??? }
>
> ??? /* Runnable implementation. */
>
> ??? public void run() {
> ??????? task.run();
> ??? }
>
> ??? /* Future implementation. */
>
> ??? public boolean cancel(boolean mayInterruptIfRunning) {
> ??????? return task.cancel(mayInterruptIfRunning);
> ??? }
>
> ??? public boolean isCancelled() {
> ??????? return task.isCancelled();
> ??? }
>
> ??? public boolean isDone() {
> ??????? return task.isDone();
> ??? }
>
> ??? public V get() throws InterruptedException, ExecutionException {
> ??????? return task.get();
> ??? }
>
> ??? public V get(long timeout, TimeUnit unit)
> ??????? throws InterruptedException, ExecutionException, TimeoutException {
> ??????? return task.get(timeout, unit);
> ??? }
> }
>
> This internal FutureTask construction is also found in SwingWorker, by the
> way.
>
> Joe
>
> 2009/4/7 P?ter Kov?cs
>>
>> Hi,
>>
>> How will a Callable implementation know that the corresponding Future
>> has been canceled?
>>
>> One mechanism I am aware of is thread interruption, but practice shows
>> that relying on this mechanism is highly unsafe. For interruption
>> checks to work safely, everyone up the call stack would have to
>> observe the related protocol, which is rarely the case.
>>
>> Is there anything else in place for this purpose? If there is not,
>> wouldn't it be reasonable to provide a "useful" default implementation
>> of Callable -- along the lines of FutureTask being an implementation
>> of Future?
>>
>> Thanks
>> Peter
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From joe.bowbeer at gmail.com  Tue Apr  7 17:34:18 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 7 Apr 2009 14:34:18 -0700
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
Message-ID: <31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>

Peter,

My uses of ExecutorService tend to be centered around Future/Runnable tasks
rather than Callables, and I've always felt that the avenue for extension in
this area is through task customization.  Callable is supported as a
convenience, but it's not convenient in every situation...

What prevents you from executing your own tasks (e.g., MyTask)?  Or rather,
why do you think it's important to support a new flavor of Callable instead?

Joe

2009/4/7 P?ter Kov?cs

> Joe,
>
> Thank you for your reply.
>
> I like your suggestion for a 'better "Callable"'. Couldn't it be
> included in the API with the ExecutorService et al. supporting it?
> Implementing against it would be, in a number of cases, much more
> straightforward than with the current apparatus.
>
> Thanks
> Peter
>
> On Tue, Apr 7, 2009 at 7:10 PM, Joe Bowbeer wrote:
> > Peter,
> >
> > Callable was designed to be useful stand-alone, like Runnable.  Because
> of
> > this, Callable has no more knowledge of its Future (if associated) than
> > Runnable has of its Thread, and therein lies the rub when one is trying
> to
> > write a Callable that knows the state of its Future.
> >
> > A better "Callable" for the purpose of detecting cancellation might have
> > been:
> >
> >   public interface MyCallable<V> {
> >       V call(Future task) throws Exception;
> >   }
> >
> > Then you could write "callables" of the form:
> >
> >   MyCallable<Integer> mc = new MyCallable<Integer>() {
> >       public Integer call(Future task) {
> >           while (!task.isCancelled()) {
> >               Thread.yield(); // Please!!
> >           }
> >           return 0;
> >       }
> >   };
> >
> > Below is a Task class that uses these instead:
> >
> > class MyTask<V> implements Future<V>, Runnable {
> >
> >     protected final FutureTask<V> task;
> >
> >     public MyTask(final MyCallable<V> mc) {
> >         task = new FutureTask<V>(new Callable<V>() {
> >             public V call() throws Exception {
> >                 return mc.call(task);
> >             }
> >         });
> >     }
> >
> >     /* Runnable implementation. */
> >
> >     public void run() {
> >         task.run();
> >     }
> >
> >     /* Future implementation. */
> >
> >     public boolean cancel(boolean mayInterruptIfRunning) {
> >         return task.cancel(mayInterruptIfRunning);
> >     }
> >
> >     public boolean isCancelled() {
> >         return task.isCancelled();
> >     }
> >
> >     public boolean isDone() {
> >         return task.isDone();
> >     }
> >
> >     public V get() throws InterruptedException, ExecutionException {
> >         return task.get();
> >     }
> >
> >     public V get(long timeout, TimeUnit unit)
> >         throws InterruptedException, ExecutionException, TimeoutException
> {
> >         return task.get(timeout, unit);
> >     }
> > }
> >
> > This internal FutureTask construction is also found in SwingWorker, by
> the
> > way.
> >
> > Joe
> >
> > 2009/4/7 P?ter Kov?cs
> >>
> >> Hi,
> >>
> >> How will a Callable implementation know that the corresponding Future
> >> has been canceled?
> >>
> >> One mechanism I am aware of is thread interruption, but practice shows
> >> that relying on this mechanism is highly unsafe. For interruption
> >> checks to work safely, everyone up the call stack would have to
> >> observe the related protocol, which is rarely the case.
> >>
> >> Is there anything else in place for this purpose? If there is not,
> >> wouldn't it be reasonable to provide a "useful" default implementation
> >> of Callable -- along the lines of FutureTask being an implementation
> >> of Future?
> >>
> >> Thanks
> >> Peter
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090407/6f3ab833/attachment.html>

From tim at peierls.net  Tue Apr  7 17:40:47 2009
From: tim at peierls.net (Tim Peierls)
Date: Tue, 7 Apr 2009 17:40:47 -0400
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
Message-ID: <63b4e4050904071440kdf1ab8ep44bb0e8c6d0e89a6@mail.gmail.com>

2009/4/7 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>

> One mechanism I am aware of is thread interruption, but practice shows
> that relying on this mechanism is highly unsafe. For interruption
> checks to work safely, everyone up the call stack would have to
> observe the related protocol, which is rarely the case.


In defense of interruption:

If you execute your Callables in a ThreadPoolExecutor, the interrupt status
is managed carefully so that an interrupt-based cancellation of one task
won't affect other tasks in the same pool thread.

Code that silently swallows the interrupt on an InterruptedException is
broken even if you aren't relying on interruption for cancellation, so I
don't think it's fair to say that "relying on this mechanism is highly
unsafe". Relying on broken code is unsafe.

Regardless of the protocol you adopt for cancellation, everyone up the call
stack has to be aware of the protocol in order to be responsive to
cancellation requests. And testing interrupt status can be done anywhere in
the call stack without passing or setting special context. That's not the
case, for example, with the technique Joe illustrated, where all code
reachable from MyCallable needs to be able to get the task cancellation
handle (the Future). Joe's technique is useful, especially when you can't be
sure your Callable will run in a protected environment like
ThreadPoolExecutor, but I'm not convinced it needs to be standardized
(doesn't sound like Joe is, either).

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090407/eae21eb8/attachment-0001.html>

From peter.kovacs.1.0rc at gmail.com  Tue Apr  7 18:23:55 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Wed, 8 Apr 2009 00:23:55 +0200
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
Message-ID: <fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>

Joe,

Thank you for your clarification.

Looking more closely at your sample code and at the API, I've come to
the conclusion that this approach will do a fairly decent job in my
case as well.

Thanks again
Peter

On Tue, Apr 7, 2009 at 11:34 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> Peter,
>
> My uses of ExecutorService tend to be centered around Future/Runnable tasks
> rather than Callables, and I've always felt that the avenue for extension in
> this area is through task customization.? Callable is supported as a
> convenience, but it's not convenient in every situation...
>
> What prevents you from executing your own tasks (e.g., MyTask)?? Or rather,
> why do you think it's important to support a new flavor of Callable instead?
>
> Joe
>
> 2009/4/7 P?ter Kov?cs
>>
>> Joe,
>>
>> Thank you for your reply.
>>
>> I like your suggestion for a 'better "Callable"'. Couldn't it be
>> included in the API with the ExecutorService et al. supporting it?
>> Implementing against it would be, in a number of cases, much more
>> straightforward than with the current apparatus.
>>
>> Thanks
>> Peter
>>
>> On Tue, Apr 7, 2009 at 7:10 PM, Joe Bowbeer wrote:
>> > Peter,
>> >
>> > Callable was designed to be useful stand-alone, like Runnable.? Because
>> > of
>> > this, Callable has no more knowledge of its Future (if associated) than
>> > Runnable has of its Thread, and therein lies the rub when one is trying
>> > to
>> > write a Callable that knows the state of its Future.
>> >
>> > A better "Callable" for the purpose of detecting cancellation might have
>> > been:
>> >
>> > ? public interface MyCallable<V> {
>> > ????? V call(Future task) throws Exception;
>> > ? }
>> >
>> > Then you could write "callables" of the form:
>> >
>> > ? MyCallable<Integer> mc = new MyCallable<Integer>() {
>> > ????? public Integer call(Future task) {
>> > ????????? while (!task.isCancelled()) {
>> > ????????????? Thread.yield(); // Please!!
>> > ????????? }
>> > ????????? return 0;
>> > ????? }
>> > ? };
>> >
>> > Below is a Task class that uses these instead:
>> >
>> > class MyTask<V> implements Future<V>, Runnable {
>> >
>> > ??? protected final FutureTask<V> task;
>> >
>> > ??? public MyTask(final MyCallable<V> mc) {
>> > ??????? task = new FutureTask<V>(new Callable<V>() {
>> > ??????????? public V call() throws Exception {
>> > ??????????????? return mc.call(task);
>> > ??????????? }
>> > ??????? });
>> > ??? }
>> >
>> > ??? /* Runnable implementation. */
>> >
>> > ??? public void run() {
>> > ??????? task.run();
>> > ??? }
>> >
>> > ??? /* Future implementation. */
>> >
>> > ??? public boolean cancel(boolean mayInterruptIfRunning) {
>> > ??????? return task.cancel(mayInterruptIfRunning);
>> > ??? }
>> >
>> > ??? public boolean isCancelled() {
>> > ??????? return task.isCancelled();
>> > ??? }
>> >
>> > ??? public boolean isDone() {
>> > ??????? return task.isDone();
>> > ??? }
>> >
>> > ??? public V get() throws InterruptedException, ExecutionException {
>> > ??????? return task.get();
>> > ??? }
>> >
>> > ??? public V get(long timeout, TimeUnit unit)
>> > ??????? throws InterruptedException, ExecutionException,
>> > TimeoutException {
>> > ??????? return task.get(timeout, unit);
>> > ??? }
>> > }
>> >
>> > This internal FutureTask construction is also found in SwingWorker, by
>> > the
>> > way.
>> >
>> > Joe
>> >
>> > 2009/4/7 P?ter Kov?cs
>> >>
>> >> Hi,
>> >>
>> >> How will a Callable implementation know that the corresponding Future
>> >> has been canceled?
>> >>
>> >> One mechanism I am aware of is thread interruption, but practice shows
>> >> that relying on this mechanism is highly unsafe. For interruption
>> >> checks to work safely, everyone up the call stack would have to
>> >> observe the related protocol, which is rarely the case.
>> >>
>> >> Is there anything else in place for this purpose? If there is not,
>> >> wouldn't it be reasonable to provide a "useful" default implementation
>> >> of Callable -- along the lines of FutureTask being an implementation
>> >> of Future?
>> >>
>> >> Thanks
>> >> Peter
>> >
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From peter.kovacs.1.0rc at gmail.com  Tue Apr  7 18:43:04 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Wed, 8 Apr 2009 00:43:04 +0200
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <63b4e4050904071440kdf1ab8ep44bb0e8c6d0e89a6@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<63b4e4050904071440kdf1ab8ep44bb0e8c6d0e89a6@mail.gmail.com>
Message-ID: <fdeb32eb0904071543w6f99ebe6p692aca687af799a7@mail.gmail.com>

Tim,

I am not sure we think of the same sort of problems when talking about
the practically unsafe nature of interruptions. I mean problems such
as the JDBC driver of a leading database vendor getting into error
conditions which are difficult/impossible to recover when interrupted
"unexpectedly". Sure, I cannot fix bugs like this in third-party code,
but can prevent execution from going through it. Sure, foregoing the
potential of short-cutting through the call stack will lead to
suboptimal cancellation techniques, but walking over a mine field is
rarely a reasonable alternative.

Or am I missing something in your comments?

Thanks
Peter

2009/4/7 Tim Peierls <tim at peierls.net>:
> 2009/4/7 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>
>>
>> One mechanism I am aware of is thread interruption, but practice shows
>> that relying on this mechanism is highly unsafe. For interruption
>> checks to work safely, everyone up the call stack would have to
>> observe the related protocol, which is rarely the case.
>
> In defense of interruption:
> If you execute your Callables in a ThreadPoolExecutor, the interrupt status
> is managed carefully so that an interrupt-based cancellation of one task
> won't affect other tasks in the same pool thread.
> Code that silently swallows the interrupt on an InterruptedException is
> broken even if you aren't relying on interruption for cancellation, so I
> don't think it's fair to say that "relying on this mechanism is highly
> unsafe". Relying on broken code is unsafe.
> Regardless of the protocol you adopt for cancellation, everyone up the call
> stack has to be aware of the protocol in order to be responsive to
> cancellation requests. And testing interrupt status can be done anywhere in
> the call stack without passing or setting special context.?That's not the
> case, for example, with the technique Joe illustrated, where all code
> reachable from MyCallable needs to be able to get the task cancellation
> handle (the Future).?Joe's technique is useful, especially when you can't be
> sure your Callable will run in a protected environment like
> ThreadPoolExecutor, but I'm not convinced it needs to be standardized
> (doesn't sound like Joe is, either).
> --tim
>


From joe.bowbeer at gmail.com  Tue Apr  7 19:31:46 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 7 Apr 2009 16:31:46 -0700
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <63b4e4050904071440kdf1ab8ep44bb0e8c6d0e89a6@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<63b4e4050904071440kdf1ab8ep44bb0e8c6d0e89a6@mail.gmail.com>
Message-ID: <31f2a7bd0904071631u626535fco17adbe3664af6153@mail.gmail.com>

2009/4/7 Tim Peierls

> 2009/4/7 P?ter Kov?cs
>
>> One mechanism I am aware of is thread interruption, but practice shows
>> that relying on this mechanism is highly unsafe. [...]
>>
>
> In defense of interruption: [...]
>
>
To be clear, I wasn't passing judgment on Thread.interrupt.  It is the only
general mechanism we have.  However, it is not always possible or desirable,
which is why Future.cancel includes a "mayInterruptIfRunning" flag.

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090407/46f825c2/attachment.html>

From brett.bernstein at gmail.com  Tue Apr  7 20:21:51 2009
From: brett.bernstein at gmail.com (Brett Bernstein)
Date: Tue, 7 Apr 2009 20:21:51 -0400
Subject: [concurrency-interest] Multi consumer queue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEKOIAAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOEKOIAAA.davidcholmes@aapt.net.au>
Message-ID: <280B728E21734E81B4CD6B66B1907EB3@BrettPC>

Sometimes a slightly larger cost each time can be better (particularly with 
a low latency requirement) than a GC every so often, even if the total cost 
of the allocations + the GC is less.  To the point that certain 
implementations are better in certain scenarios, I feel like having the 
option of using a non-allocating lock would be very useful.  In the 
multithreaded applications I write these days I would never use anything 
else.
-Brett
----- Original Message ----- 
From: "David Holmes" <davidcholmes at aapt.net.au>
To: "Brett Bernstein" <brett.bernstein at gmail.com>
Cc: <concurrency-interest at cs.oswego.edu>
Sent: Tuesday, April 07, 2009 1:03 AM
Subject: RE: [concurrency-interest] Multi consumer queue


> Brett Bernstein writes:
>> I could be entirely wrong on this, but to me it seems wasteful
>> that I could have a program with 2 threads and 1 lock, do a bunch
>> of locking and releasing over the course of the program, and as a result
>> allocate millions of Nodes in the addWaiter method of
> AbstractQueuedSynchronizer.
>> It feels like there could be a much more efficient implementation here.
>> For example, if I wanted to build a low latency network application that
> never garbage
>> collected it becomes much harder when the best locking tools allocate
> aggresively.
>
> For a particular application scenario a more efficient implementation is
> nearly always possible. But in general, allocation is dirt cheap (99% of 
> the
> time from the thread-local allocation buffer) and short-lived objects are
> easily reclaimed by the GC from the young generation. And you only get
> millions of allocations if you get millions of contentions on the lock -
> which is likely to be more of a problem with regards to performance and
> latency than the allocation itself.
>
> One alternative would be to have each Thread have it's own pre-defined
> Node - given that a Thread can only block on one synchronizer at a time.
> However this also has implications for the queuing algorithms as it means
> that dequeuing has to be precise (so the Node can't end up on more than 
> one
> queue), where presently some dequeuing (for timeouts/cancellation) is done
> lazily - and so could impact performance of individual operations. A
> practical constraint on this approach is that Thread is not part of j.u.c 
> so
> (until Java 7 modules arrive) AQS would have to use some hackery to get
> access to the Thread's Node.
>
> Object pooling for Nodes could also be used but the cost of CASing a Node
> out of the freelist might be more than the cost of a TLAB allocation; and
> CASing it back into the freelist is much more expensive than simply 
> dropping
> the Node as garbage - of course you don't get GC running (just for Nodes)
> but perhaps GCing a million garbage Nodes is cheaper than a million 
> freelist
> adds? And of course freelists have other management issues that can impact
> the worst-case cost.
>
> Cheers,
> David Holmes
>>
>>
>> ----- Original Message -----
>> From: "Doug Lea" <dl at cs.oswego.edu>
>> To: "Brett Bernstein" <brett.bernstein at gmail.com>
>> Cc: <concurrency-interest at cs.oswego.edu>
>> Sent: Sunday, February 22, 2009 7:14 PM
>> Subject: Re: [concurrency-interest] Multi consumer queue
>>
>>
>> > Brett Bernstein wrote:
>> >> As a side note, I have a bit of a qualm with ReentrantLock.
>> Specifically
>> >> the fact that waiting on the lock requires new to be called, which in
>> >> some apps/situations I try to avoid.
>> >
>> > I assume you mean internal queue nodes constructed when threads block
>> > waiting for locks? Of course the same thing happens inside
>> builtin locks,
>> > but uses the JVM's internal memory management, which in most JVMs does
>> > not have the benefit of sitting on a high-performance garbage 
>> > collector.
>> >
>> > There are a bunch of situations in (concurrent) programming where
>> > allocating memory is a bad idea, but waiting for locks is not
>> > often among them. But if you would like to trade off more time
>> > spinning rather than  allocating and blocking, you can precede
>> > calls to lock() with some number of calls to tryLock.
>> >
>> > -Doug
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> 


From joe.bowbeer at gmail.com  Tue Apr  7 21:01:16 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 7 Apr 2009 18:01:16 -0700
Subject: [concurrency-interest] Multi consumer queue
In-Reply-To: <280B728E21734E81B4CD6B66B1907EB3@BrettPC>
References: <NFBBKALFDCPFIDBNKAPCOEKOIAAA.davidcholmes@aapt.net.au>
	<280B728E21734E81B4CD6B66B1907EB3@BrettPC>
Message-ID: <31f2a7bd0904071801s37bfd29x6df11c6b8d0ff7b1@mail.gmail.com>

On Tue, Apr 7, 2009 at 5:21 PM, Brett Bernstein wrote:

> Sometimes a slightly larger cost each time can be better (particularly with
> a low latency requirement) than a GC every so often, even if the total cost
> of the allocations + the GC is less.  To the point that certain
> implementations are better in certain scenarios, I feel like having the
> option of using a non-allocating lock would be very useful.  In the
> multithreaded applications I write these days I would never use anything
> else.
> -Brett
>

Can you run benchmark these alternatives on a JVM and post the results?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090407/de4b2555/attachment.html>

From tim at peierls.net  Tue Apr  7 23:41:15 2009
From: tim at peierls.net (Tim Peierls)
Date: Tue, 7 Apr 2009 23:41:15 -0400
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <fdeb32eb0904071543w6f99ebe6p692aca687af799a7@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<63b4e4050904071440kdf1ab8ep44bb0e8c6d0e89a6@mail.gmail.com>
	<fdeb32eb0904071543w6f99ebe6p692aca687af799a7@mail.gmail.com>
Message-ID: <63b4e4050904072041v5c41bd17m885c0467ee0716e9@mail.gmail.com>

2009/4/7 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>

> I am not sure we think of the same sort of problems when talking about
> the practically unsafe nature of interruptions. I mean problems such
> as the JDBC driver of a leading database vendor getting into error
> conditions which are difficult/impossible to recover when interrupted
> "unexpectedly". Sure, I cannot fix bugs like this in third-party code,
> but can prevent execution from going through it. Sure, foregoing the
> potential of short-cutting through the call stack will lead to
> suboptimal cancellation techniques, but walking over a mine field is
> rarely a reasonable alternative.
>
> Or am I missing something in your comments?
>

I don't think so. If your tasks have to call into badly behaved code, then
their cancellation policies are necessarily constrained.

I wouldn't want to standardize cancellation policies for dealing with broken
code, but approaches such as the one Joe suggested are good to have in a
visible spot like this. Note that JCiP has an example of dealing with
nonstandard cancellation policies by overriding FutureTask.cancel; maybe it
should also have briefly mentioned this technique.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090407/f58bb7ba/attachment-0001.html>

From peter.kovacs.1.0rc at gmail.com  Wed Apr  8 13:10:53 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Wed, 8 Apr 2009 19:10:53 +0200
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
	<fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
Message-ID: <fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>

One application of this approach where I cannot find an elegant
solution is when I want to use an ExecutorCompletionService for
"joining" a definite number of "cancelable" concurrent tasks:

With a hypothetical collaboration of future-aware Callables, Futures
and Executors built into the API, I could (1) store the futures
returned by ExecutorCompletionService.submit in a list, (2) remove
from the list each future returned by ExecutorCompletionService.take,
(3) get the result off of the same future and (4) check if the list is
empty to see if more futures need to be taken from the ECS. Something
like:

while (myFutures.size() > 0) {
    Future<String> f = executorCompletionService.take();
    myFutures.remove(f);
    f.get();
}

Using the "MyTask approach" with ECS, I have to do something like this:

while (nrCompletedTasks < myTasks.size()) {
    Future<String> f = executorCompletionService.take();
    nrCompletedTasks++;
    f.get();
}

Here, I have the extra job of keeping track of the number of finished
tasks -- in addition to storing references to the tasks (MyTasks) for
potential canceling. Also -- as it is unknown which MyTask instance
corresponds to a given future returned by ECS.take() --, I end up
using MyTask only for cancellation and the ECS-generated futures for
getting the results. As Future is basically meant to be a "handle" to
an asynchronous task, having to unrelated "handle" to the same task
with split functionality is (conceptually) confusing.

Comments appreciated.

Thanks
Peter

2009/4/8 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
> Joe,
>
> Thank you for your clarification.
>
> Looking more closely at your sample code and at the API, I've come to
> the conclusion that this approach will do a fairly decent job in my
> case as well.
>
> Thanks again
> Peter
>
> On Tue, Apr 7, 2009 at 11:34 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>> Peter,
>>
>> My uses of ExecutorService tend to be centered around Future/Runnable tasks
>> rather than Callables, and I've always felt that the avenue for extension in
>> this area is through task customization.? Callable is supported as a
>> convenience, but it's not convenient in every situation...
>>
>> What prevents you from executing your own tasks (e.g., MyTask)?? Or rather,
>> why do you think it's important to support a new flavor of Callable instead?
>>
>> Joe
>>
>> 2009/4/7 P?ter Kov?cs
>>>
>>> Joe,
>>>
>>> Thank you for your reply.
>>>
>>> I like your suggestion for a 'better "Callable"'. Couldn't it be
>>> included in the API with the ExecutorService et al. supporting it?
>>> Implementing against it would be, in a number of cases, much more
>>> straightforward than with the current apparatus.
>>>
>>> Thanks
>>> Peter
>>>
>>> On Tue, Apr 7, 2009 at 7:10 PM, Joe Bowbeer wrote:
>>> > Peter,
>>> >
>>> > Callable was designed to be useful stand-alone, like Runnable.? Because
>>> > of
>>> > this, Callable has no more knowledge of its Future (if associated) than
>>> > Runnable has of its Thread, and therein lies the rub when one is trying
>>> > to
>>> > write a Callable that knows the state of its Future.
>>> >
>>> > A better "Callable" for the purpose of detecting cancellation might have
>>> > been:
>>> >
>>> > ? public interface MyCallable<V> {
>>> > ????? V call(Future task) throws Exception;
>>> > ? }
>>> >
>>> > Then you could write "callables" of the form:
>>> >
>>> > ? MyCallable<Integer> mc = new MyCallable<Integer>() {
>>> > ????? public Integer call(Future task) {
>>> > ????????? while (!task.isCancelled()) {
>>> > ????????????? Thread.yield(); // Please!!
>>> > ????????? }
>>> > ????????? return 0;
>>> > ????? }
>>> > ? };
>>> >
>>> > Below is a Task class that uses these instead:
>>> >
>>> > class MyTask<V> implements Future<V>, Runnable {
>>> >
>>> > ??? protected final FutureTask<V> task;
>>> >
>>> > ??? public MyTask(final MyCallable<V> mc) {
>>> > ??????? task = new FutureTask<V>(new Callable<V>() {
>>> > ??????????? public V call() throws Exception {
>>> > ??????????????? return mc.call(task);
>>> > ??????????? }
>>> > ??????? });
>>> > ??? }
>>> >
>>> > ??? /* Runnable implementation. */
>>> >
>>> > ??? public void run() {
>>> > ??????? task.run();
>>> > ??? }
>>> >
>>> > ??? /* Future implementation. */
>>> >
>>> > ??? public boolean cancel(boolean mayInterruptIfRunning) {
>>> > ??????? return task.cancel(mayInterruptIfRunning);
>>> > ??? }
>>> >
>>> > ??? public boolean isCancelled() {
>>> > ??????? return task.isCancelled();
>>> > ??? }
>>> >
>>> > ??? public boolean isDone() {
>>> > ??????? return task.isDone();
>>> > ??? }
>>> >
>>> > ??? public V get() throws InterruptedException, ExecutionException {
>>> > ??????? return task.get();
>>> > ??? }
>>> >
>>> > ??? public V get(long timeout, TimeUnit unit)
>>> > ??????? throws InterruptedException, ExecutionException,
>>> > TimeoutException {
>>> > ??????? return task.get(timeout, unit);
>>> > ??? }
>>> > }
>>> >
>>> > This internal FutureTask construction is also found in SwingWorker, by
>>> > the
>>> > way.
>>> >
>>> > Joe
>>> >
>>> > 2009/4/7 P?ter Kov?cs
>>> >>
>>> >> Hi,
>>> >>
>>> >> How will a Callable implementation know that the corresponding Future
>>> >> has been canceled?
>>> >>
>>> >> One mechanism I am aware of is thread interruption, but practice shows
>>> >> that relying on this mechanism is highly unsafe. For interruption
>>> >> checks to work safely, everyone up the call stack would have to
>>> >> observe the related protocol, which is rarely the case.
>>> >>
>>> >> Is there anything else in place for this purpose? If there is not,
>>> >> wouldn't it be reasonable to provide a "useful" default implementation
>>> >> of Callable -- along the lines of FutureTask being an implementation
>>> >> of Future?
>>> >>
>>> >> Thanks
>>> >> Peter
>>> >
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


From gregg at cytetech.com  Wed Apr  8 13:59:27 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 08 Apr 2009 12:59:27 -0500
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
	<fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
	<fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
Message-ID: <49DCE5FF.7080909@cytetech.com>

Peter, I just use a wrapper object in cases like this which holds the needed 
references and encompasses any actions/activities which I need to happen when 
these values change state or are used in particular ways.

This allows the specialization to be all in one place...

Gregg Wonderly

P?ter Kov?cs wrote:
> One application of this approach where I cannot find an elegant
> solution is when I want to use an ExecutorCompletionService for
> "joining" a definite number of "cancelable" concurrent tasks:
> 
> With a hypothetical collaboration of future-aware Callables, Futures
> and Executors built into the API, I could (1) store the futures
> returned by ExecutorCompletionService.submit in a list, (2) remove
> from the list each future returned by ExecutorCompletionService.take,
> (3) get the result off of the same future and (4) check if the list is
> empty to see if more futures need to be taken from the ECS. Something
> like:
> 
> while (myFutures.size() > 0) {
>     Future<String> f = executorCompletionService.take();
>     myFutures.remove(f);
>     f.get();
> }
> 
> Using the "MyTask approach" with ECS, I have to do something like this:
> 
> while (nrCompletedTasks < myTasks.size()) {
>     Future<String> f = executorCompletionService.take();
>     nrCompletedTasks++;
>     f.get();
> }
> 
> Here, I have the extra job of keeping track of the number of finished
> tasks -- in addition to storing references to the tasks (MyTasks) for
> potential canceling. Also -- as it is unknown which MyTask instance
> corresponds to a given future returned by ECS.take() --, I end up
> using MyTask only for cancellation and the ECS-generated futures for
> getting the results. As Future is basically meant to be a "handle" to
> an asynchronous task, having to unrelated "handle" to the same task
> with split functionality is (conceptually) confusing.
> 
> Comments appreciated.
> 
> Thanks
> Peter
> 
> 2009/4/8 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
>> Joe,
>>
>> Thank you for your clarification.
>>
>> Looking more closely at your sample code and at the API, I've come to
>> the conclusion that this approach will do a fairly decent job in my
>> case as well.
>>
>> Thanks again
>> Peter
>>
>> On Tue, Apr 7, 2009 at 11:34 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>>> Peter,
>>>
>>> My uses of ExecutorService tend to be centered around Future/Runnable tasks
>>> rather than Callables, and I've always felt that the avenue for extension in
>>> this area is through task customization.  Callable is supported as a
>>> convenience, but it's not convenient in every situation...
>>>
>>> What prevents you from executing your own tasks (e.g., MyTask)?  Or rather,
>>> why do you think it's important to support a new flavor of Callable instead?
>>>
>>> Joe
>>>
>>> 2009/4/7 P?ter Kov?cs
>>>> Joe,
>>>>
>>>> Thank you for your reply.
>>>>
>>>> I like your suggestion for a 'better "Callable"'. Couldn't it be
>>>> included in the API with the ExecutorService et al. supporting it?
>>>> Implementing against it would be, in a number of cases, much more
>>>> straightforward than with the current apparatus.
>>>>
>>>> Thanks
>>>> Peter
>>>>
>>>> On Tue, Apr 7, 2009 at 7:10 PM, Joe Bowbeer wrote:
>>>>> Peter,
>>>>>
>>>>> Callable was designed to be useful stand-alone, like Runnable.  Because
>>>>> of
>>>>> this, Callable has no more knowledge of its Future (if associated) than
>>>>> Runnable has of its Thread, and therein lies the rub when one is trying
>>>>> to
>>>>> write a Callable that knows the state of its Future.
>>>>>
>>>>> A better "Callable" for the purpose of detecting cancellation might have
>>>>> been:
>>>>>
>>>>>   public interface MyCallable<V> {
>>>>>       V call(Future task) throws Exception;
>>>>>   }
>>>>>
>>>>> Then you could write "callables" of the form:
>>>>>
>>>>>   MyCallable<Integer> mc = new MyCallable<Integer>() {
>>>>>       public Integer call(Future task) {
>>>>>           while (!task.isCancelled()) {
>>>>>               Thread.yield(); // Please!!
>>>>>           }
>>>>>           return 0;
>>>>>       }
>>>>>   };
>>>>>
>>>>> Below is a Task class that uses these instead:
>>>>>
>>>>> class MyTask<V> implements Future<V>, Runnable {
>>>>>
>>>>>     protected final FutureTask<V> task;
>>>>>
>>>>>     public MyTask(final MyCallable<V> mc) {
>>>>>         task = new FutureTask<V>(new Callable<V>() {
>>>>>             public V call() throws Exception {
>>>>>                 return mc.call(task);
>>>>>             }
>>>>>         });
>>>>>     }
>>>>>
>>>>>     /* Runnable implementation. */
>>>>>
>>>>>     public void run() {
>>>>>         task.run();
>>>>>     }
>>>>>
>>>>>     /* Future implementation. */
>>>>>
>>>>>     public boolean cancel(boolean mayInterruptIfRunning) {
>>>>>         return task.cancel(mayInterruptIfRunning);
>>>>>     }
>>>>>
>>>>>     public boolean isCancelled() {
>>>>>         return task.isCancelled();
>>>>>     }
>>>>>
>>>>>     public boolean isDone() {
>>>>>         return task.isDone();
>>>>>     }
>>>>>
>>>>>     public V get() throws InterruptedException, ExecutionException {
>>>>>         return task.get();
>>>>>     }
>>>>>
>>>>>     public V get(long timeout, TimeUnit unit)
>>>>>         throws InterruptedException, ExecutionException,
>>>>> TimeoutException {
>>>>>         return task.get(timeout, unit);
>>>>>     }
>>>>> }
>>>>>
>>>>> This internal FutureTask construction is also found in SwingWorker, by
>>>>> the
>>>>> way.
>>>>>
>>>>> Joe
>>>>>
>>>>> 2009/4/7 P?ter Kov?cs
>>>>>> Hi,
>>>>>>
>>>>>> How will a Callable implementation know that the corresponding Future
>>>>>> has been canceled?
>>>>>>
>>>>>> One mechanism I am aware of is thread interruption, but practice shows
>>>>>> that relying on this mechanism is highly unsafe. For interruption
>>>>>> checks to work safely, everyone up the call stack would have to
>>>>>> observe the related protocol, which is rarely the case.
>>>>>>
>>>>>> Is there anything else in place for this purpose? If there is not,
>>>>>> wouldn't it be reasonable to provide a "useful" default implementation
>>>>>> of Callable -- along the lines of FutureTask being an implementation
>>>>>> of Future?
>>>>>>
>>>>>> Thanks
>>>>>> Peter
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From joe.bowbeer at gmail.com  Wed Apr  8 18:05:07 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 8 Apr 2009 15:05:07 -0700
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
	<fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
	<fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
Message-ID: <31f2a7bd0904081505w30198e05j630dce64ec8c6979@mail.gmail.com>

Peter,

ExecutorCompletionService is dead simple.

It's based on a trivial FutureTask extension that hooks the done() method in
order to enqueue the original task on a completion queue:

  private class QueueingFuture extends FutureTask<Void> {
      QueueingFuture(RunnableFuture<V> task) {
          super(task, null);
          this.task = task;
      }
      protected void done() { completionQueue.add(task); }
      private final Future<V> task;
  }

This functionality could easily be added to MyTask.  (See one approach
below.)  Then you can eliminate the completion service and "take" directly
from the completion queue.

A few more things to point out:

If you construct the completion service with an AbstractExecutorService, the
completion service asks the your executor service to construct the task.
(See newTaskFor method).  This task, created by your executor service, is
then returned to you by the completion service, and this is the same task
that is enqueued.  You might explore this avenue for task customization.

A custom "completion task" designed for subclassing follows.  Your subclass
overrides the "compute" method.  Note that your compute method
implementation can query isCancelled.

public abstract class Worker<V> implements RunnableFuture<V> {

    protected Worker(BlockingQueue<Future<V>> completionQueue) {
        this.completionQueue = completionQueue;
    }

    private final BlockingQueue<Future<V>> completionQueue;

    /**
     * Calls the <tt>compute</tt> method to compute the result.
     */
    private final FutureTask<V> task =
        new FutureTask<V>(new Callable<V>() {
            public V call() throws Exception {
                return compute();
            }
        }) {
            @Override protected void done() {
                completionQueue.add(task);
            }
        };

    /**
     * Computes the value to be returned by the <tt>get</tt> method.
     */
    protected abstract V compute() throws Exception;

    /* RunnableFuture implementation. */

    public void run() {
        task.run();
    }

    public boolean cancel(boolean mayInterruptIfRunning) {
        return task.cancel(mayInterruptIfRunning);
    }

    public boolean isCancelled() {
        return task.isCancelled();
    }

    public boolean isDone() {
        return task.isDone();
    }

    public V get() throws InterruptedException, ExecutionException {
        return task.get();
    }

    public V get(long timeout, TimeUnit unit)
        throws InterruptedException, ExecutionException, TimeoutException {
        return task.get(timeout, unit);
    }
}

Sample construction:

  BlockingQueue<Future<Integer>> queue =
          new LinkedBlockingQueue<Future<Integer>>();

  Worker<Integer> worker = new Worker<Integer>(queue) {
      protected Integer compute() {
          while (isCancelled()) {
              Thread.yield(); // Please!!
          }
          return 0;
      }
  };

Joe

2009/4/8 P?ter Kov?cs

> One application of this approach where I cannot find an elegant
> solution is when I want to use an ExecutorCompletionService for
> "joining" a definite number of "cancelable" concurrent tasks:
>
> With a hypothetical collaboration of future-aware Callables, Futures
> and Executors built into the API, I could (1) store the futures
> returned by ExecutorCompletionService.submit in a list, (2) remove
> from the list each future returned by ExecutorCompletionService.take,
> (3) get the result off of the same future and (4) check if the list is
> empty to see if more futures need to be taken from the ECS. Something
> like:
>
> while (myFutures.size() > 0) {
>    Future<String> f = executorCompletionService.take();
>    myFutures.remove(f);
>    f.get();
> }
>
> Using the "MyTask approach" with ECS, I have to do something like this:
>
> while (nrCompletedTasks < myTasks.size()) {
>    Future<String> f = executorCompletionService.take();
>    nrCompletedTasks++;
>    f.get();
> }
>
> Here, I have the extra job of keeping track of the number of finished
> tasks -- in addition to storing references to the tasks (MyTasks) for
> potential canceling. Also -- as it is unknown which MyTask instance
> corresponds to a given future returned by ECS.take() --, I end up
> using MyTask only for cancellation and the ECS-generated futures for
> getting the results. As Future is basically meant to be a "handle" to
> an asynchronous task, having to unrelated "handle" to the same task
> with split functionality is (conceptually) confusing.
>
> Comments appreciated.
>
> Thanks
> Peter
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090408/dde0a755/attachment-0001.html>

From joe.bowbeer at gmail.com  Wed Apr  8 18:17:52 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 8 Apr 2009 15:17:52 -0700
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <31f2a7bd0904081505w30198e05j630dce64ec8c6979@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
	<fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
	<fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
	<31f2a7bd0904081505w30198e05j630dce64ec8c6979@mail.gmail.com>
Message-ID: <31f2a7bd0904081517i5bbb6b88p4d182c01343960c9@mail.gmail.com>

Correction: The internal task should add the outer task (Worker) to the
completion queue, as follows, so that the tasks you take from the queue are
the same ones you constructed:

    private final FutureTask<V> task =
        new FutureTask<V>(new Callable<V>() {
            public V call() throws Exception {
                return compute();
            }
        }) {
            @Override protected void done() {
                completionQueue.add(Worker.this);
            }
        };

Joe

On Wed, Apr 8, 2009 at 3:05 PM, Joe Bowbeer wrote:

> Peter,
>
> ExecutorCompletionService is dead simple.
>
> It's based on a trivial FutureTask extension that hooks the done() method
> in order to enqueue the original task on a completion queue:
>
>   private class QueueingFuture extends FutureTask<Void> {
>       QueueingFuture(RunnableFuture<V> task) {
>           super(task, null);
>           this.task = task;
>       }
>       protected void done() { completionQueue.add(task); }
>       private final Future<V> task;
>   }
>
> This functionality could easily be added to MyTask.  (See one approach
> below.)  Then you can eliminate the completion service and "take" directly
> from the completion queue.
>
> A few more things to point out:
>
> If you construct the completion service with an AbstractExecutorService,
> the completion service asks the your executor service to construct the
> task.  (See newTaskFor method).  This task, created by your executor
> service, is then returned to you by the completion service, and this is the
> same task that is enqueued.  You might explore this avenue for task
> customization.
>
> A custom "completion task" designed for subclassing follows.  Your subclass
> overrides the "compute" method.  Note that your compute method
> implementation can query isCancelled.
>
> public abstract class Worker<V> implements RunnableFuture<V> {
>
>     protected Worker(BlockingQueue<Future<V>> completionQueue) {
>         this.completionQueue = completionQueue;
>     }
>
>     private final BlockingQueue<Future<V>> completionQueue;
>
>     /**
>      * Calls the <tt>compute</tt> method to compute the result.
>      */
>     private final FutureTask<V> task =
>         new FutureTask<V>(new Callable<V>() {
>             public V call() throws Exception {
>                 return compute();
>             }
>         }) {
>             @Override protected void done() {
>                 completionQueue.add(task);
>             }
>         };
>
>     /**
>      * Computes the value to be returned by the <tt>get</tt> method.
>      */
>     protected abstract V compute() throws Exception;
>
>     /* RunnableFuture implementation. */
>
>     public void run() {
>         task.run();
>     }
>
>     public boolean cancel(boolean mayInterruptIfRunning) {
>         return task.cancel(mayInterruptIfRunning);
>     }
>
>     public boolean isCancelled() {
>         return task.isCancelled();
>     }
>
>     public boolean isDone() {
>         return task.isDone();
>     }
>
>     public V get() throws InterruptedException, ExecutionException {
>         return task.get();
>     }
>
>     public V get(long timeout, TimeUnit unit)
>         throws InterruptedException, ExecutionException, TimeoutException {
>         return task.get(timeout, unit);
>     }
> }
>
> Sample construction:
>
>   BlockingQueue<Future<Integer>> queue =
>           new LinkedBlockingQueue<Future<Integer>>();
>
>   Worker<Integer> worker = new Worker<Integer>(queue) {
>       protected Integer compute() {
>           while (isCancelled()) {
>               Thread.yield(); // Please!!
>           }
>           return 0;
>       }
>   };
>
> Joe
>
>
> 2009/4/8 P?ter Kov?cs
>
>> One application of this approach where I cannot find an elegant
>> solution is when I want to use an ExecutorCompletionService for
>> "joining" a definite number of "cancelable" concurrent tasks:
>>
>> With a hypothetical collaboration of future-aware Callables, Futures
>> and Executors built into the API, I could (1) store the futures
>> returned by ExecutorCompletionService.submit in a list, (2) remove
>> from the list each future returned by ExecutorCompletionService.take,
>> (3) get the result off of the same future and (4) check if the list is
>> empty to see if more futures need to be taken from the ECS. Something
>> like:
>>
>> while (myFutures.size() > 0) {
>>    Future<String> f = executorCompletionService.take();
>>    myFutures.remove(f);
>>    f.get();
>> }
>>
>> Using the "MyTask approach" with ECS, I have to do something like this:
>>
>> while (nrCompletedTasks < myTasks.size()) {
>>    Future<String> f = executorCompletionService.take();
>>    nrCompletedTasks++;
>>    f.get();
>> }
>>
>> Here, I have the extra job of keeping track of the number of finished
>> tasks -- in addition to storing references to the tasks (MyTasks) for
>> potential canceling. Also -- as it is unknown which MyTask instance
>> corresponds to a given future returned by ECS.take() --, I end up
>> using MyTask only for cancellation and the ECS-generated futures for
>> getting the results. As Future is basically meant to be a "handle" to
>> an asynchronous task, having to unrelated "handle" to the same task
>> with split functionality is (conceptually) confusing.
>>
>> Comments appreciated.
>>
>> Thanks
>> Peter
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090408/82fd6aea/attachment.html>

From joe.bowbeer at gmail.com  Wed Apr  8 18:33:21 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 8 Apr 2009 15:33:21 -0700
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <31f2a7bd0904081517i5bbb6b88p4d182c01343960c9@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
	<fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
	<fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
	<31f2a7bd0904081505w30198e05j630dce64ec8c6979@mail.gmail.com>
	<31f2a7bd0904081517i5bbb6b88p4d182c01343960c9@mail.gmail.com>
Message-ID: <31f2a7bd0904081533u6f74ac1ao28ae2bf6bd5c6bba@mail.gmail.com>

And yet another correction, this time in the sample construction:

  Worker<Integer> worker = new Worker<Integer>(queue) {
      protected Integer compute() {
          while (!isCancelled()) {
              // ...
          }
          return 0;
      }
  };

Should be while(!isCancelled()) ...

Joe

On Wed, Apr 8, 2009 at 3:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> Correction: The internal task should add the outer task (Worker) to the
> completion queue, as follows, so that the tasks you take from the queue are
> the same ones you constructed:
>
>     private final FutureTask<V> task =
>         new FutureTask<V>(new Callable<V>() {
>             public V call() throws Exception {
>                 return compute();
>             }
>         }) {
>             @Override protected void done() {
>                 completionQueue.add(Worker.this);
>             }
>         };
>
> Joe
>
>
> On Wed, Apr 8, 2009 at 3:05 PM, Joe Bowbeer wrote:
>
>> Peter,
>>
>> ExecutorCompletionService is dead simple.
>>
>> It's based on a trivial FutureTask extension that hooks the done() method
>> in order to enqueue the original task on a completion queue:
>>
>>   private class QueueingFuture extends FutureTask<Void> {
>>       QueueingFuture(RunnableFuture<V> task) {
>>           super(task, null);
>>           this.task = task;
>>       }
>>       protected void done() { completionQueue.add(task); }
>>       private final Future<V> task;
>>   }
>>
>> This functionality could easily be added to MyTask.  (See one approach
>> below.)  Then you can eliminate the completion service and "take" directly
>> from the completion queue.
>>
>> A few more things to point out:
>>
>> If you construct the completion service with an AbstractExecutorService,
>> the completion service asks your executor service to construct the task.
>> (See newTaskFor method).  This task, created by your executor service, is
>> then returned to you by the completion service, and this is the same task
>> that is enqueued.  You might explore this avenue for task customization.
>>
>> A custom "completion task" designed for subclassing follows.  Your
>> subclass overrides the "compute" method.  Note that your compute method
>> implementation can query isCancelled.
>>
>> public abstract class Worker<V> implements RunnableFuture<V> {
>>
>>     protected Worker(BlockingQueue<Future<V>> completionQueue) {
>>         this.completionQueue = completionQueue;
>>     }
>>
>>     private final BlockingQueue<Future<V>> completionQueue;
>>
>>     /**
>>      * Calls the <tt>compute</tt> method to compute the result.
>>      */
>>     private final FutureTask<V> task =
>>         new FutureTask<V>(new Callable<V>() {
>>             public V call() throws Exception {
>>                 return compute();
>>             }
>>         }) {
>>             @Override protected void done() {
>>                 completionQueue.add(Worker.this);
>>             }
>>         };
>>
>>     /**
>>      * Computes the value to be returned by the <tt>get</tt> method.
>>      */
>>     protected abstract V compute() throws Exception;
>>
>>     /* RunnableFuture implementation. */
>>     public void run() {
>>         task.run();
>>     }
>>
>>     public boolean cancel(boolean mayInterruptIfRunning) {
>>         return task.cancel(mayInterruptIfRunning);
>>     }
>>
>>     public boolean isCancelled() {
>>         return task.isCancelled();
>>     }
>>
>>     public boolean isDone() {
>>         return task.isDone();
>>     }
>>
>>     public V get() throws InterruptedException, ExecutionException {
>>         return task.get();
>>     }
>>
>>     public V get(long timeout, TimeUnit unit)
>>         throws InterruptedException, ExecutionException, TimeoutException
>> {
>>         return task.get(timeout, unit);
>>     }
>> }
>>
>> Sample construction:
>>
>>   BlockingQueue<Future<Integer>> queue =
>>           new LinkedBlockingQueue<Future<Integer>>();
>>
>>   Worker<Integer> worker = new Worker<Integer>(queue) {
>>       protected Integer compute() {
>>           while (!isCancelled()) {
>>               Thread.yield(); // Please!!
>>           }
>>           return 0;
>>       }
>>   };
>>
>> Joe
>>
>>
>> 2009/4/8 P?ter Kov?cs
>>
>>> One application of this approach where I cannot find an elegant
>>> solution is when I want to use an ExecutorCompletionService for
>>> "joining" a definite number of "cancelable" concurrent tasks:
>>>
>>> With a hypothetical collaboration of future-aware Callables, Futures
>>> and Executors built into the API, I could (1) store the futures
>>> returned by ExecutorCompletionService.submit in a list, (2) remove
>>> from the list each future returned by ExecutorCompletionService.take,
>>> (3) get the result off of the same future and (4) check if the list is
>>> empty to see if more futures need to be taken from the ECS. Something
>>> like:
>>>
>>> while (myFutures.size() > 0) {
>>>    Future<String> f = executorCompletionService.take();
>>>    myFutures.remove(f);
>>>    f.get();
>>> }
>>>
>>> Using the "MyTask approach" with ECS, I have to do something like this:
>>>
>>> while (nrCompletedTasks < myTasks.size()) {
>>>    Future<String> f = executorCompletionService.take();
>>>    nrCompletedTasks++;
>>>    f.get();
>>> }
>>>
>>> Here, I have the extra job of keeping track of the number of finished
>>> tasks -- in addition to storing references to the tasks (MyTasks) for
>>> potential canceling. Also -- as it is unknown which MyTask instance
>>> corresponds to a given future returned by ECS.take() --, I end up
>>> using MyTask only for cancellation and the ECS-generated futures for
>>> getting the results. As Future is basically meant to be a "handle" to
>>> an asynchronous task, having to unrelated "handle" to the same task
>>> with split functionality is (conceptually) confusing.
>>>
>>> Comments appreciated.
>>>
>>> Thanks
>>> Peter
>>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090408/c3bd0874/attachment-0001.html>

From peter.kovacs.1.0rc at gmail.com  Thu Apr  9 06:50:47 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Thu, 9 Apr 2009 12:50:47 +0200
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <31f2a7bd0904081533u6f74ac1ao28ae2bf6bd5c6bba@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
	<fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
	<fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
	<31f2a7bd0904081505w30198e05j630dce64ec8c6979@mail.gmail.com>
	<31f2a7bd0904081517i5bbb6b88p4d182c01343960c9@mail.gmail.com>
	<31f2a7bd0904081533u6f74ac1ao28ae2bf6bd5c6bba@mail.gmail.com>
Message-ID: <fdeb32eb0904090350w354e789fv382d7582989ed631@mail.gmail.com>

Joe,

Thank you so much for all your suggestions. I found them very helpful indeed.

Peter

On Thu, Apr 9, 2009 at 12:33 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> And yet another correction, this time in the sample construction:
>
> ? Worker<Integer> worker = new Worker<Integer>(queue) {
> ????? protected Integer compute() {
> ????????? while (!isCancelled()) {
> ????????????? // ...
> ????????? }
> ????????? return 0;
> ????? }
> ? };
>
> Should be while(!isCancelled()) ...
>
> Joe
>
> On Wed, Apr 8, 2009 at 3:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>>
>> Correction: The internal task should add the outer task (Worker) to the
>> completion queue, as follows, so that the tasks you take from the queue are
>> the same ones you constructed:
>>
>> ??? private final FutureTask<V> task =
>> ??????? new FutureTask<V>(new Callable<V>() {
>> ??????????? public V call() throws Exception {
>> ??????????????? return compute();
>> ??????????? }
>> ??????? }) {
>> ??????????? @Override protected void done() {
>> ??????????????? completionQueue.add(Worker.this);
>> ??????????? }
>> ??????? };
>>
>> Joe
>>
>> On Wed, Apr 8, 2009 at 3:05 PM, Joe Bowbeer wrote:
>>>
>>> Peter,
>>>
>>> ExecutorCompletionService is dead simple.
>>>
>>> It's based on a trivial FutureTask extension that hooks the done() method
>>> in order to enqueue the original task on a completion queue:
>>>
>>> ? private class QueueingFuture extends FutureTask<Void> {
>>> ????? QueueingFuture(RunnableFuture<V> task) {
>>> ????????? super(task, null);
>>> ????????? this.task = task;
>>> ????? }
>>> ????? protected void done() { completionQueue.add(task); }
>>> ????? private final Future<V> task;
>>> ? }
>>>
>>> This functionality could easily be added to MyTask.? (See one approach
>>> below.)? Then you can eliminate the completion service and "take" directly
>>> from the completion queue.
>>>
>>> A few more things to point out:
>>>
>>> If you construct the completion service with an AbstractExecutorService,
>>> the completion service asks your executor service to construct the task.
>>> (See newTaskFor method).? This task, created by your executor service, is
>>> then returned to you by the completion service, and this is the same task
>>> that is enqueued.? You might explore this avenue for task customization.
>>>
>>> A custom "completion task" designed for subclassing follows.? Your
>>> subclass overrides the "compute" method.? Note that your compute method
>>> implementation can query isCancelled.
>>>
>>> public abstract class Worker<V> implements RunnableFuture<V> {
>>>
>>> ??? protected Worker(BlockingQueue<Future<V>> completionQueue) {
>>> ??????? this.completionQueue = completionQueue;
>>> ??? }
>>>
>>> ??? private final BlockingQueue<Future<V>> completionQueue;
>>>
>>> ??? /**
>>> ???? * Calls the <tt>compute</tt> method to compute the result.
>>> ???? */
>>> ??? private final FutureTask<V> task =
>>> ??????? new FutureTask<V>(new Callable<V>() {
>>> ??????????? public V call() throws Exception {
>>> ??????????????? return compute();
>>> ??????????? }
>>> ??????? }) {
>>> ??????????? @Override protected void done() {
>>> ??????????????? completionQueue.add(Worker.this);
>>> ??????????? }
>>> ??????? };
>>>
>>> ??? /**
>>> ???? * Computes the value to be returned by the <tt>get</tt> method.
>>> ???? */
>>> ??? protected abstract V compute() throws Exception;
>>>
>>> ??? /* RunnableFuture implementation. */
>>> ??? public void run() {
>>> ??????? task.run();
>>> ??? }
>>>
>>> ??? public boolean cancel(boolean mayInterruptIfRunning) {
>>> ??????? return task.cancel(mayInterruptIfRunning);
>>> ??? }
>>>
>>> ??? public boolean isCancelled() {
>>> ??????? return task.isCancelled();
>>> ??? }
>>>
>>> ??? public boolean isDone() {
>>> ??????? return task.isDone();
>>> ??? }
>>>
>>> ??? public V get() throws InterruptedException, ExecutionException {
>>> ??????? return task.get();
>>> ??? }
>>>
>>> ??? public V get(long timeout, TimeUnit unit)
>>> ??????? throws InterruptedException, ExecutionException, TimeoutException
>>> {
>>> ??????? return task.get(timeout, unit);
>>> ??? }
>>> }
>>>
>>> Sample construction:
>>>
>>> ? BlockingQueue<Future<Integer>> queue =
>>> ????????? new LinkedBlockingQueue<Future<Integer>>();
>>>
>>> ? Worker<Integer> worker = new Worker<Integer>(queue) {
>>> ????? protected Integer compute() {
>>> ????????? while (!isCancelled()) {
>>> ????????????? Thread.yield(); // Please!!
>>> ????????? }
>>> ????????? return 0;
>>> ????? }
>>> ? };
>>>
>>> Joe
>>>
>>> 2009/4/8 P?ter Kov?cs
>>>>
>>>> One application of this approach where I cannot find an elegant
>>>> solution is when I want to use an ExecutorCompletionService for
>>>> "joining" a definite number of "cancelable" concurrent tasks:
>>>>
>>>> With a hypothetical collaboration of future-aware Callables, Futures
>>>> and Executors built into the API, I could (1) store the futures
>>>> returned by ExecutorCompletionService.submit in a list, (2) remove
>>>> from the list each future returned by ExecutorCompletionService.take,
>>>> (3) get the result off of the same future and (4) check if the list is
>>>> empty to see if more futures need to be taken from the ECS. Something
>>>> like:
>>>>
>>>> while (myFutures.size() > 0) {
>>>> ? ?Future<String> f = executorCompletionService.take();
>>>> ? ?myFutures.remove(f);
>>>> ? ?f.get();
>>>> }
>>>>
>>>> Using the "MyTask approach" with ECS, I have to do something like this:
>>>>
>>>> while (nrCompletedTasks < myTasks.size()) {
>>>> ? ?Future<String> f = executorCompletionService.take();
>>>> ? ?nrCompletedTasks++;
>>>> ? ?f.get();
>>>> }
>>>>
>>>> Here, I have the extra job of keeping track of the number of finished
>>>> tasks -- in addition to storing references to the tasks (MyTasks) for
>>>> potential canceling. Also -- as it is unknown which MyTask instance
>>>> corresponds to a given future returned by ECS.take() --, I end up
>>>> using MyTask only for cancellation and the ECS-generated futures for
>>>> getting the results. As Future is basically meant to be a "handle" to
>>>> an asynchronous task, having to unrelated "handle" to the same task
>>>> with split functionality is (conceptually) confusing.
>>>>
>>>> Comments appreciated.
>>>>
>>>> Thanks
>>>> Peter
>>>
>>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From peter.kovacs.1.0rc at gmail.com  Thu Apr  9 13:56:42 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Thu, 9 Apr 2009 19:56:42 +0200
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <fdeb32eb0904090350w354e789fv382d7582989ed631@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<31f2a7bd0904071010l65ec7c2bx78a075b1d1122f40@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
	<fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
	<fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
	<31f2a7bd0904081505w30198e05j630dce64ec8c6979@mail.gmail.com>
	<31f2a7bd0904081517i5bbb6b88p4d182c01343960c9@mail.gmail.com>
	<31f2a7bd0904081533u6f74ac1ao28ae2bf6bd5c6bba@mail.gmail.com>
	<fdeb32eb0904090350w354e789fv382d7582989ed631@mail.gmail.com>
Message-ID: <fdeb32eb0904091056t4500e123oa34e1273693c6ea5@mail.gmail.com>

Just one more question:

I noticed the Thread.yield() calls in your samples. I thought this
call had any real effect only back when green threads were used. Can
it still make any difference?

Thanks
Peter

2009/4/9 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
> Joe,
>
> Thank you so much for all your suggestions. I found them very helpful indeed.
>
> Peter
>
> On Thu, Apr 9, 2009 at 12:33 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>> And yet another correction, this time in the sample construction:
>>
>> ? Worker<Integer> worker = new Worker<Integer>(queue) {
>> ????? protected Integer compute() {
>> ????????? while (!isCancelled()) {
>> ????????????? // ...
>> ????????? }
>> ????????? return 0;
>> ????? }
>> ? };
>>
>> Should be while(!isCancelled()) ...
>>
>> Joe
>>
>> On Wed, Apr 8, 2009 at 3:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>>>
>>> Correction: The internal task should add the outer task (Worker) to the
>>> completion queue, as follows, so that the tasks you take from the queue are
>>> the same ones you constructed:
>>>
>>> ??? private final FutureTask<V> task =
>>> ??????? new FutureTask<V>(new Callable<V>() {
>>> ??????????? public V call() throws Exception {
>>> ??????????????? return compute();
>>> ??????????? }
>>> ??????? }) {
>>> ??????????? @Override protected void done() {
>>> ??????????????? completionQueue.add(Worker.this);
>>> ??????????? }
>>> ??????? };
>>>
>>> Joe
>>>
>>> On Wed, Apr 8, 2009 at 3:05 PM, Joe Bowbeer wrote:
>>>>
>>>> Peter,
>>>>
>>>> ExecutorCompletionService is dead simple.
>>>>
>>>> It's based on a trivial FutureTask extension that hooks the done() method
>>>> in order to enqueue the original task on a completion queue:
>>>>
>>>> ? private class QueueingFuture extends FutureTask<Void> {
>>>> ????? QueueingFuture(RunnableFuture<V> task) {
>>>> ????????? super(task, null);
>>>> ????????? this.task = task;
>>>> ????? }
>>>> ????? protected void done() { completionQueue.add(task); }
>>>> ????? private final Future<V> task;
>>>> ? }
>>>>
>>>> This functionality could easily be added to MyTask.? (See one approach
>>>> below.)? Then you can eliminate the completion service and "take" directly
>>>> from the completion queue.
>>>>
>>>> A few more things to point out:
>>>>
>>>> If you construct the completion service with an AbstractExecutorService,
>>>> the completion service asks your executor service to construct the task.
>>>> (See newTaskFor method).? This task, created by your executor service, is
>>>> then returned to you by the completion service, and this is the same task
>>>> that is enqueued.? You might explore this avenue for task customization.
>>>>
>>>> A custom "completion task" designed for subclassing follows.? Your
>>>> subclass overrides the "compute" method.? Note that your compute method
>>>> implementation can query isCancelled.
>>>>
>>>> public abstract class Worker<V> implements RunnableFuture<V> {
>>>>
>>>> ??? protected Worker(BlockingQueue<Future<V>> completionQueue) {
>>>> ??????? this.completionQueue = completionQueue;
>>>> ??? }
>>>>
>>>> ??? private final BlockingQueue<Future<V>> completionQueue;
>>>>
>>>> ??? /**
>>>> ???? * Calls the <tt>compute</tt> method to compute the result.
>>>> ???? */
>>>> ??? private final FutureTask<V> task =
>>>> ??????? new FutureTask<V>(new Callable<V>() {
>>>> ??????????? public V call() throws Exception {
>>>> ??????????????? return compute();
>>>> ??????????? }
>>>> ??????? }) {
>>>> ??????????? @Override protected void done() {
>>>> ??????????????? completionQueue.add(Worker.this);
>>>> ??????????? }
>>>> ??????? };
>>>>
>>>> ??? /**
>>>> ???? * Computes the value to be returned by the <tt>get</tt> method.
>>>> ???? */
>>>> ??? protected abstract V compute() throws Exception;
>>>>
>>>> ??? /* RunnableFuture implementation. */
>>>> ??? public void run() {
>>>> ??????? task.run();
>>>> ??? }
>>>>
>>>> ??? public boolean cancel(boolean mayInterruptIfRunning) {
>>>> ??????? return task.cancel(mayInterruptIfRunning);
>>>> ??? }
>>>>
>>>> ??? public boolean isCancelled() {
>>>> ??????? return task.isCancelled();
>>>> ??? }
>>>>
>>>> ??? public boolean isDone() {
>>>> ??????? return task.isDone();
>>>> ??? }
>>>>
>>>> ??? public V get() throws InterruptedException, ExecutionException {
>>>> ??????? return task.get();
>>>> ??? }
>>>>
>>>> ??? public V get(long timeout, TimeUnit unit)
>>>> ??????? throws InterruptedException, ExecutionException, TimeoutException
>>>> {
>>>> ??????? return task.get(timeout, unit);
>>>> ??? }
>>>> }
>>>>
>>>> Sample construction:
>>>>
>>>> ? BlockingQueue<Future<Integer>> queue =
>>>> ????????? new LinkedBlockingQueue<Future<Integer>>();
>>>>
>>>> ? Worker<Integer> worker = new Worker<Integer>(queue) {
>>>> ????? protected Integer compute() {
>>>> ????????? while (!isCancelled()) {
>>>> ????????????? Thread.yield(); // Please!!
>>>> ????????? }
>>>> ????????? return 0;
>>>> ????? }
>>>> ? };
>>>>
>>>> Joe
>>>>
>>>> 2009/4/8 P?ter Kov?cs
>>>>>
>>>>> One application of this approach where I cannot find an elegant
>>>>> solution is when I want to use an ExecutorCompletionService for
>>>>> "joining" a definite number of "cancelable" concurrent tasks:
>>>>>
>>>>> With a hypothetical collaboration of future-aware Callables, Futures
>>>>> and Executors built into the API, I could (1) store the futures
>>>>> returned by ExecutorCompletionService.submit in a list, (2) remove
>>>>> from the list each future returned by ExecutorCompletionService.take,
>>>>> (3) get the result off of the same future and (4) check if the list is
>>>>> empty to see if more futures need to be taken from the ECS. Something
>>>>> like:
>>>>>
>>>>> while (myFutures.size() > 0) {
>>>>> ? ?Future<String> f = executorCompletionService.take();
>>>>> ? ?myFutures.remove(f);
>>>>> ? ?f.get();
>>>>> }
>>>>>
>>>>> Using the "MyTask approach" with ECS, I have to do something like this:
>>>>>
>>>>> while (nrCompletedTasks < myTasks.size()) {
>>>>> ? ?Future<String> f = executorCompletionService.take();
>>>>> ? ?nrCompletedTasks++;
>>>>> ? ?f.get();
>>>>> }
>>>>>
>>>>> Here, I have the extra job of keeping track of the number of finished
>>>>> tasks -- in addition to storing references to the tasks (MyTasks) for
>>>>> potential canceling. Also -- as it is unknown which MyTask instance
>>>>> corresponds to a given future returned by ECS.take() --, I end up
>>>>> using MyTask only for cancellation and the ECS-generated futures for
>>>>> getting the results. As Future is basically meant to be a "handle" to
>>>>> an asynchronous task, having to unrelated "handle" to the same task
>>>>> with split functionality is (conceptually) confusing.
>>>>>
>>>>> Comments appreciated.
>>>>>
>>>>> Thanks
>>>>> Peter
>>>>
>>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


From joe.bowbeer at gmail.com  Thu Apr  9 14:20:33 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 9 Apr 2009 11:20:33 -0700
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <fdeb32eb0904091056t4500e123oa34e1273693c6ea5@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
	<fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
	<fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
	<31f2a7bd0904081505w30198e05j630dce64ec8c6979@mail.gmail.com>
	<31f2a7bd0904081517i5bbb6b88p4d182c01343960c9@mail.gmail.com>
	<31f2a7bd0904081533u6f74ac1ao28ae2bf6bd5c6bba@mail.gmail.com>
	<fdeb32eb0904090350w354e789fv382d7582989ed631@mail.gmail.com>
	<fdeb32eb0904091056t4500e123oa34e1273693c6ea5@mail.gmail.com>
Message-ID: <31f2a7bd0904091120t78234a96k80674810e29e1ca0@mail.gmail.com>

I was wondering if Thread.yield would raise a flag.  (If this were a real
code sample, it should raise a flag.)

I added Thread.yield, half as a joke, to indicate that my example was
spinning.

Thread.yield is a suggestion to the JVM that maybe there is better use of
the CPU elsewhere.  In this instance, I hope the JVM takes the hint, but I
don't care.  (Well, I would care if I ran my code on a non-preemptive
single-CPU system.)

I don't know which platforms ignore Thread.yield and which don't.

Joe

2009/4/9 P?ter Kov?cs

> Just one more question:
>
> I noticed the Thread.yield() calls in your samples. I thought this
> call had any real effect only back when green threads were used. Can
> it still make any difference?
>
> Thanks
> Peter
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090409/a01f9343/attachment.html>

From gregg at cytetech.com  Thu Apr  9 15:12:28 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 09 Apr 2009 14:12:28 -0500
Subject: [concurrency-interest] Canceling Futures - Callable
	implementations
In-Reply-To: <31f2a7bd0904091120t78234a96k80674810e29e1ca0@mail.gmail.com>
References: <fdeb32eb0904070004v522e25adh61fac2c38fcb5143@mail.gmail.com>
	<fdeb32eb0904071345y6ef74551yfcec41425b02cc21@mail.gmail.com>
	<31f2a7bd0904071434h19425828vf2388638e641542d@mail.gmail.com>
	<fdeb32eb0904071523m41a7bf80r817cfbd752f6fceb@mail.gmail.com>
	<fdeb32eb0904081010w15d3d1c8t28a8c205deaf6c42@mail.gmail.com>
	<31f2a7bd0904081505w30198e05j630dce64ec8c6979@mail.gmail.com>
	<31f2a7bd0904081517i5bbb6b88p4d182c01343960c9@mail.gmail.com>
	<31f2a7bd0904081533u6f74ac1ao28ae2bf6bd5c6bba@mail.gmail.com>
	<fdeb32eb0904090350w354e789fv382d7582989ed631@mail.gmail.com>
	<fdeb32eb0904091056t4500e123oa34e1273693c6ea5@mail.gmail.com>
	<31f2a7bd0904091120t78234a96k80674810e29e1ca0@mail.gmail.com>
Message-ID: <49DE489C.5040608@cytetech.com>

Joe Bowbeer wrote:
> I was wondering if Thread.yield would raise a flag.  (If this were a 
> real code sample, it should raise a flag.)

I use Thread.yield as a concurrency test mechanism.  I've found that if  you are 
tightly spinning the CPUs on multiple threads which have concurrent access 
behaviors that occasionally I can find some unintended contention and even 
sometimes some concurrency bugs by yielding in places which let dispatched 
threads of execution start sooner rather than later, at quantum exhaustion etc.

Gregg Wonderly

From peter.kovacs.1.0rc at gmail.com  Fri Apr 10 02:20:57 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Fri, 10 Apr 2009 08:20:57 +0200
Subject: [concurrency-interest] Incompatible API change between Java 5 and
	Java 6?
Message-ID: <fdeb32eb0904092320t6a5df9e5y86d6a48b21984ad9@mail.gmail.com>

Hi,

It appears that the signature of invokeAny and invokeAll in
ExecutorService has changed from Java 5 and Java 6. I want to
implement this interface, but I cannot find a signature which is
accepted by both versions of javac. I have tried this one (accepted by
Java 5 only):

public <T> List<Future<T>> invokeAll(Collection<Callable<T>> tasks)
            throws InterruptedException

and this one  (accepted by Java 6 only):

public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)
            throws InterruptedException

Any suggestion appreciated.

Thanks
Peter

PS:
Generics: much ado about nothing.

From forax at univ-mlv.fr  Fri Apr 10 02:56:57 2009
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Fri, 10 Apr 2009 08:56:57 +0200
Subject: [concurrency-interest] Incompatible API change between Java 5
 and	Java 6?
In-Reply-To: <fdeb32eb0904092320t6a5df9e5y86d6a48b21984ad9@mail.gmail.com>
References: <fdeb32eb0904092320t6a5df9e5y86d6a48b21984ad9@mail.gmail.com>
Message-ID: <49DEEDB9.5040803@univ-mlv.fr>

P?ter Kov?cs a ?crit :
> Hi,
>
> It appears that the signature of invokeAny and invokeAll in
> ExecutorService has changed from Java 5 and Java 6. I want to
> implement this interface, but I cannot find a signature which is
> accepted by both versions of javac. I have tried this one (accepted by
> Java 5 only):
>
> public <T> List<Future<T>> invokeAll(Collection<Callable<T>> tasks)
>             throws InterruptedException
>
> and this one  (accepted by Java 6 only):
>
> public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)
>             throws InterruptedException
>
> Any suggestion appreciated.
>
> Thanks
> Peter
>   
You can use a raw type, you will get warnings but no error.

public <T> List<Future<T>> invokeAll(Collection tasks)
            throws InterruptedException


> PS:
> Generics: much ado about nothing.
>   
R?mi

From peter.kovacs.1.0rc at gmail.com  Fri Apr 10 07:47:55 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Fri, 10 Apr 2009 13:47:55 +0200
Subject: [concurrency-interest] Incompatible API change between Java 5
	and Java 6?
In-Reply-To: <49DEEDB9.5040803@univ-mlv.fr>
References: <fdeb32eb0904092320t6a5df9e5y86d6a48b21984ad9@mail.gmail.com>
	<49DEEDB9.5040803@univ-mlv.fr>
Message-ID: <fdeb32eb0904100447x3458a79by6a042660fcfe8f5e@mail.gmail.com>

You presumably meant:

public List invokeAll(Collection tasks)
          throws InterruptedException

Thanks a lot, that appears to work.
Peter

2009/4/10 R?mi Forax <forax at univ-mlv.fr>:
> P?ter Kov?cs a ?crit :
>>
>> Hi,
>>
>> It appears that the signature of invokeAny and invokeAll in
>> ExecutorService has changed from Java 5 and Java 6. I want to
>> implement this interface, but I cannot find a signature which is
>> accepted by both versions of javac. I have tried this one (accepted by
>> Java 5 only):
>>
>> public <T> List<Future<T>> invokeAll(Collection<Callable<T>> tasks)
>> ? ? ? ? ? ?throws InterruptedException
>>
>> and this one ?(accepted by Java 6 only):
>>
>> public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>>
>> tasks)
>> ? ? ? ? ? ?throws InterruptedException
>>
>> Any suggestion appreciated.
>>
>> Thanks
>> Peter
>>
>
> You can use a raw type, you will get warnings but no error.
>
> public <T> List<Future<T>> invokeAll(Collection tasks)
> ? ? ? ? ? throws InterruptedException
>
>
>> PS:
>> Generics: much ado about nothing.
>>
>
> R?mi
>


From Online at Stolsvik.com  Tue Apr 14 18:06:11 2009
From: Online at Stolsvik.com (=?UTF-8?Q?Endre_St=C3=B8lsvik?=)
Date: Wed, 15 Apr 2009 00:06:11 +0200
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <90622e530904061200p2d06123pe04d16dc1bc0d4c3@mail.gmail.com>
References: <635c64210904060809s1b952ee9m4899c9742fb27dd2@mail.gmail.com> 
	<90622e530904060846p30c5806cte7db1e0dbd4c1b7e@mail.gmail.com> 
	<635c64210904060855m1db6bab8wa2f5661eee810b90@mail.gmail.com> 
	<90622e530904061200p2d06123pe04d16dc1bc0d4c3@mail.gmail.com>
Message-ID: <1501fdf40904141506y51facf81ta9e26724d4a66a5c@mail.gmail.com>

On Mon, Apr 6, 2009 at 21:00, Christian Vest Hansen
<karmazilla at gmail.com> wrote:
> Have a lock for each entity; L(A), L(B), L(C) etc. such that each
> entity at any given point in time has exactly one lock. These locks
> have a universal and immutable ordering to them (like
> System.identityHashCode, for instance).

System.identityHashCode can give you the same result for two different
objects, and you'd not be guaranteed a single order on the two
different threads. Use a long sequencer (in addition!).

Endre.

From gergg at cox.net  Wed Apr 15 12:23:38 2009
From: gergg at cox.net (Gregg Wonderly)
Date: Wed, 15 Apr 2009 11:23:38 -0500
Subject: [concurrency-interest] Multi consumer queue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEKOIAAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOEKOIAAA.davidcholmes@aapt.net.au>
Message-ID: <49E60A0A.2050707@cox.net>

David Holmes wrote:
> For a particular application scenario a more efficient implementation is
> nearly always possible. But in general, allocation is dirt cheap (99% of the
> time from the thread-local allocation buffer) and short-lived objects are
> easily reclaimed by the GC from the young generation. And you only get
> millions of allocations if you get millions of contentions on the lock -
> which is likely to be more of a problem with regards to performance and
> latency than the allocation itself.

Just as a casual reminder, there are small devices such as the AJile processor 
and the Sunspot environment, which could really make good use of efficient, 
non-allocation based locking schemes in the J2ME profile.  As devices get faster 
and can use more memory, we start to lose sight of the basic issue of latency, 
and realtime (soft and hard) requirements that many applications can have to 
benefit the user experience.

I really do thing that from a performance perspective, the time is getting right 
for a refocus on a totally real-time runtime environment for Java that allows 
the developer to guarantee a lot more about the characteristics of the 
applications performance relative to a particular class of hardware.

And we have come such a long way...  I remember the example HTTP session at 
JavaOne about 3 years ago where they showed how just putting forth a few simple 
real-time constraints for response time, greatly smoothed the users experience 
of response-time to requests.

Gregg Wonderly

From mr.tom.strickland at gmail.com  Thu Apr 16 08:42:36 2009
From: mr.tom.strickland at gmail.com (tom strickland)
Date: Thu, 16 Apr 2009 13:42:36 +0100
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <1501fdf40904141506y51facf81ta9e26724d4a66a5c@mail.gmail.com>
References: <635c64210904060809s1b952ee9m4899c9742fb27dd2@mail.gmail.com>
	<90622e530904060846p30c5806cte7db1e0dbd4c1b7e@mail.gmail.com>
	<635c64210904060855m1db6bab8wa2f5661eee810b90@mail.gmail.com>
	<90622e530904061200p2d06123pe04d16dc1bc0d4c3@mail.gmail.com>
	<1501fdf40904141506y51facf81ta9e26724d4a66a5c@mail.gmail.com>
Message-ID: <635c64210904160542t57be55a2v281acd15de6a058c@mail.gmail.com>

Thanks Endre, Christian for your replies and I'm sorry that I've been late
in replying.

Perhaps I should have been more explicit in my original scenario. It may be
that I'm too close to the problem to see an obvious solution, but I don't
think that the proposed solution will work.

Here's a more detailed description of the problem:
This is a communications system in which 2 parties can be in a communication
session with each other - think of it as an IM session or a phone call.

Alice and Bob are in a call.
Alice and Carol are also in a call.
Alice's calls are being coordinated by a service. One function offered by
this service is "transfer", whereby the service hooks Bob and Carol together
and drops Alice out of the loop.

These calls have moderately complex state machines that are guarded by
locks. Triggers can come into these state machines from either end of a call
and must acquire the lock to protect memory state and to prevent races on
the state machines.
Each user has a state machine for their side of the call and each call has
its own higher-level state machine. To simplify modelling, the two users and
call operate under a single lock, so that a trigger into one user's state
machine flows across the call and state machines atomically, without having
to worry about races from other triggers.

My problem is this:
Alice and Bob have a call that is protected by a single lock L(B)
Alice and Carol have a call that is protected by a single lock L(C)
We need to end up with Bob and Carol in a call without running into a
deadlock between the two locks.

If I switch both calls to having to acquire both locks, in order, perhaps
there is a race.
Let's say that the transfer trigger comes in and acquires L(B). The system
realises that this is a transfer and acquires L(C) so that it has control of
both calls locks and thus can be sure that nothing else will race over the
state machines. After this, all triggers to the call Alice-Bob will have to
acquire locks L(B) and L(C) in that order. Similarly, all triggers to
Alice-Carol will have to acquire both locks, in the same order. The problem
is that there is a race in which a thread can start to wait for L(C) while
this changeover is happening and be left waiting for L(C) without knowing
that it should have acquired L(B) first.
Sequence for two threads, labelled tB and tC:
tB   transfer trigger comes in
tB   acquire L(B)
tB   acquire L(C)
tC   trigger arrives, starts waiting for L(C)
tB   sets Alice-Carol call to need to acquire both L(B) and L(C) from now on
tB   does stuff to set off the transfer and then releases L(C) and L(B)
tC   acquires L(C), but has not acquired L(B)


Again, I have some thoughts on how to solve this, but I'm worried that I'm
missing the point or overcomplicating matters in the statement of my
problem. Am I missing something obvious?

Thanks,

Tom


2009/4/14 Endre St?lsvik <Online at stolsvik.com>

> On Mon, Apr 6, 2009 at 21:00, Christian Vest Hansen
> <karmazilla at gmail.com> wrote:
> > Have a lock for each entity; L(A), L(B), L(C) etc. such that each
> > entity at any given point in time has exactly one lock. These locks
> > have a universal and immutable ordering to them (like
> > System.identityHashCode, for instance).
>
> System.identityHashCode can give you the same result for two different
> objects, and you'd not be guaranteed a single order on the two
> different threads. Use a long sequencer (in addition!).
>
> Endre.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090416/aa7869b8/attachment.html>

From davidcholmes at aapt.net.au  Thu Apr 16 18:57:01 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 17 Apr 2009 08:57:01 +1000
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <635c64210904160542t57be55a2v281acd15de6a058c@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEPEIAAA.davidcholmes@aapt.net.au>

Hi Tom,

This kind of scenario does arise whenever you have entities that most of the
time have to handle their exclusion constraints (locking) independently, but
then at times have to co-operate to provide atomic operations across sets of
objects. Acquiring multiple locks is the main way of extending the
independent use of locks - with ordering to avoid deadlocks.

What you describe is a little different in that an operation that initially
needs only a single lock, may turn into an operation that requires multiple
locks. In that case all I can suggest is that after acquiring l(c) the code
checks if that was sufficient, and if not it grabs l(b) - perhaps releasing
l(c) first to maintain ordering.

It really all depends on interaction between the different actions as to how
best to partition the locking and what strategies to try and use. I don't
think you are missing anything obvious.

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
strickland
  Sent: Thursday, 16 April 2009 10:43 PM
  To: Endre St?lsvik
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] transferring queued threads


  Thanks Endre, Christian for your replies and I'm sorry that I've been late
in replying.

  Perhaps I should have been more explicit in my original scenario. It may
be that I'm too close to the problem to see an obvious solution, but I don't
think that the proposed solution will work.

  Here's a more detailed description of the problem:
  This is a communications system in which 2 parties can be in a
communication session with each other - think of it as an IM session or a
phone call.

  Alice and Bob are in a call.
  Alice and Carol are also in a call.
  Alice's calls are being coordinated by a service. One function offered by
this service is "transfer", whereby the service hooks Bob and Carol together
and drops Alice out of the loop.

  These calls have moderately complex state machines that are guarded by
locks. Triggers can come into these state machines from either end of a call
and must acquire the lock to protect memory state and to prevent races on
the state machines.
  Each user has a state machine for their side of the call and each call has
its own higher-level state machine. To simplify modelling, the two users and
call operate under a single lock, so that a trigger into one user's state
machine flows across the call and state machines atomically, without having
to worry about races from other triggers.

  My problem is this:
  Alice and Bob have a call that is protected by a single lock L(B)
  Alice and Carol have a call that is protected by a single lock L(C)
  We need to end up with Bob and Carol in a call without running into a
deadlock between the two locks.

  If I switch both calls to having to acquire both locks, in order, perhaps
there is a race.
  Let's say that the transfer trigger comes in and acquires L(B). The system
realises that this is a transfer and acquires L(C) so that it has control of
both calls locks and thus can be sure that nothing else will race over the
state machines. After this, all triggers to the call Alice-Bob will have to
acquire locks L(B) and L(C) in that order. Similarly, all triggers to
Alice-Carol will have to acquire both locks, in the same order. The problem
is that there is a race in which a thread can start to wait for L(C) while
this changeover is happening and be left waiting for L(C) without knowing
that it should have acquired L(B) first.
  Sequence for two threads, labelled tB and tC:
  tB   transfer trigger comes in
  tB   acquire L(B)
  tB   acquire L(C)
  tC   trigger arrives, starts waiting for L(C)
  tB   sets Alice-Carol call to need to acquire both L(B) and L(C) from now
on
  tB   does stuff to set off the transfer and then releases L(C) and L(B)
  tC   acquires L(C), but has not acquired L(B)


  Again, I have some thoughts on how to solve this, but I'm worried that I'm
missing the point or overcomplicating matters in the statement of my
problem. Am I missing something obvious?

  Thanks,

  Tom



  2009/4/14 Endre St?lsvik <Online at stolsvik.com>

    On Mon, Apr 6, 2009 at 21:00, Christian Vest Hansen
    <karmazilla at gmail.com> wrote:
    > Have a lock for each entity; L(A), L(B), L(C) etc. such that each
    > entity at any given point in time has exactly one lock. These locks
    > have a universal and immutable ordering to them (like
    > System.identityHashCode, for instance).


    System.identityHashCode can give you the same result for two different
    objects, and you'd not be guaranteed a single order on the two
    different threads. Use a long sequencer (in addition!).

    Endre.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090417/bcebc93e/attachment.html>

From jim.andreou at gmail.com  Fri Apr 17 10:44:46 2009
From: jim.andreou at gmail.com (Jim Andreou)
Date: Fri, 17 Apr 2009 15:44:46 +0100
Subject: [concurrency-interest] Machine reflection api?
Message-ID: <7d7138c10904170744t79319c0u75400a1d515deec@mail.gmail.com>

Shouldn't there be an api that offered information such as cache line width,
cache sizes, and various other architectural traits? We only have is the
processor count up to now AFAIK. Is there any effort towards such a
direction? It would be very useful for developing cache-conscious software
in java. (I've seen people developing such software in C++ so all this
information must be available from somewhere).
Thanks,
Dimitris Andreou
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090417/0c2458ea/attachment.html>

From mr.tom.strickland at gmail.com  Fri Apr 17 12:11:04 2009
From: mr.tom.strickland at gmail.com (tom strickland)
Date: Fri, 17 Apr 2009 17:11:04 +0100
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEPEIAAA.davidcholmes@aapt.net.au>
References: <635c64210904160542t57be55a2v281acd15de6a058c@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCKEPEIAAA.davidcholmes@aapt.net.au>
Message-ID: <635c64210904170911u6811ffcboff0ceb5d378b4476@mail.gmail.com>

Wonderful! That's very helpful.

Thanks to everyone for the feedback.

Tom

2009/4/16 David Holmes <davidcholmes at aapt.net.au>

>  Hi Tom,
>
> This kind of scenario does arise whenever you have entities that most of
> the time have to handle their exclusion constraints (locking) independently,
> but then at times have to co-operate to provide atomic operations across
> sets of objects. Acquiring multiple locks is the main way of extending the
> independent use of locks - with ordering to avoid deadlocks.
>
> What you describe is a little different in that an operation that initially
> needs only a single lock, may turn into an operation that requires multiple
> locks. In that case all I can suggest is that after acquiring l(c) the code
> checks if that was sufficient, and if not it grabs l(b) - perhaps releasing
> l(c) first to maintain ordering.
>
> It really all depends on interaction between the different actions as to
> how best to partition the locking and what strategies to try and use. I
> don't think you are missing anything obvious.
>
> Cheers,
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *tom strickland
> *Sent:* Thursday, 16 April 2009 10:43 PM
> *To:* Endre St?lsvik
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] transferring queued threads
>
> Thanks Endre, Christian for your replies and I'm sorry that I've been late
> in replying.
>
> Perhaps I should have been more explicit in my original scenario. It may be
> that I'm too close to the problem to see an obvious solution, but I don't
> think that the proposed solution will work.
>
> Here's a more detailed description of the problem:
> This is a communications system in which 2 parties can be in a
> communication session with each other - think of it as an IM session or a
> phone call.
>
> Alice and Bob are in a call.
> Alice and Carol are also in a call.
> Alice's calls are being coordinated by a service. One function offered by
> this service is "transfer", whereby the service hooks Bob and Carol together
> and drops Alice out of the loop.
>
> These calls have moderately complex state machines that are guarded by
> locks. Triggers can come into these state machines from either end of a call
> and must acquire the lock to protect memory state and to prevent races on
> the state machines.
> Each user has a state machine for their side of the call and each call has
> its own higher-level state machine. To simplify modelling, the two users and
> call operate under a single lock, so that a trigger into one user's state
> machine flows across the call and state machines atomically, without having
> to worry about races from other triggers.
>
> My problem is this:
> Alice and Bob have a call that is protected by a single lock L(B)
> Alice and Carol have a call that is protected by a single lock L(C)
> We need to end up with Bob and Carol in a call without running into a
> deadlock between the two locks.
>
> If I switch both calls to having to acquire both locks, in order, perhaps
> there is a race.
> Let's say that the transfer trigger comes in and acquires L(B). The system
> realises that this is a transfer and acquires L(C) so that it has control of
> both calls locks and thus can be sure that nothing else will race over the
> state machines. After this, all triggers to the call Alice-Bob will have to
> acquire locks L(B) and L(C) in that order. Similarly, all triggers to
> Alice-Carol will have to acquire both locks, in the same order. The problem
> is that there is a race in which a thread can start to wait for L(C) while
> this changeover is happening and be left waiting for L(C) without knowing
> that it should have acquired L(B) first.
> Sequence for two threads, labelled tB and tC:
> tB   transfer trigger comes in
> tB   acquire L(B)
> tB   acquire L(C)
> tC   trigger arrives, starts waiting for L(C)
> tB   sets Alice-Carol call to need to acquire both L(B) and L(C) from now
> on
> tB   does stuff to set off the transfer and then releases L(C) and L(B)
> tC   acquires L(C), but has not acquired L(B)
>
>
> Again, I have some thoughts on how to solve this, but I'm worried that I'm
> missing the point or overcomplicating matters in the statement of my
> problem. Am I missing something obvious?
>
> Thanks,
>
> Tom
>
>
> 2009/4/14 Endre St?lsvik <Online at stolsvik.com>
>
>> On Mon, Apr 6, 2009 at 21:00, Christian Vest Hansen
>> <karmazilla at gmail.com> wrote:
>> > Have a lock for each entity; L(A), L(B), L(C) etc. such that each
>> > entity at any given point in time has exactly one lock. These locks
>> > have a universal and immutable ordering to them (like
>> > System.identityHashCode, for instance).
>>
>> System.identityHashCode can give you the same result for two different
>> objects, and you'd not be guaranteed a single order on the two
>> different threads. Use a long sequencer (in addition!).
>>
>> Endre.
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090417/542e2af1/attachment.html>

From jim.andreou at gmail.com  Fri Apr 17 13:05:29 2009
From: jim.andreou at gmail.com (Jim Andreou)
Date: Fri, 17 Apr 2009 18:05:29 +0100
Subject: [concurrency-interest] Machine reflection api?
In-Reply-To: <94C476C03BFF5E42AC3518FDAC9643C4D046B45CF2@HQMAIL.rocketsoftware.com>
References: <7d7138c10904170744t79319c0u75400a1d515deec@mail.gmail.com>
	<94C476C03BFF5E42AC3518FDAC9643C4D046B45CF2@HQMAIL.rocketsoftware.com>
Message-ID: <7d7138c10904171005s28ae0c75m6fb48d2be03c3f0e@mail.gmail.com>

Thanks for the pointer, Gary. Just tried out the cache latency computation
tool, and looks very interesting! Nice. (Although it still leaves much to be
desired, especially for java or non-windows developers.)
Dimitris
2009/4/17 Gary Gregory <GGregory at seagullsoftware.com>

>  One way you can get CPU information now is to call CPU-Z [1] and output
> the information to a file, which you can then parse.
>
>
>
> Gary
>
> [1] http://www.cpuid.com/cpuz.php
>
>
>   ------------------------------
>
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *Jim Andreou
> *Sent:* Friday, April 17, 2009 7:45 AM
> *To:* concurrency-interest
> *Subject:* [concurrency-interest] Machine reflection api?
>
>
>
> Shouldn't there be an api that offered information such as cache line
> width, cache sizes, and various other architectural traits? We only have is
> the processor count up to now AFAIK. Is there any effort towards such a
> direction? It would be very useful for developing cache-conscious software
> in java. (I've seen people developing such software in C++ so all this
> information must be available from somewhere).
>
>
>
> Thanks,
>
> Dimitris Andreou
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090417/663fc34a/attachment.html>

From barnett.thomas at gmail.com  Fri Apr 17 14:03:26 2009
From: barnett.thomas at gmail.com (Thomas Barnett)
Date: Fri, 17 Apr 2009 19:03:26 +0100
Subject: [concurrency-interest] Shutting down ExecutorService
Message-ID: <c5cb1c590904171103h23f27f34x77882a567969b04f@mail.gmail.com>

I am trying to wrap my head around concurrency package. There is an
example of use of ExecutorService in LifecycleWebServer listing in the
book JCIP by Goetz et al [1]. It goes:

...
while(!exec.isShutDown()) {
  try {
    final Socket conn = socket.accept();
...

and has a stop() method that shuts down executor service. The problem
is, unless there is a client connection, socket.accept blocks and the
condition is never checked so LifecycleWebServer never terminates even
after calling stop().  Trying to use interruption for cancellation as
in listing 7.5 of the same book does not work either when the blocking
method is not interruptible. I just want to make sure that I am not
misunderstanding something here.

A workaround I could think of was to set timeout on server socket and
consume the SocketTimeoutException which seem to work but would that
be a correct way to do this?

Regards
Thomas

[1] http://jcip.net/

From amalyuk at gmail.com  Fri Apr 17 15:26:18 2009
From: amalyuk at gmail.com (Alexander Malyuk)
Date: Fri, 17 Apr 2009 14:26:18 -0500
Subject: [concurrency-interest] Shutting down ExecutorService
In-Reply-To: <c5cb1c590904171103h23f27f34x77882a567969b04f@mail.gmail.com>
References: <c5cb1c590904171103h23f27f34x77882a567969b04f@mail.gmail.com>
Message-ID: <2618f4090904171226y7895f273m182dc8833f43b942@mail.gmail.com>

http://gee.cs.oswego.edu/dl/cpj/cancel.html
On Fri, Apr 17, 2009 at 1:03 PM, Thomas Barnett <barnett.thomas at gmail.com>wrote:

> I am trying to wrap my head around concurrency package. There is an
> example of use of ExecutorService in LifecycleWebServer listing in the
> book JCIP by Goetz et al [1]. It goes:
>
> ...
> while(!exec.isShutDown()) {
>  try {
>    final Socket conn = socket.accept();
> ...
>
> and has a stop() method that shuts down executor service. The problem
> is, unless there is a client connection, socket.accept blocks and the
> condition is never checked so LifecycleWebServer never terminates even
> after calling stop().  Trying to use interruption for cancellation as
> in listing 7.5 of the same book does not work either when the blocking
> method is not interruptible. I just want to make sure that I am not
> misunderstanding something here.
>
> A workaround I could think of was to set timeout on server socket and
> consume the SocketTimeoutException which seem to work but would that
> be a correct way to do this?
>
> Regards
> Thomas
>
> [1] http://jcip.net/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090417/ed3f60b2/attachment-0001.html>

From tlee at redhat.com  Fri Apr 17 22:38:22 2009
From: tlee at redhat.com (Trustin Lee)
Date: Sat, 18 Apr 2009 11:38:22 +0900
Subject: [concurrency-interest] Shutting down ExecutorService
In-Reply-To: <c5cb1c590904171103h23f27f34x77882a567969b04f@mail.gmail.com>
References: <c5cb1c590904171103h23f27f34x77882a567969b04f@mail.gmail.com>
Message-ID: <aa4319810904171938s6a0aed21v4355b52eeac3abd7@mail.gmail.com>

Closing the ServerSocket from a different thread will cause
ServerSocket.accept() to throw an exception immediately.  Confirmed on
Sun JDK 1.5/1.6, OpenJDK, JRockit 1.5/1.6, and IBM JDK 1.5/1.6.

HTH,

? Trustin Lee, http://gleamynode.net/

On Sat, Apr 18, 2009 at 3:03 AM, Thomas Barnett
<barnett.thomas at gmail.com> wrote:
> I am trying to wrap my head around concurrency package. There is an
> example of use of ExecutorService in LifecycleWebServer listing in the
> book JCIP by Goetz et al [1]. It goes:
>
> ...
> while(!exec.isShutDown()) {
> ?try {
> ? ?final Socket conn = socket.accept();
> ...
>
> and has a stop() method that shuts down executor service. The problem
> is, unless there is a client connection, socket.accept blocks and the
> condition is never checked so LifecycleWebServer never terminates even
> after calling stop(). ?Trying to use interruption for cancellation as
> in listing 7.5 of the same book does not work either when the blocking
> method is not interruptible. I just want to make sure that I am not
> misunderstanding something here.
>
> A workaround I could think of was to set timeout on server socket and
> consume the SocketTimeoutException which seem to work but would that
> be a correct way to do this?
>
> Regards
> Thomas
>
> [1] http://jcip.net/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From davidcholmes at aapt.net.au  Sat Apr 18 00:24:06 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 18 Apr 2009 14:24:06 +1000
Subject: [concurrency-interest] Shutting down ExecutorService
In-Reply-To: <c5cb1c590904171103h23f27f34x77882a567969b04f@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEPKIAAA.davidcholmes@aapt.net.au>

Hi Thomas,

Thomas Barnett writes:
> ... The problem
> is, unless there is a client connection, socket.accept blocks and the
> condition is never checked so LifecycleWebServer never terminates even
> after calling stop().  Trying to use interruption for cancellation as
> in listing 7.5 of the same book does not work either when the blocking
> method is not interruptible. I just want to make sure that I am not
> misunderstanding something here.

The entire contents of Chapter 7 is about cancellation and shutdown, and
covers interruption and its limitations, and what you can do for some
non-interruptible methods ( see 7.1.6) ...

> A workaround I could think of was to set timeout on server socket and
> consume the SocketTimeoutException which seem to work but would that
> be a correct way to do this?

... but for some strange reason in the list in 7.1.6 we forgot to include
setting the socket timeout for things like socket.accept(). Don't know how
we overlooked that given it's been part of an example we used for years
prior to JCiP being written.

Yes setting the socket timeout is exactly how you would deal with this.

Cheers,
David Holmes



From mr.tom.strickland at gmail.com  Mon Apr 20 09:29:12 2009
From: mr.tom.strickland at gmail.com (tom strickland)
Date: Mon, 20 Apr 2009 14:29:12 +0100
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEPEIAAA.davidcholmes@aapt.net.au>
References: <635c64210904160542t57be55a2v281acd15de6a058c@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCKEPEIAAA.davidcholmes@aapt.net.au>
Message-ID: <635c64210904200629n62389076gcf81d57cbf327fb4@mail.gmail.com>

After more thought, I thought I'd ask if I could just shift threads from
lock-to-lock, as follows:

Each entity has its own data lock that does not change. It also has a second
"call" lock that can change. Thus, a call from Alice that turns into a call
between Alice and Bob will use a single call lock. All traffic between Alice
and Bob uses this lock to all serialise operations on the state machines for
Alice, Bob and their call. If Bob is to be shunted into a different call
with Carol, then a new lock will be created for that call. The problem is
that there may be races here. So, could the following approach work and what
would be the best starting point for implementing it?

1. Any thread that has acquired a call lock will be allowed to proceed to
completion. To change the call lock, a thread must first acquire it.
2. Any thread that is waiting to acquire the call lock (because it is
already owned by another thread) is essentially waiting for one of two
conditions:
   a. the lock has been acquired by the waiting thread
   b. the lock has changed and the waiting thread must go and wait on this
new changed lock

This seems feasible to me, but I am unsure of how to wait for two
conditions. Is this a situation where the AbstractQueuedSynchronizer would
prove useful? I don't think that I've missed anything subtle, but this
approach is using one lock to get/set the value of the current lock (a data
lock) and a separate lock, that may change, to coordinate actions and I
could easily have missed something.

Thanks,

Tom

2009/4/16 David Holmes <davidcholmes at aapt.net.au>

>  Hi Tom,
>
> This kind of scenario does arise whenever you have entities that most of
> the time have to handle their exclusion constraints (locking) independently,
> but then at times have to co-operate to provide atomic operations across
> sets of objects. Acquiring multiple locks is the main way of extending the
> independent use of locks - with ordering to avoid deadlocks.
>
> What you describe is a little different in that an operation that initially
> needs only a single lock, may turn into an operation that requires multiple
> locks. In that case all I can suggest is that after acquiring l(c) the code
> checks if that was sufficient, and if not it grabs l(b) - perhaps releasing
> l(c) first to maintain ordering.
>
> It really all depends on interaction between the different actions as to
> how best to partition the locking and what strategies to try and use. I
> don't think you are missing anything obvious.
>
> Cheers,
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *tom strickland
> *Sent:* Thursday, 16 April 2009 10:43 PM
> *To:* Endre St?lsvik
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] transferring queued threads
>
> Thanks Endre, Christian for your replies and I'm sorry that I've been late
> in replying.
>
> Perhaps I should have been more explicit in my original scenario. It may be
> that I'm too close to the problem to see an obvious solution, but I don't
> think that the proposed solution will work.
>
> Here's a more detailed description of the problem:
> This is a communications system in which 2 parties can be in a
> communication session with each other - think of it as an IM session or a
> phone call.
>
> Alice and Bob are in a call.
> Alice and Carol are also in a call.
> Alice's calls are being coordinated by a service. One function offered by
> this service is "transfer", whereby the service hooks Bob and Carol together
> and drops Alice out of the loop.
>
> These calls have moderately complex state machines that are guarded by
> locks. Triggers can come into these state machines from either end of a call
> and must acquire the lock to protect memory state and to prevent races on
> the state machines.
> Each user has a state machine for their side of the call and each call has
> its own higher-level state machine. To simplify modelling, the two users and
> call operate under a single lock, so that a trigger into one user's state
> machine flows across the call and state machines atomically, without having
> to worry about races from other triggers.
>
> My problem is this:
> Alice and Bob have a call that is protected by a single lock L(B)
> Alice and Carol have a call that is protected by a single lock L(C)
> We need to end up with Bob and Carol in a call without running into a
> deadlock between the two locks.
>
> If I switch both calls to having to acquire both locks, in order, perhaps
> there is a race.
> Let's say that the transfer trigger comes in and acquires L(B). The system
> realises that this is a transfer and acquires L(C) so that it has control of
> both calls locks and thus can be sure that nothing else will race over the
> state machines. After this, all triggers to the call Alice-Bob will have to
> acquire locks L(B) and L(C) in that order. Similarly, all triggers to
> Alice-Carol will have to acquire both locks, in the same order. The problem
> is that there is a race in which a thread can start to wait for L(C) while
> this changeover is happening and be left waiting for L(C) without knowing
> that it should have acquired L(B) first.
> Sequence for two threads, labelled tB and tC:
> tB   transfer trigger comes in
> tB   acquire L(B)
> tB   acquire L(C)
> tC   trigger arrives, starts waiting for L(C)
> tB   sets Alice-Carol call to need to acquire both L(B) and L(C) from now
> on
> tB   does stuff to set off the transfer and then releases L(C) and L(B)
> tC   acquires L(C), but has not acquired L(B)
>
>
> Again, I have some thoughts on how to solve this, but I'm worried that I'm
> missing the point or overcomplicating matters in the statement of my
> problem. Am I missing something obvious?
>
> Thanks,
>
> Tom
>
>
> 2009/4/14 Endre St?lsvik <Online at stolsvik.com>
>
>> On Mon, Apr 6, 2009 at 21:00, Christian Vest Hansen
>> <karmazilla at gmail.com> wrote:
>> > Have a lock for each entity; L(A), L(B), L(C) etc. such that each
>> > entity at any given point in time has exactly one lock. These locks
>> > have a universal and immutable ordering to them (like
>> > System.identityHashCode, for instance).
>>
>> System.identityHashCode can give you the same result for two different
>> objects, and you'd not be guaranteed a single order on the two
>> different threads. Use a long sequencer (in addition!).
>>
>> Endre.
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090420/df750c28/attachment.html>

From javamann at cox.net  Mon Apr 20 11:02:07 2009
From: javamann at cox.net (javamann at cox.net)
Date: Mon, 20 Apr 2009 11:02:07 -0400
Subject: [concurrency-interest] CyclicBarrier / Count Down Latch
Message-ID: <20090420110207.X7PUN.680646.imail@fed1rmwml33>

Howdy,
  I have an application that generates an unknown amount of threads at startup (via JMX MLet) and I was looking for a way to block all the threads at a certain point and then 'release' them to continue. I looked at the CyclicBarrier and CountDownLatch but they require you know the number of threads ahead of time. Currently I am just using a 'global' Condition for them to acquire and then Signal them all to continue. This seems like a hack and I am hoping there is a better way to accomplish this. Any Ideas?

Thanks

-Pete

--

1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?

2. Behind every great woman... Is a man checking out her ass

3. I am not a member of any organized political party. I am a Democrat.*

4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*

5. A process is what you need when all your good people have left.


*Will Rogers

From david.lloyd at redhat.com  Mon Apr 20 11:34:06 2009
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Mon, 20 Apr 2009 10:34:06 -0500
Subject: [concurrency-interest] CyclicBarrier / Count Down Latch
In-Reply-To: <20090420110207.X7PUN.680646.imail@fed1rmwml33>
References: <20090420110207.X7PUN.680646.imail@fed1rmwml33>
Message-ID: <49EC95EE.4090503@redhat.com>

On 04/20/2009 10:02 AM, javamann at cox.net wrote:
> Howdy,
>   I have an application that generates an unknown amount of threads at startup (via JMX MLet) and I was looking for a way to block all the threads at a certain point and then 'release' them to continue. I looked at the CyclicBarrier and CountDownLatch but they require you know the number of threads ahead of time. Currently I am just using a 'global' Condition for them to acquire and then Signal them all to continue. This seems like a hack and I am hoping there is a better way to accomplish this. Any Ideas?

I think a global condition is a perfectly adequate tool.  Don't forget to 
account for spurious wakeups and interruption though.

The only "problem" with a global condition is that all the threads will be 
woken up at the same time, but that's really exactly what you're trying to 
do, so it's not a problem in your case.

The only performance hazard I can think of offhand would be that every 
awoken thread will immediately contend for the condition lock, so you'll 
want to let it go as soon as possible.  For very large thread+CPU counts 
this might become an issue.

- DML

From dl at cs.oswego.edu  Mon Apr 20 11:44:40 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 20 Apr 2009 11:44:40 -0400
Subject: [concurrency-interest] CyclicBarrier / Count Down Latch
In-Reply-To: <20090420110207.X7PUN.680646.imail@fed1rmwml33>
References: <20090420110207.X7PUN.680646.imail@fed1rmwml33>
Message-ID: <49EC9868.5070200@cs.oswego.edu>

javamann at cox.net wrote:
> Howdy, I have an application that generates an unknown amount of threads at
> startup (via JMX MLet) and I was looking for a way to block all the threads
> at a certain point and then 'release' them to continue. I looked at the
> CyclicBarrier and CountDownLatch but they require you know the number of
> threads ahead of time. Currently I am just using a 'global' Condition for
> them to acquire and then Signal them all to continue. This seems like a hack
> and I am hoping there is a better way to accomplish this. Any Ideas?
> 

This is one of the long-missing usages that we addressed in Phasers --
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/Phaser.html
This is targeted for Java7 but available now in jsr166y package --
See http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
for jars, sources, etc

-Doug





From gregg at cytetech.com  Mon Apr 20 15:41:00 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 20 Apr 2009 14:41:00 -0500
Subject: [concurrency-interest] CyclicBarrier / Count Down Latch
In-Reply-To: <20090420110207.X7PUN.680646.imail@fed1rmwml33>
References: <20090420110207.X7PUN.680646.imail@fed1rmwml33>
Message-ID: <49ECCFCC.2000004@cytetech.com>

In many of my applications, I have some startup synchronization that needs to 
occur before the system can proceed to an active state.  I always have something 
like the following that I use because it makes the system independent of new 
threads of changes in threading that I make over time.

public class Progress {
	private boolean ready;
	private final Object lock = new Object();
	public void await() {
		synchronized( lock ) {
			while( !ready )
				lock.wait();
		}
	}
	public void ready() {
		synchronized( lock ) {
			ready = true;
			lock.notifyAll();
		}
	}
}

I can then just create an instance and use it for for whatever synchronization I 
need to wait for.  Threads of execution can then be blocked until everything is
ready to go and the appropriate ready() call is made.

Gregg Wonderly

javamann at cox.net wrote:
> Howdy,
>   I have an application that generates an unknown amount of threads at startup (via JMX MLet) and I was looking for a way to block all the threads at a certain point and then 'release' them to continue. I looked at the CyclicBarrier and CountDownLatch but they require you know the number of threads ahead of time. Currently I am just using a 'global' Condition for them to acquire and then Signal them all to continue. This seems like a hack and I am hoping there is a better way to accomplish this. Any Ideas?
> 
> Thanks
> 
> -Pete
> 
> --
> 
> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
> 
> 2. Behind every great woman... Is a man checking out her ass
> 
> 3. I am not a member of any organized political party. I am a Democrat.*
> 
> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
> 
> 5. A process is what you need when all your good people have left.
> 
> 
> *Will Rogers
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From gregg at cytetech.com  Mon Apr 20 17:52:00 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 20 Apr 2009 16:52:00 -0500
Subject: [concurrency-interest] Shutting down ExecutorService
In-Reply-To: <c5cb1c590904171103h23f27f34x77882a567969b04f@mail.gmail.com>
References: <c5cb1c590904171103h23f27f34x77882a567969b04f@mail.gmail.com>
Message-ID: <49ECEE80.4090400@cytetech.com>

Anytime you have a ServerSocket opened in a thread somewhere, any other thread 
can close that ServerSocket.  This is the most reliable way to intercede and 
wake up a blocked thread that is at ServerSocket.accept().  In general, Java I/O 
has always allowed one thread to close an endpoint that another thread is 
blocked on and wake that thread up.

Gregg Wonderly

Thomas Barnett wrote:
> I am trying to wrap my head around concurrency package. There is an
> example of use of ExecutorService in LifecycleWebServer listing in the
> book JCIP by Goetz et al [1]. It goes:
> 
> ...
> while(!exec.isShutDown()) {
>   try {
>     final Socket conn = socket.accept();
> ...
> 
> and has a stop() method that shuts down executor service. The problem
> is, unless there is a client connection, socket.accept blocks and the
> condition is never checked so LifecycleWebServer never terminates even
> after calling stop().  Trying to use interruption for cancellation as
> in listing 7.5 of the same book does not work either when the blocking
> method is not interruptible. I just want to make sure that I am not
> misunderstanding something here.
> 
> A workaround I could think of was to set timeout on server socket and
> consume the SocketTimeoutException which seem to work but would that
> be a correct way to do this?
> 
> Regards
> Thomas
> 
> [1] http://jcip.net/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From karmazilla at gmail.com  Tue Apr 21 05:01:03 2009
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Tue, 21 Apr 2009 11:01:03 +0200
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <635c64210904200629n62389076gcf81d57cbf327fb4@mail.gmail.com>
References: <635c64210904160542t57be55a2v281acd15de6a058c@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCKEPEIAAA.davidcholmes@aapt.net.au>
	<635c64210904200629n62389076gcf81d57cbf327fb4@mail.gmail.com>
Message-ID: <90622e530904210201m1f7d80e4m881462f4ea1d9f83@mail.gmail.com>

On Mon, Apr 20, 2009 at 3:29 PM, tom strickland
<mr.tom.strickland at gmail.com> wrote:
> After more thought, I thought I'd ask if I could just shift threads from
> lock-to-lock, as follows:
>
> Each entity has its own data lock that does not change. It also has a second
> "call" lock that can change. Thus, a call from Alice that turns into a call
> between Alice and Bob will use a single call lock. All traffic between Alice
> and Bob uses this lock to all serialise operations on the state machines for
> Alice, Bob and their call. If Bob is to be shunted into a different call
> with Carol, then a new lock will be created for that call. The problem is
> that there may be races here. So, could the following approach work and what
> would be the best starting point for implementing it?
>
> 1. Any thread that has acquired a call lock will be allowed to proceed to
> completion. To change the call lock, a thread must first acquire it.
> 2. Any thread that is waiting to acquire the call lock (because it is
> already owned by another thread) is essentially waiting for one of two
> conditions:
> ?? a. the lock has been acquired by the waiting thread
> ?? b. the lock has changed and the waiting thread must go and wait on this
> new changed lock
>
> This seems feasible to me, but I am unsure of how to wait for two
> conditions. Is this a situation where the AbstractQueuedSynchronizer would
> prove useful? I don't think that I've missed anything subtle, but this
> approach is using one lock to get/set the value of the current lock (a data
> lock) and a separate lock, that may change, to coordinate actions and I
> could easily have missed something.

What about this: once a thread changes the call lock, it releases the
old one. Then, any thread that acquires the call lock must then
immediately check that it is still the current active call lock and if
not, it must be released again and the thread must start waiting on
the new lock.

>
> Thanks,
>
> Tom
>
> 2009/4/16 David Holmes <davidcholmes at aapt.net.au>
>>
>> Hi Tom,
>>
>> This kind of scenario does arise whenever you have entities that most of
>> the time have to handle their exclusion constraints (locking) independently,
>> but then at times have to co-operate to provide atomic operations across
>> sets of?objects. Acquiring multiple locks is the main way of extending the
>> independent use of locks - with ordering to avoid deadlocks.
>>
>> What you describe is a little different in that an operation that
>> initially needs only a single lock, may turn into an operation that requires
>> multiple locks. In that case all I can suggest is that after acquiring l(c)
>> the code checks if that was sufficient, and if not it grabs l(b) - perhaps
>> releasing l(c) first to maintain ordering.
>>
>> It really all depends on interaction between the different actions as to
>> how best to partition the locking and what strategies to try and use. I
>> don't think you are missing anything obvious.
>>
>> Cheers,
>> David Holmes
>>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
>> strickland
>> Sent: Thursday, 16 April 2009 10:43 PM
>> To: Endre St?lsvik
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] transferring queued threads
>>
>> Thanks Endre, Christian for your replies and I'm sorry that I've been late
>> in replying.
>>
>> Perhaps I should have been more explicit in my original scenario. It may
>> be that I'm too close to the problem to see an obvious solution, but I don't
>> think that the proposed solution will work.
>>
>> Here's a more detailed description of the problem:
>> This is a communications system in which 2 parties can be in a
>> communication session with each other - think of it as an IM session or a
>> phone call.
>>
>> Alice and Bob are in a call.
>> Alice and Carol are also in a call.
>> Alice's calls are being coordinated by a service. One function offered by
>> this service is "transfer", whereby the service hooks Bob and Carol together
>> and drops Alice out of the loop.
>>
>> These calls have moderately complex state machines that are guarded by
>> locks. Triggers can come into these state machines from either end of a call
>> and must acquire the lock to protect memory state and to prevent races on
>> the state machines.
>> Each user has a state machine for their side of the call and each call has
>> its own higher-level state machine. To simplify modelling, the two users and
>> call operate under a single lock, so that a trigger into one user's state
>> machine flows across the call and state machines atomically, without having
>> to worry about races from other triggers.
>>
>> My problem is this:
>> Alice and Bob have a call that is protected by a single lock L(B)
>> Alice and Carol have a call that is protected by a single lock L(C)
>> We need to end up with Bob and Carol in a call without running into a
>> deadlock between the two locks.
>>
>> If I switch both calls to having to acquire both locks, in order, perhaps
>> there is a race.
>> Let's say that the transfer trigger comes in and acquires L(B). The system
>> realises that this is a transfer and acquires L(C) so that it has control of
>> both calls locks and thus can be sure that nothing else will race over the
>> state machines. After this, all triggers to the call Alice-Bob will have to
>> acquire locks L(B) and L(C) in that order. Similarly, all triggers to
>> Alice-Carol will have to acquire both locks, in the same order. The problem
>> is that there is a race in which a thread can start to wait for L(C) while
>> this changeover is happening and be left waiting for L(C) without knowing
>> that it should have acquired L(B) first.
>> Sequence for two threads, labelled tB and tC:
>> tB?? transfer trigger comes in
>> tB?? acquire L(B)
>> tB?? acquire L(C)
>> tC?? trigger arrives, starts waiting for L(C)
>> tB?? sets Alice-Carol call to need to acquire both L(B) and L(C) from now
>> on
>> tB?? does stuff to set off the transfer and then releases L(C) and L(B)
>> tC?? acquires L(C), but has not acquired L(B)
>>
>>
>> Again, I have some thoughts on how to solve this, but I'm worried that I'm
>> missing the point or overcomplicating matters in the statement of my
>> problem. Am I missing something obvious?
>>
>> Thanks,
>>
>> Tom
>>
>>
>> 2009/4/14 Endre St?lsvik <Online at stolsvik.com>
>>>
>>> On Mon, Apr 6, 2009 at 21:00, Christian Vest Hansen
>>> <karmazilla at gmail.com> wrote:
>>> > Have a lock for each entity; L(A), L(B), L(C) etc. such that each
>>> > entity at any given point in time has exactly one lock. These locks
>>> > have a universal and immutable ordering to them (like
>>> > System.identityHashCode, for instance).
>>>
>>> System.identityHashCode can give you the same result for two different
>>> objects, and you'd not be guaranteed a single order on the two
>>> different threads. Use a long sequencer (in addition!).
>>>
>>> Endre.
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.


From davidcholmes at aapt.net.au  Tue Apr 21 06:13:28 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 21 Apr 2009 20:13:28 +1000
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <635c64210904200629n62389076gcf81d57cbf327fb4@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEAEIBAA.davidcholmes@aapt.net.au>

Tom,

You could use AQS to devise something that waits for two conditions but it
is non-trivial to say the least. You could also just check to see after
acquiring a call-lock whether it is still the call-lock as described ie not
use AQS but just two normal Locks. Requiring you to hold the current
call-lock to change the call-lock should address race conditions, but you
may need to ensure you release the local lock before-hand to avoid
deadlocks.

It really depends on the details of the use cases here - eg who has the
right to shunt Bob off his call with Alice, and how is this communicated to
Bob?

David Holmes
  -----Original Message-----
  From: tom strickland [mailto:mr.tom.strickland at gmail.com]
  Sent: Monday, 20 April 2009 11:29 PM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] transferring queued threads


  After more thought, I thought I'd ask if I could just shift threads from
lock-to-lock, as follows:

  Each entity has its own data lock that does not change. It also has a
second "call" lock that can change. Thus, a call from Alice that turns into
a call between Alice and Bob will use a single call lock. All traffic
between Alice and Bob uses this lock to all serialise operations on the
state machines for Alice, Bob and their call. If Bob is to be shunted into a
different call with Carol, then a new lock will be created for that call.
The problem is that there may be races here. So, could the following
approach work and what would be the best starting point for implementing it?

  1. Any thread that has acquired a call lock will be allowed to proceed to
completion. To change the call lock, a thread must first acquire it.
  2. Any thread that is waiting to acquire the call lock (because it is
already owned by another thread) is essentially waiting for one of two
conditions:
     a. the lock has been acquired by the waiting thread
     b. the lock has changed and the waiting thread must go and wait on this
new changed lock

  This seems feasible to me, but I am unsure of how to wait for two
conditions. Is this a situation where the AbstractQueuedSynchronizer would
prove useful? I don't think that I've missed anything subtle, but this
approach is using one lock to get/set the value of the current lock (a data
lock) and a separate lock, that may change, to coordinate actions and I
could easily have missed something.

  Thanks,

  Tom


  2009/4/16 David Holmes <davidcholmes at aapt.net.au>

    Hi Tom,

    This kind of scenario does arise whenever you have entities that most of
the time have to handle their exclusion constraints (locking) independently,
but then at times have to co-operate to provide atomic operations across
sets of objects. Acquiring multiple locks is the main way of extending the
independent use of locks - with ordering to avoid deadlocks.

    What you describe is a little different in that an operation that
initially needs only a single lock, may turn into an operation that requires
multiple locks. In that case all I can suggest is that after acquiring l(c)
the code checks if that was sufficient, and if not it grabs l(b) - perhaps
releasing l(c) first to maintain ordering.

    It really all depends on interaction between the different actions as to
how best to partition the locking and what strategies to try and use. I
don't think you are missing anything obvious.

    Cheers,
    David Holmes
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
strickland
      Sent: Thursday, 16 April 2009 10:43 PM
      To: Endre St?lsvik
      Cc: concurrency-interest at cs.oswego.edu

      Subject: Re: [concurrency-interest] transferring queued threads


      Thanks Endre, Christian for your replies and I'm sorry that I've been
late in replying.

      Perhaps I should have been more explicit in my original scenario. It
may be that I'm too close to the problem to see an obvious solution, but I
don't think that the proposed solution will work.

      Here's a more detailed description of the problem:
      This is a communications system in which 2 parties can be in a
communication session with each other - think of it as an IM session or a
phone call.

      Alice and Bob are in a call.
      Alice and Carol are also in a call.
      Alice's calls are being coordinated by a service. One function offered
by this service is "transfer", whereby the service hooks Bob and Carol
together and drops Alice out of the loop.

      These calls have moderately complex state machines that are guarded by
locks. Triggers can come into these state machines from either end of a call
and must acquire the lock to protect memory state and to prevent races on
the state machines.
      Each user has a state machine for their side of the call and each call
has its own higher-level state machine. To simplify modelling, the two users
and call operate under a single lock, so that a trigger into one user's
state machine flows across the call and state machines atomically, without
having to worry about races from other triggers.

      My problem is this:
      Alice and Bob have a call that is protected by a single lock L(B)
      Alice and Carol have a call that is protected by a single lock L(C)
      We need to end up with Bob and Carol in a call without running into a
deadlock between the two locks.

      If I switch both calls to having to acquire both locks, in order,
perhaps there is a race.
      Let's say that the transfer trigger comes in and acquires L(B). The
system realises that this is a transfer and acquires L(C) so that it has
control of both calls locks and thus can be sure that nothing else will race
over the state machines. After this, all triggers to the call Alice-Bob will
have to acquire locks L(B) and L(C) in that order. Similarly, all triggers
to Alice-Carol will have to acquire both locks, in the same order. The
problem is that there is a race in which a thread can start to wait for L(C)
while this changeover is happening and be left waiting for L(C) without
knowing that it should have acquired L(B) first.
      Sequence for two threads, labelled tB and tC:
      tB   transfer trigger comes in
      tB   acquire L(B)
      tB   acquire L(C)
      tC   trigger arrives, starts waiting for L(C)
      tB   sets Alice-Carol call to need to acquire both L(B) and L(C) from
now on
      tB   does stuff to set off the transfer and then releases L(C) and
L(B)
      tC   acquires L(C), but has not acquired L(B)


      Again, I have some thoughts on how to solve this, but I'm worried that
I'm missing the point or overcomplicating matters in the statement of my
problem. Am I missing something obvious?

      Thanks,

      Tom



      2009/4/14 Endre St?lsvik <Online at stolsvik.com>

        On Mon, Apr 6, 2009 at 21:00, Christian Vest Hansen
        <karmazilla at gmail.com> wrote:
        > Have a lock for each entity; L(A), L(B), L(C) etc. such that each
        > entity at any given point in time has exactly one lock. These
locks
        > have a universal and immutable ordering to them (like
        > System.identityHashCode, for instance).


        System.identityHashCode can give you the same result for two
different
        objects, and you'd not be guaranteed a single order on the two
        different threads. Use a long sequencer (in addition!).

        Endre.




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090421/df011cbe/attachment-0001.html>

From mr.tom.strickland at gmail.com  Tue Apr 21 09:28:28 2009
From: mr.tom.strickland at gmail.com (tom strickland)
Date: Tue, 21 Apr 2009 14:28:28 +0100
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEAEIBAA.davidcholmes@aapt.net.au>
References: <635c64210904200629n62389076gcf81d57cbf327fb4@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEAEIBAA.davidcholmes@aapt.net.au>
Message-ID: <635c64210904210628n7f2722b8uea5cf93406ac8188@mail.gmail.com>

Thanks for your replies Christian & David.
I like the sound of the suggested approach, so if I can summarise to check
that I've got everything:

The data remains under a data lock per entity that does not change. The
state machines involved in a call are protected by a call lock, which is
accessed through the data lock and may change. To change a call lock:
1. Acquire the new call lock.
2. Get the existing call lock, using the data lock.
3. Acquire the existing call lock.
4. Change the entity's reference to the call-lock to point to the new lock,
using the data lock.
5. Release the old call lock.

To acquire a call-lock:
1. Get the call lock from the entity's data store, using the data lock.
2. Acquire the call lock.
3. Once acquired, get the call lock again from the entity's data store,
using the data lock.
4. Compare the two values for the lock.
   a. If the lock has changed, release the old lock and acquire the new one.
   b. If the lock has not changed, proceed.

The locks will have to be explicit locks, to enable the hand-over-hand
approach of changeover.

Only Alice can initiate such a change of call-lock and it can only come from
one of Alice's calls at a time, so I am not too worried about two threads
deadlocking by attempting this simultaneously. To address the points at the
end of David's comments, only Alice can cause Bob to be shunted off his call
and she cannot deadlock herself. The communicating-this-to-Bob bit is part
of the communications protocol itself and is straightforward, although it
would be polite to tell Bob that he was going to be shunted :-)

Thanks,

Tom

2009/4/21 David Holmes <davidcholmes at aapt.net.au>

>  Tom,
>
> You could use AQS to devise something that waits for two conditions but it
> is non-trivial to say the least. You could also just check to see after
> acquiring a call-lock whether it is still the call-lock as described ie not
> use AQS but just two normal Locks. Requiring you to hold the current
> call-lock to change the call-lock should address race conditions, but you
> may need to ensure you release the local lock before-hand to avoid
> deadlocks.
>
> It really depends on the details of the use cases here - eg who has the
> right to shunt Bob off his call with Alice, and how is this communicated to
> Bob?
>
> David Holmes
>


2009/4/21 Christian Vest Hansen <karmazilla at gmail.com>
>
> What about this: once a thread changes the call lock, it releases the
> old one. Then, any thread that acquires the call lock must then
> immediately check that it is still the current active call lock and if
> not, it must be released again and the thread must start waiting on
> the new lock.

>  -----Original Message-----
> *From:* tom strickland [mailto:mr.tom.strickland at gmail.com]
> *Sent:* Monday, 20 April 2009 11:29 PM
> *To:* dholmes at ieee.org
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] transferring queued threads
>
> After more thought, I thought I'd ask if I could just shift threads from
> lock-to-lock, as follows:
>
> Each entity has its own data lock that does not change. It also has a
> second "call" lock that can change. Thus, a call from Alice that turns into
> a call between Alice and Bob will use a single call lock. All traffic
> between Alice and Bob uses this lock to all serialise operations on the
> state machines for Alice, Bob and their call. If Bob is to be shunted into a
> different call with Carol, then a new lock will be created for that call.
> The problem is that there may be races here. So, could the following
> approach work and what would be the best starting point for implementing it?
>
> 1. Any thread that has acquired a call lock will be allowed to proceed to
> completion. To change the call lock, a thread must first acquire it.
> 2. Any thread that is waiting to acquire the call lock (because it is
> already owned by another thread) is essentially waiting for one of two
> conditions:
>    a. the lock has been acquired by the waiting thread
>    b. the lock has changed and the waiting thread must go and wait on this
> new changed lock
>
> This seems feasible to me, but I am unsure of how to wait for two
> conditions. Is this a situation where the AbstractQueuedSynchronizer would
> prove useful? I don't think that I've missed anything subtle, but this
> approach is using one lock to get/set the value of the current lock (a data
> lock) and a separate lock, that may change, to coordinate actions and I
> could easily have missed something.
>
> Thanks,
>
> Tom
>
> 2009/4/16 David Holmes <davidcholmes at aapt.net.au>
>
>>  Hi Tom,
>>
>> This kind of scenario does arise whenever you have entities that most of
>> the time have to handle their exclusion constraints (locking) independently,
>> but then at times have to co-operate to provide atomic operations across
>> sets of objects. Acquiring multiple locks is the main way of extending the
>> independent use of locks - with ordering to avoid deadlocks.
>>
>> What you describe is a little different in that an operation that
>> initially needs only a single lock, may turn into an operation that requires
>> multiple locks. In that case all I can suggest is that after acquiring l(c)
>> the code checks if that was sufficient, and if not it grabs l(b) - perhaps
>> releasing l(c) first to maintain ordering.
>>
>> It really all depends on interaction between the different actions as to
>> how best to partition the locking and what strategies to try and use. I
>> don't think you are missing anything obvious.
>>
>> Cheers,
>> David Holmes
>>
>>  -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *tom strickland
>> *Sent:* Thursday, 16 April 2009 10:43 PM
>> *To:* Endre St?lsvik
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] transferring queued threads
>>
>>  Thanks Endre, Christian for your replies and I'm sorry that I've been
>> late in replying.
>>
>> Perhaps I should have been more explicit in my original scenario. It may
>> be that I'm too close to the problem to see an obvious solution, but I don't
>> think that the proposed solution will work.
>>
>> Here's a more detailed description of the problem:
>> This is a communications system in which 2 parties can be in a
>> communication session with each other - think of it as an IM session or a
>> phone call.
>>
>> Alice and Bob are in a call.
>> Alice and Carol are also in a call.
>> Alice's calls are being coordinated by a service. One function offered by
>> this service is "transfer", whereby the service hooks Bob and Carol together
>> and drops Alice out of the loop.
>>
>> These calls have moderately complex state machines that are guarded by
>> locks. Triggers can come into these state machines from either end of a call
>> and must acquire the lock to protect memory state and to prevent races on
>> the state machines.
>> Each user has a state machine for their side of the call and each call has
>> its own higher-level state machine. To simplify modelling, the two users and
>> call operate under a single lock, so that a trigger into one user's state
>> machine flows across the call and state machines atomically, without having
>> to worry about races from other triggers.
>>
>> My problem is this:
>> Alice and Bob have a call that is protected by a single lock L(B)
>> Alice and Carol have a call that is protected by a single lock L(C)
>> We need to end up with Bob and Carol in a call without running into a
>> deadlock between the two locks.
>>
>> If I switch both calls to having to acquire both locks, in order, perhaps
>> there is a race.
>> Let's say that the transfer trigger comes in and acquires L(B). The system
>> realises that this is a transfer and acquires L(C) so that it has control of
>> both calls locks and thus can be sure that nothing else will race over the
>> state machines. After this, all triggers to the call Alice-Bob will have to
>> acquire locks L(B) and L(C) in that order. Similarly, all triggers to
>> Alice-Carol will have to acquire both locks, in the same order. The problem
>> is that there is a race in which a thread can start to wait for L(C) while
>> this changeover is happening and be left waiting for L(C) without knowing
>> that it should have acquired L(B) first.
>> Sequence for two threads, labelled tB and tC:
>> tB   transfer trigger comes in
>> tB   acquire L(B)
>> tB   acquire L(C)
>> tC   trigger arrives, starts waiting for L(C)
>> tB   sets Alice-Carol call to need to acquire both L(B) and L(C) from now
>> on
>> tB   does stuff to set off the transfer and then releases L(C) and L(B)
>> tC   acquires L(C), but has not acquired L(B)
>>
>>
>> Again, I have some thoughts on how to solve this, but I'm worried that I'm
>> missing the point or overcomplicating matters in the statement of my
>> problem. Am I missing something obvious?
>>
>> Thanks,
>>
>> Tom
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090421/8e180dc9/attachment.html>

From davidcholmes at aapt.net.au  Tue Apr 21 09:47:55 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 21 Apr 2009 23:47:55 +1000
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <635c64210904210628n7f2722b8uea5cf93406ac8188@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEAFIBAA.davidcholmes@aapt.net.au>

Hi Tom,

I don't think your protocol is quite right, or perhaps is overly
complicated:

> To change a call lock:
> 1. Acquire the new call lock.

Ok.

> 2. Get the existing call lock, using the data lock.

Don't think you need to lock to do this. But the field should probably be
volatile because it will be protected by different locks at different times.

> 3. Acquire the existing call lock.

Ok.

> 4. Change the entity's reference to the call-lock to point to the new
lock, using the data lock.

Again you don't need a data-lock here, but you do need to check if the
call-lock you just acquired is still the right call-lock. If not then
release it and acquire the current call-lock - repeat as necessary

> 5. Release the old call lock.

Ok.

> To acquire a call-lock:
> 1. Get the call lock from the entity's data store, using the data lock.
> 2. Acquire the call lock.
> 3. Once acquired, get the call lock again from the entity's data store,
using the data lock.
> 4. Compare the two values for the lock.
>   a. If the lock has changed, release the old lock and acquire the new
one.
>   b. If the lock has not changed, proceed.

Again no need for the data-lock here as far as I can see. AScquire the
call-lock then check if it is still the call-lock. If not, drop it, repeat
the process.

The reference to the call-lock is protected by the call-lock itself. But
this means that you will need an initial call-lock to bootstrap the process.

David

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
strickland
Sent: Tuesday, 21 April 2009 11:28 PM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] transferring queued threads


Thanks for your replies Christian & David.
I like the sound of the suggested approach, so if I can summarise to check
that I've got everything:

The data remains under a data lock per entity that does not change. The
state machines involved in a call are protected by a call lock, which is
accessed through the data lock and may change. To change a call lock:
1. Acquire the new call lock.
2. Get the existing call lock, using the data lock.
3. Acquire the existing call lock.
4. Change the entity's reference to the call-lock to point to the new lock,
using the data lock.
5. Release the old call lock.

To acquire a call-lock:
1. Get the call lock from the entity's data store, using the data lock.
2. Acquire the call lock.
3. Once acquired, get the call lock again from the entity's data store,
using the data lock.
4. Compare the two values for the lock.
   a. If the lock has changed, release the old lock and acquire the new one.
   b. If the lock has not changed, proceed.

The locks will have to be explicit locks, to enable the hand-over-hand
approach of changeover.

Only Alice can initiate such a change of call-lock and it can only come from
one of Alice's calls at a time, so I am not too worried about two threads
deadlocking by attempting this simultaneously. To address the points at the
end of David's comments, only Alice can cause Bob to be shunted off his call
and she cannot deadlock herself. The communicating-this-to-Bob bit is part
of the communications protocol itself and is straightforward, although it
would be polite to tell Bob that he was going to be shunted :-)

Thanks,

Tom


2009/4/21 David Holmes <davidcholmes at aapt.net.au>

Tom,

You could use AQS to devise something that waits for two conditions but it
is non-trivial to say the least. You could also just check to see after
acquiring a call-lock whether it is still the call-lock as described ie not
use AQS but just two normal Locks. Requiring you to hold the current
call-lock to change the call-lock should address race conditions, but you
may need to ensure you release the local lock before-hand to avoid
deadlocks.

It really depends on the details of the use cases here - eg who has the
right to shunt Bob off his call with Alice, and how is this communicated to
Bob?

David Holmes


2009/4/21 Christian Vest Hansen <karmazilla at gmail.com>

>

> What about this: once a thread changes the call lock, it releases the
> old one. Then, any thread that acquires the call lock must then
> immediately check that it is still the current active call lock and if
> not, it must be released again and the thread must start waiting on
> the new lock.

-----Original Message-----
From: tom strickland [mailto:mr.tom.strickland at gmail.com]
Sent: Monday, 20 April 2009 11:29 PM
To: dholmes at ieee.org
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] transferring queued threads


After more thought, I thought I'd ask if I could just shift threads from
lock-to-lock, as follows:

Each entity has its own data lock that does not change. It also has a second
"call" lock that can change. Thus, a call from Alice that turns into a call
between Alice and Bob will use a single call lock. All traffic between Alice
and Bob uses this lock to all serialise operations on the state machines for
Alice, Bob and their call. If Bob is to be shunted into a different call
with Carol, then a new lock will be created for that call. The problem is
that there may be races here. So, could the following approach work and what
would be the best starting point for implementing it?

1. Any thread that has acquired a call lock will be allowed to proceed to
completion. To change the call lock, a thread must first acquire it.
2. Any thread that is waiting to acquire the call lock (because it is
already owned by another thread) is essentially waiting for one of two
conditions:
   a. the lock has been acquired by the waiting thread
   b. the lock has changed and the waiting thread must go and wait on this
new changed lock

This seems feasible to me, but I am unsure of how to wait for two
conditions. Is this a situation where the AbstractQueuedSynchronizer would
prove useful? I don't think that I've missed anything subtle, but this
approach is using one lock to get/set the value of the current lock (a data
lock) and a separate lock, that may change, to coordinate actions and I
could easily have missed something.

Thanks,

Tom


2009/4/16 David Holmes <davidcholmes at aapt.net.au>

Hi Tom,

This kind of scenario does arise whenever you have entities that most of the
time have to handle their exclusion constraints (locking) independently, but
then at times have to co-operate to provide atomic operations across sets of
objects. Acquiring multiple locks is the main way of extending the
independent use of locks - with ordering to avoid deadlocks.

What you describe is a little different in that an operation that initially
needs only a single lock, may turn into an operation that requires multiple
locks. In that case all I can suggest is that after acquiring l(c) the code
checks if that was sufficient, and if not it grabs l(b) - perhaps releasing
l(c) first to maintain ordering.

It really all depends on interaction between the different actions as to how
best to partition the locking and what strategies to try and use. I don't
think you are missing anything obvious.

Cheers,
David Holmes
-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
strickland
Sent: Thursday, 16 April 2009 10:43 PM
To: Endre St?lsvik
Cc: concurrency-interest at cs.oswego.edu

Subject: Re: [concurrency-interest] transferring queued threads


Thanks Endre, Christian for your replies and I'm sorry that I've been late
in replying.

Perhaps I should have been more explicit in my original scenario. It may be
that I'm too close to the problem to see an obvious solution, but I don't
think that the proposed solution will work.

Here's a more detailed description of the problem:
This is a communications system in which 2 parties can be in a communication
session with each other - think of it as an IM session or a phone call.

Alice and Bob are in a call.
Alice and Carol are also in a call.
Alice's calls are being coordinated by a service. One function offered by
this service is "transfer", whereby the service hooks Bob and Carol together
and drops Alice out of the loop.

These calls have moderately complex state machines that are guarded by
locks. Triggers can come into these state machines from either end of a call
and must acquire the lock to protect memory state and to prevent races on
the state machines.
Each user has a state machine for their side of the call and each call has
its own higher-level state machine. To simplify modelling, the two users and
call operate under a single lock, so that a trigger into one user's state
machine flows across the call and state machines atomically, without having
to worry about races from other triggers.

My problem is this:
Alice and Bob have a call that is protected by a single lock L(B)
Alice and Carol have a call that is protected by a single lock L(C)
We need to end up with Bob and Carol in a call without running into a
deadlock between the two locks.

If I switch both calls to having to acquire both locks, in order, perhaps
there is a race.
Let's say that the transfer trigger comes in and acquires L(B). The system
realises that this is a transfer and acquires L(C) so that it has control of
both calls locks and thus can be sure that nothing else will race over the
state machines. After this, all triggers to the call Alice-Bob will have to
acquire locks L(B) and L(C) in that order. Similarly, all triggers to
Alice-Carol will have to acquire both locks, in the same order. The problem
is that there is a race in which a thread can start to wait for L(C) while
this changeover is happening and be left waiting for L(C) without knowing
that it should have acquired L(B) first.
Sequence for two threads, labelled tB and tC:
tB   transfer trigger comes in
tB   acquire L(B)
tB   acquire L(C)
tC   trigger arrives, starts waiting for L(C)
tB   sets Alice-Carol call to need to acquire both L(B) and L(C) from now on
tB   does stuff to set off the transfer and then releases L(C) and L(B)
tC   acquires L(C), but has not acquired L(B)


Again, I have some thoughts on how to solve this, but I'm worried that I'm
missing the point or overcomplicating matters in the statement of my
problem. Am I missing something obvious?

Thanks,

Tom



From mr.tom.strickland at gmail.com  Tue Apr 21 10:14:43 2009
From: mr.tom.strickland at gmail.com (tom strickland)
Date: Tue, 21 Apr 2009 15:14:43 +0100
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEAFIBAA.davidcholmes@aapt.net.au>
References: <635c64210904210628n7f2722b8uea5cf93406ac8188@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEAFIBAA.davidcholmes@aapt.net.au>
Message-ID: <635c64210904210714s7f93bb8dpc9018c48fc116f3d@mail.gmail.com>

Hi David,

2009/4/21 David Holmes <davidcholmes at aapt.net.au>

> Hi Tom,
> I don't think your protocol is quite right, or perhaps is overly
> complicated:
> > To change a call lock:
> > 1. Acquire the new call lock.
> Ok.
>
> > 2. Get the existing call lock, using the data lock.
> Don't think you need to lock to do this. But the field should probably be
> volatile because it will be protected by different locks at different
> times.


I think I agree that there is probably no need for the data lock: any data
is kept in a data store, which is thread-safe with respect to reads and
writes. The internals of the lock will be thread-safe and so no extra sync
is needed for memory safety.

> 3. Acquire the existing call lock.
> Ok.
>
> > 4. Change the entity's reference to the call-lock to point to the new
>
>    lock, using the data lock.
> Again you don't need a data-lock here, but you do need to check if the
> call-lock you just acquired is still the right call-lock. If not then
> release it and acquire the current call-lock - repeat as necessary
>

OK.


> > 5. Release the old call lock.
> Ok.
>
> > To acquire a call-lock:
> > 1. Get the call lock from the entity's data store, using the data lock.
> > 2. Acquire the call lock.
> > 3. Once acquired, get the call lock again from the entity's data store,
> >     using the data lock.
> > 4. Compare the two values for the lock.
> >   a. If the lock has changed, release the old lock and acquire the new
> >       one.
> >   b. If the lock has not changed, proceed.
>
> Again no need for the data-lock here as far as I can see. AScquire the
> call-lock then check if it is still the call-lock. If not, drop it, repeat
> the process.
>
> The reference to the call-lock is protected by the call-lock itself. But
> this means that you will need an initial call-lock to bootstrap the
> process.


That's great. The bootstrap shouldn't be a problem and this solution seems
straightforward - I understand it and will be able to communicate it to my
colleagues. This is preferable to messing around with AQS!

Thanks,

Tom

David
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
> strickland
> Sent: Tuesday, 21 April 2009 11:28 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] transferring queued threads
>
>
> Thanks for your replies Christian & David.
> I like the sound of the suggested approach, so if I can summarise to check
> that I've got everything:
>
> The data remains under a data lock per entity that does not change. The
> state machines involved in a call are protected by a call lock, which is
> accessed through the data lock and may change. To change a call lock:
> 1. Acquire the new call lock.
> 2. Get the existing call lock, using the data lock.
> 3. Acquire the existing call lock.
> 4. Change the entity's reference to the call-lock to point to the new lock,
> using the data lock.
> 5. Release the old call lock.
>
> To acquire a call-lock:
> 1. Get the call lock from the entity's data store, using the data lock.
> 2. Acquire the call lock.
> 3. Once acquired, get the call lock again from the entity's data store,
> using the data lock.
> 4. Compare the two values for the lock.
>   a. If the lock has changed, release the old lock and acquire the new one.
>   b. If the lock has not changed, proceed.
>
> The locks will have to be explicit locks, to enable the hand-over-hand
> approach of changeover.
>
> Only Alice can initiate such a change of call-lock and it can only come
> from
> one of Alice's calls at a time, so I am not too worried about two threads
> deadlocking by attempting this simultaneously. To address the points at the
> end of David's comments, only Alice can cause Bob to be shunted off his
> call
> and she cannot deadlock herself. The communicating-this-to-Bob bit is part
> of the communications protocol itself and is straightforward, although it
> would be polite to tell Bob that he was going to be shunted :-)
>
> Thanks,
>
> Tom
>
>
> 2009/4/21 David Holmes <davidcholmes at aapt.net.au>
>
> Tom,
>
> You could use AQS to devise something that waits for two conditions but it
> is non-trivial to say the least. You could also just check to see after
> acquiring a call-lock whether it is still the call-lock as described ie not
> use AQS but just two normal Locks. Requiring you to hold the current
> call-lock to change the call-lock should address race conditions, but you
> may need to ensure you release the local lock before-hand to avoid
> deadlocks.
>
> It really depends on the details of the use cases here - eg who has the
> right to shunt Bob off his call with Alice, and how is this communicated to
> Bob?
>
> David Holmes
>
>
> 2009/4/21 Christian Vest Hansen <karmazilla at gmail.com>
>
> >
>
> > What about this: once a thread changes the call lock, it releases the
> > old one. Then, any thread that acquires the call lock must then
> > immediately check that it is still the current active call lock and if
> > not, it must be released again and the thread must start waiting on
> > the new lock.
>
> -----Original Message-----
> From: tom strickland [mailto:mr.tom.strickland at gmail.com]
> Sent: Monday, 20 April 2009 11:29 PM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] transferring queued threads
>
>
> After more thought, I thought I'd ask if I could just shift threads from
> lock-to-lock, as follows:
>
> Each entity has its own data lock that does not change. It also has a
> second
> "call" lock that can change. Thus, a call from Alice that turns into a call
> between Alice and Bob will use a single call lock. All traffic between
> Alice
> and Bob uses this lock to all serialise operations on the state machines
> for
> Alice, Bob and their call. If Bob is to be shunted into a different call
> with Carol, then a new lock will be created for that call. The problem is
> that there may be races here. So, could the following approach work and
> what
> would be the best starting point for implementing it?
>
> 1. Any thread that has acquired a call lock will be allowed to proceed to
> completion. To change the call lock, a thread must first acquire it.
> 2. Any thread that is waiting to acquire the call lock (because it is
> already owned by another thread) is essentially waiting for one of two
> conditions:
>   a. the lock has been acquired by the waiting thread
>   b. the lock has changed and the waiting thread must go and wait on this
> new changed lock
>
> This seems feasible to me, but I am unsure of how to wait for two
> conditions. Is this a situation where the AbstractQueuedSynchronizer would
> prove useful? I don't think that I've missed anything subtle, but this
> approach is using one lock to get/set the value of the current lock (a data
> lock) and a separate lock, that may change, to coordinate actions and I
> could easily have missed something.
>
> Thanks,
>
> Tom
>
>
> 2009/4/16 David Holmes <davidcholmes at aapt.net.au>
>
> Hi Tom,
>
> This kind of scenario does arise whenever you have entities that most of
> the
> time have to handle their exclusion constraints (locking) independently,
> but
> then at times have to co-operate to provide atomic operations across sets
> of
> objects. Acquiring multiple locks is the main way of extending the
> independent use of locks - with ordering to avoid deadlocks.
>
> What you describe is a little different in that an operation that initially
> needs only a single lock, may turn into an operation that requires multiple
> locks. In that case all I can suggest is that after acquiring l(c) the code
> checks if that was sufficient, and if not it grabs l(b) - perhaps releasing
> l(c) first to maintain ordering.
>
> It really all depends on interaction between the different actions as to
> how
> best to partition the locking and what strategies to try and use. I don't
> think you are missing anything obvious.
>
> Cheers,
> David Holmes
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
> strickland
> Sent: Thursday, 16 April 2009 10:43 PM
> To: Endre St?lsvik
> Cc: concurrency-interest at cs.oswego.edu
>
> Subject: Re: [concurrency-interest] transferring queued threads
>
>
> Thanks Endre, Christian for your replies and I'm sorry that I've been late
> in replying.
>
> Perhaps I should have been more explicit in my original scenario. It may be
> that I'm too close to the problem to see an obvious solution, but I don't
> think that the proposed solution will work.
>
> Here's a more detailed description of the problem:
> This is a communications system in which 2 parties can be in a
> communication
> session with each other - think of it as an IM session or a phone call.
>
> Alice and Bob are in a call.
> Alice and Carol are also in a call.
> Alice's calls are being coordinated by a service. One function offered by
> this service is "transfer", whereby the service hooks Bob and Carol
> together
> and drops Alice out of the loop.
>
> These calls have moderately complex state machines that are guarded by
> locks. Triggers can come into these state machines from either end of a
> call
> and must acquire the lock to protect memory state and to prevent races on
> the state machines.
> Each user has a state machine for their side of the call and each call has
> its own higher-level state machine. To simplify modelling, the two users
> and
> call operate under a single lock, so that a trigger into one user's state
> machine flows across the call and state machines atomically, without having
> to worry about races from other triggers.
>
> My problem is this:
> Alice and Bob have a call that is protected by a single lock L(B)
> Alice and Carol have a call that is protected by a single lock L(C)
> We need to end up with Bob and Carol in a call without running into a
> deadlock between the two locks.
>
> If I switch both calls to having to acquire both locks, in order, perhaps
> there is a race.
> Let's say that the transfer trigger comes in and acquires L(B). The system
> realises that this is a transfer and acquires L(C) so that it has control
> of
> both calls locks and thus can be sure that nothing else will race over the
> state machines. After this, all triggers to the call Alice-Bob will have to
> acquire locks L(B) and L(C) in that order. Similarly, all triggers to
> Alice-Carol will have to acquire both locks, in the same order. The problem
> is that there is a race in which a thread can start to wait for L(C) while
> this changeover is happening and be left waiting for L(C) without knowing
> that it should have acquired L(B) first.
> Sequence for two threads, labelled tB and tC:
> tB   transfer trigger comes in
> tB   acquire L(B)
> tB   acquire L(C)
> tC   trigger arrives, starts waiting for L(C)
> tB   sets Alice-Carol call to need to acquire both L(B) and L(C) from now
> on
> tB   does stuff to set off the transfer and then releases L(C) and L(B)
> tC   acquires L(C), but has not acquired L(B)
>
>
> Again, I have some thoughts on how to solve this, but I'm worried that I'm
> missing the point or overcomplicating matters in the statement of my
> problem. Am I missing something obvious?
>
> Thanks,
>
> Tom
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090421/e1fe425e/attachment.html>

From gregg at cytetech.com  Tue Apr 21 10:28:15 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 21 Apr 2009 09:28:15 -0500
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEAFIBAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEAFIBAA.davidcholmes@aapt.net.au>
Message-ID: <49EDD7FF.9020703@cytetech.com>

It sounds like the data-lock is being used to manage the life-cycle of the 
call-lock.  But, it also sounds like the call can be shunted/interrupted/dropped 
at any moment, so it's not clear that there is any predictable life-cycle of any 
call.  David, are you seeing this too, and suggesting that the data lock is not 
necessary because of this?  It seems that if you just need to make sure that 
only one such call exists, that the single object with a single lock, is in fact 
okay to me.

Gregg Wonderly

David Holmes wrote:
> Hi Tom,
> 
> I don't think your protocol is quite right, or perhaps is overly
> complicated:
> 
>> To change a call lock:
>> 1. Acquire the new call lock.
> 
> Ok.
> 
>> 2. Get the existing call lock, using the data lock.
> 
> Don't think you need to lock to do this. But the field should probably be
> volatile because it will be protected by different locks at different times.
> 
>> 3. Acquire the existing call lock.
> 
> Ok.
> 
>> 4. Change the entity's reference to the call-lock to point to the new
> lock, using the data lock.
> 
> Again you don't need a data-lock here, but you do need to check if the
> call-lock you just acquired is still the right call-lock. If not then
> release it and acquire the current call-lock - repeat as necessary
> 
>> 5. Release the old call lock.
> 
> Ok.
> 
>> To acquire a call-lock:
>> 1. Get the call lock from the entity's data store, using the data lock.
>> 2. Acquire the call lock.
>> 3. Once acquired, get the call lock again from the entity's data store,
> using the data lock.
>> 4. Compare the two values for the lock.
>>   a. If the lock has changed, release the old lock and acquire the new
> one.
>>   b. If the lock has not changed, proceed.
> 
> Again no need for the data-lock here as far as I can see. AScquire the
> call-lock then check if it is still the call-lock. If not, drop it, repeat
> the process.
> 
> The reference to the call-lock is protected by the call-lock itself. But
> this means that you will need an initial call-lock to bootstrap the process.
> 
> David
> 
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
> strickland
> Sent: Tuesday, 21 April 2009 11:28 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] transferring queued threads
> 
> 
> Thanks for your replies Christian & David.
> I like the sound of the suggested approach, so if I can summarise to check
> that I've got everything:
> 
> The data remains under a data lock per entity that does not change. The
> state machines involved in a call are protected by a call lock, which is
> accessed through the data lock and may change. To change a call lock:
> 1. Acquire the new call lock.
> 2. Get the existing call lock, using the data lock.
> 3. Acquire the existing call lock.
> 4. Change the entity's reference to the call-lock to point to the new lock,
> using the data lock.
> 5. Release the old call lock.
> 
> To acquire a call-lock:
> 1. Get the call lock from the entity's data store, using the data lock.
> 2. Acquire the call lock.
> 3. Once acquired, get the call lock again from the entity's data store,
> using the data lock.
> 4. Compare the two values for the lock.
>    a. If the lock has changed, release the old lock and acquire the new one.
>    b. If the lock has not changed, proceed.
> 
> The locks will have to be explicit locks, to enable the hand-over-hand
> approach of changeover.
> 
> Only Alice can initiate such a change of call-lock and it can only come from
> one of Alice's calls at a time, so I am not too worried about two threads
> deadlocking by attempting this simultaneously. To address the points at the
> end of David's comments, only Alice can cause Bob to be shunted off his call
> and she cannot deadlock herself. The communicating-this-to-Bob bit is part
> of the communications protocol itself and is straightforward, although it
> would be polite to tell Bob that he was going to be shunted :-)
> 
> Thanks,
> 
> Tom
> 
> 
> 2009/4/21 David Holmes <davidcholmes at aapt.net.au>
> 
> Tom,
> 
> You could use AQS to devise something that waits for two conditions but it
> is non-trivial to say the least. You could also just check to see after
> acquiring a call-lock whether it is still the call-lock as described ie not
> use AQS but just two normal Locks. Requiring you to hold the current
> call-lock to change the call-lock should address race conditions, but you
> may need to ensure you release the local lock before-hand to avoid
> deadlocks.
> 
> It really depends on the details of the use cases here - eg who has the
> right to shunt Bob off his call with Alice, and how is this communicated to
> Bob?
> 
> David Holmes
> 
> 
> 2009/4/21 Christian Vest Hansen <karmazilla at gmail.com>
> 
> 
>> What about this: once a thread changes the call lock, it releases the
>> old one. Then, any thread that acquires the call lock must then
>> immediately check that it is still the current active call lock and if
>> not, it must be released again and the thread must start waiting on
>> the new lock.
> 
> -----Original Message-----
> From: tom strickland [mailto:mr.tom.strickland at gmail.com]
> Sent: Monday, 20 April 2009 11:29 PM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] transferring queued threads
> 
> 
> After more thought, I thought I'd ask if I could just shift threads from
> lock-to-lock, as follows:
> 
> Each entity has its own data lock that does not change. It also has a second
> "call" lock that can change. Thus, a call from Alice that turns into a call
> between Alice and Bob will use a single call lock. All traffic between Alice
> and Bob uses this lock to all serialise operations on the state machines for
> Alice, Bob and their call. If Bob is to be shunted into a different call
> with Carol, then a new lock will be created for that call. The problem is
> that there may be races here. So, could the following approach work and what
> would be the best starting point for implementing it?
> 
> 1. Any thread that has acquired a call lock will be allowed to proceed to
> completion. To change the call lock, a thread must first acquire it.
> 2. Any thread that is waiting to acquire the call lock (because it is
> already owned by another thread) is essentially waiting for one of two
> conditions:
>    a. the lock has been acquired by the waiting thread
>    b. the lock has changed and the waiting thread must go and wait on this
> new changed lock
> 
> This seems feasible to me, but I am unsure of how to wait for two
> conditions. Is this a situation where the AbstractQueuedSynchronizer would
> prove useful? I don't think that I've missed anything subtle, but this
> approach is using one lock to get/set the value of the current lock (a data
> lock) and a separate lock, that may change, to coordinate actions and I
> could easily have missed something.
> 
> Thanks,
> 
> Tom
> 
> 
> 2009/4/16 David Holmes <davidcholmes at aapt.net.au>
> 
> Hi Tom,
> 
> This kind of scenario does arise whenever you have entities that most of the
> time have to handle their exclusion constraints (locking) independently, but
> then at times have to co-operate to provide atomic operations across sets of
> objects. Acquiring multiple locks is the main way of extending the
> independent use of locks - with ordering to avoid deadlocks.
> 
> What you describe is a little different in that an operation that initially
> needs only a single lock, may turn into an operation that requires multiple
> locks. In that case all I can suggest is that after acquiring l(c) the code
> checks if that was sufficient, and if not it grabs l(b) - perhaps releasing
> l(c) first to maintain ordering.
> 
> It really all depends on interaction between the different actions as to how
> best to partition the locking and what strategies to try and use. I don't
> think you are missing anything obvious.
> 
> Cheers,
> David Holmes
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
> strickland
> Sent: Thursday, 16 April 2009 10:43 PM
> To: Endre St?lsvik
> Cc: concurrency-interest at cs.oswego.edu
> 
> Subject: Re: [concurrency-interest] transferring queued threads
> 
> 
> Thanks Endre, Christian for your replies and I'm sorry that I've been late
> in replying.
> 
> Perhaps I should have been more explicit in my original scenario. It may be
> that I'm too close to the problem to see an obvious solution, but I don't
> think that the proposed solution will work.
> 
> Here's a more detailed description of the problem:
> This is a communications system in which 2 parties can be in a communication
> session with each other - think of it as an IM session or a phone call.
> 
> Alice and Bob are in a call.
> Alice and Carol are also in a call.
> Alice's calls are being coordinated by a service. One function offered by
> this service is "transfer", whereby the service hooks Bob and Carol together
> and drops Alice out of the loop.
> 
> These calls have moderately complex state machines that are guarded by
> locks. Triggers can come into these state machines from either end of a call
> and must acquire the lock to protect memory state and to prevent races on
> the state machines.
> Each user has a state machine for their side of the call and each call has
> its own higher-level state machine. To simplify modelling, the two users and
> call operate under a single lock, so that a trigger into one user's state
> machine flows across the call and state machines atomically, without having
> to worry about races from other triggers.
> 
> My problem is this:
> Alice and Bob have a call that is protected by a single lock L(B)
> Alice and Carol have a call that is protected by a single lock L(C)
> We need to end up with Bob and Carol in a call without running into a
> deadlock between the two locks.
> 
> If I switch both calls to having to acquire both locks, in order, perhaps
> there is a race.
> Let's say that the transfer trigger comes in and acquires L(B). The system
> realises that this is a transfer and acquires L(C) so that it has control of
> both calls locks and thus can be sure that nothing else will race over the
> state machines. After this, all triggers to the call Alice-Bob will have to
> acquire locks L(B) and L(C) in that order. Similarly, all triggers to
> Alice-Carol will have to acquire both locks, in the same order. The problem
> is that there is a race in which a thread can start to wait for L(C) while
> this changeover is happening and be left waiting for L(C) without knowing
> that it should have acquired L(B) first.
> Sequence for two threads, labelled tB and tC:
> tB   transfer trigger comes in
> tB   acquire L(B)
> tB   acquire L(C)
> tC   trigger arrives, starts waiting for L(C)
> tB   sets Alice-Carol call to need to acquire both L(B) and L(C) from now on
> tB   does stuff to set off the transfer and then releases L(C) and L(B)
> tC   acquires L(C), but has not acquired L(B)
> 
> 
> Again, I have some thoughts on how to solve this, but I'm worried that I'm
> missing the point or overcomplicating matters in the statement of my
> problem. Am I missing something obvious?
> 
> Thanks,
> 
> Tom
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From davidcholmes at aapt.net.au  Tue Apr 21 10:55:48 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 22 Apr 2009 00:55:48 +1000
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <49EDD7FF.9020703@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEAGIBAA.davidcholmes@aapt.net.au>

Hi Gregg,

Honestly I'm not too clear on the details here, it just seemed that a
data-lock (ie the lock on an individual object) would not be necessary to
guard the reference to the call-lock, and that it wouldn't anyway because
the attempt to change the call-lock would always have to come through the
same object, which doesn't seem right.

The call-lock is required to guard access to multi-party actions, but I'm
not clear on the exact protocols.

David

> -----Original Message-----
> From: Gregg Wonderly [mailto:gregg at cytetech.com]
> Sent: Wednesday, 22 April 2009 12:28 AM
> To: dholmes at IEEE.org
> Cc: tom strickland; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] transferring queued threads
>
>
>
> It sounds like the data-lock is being used to manage the
> life-cycle of the
> call-lock.  But, it also sounds like the call can be
> shunted/interrupted/dropped
> at any moment, so it's not clear that there is any predictable
> life-cycle of any
> call.  David, are you seeing this too, and suggesting that the
> data lock is not
> necessary because of this?  It seems that if you just need to
> make sure that
> only one such call exists, that the single object with a single
> lock, is in fact
> okay to me.
>
> Gregg Wonderly
>
> David Holmes wrote:
> > Hi Tom,
> >
> > I don't think your protocol is quite right, or perhaps is overly
> > complicated:
> >
> >> To change a call lock:
> >> 1. Acquire the new call lock.
> >
> > Ok.
> >
> >> 2. Get the existing call lock, using the data lock.
> >
> > Don't think you need to lock to do this. But the field should
> probably be
> > volatile because it will be protected by different locks at
> different times.
> >
> >> 3. Acquire the existing call lock.
> >
> > Ok.
> >
> >> 4. Change the entity's reference to the call-lock to point to the new
> > lock, using the data lock.
> >
> > Again you don't need a data-lock here, but you do need to check if the
> > call-lock you just acquired is still the right call-lock. If not then
> > release it and acquire the current call-lock - repeat as necessary
> >
> >> 5. Release the old call lock.
> >
> > Ok.
> >
> >> To acquire a call-lock:
> >> 1. Get the call lock from the entity's data store, using the data lock.
> >> 2. Acquire the call lock.
> >> 3. Once acquired, get the call lock again from the entity's data store,
> > using the data lock.
> >> 4. Compare the two values for the lock.
> >>   a. If the lock has changed, release the old lock and acquire the new
> > one.
> >>   b. If the lock has not changed, proceed.
> >
> > Again no need for the data-lock here as far as I can see. AScquire the
> > call-lock then check if it is still the call-lock. If not, drop
> it, repeat
> > the process.
> >
> > The reference to the call-lock is protected by the call-lock itself. But
> > this means that you will need an initial call-lock to bootstrap
> the process.
> >
> > David
> >
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
> > strickland
> > Sent: Tuesday, 21 April 2009 11:28 PM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] transferring queued threads
> >
> >
> > Thanks for your replies Christian & David.
> > I like the sound of the suggested approach, so if I can
> summarise to check
> > that I've got everything:
> >
> > The data remains under a data lock per entity that does not change. The
> > state machines involved in a call are protected by a call lock, which is
> > accessed through the data lock and may change. To change a call lock:
> > 1. Acquire the new call lock.
> > 2. Get the existing call lock, using the data lock.
> > 3. Acquire the existing call lock.
> > 4. Change the entity's reference to the call-lock to point to
> the new lock,
> > using the data lock.
> > 5. Release the old call lock.
> >
> > To acquire a call-lock:
> > 1. Get the call lock from the entity's data store, using the data lock.
> > 2. Acquire the call lock.
> > 3. Once acquired, get the call lock again from the entity's data store,
> > using the data lock.
> > 4. Compare the two values for the lock.
> >    a. If the lock has changed, release the old lock and acquire
> the new one.
> >    b. If the lock has not changed, proceed.
> >
> > The locks will have to be explicit locks, to enable the hand-over-hand
> > approach of changeover.
> >
> > Only Alice can initiate such a change of call-lock and it can
> only come from
> > one of Alice's calls at a time, so I am not too worried about
> two threads
> > deadlocking by attempting this simultaneously. To address the
> points at the
> > end of David's comments, only Alice can cause Bob to be shunted
> off his call
> > and she cannot deadlock herself. The communicating-this-to-Bob
> bit is part
> > of the communications protocol itself and is straightforward,
> although it
> > would be polite to tell Bob that he was going to be shunted :-)
> >
> > Thanks,
> >
> > Tom
> >
> >
> > 2009/4/21 David Holmes <davidcholmes at aapt.net.au>
> >
> > Tom,
> >
> > You could use AQS to devise something that waits for two
> conditions but it
> > is non-trivial to say the least. You could also just check to see after
> > acquiring a call-lock whether it is still the call-lock as
> described ie not
> > use AQS but just two normal Locks. Requiring you to hold the current
> > call-lock to change the call-lock should address race
> conditions, but you
> > may need to ensure you release the local lock before-hand to avoid
> > deadlocks.
> >
> > It really depends on the details of the use cases here - eg who has the
> > right to shunt Bob off his call with Alice, and how is this
> communicated to
> > Bob?
> >
> > David Holmes
> >
> >
> > 2009/4/21 Christian Vest Hansen <karmazilla at gmail.com>
> >
> >
> >> What about this: once a thread changes the call lock, it releases the
> >> old one. Then, any thread that acquires the call lock must then
> >> immediately check that it is still the current active call lock and if
> >> not, it must be released again and the thread must start waiting on
> >> the new lock.
> >
> > -----Original Message-----
> > From: tom strickland [mailto:mr.tom.strickland at gmail.com]
> > Sent: Monday, 20 April 2009 11:29 PM
> > To: dholmes at ieee.org
> > Cc: concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] transferring queued threads
> >
> >
> > After more thought, I thought I'd ask if I could just shift threads from
> > lock-to-lock, as follows:
> >
> > Each entity has its own data lock that does not change. It also
> has a second
> > "call" lock that can change. Thus, a call from Alice that turns
> into a call
> > between Alice and Bob will use a single call lock. All traffic
> between Alice
> > and Bob uses this lock to all serialise operations on the state
> machines for
> > Alice, Bob and their call. If Bob is to be shunted into a different call
> > with Carol, then a new lock will be created for that call. The
> problem is
> > that there may be races here. So, could the following approach
> work and what
> > would be the best starting point for implementing it?
> >
> > 1. Any thread that has acquired a call lock will be allowed to
> proceed to
> > completion. To change the call lock, a thread must first acquire it.
> > 2. Any thread that is waiting to acquire the call lock (because it is
> > already owned by another thread) is essentially waiting for one of two
> > conditions:
> >    a. the lock has been acquired by the waiting thread
> >    b. the lock has changed and the waiting thread must go and
> wait on this
> > new changed lock
> >
> > This seems feasible to me, but I am unsure of how to wait for two
> > conditions. Is this a situation where the
> AbstractQueuedSynchronizer would
> > prove useful? I don't think that I've missed anything subtle, but this
> > approach is using one lock to get/set the value of the current
> lock (a data
> > lock) and a separate lock, that may change, to coordinate actions and I
> > could easily have missed something.
> >
> > Thanks,
> >
> > Tom
> >
> >
> > 2009/4/16 David Holmes <davidcholmes at aapt.net.au>
> >
> > Hi Tom,
> >
> > This kind of scenario does arise whenever you have entities
> that most of the
> > time have to handle their exclusion constraints (locking)
> independently, but
> > then at times have to co-operate to provide atomic operations
> across sets of
> > objects. Acquiring multiple locks is the main way of extending the
> > independent use of locks - with ordering to avoid deadlocks.
> >
> > What you describe is a little different in that an operation
> that initially
> > needs only a single lock, may turn into an operation that
> requires multiple
> > locks. In that case all I can suggest is that after acquiring
> l(c) the code
> > checks if that was sufficient, and if not it grabs l(b) -
> perhaps releasing
> > l(c) first to maintain ordering.
> >
> > It really all depends on interaction between the different
> actions as to how
> > best to partition the locking and what strategies to try and
> use. I don't
> > think you are missing anything obvious.
> >
> > Cheers,
> > David Holmes
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
> > strickland
> > Sent: Thursday, 16 April 2009 10:43 PM
> > To: Endre St?lsvik
> > Cc: concurrency-interest at cs.oswego.edu
> >
> > Subject: Re: [concurrency-interest] transferring queued threads
> >
> >
> > Thanks Endre, Christian for your replies and I'm sorry that
> I've been late
> > in replying.
> >
> > Perhaps I should have been more explicit in my original
> scenario. It may be
> > that I'm too close to the problem to see an obvious solution,
> but I don't
> > think that the proposed solution will work.
> >
> > Here's a more detailed description of the problem:
> > This is a communications system in which 2 parties can be in a
> communication
> > session with each other - think of it as an IM session or a phone call.
> >
> > Alice and Bob are in a call.
> > Alice and Carol are also in a call.
> > Alice's calls are being coordinated by a service. One function
> offered by
> > this service is "transfer", whereby the service hooks Bob and
> Carol together
> > and drops Alice out of the loop.
> >
> > These calls have moderately complex state machines that are guarded by
> > locks. Triggers can come into these state machines from either
> end of a call
> > and must acquire the lock to protect memory state and to
> prevent races on
> > the state machines.
> > Each user has a state machine for their side of the call and
> each call has
> > its own higher-level state machine. To simplify modelling, the
> two users and
> > call operate under a single lock, so that a trigger into one
> user's state
> > machine flows across the call and state machines atomically,
> without having
> > to worry about races from other triggers.
> >
> > My problem is this:
> > Alice and Bob have a call that is protected by a single lock L(B)
> > Alice and Carol have a call that is protected by a single lock L(C)
> > We need to end up with Bob and Carol in a call without running into a
> > deadlock between the two locks.
> >
> > If I switch both calls to having to acquire both locks, in
> order, perhaps
> > there is a race.
> > Let's say that the transfer trigger comes in and acquires L(B).
> The system
> > realises that this is a transfer and acquires L(C) so that it
> has control of
> > both calls locks and thus can be sure that nothing else will
> race over the
> > state machines. After this, all triggers to the call Alice-Bob
> will have to
> > acquire locks L(B) and L(C) in that order. Similarly, all triggers to
> > Alice-Carol will have to acquire both locks, in the same order.
> The problem
> > is that there is a race in which a thread can start to wait for
> L(C) while
> > this changeover is happening and be left waiting for L(C)
> without knowing
> > that it should have acquired L(B) first.
> > Sequence for two threads, labelled tB and tC:
> > tB   transfer trigger comes in
> > tB   acquire L(B)
> > tB   acquire L(C)
> > tC   trigger arrives, starts waiting for L(C)
> > tB   sets Alice-Carol call to need to acquire both L(B) and
> L(C) from now on
> > tB   does stuff to set off the transfer and then releases L(C) and L(B)
> > tC   acquires L(C), but has not acquired L(B)
> >
> >
> > Again, I have some thoughts on how to solve this, but I'm
> worried that I'm
> > missing the point or overcomplicating matters in the statement of my
> > problem. Am I missing something obvious?
> >
> > Thanks,
> >
> > Tom
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>



From mr.tom.strickland at gmail.com  Tue Apr 21 13:11:01 2009
From: mr.tom.strickland at gmail.com (tom strickland)
Date: Tue, 21 Apr 2009 18:11:01 +0100
Subject: [concurrency-interest] transferring queued threads
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEAGIBAA.davidcholmes@aapt.net.au>
References: <49EDD7FF.9020703@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCAEAGIBAA.davidcholmes@aapt.net.au>
Message-ID: <635c64210904211011s82493a6n8566cf4e29995bf4@mail.gmail.com>

Hi Greg, David,
We do need a data lock elsewhere, but we do not need it here. It was a
mistake to use it in the first place. We need a data lock to ensure safe
publication of mutable objects in the data store (which is really a
session). The data store's get/set/remove operations are thread-safe and so
a data lock is not needed to access immutable objects or objects whose
internal values are thread-safe, such as an explicit lock from j.u.c.

As for the call-locks and changing them: calls exist in sets of calls. A
call must always have a lock and that lock may change. Calls exist in sets
and a call's lock can only be changed by another call in that set. At a
given point in time, only one call in the set may do this, so there should
be no no chance of deadlock and no need for any additional protection.

Thanks,

Tom

2009/4/21 David Holmes <davidcholmes at aapt.net.au>

> Hi Gregg,
>
> Honestly I'm not too clear on the details here, it just seemed that a
> data-lock (ie the lock on an individual object) would not be necessary to
> guard the reference to the call-lock, and that it wouldn't anyway because
> the attempt to change the call-lock would always have to come through the
> same object, which doesn't seem right.
>
> The call-lock is required to guard access to multi-party actions, but I'm
> not clear on the exact protocols.
>
> David
>
> > -----Original Message-----
> > From: Gregg Wonderly [mailto:gregg at cytetech.com]
> > Sent: Wednesday, 22 April 2009 12:28 AM
> > To: dholmes at IEEE.org
> > Cc: tom strickland; concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] transferring queued threads
> >
> >
> >
> > It sounds like the data-lock is being used to manage the
> > life-cycle of the
> > call-lock.  But, it also sounds like the call can be
> > shunted/interrupted/dropped
> > at any moment, so it's not clear that there is any predictable
> > life-cycle of any
> > call.  David, are you seeing this too, and suggesting that the
> > data lock is not
> > necessary because of this?  It seems that if you just need to
> > make sure that
> > only one such call exists, that the single object with a single
> > lock, is in fact
> > okay to me.
> >
> > Gregg Wonderly
> >
> > David Holmes wrote:
> > > Hi Tom,
> > >
> > > I don't think your protocol is quite right, or perhaps is overly
> > > complicated:
> > >
> > >> To change a call lock:
> > >> 1. Acquire the new call lock.
> > >
> > > Ok.
> > >
> > >> 2. Get the existing call lock, using the data lock.
> > >
> > > Don't think you need to lock to do this. But the field should
> > probably be
> > > volatile because it will be protected by different locks at
> > different times.
> > >
> > >> 3. Acquire the existing call lock.
> > >
> > > Ok.
> > >
> > >> 4. Change the entity's reference to the call-lock to point to the new
> > > lock, using the data lock.
> > >
> > > Again you don't need a data-lock here, but you do need to check if the
> > > call-lock you just acquired is still the right call-lock. If not then
> > > release it and acquire the current call-lock - repeat as necessary
> > >
> > >> 5. Release the old call lock.
> > >
> > > Ok.
> > >
> > >> To acquire a call-lock:
> > >> 1. Get the call lock from the entity's data store, using the data
> lock.
> > >> 2. Acquire the call lock.
> > >> 3. Once acquired, get the call lock again from the entity's data
> store,
> > > using the data lock.
> > >> 4. Compare the two values for the lock.
> > >>   a. If the lock has changed, release the old lock and acquire the new
> > > one.
> > >>   b. If the lock has not changed, proceed.
> > >
> > > Again no need for the data-lock here as far as I can see. AScquire the
> > > call-lock then check if it is still the call-lock. If not, drop
> > it, repeat
> > > the process.
> > >
> > > The reference to the call-lock is protected by the call-lock itself.
> But
> > > this means that you will need an initial call-lock to bootstrap
> > the process.
> > >
> > > David
> > >
> > > -----Original Message-----
> > > From: concurrency-interest-bounces at cs.oswego.edu
> > > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
> > > strickland
> > > Sent: Tuesday, 21 April 2009 11:28 PM
> > > To: concurrency-interest at cs.oswego.edu
> > > Subject: Re: [concurrency-interest] transferring queued threads
> > >
> > >
> > > Thanks for your replies Christian & David.
> > > I like the sound of the suggested approach, so if I can
> > summarise to check
> > > that I've got everything:
> > >
> > > The data remains under a data lock per entity that does not change. The
> > > state machines involved in a call are protected by a call lock, which
> is
> > > accessed through the data lock and may change. To change a call lock:
> > > 1. Acquire the new call lock.
> > > 2. Get the existing call lock, using the data lock.
> > > 3. Acquire the existing call lock.
> > > 4. Change the entity's reference to the call-lock to point to
> > the new lock,
> > > using the data lock.
> > > 5. Release the old call lock.
> > >
> > > To acquire a call-lock:
> > > 1. Get the call lock from the entity's data store, using the data lock.
> > > 2. Acquire the call lock.
> > > 3. Once acquired, get the call lock again from the entity's data store,
> > > using the data lock.
> > > 4. Compare the two values for the lock.
> > >    a. If the lock has changed, release the old lock and acquire
> > the new one.
> > >    b. If the lock has not changed, proceed.
> > >
> > > The locks will have to be explicit locks, to enable the hand-over-hand
> > > approach of changeover.
> > >
> > > Only Alice can initiate such a change of call-lock and it can
> > only come from
> > > one of Alice's calls at a time, so I am not too worried about
> > two threads
> > > deadlocking by attempting this simultaneously. To address the
> > points at the
> > > end of David's comments, only Alice can cause Bob to be shunted
> > off his call
> > > and she cannot deadlock herself. The communicating-this-to-Bob
> > bit is part
> > > of the communications protocol itself and is straightforward,
> > although it
> > > would be polite to tell Bob that he was going to be shunted :-)
> > >
> > > Thanks,
> > >
> > > Tom
> > >
> > >
> > > 2009/4/21 David Holmes <davidcholmes at aapt.net.au>
> > >
> > > Tom,
> > >
> > > You could use AQS to devise something that waits for two
> > conditions but it
> > > is non-trivial to say the least. You could also just check to see after
> > > acquiring a call-lock whether it is still the call-lock as
> > described ie not
> > > use AQS but just two normal Locks. Requiring you to hold the current
> > > call-lock to change the call-lock should address race
> > conditions, but you
> > > may need to ensure you release the local lock before-hand to avoid
> > > deadlocks.
> > >
> > > It really depends on the details of the use cases here - eg who has the
> > > right to shunt Bob off his call with Alice, and how is this
> > communicated to
> > > Bob?
> > >
> > > David Holmes
> > >
> > >
> > > 2009/4/21 Christian Vest Hansen <karmazilla at gmail.com>
> > >
> > >
> > >> What about this: once a thread changes the call lock, it releases the
> > >> old one. Then, any thread that acquires the call lock must then
> > >> immediately check that it is still the current active call lock and if
> > >> not, it must be released again and the thread must start waiting on
> > >> the new lock.
> > >
> > > -----Original Message-----
> > > From: tom strickland [mailto:mr.tom.strickland at gmail.com]
> > > Sent: Monday, 20 April 2009 11:29 PM
> > > To: dholmes at ieee.org
> > > Cc: concurrency-interest at cs.oswego.edu
> > > Subject: Re: [concurrency-interest] transferring queued threads
> > >
> > >
> > > After more thought, I thought I'd ask if I could just shift threads
> from
> > > lock-to-lock, as follows:
> > >
> > > Each entity has its own data lock that does not change. It also
> > has a second
> > > "call" lock that can change. Thus, a call from Alice that turns
> > into a call
> > > between Alice and Bob will use a single call lock. All traffic
> > between Alice
> > > and Bob uses this lock to all serialise operations on the state
> > machines for
> > > Alice, Bob and their call. If Bob is to be shunted into a different
> call
> > > with Carol, then a new lock will be created for that call. The
> > problem is
> > > that there may be races here. So, could the following approach
> > work and what
> > > would be the best starting point for implementing it?
> > >
> > > 1. Any thread that has acquired a call lock will be allowed to
> > proceed to
> > > completion. To change the call lock, a thread must first acquire it.
> > > 2. Any thread that is waiting to acquire the call lock (because it is
> > > already owned by another thread) is essentially waiting for one of two
> > > conditions:
> > >    a. the lock has been acquired by the waiting thread
> > >    b. the lock has changed and the waiting thread must go and
> > wait on this
> > > new changed lock
> > >
> > > This seems feasible to me, but I am unsure of how to wait for two
> > > conditions. Is this a situation where the
> > AbstractQueuedSynchronizer would
> > > prove useful? I don't think that I've missed anything subtle, but this
> > > approach is using one lock to get/set the value of the current
> > lock (a data
> > > lock) and a separate lock, that may change, to coordinate actions and I
> > > could easily have missed something.
> > >
> > > Thanks,
> > >
> > > Tom
> > >
> > >
> > > 2009/4/16 David Holmes <davidcholmes at aapt.net.au>
> > >
> > > Hi Tom,
> > >
> > > This kind of scenario does arise whenever you have entities
> > that most of the
> > > time have to handle their exclusion constraints (locking)
> > independently, but
> > > then at times have to co-operate to provide atomic operations
> > across sets of
> > > objects. Acquiring multiple locks is the main way of extending the
> > > independent use of locks - with ordering to avoid deadlocks.
> > >
> > > What you describe is a little different in that an operation
> > that initially
> > > needs only a single lock, may turn into an operation that
> > requires multiple
> > > locks. In that case all I can suggest is that after acquiring
> > l(c) the code
> > > checks if that was sufficient, and if not it grabs l(b) -
> > perhaps releasing
> > > l(c) first to maintain ordering.
> > >
> > > It really all depends on interaction between the different
> > actions as to how
> > > best to partition the locking and what strategies to try and
> > use. I don't
> > > think you are missing anything obvious.
> > >
> > > Cheers,
> > > David Holmes
> > > -----Original Message-----
> > > From: concurrency-interest-bounces at cs.oswego.edu
> > > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of tom
> > > strickland
> > > Sent: Thursday, 16 April 2009 10:43 PM
> > > To: Endre St?lsvik
> > > Cc: concurrency-interest at cs.oswego.edu
> > >
> > > Subject: Re: [concurrency-interest] transferring queued threads
> > >
> > >
> > > Thanks Endre, Christian for your replies and I'm sorry that
> > I've been late
> > > in replying.
> > >
> > > Perhaps I should have been more explicit in my original
> > scenario. It may be
> > > that I'm too close to the problem to see an obvious solution,
> > but I don't
> > > think that the proposed solution will work.
> > >
> > > Here's a more detailed description of the problem:
> > > This is a communications system in which 2 parties can be in a
> > communication
> > > session with each other - think of it as an IM session or a phone call.
> > >
> > > Alice and Bob are in a call.
> > > Alice and Carol are also in a call.
> > > Alice's calls are being coordinated by a service. One function
> > offered by
> > > this service is "transfer", whereby the service hooks Bob and
> > Carol together
> > > and drops Alice out of the loop.
> > >
> > > These calls have moderately complex state machines that are guarded by
> > > locks. Triggers can come into these state machines from either
> > end of a call
> > > and must acquire the lock to protect memory state and to
> > prevent races on
> > > the state machines.
> > > Each user has a state machine for their side of the call and
> > each call has
> > > its own higher-level state machine. To simplify modelling, the
> > two users and
> > > call operate under a single lock, so that a trigger into one
> > user's state
> > > machine flows across the call and state machines atomically,
> > without having
> > > to worry about races from other triggers.
> > >
> > > My problem is this:
> > > Alice and Bob have a call that is protected by a single lock L(B)
> > > Alice and Carol have a call that is protected by a single lock L(C)
> > > We need to end up with Bob and Carol in a call without running into a
> > > deadlock between the two locks.
> > >
> > > If I switch both calls to having to acquire both locks, in
> > order, perhaps
> > > there is a race.
> > > Let's say that the transfer trigger comes in and acquires L(B).
> > The system
> > > realises that this is a transfer and acquires L(C) so that it
> > has control of
> > > both calls locks and thus can be sure that nothing else will
> > race over the
> > > state machines. After this, all triggers to the call Alice-Bob
> > will have to
> > > acquire locks L(B) and L(C) in that order. Similarly, all triggers to
> > > Alice-Carol will have to acquire both locks, in the same order.
> > The problem
> > > is that there is a race in which a thread can start to wait for
> > L(C) while
> > > this changeover is happening and be left waiting for L(C)
> > without knowing
> > > that it should have acquired L(B) first.
> > > Sequence for two threads, labelled tB and tC:
> > > tB   transfer trigger comes in
> > > tB   acquire L(B)
> > > tB   acquire L(C)
> > > tC   trigger arrives, starts waiting for L(C)
> > > tB   sets Alice-Carol call to need to acquire both L(B) and
> > L(C) from now on
> > > tB   does stuff to set off the transfer and then releases L(C) and L(B)
> > > tC   acquires L(C), but has not acquired L(B)
> > >
> > >
> > > Again, I have some thoughts on how to solve this, but I'm
> > worried that I'm
> > > missing the point or overcomplicating matters in the statement of my
> > > problem. Am I missing something obvious?
> > >
> > > Thanks,
> > >
> > > Tom
> > >
> > >
> > > _______________________________________________
> > > Concurrency-interest mailing list
> > > Concurrency-interest at cs.oswego.edu
> > > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > >
> > >
> >
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090421/84d381ac/attachment-0001.html>

From peter.kovacs.1.0rc at gmail.com  Wed Apr 22 11:10:15 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Wed, 22 Apr 2009 17:10:15 +0200
Subject: [concurrency-interest] Thread safety when loading and linking
	classes
Message-ID: <fdeb32eb0904220810x3be9d3ccs7e744aff632069cd@mail.gmail.com>

Hi,

Is the Class.forName(String) call thread-safe? Must the underlying
ClassLoader instances be thread safe or is their thread-safety
guaranteed "automagically" by the JVM? If ClassLoader instances must
be thread-safe, is the default application class loader known to be
thread-safe? (Based on the reference to JLS 12.4.2 in JCIP 3.5.3,
there appears to be fairly strong guarantees about the thread-safety
of class initialization, but it is not immediately clear to me whether
similar guarantees can be assumed about the loading and liking phase
as well.)

(We're having a rare random problem and [short of any better, more
realistic candidate] Class.forName(String) has come under suspicion.)

Thanks
Peter

From ashwin.jayaprakash at gmail.com  Wed Apr 22 13:33:18 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Wed, 22 Apr 2009 10:33:18 -0700
Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor does
	not execute jobs in some cases
Message-ID: <837c23d40904221033k5061dfa1k47b6baf10e7e9106@mail.gmail.com>

Hi, I've raised a bug in the Sun Bug Database. It's still under review. But
here it is:

java.util.concurrent.ThreadPoolExecutor does not execute jobs in some cases.

If the corePoolSize if less than the maximumPoolSize, then the thread pool
just does not execute the submitted jobs. The jobs just sit there.

If the corePoolSize is set to 1 instead of 0, then only 1 thread executes
the jobs sequentially.

==================================================
When the corePoolSize is 0:
==================================================

Submitting job 1
Submitting job 2
Submitting job 3
Submitting job 4
Shutting down...
Waiting for job to complete.

(Program never exits)


==================================================
When the corePoolSize is 1:
==================================================
Submitting job 1
Submitting job 2
Submitting job 3
Submitting job 4
Shutting down...
Waiting for job to complete.
Starting job: temp.Temp$StuckJob at 140c281
Finished job: temp.Temp$StuckJob at 140c281
Waiting for job to complete.
Starting job: temp.Temp$StuckJob at a1d1f4
Finished job: temp.Temp$StuckJob at a1d1f4
Waiting for job to complete.
Starting job: temp.Temp$StuckJob at 1df280b
Finished job: temp.Temp$StuckJob at 1df280b
Waiting for job to complete.
Starting job: temp.Temp$StuckJob at 1be0f0a
Finished job: temp.Temp$StuckJob at 1be0f0a
Shut down completed.

REPRODUCIBILITY :
This bug can be reproduced always.

---------- BEGIN SOURCE ----------
public class Temp {
   public static void main(String[] args) throws ExecutionException,
InterruptedException {
       ThreadPoolExecutor tpe = new ThreadPoolExecutor(0, 512,
               3 * 60, TimeUnit.SECONDS,
               new LinkedBlockingQueue<Runnable>(),
               new SimpleThreadFactory("test"));

       LinkedList<Future> futures = new LinkedList<Future>();

       System.out.println("Submitting job 1");
       futures.add(tpe.submit(new StuckJob()));

       System.out.println("Submitting job 2");
       futures.add(tpe.submit(new StuckJob()));

       System.out.println("Submitting job 3");
       futures.add(tpe.submit(new StuckJob()));

       System.out.println("Submitting job 4");
       futures.add(tpe.submit(new StuckJob()));

       System.out.println("Shutting down...");

       for (Future future : futures) {
           System.out.println("Waiting for job to complete.");
           future.get();
       }

       tpe.shutdown();
       System.out.println("Shut down completed.");
   }

   public static class StuckJob implements Runnable {
       public void run() {
           try {
               System.out.println("Starting job: " + this);
               Thread.sleep(5000);
               System.out.println("Finished job: " + this);
           }
           catch (InterruptedException e) {
               e.printStackTrace();
           }
       }
   }
}

---------- END SOURCE ----------

Workaround:
Set the corePoolSize to be equal to the maximumPoolSize. But this is scary
because if the pool ever reaches its max limit then all those threads will
just sit there instead of shutting down after the idle time out.

Ashwin (http://javaforu.blogspot.com)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090422/8f7ac5e9/attachment.html>

From martinrb at google.com  Wed Apr 22 13:50:49 2009
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 22 Apr 2009 10:50:49 -0700
Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor
	does not execute jobs in some cases
In-Reply-To: <837c23d40904221033k5061dfa1k47b6baf10e7e9106@mail.gmail.com>
References: <837c23d40904221033k5061dfa1k47b6baf10e7e9106@mail.gmail.com>
Message-ID: <1ccfd1c10904221050n43d90a82r5bd6b287ae7b767f@mail.gmail.com>

There were many bug fixes to ThreadPoolExecutor that have not yet been
included in jdk6 update releases.  Have you tried latest jdk7 builds?

Martin

On Wed, Apr 22, 2009 at 10:33, Ashwin Jayaprakash
<ashwin.jayaprakash at gmail.com> wrote:
> Hi, I've raised a bug in the Sun Bug Database. It's still under review. But
> here it is:
>
> java.util.concurrent.ThreadPoolExecutor does not execute jobs in some cases.
>
> If the corePoolSize if less than the maximumPoolSize, then the thread pool
> just does not execute the submitted jobs. The jobs just sit there.
>
> If the corePoolSize is set to 1 instead of 0, then only 1 thread executes
> the jobs sequentially.
>
> ==================================================
> When the corePoolSize is 0:
> ==================================================
>
> Submitting job 1
> Submitting job 2
> Submitting job 3
> Submitting job 4
> Shutting down...
> Waiting for job to complete.
>
> (Program never exits)
>
>
> ==================================================
> When the corePoolSize is 1:
> ==================================================
> Submitting job 1
> Submitting job 2
> Submitting job 3
> Submitting job 4
> Shutting down...
> Waiting for job to complete.
> Starting job: temp.Temp$StuckJob at 140c281
> Finished job: temp.Temp$StuckJob at 140c281
> Waiting for job to complete.
> Starting job: temp.Temp$StuckJob at a1d1f4
> Finished job: temp.Temp$StuckJob at a1d1f4
> Waiting for job to complete.
> Starting job: temp.Temp$StuckJob at 1df280b
> Finished job: temp.Temp$StuckJob at 1df280b
> Waiting for job to complete.
> Starting job: temp.Temp$StuckJob at 1be0f0a
> Finished job: temp.Temp$StuckJob at 1be0f0a
> Shut down completed.
>
> REPRODUCIBILITY :
> This bug can be reproduced always.
>
> ---------- BEGIN SOURCE ----------
> public class Temp {
> ?? public static void main(String[] args) throws ExecutionException,
> InterruptedException {
> ?????? ThreadPoolExecutor tpe = new ThreadPoolExecutor(0, 512,
> ?????????????? 3 * 60, TimeUnit.SECONDS,
> ?????????????? new LinkedBlockingQueue<Runnable>(),
> ?????????????? new SimpleThreadFactory("test"));
>
> ?????? LinkedList<Future> futures = new LinkedList<Future>();
>
> ?????? System.out.println("Submitting job 1");
> ?????? futures.add(tpe.submit(new StuckJob()));
>
> ?????? System.out.println("Submitting job 2");
> ?????? futures.add(tpe.submit(new StuckJob()));
>
> ?????? System.out.println("Submitting job 3");
> ?????? futures.add(tpe.submit(new StuckJob()));
>
> ?????? System.out.println("Submitting job 4");
> ?????? futures.add(tpe.submit(new StuckJob()));
>
> ?????? System.out.println("Shutting down...");
>
> ?????? for (Future future : futures) {
> ?????????? System.out.println("Waiting for job to complete.");
> ?????????? future.get();
> ?????? }
>
> ?????? tpe.shutdown();
> ?????? System.out.println("Shut down completed.");
> ?? }
>
> ?? public static class StuckJob implements Runnable {
> ?????? public void run() {
> ?????????? try {
> ?????????????? System.out.println("Starting job: " + this);
> ?????????????? Thread.sleep(5000);
> ?????????????? System.out.println("Finished job: " + this);
> ?????????? }
> ?????????? catch (InterruptedException e) {
> ?????????????? e.printStackTrace();
> ?????????? }
> ?????? }
> ?? }
> }
>
> ---------- END SOURCE ----------
>
> Workaround:
> Set the corePoolSize to be equal to the maximumPoolSize. But this is scary
> because if the pool ever reaches its max limit then all those threads will
> just sit there instead of shutting down after the idle time out.
>
> Ashwin (http://javaforu.blogspot.com)
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From davidcholmes at aapt.net.au  Wed Apr 22 17:49:24 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 23 Apr 2009 07:49:24 +1000
Subject: [concurrency-interest] Thread safety when loading and
	linkingclasses
In-Reply-To: <fdeb32eb0904220810x3be9d3ccs7e744aff632069cd@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEBHIBAA.davidcholmes@aapt.net.au>

Peter,

The simple answer is that "yes it should all be thread-safe". But there is a
more complex answer that can lead to problems in some complex circumstances.
In short the VM doesn't expect/assume that ClassLoader implementations are
thread-safe, so it uses synchronization on the ClassLoader instance to
ensure that. But that can lead to potential deadlocks if class-loaders don't
obey strict delegation, so there are some hacks that can be employed to
avoid this.

If you have more specific details on the problem drop me a direct email and
I'll see if I can match the symptoms to any known issues.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Kovacs
> Sent: Thursday, 23 April 2009 1:10 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Thread safety when loading and
> linkingclasses
>
>
>
> Hi,
>
> Is the Class.forName(String) call thread-safe? Must the underlying
> ClassLoader instances be thread safe or is their thread-safety
> guaranteed "automagically" by the JVM? If ClassLoader instances must
> be thread-safe, is the default application class loader known to be
> thread-safe? (Based on the reference to JLS 12.4.2 in JCIP 3.5.3,
> there appears to be fairly strong guarantees about the thread-safety
> of class initialization, but it is not immediately clear to me whether
> similar guarantees can be assumed about the loading and liking phase
> as well.)
>
> (We're having a rare random problem and [short of any better, more
> realistic candidate] Class.forName(String) has come under suspicion.)
>
> Thanks
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From davidcholmes at aapt.net.au  Wed Apr 22 18:02:58 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 23 Apr 2009 08:02:58 +1000
Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor
	doesnot execute jobs in some cases
In-Reply-To: <837c23d40904221033k5061dfa1k47b6baf10e7e9106@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEBIIBAA.davidcholmes@aapt.net.au>

I don't think the issue here is "corePoolSize is less than maximumPoolSize"
I think this is a particular issue with corePoolSize==0, and looking at the
current JDK7 code I'm pretty sure this has been addressed so there is always
at least one thread even if corePoolSize is zero.

Remember the way this is supposed to work is that on a submission the
executor creates a thread if there are less than corePoolSize threads, else
the task is queued. If the queue is bounded and is full then a thread is
again created, provided the number of threads is less than maximumPoolSize.

So unless you use a bounded queue that gets full, you will not see the
number of threads grow from corePoolSizse to mnaxPoolSize.

David Holmes

  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ashwin
Jayaprakash
  Sent: Thursday, 23 April 2009 3:33 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor
doesnot execute jobs in some cases


  Hi, I've raised a bug in the Sun Bug Database. It's still under review.
But here it is:

  java.util.concurrent.ThreadPoolExecutor does not execute jobs in some
cases.

  If the corePoolSize if less than the maximumPoolSize, then the thread pool
just does not execute the submitted jobs. The jobs just sit there.

  If the corePoolSize is set to 1 instead of 0, then only 1 thread executes
the jobs sequentially.

  ==================================================
  When the corePoolSize is 0:
  ==================================================

  Submitting job 1
  Submitting job 2
  Submitting job 3
  Submitting job 4
  Shutting down...
  Waiting for job to complete.

  (Program never exits)


  ==================================================
  When the corePoolSize is 1:
  ==================================================
  Submitting job 1
  Submitting job 2
  Submitting job 3
  Submitting job 4
  Shutting down...
  Waiting for job to complete.
  Starting job: temp.Temp$StuckJob at 140c281
  Finished job: temp.Temp$StuckJob at 140c281
  Waiting for job to complete.
  Starting job: temp.Temp$StuckJob at a1d1f4
  Finished job: temp.Temp$StuckJob at a1d1f4
  Waiting for job to complete.
  Starting job: temp.Temp$StuckJob at 1df280b
  Finished job: temp.Temp$StuckJob at 1df280b
  Waiting for job to complete.
  Starting job: temp.Temp$StuckJob at 1be0f0a
  Finished job: temp.Temp$StuckJob at 1be0f0a
  Shut down completed.

  REPRODUCIBILITY :
  This bug can be reproduced always.

  ---------- BEGIN SOURCE ----------
  public class Temp {
     public static void main(String[] args) throws ExecutionException,
InterruptedException {
         ThreadPoolExecutor tpe = new ThreadPoolExecutor(0, 512,
                 3 * 60, TimeUnit.SECONDS,
                 new LinkedBlockingQueue<Runnable>(),
                 new SimpleThreadFactory("test"));

         LinkedList<Future> futures = new LinkedList<Future>();

         System.out.println("Submitting job 1");
         futures.add(tpe.submit(new StuckJob()));

         System.out.println("Submitting job 2");
         futures.add(tpe.submit(new StuckJob()));

         System.out.println("Submitting job 3");
         futures.add(tpe.submit(new StuckJob()));

         System.out.println("Submitting job 4");
         futures.add(tpe.submit(new StuckJob()));

         System.out.println("Shutting down...");

         for (Future future : futures) {
             System.out.println("Waiting for job to complete.");
             future.get();
         }

         tpe.shutdown();
         System.out.println("Shut down completed.");
     }

     public static class StuckJob implements Runnable {
         public void run() {
             try {
                 System.out.println("Starting job: " + this);
                 Thread.sleep(5000);
                 System.out.println("Finished job: " + this);
             }
             catch (InterruptedException e) {
                 e.printStackTrace();
             }
         }
     }
  }

  ---------- END SOURCE ----------

  Workaround:
  Set the corePoolSize to be equal to the maximumPoolSize. But this is scary
because if the pool ever reaches its max limit then all those threads will
just sit there instead of shutting down after the idle time out.

  Ashwin (http://javaforu.blogspot.com)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090423/b73b36f5/attachment-0001.html>

From peter.kovacs.1.0rc at gmail.com  Thu Apr 23 07:14:20 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Thu, 23 Apr 2009 13:14:20 +0200
Subject: [concurrency-interest] Thread safety when loading and
	linkingclasses
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEBHIBAA.davidcholmes@aapt.net.au>
References: <fdeb32eb0904220810x3be9d3ccs7e744aff632069cd@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOEBHIBAA.davidcholmes@aapt.net.au>
Message-ID: <fdeb32eb0904230414p3d020e14u47fd27f72efc1e85@mail.gmail.com>

Thank you, David!

After our code was corrected to pass on an exception which in the
previous version had been transformed into a meaningless
NullPointerException, it turned out that the thread executing
Class.forName was interrupted. (It is still a mystery who did the
interrupt, but it doesn't appear to make much sense to hunt around for
the interrupter as the code appears to be still in some kind of early
alpha version...)

Cheers,
Peter

On Wed, Apr 22, 2009 at 11:49 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Peter,
>
> The simple answer is that "yes it should all be thread-safe". But there is a
> more complex answer that can lead to problems in some complex circumstances.
> In short the VM doesn't expect/assume that ClassLoader implementations are
> thread-safe, so it uses synchronization on the ClassLoader instance to
> ensure that. But that can lead to potential deadlocks if class-loaders don't
> obey strict delegation, so there are some hacks that can be employed to
> avoid this.
>
> If you have more specific details on the problem drop me a direct email and
> I'll see if I can match the symptoms to any known issues.
>
> Cheers,
> David Holmes
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
>> Kovacs
>> Sent: Thursday, 23 April 2009 1:10 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Thread safety when loading and
>> linkingclasses
>>
>>
>>
>> Hi,
>>
>> Is the Class.forName(String) call thread-safe? Must the underlying
>> ClassLoader instances be thread safe or is their thread-safety
>> guaranteed "automagically" by the JVM? If ClassLoader instances must
>> be thread-safe, is the default application class loader known to be
>> thread-safe? (Based on the reference to JLS 12.4.2 in JCIP 3.5.3,
>> there appears to be fairly strong guarantees about the thread-safety
>> of class initialization, but it is not immediately clear to me whether
>> similar guarantees can be assumed about the loading and liking phase
>> as well.)
>>
>> (We're having a rare random problem and [short of any better, more
>> realistic candidate] Class.forName(String) has come under suspicion.)
>>
>> Thanks
>> Peter
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

From sberlin at gmail.com  Thu Apr 23 17:45:15 2009
From: sberlin at gmail.com (Sam Berlin)
Date: Thu, 23 Apr 2009 17:45:15 -0400
Subject: [concurrency-interest] CopyOnWriteArrayList.setAll?
Message-ID: <19196d860904231445s40bdecddw36e124083de34e2a@mail.gmail.com>

Hi Folks,

Is there a way to emulate a CoW "set all" with CopyOnWriteArrayList?  The
goal is to make it impossible for someone to view the list in a state except
for where all the new items are added and all the old ones are removed.

It doesn't seem like it's possible (without restoring to blocking & copying
the data within the locks), but I may be missing something.  If I'm not
missing something, would setAll be a good candidate for inclusion into
CoWArrayList?

Sam
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090423/b5c0bb03/attachment.html>

From jed at atlassian.com  Thu Apr 23 18:21:55 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Fri, 24 Apr 2009 08:21:55 +1000
Subject: [concurrency-interest] CopyOnWriteArrayList.setAll?
In-Reply-To: <19196d860904231445s40bdecddw36e124083de34e2a@mail.gmail.com>
References: <19196d860904231445s40bdecddw36e124083de34e2a@mail.gmail.com>
Message-ID: <CCC4AFE9-E96E-40EC-9BB6-024633E65FAF@atlassian.com>

Interesting. In the Java5 version the modifications are protected by  
COWArrayList's monitor - synchronized(this) - so you simply need to  
acquire that object's monitor - synchronized(cowList) and and  call  
clear(), addAll().

In later versions however it has been changed to use a private  
ReentrantLock - so that approach will break. There doesn't appear to  
be any way to access that lock externally or from a sub-class.

Perhaps Doug can explain why this backwards incompatible change was  
made?

cheers,
jed.

On 24/04/2009, at 7:45 AM, Sam Berlin wrote:

> Hi Folks,
>
> Is there a way to emulate a CoW "set all" with  
> CopyOnWriteArrayList?  The goal is to make it impossible for someone  
> to view the list in a state except for where all the new items are  
> added and all the old ones are removed.
>
> It doesn't seem like it's possible (without restoring to blocking &  
> copying the data within the locks), but I may be missing something.   
> If I'm not missing something, would setAll be a good candidate for  
> inclusion into CoWArrayList?
>
> Sam
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From gregg at cytetech.com  Thu Apr 23 18:45:44 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 23 Apr 2009 17:45:44 -0500
Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor
 doesnot execute jobs in some cases
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEBIIBAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEBIIBAA.davidcholmes@aapt.net.au>
Message-ID: <49F0EF98.8040402@cytetech.com>

For high latency threads, this strategy doesn't allow aggressive thread creation 
to occur to try and make sure things work as responsively as possible.  All of 
the thread pools that I've ever created have always aimed at maxPoolSize until 
all threads were running, and corePoolSize would just be the minimum threads 
that we'd fall back to in an idle state.

For network applications where there are database's and server's latency to deal 
with, a very aggressive thread creation strategy has always helped me to 
minimize latency.  For CPU bound things of short life, the current TPE strategy 
will work, but still favors some delays that don't have to exist, and create 
queue overflow situations more often than if maxPoolSize was aimed at always.

Gregg Wonderly

David Holmes wrote:
> I don't think the issue here is "corePoolSize is less than 
> maximumPoolSize" I think this is a particular issue with 
> corePoolSize==0, and looking at the current JDK7 code I'm pretty sure 
> this has been addressed so there is always at least one thread even if 
> corePoolSize is zero.
> 
> Remember the way this is supposed to work is that on a submission the 
> executor creates a thread if there are less than corePoolSize threads, 
> else the task is queued. If the queue is bounded and is full then a 
> thread is again created, provided the number of threads is less than 
> maximumPoolSize.
> 
> So unless you use a bounded queue that gets full, you will not see the 
> number of threads grow from corePoolSizse to mnaxPoolSize.
> 
> David Holmes
> 
>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Ashwin Jayaprakash
>     *Sent:* Thursday, 23 April 2009 3:33 AM
>     *To:* concurrency-interest at cs.oswego.edu
>     *Subject:* [concurrency-interest]
>     java.util.concurrent.ThreadPoolExecutor doesnot execute jobs in some
>     cases
> 
>     Hi, I've raised a bug in the Sun Bug Database. It's still under
>     review. But here it is:
> 
>     java.util.concurrent.ThreadPoolExecutor does not execute jobs in
>     some cases.
> 
>     If the corePoolSize if less than the maximumPoolSize, then the
>     thread pool just does not execute the submitted jobs. The jobs just
>     sit there.
> 
>     If the corePoolSize is set to 1 instead of 0, then only 1 thread
>     executes the jobs sequentially.
> 
>     ==================================================
>     When the corePoolSize is 0:
>     ==================================================
> 
>     Submitting job 1
>     Submitting job 2
>     Submitting job 3
>     Submitting job 4
>     Shutting down...
>     Waiting for job to complete.
> 
>     (Program never exits)
> 
> 
>     ==================================================
>     When the corePoolSize is 1:
>     ==================================================
>     Submitting job 1
>     Submitting job 2
>     Submitting job 3
>     Submitting job 4
>     Shutting down...
>     Waiting for job to complete.
>     Starting job: temp.Temp$StuckJob at 140c281
>     Finished job: temp.Temp$StuckJob at 140c281
>     Waiting for job to complete.
>     Starting job: temp.Temp$StuckJob at a1d1f4
>     Finished job: temp.Temp$StuckJob at a1d1f4
>     Waiting for job to complete.
>     Starting job: temp.Temp$StuckJob at 1df280b
>     Finished job: temp.Temp$StuckJob at 1df280b
>     Waiting for job to complete.
>     Starting job: temp.Temp$StuckJob at 1be0f0a
>     Finished job: temp.Temp$StuckJob at 1be0f0a
>     Shut down completed.
> 
>     REPRODUCIBILITY :
>     This bug can be reproduced always.
> 
>     ---------- BEGIN SOURCE ----------
>     public class Temp {
>        public static void main(String[] args) throws ExecutionException,
>     InterruptedException {
>            ThreadPoolExecutor tpe = new ThreadPoolExecutor(0, 512,
>                    3 * 60, TimeUnit.SECONDS,
>                    new LinkedBlockingQueue<Runnable>(),
>                    new SimpleThreadFactory("test"));
> 
>            LinkedList<Future> futures = new LinkedList<Future>();
> 
>            System.out.println("Submitting job 1");
>            futures.add(tpe.submit(new StuckJob()));
> 
>            System.out.println("Submitting job 2");
>            futures.add(tpe.submit(new StuckJob()));
> 
>            System.out.println("Submitting job 3");
>            futures.add(tpe.submit(new StuckJob()));
> 
>            System.out.println("Submitting job 4");
>            futures.add(tpe.submit(new StuckJob()));
> 
>            System.out.println("Shutting down...");
> 
>            for (Future future : futures) {
>                System.out.println("Waiting for job to complete.");
>                future.get();
>            }
> 
>            tpe.shutdown();
>            System.out.println("Shut down completed.");
>        }
> 
>        public static class StuckJob implements Runnable {
>            public void run() {
>                try {
>                    System.out.println("Starting job: " + this);
>                    Thread.sleep(5000);
>                    System.out.println("Finished job: " + this);
>                }
>                catch (InterruptedException e) {
>                    e.printStackTrace();
>                }
>            }
>        }
>     }
> 
>     ---------- END SOURCE ----------
> 
>     Workaround:
>     Set the corePoolSize to be equal to the maximumPoolSize. But this is
>     scary because if the pool ever reaches its max limit then all those
>     threads will just sit there instead of shutting down after the idle
>     time out.
> 
>     Ashwin (http://javaforu.blogspot.com)
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From david.lloyd at redhat.com  Thu Apr 23 19:03:19 2009
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Thu, 23 Apr 2009 18:03:19 -0500
Subject: [concurrency-interest] Atomic array field updater
Message-ID: <49F0F3B7.4090406@redhat.com>

Unsatisfied with some of the limitations of CopyOnWriteArray* (and 
indirectly reminded by Sam Berlin), I thought I'd share a class that I 
wrote which I use to efficiently manage volatile array fields with 
immutable array values.

http://anonsvn.jboss.org/repos/jbossas/projects/jboss-threads/trunk/main/src/main/java/org/jboss/threads/AtomicArray.java

The precondition is that any array that you store in the field has to be 
immutable for most of these methods to work.  Basically it's an enhanced 
AtomicReferenceFieldUpdater that provides operations to atomically read and 
update array fields with copy-on-write semantics using basic add, remove, 
find, sort, and other operations.

It has a few advantages over CopyOnWriteArrayList/CopyOnWriteArraySet:

- Only one instance of AtomicArray per target class, rather than one per 
instance (similar to how AtomicReferenceFieldUpdater compares to 
AtomicReference).
- You can take a very efficient read-only snapshot of the value by simply 
copying the array reference.  (a snapshot with CopyOnWriteArray* involves 
calling clone(), which also clones the write lock which is not useful in 
most (?) snapshot situations)
- This class supports maintaining a sorted array with O(log N) lookup time 
using Arrays.binarySearch().  Thus it could be used to implement a 
copy-on-write sorted set.

Disadvantages:

- Some discipline is necessary to ensure that the array values are never 
mutated.
- All modification of the array field should happen through the updater 
instance.

Neither here nor there:

- There's a small optimization that avoids multiple copies of empty arrays 
from hanging around (it keeps one empty array instance for that array type).
- No locking is used on write.  If a write fails (cannot be completed 
atomically), it's retried.  In some situations this could cause excessive 
spinning, so the user would have to implement write locking on their own in 
this case.
- An AtomicReferenceFieldUpdater is used to update the field; its 
performance characteristics are not well understood (by me).  In any case, 
this class is best used in situations where the array is seldom modified 
(similar to COWA*) because every modification requires a whole copy of the 
original array.

Some other stuff could be added to this class as well.  What I've put in 
here is just stuff I've used and found to be useful.

Let me know what you guys think.  It's under LGPL but that is open to 
negotiation if it looks useful enough to actually include in the JDK/JSR166 
stuff in some form (though I doubt it because it's arguably redundant with 
respect to what's already in the JDK).

- DML

Note - originally sent this from my personal account (thanks Thunderbird!) 
so if it comes through twice, sorry...  Moderator, feel free to kill the 
original.

From joe.bowbeer at gmail.com  Thu Apr 23 19:53:36 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 23 Apr 2009 16:53:36 -0700
Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor
	does not execute jobs in some cases
In-Reply-To: <837c23d40904221033k5061dfa1k47b6baf10e7e9106@mail.gmail.com>
References: <837c23d40904221033k5061dfa1k47b6baf10e7e9106@mail.gmail.com>
Message-ID: <31f2a7bd0904231653j105d741fj50e48f11852ecbb5@mail.gmail.com>

On Wed, Apr 22, 2009 at 10:33 AM, Ashwin Jayaprakash wrote:

>
> Workaround:
> Set the corePoolSize to be equal to the maximumPoolSize. But this is scary
> because if the pool ever reaches its max limit then all those threads will
> just sit there instead of shutting down after the idle time out.
>
>
You may be able to tune your workaround and reduce fear of lingering threads
by using the methods:

  allowCoreThreadTimeOut(boolean value)
  setKeepAliveTime(long time, TimeUnit unit)

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090423/2d4c4ad5/attachment.html>

From davidcholmes at aapt.net.au  Thu Apr 23 20:39:16 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 24 Apr 2009 10:39:16 +1000
Subject: [concurrency-interest] CopyOnWriteArrayList.setAll?
In-Reply-To: <CCC4AFE9-E96E-40EC-9BB6-024633E65FAF@atlassian.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGECCIBAA.davidcholmes@aapt.net.au>

> Perhaps Doug can explain why this backwards incompatible change was
> made?

The synchronization mechanism is an internal implementation detail not part
of the specification. None of the j.u.c utilities support a means to create
extenerally defined atomic operations.

The change was made for potential performance reasons, and a number of
JSR-166 members had argued for a consistent approach to using j.u.c
utilities within j.u.c ("eat our own dog food").

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jed
> Wesley-Smith
> Sent: Friday, 24 April 2009 8:22 AM
> To: Sam Berlin
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] CopyOnWriteArrayList.setAll?
>
>
>
> Interesting. In the Java5 version the modifications are protected by
> COWArrayList's monitor - synchronized(this) - so you simply need to
> acquire that object's monitor - synchronized(cowList) and and  call
> clear(), addAll().
>
> In later versions however it has been changed to use a private
> ReentrantLock - so that approach will break. There doesn't appear to
> be any way to access that lock externally or from a sub-class.
>
> Perhaps Doug can explain why this backwards incompatible change was
> made?
>
> cheers,
> jed.
>
> On 24/04/2009, at 7:45 AM, Sam Berlin wrote:
>
> > Hi Folks,
> >
> > Is there a way to emulate a CoW "set all" with
> > CopyOnWriteArrayList?  The goal is to make it impossible for someone
> > to view the list in a state except for where all the new items are
> > added and all the old ones are removed.
> >
> > It doesn't seem like it's possible (without restoring to blocking &
> > copying the data within the locks), but I may be missing something.
> > If I'm not missing something, would setAll be a good candidate for
> > inclusion into CoWArrayList?
> >
> > Sam
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From davidcholmes at aapt.net.au  Thu Apr 23 20:53:22 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 24 Apr 2009 10:53:22 +1000
Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor
	doesnot execute jobs in some cases
In-Reply-To: <49F0EF98.8040402@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAECDIBAA.davidcholmes@aapt.net.au>

Gregg,

With the right choices of corePoolSize, maxPoolSize, maximum queue length,
and the idle timeouts (for core and non-core threads) you can achieve a vast
range of policies - not all of course, but many. For example, if you want
aggressive thread creation then increase the core size and/or reduce the
queue size - and if I recall correctly using a SynchronousQueue effectively
removes the queue and allows you to grow to maxCoreSize without buffering.

The main complaint with current TPE is that it creates threads too
aggressively - preferring to create a core thread rather than hand-off to an
idle core thread. This is being addressed for JDK7.

The overall strategy is all based on queuing theory and expected workload
given the nature of tasks and their expected arrival rates etc:
- core threads represent the steady-state number of threads needed to
maintain response times to an acceptable level given the expected workload
- the queue allows buffering of work under transient overload conditions
- the expansion to maxPoolSize when the queue is full is a recovery attempt
to deal with excessive overload
- rejected execution is the ultimate overload defence

Cheers,
David Holmes

> -----Original Message-----
> From: Gregg Wonderly [mailto:gregg at cytetech.com]
> Sent: Friday, 24 April 2009 8:46 AM
> To: dholmes at ieee.org
> Cc: Ashwin Jayaprakash; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest]
> java.util.concurrent.ThreadPoolExecutor doesnot execute jobs in some
> cases
>
>
>
> For high latency threads, this strategy doesn't allow aggressive
> thread creation
> to occur to try and make sure things work as responsively as
> possible.  All of
> the thread pools that I've ever created have always aimed at
> maxPoolSize until
> all threads were running, and corePoolSize would just be the
> minimum threads
> that we'd fall back to in an idle state.
>
> For network applications where there are database's and server's
> latency to deal
> with, a very aggressive thread creation strategy has always helped me to
> minimize latency.  For CPU bound things of short life, the
> current TPE strategy
> will work, but still favors some delays that don't have to exist,
> and create
> queue overflow situations more often than if maxPoolSize was
> aimed at always.
>
> Gregg Wonderly
>
> David Holmes wrote:
> > I don't think the issue here is "corePoolSize is less than
> > maximumPoolSize" I think this is a particular issue with
> > corePoolSize==0, and looking at the current JDK7 code I'm pretty sure
> > this has been addressed so there is always at least one thread even if
> > corePoolSize is zero.
> >
> > Remember the way this is supposed to work is that on a submission the
> > executor creates a thread if there are less than corePoolSize threads,
> > else the task is queued. If the queue is bounded and is full then a
> > thread is again created, provided the number of threads is less than
> > maximumPoolSize.
> >
> > So unless you use a bounded queue that gets full, you will not see the
> > number of threads grow from corePoolSizse to mnaxPoolSize.
> >
> > David Holmes
> >
> >     -----Original Message-----
> >     *From:* concurrency-interest-bounces at cs.oswego.edu
> >     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
> >     *Ashwin Jayaprakash
> >     *Sent:* Thursday, 23 April 2009 3:33 AM
> >     *To:* concurrency-interest at cs.oswego.edu
> >     *Subject:* [concurrency-interest]
> >     java.util.concurrent.ThreadPoolExecutor doesnot execute jobs in some
> >     cases
> >
> >     Hi, I've raised a bug in the Sun Bug Database. It's still under
> >     review. But here it is:
> >
> >     java.util.concurrent.ThreadPoolExecutor does not execute jobs in
> >     some cases.
> >
> >     If the corePoolSize if less than the maximumPoolSize, then the
> >     thread pool just does not execute the submitted jobs. The jobs just
> >     sit there.
> >
> >     If the corePoolSize is set to 1 instead of 0, then only 1 thread
> >     executes the jobs sequentially.
> >
> >     ==================================================
> >     When the corePoolSize is 0:
> >     ==================================================
> >
> >     Submitting job 1
> >     Submitting job 2
> >     Submitting job 3
> >     Submitting job 4
> >     Shutting down...
> >     Waiting for job to complete.
> >
> >     (Program never exits)
> >
> >
> >     ==================================================
> >     When the corePoolSize is 1:
> >     ==================================================
> >     Submitting job 1
> >     Submitting job 2
> >     Submitting job 3
> >     Submitting job 4
> >     Shutting down...
> >     Waiting for job to complete.
> >     Starting job: temp.Temp$StuckJob at 140c281
> >     Finished job: temp.Temp$StuckJob at 140c281
> >     Waiting for job to complete.
> >     Starting job: temp.Temp$StuckJob at a1d1f4
> >     Finished job: temp.Temp$StuckJob at a1d1f4
> >     Waiting for job to complete.
> >     Starting job: temp.Temp$StuckJob at 1df280b
> >     Finished job: temp.Temp$StuckJob at 1df280b
> >     Waiting for job to complete.
> >     Starting job: temp.Temp$StuckJob at 1be0f0a
> >     Finished job: temp.Temp$StuckJob at 1be0f0a
> >     Shut down completed.
> >
> >     REPRODUCIBILITY :
> >     This bug can be reproduced always.
> >
> >     ---------- BEGIN SOURCE ----------
> >     public class Temp {
> >        public static void main(String[] args) throws ExecutionException,
> >     InterruptedException {
> >            ThreadPoolExecutor tpe = new ThreadPoolExecutor(0, 512,
> >                    3 * 60, TimeUnit.SECONDS,
> >                    new LinkedBlockingQueue<Runnable>(),
> >                    new SimpleThreadFactory("test"));
> >
> >            LinkedList<Future> futures = new LinkedList<Future>();
> >
> >            System.out.println("Submitting job 1");
> >            futures.add(tpe.submit(new StuckJob()));
> >
> >            System.out.println("Submitting job 2");
> >            futures.add(tpe.submit(new StuckJob()));
> >
> >            System.out.println("Submitting job 3");
> >            futures.add(tpe.submit(new StuckJob()));
> >
> >            System.out.println("Submitting job 4");
> >            futures.add(tpe.submit(new StuckJob()));
> >
> >            System.out.println("Shutting down...");
> >
> >            for (Future future : futures) {
> >                System.out.println("Waiting for job to complete.");
> >                future.get();
> >            }
> >
> >            tpe.shutdown();
> >            System.out.println("Shut down completed.");
> >        }
> >
> >        public static class StuckJob implements Runnable {
> >            public void run() {
> >                try {
> >                    System.out.println("Starting job: " + this);
> >                    Thread.sleep(5000);
> >                    System.out.println("Finished job: " + this);
> >                }
> >                catch (InterruptedException e) {
> >                    e.printStackTrace();
> >                }
> >            }
> >        }
> >     }
> >
> >     ---------- END SOURCE ----------
> >
> >     Workaround:
> >     Set the corePoolSize to be equal to the maximumPoolSize. But this is
> >     scary because if the pool ever reaches its max limit then all those
> >     threads will just sit there instead of shutting down after the idle
> >     time out.
> >
> >     Ashwin (http://javaforu.blogspot.com)
> >
> >
> > ------------------------------------------------------------------------
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



From jed at atlassian.com  Thu Apr 23 20:59:27 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Fri, 24 Apr 2009 10:59:27 +1000
Subject: [concurrency-interest] CopyOnWriteArrayList.setAll?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGECCIBAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGECCIBAA.davidcholmes@aapt.net.au>
Message-ID: <49F10EEF.6090300@atlassian.com>

Fair enough, I guess changing Thread.nextThreadNum() to not use the 
class as a lock would be backwards incompatible change too under my 
definition :-)

I implemented a CopyOnWriteMap* using synchronized to be consistent with 
COWArrayList but I will change this now.

cheers,
jed.

* 
http://labs.atlassian.com/source/browse/CONCURRENT/trunk/src/main/java/com/atlassian/util/concurrent/AbstractCopyOnWriteMap.java?r=1406

David Holmes wrote:
>> Perhaps Doug can explain why this backwards incompatible change was
>> made?
>>     
>
> The synchronization mechanism is an internal implementation detail not part
> of the specification. None of the j.u.c utilities support a means to create
> extenerally defined atomic operations.
>
> The change was made for potential performance reasons, and a number of
> JSR-166 members had argued for a consistent approach to using j.u.c
> utilities within j.u.c ("eat our own dog food").
>
> Cheers,
> David Holmes
>
>   
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jed
>> Wesley-Smith
>> Sent: Friday, 24 April 2009 8:22 AM
>> To: Sam Berlin
>> Cc: concurrency-interest
>> Subject: Re: [concurrency-interest] CopyOnWriteArrayList.setAll?
>>
>>
>>
>> Interesting. In the Java5 version the modifications are protected by
>> COWArrayList's monitor - synchronized(this) - so you simply need to
>> acquire that object's monitor - synchronized(cowList) and and  call
>> clear(), addAll().
>>
>> In later versions however it has been changed to use a private
>> ReentrantLock - so that approach will break. There doesn't appear to
>> be any way to access that lock externally or from a sub-class.
>>
>> Perhaps Doug can explain why this backwards incompatible change was
>> made?
>>
>> cheers,
>> jed.
>>
>> On 24/04/2009, at 7:45 AM, Sam Berlin wrote:
>>
>>     
>>> Hi Folks,
>>>
>>> Is there a way to emulate a CoW "set all" with
>>> CopyOnWriteArrayList?  The goal is to make it impossible for someone
>>> to view the list in a state except for where all the new items are
>>> added and all the old ones are removed.
>>>
>>> It doesn't seem like it's possible (without restoring to blocking &
>>> copying the data within the locks), but I may be missing something.
>>> If I'm not missing something, would setAll be a good candidate for
>>> inclusion into CoWArrayList?
>>>
>>> Sam
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>       
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>     
>
>
>   


From gregg at cytetech.com  Thu Apr 23 21:46:28 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 23 Apr 2009 20:46:28 -0500
Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor
 doesnot execute jobs in some cases
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAECDIBAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCAECDIBAA.davidcholmes@aapt.net.au>
Message-ID: <49F119F4.8080604@cytetech.com>

David Holmes wrote:
> Gregg,
> 
> With the right choices of corePoolSize, maxPoolSize, maximum queue length,
> and the idle timeouts (for core and non-core threads) you can achieve a vast
> range of policies - not all of course, but many. For example, if you want
> aggressive thread creation then increase the core size and/or reduce the
> queue size - and if I recall correctly using a SynchronousQueue effectively
> removes the queue and allows you to grow to maxCoreSize without buffering.

The current strategy differs overload handling until the queue is full.  The 
purpose of the queue should be to differ work that is beyond maxPoolSize it 
seems to me.  Having a fixed number of threads (corePoolSize) ready to run and 
handle inflow that is normal, is and okay concept.  If new threads are created 
early, instead of waiting for the queue to overflow, then the application would 
not see a huge work load dumped on the available threads as it does now.

There is some time always required for the threads to run to completion. 
Overlapping that time with available bandwidth (network, CPU or other I/O) 
really is the best choice, or we're all working on multi-thread apps for no 
reason.  Delaying execution for any length of time is always going to be a 
direct impetus for a queue overflow on the next addition to the executor.

When the queue overflows and all the threads are started, you have even more 
latency injected as they all fight for locks and resources that they all 
typically share, so the startup latency will be amplified into a sequential 
delay which is often quite visible.

The current strategy really does favor late response to an already bad 
situation. Instead, I believe a thread pool should be working hard to avoid a 
bad situation by running all threads that can be run as soon as possible, and 
then enqueuing the work that is truly overflow work.  I.e. if it make sense to 
run it in a thread pool, in parallel at all, then starting sooner, rather than 
later, can only help manage resource usage for optimal results.

If you pick too large of a maxPoolSize, than you can see a problem.  I'm 
guessing that most people really don't understand this behavior of the TPE, and 
so instead of increasing corePoolSize to get better responsivity, they increase 
maxPoolSize, and everytime the queue overflows, they see the life sucked out of 
their application as way too many threads end up running.

Gregg Wonderly

> The main complaint with current TPE is that it creates threads too
> aggressively - preferring to create a core thread rather than hand-off to an
> idle core thread. This is being addressed for JDK7.
> 
> The overall strategy is all based on queuing theory and expected workload
> given the nature of tasks and their expected arrival rates etc:
> - core threads represent the steady-state number of threads needed to
> maintain response times to an acceptable level given the expected workload
> - the queue allows buffering of work under transient overload conditions
> - the expansion to maxPoolSize when the queue is full is a recovery attempt
> to deal with excessive overload
> - rejected execution is the ultimate overload defence
> 
> Cheers,
> David Holmes
> 
>> -----Original Message-----
>> From: Gregg Wonderly [mailto:gregg at cytetech.com]
>> Sent: Friday, 24 April 2009 8:46 AM
>> To: dholmes at ieee.org
>> Cc: Ashwin Jayaprakash; concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest]
>> java.util.concurrent.ThreadPoolExecutor doesnot execute jobs in some
>> cases
>>
>>
>>
>> For high latency threads, this strategy doesn't allow aggressive
>> thread creation
>> to occur to try and make sure things work as responsively as
>> possible.  All of
>> the thread pools that I've ever created have always aimed at
>> maxPoolSize until
>> all threads were running, and corePoolSize would just be the
>> minimum threads
>> that we'd fall back to in an idle state.
>>
>> For network applications where there are database's and server's
>> latency to deal
>> with, a very aggressive thread creation strategy has always helped me to
>> minimize latency.  For CPU bound things of short life, the
>> current TPE strategy
>> will work, but still favors some delays that don't have to exist,
>> and create
>> queue overflow situations more often than if maxPoolSize was
>> aimed at always.
>>
>> Gregg Wonderly
>>
>> David Holmes wrote:
>>> I don't think the issue here is "corePoolSize is less than
>>> maximumPoolSize" I think this is a particular issue with
>>> corePoolSize==0, and looking at the current JDK7 code I'm pretty sure
>>> this has been addressed so there is always at least one thread even if
>>> corePoolSize is zero.
>>>
>>> Remember the way this is supposed to work is that on a submission the
>>> executor creates a thread if there are less than corePoolSize threads,
>>> else the task is queued. If the queue is bounded and is full then a
>>> thread is again created, provided the number of threads is less than
>>> maximumPoolSize.
>>>
>>> So unless you use a bounded queue that gets full, you will not see the
>>> number of threads grow from corePoolSizse to mnaxPoolSize.
>>>
>>> David Holmes
>>>
>>>     -----Original Message-----
>>>     *From:* concurrency-interest-bounces at cs.oswego.edu
>>>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>>>     *Ashwin Jayaprakash
>>>     *Sent:* Thursday, 23 April 2009 3:33 AM
>>>     *To:* concurrency-interest at cs.oswego.edu
>>>     *Subject:* [concurrency-interest]
>>>     java.util.concurrent.ThreadPoolExecutor doesnot execute jobs in some
>>>     cases
>>>
>>>     Hi, I've raised a bug in the Sun Bug Database. It's still under
>>>     review. But here it is:
>>>
>>>     java.util.concurrent.ThreadPoolExecutor does not execute jobs in
>>>     some cases.
>>>
>>>     If the corePoolSize if less than the maximumPoolSize, then the
>>>     thread pool just does not execute the submitted jobs. The jobs just
>>>     sit there.
>>>
>>>     If the corePoolSize is set to 1 instead of 0, then only 1 thread
>>>     executes the jobs sequentially.
>>>
>>>     ==================================================
>>>     When the corePoolSize is 0:
>>>     ==================================================
>>>
>>>     Submitting job 1
>>>     Submitting job 2
>>>     Submitting job 3
>>>     Submitting job 4
>>>     Shutting down...
>>>     Waiting for job to complete.
>>>
>>>     (Program never exits)
>>>
>>>
>>>     ==================================================
>>>     When the corePoolSize is 1:
>>>     ==================================================
>>>     Submitting job 1
>>>     Submitting job 2
>>>     Submitting job 3
>>>     Submitting job 4
>>>     Shutting down...
>>>     Waiting for job to complete.
>>>     Starting job: temp.Temp$StuckJob at 140c281
>>>     Finished job: temp.Temp$StuckJob at 140c281
>>>     Waiting for job to complete.
>>>     Starting job: temp.Temp$StuckJob at a1d1f4
>>>     Finished job: temp.Temp$StuckJob at a1d1f4
>>>     Waiting for job to complete.
>>>     Starting job: temp.Temp$StuckJob at 1df280b
>>>     Finished job: temp.Temp$StuckJob at 1df280b
>>>     Waiting for job to complete.
>>>     Starting job: temp.Temp$StuckJob at 1be0f0a
>>>     Finished job: temp.Temp$StuckJob at 1be0f0a
>>>     Shut down completed.
>>>
>>>     REPRODUCIBILITY :
>>>     This bug can be reproduced always.
>>>
>>>     ---------- BEGIN SOURCE ----------
>>>     public class Temp {
>>>        public static void main(String[] args) throws ExecutionException,
>>>     InterruptedException {
>>>            ThreadPoolExecutor tpe = new ThreadPoolExecutor(0, 512,
>>>                    3 * 60, TimeUnit.SECONDS,
>>>                    new LinkedBlockingQueue<Runnable>(),
>>>                    new SimpleThreadFactory("test"));
>>>
>>>            LinkedList<Future> futures = new LinkedList<Future>();
>>>
>>>            System.out.println("Submitting job 1");
>>>            futures.add(tpe.submit(new StuckJob()));
>>>
>>>            System.out.println("Submitting job 2");
>>>            futures.add(tpe.submit(new StuckJob()));
>>>
>>>            System.out.println("Submitting job 3");
>>>            futures.add(tpe.submit(new StuckJob()));
>>>
>>>            System.out.println("Submitting job 4");
>>>            futures.add(tpe.submit(new StuckJob()));
>>>
>>>            System.out.println("Shutting down...");
>>>
>>>            for (Future future : futures) {
>>>                System.out.println("Waiting for job to complete.");
>>>                future.get();
>>>            }
>>>
>>>            tpe.shutdown();
>>>            System.out.println("Shut down completed.");
>>>        }
>>>
>>>        public static class StuckJob implements Runnable {
>>>            public void run() {
>>>                try {
>>>                    System.out.println("Starting job: " + this);
>>>                    Thread.sleep(5000);
>>>                    System.out.println("Finished job: " + this);
>>>                }
>>>                catch (InterruptedException e) {
>>>                    e.printStackTrace();
>>>                }
>>>            }
>>>        }
>>>     }
>>>
>>>     ---------- END SOURCE ----------
>>>
>>>     Workaround:
>>>     Set the corePoolSize to be equal to the maximumPoolSize. But this is
>>>     scary because if the pool ever reaches its max limit then all those
>>>     threads will just sit there instead of shutting down after the idle
>>>     time out.
>>>
>>>     Ashwin (http://javaforu.blogspot.com)
>>>
>>>
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
> 


From davidcholmes at aapt.net.au  Thu Apr 23 22:28:54 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 24 Apr 2009 12:28:54 +1000
Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor
	doesnot execute jobs in some cases
In-Reply-To: <49F119F4.8080604@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMECEIBAA.davidcholmes@aapt.net.au>

Gregg Wonderly writes:
> The current strategy differs overload handling until the queue is
> full.

Yes. As I explained the queue is to buffer transient overflow; the increase
to max threads is to try and recover from excessive overflow. The queue
being full is the signal that the overload is now excessive and additional
threads are needed to try and recover. (Whether they will or not depends on
whether the arrival rate drops back to normal levels).

But throwing more threads at the problem only gets more jobs "in progress"
it doesn't mean that they will necessarily complete in a reasonable or
expected time. As you state, by throwing in more threads you can simply
create contention, both in the TPE/Queue and in the application code that
gets executed. Hence the programmer has to choose that this is the right
thing by setting core, max and the queue size appropriately.

> The purpose of the queue should be to differ work that is beyond
> maxPoolSize it seems to me.

Hmmm I think that is because you have a different view of what maxPoolSize
represents. It seems to me that in your view of how things should be,
corePoolSize is really minPoolSize and there is no distinction between a
core and non-core thread. So the pool lets threads get created and idle-out
between min and max, but once max is reached it starts buffering. The way
"max" operates here actually corresponds to present corePoolSize, but we
don't have a direct equivalent of minPoolSize.

That's certainly an alternative model.

> The current strategy really does favor late response to an already bad
> situation.

I don't agree.

> Instead, I believe a thread pool should be working
> hard to avoid a bad situation by running all threads that can be run
> as soon as possible, and  then enqueuing the work that is truly overflow
work.

But if the pool always ramps up to max threads before doing any buffering
then you are more likely to induce contention and additional latencies, and
cause more harm. No, let me re-state that because it depends on how you've
determined what are suitable values for core and max. In the model I
described, core is the number of threads you need to adequately handle the
expected load. If that number fully utilizes your resources then going
beyond that will be detrimental. So in that view, ramping up to max is to
use a number of threads beyond the ability of the system to handle. Of
course if you designate max as being the limit of utilization - or
equivalently core is well below the limit of the system, then continuing to
ramp up to max should (in the absence of other factors) improve things. So
it really depends on your understanding of core and max, and defining their
values appropriately. If the system can handle max threads then buffering
can certainly seem inappropriate, but then you want a next stage to handle
your "truly overflow work".

I think that your scheme can be achieved by chaining pools together. The
initial pool uses a Synchronous queue so that it effectively grows past
coreSize to maxSize. When maxSize is reached the RejectedExceutionHandler
kicks in and submits the job to an "overflow" pool. This "overflow" pool
could set a core size of zero, or one, and so effectively start buffering
straightaway.

Cheers,
David



From davidcholmes at aapt.net.au  Thu Apr 23 22:36:23 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 24 Apr 2009 12:36:23 +1000
Subject: [concurrency-interest]
	java.util.concurrent.ThreadPoolExecutordoesnot execute jobs
	in some cases
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMECEIBAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCGECFIBAA.davidcholmes@aapt.net.au>

I should probably add that Gregg's scheme could probably be realized just by
introducing a minCoreSize. (Though then you'd have to decide if
preStartCoreThreads starts minCore or core threads.)

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of David
> Holmes
> Sent: Friday, 24 April 2009 12:29 PM
> To: gregg.wonderly at pobox.com
> Cc: Ashwin Jayaprakash; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest]
> java.util.concurrent.ThreadPoolExecutordoesnot execute jobs in some
> cases
>
>
>
> Gregg Wonderly writes:
> > The current strategy differs overload handling until the queue is
> > full.
>
> Yes. As I explained the queue is to buffer transient overflow;
> the increase
> to max threads is to try and recover from excessive overflow. The queue
> being full is the signal that the overload is now excessive and additional
> threads are needed to try and recover. (Whether they will or not
> depends on
> whether the arrival rate drops back to normal levels).
>
> But throwing more threads at the problem only gets more jobs "in progress"
> it doesn't mean that they will necessarily complete in a reasonable or
> expected time. As you state, by throwing in more threads you can simply
> create contention, both in the TPE/Queue and in the application code that
> gets executed. Hence the programmer has to choose that this is the right
> thing by setting core, max and the queue size appropriately.
>
> > The purpose of the queue should be to differ work that is beyond
> > maxPoolSize it seems to me.
>
> Hmmm I think that is because you have a different view of what maxPoolSize
> represents. It seems to me that in your view of how things should be,
> corePoolSize is really minPoolSize and there is no distinction between a
> core and non-core thread. So the pool lets threads get created
> and idle-out
> between min and max, but once max is reached it starts buffering. The way
> "max" operates here actually corresponds to present corePoolSize, but we
> don't have a direct equivalent of minPoolSize.
>
> That's certainly an alternative model.
>
> > The current strategy really does favor late response to an already bad
> > situation.
>
> I don't agree.
>
> > Instead, I believe a thread pool should be working
> > hard to avoid a bad situation by running all threads that can be run
> > as soon as possible, and  then enqueuing the work that is truly overflow
> work.
>
> But if the pool always ramps up to max threads before doing any buffering
> then you are more likely to induce contention and additional
> latencies, and
> cause more harm. No, let me re-state that because it depends on how you've
> determined what are suitable values for core and max. In the model I
> described, core is the number of threads you need to adequately handle the
> expected load. If that number fully utilizes your resources then going
> beyond that will be detrimental. So in that view, ramping up to max is to
> use a number of threads beyond the ability of the system to handle. Of
> course if you designate max as being the limit of utilization - or
> equivalently core is well below the limit of the system, then
> continuing to
> ramp up to max should (in the absence of other factors) improve things. So
> it really depends on your understanding of core and max, and
> defining their
> values appropriately. If the system can handle max threads then buffering
> can certainly seem inappropriate, but then you want a next stage to handle
> your "truly overflow work".
>
> I think that your scheme can be achieved by chaining pools together. The
> initial pool uses a Synchronous queue so that it effectively grows past
> coreSize to maxSize. When maxSize is reached the RejectedExceutionHandler
> kicks in and submits the job to an "overflow" pool. This "overflow" pool
> could set a core size of zero, or one, and so effectively start buffering
> straightaway.
>
> Cheers,
> David
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From gregg at cytetech.com  Fri Apr 24 09:54:22 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 24 Apr 2009 08:54:22 -0500
Subject: [concurrency-interest] java.util.concurrent.ThreadPoolExecutor
 doesnot execute jobs in some cases
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMECEIBAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMECEIBAA.davidcholmes@aapt.net.au>
Message-ID: <49F1C48E.6000607@cytetech.com>

David Holmes wrote:
> Gregg Wonderly writes:
>> The current strategy differs overload handling until the queue is
>> full.
> 
> Yes. As I explained the queue is to buffer transient overflow; the increase
> to max threads is to try and recover from excessive overflow. The queue
> being full is the signal that the overload is now excessive and additional
> threads are needed to try and recover. (Whether they will or not depends on
> whether the arrival rate drops back to normal levels).
> 
> But throwing more threads at the problem only gets more jobs "in progress"
> it doesn't mean that they will necessarily complete in a reasonable or
> expected time. As you state, by throwing in more threads you can simply
> create contention, both in the TPE/Queue and in the application code that
> gets executed. Hence the programmer has to choose that this is the right
> thing by setting core, max and the queue size appropriately.
> 
>> The purpose of the queue should be to differ work that is beyond
>> maxPoolSize it seems to me.
> 
> Hmmm I think that is because you have a different view of what maxPoolSize
> represents. It seems to me that in your view of how things should be,
> corePoolSize is really minPoolSize and there is no distinction between a
> core and non-core thread. So the pool lets threads get created and idle-out
> between min and max, but once max is reached it starts buffering. The way
> "max" operates here actually corresponds to present corePoolSize, but we
> don't have a direct equivalent of minPoolSize.
> 
> That's certainly an alternative model.
> 
>> The current strategy really does favor late response to an already bad
>> situation.
> 
> I don't agree.
> 
>> Instead, I believe a thread pool should be working
>> hard to avoid a bad situation by running all threads that can be run
>> as soon as possible, and  then enqueuing the work that is truly overflow
> work.
> 
> But if the pool always ramps up to max threads before doing any buffering
> then you are more likely to induce contention and additional latencies, and
> cause more harm.

I think that you might be saying this because you are thinking of thread pools 
as a solution for mostly CPU bound tasks, whereas in my world, most of the time 
I am using them to parallelize latency on largely long running (relative to the 
time it takes to start a thread) high latency operations such as interactions 
with databases, and over the network trips to other parts of the systems.

In the case of CPU bound, then you want maxPoolSize to be the limit on available 
CPU resources on the local machine, and corePoolSize to represent your best 
guess at what it takes to get the normal work load done.  Instead, I have 100 
thread pools talking to 10s of different databases, and I need the minimum 
number of threads to be running as is needed to get the job done.

The maxPoolSize value would represent the loading factor that the remote system 
can take to handle the work load that I am distributing to it.

This is largely why I don't use TPE for these kinds of tasks, and instead use 
the ones that I've created over the years that aggressively create threads to 
get work started.  The tasks never come in "together", but do come in "fast 
enough" (25/second) that I need to dispatch them ASAP or things degrade because 
of the simultaneous contention for database resources and other things in other 
parts of the system.  When the work is not happening, I'd like for the thread 
resources to go away.

It's a different need than what TPE was targeted for I think, but I also still 
contend that if you all are hearing problems with too many threads created, that 
it is because people don't understand how corePoolSize vs maxPoolSize works.

Gregg Wonderly

From unmesh_joshi at hotmail.com  Sat Apr 25 07:58:35 2009
From: unmesh_joshi at hotmail.com (Unmesh joshi)
Date: Sat, 25 Apr 2009 11:58:35 +0000
Subject: [concurrency-interest] cancel method on Future API. Design rationale
Message-ID: <BAY140-W419B74108829BC6276F51EF730@phx.gbl>


Hi,

 

Future class in util.concurrent has cancel method. Generally when designing Active Objects, what is the design rationale behind having a Cancel operation? In particular, because there is no guaranty that the task will actually cancelled (in case its completed)

 

Thanks,

Unmesh

_________________________________________________________________
Twice the fun?Share photos while you chat with Windows Live Messenger.
http://www.microsoft.com/india/windows/windowslive/messenger.aspx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090425/41fa922b/attachment.html>

From joe.bowbeer at gmail.com  Sat Apr 25 11:29:36 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 25 Apr 2009 08:29:36 -0700
Subject: [concurrency-interest] cancel method on Future API. Design
	rationale
In-Reply-To: <BAY140-W419B74108829BC6276F51EF730@phx.gbl>
References: <BAY140-W419B74108829BC6276F51EF730@phx.gbl>
Message-ID: <31f2a7bd0904250829i51126c3aud692adbc110e37e2@mail.gmail.com>

The Future in j.u.c. is conceived as a promise to provide a value.  The
cancel method notifies it that there is no longer any interest in the value.

In practice, the Future is a handle for a runnable task submitted to an
executor.  Future is usually an instance of FutureTask, and in practice the
cancel method means "stop the task".  The mayInterrupt flag is a bit of
pragmatism leaking through, since effective cancellation may or may not
require that the running thread be interrupted.

I don't think of a Future as an Active Object.  I think of it as a two-way
message sent to an ActiveObject; cancel cancels the reply.

Joe

On Sat, Apr 25, 2009 at 4:58 AM, Unmesh joshi wrote:

>  Hi,
>
> Future class in util.concurrent has cancel method. Generally when designing
> Active Objects, what is the design rationale behind having a Cancel
> operation? In particular, because there is no guaranty that the task will
> actually cancelled (in case its completed)
>
> Thanks,
> Unmesh
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090425/88c75467/attachment.html>

From vishal.byakod at gmail.com  Mon Apr 27 06:47:49 2009
From: vishal.byakod at gmail.com (Vishal Byakod)
Date: Mon, 27 Apr 2009 16:17:49 +0530
Subject: [concurrency-interest] Performance improvement using
	ThreadPoolExecutor
Message-ID: <ccb85ce20904270347p6780963h9911911d7221373b@mail.gmail.com>

I am writing an API( getObject()) which returns a java object from a
cluster. The cluster can have multiple nodes and each node contains
only and only one persistent queue. This API getObject()  needs to
return one object from any one of the queues in the fastest possible
time. The API is a part of a client runtime library which executes on
the client JVM

The client program will call getObject() and block until there is an
object available.
In the getObject() implementation I maintain a ThreadPoolExecutor and
in turn submit a callable thread to fetch queued objects from each of
the cluster nodes. So if we have 3 nodes in the cluster, getObject()
submits 3 callables to the executor. The task of fetching the objects
from each cluster node is completely independent of others,  and for
each node in the cluster I maintain a local in memory queue.

Each callable after fetching the objects from the node, puts the data
in its local queue. I need to return back an object the  moment any of
the local queues are non empty.


I have come up with the foll design, let me know if there areother
ways to improve the performance using any of the concurrency API's
Earlier we were spawning one thread for each cluster node to fetch
messages.


Object obj = null;
nonEmptyQueues		

//API Implementation
protected final Object getObject() throws Exception {
	
	// check if the local queues for all the nodes are empty.
	if(nonEmptyQueues.size() == 0){
	     for (Iterator nodesList = nodes.iterator(); nodesList.hasNext();){
	     node = (Node)nodesList.next();
	     NodeWorker callable = new NodeWorker();
	    executor.submit(callable);
	 }
		
          return nonEmptyQueues.get(0).receive();
	}


/**
   * Each instance of this class is responsible for getting objects
from a physical queue on one node.
   */
	  private class NodeWorker implements Callable {

	    public Boolean call() {
	          try {
	            ClusterNode.fetchEvents();
	            // wait for objects to be fetched
	            // this call retrives more than 30 objects if available from the
	            // cluster node
	            boolean gotMessages = this.fetchMessages(timeout);
	            nonEmptyQueues.add(this);
	
	          }catch (Exception e){
	
	          }
	          return gotMessages;
	    }
	  }

From vishal.byakod at gmail.com  Mon Apr 27 09:58:21 2009
From: vishal.byakod at gmail.com (Vishal Byakod)
Date: Mon, 27 Apr 2009 19:28:21 +0530
Subject: [concurrency-interest] ConcurrentLinkedQueue implementation
Message-ID: <ccb85ce20904270658u3720d856q3d1616fb7c9a8ff0@mail.gmail.com>

Hi,

I want to replace java.util.LinkedList with the ConcurrentLinkedQueue
implementation, but am not sure what to use for LinkedList.addFirst()
and LinkedList removeFirst(). ConcurrentLinkedQueue does?nt seem to
have equivalent API?s.

Can anyone suggest what other options I have.

Thanks
Vishal


From martinrb at google.com  Mon Apr 27 10:43:32 2009
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 27 Apr 2009 07:43:32 -0700
Subject: [concurrency-interest] ConcurrentLinkedQueue implementation
In-Reply-To: <ccb85ce20904270658u3720d856q3d1616fb7c9a8ff0@mail.gmail.com>
References: <ccb85ce20904270658u3720d856q3d1616fb7c9a8ff0@mail.gmail.com>
Message-ID: <1ccfd1c10904270743k6fefa570k43c0c68bd8133ae2@mail.gmail.com>

LinkedList is a Deque, but ConcurrentLinkedQueue intentionally is not.
You could try LinkedBlockingDeque instead.

Martin

On Mon, Apr 27, 2009 at 06:58, Vishal Byakod <vishal.byakod at gmail.com> wrote:
> Hi,
>
> I want to replace java.util.LinkedList with the ConcurrentLinkedQueue
> implementation, but am not sure what to use for LinkedList.addFirst()
> and LinkedList removeFirst(). ConcurrentLinkedQueue does?nt seem to
> have equivalent API?s.
>
> Can anyone suggest what other options I have.
>
> Thanks
> Vishal
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From vishal.byakod at gmail.com  Mon Apr 27 11:15:57 2009
From: vishal.byakod at gmail.com (Vishal Byakod)
Date: Mon, 27 Apr 2009 20:45:57 +0530
Subject: [concurrency-interest] ConcurrentLinkedQueue implementation
In-Reply-To: <1ccfd1c10904270743k6fefa570k43c0c68bd8133ae2@mail.gmail.com>
References: <ccb85ce20904270658u3720d856q3d1616fb7c9a8ff0@mail.gmail.com>
	<1ccfd1c10904270743k6fefa570k43c0c68bd8133ae2@mail.gmail.com>
Message-ID: <ccb85ce20904270815u2cb2f915s79976d3bb718ca06@mail.gmail.com>

Martin,
I need an API to insert an object at the beginning of a list and also
one which can remove the first element from a list( just as  LinkedList
removeFirst() &  LinkedList addFirst(). LinkedBlockingQueue has API's
to insert at the end and retreive from the head. The collection also
has to be highly performant under concurrent access.

Can you think of anything else?

Thanks
Vishal

On Mon, Apr 27, 2009 at 8:13 PM, Martin Buchholz <martinrb at google.com> wrote:
> LinkedList is a Deque, but ConcurrentLinkedQueue intentionally is not.
> You could try LinkedBlockingDeque instead.
>
> Martin
>
> On Mon, Apr 27, 2009 at 06:58, Vishal Byakod <vishal.byakod at gmail.com> wrote:
>> Hi,
>>
>> I want to replace java.util.LinkedList with the ConcurrentLinkedQueue
>> implementation, but am not sure what to use for LinkedList.addFirst()
>> and LinkedList removeFirst(). ConcurrentLinkedQueue does?nt seem to
>> have equivalent API?s.
>>
>> Can anyone suggest what other options I have.
>>
>> Thanks
>> Vishal
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>


From bronee at gmail.com  Mon Apr 27 11:56:14 2009
From: bronee at gmail.com (Brian S O'Neill)
Date: Mon, 27 Apr 2009 08:56:14 -0700
Subject: [concurrency-interest] ConcurrentLinkedQueue implementation
In-Reply-To: <ccb85ce20904270815u2cb2f915s79976d3bb718ca06@mail.gmail.com>
References: <ccb85ce20904270658u3720d856q3d1616fb7c9a8ff0@mail.gmail.com>	<1ccfd1c10904270743k6fefa570k43c0c68bd8133ae2@mail.gmail.com>
	<ccb85ce20904270815u2cb2f915s79976d3bb718ca06@mail.gmail.com>
Message-ID: <49F5D59E.7050400@gmail.com>

Please re-read Martin's response. Note the use of the word "Deque" 
instead of "Queue". Also, if you're looking for a ConcurrentLinkedDeque 
implementation, none is provided in the JDK.

Vishal Byakod wrote:
> Martin,
> I need an API to insert an object at the beginning of a list and also
> one which can remove the first element from a list( just as  LinkedList
> removeFirst() &  LinkedList addFirst(). LinkedBlockingQueue has API's
> to insert at the end and retreive from the head. The collection also
> has to be highly performant under concurrent access.
>
> Can you think of anything else?
>
> Thanks
> Vishal
>
> On Mon, Apr 27, 2009 at 8:13 PM, Martin Buchholz <martinrb at google.com> wrote:
>   
>> LinkedList is a Deque, but ConcurrentLinkedQueue intentionally is not.
>> You could try LinkedBlockingDeque instead.
>>
>> Martin
>>
>> On Mon, Apr 27, 2009 at 06:58, Vishal Byakod <vishal.byakod at gmail.com> wrote:
>>     
>>> Hi,
>>>
>>> I want to replace java.util.LinkedList with the ConcurrentLinkedQueue
>>> implementation, but am not sure what to use for LinkedList.addFirst()
>>> and LinkedList removeFirst(). ConcurrentLinkedQueue does?nt seem to
>>> have equivalent API?s.
>>>
>>> Can anyone suggest what other options I have.
>>>
>>> Thanks
>>> Vishal
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>       
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>   

From gregg at cytetech.com  Mon Apr 27 12:28:20 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 27 Apr 2009 11:28:20 -0500
Subject: [concurrency-interest] Performance improvement using
	ThreadPoolExecutor
In-Reply-To: <ccb85ce20904270347p6780963h9911911d7221373b@mail.gmail.com>
References: <ccb85ce20904270347p6780963h9911911d7221373b@mail.gmail.com>
Message-ID: <49F5DD24.90006@cytetech.com>

Vishal Byakod wrote:
> I am writing an API( getObject()) which returns a java object from a
> cluster. The cluster can have multiple nodes and each node contains
> only and only one persistent queue. This API getObject()  needs to
> return one object from any one of the queues in the fastest possible
> time. The API is a part of a client runtime library which executes on
> the client JVM

There are existing technologies, such as Javaspaces which do this kind of thing 
really well.  Javaspaces has commercial as well as open source implementations. 
  You can look at the River apache podling, the blitz Javaspace by Dan Creswell 
and products such as gigaspaces.  Javaspaces is a very predominate player in the 
financial services market were time to "action" can cost you billions.

Gregg Wonderly

From vishal.byakod at gmail.com  Mon Apr 27 12:48:55 2009
From: vishal.byakod at gmail.com (Vishal Byakod)
Date: Mon, 27 Apr 2009 22:18:55 +0530
Subject: [concurrency-interest] ConcurrentLinkedQueue implementation
In-Reply-To: <49F5D59E.7050400@gmail.com>
References: <ccb85ce20904270658u3720d856q3d1616fb7c9a8ff0@mail.gmail.com>
	<1ccfd1c10904270743k6fefa570k43c0c68bd8133ae2@mail.gmail.com>
	<ccb85ce20904270815u2cb2f915s79976d3bb718ca06@mail.gmail.com>
	<49F5D59E.7050400@gmail.com>
Message-ID: <ccb85ce20904270948q1117d909h2a5f98b0233b821a@mail.gmail.com>

Apologize for the proof reader's mistake.

I would have wanted a ConcurrentLinkedDeque implementation as in my
case I need to write an efficient API for handling client acknowledge
calls in a JMS Provider.

I am new to this post, can someone tell me if a ConcurrentLinkedDeque
is being planned in the near future?

Vishal


On Mon, Apr 27, 2009 at 9:26 PM, Brian S O'Neill <bronee at gmail.com> wrote:
> Please re-read Martin's response. Note the use of the word "Deque" instead
> of "Queue". Also, if you're looking for a ConcurrentLinkedDeque
> implementation, none is provided in the JDK.
>
> Vishal Byakod wrote:
>>
>> Martin,
>> I need an API to insert an object at the beginning of a list and also
>> one which can remove the first element from a list( just as  LinkedList
>> removeFirst() &  LinkedList addFirst(). LinkedBlockingQueue has API's
>> to insert at the end and retreive from the head. The collection also
>> has to be highly performant under concurrent access.
>>
>> Can you think of anything else?
>>
>> Thanks
>> Vishal
>>
>> On Mon, Apr 27, 2009 at 8:13 PM, Martin Buchholz <martinrb at google.com>
>> wrote:
>>
>>>
>>> LinkedList is a Deque, but ConcurrentLinkedQueue intentionally is not.
>>> You could try LinkedBlockingDeque instead.
>>>
>>> Martin
>>>
>>> On Mon, Apr 27, 2009 at 06:58, Vishal Byakod <vishal.byakod at gmail.com>
>>> wrote:
>>>
>>>>
>>>> Hi,
>>>>
>>>> I want to replace java.util.LinkedList with the ConcurrentLinkedQueue
>>>> implementation, but am not sure what to use for LinkedList.addFirst()
>>>> and LinkedList removeFirst(). ConcurrentLinkedQueue does?nt seem to
>>>> have equivalent API?s.
>>>>
>>>> Can anyone suggest what other options I have.
>>>>
>>>> Thanks
>>>> Vishal
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


From dl at cs.oswego.edu  Mon Apr 27 20:16:37 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 27 Apr 2009 20:16:37 -0400
Subject: [concurrency-interest] ConcurrentLinkedQueue implementation
In-Reply-To: <ccb85ce20904270948q1117d909h2a5f98b0233b821a@mail.gmail.com>
References: <ccb85ce20904270658u3720d856q3d1616fb7c9a8ff0@mail.gmail.com>	<1ccfd1c10904270743k6fefa570k43c0c68bd8133ae2@mail.gmail.com>	<ccb85ce20904270815u2cb2f915s79976d3bb718ca06@mail.gmail.com>	<49F5D59E.7050400@gmail.com>
	<ccb85ce20904270948q1117d909h2a5f98b0233b821a@mail.gmail.com>
Message-ID: <49F64AE5.80308@cs.oswego.edu>

Vishal Byakod wrote:
> Apologize for the proof reader's mistake.
> 
> I would have wanted a ConcurrentLinkedDeque implementation as in my
> case I need to write an efficient API for handling client acknowledge
> calls in a JMS Provider.
> 
> I am new to this post, can someone tell me if a ConcurrentLinkedDeque
> is being planned in the near future?
> 

No; but one was planned in the near past :-)
ConcurrentLinkedDeque was put together as a candidate
for Java6. You can find it from the "jsr166x" links at
http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
including source at
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166x/

This is reasonably well tested etc. The main reason for
deciding not to release is that its performance on
plain queue operations is not as good as others
like ConcurrentLinkedQueue so you'd only want to use it
when you need full Deque capabilities,
which is not common enough to justify inclusion in JDK.

But do please use it!

-Doug


From i30817 at gmail.com  Tue Apr 28 15:43:36 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Tue, 28 Apr 2009 20:43:36 +0100
Subject: [concurrency-interest] LIFO ThreadPoolExecutor
Message-ID: <212322090904281243g383b4c8ciab6fbec908d55400@mail.gmail.com>

I'm doing requests to a web service for thumbnails over a
threadpoolexecutor, configured like this:
http://www.kimchy.org/juc-executorservice-gotcha/

As I'm using a web service I'd like to configure the Linked blocking queue
the executors use to be able to be LIFO (according to the factory method),
so that the relevant requests are processed first.

I have these static factory methods :
    /**
     * A pool with exactly nThreads that don't timeout
     * @param nThreads > 0
     * @return
     */
    public static ChainExecutorService newFixedThreadPool(int nThreads)
(using linkedBlockingQueue)

    /**
     * A pool that has no maximum number of threads
     * and will kill the threads after a timeout after the last task
     * @param secondsTimeOut > 0
     * @return
     */
    public static ChainExecutorService newCachedThreadPool(long
secondsTimeout)
(using SynchronousQueue)

    /**
     * A pool that has a maximum number of threads
     * and will kill the threads after a timeout after the last task
     * @param maximumNThreads > 0
     * @param secondsTimeOut > 0
     * @return
     */
    public static ChainExecutorService newScalingThreadPool(int
maximumNThreads, long secondsTimeOut)
(using a equivalent class to ScalingQueue from the blog above - extends
LinkedBlockingQueue)
    /**
     * A pool that has a minimum number of threads, a maximum number of
threads
     * and will kill maximumNThreads - minimumNThreads after a timeout after
the last task
     * @param minimumNThreads >= 0
     * @param maximumNThreads > 0
     * @param secondsTimeOut > 0
     * @return
     */
        public static ChainExecutorService newScalingThreadPool(int
minimumNThreads, int maximumNThreads, long secondsTimeOut)
(using a equivalent class to ScalingQueue from the blog above - extends
LinkedBlockingQueue)

I'm asking the way to modify LinkedBlockingQueue and SynchronousQueue so
that i can implement LIFO for the ThreadPoolExecutor.

I know this is a complex question - but i'm really raw at implementing data
structures like this, and i fear screwing it up.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090428/e9ea1161/attachment.html>

From alarmnummer at gmail.com  Tue Apr 28 16:16:21 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 28 Apr 2009 22:16:21 +0200
Subject: [concurrency-interest] LIFO ThreadPoolExecutor
In-Reply-To: <212322090904281243g383b4c8ciab6fbec908d55400@mail.gmail.com>
References: <212322090904281243g383b4c8ciab6fbec908d55400@mail.gmail.com>
Message-ID: <1466c1d60904281316o7cb0b210nebd4d58ea3bb2b30@mail.gmail.com>

You could wrap a Stack or Deque in a Queue implementation so that it
behaves like a stack and inject it in the executor as the workpool to
use.

so something like this:

class FifoQueue implements BlockingQueue{

	Deque deque = new LinkedBlockingDeque();

	public void put(Object item){
		deque.addFirst(item);
	}

	public Object take(){
		return deque.removeFirst();
	}

       ....
}

The disadvantage of your approach is that your system could be
suffering from starvation; some requests are possibly never processed
if new requests keep coming in.

On Tue, Apr 28, 2009 at 9:43 PM, Paulo Levi <i30817 at gmail.com> wrote:
> I'm doing requests to a web service for thumbnails over a
> threadpoolexecutor, configured like this:
> http://www.kimchy.org/juc-executorservice-gotcha/
>
> As I'm using a web service I'd like to configure the Linked blocking queue
> the executors use to be able to be LIFO (according to the factory method),
> so that the relevant requests are processed first.
>
> I have these static factory methods :
> ??? /**
> ???? * A pool with exactly nThreads that don't timeout
> ???? * @param nThreads > 0
> ???? * @return
> ???? */
> ??? public static ChainExecutorService newFixedThreadPool(int nThreads)
> (using linkedBlockingQueue)
>
> ??? /**
> ???? * A pool that has no maximum number of threads
> ???? * and will kill the threads after a timeout after the last task
> ???? * @param secondsTimeOut > 0
> ???? * @return
> ???? */
> ??? public static ChainExecutorService newCachedThreadPool(long
> secondsTimeout)
> (using SynchronousQueue)
>
> ??? /**
> ???? * A pool that has a maximum number of threads
> ???? * and will kill the threads after a timeout after the last task
> ???? * @param maximumNThreads > 0
> ???? * @param secondsTimeOut > 0
> ???? * @return
> ???? */
> ??? public static ChainExecutorService newScalingThreadPool(int
> maximumNThreads, long secondsTimeOut)
> (using a equivalent class to ScalingQueue from the blog above - extends
> LinkedBlockingQueue)
> ??? /**
> ???? * A pool that has a minimum number of threads, a maximum number of
> threads
> ???? * and will kill maximumNThreads - minimumNThreads after a timeout after
> the last task
> ???? * @param minimumNThreads >= 0
> ???? * @param maximumNThreads > 0
> ???? * @param secondsTimeOut > 0
> ???? * @return
> ???? */
> ??????? public static ChainExecutorService newScalingThreadPool(int
> minimumNThreads, int maximumNThreads, long secondsTimeOut)
> (using a equivalent class to ScalingQueue from the blog above - extends
> LinkedBlockingQueue)
>
> I'm asking the way to modify LinkedBlockingQueue and SynchronousQueue so
> that i can implement LIFO for the ThreadPoolExecutor.
>
> I know this is a complex question - but i'm really raw at implementing data
> structures like this, and i fear screwing it up.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From i30817 at gmail.com  Tue Apr 28 16:32:18 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Tue, 28 Apr 2009 21:32:18 +0100
Subject: [concurrency-interest] LIFO ThreadPoolExecutor
In-Reply-To: <1466c1d60904281316o7cb0b210nebd4d58ea3bb2b30@mail.gmail.com>
References: <212322090904281243g383b4c8ciab6fbec908d55400@mail.gmail.com>
	<1466c1d60904281316o7cb0b210nebd4d58ea3bb2b30@mail.gmail.com>
Message-ID: <212322090904281332g2607bb51r641743d1fffc4660@mail.gmail.com>

Are those the only relevant methods to implement differently?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090428/9b7cb905/attachment.html>

From i30817 at gmail.com  Tue Apr 28 16:40:34 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Tue, 28 Apr 2009 21:40:34 +0100
Subject: [concurrency-interest] LIFO ThreadPoolExecutor
In-Reply-To: <212322090904281332g2607bb51r641743d1fffc4660@mail.gmail.com>
References: <212322090904281243g383b4c8ciab6fbec908d55400@mail.gmail.com>
	<1466c1d60904281316o7cb0b210nebd4d58ea3bb2b30@mail.gmail.com>
	<212322090904281332g2607bb51r641743d1fffc4660@mail.gmail.com>
Message-ID: <212322090904281340j3da60b49u3c9f424a50204a25@mail.gmail.com>

And what about SynchronousQueue?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090428/e992dafb/attachment.html>

From joe.bowbeer at gmail.com  Tue Apr 28 17:03:05 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 28 Apr 2009 14:03:05 -0700
Subject: [concurrency-interest] LIFO ThreadPoolExecutor
In-Reply-To: <212322090904281243g383b4c8ciab6fbec908d55400@mail.gmail.com>
References: <212322090904281243g383b4c8ciab6fbec908d55400@mail.gmail.com>
Message-ID: <31f2a7bd0904281403t491f5d52t728ef4adc4353369@mail.gmail.com>

This is a complex question and I'm unable to answer it directly.

I do suggest you consider a couple alternatives:

1. CallerRunsPolicy with bounded queue and core & max threads.

Rather than rejecting a new request, this will service it in the calling
thread.  When heavily loaded, this policy throttles demand and tends to
devote more resources to newer requests.

2. DiscardOldestPolicy with bounded queue and core & max threads.

This will discard the oldest request before rejecting a new request.  With
luck, the discarded requests will no longer be relevant.  If your service is
"leaky" (that is, if it is allowed to punt when it is overloaded), this may
be a good option.

If your bounded queue is really a bounded stack, then a DiscardBottomPolicy
might work really well.  This would service requests in LIFO order until it
is swamped, and then discard the oldest requests.


Btw, I suspect the ScalingQueue implementation has a problem or two at the
corner cases, but this is only a suspicion.

Joe

On Tue, Apr 28, 2009 at 12:43 PM, Paulo Levi wrote:

> I'm doing requests to a web service for thumbnails over a
> threadpoolexecutor, configured like this:
> http://www.kimchy.org/juc-executorservice-gotcha/
>
> As I'm using a web service I'd like to configure the Linked blocking queue
> the executors use to be able to be LIFO (according to the factory method),
> so that the relevant requests are processed first. [...]
>
> I'm asking the way to modify LinkedBlockingQueue and SynchronousQueue so
> that i can implement LIFO for the ThreadPoolExecutor.
>
> I know this is a complex question - but i'm really raw at implementing data
> structures like this, and i fear screwing it up.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090428/bbe305b5/attachment.html>

