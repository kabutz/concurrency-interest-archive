From aaron.grunthal at infinite-source.de  Tue Jan  1 04:09:40 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Tue, 01 Jan 2013 10:09:40 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236967060@G9W0725.americas.hpqcorp.net>
References: <50E1904E.9030406@infinite-source.de>
	<50E1A44D.1090509@oracle.com>	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de>
	<CACuKZqFUBr3g6vB3MkXKNT56otcWpLfA9r-qyfmCfMUNNB_3LQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236967060@G9W0725.americas.hpqcorp.net>
Message-ID: <50E2A7D4.60908@infinite-source.de>

On 01.01.2013 03:58, Boehm, Hans wrote:
> I'm not sure I understand the setting here, but what prevents this from seeing an older, smaller version of varTable, with a newly allocated index, generating an out-of-bounds exception.  Is that acceptable?

I did omit bounds and null checks in my example. Seeing an older, 
smaller version is of course expected and will return null.

> There's also technically a problem, probably observed on no modern hardware, that the varTable[index] load may appear to complete before the varTable load, allowing the former to see a not-yet-copied value.  The Java memory model does not in general (aside from final fields) guarantee that visibility ordering respects data dependencies.

I'm not quite sure if I understand this problem. Do you mean that the 
arraycopy in step 2 could occur after (or become visible after) the CAS 
in step 4? Wouldn't that apply to the ordering of step 3 and 4 too then?

Well, as long as the problem is purely theoretical in nature we'll just 
ignore it for now. I assume this could be - should it ever become an 
issue - fixed using the fences API in the future?

- Aaron

From aaron.grunthal at infinite-source.de  Tue Jan  1 04:19:36 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Tue, 01 Jan 2013 10:19:36 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1C651.3010605@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
Message-ID: <50E2AA28.7050207@infinite-source.de>

On 31.12.2012 18:07, Aaron Grunthal wrote:
>>> A) use Unsafe.getObject() for a non-volatile read.
>>> This actually doesn't seem to work according to some microbenchmarks
>>> I've tried. The read does not get hoisted. My question here would be: Is
>>> this intentional/a limitation of hotspot or am I benchmarking wrong?
>>
>> Maybe both, can you post the test, and the relevant disassembly?
>> Unsafe.getObject() against non-volatile field should not entail any sort
>> of ordering in the best case (this is somewhat complicated by alias
>> analysis, etc).
>
>
> The testcase including disassembly:
> https://gist.github.com/4420897#file-test-java
>
> test1 is the volatile
> test2 is the non-volatile
> test3 is non-volatile access to volatile field through Unsafe
>
> In case2 the field accesses and subsequently some other pre-calculations
> have been optimized through LICM, which is not the case in case 1 or 3.

Does anyone here know whether getObject on a volatile field is/should be 
LICM-friendly or should I poke hotspot-dev?

I think it's more fault-prone to declare a field non-volatile and use 
Unsafe for all the volatile/atomic accesses instead of declaring it 
volatile and only using Unsafe in those cases where I want the code to 
be LICM-friendly via non-volatile reads.

- Aaron

From aleksey.shipilev at oracle.com  Tue Jan  1 04:37:39 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 01 Jan 2013 13:37:39 +0400
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E2AA28.7050207@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<50E2AA28.7050207@infinite-source.de>
Message-ID: <50E2AE63.3060008@oracle.com>

On 01/01/2013 01:19 PM, Aaron Grunthal wrote:
> Does anyone here know whether getObject on a volatile field is/should be
> LICM-friendly or should I poke hotspot-dev?

You shouldn't probably call it "LICM-friendly". Rather, I would say that
Unsafe.getObject() on volatile field does not enforce any particular
ordering (as much as I can read from HotSpot's compiler code).

-Aleksey.

From aaron.grunthal at infinite-source.de  Tue Jan  1 04:50:12 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Tue, 01 Jan 2013 10:50:12 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E2AE63.3060008@oracle.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<50E2AA28.7050207@infinite-source.de> <50E2AE63.3060008@oracle.com>
Message-ID: <50E2B154.4070605@infinite-source.de>

On 01.01.2013 10:37, Aleksey Shipilev wrote:
> You shouldn't probably call it "LICM-friendly". Rather, I would say that
> Unsafe.getObject() on volatile field does not enforce any particular
> ordering (as much as I can read from HotSpot's compiler code).
>

The issue is that it's slower in my test and, from what i can tell from 
the disassembly it does not get hoisted. So something must be different 
compared to a regular field access.
So i wonder whether that's intended behavior, something wrong on my end 
or just some oversight in the way it is implemented that ought to be fixed.


- Aaron

From aleksey.shipilev at oracle.com  Tue Jan  1 05:04:30 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 01 Jan 2013 14:04:30 +0400
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E2B154.4070605@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<50E2AA28.7050207@infinite-source.de> <50E2AE63.3060008@oracle.com>
	<50E2B154.4070605@infinite-source.de>
Message-ID: <50E2B4AE.1070402@oracle.com>

On 01/01/2013 01:50 PM, Aaron Grunthal wrote:
> On 01.01.2013 10:37, Aleksey Shipilev wrote:
>> You shouldn't probably call it "LICM-friendly". Rather, I would say that
>> Unsafe.getObject() on volatile field does not enforce any particular
>> ordering (as much as I can read from HotSpot's compiler code).
>>
> 
> The issue is that it's slower in my test and, from what i can tell from
> the disassembly it does not get hoisted. So something must be different
> compared to a regular field access.
> So i wonder whether that's intended behavior, something wrong on my end
> or just some oversight in the way it is implemented that ought to be fixed.

Unlikely to be the Hotspot C2 issue (IIRC, Unsafe gets inlined into the
high-level graph, at which code motion is still to be performed). Can
you post the complete compilable example? Gist is just the excerpt, not
the complete example.

-Aleksey.


From aaron.grunthal at infinite-source.de  Tue Jan  1 05:23:11 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Tue, 01 Jan 2013 11:23:11 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E2B4AE.1070402@oracle.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<50E2AA28.7050207@infinite-source.de> <50E2AE63.3060008@oracle.com>
	<50E2B154.4070605@infinite-source.de> <50E2B4AE.1070402@oracle.com>
Message-ID: <50E2B90F.1060101@infinite-source.de>

On 01.01.2013 11:04, Aleksey Shipilev wrote:
> Unlikely to be the Hotspot C2 issue (IIRC, Unsafe gets inlined into the
> high-level graph, at which code motion is still to be performed). Can
> you post the complete compilable example? Gist is just the excerpt, not
> the complete example.

https://gist.github.com/4426366


From aleksey.shipilev at oracle.com  Tue Jan  1 06:17:06 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 01 Jan 2013 15:17:06 +0400
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E2B90F.1060101@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<50E2AA28.7050207@infinite-source.de> <50E2AE63.3060008@oracle.com>
	<50E2B154.4070605@infinite-source.de> <50E2B4AE.1070402@oracle.com>
	<50E2B90F.1060101@infinite-source.de>
Message-ID: <50E2C5B2.40207@oracle.com>

On 01/01/2013 02:23 PM, Aaron Grunthal wrote:
> On 01.01.2013 11:04, Aleksey Shipilev wrote:
>> Unlikely to be the Hotspot C2 issue (IIRC, Unsafe gets inlined into the
>> high-level graph, at which code motion is still to be performed). Can
>> you post the complete compilable example? Gist is just the excerpt, not
>> the complete example.
> 
> https://gist.github.com/4426366

Interesting. So, rewiring the test code a little to make sure it warms
up without a hitch (basically, doing multiple thread starts/stops), on
my Linux x86_64, JDK 7u12_ea, I've got:

  volatile long[] access: ~300 ms
     plain long[] access: ~130 ms
  volatile long[] unsafe: ~200 ms

The barriers are indeed omitted in the hot loop, but the reads are still
there:

130   B16: #    B30 B17 <- B15 B20      Loop: B16-B20 inner post of N252
Freq: 0.0559008
130     movl    RDI, [R11 + #12 (8-bit)]        # volatile Test2.var1a
134     movl    R9, [R12 + RDI << 3 + #12]
139     NullCheck RDI
139
139   B17: #    B28 B18 <- B16  Freq: 0.0559007
139     movl    RCX, [R11 + #16 (8-bit)]        # volatile Test2.var1b
13d     decode_heap_oop_not_null RBX,RDI
141     testl  R9, R9   # unsigned
144     jbe,u  B28  P=0.000001 C=-1.000000
144
14a   B18: #    B31 B19 <- B17  Freq: 0.0559007
14a     movl    R10, [R12 + RCX << 3 + #12]
14f     NullCheck RCX
14f
14f   B19: #    B29 B20 <- B18  Freq: 0.0559006
14f     movq    RDX, [R12 + RDI << 3 + #16]
154     cmpl    R10, #13        # unsigned
158     jbe,u  B29  P=0.000001 C=-1.000000
158
15e   B20: #    B16 B21 <- B19  Freq: 0.0559006
15e     movq    R10, [R12 + RCX << 3 + #120]
163     addq    RDX, R10        # long
166     addq    RSI, RDX        # long
169     addq    R10, #1 # long
16d     movq    [R12 + RCX << 3 + #120]
172     movq    [R12 + RDI << 3 + #16]
177     incl    R8      # int
17a     cmpl    R8, #90000000
181     jl,s   B16      # loop end  P=0.500000 C=162116.000000

I think the real trouble is the control projections from the loop body
to the Unsafe calls which can not be untangled that easily, and so
prevent hoisting. Please bring this one up on hotspot-dev@ to see if we
can do something about it (not sure my observations are correct though
and/or if we *need* to fix this).

-Aleksey.

From stanimir at riflexo.com  Tue Jan  1 06:38:48 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 1 Jan 2013 13:38:48 +0200
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E2C5B2.40207@oracle.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<50E2AA28.7050207@infinite-source.de> <50E2AE63.3060008@oracle.com>
	<50E2B154.4070605@infinite-source.de> <50E2B4AE.1070402@oracle.com>
	<50E2B90F.1060101@infinite-source.de> <50E2C5B2.40207@oracle.com>
Message-ID: <CAEJX8oraxkVM98ud5a_Y9At=VVq7x5zvewLie_D8wtccVW9Q0Q@mail.gmail.com>

I thought the slower execution was caused by the 32bit JVM, inability to
properly allocate registers but obviously not so.



On Tue, Jan 1, 2013 at 1:17 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 01/01/2013 02:23 PM, Aaron Grunthal wrote:
> > On 01.01.2013 11:04, Aleksey Shipilev wrote:
> >> Unlikely to be the Hotspot C2 issue (IIRC, Unsafe gets inlined into the
> >> high-level graph, at which code motion is still to be performed). Can
> >> you post the complete compilable example? Gist is just the excerpt, not
> >> the complete example.
> >
> > https://gist.github.com/4426366
>
> Interesting. So, rewiring the test code a little to make sure it warms
> up without a hitch (basically, doing multiple thread starts/stops), on
> my Linux x86_64, JDK 7u12_ea, I've got:
>
>   volatile long[] access: ~300 ms
>      plain long[] access: ~130 ms
>   volatile long[] unsafe: ~200 ms
>
> The barriers are indeed omitted in the hot loop, but the reads are still
> there:
>
> 130   B16: #    B30 B17 <- B15 B20      Loop: B16-B20 inner post of N252
> Freq: 0.0559008
> 130     movl    RDI, [R11 + #12 (8-bit)]        # volatile Test2.var1a
> 134     movl    R9, [R12 + RDI << 3 + #12]
> 139     NullCheck RDI
> 139
> 139   B17: #    B28 B18 <- B16  Freq: 0.0559007
> 139     movl    RCX, [R11 + #16 (8-bit)]        # volatile Test2.var1b
> 13d     decode_heap_oop_not_null RBX,RDI
> 141     testl  R9, R9   # unsigned
> 144     jbe,u  B28  P=0.000001 C=-1.000000
> 144
> 14a   B18: #    B31 B19 <- B17  Freq: 0.0559007
> 14a     movl    R10, [R12 + RCX << 3 + #12]
> 14f     NullCheck RCX
> 14f
> 14f   B19: #    B29 B20 <- B18  Freq: 0.0559006
> 14f     movq    RDX, [R12 + RDI << 3 + #16]
> 154     cmpl    R10, #13        # unsigned
> 158     jbe,u  B29  P=0.000001 C=-1.000000
> 158
> 15e   B20: #    B16 B21 <- B19  Freq: 0.0559006
> 15e     movq    R10, [R12 + RCX << 3 + #120]
> 163     addq    RDX, R10        # long
> 166     addq    RSI, RDX        # long
> 169     addq    R10, #1 # long
> 16d     movq    [R12 + RCX << 3 + #120]
> 172     movq    [R12 + RDI << 3 + #16]
> 177     incl    R8      # int
> 17a     cmpl    R8, #90000000
> 181     jl,s   B16      # loop end  P=0.500000 C=162116.000000
>
> I think the real trouble is the control projections from the loop body
> to the Unsafe calls which can not be untangled that easily, and so
> prevent hoisting. Please bring this one up on hotspot-dev@ to see if we
> can do something about it (not sure my observations are correct though
> and/or if we *need* to fix this).
>
> -Aleksey.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/638782e3/attachment.html>

From viktor.klang at gmail.com  Tue Jan  1 09:15:03 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 1 Jan 2013 15:15:03 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E1AC9B.6040509@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu> <50E0CBA3.5000304@cs.oswego.edu>
	<CANPzfU_-gruW=4a_+8_AeaKCRgOu0CsiRGjJ=anAx6mkPps1qA@mail.gmail.com>
	<50E1AC9B.6040509@cs.oswego.edu>
Message-ID: <CANPzfU8ALtLFssSQCqeyfe6wLzfBF=RjH=yTWd6o8aLeSEh6Nw@mail.gmail.com>

On Mon, Dec 31, 2012 at 4:17 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/31/12 03:57, ?iktor ?lang wrote:
>
>>
>> On Dec 31, 2012 12:20 AM, "Doug Lea" <dl at cs.oswego.edu
>>  > forcing so many mismatches with Future.get/timed-get API. A new
>>  > (unchecked) j.u.c.CompletionException is now used in
>>  > the same way as ExecutionException, but is internally
>>  > collapsed upon propagation.
>>
>
>
>> Sounds great! How are non-recoverable Errors handled? (VMError, etc)?
>>
>
> RuntimeExceptions and Errors are treated in the same way.
> If people think they can recover from an Error (normally
> caught as CompletionException with the Error as cause), they are
> free to try.


Ok.


>
>
>
>>  >
>>  > Most usages of CFs that use functional forms will probably
>>  > want to use the added getValue() method, that reports these
>>  > as CompletionExceptions, instead of get(), that must throw
>>  > them as ExecutionExceptions.
>>
>> That's a very important distinction.
>>
>>
> Yes. CF now uses exactly the same approach used in ForkJoin (which
> also implements Future). For clarity I renamed the unchecked
> CF version as "join()". In both CF and FJ,

we support the full checked versions of get/timed-get, but don't
> expect people to use them much: If users leave the exception
> handling/propagation to us, we can always do it right. Otherwise,
> it is not always easy for people to figure out whether and how to
> propagate with completeExceptionally, and/or rethrow and/or
> exception-transforming rethrow. But when users must do any of
> these things in custom situations, they can still do so.


> With these changes, I'm claiming that CompletableFuture is for the
> moment in a stable state, and urge others to read about and try
> it out.
>
> http://gee.cs.oswego.edu/dl/**jsr166/dist/docs/java/util/**
> concurrent/CompletableFuture.**html<http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html>
>
> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**
> main/java/util/concurrent/**CompletableFuture.java?view=**log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log>
>
>
The ability to identify the task that is submitted to the Executor would be
desirable. We take advantage of that to speed up execution:

https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/dispatch/BatchingExecutor.scala

Are you going to hook into managed blocking for the join/get calls?

Cheers,
?



> -Doug
>
>
>
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/cc57821b/attachment-0001.html>

From aleksey.shipilev at oracle.com  Tue Jan  1 09:50:55 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 01 Jan 2013 18:50:55 +0400
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E2C5B2.40207@oracle.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<50E2AA28.7050207@infinite-source.de> <50E2AE63.3060008@oracle.com>
	<50E2B154.4070605@infinite-source.de> <50E2B4AE.1070402@oracle.com>
	<50E2B90F.1060101@infinite-source.de> <50E2C5B2.40207@oracle.com>
Message-ID: <50E2F7CF.6060807@oracle.com>

On 01/01/2013 03:17 PM, Aleksey Shipilev wrote:
> On 01/01/2013 02:23 PM, Aaron Grunthal wrote:
>> On 01.01.2013 11:04, Aleksey Shipilev wrote:
> I think the real trouble is the control projections from the loop body
> to the Unsafe calls which can not be untangled that easily, and so
> prevent hoisting. Please bring this one up on hotspot-dev@ to see if we
> can do something about it (not sure my observations are correct though
> and/or if we *need* to fix this).

While Aaron et al. are discussing this implementation-wise, I realized
this has an interesting tidbit: everyone knows that awaiting on plain
field in the busyloop is error-prone because it can get hoisted. Then,
we can use Aaron's finding in this scenario (extremely fragile since
Unsafe) [1].

[1]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/interrupt/UnsafeBusyLoopTest.java

From viktor.klang at gmail.com  Tue Jan  1 10:20:57 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 1 Jan 2013 16:20:57 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E2F7CF.6060807@oracle.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<50E2AA28.7050207@infinite-source.de> <50E2AE63.3060008@oracle.com>
	<50E2B154.4070605@infinite-source.de> <50E2B4AE.1070402@oracle.com>
	<50E2B90F.1060101@infinite-source.de> <50E2C5B2.40207@oracle.com>
	<50E2F7CF.6060807@oracle.com>
Message-ID: <CANPzfU8jmMdF91rbz6bbFE6aVvfdryZo_0jGBstw4=_Hvz2J3g@mail.gmail.com>

On Tue, Jan 1, 2013 at 3:50 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 01/01/2013 03:17 PM, Aleksey Shipilev wrote:
> > On 01/01/2013 02:23 PM, Aaron Grunthal wrote:
> >> On 01.01.2013 11:04, Aleksey Shipilev wrote:
> > I think the real trouble is the control projections from the loop body
> > to the Unsafe calls which can not be untangled that easily, and so
> > prevent hoisting. Please bring this one up on hotspot-dev@ to see if we
> > can do something about it (not sure my observations are correct though
> > and/or if we *need* to fix this).
>
> While Aaron et al. are discussing this implementation-wise, I realized
> this has an interesting tidbit: everyone knows that awaiting on plain
> field in the busyloop is error-prone because it can get hoisted. Then,
> we can use Aaron's finding in this scenario (extremely fragile since
> Unsafe) [1].
>

May the Lord have mercy on your soul ;-)


>
> [1]
>
> https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/interrupt/UnsafeBusyLoopTest.java
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/18d4bf2d/attachment.html>

From aleksey.shipilev at oracle.com  Tue Jan  1 10:26:09 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 01 Jan 2013 19:26:09 +0400
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CANPzfU8jmMdF91rbz6bbFE6aVvfdryZo_0jGBstw4=_Hvz2J3g@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<50E2AA28.7050207@infinite-source.de> <50E2AE63.3060008@oracle.com>
	<50E2B154.4070605@infinite-source.de> <50E2B4AE.1070402@oracle.com>
	<50E2B90F.1060101@infinite-source.de>
	<50E2C5B2.40207@oracle.com> <50E2F7CF.6060807@oracle.com>
	<CANPzfU8jmMdF91rbz6bbFE6aVvfdryZo_0jGBstw4=_Hvz2J3g@mail.gmail.com>
Message-ID: <50E30011.3000509@oracle.com>

On 01/01/2013 07:20 PM, ?iktor ?lang wrote:
> On Tue, Jan 1, 2013 at 3:50 PM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
> 
>     On 01/01/2013 03:17 PM, Aleksey Shipilev wrote:
>     > On 01/01/2013 02:23 PM, Aaron Grunthal wrote:
>     >> On 01.01.2013 11:04, Aleksey Shipilev wrote:
>     > I think the real trouble is the control projections from the loop body
>     > to the Unsafe calls which can not be untangled that easily, and so
>     > prevent hoisting. Please bring this one up on hotspot-dev@ to see
>     if we
>     > can do something about it (not sure my observations are correct though
>     > and/or if we *need* to fix this).
> 
>     While Aaron et al. are discussing this implementation-wise, I realized
>     this has an interesting tidbit: everyone knows that awaiting on plain
>     field in the busyloop is error-prone because it can get hoisted. Then,
>     we can use Aaron's finding in this scenario (extremely fragile since
>     Unsafe) [1].
> 
> May the Lord have mercy on your soul ;-)

Doug seems to be relying on the same side effect in StampedLock, so I'm
not alone in my journey to Hell! ;)

-Aleksey.

From dl at cs.oswego.edu  Tue Jan  1 10:28:11 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 01 Jan 2013 10:28:11 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU8ALtLFssSQCqeyfe6wLzfBF=RjH=yTWd6o8aLeSEh6Nw@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu> <50E0CBA3.5000304@cs.oswego.edu>
	<CANPzfU_-gruW=4a_+8_AeaKCRgOu0CsiRGjJ=anAx6mkPps1qA@mail.gmail.com>
	<50E1AC9B.6040509@cs.oswego.edu>
	<CANPzfU8ALtLFssSQCqeyfe6wLzfBF=RjH=yTWd6o8aLeSEh6Nw@mail.gmail.com>
Message-ID: <50E3008B.2020203@cs.oswego.edu>

On 01/01/13 09:15, ?iktor ?lang wrote:

>
> The ability to identify the task that is submitted to the Executor would be
> desirable.

I'm mot sure what you mean by "identify the task". Do you mean
making CF.Async*  public types so you can check instanceof?

> Are you going to hook into managed blocking for the join/get calls?

Yes, coming soon. In the mean time there's already another
added minor support feature in ForkJoinPool: FJP.awaitQuiescence
can be used, among other cases when you create a set of
async CFs, not knowing if there is a unique termination point.
You can instead wait for pool to quiesce, in which case they
must all be completed (or at least completing).

-Doug





From viktor.klang at gmail.com  Tue Jan  1 10:32:16 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 1 Jan 2013 16:32:16 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E3008B.2020203@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu> <50E0CBA3.5000304@cs.oswego.edu>
	<CANPzfU_-gruW=4a_+8_AeaKCRgOu0CsiRGjJ=anAx6mkPps1qA@mail.gmail.com>
	<50E1AC9B.6040509@cs.oswego.edu>
	<CANPzfU8ALtLFssSQCqeyfe6wLzfBF=RjH=yTWd6o8aLeSEh6Nw@mail.gmail.com>
	<50E3008B.2020203@cs.oswego.edu>
Message-ID: <CANPzfU9NwyKw4_hZXgh_6hRXsmpKO=fbgYMgzCPh0Cp5-seiQg@mail.gmail.com>

On Jan 1, 2013 4:28 PM, "Doug Lea" <dl at cs.oswego.edu> wrote:
>
> On 01/01/13 09:15, ?iktor ?lang wrote:
>
>>
>> The ability to identify the task that is submitted to the Executor would
be
>> desirable.
>
>
> I'm mot sure what you mean by "identify the task". Do you mean
> making CF.Async*  public types so you can check instanceof?

Yes, exactly.

>
>
>> Are you going to hook into managed blocking for the join/get calls?
>
>
> Yes, coming soon. In the mean time there's already another
> added minor support feature in ForkJoinPool: FJP.awaitQuiescence
> can be used, among other cases when you create a set of
> async CFs, not knowing if there is a unique termination point.
> You can instead wait for pool to quiesce, in which case they
> must all be completed (or at least completing).

>
> -Doug
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/2dd80096/attachment.html>

From dl at cs.oswego.edu  Tue Jan  1 10:50:08 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 01 Jan 2013 10:50:08 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU9NwyKw4_hZXgh_6hRXsmpKO=fbgYMgzCPh0Cp5-seiQg@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu> <50E0CBA3.5000304@cs.oswego.edu>
	<CANPzfU_-gruW=4a_+8_AeaKCRgOu0CsiRGjJ=anAx6mkPps1qA@mail.gmail.com>
	<50E1AC9B.6040509@cs.oswego.edu>
	<CANPzfU8ALtLFssSQCqeyfe6wLzfBF=RjH=yTWd6o8aLeSEh6Nw@mail.gmail.com>
	<50E3008B.2020203@cs.oswego.edu>
	<CANPzfU9NwyKw4_hZXgh_6hRXsmpKO=fbgYMgzCPh0Cp5-seiQg@mail.gmail.com>
Message-ID: <50E305B0.5080907@cs.oswego.edu>

On 01/01/13 10:32, ?iktor ?lang wrote:

>  > I'm mot sure what you mean by "identify the task". Do you mean
>  > making CF.Async*  public types so you can check instanceof?
>
> Yes, exactly.

This could be useful to others for monitoring/debugging
purposes, but only as a simple tagging interface:


     /**
      * A tagging interface identifying asynchronous tasks produced by
      * {@code async} methods. This may be useful for monitoring,
      * debugging, and tracking asynchronous activities.
      */
     public static interface AsynchronousCompletionTask {
     }

OK?

-Doug




From viktor.klang at gmail.com  Tue Jan  1 10:57:26 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 1 Jan 2013 16:57:26 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E305B0.5080907@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu> <50E0CBA3.5000304@cs.oswego.edu>
	<CANPzfU_-gruW=4a_+8_AeaKCRgOu0CsiRGjJ=anAx6mkPps1qA@mail.gmail.com>
	<50E1AC9B.6040509@cs.oswego.edu>
	<CANPzfU8ALtLFssSQCqeyfe6wLzfBF=RjH=yTWd6o8aLeSEh6Nw@mail.gmail.com>
	<50E3008B.2020203@cs.oswego.edu>
	<CANPzfU9NwyKw4_hZXgh_6hRXsmpKO=fbgYMgzCPh0Cp5-seiQg@mail.gmail.com>
	<50E305B0.5080907@cs.oswego.edu>
Message-ID: <CANPzfU-9s9wi-oySLsWD70BuiRDv2Ra8kg7L_fBV2-B_Jw8esA@mail.gmail.com>

On Jan 1, 2013 4:50 PM, "Doug Lea" <dl at cs.oswego.edu> wrote:
>
> On 01/01/13 10:32, ?iktor ?lang wrote:
>
>>  > I'm mot sure what you mean by "identify the task". Do you mean
>>  > making CF.Async*  public types so you can check instanceof?
>>
>> Yes, exactly.
>
>
> This could be useful to others for monitoring/debugging
> purposes, but only as a simple tagging interface:
>
>
>     /**
>      * A tagging interface identifying asynchronous tasks produced by
>      * {@code async} methods. This may be useful for monitoring,
>      * debugging, and tracking asynchronous activities.
>      */
>     public static interface AsynchronousCompletionTask {
>     }
>
> OK?

Yes, perfect!

>
> -Doug
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/b64dd6b0/attachment-0001.html>

From dl at cs.oswego.edu  Tue Jan  1 11:00:29 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 01 Jan 2013 11:00:29 -0500
Subject: [concurrency-interest] CompletableFuture - checked exceptions
In-Reply-To: <CA+kOe0-jnxd0v7CgwFro=VkTPB2nBpfbANEQO-QwLsUbd4WB5Q@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu>
	<1356981394.83983.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CA+kOe0-jnxd0v7CgwFro=VkTPB2nBpfbANEQO-QwLsUbd4WB5Q@mail.gmail.com>
Message-ID: <50E3081D.2090303@cs.oswego.edu>

On 12/31/12 16:30, Martin Buchholz wrote:
> So perhaps we should elide the constructors for CompletionException that do not
> provide a Throwable cause.
>

Yes; thanks. This also maintains/improves debugging support when
(as is the default) ForkJoin is used for async CompletableFutures,
since FJ already does exception translation for exceptions with
(Throawble cause) constructors to provide (when possible) both
sides of a stack trace.

-Doug


From taras.tielkes at gmail.com  Tue Jan  1 11:31:14 2013
From: taras.tielkes at gmail.com (Taras Tielkes)
Date: Tue, 1 Jan 2013 17:31:14 +0100
Subject: [concurrency-interest] concurrent enum-keyed counter map
Message-ID: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>

Hi,

In our application, we frequently maintain counts of events. Often, the
keys for these tables are values of a specific enum.
For example: number of processed messages, keyed by some message type enum.

So far, we've been using CHM instances that are pre-populated with
AtomicLong(0) values for all members of a given enum class.
That way, we can skip the putIfAbsent() call, since an AtomicInteger is
guaranteed to be present for all possible keys.

A colleague suggested to optimize away the hash-based lookup.
Using the ordinal of the enum value, we can simply index into an immutable
array of AtomicLongs:
--------------------
public class ConcurrentEnumCounter<K extends Enum<K>> {
private final AtomicLong[] counts;

private ConcurrentEnumCounter(AtomicLong[] initialCounts) {
counts = initialCounts;
}

public void increment(K key) {
counts[key.ordinal()].incrementAndGet();
}
 public static <K extends Enum<K>> ConcurrentEnumCounter<K>
forEnum(Class<K> elementType) {
K[] constants = elementType.getEnumConstants();
AtomicLong[] counts = new AtomicLong[constants.length];
for (K constant : constants) {
counts[constant.ordinal()] = new AtomicLong();
}
return new ConcurrentEnumCounter<K>(counts);
}
 // other methods ...
}
--------------------

Questions:
1) To me, the proposed optimization makes sense. I think the "effective
immutability" makes this thread-safe, or am I missing something?

2) I think that using an EnumMap with AtomicLong values would work as well.
The EnumMap javadoc mentions it being thread-unsafe. However, I assume that
pre-populating it in the constructor (like above), and only using it for
lookup would be safe. Is that correct?

3) Another option seems to be using an AtomicLongArray, and indexing it by
the enum ordinal. My concern there is that the array layout might increase
the chances of false sharing over approach 1 (using an AtomicLong[] array).
Would this be a proper usage of AtomicLongArray?

Thanks,
-tt
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/68b90aab/attachment.html>

From nathan.reynolds at oracle.com  Tue Jan  1 12:26:39 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 01 Jan 2013 10:26:39 -0700
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
Message-ID: <50E31C4F.1030203@oracle.com>


Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 1/1/2013 9:31 AM, Taras Tielkes wrote:
> Hi,
>
> In our application, we frequently maintain counts of events. Often, 
> the keys for these tables are values of a specific enum.
> For example: number of processed messages, keyed by some message type 
> enum.
>
> So far, we've been using CHM instances that are pre-populated with 
> AtomicLong(0) values for all members of a given enum class.
> That way, we can skip the putIfAbsent() call, since an AtomicInteger 
> is guaranteed to be present for all possible keys.
>
> A colleague suggested to optimize away the hash-based lookup.
> Using the ordinal of the enum value, we can simply index into an 
> immutable array of AtomicLongs:
> --------------------
> public class ConcurrentEnumCounter<K extends Enum<K>> {
> private final AtomicLong[] counts;
>
> private ConcurrentEnumCounter(AtomicLong[] initialCounts) {
> counts = initialCounts;
> }
>
> public void increment(K key) {
> counts[key.ordinal()].incrementAndGet();
> }
> public static <K extends Enum<K>> ConcurrentEnumCounter<K> 
> forEnum(Class<K> elementType) {
> K[] constants = elementType.getEnumConstants();
> AtomicLong[] counts = new AtomicLong[constants.length];
> for (K constant : constants) {
> counts[constant.ordinal()] = new AtomicLong();
> }
> return new ConcurrentEnumCounter<K>(counts);
> }
> // other methods ...
> }
> --------------------
>
> Questions:
> 1) To me, the proposed optimization makes sense. I think the 
> "effective immutability" makes this thread-safe, or am I missing 
> something?
The effective immutability makes this thread-safe.  There is another 
important subtle part.  The final keyword ensures that the contents of 
the array are globally visible before anyone can use the class... if the 
reference to this class is published in a thread-safe manner.  There are 
other email chains in this group which discuss this in depth.
>
> 2) I think that using an EnumMap with AtomicLong values would work as 
> well. The EnumMap javadoc mentions it being thread-unsafe. However, I 
> assume that pre-populating it in the constructor (like above), and 
> only using it for lookup would be safe. Is that correct?
That would work as long as the reference to the EnumMap is published in 
a thread-safe manner.  You better check to make sure that all of the 
methods you intend to call don't modify any state in the EnumMap.  Some 
classes have supposedly read only operation actually do write 
operations!  You also need a way to ensure that none of the mutating 
methods in EnumMap are called.
>
> 3) Another option seems to be using an AtomicLongArray, and indexing 
> it by the enum ordinal. My concern there is that the array layout 
> might increase the chances of false sharing over approach 1 (using an 
> AtomicLong[] array). Would this be a proper usage of AtomicLongArray?
By creating all of the AtomicLongs at the same time, they are most 
likely laid out in memory end-to-end.  False sharing will probably be a 
problem.  The advantage of this over AtomicLongArray is that the object 
headers will put a little space between elements.  So, instead of having 
8 longs on the same cache line (AtomicLongArray) you will have say 4 
longs (assuming 8 byte header + 8 byte AtomicLong object).

If you hope that GC will some how spread out the AtomicLongs and reduce 
false sharing, your hope is in vain.  GC will tend to keep them 
together.  Several have explored how to get the JVM to recognize false 
sharing and move objects in the heap or rearrange fields in a class to 
reduce it.  This has all sorts of difficulties that must be overcome first.

There is a solution in sight called @Contended.  See other email chains 
in this group.  It basically adds padding and I think they intend to use 
it in AtomicLong.

For now, you could wrap the AtomicLong in a class with its own padding.  
A more efficient solution would be to use every 8^(t)^(h) long (assuming 
64 byte cache line size) in the AtomicLongArray.  This guarantees that 
each used long will be sufficiently padded with no extra padding.
>
> Thanks,
> -tt
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/ca485da0/attachment.html>

From aleksey.shipilev at oracle.com  Tue Jan  1 12:41:56 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 1 Jan 2013 21:41:56 +0400
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
Message-ID: <CC3DB1AB-3147-4431-9CF2-D47681F07DD3@oracle.com>



On 01.01.2013, at 20:31, Taras Tielkes <taras.tielkes at gmail.com> wrote:
> Questions:
> 1) To me, the proposed optimization makes sense. I think the "effective immutability" makes this thread-safe, or am I missing something?

Yes, that is correct; modulo enum versioning which can result in out of bounds bugs even though it would compile and run without a hitch. I.e. you compile, instantiate and serialize the map vs. legacy enum, then deserialize against new enum with potentially larger enum space, but initialization had already been done against legacy enum.

> 2) I think that using an EnumMap with AtomicLong values would work as well. The EnumMap javadoc mentions it being thread-unsafe. However, I assume that pre-populating it in the constructor (like above), and only using it for lookup would be safe. Is that correct?

Yes, if you modify the EnumMap in constructor only and it is final. Note that it could break under enum update as in (1).

> 3) Another option seems to be using an AtomicLongArray, and indexing it by the enum ordinal. My concern there is that the array layout might increase the chances of false sharing over approach 1 (using an AtomicLong[] array). Would this be a proper usage of AtomicLongArray?

Yes, it will. Note that false sharing is still hitting you even in the case of AtomicLong[], although the chances of hitting it on backing long[] are larger. But, this is discussing whether you are ok with 400x performance hit, or "just" 100x.

-Aleksey

From dl at cs.oswego.edu  Tue Jan  1 12:51:49 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 01 Jan 2013 12:51:49 -0500
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CC3DB1AB-3147-4431-9CF2-D47681F07DD3@oracle.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CC3DB1AB-3147-4431-9CF2-D47681F07DD3@oracle.com>
Message-ID: <50E32235.2030700@cs.oswego.edu>

On 01/01/13 12:41, Aleksey Shipilev wrote:
>
>> 3) Another option seems to be using an AtomicLongArray, and indexing it by
>> the enum ordinal. My concern there is that the array layout might increase
>> the chances of false sharing over approach 1 (using an AtomicLong[] array).
>> Would this be a proper usage of AtomicLongArray?
>

> Yes, it will. Note that false sharing is still hitting you even in the case


If you worry about a lot of contention, then make it an array of LongAdder.
That's what this class is for.

-Doug

From stanimir at riflexo.com  Tue Jan  1 13:08:52 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 1 Jan 2013 20:08:52 +0200
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CC3DB1AB-3147-4431-9CF2-D47681F07DD3@oracle.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CC3DB1AB-3147-4431-9CF2-D47681F07DD3@oracle.com>
Message-ID: <CAEJX8oruWQ8PKmCHkVm3+es0dCEq_x4LN-xtzOFkqJNVn_E2DA@mail.gmail.com>

> > 3) Another option seems to be using an AtomicLongArray, and indexing it
> by the enum ordinal. My concern there is that the array layout might
> increase the chances of false sharing over approach 1 (using an
> AtomicLong[] array). Would this be a proper usage of AtomicLongArray?
>
> Yes, it will. Note that false sharing is still hitting you even in the
> case of AtomicLong[], although the chances of hitting it on backing long[]
> are larger. But, this is discussing whether you are ok with 400x
> performance hit, or "just" 100x.
>

I was going to say something like that but w/ AtomicLongArray, the index
can be mapped differently than enum.ordinal(), e.g. (enum.ordinal+1)<<3 and
the length of the array should be (Class.getEnumConstants().length+2) <<3
Of course LongAdder is meant to cope w/ the false sharing and concurrent
updates and I'd strongly recommend it but if you are to work w/ the metal
you can make AtomicLongArray do the same. I just don't know if the
frequency of the updates would warrant such an approach.

Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/b5c4bbbe/attachment.html>

From zhong.j.yu at gmail.com  Tue Jan  1 14:30:07 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 1 Jan 2013 13:30:07 -0600
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
Message-ID: <CACuKZqH1sjxCoX1oAfhkW-qrX=4yd+K8aHZ9BuH9be6K7KOYTg@mail.gmail.com>

Is false sharing a big concern for variables each of which is intended
to be shared? If we have N CPUs randomly updating M variables, does it
make a lot of difference whether the variables are on the same cache
line or not?

Zhong Yu

From hans.boehm at hp.com  Wed Jan  2 01:15:31 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 2 Jan 2013 06:15:31 +0000
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E2A7D4.60908@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de>
	<50E1A44D.1090509@oracle.com>	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de>
	<CACuKZqFUBr3g6vB3MkXKNT56otcWpLfA9r-qyfmCfMUNNB_3LQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236967060@G9W0725.americas.hpqcorp.net>
	<50E2A7D4.60908@infinite-source.de>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369672EB@G9W0725.americas.hpqcorp.net>

> From: Aaron Grunthal [mailto:aaron.grunthal at infinite-source.de]
> ...
> > There's also technically a problem, probably observed on no modern
> hardware, that the varTable[index] load may appear to complete before the
> varTable load, allowing the former to see a not-yet-copied value.  The Java
> memory model does not in general (aside from final fields) guarantee that
> visibility ordering respects data dependencies.
> 
> I'm not quite sure if I understand this problem. Do you mean that the
> arraycopy in step 2 could occur after (or become visible after) the CAS in step
> 4? Wouldn't that apply to the ordering of step 3 and 4 too then?
I'm concerned about ordering on the reader side.  You don't want to reading code to do something like:

1) Guess varTable value
2) load varTable[index]
3) load varTable
4) confirm that original guess was correct

It's not completely implausible that the implementation might be able to guess the correct varTable value, e.g. based on a prior execution.

DEC->Compaq->HP Alpha hardware used a cache coherency protocol that allowed similar results for different reasons.  The intent of the Java memory model was to prohibit this only for final field reads.

> 
> Well, as long as the problem is purely theoretical in nature we'll just ignore it
> for now. I assume this could be - should it ever become an issue - fixed using
> the fences API in the future?
It's at least nontrivial to fix this correctly.  C++ atomics do have an ordering specification that allows you to specify this sort of data-dependency-based ordering.  We thought at the time that the approach was reasonably clever at dodging all sorts of sticky issues.  But I think it's still TBD whether it's actually useful.

The fundamental problem is that you want (some) data dependencies to imply memory ordering, but compiler optimizations can break data dependencies, and breaking data dependencies is generally profitable.

In this particular case, I don't see an optimization that would break the dependency.  But turning that into a reasonable specification is a different matter.  Is the load of index also ordered before that of varTable[index]?  What if it were varTable[index>>1], and varTable were known to the compiler to only contain a single element?

Hans
> 
> - Aaron


From oleksandr.otenko at oracle.com  Wed Jan  2 07:44:28 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 02 Jan 2013 12:44:28 +0000
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
References: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
Message-ID: <50E42BAC.9090803@oracle.com>

What are the semantics of the wait, if more than one thread are waiting 
(possibly for the same condition)?

(that's to illustrate you will soon want a hierarchy of conditions - 
compare Semaphore.acquire(1) vs Semaphore.acquire(2) done concurrently)

Alex

On 30/12/2012 21:51, Kasper Nielsen wrote:
> Hi,
>
> Just an idea for a real simple general purpose synchronizer that I think
> would make a nice addition to the once already in JUC. While Phaser is 
> really useful
> I think it is too complex to ever be widely used. My intuition is 
> based on a google search for "import java.util.concurrent.Phaser" 
> filetype:java
>
> The synchronizer is based on supplying a Predicate in the constructor 
> of the synchronizer. And then having await methods
> that will wait until the supplied predicate accepts its argument.
>
> public class SimpleSynchronizer<T> {
>     //Trieber stack to keep track of waiting threads. keeps t, next, 
> thread
>     SimpleSynchronizer(Predicate<T> checker)
>     void await(T t) throws InterruptedException;
>     boolean await(T t, long time, TimeUnit unit) throws 
> InterruptedException;
>     void checkAll(); //checks all waiting threads to see if they 
> should be released
>     //probably some more methods
> }
>
> A simple example would be one that just awaited on some state to be 
> reached.
>
> SimpleSynchronized<Void> ss=new SimpleSynchronized(-> 
> service.isStarted());
>
> ss.await(null) <- awaits for the service to start
>
>
> A better example would be one where we want to wait on a specific 
> (monotonically increasing) state
> SimpleSynchronized<Void> ss=new SimpleSynchronized( s -> s >= 
> service.getState());
>
> to wait on a particular state you would call:
> ss.await(4); // or
> ss.await(9, 10, TimeUnit.SECONDS);
>
> Whenever the services state changed you would call:
> service.state = newState;
> ss.recheckAll();
>
> An alternative definition of the synchronizer would be one that 
> maintained some kind of state that would also be returned to the 
> waiting threads once they where released
>
> public interface SimpleStatebasedSynchronizer<S, T> {
>     SimpleSynchronizer(BinaryPredicate<S, T> checker, S initialState)
>     S await(T t) throws InterruptedException;
>     S await(T t, long time, TimeUnit unit) throws 
> InterruptedException, TimeoutException;
>     S getState(); //returns the current state
>     void setState(S newState); //checks All waiting threads to see if 
> they should be released
> }
>
> - Kasper
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130102/fd0feb1b/attachment.html>

From dl at cs.oswego.edu  Wed Jan  2 09:30:27 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 02 Jan 2013 09:30:27 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU8ALtLFssSQCqeyfe6wLzfBF=RjH=yTWd6o8aLeSEh6Nw@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu> <50E0CBA3.5000304@cs.oswego.edu>
	<CANPzfU_-gruW=4a_+8_AeaKCRgOu0CsiRGjJ=anAx6mkPps1qA@mail.gmail.com>
	<50E1AC9B.6040509@cs.oswego.edu>
	<CANPzfU8ALtLFssSQCqeyfe6wLzfBF=RjH=yTWd6o8aLeSEh6Nw@mail.gmail.com>
Message-ID: <50E44483.8080002@cs.oswego.edu>


A jsr166e (JDK7-compliant) version of CompletableFuture is now
available. More people should be able to run it now, using the
jsr166e.jar (see below).
It includes nested interface declarations for
Action, Fun, Generator (plus "Bi" versions) instead of
the likely JDK8 names (Block, Function, Supplier).
It should be painfully usable in Java7 (without
lambda support) but probably less painfully from other
languages that can match lambda literals to these functional
forms.

This (as well as the j.u.c version) also includes the
improved FJ/Async integration (tagging interface,
ManagedBlockers) mentioned yesterday.

jsr166e versions:

API specs: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/
jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar (compiled using 
Java7 javac).
Source: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/CompletableFuture.java?view=log

j.u.c versions (no jar file; you'd need to build your own using
a recent jdk8/lambda JDK snapshot):

API specs: http://gee.cs.oswego.edu/dl/jsr166/dist/docs/
Source: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log



From concurrency at kuli.org  Wed Jan  2 09:51:16 2013
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Wed, 02 Jan 2013 15:51:16 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1DA16.6050507@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de>
Message-ID: <50E44964.6030007@kuli.org>

Am 31.12.2012 19:31, schrieb Aaron Grunthal:
> Yes, that definitely needs to be fixed. So, would the following work?
> 
> 
> private Object[] varTable; // not volatile!
> 
> Read OPs:
> 
> return varTable[index];
> 
> 
> Write OPs:
> 
> 1. local = Unsafe.getObjectVolatile(varTable) // volatile read
> 2. if index >= local.length
>      local = allocate + arraycopy(local)  // grow array
> 3. local[index] = newValue // non-volatile write
> 4. Unsafe.compareAndSwapObject(varTable,local) // goto 1 on failure

> We just have to avoid losing writes when a different
> thread re-assigns the array.

That's exactly what will happen here.

Thread (A) writes at index > local.length and gets interrupted after step 3.

Thread (B) then writes at an index < local.length without any problems.

Then Thread (A) continues, checks that the varTable reference hasn't
changed yet, and puts the previously copied instance into that variable.
Of course, that copy doesn't include the changed value from thread (B).

I don't see an easy lock-free solution here even if you had an
AtomicReferenceArray in an AtomicReference instance as the varTable. You
either need locking on write access, or copy the array every time.

-Michael

From david.lloyd at redhat.com  Wed Jan  2 09:54:46 2013
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 02 Jan 2013 08:54:46 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DC94CE.5050407@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
Message-ID: <50E44A36.9000509@redhat.com>

On 12/27/2012 12:34 PM, Doug Lea wrote:
>
> An initial version of JDK8 java.util.concurrent.CompletableFuture
> is now available. See javadocs at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
>
> and source at
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log

I've been looking at this and thinking about it over the past week and I 
have some concerns.

First, I don't understand the necessity of force/obtrudeValue for error 
recovery - it makes no sense.  A future is either done, failed, or 
cancelled - if you can't fit into one of these cases you might as well 
just use AtomicReference.  I think this method should simply be dropped.

Secondly, I have in the past and continue to worry about the sloppy 
definition of "cancel" in Future, but in a CompletableFuture it seems 
that we have a chance to clarify the contract a little bit.  Let me explain.

As Doug points out, there are many different implementations of similar 
functionality out there.  And when I developed ours, I wanted a cancel 
function that was deterministic and clear.  But I ran into a 
contradiction: I do not want .cancel() to block, however because the 
corresponding work was happening in other thread(s), I also did not have 
enough information to return true/false with a deterministic answer as 
to whether the task was successfully cancelled.

Part of the problem is, I think, that the FutureTask implementation (and 
now CompletableFuture) confuses two different functions on cancel(), the 
first being the consumer's desire to cancel an operation, and the second 
being the implementation's ability to acknowledge it.  In other words, 
requesting cancellation is also acknowledging it.  I don't know of any 
use case where this behavior is desirable.

As a workaround, our AsyncFuture subinterface [1] redefines cancel() to 
block until cancellation is acknowledged or until the future completes 
some other way, and we introduce a "void asyncCancel(boolean);" method 
which indicates the desire to cancel the task if possible but without 
blocking, and allowing idempotent requests (thus avoiding uncomfortable 
race situations; the interruption flag may be set but not cleared in 
subsequent requests).

I suspect that such a mechanism might avoid the very error situations 
that the force/obtrude method were designed to protect against (though 
that's just a guess, since I don't clearly understand its purpose).

More generally, we decouple the "client" (aka the consumer of the future 
result) from the task itself (aka the producer of the 
result/cancellation/exception) by using separate setResult(), 
setCancelled(), and setFailed() methods - not dissimilar to 
CompletableFuture in fact, though ours are protected (to further isolate 
the two roles), and as I mentioned we have a separate 
request/acknowledgement of cancellation.  Like CompletableFuture, we 
return a boolean from each of these in case the caller needs to know 
whether they were the ones to complete an operation.

Experience shows that overall this mechanism is simple and works quite 
well, and is hard for users to accidentally screw up.  I would strongly 
advise that you consider a cancellation protocol like ours, and I would 
further advise that the complete*() methods (and hopefully their 
cancellation counterpart) be made unavailable on the public API, 
reserved somehow only for the task's use (be it through subclass or 
isolated instance).  I would also recommend, as I said earlier, that 
obtrudeValue be dropped.

One other unrelated gripe I have is that in order to use many (all?) of 
.then/.orThen and friends, you must either use the FJP.commonPool() or a 
supplied executor.  It would make more sense if the new future could be 
completed using the same pool as the one being built off of (or at least 
if it could be an option).

[1] http://is.gd/LzOdt9

-- 
- DML

From concurrency at kuli.org  Wed Jan  2 10:31:33 2013
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Wed, 02 Jan 2013 16:31:33 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
Message-ID: <50E452D5.5060902@kuli.org>

Am 02.01.2013 16:18, schrieb Charles Oliver Nutter:
> 
> If you're referring to updating the array reference, I don't see how
> this would be the case. If the CAS fails, it would re-get (volatile
> get) the new value written by (B). From an earlier email by Aaron:

If the write was to an index within the length range, then the array
reference doesn't change (at least when you don't create a new array on
every write). As a consequence, A's CAS operation won't fail; varTable
still holds the previous, but modified array. The problem is that A
already has copied the array when it wasn't modified by B.

So you either need to lock on write, create a new array on every write,
or use a reference object (like AtomicReference, but not volatile) for
every entry in varTable. Or use some other sophisticated algorithm, like
adding an additional varTable2 at the end when the index exceeded, or so.

I'm curious if I've overseen some way to solve this.

> 
>> resolve name -> index
>> get current array (volatile read)
>> if index >= size: new array + arraycopy + CAS
>> write value to current or new array (non-volatile write)
> 
> If you're referring to the *values* in the array, then this is not our
> concern. JRuby does not make volatility guarantees for the actual
> reads and writes of instance variables...we only want to guarantee
> that writes will not get lost due to bad ordering of the replacement
> of the varTable with a larger version. Aaron is attempting to find a
> way for us to make that guarantee without imposing volatility on all
> reads of the varTable array, since after some stable point in
> execution there will be vastly more reads of varTable than writes
> (ideally only one write per object early on).

I understand this, but you also don't want to get concurrent writes get
lost completely. Really, I'd love to see a lock free implementation that
handles this, but haven't found one yet. That's the reason why there's
no simple implementation of a "ConcurrentArrayList".

-Michael

> 
> And to answer Hans's question about the mechanism for looking up the
> index... A JRuby object has both this varTable reference and a
> reference to its "metaclass", a RubyClass instance. RubyClass
> aggregates a CHM mapping instance variable names to immutable accessor
> objects that hold the index. So the entire path to getting the index
> should be concurrency-safe (and ideally, that path should also be hit
> rarely since we call-site cache the accessors based on a metaclass ==
> check).
> 
> - Charlie


From dl at cs.oswego.edu  Wed Jan  2 10:54:04 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 02 Jan 2013 10:54:04 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E44A36.9000509@redhat.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
Message-ID: <50E4581C.3080503@cs.oswego.edu>

On 01/02/13 09:54, David M. Lloyd wrote:
> On 12/27/2012 12:34 PM, Doug Lea wrote:
>>
>> An initial version of JDK8 java.util.concurrent.CompletableFuture is now
>> available. See javadocs at:
>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
>>
>> and source at
>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>>
> I've been looking at this and thinking about it over the past week and I have
>  some concerns.

Thanks! I see that I haven't done a good enough job conveying the
basic approach here, which is identical to that of other recent
j.u.c components (FJ, new CHM, Phaser, LongAdder, etc).

Back in JDK5, we were in the position of being able to
establish common interfaces for concurrent functionality.
But now we mainly provide hunks of mechanism that others
can then use underneath their own APIs for anything running on
JVMs, in Java or other languages -- this very much includes other
JDK java APIs. For example, we mainly act as suppliers of
functionality for the new java.util.stream APIs, and only fully
integrate wrt impact on Collections APIs.

So when someone says: I don't want/need capability X in
(CF, FJ, CHM, etc), it is not too surprising -- I expect
most usages to be layered on top of these using only the
required features. (We've now seen this for all CF methods
other than "thenRun"!) On the other hand, posts about needing
absent capability X are surprising and usually lead to changes.

(Those people using "straight" unlayered CompletableFutures
outside of any other framework will need more usage guidance
than we currently have in the javadoc documentation though.
Suggestions, small copy-paste-hackable examples, etc are
welcome.)

> Part of the problem is, I think, that the FutureTask implementation (and now
>  CompletableFuture) confuses two different functions on cancel(), the first
> being the consumer's desire to cancel an operation, and the second being the
>  implementation's ability to acknowledge it.  In other words, requesting
> cancellation is also acknowledging it.  I don't know of any use case where
> this behavior is desirable.

While it is possible to layer policy restrictions on top of cancel,
cancellation is intrinsically racy, and relies on an
underlying mechanism to resolve the races (i.e., to decide
whether cancelled or not). That's all that CF supplies.

> One other unrelated gripe I have is that in order to use many (all?) of
> .then/.orThen and friends, you must either use the FJP.commonPool() or a

(only the async methods)

> supplied executor.  It would make more sense if the new future could be
> completed using the same pool as the one being built off of (or at least if
> it could be an option).

Perhaps we could supply a standard way to layer this on, but
this would require that each CF be constructed with an Executor,
which would limit range of use and cause people who don't need
this to complain about bloat. (CFs initially have only three fields
and in some usages never require other (linked) internal objects
so are very space-conscious.)


-Doug



From headius at headius.com  Wed Jan  2 11:12:22 2013
From: headius at headius.com (Charles Oliver Nutter)
Date: Wed, 2 Jan 2013 10:12:22 -0600
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E452D5.5060902@kuli.org>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
Message-ID: <CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>

Oops, I didn't reply to all. I hate lists that don't reply to the list
by default ;-)

On Wed, Jan 2, 2013 at 9:31 AM, Michael Kuhlmann <concurrency at kuli.org> wrote:
> Am 02.01.2013 16:18, schrieb Charles Oliver Nutter:
>>
>> If you're referring to updating the array reference, I don't see how
>> this would be the case. If the CAS fails, it would re-get (volatile
>> get) the new value written by (B). From an earlier email by Aaron:
>
> If the write was to an index within the length range, then the array
> reference doesn't change (at least when you don't create a new array on
> every write). As a consequence, A's CAS operation won't fail; varTable
> still holds the previous, but modified array. The problem is that A
> already has copied the array when it wasn't modified by B.
>
> So you either need to lock on write, create a new array on every write,
> or use a reference object (like AtomicReference, but not volatile) for
> every entry in varTable. Or use some other sophisticated algorithm, like
> adding an additional varTable2 at the end when the index exceeded, or so.
>
> I'm curious if I've overseen some way to solve this.

Yes, I believe you're right. We won't lose values due to two threads
growing the array at the same time, but we could still lose writes if
one of the threads simply updated the existing array.

Chained varTable is an interesting idea, but the chained element at
the end could be lost in exactly the same way, no?

My immediate thought was to move the CAS check to a separate
"updatedCount" field...but it would need to be atomically updated :-)
We're back to full volatile semantics.

>> If you're referring to the *values* in the array, then this is not our
>> concern. JRuby does not make volatility guarantees for the actual
>> reads and writes of instance variables...we only want to guarantee
>
> I understand this, but you also don't want to get concurrent writes get
> lost completely. Really, I'd love to see a lock free implementation that
> handles this, but haven't found one yet. That's the reason why there's
> no simple implementation of a "ConcurrentArrayList".

Indeed, this is largely what we're trying to specify here.
Generalizing the discussion in that direction may draw out more ideas.

I will brainstorm a bit offline and come back.

- Charlie

From david.lloyd at redhat.com  Wed Jan  2 11:33:56 2013
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 02 Jan 2013 10:33:56 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E4581C.3080503@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu>
Message-ID: <50E46174.4060306@redhat.com>

On 01/02/2013 09:54 AM, Doug Lea wrote:
> On 01/02/13 09:54, David M. Lloyd wrote:
>> On 12/27/2012 12:34 PM, Doug Lea wrote:
>>>
>>> An initial version of JDK8 java.util.concurrent.CompletableFuture is now
>>> available. See javadocs at:
>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
>>>
>>>
>>> and source at
>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>>>
>>>
>> I've been looking at this and thinking about it over the past week and
>> I have
>>  some concerns.
>
> Thanks! I see that I haven't done a good enough job conveying the
> basic approach here, which is identical to that of other recent
> j.u.c components (FJ, new CHM, Phaser, LongAdder, etc).
>
> Back in JDK5, we were in the position of being able to
> establish common interfaces for concurrent functionality.
> But now we mainly provide hunks of mechanism that others
> can then use underneath their own APIs for anything running on
> JVMs, in Java or other languages -- this very much includes other
> JDK java APIs. For example, we mainly act as suppliers of
> functionality for the new java.util.stream APIs, and only fully
> integrate wrt impact on Collections APIs.
>
> So when someone says: I don't want/need capability X in
> (CF, FJ, CHM, etc), it is not too surprising -- I expect
> most usages to be layered on top of these using only the
> required features. (We've now seen this for all CF methods
> other than "thenRun"!) On the other hand, posts about needing
> absent capability X are surprising and usually lead to changes.

Sure, but it is probably better to allow the user to override & make 
public a method than it is to have them override & make non-public (via 
exception or whatever) when they want a deviation from the specified 
behavior.

> (Those people using "straight" unlayered CompletableFutures
> outside of any other framework will need more usage guidance
> than we currently have in the javadoc documentation though.
> Suggestions, small copy-paste-hackable examples, etc are
> welcome.)
>
>> Part of the problem is, I think, that the FutureTask implementation
>> (and now
>>  CompletableFuture) confuses two different functions on cancel(), the
>> first
>> being the consumer's desire to cancel an operation, and the second
>> being the
>>  implementation's ability to acknowledge it.  In other words, requesting
>> cancellation is also acknowledging it.  I don't know of any use case
>> where
>> this behavior is desirable.
>
> While it is possible to layer policy restrictions on top of cancel,
> cancellation is intrinsically racy, and relies on an
> underlying mechanism to resolve the races (i.e., to decide
> whether cancelled or not). That's all that CF supplies.

I understand that, and that's all it should supply.  However it's the 
cancel() method itself that is the problem here.  The mechanism should 
be implemented by a method whose purpose is merely to *acknowledge* 
cancellation, not by the method *requesting* it.

-- 
- DML

From viktor.klang at gmail.com  Wed Jan  2 12:01:58 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 2 Jan 2013 18:01:58 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E46174.4060306@redhat.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
Message-ID: <CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>

On Jan 2, 2013 5:36 PM, "David M. Lloyd" <david.lloyd at redhat.com> wrote:
>
> On 01/02/2013 09:54 AM, Doug Lea wrote:
>>
>> On 01/02/13 09:54, David M. Lloyd wrote:
>>>
>>> On 12/27/2012 12:34 PM, Doug Lea wrote:
>>>>
>>>>
>>>> An initial version of JDK8 java.util.concurrent.CompletableFuture is
now
>>>> available. See javadocs at:
>>>>
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
>>>>
>>>>
>>>> and source at
>>>>
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>>>>
>>>>
>>> I've been looking at this and thinking about it over the past week and
>>> I have
>>>  some concerns.
>>
>>
>> Thanks! I see that I haven't done a good enough job conveying the
>> basic approach here, which is identical to that of other recent
>> j.u.c components (FJ, new CHM, Phaser, LongAdder, etc).
>>
>> Back in JDK5, we were in the position of being able to
>> establish common interfaces for concurrent functionality.
>> But now we mainly provide hunks of mechanism that others
>> can then use underneath their own APIs for anything running on
>> JVMs, in Java or other languages -- this very much includes other
>> JDK java APIs. For example, we mainly act as suppliers of
>> functionality for the new java.util.stream APIs, and only fully
>> integrate wrt impact on Collections APIs.
>>
>> So when someone says: I don't want/need capability X in
>> (CF, FJ, CHM, etc), it is not too surprising -- I expect
>> most usages to be layered on top of these using only the
>> required features. (We've now seen this for all CF methods
>> other than "thenRun"!) On the other hand, posts about needing
>> absent capability X are surprising and usually lead to changes.
>
>
> Sure, but it is probably better to allow the user to override & make
public a method than it is to have them override & make non-public (via
exception or whatever) when they want a deviation from the specified
behavior.
>
>
>> (Those people using "straight" unlayered CompletableFutures
>> outside of any other framework will need more usage guidance
>> than we currently have in the javadoc documentation though.
>> Suggestions, small copy-paste-hackable examples, etc are
>> welcome.)
>>
>>> Part of the problem is, I think, that the FutureTask implementation
>>> (and now
>>>  CompletableFuture) confuses two different functions on cancel(), the
>>> first
>>> being the consumer's desire to cancel an operation, and the second
>>> being the
>>>  implementation's ability to acknowledge it.  In other words, requesting
>>> cancellation is also acknowledging it.  I don't know of any use case
>>> where
>>> this behavior is desirable.
>>
>>
>> While it is possible to layer policy restrictions on top of cancel,
>> cancellation is intrinsically racy, and relies on an
>> underlying mechanism to resolve the races (i.e., to decide
>> whether cancelled or not). That's all that CF supplies.
>
>
> I understand that, and that's all it should supply.  However it's the
cancel() method itself that is the problem here.  The mechanism should be
implemented by a method whose purpose is merely to *acknowledge*
cancellation, not by the method *requesting* it.

How about "isCancelled()" ;)

>
> --
> - DML
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130102/d6212691/attachment.html>

From nathan.reynolds at oracle.com  Wed Jan  2 12:05:48 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 02 Jan 2013 10:05:48 -0700
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
Message-ID: <50E468EC.9010206@oracle.com>

JEP 159 Enhanced Class Redefinition (http://openjdk.java.net/jeps/159) 
might be the good solution... once it is available.  Every time a new 
field is assigned for an object, simply redefine the class to include 
that field.  This improves performance since fields can then be accessed 
directly instead of having to lookup an index and dereference into an array.

No funky business with volatiles, atomics and locks.  The JVM takes care 
of making this change in a thread-safe manner (i.e. stop the world) so 
that no writes are lost.

There will be a performance penalty during a warm up period while new 
fields are discovered.  This penalty, kind of like a full GC, will be 
much higher than simply resizing the array.

Another side benefit is that individual fields can be declared volatile, 
final, etc.  I don't know if any of this is needed.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 1/2/2013 9:12 AM, Charles Oliver Nutter wrote:
> Oops, I didn't reply to all. I hate lists that don't reply to the list
> by default ;-)
>
> On Wed, Jan 2, 2013 at 9:31 AM, Michael Kuhlmann <concurrency at kuli.org> wrote:
>> Am 02.01.2013 16:18, schrieb Charles Oliver Nutter:
>>> If you're referring to updating the array reference, I don't see how
>>> this would be the case. If the CAS fails, it would re-get (volatile
>>> get) the new value written by (B). From an earlier email by Aaron:
>> If the write was to an index within the length range, then the array
>> reference doesn't change (at least when you don't create a new array on
>> every write). As a consequence, A's CAS operation won't fail; varTable
>> still holds the previous, but modified array. The problem is that A
>> already has copied the array when it wasn't modified by B.
>>
>> So you either need to lock on write, create a new array on every write,
>> or use a reference object (like AtomicReference, but not volatile) for
>> every entry in varTable. Or use some other sophisticated algorithm, like
>> adding an additional varTable2 at the end when the index exceeded, or so.
>>
>> I'm curious if I've overseen some way to solve this.
> Yes, I believe you're right. We won't lose values due to two threads
> growing the array at the same time, but we could still lose writes if
> one of the threads simply updated the existing array.
>
> Chained varTable is an interesting idea, but the chained element at
> the end could be lost in exactly the same way, no?
>
> My immediate thought was to move the CAS check to a separate
> "updatedCount" field...but it would need to be atomically updated :-)
> We're back to full volatile semantics.
>
>>> If you're referring to the *values* in the array, then this is not our
>>> concern. JRuby does not make volatility guarantees for the actual
>>> reads and writes of instance variables...we only want to guarantee
>> I understand this, but you also don't want to get concurrent writes get
>> lost completely. Really, I'd love to see a lock free implementation that
>> handles this, but haven't found one yet. That's the reason why there's
>> no simple implementation of a "ConcurrentArrayList".
> Indeed, this is largely what we're trying to specify here.
> Generalizing the discussion in that direction may draw out more ideas.
>
> I will brainstorm a bit offline and come back.
>
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130102/771ed2bf/attachment.html>

From david.lloyd at redhat.com  Wed Jan  2 12:06:00 2013
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 02 Jan 2013 11:06:00 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
Message-ID: <50E468F8.7020706@redhat.com>

On 01/02/2013 11:01 AM, ?iktor ?lang wrote:
>
> On Jan 2, 2013 5:36 PM, "David M. Lloyd" <david.lloyd at redhat.com
> <mailto:david.lloyd at redhat.com>> wrote:
>  >
>  > On 01/02/2013 09:54 AM, Doug Lea wrote:
>  >>
>  >> On 01/02/13 09:54, David M. Lloyd wrote:
>  >>>
>  >>> On 12/27/2012 12:34 PM, Doug Lea wrote:
>  >>>>
>  >>>>
>  >>>> An initial version of JDK8 java.util.concurrent.CompletableFuture
> is now
>  >>>> available. See javadocs at:
>  >>>>
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
>  >>>>
>  >>>>
>  >>>> and source at
>  >>>>
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>  >>>>
>  >>>>
>  >>> I've been looking at this and thinking about it over the past week and
>  >>> I have
>  >>>  some concerns.
>  >>
>  >>
>  >> Thanks! I see that I haven't done a good enough job conveying the
>  >> basic approach here, which is identical to that of other recent
>  >> j.u.c components (FJ, new CHM, Phaser, LongAdder, etc).
>  >>
>  >> Back in JDK5, we were in the position of being able to
>  >> establish common interfaces for concurrent functionality.
>  >> But now we mainly provide hunks of mechanism that others
>  >> can then use underneath their own APIs for anything running on
>  >> JVMs, in Java or other languages -- this very much includes other
>  >> JDK java APIs. For example, we mainly act as suppliers of
>  >> functionality for the new java.util.stream APIs, and only fully
>  >> integrate wrt impact on Collections APIs.
>  >>
>  >> So when someone says: I don't want/need capability X in
>  >> (CF, FJ, CHM, etc), it is not too surprising -- I expect
>  >> most usages to be layered on top of these using only the
>  >> required features. (We've now seen this for all CF methods
>  >> other than "thenRun"!) On the other hand, posts about needing
>  >> absent capability X are surprising and usually lead to changes.
>  >
>  >
>  > Sure, but it is probably better to allow the user to override & make
> public a method than it is to have them override & make non-public (via
> exception or whatever) when they want a deviation from the specified
> behavior.
>  >
>  >
>  >> (Those people using "straight" unlayered CompletableFutures
>  >> outside of any other framework will need more usage guidance
>  >> than we currently have in the javadoc documentation though.
>  >> Suggestions, small copy-paste-hackable examples, etc are
>  >> welcome.)
>  >>
>  >>> Part of the problem is, I think, that the FutureTask implementation
>  >>> (and now
>  >>>  CompletableFuture) confuses two different functions on cancel(), the
>  >>> first
>  >>> being the consumer's desire to cancel an operation, and the second
>  >>> being the
>  >>>  implementation's ability to acknowledge it.  In other words,
> requesting
>  >>> cancellation is also acknowledging it.  I don't know of any use case
>  >>> where
>  >>> this behavior is desirable.
>  >>
>  >>
>  >> While it is possible to layer policy restrictions on top of cancel,
>  >> cancellation is intrinsically racy, and relies on an
>  >> underlying mechanism to resolve the races (i.e., to decide
>  >> whether cancelled or not). That's all that CF supplies.
>  >
>  >
>  > I understand that, and that's all it should supply.  However it's the
> cancel() method itself that is the problem here.  The mechanism should
> be implemented by a method whose purpose is merely to *acknowledge*
> cancellation, not by the method *requesting* it.
>
> How about "isCancelled()" ;)

That method seems to only query whether the invocation was cancelled. 
It does not actually cause the future to change state from incomplete to 
cancelled, which is what I'm referring to.

-- 
- DML

From dl at cs.oswego.edu  Wed Jan  2 12:16:39 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 02 Jan 2013 12:16:39 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E468F8.7020706@redhat.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com>
Message-ID: <50E46B77.5060004@cs.oswego.edu>

On 01/02/13 12:06, David M. Lloyd wrote:
>>  > I understand that, and that's all it should supply.  However it's the
>> cancel() method itself that is the problem here.  The mechanism should
>> be implemented by a method whose purpose is merely to *acknowledge*
>> cancellation, not by the method *requesting* it.
>>
>> How about "isCancelled()" ;)
>
> That method seems to only query whether the invocation was cancelled. It does
> not actually cause the future to change state from incomplete to cancelled,
> which is what I'm referring to.
>

I gather from this that the information that you would like
is whether this cancelled CF, and/or any task running it, was
never run vs never started because the caller checked
isCancelled before doing anuthing vs started but aborted
mid-execution because the function checked isCancelled vs
completed but results thrown away?

-Doug


From david.lloyd at redhat.com  Wed Jan  2 12:41:34 2013
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 02 Jan 2013 11:41:34 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E46B77.5060004@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
Message-ID: <50E4714E.6000401@redhat.com>

On 01/02/2013 11:16 AM, Doug Lea wrote:
> On 01/02/13 12:06, David M. Lloyd wrote:
>>>  > I understand that, and that's all it should supply.  However it's the
>>> cancel() method itself that is the problem here.  The mechanism should
>>> be implemented by a method whose purpose is merely to *acknowledge*
>>> cancellation, not by the method *requesting* it.
>>>
>>> How about "isCancelled()" ;)
>>
>> That method seems to only query whether the invocation was cancelled.
>> It does
>> not actually cause the future to change state from incomplete to
>> cancelled,
>> which is what I'm referring to.
>>
>
> I gather from this that the information that you would like
> is whether this cancelled CF, and/or any task running it, was
> never run vs never started because the caller checked
> isCancelled before doing anuthing vs started but aborted
> mid-execution because the function checked isCancelled vs
> completed but results thrown away?

In practice I've found the distinction between cancelled-before-started 
versus cancelled-after-partially-completed is not really useful for 
much; I'm more interested in the ability for the task to acknowledge 
cancellation or ignore a cancellation request, and our behavior for 
cancel() extends from that.

Conceptually speaking, the caller of cancel() can't know whether the 
task was indeed cancelled until/unless the task itself acknowledges its 
cancellation.  This is especially true in distributed scenarios, but is 
also true in many more mundane situations.  The specification of 
cancel() in Future states that the return value indicates whether the 
task *was* cancelled, which cannot be known until/unless the task 
acknowledges the request to cancel or completes in another way (normally 
or exceptionally).

It makes more sense if you look at it from the perspective of its 
possible states.  The states are "running", "cancelled", "failed", and 
"complete".  The task should have one method each for indicating the 
latter three states; the requester should have a method to indicate the 
desire to cancel instead of finishing (if possible), but such a method 
cannot also change the state or else the control over the result is 
wrested from the task (otherwise this makes tasks much more difficult to 
implement sensibly).

It's much simpler to implement a task when the contract is merely "call 
one of these three methods", with an isCancelRequested() pollable flag 
(putting the decision to support cancellation into the hands of the task 
implementer where it belongs).  There are definitely cases where it is 
very inconvenient to have your task be forcibly cancelled out from under 
you; this is just as bad as letting other parties complete or fail your 
task outside of the task implementation.

I hope this clarifies things.
-- 
- DML

From aaron.grunthal at infinite-source.de  Wed Jan  2 12:42:42 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Wed, 02 Jan 2013 18:42:42 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
Message-ID: <50E47192.8010406@infinite-source.de>

We're considering the option of implementing a lighter version of the 
StampedLock logic.

The logic would look like the following for write OPs:

private Object[] varTable;
private volatile int varTableGrowthStamp;


write(index,value) {

1. oldstamp = stamp // volatile read

goto 1 if oldstamp is odd // spin wait

2. local = varTable // volatile read

if index >= length
   3. CAS stamp to oldstamp+1 (odd) // goto 1 on failure
   4. allocate
   5. arraycopy
   6. local[index] = value // volatile? ordered? normal?
   7. stamp += 1 (even) // volatile write
else
   8. local[index] = value // volatile? ordered? normal?
   9. oldstamp == stamp // volatile validation. goto 1 on failure.
end

}


Since we very rarely grow the var table spinning without thread parking 
should be fine here. We just want to make the regular case of normal 
assignment the fast path.

StampedLock requires fences for the try-read + validate case (which we 
would emulate here) because step 8 and 9 could get reordered.
We have the benefit of direct Unsafe access here, so we can guarantee 
the ordering ourselves as necessary.



Questions to the experts:

a) would this work/did I miss something?
b) which write operation would be necessary for steps 6 and 8? If I 
understand correctly then a normal write for 6 and a putOrderedObject 
for 8 should be sufficient and without penalty on x86.

- Aaron

From dl at cs.oswego.edu  Wed Jan  2 12:57:42 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 02 Jan 2013 12:57:42 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E4714E.6000401@redhat.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com>
Message-ID: <50E47516.5090000@cs.oswego.edu>

On 01/02/13 12:41, David M. Lloyd wrote:

> It makes more sense if you look at it from the perspective of its possible
> states.  The states are "running", "cancelled", "failed", and "complete".  The
> task should have one method each for indicating the latter three states; the
> requester should have a method to indicate the desire to cancel instead of
> finishing (if possible), but such a method cannot also change the state or else
> the control over the result is wrested from the task (otherwise this makes tasks
> much more difficult to implement sensibly).

So it seems that you would like to be able to detect the case of:
"even though this task was cancelled, I finished it anyway, and here
is the result"?

It is possible to implement this using the dreaded obtrudeValue :-)
   if (!x.complete(result) && x.isCancelled()) x.obtrudeValue(result);

But perhaps a bit nicer with an added obtrudeException which
would allow you to define and use special CancellationException
subclass that holds the result.

-Doug


From headius at headius.com  Wed Jan  2 13:27:03 2013
From: headius at headius.com (Charles Oliver Nutter)
Date: Wed, 2 Jan 2013 12:27:03 -0600
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E468EC.9010206@oracle.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E468EC.9010206@oracle.com>
Message-ID: <CAE-f1xR3Qo9SG9WTVR1V6_M8SrBaYhMAtVqmL727w=02agKZ7Q@mail.gmail.com>

This will certainly be an option in the future; we do have the
capability to generate a class at first-construction time for a given
Ruby type, and this would additionally allow us to expand its fields.
That's an orthogonal issue, though, since we don't usually generate
that class, and the feature won't be available on older JVMs. We still
need a solution for the generic Object[] varTable reference mechanism
too.

Thanks for the link!

- Charlie

On Wed, Jan 2, 2013 at 11:05 AM, Nathan Reynolds
<nathan.reynolds at oracle.com> wrote:
> JEP 159 Enhanced Class Redefinition (http://openjdk.java.net/jeps/159) might
> be the good solution... once it is available.  Every time a new field is
> assigned for an object, simply redefine the class to include that field.
> This improves performance since fields can then be accessed directly instead
> of having to lookup an index and dereference into an array.
>
> No funky business with volatiles, atomics and locks.  The JVM takes care of
> making this change in a thread-safe manner (i.e. stop the world) so that no
> writes are lost.
>
> There will be a performance penalty during a warm up period while new fields
> are discovered.  This penalty, kind of like a full GC, will be much higher
> than simply resizing the array.
>
> Another side benefit is that individual fields can be declared volatile,
> final, etc.  I don't know if any of this is needed.
>
>
> Nathan Reynolds | Architect | 602.333.9091
> Oracle PSR Engineering | Server Technology
> On 1/2/2013 9:12 AM, Charles Oliver Nutter wrote:
>
> Oops, I didn't reply to all. I hate lists that don't reply to the list
> by default ;-)
>
> On Wed, Jan 2, 2013 at 9:31 AM, Michael Kuhlmann <concurrency at kuli.org>
> wrote:
>
> Am 02.01.2013 16:18, schrieb Charles Oliver Nutter:
>
> If you're referring to updating the array reference, I don't see how
> this would be the case. If the CAS fails, it would re-get (volatile
> get) the new value written by (B). From an earlier email by Aaron:
>
> If the write was to an index within the length range, then the array
> reference doesn't change (at least when you don't create a new array on
> every write). As a consequence, A's CAS operation won't fail; varTable
> still holds the previous, but modified array. The problem is that A
> already has copied the array when it wasn't modified by B.
>
> So you either need to lock on write, create a new array on every write,
> or use a reference object (like AtomicReference, but not volatile) for
> every entry in varTable. Or use some other sophisticated algorithm, like
> adding an additional varTable2 at the end when the index exceeded, or so.
>
> I'm curious if I've overseen some way to solve this.
>
> Yes, I believe you're right. We won't lose values due to two threads
> growing the array at the same time, but we could still lose writes if
> one of the threads simply updated the existing array.
>
> Chained varTable is an interesting idea, but the chained element at
> the end could be lost in exactly the same way, no?
>
> My immediate thought was to move the CAS check to a separate
> "updatedCount" field...but it would need to be atomically updated :-)
> We're back to full volatile semantics.
>
> If you're referring to the *values* in the array, then this is not our
> concern. JRuby does not make volatility guarantees for the actual
> reads and writes of instance variables...we only want to guarantee
>
> I understand this, but you also don't want to get concurrent writes get
> lost completely. Really, I'd love to see a lock free implementation that
> handles this, but haven't found one yet. That's the reason why there's
> no simple implementation of a "ConcurrentArrayList".
>
> Indeed, this is largely what we're trying to specify here.
> Generalizing the discussion in that direction may draw out more ideas.
>
> I will brainstorm a bit offline and come back.
>
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From david.lloyd at redhat.com  Wed Jan  2 13:46:32 2013
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 02 Jan 2013 12:46:32 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E47516.5090000@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
Message-ID: <50E48088.7080101@redhat.com>

On 01/02/2013 11:57 AM, Doug Lea wrote:
> On 01/02/13 12:41, David M. Lloyd wrote:
>
>> It makes more sense if you look at it from the perspective of its
>> possible
>> states.  The states are "running", "cancelled", "failed", and
>> "complete".  The
>> task should have one method each for indicating the latter three
>> states; the
>> requester should have a method to indicate the desire to cancel
>> instead of
>> finishing (if possible), but such a method cannot also change the
>> state or else
>> the control over the result is wrested from the task (otherwise this
>> makes tasks
>> much more difficult to implement sensibly).
>
> So it seems that you would like to be able to detect the case of:
> "even though this task was cancelled, I finished it anyway, and here
> is the result"?

Right.

> It is possible to implement this using the dreaded obtrudeValue :-)
>    if (!x.complete(result) && x.isCancelled()) x.obtrudeValue(result);
>
> But perhaps a bit nicer with an added obtrudeException which
> would allow you to define and use special CancellationException
> subclass that holds the result.

I view obtrude and friends as a big step in a hazardous direction 
without good justification.  Yes you *can* solve the problem that way; 
but adding "flexibility" like that in the end removes flexibility by 
adding more ways in which things can break, which must be accounted for 
by robust user programs.

Framework developers (like me) won't use these constructs for these 
reasons, and normal end-users (like your average ##java IRC denizen) 
presented with APIs where there are many shoot-yourself-in-the-foot 
methods don't really know what they're getting into, thus I'd warn them 
away from this construct as well.  And frankly I'm not sure who that 
leaves.  Advanced users who are lazy enough to just use what is provided 
yet know enough not to expose this as an API anywhere?  Seems unlikely 
to me.

APIs should be as simple as possible, but no simpler.  And I think 
simplicity means conceptual simplicity as well as implementation 
simplicity.  Conceptually, if a Future can change its state after a 
result (success or failure or cancellation) has been set, you've 
introduced a pretty significant abstraction leak.  And I'm especially 
not clear on why *this* is the best solution to whatever the problem is 
that it purports to solve.

-- 
- DML

From aaron.grunthal at infinite-source.de  Wed Jan  2 13:56:34 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Wed, 02 Jan 2013 19:56:34 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E47192.8010406@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
Message-ID: <50E482E2.5060705@infinite-source.de>

Forgot one step, revised code:


write(index,value) {

1. oldstamp = stamp // volatile read

goto 1 if oldstamp is odd // spin wait

2. local = varTable // volatile read

if index >= length
   3. CAS stamp to oldstamp+1 (odd) // goto 1 on failure
   4. allocate
   5. arraycopy
   6. local[index] = value // volatile? ordered? normal?
   7. varTable = local // volatile? ordered? normal?
   8. stamp += 1 (even) // volatile write
else
   9. local[index] = value // volatile? ordered? normal?
   10. oldstamp == stamp // volatile validation. goto 1 on failure.
end

}

revised questions:

a) would this work/did I miss something?
b) which write operation would be necessary for steps 6, 7  and 9? If I 
understand correctly then a normal write for 6 and a putOrderedObject 
for 7/9 should be sufficient and without penalty on x86.

From dl at cs.oswego.edu  Wed Jan  2 14:00:13 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 02 Jan 2013 14:00:13 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E48088.7080101@redhat.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com>
Message-ID: <50E483BD.7040506@cs.oswego.edu>

On 01/02/13 13:46, David M. Lloyd wrote:
>
>> But perhaps a bit nicer with an added obtrudeException which
>> would allow you to define and use special CancellationException
>> subclass that holds the result.
>
> I view obtrude and friends as a big step in a hazardous direction without good
> justification.  Yes you *can* solve the problem that way; but adding
> "flexibility" like that in the end removes flexibility by adding more ways in
> which things can break, which must be accounted for by robust user programs.
>

Sorry that I am still not seeing alternatives for your cancellation
scenario:

Suppose someone cancels a CF. This triggers all dependent completions.
But there may be other ongoing computations that haven't yet noticed
and so try to compute/complete results. If you would like to keep these
results instead of throw them away, then no matter how you do this,
there will be a race among some activities seeing vs not seeing
results. The obtrude methods are completely up front about this race,
making the pick-your-poison nature of this scenario as clear as it
could be. Anything else would be equivalently racy but less clearly so.

-Doug




From viktor.klang at gmail.com  Wed Jan  2 15:57:25 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 2 Jan 2013 21:57:25 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E483BD.7040506@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
Message-ID: <CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>

TBH the only "sane" semantics for cancellation I've seen so far is:

"cancellation as a reverse-direction signal" i.e. the signal that the
operation could be cancelled travelse "upstream", and then the producer at
the top inspects the "isCancellationRequested" and then the producer
completes the Future with a CancellationException, that then travels
downstream.

So cancel would only ever return void, and it would never directly complete
the Future with the CancellationException.

Cheers,
?


On Wed, Jan 2, 2013 at 8:00 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/02/13 13:46, David M. Lloyd wrote:
>
>>
>>  But perhaps a bit nicer with an added obtrudeException which
>>> would allow you to define and use special CancellationException
>>> subclass that holds the result.
>>>
>>
>> I view obtrude and friends as a big step in a hazardous direction without
>> good
>> justification.  Yes you *can* solve the problem that way; but adding
>> "flexibility" like that in the end removes flexibility by adding more
>> ways in
>> which things can break, which must be accounted for by robust user
>> programs.
>>
>>
> Sorry that I am still not seeing alternatives for your cancellation
> scenario:
>
> Suppose someone cancels a CF. This triggers all dependent completions.
> But there may be other ongoing computations that haven't yet noticed
> and so try to compute/complete results. If you would like to keep these
> results instead of throw them away, then no matter how you do this,
> there will be a race among some activities seeing vs not seeing
> results. The obtrude methods are completely up front about this race,
> making the pick-your-poison nature of this scenario as clear as it
> could be. Anything else would be equivalently racy but less clearly so.
>
> -Doug
>
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130102/4d45bb69/attachment-0001.html>

From hans.boehm at hp.com  Wed Jan  2 16:22:03 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 2 Jan 2013 21:22:03 +0000
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E482E2.5060705@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de>
	<50E1A44D.1090509@oracle.com>	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD236967CCC@G9W0725.americas.hpqcorp.net>

> From: Aaron Grunthal
> 
> Forgot one step, revised code:
> 
> 
> write(index,value) {
> 
> 1. oldstamp = stamp // volatile read
> 
> goto 1 if oldstamp is odd // spin wait
> 
> 2. local = varTable // volatile read
> 
> if index >= length
>    3. CAS stamp to oldstamp+1 (odd) // goto 1 on failure
>    4. allocate
>    5. arraycopy
>    6. local[index] = value // volatile? ordered? normal?
>    7. varTable = local // volatile? ordered? normal?
>    8. stamp += 1 (even) // volatile write
> else
>    9. local[index] = value // volatile? ordered? normal?
>    10. oldstamp == stamp // volatile validation. goto 1 on failure.
> end
> 
> }
> 
> revised questions:
> 
> a) would this work/did I miss something?
> b) which write operation would be necessary for steps 6, 7  and 9? If I
> understand correctly then a normal write for 6 and a putOrderedObject
> for 7/9 should be sufficient and without penalty on x86.

Readers don't check stamp?

I don't immediately see a fundamental problem beyond the read memory model issues, but I could easily be missing something.  If I understand putOrderedObject correctly, you need 9 to be a volatile store to prevent reordering with the load in 10.  In particular, on x86, you do need a fence between the two, which presumably putOrderedObject doesn't specify.  You need 7 to be at least ordered, to ensure ordering with respect to 5 and 6.  Normal had better be OK for 6, since I think its constraints are weaker than those on arraycopy.

Needless to say, everything here is on pretty thin ice, given the lack of believable precise specifications all the way around.

Hans


From dl at cs.oswego.edu  Wed Jan  2 16:24:18 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 02 Jan 2013 16:24:18 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
Message-ID: <50E4A582.8010400@cs.oswego.edu>

On 01/02/13 15:57, ?iktor ?lang wrote:
> TBH the only "sane" semantics for cancellation I've seen so far is:

I don't want to be in the business of arguing sanity :-) but ...

>
> "cancellation as a reverse-direction signal" i.e. the signal that the operation
> could be cancelled travelse "upstream", and then the producer at the top
> inspects the "isCancellationRequested" and then the producer completes the
> Future with a CancellationException, that then travels downstream.

... this version of sanity leads to the need for alternative rules
if there is only one producer and it fails or otherwise never signals,
as well as rules if there are multiple potential producers.
Which lead to different versions of the same basic issues.

In the history of dealing with cancellation-like issues, the
most successful (for example, out-of-band TCP, OS process
management, and even CSP "Alts") have coped with explicit raciness.
Alternatives have been prone to causing thread/systems
to become stuck. If anyone wants to layer such rules on top
of CF it is fine with me, so long as the base functionality
offers a way out.

-Doug





From viktor.klang at gmail.com  Wed Jan  2 17:34:01 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 2 Jan 2013 23:34:01 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E4A582.8010400@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
Message-ID: <CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>

On Wed, Jan 2, 2013 at 10:24 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/02/13 15:57, ?iktor ?lang wrote:
>
>> TBH the only "sane" semantics for cancellation I've seen so far is:
>>
>
> I don't want to be in the business of arguing sanity :-) but ...


We got to have some fun here, right? :-)


>
>
>
>> "cancellation as a reverse-direction signal" i.e. the signal that the
>> operation
>> could be cancelled travelse "upstream", and then the producer at the top
>> inspects the "isCancellationRequested" and then the producer completes the
>> Future with a CancellationException, that then travels downstream.
>>
>
> ... this version of sanity leads to the need for alternative rules
> if there is only one producer and it fails or otherwise never signals,
>

That is not an issue if there is no guarantee that a Future will ever be
completed.


> as well as rules if there are multiple potential producers.
>

See now why I don't like cancellation? :-)


> Which lead to different versions of the same basic issues.
>
> In the history of dealing with cancellation-like issues, the
> most successful (for example, out-of-band TCP, OS process
> management, and even CSP "Alts") have coped with explicit raciness.
> Alternatives have been prone to causing thread/systems
> to become stuck. If anyone wants to layer such rules on top
> of CF it is fine with me, so long as the base functionality
> offers a way out.


Can we list some potential problems, use-case wise, and see if we can find
something that ticks all the boxes?

Cheers,
?



>
>
> -Doug
>
>
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130102/0b37929b/attachment.html>

From aaron.grunthal at infinite-source.de  Wed Jan  2 17:34:41 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Wed, 02 Jan 2013 23:34:41 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236967CCC@G9W0725.americas.hpqcorp.net>
References: <50E1904E.9030406@infinite-source.de>
	<50E1A44D.1090509@oracle.com>	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236967CCC@G9W0725.americas.hpqcorp.net>
Message-ID: <50E4B601.6000005@infinite-source.de>

On 02.01.2013 22:22, Boehm, Hans wrote:
> Readers don't check stamp?

No, the only useful information we could gain from them is that a 
reassignment is in progress, but that only concerns previously-default 
values being written to. Other writes would be blocked by the spin wait 
and the newly added value would be... new anyway. So returning slightly 
stale data is perfectly fine as those are supposed to resemble 
non-volatile field accesses.

> If I understand putOrderedObject correctly, you need 9 to be a volatile store to prevent reordering with the load in 10. In particular, on x86, you do need a fence between the two, which presumably putOrderedObject doesn't specify.

Mhh, too bad. We wanted to avoid volatile writes for the normal 
assignment codepath. But I guess there's no way around that for now.

> You need 7 to be at least ordered, to ensure ordering with respect to 5 and 6.  Normal had better be OK for 6, since I think its constraints are weaker than those on arraycopy.

Ok, thanks.

> Needless to say, everything here is on pretty thin ice, given the lack of believable precise specifications all the way around.

Yes, we'll provide a synchronized() codepath as fallback and maybe one 
that uses the same logic as StampedLock with fences on JVMs that provide 
them. But we also need a fast codepath for java 6/7, so this would be it.


More generally our main problem is that we lack data structures that 
provide non-volatile read semantics (i.e. possibly stale under 
concurrent modification but fast and loop-hoistable) while using atomic 
write algorithms, mostly for internal consistency under concurrent access.

- Aaron

From yu.lin.86 at gmail.com  Wed Jan  2 21:36:59 2013
From: yu.lin.86 at gmail.com (Yu Lin)
Date: Wed, 2 Jan 2013 20:36:59 -0600
Subject: [concurrency-interest] Projects that use
	java.util.concurrent.FutureTask
Message-ID: <CAAL-3PZCqFJjpkWZcOoFrwjuSp6WYAGb+_gcikjb8oaFecf-3w@mail.gmail.com>

Hi all,

I'm a Ph.D. student at UIUC. We are now studying the usage of
java.util.concurrent.FutureTask,
but we meet problem to find real-world projects that use FutureTask correctly
(we search the code by code search engine Koder and Github).

The code pattern we're trying to find is like:

================================
FutureTask ft = new FutureTask(...)
new Thread(ft).start() // or a ThreadPool to start a task

... // Here are some other computations such that FutureTask can run
asynchronously

Result r = ft.get() // get the result from FutureTask
=================================

However, most programs we find invoke "FutureTask.get()" immediately
after starting the
FutureTask (i.e., there are no computations in between). This is
incorrect because there
is no performance improvement in such usage.

Thus, we're wondering does any one know any projects that use
FutureTask (a lot)?
Or does any one contribute to open source projects that use
FutureTask? So we can use
them as experiment subjects.

Thanks a lot,
Yu Lin

From joe.bowbeer at gmail.com  Wed Jan  2 22:08:36 2013
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 2 Jan 2013 19:08:36 -0800
Subject: [concurrency-interest] Projects that use
	java.util.concurrent.FutureTask
In-Reply-To: <CAAL-3PZCqFJjpkWZcOoFrwjuSp6WYAGb+_gcikjb8oaFecf-3w@mail.gmail.com>
References: <CAAL-3PZCqFJjpkWZcOoFrwjuSp6WYAGb+_gcikjb8oaFecf-3w@mail.gmail.com>
Message-ID: <CAHzJPEr0XTVCpZbt9u=xp=VAEakQ6ncpcnvr9cSjSxTxBORdTw@mail.gmail.com>

Java's SwingWorker and Android's AsyncTask both employ FutureTask in their
implementation, so you might obtain more hits if you widen your search.

The RemoteTable implementation in my "Last Word on Swing Threads" schedules
its SwingWorker tasks on a SingleThreadExecutor, which runs the tasks
sequentially on a single thread.

To find RemoteTable, search the WayBack machine at archive.org for "
http://java.sun.com/products/jfc/tsc/articles/threads/threads3.html"


On Wed, Jan 2, 2013 at 6:36 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> Hi all,
>
> I'm a Ph.D. student at UIUC. We are now studying the usage of
> java.util.concurrent.FutureTask,
> but we meet problem to find real-world projects that use FutureTask
> correctly
> (we search the code by code search engine Koder and Github).
>
> The code pattern we're trying to find is like:
>
> ================================
> FutureTask ft = new FutureTask(...)
> new Thread(ft).start() // or a ThreadPool to start a task
>
> ... // Here are some other computations such that FutureTask can run
> asynchronously
>
> Result r = ft.get() // get the result from FutureTask
> =================================
>
> However, most programs we find invoke "FutureTask.get()" immediately
> after starting the
> FutureTask (i.e., there are no computations in between). This is
> incorrect because there
> is no performance improvement in such usage.
>
> Thus, we're wondering does any one know any projects that use
> FutureTask (a lot)?
> Or does any one contribute to open source projects that use
> FutureTask? So we can use
> them as experiment subjects.
>
> Thanks a lot,
> Yu Lin
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130102/fce2e4dd/attachment-0001.html>

From mikeb01 at gmail.com  Thu Jan  3 00:40:11 2013
From: mikeb01 at gmail.com (Michael Barker)
Date: Thu, 3 Jan 2013 18:40:11 +1300
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CACuKZqH1sjxCoX1oAfhkW-qrX=4yd+K8aHZ9BuH9be6K7KOYTg@mail.gmail.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CACuKZqH1sjxCoX1oAfhkW-qrX=4yd+K8aHZ9BuH9be6K7KOYTg@mail.gmail.com>
Message-ID: <CALwNKeQ=9BvwsXahUozQGVG2rh-nYn+O_SF+KWKOdUT6mC0UwQ@mail.gmail.com>

The comments on false sharing remind me of the false sharing
article[1] on 1024cores.net.  The cost of the recording the counters
could be reduced by roughly an order of magnitude if, instead of using
a shared structure, the counters were stored in per thread structures
and the responsibility for aggregating the values fell to the reading
code (that is off of your main application hot code path).  The use of
a shared structure that has contended writes will introduce
scalability bottleneck.  If you can be certain that only one thread
will write to the counter then you don't need an expensive CAS
operation to handle the increment and a cheaper, get, add and lazySet
would suffice.

[1] http://www.1024cores.net/home/lock-free-algorithms/false-sharing---false

Mike.

On Wed, Jan 2, 2013 at 8:30 AM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
> Is false sharing a big concern for variables each of which is intended
> to be shared? If we have N CPUs randomly updating M variables, does it
> make a lot of difference whether the variables are on the same cache
> line or not?
>
> Zhong Yu
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From kirk at kodewerk.com  Thu Jan  3 01:11:37 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Thu, 3 Jan 2013 07:11:37 +0100
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CALwNKeQ=9BvwsXahUozQGVG2rh-nYn+O_SF+KWKOdUT6mC0UwQ@mail.gmail.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CACuKZqH1sjxCoX1oAfhkW-qrX=4yd+K8aHZ9BuH9be6K7KOYTg@mail.gmail.com>
	<CALwNKeQ=9BvwsXahUozQGVG2rh-nYn+O_SF+KWKOdUT6mC0UwQ@mail.gmail.com>
Message-ID: <66E3858A-2D40-4343-BD0F-1F7C3620605A@kodewerk.com>

Hi Michael,

This technique is useful well beyond just counters and false sharing.

Regards,
Kirk

On 2013-01-03, at 6:40 AM, Michael Barker <mikeb01 at gmail.com> wrote:

> The comments on false sharing remind me of the false sharing
> article[1] on 1024cores.net.  The cost of the recording the counters
> could be reduced by roughly an order of magnitude if, instead of using
> a shared structure, the counters were stored in per thread structures
> and the responsibility for aggregating the values fell to the reading
> code (that is off of your main application hot code path).  The use of
> a shared structure that has contended writes will introduce
> scalability bottleneck.  If you can be certain that only one thread
> will write to the counter then you don't need an expensive CAS
> operation to handle the increment and a cheaper, get, add and lazySet
> would suffice.
> 
> [1] http://www.1024cores.net/home/lock-free-algorithms/false-sharing---false
> 
> Mike.
> 
> On Wed, Jan 2, 2013 at 8:30 AM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>> Is false sharing a big concern for variables each of which is intended
>> to be shared? If we have N CPUs randomly updating M variables, does it
>> make a lot of difference whether the variables are on the same cache
>> line or not?
>> 
>> Zhong Yu
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From hans.boehm at hp.com  Thu Jan  3 01:32:06 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 3 Jan 2013 06:32:06 +0000
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E4B601.6000005@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de>
	<50E1A44D.1090509@oracle.com>	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236967CCC@G9W0725.americas.hpqcorp.net>
	<50E4B601.6000005@infinite-source.de>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD236968078@G9W0725.americas.hpqcorp.net>

> From: Aaron Grunthal [mailto:aaron.grunthal at infinite-source.de]
> 
> More generally our main problem is that we lack data structures that provide
> non-volatile read semantics (i.e. possibly stale under concurrent modification
> but fast and loop-hoistable) while using atomic write algorithms, mostly for
> internal consistency under concurrent access.
> 
We could debate how useful those are in general.  It seems to me that you need them because you're implementing a programming language that provides Java-memory-model-like semantics for data races.  That's certainly entirely reasonable.  But I'm not convinced that such data structures are correctly usable by enough programmers to make them terribly interesting for direct programmer use.  And if I were designing a programming language now, I'd work hard to avoid Java-like data race semantics to start with.

Hans


From concurrency at kuli.org  Thu Jan  3 03:06:57 2013
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Thu, 03 Jan 2013 09:06:57 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
Message-ID: <50E53C21.5080502@kuli.org>

Am 02.01.2013 17:12, schrieb Charles Oliver Nutter:
> On Wed, Jan 2, 2013 at 9:31 AM, Michael Kuhlmann <concurrency at kuli.org> wrote:
>> Am 02.01.2013 16:18, schrieb Charles Oliver Nutter:
>>>
>>> If you're referring to updating the array reference, I don't see how
>>> this would be the case. If the CAS fails, it would re-get (volatile
>>> get) the new value written by (B). From an earlier email by Aaron:
>>
>> If the write was to an index within the length range, then the array
>> reference doesn't change (at least when you don't create a new array on
>> every write). As a consequence, A's CAS operation won't fail; varTable
>> still holds the previous, but modified array. The problem is that A
>> already has copied the array when it wasn't modified by B.
>>
>> So you either need to lock on write, create a new array on every write,
>> or use a reference object (like AtomicReference, but not volatile) for
>> every entry in varTable. Or use some other sophisticated algorithm, like
>> adding an additional varTable2 at the end when the index exceeded, or so.
>>
>> I'm curious if I've overseen some way to solve this.
> 
> Yes, I believe you're right. We won't lose values due to two threads
> growing the array at the same time, but we could still lose writes if
> one of the threads simply updated the existing array.
> 
> Chained varTable is an interesting idea, but the chained element at
> the end could be lost in exactly the same way, no?
> 
> My immediate thought was to move the CAS check to a separate
> "updatedCount" field...but it would need to be atomically updated :-)
> We're back to full volatile semantics.
> 

Hmh, I've thought about this and maybe came to a solution.

The "chained varTable" looks like the most promising solution. What
about this:

Put the variable values into index 1..length. Reserve the first (0)
index as a link to an optional next varTable.

Reading a value would work like this:

local = varTable
do
  if (index < local.length) return local[index]
  index -= local.length-1
while ((local=local[0]) != null)

Writing would be like this:

local = varTable
forever do
  if (index < local.length) local[index]=value then end
  index -= local.length-1

  while ((local2 = local[0]) == null)
    local2 = unsafe.getObjectVolatile(local, index=0)
    if (local2 != null) then break while
    local2 = new next array
    if (unsafe.CAS(local, index=0, local2, expected=null)) then break
  end while

  local = local2
end forever do


The big advantage: If you're able to initialize varTable properly within
the constructor, the you can declare varTable as final and don't need
volatile access for all reads and for nearly all non-expanding writes.

If you must construct varTable lazily, then declare it as a normal
field, leave it to null first, and do the initial declaration similar to
the "0" index write above: first read normal, the volatile, then CAS.

A "cleaner code" solution of the above would be to introduce a separate
object for the varTable chain entry, containing the array itself and an
AtomicReference to the next chain element. You wouldn't need the Unsafe
object then. However, I'm assuming that many JRuby objects contain only
very few attributes, so the extra overhead of such an object would be a
memory killer.

An extra bonus is that you now can introduce volatile fields in JRuby,
and access them in the array using unsafe.get/putObjectVolatile as in
AtommicReferenceArray. You maybe need to think about fences then.

The big disadvantage is the loop for normal read and write accesses. As
long as expanding varTable is the absolute rare exception, it won't harm
much, but it can become a killer argument when it's the normal case.

Greetings,
Michael

From concurrency at kuli.org  Thu Jan  3 03:16:50 2013
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Thu, 03 Jan 2013 09:16:50 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E482E2.5060705@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de>
Message-ID: <50E53E72.20509@kuli.org>

Am 02.01.2013 19:56, schrieb Aaron Grunthal:
> Forgot one step, revised code:
> 
> 
> write(index,value) {
> 
> 1. oldstamp = stamp // volatile read
> 
> goto 1 if oldstamp is odd // spin wait
> 
> 2. local = varTable // volatile read
> 
> if index >= length
>   3. CAS stamp to oldstamp+1 (odd) // goto 1 on failure
>   4. allocate
>   5. arraycopy
>   6. local[index] = value // volatile? ordered? normal?
>   7. varTable = local // volatile? ordered? normal?
>   8. stamp += 1 (even) // volatile write
> else
>   9. local[index] = value // volatile? ordered? normal?
>   10. oldstamp == stamp // volatile validation. goto 1 on failure.
> end
> 
> }
> 
> revised questions:
> 
> a) would this work/did I miss something?

No, it won't. Thread A performs a write within the array length, but
gets interrupted after step 1 (stamp check). Then thread B performs a
write out of bounds, changes the stamp, copies the array - and gets
interrupted after step 6. The thread A continues, thinking that the
stamp was even, and performs its write against the already copied array.
After some time, thread B is back again and writes its (already
outdated) array back into the varTable field. So, the write of thread A
still gets lost.

You could implements a lock that way: For normal writes within array
length, the stamp must be positive or zero (wait for that), then gets
incremented, the write gets performed, and the stamp gets decremented
again. For expanding writes, perform a CAS from 0 to -1 (wait for that),
perform the write, and set the stamp to 0 again.

But you'd need two CAS operations for every single write, so I don't
think that this is an option. And as you already have a full volatile
write then, you can make the read access volatile as well - that
wouldn't change much any more.

-Michael

From joe.bowbeer at gmail.com  Thu Jan  3 03:17:23 2013
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 3 Jan 2013 00:17:23 -0800
Subject: [concurrency-interest] Projects that use
	java.util.concurrent.FutureTask
In-Reply-To: <CAHzJPEr0XTVCpZbt9u=xp=VAEakQ6ncpcnvr9cSjSxTxBORdTw@mail.gmail.com>
References: <CAAL-3PZCqFJjpkWZcOoFrwjuSp6WYAGb+_gcikjb8oaFecf-3w@mail.gmail.com>
	<CAHzJPEr0XTVCpZbt9u=xp=VAEakQ6ncpcnvr9cSjSxTxBORdTw@mail.gmail.com>
Message-ID: <CAHzJPEonwbMhipmx6JRnfaogHfN2g7F+ahHfieOnwsDzd-ohXw@mail.gmail.com>

I should add that in all three of the examples I mentioned, get() is only
called when it is known that the FutureTask has completed.  I can't think
of any examples off-hand where get() is called without knowing that the
task has completed.  In those cases, the get(timeout) form may be more
popular than get().


On Wed, Jan 2, 2013 at 7:08 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> Java's SwingWorker and Android's AsyncTask both employ FutureTask in their
> implementation, so you might obtain more hits if you widen your search.
>
> The RemoteTable implementation in my "Last Word on Swing Threads" schedules
> its SwingWorker tasks on a SingleThreadExecutor, which runs the tasks
> sequentially on a single thread.
>
> To find RemoteTable, search the WayBack machine at archive.org for "
> http://java.sun.com/products/jfc/tsc/articles/threads/threads3.html"
>
>
> On Wed, Jan 2, 2013 at 6:36 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>
>> Hi all,
>>
>> I'm a Ph.D. student at UIUC. We are now studying the usage of
>> java.util.concurrent.FutureTask,
>> but we meet problem to find real-world projects that use FutureTask
>> correctly
>> (we search the code by code search engine Koder and Github).
>>
>> The code pattern we're trying to find is like:
>>
>> ================================
>> FutureTask ft = new FutureTask(...)
>> new Thread(ft).start() // or a ThreadPool to start a task
>>
>> ... // Here are some other computations such that FutureTask can run
>> asynchronously
>>
>> Result r = ft.get() // get the result from FutureTask
>> =================================
>>
>> However, most programs we find invoke "FutureTask.get()" immediately
>> after starting the
>> FutureTask (i.e., there are no computations in between). This is
>> incorrect because there
>> is no performance improvement in such usage.
>>
>> Thus, we're wondering does any one know any projects that use
>> FutureTask (a lot)?
>> Or does any one contribute to open source projects that use
>> FutureTask? So we can use
>> them as experiment subjects.
>>
>> Thanks a lot,
>> Yu Lin
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/efd8317a/attachment-0001.html>

From aaron.grunthal at infinite-source.de  Thu Jan  3 05:01:55 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Thu, 03 Jan 2013 11:01:55 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E53E72.20509@kuli.org>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de> <50E53E72.20509@kuli.org>
Message-ID: <50E55713.1020104@infinite-source.de>

On 03.01.2013 09:16, Michael Kuhlmann wrote:
> Am 02.01.2013 19:56, schrieb Aaron Grunthal:
>> Forgot one step, revised code:
>>
>>
>> write(index,value) {
>>
>> 1. oldstamp = stamp // volatile read
>>
>> goto 1 if oldstamp is odd // spin wait
>>
>> 2. local = varTable // volatile read
>>
>> if index >= length
>>    3. CAS stamp to oldstamp+1 (odd) // goto 1 on failure
>>    4. allocate
>>    5. arraycopy
>>    6. local[index] = value // volatile? ordered? normal?
>>    7. varTable = local // volatile? ordered? normal?
>>    8. stamp += 1 (even) // volatile write
>> else
>>    9. local[index] = value // volatile? ordered? normal?
>>    10. oldstamp == stamp // volatile validation. goto 1 on failure.
>> end
>>
>> }
>>
>> revised questions:
>>
>> a) would this work/did I miss something?
>
> No, it won't. Thread A performs a write within the array length, but
> gets interrupted after step 1 (stamp check). Then thread B performs a
> write out of bounds, changes the stamp, copies the array - and gets
> interrupted after step 6. The thread A continues, thinking that the
> stamp was even, and performs its write against the already copied array.
> After some time, thread B is back again and writes its (already
> outdated) array back into the varTable field. So, the write of thread A
> still gets lost.

I don't think it would fail like that. If A is only past the first stamp 
read and then B goes to work the first thing B does is to increment the 
stamp.

A may do some assignment to either the old or the new array in step 9. 
But it will ultimately fail its validation in step 10 and redo until it 
can leave the loop with the guarantee that it did a write to the correct 
array.

Note that 10 does not check for even/odd but for equality with the 
originally obtained stamp.

- Aaron

From concurrency at kuli.org  Thu Jan  3 05:03:43 2013
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Thu, 03 Jan 2013 11:03:43 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CAEJX8ooQyg-ZxvLZy-m5KWtdsvJNAUVJEunxhUAOLEmdKDohDQ@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de> <50E53E72.20509@kuli.org>
	<CAEJX8ooQyg-ZxvLZy-m5KWtdsvJNAUVJEunxhUAOLEmdKDohDQ@mail.gmail.com>
Message-ID: <50E5577F.2050002@kuli.org>

Am 03.01.2013 10:55, schrieb Stanimir Simeonoff:
> Step 10 validates the stamp, the validation would fail (the
> versionCounter would be odd/different) and it loops back to 1.
> Step 9 has to be volatile write to prevent reorder w/ step 10.

You're right, I overlooked this.

To make everything sure, step 7 (writing the enlarged varTable back)
must be a CAS operation, otherwise two concurrent write operations
beyond array.length could overwrite each other. Then it's safe.

Greetings,
Michael

From concurrency at kuli.org  Thu Jan  3 05:36:22 2013
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Thu, 03 Jan 2013 11:36:22 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E55713.1020104@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de> <50E53E72.20509@kuli.org>
	<50E55713.1020104@infinite-source.de>
Message-ID: <50E55F26.5090104@kuli.org>

Am 03.01.2013 11:01, schrieb Aaron Grunthal:
> I don't think it would fail like that. If A is only past the first stamp
> read and then B goes to work the first thing B does is to increment the
> stamp.

Yes, I didn't see the loop. I should open my yes first. :)

The algorithm seems to work fine for me, at least when you implement the
CAS check for new arrays as I already wrote in the response to
Stanimir's comment. Yes, I like it. :) The disadvantage is that you
always need at least two volatile read accesses on every write operation.

Michael

From aaron.grunthal at infinite-source.de  Thu Jan  3 06:12:06 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Thu, 03 Jan 2013 12:12:06 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236968078@G9W0725.americas.hpqcorp.net>
References: <50E1904E.9030406@infinite-source.de>
	<50E1A44D.1090509@oracle.com>	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236967CCC@G9W0725.americas.hpqcorp.net>
	<50E4B601.6000005@infinite-source.de>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236968078@G9W0725.americas.hpqcorp.net>
Message-ID: <50E56786.5090301@infinite-source.de>

On 03.01.2013 07:32, Boehm, Hans wrote:
>> From: Aaron Grunthal [mailto:aaron.grunthal at infinite-source.de]
>>
>> More generally our main problem is that we lack data structures that provide
>> non-volatile read semantics (i.e. possibly stale under concurrent modification
>> but fast and loop-hoistable) while using atomic write algorithms, mostly for
>> internal consistency under concurrent access.
>>
> We could debate how useful those are in general.  It seems to me that you need them because you're implementing a programming language that provides Java-memory-model-like semantics for data races.  That's certainly entirely reasonable.  But I'm not convinced that such data structures are correctly usable by enough programmers to make them terribly interesting for direct programmer use.  And if I were designing a programming language now, I'd work hard to avoid Java-like data race semantics to start with.

Yes, we basically need two things

a) fast single-threaded performance in our internal data structures 
since we can't specialize for single and multilmultiple threads.
b) non-catastrophic behavior under multi-threaded access

There are many things that ruby allows that are perfectly sensible if 
performed by a single thread. Maybe-reasonable when performed on 
separate parts of the class hierarchy by multiple threads. And undefined 
when performed by multiple threads on the same parts.

And there are no ConcurrentModificationExceptions or anything. Many of 
the metaprogramming changes also cascade downwards in the class 
hierarchy, so using locks on internal structures would lead to worse 
single-threaded performance *and* risk deadlocks.

So atomic writes and racy reads are quite representative of ruby behavior.


But dynamic language implementations are not the only things that could 
benefit from such things. I was actually thinking of non-volatile read 
methods *in addition* to the volatile ones. E.g. a lazyGet(key) on 
concurrent maps.
These things can be useful for caches in cases where performance is more 
important than seeing the most-recent values when they are frequently 
mutated anyway and no matter what you read it might be stale within 
milliseconds anyway.

E.g. I implemented a UDP-based DHT node where throughput was more 
important than seeing the latest state updates from incoming requests, 
simply because the underlying transport itself was already lossy it was 
acceptable to have some slightly-stale reads in the application itself.


The case may also become more important with lambda expressions in java 
as code defining the lambda may not know how often the lambda is used 
and the invoking code may want to loop-hoist some reads but can't do 
that if always-concurrent data structures are used. Having finer control 
here would allow hotspot to do the performance optimizations instead of 
requiring the programmer to do them manually.

manual hoisting:

final Integer entry = concurrentLookupTable.get("key")
new Range(20,1000).each(x -> x * entry)

hotspot doing the hoisting:

new Range(20,1000).each(x -> x * concurrentLookupTable.lazyGet("key"))


With method chaining the manual version can quickly become quite verbose.

- Aaron

From aaron.grunthal at infinite-source.de  Thu Jan  3 06:17:44 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Thu, 03 Jan 2013 12:17:44 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E5577F.2050002@kuli.org>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de> <50E53E72.20509@kuli.org>
	<CAEJX8ooQyg-ZxvLZy-m5KWtdsvJNAUVJEunxhUAOLEmdKDohDQ@mail.gmail.com>
	<50E5577F.2050002@kuli.org>
Message-ID: <50E568D8.6020205@infinite-source.de>

On 03.01.2013 11:03, Michael Kuhlmann wrote:
> To make everything sure, step 7 (writing the enlarged varTable back)
> must be a CAS operation, otherwise two concurrent write operations
> beyond array.length could overwrite each other. Then it's safe.

I don't think that's needed either. There can't be two concurrent growth 
operations as incrementing the stamp to an odd value does a CAS + loop 
on failure, i.e. only one thread is allowed to grow the array at a time, 
the other one would spin.

It wouldn't be terribly costly though since this is a rarely-used code 
path. The non-growing version is the important one.

- Aaron

From concurrency at kuli.org  Thu Jan  3 06:41:14 2013
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Thu, 03 Jan 2013 12:41:14 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E568D8.6020205@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de> <50E53E72.20509@kuli.org>
	<CAEJX8ooQyg-ZxvLZy-m5KWtdsvJNAUVJEunxhUAOLEmdKDohDQ@mail.gmail.com>
	<50E5577F.2050002@kuli.org> <50E568D8.6020205@infinite-source.de>
Message-ID: <50E56E5A.3040408@kuli.org>

Am 03.01.2013 12:17, schrieb Aaron Grunthal:
> On 03.01.2013 11:03, Michael Kuhlmann wrote:
>> To make everything sure, step 7 (writing the enlarged varTable back)
>> must be a CAS operation, otherwise two concurrent write operations
>> beyond array.length could overwrite each other. Then it's safe.
> 
> I don't think that's needed either. There can't be two concurrent growth
> operations as incrementing the stamp to an odd value does a CAS + loop
> on failure, i.e. only one thread is allowed to grow the array at a time,
> the other one would spin.

Yes, that's true. Shame on me I didn't notice this either.

> 
> It wouldn't be terribly costly though since this is a rarely-used code
> path. The non-growing version is the important one.

Exactly.

Greetings,
Michael


From dl at cs.oswego.edu  Thu Jan  3 07:32:26 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 03 Jan 2013 07:32:26 -0500
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CAEJX8oruWQ8PKmCHkVm3+es0dCEq_x4LN-xtzOFkqJNVn_E2DA@mail.gmail.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CC3DB1AB-3147-4431-9CF2-D47681F07DD3@oracle.com>
	<CAEJX8oruWQ8PKmCHkVm3+es0dCEq_x4LN-xtzOFkqJNVn_E2DA@mail.gmail.com>
Message-ID: <50E57A5A.8030104@cs.oswego.edu>

On 01/01/13 13:08, Stanimir Simeonoff wrote:
>
>      > 3) Another option seems to be using an AtomicLongArray, and indexing it
>     by the enum ordinal. My concern there is that the array layout might
>     increase the chances of false sharing over approach 1 (using an AtomicLong[]
>     array). Would this be a proper usage of AtomicLongArray?
>
>     Yes, it will. Note that false sharing is still hitting you even in the case
>     of AtomicLong[], although the chances of hitting it on backing long[] are
>     larger. But, this is discussing whether you are ok with 400x performance
>     hit, or "just" 100x.
>
>
> I was going to say something like that but w/ AtomicLongArray, the index can be
> mapped differently than enum.ordinal(), e.g. (enum.ordinal+1)<<3 and the length
> of the array should be (Class.getEnumConstants().length+2) <<3
> Of course LongAdder is meant to cope w/ the false sharing and concurrent updates
> and I'd strongly recommend it but if you are to work w/ the metal you can make
> AtomicLongArray do the same. I just don't know if the frequency of the updates
> would warrant such an approach.

To avoid requiring users to cope with such issues, it might make sense to
introduce class LongAdderArray, that could internally decide whether
to populate with individual LongAdders or spaced-out cells, or dense
cells with Striped64-like transition mechanics on contention.

-Doug




From java at java4.info  Thu Jan  3 08:21:50 2013
From: java at java4.info (Florian Binder)
Date: Thu, 03 Jan 2013 14:21:50 +0100
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>
Message-ID: <50E585EE.1030406@java4.info>

What about giving the Predicate two arguments? The current state and the 
await state? This should ensure a) and b).
Even more you could add a StateChangeAssertion to ensure some state 
change criteria like monotony.

public class SimpleSynchronizer<T> {
     SimpleSynchronizer(Predicate<T> checker, StateChangeAssertion<T> 
stateAssertion)
     void await(T t) throws InterruptedException;
     boolean await(T t, long time, TimeUnit unit) throws 
InterruptedException;
     void signal(T s) throws IllegalStateException;
     //probably some more methods
}

SimpleSynchronized<Integer> ss=new SimpleSynchronized( (s,t) -> s >= t, 
(old,new) -> old >= new);

to wait on a particular state you would call:
ss.await(4); // or
ss.await(9, 10, TimeUnit.SECONDS);

Whenever the services state changed you would call:
ss.signal(newState);

/Flo


Am 31.12.2012 15:54, schrieb David Holmes:
> A latch is a thousand times simpler than using AQS. Any "wait for 
> event" can be handled using a simple latch.
> The problem with your predicate based solution is that, as I said 
> generally you need to to ensure that:
> a) the predicate is evaluated in a thread-safe way
> b) the predicate remains valid while you act on it.
> If you don;t need the above then a simple existing synchronizer is 
> likely to suffice.
> I simply don't see what you suggest as being a generally useful tool - 
> and I do see it as one easily misused because you do in fact need 
> thread-safety and atomicity.
> YMMV.
> David
>
>     -----Original Message-----
>     *From:* Kasper Nielsen [mailto:kasperni at gmail.com]
>     *Sent:* Monday, 31 December 2012 7:26 PM
>     *To:* dholmes at ieee.org
>     *Cc:* Concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] A simple predicate based
>     synchronizer
>
>     On Mon, Dec 31, 2012 at 4:04 AM, David Holmes
>     <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>
>         General edge-triggers are normally accommodated through the
>         existing synchronizers: semaphores, latches, barriers, gates.
>         Your "await start" example is just a latch.
>
>     I don't really see you point? Everything can be build using
>     AbstractQueuedSynchronizer as well. But that
>     doesn't necessarily make it easy for end users.
>
>     One area where I really miss something like this is when writing
>     unit tests. Being able to do something like this
>     T1: signal("TestDone")
>     T2: await(s -> s.equals("TestDone"), 10, TimeUnit.SECONDS) or even
>     T2: awaitEqualsTo("TestDone", 10, TimeUnit.SECONDS)
>     would make my life much easier.
>
>     While I could use CountdownLatch in places like this. I usually
>     end up with a couple of them whenever multiple states are
>     involved. Which make it much harder to read at a later time.
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/61a4ab43/attachment-0001.html>

From dl at cs.oswego.edu  Thu Jan  3 08:31:45 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 03 Jan 2013 08:31:45 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
Message-ID: <50E58841.6050905@cs.oswego.edu>

On 01/02/13 17:34, ?iktor ?lang wrote:
>
> Can we list some potential problems, use-case wise, and see if we can find
> something that ticks all the boxes?

The primary use case for cancellation is:

  "I don't want or need this result/effect, so don't bother doing it."

And the mechanism supporting it is basically to complete the CF
yourself (with a CancellationException) and hope that others
notice this before/while processing (which CF automates
for synchronous completions, but not necessarily async).
If they don't notice in time, you've wasted some resources and
possibly caused some unwanted side effects that you will have
to cope with.

All of the mentioned variants and extensions seem to require
context-specific layered actions that we can in some way
help make possible but cannot automate. As in

   "Even though cancelled, if you've computed a result, please let me
know about it."

As we've seen, this is implementable via either racy overwrites or
racy side-channels.

The cases that we cannot help much about include:

   "The computation for completing this CF is harmful, so really stop!"

To even approach this, you need a protocol to see if an Async
action fully started, and if so whether it stopped.
This is far beyond what CF itself can do: You'd need a custom
queryable Executor that can determine status and try to stop;
in the worst case blowing away the entire service by pulling
all resources, which in turn in the worst case may require
bringing down the VM or machine.

-Doug





From viktor.klang at gmail.com  Thu Jan  3 08:43:20 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 3 Jan 2013 14:43:20 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E58841.6050905@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
Message-ID: <CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>

On Thu, Jan 3, 2013 at 2:31 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/02/13 17:34, ?iktor ?lang wrote:
>
>>
>> Can we list some potential problems, use-case wise, and see if we can find
>> something that ticks all the boxes?
>>
>
> The primary use case for cancellation is:
>
>  "I don't want or need this result/effect, so don't bother doing it."
>
> And the mechanism supporting it is basically to complete the CF
> yourself (with a CancellationException) and hope that others
> notice this before/while processing (which CF automates
> for synchronous completions, but not necessarily async).
> If they don't notice in time, you've wasted some resources and
> possibly caused some unwanted side effects that you will have
> to cope with.
>

How do we distinguish this use case from: "I don't want or need this
result/effect, so don't bother doing it. (But someone else might still want
it)"

i.e.

def f1(f: Future[String]) = f.cancel()
def f2(f: Future[String]) = f map { _.toUppercase } foreach println

val f = computeString()
f1(f)
f2(f)

My point being, how does f1 know that no one else is interested in the
result of f?

(Here's where I propose to separate the notion of cancellation from the
notion of Future)



>
> All of the mentioned variants and extensions seem to require
> context-specific layered actions that we can in some way
> help make possible but cannot automate. As in
>
>   "Even though cancelled, if you've computed a result, please let me
> know about it."


> As we've seen, this is implementable via either racy overwrites or
> racy side-channels.
>

If it is up to the producer to respond to an upstream travelling signal to
cancel, then he can choose to either complete it with a
CancellationException, or the real result.


>
> The cases that we cannot help much about include:
>
>   "The computation for completing this CF is harmful, so really stop!"
>
> To even approach this, you need a protocol to see if an Async
> action fully started, and if so whether it stopped.
> This is far beyond what CF itself can do: You'd need a custom
> queryable Executor that can determine status and try to stop;
> in the worst case blowing away the entire service by pulling
> all resources, which in turn in the worst case may require
> bringing down the VM or machine.


I think this is orthogonal to Futures, it could be solved by having 2
pieces returned from submitting a task to be executed, (see C# for
instance),
one is a handle to abort the computation, and the other is the Future.

Thoughts?

Cheers,
?


>
>
> -Doug
>
>
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/e32ace37/attachment.html>

From dl at cs.oswego.edu  Thu Jan  3 08:59:25 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 03 Jan 2013 08:59:25 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
	<CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
Message-ID: <50E58EBD.6030208@cs.oswego.edu>

On 01/03/13 08:43, ?iktor ?lang wrote:
> How do we distinguish this use case from: "I don't want or need this
> result/effect, so don't bother doing it. (But someone else might still want it)"

By not using cancel in such cases. You might instead use
"or" completions. That's one of their primary use cases.

(I do realize that Future.cancel especially can be and is misused
in such cases. On the other hand, it is not a very common misuse
because usually there is only one consumer.)

>
>     The cases that we cannot help much about include:
>
>        "The computation for completing this CF is harmful, so really stop!"
>
>
> I think this is orthogonal to Futures, it could be solved by having 2 pieces
> returned from submitting a task to be executed, (see C# for instance),
> one is a handle to abort the computation, and the other is the Future.
>

Which isn't necessary if you only use cancel for these purposes.

-Doug





From viktor.klang at gmail.com  Thu Jan  3 09:04:19 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 3 Jan 2013 15:04:19 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E58EBD.6030208@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
	<CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
	<50E58EBD.6030208@cs.oswego.edu>
Message-ID: <CANPzfU9-Aykmi1zGjon2TdD5uGSXC-4UaLdwdYyDfSC=oXTn4A@mail.gmail.com>

On Thu, Jan 3, 2013 at 2:59 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/03/13 08:43, ?iktor ?lang wrote:
>
>> How do we distinguish this use case from: "I don't want or need this
>> result/effect, so don't bother doing it. (But someone else might still
>> want it)"
>>
>
> By not using cancel in such cases. You might instead use
> "or" completions. That's one of their primary use cases.
>

Yes, but that doesn't achieve the signal to the producer.


>
> (I do realize that Future.cancel especially can be and is misused
> in such cases. On the other hand, it is not a very common misuse
> because usually there is only one consumer.)
>

That's not my experience. It might be the case for current FutureTask code,
but when you go async composition, Future becomes a core building block
which is often shared freely.


>
>
>>     The cases that we cannot help much about include:
>>
>>        "The computation for completing this CF is harmful, so really
>> stop!"
>>
>>
>> I think this is orthogonal to Futures, it could be solved by having 2
>> pieces
>> returned from submitting a task to be executed, (see C# for instance),
>> one is a handle to abort the computation, and the other is the Future.
>>
>>
> Which isn't necessary if you only use cancel for these purposes.


Question is, how does a method signal that it's going to cancel a Future,
i.e. how do I as a caller that I have given the callee the right to cancel
the Future?

(Here's where I think a separation of the Cancellable and the Future is a
good idea)

Cheers,
?


>
>
> -Doug
>
>
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/fda6cdff/attachment.html>

From dl at cs.oswego.edu  Thu Jan  3 09:21:19 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 03 Jan 2013 09:21:19 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU9-Aykmi1zGjon2TdD5uGSXC-4UaLdwdYyDfSC=oXTn4A@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
	<CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
	<50E58EBD.6030208@cs.oswego.edu>
	<CANPzfU9-Aykmi1zGjon2TdD5uGSXC-4UaLdwdYyDfSC=oXTn4A@mail.gmail.com>
Message-ID: <50E593DF.5060509@cs.oswego.edu>

On 01/03/13 09:04, ?iktor ?lang wrote:

>
> Question is, ...
> how do I as a caller that I have given the callee the right to cancel the Future?

Or more simply: How do you know that there are no other possible
uses of a possibly shared completion?

And the simple answer: In general you cannot know. Even though
it may appear at some moment that there are no consumers, one
may appear by the time you act to cancel. So all fully reliable
uses require some non-local context that somehow knows that it
is OK.

>
> (Here's where I think a separation of the Cancellable and the Future is a good idea)
>

I agree. Too late for that in j.u.c though. The best we can do
is make it easy/attractive not to use it when you don't need to.

-Doug




From viktor.klang at gmail.com  Thu Jan  3 09:32:13 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 3 Jan 2013 15:32:13 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E593DF.5060509@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
	<CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
	<50E58EBD.6030208@cs.oswego.edu>
	<CANPzfU9-Aykmi1zGjon2TdD5uGSXC-4UaLdwdYyDfSC=oXTn4A@mail.gmail.com>
	<50E593DF.5060509@cs.oswego.edu>
Message-ID: <CANPzfU8W17eChdt=pL+HiOV7Jx9f2BNz3VcJhubvY1JHCO=JiA@mail.gmail.com>

On Thu, Jan 3, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/03/13 09:04, ?iktor ?lang wrote:
>
>
>> Question is, ...
>>
>> how do I as a caller that I have given the callee the right to cancel the
>> Future?
>>
>
> Or more simply: How do you know that there are no other possible
> uses of a possibly shared completion?
>
> And the simple answer: In general you cannot know. Even though
> it may appear at some moment that there are no consumers, one
> may appear by the time you act to cancel. So all fully reliable
> uses require some non-local context that somehow knows that it
> is OK.


Yes, exactly, which is why you encode it in the signature:

def f1(f: Future[String] with Cancelable) = f.cancel()
def f2(f: Future[String]) = f map { _.toUppercase } foreach println

Now I know as a caller that f1 is likely to want to be able to cancel the
Future, and f2 can't cancel it, so if all I have is a plain Future[String],
I do not have the capability to cancel myself, and as such I cannot let
anyone else do it either.

val f: Future[String] = computeFuture()

f1(f) //compiler error, this dude wants cancelability
f1(f.withNopCancel()) // returns a Future with Cancelable whose cancel
method does nothing
f2(f) // We're cool here


>
>
>
>> (Here's where I think a separation of the Cancellable and the Future is a
>> good idea)
>>
>>
> I agree. Too late for that in j.u.c though. The best we can do
> is make it easy/attractive not to use it when you don't need to.


No java.util.nconcurrent2 ? :-)

Cheers,
?


>
>
> -Doug
>
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/9d1ae5d4/attachment.html>

From mlists at juma.me.uk  Thu Jan  3 09:41:45 2013
From: mlists at juma.me.uk (Ismael Juma)
Date: Thu, 3 Jan 2013 16:41:45 +0200
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU8W17eChdt=pL+HiOV7Jx9f2BNz3VcJhubvY1JHCO=JiA@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
	<CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
	<50E58EBD.6030208@cs.oswego.edu>
	<CANPzfU9-Aykmi1zGjon2TdD5uGSXC-4UaLdwdYyDfSC=oXTn4A@mail.gmail.com>
	<50E593DF.5060509@cs.oswego.edu>
	<CANPzfU8W17eChdt=pL+HiOV7Jx9f2BNz3VcJhubvY1JHCO=JiA@mail.gmail.com>
Message-ID: <CAD5tkZb-vAPrgDS8RMAUUiajLOyiSSPftAf=88P73auPotsPuA@mail.gmail.com>

On Thu, Jan 3, 2013 at 4:32 PM, ?iktor ?lang <viktor.klang at gmail.com> wrote:

>
> No java.util.nconcurrent2 ? :-)
>

It does look like Java 9 is a good candidate for an API that is meant for
end-users instead of a building block for concurrency experts.

Best,
Ismael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/ee1a575c/attachment.html>

From radhakrishnan.mohan at gmail.com  Thu Jan  3 10:13:19 2013
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Thu, 3 Jan 2013 20:43:19 +0530
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CAD5tkZb-vAPrgDS8RMAUUiajLOyiSSPftAf=88P73auPotsPuA@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
	<CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
	<50E58EBD.6030208@cs.oswego.edu>
	<CANPzfU9-Aykmi1zGjon2TdD5uGSXC-4UaLdwdYyDfSC=oXTn4A@mail.gmail.com>
	<50E593DF.5060509@cs.oswego.edu>
	<CANPzfU8W17eChdt=pL+HiOV7Jx9f2BNz3VcJhubvY1JHCO=JiA@mail.gmail.com>
	<CAD5tkZb-vAPrgDS8RMAUUiajLOyiSSPftAf=88P73auPotsPuA@mail.gmail.com>
Message-ID: <CAOoXFP_pjNgSRSXbtcYF7s4q7PBb2PKp6T-jXfEubDD_1uW2Hg@mail.gmail.com>

The full import of the API is probably understood only by tool and
framework designers !!

Mohan

On Thu, Jan 3, 2013 at 8:11 PM, Ismael Juma <mlists at juma.me.uk> wrote:

> On Thu, Jan 3, 2013 at 4:32 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:
>
>>
>> No java.util.nconcurrent2 ? :-)
>>
>
> It does look like Java 9 is a good candidate for an API that is meant for
> end-users instead of a building block for concurrency experts.
>
> Best,
> Ismael
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/242834a5/attachment.html>

From nathan.reynolds at oracle.com  Thu Jan  3 10:38:29 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 03 Jan 2013 08:38:29 -0700
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CALwNKeQ=9BvwsXahUozQGVG2rh-nYn+O_SF+KWKOdUT6mC0UwQ@mail.gmail.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CACuKZqH1sjxCoX1oAfhkW-qrX=4yd+K8aHZ9BuH9be6K7KOYTg@mail.gmail.com>
	<CALwNKeQ=9BvwsXahUozQGVG2rh-nYn+O_SF+KWKOdUT6mC0UwQ@mail.gmail.com>
Message-ID: <50E5A5F5.9040307@oracle.com>

One of the problems with thread structures is that the structure has to 
follow the thread around every time the thread switches to another 
core.  This happens every time the thread blocks or when its time slice 
runs out. Yanking the structure into the core adds a lot of latency.  
Also, having a structure per thread creates a lot of structures which 
hog cache space.

A better solution is to use per core structures (see the ProcessorLocal 
discussion on this group).  Each core is assigned a structure.  This 
will reduce the number of structures to the number of cores which should 
be far less than the number of threads.  Cache capacity isn't as 
stressed.  When the counter needs to be updated, the thread detects 
which core it is running on, grabs the reference for the assigned 
structure and uses an atomic increment, decrement or CAS.

But, Nathan isn't the atomic instruction going to put us back to the 
original problem of poor CAS performance?  The answer is no. Since the 
structure is assigned to a core, the structure should stay in the cache 
of that core.  Depending upon read frequency, the cache lines holding 
the structure should already be in the modified or exclusive state.  
Thus, there shouldn't be any delays from having to send invalidation 
signals.  No other core will be yanking the structure to its own cache.  
Hence, the performance of the atomic instruction will be very cheap 
(i.e. several cycles) and getting cheaper with each new Intel processor.

There is 1 time where the structure could be yanked to another core's 
cache.  This happens if the thread context switches to another core 
after it determines which core it is running on but before executing the 
atomic instruction.  Since updating the structure is an in-memory 
operation, the context switch should only happen if the thread runs out 
of its time slice.  The time slice is about 200 ms on Linux.  So, 
context switching in the middle of updating the structure should be 
rare... unless of course the thread is simply updating the structure in 
a very tight loop.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 1/2/2013 10:40 PM, Michael Barker wrote:
> The comments on false sharing remind me of the false sharing
> article[1] on 1024cores.net.  The cost of the recording the counters
> could be reduced by roughly an order of magnitude if, instead of using
> a shared structure, the counters were stored in per thread structures
> and the responsibility for aggregating the values fell to the reading
> code (that is off of your main application hot code path).  The use of
> a shared structure that has contended writes will introduce
> scalability bottleneck.  If you can be certain that only one thread
> will write to the counter then you don't need an expensive CAS
> operation to handle the increment and a cheaper, get, add and lazySet
> would suffice.
>
> [1] http://www.1024cores.net/home/lock-free-algorithms/false-sharing---false
>
> Mike.
>
> On Wed, Jan 2, 2013 at 8:30 AM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>> Is false sharing a big concern for variables each of which is intended
>> to be shared? If we have N CPUs randomly updating M variables, does it
>> make a lot of difference whether the variables are on the same cache
>> line or not?
>>
>> Zhong Yu
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/b06b98dd/attachment-0001.html>

From zhong.j.yu at gmail.com  Thu Jan  3 12:21:25 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 3 Jan 2013 11:21:25 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU8W17eChdt=pL+HiOV7Jx9f2BNz3VcJhubvY1JHCO=JiA@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
	<CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
	<50E58EBD.6030208@cs.oswego.edu>
	<CANPzfU9-Aykmi1zGjon2TdD5uGSXC-4UaLdwdYyDfSC=oXTn4A@mail.gmail.com>
	<50E593DF.5060509@cs.oswego.edu>
	<CANPzfU8W17eChdt=pL+HiOV7Jx9f2BNz3VcJhubvY1JHCO=JiA@mail.gmail.com>
Message-ID: <CACuKZqE1=_E+sKNQXd5ftaWb1E1+nGwyEmVju0LQ0+4v-kdNGg@mail.gmail.com>

On Thu, Jan 3, 2013 at 8:32 AM, ?iktor ?lang <viktor.klang at gmail.com> wrote:
>
>
>
> On Thu, Jan 3, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> On 01/03/13 09:04, ?iktor ?lang wrote:
>>
>>>
>>> Question is, ...
>>>
>>> how do I as a caller that I have given the callee the right to cancel the
>>> Future?
>>
>>
>> Or more simply: How do you know that there are no other possible
>> uses of a possibly shared completion?
>>
>> And the simple answer: In general you cannot know. Even though
>> it may appear at some moment that there are no consumers, one
>> may appear by the time you act to cancel. So all fully reliable
>> uses require some non-local context that somehow knows that it
>> is OK.
>
>
> Yes, exactly, which is why you encode it in the signature:
>
> def f1(f: Future[String] with Cancelable) = f.cancel()
> def f2(f: Future[String]) = f map { _.toUppercase } foreach println
>
> Now I know as a caller that f1 is likely to want to be able to cancel the
> Future, and f2 can't cancel it, so if all I have is a plain Future[String],
> I do not have the capability to cancel myself, and as such I cannot let
> anyone else do it either.
>
> val f: Future[String] = computeFuture()
>
> f1(f) //compiler error, this dude wants cancelability
> f1(f.withNopCancel()) // returns a Future with Cancelable whose cancel
> method does nothing
> f2(f) // We're cool here
>
>>
>>
>>
>>>
>>> (Here's where I think a separation of the Cancellable and the Future is a
>>> good idea)
>>>
>>
>> I agree. Too late for that in j.u.c though. The best we can do
>> is make it easy/attractive not to use it when you don't need to.
>
>
> No java.util.nconcurrent2 ? :-)

Is it not possible to introduce a super interface to Future?

    interface Async<V>

        boolean isDone();
        V get();

        void onComplete(Runnable);

        default Async<R> thenApply(Function<V,R>){ ... }
        etc.

    interface Future<V> extends Async<V>
        void cancel();
        boolean isCancelled();

        default void onComplete(Runnable){ ... }

Zhong Yu


From viktor.klang at gmail.com  Thu Jan  3 12:26:09 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 3 Jan 2013 18:26:09 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CACuKZqE1=_E+sKNQXd5ftaWb1E1+nGwyEmVju0LQ0+4v-kdNGg@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
	<CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
	<50E58EBD.6030208@cs.oswego.edu>
	<CANPzfU9-Aykmi1zGjon2TdD5uGSXC-4UaLdwdYyDfSC=oXTn4A@mail.gmail.com>
	<50E593DF.5060509@cs.oswego.edu>
	<CANPzfU8W17eChdt=pL+HiOV7Jx9f2BNz3VcJhubvY1JHCO=JiA@mail.gmail.com>
	<CACuKZqE1=_E+sKNQXd5ftaWb1E1+nGwyEmVju0LQ0+4v-kdNGg@mail.gmail.com>
Message-ID: <CANPzfU-qGdW5biOv4VZTGjV+z0DJjV3Wn6Nj9zPR0q_W_5aQEQ@mail.gmail.com>

I don't see a way to create defender methods that would do anything useful,
which means that it'd break existing code, which is a non-starter in the
JDK world AFAIK


On Thu, Jan 3, 2013 at 6:21 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> On Thu, Jan 3, 2013 at 8:32 AM, ?iktor ?lang <viktor.klang at gmail.com>
> wrote:
> >
> >
> >
> > On Thu, Jan 3, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> >>
> >> On 01/03/13 09:04, ?iktor ?lang wrote:
> >>
> >>>
> >>> Question is, ...
> >>>
> >>> how do I as a caller that I have given the callee the right to cancel
> the
> >>> Future?
> >>
> >>
> >> Or more simply: How do you know that there are no other possible
> >> uses of a possibly shared completion?
> >>
> >> And the simple answer: In general you cannot know. Even though
> >> it may appear at some moment that there are no consumers, one
> >> may appear by the time you act to cancel. So all fully reliable
> >> uses require some non-local context that somehow knows that it
> >> is OK.
> >
> >
> > Yes, exactly, which is why you encode it in the signature:
> >
> > def f1(f: Future[String] with Cancelable) = f.cancel()
> > def f2(f: Future[String]) = f map { _.toUppercase } foreach println
> >
> > Now I know as a caller that f1 is likely to want to be able to cancel the
> > Future, and f2 can't cancel it, so if all I have is a plain
> Future[String],
> > I do not have the capability to cancel myself, and as such I cannot let
> > anyone else do it either.
> >
> > val f: Future[String] = computeFuture()
> >
> > f1(f) //compiler error, this dude wants cancelability
> > f1(f.withNopCancel()) // returns a Future with Cancelable whose cancel
> > method does nothing
> > f2(f) // We're cool here
> >
> >>
> >>
> >>
> >>>
> >>> (Here's where I think a separation of the Cancellable and the Future
> is a
> >>> good idea)
> >>>
> >>
> >> I agree. Too late for that in j.u.c though. The best we can do
> >> is make it easy/attractive not to use it when you don't need to.
> >
> >
> > No java.util.nconcurrent2 ? :-)
>
> Is it not possible to introduce a super interface to Future?
>
>     interface Async<V>
>
>         boolean isDone();
>         V get();
>
>         void onComplete(Runnable);
>
>         default Async<R> thenApply(Function<V,R>){ ... }
>         etc.
>
>     interface Future<V> extends Async<V>
>         void cancel();
>         boolean isCancelled();
>
>         default void onComplete(Runnable){ ... }
>
> Zhong Yu
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/cc20c046/attachment.html>

From mikeb01 at gmail.com  Thu Jan  3 14:43:28 2013
From: mikeb01 at gmail.com (Michael Barker)
Date: Fri, 4 Jan 2013 08:43:28 +1300
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <50E5A5F5.9040307@oracle.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CACuKZqH1sjxCoX1oAfhkW-qrX=4yd+K8aHZ9BuH9be6K7KOYTg@mail.gmail.com>
	<CALwNKeQ=9BvwsXahUozQGVG2rh-nYn+O_SF+KWKOdUT6mC0UwQ@mail.gmail.com>
	<50E5A5F5.9040307@oracle.com>
Message-ID: <CALwNKeTHMNfu5=OAJrfeiRmMnSmrGmnoDoK8E4Lpagc-c8QmVw@mail.gmail.com>

> One of the problems with thread structures is that the structure has to
> follow the thread around every time the thread switches to another core.
> This happens every time the thread blocks or when its time slice runs out.
> Yanking the structure into the core adds a lot of latency.  Also, having a
> structure per thread creates a lot of structures which hog cache space.

That's going to depend on a couple of factors.  How many threads you
have and the number of events that a thread will process before it
gets context switched out.

> A better solution is to use per core structures (see the ProcessorLocal
> discussion on this group).  Each core is assigned a structure.  This will
> reduce the number of structures to the number of cores which should be far
> less than the number of threads.  Cache capacity isn't as stressed.  When
> the counter needs to be updated, the thread detects which core it is running
> on, grabs the reference for the assigned structure and uses an atomic
> increment, decrement or CAS.
>
> But, Nathan isn't the atomic instruction going to put us back to the
> original problem of poor CAS performance?  The answer is no.  Since the
> structure is assigned to a core, the structure should stay in the cache of
> that core.  Depending upon read frequency, the cache lines holding the
> structure should already be in the modified or exclusive state.  Thus, there
> shouldn't be any delays from having to send invalidation signals.  No other
> core will be yanking the structure to its own cache.  Hence, the performance
> of the atomic instruction will be very cheap (i.e. several cycles) and
> getting cheaper with each new Intel processor.

I would like to see actual numbers on that, because cross core cache
invalidation is not the only cost of a CAS instruction.  A LOCKed
instruction is also going to stall the pipeline and flush the store
buffers before allowing other instructions to continue as well as
reducing the compiler's ability to reorder instructions.  It's pretty
easy to show a micro benchmark that demonstrates over an order of
magnitude difference between CAS and lazySet even with CPU affinity.
However, the micro benchmark doesn't tell the full story (as is
typically the case).  The LOCK instructions impact quite heavily on
the performance of the code around it, where as a lazySet is just a
MOV and will happy float through the pipeline and store buffers along
with all of the other instructions.  With a CAS you will have
monitoring code that can have a significant performance impact on the
application code (even without contention), which is the stuff that
you really want to remain fast.  We ran into a similar problem a
couple of years ago.  We had already isolated writers to individual
threads, but had used volatile variables to ensure atomicity and
visibility.  However we found that when we added all of the volatile
counters that we needed it degraded the performance of our macro
benchmarks.  In the end we just went with simple fields (didn't know
about lazySet then and relied our consistent use of 64 bit systems to
give atomicity - okay for in-house, not recommended as a generalised
solution) and we got our performance back.  The ProcessorLocal sounds
like a good idea as long as you didn't need a LOCKed instruction to do
the update.

Mike.

From stanimir at riflexo.com  Thu Jan  3 16:48:03 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 3 Jan 2013 23:48:03 +0200
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CACuKZqE1=_E+sKNQXd5ftaWb1E1+nGwyEmVju0LQ0+4v-kdNGg@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50E44A36.9000509@redhat.com>
	<50E4581C.3080503@cs.oswego.edu> <50E46174.4060306@redhat.com>
	<CANPzfU_LCj0CRT1W1aDRbbkNqDTNTfFrCE=awAAjsw2OkCWQpQ@mail.gmail.com>
	<50E468F8.7020706@redhat.com> <50E46B77.5060004@cs.oswego.edu>
	<50E4714E.6000401@redhat.com> <50E47516.5090000@cs.oswego.edu>
	<50E48088.7080101@redhat.com> <50E483BD.7040506@cs.oswego.edu>
	<CANPzfU993U62pQzJMYj-K7G4FTWQHxan9hQAn97KL+saj475cg@mail.gmail.com>
	<50E4A582.8010400@cs.oswego.edu>
	<CANPzfU9hvDiQNOa3nA3MUWrDL_gDPrTBT+6AzJ4m37dM5bUZVg@mail.gmail.com>
	<50E58841.6050905@cs.oswego.edu>
	<CANPzfU9yAxN26X_wi-B3qEDYC62E9TPtUPiy5D5O2znbbBq+RQ@mail.gmail.com>
	<50E58EBD.6030208@cs.oswego.edu>
	<CANPzfU9-Aykmi1zGjon2TdD5uGSXC-4UaLdwdYyDfSC=oXTn4A@mail.gmail.com>
	<50E593DF.5060509@cs.oswego.edu>
	<CANPzfU8W17eChdt=pL+HiOV7Jx9f2BNz3VcJhubvY1JHCO=JiA@mail.gmail.com>
	<CACuKZqE1=_E+sKNQXd5ftaWb1E1+nGwyEmVju0LQ0+4v-kdNGg@mail.gmail.com>
Message-ID: <CAEJX8oqQTec=Ysy0yLTi5VufASsNuaw5sYu5YPO3+-6nS9RY4g@mail.gmail.com>

Is it not possible to introduce a super interface to Future?
>
>
That would break any custom implemented Future if there are any methods
not present. There are cases (java.sql) to add methods to interfaces but
that's rare. It's truly pain in the neck to work w/ older drivers source
code on a newer JDK.

Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/94b7a1fd/attachment.html>

From davidcholmes at aapt.net.au  Thu Jan  3 23:28:53 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 4 Jan 2013 14:28:53 +1000
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <50E585EE.1030406@java4.info>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEFKJJAA.davidcholmes@aapt.net.au>

That suggestion doesn't change anything with respect to (a) and (b) AFAICS.
Where does any kind of atomicity or thread-safety come from? You could
detect after the fact that the state moved away from what you wanted (but
beware the ABA problem) but how does that help?

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Florian
Binder
  Sent: Thursday, 3 January 2013 11:22 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] A simple predicate based synchronizer


  What about giving the Predicate two arguments? The current state and the
await state? This should ensure a) and b).
  Even more you could add a StateChangeAssertion to ensure some state change
criteria like monotony.


  public class SimpleSynchronizer<T> {
      SimpleSynchronizer(Predicate<T> checker, StateChangeAssertion<T>
stateAssertion)

      void await(T t) throws InterruptedException;

      boolean await(T t, long time, TimeUnit unit) throws
InterruptedException;

      void signal(T s) throws IllegalStateException;

      //probably some more methods

  }


  SimpleSynchronized<Integer> ss=new SimpleSynchronized( (s,t) -> s >= t,
(old,new) -> old >= new);



  to wait on a particular state you would call:
  ss.await(4); // or
  ss.await(9, 10, TimeUnit.SECONDS);


  Whenever the services state changed you would call:
  ss.signal(newState);

  /Flo



  Am 31.12.2012 15:54, schrieb David Holmes:

    A latch is a thousand times simpler than using AQS. Any "wait for event"
can be handled using a simple latch.

    The problem with your predicate based solution is that, as I said
generally you need to to ensure that:
    a) the predicate is evaluated in a thread-safe way
    b) the predicate remains valid while you act on it.

    If you don;t need the above then a simple existing synchronizer is
likely to suffice.

    I simply don't see what you suggest as being a generally useful tool -
and I do see it as one easily misused because you do in fact need
thread-safety and atomicity.

    YMMV.

    David

      -----Original Message-----
      From: Kasper Nielsen [mailto:kasperni at gmail.com]
      Sent: Monday, 31 December 2012 7:26 PM
      To: dholmes at ieee.org
      Cc: Concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] A simple predicate based
synchronizer


      On Mon, Dec 31, 2012 at 4:04 AM, David Holmes
<davidcholmes at aapt.net.au> wrote:

        General edge-triggers are normally accommodated through the existing
synchronizers: semaphores, latches, barriers, gates. Your "await start"
example is just a latch.

      I don't really see you point? Everything can be build using
AbstractQueuedSynchronizer as well. But that doesn't necessarily make it
easy for end users.


      One area where I really miss something like this is when writing unit
tests. Being able to do something like this
      T1: signal("TestDone")
      T2: await(s -> s.equals("TestDone"), 10, TimeUnit.SECONDS) or even

      T2: awaitEqualsTo("TestDone", 10, TimeUnit.SECONDS)

      would make my life much easier.


      While I could use CountdownLatch in places like this. I usually end up
with a couple of them whenever multiple states are involved. Which make it
much harder to read at a later time.



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130104/7247e4fe/attachment-0001.html>

From nathan.reynolds at oracle.com  Fri Jan  4 01:10:16 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 03 Jan 2013 23:10:16 -0700
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CALwNKeTHMNfu5=OAJrfeiRmMnSmrGmnoDoK8E4Lpagc-c8QmVw@mail.gmail.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CACuKZqH1sjxCoX1oAfhkW-qrX=4yd+K8aHZ9BuH9be6K7KOYTg@mail.gmail.com>
	<CALwNKeQ=9BvwsXahUozQGVG2rh-nYn+O_SF+KWKOdUT6mC0UwQ@mail.gmail.com>
	<50E5A5F5.9040307@oracle.com>
	<CALwNKeTHMNfu5=OAJrfeiRmMnSmrGmnoDoK8E4Lpagc-c8QmVw@mail.gmail.com>
Message-ID: <50E67248.4060006@oracle.com>

*Background**
*
Originally, I had a standard mutex protecting some shared data. This 
mutex became very contended.  I noticed that all of the operations on 
the protected data were read operations.  This is because a write 
operation only happens when an administrator changes the configuration.  
So, the logical solution was to change the mutex out for a reader/writer 
lock.  This helped for a while until Intel produced the next processor.  
The contention came back.

I noticed that the implementation of the reader/writer lock was very 
poor.  It used a mutex to protect the state of the reader/writer lock!  
So, I changed the implementation of the reader/writer lock to use a CAS 
instruction to acquire/release the lock.  This helped quite a bit.  But, 
again only for a while until Intel produced the next processor.

This time the cache line holding the reader/writer lock's state was 
sloshing.  Sloshing is a new term to me.  Let me explain it... just in 
case it is new to you.  The cache line was flying quickly from one core 
to the next in order for threads to acquire/release the reader/writer 
lock.  So, I was a bit puzzled.  How do I make readers even faster?  
Writers are extremely rare.  I then had the idea to stripe the actual 
lock and not the data.  In other words, I created 256 reader/writer 
locks.  Each time a thread needs to acquire a lock, it simply 
incremented a 32-bit int without using any atomics.  The result was then 
bit-anded to pick a lock.  The thread would then acquire that lock.  
When the thread was done with the critical section, it would release 
that same lock.  Since no atomics were being used, the updates to the 
32-bit int were much faster and if 2 threads ended up with the same 
result no big deal.  The two threads would simply contend on acquiring 1 
lock for a few cycles.  Now, readers were spread out to different locks 
yet writers had to acquire all of the locks.  (Writer performance isn't 
a concern).  This worked great for a while until Intel produced the next 
processor, Nehalem EX.

We were trying to run the above server written in C++ on a 4-socket 
Nehalem EX machine.  If we pulled the 4^(t)^(h) socket and used only 3 
sockets, then we achieved the same throughput yet all of the CPUs were 
running at 100%!  We narrowed down the problem to cache line sloshing on 
the non-atomic increment!  I scratched my head for a while and thought 
how in the world am I going to fix this.  I can't speed up the 
non-atomic increment.  She's going as fast as she can captain!  :)  I 
came up with 4 possible solutions: Thread Local Storage, Hash Stack 
Pointer, RDTSC and APIC ID.

Thread Local Storage is where each thread gets its own reader/writer 
lock.  Readers simply acquire the assigned lock to the thread.  Writers 
acquire all of the locks.  Of course, the locks have to be published in 
a global data structure with its own lock for writers to access.  
However, this isn't a problem since threads aren't being created or 
destroyed very quickly.

Hash Stack Pointer is kind of like Thread Local Storage.  The stack 
pointer is unique to the thread and is much quicker to access than 
Thread Local Storage (i.e. read a register).  I simply hashed the stack 
pointer to select 1 lock out of 256 locks.  I hoped that the threads 
wouldn't contend on the same lock too often.

RDTSCP is similar to Hash Stack Pointer.  Instead of using the stack 
pointer as the input to the hash function, it uses the Intel RDTSCP 
instruction which returns the number of cycles since the core has been 
powered on.  I found out that the lower 2 bits are always 0 and had to 
ignore them when hashing.  Again, I hoped that the threads wouldn't 
contend on the same lock too often since it should be only a 1 in 256 
chance.

APIC ID is a unique number assigned to the core.  It is used primarily 
for interrupt dispatching hence each core must have a unique APIC ID in 
the entire system.  The value is available in user-land via CPUID 
instruction.  This is what ProcessorLocal could be based on.  However, 
if the OS supports it, RDTSCP could return a unique number for the core 
in the ECX register.  This latter mechanism would be much cheaper but 
wasn't well supported when I was addressing the issue.  The APIC ID is 
an 8-bit quantity.  This means that each core would be assigned a 
specific lock in the set of 256 locks I have been using all along.

We tested all for approaches and compared them to the original 
non-atomic increment.  If I remember right, Hash Stack Pointer and 
RDTSCP got about a 10% improvement in throughput.  Thread Local Storage 
got about a 15% improvement in throughput.  APIC ID got a 20% 
improvement in throughput.

This made me a firm believer in Processor Local Storage.  With Thread 
Local Storage, you don't need any locking on the stored objects since 
most of the time they are private to the thread. With Processor Local 
Storage, you need locking since multiple threads could be accessing the 
stored objects although extremely rarely will it be concurrently.  With 
the cache benefits Processor Local Storage provides, it makes atomic 
operations very cheap since the cache line can stay in L1 cache unless 
space is needed for other hotter cache lines.

I have a Conroe desktop processor 
(http://en.wikipedia.org/wiki/Conroe_%28microprocessor%29).  If I 
remember right, a CAS instruction takes a minimum 25 cycles on this 
processor.  This requires the cache line to be in L1 cache. I've been 
told from sources which definitely know that CAS latency has greatly 
improved on the next generation of Intel processors. They didn't give me 
any numbers.  :(  I wouldn't be surprised if CAS latency is down to 
around 9 cycles (3 cycles to load, 3 to compare and 3 to store).

The other significant problem is that CAS includes a fence.  This forces 
all subsequent loads (and stores) to wait until the CAS store has 
finished (and not for the instruction to retire).  A 9 cycle stall is 
nothing.  The problem with the CAS fence is when the CAS instruction 
takes 300+ cycles because the processor is simply waiting for the cache 
line to arrive due to excessive cache line sloshing.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 1/3/2013 12:43 PM, Michael Barker wrote:
>> One of the problems with thread structures is that the structure has to
>> follow the thread around every time the thread switches to another core.
>> This happens every time the thread blocks or when its time slice runs out.
>> Yanking the structure into the core adds a lot of latency.  Also, having a
>> structure per thread creates a lot of structures which hog cache space.
> That's going to depend on a couple of factors.  How many threads you
> have and the number of events that a thread will process before it
> gets context switched out.
>
>> A better solution is to use per core structures (see the ProcessorLocal
>> discussion on this group).  Each core is assigned a structure.  This will
>> reduce the number of structures to the number of cores which should be far
>> less than the number of threads.  Cache capacity isn't as stressed.  When
>> the counter needs to be updated, the thread detects which core it is running
>> on, grabs the reference for the assigned structure and uses an atomic
>> increment, decrement or CAS.
>>
>> But, Nathan isn't the atomic instruction going to put us back to the
>> original problem of poor CAS performance?  The answer is no.  Since the
>> structure is assigned to a core, the structure should stay in the cache of
>> that core.  Depending upon read frequency, the cache lines holding the
>> structure should already be in the modified or exclusive state.  Thus, there
>> shouldn't be any delays from having to send invalidation signals.  No other
>> core will be yanking the structure to its own cache.  Hence, the performance
>> of the atomic instruction will be very cheap (i.e. several cycles) and
>> getting cheaper with each new Intel processor.
> I would like to see actual numbers on that, because cross core cache
> invalidation is not the only cost of a CAS instruction.  A LOCKed
> instruction is also going to stall the pipeline and flush the store
> buffers before allowing other instructions to continue as well as
> reducing the compiler's ability to reorder instructions.  It's pretty
> easy to show a micro benchmark that demonstrates over an order of
> magnitude difference between CAS and lazySet even with CPU affinity.
> However, the micro benchmark doesn't tell the full story (as is
> typically the case).  The LOCK instructions impact quite heavily on
> the performance of the code around it, where as a lazySet is just a
> MOV and will happy float through the pipeline and store buffers along
> with all of the other instructions.  With a CAS you will have
> monitoring code that can have a significant performance impact on the
> application code (even without contention), which is the stuff that
> you really want to remain fast.  We ran into a similar problem a
> couple of years ago.  We had already isolated writers to individual
> threads, but had used volatile variables to ensure atomicity and
> visibility.  However we found that when we added all of the volatile
> counters that we needed it degraded the performance of our macro
> benchmarks.  In the end we just went with simple fields (didn't know
> about lazySet then and relied our consistent use of 64 bit systems to
> give atomicity - okay for in-house, not recommended as a generalised
> solution) and we got our performance back.  The ProcessorLocal sounds
> like a good idea as long as you didn't need a LOCKed instruction to do
> the update.
>
> Mike.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130103/4e63efc2/attachment.html>

From hans.boehm at hp.com  Fri Jan  4 17:44:27 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 4 Jan 2013 22:44:27 +0000
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E56786.5090301@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de>
	<50E1A44D.1090509@oracle.com>	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de> <50E44964.6030007@kuli.org>
	<CAE-f1xTb6F7SWyXUFrMnmL4nUky8V5yq5TmP3T69bFqw-6J82A@mail.gmail.com>
	<50E452D5.5060902@kuli.org>
	<CAE-f1xSMLmZCyv+aSDOf3LFidezqZZuknmwYTKMXfrMZbX+y4g@mail.gmail.com>
	<50E47192.8010406@infinite-source.de>
	<50E482E2.5060705@infinite-source.de>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236967CCC@G9W0725.americas.hpqcorp.net>
	<50E4B601.6000005@infinite-source.de>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236968078@G9W0725.americas.hpqcorp.net>
	<50E56786.5090301@infinite-source.de>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369687EB@G9W0725.americas.hpqcorp.net>

> From: Aaron Grunthal [mailto:aaron.grunthal at infinite-source.de]
> 
> On 03.01.2013 07:32, Boehm, Hans wrote:
> >> From: Aaron Grunthal [mailto:aaron.grunthal at infinite-source.de]
> >>
> >> More generally our main problem is that we lack data structures that
> provide
> >> non-volatile read semantics (i.e. possibly stale under concurrent
> modification
> >> but fast and loop-hoistable) while using atomic write algorithms,
> mostly for
> >> internal consistency under concurrent access.
> >>
> > We could debate how useful those are in general.  It seems to me that
> you need them because you're implementing a programming language that
> provides Java-memory-model-like semantics for data races.  That's
> certainly entirely reasonable.  But I'm not convinced that such data
> structures are correctly usable by enough programmers to make them
> terribly interesting for direct programmer use.  And if I were
> designing a programming language now, I'd work hard to avoid Java-like
> data race semantics to start with.
> 
> Yes, we basically need two things
> 
> a) fast single-threaded performance in our internal data structures
> since we can't specialize for single and multilmultiple threads.
> b) non-catastrophic behavior under multi-threaded access
> 
> There are many things that ruby allows that are perfectly sensible if
> performed by a single thread. Maybe-reasonable when performed on
> separate parts of the class hierarchy by multiple threads. And
> undefined
> when performed by multiple threads on the same parts.
> 
> And there are no ConcurrentModificationExceptions or anything. Many of
> the metaprogramming changes also cascade downwards in the class
> hierarchy, so using locks on internal structures would lead to worse
> single-threaded performance *and* risk deadlocks.
> 
> So atomic writes and racy reads are quite representative of ruby
> behavior.
I can understand how we got here.  If we were talking about how to design new languages, as opposed to how to implement old ones, I would still argue that unannotated data races in trusted code should just be treated as errors, hopefully with debugging tools to identify as them to the greatest extent possible.  You don't really want to expose the typical programmer to something like the Java memory model.  Supporting untrusted code is a much harder problem, to which I think we still have only partial answers.
> 
> 
> But dynamic language implementations are not the only things that could
> benefit from such things. I was actually thinking of non-volatile read
> methods *in addition* to the volatile ones. E.g. a lazyGet(key) on
> concurrent maps.
> These things can be useful for caches in cases where performance is
> more
> important than seeing the most-recent values when they are frequently
> mutated anyway and no matter what you read it might be stale within
> milliseconds anyway.
> 
> E.g. I implemented a UDP-based DHT node where throughput was more
> important than seeing the latest state updates from incoming requests,
> simply because the underlying transport itself was already lossy it was
> acceptable to have some slightly-stale reads in the application itself.
> 
> 
> The case may also become more important with lambda expressions in java
> as code defining the lambda may not know how often the lambda is used
> and the invoking code may want to loop-hoist some reads but can't do
> that if always-concurrent data structures are used. Having finer
> control
> here would allow hotspot to do the performance optimizations instead of
> requiring the programmer to do them manually.
> 
> manual hoisting:
> 
> final Integer entry = concurrentLookupTable.get("key")
> new Range(20,1000).each(x -> x * entry)
> 
> hotspot doing the hoisting:
> 
> new Range(20,1000).each(x -> x * concurrentLookupTable.lazyGet("key"))
> 
> 
> With method chaining the manual version can quickly become quite
> verbose.
The big down side is that you lose any notion of consistency between separately retrieved data, possibly even between the two halves of a long or double.  And in a Java-memory-model like setting, you have to deal with other weirdnesses, such as the fact that an int table entry that is only ever incremented by a single thread may appear to decrease (e.g. because every other reference uses an old value).  All the indications I've seen suggest that very few programmers get such code right.

My impression is that this is somewhat easier to tolerate in the DHT case, because the unit of atomicity tends to be larger, data structures tend to be more independent, and people have been trained to write distributed applications defensively so that they can recover from remote failures.

Hans


From kasperni at gmail.com  Sun Jan  6 10:12:25 2013
From: kasperni at gmail.com (Kasper Nielsen)
Date: Sun, 6 Jan 2013 16:12:25 +0100
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>
References: <CAPs6152Rfc+O3Ge4LoNJ0wx7Aw0DJ2_LUG6=RFTXkPNGEo+UOQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>
Message-ID: <CAPs61532S8jpM-OzbqzOPcny81kd4H0a-cp2deaadm2=ZgVJ2w@mail.gmail.com>

I should have probably have given a more detailed description of my problem.
I have a service, similar to the one they have in Guava
http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/util/concurrent/Service.html
.

The service can be in several well defined states as in
http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/util/concurrent/Service.State.html

Now I want an easy way to implement
*public* *final* *boolean* awaitState(State state, *long* timeout, TimeUnit
unit) *throws* InterruptedException
as in
service.awaitState(State.STOPPED, 10, TimeUnit.SECONDS) or
service.awaitState(State.RUNNING, 1, TimeUnit.SECONDS)

CountdownLatch is out of the question because it is a one-shot action.
And CyclicBarrier needs to know the number of parties before hand.

Maybe my use case is to specialized for inclusion into juc. All the
transitions are well defined and there are no cycles. Furthermore
transitions are never happening concurrently.



On Mon, Dec 31, 2012 at 3:54 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> A latch is a thousand times simpler than using AQS. Any "wait for event"
> can be handled using a simple latch.
>
> The problem with your predicate based solution is that, as I said
> generally you need to to ensure that:
> a) the predicate is evaluated in a thread-safe way
> b) the predicate remains valid while you act on it.
>
> If you don;t need the above then a simple existing synchronizer is likely
> to suffice.
>
> I simply don't see what you suggest as being a generally useful tool - and
> I do see it as one easily misused because you do in fact need thread-safety
> and atomicity.
>
> YMMV.
>
> David
>
>
> -----Original Message-----
> *From:* Kasper Nielsen [mailto:kasperni at gmail.com]
> *Sent:* Monday, 31 December 2012 7:26 PM
> *To:* dholmes at ieee.org
> *Cc:* Concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] A simple predicate based
> synchronizer
>
> On Mon, Dec 31, 2012 at 4:04 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>> **
>> General edge-triggers are normally accommodated through the existing
>> synchronizers: semaphores, latches, barriers, gates. Your "await start"
>> example is just a latch.
>>
> I don't really see you point? Everything can be build using AbstractQueuedSynchronizer as
> well. But that doesn't necessarily make it easy for end users.
>
> One area where I really miss something like this is when writing unit
> tests. Being able to do something like this
> T1: signal("TestDone")
> T2: await(s -> s.equals("TestDone"), 10, TimeUnit.SECONDS) or even
> T2: awaitEqualsTo("TestDone", 10, TimeUnit.SECONDS)
> would make my life much easier.
>
> While I could use CountdownLatch in places like this. I usually end up
> with a couple of them whenever multiple states are involved. Which make
> it much harder to read at a later time.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130106/776fdf8b/attachment.html>

From kasperni at gmail.com  Sun Jan  6 10:21:01 2013
From: kasperni at gmail.com (Kasper Nielsen)
Date: Sun, 6 Jan 2013 16:21:01 +0100
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEDJJJAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCAEDJJJAA.davidcholmes@aapt.net.au>
Message-ID: <CAPs6153vbEF1ZB_TBTOqqVaj1H5+zPJOpD=QHjWbErqYNSychw@mail.gmail.com>

On Mon, Dec 31, 2012 at 11:16 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> Further without external state synchronization you would have to resort to
> polling to avoid lost wakeups, or state changes would only be noticed at
> the next state change if any eg:
>

Depends on your implementation.


Here is what I currently do (see my earlier post for the use case). It is
not in the general Predicate based form. But could be adapted pretty easily.
*
*
*volatile* CountDownLatch awaitStateLatch = *new* CountDownLatch(1);

*
*

*public* *final* *boolean* awaitState(State state, *long* timeout, TimeUnit
unit) *throws* InterruptedException {

   *long* deadline = Math.*min*(Long.*MAX_VALUE*, System.*nanoTime*() +
unit.toNanos(timeout));

   CountDownLatch latch;

    *while* ((latch = awaitStateLatch) != *null* && state.ordinal() >
getState().ordinal()) {

       *if* (awaitStateLatch == latch && !latch.await(deadline - System.*
nanoTime*(), TimeUnit.*NANOSECONDS*)) {

          *return* *false*;

       }

    }

   *return* *true*;

}

*void* updateState(*State* currentState, *State* newState) {

  state = newState;

  CountDownLatch prev = awaitStateLatch;

  awaitStateLatch = *new* CountDownLatch(1);

  prev.countDown();

}
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130106/18e80667/attachment.html>

From zhong.j.yu at gmail.com  Sun Jan  6 10:30:03 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sun, 6 Jan 2013 09:30:03 -0600
Subject: [concurrency-interest] RFR 8005311: Add Scalable Updatable
 Variables, DoubleAccumulator, DoubleAdder, LongAccumulator, LongAdder
In-Reply-To: <50E96278.8020106@cs.oswego.edu>
References: <50E86CAC.8090606@oracle.com> <50E8BA6B.3070809@univ-mlv.fr>
	<50E96278.8020106@cs.oswego.edu>
Message-ID: <CACuKZqHi+53yMuephL34nSp4GS+vW=qv5sKSu6B4DRyVR2X5hg@mail.gmail.com>

Just to confirm, in
    (a=b)==null
or
    x = (a=b);
variable `a` is not read, as far as JMM is concerned, correct?

Zhong Yu

On Sun, Jan 6, 2013 at 5:39 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 01/05/13 18:42, Remi Forax wrote:
>
>> The code is not very java-ish,

>> by example, in Striped64,

>> Cell[] as; Cell a; int n; long v;
>> if ((as = cells) != null && (n = as.length) > 0) {

instead

>> int n;
>> Cell[] as = cells;
>> if (as != null && (n = as.length) > 0) {

> But is very consistent across j.u.c. As a convention,
> inline assignments to locals are used to hold field reads
> to visually ensure use of consistent snapshots. The C-like
> look-and-feel is a less important than is ability to simply
> check these cases by inspection.
>
>
>>
>> Overall, I think there are too many lazy initializations.
>> Unlike HashMap, if a developer uses let say LongAccumulator it's because
>> AtomicLong doesn't work well,
>> so not having the array of cells initialized by default seems weird.
>
>
> You wouldn't say this if you were on a 256-way machine with
> millions of LongAdders, where only a thousand of them heavily
> contended :-)
>
> -Doug
>

From dl at cs.oswego.edu  Sun Jan  6 10:44:46 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 06 Jan 2013 10:44:46 -0500
Subject: [concurrency-interest] RFR 8005311: Add Scalable Updatable
 Variables, DoubleAccumulator, DoubleAdder, LongAccumulator, LongAdder
In-Reply-To: <CACuKZqHi+53yMuephL34nSp4GS+vW=qv5sKSu6B4DRyVR2X5hg@mail.gmail.com>
References: <50E86CAC.8090606@oracle.com> <50E8BA6B.3070809@univ-mlv.fr>
	<50E96278.8020106@cs.oswego.edu>
	<CACuKZqHi+53yMuephL34nSp4GS+vW=qv5sKSu6B4DRyVR2X5hg@mail.gmail.com>
Message-ID: <50E99BEE.7020800@cs.oswego.edu>

On 01/06/13 10:30, Zhong Yu wrote:
> Just to confirm, in
>      (a=b)==null
> or
>      x = (a=b);
> variable `a` is not read, as far as JMM is concerned, correct?
>

Yes, the value of an assignment statement is the value of its right-hand
side. See the JLS specs:
   http://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.26

-Doug





From forax at univ-mlv.fr  Sun Jan  6 11:27:40 2013
From: forax at univ-mlv.fr (Remi Forax)
Date: Sun, 06 Jan 2013 17:27:40 +0100
Subject: [concurrency-interest] RFR 8005311: Add Scalable Updatable
 Variables, DoubleAccumulator, DoubleAdder, LongAccumulator, LongAdder
In-Reply-To: <50E99BEE.7020800@cs.oswego.edu>
References: <50E86CAC.8090606@oracle.com> <50E8BA6B.3070809@univ-mlv.fr>
	<50E96278.8020106@cs.oswego.edu>
	<CACuKZqHi+53yMuephL34nSp4GS+vW=qv5sKSu6B4DRyVR2X5hg@mail.gmail.com>
	<50E99BEE.7020800@cs.oswego.edu>
Message-ID: <50E9A5FC.5040500@univ-mlv.fr>

On 01/06/2013 04:44 PM, Doug Lea wrote:
> On 01/06/13 10:30, Zhong Yu wrote:
>> Just to confirm, in
>>      (a=b)==null
>> or
>>      x = (a=b);
>> variable `a` is not read, as far as JMM is concerned, correct?
>>
>
> Yes, the value of an assignment statement is the value of its right-hand
> side. See the JLS specs:
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.26
>
> -Doug

yes, and this is inconsistent with the type checker,
the type of a=b is the type of a and not the type of b.

R?mi


From martinrb at google.com  Sun Jan  6 15:42:54 2013
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 6 Jan 2013 12:42:54 -0800
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs61532S8jpM-OzbqzOPcny81kd4H0a-cp2deaadm2=ZgVJ2w@mail.gmail.com>
References: <CAPs6152Rfc+O3Ge4LoNJ0wx7Aw0DJ2_LUG6=RFTXkPNGEo+UOQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>
	<CAPs61532S8jpM-OzbqzOPcny81kd4H0a-cp2deaadm2=ZgVJ2w@mail.gmail.com>
Message-ID: <CA+kOe0-SE9sSsgj-cLO5fK9GhrJdDJngs8CRBmQbq+iMQ-+yvA@mail.gmail.com>

On Sun, Jan 6, 2013 at 7:12 AM, Kasper Nielsen <kasperni at gmail.com> wrote:

> And CyclicBarrier needs to know the number of parties before hand.
>
>
Phaser does not have that limitation.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130106/5e32a0ca/attachment.html>

From kasperni at gmail.com  Sun Jan  6 16:46:26 2013
From: kasperni at gmail.com (Kasper Nielsen)
Date: Sun, 6 Jan 2013 22:46:26 +0100
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CA+kOe0-SE9sSsgj-cLO5fK9GhrJdDJngs8CRBmQbq+iMQ-+yvA@mail.gmail.com>
References: <CAPs6152Rfc+O3Ge4LoNJ0wx7Aw0DJ2_LUG6=RFTXkPNGEo+UOQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>
	<CAPs61532S8jpM-OzbqzOPcny81kd4H0a-cp2deaadm2=ZgVJ2w@mail.gmail.com>
	<CA+kOe0-SE9sSsgj-cLO5fK9GhrJdDJngs8CRBmQbq+iMQ-+yvA@mail.gmail.com>
Message-ID: <CAPs61520sb1NqJgbfxhVG-sQO3J_=GRv+kKgBYCNv0aCCPTA1Q@mail.gmail.com>

On Sun, Jan 6, 2013 at 9:42 PM, Martin Buchholz <martinrb at google.com> wrote:

>
>
> On Sun, Jan 6, 2013 at 7:12 AM, Kasper Nielsen <kasperni at gmail.com> wrote:
>
>> And CyclicBarrier needs to know the number of parties before hand.
>>
>>
> Phaser does not have that limitation.
>

Right, forgot Phasers.

But as I see it. You cannot avoid the lost lost update problem (with
general Predicates) that David describes when using Phasers.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130106/8023d48b/attachment.html>

From davidcholmes at aapt.net.au  Sun Jan  6 19:52:31 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 7 Jan 2013 10:52:31 +1000
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs6153vbEF1ZB_TBTOqqVaj1H5+zPJOpD=QHjWbErqYNSychw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEGCJJAA.davidcholmes@aapt.net.au>

But now you have internalized the state change into your synchronizer. That
is quite different to the original API and usage that you outlined.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Kasper
Nielsen
  Sent: Monday, 7 January 2013 1:21 AM
  To: Concurrency-interest at cs.oswego.edu
  Cc: dholmes at ieee.org
  Subject: Re: [concurrency-interest] A simple predicate based synchronizer







  On Mon, Dec 31, 2012 at 11:16 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

    Further without external state synchronization you would have to resort
to polling to avoid lost wakeups, or state changes would only be noticed at
the next state change if any eg:


  Depends on your implementation.




  Here is what I currently do (see my earlier post for the use case). It is
not in the general Predicate based form. But could be adapted pretty easily.


  volatile CountDownLatch awaitStateLatch = new CountDownLatch(1);





  public final boolean awaitState(State state, long timeout, TimeUnit unit)
throws InterruptedException {


     long deadline = Math.min(Long.MAX_VALUE, System.nanoTime() +
unit.toNanos(timeout));


     CountDownLatch latch;

      while ((latch = awaitStateLatch) != null && state.ordinal() >
getState().ordinal()) {

         if (awaitStateLatch == latch && !latch.await(deadline -
System.nanoTime(), TimeUnit.NANOSECONDS)) {


            return false;

         }

      }

     return true;

  }


  void updateState(State currentState, State newState) {

    state = newState;

    CountDownLatch prev = awaitStateLatch;


    awaitStateLatch = new CountDownLatch(1);

    prev.countDown();

  }



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130107/94aec89f/attachment.html>

From hans.boehm at hp.com  Mon Jan  7 17:02:46 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 7 Jan 2013 22:02:46 +0000
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs6153vbEF1ZB_TBTOqqVaj1H5+zPJOpD=QHjWbErqYNSychw@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCAEDJJJAA.davidcholmes@aapt.net.au>
	<CAPs6153vbEF1ZB_TBTOqqVaj1H5+zPJOpD=QHjWbErqYNSychw@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369692DB@G9W0725.americas.hpqcorp.net>

Maybe I missed something, but what's wrong with Object.wait() here?

Presumably getState() below is synchronized or the state itself is volatile.  Otherwise it seems quite broken.  It's then not clear to me why this is superior to a traditional monitor-based solution.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Kasper Nielsen
Sent: Sunday, January 06, 2013 7:21 AM
To: Concurrency-interest at cs.oswego.edu
Cc: dholmes at ieee.org
Subject: Re: [concurrency-interest] A simple predicate based synchronizer



On Mon, Dec 31, 2012 at 11:16 PM, David Holmes <davidcholmes at aapt.net.au<mailto:davidcholmes at aapt.net.au>> wrote:
Further without external state synchronization you would have to resort to polling to avoid lost wakeups, or state changes would only be noticed at the next state change if any eg:

Depends on your implementation.


Here is what I currently do (see my earlier post for the use case). It is not in the general Predicate based form. But could be adapted pretty easily.

volatile CountDownLatch awaitStateLatch = new CountDownLatch(1);



public final boolean awaitState(State state, long timeout, TimeUnit unit) throws InterruptedException {

   long deadline = Math.min(Long.MAX_VALUE, System.nanoTime() + unit.toNanos(timeout));

   CountDownLatch latch;

    while ((latch = awaitStateLatch) != null && state.ordinal() > getState().ordinal()) {

       if (awaitStateLatch == latch && !latch.await(deadline - System.nanoTime(), TimeUnit.NANOSECONDS)) {

          return false;

       }

    }

   return true;

}

void updateState(State currentState, State newState) {

  state = newState;

  CountDownLatch prev = awaitStateLatch;

  awaitStateLatch = new CountDownLatch(1);

  prev.countDown();

}


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130107/7f59add7/attachment.html>

From aleksey.shipilev at oracle.com  Wed Jan  9 08:17:06 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 09 Jan 2013 17:17:06 +0400
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <CAEJX8opxcWQNUfqJd2Kwa_QJO=JjKYGyvT8C6_zzgnYiVr6P7g@mail.gmail.com>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
	<50DDF036.7000900@oracle.com>
	<CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>
	<50DDFA11.1050602@oracle.com> <50DE050F.8070202@oracle.com>
	<50DE0863.3010402@oracle.com> <50DE0BE6.9020109@oracle.com>
	<50DEC296.70207@oracle.com>
	<CAEJX8opxcWQNUfqJd2Kwa_QJO=JjKYGyvT8C6_zzgnYiVr6P7g@mail.gmail.com>
Message-ID: <50ED6DD2.9030806@oracle.com>

(dugging up myself from under the NY mail)

Nope, this seems to be the genuine scheduling "corner case". To recap,
in this scenario:

   Thread 1 |  Thread 2  |  Thread 3  |  Thread 4
  ----------+------------+------------+------------
     x = 1  |    y = 1   |   r1 = x   |  r4 = y
            |            |   r2 = y   |  r3 = x

The outcome <r1,r2,r3,r4> = <0, 1, 1, 0> is possible with this execution:

   Thread 1 |  Thread 2  |  Thread 3  |  Thread 4
  ----------+------------+------------+------------
            |            |   r1 = x   |  r4 = y
     x = 1  |    y = 1   |            |
            |            |   r2 = y   |  r3 = x

...and so it relies heavily on T3 and T4 being in the tricky timings vs
each other. Inserting the yields, like this:

   Thread 1 |  Thread 2  |  Thread 3  |  Thread 4
  ----------+------------+------------+------------
            |            |   r1 = x   |  r4 = y
     x = 1  |    y = 1   |   yield()  |  yield()
            |            |   r2 = y   |  r3 = x

...brings the probability up to the same order of magnitude as the other
events:

     Observed state  Occurrences          Expectation
       [0, 1, 1, 0] (    120698)     KNOWN_ACCEPTABLE // !!!
       [1, 0, 0, 1] (         0)            FORBIDDEN
       [0, 0, 1, 1] (   3132752)           ACCEPTABLE
       [0, 0, 1, 0] (    870258)           ACCEPTABLE
       [1, 0, 1, 1] (   2803945)           ACCEPTABLE
       [1, 0, 1, 0] (   1027604)           ACCEPTABLE
       [0, 1, 0, 1] (    976260)           ACCEPTABLE
       [0, 1, 0, 0] (    873921)           ACCEPTABLE
       [1, 1, 0, 1] (   2519296)           ACCEPTABLE
       [1, 1, 0, 0] (   3066130)           ACCEPTABLE
       [0, 1, 1, 1] (  12646666)           ACCEPTABLE
       [0, 0, 0, 1] (     28889)           ACCEPTABLE
       [0, 0, 0, 0] (    179446)           ACCEPTABLE
       [1, 1, 1, 1] ( 157122877)           ACCEPTABLE
       [1, 1, 1, 0] (  14613351)           ACCEPTABLE
       [1, 0, 0, 0] (     24287)           ACCEPTABLE

-Aleksey.

On 12/29/2012 09:59 PM, Stanimir Simeonoff wrote:
> Aleksey,
> Perhaps you can disable the hyperthreading and check the results again,
> it could be that cache misses doesn't favor one of settings?
> 
> Stanimir
>  
> 
> On Sat, Dec 29, 2012 at 12:14 PM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
> 
>     On 12/29/2012 01:15 AM, Nathan Reynolds wrote:
>     > I looked at the longest answer.  Do the results change if you get
>     rid of
>     > the yield?
> 
>     Disabling yielding will severely limit the throughput (because actors
>     cannibalize the CPUs not letting the injector to push the new set of
>     states to work on).
> 
>     > I join your suspicions that the delta is due to thread scheduling.
> 
>     Yeah, and the interesting part is that only this state is rare. All
>     others are orders of magnitude more frequent. Go figure.
> 
>     -Aleksey.
> 
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From vitalyd at gmail.com  Wed Jan  9 08:29:45 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 9 Jan 2013 08:29:45 -0500
Subject: [concurrency-interest] Linux kernel ticket spinlock article
Message-ID: <CAHjP37FyJE-r+SGzJ+4KYzras0qJhp8sAzYaxF9WsZagH1oadg@mail.gmail.com>

I know there have been discussions on this list about cache line issues
recently, so thought some people may find this Linux kernel issue
interesting: http://lwn.net/SubscriberLink/531254/027006e5f05701e0/

Nothing surprising there, but just reinforces the fragile state we're in. :)

Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130109/dd4ee1c3/attachment.html>

From heinz at javaspecialists.eu  Wed Jan  9 17:00:56 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 10 Jan 2013 00:00:56 +0200
Subject: [concurrency-interest] Concurrency Testing using Alternative
	Hardware
Message-ID: <50EDE898.5060700@javaspecialists.eu>

We are trying to improve the confidence in one of our systems, targeted 
for the x86 processor family.  One of the ideas is to do stress testing 
on a different hardware architecture, for example, on (non-Dalvik) ARM.  
One of my other customers bought an Azul system just to run their 
tests.  Have any of you tried testing your concurrent code on ARM and 
what experiences did you have?

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz 


From aleksey.shipilev at oracle.com  Wed Jan  9 17:14:58 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 10 Jan 2013 02:14:58 +0400
Subject: [concurrency-interest] Concurrency Testing using Alternative
 Hardware
In-Reply-To: <50EDE898.5060700@javaspecialists.eu>
References: <50EDE898.5060700@javaspecialists.eu>
Message-ID: <50EDEBE2.7090604@oracle.com>

On 01/10/2013 02:00 AM, Dr Heinz M. Kabutz wrote:
> We are trying to improve the confidence in one of our systems, targeted
> for the x86 processor family.  One of the ideas is to do stress testing
> on a different hardware architecture, for example, on (non-Dalvik) ARM. 
> One of my other customers bought an Azul system just to run their
> tests.  Have any of you tried testing your concurrent code on ARM and
> what experiences did you have?

I did a few positive runs for java-concurrency-torture on OpenJDK ARM
port on 4-core Cortex-A9. OpenJDK Shark guys seems to be running the
suite as well on ARM/PPC.

It would be interesting to have Azul box to be tested with j-c-t,
although the runners are missing the support for these massively
parallel monsters yet: you will utilize only the small amount of cores,
which would make the test less interesting. Supporting multiple groups
of actor threads in j-c-t is on my priority list.

-Aleksey.


From dawid.weiss at gmail.com  Thu Jan 10 03:32:35 2013
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Thu, 10 Jan 2013 09:32:35 +0100
Subject: [concurrency-interest] Concurrency Testing using Alternative
	Hardware
In-Reply-To: <50EDEBE2.7090604@oracle.com>
References: <50EDE898.5060700@javaspecialists.eu> <50EDEBE2.7090604@oracle.com>
Message-ID: <CAM21Rt962EFDL0b-AnosGE2yqmaG9V4g-juK+=7aSm_TOn8cHA@mail.gmail.com>

> We are trying to improve the confidence in one of our systems, targeted for the x86 processor family.

If it's targeted specifically for the x86 why are you trying to test
on non-x86 hardware? :)

Apache Lucene/Solr has an extensive test suite that runs on a
multitude of platforms (I think all of them x86 based though). The
experience is that you can easily get very far at spotting
concurrency/ code issues by shuffling JVM vendors, JVM options (GC
configuration, bitness) and operating systems. A good few bugs have
been found only because they occur on only one particular
configuration of the above. There are also notorious test failures due
to (known and unknown) JVM bugs and crashes. Like this one on MacOSX,
recently (probably due to a more restrictive malloc implementation):

http://jenkins.thetaphi.de/job/Lucene-Solr-4.x-MacOSX/45/consoleText
(grep for ">>> JVM J0: stderr (verbatim)" near the end of the file).

Dawid

From heinz at javaspecialists.eu  Thu Jan 10 04:49:15 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 10 Jan 2013 11:49:15 +0200
Subject: [concurrency-interest] Concurrency Testing using Alternative
 Hardware
In-Reply-To: <CAM21Rt962EFDL0b-AnosGE2yqmaG9V4g-juK+=7aSm_TOn8cHA@mail.gmail.com>
References: <50EDE898.5060700@javaspecialists.eu> <50EDEBE2.7090604@oracle.com>
	<CAM21Rt962EFDL0b-AnosGE2yqmaG9V4g-juK+=7aSm_TOn8cHA@mail.gmail.com>
Message-ID: <50EE8E9B.1060105@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130110/5f280f33/attachment.html>

From dawid.weiss at gmail.com  Thu Jan 10 05:10:22 2013
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Thu, 10 Jan 2013 11:10:22 +0100
Subject: [concurrency-interest] Concurrency Testing using Alternative
	Hardware
In-Reply-To: <50EE8E9B.1060105@javaspecialists.eu>
References: <50EDE898.5060700@javaspecialists.eu> <50EDEBE2.7090604@oracle.com>
	<CAM21Rt962EFDL0b-AnosGE2yqmaG9V4g-juK+=7aSm_TOn8cHA@mail.gmail.com>
	<50EE8E9B.1060105@javaspecialists.eu>
Message-ID: <CAM21Rt9UcLUnov7nubCh2RLh+-kBw8sNJSM_vjLC4cn9K7GR3g@mail.gmail.com>

> Because there might be bugs that are masked by the x86 hardware.

I was ironic here (the "customer's" point of view -- why bother if it
"works" on the only target platform).

> using different hardware might show even more errors.

Lucene tests do use various hardware configurations (wildly different
machines, processors, etc.). I also know at least one person runs
tests on Azul's JVM but on x86-compatible hardware. I think some folks
also run them on Sparcs but I'm not sure of that.

In any case, I fully agree with you and I'm also interested in
whatever experiences people have.

Dawid

From aleksey.shipilev at oracle.com  Thu Jan 10 17:09:20 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 11 Jan 2013 02:09:20 +0400
Subject: [concurrency-interest] Concurrency Testing using Alternative
 Hardware
In-Reply-To: <50EDEBE2.7090604@oracle.com>
References: <50EDE898.5060700@javaspecialists.eu> <50EDEBE2.7090604@oracle.com>
Message-ID: <50EF3C10.3040200@oracle.com>

On 01/10/2013 02:14 AM, Aleksey Shipilev wrote:
> It would be interesting to have Azul box to be tested with j-c-t,
> although the runners are missing the support for these massively
> parallel monsters yet: you will utilize only the small amount of cores,
> which would make the test less interesting. Supporting multiple groups
> of actor threads in j-c-t is on my priority list.

Done [1]. Everything is automatic, just pull out the new version, and
you are good to go. Saturates my handy 2x8x2 lab server without a hassle.

-Aleksey.

[1]
https://github.com/shipilev/java-concurrency-torture/commit/e2ad819e7089092e37530a2438c83762b763f60a

From blackdrag at gmx.org  Mon Jan 14 18:35:22 2013
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Tue, 15 Jan 2013 00:35:22 +0100
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
Message-ID: <50F4963A.7010603@gmx.org>

hi all,

I am currently wondering about using a persistent map in my program, but 
I am unsure if I would really have an advantage of it. Since it is a 
framework more or less, I cannot simply test this. I cannot foresee all 
kinds of usages. I am looking at Java7 and partially Java8 for this.

ConcurrentHashMap does a very big job imho to avoid synchronization if 
it is not needed. HashPMap (from pcollections on google code) on the 
other hand does not synchronize at all. I don't need a put be visible 
early in another thread, but multiple threads will eventually have to 
synchronize somehow. So I see a possible bottleneck in put operations 
since ConcurrentHashMap goes to some lengths to allow concurrent puts. 
On the other hand I have most likely more reads than writes and I know 
ConcurrentHashMap (at least in the Java7 versions) goes with at least 
one volatile read, that is avoided in the other map. So I would assume 
an adavantage here for the pcollections map.

Am I right with my assumptions? Did I forget something very important?

bye Jochen

-- 
Jochen "blackdrag" Theodorou - Groovy Project Tech Lead
blog: http://blackdragsview.blogspot.com/
german groovy discussion newsgroup: de.comp.lang.misc
For Groovy programming sources visit http://groovy-lang.org


From nathan.reynolds at oracle.com  Mon Jan 14 19:36:06 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 14 Jan 2013 17:36:06 -0700
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
 HashPMap
In-Reply-To: <50F4963A.7010603@gmx.org>
References: <50F4963A.7010603@gmx.org>
Message-ID: <50F4A476.7090804@oracle.com>

ConcurrentHashMap as found in JDK 7 does have a lock which is striped.  
Each part of the hash table has its own lock.  If I understand 
correctly, ConcurrentHashMap in JDK 8 will not have any locks at all and 
simply use atomic instructions.

HashMap doesn't have any synchronization.  If you are looking for low 
CPU overhead, then this is probably the best hash table you can use.

HashPMap doesn't have any synchronization either.  I am not sure what 
features HashPMap provides above and beyond HashMap.  I have heard that 
some of the methods in HashMap have hand-optimized assembly code.

ConcurrentHashMap is built for high concurrency at the cost of a higher 
CPU overhead.  With HashMap and HashPMap, you have to use a lock which 
means only 1 thread can access the hash table at a time.

I would start by using a HashMap or HashPMap with a synchronized block.  
If the lock becomes contended enough, then switch over to ConcurrentHashMap.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 1/14/2013 4:35 PM, Jochen Theodorou wrote:
> hi all,
>
> I am currently wondering about using a persistent map in my program, 
> but I am unsure if I would really have an advantage of it. Since it is 
> a framework more or less, I cannot simply test this. I cannot foresee 
> all kinds of usages. I am looking at Java7 and partially Java8 for this.
>
> ConcurrentHashMap does a very big job imho to avoid synchronization if 
> it is not needed. HashPMap (from pcollections on google code) on the 
> other hand does not synchronize at all. I don't need a put be visible 
> early in another thread, but multiple threads will eventually have to 
> synchronize somehow. So I see a possible bottleneck in put operations 
> since ConcurrentHashMap goes to some lengths to allow concurrent puts. 
> On the other hand I have most likely more reads than writes and I know 
> ConcurrentHashMap (at least in the Java7 versions) goes with at least 
> one volatile read, that is avoided in the other map. So I would assume 
> an adavantage here for the pcollections map.
>
> Am I right with my assumptions? Did I forget something very important?
>
> bye Jochen
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130114/6b6d32d7/attachment.html>

From blackdrag at gmx.org  Mon Jan 14 20:38:17 2013
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Tue, 15 Jan 2013 02:38:17 +0100
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
 HashPMap
In-Reply-To: <50F4A476.7090804@oracle.com>
References: <50F4963A.7010603@gmx.org> <50F4A476.7090804@oracle.com>
Message-ID: <50F4B309.7050407@gmx.org>

Am 15.01.2013 01:36, schrieb Nathan Reynolds:
> ConcurrentHashMap as found in JDK 7 does have a lock which is striped.
> Each part of the hash table has its own lock.  If I understand
> correctly, ConcurrentHashMap in JDK 8 will not have any locks at all and
> simply use atomic instructions.
>
> HashMap doesn't have any synchronization.  If you are looking for low
> CPU overhead, then this is probably the best hash table you can use.

The map is shared among different threads. With HashMap imho not being 
threadsafe, this is no option.

> HashPMap doesn't have any synchronization either.  I am not sure what
> features HashPMap provides above and beyond HashMap.  I have heard that
> some of the methods in HashMap have hand-optimized assembly code.

it is a persistent collection, meaning it is effectively immutable and 
threadsafe, all fields are final for example. And as long as you use 
entries that are effectively immutable and threadsafe too you won't run 
into the problem of partially updated instances and other concurrency 
effects.

> ConcurrentHashMap is built for high concurrency at the cost of a higher
> CPU overhead.  With HashMap and HashPMap, you have to use a lock which
> means only 1 thread can access the hash table at a time.

a lock to let only 1 thread access the hash table at a time is surely 
true for HashMap, but not so for HashPMap. Concurrent reads are no 
problem - as is for HashMap, which leaves writes. for HashMap I have to 
synchronize read and write, once a write can occur concurrently with a 
read, since the structures internally are changed and not threadsafe. 
HashPMap is not mutating, it creates a new instance. If I am ok with 
seeing an older instance (maybe forever), then I won't need to 
synchronize a single write and multiple reads. Only if I want to have 
multiple concurrent writes with the actual map, I will need to 
synchronize the writes. During the synchronized write any number of 
Threads can still read (as long as they don't need to see the most 
actual version only of course) without meeting any memory barrier. That 
means the read is always without any barrier. Imho in ConcurrentHashMap 
many writes concurrently with many reads will cause a lot of memory 
synchronization, slowing down performance, while the HashPMap can do 
this without slowing down the reads, at the expense of allowing no 
parallel writes in my scenario.

bye Jochen

-- 
Jochen "blackdrag" Theodorou - Groovy Project Tech Lead
blog: http://blackdragsview.blogspot.com/
german groovy discussion newsgroup: de.comp.lang.misc
For Groovy programming sources visit http://groovy-lang.org


From szegedia at gmail.com  Tue Jan 15 04:19:36 2013
From: szegedia at gmail.com (Attila Szegedi)
Date: Tue, 15 Jan 2013 10:19:36 +0100
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
In-Reply-To: <50F4963A.7010603@gmx.org>
References: <50F4963A.7010603@gmx.org>
Message-ID: <06D09DC9-2B11-4774-8BB3-3B7C35628E8A@gmail.com>

You should also be mindful of the increased memory footprint of the ConcurrentHashMap (at least the JDK7 striped-locked one). With 16 stripes by default, it weighs at about 2.5KBytes/instance. If you expect to have many of 'em (I dealt with Java services in the past that had millions), it can be an issue, and you might want to manually specify a lower number of stripes on it.

Somewhat counterintuitively, even a ConcurrentHashMap with a concurrency level of 1 can make sense in certain use cases; it's approximately as bad as a Collections.synchronizedMap(new HashMap()) performancewise (probably even a bit worse), but its iterators won't fail with ConcurrentModificationException - if that's a property your system would find useful, you might consider it.

On Jan 15, 2013, at 12:35 AM, Jochen Theodorou <blackdrag at gmx.org> wrote:

> hi all,
> 
> I am currently wondering about using a persistent map in my program, but I am unsure if I would really have an advantage of it. Since it is a framework more or less, I cannot simply test this. I cannot foresee all kinds of usages. I am looking at Java7 and partially Java8 for this.
> 
> ConcurrentHashMap does a very big job imho to avoid synchronization if it is not needed. HashPMap (from pcollections on google code) on the other hand does not synchronize at all. I don't need a put be visible early in another thread, but multiple threads will eventually have to synchronize somehow. So I see a possible bottleneck in put operations since ConcurrentHashMap goes to some lengths to allow concurrent puts. On the other hand I have most likely more reads than writes and I know ConcurrentHashMap (at least in the Java7 versions) goes with at least one volatile read, that is avoided in the other map. So I would assume an adavantage here for the pcollections map.
> 
> Am I right with my assumptions? Did I forget something very important?
> 
> bye Jochen
> 
> -- 
> Jochen "blackdrag" Theodorou - Groovy Project Tech Lead
> blog: http://blackdragsview.blogspot.com/
> german groovy discussion newsgroup: de.comp.lang.misc
> For Groovy programming sources visit http://groovy-lang.org
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From stanimir at riflexo.com  Tue Jan 15 04:50:01 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 15 Jan 2013 11:50:01 +0200
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
In-Reply-To: <06D09DC9-2B11-4774-8BB3-3B7C35628E8A@gmail.com>
References: <50F4963A.7010603@gmx.org>
	<06D09DC9-2B11-4774-8BB3-3B7C35628E8A@gmail.com>
Message-ID: <CAEJX8ormKMm_SpMq7Bap7C6v9echUzw-TbxkfDLunq0yhC5UGQ@mail.gmail.com>

On Tue, Jan 15, 2013 at 11:19 AM, Attila Szegedi <szegedia at gmail.com> wrote:

> You should also be mindful of the increased memory footprint of the
> ConcurrentHashMap (at least the JDK7 striped-locked one). With 16 stripes
> by default, it weighs at about 2.5KBytes/instance. If you expect to have
> many of 'em (I dealt with Java services in the past that had millions), it
> can be an issue, and you might want to manually specify a lower number of
> stripes on it.
>

Somewhat counterintuitively, even a ConcurrentHashMap with a concurrency
> level of 1 can make sense in certain use cases; it's approximately as bad
> as a Collections.synchronizedMap(new HashMap()) performancewise (probably
> even a bit worse), but its iterators won't fail with
> ConcurrentModificationException - if that's a property your system would
> find useful, you might consider it.
>
>
It has lock-free reading, which is quite important. Morealso HashMap uses
"volatile int modCount" which translate to volatile write (and read) on
each operation.

Stanimir


> On Jan 15, 2013, at 12:35 AM, Jochen Theodorou <blackdrag at gmx.org> wrote:
>
> > hi all,
> >
> > I am currently wondering about using a persistent map in my program, but
> I am unsure if I would really have an advantage of it. Since it is a
> framework more or less, I cannot simply test this. I cannot foresee all
> kinds of usages. I am looking at Java7 and partially Java8 for this.
> >
> > ConcurrentHashMap does a very big job imho to avoid synchronization if
> it is not needed. HashPMap (from pcollections on google code) on the other
> hand does not synchronize at all. I don't need a put be visible early in
> another thread, but multiple threads will eventually have to synchronize
> somehow. So I see a possible bottleneck in put operations since
> ConcurrentHashMap goes to some lengths to allow concurrent puts. On the
> other hand I have most likely more reads than writes and I know
> ConcurrentHashMap (at least in the Java7 versions) goes with at least one
> volatile read, that is avoided in the other map. So I would assume an
> adavantage here for the pcollections map.
> >
> > Am I right with my assumptions? Did I forget something very important?
> >
> > bye Jochen
> >
> > --
> > Jochen "blackdrag" Theodorou - Groovy Project Tech Lead
> > blog: http://blackdragsview.blogspot.com/
> > german groovy discussion newsgroup: de.comp.lang.misc
> > For Groovy programming sources visit http://groovy-lang.org
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130115/f068c3ce/attachment-0001.html>

From blackdrag at gmx.org  Tue Jan 15 05:06:21 2013
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Tue, 15 Jan 2013 11:06:21 +0100
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
 HashPMap
In-Reply-To: <CAEJX8ormKMm_SpMq7Bap7C6v9echUzw-TbxkfDLunq0yhC5UGQ@mail.gmail.com>
References: <50F4963A.7010603@gmx.org>
	<06D09DC9-2B11-4774-8BB3-3B7C35628E8A@gmail.com>
	<CAEJX8ormKMm_SpMq7Bap7C6v9echUzw-TbxkfDLunq0yhC5UGQ@mail.gmail.com>
Message-ID: <50F52A1D.8000402@gmx.org>

Am 15.01.2013 10:50, schrieb Stanimir Simeonoff:
[...]
> It has lock-free reading, which is quite important. Morealso HashMap
> uses "volatile int modCount" which translate to volatile write (and
> read) on each operation.

but sin't the segment table also a volatile? doesn't that mean each read 
is a volatile read?

bye Jochen


-- 
Jochen "blackdrag" Theodorou - Groovy Project Tech Lead
blog: http://blackdragsview.blogspot.com/
german groovy discussion newsgroup: de.comp.lang.misc
For Groovy programming sources visit http://groovy-lang.org


From mlists at juma.me.uk  Tue Jan 15 05:12:07 2013
From: mlists at juma.me.uk (Ismael Juma)
Date: Tue, 15 Jan 2013 10:12:07 +0000
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
In-Reply-To: <CAEJX8ormKMm_SpMq7Bap7C6v9echUzw-TbxkfDLunq0yhC5UGQ@mail.gmail.com>
References: <50F4963A.7010603@gmx.org>
	<06D09DC9-2B11-4774-8BB3-3B7C35628E8A@gmail.com>
	<CAEJX8ormKMm_SpMq7Bap7C6v9echUzw-TbxkfDLunq0yhC5UGQ@mail.gmail.com>
Message-ID: <CAD5tkZZ5sHP1g9uEGo+oHyVF3W36k_Vjoc=qRjYBBKKCvyDe4w@mail.gmail.com>

On Tue, Jan 15, 2013 at 9:50 AM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:

> Morealso HashMap uses "volatile int modCount" which translate to volatile
> write (and read) on each operation.
>

This hasn't been the case for a while now. The definition in latest build
of JDK 7 update 12:

transient int modCount;

Best,
Ismael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130115/b16b719b/attachment.html>

From stanimir at riflexo.com  Tue Jan 15 05:14:47 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 15 Jan 2013 12:14:47 +0200
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
In-Reply-To: <50F52A1D.8000402@gmx.org>
References: <50F4963A.7010603@gmx.org>
	<06D09DC9-2B11-4774-8BB3-3B7C35628E8A@gmail.com>
	<CAEJX8ormKMm_SpMq7Bap7C6v9echUzw-TbxkfDLunq0yhC5UGQ@mail.gmail.com>
	<50F52A1D.8000402@gmx.org>
Message-ID: <CAEJX8ooLRW0KRSdmALQ-RiLOO8UFFqaipWa8tGMXYpxQPHG6hw@mail.gmail.com>

CHM has no qualms at volatile reads - they must happen to ensure
correctness.
My previous post was unfinished - the modcount is read on iteration only
but written(+read) on each *modification* operation. Volatile reads are
often ignored as cost as they are sort of free on TSO architectures.

The idea was not comparing CHM to HashMap directly. CHM has more
indirections as well.

Stanimir

On Tue, Jan 15, 2013 at 12:06 PM, Jochen Theodorou <blackdrag at gmx.org>wrote:

> Am 15.01.2013 10:50, schrieb Stanimir Simeonoff:
> [...]
>
>  It has lock-free reading, which is quite important. Morealso HashMap
>> uses "volatile int modCount" which translate to volatile write (and
>> read) on each operation.
>>
>
> but sin't the segment table also a volatile? doesn't that mean each read
> is a volatile read?
>
>
> bye Jochen
>
>
> --
> Jochen "blackdrag" Theodorou - Groovy Project Tech Lead
> blog: http://blackdragsview.**blogspot.com/<http://blackdragsview.blogspot.com/>
> german groovy discussion newsgroup: de.comp.lang.misc
> For Groovy programming sources visit http://groovy-lang.org
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130115/5c162b7a/attachment.html>

From stanimir at riflexo.com  Tue Jan 15 05:19:48 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 15 Jan 2013 12:19:48 +0200
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
In-Reply-To: <CAD5tkZZ5sHP1g9uEGo+oHyVF3W36k_Vjoc=qRjYBBKKCvyDe4w@mail.gmail.com>
References: <50F4963A.7010603@gmx.org>
	<06D09DC9-2B11-4774-8BB3-3B7C35628E8A@gmail.com>
	<CAEJX8ormKMm_SpMq7Bap7C6v9echUzw-TbxkfDLunq0yhC5UGQ@mail.gmail.com>
	<CAD5tkZZ5sHP1g9uEGo+oHyVF3W36k_Vjoc=qRjYBBKKCvyDe4w@mail.gmail.com>
Message-ID: <CAEJX8opOEQ7fxS0945a53XLRJSKMrZyLpmchT6=LdtQOZ4oj2A@mail.gmail.com>

Yes in JDK7 "volatile" was removed. I think it was removed quite early even
alphas, always forget about it.

Stanimir

On Tue, Jan 15, 2013 at 12:12 PM, Ismael Juma <mlists at juma.me.uk> wrote:

> On Tue, Jan 15, 2013 at 9:50 AM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>
>> Morealso HashMap uses "volatile int modCount" which translate to volatile
>> write (and read) on each operation.
>>
>
> This hasn't been the case for a while now. The definition in latest build
> of JDK 7 update 12:
>
> transient int modCount;
>
> Best,
> Ismael
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130115/a5c9014f/attachment.html>

From aleksey.shipilev at oracle.com  Tue Jan 15 06:49:13 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 15 Jan 2013 15:49:13 +0400
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
 HashPMap
In-Reply-To: <50F4A476.7090804@oracle.com>
References: <50F4963A.7010603@gmx.org> <50F4A476.7090804@oracle.com>
Message-ID: <50F54239.4010307@oracle.com>

On 01/15/2013 04:36 AM, Nathan Reynolds wrote:
> If I understand correctly, ConcurrentHashMap in JDK 8 will not have
> any locks at all and simply use atomic instructions.

Nope, this is misguided. CHMv8 will have the per-bucket lock and
completely lock-less reads, see [1].

-Aleksey.

[1]
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/ConcurrentHashMapV8.java?view=markup

From aleksey.shipilev at oracle.com  Tue Jan 15 06:56:38 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 15 Jan 2013 15:56:38 +0400
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
 HashPMap
In-Reply-To: <50F4B309.7050407@gmx.org>
References: <50F4963A.7010603@gmx.org> <50F4A476.7090804@oracle.com>
	<50F4B309.7050407@gmx.org>
Message-ID: <50F543F6.40503@oracle.com>

On 01/15/2013 05:38 AM, Jochen Theodorou wrote:
> Imho in ConcurrentHashMap many writes concurrently with many reads
> will cause a lot of memory synchronization, slowing down performance,
> while the HashPMap can do this without slowing down the reads, at the
> expense of allowing no parallel writes in my scenario.

But you will pay for copy-on-write in that case, right? And while you
can think there is a benefit of "not locking", there are two important
considerations which may or may not relate to your case:
  a) COW can lead to hitting the allocation wall much sooner than
coherence wall; this is especially important for huge machines: we've
seen allocation wall being more troublesome having enough garbage
generators.
  b) Not familiar with pcollection internals, but it looks like
consistent reads for COW will require some sort of internal
synchronization to snapshot the consistent parent collection. Which
makes it on par the the same kind of memory consistency things CHM is doing.

Quantitative benchmarks would help to untangle this ;) That would make a
really fun student project (which I can offer some assistance for).

-Aleksey.

From kirk at kodewerk.com  Tue Jan 15 07:04:42 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 15 Jan 2013 04:04:42 -0800
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
In-Reply-To: <50F543F6.40503@oracle.com>
References: <50F4963A.7010603@gmx.org> <50F4A476.7090804@oracle.com>
	<50F4B309.7050407@gmx.org> <50F543F6.40503@oracle.com>
Message-ID: <FCBCDD2F-BAE2-4613-BB87-8FA3DB0D8068@kodewerk.com>

Hi, 

>  a) COW can lead to hitting the allocation wall much sooner than
> coherence wall; this is especially important for huge machines: we've
> seen allocation wall being more troublesome having enough garbage
> generators.

You don't need that large a machine to hit the allocation wall. I've seen that happening on machines with 8 cores.

Regards,
Kirk



From viktor.klang at gmail.com  Tue Jan 15 07:28:30 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 15 Jan 2013 13:28:30 +0100
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
In-Reply-To: <50F543F6.40503@oracle.com>
References: <50F4963A.7010603@gmx.org> <50F4A476.7090804@oracle.com>
	<50F4B309.7050407@gmx.org> <50F543F6.40503@oracle.com>
Message-ID: <CANPzfU-kJ9_OSAH02QC8gbEu_1yTXGMFqBuP8kER3iT3D29bhg@mail.gmail.com>

On Tue, Jan 15, 2013 at 12:56 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 01/15/2013 05:38 AM, Jochen Theodorou wrote:
> > Imho in ConcurrentHashMap many writes concurrently with many reads
> > will cause a lot of memory synchronization, slowing down performance,
> > while the HashPMap can do this without slowing down the reads, at the
> > expense of allowing no parallel writes in my scenario.
>
> But you will pay for copy-on-write in that case, right? And while you
> can think there is a benefit of "not locking", there are two important
> considerations which may or may not relate to your case:
>   a) COW can lead to hitting the allocation wall much sooner than
> coherence wall; this is especially important for huge machines: we've
> seen allocation wall being more troublesome having enough garbage
> generators.
>

Depends on how much of the old structure can be safely shared&reused.

Cheers,
?


>   b) Not familiar with pcollection internals, but it looks like
> consistent reads for COW will require some sort of internal
> synchronization to snapshot the consistent parent collection. Which
> makes it on par the the same kind of memory consistency things CHM is
> doing.
>
> Quantitative benchmarks would help to untangle this ;) That would make a
> really fun student project (which I can offer some assistance for).
>
> -Aleksey.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130115/9b9d6656/attachment-0001.html>

From blackdrag at gmx.org  Tue Jan 15 09:15:32 2013
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Tue, 15 Jan 2013 15:15:32 +0100
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
 HashPMap
In-Reply-To: <50F543F6.40503@oracle.com>
References: <50F4963A.7010603@gmx.org> <50F4A476.7090804@oracle.com>
	<50F4B309.7050407@gmx.org> <50F543F6.40503@oracle.com>
Message-ID: <50F56484.5050004@gmx.org>

Am 15.01.2013 12:56, schrieb Aleksey Shipilev:
> On 01/15/2013 05:38 AM, Jochen Theodorou wrote:
>> Imho in ConcurrentHashMap many writes concurrently with many reads
>> will cause a lot of memory synchronization, slowing down performance,
>> while the HashPMap can do this without slowing down the reads, at the
>> expense of allowing no parallel writes in my scenario.
>
> But you will pay for copy-on-write in that case, right?And while you
> can think there is a benefit of "not locking", there are two important
> considerations which may or may not relate to your case:
>    a) COW can lead to hitting the allocation wall much sooner than
> coherence wall; this is especially important for huge machines: we've
> seen allocation wall being more troublesome having enough garbage
> generators.

Afaik the map uses most of the objects of the old map, so garbage should 
be low, but of course there are objects created. How much more compared 
to ConcurrentHashMap that is, is difficult for me to say

>    b) Not familiar with pcollection internals, but it looks like
> consistent reads for COW will require some sort of internal
> synchronization to snapshot the consistent parent collection. Which
> makes it on par the the same kind of memory consistency things CHM is doing.

I don't need consistent reads. Not consistent in terms of a change not 
being visible to other threads right away.

> Quantitative benchmarks would help to untangle this ;) That would make a
> really fun student project (which I can offer some assistance for).

A serious benchmark for this kind of thing would be indeed nice.

bye Jochen



-- 
Jochen "blackdrag" Theodorou - Groovy Project Tech Lead
blog: http://blackdragsview.blogspot.com/
german groovy discussion newsgroup: de.comp.lang.misc
For Groovy programming sources visit http://groovy-lang.org


From martinrb at google.com  Tue Jan 15 12:40:26 2013
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 15 Jan 2013 09:40:26 -0800
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
In-Reply-To: <CAEJX8ormKMm_SpMq7Bap7C6v9echUzw-TbxkfDLunq0yhC5UGQ@mail.gmail.com>
References: <50F4963A.7010603@gmx.org>
	<06D09DC9-2B11-4774-8BB3-3B7C35628E8A@gmail.com>
	<CAEJX8ormKMm_SpMq7Bap7C6v9echUzw-TbxkfDLunq0yhC5UGQ@mail.gmail.com>
Message-ID: <CA+kOe0-=UKojvykD7aERGYbA-PaNtPX-1ve_ajaBRHfY-xh-HA@mail.gmail.com>

On Tue, Jan 15, 2013 at 1:50 AM, Stanimir Simeonoff
<stanimir at riflexo.com> wrote:
>
> It has lock-free reading, which is quite important. Morealso HashMap uses
> "volatile int modCount" which translate to volatile write (and read) on each
> operation.

That "volatile" got removed from HashMap in jdk7.

From mike at uark.edu  Tue Jan 15 13:28:34 2013
From: mike at uark.edu (Michael Akerman)
Date: Tue, 15 Jan 2013 18:28:34 +0000
Subject: [concurrency-interest] ConcurrentLinkedQueue.poll() behaves like
 peek() and is not removing the head
Message-ID: <8821AE6678928F4EA5A3D4CA8417165F3FB24545@ex-mbx1b.uark.edu>

I find it hard to believe, but it appears to me that ConcurrentLinkedQueue.poll() in RHEL's Java 1.6.0  is not removing from the head of the queue as advertised in the JavaDoc.  Instead it appears to be behaving like peek().



I have the following code:



private final ConcurrentLinkedQueue<Connection> inactive = new ConcurrentLinkedQueue<Connection>();



protected void close ()

{

while( true )

{

final Connection con = inactive.poll();

System.out.println( con );

if ( con == null ) break;

DbUtils.closeQuietly( con );

}

}



This was running in an endless loop until I added the highlighted line below:



private final ConcurrentLinkedQueue<Connection> inactive = new ConcurrentLinkedQueue<Connection>();



protected void close ()

{

while( true )

{

final Connection con = inactive.poll();

System.out.println( con );

if ( con == null ) break;

inactive.remove( con );

DbUtils.closeQuietly( con );

}

}



Obviously, the latter code is undesirable when called from multiple threads if close is not idempotent and the poll() is behaving like a peek().



Surely, I'm missing something obvious?



Michael Akerman

Systems Analyst

IT Services

University of Arkansas
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130115/dc9fa2c3/attachment.html>

From martinrb at google.com  Tue Jan 15 14:13:42 2013
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 15 Jan 2013 11:13:42 -0800
Subject: [concurrency-interest] ConcurrentLinkedQueue.poll() behaves
 like peek() and is not removing the head
In-Reply-To: <8821AE6678928F4EA5A3D4CA8417165F3FB24545@ex-mbx1b.uark.edu>
References: <8821AE6678928F4EA5A3D4CA8417165F3FB24545@ex-mbx1b.uark.edu>
Message-ID: <CA+kOe0_SDYA0=Jx7Qk=83_-EA+cSrepJDyHr=fLpFQ-p4jUUNQ@mail.gmail.com>

On Tue, Jan 15, 2013 at 10:28 AM, Michael Akerman <mike at uark.edu> wrote:
> I find it hard to believe

We all find that hard to believe as well.
java.util.concurrent has bugs, but bugs such as the one you report
will tend to be tested, reported quickly and fixed quickly.

Can you come up with a small reproducible test case?

You can also try compiling your own CLQ from latest sources and using
-Xbootclasspath/p:

Martin

, but it appears to me that
> ConcurrentLinkedQueue.poll() in RHEL's Java 1.6.0  is not removing from the
> head of the queue as advertised in the JavaDoc.  Instead it appears to be
> behaving like peek().
>
>
>
> I have the following code:
>
>
>
> private final ConcurrentLinkedQueue<Connection> inactive = new
> ConcurrentLinkedQueue<Connection>();
>
>
>
> protected void close ()
>
> {
>
> while( true )
>
> {
>
> final Connection con = inactive.poll();
>
> System.out.println( con );
>
> if ( con == null ) break;
>
> DbUtils.closeQuietly( con );
>
> }
>
> }
>
>
>
> This was running in an endless loop until I added the highlighted line
> below:
>
>
>
> private final ConcurrentLinkedQueue<Connection> inactive = new
> ConcurrentLinkedQueue<Connection>();
>
>
>
> protected void close ()
>
> {
>
> while( true )
>
> {
>
> final Connection con = inactive.poll();
>
> System.out.println( con );
>
> if ( con == null ) break;
>
> inactive.remove( con );
>
> DbUtils.closeQuietly( con );
>
> }
>
> }
>
>
>
> Obviously, the latter code is undesirable when called from multiple threads
> if close is not idempotent and the poll() is behaving like a peek().
>
>
>
> Surely, I'm missing something obvious?
>
>
>
> Michael Akerman
>
> Systems Analyst
>
> IT Services
>
> University of Arkansas
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From aleksey.shipilev at oracle.com  Tue Jan 15 14:46:21 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 15 Jan 2013 23:46:21 +0400
Subject: [concurrency-interest] ConcurrentLinkedQueue.poll() behaves
 like peek() and is not removing the head
In-Reply-To: <CA+kOe0_SDYA0=Jx7Qk=83_-EA+cSrepJDyHr=fLpFQ-p4jUUNQ@mail.gmail.com>
References: <8821AE6678928F4EA5A3D4CA8417165F3FB24545@ex-mbx1b.uark.edu>
	<CA+kOe0_SDYA0=Jx7Qk=83_-EA+cSrepJDyHr=fLpFQ-p4jUUNQ@mail.gmail.com>
Message-ID: <50F5B20D.7090509@oracle.com>

On 01/15/2013 11:13 PM, Martin Buchholz wrote:
> On Tue, Jan 15, 2013 at 10:28 AM, Michael Akerman <mike at uark.edu> wrote:
>> I find it hard to believe
> 
> We all find that hard to believe as well.

+1.
Nothing known in this area pops in my mind.

> java.util.concurrent has bugs, but bugs such as the one you report
> will tend to be tested, reported quickly and fixed quickly.

+1, but only if we have the minimal test case. Also, it would be nice to
hear what JDK you are running, and the Java options you are using.

> Can you come up with a small reproducible test case?  You can also
> try compiling your own CLQ from latest sources and using 
> -Xbootclasspath/p:

I think at this point jsr166 repo is a pre-jdk8-migration mess, so don't
be surprised if that does not work.

-Aleksey.


From martinrb at google.com  Tue Jan 15 15:26:38 2013
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 15 Jan 2013 12:26:38 -0800
Subject: [concurrency-interest] ConcurrentLinkedQueue.poll() behaves
 like peek() and is not removing the head
In-Reply-To: <50F5B20D.7090509@oracle.com>
References: <8821AE6678928F4EA5A3D4CA8417165F3FB24545@ex-mbx1b.uark.edu>
	<CA+kOe0_SDYA0=Jx7Qk=83_-EA+cSrepJDyHr=fLpFQ-p4jUUNQ@mail.gmail.com>
	<50F5B20D.7090509@oracle.com>
Message-ID: <CA+kOe097_D03AKaVtKnjoHmUFO7yWMcVAqMWcYynmfewOLTZ7Q@mail.gmail.com>

On Tue, Jan 15, 2013 at 11:46 AM, Aleksey Shipilev
> I think at this point jsr166 repo is a pre-jdk8-migration mess, so don't
> be surprised if that does not work.

Get your jsr166 for jdk7 without jdk8 gunk here:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jdk7/java/util/concurrent/

From robert at politext.info  Wed Jan 16 04:04:18 2013
From: robert at politext.info (Robert Varga)
Date: Wed, 16 Jan 2013 09:04:18 +0000
Subject: [concurrency-interest] How does ConcurrentHashMap compare to
	HashPMap
In-Reply-To: <50F56484.5050004@gmx.org>
References: <50F4963A.7010603@gmx.org> <50F4A476.7090804@oracle.com>
	<50F4B309.7050407@gmx.org> <50F543F6.40503@oracle.com>
	<50F56484.5050004@gmx.org>
Message-ID: <CAJPpd-DsxjUKrX=BbFmUK_P6=Bdvw2T44sAxV_2eiOVck5ceVw@mail.gmail.com>

On Tue, Jan 15, 2013 at 2:15 PM, Jochen Theodorou <blackdrag at gmx.org> wrote:

> Am 15.01.2013 12:56, schrieb Aleksey Shipilev:
>
>> On 01/15/2013 05:38 AM, Jochen Theodorou wrote:
>>
>>> Imho in ConcurrentHashMap many writes concurrently with many reads
>>> will cause a lot of memory synchronization, slowing down performance,
>>> while the HashPMap can do this without slowing down the reads, at the
>>> expense of allowing no parallel writes in my scenario.
>>>
>>
>> But you will pay for copy-on-write in that case, right?And while you
>>
>> can think there is a benefit of "not locking", there are two important
>> considerations which may or may not relate to your case:
>>    a) COW can lead to hitting the allocation wall much sooner than
>> coherence wall; this is especially important for huge machines: we've
>> seen allocation wall being more troublesome having enough garbage
>> generators.
>>
>
> Afaik the map uses most of the objects of the old map, so garbage should
> be low, but of course there are objects created. How much more compared to
> ConcurrentHashMap that is, is difficult for me to say
>
>
About *PMap and PCollections from Google Code in general:

The immutability of the instances which gives you complete thread-safety
comes with its own tradeoffs.

1. We did experience issues with pcollections generating too much garbage.
You have to re-allocate the complete object path leading from the parent
object (*PMap or whatever collection instance you alter) to the entry, and
you get a new parent object.

2. Because of this, there are no such things in this data structure as a
concurrent write, as each "mutation" actually creates a new data structure
which shares part of the content and structure of the input one. While this
is nice and purely functional, the result is going to be a distinct logical
data structure instance for each write. So if you want to get all writers
changes into the same data structure, you have to serialize all such
writers one-by-one so that each can have the return value of the preceeding
call writer's operation.

Therefore this is not really suitable for concurrent writes. It is useful
for thread-safely and consistently seeing multiple versions from another
single thread's writes. I.e. it could be good in single-writer
multiple-reader scenarios.

Unfortunately there were other problems in the version we checked (2.10 if
I correctly remember): it was written so much with nice code in mind, that
the coder did not seem to have thought on large collections: it exhibits
StackOverflows when the data structures you mutate require too many
traversal steps.
This is most pronounced at ConsPStack due to it being a simple chain and
not a tree structure, but I guess the same issue is there for all other
classes only you don't face it that frequently due to the logarithmic
traversal costs for tree structures (but you could still face it in
HashPMap with an all-out hash collision which puts all entries into the
"same" ConsPStack) .
Of course it is possible to implement a workaround for this issue by
changing to allocating space and iterating instead of recursion at some
point (e.g. after already being 100 recursions deep), but it was not there
in the version I checked.

Also, due to the increased traversal costs to find the entry, it would pay
off to add additional operations similar to what compute and recompute
methods in CHMv8 do, i.e. compute the new value from the old entry and a
parameter or two so that you do not need to traverse the tree once for get
and forward+backwards for put.

Also, I may be wrong, but as far as I remember, an optimized creation
method for a pre-populated map would allocate less than adding entries
one-by-one (at least due to the wrapping HashPMap not being reallocated
upon every entry... these temporary HashPMap instances do not escape the
thread but this may or may not be picked up by escape analysis).

Best regards,

Robert Varga
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130116/4a915a0a/attachment-0001.html>

From mikeb01 at gmail.com  Thu Jan 17 17:46:34 2013
From: mikeb01 at gmail.com (Michael Barker)
Date: Fri, 18 Jan 2013 11:46:34 +1300
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <50E67248.4060006@oracle.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CACuKZqH1sjxCoX1oAfhkW-qrX=4yd+K8aHZ9BuH9be6K7KOYTg@mail.gmail.com>
	<CALwNKeQ=9BvwsXahUozQGVG2rh-nYn+O_SF+KWKOdUT6mC0UwQ@mail.gmail.com>
	<50E5A5F5.9040307@oracle.com>
	<CALwNKeTHMNfu5=OAJrfeiRmMnSmrGmnoDoK8E4Lpagc-c8QmVw@mail.gmail.com>
	<50E67248.4060006@oracle.com>
Message-ID: <CALwNKeRt6NaEDrAhHq7sJP-0gPvJaNeaTGES6wVTmjLgrCKE1A@mail.gmail.com>

This is quite an interesting approach and one I should try out.  I
assume to make it work well we'd still need a JVM intrinsic otherwise
we're into JNI land (I know JNI is cheap now days, but it would still
be slower than some inline assembler).

> Originally, I had a standard mutex protecting some shared data.  This mutex
> became very contended.  I noticed that all of the operations on the
> protected data were read operations.  This is because a write operation only
> happens when an administrator changes the configuration.  So, the logical
> solution was to change the mutex out for a reader/writer lock.  This helped
> for a while until Intel produced the next processor.  The contention came
> back.

I think there is a subtle difference between this problem and that of
the OP (and similarly the problem that I was working on).  I'm
guessing that with the application of a R/W lock there is causal
relationship between the acquisition of the read lock and the
subsequent application code, therefore a cache miss associated with
the locking operation will be felt by the application code.  With the
simple performance/event counter case there is no local causal
relationship (or it is in the inverse direction), simply an ordering
relationship on writes.  My theory would be that this gives the
CPU/compiler the ability to re-order such that some (or hopefully all)
of the cache miss latency can be hidden.  I need to see if I can come
up with an effective test for this.  Certainly in our case the cost of
having the counters is negligible, there was no measurable difference
the code with them and without (after removing the volatile).

<snip>

> This made me a firm believer in Processor Local Storage.  With Thread Local
> Storage, you don't need any locking on the stored objects since most of the
> time they are private to the thread.  With Processor Local Storage, you need
> locking since multiple threads could be accessing the stored objects
> although extremely rarely will it be concurrently.  With the cache benefits
> Processor Local Storage provides, it makes atomic operations very cheap
> since the cache line can stay in L1 cache unless space is needed for other
> hotter cache lines.

How much data would you put into the ProcessLocal?  I can imagine
cases where too much can be pushed into the ProcessorLocal and the
threads (if migrating CPUs reasonably heavily) still end up spending
most of their time pushing each others data out of cache you could end
up with some problems.  Would you reserve it for shared/contented
variables?

With the level of cache migration that you were experiencing, did you
not run into caching problems with the application data too?

> I have a Conroe desktop processor
> (http://en.wikipedia.org/wiki/Conroe_%28microprocessor%29).  If I remember
> right, a CAS instruction takes a minimum 25 cycles on this processor.  This
> requires the cache line to be in L1 cache.  I've been told from sources
> which definitely know that CAS latency has greatly improved on the next
> generation of Intel processors.  They didn't give me any numbers.  :(  I
> wouldn't be surprised if CAS latency is down to around 9 cycles (3 cycles to
> load, 3 to compare and 3 to store).

My i7-3770 (ivy-bridge) seems cost around 9.5ns - 10ns so somewhere in
the region of 20-40 cycles for a CAS in a tight loop bound to a single
core.  Still an order of magnitude less than a cache miss.  The
question becomes how much of an impact does that cache miss have on
the application code.

From nathan.reynolds at oracle.com  Thu Jan 17 18:30:06 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 17 Jan 2013 16:30:06 -0700
Subject: [concurrency-interest] concurrent enum-keyed counter map
In-Reply-To: <CALwNKeRt6NaEDrAhHq7sJP-0gPvJaNeaTGES6wVTmjLgrCKE1A@mail.gmail.com>
References: <CA+R7V7-FqSVbCqrJ6n=a_cBT6CDDVMNdpDbB045J7mqbTuBk7w@mail.gmail.com>
	<CACuKZqH1sjxCoX1oAfhkW-qrX=4yd+K8aHZ9BuH9be6K7KOYTg@mail.gmail.com>
	<CALwNKeQ=9BvwsXahUozQGVG2rh-nYn+O_SF+KWKOdUT6mC0UwQ@mail.gmail.com>
	<50E5A5F5.9040307@oracle.com>
	<CALwNKeTHMNfu5=OAJrfeiRmMnSmrGmnoDoK8E4Lpagc-c8QmVw@mail.gmail.com>
	<50E67248.4060006@oracle.com>
	<CALwNKeRt6NaEDrAhHq7sJP-0gPvJaNeaTGES6wVTmjLgrCKE1A@mail.gmail.com>
Message-ID: <50F8897E.7010102@oracle.com>

 > This is quite an interesting approach and one I should try out. I 
assume to make it work well we'd still need a JVM intrinsic otherwise 
we're into JNI land (I know JNI is cheap now days, but it would still be 
slower than some inline assembler).

If the contention on the cache line is high enough, then JNI will look 
cheap in comparison and so would be a solution.  However, I am with 
you.  A JVM intrinsic is the way to go.  On x86, I guess the rdtscp 
instruction would only cost about 3 cycles.

 > How much data would you put into the ProcessLocal?  I can imagine 
cases where too much can be pushed into the ProcessorLocal and the 
threads (if migrating CPUs reasonably heavily) still end up spending 
most of their time pushing each others data out of cache you could end 
up with some problems.  Would you reserve it for shared/contented variables?

That's the great thing about ProcessorLocal.  The data is tied to a 
processor/core and not the thread.  If the thread migrates, it will use 
the data associated with that new processor/core.  So, data won't be 
yanked around following the thread and ruin cache performance.

Let's consider a 2-socket Sandy Bridge EP machine.  This machine has 32 
logical cores (16 physical cores).  An application server might have 500 
threads.  If we don't do any striping, then there will be 1 copy of the 
data.  If we stripe by logical core, then there will be 32 copies of the 
data.  If we stripe by thread, then there will be 500 copies of the 
data.  So, striping by threads is going to put a lot more pressure on 
cache capacity than logical core.  Of course, striping on logical cores 
is going to put more pressure on cache capacity than no striping.

The moral of the story is don't stripe until there is a definite 
performance bottleneck on the data on a worthwhile usage of the data 
(i.e. don't optimize prematurely).  Once you have a performance 
bottleneck on the data, the negative impact from the extra cache 
capacity pressure is outweighed by the gains in performance by getting 
rid of bottleneck on the data.

 > With the level of cache migration that you were experiencing, did you 
not run into caching problems with the application data too?

I am not sure.  I haven't ever measured it.  It has never popped up on 
any of the hardware counters or if it has I didn't recognize it.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 1/17/2013 3:46 PM, Michael Barker wrote:
> This is quite an interesting approach and one I should try out.  I
> assume to make it work well we'd still need a JVM intrinsic otherwise
> we're into JNI land (I know JNI is cheap now days, but it would still
> be slower than some inline assembler).
>
>> Originally, I had a standard mutex protecting some shared data.  This mutex
>> became very contended.  I noticed that all of the operations on the
>> protected data were read operations.  This is because a write operation only
>> happens when an administrator changes the configuration.  So, the logical
>> solution was to change the mutex out for a reader/writer lock.  This helped
>> for a while until Intel produced the next processor.  The contention came
>> back.
> I think there is a subtle difference between this problem and that of
> the OP (and similarly the problem that I was working on).  I'm
> guessing that with the application of a R/W lock there is causal
> relationship between the acquisition of the read lock and the
> subsequent application code, therefore a cache miss associated with
> the locking operation will be felt by the application code.  With the
> simple performance/event counter case there is no local causal
> relationship (or it is in the inverse direction), simply an ordering
> relationship on writes.  My theory would be that this gives the
> CPU/compiler the ability to re-order such that some (or hopefully all)
> of the cache miss latency can be hidden.  I need to see if I can come
> up with an effective test for this.  Certainly in our case the cost of
> having the counters is negligible, there was no measurable difference
> the code with them and without (after removing the volatile).
>
> <snip>
>
>> This made me a firm believer in Processor Local Storage.  With Thread Local
>> Storage, you don't need any locking on the stored objects since most of the
>> time they are private to the thread.  With Processor Local Storage, you need
>> locking since multiple threads could be accessing the stored objects
>> although extremely rarely will it be concurrently.  With the cache benefits
>> Processor Local Storage provides, it makes atomic operations very cheap
>> since the cache line can stay in L1 cache unless space is needed for other
>> hotter cache lines.
> How much data would you put into the ProcessLocal?  I can imagine
> cases where too much can be pushed into the ProcessorLocal and the
> threads (if migrating CPUs reasonably heavily) still end up spending
> most of their time pushing each others data out of cache you could end
> up with some problems.  Would you reserve it for shared/contented
> variables?
>
> With the level of cache migration that you were experiencing, did you
> not run into caching problems with the application data too?
>
>> I have a Conroe desktop processor
>> (http://en.wikipedia.org/wiki/Conroe_%28microprocessor%29).  If I remember
>> right, a CAS instruction takes a minimum 25 cycles on this processor.  This
>> requires the cache line to be in L1 cache.  I've been told from sources
>> which definitely know that CAS latency has greatly improved on the next
>> generation of Intel processors.  They didn't give me any numbers.  :(  I
>> wouldn't be surprised if CAS latency is down to around 9 cycles (3 cycles to
>> load, 3 to compare and 3 to store).
> My i7-3770 (ivy-bridge) seems cost around 9.5ns - 10ns so somewhere in
> the region of 20-40 cycles for a CAS in a tight loop bound to a single
> core.  Still an order of magnitude less than a cache miss.  The
> question becomes how much of an impact does that cache miss have on
> the application code.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130117/b3f46eaa/attachment.html>

From cos at illinois.edu  Sun Jan 20 13:31:35 2013
From: cos at illinois.edu (Cosmin Radoi)
Date: Sun, 20 Jan 2013 12:31:35 -0600
Subject: [concurrency-interest] Is DateFormat.getDateInstance(...) factory
	method thread-safe?
Message-ID: <1EB67315-868A-4E0C-8E93-4E87D128E683@illinois.edu>


Maybe this has been touched upon before but I couldn't find it in history.

The javadoc for DateFormat warns it is not thread safe and advises the creation of separate instances for each thread - this matter has been discussed thoroughly. 

> Date formats are not synchronized. It is recommended to create separate format instances for each thread. If multiple threads access a format concurrently, it must be synchronized externally.

Still, are the DateFormat static factory methods thread-safe? They all end up in this private static method:

private static DateFormat get(int timeStyle, int dateStyle, int flags, Locale loc)

The method returns fresh SimpleDateFormat objects which seem to have a thread-safe creation. Still, it also allows a DateFormatProvider (obtained from a LocalServiceProviderPool) to give "an implementation that's closer to the requested locale than what the Java runtime itself can provide":

> // Check whether a provider can provide an implementation that's closer
> // to the requested locale than what the Java runtime itself can provide.
> LocaleServiceProviderPool pool =
> 	LocaleServiceProviderPool.getPool(DateFormatProvider.class);
> 	if (pool.hasProviders()) {
>                DateFormat providersInstance = pool.getLocalizedObject(
>                                                    DateFormatGetter.INSTANCE,
>                                                    loc,
>                                                    timeStyle,
>                                                    dateStyle,
>                                                    flags);
>                if (providersInstance != null) {
>                    return providersInstance;
>                }
>            }


The getPool method is the following, with the poolOfPools being a ConcurrentHashMap:

>  104       /**
>  105        * A factory method that returns a singleton instance
>  106        */
>  107       public static LocaleServiceProviderPool getPool(Class<? extends LocaleServiceProvider> providerClass) {
>  108           LocaleServiceProviderPool pool = poolOfPools.get(providerClass);
>  109           if (pool == null) {
>  110               LocaleServiceProviderPool newPool =
>  111                   new LocaleServiceProviderPool(providerClass);
>  112               pool = poolOfPools.put(providerClass, newPool);
>  113               if (pool == null) {
>  114                   pool = newPool;
>  115               }
>  116           }
>  117   
>  118           return pool;
>  119       }
(Java 7, Build b147)

One issue would be that the above code looks buggy. The intent seems to be a putIfAbsent semantics but it actually allows the value object to be replaced if at least two threads race on initialization. The two racing threads would get the same value, the one created by the first thread reaching poolOfPools.put, but any subsequent threads would get the value created by the second thread. 

The second issue is whether LocalServiceProviderPools are guaranteed to return fresh DateFormat instances. The documentation doesn't mention such a constraint.

Sorry for opening the date can of worms and please let me know if this is not the appropriate venue for this discussion.

Thank you,

Cosmin Radoi



From henri.tremblay at gmail.com  Sun Jan 20 14:22:20 2013
From: henri.tremblay at gmail.com (Henri Tremblay)
Date: Sun, 20 Jan 2013 20:22:20 +0100
Subject: [concurrency-interest] Is DateFormat.getDateInstance(...)
 factory method thread-safe?
In-Reply-To: <1EB67315-868A-4E0C-8E93-4E87D128E683@illinois.edu>
References: <1EB67315-868A-4E0C-8E93-4E87D128E683@illinois.edu>
Message-ID: <CADZL2=u+HiaWh5bDaJ=rwkaAu_=uPZwQ8gafGiZ8NY0TNhzdtw@mail.gmail.com>

I did a quick reading.

The DateFormat seems to come from a DateFormatProvider which specifies that
a new instance will be returned. So I would say "yes, it's always a new
instance". Then, my 1.7.0_11-b21 version does indeed use putIfAbsent.

    Object localObject =
(LocaleServiceProviderPool)poolOfPools.get(paramClass);
    if (localObject == null) {
      LocaleServiceProviderPool localLocaleServiceProviderPool = new
LocaleServiceProviderPool(paramClass);

      localObject =
(LocaleServiceProviderPool)poolOfPools.putIfAbsent(paramClass,
localLocaleServiceProviderPool);
      if (localObject == null) {
        localObject = localLocaleServiceProviderPool;
      }
    }

Then, I don't think it really matters. At worst, two idempotent objects
will be put one after the other. It doesn't really matter that two
instances were created. They will do the exact same thing anyway.

-
Henri




On 20 January 2013 19:31, Cosmin Radoi <cos at illinois.edu> wrote:

>
> Maybe this has been touched upon before but I couldn't find it in history.
>
> The javadoc for DateFormat warns it is not thread safe and advises the
> creation of separate instances for each thread - this matter has been
> discussed thoroughly.
>
> > Date formats are not synchronized. It is recommended to create separate
> format instances for each thread. If multiple threads access a format
> concurrently, it must be synchronized externally.
>
> Still, are the DateFormat static factory methods thread-safe? They all end
> up in this private static method:
>
> private static DateFormat get(int timeStyle, int dateStyle, int flags,
> Locale loc)
>
> The method returns fresh SimpleDateFormat objects which seem to have a
> thread-safe creation. Still, it also allows a DateFormatProvider (obtained
> from a LocalServiceProviderPool) to give "an implementation that's closer
> to the requested locale than what the Java runtime itself can provide":
>
> > // Check whether a provider can provide an implementation that's closer
> > // to the requested locale than what the Java runtime itself can provide.
> > LocaleServiceProviderPool pool =
> >       LocaleServiceProviderPool.getPool(DateFormatProvider.class);
> >       if (pool.hasProviders()) {
> >                DateFormat providersInstance = pool.getLocalizedObject(
> >
>  DateFormatGetter.INSTANCE,
> >                                                    loc,
> >                                                    timeStyle,
> >                                                    dateStyle,
> >                                                    flags);
> >                if (providersInstance != null) {
> >                    return providersInstance;
> >                }
> >            }
>
>
> The getPool method is the following, with the poolOfPools being a
> ConcurrentHashMap:
>
> >  104       /**
> >  105        * A factory method that returns a singleton instance
> >  106        */
> >  107       public static LocaleServiceProviderPool getPool(Class<?
> extends LocaleServiceProvider> providerClass) {
> >  108           LocaleServiceProviderPool pool =
> poolOfPools.get(providerClass);
> >  109           if (pool == null) {
> >  110               LocaleServiceProviderPool newPool =
> >  111                   new LocaleServiceProviderPool(providerClass);
> >  112               pool = poolOfPools.put(providerClass, newPool);
> >  113               if (pool == null) {
> >  114                   pool = newPool;
> >  115               }
> >  116           }
> >  117
> >  118           return pool;
> >  119       }
> (Java 7, Build b147)
>
> One issue would be that the above code looks buggy. The intent seems to be
> a putIfAbsent semantics but it actually allows the value object to be
> replaced if at least two threads race on initialization. The two racing
> threads would get the same value, the one created by the first thread
> reaching poolOfPools.put, but any subsequent threads would get the value
> created by the second thread.
>
> The second issue is whether LocalServiceProviderPools are guaranteed to
> return fresh DateFormat instances. The documentation doesn't mention such a
> constraint.
>
> Sorry for opening the date can of worms and please let me know if this is
> not the appropriate venue for this discussion.
>
> Thank you,
>
> Cosmin Radoi
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130120/29da0e24/attachment.html>

From vitalyd at gmail.com  Sun Jan 20 15:12:46 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sun, 20 Jan 2013 15:12:46 -0500
Subject: [concurrency-interest] Is DateFormat.getDateInstance(...)
 factory method thread-safe?
In-Reply-To: <CADZL2=u+HiaWh5bDaJ=rwkaAu_=uPZwQ8gafGiZ8NY0TNhzdtw@mail.gmail.com>
References: <1EB67315-868A-4E0C-8E93-4E87D128E683@illinois.edu>
	<CADZL2=u+HiaWh5bDaJ=rwkaAu_=uPZwQ8gafGiZ8NY0TNhzdtw@mail.gmail.com>
Message-ID: <CAHjP37FheLMYoN=0TVA-w_7+B14X3ZAVJPLcdp6WMVw20NzoYQ@mail.gmail.com>

Yeah, it doesn't matter here.  Although whenever you see a put() followed
by checking if return value is not null in CHM, it's a sign that
putIfAbsent() was meant (this is probably what Cosmin meant); I believe
findbugs will spot this pattern even.  It's fixed though, so all good.

Sent from my phone
On Jan 20, 2013 2:26 PM, "Henri Tremblay" <henri.tremblay at gmail.com> wrote:

> I did a quick reading.
>
> The DateFormat seems to come from a DateFormatProvider which specifies
> that a new instance will be returned. So I would say "yes, it's always a
> new instance". Then, my 1.7.0_11-b21 version does indeed use putIfAbsent.
>
>     Object localObject =
> (LocaleServiceProviderPool)poolOfPools.get(paramClass);
>     if (localObject == null) {
>       LocaleServiceProviderPool localLocaleServiceProviderPool = new
> LocaleServiceProviderPool(paramClass);
>
>       localObject =
> (LocaleServiceProviderPool)poolOfPools.putIfAbsent(paramClass,
> localLocaleServiceProviderPool);
>       if (localObject == null) {
>         localObject = localLocaleServiceProviderPool;
>       }
>     }
>
> Then, I don't think it really matters. At worst, two idempotent objects
> will be put one after the other. It doesn't really matter that two
> instances were created. They will do the exact same thing anyway.
>
> -
> Henri
>
>
>
>
> On 20 January 2013 19:31, Cosmin Radoi <cos at illinois.edu> wrote:
>
>>
>> Maybe this has been touched upon before but I couldn't find it in history.
>>
>> The javadoc for DateFormat warns it is not thread safe and advises the
>> creation of separate instances for each thread - this matter has been
>> discussed thoroughly.
>>
>> > Date formats are not synchronized. It is recommended to create separate
>> format instances for each thread. If multiple threads access a format
>> concurrently, it must be synchronized externally.
>>
>> Still, are the DateFormat static factory methods thread-safe? They all
>> end up in this private static method:
>>
>> private static DateFormat get(int timeStyle, int dateStyle, int flags,
>> Locale loc)
>>
>> The method returns fresh SimpleDateFormat objects which seem to have a
>> thread-safe creation. Still, it also allows a DateFormatProvider (obtained
>> from a LocalServiceProviderPool) to give "an implementation that's closer
>> to the requested locale than what the Java runtime itself can provide":
>>
>> > // Check whether a provider can provide an implementation that's closer
>> > // to the requested locale than what the Java runtime itself can
>> provide.
>> > LocaleServiceProviderPool pool =
>> >       LocaleServiceProviderPool.getPool(DateFormatProvider.class);
>> >       if (pool.hasProviders()) {
>> >                DateFormat providersInstance = pool.getLocalizedObject(
>> >
>>  DateFormatGetter.INSTANCE,
>> >                                                    loc,
>> >                                                    timeStyle,
>> >                                                    dateStyle,
>> >                                                    flags);
>> >                if (providersInstance != null) {
>> >                    return providersInstance;
>> >                }
>> >            }
>>
>>
>> The getPool method is the following, with the poolOfPools being a
>> ConcurrentHashMap:
>>
>> >  104       /**
>> >  105        * A factory method that returns a singleton instance
>> >  106        */
>> >  107       public static LocaleServiceProviderPool getPool(Class<?
>> extends LocaleServiceProvider> providerClass) {
>> >  108           LocaleServiceProviderPool pool =
>> poolOfPools.get(providerClass);
>> >  109           if (pool == null) {
>> >  110               LocaleServiceProviderPool newPool =
>> >  111                   new LocaleServiceProviderPool(providerClass);
>> >  112               pool = poolOfPools.put(providerClass, newPool);
>> >  113               if (pool == null) {
>> >  114                   pool = newPool;
>> >  115               }
>> >  116           }
>> >  117
>> >  118           return pool;
>> >  119       }
>> (Java 7, Build b147)
>>
>> One issue would be that the above code looks buggy. The intent seems to
>> be a putIfAbsent semantics but it actually allows the value object to be
>> replaced if at least two threads race on initialization. The two racing
>> threads would get the same value, the one created by the first thread
>> reaching poolOfPools.put, but any subsequent threads would get the value
>> created by the second thread.
>>
>> The second issue is whether LocalServiceProviderPools are guaranteed to
>> return fresh DateFormat instances. The documentation doesn't mention such a
>> constraint.
>>
>> Sorry for opening the date can of worms and please let me know if this is
>> not the appropriate venue for this discussion.
>>
>> Thank you,
>>
>> Cosmin Radoi
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130120/86432427/attachment-0001.html>

From zhong.j.yu at gmail.com  Sun Jan 20 23:05:58 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sun, 20 Jan 2013 22:05:58 -0600
Subject: [concurrency-interest] Is DateFormat.getDateInstance(...)
 factory method thread-safe?
In-Reply-To: <CAHjP37FheLMYoN=0TVA-w_7+B14X3ZAVJPLcdp6WMVw20NzoYQ@mail.gmail.com>
References: <1EB67315-868A-4E0C-8E93-4E87D128E683@illinois.edu>
	<CADZL2=u+HiaWh5bDaJ=rwkaAu_=uPZwQ8gafGiZ8NY0TNhzdtw@mail.gmail.com>
	<CAHjP37FheLMYoN=0TVA-w_7+B14X3ZAVJPLcdp6WMVw20NzoYQ@mail.gmail.com>
Message-ID: <CACuKZqEM7wetEBigXCJ7XcJGdHbckdmRerH68Mv07NM2uC=8ew@mail.gmail.com>

On Sun, Jan 20, 2013 at 2:12 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> Yeah, it doesn't matter here.  Although whenever you see a put() followed by
> checking if return value is not null in CHM, it's a sign that putIfAbsent()
> was meant (this is probably what Cosmin meant); I believe findbugs will spot
> this pattern even.  It's fixed though, so all good.

And then most putIfAbsent() should be replaced by computeIfAbsent()...

About DateFormat - it's ok that it's not thread safe, even though that
is a little odd; the more serious problem is that new
SimpleDateFormat()  is slow, as slow as format(), so some people
resort to thread local cached ones.

>
> Sent from my phone
>
> On Jan 20, 2013 2:26 PM, "Henri Tremblay" <henri.tremblay at gmail.com> wrote:
>>
>> I did a quick reading.
>>
>> The DateFormat seems to come from a DateFormatProvider which specifies
>> that a new instance will be returned. So I would say "yes, it's always a new
>> instance". Then, my 1.7.0_11-b21 version does indeed use putIfAbsent.
>>
>>     Object localObject =
>> (LocaleServiceProviderPool)poolOfPools.get(paramClass);
>>     if (localObject == null) {
>>       LocaleServiceProviderPool localLocaleServiceProviderPool = new
>> LocaleServiceProviderPool(paramClass);
>>
>>       localObject =
>> (LocaleServiceProviderPool)poolOfPools.putIfAbsent(paramClass,
>> localLocaleServiceProviderPool);
>>       if (localObject == null) {
>>         localObject = localLocaleServiceProviderPool;
>>       }
>>     }
>>
>> Then, I don't think it really matters. At worst, two idempotent objects
>> will be put one after the other. It doesn't really matter that two instances
>> were created. They will do the exact same thing anyway.
>>
>> -
>> Henri
>>
>>
>>
>>
>> On 20 January 2013 19:31, Cosmin Radoi <cos at illinois.edu> wrote:
>>>
>>>
>>> Maybe this has been touched upon before but I couldn't find it in
>>> history.
>>>
>>> The javadoc for DateFormat warns it is not thread safe and advises the
>>> creation of separate instances for each thread - this matter has been
>>> discussed thoroughly.
>>>
>>> > Date formats are not synchronized. It is recommended to create separate
>>> > format instances for each thread. If multiple threads access a format
>>> > concurrently, it must be synchronized externally.
>>>
>>> Still, are the DateFormat static factory methods thread-safe? They all
>>> end up in this private static method:
>>>
>>> private static DateFormat get(int timeStyle, int dateStyle, int flags,
>>> Locale loc)
>>>
>>> The method returns fresh SimpleDateFormat objects which seem to have a
>>> thread-safe creation. Still, it also allows a DateFormatProvider (obtained
>>> from a LocalServiceProviderPool) to give "an implementation that's closer to
>>> the requested locale than what the Java runtime itself can provide":
>>>
>>> > // Check whether a provider can provide an implementation that's closer
>>> > // to the requested locale than what the Java runtime itself can
>>> > provide.
>>> > LocaleServiceProviderPool pool =
>>> >       LocaleServiceProviderPool.getPool(DateFormatProvider.class);
>>> >       if (pool.hasProviders()) {
>>> >                DateFormat providersInstance = pool.getLocalizedObject(
>>> >
>>> > DateFormatGetter.INSTANCE,
>>> >                                                    loc,
>>> >                                                    timeStyle,
>>> >                                                    dateStyle,
>>> >                                                    flags);
>>> >                if (providersInstance != null) {
>>> >                    return providersInstance;
>>> >                }
>>> >            }
>>>
>>>
>>> The getPool method is the following, with the poolOfPools being a
>>> ConcurrentHashMap:
>>>
>>> >  104       /**
>>> >  105        * A factory method that returns a singleton instance
>>> >  106        */
>>> >  107       public static LocaleServiceProviderPool getPool(Class<?
>>> > extends LocaleServiceProvider> providerClass) {
>>> >  108           LocaleServiceProviderPool pool =
>>> > poolOfPools.get(providerClass);
>>> >  109           if (pool == null) {
>>> >  110               LocaleServiceProviderPool newPool =
>>> >  111                   new LocaleServiceProviderPool(providerClass);
>>> >  112               pool = poolOfPools.put(providerClass, newPool);
>>> >  113               if (pool == null) {
>>> >  114                   pool = newPool;
>>> >  115               }
>>> >  116           }
>>> >  117
>>> >  118           return pool;
>>> >  119       }
>>> (Java 7, Build b147)
>>>
>>> One issue would be that the above code looks buggy. The intent seems to
>>> be a putIfAbsent semantics but it actually allows the value object to be
>>> replaced if at least two threads race on initialization. The two racing
>>> threads would get the same value, the one created by the first thread
>>> reaching poolOfPools.put, but any subsequent threads would get the value
>>> created by the second thread.
>>>
>>> The second issue is whether LocalServiceProviderPools are guaranteed to
>>> return fresh DateFormat instances. The documentation doesn't mention such a
>>> constraint.
>>>
>>> Sorry for opening the date can of worms and please let me know if this is
>>> not the appropriate venue for this discussion.
>>>
>>> Thank you,
>>>
>>> Cosmin Radoi
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From peter.levart at gmail.com  Tue Jan 22 10:44:39 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 22 Jan 2013 16:44:39 +0100
Subject: [concurrency-interest] WeakConcurrentHashMap
Message-ID: <50FEB3E7.5040301@gmail.com>

Hello,

I hope this is the list to discuss such things. This is my 1st post to it.

Recently I stumbled upon the scalability problem of certain methods in 
JDK. Namely the java.lang.reflect.Proxy.isProxyClass(Class<?> cl) and 
transitively the java.lang.reflect.Proxy.getInvocationHandler(Object 
proxy). They use a single synchronized wrapper over the 
java.lang.WeakHashMap. There are places where those methods are used 
even in JDK (for example the 
sun.reflect.annotation.AnnotationInvocationHandler.equalsImpl that is 
used to implement the equals methods for annotations). There are many 
places where a WeakHashMap<Class<?>, ...> is wrapped with synchronized 
wrapper in JDK, although I don't claim they are on the contended code 
paths. Unfortunately there is no easy way to fix this, since there are 
no obvious ready made tools for it in the JDK (or at least I can't find 
them).

So I thought what would take to create a WeakConcurrentHashMap. I know 
there exists something like that in Guava, but JDK does not have it and 
I think this is something that is missing. Since I know it's hard to do 
a well balanced implementation of a HashMap, let alone a 
ConcurrentHashMap, my take on that is leverage the existing work:

https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/ReferenceConcurrentHashMap.java
https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/WeakConcurrentHashMap.java
https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/IdentityWeakConcurrentHashMap.java

What do you think of it? Is such thing as WeakConcurrentHashMap 
(regardless of implementation) planned to be included in a future JDK?

Regards, Peter


From nathan.reynolds at oracle.com  Tue Jan 22 11:05:14 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 22 Jan 2013 09:05:14 -0700
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <50FEB3E7.5040301@gmail.com>
References: <50FEB3E7.5040301@gmail.com>
Message-ID: <50FEB8BA.1050206@oracle.com>

I have seen several different implementations for normal, weak, soft and 
phantom keys and/or values in HashMaps and ConcurrentHashMaps.  It would 
help standardize the implementations if the JDK provide one.  It would 
also make it possible for tools to watch out for CPU performance 
problems, memory usage as well as JVM intrinsics.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 1/22/2013 8:44 AM, Peter Levart wrote:
> Hello,
>
> I hope this is the list to discuss such things. This is my 1st post to 
> it.
>
> Recently I stumbled upon the scalability problem of certain methods in 
> JDK. Namely the java.lang.reflect.Proxy.isProxyClass(Class<?> cl) and 
> transitively the java.lang.reflect.Proxy.getInvocationHandler(Object 
> proxy). They use a single synchronized wrapper over the 
> java.lang.WeakHashMap. There are places where those methods are used 
> even in JDK (for example the 
> sun.reflect.annotation.AnnotationInvocationHandler.equalsImpl that is 
> used to implement the equals methods for annotations). There are many 
> places where a WeakHashMap<Class<?>, ...> is wrapped with synchronized 
> wrapper in JDK, although I don't claim they are on the contended code 
> paths. Unfortunately there is no easy way to fix this, since there are 
> no obvious ready made tools for it in the JDK (or at least I can't 
> find them).
>
> So I thought what would take to create a WeakConcurrentHashMap. I know 
> there exists something like that in Guava, but JDK does not have it 
> and I think this is something that is missing. Since I know it's hard 
> to do a well balanced implementation of a HashMap, let alone a 
> ConcurrentHashMap, my take on that is leverage the existing work:
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/ReferenceConcurrentHashMap.java 
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/WeakConcurrentHashMap.java 
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/IdentityWeakConcurrentHashMap.java 
>
>
> What do you think of it? Is such thing as WeakConcurrentHashMap 
> (regardless of implementation) planned to be included in a future JDK?
>
> Regards, Peter
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/c7435bde/attachment.html>

From stanimir at riflexo.com  Tue Jan 22 11:36:42 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 22 Jan 2013 18:36:42 +0200
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <50FEB3E7.5040301@gmail.com>
References: <50FEB3E7.5040301@gmail.com>
Message-ID: <CAEJX8oosghVSQDfMo+YEgO7Fp1MJpYiirPqdN3dE-SU3DpsRaw@mail.gmail.com>

Creating the expunge thread leaks the ContextClassLoader and
AccessControlContext of the calling thread (also requires security
permissions to put itself in the main threadgroup).

I am not sure if a background thread would be the best choice always. You
may call 'expunge' on put operations. Checking a ReferenceQueue is
relatively fast and spares the horror of the ClassLoader leaks, in that
case the ReferenceQueue should be per Map.

Other than that the code looks fine, I have implemented such
Weak/Soft/Identity Maps as well.

Stanimir

On Tue, Jan 22, 2013 at 5:44 PM, Peter Levart <peter.levart at gmail.com>wrote:

> Hello,
>
> I hope this is the list to discuss such things. This is my 1st post to it.
>
> Recently I stumbled upon the scalability problem of certain methods in
> JDK. Namely the java.lang.reflect.Proxy.**isProxyClass(Class<?> cl) and
> transitively the java.lang.reflect.Proxy.**getInvocationHandler(Object
> proxy). They use a single synchronized wrapper over the
> java.lang.WeakHashMap. There are places where those methods are used even
> in JDK (for example the sun.reflect.annotation.**
> AnnotationInvocationHandler.**equalsImpl that is used to implement the
> equals methods for annotations). There are many places where a
> WeakHashMap<Class<?>, ...> is wrapped with synchronized wrapper in JDK,
> although I don't claim they are on the contended code paths. Unfortunately
> there is no easy way to fix this, since there are no obvious ready made
> tools for it in the JDK (or at least I can't find them).
>
> So I thought what would take to create a WeakConcurrentHashMap. I know
> there exists something like that in Guava, but JDK does not have it and I
> think this is something that is missing. Since I know it's hard to do a
> well balanced implementation of a HashMap, let alone a ConcurrentHashMap,
> my take on that is leverage the existing work:
>
> https://github.com/plevart/**jdk8-tl/blob/anno-map/jdk/src/**
> share/classes/java/util/**concurrent/**ReferenceConcurrentHashMap.**java<https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/ReferenceConcurrentHashMap.java>
> https://github.com/plevart/**jdk8-tl/blob/anno-map/jdk/src/**
> share/classes/java/util/**concurrent/**WeakConcurrentHashMap.java<https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/WeakConcurrentHashMap.java>
> https://github.com/plevart/**jdk8-tl/blob/anno-map/jdk/src/**
> share/classes/java/util/**concurrent/**IdentityWeakConcurrentHashMap.**
> java<https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/IdentityWeakConcurrentHashMap.java>
>
> What do you think of it? Is such thing as WeakConcurrentHashMap
> (regardless of implementation) planned to be included in a future JDK?
>
> Regards, Peter
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/5225177b/attachment.html>

From zhong.j.yu at gmail.com  Tue Jan 22 11:41:00 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 22 Jan 2013 10:41:00 -0600
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <50FEB8BA.1050206@oracle.com>
References: <50FEB3E7.5040301@gmail.com>
	<50FEB8BA.1050206@oracle.com>
Message-ID: <CACuKZqHfEWJkRr++LsYPPmHjW4jDqA0XMa0oBEnjTdHF=3en=w@mail.gmail.com>

It'll be a very desirable tool for application modeling, which
basically adds aftermarket statically-typed volatile fields to
objects. (I wonder whether Object can host the map directly, like
Thread hosting a map for ThreadLocal, Class hosting a map for
ClassValue)

On Tue, Jan 22, 2013 at 10:05 AM, Nathan Reynolds
<nathan.reynolds at oracle.com> wrote:
> I have seen several different implementations for normal, weak, soft and
> phantom keys and/or values in HashMaps and ConcurrentHashMaps.  It would
> help standardize the implementations if the JDK provide one.  It would also
> make it possible for tools to watch out for CPU performance problems, memory
> usage as well as JVM intrinsics.
>
> Nathan Reynolds | Architect | 602.333.9091
> Oracle PSR Engineering | Server Technology
> On 1/22/2013 8:44 AM, Peter Levart wrote:
>
> Hello,
>
> I hope this is the list to discuss such things. This is my 1st post to it.
>
> Recently I stumbled upon the scalability problem of certain methods in JDK.
> Namely the java.lang.reflect.Proxy.isProxyClass(Class<?> cl) and
> transitively the java.lang.reflect.Proxy.getInvocationHandler(Object proxy).
> They use a single synchronized wrapper over the java.lang.WeakHashMap. There
> are places where those methods are used even in JDK (for example the
> sun.reflect.annotation.AnnotationInvocationHandler.equalsImpl that is used
> to implement the equals methods for annotations). There are many places
> where a WeakHashMap<Class<?>, ...> is wrapped with synchronized wrapper in
> JDK, although I don't claim they are on the contended code paths.
> Unfortunately there is no easy way to fix this, since there are no obvious
> ready made tools for it in the JDK (or at least I can't find them).
>
> So I thought what would take to create a WeakConcurrentHashMap. I know there
> exists something like that in Guava, but JDK does not have it and I think
> this is something that is missing. Since I know it's hard to do a well
> balanced implementation of a HashMap, let alone a ConcurrentHashMap, my take
> on that is leverage the existing work:
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/ReferenceConcurrentHashMap.java
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/WeakConcurrentHashMap.java
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/IdentityWeakConcurrentHashMap.java
>
> What do you think of it? Is such thing as WeakConcurrentHashMap (regardless
> of implementation) planned to be included in a future JDK?
>
> Regards, Peter
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From stanimir at riflexo.com  Tue Jan 22 11:42:58 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 22 Jan 2013 18:42:58 +0200
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <50FEB3E7.5040301@gmail.com>
References: <50FEB3E7.5040301@gmail.com>
Message-ID: <CAEJX8opzGfzPA+Su9NdTh7MDGWx3PfN3vbTHpU-zA7dATezkvg@mail.gmail.com>

On Tue, Jan 22, 2013 at 5:44 PM, Peter Levart <peter.levart at gmail.com>wrote:

> Hello,
>
> I hope this is the list to discuss such things. This is my 1st post to it.
>
> Recently I stumbled upon the scalability problem of certain methods in
> JDK. Namely the java.lang.reflect.Proxy.isProxyClass(Class<?> cl) and
> transitively the java.lang.reflect.Proxy.getInvocationHandler(Object
> proxy). They use a single synchronized wrapper over the
> java.lang.WeakHashMap.
>

If I have to call isProxyClass I definitely check
Proxy.class.isAssignagleFrom(clazz) && Proxy.isProxyClass(). That helps
since it greatly narrows the candidates, unfortunately
ObjectOutputStreamClass does follow suit.

Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/a2d89924/attachment-0001.html>

From peter.levart at gmail.com  Tue Jan 22 12:13:06 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 22 Jan 2013 18:13:06 +0100
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <CAEJX8opzGfzPA+Su9NdTh7MDGWx3PfN3vbTHpU-zA7dATezkvg@mail.gmail.com>
References: <50FEB3E7.5040301@gmail.com>
	<CAEJX8opzGfzPA+Su9NdTh7MDGWx3PfN3vbTHpU-zA7dATezkvg@mail.gmail.com>
Message-ID: <50FEC8A2.1030207@gmail.com>

On 01/22/2013 05:42 PM, Stanimir Simeonoff wrote:
>
>
> On Tue, Jan 22, 2013 at 5:44 PM, Peter Levart <peter.levart at gmail.com 
> <mailto:peter.levart at gmail.com>> wrote:
>
>     Hello,
>
>     I hope this is the list to discuss such things. This is my 1st
>     post to it.
>
>     Recently I stumbled upon the scalability problem of certain
>     methods in JDK. Namely the
>     java.lang.reflect.Proxy.isProxyClass(Class<?> cl) and transitively
>     the java.lang.reflect.Proxy.getInvocationHandler(Object proxy).
>     They use a single synchronized wrapper over the java.lang.WeakHashMap.
>
>
> If I have to call isProxyClass I definitely check 
> Proxy.class.isAssignagleFrom(clazz) && Proxy.isProxyClass(). That 
> helps since it greatly narrows the candidates, unfortunately 
> ObjectOutputStreamClass does follow suit.

That helps, when you expect a *false* answer. If your expected answer is 
*true* most of times and you afterwards call 
java.lang.reflect.Proxy.getInvocationHandler(Object proxy), you get hit 
twice (since Proxy.isProxyClass() is called again in that method).

Regards, Peter

>
> Stanimir

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/d4206091/attachment.html>

From oleksandr.otenko at oracle.com  Tue Jan 22 12:26:14 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 22 Jan 2013 17:26:14 +0000
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <50FEB3E7.5040301@gmail.com>
References: <50FEB3E7.5040301@gmail.com>
Message-ID: <50FECBB6.7090108@oracle.com>

You know, all problems in CS can be solved by adding one more level of 
indirection.


I think the actual solution is to not have a wrapper for lookup, but to 
provide a lens that can deconstruct the key already in the map to 
compare to the lookup key.

Demand an interface like:

interface Functor<K, W> { // uh... yeah, it only lifts one function, 
this one
   public W wrap(K dis);
   public boolean equalsLifted(K dis, W dat);
   public K unwrap(W dat); // not part of a canonical functor definition 
-- but it is even not important
}

If wrap and equalsLifted are pure, the implementation is thread safe.

A map storing a value would use the hashCode of the actual key to lookup 
a slot, then a KeyMapper.wrap to construct the key to be stored, and a 
ValueWrapper.wrap to construct a value to be stored (trivially we obtain 
a Soft/Weak-refed key, or a Soft/Weak-refed value, or both; 
equalsLifted(K dis, W dat) { K v=dat.get(); return v == dis || 
dis.equals(v);})

A map looking up a value uses the hashCode of the raw lookup value to 
lookup a slot, then a KeyMapper.equalsLifted to compare the lookup key 
with the wrapped key stored in the map. Finally, ValueWrapper.unwrap 
extracts the value from the wrapper.


In fact, we can even get a mapping from one equality function to another:

class IdentityMapper<O> extends Functor<O, O> {
   public O wrap(O dis) { return dis; }
   public boolean equalsLifted(O dis, O dat) { return dis == dat; }
   public O unwrap(O dat) { return dat; }
}

Thus we get a IdentityHashMap.


Alex


On 22/01/2013 15:44, Peter Levart wrote:
> Hello,
>
> I hope this is the list to discuss such things. This is my 1st post to 
> it.
>
> Recently I stumbled upon the scalability problem of certain methods in 
> JDK. Namely the java.lang.reflect.Proxy.isProxyClass(Class<?> cl) and 
> transitively the java.lang.reflect.Proxy.getInvocationHandler(Object 
> proxy). They use a single synchronized wrapper over the 
> java.lang.WeakHashMap. There are places where those methods are used 
> even in JDK (for example the 
> sun.reflect.annotation.AnnotationInvocationHandler.equalsImpl that is 
> used to implement the equals methods for annotations). There are many 
> places where a WeakHashMap<Class<?>, ...> is wrapped with synchronized 
> wrapper in JDK, although I don't claim they are on the contended code 
> paths. Unfortunately there is no easy way to fix this, since there are 
> no obvious ready made tools for it in the JDK (or at least I can't 
> find them).
>
> So I thought what would take to create a WeakConcurrentHashMap. I know 
> there exists something like that in Guava, but JDK does not have it 
> and I think this is something that is missing. Since I know it's hard 
> to do a well balanced implementation of a HashMap, let alone a 
> ConcurrentHashMap, my take on that is leverage the existing work:
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/ReferenceConcurrentHashMap.java 
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/WeakConcurrentHashMap.java 
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/IdentityWeakConcurrentHashMap.java 
>
>
> What do you think of it? Is such thing as WeakConcurrentHashMap 
> (regardless of implementation) planned to be included in a future JDK?
>
> Regards, Peter
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/4ffd3a2b/attachment.html>

From stanimir at riflexo.com  Tue Jan 22 13:39:02 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 22 Jan 2013 20:39:02 +0200
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <50FECBB6.7090108@oracle.com>
References: <50FEB3E7.5040301@gmail.com>
	<50FECBB6.7090108@oracle.com>
Message-ID: <CAEJX8oqtDK8W1oeXPgQLOtrvLtYrPittPGvv+kYDwzC2U+stdw@mail.gmail.com>

But then you add the indirection on the matching function which may lead to
virtual call for "equals". The latter is probably undesirable (performance
hit) if it's implemented directly in CHM.
At least this is what I attributed the lack of customizable hash/equals
provider to.

Btw IdentityMap would be a lot better off with open address hashtable
(IdentityHashMap and Cliff Click's one) due to the near perfect
distribution of System.identityHashCode. For identity map
System.identityHashCode makes a lot more sense than overridden hashCode().

Stanimir

On Tue, Jan 22, 2013 at 7:26 PM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  You know, all problems in CS can be solved by adding one more level of
> indirection.
>
>
> I think the actual solution is to not have a wrapper for lookup, but to
> provide a lens that can deconstruct the key already in the map to compare
> to the lookup key.
>
> Demand an interface like:
>
> interface Functor<K, W> { // uh... yeah, it only lifts one function, this
> one
>   public W wrap(K dis);
>   public boolean equalsLifted(K dis, W dat);
>   public K unwrap(W dat); // not part of a canonical functor definition --
> but it is even not important
> }
>
> If wrap and equalsLifted are pure, the implementation is thread safe.
>
> A map storing a value would use the hashCode of the actual key to lookup a
> slot, then a KeyMapper.wrap to construct the key to be stored, and a
> ValueWrapper.wrap to construct a value to be stored (trivially we obtain a
> Soft/Weak-refed key, or a Soft/Weak-refed value, or both; equalsLifted(K
> dis, W dat) { K v=dat.get(); return v == dis || dis.equals(v);})
>
> A map looking up a value uses the hashCode of the raw lookup value to
> lookup a slot, then a KeyMapper.equalsLifted to compare the lookup key with
> the wrapped key stored in the map. Finally, ValueWrapper.unwrap extracts
> the value from the wrapper.
>
>
> In fact, we can even get a mapping from one equality function to another:
>
> class IdentityMapper<O> extends Functor<O, O> {
>   public O wrap(O dis) { return dis; }
>   public boolean equalsLifted(O dis, O dat) { return dis == dat; }
>   public O unwrap(O dat) { return dat; }
> }
>
> Thus we get a IdentityHashMap.
>
>
> Alex
>
>
>  On 22/01/2013 15:44, Peter Levart wrote:
>
> Hello,
>
> I hope this is the list to discuss such things. This is my 1st post to it.
>
> Recently I stumbled upon the scalability problem of certain methods in
> JDK. Namely the java.lang.reflect.Proxy.isProxyClass(Class<?> cl) and
> transitively the java.lang.reflect.Proxy.getInvocationHandler(Object
> proxy). They use a single synchronized wrapper over the
> java.lang.WeakHashMap. There are places where those methods are used even
> in JDK (for example the
> sun.reflect.annotation.AnnotationInvocationHandler.equalsImpl that is used
> to implement the equals methods for annotations). There are many places
> where a WeakHashMap<Class<?>, ...> is wrapped with synchronized wrapper in
> JDK, although I don't claim they are on the contended code paths.
> Unfortunately there is no easy way to fix this, since there are no obvious
> ready made tools for it in the JDK (or at least I can't find them).
>
> So I thought what would take to create a WeakConcurrentHashMap. I know
> there exists something like that in Guava, but JDK does not have it and I
> think this is something that is missing. Since I know it's hard to do a
> well balanced implementation of a HashMap, let alone a ConcurrentHashMap,
> my take on that is leverage the existing work:
>
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/ReferenceConcurrentHashMap.java
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/WeakConcurrentHashMap.java
>
> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/IdentityWeakConcurrentHashMap.java
>
> What do you think of it? Is such thing as WeakConcurrentHashMap
> (regardless of implementation) planned to be included in a future JDK?
>
> Regards, Peter
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/3ceb16ab/attachment-0001.html>

From stanimir at riflexo.com  Tue Jan 22 13:55:30 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 22 Jan 2013 20:55:30 +0200
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <50FEC8A2.1030207@gmail.com>
References: <50FEB3E7.5040301@gmail.com>
	<CAEJX8opzGfzPA+Su9NdTh7MDGWx3PfN3vbTHpU-zA7dATezkvg@mail.gmail.com>
	<50FEC8A2.1030207@gmail.com>
Message-ID: <CAEJX8orZDe+hy6b+nH9_F6QE0drnn3FmY+g8VtC3_pYV9nXSWA@mail.gmail.com>

> If I have to call isProxyClass I definitely check
> Proxy.class.isAssignagleFrom(clazz) && Proxy.isProxyClass(). That helps
> since it greatly narrows the candidates, unfortunately
> ObjectOutputStreamClass does follow suit.
>
>
> That helps, when you expect a *false* answer. If your expected answer is
> *true* most of times and you afterwards call
> java.lang.reflect.Proxy.getInvocationHandler(Object proxy), you get hit
> twice (since Proxy.isProxyClass() is called again in that method).
>
>
Well, of course. The other possible bottleneck (aside the reckless call
during serialization) could be RemoteObject.toStub that calls both
isProxyClass and getInvocationHandler.
If you expect a positive answer check instanceof Proxy instead calling
isProxyClass. Since only hacks would extend the class naturally and then
follow w/ Proxy.getInvocationHandler, catching the IllegalArgumentException
in case of a hack.
Yes the code looks like a hack, itself, but a comment should suffice.
Basically this is exactly what I do.

Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/115258b8/attachment.html>

From oleksandr.otenko at oracle.com  Tue Jan 22 14:16:21 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 22 Jan 2013 19:16:21 +0000
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <CAEJX8oqtDK8W1oeXPgQLOtrvLtYrPittPGvv+kYDwzC2U+stdw@mail.gmail.com>
References: <50FEB3E7.5040301@gmail.com> <50FECBB6.7090108@oracle.com>
	<CAEJX8oqtDK8W1oeXPgQLOtrvLtYrPittPGvv+kYDwzC2U+stdw@mail.gmail.com>
Message-ID: <50FEE585.1050003@oracle.com>

Yes, it is for Lambda optimization project to work out how to inline the 
call to equalsLifted of the instance of KeyWrapper passed into the 
constructor.

identityHashCode has its own limitations. Try to synchronize on an 
object on which you computed identityHashCode - I think the lock is 
never thin after that.


Nevermind the virtual calls. The intention was to show what is needed to 
cut the copy-pasting of Concurrent*Map code, which is much trickier to 
do right. Even Comparator interface is not good enough, because a. it 
requires both arguments to be of the same type; b. it actually needs the 
instances to have some total order. What is really needed is a way to 
compare "raw" values we pass from outside the map to "lifted" values 
inside the map, thereby abstracting away one effectful part of the 
program - the semi-references.

You don't need to always lift the lookup value just to find the key, 
which was part of the original Peter's proposal; if we always construct 
a wrapped value just to look up, we waste cycles creating the wrapper 
objects, then garbage collecting them. (see wrapForLookup method) This 
is not just Peter's proposal, this is a common way to deal with this 
situation (see also pre-JDK-7 object stream map of classes).

Alex


On 22/01/2013 18:39, Stanimir Simeonoff wrote:
> But then you add the indirection on the matching function which may 
> lead to virtual call for "equals". The latter is probably undesirable 
> (performance hit) if it's implemented directly in CHM.
> At least this is what I attributed the lack of customizable 
> hash/equals provider to.
>
> Btw IdentityMap would be a lot better off with open address hashtable 
> (IdentityHashMap and Cliff Click's one) due to the near perfect 
> distribution of System.identityHashCode. For identity map 
> System.identityHashCode makes a lot more sense than overridden hashCode().
>
> Stanimir
>
> On Tue, Jan 22, 2013 at 7:26 PM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     You know, all problems in CS can be solved by adding one more
>     level of indirection.
>
>
>     I think the actual solution is to not have a wrapper for lookup,
>     but to provide a lens that can deconstruct the key already in the
>     map to compare to the lookup key.
>
>     Demand an interface like:
>
>     interface Functor<K, W> { // uh... yeah, it only lifts one
>     function, this one
>       public W wrap(K dis);
>       public boolean equalsLifted(K dis, W dat);
>       public K unwrap(W dat); // not part of a canonical functor
>     definition -- but it is even not important
>     }
>
>     If wrap and equalsLifted are pure, the implementation is thread safe.
>
>     A map storing a value would use the hashCode of the actual key to
>     lookup a slot, then a KeyMapper.wrap to construct the key to be
>     stored, and a ValueWrapper.wrap to construct a value to be stored
>     (trivially we obtain a Soft/Weak-refed key, or a Soft/Weak-refed
>     value, or both; equalsLifted(K dis, W dat) { K v=dat.get(); return
>     v == dis || dis.equals(v);})
>
>     A map looking up a value uses the hashCode of the raw lookup value
>     to lookup a slot, then a KeyMapper.equalsLifted to compare the
>     lookup key with the wrapped key stored in the map. Finally,
>     ValueWrapper.unwrap extracts the value from the wrapper.
>
>
>     In fact, we can even get a mapping from one equality function to
>     another:
>
>     class IdentityMapper<O> extends Functor<O, O> {
>       public O wrap(O dis) { return dis; }
>       public boolean equalsLifted(O dis, O dat) { return dis == dat; }
>       public O unwrap(O dat) { return dat; }
>     }
>
>     Thus we get a IdentityHashMap.
>
>
>     Alex
>
>
>     On 22/01/2013 15:44, Peter Levart wrote:
>>     Hello,
>>
>>     I hope this is the list to discuss such things. This is my 1st
>>     post to it.
>>
>>     Recently I stumbled upon the scalability problem of certain
>>     methods in JDK. Namely the
>>     java.lang.reflect.Proxy.isProxyClass(Class<?> cl) and
>>     transitively the
>>     java.lang.reflect.Proxy.getInvocationHandler(Object proxy). They
>>     use a single synchronized wrapper over the java.lang.WeakHashMap.
>>     There are places where those methods are used even in JDK (for
>>     example the
>>     sun.reflect.annotation.AnnotationInvocationHandler.equalsImpl
>>     that is used to implement the equals methods for annotations).
>>     There are many places where a WeakHashMap<Class<?>, ...> is
>>     wrapped with synchronized wrapper in JDK, although I don't claim
>>     they are on the contended code paths. Unfortunately there is no
>>     easy way to fix this, since there are no obvious ready made tools
>>     for it in the JDK (or at least I can't find them).
>>
>>     So I thought what would take to create a WeakConcurrentHashMap. I
>>     know there exists something like that in Guava, but JDK does not
>>     have it and I think this is something that is missing. Since I
>>     know it's hard to do a well balanced implementation of a HashMap,
>>     let alone a ConcurrentHashMap, my take on that is leverage the
>>     existing work:
>>
>>     https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/ReferenceConcurrentHashMap.java
>>
>>     https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/WeakConcurrentHashMap.java
>>
>>     https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/IdentityWeakConcurrentHashMap.java
>>
>>
>>     What do you think of it? Is such thing as WeakConcurrentHashMap
>>     (regardless of implementation) planned to be included in a future
>>     JDK?
>>
>>     Regards, Peter
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/03a1d635/attachment.html>

From stanimir at riflexo.com  Tue Jan 22 14:39:45 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 22 Jan 2013 21:39:45 +0200
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <50FEE585.1050003@oracle.com>
References: <50FEB3E7.5040301@gmail.com> <50FECBB6.7090108@oracle.com>
	<CAEJX8oqtDK8W1oeXPgQLOtrvLtYrPittPGvv+kYDwzC2U+stdw@mail.gmail.com>
	<50FEE585.1050003@oracle.com>
Message-ID: <CAEJX8orzBNSgr8pU-BruuK62JrJSAsmXRAw9uUZ=dK8hZLsXFQ@mail.gmail.com>

> identityHashCode has its own limitations. Try to synchronize on an object
> on which you computed identityHashCode - I think the lock is never thin
> after that.
>
Locks are thin only w/ biased locking enabled and if used by the same
thread (no wait/notify), once another thread attempts monitor_enter, the
header will be inflated anyways. So they are useful on stuff like
vector/hashtable that are kept single threaded.
While there might be cases to put the objects as keys and synchronize on
them on a single thread only I'd say they are not spread enough to worry
about the object header inflation. identityHashCode would be generally
faster than custom hashCode().

Stanimir



>
>
>  On 22/01/2013 18:39, Stanimir Simeonoff wrote:
>
> But then you add the indirection on the matching function which may lead
> to virtual call for "equals". The latter is probably undesirable
> (performance hit) if it's implemented directly in CHM.
> At least this is what I attributed the lack of customizable hash/equals
> provider to.
>
> Btw IdentityMap would be a lot better off with open address hashtable
> (IdentityHashMap and Cliff Click's one) due to the near perfect
> distribution of System.identityHashCode. For identity map
> System.identityHashCode makes a lot more sense than overridden hashCode().
>
> Stanimir
>
> On Tue, Jan 22, 2013 at 7:26 PM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  You know, all problems in CS can be solved by adding one more level of
>> indirection.
>>
>>
>> I think the actual solution is to not have a wrapper for lookup, but to
>> provide a lens that can deconstruct the key already in the map to compare
>> to the lookup key.
>>
>> Demand an interface like:
>>
>> interface Functor<K, W> { // uh... yeah, it only lifts one function, this
>> one
>>   public W wrap(K dis);
>>   public boolean equalsLifted(K dis, W dat);
>>   public K unwrap(W dat); // not part of a canonical functor definition
>> -- but it is even not important
>> }
>>
>> If wrap and equalsLifted are pure, the implementation is thread safe.
>>
>> A map storing a value would use the hashCode of the actual key to lookup
>> a slot, then a KeyMapper.wrap to construct the key to be stored, and a
>> ValueWrapper.wrap to construct a value to be stored (trivially we obtain a
>> Soft/Weak-refed key, or a Soft/Weak-refed value, or both; equalsLifted(K
>> dis, W dat) { K v=dat.get(); return v == dis || dis.equals(v);})
>>
>> A map looking up a value uses the hashCode of the raw lookup value to
>> lookup a slot, then a KeyMapper.equalsLifted to compare the lookup key with
>> the wrapped key stored in the map. Finally, ValueWrapper.unwrap extracts
>> the value from the wrapper.
>>
>>
>> In fact, we can even get a mapping from one equality function to another:
>>
>> class IdentityMapper<O> extends Functor<O, O> {
>>   public O wrap(O dis) { return dis; }
>>   public boolean equalsLifted(O dis, O dat) { return dis == dat; }
>>   public O unwrap(O dat) { return dat; }
>> }
>>
>> Thus we get a IdentityHashMap.
>>
>>
>> Alex
>>
>>
>>   On 22/01/2013 15:44, Peter Levart wrote:
>>
>> Hello,
>>
>> I hope this is the list to discuss such things. This is my 1st post to
>> it.
>>
>> Recently I stumbled upon the scalability problem of certain methods in
>> JDK. Namely the java.lang.reflect.Proxy.isProxyClass(Class<?> cl) and
>> transitively the java.lang.reflect.Proxy.getInvocationHandler(Object
>> proxy). They use a single synchronized wrapper over the
>> java.lang.WeakHashMap. There are places where those methods are used even
>> in JDK (for example the
>> sun.reflect.annotation.AnnotationInvocationHandler.equalsImpl that is used
>> to implement the equals methods for annotations). There are many places
>> where a WeakHashMap<Class<?>, ...> is wrapped with synchronized wrapper in
>> JDK, although I don't claim they are on the contended code paths.
>> Unfortunately there is no easy way to fix this, since there are no obvious
>> ready made tools for it in the JDK (or at least I can't find them).
>>
>> So I thought what would take to create a WeakConcurrentHashMap. I know
>> there exists something like that in Guava, but JDK does not have it and I
>> think this is something that is missing. Since I know it's hard to do a
>> well balanced implementation of a HashMap, let alone a ConcurrentHashMap,
>> my take on that is leverage the existing work:
>>
>>
>> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/ReferenceConcurrentHashMap.java
>>
>> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/WeakConcurrentHashMap.java
>>
>> https://github.com/plevart/jdk8-tl/blob/anno-map/jdk/src/share/classes/java/util/concurrent/IdentityWeakConcurrentHashMap.java
>>
>> What do you think of it? Is such thing as WeakConcurrentHashMap
>> (regardless of implementation) planned to be included in a future JDK?
>>
>> Regards, Peter
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/c8cf3de4/attachment-0001.html>

From dl at cs.oswego.edu  Tue Jan 22 15:04:38 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 22 Jan 2013 15:04:38 -0500
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <50FEB3E7.5040301@gmail.com>
References: <50FEB3E7.5040301@gmail.com>
Message-ID: <50FEF0D6.3000108@cs.oswego.edu>

On 01/22/13 10:44, Peter Levart wrote:
> Hello,
>
> I hope this is the list to discuss such things. This is my 1st post to it.

In which case you are forgiven for not knowing that this topic
is periodically discussed here :-) And that there is a non-JDK
"extra" class available (based on a previous version of
ConcurrentHashMap in "extra166y.CustomConcurrentHashMap
(http://gee.cs.oswego.edu/dl/concurrency-interest/index.html)

The main issue is that once you have a Weak version,
it becomes the beginning of a large set of
policy/parameterization choices. How about Weak values?
How about both weak keys and values (requiring Ephemerons,
not coming soon.) What about automatic eviction?
What about declarative policies for eviction?
What about supporting the JSR107 Cache API?
And so on.

No finite number of choices seems defensible for something
that will live in JDK. The Google Guava folks created
some with an extensible front-end (MapMaker) that covers
a lot of needs. A long-standing j.u.c item is to someday
try to leverage the experience gained with this and others
and make another stab at a good general support class.

-Doug



From jeffhain at rocketmail.com  Tue Jan 22 18:28:38 2013
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Tue, 22 Jan 2013 23:28:38 +0000 (GMT)
Subject: [concurrency-interest]  volatile acrobacy
Message-ID: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>

Hello.

A few months ago I've made a sort of Disruptor implementation,
and in the process went into a frenzy of optimizations.

Since all my tests were always passing, I stayed confident
(test driven development at its worse ;), but as I recently
realized, I ended up with what I think is broken code:
in my frenzy I forgot about the basics of Disruptor
(the volatile write and then read of a proper variable
by proper threads to ensure event visibility).


It's easily fixable, and the correction has almost no impact
on performances for nbr of processors < nbr of cores.

My problem is that I can't manage to prove my code broken
with tests, and I don't understand why it "works".

Here is the (afaik-non-founded) assumption that I
(semi-unconsciously) made in the broken code:If a thread lazily-sets a volatile variable A, and then that
another thread reads A and sees its new value, and then
sets (lazily or not) a volatile variable B, then if a third
thread reads B and sees its new value, if it reads A, it
should also see its new value (or more particularly
in the case of a ring buffer, if it reads a non-volatile
variable that was set by the first thread just before the
lazy-set of A, it should see its new value).
(It might still be wrong if the first set is non-lazy,
but laziness should make it worse.)


I attached code with which I unsuccessfully attempted to
prove this assumption wrong.

Can anyone confirm that my assumption was wrong, and
point out reasons why it could still "work" so often?
-Jeff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/a1d73df9/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: VolatileAcrobacy.java
Type: application/octet-stream
Size: 7006 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/a1d73df9/attachment.obj>

From vitalyd at gmail.com  Tue Jan 22 19:09:21 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 22 Jan 2013 19:09:21 -0500
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
Message-ID: <CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>

By lazy set do you mean AtomicXXX.lazySet (or Unsafe.putXXXOrdered)? Sorry,
can't open the attachment on my phone.

Anyway, assuming it's the above, then yes there's no guarantee that your
third thread sees updated A despite seeing updated B.  There's no total
order established here if you're using lazy writes - all lazy writes
guarantee is that if a thread observes the lazy write, it observes all
previous stores (by writing thread) as well.

As for why you can't see this break, it could be a combo of:

1) enough time goes by in between for A to flush and be globally visible by
the time third thread reads it
2) there could be other synchronizing actions in between that make writes
visible immediately
3) your threads might be running on the cores doing the actual writes so
observe their own stores

Sent from my phone
On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com> wrote:

> Hello.
>
> A few months ago I've made a sort of Disruptor implementation,
> and in the process went into a frenzy of optimizations.
>
> Since all my tests were always passing, I stayed confident
> (test driven development at its worse ;), but as I recently
> realized, I ended up with what I think is broken code:
> in my frenzy I forgot about the basics of Disruptor
> (the volatile write and then read of a proper variable
> by proper threads to ensure event visibility).
>
> It's easily fixable, and the correction has almost no impact
> on performances for nbr of processors < nbr of cores.
> My problem is that I can't manage to prove my code broken
> with tests, and I don't understand why it "works".
>
> Here is the (afaik-non-founded) assumption that I
> (semi-unconsciously) made in the broken code:
> If a thread lazily-sets a volatile variable A, and then that
> another thread reads A and sees its new value, and then
> sets (lazily or not) a volatile variable B, then if a third
> thread reads B and sees its new value, if it reads A, it
> should also see its new value (or more particularly
> in the case of a ring buffer, if it reads a non-volatile
> variable that was set by the first thread just before the
> lazy-set of A, it should see its new value).
> (It might still be wrong if the first set is non-lazy,
> but laziness should make it worse.)
>
> I attached code with which I unsuccessfully attempted to
> prove this assumption wrong.
>
> Can anyone confirm that my assumption was wrong, and
> point out reasons why it could still "work" so often?
>
> -Jeff
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130122/7663c528/attachment-0001.html>

From dl at cs.oswego.edu  Tue Jan 22 19:33:16 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 22 Jan 2013 19:33:16 -0500
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
Message-ID: <50FF2FCC.70904@cs.oswego.edu>

On 01/22/13 18:28, Jeff Hain wrote:

>
> Here is the (afaik-non-founded) assumption that I
> (semi-unconsciously) made in the broken code:
> If a thread lazily-sets a volatile variable A, and then that
> another thread reads A and sees its new value, and then
> sets (lazily or not) a volatile variable B, then if a third
> thread reads B and sees its new value, if it reads A, it
> should also see its new value

For some discussion of this and related cases, at least
with respect to TSO (not the JMM),  find links to
the paper:

Reasoning about the Implementation of Concurrency Abstractions on x86-TSO. Scott 
Owens. In ECOOP 2010.

at http://www.cl.cam.ac.uk/~pes20/weakmemory/

-Doug


From cheremin at gmail.com  Wed Jan 23 00:21:39 2013
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Wed, 23 Jan 2013 09:21:39 +0400
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
Message-ID: <CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>

Well, apart from lazySet semantics not being strictly defined, doesn't
assumed semantics is "A.lazySet() hb A.get() with value returned by
get being same as written by lazySet"? And if it is -- can't I
transitively continue this HB from first to third thread?

Generally, I was sure total order was introduced in JMM for
simplifying prove of it's overall correctness, but in most specific
cases partial order is enough to prove SC.




2013/1/23 Vitaly Davidovich <vitalyd at gmail.com>:
> By lazy set do you mean AtomicXXX.lazySet (or Unsafe.putXXXOrdered)? Sorry,
> can't open the attachment on my phone.
>
> Anyway, assuming it's the above, then yes there's no guarantee that your
> third thread sees updated A despite seeing updated B.  There's no total
> order established here if you're using lazy writes - all lazy writes
> guarantee is that if a thread observes the lazy write, it observes all
> previous stores (by writing thread) as well.
>
> As for why you can't see this break, it could be a combo of:
>
> 1) enough time goes by in between for A to flush and be globally visible by
> the time third thread reads it
> 2) there could be other synchronizing actions in between that make writes
> visible immediately
> 3) your threads might be running on the cores doing the actual writes so
> observe their own stores
>
> Sent from my phone
>
> On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com> wrote:
>>
>> Hello.
>>
>> A few months ago I've made a sort of Disruptor implementation,
>> and in the process went into a frenzy of optimizations.
>>
>> Since all my tests were always passing, I stayed confident
>> (test driven development at its worse ;), but as I recently
>> realized, I ended up with what I think is broken code:
>> in my frenzy I forgot about the basics of Disruptor
>> (the volatile write and then read of a proper variable
>> by proper threads to ensure event visibility).
>>
>> It's easily fixable, and the correction has almost no impact
>> on performances for nbr of processors < nbr of cores.
>> My problem is that I can't manage to prove my code broken
>> with tests, and I don't understand why it "works".
>>
>> Here is the (afaik-non-founded) assumption that I
>> (semi-unconsciously) made in the broken code:
>> If a thread lazily-sets a volatile variable A, and then that
>> another thread reads A and sees its new value, and then
>> sets (lazily or not) a volatile variable B, then if a third
>> thread reads B and sees its new value, if it reads A, it
>> should also see its new value (or more particularly
>> in the case of a ring buffer, if it reads a non-volatile
>> variable that was set by the first thread just before the
>> lazy-set of A, it should see its new value).
>> (It might still be wrong if the first set is non-lazy,
>> but laziness should make it worse.)
>>
>> I attached code with which I unsuccessfully attempted to
>> prove this assumption wrong.
>>
>> Can anyone confirm that my assumption was wrong, and
>> point out reasons why it could still "work" so often?
>>
>> -Jeff
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From vitalyd at gmail.com  Wed Jan 23 07:00:14 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 23 Jan 2013 07:00:14 -0500
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
Message-ID: <CAHjP37Eeosc5+QT5CxNFQXUx5m5KW5R4VtQMJOJYEFdExSj9Yg@mail.gmail.com>

lazySet semantics are: don't worry about making this write immediately
visible to other CPUs, but ensure that when it is that all preceding writes
are made visible as well (volatile or not).  It follows that reading the
value via get() doesn't guarantee you see latest value.  On x86, for
example, lazySet is a compiler barrier and plain MOV instruction.  On CPUs
that don't order writes, it'll turn into some release fence.

So in Jeff's case, thinking about this at x86 hardware level, I'll correct
myself.  If third thread sees the write of B, it means either the write of
B has been made visible globally or it's running on the same CPU as thread
2.  If thread 2 observed A then it means either it's running on same CPU as
thread 1 or A is globally visible.  So yes, in either case you get the
desired transitive relationship since either the write is globally visible
or they're all running on same CPU.  Not sure how JMM would fit in here
though ...

Sent from my phone
On Jan 23, 2013 12:21 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

> Well, apart from lazySet semantics not being strictly defined, doesn't
> assumed semantics is "A.lazySet() hb A.get() with value returned by
> get being same as written by lazySet"? And if it is -- can't I
> transitively continue this HB from first to third thread?
>
> Generally, I was sure total order was introduced in JMM for
> simplifying prove of it's overall correctness, but in most specific
> cases partial order is enough to prove SC.
>
>
>
>
> 2013/1/23 Vitaly Davidovich <vitalyd at gmail.com>:
> > By lazy set do you mean AtomicXXX.lazySet (or Unsafe.putXXXOrdered)?
> Sorry,
> > can't open the attachment on my phone.
> >
> > Anyway, assuming it's the above, then yes there's no guarantee that your
> > third thread sees updated A despite seeing updated B.  There's no total
> > order established here if you're using lazy writes - all lazy writes
> > guarantee is that if a thread observes the lazy write, it observes all
> > previous stores (by writing thread) as well.
> >
> > As for why you can't see this break, it could be a combo of:
> >
> > 1) enough time goes by in between for A to flush and be globally visible
> by
> > the time third thread reads it
> > 2) there could be other synchronizing actions in between that make writes
> > visible immediately
> > 3) your threads might be running on the cores doing the actual writes so
> > observe their own stores
> >
> > Sent from my phone
> >
> > On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com> wrote:
> >>
> >> Hello.
> >>
> >> A few months ago I've made a sort of Disruptor implementation,
> >> and in the process went into a frenzy of optimizations.
> >>
> >> Since all my tests were always passing, I stayed confident
> >> (test driven development at its worse ;), but as I recently
> >> realized, I ended up with what I think is broken code:
> >> in my frenzy I forgot about the basics of Disruptor
> >> (the volatile write and then read of a proper variable
> >> by proper threads to ensure event visibility).
> >>
> >> It's easily fixable, and the correction has almost no impact
> >> on performances for nbr of processors < nbr of cores.
> >> My problem is that I can't manage to prove my code broken
> >> with tests, and I don't understand why it "works".
> >>
> >> Here is the (afaik-non-founded) assumption that I
> >> (semi-unconsciously) made in the broken code:
> >> If a thread lazily-sets a volatile variable A, and then that
> >> another thread reads A and sees its new value, and then
> >> sets (lazily or not) a volatile variable B, then if a third
> >> thread reads B and sees its new value, if it reads A, it
> >> should also see its new value (or more particularly
> >> in the case of a ring buffer, if it reads a non-volatile
> >> variable that was set by the first thread just before the
> >> lazy-set of A, it should see its new value).
> >> (It might still be wrong if the first set is non-lazy,
> >> but laziness should make it worse.)
> >>
> >> I attached code with which I unsuccessfully attempted to
> >> prove this assumption wrong.
> >>
> >> Can anyone confirm that my assumption was wrong, and
> >> point out reasons why it could still "work" so often?
> >>
> >> -Jeff
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130123/2b4523e7/attachment.html>

From heinz at javaspecialists.eu  Wed Jan 23 07:20:19 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 23 Jan 2013 14:20:19 +0200
Subject: [concurrency-interest] Interview about Concurrency
Message-ID: <50FFD583.5000009@javaspecialists.eu>

I don't think I say anything that would be particularly surprising or 
new to members of this elite group, but in case you are interested, here 
is an interview that Roger Brinkley did with me about Java concurrency 
and why this is becoming an increasingly important topic.

https://blogs.oracle.com/javaspotlight/entry/java_spotlight_episode_116_dr

Apologies in advance for rambling on a bit towards the end.  I've 
already been approached by the Institute for Sleep Therapy to ask if 
they could use it for treating their patients ;-)

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz 


From aleksey.shipilev at oracle.com  Wed Jan 23 07:22:53 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 23 Jan 2013 16:22:53 +0400
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
Message-ID: <50FFD61D.50205@oracle.com>

I think both Jeff and Ruslan are in the "wishful thinking" land. ;)

     Thread 1     |    Thread 2      |     Thread 3
------------------+------------------+------------------
 1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
                  | 3: b.set(r1)     |  5: r3 = a.get()

While there is (3) hb (4), it does not follow there is transitive (1) hb
(5) edge.

Under the JMM, the result values for (r2,r3) are:
  (0, 0) -- OK, no updates seen
  (0, 1) -- OK, early update of $a seen
  (1, 0) -- OK, update of $b is seen, but update to $a is not visible
  (1, 1) -- OK, both updates are seen

Although (1, 0) clashes with intuitive model of global consistent
memory, and can be masked by TSO. This can be demonstrated by
java-concurrency-torture test [1]. Hey Jeff, see no boilerplate code
there ;)

Run it on 4-way x86 (I have masked the irrelevant states with "x", other
three are r1, r2, r3, respectively) for a minute:

       Observed state  Occurrences  Expectation
   [x, x, 1, x, 0, 1] (  62630154)  ACCEPTABLE
   [x, x, 1, x, 0, 0] (  30019106)  ACCEPTABLE
   [x, x, 0, x, 0, 1] (  25647080)  ACCEPTABLE
   [x, x, 0, x, 0, 0] (  27678732)  ACCEPTABLE
   [x, x, 1, x, 1, 1] (  23509348)  ACCEPTABLE
   [x, x, 1, x, 1, 0] (         0)  ACCEPTABLE

Note the latest case is always zero, meaning the (1, 0) case not
detected. It changes if you run it on 6-way PPC for a minute:

       Observed state  Occurrences  Expectation
   [x, x, 1, x, 0, 1] (  51334629)  ACCEPTABLE
   [x, x, 1, x, 0, 0] (  33056225)  ACCEPTABLE
   [x, x, 0, x, 0, 1] (  11579848)  ACCEPTABLE
   [x, x, 0, x, 0, 0] (  51182907)  ACCEPTABLE
   [x, x, 1, x, 1, 1] (  15334538)  ACCEPTABLE
   [x, x, 1, x, 1, 0] (         3)  ACCEPTABLE

Here, the (1,0) case although rare, but possible, because the PPC in
question is not TSO. Modulo I'm completely wrong, and this is the
JVM/test bug, of course. Hope this helps to unwind some of the
theoretical mess.

-Aleksey.

[1]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/volatiles/LazySetTransitivityTest.java

On 01/23/2013 09:21 AM, Ruslan Cheremin wrote:
> Well, apart from lazySet semantics not being strictly defined, doesn't
> assumed semantics is "A.lazySet() hb A.get() with value returned by
> get being same as written by lazySet"? And if it is -- can't I
> transitively continue this HB from first to third thread?
> 
> Generally, I was sure total order was introduced in JMM for
> simplifying prove of it's overall correctness, but in most specific
> cases partial order is enough to prove SC.
> 
> 
> 
> 
> 2013/1/23 Vitaly Davidovich <vitalyd at gmail.com>:
>> By lazy set do you mean AtomicXXX.lazySet (or Unsafe.putXXXOrdered)? Sorry,
>> can't open the attachment on my phone.
>>
>> Anyway, assuming it's the above, then yes there's no guarantee that your
>> third thread sees updated A despite seeing updated B.  There's no total
>> order established here if you're using lazy writes - all lazy writes
>> guarantee is that if a thread observes the lazy write, it observes all
>> previous stores (by writing thread) as well.
>>
>> As for why you can't see this break, it could be a combo of:
>>
>> 1) enough time goes by in between for A to flush and be globally visible by
>> the time third thread reads it
>> 2) there could be other synchronizing actions in between that make writes
>> visible immediately
>> 3) your threads might be running on the cores doing the actual writes so
>> observe their own stores
>>
>> Sent from my phone
>>
>> On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com> wrote:
>>>
>>> Hello.
>>>
>>> A few months ago I've made a sort of Disruptor implementation,
>>> and in the process went into a frenzy of optimizations.
>>>
>>> Since all my tests were always passing, I stayed confident
>>> (test driven development at its worse ;), but as I recently
>>> realized, I ended up with what I think is broken code:
>>> in my frenzy I forgot about the basics of Disruptor
>>> (the volatile write and then read of a proper variable
>>> by proper threads to ensure event visibility).
>>>
>>> It's easily fixable, and the correction has almost no impact
>>> on performances for nbr of processors < nbr of cores.
>>> My problem is that I can't manage to prove my code broken
>>> with tests, and I don't understand why it "works".
>>>
>>> Here is the (afaik-non-founded) assumption that I
>>> (semi-unconsciously) made in the broken code:
>>> If a thread lazily-sets a volatile variable A, and then that
>>> another thread reads A and sees its new value, and then
>>> sets (lazily or not) a volatile variable B, then if a third
>>> thread reads B and sees its new value, if it reads A, it
>>> should also see its new value (or more particularly
>>> in the case of a ring buffer, if it reads a non-volatile
>>> variable that was set by the first thread just before the
>>> lazy-set of A, it should see its new value).
>>> (It might still be wrong if the first set is non-lazy,
>>> but laziness should make it worse.)
>>>
>>> I attached code with which I unsuccessfully attempted to
>>> prove this assumption wrong.
>>>
>>> Can anyone confirm that my assumption was wrong, and
>>> point out reasons why it could still "work" so often?
>>>
>>> -Jeff
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From vitalyd at gmail.com  Wed Jan 23 08:23:53 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 23 Jan 2013 08:23:53 -0500
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <50FFD61D.50205@oracle.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
Message-ID: <CAHjP37HQAMb0G80Cc5ko-WCOAum3-+53GtB5nJx2BVFyVOe3cA@mail.gmail.com>

Hmm, I don't see how (1,0) can be seen even on weak memory.  b is a
volatile write, observing its update should mean that all prior stores are
visible too - so why is a not visible? Was b.set() supposed to be lazySet
in your diagram cause then it makes sense.

Sent from my phone
On Jan 23, 2013 7:23 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com>
wrote:

> I think both Jeff and Ruslan are in the "wishful thinking" land. ;)
>
>      Thread 1     |    Thread 2      |     Thread 3
> ------------------+------------------+------------------
>  1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
>                   | 3: b.set(r1)     |  5: r3 = a.get()
>
> While there is (3) hb (4), it does not follow there is transitive (1) hb
> (5) edge.
>
> Under the JMM, the result values for (r2,r3) are:
>   (0, 0) -- OK, no updates seen
>   (0, 1) -- OK, early update of $a seen
>   (1, 0) -- OK, update of $b is seen, but update to $a is not visible
>   (1, 1) -- OK, both updates are seen
>
> Although (1, 0) clashes with intuitive model of global consistent
> memory, and can be masked by TSO. This can be demonstrated by
> java-concurrency-torture test [1]. Hey Jeff, see no boilerplate code
> there ;)
>
> Run it on 4-way x86 (I have masked the irrelevant states with "x", other
> three are r1, r2, r3, respectively) for a minute:
>
>        Observed state  Occurrences  Expectation
>    [x, x, 1, x, 0, 1] (  62630154)  ACCEPTABLE
>    [x, x, 1, x, 0, 0] (  30019106)  ACCEPTABLE
>    [x, x, 0, x, 0, 1] (  25647080)  ACCEPTABLE
>    [x, x, 0, x, 0, 0] (  27678732)  ACCEPTABLE
>    [x, x, 1, x, 1, 1] (  23509348)  ACCEPTABLE
>    [x, x, 1, x, 1, 0] (         0)  ACCEPTABLE
>
> Note the latest case is always zero, meaning the (1, 0) case not
> detected. It changes if you run it on 6-way PPC for a minute:
>
>        Observed state  Occurrences  Expectation
>    [x, x, 1, x, 0, 1] (  51334629)  ACCEPTABLE
>    [x, x, 1, x, 0, 0] (  33056225)  ACCEPTABLE
>    [x, x, 0, x, 0, 1] (  11579848)  ACCEPTABLE
>    [x, x, 0, x, 0, 0] (  51182907)  ACCEPTABLE
>    [x, x, 1, x, 1, 1] (  15334538)  ACCEPTABLE
>    [x, x, 1, x, 1, 0] (         3)  ACCEPTABLE
>
> Here, the (1,0) case although rare, but possible, because the PPC in
> question is not TSO. Modulo I'm completely wrong, and this is the
> JVM/test bug, of course. Hope this helps to unwind some of the
> theoretical mess.
>
> -Aleksey.
>
> [1]
>
> https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/volatiles/LazySetTransitivityTest.java
>
> On 01/23/2013 09:21 AM, Ruslan Cheremin wrote:
> > Well, apart from lazySet semantics not being strictly defined, doesn't
> > assumed semantics is "A.lazySet() hb A.get() with value returned by
> > get being same as written by lazySet"? And if it is -- can't I
> > transitively continue this HB from first to third thread?
> >
> > Generally, I was sure total order was introduced in JMM for
> > simplifying prove of it's overall correctness, but in most specific
> > cases partial order is enough to prove SC.
> >
> >
> >
> >
> > 2013/1/23 Vitaly Davidovich <vitalyd at gmail.com>:
> >> By lazy set do you mean AtomicXXX.lazySet (or Unsafe.putXXXOrdered)?
> Sorry,
> >> can't open the attachment on my phone.
> >>
> >> Anyway, assuming it's the above, then yes there's no guarantee that your
> >> third thread sees updated A despite seeing updated B.  There's no total
> >> order established here if you're using lazy writes - all lazy writes
> >> guarantee is that if a thread observes the lazy write, it observes all
> >> previous stores (by writing thread) as well.
> >>
> >> As for why you can't see this break, it could be a combo of:
> >>
> >> 1) enough time goes by in between for A to flush and be globally
> visible by
> >> the time third thread reads it
> >> 2) there could be other synchronizing actions in between that make
> writes
> >> visible immediately
> >> 3) your threads might be running on the cores doing the actual writes so
> >> observe their own stores
> >>
> >> Sent from my phone
> >>
> >> On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com> wrote:
> >>>
> >>> Hello.
> >>>
> >>> A few months ago I've made a sort of Disruptor implementation,
> >>> and in the process went into a frenzy of optimizations.
> >>>
> >>> Since all my tests were always passing, I stayed confident
> >>> (test driven development at its worse ;), but as I recently
> >>> realized, I ended up with what I think is broken code:
> >>> in my frenzy I forgot about the basics of Disruptor
> >>> (the volatile write and then read of a proper variable
> >>> by proper threads to ensure event visibility).
> >>>
> >>> It's easily fixable, and the correction has almost no impact
> >>> on performances for nbr of processors < nbr of cores.
> >>> My problem is that I can't manage to prove my code broken
> >>> with tests, and I don't understand why it "works".
> >>>
> >>> Here is the (afaik-non-founded) assumption that I
> >>> (semi-unconsciously) made in the broken code:
> >>> If a thread lazily-sets a volatile variable A, and then that
> >>> another thread reads A and sees its new value, and then
> >>> sets (lazily or not) a volatile variable B, then if a third
> >>> thread reads B and sees its new value, if it reads A, it
> >>> should also see its new value (or more particularly
> >>> in the case of a ring buffer, if it reads a non-volatile
> >>> variable that was set by the first thread just before the
> >>> lazy-set of A, it should see its new value).
> >>> (It might still be wrong if the first set is non-lazy,
> >>> but laziness should make it worse.)
> >>>
> >>> I attached code with which I unsuccessfully attempted to
> >>> prove this assumption wrong.
> >>>
> >>> Can anyone confirm that my assumption was wrong, and
> >>> point out reasons why it could still "work" so often?
> >>>
> >>> -Jeff
> >>>
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130123/6c1a985a/attachment.html>

From aleksey.shipilev at oracle.com  Wed Jan 23 08:27:52 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 23 Jan 2013 17:27:52 +0400
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAHjP37HQAMb0G80Cc5ko-WCOAum3-+53GtB5nJx2BVFyVOe3cA@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<CAHjP37HQAMb0G80Cc5ko-WCOAum3-+53GtB5nJx2BVFyVOe3cA@mail.gmail.com>
Message-ID: <50FFE558.1060605@oracle.com>

On 01/23/2013 05:23 PM, Vitaly Davidovich wrote:
> Hmm, I don't see how (1,0) can be seen even on weak memory.  b is a
> volatile write, observing its update should mean that all prior stores
> are visible too - so why is a not visible?

     Thread 1     |    Thread 2      |     Thread 3
------------------+------------------+------------------
 1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
                  | 3: b.set(r1)     |  5: r3 = a.get()

Prior stores in the *same* thread, you might want to add? The mere fact
T2 observed the updated value of $a, does not mean that T3 will observe
the same. T3 will observe the prior stores in T2 though.

-Aleksey.


From vitalyd at gmail.com  Wed Jan 23 08:35:10 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 23 Jan 2013 08:35:10 -0500
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <50FFE558.1060605@oracle.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<CAHjP37HQAMb0G80Cc5ko-WCOAum3-+53GtB5nJx2BVFyVOe3cA@mail.gmail.com>
	<50FFE558.1060605@oracle.com>
Message-ID: <CAHjP37GsngJvnL8oA-B1Etm0MRU2hF+yk910iVfcOS-GRPt3hw@mail.gmail.com>

Maybe I misread the diagram - is T1 doing both writes without interruption
in between? If so, any thread observing the volatile write sees the prior
writes unless that volatile write is also lazySet.

Sent from my phone
On Jan 23, 2013 8:28 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com>
wrote:

> On 01/23/2013 05:23 PM, Vitaly Davidovich wrote:
> > Hmm, I don't see how (1,0) can be seen even on weak memory.  b is a
> > volatile write, observing its update should mean that all prior stores
> > are visible too - so why is a not visible?
>
>      Thread 1     |    Thread 2      |     Thread 3
> ------------------+------------------+------------------
>  1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
>                   | 3: b.set(r1)     |  5: r3 = a.get()
>
> Prior stores in the *same* thread, you might want to add? The mere fact
> T2 observed the updated value of $a, does not mean that T3 will observe
> the same. T3 will observe the prior stores in T2 though.
>
> -Aleksey.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130123/314defde/attachment.html>

From aleksey.shipilev at oracle.com  Wed Jan 23 08:37:50 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 23 Jan 2013 17:37:50 +0400
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAHjP37GsngJvnL8oA-B1Etm0MRU2hF+yk910iVfcOS-GRPt3hw@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<CAHjP37HQAMb0G80Cc5ko-WCOAum3-+53GtB5nJx2BVFyVOe3cA@mail.gmail.com>
	<50FFE558.1060605@oracle.com>
	<CAHjP37GsngJvnL8oA-B1Etm0MRU2hF+yk910iVfcOS-GRPt3hw@mail.gmail.com>
Message-ID: <50FFE7AE.1010001@oracle.com>

Oh, that's your iPhone playing games with plain text messages? T1 is
only doing lazySet. Here's the iPhone-friendly listing:

Thread 1:
   a.lazySet(1)

Thread 2:
   r1 = a.get()
   b.set(r1)

Thread 3:
   r2 = b.get()
   r3 = a.get()

-Aleksey.

On 01/23/2013 05:35 PM, Vitaly Davidovich wrote:
> Maybe I misread the diagram - is T1 doing both writes without
> interruption in between? If so, any thread observing the volatile write
> sees the prior writes unless that volatile write is also lazySet.
> 
> Sent from my phone
> 
> On Jan 23, 2013 8:28 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com
> <mailto:aleksey.shipilev at oracle.com>> wrote:
> 
>     On 01/23/2013 05:23 PM, Vitaly Davidovich wrote:
>     > Hmm, I don't see how (1,0) can be seen even on weak memory.  b is a
>     > volatile write, observing its update should mean that all prior stores
>     > are visible too - so why is a not visible?
> 
>          Thread 1     |    Thread 2      |     Thread 3
>     ------------------+------------------+------------------
>      1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
>                       | 3: b.set(r1)     |  5: r3 = a.get()
> 
>     Prior stores in the *same* thread, you might want to add? The mere fact
>     T2 observed the updated value of $a, does not mean that T3 will observe
>     the same. T3 will observe the prior stores in T2 though.
> 
>     -Aleksey.
> 


From vitalyd at gmail.com  Wed Jan 23 08:48:00 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 23 Jan 2013 08:48:00 -0500
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <50FFE7AE.1010001@oracle.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<CAHjP37HQAMb0G80Cc5ko-WCOAum3-+53GtB5nJx2BVFyVOe3cA@mail.gmail.com>
	<50FFE558.1060605@oracle.com>
	<CAHjP37GsngJvnL8oA-B1Etm0MRU2hF+yk910iVfcOS-GRPt3hw@mail.gmail.com>
	<50FFE7AE.1010001@oracle.com>
Message-ID: <CAHjP37HnX3oZV=TpKhvQ_M36Naru26SgwMKfPaiXaWRe=tktYg@mail.gmail.com>

Ah, much better - thanks (android though not iPhone :)).

If this is a cache coherent system and all 3 threads running on diff CPUs
then if thread 2 sees a then it means it's globally visible - any other CPU
reading a at this point should also see it.  There's no promise on order of
other writes, but I don't see how one cpu can see it but not another if the
wall clock time of the reads is same or thread 3 is after thread 2.

Sent from my phone
On Jan 23, 2013 8:38 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com>
wrote:

> Oh, that's your iPhone playing games with plain text messages? T1 is
> only doing lazySet. Here's the iPhone-friendly listing:
>
> Thread 1:
>    a.lazySet(1)
>
> Thread 2:
>    r1 = a.get()
>    b.set(r1)
>
> Thread 3:
>    r2 = b.get()
>    r3 = a.get()
>
> -Aleksey.
>
> On 01/23/2013 05:35 PM, Vitaly Davidovich wrote:
> > Maybe I misread the diagram - is T1 doing both writes without
> > interruption in between? If so, any thread observing the volatile write
> > sees the prior writes unless that volatile write is also lazySet.
> >
> > Sent from my phone
> >
> > On Jan 23, 2013 8:28 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com
> > <mailto:aleksey.shipilev at oracle.com>> wrote:
> >
> >     On 01/23/2013 05:23 PM, Vitaly Davidovich wrote:
> >     > Hmm, I don't see how (1,0) can be seen even on weak memory.  b is a
> >     > volatile write, observing its update should mean that all prior
> stores
> >     > are visible too - so why is a not visible?
> >
> >          Thread 1     |    Thread 2      |     Thread 3
> >     ------------------+------------------+------------------
> >      1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
> >                       | 3: b.set(r1)     |  5: r3 = a.get()
> >
> >     Prior stores in the *same* thread, you might want to add? The mere
> fact
> >     T2 observed the updated value of $a, does not mean that T3 will
> observe
> >     the same. T3 will observe the prior stores in T2 though.
> >
> >     -Aleksey.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130123/a47683b6/attachment.html>

From nitsanw at yahoo.com  Wed Jan 23 11:02:42 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Wed, 23 Jan 2013 08:02:42 -0800 (PST)
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAHjP37HnX3oZV=TpKhvQ_M36Naru26SgwMKfPaiXaWRe=tktYg@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<CAHjP37HQAMb0G80Cc5ko-WCOAum3-+53GtB5nJx2BVFyVOe3cA@mail.gmail.com>
	<50FFE558.1060605@oracle.com>
	<CAHjP37GsngJvnL8oA-B1Etm0MRU2hF+yk910iVfcOS-GRPt3hw@mail.gmail.com>
	<50FFE7AE.1010001@oracle.com>
	<CAHjP37HnX3oZV=TpKhvQ_M36Naru26SgwMKfPaiXaWRe=tktYg@mail.gmail.com>
Message-ID: <1358956962.18340.YahooMailNeo@web120701.mail.ne1.yahoo.com>

My understanding of lazySet is that your comment: "any thread observing the volatile write sees the prior writes unless that volatile write is also lazySet." is incorrect. lazySet at least according to various statements made by Doug on this thread, has unofficial happens before guarantees for same thread.
T1:
a.lazySet(1);
b.lazySet(1);

T2:
r1 = b.get();
r2 = a.get();

can lead to (0,1) but not (1,0)
Am I missing something here?



________________________________
 From: Vitaly Davidovich <vitalyd at gmail.com>
To: Aleksey Shipilev <aleksey.shipilev at oracle.com> 
Cc: concurrency-interest at cs.oswego.edu 
Sent: Wednesday, January 23, 2013 1:48 PM
Subject: Re: [concurrency-interest] volatile acrobacy
 

Ah, much better - thanks (android though not iPhone :)).
If this is a cache coherent system and all 3 threads running on diff CPUs then if thread 2 sees a then it means it's globally visible - any other CPU reading a at this point should also see it.? There's no promise on order of other writes, but I don't see how one cpu can see it but not another if the wall clock time of the reads is same or thread 3 is after thread 2.

Sent from my phone
On Jan 23, 2013 8:38 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com> wrote:

Oh, that's your iPhone playing games with plain text messages? T1 is
>only doing lazySet. Here's the iPhone-friendly listing:
>
>Thread 1:
>? ?a.lazySet(1)
>
>Thread 2:
>? ?r1 = a.get()
>? ?b.set(r1)
>
>Thread 3:
>? ?r2 = b.get()
>? ?r3 = a.get()
>
>-Aleksey.
>
>On 01/23/2013 05:35 PM, Vitaly Davidovich wrote:
>> Maybe I misread the diagram - is T1 doing both writes without
>> interruption in between? If so, any thread observing the volatile write
>> sees the prior writes unless that volatile write is also lazySet.
>>
>> Sent from my phone
>>
>> On Jan 23, 2013 8:28 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com
>> <mailto:aleksey.shipilev at oracle.com>> wrote:
>>
>> ? ? On 01/23/2013 05:23 PM, Vitaly Davidovich wrote:
>> ? ? > Hmm, I don't see how (1,0) can be seen even on weak memory. ?b is a
>> ? ? > volatile write, observing its update should mean that all prior stores
>> ? ? > are visible too - so why is a not visible?
>>
>> ? ? ? ? ?Thread 1 ? ? | ? ?Thread 2 ? ? ?| ? ? Thread 3
>> ? ? ------------------+------------------+------------------
>> ? ? ?1: a.lazySet(1) ?| 2: r1 = a.get() ?| ?4: r2 = b.get()
>> ? ? ? ? ? ? ? ? ? ? ? | 3: b.set(r1) ? ? | ?5: r3 = a.get()
>>
>> ? ? Prior stores in the *same* thread, you might want to add? The mere fact
>> ? ? T2 observed the updated value of $a, does not mean that T3 will observe
>> ? ? the same. T3 will observe the prior stores in T2 though.
>>
>> ? ? -Aleksey.
>>
>
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130123/f2fb664a/attachment.html>

From nitsanw at yahoo.com  Wed Jan 23 11:21:18 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Wed, 23 Jan 2013 08:21:18 -0800 (PST)
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAHjP37HnX3oZV=TpKhvQ_M36Naru26SgwMKfPaiXaWRe=tktYg@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<CAHjP37HQAMb0G80Cc5ko-WCOAum3-+53GtB5nJx2BVFyVOe3cA@mail.gmail.com>
	<50FFE558.1060605@oracle.com>
	<CAHjP37GsngJvnL8oA-B1Etm0MRU2hF+yk910iVfcOS-GRPt3hw@mail.gmail.com>
	<50FFE7AE.1010001@oracle.com>
	<CAHjP37HnX3oZV=TpKhvQ_M36Naru26SgwMKfPaiXaWRe=tktYg@mail.gmail.com>
Message-ID: <1358958078.76748.YahooMailNeo@web120704.mail.ne1.yahoo.com>

While on the topic, would be very happy if members of this forum could slap me (gently) if I'm wrong on this blog post concerning lazySet:
http://psy-lob-saw.blogspot.com/2012/12/atomiclazyset-is-performance-win-for.html
I relied on some comments made on this mailing group, so would like to correct any misunderstanding I may have had.
Thanks,
Nitsan


________________________________
 From: Vitaly Davidovich <vitalyd at gmail.com>
To: Aleksey Shipilev <aleksey.shipilev at oracle.com> 
Cc: concurrency-interest at cs.oswego.edu 
Sent: Wednesday, January 23, 2013 1:48 PM
Subject: Re: [concurrency-interest] volatile acrobacy
 

Ah, much better - thanks (android though not iPhone :)).
If this is a cache coherent system and all 3 threads running on diff CPUs then if thread 2 sees a then it means it's globally visible - any other CPU reading a at this point should also see it.? There's no promise on order of other writes, but I don't see how one cpu can see it but not another if the wall clock time of the reads is same or thread 3 is after thread 2.

Sent from my phone
On Jan 23, 2013 8:38 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com> wrote:

Oh, that's your iPhone playing games with plain text messages? T1 is
>only doing lazySet. Here's the iPhone-friendly listing:
>
>Thread 1:
>? ?a.lazySet(1)
>
>Thread 2:
>? ?r1 = a.get()
>? ?b.set(r1)
>
>Thread 3:
>? ?r2 = b.get()
>? ?r3 = a.get()
>
>-Aleksey.
>
>On 01/23/2013 05:35 PM, Vitaly Davidovich wrote:
>> Maybe I misread the diagram - is T1 doing both writes without
>> interruption in between? If so, any thread observing the volatile write
>> sees the prior writes unless that volatile write is also lazySet.
>>
>> Sent from my phone
>>
>> On Jan 23, 2013 8:28 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com
>> <mailto:aleksey.shipilev at oracle.com>> wrote:
>>
>> ? ? On 01/23/2013 05:23 PM, Vitaly Davidovich wrote:
>> ? ? > Hmm, I don't see how (1,0) can be seen even on weak memory. ?b is a
>> ? ? > volatile write, observing its update should mean that all prior stores
>> ? ? > are visible too - so why is a not visible?
>>
>> ? ? ? ? ?Thread 1 ? ? | ? ?Thread 2 ? ? ?| ? ? Thread 3
>> ? ? ------------------+------------------+------------------
>> ? ? ?1: a.lazySet(1) ?| 2: r1 = a.get() ?| ?4: r2 = b.get()
>> ? ? ? ? ? ? ? ? ? ? ? | 3: b.set(r1) ? ? | ?5: r3 = a.get()
>>
>> ? ? Prior stores in the *same* thread, you might want to add? The mere fact
>> ? ? T2 observed the updated value of $a, does not mean that T3 will observe
>> ? ? the same. T3 will observe the prior stores in T2 though.
>>
>> ? ? -Aleksey.
>>
>
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130123/963b8022/attachment-0001.html>

From cheremin at gmail.com  Wed Jan 23 15:04:29 2013
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Thu, 24 Jan 2013 00:04:29 +0400
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <1358956962.18340.YahooMailNeo@web120701.mail.ne1.yahoo.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<CAHjP37HQAMb0G80Cc5ko-WCOAum3-+53GtB5nJx2BVFyVOe3cA@mail.gmail.com>
	<50FFE558.1060605@oracle.com>
	<CAHjP37GsngJvnL8oA-B1Etm0MRU2hF+yk910iVfcOS-GRPt3hw@mail.gmail.com>
	<50FFE7AE.1010001@oracle.com>
	<CAHjP37HnX3oZV=TpKhvQ_M36Naru26SgwMKfPaiXaWRe=tktYg@mail.gmail.com>
	<1358956962.18340.YahooMailNeo@web120701.mail.ne1.yahoo.com>
Message-ID: <CAOwENiLJ4MvJdTMo4GvS6NODBRNE-tHc2inbd2UU-SFKQKWjoQ@mail.gmail.com>

My understanding is quite similar. Apart from not being somewhere
specified officially, there was draft of Fences API with
.orderWrites() method suggested, and, AFAIK, it's suggested semantics
is exactly what lazySet is given us currently. And my understanding of
that semantics (yes, another level of "my understanding of") is that
just [a.lazySet(1)] hb [a.get() == 1], and that hb edge is transitive
with other, like program order hb, and volatile induced hb.

Alexey: in you example I do not understand how (1,0) can be observed.
0. [4] hb [5] by program order
1. If [4] (r2 == 1) => [3] hb [4] because of SO
2. [2] hb [3] because of SO again
3. [1] hb [2] because of lazySet definition

If [1] hb [2] is some separate HB ordering, not transitive with? with
what? It must be transitive close with program order in thread1 _and_
program order in thread2 (in other case it does not have any sense at
all) -- but that is all we need here. Am I missed something?


2013/1/23 Nitsan Wakart <nitsanw at yahoo.com>:
> My understanding of lazySet is that your comment: "any thread observing the
> volatile write sees the prior writes unless that volatile write is also
> lazySet." is incorrect. lazySet at least according to various statements
> made by Doug on this thread, has unofficial happens before guarantees for
> same thread.
> T1:
> a.lazySet(1);
> b.lazySet(1);
> T2:
> r1 = b.get();
> r2 = a.get();
>
> can lead to (0,1) but not (1,0)
> Am I missing something here?
>
> ________________________________
> From: Vitaly Davidovich <vitalyd at gmail.com>
> To: Aleksey Shipilev <aleksey.shipilev at oracle.com>
> Cc: concurrency-interest at cs.oswego.edu
> Sent: Wednesday, January 23, 2013 1:48 PM
> Subject: Re: [concurrency-interest] volatile acrobacy
>
> Ah, much better - thanks (android though not iPhone :)).
> If this is a cache coherent system and all 3 threads running on diff CPUs
> then if thread 2 sees a then it means it's globally visible - any other CPU
> reading a at this point should also see it.  There's no promise on order of
> other writes, but I don't see how one cpu can see it but not another if the
> wall clock time of the reads is same or thread 3 is after thread 2.
> Sent from my phone
> On Jan 23, 2013 8:38 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com>
> wrote:
>
> Oh, that's your iPhone playing games with plain text messages? T1 is
> only doing lazySet. Here's the iPhone-friendly listing:
>
> Thread 1:
>    a.lazySet(1)
>
> Thread 2:
>    r1 = a.get()
>    b.set(r1)
>
> Thread 3:
>    r2 = b.get()
>    r3 = a.get()
>
> -Aleksey.
>
> On 01/23/2013 05:35 PM, Vitaly Davidovich wrote:
>> Maybe I misread the diagram - is T1 doing both writes without
>> interruption in between? If so, any thread observing the volatile write
>> sees the prior writes unless that volatile write is also lazySet.
>>
>> Sent from my phone
>>
>> On Jan 23, 2013 8:28 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com
>> <mailto:aleksey.shipilev at oracle.com>> wrote:
>>
>>     On 01/23/2013 05:23 PM, Vitaly Davidovich wrote:
>>     > Hmm, I don't see how (1,0) can be seen even on weak memory.  b is a
>>     > volatile write, observing its update should mean that all prior
>> stores
>>     > are visible too - so why is a not visible?
>>
>>          Thread 1     |    Thread 2      |     Thread 3
>>     ------------------+------------------+------------------
>>      1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
>>                       | 3: b.set(r1)     |  5: r3 = a.get()
>>
>>     Prior stores in the *same* thread, you might want to add? The mere
>> fact
>>     T2 observed the updated value of $a, does not mean that T3 will
>> observe
>>     the same. T3 will observe the prior stores in T2 though.
>>
>>     -Aleksey.
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From hans.boehm at hp.com  Wed Jan 23 15:08:15 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 23 Jan 2013 20:08:15 +0000
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <50FFD61D.50205@oracle.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>

Thanks for some interesting results.

These are unfortunately at odds with my understanding of lazySet() semantics.  My understanding was that lazySet() corresponds to C++ memory_order_release and get() corresponds to memory_order_seq_cst.  Both lazySet() and set() have the same effect on happens-before.  In the interesting executions of the example below, statement 1 happens before statement 5.  Thus r3 must be 1.

The difference between set() and lazySet() is whether the set operation participates in a total order of sequentially consistent operations, roughly analogous to Java synchronization order.

Can you determine what PPC code is generated for get() and lazySet().  Is it consistent with the recipes in http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html?

In particular:

get() should have at least a trailing cmp; bc; isync or lwsync.  (It should also have a leading hwsync, though I don't see how that can matter for this test case, since C++ memory_order_acquire should suffice here.)

lazySet() should have a leading lwsync.

My hope is that this is a JVM bug (or conceivably even a hardware bug), and the equivalence with C++ memory_order_acquire is intended to hold.  I personally wouldn't consider a lazySet() that doesn't participate in happens-before to be usable.  I think it would, for example, make internal parallelism in library routines, particularly constructors, visible to clients, and would require the library to document which effects are performed in the calling threads, vs. just happening before the return.

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> interest-bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
> Sent: Wednesday, January 23, 2013 4:23 AM
> To: Ruslan Cheremin
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] volatile acrobacy
> 
> I think both Jeff and Ruslan are in the "wishful thinking" land. ;)
> 
>      Thread 1     |    Thread 2      |     Thread 3
> ------------------+------------------+------------------
>  1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
>                   | 3: b.set(r1)     |  5: r3 = a.get()
> 
> While there is (3) hb (4), it does not follow there is transitive (1)
> hb
> (5) edge.
> 
> Under the JMM, the result values for (r2,r3) are:
>   (0, 0) -- OK, no updates seen
>   (0, 1) -- OK, early update of $a seen
>   (1, 0) -- OK, update of $b is seen, but update to $a is not visible
>   (1, 1) -- OK, both updates are seen
> 
> Although (1, 0) clashes with intuitive model of global consistent
> memory, and can be masked by TSO. This can be demonstrated by
> java-concurrency-torture test [1]. Hey Jeff, see no boilerplate code
> there ;)
> 
> Run it on 4-way x86 (I have masked the irrelevant states with "x",
> other
> three are r1, r2, r3, respectively) for a minute:
> 
>        Observed state  Occurrences  Expectation
>    [x, x, 1, x, 0, 1] (  62630154)  ACCEPTABLE
>    [x, x, 1, x, 0, 0] (  30019106)  ACCEPTABLE
>    [x, x, 0, x, 0, 1] (  25647080)  ACCEPTABLE
>    [x, x, 0, x, 0, 0] (  27678732)  ACCEPTABLE
>    [x, x, 1, x, 1, 1] (  23509348)  ACCEPTABLE
>    [x, x, 1, x, 1, 0] (         0)  ACCEPTABLE
> 
> Note the latest case is always zero, meaning the (1, 0) case not
> detected. It changes if you run it on 6-way PPC for a minute:
> 
>        Observed state  Occurrences  Expectation
>    [x, x, 1, x, 0, 1] (  51334629)  ACCEPTABLE
>    [x, x, 1, x, 0, 0] (  33056225)  ACCEPTABLE
>    [x, x, 0, x, 0, 1] (  11579848)  ACCEPTABLE
>    [x, x, 0, x, 0, 0] (  51182907)  ACCEPTABLE
>    [x, x, 1, x, 1, 1] (  15334538)  ACCEPTABLE
>    [x, x, 1, x, 1, 0] (         3)  ACCEPTABLE
> 
> Here, the (1,0) case although rare, but possible, because the PPC in
> question is not TSO. Modulo I'm completely wrong, and this is the
> JVM/test bug, of course. Hope this helps to unwind some of the
> theoretical mess.
> 
> -Aleksey.
> 
> [1]
> https://github.com/shipilev/java-concurrency-
> torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/
> volatiles/LazySetTransitivityTest.java
> 
> On 01/23/2013 09:21 AM, Ruslan Cheremin wrote:
> > Well, apart from lazySet semantics not being strictly defined,
> doesn't
> > assumed semantics is "A.lazySet() hb A.get() with value returned by
> > get being same as written by lazySet"? And if it is -- can't I
> > transitively continue this HB from first to third thread?
> >
> > Generally, I was sure total order was introduced in JMM for
> > simplifying prove of it's overall correctness, but in most specific
> > cases partial order is enough to prove SC.
> >
> >
> >
> >
> > 2013/1/23 Vitaly Davidovich <vitalyd at gmail.com>:
> >> By lazy set do you mean AtomicXXX.lazySet (or Unsafe.putXXXOrdered)?
> Sorry,
> >> can't open the attachment on my phone.
> >>
> >> Anyway, assuming it's the above, then yes there's no guarantee that
> your
> >> third thread sees updated A despite seeing updated B.  There's no
> total
> >> order established here if you're using lazy writes - all lazy writes
> >> guarantee is that if a thread observes the lazy write, it observes
> all
> >> previous stores (by writing thread) as well.
> >>
> >> As for why you can't see this break, it could be a combo of:
> >>
> >> 1) enough time goes by in between for A to flush and be globally
> visible by
> >> the time third thread reads it
> >> 2) there could be other synchronizing actions in between that make
> writes
> >> visible immediately
> >> 3) your threads might be running on the cores doing the actual
> writes so
> >> observe their own stores
> >>
> >> Sent from my phone
> >>
> >> On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com>
> wrote:
> >>>
> >>> Hello.
> >>>
> >>> A few months ago I've made a sort of Disruptor implementation,
> >>> and in the process went into a frenzy of optimizations.
> >>>
> >>> Since all my tests were always passing, I stayed confident
> >>> (test driven development at its worse ;), but as I recently
> >>> realized, I ended up with what I think is broken code:
> >>> in my frenzy I forgot about the basics of Disruptor
> >>> (the volatile write and then read of a proper variable
> >>> by proper threads to ensure event visibility).
> >>>
> >>> It's easily fixable, and the correction has almost no impact
> >>> on performances for nbr of processors < nbr of cores.
> >>> My problem is that I can't manage to prove my code broken
> >>> with tests, and I don't understand why it "works".
> >>>
> >>> Here is the (afaik-non-founded) assumption that I
> >>> (semi-unconsciously) made in the broken code:
> >>> If a thread lazily-sets a volatile variable A, and then that
> >>> another thread reads A and sees its new value, and then
> >>> sets (lazily or not) a volatile variable B, then if a third
> >>> thread reads B and sees its new value, if it reads A, it
> >>> should also see its new value (or more particularly
> >>> in the case of a ring buffer, if it reads a non-volatile
> >>> variable that was set by the first thread just before the
> >>> lazy-set of A, it should see its new value).
> >>> (It might still be wrong if the first set is non-lazy,
> >>> but laziness should make it worse.)
> >>>
> >>> I attached code with which I unsuccessfully attempted to
> >>> prove this assumption wrong.
> >>>
> >>> Can anyone confirm that my assumption was wrong, and
> >>> point out reasons why it could still "work" so often?
> >>>
> >>> -Jeff
> >>>
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From cheremin at gmail.com  Wed Jan 23 15:46:59 2013
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Thu, 24 Jan 2013 00:46:59 +0400
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
Message-ID: <CAOwENiJK_LHvbdg9EOz-F3-aAjaA9+SycaLCZpXxTaJe8zSLmQ@mail.gmail.com>

Hans, thank you for pointing on C memory model as an example of more
precise something-like-lazySet specification.

While we are at this topic -- it is question I looking for answer long
ago: why at all we really need _total_ synchronization order in JMM? I
mean what was the primary reason for introducing "backbone" of total
ordered synchronized actions, and pair them with ribs of partial
ordered ordinary actions? Could be consistent memory model build up
with partial HB order only, with synchronized actions just introducing
inter-thread HB, not total order? I think, new C model is something
like this, but I haven't chance to dig deep into it yet, so not sure
here. But I could recall some notes in this list about total SO being
limit for scalability with number of cores grows (may be it was even
your note, Hans).



2013/1/24 Boehm, Hans <hans.boehm at hp.com>:
> Thanks for some interesting results.
>
> These are unfortunately at odds with my understanding of lazySet() semantics.  My understanding was that lazySet() corresponds to C++ memory_order_release and get() corresponds to memory_order_seq_cst.  Both lazySet() and set() have the same effect on happens-before.  In the interesting executions of the example below, statement 1 happens before statement 5.  Thus r3 must be 1.
>
> The difference between set() and lazySet() is whether the set operation participates in a total order of sequentially consistent operations, roughly analogous to Java synchronization order.
>
> Can you determine what PPC code is generated for get() and lazySet().  Is it consistent with the recipes in http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html?
>
> In particular:
>
> get() should have at least a trailing cmp; bc; isync or lwsync.  (It should also have a leading hwsync, though I don't see how that can matter for this test case, since C++ memory_order_acquire should suffice here.)
>
> lazySet() should have a leading lwsync.
>
> My hope is that this is a JVM bug (or conceivably even a hardware bug), and the equivalence with C++ memory_order_acquire is intended to hold.  I personally wouldn't consider a lazySet() that doesn't participate in happens-before to be usable.  I think it would, for example, make internal parallelism in library routines, particularly constructors, visible to clients, and would require the library to document which effects are performed in the calling threads, vs. just happening before the return.
>
> Hans
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
>> interest-bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
>> Sent: Wednesday, January 23, 2013 4:23 AM
>> To: Ruslan Cheremin
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] volatile acrobacy
>>
>> I think both Jeff and Ruslan are in the "wishful thinking" land. ;)
>>
>>      Thread 1     |    Thread 2      |     Thread 3
>> ------------------+------------------+------------------
>>  1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
>>                   | 3: b.set(r1)     |  5: r3 = a.get()
>>
>> While there is (3) hb (4), it does not follow there is transitive (1)
>> hb
>> (5) edge.
>>
>> Under the JMM, the result values for (r2,r3) are:
>>   (0, 0) -- OK, no updates seen
>>   (0, 1) -- OK, early update of $a seen
>>   (1, 0) -- OK, update of $b is seen, but update to $a is not visible
>>   (1, 1) -- OK, both updates are seen
>>
>> Although (1, 0) clashes with intuitive model of global consistent
>> memory, and can be masked by TSO. This can be demonstrated by
>> java-concurrency-torture test [1]. Hey Jeff, see no boilerplate code
>> there ;)
>>
>> Run it on 4-way x86 (I have masked the irrelevant states with "x",
>> other
>> three are r1, r2, r3, respectively) for a minute:
>>
>>        Observed state  Occurrences  Expectation
>>    [x, x, 1, x, 0, 1] (  62630154)  ACCEPTABLE
>>    [x, x, 1, x, 0, 0] (  30019106)  ACCEPTABLE
>>    [x, x, 0, x, 0, 1] (  25647080)  ACCEPTABLE
>>    [x, x, 0, x, 0, 0] (  27678732)  ACCEPTABLE
>>    [x, x, 1, x, 1, 1] (  23509348)  ACCEPTABLE
>>    [x, x, 1, x, 1, 0] (         0)  ACCEPTABLE
>>
>> Note the latest case is always zero, meaning the (1, 0) case not
>> detected. It changes if you run it on 6-way PPC for a minute:
>>
>>        Observed state  Occurrences  Expectation
>>    [x, x, 1, x, 0, 1] (  51334629)  ACCEPTABLE
>>    [x, x, 1, x, 0, 0] (  33056225)  ACCEPTABLE
>>    [x, x, 0, x, 0, 1] (  11579848)  ACCEPTABLE
>>    [x, x, 0, x, 0, 0] (  51182907)  ACCEPTABLE
>>    [x, x, 1, x, 1, 1] (  15334538)  ACCEPTABLE
>>    [x, x, 1, x, 1, 0] (         3)  ACCEPTABLE
>>
>> Here, the (1,0) case although rare, but possible, because the PPC in
>> question is not TSO. Modulo I'm completely wrong, and this is the
>> JVM/test bug, of course. Hope this helps to unwind some of the
>> theoretical mess.
>>
>> -Aleksey.
>>
>> [1]
>> https://github.com/shipilev/java-concurrency-
>> torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/
>> volatiles/LazySetTransitivityTest.java
>>
>> On 01/23/2013 09:21 AM, Ruslan Cheremin wrote:
>> > Well, apart from lazySet semantics not being strictly defined,
>> doesn't
>> > assumed semantics is "A.lazySet() hb A.get() with value returned by
>> > get being same as written by lazySet"? And if it is -- can't I
>> > transitively continue this HB from first to third thread?
>> >
>> > Generally, I was sure total order was introduced in JMM for
>> > simplifying prove of it's overall correctness, but in most specific
>> > cases partial order is enough to prove SC.
>> >
>> >
>> >
>> >
>> > 2013/1/23 Vitaly Davidovich <vitalyd at gmail.com>:
>> >> By lazy set do you mean AtomicXXX.lazySet (or Unsafe.putXXXOrdered)?
>> Sorry,
>> >> can't open the attachment on my phone.
>> >>
>> >> Anyway, assuming it's the above, then yes there's no guarantee that
>> your
>> >> third thread sees updated A despite seeing updated B.  There's no
>> total
>> >> order established here if you're using lazy writes - all lazy writes
>> >> guarantee is that if a thread observes the lazy write, it observes
>> all
>> >> previous stores (by writing thread) as well.
>> >>
>> >> As for why you can't see this break, it could be a combo of:
>> >>
>> >> 1) enough time goes by in between for A to flush and be globally
>> visible by
>> >> the time third thread reads it
>> >> 2) there could be other synchronizing actions in between that make
>> writes
>> >> visible immediately
>> >> 3) your threads might be running on the cores doing the actual
>> writes so
>> >> observe their own stores
>> >>
>> >> Sent from my phone
>> >>
>> >> On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com>
>> wrote:
>> >>>
>> >>> Hello.
>> >>>
>> >>> A few months ago I've made a sort of Disruptor implementation,
>> >>> and in the process went into a frenzy of optimizations.
>> >>>
>> >>> Since all my tests were always passing, I stayed confident
>> >>> (test driven development at its worse ;), but as I recently
>> >>> realized, I ended up with what I think is broken code:
>> >>> in my frenzy I forgot about the basics of Disruptor
>> >>> (the volatile write and then read of a proper variable
>> >>> by proper threads to ensure event visibility).
>> >>>
>> >>> It's easily fixable, and the correction has almost no impact
>> >>> on performances for nbr of processors < nbr of cores.
>> >>> My problem is that I can't manage to prove my code broken
>> >>> with tests, and I don't understand why it "works".
>> >>>
>> >>> Here is the (afaik-non-founded) assumption that I
>> >>> (semi-unconsciously) made in the broken code:
>> >>> If a thread lazily-sets a volatile variable A, and then that
>> >>> another thread reads A and sees its new value, and then
>> >>> sets (lazily or not) a volatile variable B, then if a third
>> >>> thread reads B and sees its new value, if it reads A, it
>> >>> should also see its new value (or more particularly
>> >>> in the case of a ring buffer, if it reads a non-volatile
>> >>> variable that was set by the first thread just before the
>> >>> lazy-set of A, it should see its new value).
>> >>> (It might still be wrong if the first set is non-lazy,
>> >>> but laziness should make it worse.)
>> >>>
>> >>> I attached code with which I unsuccessfully attempted to
>> >>> prove this assumption wrong.
>> >>>
>> >>> Can anyone confirm that my assumption was wrong, and
>> >>> point out reasons why it could still "work" so often?
>> >>>
>> >>> -Jeff
>> >>>
>> >>>
>> >>> _______________________________________________
>> >>> Concurrency-interest mailing list
>> >>> Concurrency-interest at cs.oswego.edu
>> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathan.reynolds at oracle.com  Wed Jan 23 16:09:31 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 23 Jan 2013 14:09:31 -0700
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAOwENiJK_LHvbdg9EOz-F3-aAjaA9+SycaLCZpXxTaJe8zSLmQ@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
	<CAOwENiJK_LHvbdg9EOz-F3-aAjaA9+SycaLCZpXxTaJe8zSLmQ@mail.gmail.com>
Message-ID: <5100518B.7040601@oracle.com>

Sparc processors had 3 different memory models and the application could 
select which one it wants to use.  TSO won because programmers couldn't 
write bug free code with the more relaxed memory models.  From this 
experience, it seems the programming language should provide TSO 
guarantees but maybe the hardware could relax the memory model and the 
JVM deals with adding the appropriate fence instructions where needed.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 1/23/2013 1:46 PM, Ruslan Cheremin wrote:
> Hans, thank you for pointing on C memory model as an example of more
> precise something-like-lazySet specification.
>
> While we are at this topic -- it is question I looking for answer long
> ago: why at all we really need _total_ synchronization order in JMM? I
> mean what was the primary reason for introducing "backbone" of total
> ordered synchronized actions, and pair them with ribs of partial
> ordered ordinary actions? Could be consistent memory model build up
> with partial HB order only, with synchronized actions just introducing
> inter-thread HB, not total order? I think, new C model is something
> like this, but I haven't chance to dig deep into it yet, so not sure
> here. But I could recall some notes in this list about total SO being
> limit for scalability with number of cores grows (may be it was even
> your note, Hans).
>
>
>
> 2013/1/24 Boehm, Hans <hans.boehm at hp.com>:
>> Thanks for some interesting results.
>>
>> These are unfortunately at odds with my understanding of lazySet() semantics.  My understanding was that lazySet() corresponds to C++ memory_order_release and get() corresponds to memory_order_seq_cst.  Both lazySet() and set() have the same effect on happens-before.  In the interesting executions of the example below, statement 1 happens before statement 5.  Thus r3 must be 1.
>>
>> The difference between set() and lazySet() is whether the set operation participates in a total order of sequentially consistent operations, roughly analogous to Java synchronization order.
>>
>> Can you determine what PPC code is generated for get() and lazySet().  Is it consistent with the recipes in http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html?
>>
>> In particular:
>>
>> get() should have at least a trailing cmp; bc; isync or lwsync.  (It should also have a leading hwsync, though I don't see how that can matter for this test case, since C++ memory_order_acquire should suffice here.)
>>
>> lazySet() should have a leading lwsync.
>>
>> My hope is that this is a JVM bug (or conceivably even a hardware bug), and the equivalence with C++ memory_order_acquire is intended to hold.  I personally wouldn't consider a lazySet() that doesn't participate in happens-before to be usable.  I think it would, for example, make internal parallelism in library routines, particularly constructors, visible to clients, and would require the library to document which effects are performed in the calling threads, vs. just happening before the return.
>>
>> Hans
>>
>>> -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
>>> interest-bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
>>> Sent: Wednesday, January 23, 2013 4:23 AM
>>> To: Ruslan Cheremin
>>> Cc: concurrency-interest at cs.oswego.edu
>>> Subject: Re: [concurrency-interest] volatile acrobacy
>>>
>>> I think both Jeff and Ruslan are in the "wishful thinking" land. ;)
>>>
>>>       Thread 1     |    Thread 2      |     Thread 3
>>> ------------------+------------------+------------------
>>>   1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
>>>                    | 3: b.set(r1)     |  5: r3 = a.get()
>>>
>>> While there is (3) hb (4), it does not follow there is transitive (1)
>>> hb
>>> (5) edge.
>>>
>>> Under the JMM, the result values for (r2,r3) are:
>>>    (0, 0) -- OK, no updates seen
>>>    (0, 1) -- OK, early update of $a seen
>>>    (1, 0) -- OK, update of $b is seen, but update to $a is not visible
>>>    (1, 1) -- OK, both updates are seen
>>>
>>> Although (1, 0) clashes with intuitive model of global consistent
>>> memory, and can be masked by TSO. This can be demonstrated by
>>> java-concurrency-torture test [1]. Hey Jeff, see no boilerplate code
>>> there ;)
>>>
>>> Run it on 4-way x86 (I have masked the irrelevant states with "x",
>>> other
>>> three are r1, r2, r3, respectively) for a minute:
>>>
>>>         Observed state  Occurrences  Expectation
>>>     [x, x, 1, x, 0, 1] (  62630154)  ACCEPTABLE
>>>     [x, x, 1, x, 0, 0] (  30019106)  ACCEPTABLE
>>>     [x, x, 0, x, 0, 1] (  25647080)  ACCEPTABLE
>>>     [x, x, 0, x, 0, 0] (  27678732)  ACCEPTABLE
>>>     [x, x, 1, x, 1, 1] (  23509348)  ACCEPTABLE
>>>     [x, x, 1, x, 1, 0] (         0)  ACCEPTABLE
>>>
>>> Note the latest case is always zero, meaning the (1, 0) case not
>>> detected. It changes if you run it on 6-way PPC for a minute:
>>>
>>>         Observed state  Occurrences  Expectation
>>>     [x, x, 1, x, 0, 1] (  51334629)  ACCEPTABLE
>>>     [x, x, 1, x, 0, 0] (  33056225)  ACCEPTABLE
>>>     [x, x, 0, x, 0, 1] (  11579848)  ACCEPTABLE
>>>     [x, x, 0, x, 0, 0] (  51182907)  ACCEPTABLE
>>>     [x, x, 1, x, 1, 1] (  15334538)  ACCEPTABLE
>>>     [x, x, 1, x, 1, 0] (         3)  ACCEPTABLE
>>>
>>> Here, the (1,0) case although rare, but possible, because the PPC in
>>> question is not TSO. Modulo I'm completely wrong, and this is the
>>> JVM/test bug, of course. Hope this helps to unwind some of the
>>> theoretical mess.
>>>
>>> -Aleksey.
>>>
>>> [1]
>>> https://github.com/shipilev/java-concurrency-
>>> torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/
>>> volatiles/LazySetTransitivityTest.java
>>>
>>> On 01/23/2013 09:21 AM, Ruslan Cheremin wrote:
>>>> Well, apart from lazySet semantics not being strictly defined,
>>> doesn't
>>>> assumed semantics is "A.lazySet() hb A.get() with value returned by
>>>> get being same as written by lazySet"? And if it is -- can't I
>>>> transitively continue this HB from first to third thread?
>>>>
>>>> Generally, I was sure total order was introduced in JMM for
>>>> simplifying prove of it's overall correctness, but in most specific
>>>> cases partial order is enough to prove SC.
>>>>
>>>>
>>>>
>>>>
>>>> 2013/1/23 Vitaly Davidovich <vitalyd at gmail.com>:
>>>>> By lazy set do you mean AtomicXXX.lazySet (or Unsafe.putXXXOrdered)?
>>> Sorry,
>>>>> can't open the attachment on my phone.
>>>>>
>>>>> Anyway, assuming it's the above, then yes there's no guarantee that
>>> your
>>>>> third thread sees updated A despite seeing updated B.  There's no
>>> total
>>>>> order established here if you're using lazy writes - all lazy writes
>>>>> guarantee is that if a thread observes the lazy write, it observes
>>> all
>>>>> previous stores (by writing thread) as well.
>>>>>
>>>>> As for why you can't see this break, it could be a combo of:
>>>>>
>>>>> 1) enough time goes by in between for A to flush and be globally
>>> visible by
>>>>> the time third thread reads it
>>>>> 2) there could be other synchronizing actions in between that make
>>> writes
>>>>> visible immediately
>>>>> 3) your threads might be running on the cores doing the actual
>>> writes so
>>>>> observe their own stores
>>>>>
>>>>> Sent from my phone
>>>>>
>>>>> On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com>
>>> wrote:
>>>>>> Hello.
>>>>>>
>>>>>> A few months ago I've made a sort of Disruptor implementation,
>>>>>> and in the process went into a frenzy of optimizations.
>>>>>>
>>>>>> Since all my tests were always passing, I stayed confident
>>>>>> (test driven development at its worse ;), but as I recently
>>>>>> realized, I ended up with what I think is broken code:
>>>>>> in my frenzy I forgot about the basics of Disruptor
>>>>>> (the volatile write and then read of a proper variable
>>>>>> by proper threads to ensure event visibility).
>>>>>>
>>>>>> It's easily fixable, and the correction has almost no impact
>>>>>> on performances for nbr of processors < nbr of cores.
>>>>>> My problem is that I can't manage to prove my code broken
>>>>>> with tests, and I don't understand why it "works".
>>>>>>
>>>>>> Here is the (afaik-non-founded) assumption that I
>>>>>> (semi-unconsciously) made in the broken code:
>>>>>> If a thread lazily-sets a volatile variable A, and then that
>>>>>> another thread reads A and sees its new value, and then
>>>>>> sets (lazily or not) a volatile variable B, then if a third
>>>>>> thread reads B and sees its new value, if it reads A, it
>>>>>> should also see its new value (or more particularly
>>>>>> in the case of a ring buffer, if it reads a non-volatile
>>>>>> variable that was set by the first thread just before the
>>>>>> lazy-set of A, it should see its new value).
>>>>>> (It might still be wrong if the first set is non-lazy,
>>>>>> but laziness should make it worse.)
>>>>>>
>>>>>> I attached code with which I unsuccessfully attempted to
>>>>>> prove this assumption wrong.
>>>>>>
>>>>>> Can anyone confirm that my assumption was wrong, and
>>>>>> point out reasons why it could still "work" so often?
>>>>>>
>>>>>> -Jeff
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130123/c5ef2f6d/attachment-0001.html>

From aleksey.shipilev at oracle.com  Wed Jan 23 16:27:42 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 24 Jan 2013 01:27:42 +0400
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
Message-ID: <510055CE.4080605@oracle.com>

On 01/24/2013 12:08 AM, Boehm, Hans wrote:
> Thanks for some interesting results.

I think the name "concurrency-torture" is rather successful, because
sitting at 1:30am trying to figure out whether the simple test is wrong
is... well... :)

> These are unfortunately at odds with my understanding of lazySet()
> semantics.  My understanding was that lazySet() corresponds to C++
> memory_order_release and get() corresponds to memory_order_seq_cst.
> Both lazySet() and set() have the same effect on happens-before.  In
> the interesting executions of the example below, statement 1 happens
> before statement 5.  Thus r3 must be 1.

Damn, at this point I've got to admit I was confused over semantics
again. Yes, the (r2, r3) = (1, 0) is incorrect, I've adjusted the test
grading accordingly.

> Can you determine what PPC code is generated for get() and lazySet().
> Is it consistent with the recipes in
> http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html?

Good idea. That PPC port is young enough to have severe bugs.
Unfortunately, there can be some substantial time before I can look at
that again. Alternatively, this could be the bug in the test
infrastructure (and no soul had volunteered to make the runs on PPC yet!)

-Aleksey.

From hans.boehm at hp.com  Wed Jan 23 16:57:18 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 23 Jan 2013 21:57:18 +0000
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAOwENiJK_LHvbdg9EOz-F3-aAjaA9+SycaLCZpXxTaJe8zSLmQ@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
	<CAOwENiJK_LHvbdg9EOz-F3-aAjaA9+SycaLCZpXxTaJe8zSLmQ@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369787A0@G9W0725.americas.hpqcorp.net>

You need something like a total synchronization order to make examples like Dekker's (and IRIW) work with volatiles:

x,y volatile

Thread 1:
x = 1;
r1 = y;

Thread 2:
y = 1;
r2 = x;

r1 = r2 = 0 is happens-before consistent.  In that setting, there are no synchronizes-with relationships (except possibly with the initialization code).  Thus in the simple happens-before model, both loads can see the initializing store of zero.  This is bad for Dekker's mutual exclusion algorithm (and a bunch of other algorithms that actually matter).

We want to disallow that.  But my understanding of lazySet is that it was introduced precisely to again allow this outcome, avoiding the extra fence usually needed to enforce it for those cases that don't care.  This relaxation has nothing to do with happens before as defined in the Java memory model.

C++ also has an analogous total order, but it takes a somewhat different form.  See 29.3p3 in N3376.

Hans

> -----Original Message-----
> From: Ruslan Cheremin [mailto:cheremin at gmail.com]
> Sent: Wednesday, January 23, 2013 12:47 PM
> To: Boehm, Hans
> Cc: Aleksey Shipilev; concurrency-interest at cs.oswego.edu;
> Peter.Sewell at cl.cam.ac.uk
> Subject: Re: [concurrency-interest] volatile acrobacy
> 
> Hans, thank you for pointing on C memory model as an example of more
> precise something-like-lazySet specification.
> 
> While we are at this topic -- it is question I looking for answer long
> ago: why at all we really need _total_ synchronization order in JMM? I
> mean what was the primary reason for introducing "backbone" of total
> ordered synchronized actions, and pair them with ribs of partial
> ordered ordinary actions? Could be consistent memory model build up
> with partial HB order only, with synchronized actions just introducing
> inter-thread HB, not total order? I think, new C model is something
> like this, but I haven't chance to dig deep into it yet, so not sure
> here. But I could recall some notes in this list about total SO being
> limit for scalability with number of cores grows (may be it was even
> your note, Hans).
> 
> 
> 
> 2013/1/24 Boehm, Hans <hans.boehm at hp.com>:
> > Thanks for some interesting results.
> >
> > These are unfortunately at odds with my understanding of lazySet()
> semantics.  My understanding was that lazySet() corresponds to C++
> memory_order_release and get() corresponds to memory_order_seq_cst.
> Both lazySet() and set() have the same effect on happens-before.  In
> the interesting executions of the example below, statement 1 happens
> before statement 5.  Thus r3 must be 1.
> >
> > The difference between set() and lazySet() is whether the set
> operation participates in a total order of sequentially consistent
> operations, roughly analogous to Java synchronization order.
> >
> > Can you determine what PPC code is generated for get() and lazySet().
> Is it consistent with the recipes in
> http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html?
> >
> > In particular:
> >
> > get() should have at least a trailing cmp; bc; isync or lwsync.  (It
> should also have a leading hwsync, though I don't see how that can
> matter for this test case, since C++ memory_order_acquire should
> suffice here.)
> >
> > lazySet() should have a leading lwsync.
> >
> > My hope is that this is a JVM bug (or conceivably even a hardware
> bug), and the equivalence with C++ memory_order_acquire is intended to
> hold.  I personally wouldn't consider a lazySet() that doesn't
> participate in happens-before to be usable.  I think it would, for
> example, make internal parallelism in library routines, particularly
> constructors, visible to clients, and would require the library to
> document which effects are performed in the calling threads, vs. just
> happening before the return.
> >
> > Hans
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-
> >> interest-bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
> >> Sent: Wednesday, January 23, 2013 4:23 AM
> >> To: Ruslan Cheremin
> >> Cc: concurrency-interest at cs.oswego.edu
> >> Subject: Re: [concurrency-interest] volatile acrobacy
> >>
> >> I think both Jeff and Ruslan are in the "wishful thinking" land. ;)
> >>
> >>      Thread 1     |    Thread 2      |     Thread 3
> >> ------------------+------------------+------------------
> >>  1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
> >>                   | 3: b.set(r1)     |  5: r3 = a.get()
> >>
> >> While there is (3) hb (4), it does not follow there is transitive
> (1)
> >> hb
> >> (5) edge.
> >>
> >> Under the JMM, the result values for (r2,r3) are:
> >>   (0, 0) -- OK, no updates seen
> >>   (0, 1) -- OK, early update of $a seen
> >>   (1, 0) -- OK, update of $b is seen, but update to $a is not
> visible
> >>   (1, 1) -- OK, both updates are seen
> >>
> >> Although (1, 0) clashes with intuitive model of global consistent
> >> memory, and can be masked by TSO. This can be demonstrated by
> >> java-concurrency-torture test [1]. Hey Jeff, see no boilerplate code
> >> there ;)
> >>
> >> Run it on 4-way x86 (I have masked the irrelevant states with "x",
> >> other
> >> three are r1, r2, r3, respectively) for a minute:
> >>
> >>        Observed state  Occurrences  Expectation
> >>    [x, x, 1, x, 0, 1] (  62630154)  ACCEPTABLE
> >>    [x, x, 1, x, 0, 0] (  30019106)  ACCEPTABLE
> >>    [x, x, 0, x, 0, 1] (  25647080)  ACCEPTABLE
> >>    [x, x, 0, x, 0, 0] (  27678732)  ACCEPTABLE
> >>    [x, x, 1, x, 1, 1] (  23509348)  ACCEPTABLE
> >>    [x, x, 1, x, 1, 0] (         0)  ACCEPTABLE
> >>
> >> Note the latest case is always zero, meaning the (1, 0) case not
> >> detected. It changes if you run it on 6-way PPC for a minute:
> >>
> >>        Observed state  Occurrences  Expectation
> >>    [x, x, 1, x, 0, 1] (  51334629)  ACCEPTABLE
> >>    [x, x, 1, x, 0, 0] (  33056225)  ACCEPTABLE
> >>    [x, x, 0, x, 0, 1] (  11579848)  ACCEPTABLE
> >>    [x, x, 0, x, 0, 0] (  51182907)  ACCEPTABLE
> >>    [x, x, 1, x, 1, 1] (  15334538)  ACCEPTABLE
> >>    [x, x, 1, x, 1, 0] (         3)  ACCEPTABLE
> >>
> >> Here, the (1,0) case although rare, but possible, because the PPC in
> >> question is not TSO. Modulo I'm completely wrong, and this is the
> >> JVM/test bug, of course. Hope this helps to unwind some of the
> >> theoretical mess.
> >>
> >> -Aleksey.
> >>
> >> [1]
> >> https://github.com/shipilev/java-concurrency-
> >>
> torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/
> >> volatiles/LazySetTransitivityTest.java
> >>
> >> On 01/23/2013 09:21 AM, Ruslan Cheremin wrote:
> >> > Well, apart from lazySet semantics not being strictly defined,
> >> doesn't
> >> > assumed semantics is "A.lazySet() hb A.get() with value returned
> by
> >> > get being same as written by lazySet"? And if it is -- can't I
> >> > transitively continue this HB from first to third thread?
> >> >
> >> > Generally, I was sure total order was introduced in JMM for
> >> > simplifying prove of it's overall correctness, but in most
> specific
> >> > cases partial order is enough to prove SC.
> >> >
> >> >
> >> >
> >> >
> >> > 2013/1/23 Vitaly Davidovich <vitalyd at gmail.com>:
> >> >> By lazy set do you mean AtomicXXX.lazySet (or
> Unsafe.putXXXOrdered)?
> >> Sorry,
> >> >> can't open the attachment on my phone.
> >> >>
> >> >> Anyway, assuming it's the above, then yes there's no guarantee
> that
> >> your
> >> >> third thread sees updated A despite seeing updated B.  There's no
> >> total
> >> >> order established here if you're using lazy writes - all lazy
> writes
> >> >> guarantee is that if a thread observes the lazy write, it
> observes
> >> all
> >> >> previous stores (by writing thread) as well.
> >> >>
> >> >> As for why you can't see this break, it could be a combo of:
> >> >>
> >> >> 1) enough time goes by in between for A to flush and be globally
> >> visible by
> >> >> the time third thread reads it
> >> >> 2) there could be other synchronizing actions in between that
> make
> >> writes
> >> >> visible immediately
> >> >> 3) your threads might be running on the cores doing the actual
> >> writes so
> >> >> observe their own stores
> >> >>
> >> >> Sent from my phone
> >> >>
> >> >> On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com>
> >> wrote:
> >> >>>
> >> >>> Hello.
> >> >>>
> >> >>> A few months ago I've made a sort of Disruptor implementation,
> >> >>> and in the process went into a frenzy of optimizations.
> >> >>>
> >> >>> Since all my tests were always passing, I stayed confident
> >> >>> (test driven development at its worse ;), but as I recently
> >> >>> realized, I ended up with what I think is broken code:
> >> >>> in my frenzy I forgot about the basics of Disruptor
> >> >>> (the volatile write and then read of a proper variable
> >> >>> by proper threads to ensure event visibility).
> >> >>>
> >> >>> It's easily fixable, and the correction has almost no impact
> >> >>> on performances for nbr of processors < nbr of cores.
> >> >>> My problem is that I can't manage to prove my code broken
> >> >>> with tests, and I don't understand why it "works".
> >> >>>
> >> >>> Here is the (afaik-non-founded) assumption that I
> >> >>> (semi-unconsciously) made in the broken code:
> >> >>> If a thread lazily-sets a volatile variable A, and then that
> >> >>> another thread reads A and sees its new value, and then
> >> >>> sets (lazily or not) a volatile variable B, then if a third
> >> >>> thread reads B and sees its new value, if it reads A, it
> >> >>> should also see its new value (or more particularly
> >> >>> in the case of a ring buffer, if it reads a non-volatile
> >> >>> variable that was set by the first thread just before the
> >> >>> lazy-set of A, it should see its new value).
> >> >>> (It might still be wrong if the first set is non-lazy,
> >> >>> but laziness should make it worse.)
> >> >>>
> >> >>> I attached code with which I unsuccessfully attempted to
> >> >>> prove this assumption wrong.
> >> >>>
> >> >>> Can anyone confirm that my assumption was wrong, and
> >> >>> point out reasons why it could still "work" so often?
> >> >>>
> >> >>> -Jeff
> >> >>>
> >> >>>
> >> >>> _______________________________________________
> >> >>> Concurrency-interest mailing list
> >> >>> Concurrency-interest at cs.oswego.edu
> >> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >>>
> >> >>
> >> >> _______________________________________________
> >> >> Concurrency-interest mailing list
> >> >> Concurrency-interest at cs.oswego.edu
> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >>
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From hans.boehm at hp.com  Wed Jan 23 17:04:42 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 23 Jan 2013 22:04:42 +0000
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <CAG65PLpQtt+PxGe5wkSvwZLmAS7fLAa7pt+YOsJJ+zskTCwQOQ@mail.gmail.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
	<CAG65PLpQtt+PxGe5wkSvwZLmAS7fLAa7pt+YOsJJ+zskTCwQOQ@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369787BB@G9W0725.americas.hpqcorp.net>

That's definitely correct.  TSO architectures outlaw this kind of behavior, but PPC allows it.  At the machine level things can be messy.  And that's clearly visible for ordinary racing accesses in Java.  That's one reason you should avoid them if at all possible.  But my understanding is that lazySet() was designed to isolate you from this sort of ugliness (and a few others).

Hans

> -----Original Message-----
> From: Adam Browning [mailto:adbrowning at gmail.com]
> Sent: Wednesday, January 23, 2013 1:08 PM
> To: Boehm, Hans
> Cc: Aleksey Shipilev; Ruslan Cheremin; Peter.Sewell at cl.cam.ac.uk;
> concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] volatile acrobacy
> 
> I believe that one way you can lose the transitive nature of the
> happens-before relationship is if two threads are sharing a cache and
> the third is not. So thread1 does a lazySet of a value, thread2 reads
> that value and just happens to read it from the same cache to which
> the processor of thread1 wrote it. thread2 then does a write to a
> variable in a  different cache line and issues a barrier that causes
> that value to be flushed to the other processors. If the first cache
> line isn't also flushed, then thread3 can see thread2's change, but
> not thread1's change.
> 
> On Wed, Jan 23, 2013 at 3:08 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
> > Thanks for some interesting results.
> >
> > These are unfortunately at odds with my understanding of lazySet()
> semantics.  My understanding was that lazySet() corresponds to C++
> memory_order_release and get() corresponds to memory_order_seq_cst.
> Both lazySet() and set() have the same effect on happens-before.  In
> the interesting executions of the example below, statement 1 happens
> before statement 5.  Thus r3 must be 1.
> >
> > The difference between set() and lazySet() is whether the set
> operation participates in a total order of sequentially consistent
> operations, roughly analogous to Java synchronization order.
> >
> > Can you determine what PPC code is generated for get() and lazySet().
> Is it consistent with the recipes in
> http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html?
> >
> > In particular:
> >
> > get() should have at least a trailing cmp; bc; isync or lwsync.  (It
> should also have a leading hwsync, though I don't see how that can
> matter for this test case, since C++ memory_order_acquire should
> suffice here.)
> >
> > lazySet() should have a leading lwsync.
> >
> > My hope is that this is a JVM bug (or conceivably even a hardware
> bug), and the equivalence with C++ memory_order_acquire is intended to
> hold.  I personally wouldn't consider a lazySet() that doesn't
> participate in happens-before to be usable.  I think it would, for
> example, make internal parallelism in library routines, particularly
> constructors, visible to clients, and would require the library to
> document which effects are performed in the calling threads, vs. just
> happening before the return.
> >
> > Hans
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-
> >> interest-bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
> >> Sent: Wednesday, January 23, 2013 4:23 AM
> >> To: Ruslan Cheremin
> >> Cc: concurrency-interest at cs.oswego.edu
> >> Subject: Re: [concurrency-interest] volatile acrobacy
> >>
> >> I think both Jeff and Ruslan are in the "wishful thinking" land. ;)
> >>
> >>      Thread 1     |    Thread 2      |     Thread 3
> >> ------------------+------------------+------------------
> >>  1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()
> >>                   | 3: b.set(r1)     |  5: r3 = a.get()
> >>
> >> While there is (3) hb (4), it does not follow there is transitive
> (1)
> >> hb
> >> (5) edge.
> >>
> >> Under the JMM, the result values for (r2,r3) are:
> >>   (0, 0) -- OK, no updates seen
> >>   (0, 1) -- OK, early update of $a seen
> >>   (1, 0) -- OK, update of $b is seen, but update to $a is not
> visible
> >>   (1, 1) -- OK, both updates are seen
> >>
> >> Although (1, 0) clashes with intuitive model of global consistent
> >> memory, and can be masked by TSO. This can be demonstrated by
> >> java-concurrency-torture test [1]. Hey Jeff, see no boilerplate code
> >> there ;)
> >>
> >> Run it on 4-way x86 (I have masked the irrelevant states with "x",
> >> other
> >> three are r1, r2, r3, respectively) for a minute:
> >>
> >>        Observed state  Occurrences  Expectation
> >>    [x, x, 1, x, 0, 1] (  62630154)  ACCEPTABLE
> >>    [x, x, 1, x, 0, 0] (  30019106)  ACCEPTABLE
> >>    [x, x, 0, x, 0, 1] (  25647080)  ACCEPTABLE
> >>    [x, x, 0, x, 0, 0] (  27678732)  ACCEPTABLE
> >>    [x, x, 1, x, 1, 1] (  23509348)  ACCEPTABLE
> >>    [x, x, 1, x, 1, 0] (         0)  ACCEPTABLE
> >>
> >> Note the latest case is always zero, meaning the (1, 0) case not
> >> detected. It changes if you run it on 6-way PPC for a minute:
> >>
> >>        Observed state  Occurrences  Expectation
> >>    [x, x, 1, x, 0, 1] (  51334629)  ACCEPTABLE
> >>    [x, x, 1, x, 0, 0] (  33056225)  ACCEPTABLE
> >>    [x, x, 0, x, 0, 1] (  11579848)  ACCEPTABLE
> >>    [x, x, 0, x, 0, 0] (  51182907)  ACCEPTABLE
> >>    [x, x, 1, x, 1, 1] (  15334538)  ACCEPTABLE
> >>    [x, x, 1, x, 1, 0] (         3)  ACCEPTABLE
> >>
> >> Here, the (1,0) case although rare, but possible, because the PPC in
> >> question is not TSO. Modulo I'm completely wrong, and this is the
> >> JVM/test bug, of course. Hope this helps to unwind some of the
> >> theoretical mess.
> >>
> >> -Aleksey.
> >>
> >> [1]
> >> https://github.com/shipilev/java-concurrency-
> >>
> torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/
> >> volatiles/LazySetTransitivityTest.java
> >>
> >> On 01/23/2013 09:21 AM, Ruslan Cheremin wrote:
> >> > Well, apart from lazySet semantics not being strictly defined,
> >> doesn't
> >> > assumed semantics is "A.lazySet() hb A.get() with value returned
> by
> >> > get being same as written by lazySet"? And if it is -- can't I
> >> > transitively continue this HB from first to third thread?
> >> >
> >> > Generally, I was sure total order was introduced in JMM for
> >> > simplifying prove of it's overall correctness, but in most
> specific
> >> > cases partial order is enough to prove SC.
> >> >
> >> >
> >> >
> >> >
> >> > 2013/1/23 Vitaly Davidovich <vitalyd at gmail.com>:
> >> >> By lazy set do you mean AtomicXXX.lazySet (or
> Unsafe.putXXXOrdered)?
> >> Sorry,
> >> >> can't open the attachment on my phone.
> >> >>
> >> >> Anyway, assuming it's the above, then yes there's no guarantee
> that
> >> your
> >> >> third thread sees updated A despite seeing updated B.  There's no
> >> total
> >> >> order established here if you're using lazy writes - all lazy
> writes
> >> >> guarantee is that if a thread observes the lazy write, it
> observes
> >> all
> >> >> previous stores (by writing thread) as well.
> >> >>
> >> >> As for why you can't see this break, it could be a combo of:
> >> >>
> >> >> 1) enough time goes by in between for A to flush and be globally
> >> visible by
> >> >> the time third thread reads it
> >> >> 2) there could be other synchronizing actions in between that
> make
> >> writes
> >> >> visible immediately
> >> >> 3) your threads might be running on the cores doing the actual
> >> writes so
> >> >> observe their own stores
> >> >>
> >> >> Sent from my phone
> >> >>
> >> >> On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com>
> >> wrote:
> >> >>>
> >> >>> Hello.
> >> >>>
> >> >>> A few months ago I've made a sort of Disruptor implementation,
> >> >>> and in the process went into a frenzy of optimizations.
> >> >>>
> >> >>> Since all my tests were always passing, I stayed confident
> >> >>> (test driven development at its worse ;), but as I recently
> >> >>> realized, I ended up with what I think is broken code:
> >> >>> in my frenzy I forgot about the basics of Disruptor
> >> >>> (the volatile write and then read of a proper variable
> >> >>> by proper threads to ensure event visibility).
> >> >>>
> >> >>> It's easily fixable, and the correction has almost no impact
> >> >>> on performances for nbr of processors < nbr of cores.
> >> >>> My problem is that I can't manage to prove my code broken
> >> >>> with tests, and I don't understand why it "works".
> >> >>>
> >> >>> Here is the (afaik-non-founded) assumption that I
> >> >>> (semi-unconsciously) made in the broken code:
> >> >>> If a thread lazily-sets a volatile variable A, and then that
> >> >>> another thread reads A and sees its new value, and then
> >> >>> sets (lazily or not) a volatile variable B, then if a third
> >> >>> thread reads B and sees its new value, if it reads A, it
> >> >>> should also see its new value (or more particularly
> >> >>> in the case of a ring buffer, if it reads a non-volatile
> >> >>> variable that was set by the first thread just before the
> >> >>> lazy-set of A, it should see its new value).
> >> >>> (It might still be wrong if the first set is non-lazy,
> >> >>> but laziness should make it worse.)
> >> >>>
> >> >>> I attached code with which I unsuccessfully attempted to
> >> >>> prove this assumption wrong.
> >> >>>
> >> >>> Can anyone confirm that my assumption was wrong, and
> >> >>> point out reasons why it could still "work" so often?
> >> >>>
> >> >>> -Jeff
> >> >>>
> >> >>>
> >> >>> _______________________________________________
> >> >>> Concurrency-interest mailing list
> >> >>> Concurrency-interest at cs.oswego.edu
> >> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >>>
> >> >>
> >> >> _______________________________________________
> >> >> Concurrency-interest mailing list
> >> >> Concurrency-interest at cs.oswego.edu
> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >>
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From hans.boehm at hp.com  Wed Jan 23 17:18:11 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 23 Jan 2013 22:18:11 +0000
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <5100518B.7040601@oracle.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
	<CAOwENiJK_LHvbdg9EOz-F3-aAjaA9+SycaLCZpXxTaJe8zSLmQ@mail.gmail.com>
	<5100518B.7040601@oracle.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369787D7@G9W0725.americas.hpqcorp.net>

Currently we have a model where the hardware can have a weak memory model, but various synchronization operations add fences or the like.

In spite of the SPARC experience, I'm personally unconvinced that if we got to design language and hardware from scratch, that enforcing a strong memory model like TSO for ordinary memory accesses is worth the compiler and hardware cost.  Even in a TSO model, data races are a mess because you have to reason about data access granularity.  You really just want to avoid them, at which point TSO doesn't matter.  If I can get the hardware to help me, I'd much rather have it detect and report data races rather than having the data race bugs  produce more predictable outcomes.  Whether I can do that at the same cost as TSO is TBD.

But that's all a different issue from the question of what the Java memory model is and should be, since we get to design neither the language nor the hardware from scratch.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nathan Reynolds
Sent: Wednesday, January 23, 2013 1:10 PM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] volatile acrobacy

Sparc processors had 3 different memory models and the application could select which one it wants to use.  TSO won because programmers couldn't write bug free code with the more relaxed memory models.  From this experience, it seems the programming language should provide TSO guarantees but maybe the hardware could relax the memory model and the JVM deals with adding the appropriate fence instructions where needed.
Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | Architect | 602.333.9091
Oracle PSR Engineering<http://psr.us.oracle.com/> | Server Technology
On 1/23/2013 1:46 PM, Ruslan Cheremin wrote:

Hans, thank you for pointing on C memory model as an example of more

precise something-like-lazySet specification.



While we are at this topic -- it is question I looking for answer long

ago: why at all we really need _total_ synchronization order in JMM? I

mean what was the primary reason for introducing "backbone" of total

ordered synchronized actions, and pair them with ribs of partial

ordered ordinary actions? Could be consistent memory model build up

with partial HB order only, with synchronized actions just introducing

inter-thread HB, not total order? I think, new C model is something

like this, but I haven't chance to dig deep into it yet, so not sure

here. But I could recall some notes in this list about total SO being

limit for scalability with number of cores grows (may be it was even

your note, Hans).







2013/1/24 Boehm, Hans <hans.boehm at hp.com><mailto:hans.boehm at hp.com>:

Thanks for some interesting results.



These are unfortunately at odds with my understanding of lazySet() semantics.  My understanding was that lazySet() corresponds to C++ memory_order_release and get() corresponds to memory_order_seq_cst.  Both lazySet() and set() have the same effect on happens-before.  In the interesting executions of the example below, statement 1 happens before statement 5.  Thus r3 must be 1.



The difference between set() and lazySet() is whether the set operation participates in a total order of sequentially consistent operations, roughly analogous to Java synchronization order.



Can you determine what PPC code is generated for get() and lazySet().  Is it consistent with the recipes in http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html?



In particular:



get() should have at least a trailing cmp; bc; isync or lwsync.  (It should also have a leading hwsync, though I don't see how that can matter for this test case, since C++ memory_order_acquire should suffice here.)



lazySet() should have a leading lwsync.



My hope is that this is a JVM bug (or conceivably even a hardware bug), and the equivalence with C++ memory_order_acquire is intended to hold.  I personally wouldn't consider a lazySet() that doesn't participate in happens-before to be usable.  I think it would, for example, make internal parallelism in library routines, particularly constructors, visible to clients, and would require the library to document which effects are performed in the calling threads, vs. just happening before the return.



Hans



-----Original Message-----

From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-

interest-bounces at cs.oswego.edu<mailto:interest-bounces at cs.oswego.edu>] On Behalf Of Aleksey Shipilev

Sent: Wednesday, January 23, 2013 4:23 AM

To: Ruslan Cheremin

Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>

Subject: Re: [concurrency-interest] volatile acrobacy



I think both Jeff and Ruslan are in the "wishful thinking" land. ;)



     Thread 1     |    Thread 2      |     Thread 3

------------------+------------------+------------------

 1: a.lazySet(1)  | 2: r1 = a.get()  |  4: r2 = b.get()

                  | 3: b.set(r1)     |  5: r3 = a.get()



While there is (3) hb (4), it does not follow there is transitive (1)

hb

(5) edge.



Under the JMM, the result values for (r2,r3) are:

  (0, 0) -- OK, no updates seen

  (0, 1) -- OK, early update of $a seen

  (1, 0) -- OK, update of $b is seen, but update to $a is not visible

  (1, 1) -- OK, both updates are seen



Although (1, 0) clashes with intuitive model of global consistent

memory, and can be masked by TSO. This can be demonstrated by

java-concurrency-torture test [1]. Hey Jeff, see no boilerplate code

there ;)



Run it on 4-way x86 (I have masked the irrelevant states with "x",

other

three are r1, r2, r3, respectively) for a minute:



       Observed state  Occurrences  Expectation

   [x, x, 1, x, 0, 1] (  62630154)  ACCEPTABLE

   [x, x, 1, x, 0, 0] (  30019106)  ACCEPTABLE

   [x, x, 0, x, 0, 1] (  25647080)  ACCEPTABLE

   [x, x, 0, x, 0, 0] (  27678732)  ACCEPTABLE

   [x, x, 1, x, 1, 1] (  23509348)  ACCEPTABLE

   [x, x, 1, x, 1, 0] (         0)  ACCEPTABLE



Note the latest case is always zero, meaning the (1, 0) case not

detected. It changes if you run it on 6-way PPC for a minute:



       Observed state  Occurrences  Expectation

   [x, x, 1, x, 0, 1] (  51334629)  ACCEPTABLE

   [x, x, 1, x, 0, 0] (  33056225)  ACCEPTABLE

   [x, x, 0, x, 0, 1] (  11579848)  ACCEPTABLE

   [x, x, 0, x, 0, 0] (  51182907)  ACCEPTABLE

   [x, x, 1, x, 1, 1] (  15334538)  ACCEPTABLE

   [x, x, 1, x, 1, 0] (         3)  ACCEPTABLE



Here, the (1,0) case although rare, but possible, because the PPC in

question is not TSO. Modulo I'm completely wrong, and this is the

JVM/test bug, of course. Hope this helps to unwind some of the

theoretical mess.



-Aleksey.



[1]

https://github.com/shipilev/java-concurrency-

torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/

volatiles/LazySetTransitivityTest.java



On 01/23/2013 09:21 AM, Ruslan Cheremin wrote:

Well, apart from lazySet semantics not being strictly defined,

doesn't

assumed semantics is "A.lazySet() hb A.get() with value returned by

get being same as written by lazySet"? And if it is -- can't I

transitively continue this HB from first to third thread?



Generally, I was sure total order was introduced in JMM for

simplifying prove of it's overall correctness, but in most specific

cases partial order is enough to prove SC.









2013/1/23 Vitaly Davidovich <vitalyd at gmail.com><mailto:vitalyd at gmail.com>:

By lazy set do you mean AtomicXXX.lazySet (or Unsafe.putXXXOrdered)?

Sorry,

can't open the attachment on my phone.



Anyway, assuming it's the above, then yes there's no guarantee that

your

third thread sees updated A despite seeing updated B.  There's no

total

order established here if you're using lazy writes - all lazy writes

guarantee is that if a thread observes the lazy write, it observes

all

previous stores (by writing thread) as well.



As for why you can't see this break, it could be a combo of:



1) enough time goes by in between for A to flush and be globally

visible by

the time third thread reads it

2) there could be other synchronizing actions in between that make

writes

visible immediately

3) your threads might be running on the cores doing the actual

writes so

observe their own stores



Sent from my phone



On Jan 22, 2013 6:32 PM, "Jeff Hain" <jeffhain at rocketmail.com><mailto:jeffhain at rocketmail.com>

wrote:



Hello.



A few months ago I've made a sort of Disruptor implementation,

and in the process went into a frenzy of optimizations.



Since all my tests were always passing, I stayed confident

(test driven development at its worse ;), but as I recently

realized, I ended up with what I think is broken code:

in my frenzy I forgot about the basics of Disruptor

(the volatile write and then read of a proper variable

by proper threads to ensure event visibility).



It's easily fixable, and the correction has almost no impact

on performances for nbr of processors < nbr of cores.

My problem is that I can't manage to prove my code broken

with tests, and I don't understand why it "works".



Here is the (afaik-non-founded) assumption that I

(semi-unconsciously) made in the broken code:

If a thread lazily-sets a volatile variable A, and then that

another thread reads A and sees its new value, and then

sets (lazily or not) a volatile variable B, then if a third

thread reads B and sees its new value, if it reads A, it

should also see its new value (or more particularly

in the case of a ring buffer, if it reads a non-volatile

variable that was set by the first thread just before the

lazy-set of A, it should see its new value).

(It might still be wrong if the first set is non-lazy,

but laziness should make it worse.)



I attached code with which I unsuccessfully attempted to

prove this assumption wrong.



Can anyone confirm that my assumption was wrong, and

point out reasons why it could still "work" so often?



-Jeff





_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest





_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest





_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130123/911bbded/attachment-0001.html>

From jeffhain at rocketmail.com  Wed Jan 23 15:00:35 2013
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Wed, 23 Jan 2013 20:00:35 +0000 (GMT)
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <50FFD61D.50205@oracle.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
Message-ID: <1358971235.61196.YahooMailNeo@web132104.mail.ird.yahoo.com>



Aleksey Shipilev wrote:
>Although (1, 0) clashes with intuitive model of global consistent
>memory, and can be masked by TSO

TSO! Now I get why the bug doesn't want to happen on my computer :)



What I had in mind before was that either:
- T2 was on another core than T1 (for reading A), hence that T1
? had flushed A into main memory, hence that T3 (or anyone) could
? see the new value of A.

or
- T2 was on same core than T1, but then we had a context switch
? (unless hyper threading), hence (maybe) a flush of T1 into
? main memory, hence that T3 (or anyone) could see the new
? value of A as well.

But since I have hyper threading that couldn't hold.


-Jeff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130123/c1af245c/attachment.html>

From peter.levart at gmail.com  Thu Jan 24 05:15:21 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 24 Jan 2013 11:15:21 +0100
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <CAEJX8orZDe+hy6b+nH9_F6QE0drnn3FmY+g8VtC3_pYV9nXSWA@mail.gmail.com>
References: <50FEB3E7.5040301@gmail.com>
	<CAEJX8opzGfzPA+Su9NdTh7MDGWx3PfN3vbTHpU-zA7dATezkvg@mail.gmail.com>
	<50FEC8A2.1030207@gmail.com>
	<CAEJX8orZDe+hy6b+nH9_F6QE0drnn3FmY+g8VtC3_pYV9nXSWA@mail.gmail.com>
Message-ID: <510109B9.4010601@gmail.com>

On 01/22/2013 07:55 PM, Stanimir Simeonoff wrote:
>
>>     If I have to call isProxyClass I definitely check
>>     Proxy.class.isAssignagleFrom(clazz) && Proxy.isProxyClass(). That
>>     helps since it greatly narrows the candidates, unfortunately
>>     ObjectOutputStreamClass does follow suit.
>
>     That helps, when you expect a *false* answer. If your expected
>     answer is *true* most of times and you afterwards call
>     java.lang.reflect.Proxy.getInvocationHandler(Object proxy), you
>     get hit twice (since Proxy.isProxyClass() is called again in that
>     method).
>
> Well, of course. The other possible bottleneck (aside the reckless 
> call during serialization) could be RemoteObject.toStub that calls 
> both isProxyClass and getInvocationHandler.
> If you expect a positive answer check instanceof Proxy instead calling 
> isProxyClass. Since only hacks would extend the class naturally and 
> then follow w/ Proxy.getInvocationHandler, catching the 
> IllegalArgumentException in case of a hack.
> Yes the code looks like a hack, itself, but a comment should suffice. 
> Basically this is exactly what I do.
That only buys me one Proxy.isProxyClass call. The other one is in the 
Proxy.getInvocationHandler method. I can't avoid that one.

I was looking for a way to fix Proxy.isProxyClass in the JDK.

One idea that I scraped was to have a parallel cache copy of proxy 
classes WeakHashMap exposed via a volatile field to other threads for 
reading-only and which would be lazily constructed from the basic 
WeakHashMap and invalidated on each modification of the basic 
WeakHashMap. That would work with a map like HashMap, but not with 
WeakHashMap where even the get method can modify the internal state 
(expunging stale entries).

The other idea that works is to have a cache for Proxy.isProxyClass 
implemented by ClassValue, but has a drawback that for applications 
which invoke Proxy.isProxyClass with lots of non-proxy classes, it would 
cause lots of ClassValue.ClassValueMap instances to be allocated and 
attached to Class instances...

Does anyone have any other bright idea for fixing this?

Regards, Peter

>
> Stanimir
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130124/8a969b1e/attachment.html>

From stanimir at riflexo.com  Thu Jan 24 05:55:52 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 24 Jan 2013 12:55:52 +0200
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <510109B9.4010601@gmail.com>
References: <50FEB3E7.5040301@gmail.com>
	<CAEJX8opzGfzPA+Su9NdTh7MDGWx3PfN3vbTHpU-zA7dATezkvg@mail.gmail.com>
	<50FEC8A2.1030207@gmail.com>
	<CAEJX8orZDe+hy6b+nH9_F6QE0drnn3FmY+g8VtC3_pYV9nXSWA@mail.gmail.com>
	<510109B9.4010601@gmail.com>
Message-ID: <CAEJX8orqdF4ynrJPyyEgD6q82gpX2RhX_fHSmBerTPXXVee+qw@mail.gmail.com>

> The other idea that works is to have a cache for Proxy.isProxyClass
> implemented by ClassValue, but has a drawback that for applications which
> invoke Proxy.isProxyClass with lots of non-proxy classes, it would cause
> lots of ClassValue.ClassValueMap instances to be allocated and attached to
> Class instances...
>
>
Why do you worry about non-proxy classes, they are trivially filtered via
Proxy.isAssignableFrom(clazz)? You can even go further and check if
classname starts w/ "Proxy$" or whatever name the proxy generator chooses.

Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130124/4977d5e1/attachment.html>

From stanimir at riflexo.com  Thu Jan 24 05:59:45 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 24 Jan 2013 12:59:45 +0200
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <CAEJX8orqdF4ynrJPyyEgD6q82gpX2RhX_fHSmBerTPXXVee+qw@mail.gmail.com>
References: <50FEB3E7.5040301@gmail.com>
	<CAEJX8opzGfzPA+Su9NdTh7MDGWx3PfN3vbTHpU-zA7dATezkvg@mail.gmail.com>
	<50FEC8A2.1030207@gmail.com>
	<CAEJX8orZDe+hy6b+nH9_F6QE0drnn3FmY+g8VtC3_pYV9nXSWA@mail.gmail.com>
	<510109B9.4010601@gmail.com>
	<CAEJX8orqdF4ynrJPyyEgD6q82gpX2RhX_fHSmBerTPXXVee+qw@mail.gmail.com>
Message-ID: <CAEJX8ooKPG-Zn=BDuWopggZuCxFex4uv2FXcpH=M+BUZ8bnKwQ@mail.gmail.com>

On a second thought: if you can modify the JDK, just add an extra field in
the Class class to be marked as proxy, no need for any maps. It may
increase the memory footprint slight but it's well worth the price, imo.

Stanimir

On Thu, Jan 24, 2013 at 12:55 PM, Stanimir Simeonoff
<stanimir at riflexo.com>wrote:

>
> The other idea that works is to have a cache for Proxy.isProxyClass
>> implemented by ClassValue, but has a drawback that for applications which
>> invoke Proxy.isProxyClass with lots of non-proxy classes, it would cause
>> lots of ClassValue.ClassValueMap instances to be allocated and attached to
>> Class instances...
>>
>>
> Why do you worry about non-proxy classes, they are trivially filtered via
> Proxy.isAssignableFrom(clazz)? You can even go further and check if
> classname starts w/ "Proxy$" or whatever name the proxy generator chooses.
>
> Stanimir
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130124/349f9975/attachment.html>

From peter.levart at gmail.com  Thu Jan 24 06:10:10 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 24 Jan 2013 12:10:10 +0100
Subject: [concurrency-interest] WeakConcurrentHashMap
In-Reply-To: <CAEJX8ooKPG-Zn=BDuWopggZuCxFex4uv2FXcpH=M+BUZ8bnKwQ@mail.gmail.com>
References: <50FEB3E7.5040301@gmail.com>
	<CAEJX8opzGfzPA+Su9NdTh7MDGWx3PfN3vbTHpU-zA7dATezkvg@mail.gmail.com>
	<50FEC8A2.1030207@gmail.com>
	<CAEJX8orZDe+hy6b+nH9_F6QE0drnn3FmY+g8VtC3_pYV9nXSWA@mail.gmail.com>
	<510109B9.4010601@gmail.com>
	<CAEJX8orqdF4ynrJPyyEgD6q82gpX2RhX_fHSmBerTPXXVee+qw@mail.gmail.com>
	<CAEJX8ooKPG-Zn=BDuWopggZuCxFex4uv2FXcpH=M+BUZ8bnKwQ@mail.gmail.com>
Message-ID: <51011692.3010809@gmail.com>

On 01/24/2013 11:59 AM, Stanimir Simeonoff wrote:
> On a second thought: if you can modify the JDK, just add an extra 
> field in the Class class to be marked as proxy, no need for any maps. 
> It may increase the memory footprint slight but it's well worth the 
> price, imo.
I'm trying to devise a fix that might possibly be considered for 
inclusion in OpenJDK. I don't think a new field in j.l.Class would pass.

But you're right. The check for Proxy.isAssignableFrom(clazz) before 
going to ClassValue cache would do just fine.

Thanks,

Peter

>
> Stanimir
>
> On Thu, Jan 24, 2013 at 12:55 PM, Stanimir Simeonoff 
> <stanimir at riflexo.com <mailto:stanimir at riflexo.com>> wrote:
>
>
>         The other idea that works is to have a cache for
>         Proxy.isProxyClass implemented by ClassValue, but has a
>         drawback that for applications which invoke Proxy.isProxyClass
>         with lots of non-proxy classes, it would cause lots of
>         ClassValue.ClassValueMap instances to be allocated and
>         attached to Class instances...
>
>     Why do you worry about non-proxy classes, they are trivially
>     filtered via Proxy.isAssignableFrom(clazz)? You can even go
>     further and check if classname starts w/ "Proxy$" or whatever name
>     the proxy generator chooses.
>
>     Stanimir
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130124/577a60e9/attachment-0001.html>

From javapk at gmail.com  Fri Jan 25 02:09:00 2013
From: javapk at gmail.com (Praveen Kumar Jha)
Date: Fri, 25 Jan 2013 12:39:00 +0530
Subject: [concurrency-interest] Is it guaranteed that compareAndSet can NOT
 fail for ALL the participating threads?
Message-ID: <CAKYmnUrd29pbCb0WSz9=i0gXkQLgQgxudzDnsLqe7Z84XcsNfA@mail.gmail.com>

Suppose some "N" number of threads are trying to CAS an AtomicInteger
variable, Is it guaranteed that the CAS MUST succeed for exactly ONE
thread?

Is there a possibility that all the "N" threads fail in an attempt?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130125/957e4bb7/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 25 03:16:21 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 25 Jan 2013 12:16:21 +0400
Subject: [concurrency-interest] Is it guaranteed that compareAndSet can
 NOT fail for ALL the participating threads?
In-Reply-To: <CAKYmnUrd29pbCb0WSz9=i0gXkQLgQgxudzDnsLqe7Z84XcsNfA@mail.gmail.com>
References: <CAKYmnUrd29pbCb0WSz9=i0gXkQLgQgxudzDnsLqe7Z84XcsNfA@mail.gmail.com>
Message-ID: <51023F55.20708@oracle.com>

On 01/25/2013 11:09 AM, Praveen Kumar Jha wrote:
> Suppose some "N" number of threads are trying to CAS an AtomicInteger
> variable, Is it guaranteed that the CAS MUST succeed for exactly ONE
> thread?

Yes, it is guaranteed for AtomicX.compareAndSet.

Really, there is the distinction between "strong" and "weak" CAS, first
one can not fail spuriously, and second one can. AtomicX.compareAndSet
is the "strong" CAS. AtomicX.weakCompareAndSet is the example of "weak"
CAS, and can fails spuriously for all threads without a particular reason.

-Aleksey.


From cheremin at gmail.com  Fri Jan 25 06:21:23 2013
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Fri, 25 Jan 2013 15:21:23 +0400
Subject: [concurrency-interest] Is it guaranteed that compareAndSet can
 NOT fail for ALL the participating threads?
In-Reply-To: <51023F55.20708@oracle.com>
References: <CAKYmnUrd29pbCb0WSz9=i0gXkQLgQgxudzDnsLqe7Z84XcsNfA@mail.gmail.com>
	<51023F55.20708@oracle.com>
Message-ID: <CAOwENiKgUiLYUd9W4oDcb19VfuYvAcFr-HuPYcOKtw53tJ6NCg@mail.gmail.com>

There is also weakened memory semantics for weakCAS -- it does not
enforce memory ordering, opposite to "strong" CAS. Although I know
only about Azul actually has CAS without membar in it's hardware.

2013/1/25 Aleksey Shipilev <aleksey.shipilev at oracle.com>:
> On 01/25/2013 11:09 AM, Praveen Kumar Jha wrote:
>> Suppose some "N" number of threads are trying to CAS an AtomicInteger
>> variable, Is it guaranteed that the CAS MUST succeed for exactly ONE
>> thread?
>
> Yes, it is guaranteed for AtomicX.compareAndSet.
>
> Really, there is the distinction between "strong" and "weak" CAS, first
> one can not fail spuriously, and second one can. AtomicX.compareAndSet
> is the "strong" CAS. AtomicX.weakCompareAndSet is the example of "weak"
> CAS, and can fails spuriously for all threads without a particular reason.
>
> -Aleksey.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From nathan.reynolds at oracle.com  Fri Jan 25 09:50:01 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 25 Jan 2013 07:50:01 -0700
Subject: [concurrency-interest] Is it guaranteed that compareAndSet can
 NOT fail for ALL the participating threads?
In-Reply-To: <CAOwENiKgUiLYUd9W4oDcb19VfuYvAcFr-HuPYcOKtw53tJ6NCg@mail.gmail.com>
References: <CAKYmnUrd29pbCb0WSz9=i0gXkQLgQgxudzDnsLqe7Z84XcsNfA@mail.gmail.com>
	<51023F55.20708@oracle.com>
	<CAOwENiKgUiLYUd9W4oDcb19VfuYvAcFr-HuPYcOKtw53tJ6NCg@mail.gmail.com>
Message-ID: <51029B99.2030908@oracle.com>

On Power PC, isn't CAS implemented with LL/SC?  Is this guaranteed to 
succeed?

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 1/25/2013 4:21 AM, Ruslan Cheremin wrote:
> There is also weakened memory semantics for weakCAS -- it does not
> enforce memory ordering, opposite to "strong" CAS. Although I know
> only about Azul actually has CAS without membar in it's hardware.
>
> 2013/1/25 Aleksey Shipilev <aleksey.shipilev at oracle.com>:
>> On 01/25/2013 11:09 AM, Praveen Kumar Jha wrote:
>>> Suppose some "N" number of threads are trying to CAS an AtomicInteger
>>> variable, Is it guaranteed that the CAS MUST succeed for exactly ONE
>>> thread?
>> Yes, it is guaranteed for AtomicX.compareAndSet.
>>
>> Really, there is the distinction between "strong" and "weak" CAS, first
>> one can not fail spuriously, and second one can. AtomicX.compareAndSet
>> is the "strong" CAS. AtomicX.weakCompareAndSet is the example of "weak"
>> CAS, and can fails spuriously for all threads without a particular reason.
>>
>> -Aleksey.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130125/bdd77633/attachment.html>

From vitalyd at gmail.com  Fri Jan 25 10:04:07 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 25 Jan 2013 10:04:07 -0500
Subject: [concurrency-interest] Is it guaranteed that compareAndSet can
 NOT fail for ALL the participating threads?
In-Reply-To: <51029B99.2030908@oracle.com>
References: <CAKYmnUrd29pbCb0WSz9=i0gXkQLgQgxudzDnsLqe7Z84XcsNfA@mail.gmail.com>
	<51023F55.20708@oracle.com>
	<CAOwENiKgUiLYUd9W4oDcb19VfuYvAcFr-HuPYcOKtw53tJ6NCg@mail.gmail.com>
	<51029B99.2030908@oracle.com>
Message-ID: <CAHjP37F=b=rM0bNxs2i3cmW4eVNxoq1PDNMcpcC8vk8v0xS5UQ@mail.gmail.com>

Some LL/SC impls can fail spuriously, but one would expect hotspot wraps
them in a loop to fulfill the documented contract of the strong cas.

Sent from my phone
On Jan 25, 2013 9:53 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  On Power PC, isn't CAS implemented with LL/SC?  Is this guaranteed to
> succeed?
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 1/25/2013 4:21 AM, Ruslan Cheremin wrote:
>
> There is also weakened memory semantics for weakCAS -- it does not
> enforce memory ordering, opposite to "strong" CAS. Although I know
> only about Azul actually has CAS without membar in it's hardware.
>
> 2013/1/25 Aleksey Shipilev <aleksey.shipilev at oracle.com> <aleksey.shipilev at oracle.com>:
>
>  On 01/25/2013 11:09 AM, Praveen Kumar Jha wrote:
>
>  Suppose some "N" number of threads are trying to CAS an AtomicInteger
> variable, Is it guaranteed that the CAS MUST succeed for exactly ONE
> thread?
>
>
> Yes, it is guaranteed for AtomicX.compareAndSet.
>
> Really, there is the distinction between "strong" and "weak" CAS, first
> one can not fail spuriously, and second one can. AtomicX.compareAndSet
> is the "strong" CAS. AtomicX.weakCompareAndSet is the example of "weak"
> CAS, and can fails spuriously for all threads without a particular reason.
>
> -Aleksey.
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130125/937dcc28/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 25 10:13:11 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 25 Jan 2013 19:13:11 +0400
Subject: [concurrency-interest] Is it guaranteed that compareAndSet can
 NOT fail for ALL the participating threads?
In-Reply-To: <CAHjP37F=b=rM0bNxs2i3cmW4eVNxoq1PDNMcpcC8vk8v0xS5UQ@mail.gmail.com>
References: <CAKYmnUrd29pbCb0WSz9=i0gXkQLgQgxudzDnsLqe7Z84XcsNfA@mail.gmail.com>
	<51023F55.20708@oracle.com>
	<CAOwENiKgUiLYUd9W4oDcb19VfuYvAcFr-HuPYcOKtw53tJ6NCg@mail.gmail.com>
	<51029B99.2030908@oracle.com>
	<CAHjP37F=b=rM0bNxs2i3cmW4eVNxoq1PDNMcpcC8vk8v0xS5UQ@mail.gmail.com>
Message-ID: <5102A107.8010804@oracle.com>

That's right. Citing the C++ reference (again):
http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html

Cmpxchg SeqCst (32 bit):
          hwsync
   _loop: lwarx ...
          cmp ...
          bc _exit
          stwcx. ...
          bc _loop
          isync
   _exit:

-Aleksey.

On 01/25/2013 07:04 PM, Vitaly Davidovich wrote:
> Some LL/SC impls can fail spuriously, but one would expect hotspot wraps
> them in a loop to fulfill the documented contract of the strong cas.
> 
> Sent from my phone
> 
> On Jan 25, 2013 9:53 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com
> <mailto:nathan.reynolds at oracle.com>> wrote:
> 
>     On Power PC, isn't CAS implemented with LL/SC?  Is this guaranteed
>     to succeed?
> 
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 1/25/2013 4:21 AM, Ruslan Cheremin wrote:
>>     There is also weakened memory semantics for weakCAS -- it does not
>>     enforce memory ordering, opposite to "strong" CAS. Although I know
>>     only about Azul actually has CAS without membar in it's hardware.
>>
>>     2013/1/25 Aleksey Shipilev <aleksey.shipilev at oracle.com> <mailto:aleksey.shipilev at oracle.com>:
>>>     On 01/25/2013 11:09 AM, Praveen Kumar Jha wrote:
>>>>     Suppose some "N" number of threads are trying to CAS an AtomicInteger
>>>>     variable, Is it guaranteed that the CAS MUST succeed for exactly ONE
>>>>     thread?
>>>     Yes, it is guaranteed for AtomicX.compareAndSet.
>>>
>>>     Really, there is the distinction between "strong" and "weak" CAS, first
>>>     one can not fail spuriously, and second one can. AtomicX.compareAndSet
>>>     is the "strong" CAS. AtomicX.weakCompareAndSet is the example of "weak"
>>>     CAS, and can fails spuriously for all threads without a particular reason.
>>>
>>>     -Aleksey.
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From heinz at javaspecialists.eu  Wed Jan 30 15:42:42 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 30 Jan 2013 22:42:42 +0200
Subject: [concurrency-interest] Splitting Phasers
Message-ID: <510985C2.4070607@javaspecialists.eu>

In preparation for a talk I'm giving next week on Phasers, I had another 
look at what you can do with them.  I have some nice use cases that I 
posted to this list in the past.  However, I also wanted to try out some 
things that I had not used before, such as hierarchies of phasers and 
combining phaser with Fork/Join.

I tried to do something that should be really simple:

- Use recursive decomposition to break up a bunch of tasks that are 
going to cooperate via the Phaser. 
- Minimize contention by building up a tree of phasers.

I tried different approaches of building up a tree of phasers at the 
same time that I was doing my recursive decomposition.  The most obvious 
is to create the phasers as you walk down the tree.  The problem with 
this approach is that if you aren't careful, some of the threads will 
arrive and perhaps advance to the next phase before the tree is fully 
built up.

After some to-and-fro, I wrote a little function to break away some 
permits from a phaser, like so:

public class Phasers {
  /**
   * This method is used to split a Phaser into two.  First a new Phaser is
   * created with the old Phaser as the parent and with the number of 
required
   * parties.  The number of parties is then decreased on the parent phaser.
   */
  public static Phaser split(Phaser parent, int parties) {
    Phaser child = new Phaser(parent, parties);
    for (int i = 0; i < parties; i++) {
      parent.arriveAndDeregister();
    }
    return child;
  }
}

Obviously this is not the most efficient approach and should probably be 
done inside the Phaser class itself.  It would be nice to have a way to 
do a bulkDeregister(int parties) as well.  Or a split(int parties) 
method inside Phaser that returns a new child Phaser with the correct 
number of parties that it would have stolen (borrowed) from the parent.

Here's how I'm combining the ForkJoinPool with the Phaser, just to try 
out how it works.  I'm sure there are ways of writing this differently, 
so that you can do away with split(), but the method makes sense to me, 
if we want to support hierarchies of phasers.

public class ForkJoinPhaser2 {
  private static final int PHASER_THRESHOLD = 2;
  private static final int PARTIES = 180;

  public static void main(String[] args) {
    ForkJoinPool fjp = new ForkJoinPool();
    Phaser phaser = new Phaser(PARTIES);
    fjp.invoke(new PhasedAction(PARTIES, phaser));
    System.out.println(phaser);
    System.out.println(fjp);
  }

  private static class PhasedAction extends RecursiveAction {
    private final int parties;
    private final Phaser ph;

    private PhasedAction(int parties, Phaser ph) {
      this.parties = parties;
      this.ph = ph;
    }

    protected void compute() {
      if (parties == 1) {
        System.out.printf("wait: %H: %s: %d: %s%n",
            System.identityHashCode(this), Thread.currentThread(),
            ph.getUnarrivedParties(), ph);
        ph.arriveAndAwaitAdvance();
        System.out.printf("done: %H: %s%n",
            System.identityHashCode(this), Thread.currentThread());
      } else {
        int left = parties / 2;
        int right = parties - left;
        System.out.println("left = " + left + ", right = " + right);
        Phaser leftPhaser =
            left > PHASER_THRESHOLD ? Phasers.split(ph, left) : ph;
        Phaser rightPhaser =
            right > PHASER_THRESHOLD ? Phasers.split(ph, right) : ph;
        invokeAll(new PhasedAction(left, leftPhaser),
            new PhasedAction(right, rightPhaser));
      }
    }
  }
}

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz 


From aleksey.shipilev at oracle.com  Wed Jan 30 19:00:13 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 31 Jan 2013 04:00:13 +0400
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <510055CE.4080605@oracle.com>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
	<510055CE.4080605@oracle.com>
Message-ID: <5109B40D.1020402@oracle.com>

On 01/24/2013 01:27 AM, Aleksey Shipilev wrote:
>> Can you determine what PPC code is generated for get() and lazySet().
>> Is it consistent with the recipes in
>> http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html?
> 
> Good idea. That PPC port is young enough to have severe bugs.
> Unfortunately, there can be some substantial time before I can look at
> that again. Alternatively, this could be the bug in the test
> infrastructure (and no soul had volunteered to make the runs on PPC yet!)

Ok, finally following up on this. This is the relevant disassembly:

lazySet():
  0xf5858318: lwz     r3,8(r3)          ;*getfield a
  0xf585831c: lwz     r0,0(r3)          ;*invokevirtual lazySet
  0xf5858320: li      r4,1
  0xf5858324: li      r5,8
  0xf5858328: lwsync
  0xf585832c: stwx    r4,r3,r5          ; write $value = 1
  0xf5858330: msync

get():
  0xf585a864: lwz     r3,8(r3)          ;*getfield a
  0xf585a868: lwz     r3,8(r3)          ; read r3 <- $value
  0xf585a86c: lwz     r0,0(r1)
  0xf585a870: cmpw    cr7,r0,r0
  0xf585a874: bne     cr7,0xf585a870
  0xf585a878: isync                     ;*getfield value

Seems to be in line with C++11 mappings for store_release and
load_acquire, respectively. (I'm not sure about the relevance of
trailing msync in lazySet though, this seems like the redundant full
barrier, with crumbtrail leading up to Unsafe.putOrdered*.)

So, rewinding the history back, I suspect the infrastructure bug, and
being lazy enough, I just rewired the infra during the pending
refactoring to get more precise control over the concurrency there. The
LazySetTransitivityTest [1] failures were gone since. I'll do a few more
runs overnight on that machine to get more confidence in this.

-Aleksey.

[1]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/volatiles/LazySetTransitivityTest.java

