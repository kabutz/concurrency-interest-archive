From federico.fissore at gmail.com  Fri Sep  2 06:18:18 2016
From: federico.fissore at gmail.com (federico.fissore at gmail.com)
Date: Fri, 2 Sep 2016 12:18:18 +0200
Subject: [concurrency-interest] CompletableFuture.stopIf
Message-ID: <a73cf558-2b4c-2053-42a4-ffe317eec5f6@gmail.com>

Hi all

Is there still time to add a method to CompletableFuture?

I would like to have a stopIf/interruptIf method that stops a chain of 
CFs when the given predicate is true

Consider the following example:

fetchUser(id)
  - changeEmail(user)
  - notifyOtherSystems(user)

If fetchUser returns null, the following steps are nonsense. At the 
moment, you have to copy/paste a check for a null user in both 
changeEmail and notifyOtherSystems steps.

What about:

fetchUser(id)
  - stopIf((user) -> user == null)
  - changeEmail(user)
  - notifyOtherSystemsOfNewEmail(user)

stopIf exceptionally completes the CF with a CompletionException.

Please note that fetchUser cannot complete exceptionally on its own, 
since it may be used elsewhere in the code base, where a null result has 
different semantics

Best regards

Federico Fissore

From akarnokd at gmail.com  Fri Sep  2 07:15:47 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Fri, 2 Sep 2016 13:15:47 +0200
Subject: [concurrency-interest] CompletableFuture.stopIf
In-Reply-To: <a73cf558-2b4c-2053-42a4-ffe317eec5f6@gmail.com>
References: <a73cf558-2b4c-2053-42a4-ffe317eec5f6@gmail.com>
Message-ID: <CAAWwtm-6vgM+bBharbxk2FgR6kAkCt_2sx+ns6=CXo++yok3fg@mail.gmail.com>

Have you considered using Reactor-Core (Java 8+) instead? Assuming your
methods return CompletableFuture

Mono.fromFuture(fetchUser(id))
.filter(user -> user != null)
.flatMap(user ->
    Mono.fromFuture(changeEmail(user))
    .then(Mono.fromFuture(notifyOtherSystemsOfNewEmail(user)))
)
.subscribe(v -> { }, e -> log.error(e));


2016-09-02 12:18 GMT+02:00 federico.fissore at gmail.com <
federico.fissore at gmail.com>:

> Hi all
>
> Is there still time to add a method to CompletableFuture?
>
> I would like to have a stopIf/interruptIf method that stops a chain of CFs
> when the given predicate is true
>
> Consider the following example:
>
> fetchUser(id)
>  - changeEmail(user)
>  - notifyOtherSystems(user)
>
> If fetchUser returns null, the following steps are nonsense. At the
> moment, you have to copy/paste a check for a null user in both changeEmail
> and notifyOtherSystems steps.
>
> What about:
>
> fetchUser(id)
>  - stopIf((user) -> user == null)
>  - changeEmail(user)
>  - notifyOtherSystemsOfNewEmail(user)
>
> stopIf exceptionally completes the CF with a CompletionException.
>
> Please note that fetchUser cannot complete exceptionally on its own, since
> it may be used elsewhere in the code base, where a null result has
> different semantics
>
> Best regards
>
> Federico Fissore
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160902/93097003/attachment.html>

From federico.fissore at gmail.com  Fri Sep  2 08:08:36 2016
From: federico.fissore at gmail.com (federico.fissore at gmail.com)
Date: Fri, 2 Sep 2016 14:08:36 +0200
Subject: [concurrency-interest] CompletableFuture.stopIf
In-Reply-To: <CAAWwtm-6vgM+bBharbxk2FgR6kAkCt_2sx+ns6=CXo++yok3fg@mail.gmail.com>
References: <a73cf558-2b4c-2053-42a4-ffe317eec5f6@gmail.com>
 <CAAWwtm-6vgM+bBharbxk2FgR6kAkCt_2sx+ns6=CXo++yok3fg@mail.gmail.com>
Message-ID: <b1542f3f-7364-4085-bd88-3edd3a960361@gmail.com>

Dávid Karnok ha scritto il 02/09/2016 alle 13:15:
> Have you considered using Reactor-Core (Java 8+) instead? Assuming your
> methods return CompletableFuture
>
> Mono.fromFuture(fetchUser(id))
> .filter(user -> user != null)
> .flatMap(user ->
>     Mono.fromFuture(changeEmail(user))
>     .then(Mono.fromFuture(notifyOtherSystemsOfNewEmail(user)))
> )
> .subscribe(v -> { }, e -> log.error(e));
>

Ah that's a nice syntax. Thank you. I didn't know about reactor-core

Federico

From martinrb at google.com  Wed Sep 21 16:43:13 2016
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 21 Sep 2016 13:43:13 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
Message-ID: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>

(Sorry to re-open this discussion)

The separation of a read-only CompletionStage from CompletableFuture is
great.  I'm a fan of the scala style Promise/Future split as described in
http://docs.scala-lang.org/overviews/core/futures.html, but: we need to
re-add (safe, read-only) blocking methods like join.  Java is not Node.js,
where there are no threads but there is a universal event loop.  Java
programmers are used to Future, where the *only* way to use a future's
value is to block waiting for it.  The existing CompletionStage methods are
a better scaling alternative to blocking all the time, but blocking is
almost always eventually necessary in Java.  For example, junit test
methods that start any asynchronous computation need to block until the
computation is done, before returning.

As Viktor has pointed out, users can always implement blocking themselves
by writing

    static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T>
stage) {
        CompletableFuture<T> f = new CompletableFuture<>();
        stage.handle((T t, Throwable ex) -> {
                         if (ex != null) f.completeExceptionally(ex);
                         else f.complete(t);
                         return null;
                     });
        return f;
    }

    static <T> T join(CompletionStage<T> stage) {
        return toCompletableFuture(stage).join();
    }

but unlike Viktor, I think it's unreasonable to not provide this for users
(especially when we can do so more efficiently).  What is happening instead
is API providers not using CompletionStage as return values in public APIs
because of the lack of convenient blocking, and instead returning
CompletableFuture, which is a tragic software engineering failure.

Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture
from throwing UnsupportedOperationException, and implement join as:

    public default T join() { return toCompletableFuture().join(); }

There is a risk of multiple-inheritance conflict with Future if we add e.g.
isDone(), but there are no current plans to turn those Future methods into
default methods, and even if we did in some future release, it would be
only a source, not binary incompatibility, so far less serious.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160921/e74a1d7a/attachment.html>

From ben.manes at gmail.com  Wed Sep 21 17:25:58 2016
From: ben.manes at gmail.com (Benjamin Manes)
Date: Wed, 21 Sep 2016 14:25:58 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
Message-ID: <CAGu0=MOuPyWauLZ9qeP=ZMmOMeoDBXU_w_xxRoopOhM-MEZuMw@mail.gmail.com>

My limited understanding is that the original API was
only CompletableFuture and that CompletionStage introduced as a compromise.
It did not appear to be an attempt to strictly follow an
interface-implementation separation, e.g. collections. As you said
#toCompletableFuture()
may throw an UOE, which means some use-cases can't rely on CompletionState
which limits its usefulness. In my case that would be an AsyncLoadingCache
with a synchronous LoadingCache view. I think having to code that the
resulting implementation would be worse if it called toCompletableFuture,
caught the exception, and then adapted as you said.

When the new future class was introduced it was stated,

"In other words, we (j.u.c) are not now in a position to dictate a common
interface for all SettableFuture, FutureValue, Promise, ListenableFuture,
etc like APIs. And as we've seen, different audiences want/need different
subsets of this API exposed as interfaces for their usages, and are in any
case unlikely to want change all their existing interfaces. However, what
we can do is provide a common underlying implementation that is as fast,
scalable, space-conserving, carefully-specified, and reliable as possible.
It should then be easy and attractive for others creating or reworking
higher-level APIs to relay all functionality to the CompletableFuture
implementation."  - Doug Lea, '12

I've gradually come to terms using CF as part of an API and haven't
experienced a downside yet.

On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com>
wrote:

> (Sorry to re-open this discussion)
>
> The separation of a read-only CompletionStage from CompletableFuture is
> great.  I'm a fan of the scala style Promise/Future split as described in
> http://docs.scala-lang.org/overviews/core/futures.html, but: we need to
> re-add (safe, read-only) blocking methods like join.  Java is not Node.js,
> where there are no threads but there is a universal event loop.  Java
> programmers are used to Future, where the *only* way to use a future's
> value is to block waiting for it.  The existing CompletionStage methods are
> a better scaling alternative to blocking all the time, but blocking is
> almost always eventually necessary in Java.  For example, junit test
> methods that start any asynchronous computation need to block until the
> computation is done, before returning.
>
> As Viktor has pointed out, users can always implement blocking themselves
> by writing
>
>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T>
> stage) {
>         CompletableFuture<T> f = new CompletableFuture<>();
>         stage.handle((T t, Throwable ex) -> {
>                          if (ex != null) f.completeExceptionally(ex);
>                          else f.complete(t);
>                          return null;
>                      });
>         return f;
>     }
>
>     static <T> T join(CompletionStage<T> stage) {
>         return toCompletableFuture(stage).join();
>     }
>
> but unlike Viktor, I think it's unreasonable to not provide this for users
> (especially when we can do so more efficiently).  What is happening instead
> is API providers not using CompletionStage as return values in public APIs
> because of the lack of convenient blocking, and instead returning
> CompletableFuture, which is a tragic software engineering failure.
>
> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture
> from throwing UnsupportedOperationException, and implement join as:
>
>     public default T join() { return toCompletableFuture().join(); }
>
> There is a risk of multiple-inheritance conflict with Future if we add
> e.g. isDone(), but there are no current plans to turn those Future methods
> into default methods, and even if we did in some future release, it would
> be only a source, not binary incompatibility, so far less serious.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160921/c03d0be5/attachment.html>

From pavel.rappo at gmail.com  Wed Sep 21 17:38:28 2016
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Wed, 21 Sep 2016 22:38:28 +0100
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
Message-ID: <CAChcVukJiufjZsO2LTFw43J+sYX9CUZ9NkeJaSoUaa6A-97CPw@mail.gmail.com>

On Wed, Sep 21, 2016 at 9:43 PM, Martin Buchholz <martinrb at google.com> wrote:
> What is happening instead is API providers not using CompletionStage as
> return values in public APIs because of the lack of convenient blocking, and
> instead returning CompletableFuture, which is a tragic software engineering
> failure.

On Wed, Sep 21, 2016 at 10:25 PM, Benjamin Manes <ben.manes at gmail.com> wrote:
> I've gradually come to terms using CF as part of an API and haven't
> experienced a downside yet.

I agree with Benjamin and would like to hear more about how it's a
"tragic software engineering failure".

From martinrb at google.com  Wed Sep 21 17:44:46 2016
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 21 Sep 2016 14:44:46 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CAChcVukJiufjZsO2LTFw43J+sYX9CUZ9NkeJaSoUaa6A-97CPw@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CAChcVukJiufjZsO2LTFw43J+sYX9CUZ9NkeJaSoUaa6A-97CPw@mail.gmail.com>
Message-ID: <CA+kOe0_CVfi+ixLaPEQ8fLk4HjvJ8mGpCca1dQ9=kBXNGrGBwA@mail.gmail.com>

On Wed, Sep 21, 2016 at 2:38 PM, Pavel Rappo <pavel.rappo at gmail.com> wrote:

> On Wed, Sep 21, 2016 at 9:43 PM, Martin Buchholz <martinrb at google.com>
> wrote:
> > What is happening instead is API providers not using CompletionStage as
> > return values in public APIs because of the lack of convenient blocking,
> and
> > instead returning CompletableFuture, which is a tragic software
> engineering
> > failure.
>
> On Wed, Sep 21, 2016 at 10:25 PM, Benjamin Manes <ben.manes at gmail.com>
> wrote:
> > I've gradually come to terms using CF as part of an API and haven't
> > experienced a downside yet.
>
> I agree with Benjamin and would like to hear more about how it's a
> "tragic software engineering failure".
>

Obviously I was exaggerating, but the software engineering benefits of
separating the producer and consumer users into separate types seems big
(like the Builder pattern for immutable collections).  Would you use
CompletionStage if it had the other methods consumers need?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160921/679308b7/attachment-0001.html>

From chris.hegarty at oracle.com  Thu Sep 22 07:47:27 2016
From: chris.hegarty at oracle.com (Chris Hegarty)
Date: Thu, 22 Sep 2016 12:47:27 +0100
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
Message-ID: <EE95AFC1-B4D4-46B8-96D9-37FA3E77F673@oracle.com>

Until now CS and CF have not appeared in Java SE API signatures,
outside of the j.u.c package. They are, however, currently being
proposed for use in API signatures for Java SE 9 [1][2], namely
j.l.Process[Handle]::onExit, and more extensively in the proposed new
HTTP Client.

CF was chosen, in some cases, mainly because it provides 'join'. While
some may not like it, a large number of developers still, at some point,
want to be able to block. It was felt that CF was more convenient,
rather than the CS::toCF dance.

In some cases CF provides too many knobs and is not quite suited. For
example, the java.net.http package description has the following
paragraph [3]:

 * <p> {@code CompletableFuture}s returned by this API will throw
 * {@link java.lang.UnsupportedOperationException} for their {@link
 * java.util.concurrent.CompletableFuture#obtrudeValue(Object) obtrudeValue}
 * and {@link java.util.concurrent.CompletableFuture#obtrudeException(Throwable)
 * obtrudeException} methods. Invoking the {@link
 * java.util.concurrent.CompletableFuture#cancel cancel} method on a
 * {@code CompletableFuture} returned by this API will not interrupt
 * the underlying operation, but may be useful to complete, exceptionally,
 * dependent stages that have not already completed.

At a minimum adding 'join' to CS would help ( and may cause the above
usages in JDK 9 to be revisited ). If this is not acceptable, or maybe
even separately, is there any appetite for a type between CS and CF,
that would expose a select set of methods ( which methods tbd ) that are
"more suited" [*] for use in the above kind of APIs, where the platform
or library is using them as a notification mechanism ( cancel may, or
may not, be useful to notify the platform / library that the operation /
result is no longer interesting, albeit somewhat of a hint ).

-Chris.

[1] http://download.java.net/java/jdk9/docs/api/java/util/concurrent/class-use/CompletableFuture.html
[2] http://download.java.net/java/jdk9/docs/api/java/util/concurrent/class-use/CompletionStage.html
[3] http://hg.openjdk.java.net/jdk9/sandbox/jdk/file/1fdd889687c8/src/java.httpclient/share/classes/java/net/http/package-info.java#l46
[*] for some definition of ...

> On 21 Sep 2016, at 21:43, Martin Buchholz <martinrb at google.com> wrote:
> 
> (Sorry to re-open this discussion)
> 
> The separation of a read-only CompletionStage from CompletableFuture is great.  I'm a fan of the scala style Promise/Future split as described in http://docs.scala-lang.org/overviews/core/futures.html, but: we need to re-add (safe, read-only) blocking methods like join.  Java is not Node.js, where there are no threads but there is a universal event loop.  Java programmers are used to Future, where the *only* way to use a future's value is to block waiting for it.  The existing CompletionStage methods are a better scaling alternative to blocking all the time, but blocking is almost always eventually necessary in Java.  For example, junit test methods that start any asynchronous computation need to block until the computation is done, before returning.
> 
> As Viktor has pointed out, users can always implement blocking themselves by writing
> 
>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T> stage) {
>         CompletableFuture<T> f = new CompletableFuture<>();
>         stage.handle((T t, Throwable ex) -> {
>                          if (ex != null) f.completeExceptionally(ex);
>                          else f.complete(t);
>                          return null;
>                      });
>         return f;
>     }
> 
>     static <T> T join(CompletionStage<T> stage) {
>         return toCompletableFuture(stage).join();
>     }
> 
> but unlike Viktor, I think it's unreasonable to not provide this for users (especially when we can do so more efficiently).  What is happening instead is API providers not using CompletionStage as return values in public APIs because of the lack of convenient blocking, and instead returning CompletableFuture, which is a tragic software engineering failure.
> 
> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture from throwing UnsupportedOperationException, and implement join as:
> 
>     public default T join() { return toCompletableFuture().join(); }
> 
> There is a risk of multiple-inheritance conflict with Future if we add e.g. isDone(), but there are no current plans to turn those Future methods into default methods, and even if we did in some future release, it would be only a source, not binary incompatibility, so far less serious.


From Maciej.Bobrowski at morganstanley.com  Thu Sep 22 07:57:14 2016
From: Maciej.Bobrowski at morganstanley.com (Bobrowski, Maciej)
Date: Thu, 22 Sep 2016 11:57:14 +0000
Subject: [concurrency-interest] AbstractQueuedSynchronizer
Message-ID: <9838003A10254741BC6FFADE2ED02B457D5E4D50@OZWEX0205N1.msad.ms.com>

I have tried to understand the inner working of abstractQueuedSynchronizer and one thing I cannot quite explain.

When a thread decides it needs to park itself (let's say the CountDownLatch counter > 0), it does so by calling LockSuport.park. In the meantime, of course, the latch can be fully counted down, all awaiting threads unparked, while this thread is just about to park itself. Obviously that would not work, as now we have one thread parked, on a latch that is fully counted down, so no good.

This does not happen actually (I wrote a small test and verified), and it is down to the code of LockSupport.park()

public static void park(Object blocker) {
    Thread t = Thread.currentThread();
    setBlocker(t, blocker);
    unsafe.park(false, 0L);
    setBlocker(t, null);
}

Somehow, when the latch is fully counted down and unsafe.park is called, it immediately returns, so there is some magic behind the scenes that tells the thread not to actually park itself. Anyone could cast some light onto what is actually happening?

Thanks,
Maciej


________________________________

NOTICE: Morgan Stanley is not acting as a municipal advisor and the opinions or views contained herein are not intended to be, and do not constitute, advice within the meaning of Section 975 of the Dodd-Frank Wall Street Reform and Consumer Protection Act. If you have received this communication in error, please destroy all electronic and paper copies and notify the sender immediately. Mistransmission is not intended to waive confidentiality or privilege. Morgan Stanley reserves the right, to the extent permitted under applicable law, to monitor electronic communications. This message is subject to terms available at the following link: http://www.morganstanley.com/disclaimers  If you cannot access these links, please notify us by reply message and we will send the contents to you. By communicating with Morgan Stanley you consent to the foregoing and to the voice recording of conversations with personnel of Morgan Stanley.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160922/31ca7a1a/attachment.html>

From davidcholmes at aapt.net.au  Thu Sep 22 08:24:58 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 22 Sep 2016 22:24:58 +1000
Subject: [concurrency-interest] AbstractQueuedSynchronizer
In-Reply-To: <9838003A10254741BC6FFADE2ED02B457D5E4D50@OZWEX0205N1.msad.ms.com>
References: <9838003A10254741BC6FFADE2ED02B457D5E4D50@OZWEX0205N1.msad.ms.com>
Message-ID: <013801d214cc$557ec160$007c4420$@aapt.net.au>

Park() has a token associated with it, so if unpark() is called before the
park() the park() will return immediately and consume the token.

 

David

 

From: Concurrency-interest
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Bobrowski,
Maciej
Sent: Thursday, September 22, 2016 9:57 PM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] AbstractQueuedSynchronizer

 

I have tried to understand the inner working of abstractQueuedSynchronizer
and one thing I cannot quite explain.

 

When a thread decides it needs to park itself (let's say the CountDownLatch
counter > 0), it does so by calling LockSuport.park. In the meantime, of
course, the latch can be fully counted down, all awaiting threads unparked,
while this thread is just about to park itself. Obviously that would not
work, as now we have one thread parked, on a latch that is fully counted
down, so no good.

 

This does not happen actually (I wrote a small test and verified), and it is
down to the code of LockSupport.park()

 

public static void park(Object blocker) {
    Thread t = Thread.currentThread();
    setBlocker(t, blocker);
    unsafe.park(false, 0L);
    setBlocker(t, null);
}

 

Somehow, when the latch is fully counted down and unsafe.park is called, it
immediately returns, so there is some magic behind the scenes that tells the
thread not to actually park itself. Anyone could cast some light onto what
is actually happening?

 

Thanks,

Maciej

 

 

  _____  


NOTICE: Morgan Stanley is not acting as a municipal advisor and the opinions
or views contained herein are not intended to be, and do not constitute,
advice within the meaning of Section 975 of the Dodd-Frank Wall Street
Reform and Consumer Protection Act. If you have received this communication
in error, please destroy all electronic and paper copies and notify the
sender immediately. Mistransmission is not intended to waive confidentiality
or privilege. Morgan Stanley reserves the right, to the extent permitted
under applicable law, to monitor electronic communications. This message is
subject to terms available at the following link:
http://www.morganstanley.com/disclaimers  If you cannot access these links,
please notify us by reply message and we will send the contents to you. By
communicating with Morgan Stanley you consent to the foregoing and to the
voice recording of conversations with personnel of Morgan Stanley.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160922/3d9be801/attachment-0001.html>

From Maciej.Bobrowski at morganstanley.com  Thu Sep 22 08:37:18 2016
From: Maciej.Bobrowski at morganstanley.com (Bobrowski, Maciej)
Date: Thu, 22 Sep 2016 12:37:18 +0000
Subject: [concurrency-interest] AbstractQueuedSynchronizer
In-Reply-To: <013801d214cc$557ec160$007c4420$@aapt.net.au>
References: <9838003A10254741BC6FFADE2ED02B457D5E4D50@OZWEX0205N1.msad.ms.com>
 <013801d214cc$557ec160$007c4420$@aapt.net.au>
Message-ID: <9838003A10254741BC6FFADE2ED02B457D5E4EDA@OZWEX0205N1.msad.ms.com>

Thanks. So two questions:


1.       What does the set/unset blocker do?

2.       What is the token you are referring to? How does it relate to a latch, which is an entirely different object? In this case, there is no unpark called for that thread as it is not yet part of the wait queue AFAIK

From: David Holmes [mailto:davidcholmes at aapt.net.au]
Sent: 22 September 2016 13:25
To: Bobrowski, Maciej (IST); concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] AbstractQueuedSynchronizer

Park() has a token associated with it, so if unpark() is called before the park() the park() will return immediately and consume the token.

David

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Bobrowski, Maciej
Sent: Thursday, September 22, 2016 9:57 PM
To: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] AbstractQueuedSynchronizer

I have tried to understand the inner working of abstractQueuedSynchronizer and one thing I cannot quite explain.

When a thread decides it needs to park itself (let's say the CountDownLatch counter > 0), it does so by calling LockSuport.park. In the meantime, of course, the latch can be fully counted down, all awaiting threads unparked, while this thread is just about to park itself. Obviously that would not work, as now we have one thread parked, on a latch that is fully counted down, so no good.

This does not happen actually (I wrote a small test and verified), and it is down to the code of LockSupport.park()

public static void park(Object blocker) {
    Thread t = Thread.currentThread();
    setBlocker(t, blocker);
    unsafe.park(false, 0L);
    setBlocker(t, null);
}

Somehow, when the latch is fully counted down and unsafe.park is called, it immediately returns, so there is some magic behind the scenes that tells the thread not to actually park itself. Anyone could cast some light onto what is actually happening?

Thanks,
Maciej


________________________________

NOTICE: Morgan Stanley is not acting as a municipal advisor and the opinions or views contained herein are not intended to be, and do not constitute, advice within the meaning of Section 975 of the Dodd-Frank Wall Street Reform and Consumer Protection Act. If you have received this communication in error, please destroy all electronic and paper copies and notify the sender immediately. Mistransmission is not intended to waive confidentiality or privilege. Morgan Stanley reserves the right, to the extent permitted under applicable law, to monitor electronic communications. This message is subject to terms available at the following link: http://www.morganstanley.com/disclaimers  If you cannot access these links, please notify us by reply message and we will send the contents to you. By communicating with Morgan Stanley you consent to the foregoing and to the voice recording of conversations with personnel of Morgan Stanley.


________________________________

NOTICE: Morgan Stanley is not acting as a municipal advisor and the opinions or views contained herein are not intended to be, and do not constitute, advice within the meaning of Section 975 of the Dodd-Frank Wall Street Reform and Consumer Protection Act. If you have received this communication in error, please destroy all electronic and paper copies and notify the sender immediately. Mistransmission is not intended to waive confidentiality or privilege. Morgan Stanley reserves the right, to the extent permitted under applicable law, to monitor electronic communications. This message is subject to terms available at the following link: http://www.morganstanley.com/disclaimers  If you cannot access these links, please notify us by reply message and we will send the contents to you. By communicating with Morgan Stanley you consent to the foregoing and to the voice recording of conversations with personnel of Morgan Stanley.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160922/3bb19f47/attachment.html>

From akarnokd at gmail.com  Thu Sep 22 09:35:42 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Thu, 22 Sep 2016 15:35:42 +0200
Subject: [concurrency-interest] JDK 9's compareAndSet vs compareAndExchange
Message-ID: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>

JDK 9's VarHandle API (and AtomicXXX classes) specify the new
 compareAndExchange() method. Traditionally, I wrote CAS loops like this:

public static long getAddAndCap(AtomicLong requested, long n) {
    for (;;) {
        long current = requested.get();

        if (current == Long.MAX_VALUE) {
           return Long.MAX_VALUE;
        }
        long next = current + n;
        if (next < 0L) {
            next = Long.MAX_VALUE;
        }
        if (requested.compareAndSet(current, next)) {
            return current;
        }
    }
}

Now I can write this:

public static long getAddAndCap(AtomicLong requested, long n) {
    long current = requested.get();
    for (;;) {

        if (current == Long.MAX_VALUE) {
           return Long.MAX_VALUE;
        }
        long next = current + n;
        if (next < 0L) {
            next = Long.MAX_VALUE;
        }
        long actual = requested.compareAndExchange(current, next);
        if (actual == current) {
           return current;
        }
        current = actual;
    }
}

I'm not sure I could JMH benchmark these under JDK 9 now so my question is
whether the latter pattern has lower overhead (on x86) since it reuses the
value returned by the underlying LOCK CMPXCHG instead of re-reading the
target field again (or the JIT was always smart enough to detect the
unnecessary re-read with compareAndSet; or the hardware is smart enough by
doing speculative reads in this case to hide its latency ?).

-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160922/e55870cf/attachment.html>

From vitalyd at gmail.com  Thu Sep 22 10:06:41 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 22 Sep 2016 10:06:41 -0400
Subject: [concurrency-interest] JDK 9's compareAndSet vs
	compareAndExchange
In-Reply-To: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
References: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
Message-ID: <CAHjP37GfcCr1CkECRuTyXdbOnrckiA18AuHz5rttWfrh=qrH0g@mail.gmail.com>

On Thu, Sep 22, 2016 at 9:35 AM, Dávid Karnok <akarnokd at gmail.com> wrote:

> JDK 9's VarHandle API (and AtomicXXX classes) specify the new
>  compareAndExchange() method. Traditionally, I wrote CAS loops like this:
>
> public static long getAddAndCap(AtomicLong requested, long n) {
>     for (;;) {
>         long current = requested.get();
>
>         if (current == Long.MAX_VALUE) {
>            return Long.MAX_VALUE;
>         }
>         long next = current + n;
>         if (next < 0L) {
>             next = Long.MAX_VALUE;
>         }
>         if (requested.compareAndSet(current, next)) {
>             return current;
>         }
>     }
> }
>
> Now I can write this:
>
> public static long getAddAndCap(AtomicLong requested, long n) {
>     long current = requested.get();
>     for (;;) {
>
>         if (current == Long.MAX_VALUE) {
>            return Long.MAX_VALUE;
>         }
>         long next = current + n;
>         if (next < 0L) {
>             next = Long.MAX_VALUE;
>         }
>         long actual = requested.compareAndExchange(current, next);
>         if (actual == current) {
>            return current;
>         }
>         current = actual;
>     }
> }
>
> I'm not sure I could JMH benchmark these under JDK 9 now so my question is
> whether the latter pattern has lower overhead (on x86) since it reuses the
> value returned by the underlying LOCK CMPXCHG instead of re-reading the
> target field again (or the JIT was always smart enough to detect the
> unnecessary re-read with compareAndSet; or the hardware is smart enough by
> doing speculative reads in this case to hide its latency ?).
>
I don't see how the JIT could've avoided the "re-read" since the semantics
are different; you're basically asking whether it pattern matched a
CAS+get(), assumed the intention was CAE, and fused that as CAE
internally.  I'm pretty sure the answer is no.

It's very likely the subsequent get() is a L1 hitting load (unless there's
very serious contention and the cacheline is invalidated again), so it
shouldn't be too bad.  However, additional atomic ops per loop iteration
will serve as barriers for JIT optimizations, which may play some role in
effective performance.  Although in your particular example above, if
CAS/CAE fails multiple times, your performance will likely be dominated by
the associated coherence traffic.

>
> --
> Best regards,
> David Karnok
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160922/606d01bf/attachment-0001.html>

From shade at redhat.com  Thu Sep 22 10:20:21 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Thu, 22 Sep 2016 16:20:21 +0200
Subject: [concurrency-interest] JDK 9's compareAndSet vs
 compareAndExchange
In-Reply-To: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
References: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
Message-ID: <cc8cb16f-cc66-e98e-510a-08914c315ec2@redhat.com>

Hi,

Somewhat counter-intuitively, reusing compareAndExchange return value as
the basis for retry is slower than re-reading actual value for
compareAndSet. Because under contention, the overall progress depends on
the width on the "collision window" for your RMW operation:
 https://bugs.openjdk.java.net/browse/JDK-8141640

compareAndExchange is useful when you *have* to have the "witness value"
against which you failed. Choosing compareAndExchange over compareAndSet
within the loop is futile. Choosing compareAndExchange *instead* of
compareAndSet + re-reads is sane, e.g. in
https://github.com/boundary/high-scale-lib/blob/master/src/main/java/org/cliffc/high_scale_lib/NonBlockingHashMap.java#L583

Thanks,
-Aleksey

On 09/22/2016 03:35 PM, Dávid Karnok wrote:
> JDK 9's VarHandle API (and AtomicXXX classes) specify the new
>  compareAndExchange() method. Traditionally, I wrote CAS loops like this:
> 
> public static long getAddAndCap(AtomicLong requested, long n) {
>     for (;;) {
>         long current = requested.get();
> 
>         if (current == Long.MAX_VALUE) {
>            return Long.MAX_VALUE;
>         }
>         long next = current + n;
>         if (next < 0L) {
>             next = Long.MAX_VALUE;
>         }
>         if (requested.compareAndSet(current, next)) {
>             return current;
>         }
>     }
> }
> 
> Now I can write this:
> 
> public static long getAddAndCap(AtomicLong requested, long n) {
>     long current = requested.get();
>     for (;;) {
> 
>         if (current == Long.MAX_VALUE) {
>            return Long.MAX_VALUE;
>         }
>         long next = current + n;
>         if (next < 0L) {
>             next = Long.MAX_VALUE;
>         }
>         long actual = requested.compareAndExchange(current, next);
>         if (actual == current) {
>            return current;
>         }
>         current = actual;
>     }
> }
> 
> I'm not sure I could JMH benchmark these under JDK 9 now so my question
> is whether the latter pattern has lower overhead (on x86) since it
> reuses the value returned by the underlying LOCK CMPXCHG instead of
> re-reading the target field again (or the JIT was always smart enough to
> detect the unnecessary re-read with compareAndSet; or the hardware is
> smart enough by doing speculative reads in this case to hide its latency ?).
> 
> -- 
> Best regards,
> David Karnok
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160922/3959749c/attachment.sig>

From martinrb at google.com  Thu Sep 22 10:50:41 2016
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 22 Sep 2016 07:50:41 -0700
Subject: [concurrency-interest] AbstractQueuedSynchronizer
In-Reply-To: <9838003A10254741BC6FFADE2ED02B457D5E4EDA@OZWEX0205N1.msad.ms.com>
References: <9838003A10254741BC6FFADE2ED02B457D5E4D50@OZWEX0205N1.msad.ms.com>
 <013801d214cc$557ec160$007c4420$@aapt.net.au>
 <9838003A10254741BC6FFADE2ED02B457D5E4EDA@OZWEX0205N1.msad.ms.com>
Message-ID: <CA+kOe0_x-SNoS7qiYH898F3XgZ_8XJvZvST_U8x28XKv7unVOg@mail.gmail.com>

On Thu, Sep 22, 2016 at 5:37 AM, Bobrowski, Maciej <
Maciej.Bobrowski at morganstanley.com> wrote:

> Thanks. So two questions:
>
>
>
> 1.       What does the set/unset blocker do?
>
Just for monitoring (identifying lock owner in stack traces), not
concurrency control.

> 2.       What is the token you are referring to? How does it relate to a
> latch, which is an entirely different object? In this case, there is no
> unpark called for that thread as it is not yet part of the wait queue AFAIK
>
When a thread can't make progress, it publishes a request to unpark in some
shared data structure, then parks.  If the unpark racily arrives before the
park, the park returns immediately.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160922/aa691361/attachment.html>

From vitalyd at gmail.com  Thu Sep 22 11:06:13 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 22 Sep 2016 11:06:13 -0400
Subject: [concurrency-interest] JDK 9's compareAndSet vs
	compareAndExchange
In-Reply-To: <cc8cb16f-cc66-e98e-510a-08914c315ec2@redhat.com>
References: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
 <cc8cb16f-cc66-e98e-510a-08914c315ec2@redhat.com>
Message-ID: <CAHjP37GrfwH6hkqOowEBLxNbAJfjn+bXVY0Tz=KZ3HQm4xGSsw@mail.gmail.com>

On Thu, Sep 22, 2016 at 10:20 AM, Aleksey Shipilev <shade at redhat.com> wrote:

> Hi,
>
> Somewhat counter-intuitively, reusing compareAndExchange return value as
> the basis for retry is slower than re-reading actual value for
> compareAndSet. Because under contention, the overall progress depends on
> the width on the "collision window" for your RMW operation:
>  https://bugs.openjdk.java.net/browse/JDK-8141640

Interesting result.  What CPU was this test done on, if you recall? The #
of cycles in the "collision window" in that benchmark is fairly small (just
a bunch of cheap ALU instructions and register-to-register movs that ought
to be renamed).  Did you look at any PMU counters that would back up the
theory?

Thanks

>
>
> compareAndExchange is useful when you *have* to have the "witness value"
> against which you failed. Choosing compareAndExchange over compareAndSet
> within the loop is futile. Choosing compareAndExchange *instead* of
> compareAndSet + re-reads is sane, e.g. in
> https://github.com/boundary/high-scale-lib/blob/master/
> src/main/java/org/cliffc/high_scale_lib/NonBlockingHashMap.java#L583
>
> Thanks,
> -Aleksey
>
> On 09/22/2016 03:35 PM, Dávid Karnok wrote:
> > JDK 9's VarHandle API (and AtomicXXX classes) specify the new
> >  compareAndExchange() method. Traditionally, I wrote CAS loops like this:
> >
> > public static long getAddAndCap(AtomicLong requested, long n) {
> >     for (;;) {
> >         long current = requested.get();
> >
> >         if (current == Long.MAX_VALUE) {
> >            return Long.MAX_VALUE;
> >         }
> >         long next = current + n;
> >         if (next < 0L) {
> >             next = Long.MAX_VALUE;
> >         }
> >         if (requested.compareAndSet(current, next)) {
> >             return current;
> >         }
> >     }
> > }
> >
> > Now I can write this:
> >
> > public static long getAddAndCap(AtomicLong requested, long n) {
> >     long current = requested.get();
> >     for (;;) {
> >
> >         if (current == Long.MAX_VALUE) {
> >            return Long.MAX_VALUE;
> >         }
> >         long next = current + n;
> >         if (next < 0L) {
> >             next = Long.MAX_VALUE;
> >         }
> >         long actual = requested.compareAndExchange(current, next);
> >         if (actual == current) {
> >            return current;
> >         }
> >         current = actual;
> >     }
> > }
> >
> > I'm not sure I could JMH benchmark these under JDK 9 now so my question
> > is whether the latter pattern has lower overhead (on x86) since it
> > reuses the value returned by the underlying LOCK CMPXCHG instead of
> > re-reading the target field again (or the JIT was always smart enough to
> > detect the unnecessary re-read with compareAndSet; or the hardware is
> > smart enough by doing speculative reads in this case to hide its latency
> ?).
> >
> > --
> > Best regards,
> > David Karnok
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160922/6e6e4820/attachment.html>

From martinrb at google.com  Thu Sep 22 17:27:01 2016
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 22 Sep 2016 14:27:01 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CABY0rKPD54W8d7DNHM+BKsokdZ5PMDvm_qSMB5cJyBV30AOW3w@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CABY0rKPD54W8d7DNHM+BKsokdZ5PMDvm_qSMB5cJyBV30AOW3w@mail.gmail.com>
Message-ID: <CA+kOe0_hgbj=qJQXL-1vG7WdLD-OpDrKJ2R_PbRDdb+C+irC6A@mail.gmail.com>

Thanks for the lesson, James!

On Wed, Sep 21, 2016 at 3:57 PM, James Roper <james at lightbend.com> wrote:

> On 22 September 2016 at 06:43, Martin Buchholz <martinrb at google.com>
> wrote:
>
>> What is happening instead is API providers not using CompletionStage as
>> return values in public APIs because of the lack of convenient blocking,
>> and instead returning CompletableFuture, which is a tragic software
>> engineering failure.
>>
>
> Out of interest, which APIs are returning CompletableFuture rather than
> CompletionStage?  In the Play Framework Java API, we have embraced
> CompletionStage, we use it absolutely everywhere.  Likewise in Lagom
> Framework and the Java API for Akka streams.
>
> When it comes to blocking, we strongly advocate never blocking (in the
> threading models of these projects, blocking on a future will make you very
> prone to deadlocks).
>

I took a look at the Scala/Akka/LightBend world.  Even there, blocking
always remains a possibility, even if discouraged, e.g.
scala.concurrent.Future extends Awaitable (!), and
http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#Blocking_Needs_Careful_Management
.
And any general purpose Java API needs to be useful outside of a framework.


>   But of course, the exception is junit tests, in that case, we encourage
> the use of CompletionStage.toCompletableFuture to block.
>

We're currently fixing jdk9
CompletableFuture.minimalCompletionStage().toCompletableFuture() to be
awaitable.

To make toCompletableFuture().join() more reliable as a recommended way to
block, I think we should remove the encouragement to throw UOE from:
http://download.java.net/java/jdk9/docs/api/java/util/concurrent/CompletionStage.html#toCompletableFuture--

It sounds like the Akka/LightBend World is happy with current
CompletionStage API (may I call this the "actor purist API"?), while other
users will want a bigger, more general CompletableFuture subset for
consuming future values.  Frustratingly, some of them will want cancel(),
some not; and we have no good names for any new interfaces; right now all I
can think of is CompletionStage2 and CompletionStage3 !)

The current implementation of CompletableFuture has a "memory leak problem"
in that e.g. minimalStage().toCompletableFuture().isDone() will leak a
Completion object until the source stage itself is collected.  Which
doesn't happen if e.g. a direct isDone() is provided.
JDK-8161600: Garbage retention when source CompletableFutures are never
completed
(but no one has ever complained!)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160922/827df06c/attachment.html>

From davidcholmes at aapt.net.au  Thu Sep 22 17:43:07 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 23 Sep 2016 07:43:07 +1000
Subject: [concurrency-interest] AbstractQueuedSynchronizer
In-Reply-To: <CA+kOe0_x-SNoS7qiYH898F3XgZ_8XJvZvST_U8x28XKv7unVOg@mail.gmail.com>
References: <9838003A10254741BC6FFADE2ED02B457D5E4D50@OZWEX0205N1.msad.ms.com>
 <013801d214cc$557ec160$007c4420$@aapt.net.au>
 <9838003A10254741BC6FFADE2ED02B457D5E4EDA@OZWEX0205N1.msad.ms.com>
 <CA+kOe0_x-SNoS7qiYH898F3XgZ_8XJvZvST_U8x28XKv7unVOg@mail.gmail.com>
Message-ID: <017e01d2151a$4ea4ec90$ebeec5b0$@aapt.net.au>

Just to add on to #2. The token is part of the park/unpark implementation – as described in its Javadoc.

 

There is an inherent race between a thread enqueuing itself to await for a condition, releasing all associated “locks” and calling park() to actually suspend itself. If the condition changes before the park() occurs we need to ensure that the park() will return as soon as it is actually called, and unpark() achieves that by setting the token. So the thread that is changing the state of the AQS sees that a thread is queued, unqueues it and unpark()s the thread.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin Buchholz
Sent: Friday, September 23, 2016 12:51 AM
To: Bobrowski, Maciej <Maciej.Bobrowski at morganstanley.com>
Cc: concurrency-interest at cs.oswego.edu; dholmes at ieee.org
Subject: Re: [concurrency-interest] AbstractQueuedSynchronizer

 

 

 

On Thu, Sep 22, 2016 at 5:37 AM, Bobrowski, Maciej <Maciej.Bobrowski at morganstanley.com <mailto:Maciej.Bobrowski at morganstanley.com> > wrote:

Thanks. So two questions:

 

1.       What does the set/unset blocker do?

Just for monitoring (identifying lock owner in stack traces), not concurrency control.

2.       What is the token you are referring to? How does it relate to a latch, which is an entirely different object? In this case, there is no unpark called for that thread as it is not yet part of the wait queue AFAIK

When a thread can't make progress, it publishes a request to unpark in some shared data structure, then parks.  If the unpark racily arrives before the park, the park returns immediately.

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160923/833e3085/attachment.html>

From kasperni at gmail.com  Fri Sep 23 02:45:42 2016
From: kasperni at gmail.com (Kasper Nielsen)
Date: Fri, 23 Sep 2016 08:45:42 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
Message-ID: <CAPs6152C=_fc9sn9bH22vyPCe5k6oGWQ5qrwXsukw2GLFqDXwA@mail.gmail.com>

On 21 September 2016 at 22:43, Martin Buchholz <martinrb at google.com> wrote:

> (Sorry to re-open this discussion)
>
> The separation of a read-only CompletionStage from CompletableFuture is
> great.  I'm a fan of the scala style Promise/Future split as described in
> http://docs.scala-lang.org/overviews/core/futures.html, but: we need to
> re-add (safe, read-only) blocking methods like join.


Just want to say that I agree. I have using CS/CF for APIs extensively
since Java 8. And all my usage basically boils down to 3 basic use cases.
And I think most others will end up with the same use cases.

1) Observing
The user receives a CS where he can query about the state of the future and
add follow up actions. I really would like to see the rest of the
non-mutating methods from CompletableFuture added to CompletionStage here
if possible.
get()
get(long, TimeUnit)
getNow()
isCompletedExceptionally
isDone()
isCompletedNormally() (isDone && !isCompletedExceptionally) <- new method

2) Cancellable
Tasks that can be cancelled by the user but where the user should not be
able to modify the result or set a custom exception.
For example, cancel connecting to a remote host or cancel some internal
computation.

Right not this is a bit painfull. I have to wrap CompletableFuture to allow
cancel but hide complete/obtrude

Would not mind a CancellableCompletionStage interface
CancellableCompletionStage extends CompletionStage {
  isCancelled();
  cancel(boolean)
}

3) Mutable
The user has complete control. Basically just these three additional
methods compared to CancellableCompletionStage.
  complete()
  completeExceptionally()
  obtrudeException()
I'm fine with returning CompletableFuture here.

Best
  Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160923/1448b009/attachment.html>

From akarnokd at gmail.com  Fri Sep 23 03:11:20 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Fri, 23 Sep 2016 09:11:20 +0200
Subject: [concurrency-interest] JDK 9's compareAndSet vs
	compareAndExchange
In-Reply-To: <CAHjP37GrfwH6hkqOowEBLxNbAJfjn+bXVY0Tz=KZ3HQm4xGSsw@mail.gmail.com>
References: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
 <cc8cb16f-cc66-e98e-510a-08914c315ec2@redhat.com>
 <CAHjP37GrfwH6hkqOowEBLxNbAJfjn+bXVY0Tz=KZ3HQm4xGSsw@mail.gmail.com>
Message-ID: <CAAWwtm_79hj3MzAnCMJk-y5Gt91VjWCr5Jb53AjxtHCHgmawfA@mail.gmail.com>

Thanks for the information.

FYI, I do have places where compareAndExchange is useful and not part of a
loop:

public static boolean setOnce(VarHandle h, Object instance, Subscription s)
{
    Subscription o = (Subscription)h.compareAndExchange(instance, null, s);
    if (o != null) {
        s.cancel();
        if (o != CANCELLED) {
            CatchAll.onError(new IllegalStateException("Subscription
already set!"));
        }
        return false;
    }
    return true;
}



2016-09-22 17:06 GMT+02:00 Vitaly Davidovich <vitalyd at gmail.com>:

>
>
> On Thu, Sep 22, 2016 at 10:20 AM, Aleksey Shipilev <shade at redhat.com>
> wrote:
>
>> Hi,
>>
>> Somewhat counter-intuitively, reusing compareAndExchange return value as
>> the basis for retry is slower than re-reading actual value for
>> compareAndSet. Because under contention, the overall progress depends on
>> the width on the "collision window" for your RMW operation:
>>  https://bugs.openjdk.java.net/browse/JDK-8141640
>
> Interesting result.  What CPU was this test done on, if you recall? The #
> of cycles in the "collision window" in that benchmark is fairly small (just
> a bunch of cheap ALU instructions and register-to-register movs that ought
> to be renamed).  Did you look at any PMU counters that would back up the
> theory?
>
> Thanks
>
>>
>>
>> compareAndExchange is useful when you *have* to have the "witness value"
>> against which you failed. Choosing compareAndExchange over compareAndSet
>> within the loop is futile. Choosing compareAndExchange *instead* of
>> compareAndSet + re-reads is sane, e.g. in
>> https://github.com/boundary/high-scale-lib/blob/master/src/
>> main/java/org/cliffc/high_scale_lib/NonBlockingHashMap.java#L583
>>
>> Thanks,
>> -Aleksey
>>
>> On 09/22/2016 03:35 PM, Dávid Karnok wrote:
>> > JDK 9's VarHandle API (and AtomicXXX classes) specify the new
>> >  compareAndExchange() method. Traditionally, I wrote CAS loops like
>> this:
>> >
>> > public static long getAddAndCap(AtomicLong requested, long n) {
>> >     for (;;) {
>> >         long current = requested.get();
>> >
>> >         if (current == Long.MAX_VALUE) {
>> >            return Long.MAX_VALUE;
>> >         }
>> >         long next = current + n;
>> >         if (next < 0L) {
>> >             next = Long.MAX_VALUE;
>> >         }
>> >         if (requested.compareAndSet(current, next)) {
>> >             return current;
>> >         }
>> >     }
>> > }
>> >
>> > Now I can write this:
>> >
>> > public static long getAddAndCap(AtomicLong requested, long n) {
>> >     long current = requested.get();
>> >     for (;;) {
>> >
>> >         if (current == Long.MAX_VALUE) {
>> >            return Long.MAX_VALUE;
>> >         }
>> >         long next = current + n;
>> >         if (next < 0L) {
>> >             next = Long.MAX_VALUE;
>> >         }
>> >         long actual = requested.compareAndExchange(current, next);
>> >         if (actual == current) {
>> >            return current;
>> >         }
>> >         current = actual;
>> >     }
>> > }
>> >
>> > I'm not sure I could JMH benchmark these under JDK 9 now so my question
>> > is whether the latter pattern has lower overhead (on x86) since it
>> > reuses the value returned by the underlying LOCK CMPXCHG instead of
>> > re-reading the target field again (or the JIT was always smart enough to
>> > detect the unnecessary re-read with compareAndSet; or the hardware is
>> > smart enough by doing speculative reads in this case to hide its
>> latency ?).
>> >
>> > --
>> > Best regards,
>> > David Karnok
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160923/5b8e8adc/attachment-0001.html>

From kasperni at gmail.com  Fri Sep 23 03:41:05 2016
From: kasperni at gmail.com (Kasper Nielsen)
Date: Fri, 23 Sep 2016 09:41:05 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CAPs6152C=_fc9sn9bH22vyPCe5k6oGWQ5qrwXsukw2GLFqDXwA@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CAPs6152C=_fc9sn9bH22vyPCe5k6oGWQ5qrwXsukw2GLFqDXwA@mail.gmail.com>
Message-ID: <CAPs61520c7eGJcp6S61zPme=os2S+xFu3xH_NMCpS4CVepaaHw@mail.gmail.com>

>
> Would not mind a CancellableCompletionStage interface
> CancellableCompletionStage extends CompletionStage {
>   isCancelled();
>   cancel(boolean)
> }
>
>
Just wanted to note. This is not just a question about functionality. It is
also a signal to users
about "hey this a computation you can cancel". For example, if you where to
retrofit all of
java.nio you would probably use CancellableCompletionStage (or whatever the
name)
for all methods. You do not really need to set an int or whatever the
read/write methods return
in 99% of all cases.

Best
  Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160923/abbc2d04/attachment.html>

From shade at redhat.com  Fri Sep 23 05:16:38 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Fri, 23 Sep 2016 11:16:38 +0200
Subject: [concurrency-interest] JDK 9's compareAndSet vs
 compareAndExchange
In-Reply-To: <CAAWwtm_79hj3MzAnCMJk-y5Gt91VjWCr5Jb53AjxtHCHgmawfA@mail.gmail.com>
References: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
 <cc8cb16f-cc66-e98e-510a-08914c315ec2@redhat.com>
 <CAHjP37GrfwH6hkqOowEBLxNbAJfjn+bXVY0Tz=KZ3HQm4xGSsw@mail.gmail.com>
 <CAAWwtm_79hj3MzAnCMJk-y5Gt91VjWCr5Jb53AjxtHCHgmawfA@mail.gmail.com>
Message-ID: <4583de9f-ed55-6087-db77-26bf7a9b1adc@redhat.com>

On 09/23/2016 09:11 AM, Dávid Karnok wrote:
> Thanks for the information.
> 
> FYI, I do have places where compareAndExchange is useful and not part of
> a loop:
> 
> public static boolean setOnce(VarHandle h, Object instance, Subscription
> s) {
>     Subscription o = (Subscription)h.compareAndExchange(instance, null, s);
>     if (o != null) {
>         s.cancel();
>         if (o != CANCELLED) {
>             CatchAll.onError(new IllegalStateException("Subscription
> already set!"));
>         }
>         return false;
>     }
>     return true;
> }

This seems like a fair use.

Not sure this is significantly better than compareAndSet + re-read on
failure, because the state changes seem only monotonic (e.g. from null
to one canonical instance), and re-read would not produce false results.

The caveat for VarHandles though is, $h is better to come from the
(static final) constant, otherwise you will have lots of unfolded checks
in VH mechanics, which may affect performance even more.

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160923/e0a2d3d2/attachment.sig>

From akarnokd at gmail.com  Fri Sep 23 05:36:57 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Fri, 23 Sep 2016 11:36:57 +0200
Subject: [concurrency-interest] JDK 9's compareAndSet vs
	compareAndExchange
In-Reply-To: <4583de9f-ed55-6087-db77-26bf7a9b1adc@redhat.com>
References: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
 <cc8cb16f-cc66-e98e-510a-08914c315ec2@redhat.com>
 <CAHjP37GrfwH6hkqOowEBLxNbAJfjn+bXVY0Tz=KZ3HQm4xGSsw@mail.gmail.com>
 <CAAWwtm_79hj3MzAnCMJk-y5Gt91VjWCr5Jb53AjxtHCHgmawfA@mail.gmail.com>
 <4583de9f-ed55-6087-db77-26bf7a9b1adc@redhat.com>
Message-ID: <CAAWwtm9gygLJMRw0_qrL9xuvtAj2U--G1-XfYS7ysA23_CK0vA@mail.gmail.com>

>
> This seems like a fair use.
>
> Not sure this is significantly better than compareAndSet + re-read on
> failure, because the state changes seem only monotonic (e.g. from null
> to one canonical instance), and re-read would not produce false results.
>
>
The code supposed to detect multiple calls to a setOnce. If the witness is
not the CANCELLED instance, that is considered a bug (detected by looping a
bit in unit tests - not perfect but helped many times).


> The caveat for VarHandles though is, $h is better to come from the
> (static final) constant, otherwise you will have lots of unfolded checks
> in VH mechanics, which may affect performance even more.
>
>
I was considering asking this separately. Former code used either
AtomicReferenceFieldUpdater + instance or AtomicReference as input
parameter. I was under the assumption that the method would get inlined and
the $h is $this.field and $instance becomes $this.

public static <T> boolean setOnce(AtomicReferenceFieldUpdater<T,
Flow.Subscription>
h, T instance, Flow.Subscription s) {
    if (h.compareAndSet(instance, null, s)) {
        s.cancel();
        if (h.get(instance) != CANCELLED) {
            CatchAll.onError(new IllegalStateException("Subscription
already set!"));
        }
        return false;
    }
    return true;
}

final class SomeOperator implements Flow.Subscriber<Object> {
    volatile Flow.Subscription s;
    static final AtomicReferenceFieldUpdater<SomeOperator, Flow.Subscription>
S
        = AtomicReferenceFieldUpdater.newUpdater(SomeOperator.class,
Flow.Subscription.class,
"s");

   public void onSubscribe(Flow.Subscription s) {
       if (SubscriptionHelper.setOnce(S, this, s)) {
           s.request(Long.MAX_VALUE);
       }
   }

   // ...
}





> Thanks,
> -Aleksey
>
>


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160923/f53e4248/attachment.html>

From shade at redhat.com  Fri Sep 23 05:57:58 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Fri, 23 Sep 2016 11:57:58 +0200
Subject: [concurrency-interest] JDK 9's compareAndSet vs
 compareAndExchange
In-Reply-To: <CAAWwtm9gygLJMRw0_qrL9xuvtAj2U--G1-XfYS7ysA23_CK0vA@mail.gmail.com>
References: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
 <cc8cb16f-cc66-e98e-510a-08914c315ec2@redhat.com>
 <CAHjP37GrfwH6hkqOowEBLxNbAJfjn+bXVY0Tz=KZ3HQm4xGSsw@mail.gmail.com>
 <CAAWwtm_79hj3MzAnCMJk-y5Gt91VjWCr5Jb53AjxtHCHgmawfA@mail.gmail.com>
 <4583de9f-ed55-6087-db77-26bf7a9b1adc@redhat.com>
 <CAAWwtm9gygLJMRw0_qrL9xuvtAj2U--G1-XfYS7ysA23_CK0vA@mail.gmail.com>
Message-ID: <f8dd863d-f9d3-028f-9855-c85bedd9f3eb@redhat.com>



On 09/23/2016 11:36 AM, Dávid Karnok wrote:
>     The caveat for VarHandles though is, $h is better to come from the
>     (static final) constant, otherwise you will have lots of unfolded checks
>     in VH mechanics, which may affect performance even more.
> 
> 
> I was considering asking this separately. Former code used either
> AtomicReferenceFieldUpdater + instance or AtomicReference as input
> parameter. I was under the assumption that the method would get inlined
> and the $h is $this.field and $instance becomes $this.

Yes, that should happen. But on off-chance inlining breaks, it is a good
idea to have the accessor methods closer to A*FU/VH fields, so that you
can reference them directly. This is a common practice for using A*FU
(and by extension, VHs). Passing things around may run into surprises.

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160923/6ef94171/attachment.sig>

From akarnokd at gmail.com  Fri Sep 23 06:07:25 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Fri, 23 Sep 2016 12:07:25 +0200
Subject: [concurrency-interest] JDK 9's compareAndSet vs
	compareAndExchange
In-Reply-To: <f8dd863d-f9d3-028f-9855-c85bedd9f3eb@redhat.com>
References: <CAAWwtm-arN-7+3eV9psKGjfKULnkMnw5LOT6=7ETCHA44PG1vg@mail.gmail.com>
 <cc8cb16f-cc66-e98e-510a-08914c315ec2@redhat.com>
 <CAHjP37GrfwH6hkqOowEBLxNbAJfjn+bXVY0Tz=KZ3HQm4xGSsw@mail.gmail.com>
 <CAAWwtm_79hj3MzAnCMJk-y5Gt91VjWCr5Jb53AjxtHCHgmawfA@mail.gmail.com>
 <4583de9f-ed55-6087-db77-26bf7a9b1adc@redhat.com>
 <CAAWwtm9gygLJMRw0_qrL9xuvtAj2U--G1-XfYS7ysA23_CK0vA@mail.gmail.com>
 <f8dd863d-f9d3-028f-9855-c85bedd9f3eb@redhat.com>
Message-ID: <CAAWwtm_Br4ZmYmxKHUbFPt=g87Kj35XEb4QCGHb1gZXX2eny6A@mail.gmail.com>

>
> Yes, that should happen. But on off-chance inlining breaks, it is a good
> idea to have the accessor methods closer to A*FU/VH fields, so that you
> can reference them directly. This is a common practice for using A*FU
> (and by extension, VHs). Passing things around may run into surprises.
>
>
Thanks. (Too bad the utility method call appears 200+ times in my codebase
along with other similar helper methods).


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160923/f6e102cc/attachment-0001.html>

From viktor.klang at gmail.com  Fri Sep 23 17:24:55 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 23 Sep 2016 23:24:55 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
Message-ID: <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>

Hi Martin,

*Unsurprisingly*, I think it is a bad idea to pollute something which was
created as a non-blocking superset intended to provide maximum utility with
minimum of sharp edges.

However, I think you have a point in that toCompletableFuture throwing UOE
is rather unhelpful, and if some people are huge fans of parking threads
then let's see if we can come to terms on a solution which doesn't
compromise the solution for everyone else.

The subject of this thread, "We need to add blocking methods to
CompletionStage!", gave me a few questions: "who is 'we'?", "when do we
need blocking methods?" and "Why is CompletionStage the right place to add
this?"

I think it's great that you point out that Scala's Future (full disclosure:
I am a co-designer of that) extends the Awaitable[1] trait (think
interface).
It is worth pointing out that those methods are not callable on the
instance directly (notice the implicit evidence type) so all invocations
need to go through an external construct, Await, which makes it abundantly
clear that something different is going to happen.

I wrote a long explanation of the design process (at least parts of it)
here[2], but I'm including the relevant section on Await below:

There’s an underlying question that I think is worth answering: “Why does
Await.result exist, and why doesn’t it exist *on* Future?”

When we designed Scala’s Future one thing we based it on was the experience
with Akka Future (
http://doc.akka.io/api/akka/1.3.1/?_ga=1.21043707.1579561034.1353497989#akka.dispatch.Future
)

Note how it had `get` and `await` methods, which do similar things as the
C# `Result` method — block the currently executing Thread from progressing
until the value is available. Such a method is the counterpart to
asynchronous, it is synchronizing the current Thread with the Thread which
executes the Future — i.e. an API for synchronous programming.

Not only did we find that methods like that introduce performance problems
due to blocking Threads (delay until the value is available but also due to
Thread scheduler wakeup lag), but these methods also produce programs that
are difficult to reason about since they can deadlock and become
non-deterministic in their runtime behavior as things like GC-pauses etc
may cause either spurious failures (where timeouts are supplied) or
prolonged resource starvation due to unavailability of Threads to execute
other logic.

Having a single method for performing these potentially dangerous
operations and putting it outside of the Future API itself meant that it is
easy to spot where blocking is performed as well as easy to outlaw it (by
disallowing it to be present in source code). But we also took it a step
further, by creating the BlockContext mechanism we also made it possible to
the runtime to perform evasive manoeuvres in the presence of blocking. (
http://www.scala-lang.org/api/current/index.html#scala.concurrent.BlockContext
)






*Option 1:*

Now, if Java's type system was just a tad more powerful, it would support
intersection types and APIs expressing the problem of wanting to expose
more than CompletionStage but less than the concrete CompletableFuture type
could very easily return `*CompletionStage[T] with Future[T]*`.

Now we don't really have that, so it needs to be poorly emulated using
compound interfaces, becoming something like `*FutureCompletionStage[T]*`
which would have the `join`-method on Future. And that may actually not be
a bad idea at all. So let's call that *Option 1.*



*Option 2:*

Add a `toFuture` method to CompletionStage such that if you want to go and
use the blocking methods of Future, it is both shorter and does not expose
a concrete type in the signature.

*Futher proposals:*

* Adding a constructor to `CompletableFuture` which takes a
`CompletionStage` as parameter would make the boilerplate discussed
earlier[3] obsolete.

* I'd like to see if we could make the `toCompletableFuture` be a default
method which delegates to `new CompletableFuture<>(this)`, meaning that the
UOE problem would sort of solvable.



*Summary:*

* I think there is already a great place to host blocking methods:
j.u.c.Future

* We can get around the UOE by introducing the constructor on
CompletableFuture both as a fallback to things which do throw UOE on
toCompletableFuture, but also provides a terrific default implementation
for toCompletableFuture.

* We can introduce a toFuture-method with a default implementation which
calls `new CompletableFuture<>(this)`


Have a great weekend!



*Footnotes:*
1:
http://www.scala-lang.org/files/archive/api/2.11.8/index.html#scala.concurrent.Awaitable
2: https://medium.com/@viktorklang/hi-eef4acf316a8#.uesy1fqgo
3: static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T>
stage) { … }


PS. As a sidenote, Martin, and in all friendliness, "actor purist API"?
C'mon, I know you're better than that! CompletionStage's design has nothing
to do with Actors and if Single Responsibility Principle is considered
purism then I'm not sure why we don't have a single interface with all
methods in it.
Let's try to keep things friendly.

PPS: A misunderstanding is that CompletionStage represents a running task
of sorts, one that can be cancelled etc. This is not the case.

PPPS: Adding blocking methods without mandatory timeouts has in practice
proven to be a recipe for disaster.

PPPPS: "I think it's unreasonable to not provide this for users (especially
when we can do so more efficiently)." <- If efficiency is desired then
blocking is definitely not the right solution.

PPPPPS: I'm currently moving cross-country so there will be delays of any
responses from me


On Wed, Sep 21, 2016 at 10:43 PM, Martin Buchholz <martinrb at google.com>
wrote:

> (Sorry to re-open this discussion)
>
> The separation of a read-only CompletionStage from CompletableFuture is
> great.  I'm a fan of the scala style Promise/Future split as described in
> http://docs.scala-lang.org/overviews/core/futures.html, but: we need to
> re-add (safe, read-only) blocking methods like join.  Java is not Node.js,
> where there are no threads but there is a universal event loop.  Java
> programmers are used to Future, where the *only* way to use a future's
> value is to block waiting for it.  The existing CompletionStage methods are
> a better scaling alternative to blocking all the time, but blocking is
> almost always eventually necessary in Java.  For example, junit test
> methods that start any asynchronous computation need to block until the
> computation is done, before returning.
>
> As Viktor has pointed out, users can always implement blocking themselves
> by writing
>
>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T>
> stage) {
>         CompletableFuture<T> f = new CompletableFuture<>();
>         stage.handle((T t, Throwable ex) -> {
>                          if (ex != null) f.completeExceptionally(ex);
>                          else f.complete(t);
>                          return null;
>                      });
>         return f;
>     }
>
>     static <T> T join(CompletionStage<T> stage) {
>         return toCompletableFuture(stage).join();
>     }
>
> but unlike Viktor, I think it's unreasonable to not provide this for users
> (especially when we can do so more efficiently).  What is happening instead
> is API providers not using CompletionStage as return values in public APIs
> because of the lack of convenient blocking, and instead returning
> CompletableFuture, which is a tragic software engineering failure.
>
> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture
> from throwing UnsupportedOperationException, and implement join as:
>
>     public default T join() { return toCompletableFuture().join(); }
>
> There is a risk of multiple-inheritance conflict with Future if we add
> e.g. isDone(), but there are no current plans to turn those Future methods
> into default methods, and even if we did in some future release, it would
> be only a source, not binary incompatibility, so far less serious.
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160923/159e0eba/attachment.html>

From akarnokd at gmail.com  Sat Sep 24 09:19:16 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Sat, 24 Sep 2016 15:19:16 +0200
Subject: [concurrency-interest] AtomicReference get vs. getAcquire;
	get/set Opaque
Message-ID: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>

I have a one element single-producer single-consumer "queue" implemented as
this:

boolean offer(AtomicReference<T> ref, T value) {
    Objects.requireNonNull(value);
    if (ref.get() == null) {
        ref.lazySet(value);
        return true;
    }
    return false;
}

T poll(AtomicReference<T> ref) {
    T v = ref.get();
    if (v != null) {
       ref.lazySet(null);
    }
    return v;
}

Is it okay to turn get() into getAcquire() and lazySet into setRelease() (I
can see lazySet delegates to setRelease)? More generally, are there
consequences turning CAS loops of get()+compareAndSet into getAcquire() +
weakCompareAndSetRelease?

In addition, are there examples (or explanations) of when it is okay to use
getOpaque/setOpaque  and would they work with the concurrent "queue" above
at all?

(I guess there is not much difference for x86 but my code may end up on
weaker platforms eventually.)



-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/17b53961/attachment-0001.html>

From akarnokd at gmail.com  Sat Sep 24 10:08:38 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Sat, 24 Sep 2016 16:08:38 +0200
Subject: [concurrency-interest] JDK 9 Flow javadoc example SampleSubscriber
	requesting
Message-ID: <CAAWwtm-apZR22iiBemApHC_oeYOgKprARMa7jbrG4d0GGjhAmQ@mail.gmail.com>

Given a synchronous end-consumer Flow.Subscriber of a Flow.Publisher,
generally there is no reason to request in parts/batches and almost all
should be able to request Long.MAX_VALUE at the beginning. The reason for
it is that such consumers have a natural call-stack blocking and until the
onNext returns, they won't receive another onNext.

One of the few reasons one would use such batching if the onNext
implementation goes async to execute the consumer.accept:

public void onNext(T item) {
  singleThreadedExecutor.execute(() -> {
    if (--count <= 0)
      subscription.request(count = bufferSize - bufferSize / 2);
      consumer.accept(item);
    }
  });
}

Batching has more sense in operators built with the help of Subscribers
(like flatMap and observeOn) but those are too complicated in on themselves
to be shown as examples.

I can understand the example is to show off the features/patterns one can
utilize but it might be worth mentioning the effects of synchronous
consumption with respect to requesting.

Another reason to clarify is that people have consumers implemented as such
in RxJava and wondered why the top source didn't respect their request
amount: it generated 128 elements upfront even though they requested 1 over
a chain of various async stages (and in fact received only one but there
was another 127 ready to be emitted).

RS requesting is a property between two subsequent stages and each stage
can decide - while respecting the spec - how much to prefetch and how to
translate downstream request into an upstream request.




-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/99799861/attachment.html>

From aph at redhat.com  Sat Sep 24 11:47:04 2016
From: aph at redhat.com (Andrew Haley)
Date: Sat, 24 Sep 2016 16:47:04 +0100
Subject: [concurrency-interest] On the specification of park()
Message-ID: <177d8c85-b4ce-25eb-33cb-a1d9a2b14a2e@redhat.com>

The spec says

> The park method may also return at any other time, for "no reason",

Looking at the OpenJDK implementation, I don't think it will do that.
What is the reason park() is specified this way?

Andrew.



From jason at tedor.me  Sat Sep 24 12:02:24 2016
From: jason at tedor.me (Jason Tedor)
Date: Sat, 24 Sep 2016 16:02:24 +0000
Subject: [concurrency-interest] On the specification of park()
In-Reply-To: <177d8c85-b4ce-25eb-33cb-a1d9a2b14a2e@redhat.com>
References: <177d8c85-b4ce-25eb-33cb-a1d9a2b14a2e@redhat.com>
Message-ID: <CAMYd3R7PsjQuUDnq3vRn5R=+wMw=mG4=8J6SAFHGpUUgrXh+ig@mail.gmail.com>

I think this is due to the notion of spurious wakeups (
https://en.wikipedia.org/wiki/Spurious_wakeup).

On Sat, Sep 24, 2016 at 11:51 AM Andrew Haley <aph at redhat.com> wrote:

> The spec says
>
> > The park method may also return at any other time, for "no reason",
>
> Looking at the OpenJDK implementation, I don't think it will do that.
> What is the reason park() is specified this way?
>
> Andrew.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/456f4617/attachment.html>

From dl at cs.oswego.edu  Sat Sep 24 12:23:08 2016
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 24 Sep 2016 12:23:08 -0400
Subject: [concurrency-interest] JDK 9 Flow javadoc example
 SampleSubscriber requesting
In-Reply-To: <CAAWwtm-apZR22iiBemApHC_oeYOgKprARMa7jbrG4d0GGjhAmQ@mail.gmail.com>
References: <CAAWwtm-apZR22iiBemApHC_oeYOgKprARMa7jbrG4d0GGjhAmQ@mail.gmail.com>
Message-ID: <50ca0591-767a-35ca-2513-7e8bca12e49d@cs.oswego.edu>

On 09/24/2016 10:08 AM, Dávid Karnok wrote:
> Given a synchronous end-consumer Flow.Subscriber of a Flow.Publisher, generally
> there is no reason to request in parts/batches and almost all should be able to
> request Long.MAX_VALUE at the beginning.

The UnboundedSubscriber code example shows this usage, prefaced with

"...when flow control is never needed, a subscriber may initially request an 
effectively unbounded number of items..."

Suggestions for improving that sentence would be welcome. But
I'm not sure we can/should say anything in top-level specs
about the situations in which "flow control is never needed".
Even some synchronous cases might use buffering.

-Doug



From akarnokd at gmail.com  Sat Sep 24 12:48:25 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Sat, 24 Sep 2016 18:48:25 +0200
Subject: [concurrency-interest] JDK 9 Flow javadoc example
 SampleSubscriber requesting
In-Reply-To: <50ca0591-767a-35ca-2513-7e8bca12e49d@cs.oswego.edu>
References: <CAAWwtm-apZR22iiBemApHC_oeYOgKprARMa7jbrG4d0GGjhAmQ@mail.gmail.com>
 <50ca0591-767a-35ca-2513-7e8bca12e49d@cs.oswego.edu>
Message-ID: <CAAWwtm88+oCDnLr1hGznwwcM1M4fF5iWS6La0XL1xZ+a6pj=Og@mail.gmail.com>

Hi Doug and thanks for the reply.

I wish I could offer a better sentence but I'm always struggling with
explaining such things in our javadocs as well. All I can suggest is to
have the unbounded example before the bounded example so people really
needing bounded tips can continue reading while most people can stop at the
unbounded example.

2016-09-24 18:23 GMT+02:00 Doug Lea <dl at cs.oswego.edu>:

> On 09/24/2016 10:08 AM, Dávid Karnok wrote:
>
>> Given a synchronous end-consumer Flow.Subscriber of a Flow.Publisher,
>> generally
>> there is no reason to request in parts/batches and almost all should be
>> able to
>> request Long.MAX_VALUE at the beginning.
>>
>
> The UnboundedSubscriber code example shows this usage, prefaced with
>
> "...when flow control is never needed, a subscriber may initially request
> an effectively unbounded number of items..."
>
> Suggestions for improving that sentence would be welcome. But
> I'm not sure we can/should say anything in top-level specs
> about the situations in which "flow control is never needed".
> Even some synchronous cases might use buffering.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/def05f6f/attachment.html>

From martinrb at google.com  Sat Sep 24 12:51:43 2016
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 24 Sep 2016 09:51:43 -0700
Subject: [concurrency-interest] The very best CAS loop
Message-ID: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>

Discussion on CAS loops got me looking again at our own:

    public final V getAndUpdate(UnaryOperator<V> updateFunction) {
        V prev = get(), next = null;
        for (boolean haveNext = false;;) {
            if (!haveNext)
                next = updateFunction.apply(prev);
            if (weakCompareAndSetVolatile(prev, next))
                return prev;
            haveNext = (prev == (prev = get()));
        }
    }

The haveNext boolean and useless initialization of next bothers me.  We can
do better!

    public final V getAndUpdate(UnaryOperator<V> updateFunction) {
        for (V prev = get();;) {
            V next = updateFunction.apply(prev);
            do {
                if (weakCompareAndSetVolatile(prev, next))
                    return prev;
            } while (prev == (prev = get()));
        }
    }

even though it probably saves more bytecodes than cycles.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/5ca87df9/attachment.html>

From dl at cs.oswego.edu  Sat Sep 24 12:51:53 2016
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 24 Sep 2016 12:51:53 -0400
Subject: [concurrency-interest] On the specification of park()
In-Reply-To: <177d8c85-b4ce-25eb-33cb-a1d9a2b14a2e@redhat.com>
References: <177d8c85-b4ce-25eb-33cb-a1d9a2b14a2e@redhat.com>
Message-ID: <2bd1f120-1c32-bd24-d24a-b9c305733e3e@cs.oswego.edu>

On 09/24/2016 11:47 AM, Andrew Haley wrote:
> The spec says
>
>> The park method may also return at any other time, for "no reason",
>
> Looking at the OpenJDK implementation, I don't think it will do that.
> What is the reason park() is specified this way?
>

There's a lot of mythology and controversy about this
(as the first bunch of google hits will point you to), but the
main reason is that when finite resources (hardware or memory) are
used to implement blocking for a series of conditions, there can
be races in which a signal for the previous usage cannot be
distinguished from a signal to the new usage, so a wakeup occurs
to be safe. This is usually the case for example when using
immortal typed memory pools, which is (or at least was) the case
for hotspot's thread.parker field.

-Doug


From martinrb at google.com  Sat Sep 24 13:10:21 2016
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 24 Sep 2016 10:10:21 -0700
Subject: [concurrency-interest] AtomicReference get vs. getAcquire;
 get/set Opaque
In-Reply-To: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>
References: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>
Message-ID: <CA+kOe09rEfE3eJr2NfQARMbFDHnzT_hxbYCjcoHtnKVuZMmt=A@mail.gmail.com>

On Sat, Sep 24, 2016 at 6:19 AM, Dávid Karnok <akarnokd at gmail.com> wrote:

> I have a one element single-producer single-consumer "queue" implemented
> as this:
>
> boolean offer(AtomicReference<T> ref, T value) {
>     Objects.requireNonNull(value);
>     if (ref.get() == null) {
>         ref.lazySet(value);
>         return true;
>     }
>     return false;
> }
>
> T poll(AtomicReference<T> ref) {
>     T v = ref.get();
>     if (v != null) {
>        ref.lazySet(null);
>     }
>     return v;
> }
>
> Is it okay to turn get() into getAcquire() and lazySet into setRelease()
> (I can see lazySet delegates to setRelease)?
>

Yes, but ... the poll and offer operations will no longer be part of the
global sequentially consistent order of synchronization actions.

More generally, are there consequences turning CAS loops of
> get()+compareAndSet into getAcquire() + weakCompareAndSetRelease?
>



> In addition, are there examples (or explanations) of when it is okay to
> use getOpaque/setOpaque  and would they work with the concurrent "queue"
> above at all?
>
> (I guess there is not much difference for x86 but my code may end up on
> weaker platforms eventually.)
>

I think there is a difference on x86.  Conceptually, set/volatile write
"drains the write buffer", while setRelease does not.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/84c624b0/attachment-0001.html>

From akarnokd at gmail.com  Sat Sep 24 16:34:13 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Sat, 24 Sep 2016 22:34:13 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
Message-ID: <CAAWwtm_py+gRYkhCd8g5CHfiOpY5U_Y+4WZQXtCVF7XFJs5c=w@mail.gmail.com>

Isn't the second one getting 2 safepoint polls when the weak CAS failed due
to different actually changed value?

2016-09-24 18:51 GMT+02:00 Martin Buchholz <martinrb at google.com>:

> Discussion on CAS loops got me looking again at our own:
>
>     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>         V prev = get(), next = null;
>         for (boolean haveNext = false;;) {
>             if (!haveNext)
>                 next = updateFunction.apply(prev);
>             if (weakCompareAndSetVolatile(prev, next))
>                 return prev;
>             haveNext = (prev == (prev = get()));
>         }
>     }
>
> The haveNext boolean and useless initialization of next bothers me.  We
> can do better!
>
>     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>         for (V prev = get();;) {
>             V next = updateFunction.apply(prev);
>             do {
>                 if (weakCompareAndSetVolatile(prev, next))
>                     return prev;
>             } while (prev == (prev = get()));
>         }
>     }
>
> even though it probably saves more bytecodes than cycles.
>



-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/b5657f4f/attachment.html>

From martinrb at google.com  Sat Sep 24 16:41:46 2016
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 24 Sep 2016 13:41:46 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
Message-ID: <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>

No one is suggesting we add cancel to CompletionStage - I agree that would
break users, by making an immutable interface mutable.  This also means
that CompletionStage cannot extend Future. I also would not want to have a
toFuture method that would return a j.u.c.Future, because of misfit
Future.cancel.  If we are adding cancel, then it will be to make a new
interface, such as the suggested CancellableCompletionStage.

I also agree that CompletionStage does *not* represent "a running task of
sorts", j.u.c. Future specs are still confusing in that way due to
FutureTask heritage.

On Thu, Sep 22, 2016 at 7:51 PM, James Roper <james at lightbend.com> wrote:

> For example, we often cache futures and return them from a libraries API,
> if a client could cancel a future, that would break everything else that
> received that future.


On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com>
wrote:

> PPS: A misunderstanding is that CompletionStage represents a running task
> of sorts, one that can be cancelled etc. This is not the case.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/0c56005c/attachment.html>

From vitalyd at gmail.com  Sat Sep 24 16:50:52 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 24 Sep 2016 16:50:52 -0400
Subject: [concurrency-interest] AtomicReference get vs. getAcquire;
 get/set Opaque
In-Reply-To: <CA+kOe09rEfE3eJr2NfQARMbFDHnzT_hxbYCjcoHtnKVuZMmt=A@mail.gmail.com>
References: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>
 <CA+kOe09rEfE3eJr2NfQARMbFDHnzT_hxbYCjcoHtnKVuZMmt=A@mail.gmail.com>
Message-ID: <CAHjP37H7K7=C=D_AF=GQWNubPTZ0UohMSX-GeqdUyXdt5CGocQ@mail.gmail.com>

On Saturday, September 24, 2016, Martin Buchholz <martinrb at google.com>
wrote:

>
>
> On Sat, Sep 24, 2016 at 6:19 AM, Dávid Karnok <akarnokd at gmail.com
> <javascript:_e(%7B%7D,'cvml','akarnokd at gmail.com');>> wrote:
>
>> I have a one element single-producer single-consumer "queue" implemented
>> as this:
>>
>> boolean offer(AtomicReference<T> ref, T value) {
>>     Objects.requireNonNull(value);
>>     if (ref.get() == null) {
>>         ref.lazySet(value);
>>         return true;
>>     }
>>     return false;
>> }
>>
>> T poll(AtomicReference<T> ref) {
>>     T v = ref.get();
>>     if (v != null) {
>>        ref.lazySet(null);
>>     }
>>     return v;
>> }
>>
>> Is it okay to turn get() into getAcquire() and lazySet into setRelease()
>> (I can see lazySet delegates to setRelease)?
>>
>
> Yes, but ... the poll and offer operations will no longer be part of the
> global sequentially consistent order of synchronization actions.
>
How so?

Volatile load (get()) prevents subsequent stores and loads from moving
above it - so does getAcquire.

lazySet never really had a formal definition, certainly wasn't part of
JMM.  Most people assumed it ensured prior stores didn't move past the
lazySet - nothing more.  setRelease appears to prevent prior loads and
stores from moving past it.  Given lazySet delegates to setRelease, it
would imply that lazySet also didn't allow prior loads to move past it, not
just stores.

So talking about the global synchronization order when there was already
lazySet, rather than volatile store, seems iffy.

>
> More generally, are there consequences turning CAS loops of
>> get()+compareAndSet into getAcquire() + weakCompareAndSetRelease?
>>
>
>
>
>> In addition, are there examples (or explanations) of when it is okay to
>> use getOpaque/setOpaque  and would they work with the concurrent "queue"
>> above at all?
>>
> I've asked for more explanation of opaque before, not sure that's
happened.  In particular, it's unclear if atomicity is guaranteed (e.g.
load/store a long on 32bit archs).  It would seem it's mirroring
memory_order_relaxed in C++ otherwise.

For the single queue in this thread, it's unclear if it would work.  An
object published via setOpaque may have its stores happen after since no
ordering, other than program, is guaranteed - CPU can reorder internally
though.

One way to think about the opaque operations is to pretend they happen
inside a method the JIT didn't inline.  It therefore cannot
optimize across it because it doesn't know if the method would invalidate
its operations.  So it's kind of a full compiler fence in that sense.

My personal impression is that opaque is useful for ensuring atomicity of a
read/store only, like the memory_order_relaxed in C++, although the
atomicity hasn't been clarified, AFAIK.

>
>> (I guess there is not much difference for x86 but my code may end up on
>> weaker platforms eventually.)
>>
>
> I think there is a difference on x86.  Conceptually, set/volatile write
> "drains the write buffer", while setRelease does not.
>
Minor nit: nothing "drains the write buffer" - normal CPU machinery drains
it on its own.  What the volatile set would end up doing is preventing
subsequent instructions from executing until the store buffer is drained.
So it's a pipeline bubble/hazard.




-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/d6601ad6/attachment.html>

From martinrb at google.com  Sat Sep 24 17:20:15 2016
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 24 Sep 2016 14:20:15 -0700
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CAAWwtm_py+gRYkhCd8g5CHfiOpY5U_Y+4WZQXtCVF7XFJs5c=w@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <CAAWwtm_py+gRYkhCd8g5CHfiOpY5U_Y+4WZQXtCVF7XFJs5c=w@mail.gmail.com>
Message-ID: <CA+kOe095OS1bPy3e7LNRjTM2qQsqOuj8rL44b4DZVXEWFN9N8A@mail.gmail.com>

On Sat, Sep 24, 2016 at 1:34 PM, Dávid Karnok <akarnokd at gmail.com> wrote:

> Isn't the second one getting 2 safepoint polls when the weak CAS failed
> due to different actually changed value?
>

I hope not; the tailing bytecodes are

      33: lcmp
      34: ifeq          14
      37: goto          5

 I expect a safepoint on a backward jump, and only one is taken, so one
safepoint per failed CAS?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/ae1f9cdc/attachment-0001.html>

From oleksandr.otenko at gmail.com  Sat Sep 24 17:34:27 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Sat, 24 Sep 2016 22:34:27 +0100
Subject: [concurrency-interest] AtomicReference get vs. getAcquire;
	get/set Opaque
In-Reply-To: <CA+kOe09rEfE3eJr2NfQARMbFDHnzT_hxbYCjcoHtnKVuZMmt=A@mail.gmail.com>
References: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>
 <CA+kOe09rEfE3eJr2NfQARMbFDHnzT_hxbYCjcoHtnKVuZMmt=A@mail.gmail.com>
Message-ID: <9F96469F-FB12-4C38-AFDA-7C06A9D3CF7E@gmail.com>

In the documentation for getAcquire / setRelease there is not a word about ordering of getAcquire and setRelease between themselves.

So it’d be important to know if a subsequent getAcquire can be reordered with a preceding setRelease - eg will polling this queue twice in a loop return the same v.

Alex

> On 24 Sep 2016, at 18:10, Martin Buchholz <martinrb at google.com> wrote:
> 
> 
> 
> On Sat, Sep 24, 2016 at 6:19 AM, Dávid Karnok <akarnokd at gmail.com <mailto:akarnokd at gmail.com>> wrote:
> I have a one element single-producer single-consumer "queue" implemented as this:
> 
> boolean offer(AtomicReference<T> ref, T value) {
>     Objects.requireNonNull(value);
>     if (ref.get() == null) {
>         ref.lazySet(value);
>         return true;
>     }
>     return false;
> }
> 
> T poll(AtomicReference<T> ref) {
>     T v = ref.get();
>     if (v != null) {
>        ref.lazySet(null);
>     }
>     return v;
> }
> 
> Is it okay to turn get() into getAcquire() and lazySet into setRelease() (I can see lazySet delegates to setRelease)?
> 
> Yes, but ... the poll and offer operations will no longer be part of the global sequentially consistent order of synchronization actions.
> 
> More generally, are there consequences turning CAS loops of get()+compareAndSet into getAcquire() + weakCompareAndSetRelease?
> 
>  
> In addition, are there examples (or explanations) of when it is okay to use getOpaque/setOpaque  and would they work with the concurrent "queue" above at all?
> 
> (I guess there is not much difference for x86 but my code may end up on weaker platforms eventually.)
> 
> I think there is a difference on x86.  Conceptually, set/volatile write "drains the write buffer", while setRelease does not.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/50cbc20d/attachment.html>

From martinrb at google.com  Sat Sep 24 19:04:19 2016
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 24 Sep 2016 16:04:19 -0700
Subject: [concurrency-interest] AtomicReference get vs. getAcquire;
 get/set Opaque
In-Reply-To: <9F96469F-FB12-4C38-AFDA-7C06A9D3CF7E@gmail.com>
References: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>
 <CA+kOe09rEfE3eJr2NfQARMbFDHnzT_hxbYCjcoHtnKVuZMmt=A@mail.gmail.com>
 <9F96469F-FB12-4C38-AFDA-7C06A9D3CF7E@gmail.com>
Message-ID: <CA+kOe09Zf1XjzrbsZaq9PXDSGSkUyc4cbQGG7wH-Gqc1fJYEdA@mail.gmail.com>

We know the docs could be better, but we do have
"""Ignoring the many semantic differences from C and C++, this method has
memory ordering effects compatible with memory_order_acquire ordering."""
http://download.java.net/java/jdk9/docs/api/java/lang/invoke/VarHandle.html#getAcquire-java.lang.Object...-

On Sat, Sep 24, 2016 at 2:34 PM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> In the documentation for getAcquire / setRelease there is not a word about
> ordering of getAcquire and setRelease between themselves.
>
> So it’d be important to know if a subsequent getAcquire can be reordered
> with a preceding setRelease - eg will polling this queue twice in a loop
> return the same v.
>
> Alex
>
> On 24 Sep 2016, at 18:10, Martin Buchholz <martinrb at google.com> wrote:
>
>
>
> On Sat, Sep 24, 2016 at 6:19 AM, Dávid Karnok <akarnokd at gmail.com> wrote:
>
>> I have a one element single-producer single-consumer "queue" implemented
>> as this:
>>
>> boolean offer(AtomicReference<T> ref, T value) {
>>     Objects.requireNonNull(value);
>>     if (ref.get() == null) {
>>         ref.lazySet(value);
>>         return true;
>>     }
>>     return false;
>> }
>>
>> T poll(AtomicReference<T> ref) {
>>     T v = ref.get();
>>     if (v != null) {
>>        ref.lazySet(null);
>>     }
>>     return v;
>> }
>>
>> Is it okay to turn get() into getAcquire() and lazySet into setRelease()
>> (I can see lazySet delegates to setRelease)?
>>
>
> Yes, but ... the poll and offer operations will no longer be part of the
> global sequentially consistent order of synchronization actions.
>
> More generally, are there consequences turning CAS loops of
>> get()+compareAndSet into getAcquire() + weakCompareAndSetRelease?
>>
>
>
>
>> In addition, are there examples (or explanations) of when it is okay to
>> use getOpaque/setOpaque  and would they work with the concurrent "queue"
>> above at all?
>>
>> (I guess there is not much difference for x86 but my code may end up on
>> weaker platforms eventually.)
>>
>
> I think there is a difference on x86.  Conceptually, set/volatile write
> "drains the write buffer", while setRelease does not.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/7ec088df/attachment.html>

From vitalyd at gmail.com  Sat Sep 24 19:24:30 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 24 Sep 2016 19:24:30 -0400
Subject: [concurrency-interest] AtomicReference get vs. getAcquire;
 get/set Opaque
In-Reply-To: <9F96469F-FB12-4C38-AFDA-7C06A9D3CF7E@gmail.com>
References: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>
 <CA+kOe09rEfE3eJr2NfQARMbFDHnzT_hxbYCjcoHtnKVuZMmt=A@mail.gmail.com>
 <9F96469F-FB12-4C38-AFDA-7C06A9D3CF7E@gmail.com>
Message-ID: <CAHjP37FDs0eY4BBfFZCg8QDKKmKH9Dy2U3VkvWusYKpABHmGLw@mail.gmail.com>

On Saturday, September 24, 2016, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> In the documentation for getAcquire / setRelease there is not a word about
> ordering of getAcquire and setRelease between themselves.
>
> So it’d be important to know if a subsequent getAcquire can be reordered
> with a preceding setRelease - eg will polling this queue twice in a loop
> return the same v.
>
That would violate normal data dependence, wouldn't it? However, a good
question is can compiler perform store-load forwarding, yielding null on
the second read.  I suspect no because I'd expect acquire/release to
encompass opaque as well, and so compiler must perform the read on 2nd
iteration.

>
> Alex
>
> On 24 Sep 2016, at 18:10, Martin Buchholz <martinrb at google.com
> <javascript:_e(%7B%7D,'cvml','martinrb at google.com');>> wrote:
>
>
>
> On Sat, Sep 24, 2016 at 6:19 AM, Dávid Karnok <akarnokd at gmail.com
> <javascript:_e(%7B%7D,'cvml','akarnokd at gmail.com');>> wrote:
>
>> I have a one element single-producer single-consumer "queue" implemented
>> as this:
>>
>> boolean offer(AtomicReference<T> ref, T value) {
>>     Objects.requireNonNull(value);
>>     if (ref.get() == null) {
>>         ref.lazySet(value);
>>         return true;
>>     }
>>     return false;
>> }
>>
>> T poll(AtomicReference<T> ref) {
>>     T v = ref.get();
>>     if (v != null) {
>>        ref.lazySet(null);
>>     }
>>     return v;
>> }
>>
>> Is it okay to turn get() into getAcquire() and lazySet into setRelease()
>> (I can see lazySet delegates to setRelease)?
>>
>
> Yes, but ... the poll and offer operations will no longer be part of the
> global sequentially consistent order of synchronization actions.
>
> More generally, are there consequences turning CAS loops of
>> get()+compareAndSet into getAcquire() + weakCompareAndSetRelease?
>>
>
>
>
>> In addition, are there examples (or explanations) of when it is okay to
>> use getOpaque/setOpaque  and would they work with the concurrent "queue"
>> above at all?
>>
>> (I guess there is not much difference for x86 but my code may end up on
>> weaker platforms eventually.)
>>
>
> I think there is a difference on x86.  Conceptually, set/volatile write
> "drains the write buffer", while setRelease does not.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> <javascript:_e(%7B%7D,'cvml','Concurrency-interest at cs.oswego.edu');>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160924/1df59e9f/attachment-0001.html>

From rco at quartetfs.com  Sun Sep 25 02:45:25 2016
From: rco at quartetfs.com (Romain Colle)
Date: Sun, 25 Sep 2016 08:45:25 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe095OS1bPy3e7LNRjTM2qQsqOuj8rL44b4DZVXEWFN9N8A@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <CAAWwtm_py+gRYkhCd8g5CHfiOpY5U_Y+4WZQXtCVF7XFJs5c=w@mail.gmail.com>
 <CA+kOe095OS1bPy3e7LNRjTM2qQsqOuj8rL44b4DZVXEWFN9N8A@mail.gmail.com>
Message-ID: <0ba1ca6b-b5cd-460e-afae-bb7751950370@quartetfs.com>

Out of curiosity, why are we using a weak CAS instead of a regular one in this method?

Thanks,
Romain

On Sat, Sep 24, 2016 at 11:24pm, Martin Buchholz < martinrb at google.com [martinrb at google.com] > wrote:

On Sat, Sep 24, 2016 at 1:34 PM, Dávid Karnok < akarnokd at gmail.com [akarnokd at gmail.com] > wrote:
Isn't the second one getting 2 safepoint polls when the weak CAS failed due to different actually changed value?
I hope not; the tailing bytecodes are
33: lcmp 34: ifeq 14 37: goto 5
I expect a safepoint on a backward jump, and only one is taken, so one safepoint per failed CAS?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/646fd63f/attachment.html>

From aph at redhat.com  Sun Sep 25 03:02:55 2016
From: aph at redhat.com (Andrew Haley)
Date: Sun, 25 Sep 2016 08:02:55 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <0ba1ca6b-b5cd-460e-afae-bb7751950370@quartetfs.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <CAAWwtm_py+gRYkhCd8g5CHfiOpY5U_Y+4WZQXtCVF7XFJs5c=w@mail.gmail.com>
 <CA+kOe095OS1bPy3e7LNRjTM2qQsqOuj8rL44b4DZVXEWFN9N8A@mail.gmail.com>
 <0ba1ca6b-b5cd-460e-afae-bb7751950370@quartetfs.com>
Message-ID: <6276483d-6980-ea6b-ee2f-0acffee5eb44@redhat.com>

On 25/09/16 07:45, Romain Colle wrote:
> Out of curiosity, why are we using a weak CAS instead of a regular one in this method?

It's an efficiency concern.  On machines which support weak CAS, a CAS
can fail because of contention (or a spurious wakeup; these can happen
for any reason) or because the variable actually changed.  On such
machines, a strong CAS is usually implemented as a loop which spins.
In that case it makes more sense to fetch the variable again and retry
the CAS.

All of this is perhaps easier to see with an assembly-language
example, which I can provide on request.  :-)

Andrew.



From viktor.klang at gmail.com  Sun Sep 25 06:34:58 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 25 Sep 2016 12:34:58 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
Message-ID: <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>

On Sat, Sep 24, 2016 at 10:41 PM, Martin Buchholz <martinrb at google.com>
wrote:

> No one is suggesting we add cancel to CompletionStage - I agree that would
> break users, by making an immutable interface mutable.
>

+10000


> This also means that CompletionStage cannot extend Future.
>

+10000


> I also would not want to have a toFuture method that would return a
> j.u.c.Future, because of misfit Future.cancel.
>

Would you mind elaborating here? According to the cancel method spec on
Future it is completely fine for it to be a no-op which always returns
false:

"This attempt will fail if the task has already completed, has already been
cancelled, *or could not be cancelled for some other reason.*"

Source:
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Future.html



>   If we are adding cancel, then it will be to make a new interface, such
> as the suggested CancellableCompletionStage.
>
> I also agree that CompletionStage does *not* represent "a running task of
> sorts", j.u.c. Future specs are still confusing in that way due to
> FutureTask heritage.
>

+10000


>
> On Thu, Sep 22, 2016 at 7:51 PM, James Roper <james at lightbend.com> wrote:
>
>> For example, we often cache futures and return them from a libraries API,
>> if a client could cancel a future, that would break everything else that
>> received that future.
>
>
> On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>> PPS: A misunderstanding is that CompletionStage represents a running task
>> of sorts, one that can be cancelled etc. This is not the case.
>>
>
>
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/8edd1baf/attachment.html>

From viktor.klang at gmail.com  Sun Sep 25 07:03:21 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 25 Sep 2016 13:03:21 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
Message-ID: <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>

On Sun, Sep 25, 2016 at 12:34 PM, Viktor Klang <viktor.klang at gmail.com>
wrote:

>
>
> On Sat, Sep 24, 2016 at 10:41 PM, Martin Buchholz <martinrb at google.com>
> wrote:
>
>> No one is suggesting we add cancel to CompletionStage - I agree that
>> would break users, by making an immutable interface mutable.
>>
>
> +10000
>
>
>> This also means that CompletionStage cannot extend Future.
>>
>
> +10000
>
>
>> I also would not want to have a toFuture method that would return a
>> j.u.c.Future, because of misfit Future.cancel.
>>
>
> Would you mind elaborating here? According to the cancel method spec on
> Future it is completely fine for it to be a no-op which always returns
> false:
>
> "This attempt will fail if the task has already completed, has already
> been cancelled, *or could not be cancelled for some other reason.*"
>
> Source: https://docs.oracle.com/javase/8/docs/api/java/util/
> concurrent/Future.html
>

There's an interesting (read: weird) spec clause in cancel:

"*After this method returns, subsequent calls to isDone() will always
return true*. Subsequent calls to isCancelled() will always return true if
this method returned true."

That clause is in contradiction with the previously quoted line.


>
>
>
>>   If we are adding cancel, then it will be to make a new interface, such
>> as the suggested CancellableCompletionStage.
>>
>> I also agree that CompletionStage does *not* represent "a running task of
>> sorts", j.u.c. Future specs are still confusing in that way due to
>> FutureTask heritage.
>>
>
> +10000
>
>
>>
>> On Thu, Sep 22, 2016 at 7:51 PM, James Roper <james at lightbend.com> wrote:
>>
>>> For example, we often cache futures and return them from a libraries
>>> API, if a client could cancel a future, that would break everything else
>>> that received that future.
>>
>>
>> On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com>
>> wrote:
>>
>>> PPS: A misunderstanding is that CompletionStage represents a running
>>> task of sorts, one that can be cancelled etc. This is not the case.
>>>
>>
>>
>>
>
>
>
> --
> Cheers,
> √
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/1ec2bb32/attachment-0001.html>

From davidcholmes at aapt.net.au  Sun Sep 25 07:45:44 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 25 Sep 2016 21:45:44 +1000
Subject: [concurrency-interest] We need to add blocking methods
	to	CompletionStage!
In-Reply-To: <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
Message-ID: <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>

I think that was meant to read “After this method returns _true_, subsequent calls …”

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Viktor Klang
Sent: Sunday, September 25, 2016 9:03 PM
To: Martin Buchholz <martinrb at google.com>
Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>; core-libs-dev <core-libs-dev at openjdk.java.net>
Subject: Re: [concurrency-interest] We need to add blocking methods to CompletionStage!

 

 

 

On Sun, Sep 25, 2016 at 12:34 PM, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com> > wrote:

 

 

On Sat, Sep 24, 2016 at 10:41 PM, Martin Buchholz <martinrb at google.com <mailto:martinrb at google.com> > wrote:

No one is suggesting we add cancel to CompletionStage - I agree that would break users, by making an immutable interface mutable. 

 

+10000

 

This also means that CompletionStage cannot extend Future.

 

+10000

 

I also would not want to have a toFuture method that would return a j.u.c.Future, because of misfit Future.cancel.

 

Would you mind elaborating here? According to the cancel method spec on Future it is completely fine for it to be a no-op which always returns false:

 

"This attempt will fail if the task has already completed, has already been cancelled, or could not be cancelled for some other reason."

 

Source: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Future.html

 

There's an interesting (read: weird) spec clause in cancel:

 

"After this method returns, subsequent calls to isDone() will always return true. Subsequent calls to isCancelled() will always return true if this method returned true."

 

That clause is in contradiction with the previously quoted line.

 

 

 

 

  If we are adding cancel, then it will be to make a new interface, such as the suggested CancellableCompletionStage.

 

I also agree that CompletionStage does *not* represent "a running task of sorts", j.u.c. Future specs are still confusing in that way due to FutureTask heritage.

 

+10000

 

 

On Thu, Sep 22, 2016 at 7:51 PM, James Roper <james at lightbend.com <mailto:james at lightbend.com> > wrote:

For example, we often cache futures and return them from a libraries API, if a client could cancel a future, that would break everything else that received that future.

 

On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com> > wrote:

PPS: A misunderstanding is that CompletionStage represents a running task of sorts, one that can be cancelled etc. This is not the case.

 

 





 

-- 

Cheers,

√





 

-- 

Cheers,

√

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/9425ec8f/attachment.html>

From dl at cs.oswego.edu  Sun Sep 25 08:03:40 2016
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 25 Sep 2016 08:03:40 -0400
Subject: [concurrency-interest] We need to add blocking methods to
 CompletionStage!
In-Reply-To: <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
Message-ID: <3407be66-781a-643e-c1f6-f5a6913839e8@cs.oswego.edu>


Thanks to Martin for trying to clarify disagreements about and
possible improvements to CompletableFuture and CompletionStage APIs.
A few notes:

The main issue continues to be: what would you like clients to be able
to do with a CompletableFuture returned to them.  This is reminiscent
of Collections APIs: is it OK for clients to add or remove items in a
collection passed or returned to them? In some cases, the consequences
of nonsensical modifications are no different in scope than other
usage errors, so the issue amounts to API usage guidance.  But
sometimes you need to prevent modifications. In which case, similar
answers apply for collections and completions (although it is not a
perfect analogy because you cannot sidestep issues using only pure
ImmutableCompletions -- someone needs to complete them).

1.  Producers can let clients do anything if the clients only modify a
copy. Unlike some Collections, "defensive copying" CompletableFutures
is fairly cheap, so is usually a good solution. Similarly, clients
should be able to easily make a copy themselves if they are in doubt
about for example upstream effects of calling cancel(). We now know
that copy() should have been in jdk8 (as it now is in jdk9) rather
than forcing people to do this via thenApply(x->x), which made the
defensive copying approach underused.

2. Alternatively, producers can return a type that does not allow some
operations.  Method minimalCompletionStage is analogous to Collections
unmodifiableCollection, but to some people, too minimal. Even joining
and status checks can lead to deadlocks and livelocks that some
producers want to make impossible to encounter without explicit client
action. Introducing only CompletionStage/minimalCompletionStage might
not have been the best approach, because further extending this
strategy leads to further controversy choosing among the many
plausible intermediary points of sets of methods to expose between
CompletionStage and CompletableFuture. But if we were to pick one, we
might consider something along the lines suggested by Kasper and
Viktor of
   interface FutureCompletion extends CompletionStage, Future {}
This allows joins and queries, but also raises the policy question of
whether cancel() should only cancel a copy or the source.


Separately, we also now know that it was uselessly mean-spirited to
allow CompletionStage.toCompletableFuture to throw UOE, since any
client can awkwardly emulate this using whenComplete.  I'm not sure
whether was can compatibly revise to disallow UOE and also
default-implement it.

-Doug


From joe.bowbeer at gmail.com  Sun Sep 25 10:20:04 2016
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 25 Sep 2016 07:20:04 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
Message-ID: <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>

This statement regarding what happens after cancel is called is correct:

"*After this method returns, subsequent calls to **isDone**() will always
return true*. Subsequent calls to isCancelled() will always return true if
this method returned true."

After cancel returns, the future is completed, hence isDone. If cancel
returns true, i.e. it was cancelled, then  isCancelled returns true. But,
for example if the future is already completed when cancel is called, then
cancel will return false and isCancelled will return false.

On Sep 25, 2016 6:49 AM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

> I think that was meant to read “After this method returns _*true*_,
> subsequent calls …”
>
>
>
> David
>
>
>
> *From:* Concurrency-interest [mailto:concurrency-interest-
> bounces at cs.oswego.edu] *On Behalf Of *Viktor Klang
> *Sent:* Sunday, September 25, 2016 9:03 PM
> *To:* Martin Buchholz <martinrb at google.com>
> *Cc:* concurrency-interest <concurrency-interest at cs.oswego.edu>;
> core-libs-dev <core-libs-dev at openjdk.java.net>
> *Subject:* Re: [concurrency-interest] We need to add blocking methods to
> CompletionStage!
>
>
>
>
>
>
>
> On Sun, Sep 25, 2016 at 12:34 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>
>
>
>
> On Sat, Sep 24, 2016 at 10:41 PM, Martin Buchholz <martinrb at google.com>
> wrote:
>
> No one is suggesting we add cancel to CompletionStage - I agree that would
> break users, by making an immutable interface mutable.
>
>
>
> +10000
>
>
>
> This also means that CompletionStage cannot extend Future.
>
>
>
> +10000
>
>
>
> I also would not want to have a toFuture method that would return a
> j.u.c.Future, because of misfit Future.cancel.
>
>
>
> Would you mind elaborating here? According to the cancel method spec on
> Future it is completely fine for it to be a no-op which always returns
> false:
>
>
>
> "This attempt will fail if the task has already completed, has already
> been cancelled, *or could not be cancelled for some other reason.*"
>
>
>
> Source: https://docs.oracle.com/javase/8/docs/api/java/util/
> concurrent/Future.html
>
>
>
> There's an interesting (read: weird) spec clause in cancel:
>
>
>
> "*After this method returns, subsequent calls to isDone() will always
> return true*. Subsequent calls to isCancelled() will always return true
> if this method returned true."
>
>
>
> That clause is in contradiction with the previously quoted line.
>
>
>
>
>
>
>
>
>
>   If we are adding cancel, then it will be to make a new interface, such
> as the suggested CancellableCompletionStage.
>
>
>
> I also agree that CompletionStage does *not* represent "a running task of
> sorts", j.u.c. Future specs are still confusing in that way due to
> FutureTask heritage.
>
>
>
> +10000
>
>
>
>
>
> On Thu, Sep 22, 2016 at 7:51 PM, James Roper <james at lightbend.com> wrote:
>
> For example, we often cache futures and return them from a libraries API,
> if a client could cancel a future, that would break everything else that
> received that future.
>
>
>
> On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
> PPS: A misunderstanding is that CompletionStage represents a running task
> of sorts, one that can be cancelled etc. This is not the case.
>
>
>
>
>
>
>
>
>
> --
>
> Cheers,
>
> √
>
>
>
>
>
> --
>
> Cheers,
>
> √
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/edd86d5f/attachment-0001.html>

From viktor.klang at gmail.com  Sun Sep 25 10:34:12 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 25 Sep 2016 16:34:12 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
Message-ID: <CANPzfU_FApFOp-J5DVRsp+N7DM13aJv-s2sRSo29wx_Vb0PJxw@mail.gmail.com>

If that truely is the case then the only way of implementing a readonly
Future is by throwing an exception from cancel...

-- 
Cheers,
√

On Sep 25, 2016 4:20 PM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:

> This statement regarding what happens after cancel is called is correct:
>
> "*After this method returns, subsequent calls to **isDone**() will always
> return true*. Subsequent calls to isCancelled() will always return true
> if this method returned true."
>
> After cancel returns, the future is completed, hence isDone. If cancel
> returns true, i.e. it was cancelled, then  isCancelled returns true. But,
> for example if the future is already completed when cancel is called, then
> cancel will return false and isCancelled will return false.
>
> On Sep 25, 2016 6:49 AM, "David Holmes" <davidcholmes at aapt.net.au> wrote:
>
>> I think that was meant to read “After this method returns _*true*_,
>> subsequent calls …”
>>
>>
>>
>> David
>>
>>
>>
>> *From:* Concurrency-interest [mailto:concurrency-interest-b
>> ounces at cs.oswego.edu] *On Behalf Of *Viktor Klang
>> *Sent:* Sunday, September 25, 2016 9:03 PM
>> *To:* Martin Buchholz <martinrb at google.com>
>> *Cc:* concurrency-interest <concurrency-interest at cs.oswego.edu>;
>> core-libs-dev <core-libs-dev at openjdk.java.net>
>> *Subject:* Re: [concurrency-interest] We need to add blocking methods to
>> CompletionStage!
>>
>>
>>
>>
>>
>>
>>
>> On Sun, Sep 25, 2016 at 12:34 PM, Viktor Klang <viktor.klang at gmail.com>
>> wrote:
>>
>>
>>
>>
>>
>> On Sat, Sep 24, 2016 at 10:41 PM, Martin Buchholz <martinrb at google.com>
>> wrote:
>>
>> No one is suggesting we add cancel to CompletionStage - I agree that
>> would break users, by making an immutable interface mutable.
>>
>>
>>
>> +10000
>>
>>
>>
>> This also means that CompletionStage cannot extend Future.
>>
>>
>>
>> +10000
>>
>>
>>
>> I also would not want to have a toFuture method that would return a
>> j.u.c.Future, because of misfit Future.cancel.
>>
>>
>>
>> Would you mind elaborating here? According to the cancel method spec on
>> Future it is completely fine for it to be a no-op which always returns
>> false:
>>
>>
>>
>> "This attempt will fail if the task has already completed, has already
>> been cancelled, *or could not be cancelled for some other reason.*"
>>
>>
>>
>> Source: https://docs.oracle.com/javase/8/docs/api/java/util/concurre
>> nt/Future.html
>>
>>
>>
>> There's an interesting (read: weird) spec clause in cancel:
>>
>>
>>
>> "*After this method returns, subsequent calls to isDone() will always
>> return true*. Subsequent calls to isCancelled() will always return true
>> if this method returned true."
>>
>>
>>
>> That clause is in contradiction with the previously quoted line.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>   If we are adding cancel, then it will be to make a new interface, such
>> as the suggested CancellableCompletionStage.
>>
>>
>>
>> I also agree that CompletionStage does *not* represent "a running task of
>> sorts", j.u.c. Future specs are still confusing in that way due to
>> FutureTask heritage.
>>
>>
>>
>> +10000
>>
>>
>>
>>
>>
>> On Thu, Sep 22, 2016 at 7:51 PM, James Roper <james at lightbend.com> wrote:
>>
>> For example, we often cache futures and return them from a libraries API,
>> if a client could cancel a future, that would break everything else that
>> received that future.
>>
>>
>>
>> On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com>
>> wrote:
>>
>> PPS: A misunderstanding is that CompletionStage represents a running task
>> of sorts, one that can be cancelled etc. This is not the case.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> --
>>
>> Cheers,
>>
>> √
>>
>>
>>
>>
>>
>> --
>>
>> Cheers,
>>
>> √
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/632df514/attachment.html>

From oleksandr.otenko at gmail.com  Sun Sep 25 11:07:56 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Sun, 25 Sep 2016 16:07:56 +0100
Subject: [concurrency-interest] AtomicReference get vs. getAcquire;
	get/set Opaque
In-Reply-To: <CAHjP37FDs0eY4BBfFZCg8QDKKmKH9Dy2U3VkvWusYKpABHmGLw@mail.gmail.com>
References: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>
 <CA+kOe09rEfE3eJr2NfQARMbFDHnzT_hxbYCjcoHtnKVuZMmt=A@mail.gmail.com>
 <9F96469F-FB12-4C38-AFDA-7C06A9D3CF7E@gmail.com>
 <CAHjP37FDs0eY4BBfFZCg8QDKKmKH9Dy2U3VkvWusYKpABHmGLw@mail.gmail.com>
Message-ID: <AB7D9788-5DA3-45FF-B21E-47B22B3F6075@gmail.com>

Well, knowing there are architectures where data dependencies have to be enforced, it’s hard to tell how far we relax the ordering. There is a reference to C++, and that does clarify what ordering is meant.


Alex


> On 25 Sep 2016, at 00:24, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> 
> 
> 
> On Saturday, September 24, 2016, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
> In the documentation for getAcquire / setRelease there is not a word about ordering of getAcquire and setRelease between themselves.
> 
> So it’d be important to know if a subsequent getAcquire can be reordered with a preceding setRelease - eg will polling this queue twice in a loop return the same v.
> That would violate normal data dependence, wouldn't it? However, a good question is can compiler perform store-load forwarding, yielding null on the second read.  I suspect no because I'd expect acquire/release to encompass opaque as well, and so compiler must perform the read on 2nd iteration. 
> 
> Alex
> 
>> On 24 Sep 2016, at 18:10, Martin Buchholz <martinrb at google.com <>> wrote:
>> 
>> 
>> 
>> On Sat, Sep 24, 2016 at 6:19 AM, Dávid Karnok <akarnokd at gmail.com <>> wrote:
>> I have a one element single-producer single-consumer "queue" implemented as this:
>> 
>> boolean offer(AtomicReference<T> ref, T value) {
>>     Objects.requireNonNull(value);
>>     if (ref.get() == null) {
>>         ref.lazySet(value);
>>         return true;
>>     }
>>     return false;
>> }
>> 
>> T poll(AtomicReference<T> ref) {
>>     T v = ref.get();
>>     if (v != null) {
>>        ref.lazySet(null);
>>     }
>>     return v;
>> }
>> 
>> Is it okay to turn get() into getAcquire() and lazySet into setRelease() (I can see lazySet delegates to setRelease)? 
>> 
>> Yes, but ... the poll and offer operations will no longer be part of the global sequentially consistent order of synchronization actions.
>> 
>> More generally, are there consequences turning CAS loops of get()+compareAndSet into getAcquire() + weakCompareAndSetRelease?
>> 
>>  
>> In addition, are there examples (or explanations) of when it is okay to use getOpaque/setOpaque  and would they work with the concurrent "queue" above at all?
>> 
>> (I guess there is not much difference for x86 but my code may end up on weaker platforms eventually.)
>> 
>> I think there is a difference on x86.  Conceptually, set/volatile write "drains the write buffer", while setRelease does not.
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> 
> 
> -- 
> Sent from my phone

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/ec3cc6cb/attachment-0001.html>

From martinrb at google.com  Sun Sep 25 16:01:27 2016
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 25 Sep 2016 13:01:27 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU_FApFOp-J5DVRsp+N7DM13aJv-s2sRSo29wx_Vb0PJxw@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
 <CANPzfU_FApFOp-J5DVRsp+N7DM13aJv-s2sRSo29wx_Vb0PJxw@mail.gmail.com>
Message-ID: <CA+kOe0_D4C2D+rSrmAdqr1eekBF25jMC=3QPbah1Rn_zks0p8w@mail.gmail.com>

On Sun, Sep 25, 2016 at 7:34 AM, Viktor Klang <viktor.klang at gmail.com>
wrote:

> If that truely is the case then the only way of implementing a readonly
> Future is by throwing an exception from cancel...
>
We the maintainers of j.u.c.Future have always thought that canceling a
Future will surely leave it completed.  Of course, implementers of any Java
interface can choose to throw UOE for any method, but this is not intended
for Future.cancel.  An interface without cancel probably should not be
extending Future.

---

Here's another way to think of the range of functionality between current
CompletionStage and current CompletableFuture:

- Add Polling methods from scala Future such as isCompleted
<http://www.scala-lang.org/api/2.12.0-RC1/scala/concurrent/Future.html#isCompleted:Boolean>
These are also discouraged, but less so than Await methods
"""Note: using this method yields nondeterministic dataflow programs."""
Will adding these to CompletionStage be less controversial?

- Add Await blocking methods (that in scala cannot be called directly,
using the CanAwait mechanism)

- Add cancellation

- Add other methods to complete the future ("Promise")

- Add the full CompletableFuture API, including the obtrude methods

Cancellation is interesting, because it's a mutating method, and so cannot
be used with a future shared with other users, but it also seems very
reasonable as a "client" operation.  One can think of obtaining a
non-shared future as a subscription to a one-time event, and that
subscription takes up resources in the form of an object.  If the client is
no longer interested in the event, then cancellation of the future can free
up resources.  I'm still thinking of what Process.onExit
<http://download.java.net/java/jdk9/docs/api/java/lang/Process.html#onExit-->
should return.  There's a tension between the various roles that
CompletableFuture serves - as a container to write a value, as a possibly
cancellable subscription to that value, and as a way to chain async
computations together.

On Wed, Sep 21, 2016 at 2:25 PM, Benjamin Manes <ben.manes at gmail.com> wrote:

> I've gradually come to terms using CF as part of an API and haven't
> experienced a downside yet.
>

Hmmmm.... I seem to be moving in that direction as well...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/29709d44/attachment.html>

From davidcholmes at aapt.net.au  Sun Sep 25 17:22:00 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 26 Sep 2016 07:22:00 +1000
Subject: [concurrency-interest] We need to add blocking methods
	to	CompletionStage!
In-Reply-To: <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
Message-ID: <030e01d21772$dbe934d0$93bb9e70$@aapt.net.au>

Joe,

 

That is ignoring the error case. If the cancel fails then it is not complete and it is not cancelled. We added the extra wording back in August 2005. It is interesting to note that Martin’s initial query then only related to the state of the thread, but that it was clear about things only happening if cancel returned true:

 

“My guess is that if cancel(true) returns true, then subsequent cals to isCancelled() and isDone() will return true, *even if the thread executing the task is still running*“

 

Yet we somehow added the clarification with no regard as to whether cancel returned true or not. That seems wrong.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Joe Bowbeer
Sent: Monday, September 26, 2016 12:20 AM
To: David Holmes <dholmes at ieee.org>
Cc: Martin Buchholz <martinrb at google.com>; concurrency-interest <concurrency-interest at cs.oswego.edu>; core-libs-dev <core-libs-dev at openjdk.java.net>
Subject: Re: [concurrency-interest] We need to add blocking methods to CompletionStage!

 

This statement regarding what happens after cancel is called is correct:

"After this method returns, subsequent calls to isDone() will always return true. Subsequent calls to isCancelled() will always return true if this method returned true."

After cancel returns, the future is completed, hence isDone. If cancel returns true, i.e. it was cancelled, then  isCancelled returns true. But, for example if the future is already completed when cancel is called, then cancel will return false and isCancelled will return false.

 

On Sep 25, 2016 6:49 AM, "David Holmes" <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au> > wrote:

I think that was meant to read “After this method returns _true_, subsequent calls …”

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu <mailto:concurrency-interest-bounces at cs.oswego.edu> ] On Behalf Of Viktor Klang
Sent: Sunday, September 25, 2016 9:03 PM
To: Martin Buchholz <martinrb at google.com <mailto:martinrb at google.com> >
Cc: concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> >; core-libs-dev <core-libs-dev at openjdk.java.net <mailto:core-libs-dev at openjdk.java.net> >
Subject: Re: [concurrency-interest] We need to add blocking methods to CompletionStage!

 

 

 

On Sun, Sep 25, 2016 at 12:34 PM, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com> > wrote:

 

 

On Sat, Sep 24, 2016 at 10:41 PM, Martin Buchholz <martinrb at google.com <mailto:martinrb at google.com> > wrote:

No one is suggesting we add cancel to CompletionStage - I agree that would break users, by making an immutable interface mutable. 

 

+10000

 

This also means that CompletionStage cannot extend Future.

 

+10000

 

I also would not want to have a toFuture method that would return a j.u.c.Future, because of misfit Future.cancel.

 

Would you mind elaborating here? According to the cancel method spec on Future it is completely fine for it to be a no-op which always returns false:

 

"This attempt will fail if the task has already completed, has already been cancelled, or could not be cancelled for some other reason."

 

Source: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Future.html

 

There's an interesting (read: weird) spec clause in cancel:

 

"After this method returns, subsequent calls to isDone() will always return true. Subsequent calls to isCancelled() will always return true if this method returned true."

 

That clause is in contradiction with the previously quoted line.

 

 

 

 

  If we are adding cancel, then it will be to make a new interface, such as the suggested CancellableCompletionStage.

 

I also agree that CompletionStage does *not* represent "a running task of sorts", j.u.c. Future specs are still confusing in that way due to FutureTask heritage.

 

+10000

 

 

On Thu, Sep 22, 2016 at 7:51 PM, James Roper <james at lightbend.com <mailto:james at lightbend.com> > wrote:

For example, we often cache futures and return them from a libraries API, if a client could cancel a future, that would break everything else that received that future.

 

On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com> > wrote:

PPS: A misunderstanding is that CompletionStage represents a running task of sorts, one that can be cancelled etc. This is not the case.

 

 





 

-- 

Cheers,

√





 

-- 

Cheers,

√


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/02713af7/attachment-0001.html>

From martinrb at google.com  Sun Sep 25 17:49:23 2016
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 25 Sep 2016 14:49:23 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
Message-ID: <CA+kOe084Kfma7D3ZA8qc_QnSQiUKV-Nt7uPE7A5m=nPY-=AS3w@mail.gmail.com>

On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com>
wrote:

>
> PS. As a sidenote, Martin, and in all friendliness, "actor purist API"?
> C'mon, I know you're better than that! CompletionStage's design has nothing
> to do with Actors and if Single Responsibility Principle is considered
> purism then I'm not sure why we don't have a single interface with all
> methods in it.
> Let's try to keep things friendly.
>

Let's be best friends!

The programming profession is still learning how to write concurrent
programs, fumbling in the dark, and that's particularly true for myself!
There are competing approaches and we don't know which will prove
successful.

PPPS: Adding blocking methods without mandatory timeouts has in practice
> proven to be a recipe for disaster.
>

Hmmmm... By design, Unix processes that create subprocesses should dedicate
a thread for each subprocess, that waits indefinitely in waitpid.  Linux
kernel folks deliberately decided this was fine.  In the Real World, one
needs to interface with Other Peoples' APIs.

Software that rarely blocks is a great idea.  Especially the folks who
write network server infrastructure increasingly agree.  But it is truly a
difficult paradigm shift; we will get there slowly.

---

Say you are implementing some existing function with a traditional
synchronous API.  Your implementation is multi-threaded, but that's an
implementation detail.  Before you return to the caller, you must wait for
the various parts of your computation to complete (the "join" part of
fork/join).  It seems reasonable to wait forever.  If some part of your
computation is interacting with an unreliable external resource, then it
makes sense for that particular "wait" (which need not occupy a thread) to
have a timeout.  Test methods, main methods and unix process reapers are
all examples where it's reasonable to block forever.


> PPPPS: "I think it's unreasonable to not provide this for users
> (especially when we can do so more efficiently)." <- If efficiency is
> desired then blocking is definitely not the right solution.
>

What about efficiently providing isComplete?

The result may already be available without actually blocking.  It may even
be known to be available immediately.  One would like to get the value
without additional allocation.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/99d808a5/attachment.html>

From oleksandr.otenko at gmail.com  Sun Sep 25 18:22:35 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Sun, 25 Sep 2016 23:22:35 +0100
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe084Kfma7D3ZA8qc_QnSQiUKV-Nt7uPE7A5m=nPY-=AS3w@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe084Kfma7D3ZA8qc_QnSQiUKV-Nt7uPE7A5m=nPY-=AS3w@mail.gmail.com>
Message-ID: <0D29F308-9D16-44A0-9232-CCD9183FFC94@gmail.com>


> On 25 Sep 2016, at 22:49, Martin Buchholz <martinrb at google.com> wrote:
> ...Say you are implementing some existing function with a traditional synchronous API.  Your implementation is multi-threaded, but that's an implementation detail.  Before you return to the caller, you must wait for the various parts of your computation to complete (the "join" part of fork/join).  It seems reasonable to wait forever.  If some part of your computation is interacting with an unreliable external resource, then it makes sense for that particular "wait" (which need not occupy a thread) to have a timeout.  Test methods, main methods and unix process reapers are all examples where it's reasonable to block forever.

More than that, cancelling the wait does not cancel the computation it was waiting for.

You need to time out / cancel the wait at the junctions where you have the power to arbitrate fair use of resource locked up in the caller, by the caller.

Alex

From martinrb at google.com  Sun Sep 25 20:39:32 2016
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 25 Sep 2016 17:39:32 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <030e01d21772$dbe934d0$93bb9e70$@aapt.net.au>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
 <030e01d21772$dbe934d0$93bb9e70$@aapt.net.au>
Message-ID: <CA+kOe0_9OHd3Or0JwRhe-q80etH4AkchSKGfTOzjQWTaxjzNbw@mail.gmail.com>

On Sun, Sep 25, 2016 at 2:22 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>
> Yet we somehow added the clarification with no regard as to whether cancel
> returned true or not. That seems wrong.
>

Yikes!  I had always assumed that cancel was not permitted to leave the
Future incomplete, perhaps influenced by the wording,

"""After this method returns, subsequent calls to {@link #isDone} will
always return {@code true}."""

It's much more in the spirit of Java to throw an exception if the future
cannot be completed.  It's never come up, I think.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/e8f90605/attachment.html>

From joe.bowbeer at gmail.com  Mon Sep 26 02:00:44 2016
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 25 Sep 2016 23:00:44 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0_9OHd3Or0JwRhe-q80etH4AkchSKGfTOzjQWTaxjzNbw@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
 <030e01d21772$dbe934d0$93bb9e70$@aapt.net.au>
 <CA+kOe0_9OHd3Or0JwRhe-q80etH4AkchSKGfTOzjQWTaxjzNbw@mail.gmail.com>
Message-ID: <CAHzJPEqMMP8njjG4J=4GwOzGYuJiCd++zsk-YCuhJ6ZP6xqRJA@mail.gmail.com>

Cancellation: David, I can see your point. Future.cancel(true) was
discussed 8/27/05 and the extra text was added to make it clearer that the
state of Future after cancel is called is separate from the state of any
associated thread or task.

However, I think the added text corresponded too closely to the only
implementation of Future.cancel that existed at the time. Elsewhere the
spec tries to permit Future.cancel to fail for some reason other than
because the Future had already completed:

  "This attempt will fail if the task [...] could not be cancelled for some
other reason."

  "returns false if the task could not be cancelled, typically because it
has already completed normally"

Without the added text, my interpretation would be that if cancel returns
false and then isDone returns false, then cancel failed for some other
reason.

On Sun, Sep 25, 2016 at 5:39 PM, Martin Buchholz <martinrb at google.com>
wrote:

>
>
> On Sun, Sep 25, 2016 at 2:22 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>
>> Yet we somehow added the clarification with no regard as to whether
>> cancel returned true or not. That seems wrong.
>>
>
> Yikes!  I had always assumed that cancel was not permitted to leave the
> Future incomplete, perhaps influenced by the wording,
>
> """After this method returns, subsequent calls to {@link #isDone} will
> always return {@code true}."""
>
> It's much more in the spirit of Java to throw an exception if the future
> cannot be completed.  It's never come up, I think.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160925/6468c87f/attachment.html>

From shade at redhat.com  Mon Sep 26 03:20:53 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Mon, 26 Sep 2016 09:20:53 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
Message-ID: <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>

On 09/24/2016 06:51 PM, Martin Buchholz wrote:
> Discussion on CAS loops got me looking again at our own:
> 
>     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>         V prev = get(), next = null;
>         for (boolean haveNext = false;;) {
>             if (!haveNext)
>                 next = updateFunction.apply(prev);
>             if (weakCompareAndSetVolatile(prev, next))
>                 return prev;
>             haveNext = (prev == (prev = get()));
>         }
>     }
> 
> The haveNext boolean and useless initialization of next bothers me.  We
> can do better!
> 
>     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>         for (V prev = get();;) {
>             V next = updateFunction.apply(prev);
>             do {
>                 if (weakCompareAndSetVolatile(prev, next))
>                     return prev;
>             } while (prev == (prev = get()));
>         }
>     }
> 
> even though it probably saves more bytecodes than cycles.

I don't quite get why do we need to retry on the same "prev" to begin
with: we are allowed to call function several times on the same value.
If so, there is a more canonical form:

     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
         V prev, next;
         do {
             prev = get();
             next = updateFunction.apply(prev);
         } while (!weakCompareAndSetVolatile(prev, next));
         return prev;
     }

CAS-vs-CAE example shows us that additional branches in the hot CAS loop
would be detrimental for performance under contention (and an quick
update function here, probably). Accurate benchmarking would tell you a
story, we cannot do this by looking at the code.

Also, retrying on (spurious) failure when prev had not changed negates
the reason to use weakCAS: now you have just constructed the retry loop
that strong CAS does with LL/SC, and we have tried to avoid that with
weakCAS!

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/63a8f11f/attachment-0001.sig>

From shade at redhat.com  Mon Sep 26 04:14:26 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Mon, 26 Sep 2016 10:14:26 +0200
Subject: [concurrency-interest] AtomicReference get vs. getAcquire;
 get/set Opaque
In-Reply-To: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>
References: <CAAWwtm9m_eenhXovtBzK4g1e5_tnpXtmyox-s8yQph=3imXvPA@mail.gmail.com>
Message-ID: <cefec72d-00b9-286f-75d8-b5021dbf681f@redhat.com>

On 09/24/2016 03:19 PM, Dávid Karnok wrote:
> I have a one element single-producer single-consumer "queue" implemented
> as this:
> 
> boolean offer(AtomicReference<T> ref, T value) {
>     Objects.requireNonNull(value);
>     if (ref.get() == null) {
>         ref.lazySet(value);
>         return true;
>     }
>     return false;
> }
> 
> T poll(AtomicReference<T> ref) {
>     T v = ref.get();
>     if (v != null) {
>        ref.lazySet(null);
>     }
>     return v;
> }
> 
> Is it okay to turn get() into getAcquire() and lazySet into setRelease()
> (I can see lazySet delegates to setRelease)? 

Replacing {get,set}Volatile with {get,set}Acquire/Release is generally
not okay, as you will relax the sequential consistency. See:
 http://cs.oswego.edu/pipermail/concurrency-interest/2016-May/015104.html
 http://cs.oswego.edu/pipermail/concurrency-interest/2016-March/015037.html

SPSC is more forgiving, and it _appears_ fine to replace with
getAcq/setRel in this example: a) you can publish the value correctly
through setRelease/getAcquire chain; b) you will never observe null in
offer() before poll() pulls the value, since there are WAR data
dependencies between get and lazySet on the same variable.


> In addition, are there examples (or explanations) of when it is okay to
> use getOpaque/setOpaque  and would they work with the concurrent "queue"
> above at all?

It is almost never okay to replace {get,set}{Volatile,Acquire,Release}
with {get,set}Opaque. Opaque does NOT guarantee ANY inter-thread ordering.


Thanks,
-Aleksey


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/40156a01/attachment.sig>

From viktor.klang at gmail.com  Mon Sep 26 10:29:48 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 26 Sep 2016 16:29:48 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0_D4C2D+rSrmAdqr1eekBF25jMC=3QPbah1Rn_zks0p8w@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
 <CANPzfU_FApFOp-J5DVRsp+N7DM13aJv-s2sRSo29wx_Vb0PJxw@mail.gmail.com>
 <CA+kOe0_D4C2D+rSrmAdqr1eekBF25jMC=3QPbah1Rn_zks0p8w@mail.gmail.com>
Message-ID: <CANPzfU-SgeQy=QeSOj5oYcf23ubmViSBKOhmQTbMTOv=PCAApA@mail.gmail.com>

On Sun, Sep 25, 2016 at 10:01 PM, Martin Buchholz <martinrb at google.com>
wrote:

>
>
> On Sun, Sep 25, 2016 at 7:34 AM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>> If that truely is the case then the only way of implementing a readonly
>> Future is by throwing an exception from cancel...
>>
> We the maintainers of j.u.c.Future have always thought that canceling a
> Future will surely leave it completed.  Of course, implementers of any Java
> interface can choose to throw UOE for any method, but this is not intended
> for Future.cancel.  An interface without cancel probably should not be
> extending Future.
>

This seems to have been "addressed" further down this thread.


>
> ---
>
> Here's another way to think of the range of functionality between current
> CompletionStage and current CompletableFuture:
>
> - Add Polling methods from scala Future such as isCompleted
> <http://www.scala-lang.org/api/2.12.0-RC1/scala/concurrent/Future.html#isCompleted:Boolean>
> These are also discouraged, but less so than Await methods
> """Note: using this method yields nondeterministic dataflow programs."""
> Will adding these to CompletionStage be less controversial?
>
> - Add Await blocking methods (that in scala cannot be called directly,
> using the CanAwait mechanism)
>
> - Add cancellation
>
> - Add other methods to complete the future ("Promise")
>
> - Add the full CompletableFuture API, including the obtrude methods
>

> Cancellation is interesting, because it's a mutating method, and so cannot
> be used with a future shared with other users, but it also seems very
> reasonable as a "client" operation.  One can think of obtaining a
> non-shared future as a subscription to a one-time event, and that
> subscription takes up resources in the form of an object.  If the client is
> no longer interested in the event, then cancellation of the future can free
> up resources.  I'm still thinking of what Process.onExit
> <http://download.java.net/java/jdk9/docs/api/java/lang/Process.html#onExit-->
> should return.  There's a tension between the various roles that
> CompletableFuture serves - as a container to write a value, as a possibly
> cancellable subscription to that value, and as a way to chain async
> computations together.
>

I'd rather suggest creating a 2.0 of FutureTask (i.e. represent the Task
part separately from the Future part)
Have something represents something being executed somewhere else, that can
support cancellation but also expose a read-only facet of itself.

I think you're right in that something is missing which would lie somewhere
in between CompletionStage and CompletableFuture.
What methods would you propose to add to such an interface and why?


>
> On Wed, Sep 21, 2016 at 2:25 PM, Benjamin Manes <ben.manes at gmail.com>
> wrote:
>
>> I've gradually come to terms using CF as part of an API and haven't
>> experienced a downside yet.
>>
>
> Hmmmm.... I seem to be moving in that direction as well...
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/713d3a9d/attachment.html>

From viktor.klang at gmail.com  Mon Sep 26 10:30:21 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 26 Sep 2016 16:30:21 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <030e01d21772$dbe934d0$93bb9e70$@aapt.net.au>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
 <030e01d21772$dbe934d0$93bb9e70$@aapt.net.au>
Message-ID: <CANPzfU_i3U98DDqEz4KGE4A-nMtKhXtx7zbhfsHEDpiHh8SBOQ@mail.gmail.com>

On Sun, Sep 25, 2016 at 11:22 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

> Joe,
>
>
>
> That is ignoring the error case. If the cancel fails then it is not
> complete and it is not cancelled. We added the extra wording back in August
> 2005. It is interesting to note that Martin’s initial query then only
> related to the state of the thread, but that it was clear about things only
> happening if cancel returned true:
>
>
>
> “My guess is that if cancel(true) returns true, then subsequent cals to
> isCancelled() and isDone() will return true, *even if the thread executing
> the task is still running*“
>
>
>
> Yet we somehow added the clarification with no regard as to whether cancel
> returned true or not. That seems wrong.
>

Agreed.


>
>
> David
>
>
>
> *From:* Concurrency-interest [mailto:concurrency-interest-
> bounces at cs.oswego.edu] *On Behalf Of *Joe Bowbeer
> *Sent:* Monday, September 26, 2016 12:20 AM
> *To:* David Holmes <dholmes at ieee.org>
> *Cc:* Martin Buchholz <martinrb at google.com>; concurrency-interest <
> concurrency-interest at cs.oswego.edu>; core-libs-dev <
> core-libs-dev at openjdk.java.net>
>
> *Subject:* Re: [concurrency-interest] We need to add blocking methods to
> CompletionStage!
>
>
>
> This statement regarding what happens after cancel is called is correct:
>
> "*After this method returns, subsequent calls to isDone() will always
> return true*. Subsequent calls to isCancelled() will always return true
> if this method returned true."
>
> After cancel returns, the future is completed, hence isDone. If cancel
> returns true, i.e. it was cancelled, then  isCancelled returns true. But,
> for example if the future is already completed when cancel is called, then
> cancel will return false and isCancelled will return false.
>
>
>
> On Sep 25, 2016 6:49 AM, "David Holmes" <davidcholmes at aapt.net.au> wrote:
>
> I think that was meant to read “After this method returns _*true*_,
> subsequent calls …”
>
>
>
> David
>
>
>
> *From:* Concurrency-interest [mailto:concurrency-interest-
> bounces at cs.oswego.edu] *On Behalf Of *Viktor Klang
> *Sent:* Sunday, September 25, 2016 9:03 PM
> *To:* Martin Buchholz <martinrb at google.com>
> *Cc:* concurrency-interest <concurrency-interest at cs.oswego.edu>;
> core-libs-dev <core-libs-dev at openjdk.java.net>
> *Subject:* Re: [concurrency-interest] We need to add blocking methods to
> CompletionStage!
>
>
>
>
>
>
>
> On Sun, Sep 25, 2016 at 12:34 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>
>
>
>
> On Sat, Sep 24, 2016 at 10:41 PM, Martin Buchholz <martinrb at google.com>
> wrote:
>
> No one is suggesting we add cancel to CompletionStage - I agree that would
> break users, by making an immutable interface mutable.
>
>
>
> +10000
>
>
>
> This also means that CompletionStage cannot extend Future.
>
>
>
> +10000
>
>
>
> I also would not want to have a toFuture method that would return a
> j.u.c.Future, because of misfit Future.cancel.
>
>
>
> Would you mind elaborating here? According to the cancel method spec on
> Future it is completely fine for it to be a no-op which always returns
> false:
>
>
>
> "This attempt will fail if the task has already completed, has already
> been cancelled, *or could not be cancelled for some other reason.*"
>
>
>
> Source: https://docs.oracle.com/javase/8/docs/api/java/util/
> concurrent/Future.html
>
>
>
> There's an interesting (read: weird) spec clause in cancel:
>
>
>
> "*After this method returns, subsequent calls to isDone() will always
> return true*. Subsequent calls to isCancelled() will always return true
> if this method returned true."
>
>
>
> That clause is in contradiction with the previously quoted line.
>
>
>
>
>
>
>
>
>
>   If we are adding cancel, then it will be to make a new interface, such
> as the suggested CancellableCompletionStage.
>
>
>
> I also agree that CompletionStage does *not* represent "a running task of
> sorts", j.u.c. Future specs are still confusing in that way due to
> FutureTask heritage.
>
>
>
> +10000
>
>
>
>
>
> On Thu, Sep 22, 2016 at 7:51 PM, James Roper <james at lightbend.com> wrote:
>
> For example, we often cache futures and return them from a libraries API,
> if a client could cancel a future, that would break everything else that
> received that future.
>
>
>
> On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
> PPS: A misunderstanding is that CompletionStage represents a running task
> of sorts, one that can be cancelled etc. This is not the case.
>
>
>
>
>
>
>
>
>
> --
>
> Cheers,
>
> √
>
>
>
>
>
> --
>
> Cheers,
>
> √
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/0a8dbcc8/attachment-0001.html>

From viktor.klang at gmail.com  Mon Sep 26 10:55:17 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 26 Sep 2016 16:55:17 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe084Kfma7D3ZA8qc_QnSQiUKV-Nt7uPE7A5m=nPY-=AS3w@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe084Kfma7D3ZA8qc_QnSQiUKV-Nt7uPE7A5m=nPY-=AS3w@mail.gmail.com>
Message-ID: <CANPzfU8hySBTwZj6-Y0O97LLm5sK+fDH8Js1sDZxgxkJubhbBg@mail.gmail.com>

On Sun, Sep 25, 2016 at 11:49 PM, Martin Buchholz <martinrb at google.com>
wrote:

>
>
> On Fri, Sep 23, 2016 at 2:24 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>>
>> PS. As a sidenote, Martin, and in all friendliness, "actor purist API"?
>> C'mon, I know you're better than that! CompletionStage's design has nothing
>> to do with Actors and if Single Responsibility Principle is considered
>> purism then I'm not sure why we don't have a single interface with all
>> methods in it.
>> Let's try to keep things friendly.
>>
>
> Let's be best friends!
>

One step at a time… :)


>
> The programming profession is still learning how to write concurrent
> programs, fumbling in the dark, and that's particularly true for myself!
> There are competing approaches and we don't know which will prove
> successful.
>

I'm not sure that they are all that competing TBH! There's always the
tradeoff between utility and safety. The problem is that the JDK is a
single layer for ALL Java programmers. Sometimes I think it would be wiser
to have one layer of high-utility/unsafer for library developers and other
"low-level tasks" and have a lower-utility/safer layer for more App-level
development. (I.e. always choose the least powerful thing with the highest
safety which gets the job done.)


>
> PPPS: Adding blocking methods without mandatory timeouts has in practice
>> proven to be a recipe for disaster.
>>
>
> Hmmmm... By design, Unix processes that create subprocesses should
> dedicate a thread for each subprocess, that waits indefinitely in waitpid.
> Linux kernel folks deliberately decided this was fine.  In the Real World,
> one needs to interface with Other Peoples' APIs.
>

If you expect the machine to be up until the end of the universe, I think
it's fine to put an unbounded wait.
In the Real World—nothing waits forever for anything. (Evolution and all
that…)


>
> Software that rarely blocks is a great idea.  Especially the folks who
> write network server infrastructure increasingly agree.  But it is truly a
> difficult paradigm shift; we will get there slowly.
>

Trust me—having spent the past 6+ years on that journey—I know exactly what
you're talking about!


>
> ---
>
> Say you are implementing some existing function with a traditional
> synchronous API.  Your implementation is multi-threaded, but that's an
> implementation detail.  Before you return to the caller, you must wait for
> the various parts of your computation to complete (the "join" part of
> fork/join).  It seems reasonable to wait forever.  If some part of your
> computation is interacting with an unreliable external resource, then it
> makes sense for that particular "wait" (which need not occupy a thread) to
> have a timeout.
>

If you're implementing some existing method which expects a synchronous
response, but are doing so in a deferred way, that is exactly the kids of
situations where that tends to come back haunt you after a while.
Especially if calls to said implementation by some chance ends up being
executed on said multithreaded implementation's thread pool. (Enqueue
"hilarious" production deadlock-forever)


> Test methods,
>

Yeah, I thought so as well, but it turns out that when you have tons of
async tests, not being able to start new tests until either that timeout or
result makes for a very slow test suite, so that's why most serious test
frameworks are growing support for dealing with async code. Then all you
want to be able to limit is work-in-progress (sloppily called parallelism)


> main methods
>

That's a common scenario, but one that can be solved by having non-daemonic
pooled worker threads.


> and unix process reapers are all examples where it's reasonable to block
> forever.
>

What about waitpid() + WNOHANG?


>
>
>> PPPPS: "I think it's unreasonable to not provide this for users
>> (especially when we can do so more efficiently)." <- If efficiency is
>> desired then blocking is definitely not the right solution.
>>
>
> What about efficiently providing isComplete?
>

In my experience isComplete is virtually useless without being able to
extract the value, in which case you may as well introduce a non-blocking
`Optional<T> poll()`


>
> The result may already be available without actually blocking.  It may
> even be known to be available immediately.  One would like to get the value
> without additional allocation.
>

I've seen that use-case :), and it tends to either be a situation where the
value is available by pure luck (or…scheduling artifacts) or when one is
keeping CompletionStages where strict values could be kept instead (think
rebinding a value on completion).

Reading what your'e writing, may I dare propose that what you're after is
something along the lines of a: PollableCompletionStage which sits in
between CompletionStage and CompletableFuture?

-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/7ac359a8/attachment.html>

From martinrb at google.com  Mon Sep 26 13:01:22 2016
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 26 Sep 2016 10:01:22 -0700
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
Message-ID: <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>

Hi Aleksey,

My goals were to
- have the shortest code path if the CAS succeeds the first time, which is
hopefully the common case
- never call the update function multiple times in the case of spurious
failure, only for real contention.  We don't know how expensive the update
function is, and the number of times it is called is user-detectable.
- yes, I have sort-of reconstructed the retry loop that strong CAS does
with LL/SC, BUT we can save the value obtained from get(), which does not
happen in the emulation below.  We don't have multiple value return in java!

    public final V getAndUpdate(UnaryOperator<V> updateFunction) {
        for (;;) {
            V prev = get();
            V next = updateFunction.apply(prev);
            if (strongCas(prev, next))
                return prev;
        }
    }
    /** Implement strong CAS using weak CAS. */
    boolean strongCas(V expectedValue, V newValue) {
        while (get() == expectedValue) {
            if (weakCompareAndSetVolatile(expectedValue, newValue))
                return true;
        }
        return false;
    }


On Mon, Sep 26, 2016 at 12:20 AM, Aleksey Shipilev <shade at redhat.com> wrote:

> On 09/24/2016 06:51 PM, Martin Buchholz wrote:
> > Discussion on CAS loops got me looking again at our own:
> >
> >     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
> >         V prev = get(), next = null;
> >         for (boolean haveNext = false;;) {
> >             if (!haveNext)
> >                 next = updateFunction.apply(prev);
> >             if (weakCompareAndSetVolatile(prev, next))
> >                 return prev;
> >             haveNext = (prev == (prev = get()));
> >         }
> >     }
> >
> > The haveNext boolean and useless initialization of next bothers me.  We
> > can do better!
> >
> >     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
> >         for (V prev = get();;) {
> >             V next = updateFunction.apply(prev);
> >             do {
> >                 if (weakCompareAndSetVolatile(prev, next))
> >                     return prev;
> >             } while (prev == (prev = get()));
> >         }
> >     }
> >
> > even though it probably saves more bytecodes than cycles.
>
> I don't quite get why do we need to retry on the same "prev" to begin
> with: we are allowed to call function several times on the same value.
> If so, there is a more canonical form:
>
>      public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>          V prev, next;
>          do {
>              prev = get();
>              next = updateFunction.apply(prev);
>          } while (!weakCompareAndSetVolatile(prev, next));
>          return prev;
>      }
>
> CAS-vs-CAE example shows us that additional branches in the hot CAS loop
> would be detrimental for performance under contention (and an quick
> update function here, probably). Accurate benchmarking would tell you a
> story, we cannot do this by looking at the code.
>
> Also, retrying on (spurious) failure when prev had not changed negates
> the reason to use weakCAS: now you have just constructed the retry loop
> that strong CAS does with LL/SC, and we have tried to avoid that with
> weakCAS!
>
> Thanks,
> -Aleksey
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/5d219e12/attachment.html>

From aph at redhat.com  Mon Sep 26 13:15:38 2016
From: aph at redhat.com (Andrew Haley)
Date: Mon, 26 Sep 2016 18:15:38 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
Message-ID: <6973c7b7-d26f-f248-df52-106a17d01879@redhat.com>

On 26/09/16 18:01, Martin Buchholz wrote:
> My goals were to
> - have the shortest code path if the CAS succeeds the first time, which is
> hopefully the common case
> - never call the update function multiple times in the case of spurious
> failure, only for real contention.  We don't know how expensive the update
> function is, and the number of times it is called is user-detectable.

That's probably suboptimal.  The ratio of truly spurious failures
(caused by e.g pre-emption) to actual contention is probably pretty
small, so if a SC fails it's probably ture contention, and the value
in memory really has changed.  In that case Aleksey's version is
better.

It's impossible to tell for sure because it depends on your actual
contention rate in your application.

Andrew.

From paul.sandoz at oracle.com  Mon Sep 26 13:36:26 2016
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Mon, 26 Sep 2016 10:36:26 -0700
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
Message-ID: <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>


> On 26 Sep 2016, at 10:01, Martin Buchholz <martinrb at google.com> wrote:
> 
> Hi Aleksey,
> 
> My goals were to
> - have the shortest code path if the CAS succeeds the first time, which is hopefully the common case
> - never call the update function multiple times in the case of spurious failure, only for real contention.  We don't know how expensive the update function is, and the number of times it is called is user-detectable.
> - yes, I have sort-of reconstructed the retry loop that strong CAS does with LL/SC, BUT we can save the value obtained from get(), which does not happen in the emulation below.  We don't have multiple value return in java!
> 
>     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>         for (;;) {
>             V prev = get();
>             V next = updateFunction.apply(prev);
>             if (strongCas(prev, next))
>                 return prev;
>         }
>     }
>     /** Implement strong CAS using weak CAS. */
>     boolean strongCas(V expectedValue, V newValue) {
>         while (get() == expectedValue) {
>             if (weakCompareAndSetVolatile(expectedValue, newValue))
>                 return true;
>         }
>         return false;
>     }
> 

If you are gonna do why not just call “compareAndSet" instead of emulating with your own “strongCas” method? and then, i presume, we are back to where we started.

I don’t think the “updateFunction” can tell a spurious failure from another failure given the variable, under contention, can change from and back to the previous between the “strongCas” call.

I prefer Aleksey’s version.

Paul.



> 
> On Mon, Sep 26, 2016 at 12:20 AM, Aleksey Shipilev <shade at redhat.com <mailto:shade at redhat.com>> wrote:
> On 09/24/2016 06:51 PM, Martin Buchholz wrote:
> > Discussion on CAS loops got me looking again at our own:
> >
> >     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
> >         V prev = get(), next = null;
> >         for (boolean haveNext = false;;) {
> >             if (!haveNext)
> >                 next = updateFunction.apply(prev);
> >             if (weakCompareAndSetVolatile(prev, next))
> >                 return prev;
> >             haveNext = (prev == (prev = get()));
> >         }
> >     }
> >
> > The haveNext boolean and useless initialization of next bothers me.  We
> > can do better!
> >
> >     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
> >         for (V prev = get();;) {
> >             V next = updateFunction.apply(prev);
> >             do {
> >                 if (weakCompareAndSetVolatile(prev, next))
> >                     return prev;
> >             } while (prev == (prev = get()));
> >         }
> >     }
> >
> > even though it probably saves more bytecodes than cycles.
> 
> I don't quite get why do we need to retry on the same "prev" to begin
> with: we are allowed to call function several times on the same value.
> If so, there is a more canonical form:
> 
>      public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>          V prev, next;
>          do {
>              prev = get();
>              next = updateFunction.apply(prev);
>          } while (!weakCompareAndSetVolatile(prev, next));
>          return prev;
>      }
> 
> CAS-vs-CAE example shows us that additional branches in the hot CAS loop
> would be detrimental for performance under contention (and an quick
> update function here, probably). Accurate benchmarking would tell you a
> story, we cannot do this by looking at the code.
> 
> Also, retrying on (spurious) failure when prev had not changed negates
> the reason to use weakCAS: now you have just constructed the retry loop
> that strong CAS does with LL/SC, and we have tried to avoid that with
> weakCAS!
> 
> Thanks,
> -Aleksey
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/74317c78/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/74317c78/attachment-0001.sig>

From martinrb at google.com  Mon Sep 26 13:51:59 2016
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 26 Sep 2016 10:51:59 -0700
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
Message-ID: <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>

On Mon, Sep 26, 2016 at 10:36 AM, Paul Sandoz <paul.sandoz at oracle.com>
wrote:

>
> On 26 Sep 2016, at 10:01, Martin Buchholz <martinrb at google.com> wrote:
>
> Hi Aleksey,
>
> My goals were to
> - have the shortest code path if the CAS succeeds the first time, which is
> hopefully the common case
> - never call the update function multiple times in the case of spurious
> failure, only for real contention.  We don't know how expensive the update
> function is, and the number of times it is called is user-detectable.
> - yes, I have sort-of reconstructed the retry loop that strong CAS does
> with LL/SC, BUT we can save the value obtained from get(), which does not
> happen in the emulation below.  We don't have multiple value return in java!
>
>     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>         for (;;) {
>             V prev = get();
>             V next = updateFunction.apply(prev);
>             if (strongCas(prev, next))
>                 return prev;
>         }
>     }
>     /** Implement strong CAS using weak CAS. */
>     boolean strongCas(V expectedValue, V newValue) {
>         while (get() == expectedValue) {
>             if (weakCompareAndSetVolatile(expectedValue, newValue))
>                 return true;
>         }
>         return false;
>     }
>
>
> If you are gonna do why not just call “compareAndSet" instead of emulating
> with your own “strongCas” method? and then, i presume, we are back to where
> we started.
>
>
That code was just demonstrating how the return value from get() was
getting lost, whereas if you inline a weak cas loop as I have done, you
save the extra call to get().


> I don’t think the “updateFunction” can tell a spurious failure from
> another failure given the variable, under contention, can change from and
> back to the previous between the “strongCas” call.
>

I'm imagining user code that counts all the calls to getAndUpdate, and all
the calls to updateFunction.  If they differ, I would like that to be
evidence of true contention.   True, we tell users updateFunction is
supposed to be side-effect-free.  OTOH, we say that attempted updates may
fail due to "contention among threads", suggesting we don't call
updateFunction spuriously.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/cb1db864/attachment.html>

From martinrb at google.com  Mon Sep 26 19:18:52 2016
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 26 Sep 2016 16:18:52 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU8hySBTwZj6-Y0O97LLm5sK+fDH8Js1sDZxgxkJubhbBg@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe084Kfma7D3ZA8qc_QnSQiUKV-Nt7uPE7A5m=nPY-=AS3w@mail.gmail.com>
 <CANPzfU8hySBTwZj6-Y0O97LLm5sK+fDH8Js1sDZxgxkJubhbBg@mail.gmail.com>
Message-ID: <CA+kOe0_KaKKfzN0pZhCRXRfLu8ynmxfm5rFYF7bgpZBmPGEFDQ@mail.gmail.com>

On Mon, Sep 26, 2016 at 7:55 AM, Viktor Klang <viktor.klang at gmail.com>
wrote:

>
>>
> Test methods,
>>
>
> Yeah, I thought so as well, but it turns out that when you have tons of
> async tests, not being able to start new tests until either that timeout or
> result makes for a very slow test suite, so that's why most serious test
> frameworks are growing support for dealing with async code. Then all you
> want to be able to limit is work-in-progress (sloppily called parallelism)
>

I don't see any support in junit or testng for multi-threaded tests.  jtreg
uses the strategy of having a set of reusable JVM processes, each of which
is running only one test at a time, which provides "pretty good" isolation,
and seems to work well.


> main methods
>>
>
> That's a common scenario, but one that can be solved by having
> non-daemonic pooled worker threads.
>

Do you have docs for such a thread pool?


> and unix process reapers are all examples where it's reasonable to block
>> forever.
>>
>
> What about waitpid() + WNOHANG?
>

Are you suggesting periodic polling is better than blocking?


> PPPPS: "I think it's unreasonable to not provide this for users
>>> (especially when we can do so more efficiently)." <- If efficiency is
>>> desired then blocking is definitely not the right solution.
>>>
>>
>> What about efficiently providing isComplete?
>>
>
> In my experience isComplete is virtually useless without being able to
> extract the value, in which case you may as well introduce a non-blocking
> `Optional<T> poll()`
>

Do you support adding the Polling methods from
http://www.scala-lang.org/api/2.12.0-RC1/scala/concurrent/Future.html
to CompletionStage, i.e. isDone and getNow?


> The result may already be available without actually blocking.  It may
>> even be known to be available immediately.  One would like to get the value
>> without additional allocation.
>>
>
> I've seen that use-case :), and it tends to either be a situation where
> the value is available by pure luck (or…scheduling artifacts) or when one
> is keeping CompletionStages where strict values could be kept instead
> (think rebinding a value on completion).
>
> Reading what your'e writing, may I dare propose that what you're after is
> something along the lines of a: PollableCompletionStage which sits in
> between CompletionStage and CompletableFuture?
>

I've been waffling!  Right now, I'm leaning towards having Java APIs return
fully mutable CompletableFutures as Benjamin and Pavel suggested upthread.
Why?  Because a completion stage can be thought of as all of:
- a consumer of an upstream value
- a subscription to the event that makes the upstream value available, and
- the producer of a possible value for downstream consumers.
Cancellation could be interpreted as a request to unsubscribe from upstream
producer or a notification to downstream consumers that no value will be
forthcoming, or both.

Here's a problem with jdk9 minimalCompletionStage: even if you are happy
with the minimality of the source stage (perhaps it's shared) you might
want downstream stages to be mutable, but the methods such as thenApply
also create minimal completion stages.  If you want to convert to a mutable
future, you can try calling toCompletableFuture, but there's no guarantee
that will work, even if it doesn't throw UOE.  You can hand-construct a
CompletableFuture which is then completed in a Runnable via thenRun, but
then this is opaque to the CompletableFuture implementation, and
implementation features such as stack overflow prevention will be
ineffective.

So right now I'm OK with e.g. Process#onExit simply returning
CompletableFuture.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/83c2573f/attachment.html>

From peter.levart at gmail.com  Mon Sep 26 19:32:54 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 27 Sep 2016 01:32:54 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
Message-ID: <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>

Hi,

I don't know about you, but on my i7 PC, I get consistently better 
results using strong CAS as opposed to weak CAS for the following benchmark:

http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench.java

The results:

Benchmark                                      Mode  Cnt Score   Error  
Units
GetAndUpdateBench.dflt                         avgt   20 53.947 ± 0.770  
ns/op
GetAndUpdateBench.dflt:getAndUpdate1_dflt      avgt   20 54.103 ± 1.083  
ns/op
GetAndUpdateBench.dflt:getAndUpdate2_dflt      avgt   20 53.791 ± 1.014  
ns/op
GetAndUpdateBench.martin                       avgt   20 52.724 ± 0.218  
ns/op
GetAndUpdateBench.martin:getAndUpdate1_martin  avgt   20 52.379 ± 0.972  
ns/op
GetAndUpdateBench.martin:getAndUpdate2_martin  avgt   20 53.070 ± 1.005  
ns/op
GetAndUpdateBench.shade                        avgt   20 53.437 ± 0.624  
ns/op
GetAndUpdateBench.shade:getAndUpdate1_shade    avgt   20 53.401 ± 0.615  
ns/op
GetAndUpdateBench.shade:getAndUpdate2_shade    avgt   20 53.474 ± 0.652  
ns/op
GetAndUpdateBench.strong                       avgt   20 38.090 ± 0.727  
ns/op
GetAndUpdateBench.strong:getAndUpdate1_strong  avgt   20 37.784 ± 1.633  
ns/op
GetAndUpdateBench.strong:getAndUpdate2_strong  avgt   20 38.397 ± 1.691  
ns/op


The benchmark exhibits 2 threads that don't have data contention (each 
of them is modifying its own variable), but most probably those threads 
do modify the same cache line. The update function does have a non-zero 
cost.

To complement above results, I also ran the same code on Raspbery PI 2 
with Oracle build of JDK 9 (b137):

Benchmark                                      Mode  Cnt Score   Error  
Units
GetAndUpdateBench.dflt                         avgt   20 2347.811 ± 
2.492  ns/op
GetAndUpdateBench.dflt:getAndUpdate1_dflt      avgt   20 2347.328 ± 
6.697  ns/op
GetAndUpdateBench.dflt:getAndUpdate2_dflt      avgt   20 2348.294 ± 
7.052  ns/op
GetAndUpdateBench.martin                       avgt   20 2342.773 ± 
5.618  ns/op
GetAndUpdateBench.martin:getAndUpdate1_martin  avgt   20 2342.398 ± 
8.662  ns/op
GetAndUpdateBench.martin:getAndUpdate2_martin  avgt   20 2343.148 ± 
9.514  ns/op
GetAndUpdateBench.shade                        avgt   20 1844.426 ± 
4.304  ns/op
GetAndUpdateBench.shade:getAndUpdate1_shade    avgt   20 1841.747 ± 
5.287  ns/op
GetAndUpdateBench.shade:getAndUpdate2_shade    avgt   20 1847.105 ± 
6.416  ns/op
GetAndUpdateBench.strong                       avgt   20 1846.413 ± 
1.155  ns/op
GetAndUpdateBench.strong:getAndUpdate1_strong  avgt   20 1846.312 ± 
8.095  ns/op
GetAndUpdateBench.strong:getAndUpdate2_strong  avgt   20 1846.513 ± 
8.110  ns/op


Here we may be observing a strange phenomena. Reexecuting the 
non-zero-cost update function on spurious failure (shade) surprisingly 
gives better throughput than immediately retrying weak CAS (dflt, martin).

Also, wasn't strong CAS on ARM supposed to be mapped to a weak CAS loop? 
Why the difference between dflt/martin and strong then?

You've got something to study now... ;-)

Regards, Peter



On 09/26/2016 07:51 PM, Martin Buchholz wrote:
>
>
> On Mon, Sep 26, 2016 at 10:36 AM, Paul Sandoz <paul.sandoz at oracle.com 
> <mailto:paul.sandoz at oracle.com>> wrote:
>
>
>>     On 26 Sep 2016, at 10:01, Martin Buchholz <martinrb at google.com
>>     <mailto:martinrb at google.com>> wrote:
>>
>>     Hi Aleksey,
>>
>>     My goals were to
>>     - have the shortest code path if the CAS succeeds the first time,
>>     which is hopefully the common case
>>     - never call the update function multiple times in the case of
>>     spurious failure, only for real contention.  We don't know how
>>     expensive the update function is, and the number of times it is
>>     called is user-detectable.
>>     - yes, I have sort-of reconstructed the retry loop that strong
>>     CAS does with LL/SC, BUT we can save the value obtained from
>>     get(), which does not happen in the emulation below.  We don't
>>     have multiple value return in java!
>>
>>         public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>>             for (;;) {
>>                 V prev = get();
>>                 V next = updateFunction.apply(prev);
>>                 if (strongCas(prev, next))
>>                     return prev;
>>             }
>>         }
>>         /** Implement strong CAS using weak CAS. */
>>         boolean strongCas(V expectedValue, V newValue) {
>>             while (get() == expectedValue) {
>>                 if (weakCompareAndSetVolatile(expectedValue, newValue))
>>                     return true;
>>             }
>>             return false;
>>         }
>>
>
>     If you are gonna do why not just call “compareAndSet" instead of
>     emulating with your own “strongCas” method? and then, i presume,
>     we are back to where we started.
>
>
> That code was just demonstrating how the return value from get() was 
> getting lost, whereas if you inline a weak cas loop as I have done, 
> you save the extra call to get().
>
>     I don’t think the “updateFunction” can tell a spurious failure
>     from another failure given the variable, under contention, can
>     change from and back to the previous between the “strongCas” call.
>
>
> I'm imagining user code that counts all the calls to getAndUpdate, and 
> all the calls to updateFunction.  If they differ, I would like that to 
> be evidence of true contention.   True, we tell users updateFunction 
> is supposed to be side-effect-free.  OTOH, we say that attempted 
> updates may fail due to "contention among threads", suggesting we 
> don't call updateFunction spuriously.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160927/0a7aff50/attachment-0001.html>

From ben.manes at gmail.com  Mon Sep 26 20:21:38 2016
From: ben.manes at gmail.com (Benjamin Manes)
Date: Mon, 26 Sep 2016 17:21:38 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0_KaKKfzN0pZhCRXRfLu8ynmxfm5rFYF7bgpZBmPGEFDQ@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe084Kfma7D3ZA8qc_QnSQiUKV-Nt7uPE7A5m=nPY-=AS3w@mail.gmail.com>
 <CANPzfU8hySBTwZj6-Y0O97LLm5sK+fDH8Js1sDZxgxkJubhbBg@mail.gmail.com>
 <CA+kOe0_KaKKfzN0pZhCRXRfLu8ynmxfm5rFYF7bgpZBmPGEFDQ@mail.gmail.com>
Message-ID: <CAGu0=MOxC4ixc1ND=st3qd6SROmWNK=oVg7rSA2PBLAtW25BxA@mail.gmail.com>

>
> I don't see any support in junit or testng for multi-threaded tests.


TestNG has basic support on @Test for running a test method concurrently.
This assumes synchronous code that does not perform complex coordination,
e.g. simple writes into a ConcurrentMap. Specifically the annotation
provides "threadPoolSize", "invocationCount", and "invocationTimeOut". It
does support multi-threaded test execution at the class or method level, or
isolation by JVM forking, for faster build times.

For test methods I most often use Awaitility
<https://github.com/awaitility/awaitility> with a JCiP-style test harness
<http://jcip.net/listings/TestHarness.java>. Lots of other options out
there depending on your needs.

I do think Doug's original design statement, quoted earlier, is probably
the best answer. It seems impossible for j.u.c. to provide the perfect API
for everyone for any given scenario. Instead CF is a building block that
you can limit or enhance by exposing a custom class. That may mean a
proliferation of subsets or over use of returning CF, but that could happen
regardless. Since it is easy to compose with lambdas, I consider the
approach taken pragmatic and the best of unsatisfying options.

Other than a few minor enhancements coming in JDK9, I'm pretty happy with
CF as is.

On Mon, Sep 26, 2016 at 4:18 PM, Martin Buchholz <martinrb at google.com>
wrote:

>
>
> On Mon, Sep 26, 2016 at 7:55 AM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>>
>>>
>> Test methods,
>>>
>>
>> Yeah, I thought so as well, but it turns out that when you have tons of
>> async tests, not being able to start new tests until either that timeout or
>> result makes for a very slow test suite, so that's why most serious test
>> frameworks are growing support for dealing with async code. Then all you
>> want to be able to limit is work-in-progress (sloppily called parallelism)
>>
>
> I don't see any support in junit or testng for multi-threaded tests.
>  jtreg uses the strategy of having a set of reusable JVM processes, each of
> which is running only one test at a time, which provides "pretty good"
> isolation, and seems to work well.
>
>
>> main methods
>>>
>>
>> That's a common scenario, but one that can be solved by having
>> non-daemonic pooled worker threads.
>>
>
> Do you have docs for such a thread pool?
>
>
>> and unix process reapers are all examples where it's reasonable to block
>>> forever.
>>>
>>
>> What about waitpid() + WNOHANG?
>>
>
> Are you suggesting periodic polling is better than blocking?
>
>
>> PPPPS: "I think it's unreasonable to not provide this for users
>>>> (especially when we can do so more efficiently)." <- If efficiency is
>>>> desired then blocking is definitely not the right solution.
>>>>
>>>
>>> What about efficiently providing isComplete?
>>>
>>
>> In my experience isComplete is virtually useless without being able to
>> extract the value, in which case you may as well introduce a non-blocking
>> `Optional<T> poll()`
>>
>
> Do you support adding the Polling methods from
> http://www.scala-lang.org/api/2.12.0-RC1/scala/concurrent/Future.html
> to CompletionStage, i.e. isDone and getNow?
>
>
>> The result may already be available without actually blocking.  It may
>>> even be known to be available immediately.  One would like to get the value
>>> without additional allocation.
>>>
>>
>> I've seen that use-case :), and it tends to either be a situation where
>> the value is available by pure luck (or…scheduling artifacts) or when one
>> is keeping CompletionStages where strict values could be kept instead
>> (think rebinding a value on completion).
>>
>> Reading what your'e writing, may I dare propose that what you're after is
>> something along the lines of a: PollableCompletionStage which sits in
>> between CompletionStage and CompletableFuture?
>>
>
> I've been waffling!  Right now, I'm leaning towards having Java APIs
> return fully mutable CompletableFutures as Benjamin and Pavel suggested
> upthread.  Why?  Because a completion stage can be thought of as all of:
> - a consumer of an upstream value
> - a subscription to the event that makes the upstream value available, and
> - the producer of a possible value for downstream consumers.
> Cancellation could be interpreted as a request to unsubscribe from
> upstream producer or a notification to downstream consumers that no value
> will be forthcoming, or both.
>
> Here's a problem with jdk9 minimalCompletionStage: even if you are happy
> with the minimality of the source stage (perhaps it's shared) you might
> want downstream stages to be mutable, but the methods such as thenApply
> also create minimal completion stages.  If you want to convert to a mutable
> future, you can try calling toCompletableFuture, but there's no guarantee
> that will work, even if it doesn't throw UOE.  You can hand-construct a
> CompletableFuture which is then completed in a Runnable via thenRun, but
> then this is opaque to the CompletableFuture implementation, and
> implementation features such as stack overflow prevention will be
> ineffective.
>
> So right now I'm OK with e.g. Process#onExit simply returning
> CompletableFuture.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/1d1a5439/attachment.html>

From paul.sandoz at oracle.com  Mon Sep 26 21:07:46 2016
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Mon, 26 Sep 2016 18:07:46 -0700
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
Message-ID: <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>


> On 26 Sep 2016, at 16:32, Peter Levart <peter.levart at gmail.com> wrote:
> 
> Hi,
> 
> I don't know about you, but on my i7 PC, I get consistently better results using strong CAS as opposed to weak CAS for the following benchmark:
> 
> http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench.java
> 
> The results:
> 
> Benchmark                                      Mode  Cnt   Score   Error  Units
> GetAndUpdateBench.dflt                         avgt   20  53.947 ± 0.770  ns/op
> GetAndUpdateBench.dflt:getAndUpdate1_dflt      avgt   20  54.103 ± 1.083  ns/op
> GetAndUpdateBench.dflt:getAndUpdate2_dflt      avgt   20  53.791 ± 1.014  ns/op
> GetAndUpdateBench.martin                       avgt   20  52.724 ± 0.218  ns/op
> GetAndUpdateBench.martin:getAndUpdate1_martin  avgt   20  52.379 ± 0.972  ns/op
> GetAndUpdateBench.martin:getAndUpdate2_martin  avgt   20  53.070 ± 1.005  ns/op
> GetAndUpdateBench.shade                        avgt   20  53.437 ± 0.624  ns/op
> GetAndUpdateBench.shade:getAndUpdate1_shade    avgt   20  53.401 ± 0.615  ns/op
> GetAndUpdateBench.shade:getAndUpdate2_shade    avgt   20  53.474 ± 0.652  ns/op
> GetAndUpdateBench.strong                       avgt   20  38.090 ± 0.727  ns/op
> GetAndUpdateBench.strong:getAndUpdate1_strong  avgt   20  37.784 ± 1.633  ns/op
> GetAndUpdateBench.strong:getAndUpdate2_strong  avgt   20  38.397 ± 1.691  ns/op
> 

For C2 on x86, the weak CAS intrinsic wires up in the same manner as the strong CAS intrinsic. IIRC for C1 on x86, weak CAS is not intrinsic.

On platforms/compilers that do not support the weak CAS intrinsic, the Unsafe weak CAS implementation defers to the Unsafe strong CAS.

I would be inclined to switch off tiered compilation and use parallel GC (not G1) and obtain results with a number of forks to see if that makes a difference. I am speculating, i really need to play with the benchmark and look at generated code/perfasm output.

I believe a Raspberry Pi 2 is based on an ARM Cortex-A7 processor, which is 32-bit, and i am unsure of the state of affairs w.r.t. HotSpot on that platform.

Paul.

> 
> The benchmark exhibits 2 threads that don't have data contention (each of them is modifying its own variable), but most probably those threads do modify the same cache line. The update function does have a non-zero cost.
> 
> To complement above results, I also ran the same code on Raspbery PI 2 with Oracle build of JDK 9 (b137):
> 
> Benchmark                                      Mode  Cnt     Score   Error  Units
> GetAndUpdateBench.dflt                         avgt   20  2347.811 ± 2.492  ns/op
> GetAndUpdateBench.dflt:getAndUpdate1_dflt      avgt   20  2347.328 ± 6.697  ns/op
> GetAndUpdateBench.dflt:getAndUpdate2_dflt      avgt   20  2348.294 ± 7.052  ns/op
> GetAndUpdateBench.martin                       avgt   20  2342.773 ± 5.618  ns/op
> GetAndUpdateBench.martin:getAndUpdate1_martin  avgt   20  2342.398 ± 8.662  ns/op
> GetAndUpdateBench.martin:getAndUpdate2_martin  avgt   20  2343.148 ± 9.514  ns/op
> GetAndUpdateBench.shade                        avgt   20  1844.426 ± 4.304  ns/op
> GetAndUpdateBench.shade:getAndUpdate1_shade    avgt   20  1841.747 ± 5.287  ns/op
> GetAndUpdateBench.shade:getAndUpdate2_shade    avgt   20  1847.105 ± 6.416  ns/op
> GetAndUpdateBench.strong                       avgt   20  1846.413 ± 1.155  ns/op
> GetAndUpdateBench.strong:getAndUpdate1_strong  avgt   20  1846.312 ± 8.095  ns/op
> GetAndUpdateBench.strong:getAndUpdate2_strong  avgt   20  1846.513 ± 8.110  ns/op
> 
> 
> Here we may be observing a strange phenomena. Reexecuting the non-zero-cost update function on spurious failure (shade) surprisingly gives better throughput than immediately retrying weak CAS (dflt, martin).
> 
> Also, wasn't strong CAS on ARM supposed to be mapped to a weak CAS loop? Why the difference between dflt/martin and strong then?
> 
> You've got something to study now... ;-)
> 
> Regards, Peter
> 
> 
> 
> On 09/26/2016 07:51 PM, Martin Buchholz wrote:
>> 
>> 
>> On Mon, Sep 26, 2016 at 10:36 AM, Paul Sandoz <paul.sandoz at oracle.com> wrote:
>> 
>>> On 26 Sep 2016, at 10:01, Martin Buchholz <martinrb at google.com> wrote:
>>> 
>>> Hi Aleksey,
>>> 
>>> My goals were to
>>> - have the shortest code path if the CAS succeeds the first time, which is hopefully the common case
>>> - never call the update function multiple times in the case of spurious failure, only for real contention.  We don't know how expensive the update function is, and the number of times it is called is user-detectable.
>>> - yes, I have sort-of reconstructed the retry loop that strong CAS does with LL/SC, BUT we can save the value obtained from get(), which does not happen in the emulation below.  We don't have multiple value return in java!
>>> 
>>>     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>>>         for (;;) {
>>>             V prev = get();
>>>             V next = updateFunction.apply(prev);
>>>             if (strongCas(prev, next))
>>>                 return prev;
>>>         }
>>>     }
>>>     /** Implement strong CAS using weak CAS. */
>>>     boolean strongCas(V expectedValue, V newValue) {
>>>         while (get() == expectedValue) {
>>>             if (weakCompareAndSetVolatile(expectedValue, newValue))
>>>                 return true;
>>>         }
>>>         return false;
>>>     }
>>> 
>> 
>> If you are gonna do why not just call “compareAndSet" instead of emulating with your own “strongCas” method? and then, i presume, we are back to where we started.
>> 
>> 
>> That code was just demonstrating how the return value from get() was getting lost, whereas if you inline a weak cas loop as I have done, you save the extra call to get().
>> 
>> I don’t think the “updateFunction” can tell a spurious failure from another failure given the variable, under contention, can change from and back to the previous between the “strongCas” call.
>> 
>> I'm imagining user code that counts all the calls to getAndUpdate, and all the calls to updateFunction.  If they differ, I would like that to be evidence of true contention.   True, we tell users updateFunction is supposed to be side-effect-free.  OTOH, we say that attempted updates may fail due to "contention among threads", suggesting we don't call updateFunction spuriously.
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> 
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/4129072d/attachment-0001.sig>

From martinrb at google.com  Mon Sep 26 22:31:59 2016
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 26 Sep 2016 19:31:59 -0700
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
Message-ID: <CA+kOe09WCsYROLAMCUx7RXdS6UX622_k1J9gmY7NmYPv-7rhWA@mail.gmail.com>

weak cas should never be slower than strong cas.  There's still time to fix
it before jdk9 ships!

Measuring performance in the presence of heavy contention is troublesome.
Optimizing code can make such microbenchmarks slower by increasing
contention.  Will this benchmark get better if we change the implementation
to grab a global lock and starve the other thread?
https://blogs.oracle.com/dave/resource/europar13-dice-EfficientCAS-final.pdf
"""A key weakness of the CAS operation, known to both researchers and
practitioners of concurrent programming, is its performance in the presence
of memory contention. When multiple threads concurrently attempt to apply
CAS operations to the same shared variable, typically at most a single
thread will succeed in changing the shared variable’s value and the CAS
operations of all other threads will fail. Moreover, significant
degradation in performance occurs when variables manipulated by CAS become
contention “hot spots”: as failed CAS operations generate coherence traffic
on most architectures, they congest the interconnect and memory devices and
slow down successful CAS operations,"""
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/02f7f726/attachment.html>

From martinrb at google.com  Mon Sep 26 23:42:32 2016
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 26 Sep 2016 20:42:32 -0700
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CAHzJPEqMMP8njjG4J=4GwOzGYuJiCd++zsk-YCuhJ6ZP6xqRJA@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
 <030e01d21772$dbe934d0$93bb9e70$@aapt.net.au>
 <CA+kOe0_9OHd3Or0JwRhe-q80etH4AkchSKGfTOzjQWTaxjzNbw@mail.gmail.com>
 <CAHzJPEqMMP8njjG4J=4GwOzGYuJiCd++zsk-YCuhJ6ZP6xqRJA@mail.gmail.com>
Message-ID: <CA+kOe082agfX_K+cmpAAVSo_igR=XTDZzev3gyPjo44QGHD5iQ@mail.gmail.com>

I don't remember what happened in 2005, but records say that I wrote:
"""Why not ......
- add to the spec of Future.cancel() a guarantee that subsequent calls
  to Future.isDone() always return true?
"""
which led to the spec:
"""After this method returns, subsequent calls to isDone()
<http://download.java.net/java/jdk9/docs/api/java/util/concurrent/Future.html#isDone-->
will
always return true."""

which does sort-of seem to contradict
"""This attempt will fail if ... or could not be cancelled for some other
reason."""
although if you squint, you can interpret that as a requirement to throw
rather than return normally if ... could not be cancelled for some other
reason.  There's no @throws clause, but runtime exception specs are often
incomplete!

The Future javadoc is in need of some love, but perhaps there's little
progress because the troubles run deep.

On Sun, Sep 25, 2016 at 11:00 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> Cancellation: David, I can see your point. Future.cancel(true) was
> discussed 8/27/05 and the extra text was added to make it clearer that the
> state of Future after cancel is called is separate from the state of any
> associated thread or task.
>
> However, I think the added text corresponded too closely to the only
> implementation of Future.cancel that existed at the time. Elsewhere the
> spec tries to permit Future.cancel to fail for some reason other than
> because the Future had already completed:
>
>   "This attempt will fail if the task [...] could not be cancelled for
> some other reason."
>
>   "returns false if the task could not be cancelled, typically because it
> has already completed normally"
>
> Without the added text, my interpretation would be that if cancel returns
> false and then isDone returns false, then cancel failed for some other
> reason.
>
> On Sun, Sep 25, 2016 at 5:39 PM, Martin Buchholz <martinrb at google.com>
> wrote:
>
>>
>>
>> On Sun, Sep 25, 2016 at 2:22 PM, David Holmes <davidcholmes at aapt.net.au>
>> wrote:
>>
>>>
>>> Yet we somehow added the clarification with no regard as to whether
>>> cancel returned true or not. That seems wrong.
>>>
>>
>> Yikes!  I had always assumed that cancel was not permitted to leave the
>> Future incomplete, perhaps influenced by the wording,
>>
>> """After this method returns, subsequent calls to {@link #isDone} will
>> always return {@code true}."""
>>
>> It's much more in the spirit of Java to throw an exception if the future
>> cannot be completed.  It's never come up, I think.
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160926/5eee4034/attachment.html>

From viktor.klang at gmail.com  Tue Sep 27 04:39:19 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 27 Sep 2016 10:39:19 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <A8D4E76F-B850-48A6-BE65-1446B1C00398@gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
 <CANPzfU_FApFOp-J5DVRsp+N7DM13aJv-s2sRSo29wx_Vb0PJxw@mail.gmail.com>
 <A8D4E76F-B850-48A6-BE65-1446B1C00398@gmail.com>
Message-ID: <CANPzfU-T_s8C5JVaDJTK+DULTB7=yDcyYRLMc0PrcukNUvU9BQ@mail.gmail.com>

Seems legit

-- 
Cheers,
√

On Sep 26, 2016 23:29, "Attila Szegedi" <szegedia at gmail.com> wrote:

> Not at all, you could just have a call to cancel() block until the future
> completes.
>
> *ducks*
>
> Attila.
>
> > On 25 Sep 2016, at 16:34, Viktor Klang <viktor.klang at gmail.com> wrote:
> >
> > If that truely is the case then the only way of implementing a readonly
> > Future is by throwing an exception from cancel...
> >
> > --
> > Cheers,
> > √
> >
> > On Sep 25, 2016 4:20 PM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:
> >
> >> This statement regarding what happens after cancel is called is correct:
> >>
> >> "*After this method returns, subsequent calls to **isDone**() will
> always
> >> return true*. Subsequent calls to isCancelled() will always return true
> >> if this method returned true."
> >>
> >> After cancel returns, the future is completed, hence isDone. If cancel
> >> returns true, i.e. it was cancelled, then  isCancelled returns true.
> But,
> >> for example if the future is already completed when cancel is called,
> then
> >> cancel will return false and isCancelled will return false.
> >>
> >> On Sep 25, 2016 6:49 AM, "David Holmes" <davidcholmes at aapt.net.au>
> wrote:
> >>
> >>> I think that was meant to read “After this method returns _*true*_,
> >>> subsequent calls …”
> >>>
> >>>
> >>>
> >>> David
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160927/eb5412ba/attachment.html>

From akarnokd at gmail.com  Tue Sep 27 04:53:10 2016
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Tue, 27 Sep 2016 10:53:10 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU-T_s8C5JVaDJTK+DULTB7=yDcyYRLMc0PrcukNUvU9BQ@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe0-S=rVY1mRZ0cvpwk4AfHHvHJFhHiJxEbPtQVUfM5qj8w@mail.gmail.com>
 <CANPzfU93pkoGYXX3ojq0a10rvokYSJvTn1WBwa9=b4aVJcdNUg@mail.gmail.com>
 <CANPzfU_uvzU50niHuVBvTRv7N8Ey+VHq827dPk_CLpg6RZ9zJw@mail.gmail.com>
 <02e401d21722$59ec48f0$0dc4dad0$@aapt.net.au>
 <CAHzJPEoc+U10gg41b7T_6KL3MAzD6oHoZxDObxxrFmxO_mOTSg@mail.gmail.com>
 <CANPzfU_FApFOp-J5DVRsp+N7DM13aJv-s2sRSo29wx_Vb0PJxw@mail.gmail.com>
 <A8D4E76F-B850-48A6-BE65-1446B1C00398@gmail.com>
 <CANPzfU-T_s8C5JVaDJTK+DULTB7=yDcyYRLMc0PrcukNUvU9BQ@mail.gmail.com>
Message-ID: <CAAWwtm-7KVJXuOtPVMmAPNrdhMr4a-JtdmRdSWsoFPAE4qUsFg@mail.gmail.com>

If not a straight blocking method, a fluent conversion method to type R
could be useful IMO:

R to(Function<? super CompletionStage<T>, R> converter) {
    return converter.apply(this);
}

This way, who needs a blocking get can have a function prepared with a
blocking operation:

<T> Function<CompletionStage<T>, T> blockingGet() {
   cs -> {
       Object[] result = { null, null };
       CountDownLatch cdl = new CountDownLatch();
       cs.whenComplete((v, e) -> {
           if (e != null) {
              result[1] = e;
           } else {
              result[0] = v;
           }
           cdl.countDown();
       });

       try {
           cdl.await();
       } catch (InterruptedException ex) {
           throw new RuntimeException(ex);
       }

       if (result[1] != null) {
          throw new RuntimeException(result[1]);
       }
       return result[0];
   }
}

T t = cs.to(blockingGet());


while others can convert it to a more enabled dataflow system:

cs.to(Flux::fromFuture)
.filter(v -> v < 50)
.defaultIfEmpty(-10)
.subscribe(System.out::println, Throwable::printStackTrace);

2016-09-27 10:39 GMT+02:00 Viktor Klang <viktor.klang at gmail.com>:

> Seems legit
>
> --
> Cheers,
> √
>
> On Sep 26, 2016 23:29, "Attila Szegedi" <szegedia at gmail.com> wrote:
>
>> Not at all, you could just have a call to cancel() block until the future
>> completes.
>>
>> *ducks*
>>
>> Attila.
>>
>> > On 25 Sep 2016, at 16:34, Viktor Klang <viktor.klang at gmail.com> wrote:
>> >
>> > If that truely is the case then the only way of implementing a readonly
>> > Future is by throwing an exception from cancel...
>> >
>> > --
>> > Cheers,
>> > √
>> >
>> > On Sep 25, 2016 4:20 PM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:
>> >
>> >> This statement regarding what happens after cancel is called is
>> correct:
>> >>
>> >> "*After this method returns, subsequent calls to **isDone**() will
>> always
>> >> return true*. Subsequent calls to isCancelled() will always return true
>> >> if this method returned true."
>> >>
>> >> After cancel returns, the future is completed, hence isDone. If cancel
>> >> returns true, i.e. it was cancelled, then  isCancelled returns true.
>> But,
>> >> for example if the future is already completed when cancel is called,
>> then
>> >> cancel will return false and isCancelled will return false.
>> >>
>> >> On Sep 25, 2016 6:49 AM, "David Holmes" <davidcholmes at aapt.net.au>
>> wrote:
>> >>
>> >>> I think that was meant to read “After this method returns _*true*_,
>> >>> subsequent calls …”
>> >>>
>> >>>
>> >>>
>> >>> David
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160927/a3e1d8f2/attachment-0001.html>

From viktor.klang at gmail.com  Tue Sep 27 05:07:40 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 27 Sep 2016 11:07:40 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CA+kOe0_KaKKfzN0pZhCRXRfLu8ynmxfm5rFYF7bgpZBmPGEFDQ@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <CANPzfU9ieQzUzpB7iiCmSR4NZpEHQKa8OJX-AZQp44uouY1Ang@mail.gmail.com>
 <CA+kOe084Kfma7D3ZA8qc_QnSQiUKV-Nt7uPE7A5m=nPY-=AS3w@mail.gmail.com>
 <CANPzfU8hySBTwZj6-Y0O97LLm5sK+fDH8Js1sDZxgxkJubhbBg@mail.gmail.com>
 <CA+kOe0_KaKKfzN0pZhCRXRfLu8ynmxfm5rFYF7bgpZBmPGEFDQ@mail.gmail.com>
Message-ID: <CANPzfU8vtcrLU=rk1Q_cX5ZB+LO+N4zpy-WCq=ROEk1Q9_87OQ@mail.gmail.com>

On Sep 27, 2016 01:18, "Martin Buchholz" <martinrb at google.com> wrote:
>
>
>
> On Mon, Sep 26, 2016 at 7:55 AM, Viktor Klang <viktor.klang at gmail.com>
wrote:
>>>
>>>
>>>
>>> Test methods,
>>
>>
>> Yeah, I thought so as well, but it turns out that when you have tons of
async tests, not being able to start new tests until either that timeout or
result makes for a very slow test suite, so that's why most serious test
frameworks are growing support for dealing with async code. Then all you
want to be able to limit is work-in-progress (sloppily called parallelism)
>
>
> I don't see any support in junit or testng for multi-threaded tests.
 jtreg uses the strategy of having a set of reusable JVM processes, each of
which is running only one test at a time, which provides "pretty good"
isolation, and seems to work well.

It's less about multithreading per-se and more about async/non-blocking.

Here's one example of a fwk which supports it:
http://www.scalatest.org/user_guide/async_testing

>
>>>
>>> main methods
>>
>>
>> That's a common scenario, but one that can be solved by having
non-daemonic pooled worker threads.
>
>
> Do you have docs for such a thread pool?

You mean one which has a ThreadFactory which enables setDaemonic(false)?

>
>>>
>>> and unix process reapers are all examples where it's reasonable to
block forever.
>>
>>
>> What about waitpid() + WNOHANG?
>
>
> Are you suggesting periodic polling is better than blocking?

The botion of "better" requires some context to measure against:

>From a liveness, responsiveness and scalability PoV, I'd say, barring
selector support (which is arguably also polling), it's the next best
thing. Since it allows you to monitor N number of external processes with a
low constant factor.

>
>>>>
>>>> PPPPS: "I think it's unreasonable to not provide this for users
(especially when we can do so more efficiently)." <- If efficiency is
desired then blocking is definitely not the right solution.
>>>
>>>
>>> What about efficiently providing isComplete?
>>
>>
>> In my experience isComplete is virtually useless without being able to
extract the value, in which case you may as well introduce a non-blocking
`Optional<T> poll()`
>
>
> Do you support adding the Polling methods from
> http://www.scala-lang.org/api/2.12.0-RC1/scala/concurrent/Future.html
> to CompletionStage, i.e. isDone and getNow?

After thinking about it, I'd say 'No'. Since then people would ask to add
more polling options: isFailed etc.
Having been a part of creating s.c.Future I'd say adding the polling
methods (barring possibly 'value: Option[Try[T]]') has been proven to add
little in terms of value while adding a much larger API surface area.
Since Java lacks value-level try-catch (scala.util.Try) and Java lacks
disjoint union types, the signature of 'getNow' would have to throw, which
is generally a bad thing.

So I think a PollableCompletionStage would be a better option, where the
broader surface area of these kinds of ops can be contained.

Adding a CompletionStage constructor to CompletableFuture would make
conversions simple.

Reiterating Dougs sentiment: there is no API which will appease everyone,
and if everything always should be added, not only does every API become a
kitchen sink, but also all implementations thereof will be needlessly
complex, slow and hard to maintain.

>
>>>
>>> The result may already be available without actually blocking.  It may
even be known to be available immediately.  One would like to get the value
without additional allocation.
>>
>>
>> I've seen that use-case :), and it tends to either be a situation where
the value is available by pure luck (or…scheduling artifacts) or when one
is keeping CompletionStages where strict values could be kept instead
(think rebinding a value on completion).
>>
>> Reading what your'e writing, may I dare propose that what you're after
is something along the lines of a: PollableCompletionStage which sits in
between CompletionStage and CompletableFuture?
>
>
> I've been waffling!  Right now, I'm leaning towards having Java APIs
return fully mutable CompletableFutures as Benjamin and Pavel suggested
upthread.  Why?  Because a completion stage can be thought of as all of:
> - a consumer of an upstream value

Not this, CompletionStage doesn't specify how its value comes to be.

> - a subscription to the event that makes the upstream value available,
and

Never this, since CompletionStage is not cancellable.

> - the producer of a possible value for downstream consumers.

This is exactly what it is.

> Cancellation could be interpreted as a request to unsubscribe from
upstream producer or a notification to downstream consumers that no value
will be forthcoming, or both.

CompletionStage is not cancellable so this concern does not exist.

>
> Here's a problem with jdk9 minimalCompletionStage: even if you are happy
with the minimality of the source stage (perhaps it's shared) you might
want downstream stages to be mutable, but the methods such as thenApply
also create minimal completion stages.  If you want to convert to a mutable
future, you can try calling toCompletableFuture, but there's no guarantee
that will work, even if it doesn't throw UOE.  You can hand-construct a
CompletableFuture which is then completed in a Runnable via thenRun, but
then this is opaque to the CompletableFuture implementation, and
implementation features such as stack overflow prevention will be
ineffective.

This is what I mean by a missing abstraction: There is no API to define a
workflow which would be cancellable.
Fortunately the basis (an SPI) of such thing will be available: j.u.c.Flow

>
> So right now I'm OK with e.g. Process#onExit simply returning
CompletableFuture.

I would not name it onExit since that signals, to me, a callback, and in
which case it ought to take a callback as a parameter.

Also, what does cancellation of onExit mean?

Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160927/56ef14e3/attachment.html>

From peter.levart at gmail.com  Tue Sep 27 05:39:42 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 27 Sep 2016 11:39:42 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
Message-ID: <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>

Hi,

Ah, the pitfalls of benchmarking...

Even though I ran the benchmarks for several times and got consistent 
results, it seems that the consistency was accidental. As I said: "The 
benchmark exhibits 2 threads that don't have data contention (each of 
them is modifying its own variable), but most probably those threads do 
modify the same cache line."

"most probably" == "not necessarily"!!! Murphy interfering with "strong" 
benchmark it was. The Vars object was consistently allocated at an 
address so that value1 and value2 were not in the same cache line only 
in "strong" benchmark. Corrected benchmark that guarantees same 
cache-line for two int values in an aligned direct byte buffer:

http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench2.java

...shows more uniform results on x86_64:

Benchmark                                       Mode  Cnt Score   Error  
Units
GetAndUpdateBench2.dflt                         avgt   20 56.683 ± 
0.539  ns/op
GetAndUpdateBench2.dflt:getAndUpdate1_dflt      avgt   20 56.703 ± 
0.640  ns/op
GetAndUpdateBench2.dflt:getAndUpdate2_dflt      avgt   20 56.662 ± 
0.519  ns/op
GetAndUpdateBench2.martin                       avgt   20 56.343 ± 
0.496  ns/op
GetAndUpdateBench2.martin:getAndUpdate1_martin  avgt   20 56.382 ± 
0.486  ns/op
GetAndUpdateBench2.martin:getAndUpdate2_martin  avgt   20 56.305 ± 
0.520  ns/op
GetAndUpdateBench2.shade                        avgt   20 55.447 ± 
0.398  ns/op
GetAndUpdateBench2.shade:getAndUpdate1_shade    avgt   20 55.408 ± 
0.526  ns/op
GetAndUpdateBench2.shade:getAndUpdate2_shade    avgt   20 55.486 ± 
0.506  ns/op
GetAndUpdateBench2.strong                       avgt   20 55.689 ± 
0.617  ns/op
GetAndUpdateBench2.strong:getAndUpdate1_strong  avgt   20 55.612 ± 
0.601  ns/op
GetAndUpdateBench2.strong:getAndUpdate2_strong  avgt   20 55.766 ± 
0.701  ns/op


And on Raspberry PI 2 (ARMv7) too:

Benchmark                                       Mode  Cnt Score    
Error  Units
GetAndUpdateBench2.dflt                         avgt   20 1953.743 ±  
3.430  ns/op
GetAndUpdateBench2.dflt:getAndUpdate1_dflt      avgt   20 1953.332 ±  
8.565  ns/op
GetAndUpdateBench2.dflt:getAndUpdate2_dflt      avgt   20 1954.154 ±  
8.334  ns/op
GetAndUpdateBench2.martin                       avgt   20 1955.770 ±  
6.025  ns/op
GetAndUpdateBench2.martin:getAndUpdate1_martin  avgt   20 1956.913 ± 
10.290  ns/op
GetAndUpdateBench2.martin:getAndUpdate2_martin  avgt   20 1954.626 ±  
6.957  ns/op
GetAndUpdateBench2.shade                        avgt   20 1959.441 ±  
2.337  ns/op
GetAndUpdateBench2.shade:getAndUpdate1_shade    avgt   20 1961.199 ±  
6.367  ns/op
GetAndUpdateBench2.shade:getAndUpdate2_shade    avgt   20 1957.682 ±  
5.713  ns/op
GetAndUpdateBench2.strong                       avgt   20 1960.668 ±  
8.764  ns/op
GetAndUpdateBench2.strong:getAndUpdate1_strong  avgt   20 1959.773 ± 
10.804  ns/op
GetAndUpdateBench2.strong:getAndUpdate2_strong  avgt   20 1961.564 ±  
8.086  ns/op


Regards, Peter


On 09/27/2016 03:07 AM, Paul Sandoz wrote:
>> On 26 Sep 2016, at 16:32, Peter Levart <peter.levart at gmail.com> wrote:
>>
>> Hi,
>>
>> I don't know about you, but on my i7 PC, I get consistently better results using strong CAS as opposed to weak CAS for the following benchmark:
>>
>> http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench.java
>>
>> The results:
>>
>> Benchmark                                      Mode  Cnt   Score   Error  Units
>> GetAndUpdateBench.dflt                         avgt   20  53.947 ± 0.770  ns/op
>> GetAndUpdateBench.dflt:getAndUpdate1_dflt      avgt   20  54.103 ± 1.083  ns/op
>> GetAndUpdateBench.dflt:getAndUpdate2_dflt      avgt   20  53.791 ± 1.014  ns/op
>> GetAndUpdateBench.martin                       avgt   20  52.724 ± 0.218  ns/op
>> GetAndUpdateBench.martin:getAndUpdate1_martin  avgt   20  52.379 ± 0.972  ns/op
>> GetAndUpdateBench.martin:getAndUpdate2_martin  avgt   20  53.070 ± 1.005  ns/op
>> GetAndUpdateBench.shade                        avgt   20  53.437 ± 0.624  ns/op
>> GetAndUpdateBench.shade:getAndUpdate1_shade    avgt   20  53.401 ± 0.615  ns/op
>> GetAndUpdateBench.shade:getAndUpdate2_shade    avgt   20  53.474 ± 0.652  ns/op
>> GetAndUpdateBench.strong                       avgt   20  38.090 ± 0.727  ns/op
>> GetAndUpdateBench.strong:getAndUpdate1_strong  avgt   20  37.784 ± 1.633  ns/op
>> GetAndUpdateBench.strong:getAndUpdate2_strong  avgt   20  38.397 ± 1.691  ns/op
>>
> For C2 on x86, the weak CAS intrinsic wires up in the same manner as the strong CAS intrinsic. IIRC for C1 on x86, weak CAS is not intrinsic.
>
> On platforms/compilers that do not support the weak CAS intrinsic, the Unsafe weak CAS implementation defers to the Unsafe strong CAS.
>
> I would be inclined to switch off tiered compilation and use parallel GC (not G1) and obtain results with a number of forks to see if that makes a difference. I am speculating, i really need to play with the benchmark and look at generated code/perfasm output.
>
> I believe a Raspberry Pi 2 is based on an ARM Cortex-A7 processor, which is 32-bit, and i am unsure of the state of affairs w.r.t. HotSpot on that platform.
>
> Paul.
>
>> The benchmark exhibits 2 threads that don't have data contention (each of them is modifying its own variable), but most probably those threads do modify the same cache line. The update function does have a non-zero cost.
>>
>> To complement above results, I also ran the same code on Raspbery PI 2 with Oracle build of JDK 9 (b137):
>>
>> Benchmark                                      Mode  Cnt     Score   Error  Units
>> GetAndUpdateBench.dflt                         avgt   20  2347.811 ± 2.492  ns/op
>> GetAndUpdateBench.dflt:getAndUpdate1_dflt      avgt   20  2347.328 ± 6.697  ns/op
>> GetAndUpdateBench.dflt:getAndUpdate2_dflt      avgt   20  2348.294 ± 7.052  ns/op
>> GetAndUpdateBench.martin                       avgt   20  2342.773 ± 5.618  ns/op
>> GetAndUpdateBench.martin:getAndUpdate1_martin  avgt   20  2342.398 ± 8.662  ns/op
>> GetAndUpdateBench.martin:getAndUpdate2_martin  avgt   20  2343.148 ± 9.514  ns/op
>> GetAndUpdateBench.shade                        avgt   20  1844.426 ± 4.304  ns/op
>> GetAndUpdateBench.shade:getAndUpdate1_shade    avgt   20  1841.747 ± 5.287  ns/op
>> GetAndUpdateBench.shade:getAndUpdate2_shade    avgt   20  1847.105 ± 6.416  ns/op
>> GetAndUpdateBench.strong                       avgt   20  1846.413 ± 1.155  ns/op
>> GetAndUpdateBench.strong:getAndUpdate1_strong  avgt   20  1846.312 ± 8.095  ns/op
>> GetAndUpdateBench.strong:getAndUpdate2_strong  avgt   20  1846.513 ± 8.110  ns/op
>>
>>
>> Here we may be observing a strange phenomena. Reexecuting the non-zero-cost update function on spurious failure (shade) surprisingly gives better throughput than immediately retrying weak CAS (dflt, martin).
>>
>> Also, wasn't strong CAS on ARM supposed to be mapped to a weak CAS loop? Why the difference between dflt/martin and strong then?
>>
>> You've got something to study now... ;-)
>>
>> Regards, Peter
>>
>>
>>
>> On 09/26/2016 07:51 PM, Martin Buchholz wrote:
>>> On Mon, Sep 26, 2016 at 10:36 AM, Paul Sandoz <paul.sandoz at oracle.com> wrote:
>>>
>>>> On 26 Sep 2016, at 10:01, Martin Buchholz <martinrb at google.com> wrote:
>>>>
>>>> Hi Aleksey,
>>>>
>>>> My goals were to
>>>> - have the shortest code path if the CAS succeeds the first time, which is hopefully the common case
>>>> - never call the update function multiple times in the case of spurious failure, only for real contention.  We don't know how expensive the update function is, and the number of times it is called is user-detectable.
>>>> - yes, I have sort-of reconstructed the retry loop that strong CAS does with LL/SC, BUT we can save the value obtained from get(), which does not happen in the emulation below.  We don't have multiple value return in java!
>>>>
>>>>      public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>>>>          for (;;) {
>>>>              V prev = get();
>>>>              V next = updateFunction.apply(prev);
>>>>              if (strongCas(prev, next))
>>>>                  return prev;
>>>>          }
>>>>      }
>>>>      /** Implement strong CAS using weak CAS. */
>>>>      boolean strongCas(V expectedValue, V newValue) {
>>>>          while (get() == expectedValue) {
>>>>              if (weakCompareAndSetVolatile(expectedValue, newValue))
>>>>                  return true;
>>>>          }
>>>>          return false;
>>>>      }
>>>>
>>> If you are gonna do why not just call “compareAndSet" instead of emulating with your own “strongCas” method? and then, i presume, we are back to where we started.
>>>
>>>
>>> That code was just demonstrating how the return value from get() was getting lost, whereas if you inline a weak cas loop as I have done, you save the extra call to get().
>>>
>>> I don’t think the “updateFunction” can tell a spurious failure from another failure given the variable, under contention, can change from and back to the previous between the “strongCas” call.
>>>
>>> I'm imagining user code that counts all the calls to getAndUpdate, and all the calls to updateFunction.  If they differ, I would like that to be evidence of true contention.   True, we tell users updateFunction is supposed to be side-effect-free.  OTOH, we say that attempted updates may fail due to "contention among threads", suggesting we don't call updateFunction spuriously.
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>>
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160927/db826d1a/attachment-0001.html>

From peter.levart at gmail.com  Tue Sep 27 06:44:35 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 27 Sep 2016 12:44:35 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
Message-ID: <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>

What I was hoping to measure with the benchmark is some difference 
between using weak CAS and strong CAS in a getAndUpdate retry loop that 
always invokes the update function (even on spurious failures of weak 
CAS). I can understand that this is not possible on x86_64 as both are 
mapped to the same strong CAS intrinsic. But I also haven't observed any 
meaningful difference on arm32. So is it possible that Oracle's JDK 9 
b137 for arm32 also maps both weak and strong CAS to the same 
implementation which is a strong CAS simulation? Is there any platform 
where weak CAS is actually a weak CAS currently?


Regards, Peter


On 09/27/2016 11:39 AM, Peter Levart wrote:
> Hi,
>
> Ah, the pitfalls of benchmarking...
>
> Even though I ran the benchmarks for several times and got consistent 
> results, it seems that the consistency was accidental. As I said: "The 
> benchmark exhibits 2 threads that don't have data contention (each of 
> them is modifying its own variable), but most probably those threads 
> do modify the same cache line."
>
> "most probably" == "not necessarily"!!! Murphy interfering with 
> "strong" benchmark it was. The Vars object was consistently allocated 
> at an address so that value1 and value2 were not in the same cache 
> line only in "strong" benchmark. Corrected benchmark that guarantees 
> same cache-line for two int values in an aligned direct byte buffer:
>
> http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench2.java
>
> ...shows more uniform results on x86_64:
>
> Benchmark                                       Mode  Cnt Score   
> Error  Units
> GetAndUpdateBench2.dflt                         avgt 20  56.683 ± 
> 0.539  ns/op
> GetAndUpdateBench2.dflt:getAndUpdate1_dflt      avgt 20  56.703 ± 
> 0.640  ns/op
> GetAndUpdateBench2.dflt:getAndUpdate2_dflt      avgt 20  56.662 ± 
> 0.519  ns/op
> GetAndUpdateBench2.martin                       avgt 20  56.343 ± 
> 0.496  ns/op
> GetAndUpdateBench2.martin:getAndUpdate1_martin  avgt 20  56.382 ± 
> 0.486  ns/op
> GetAndUpdateBench2.martin:getAndUpdate2_martin  avgt 20  56.305 ± 
> 0.520  ns/op
> GetAndUpdateBench2.shade                        avgt 20  55.447 ± 
> 0.398  ns/op
> GetAndUpdateBench2.shade:getAndUpdate1_shade    avgt 20  55.408 ± 
> 0.526  ns/op
> GetAndUpdateBench2.shade:getAndUpdate2_shade    avgt 20  55.486 ± 
> 0.506  ns/op
> GetAndUpdateBench2.strong                       avgt 20  55.689 ± 
> 0.617  ns/op
> GetAndUpdateBench2.strong:getAndUpdate1_strong  avgt 20  55.612 ± 
> 0.601  ns/op
> GetAndUpdateBench2.strong:getAndUpdate2_strong  avgt 20  55.766 ± 
> 0.701  ns/op
>
>
> And on Raspberry PI 2 (ARMv7) too:
>
> Benchmark                                       Mode  Cnt Score    
> Error  Units
> GetAndUpdateBench2.dflt                         avgt 20  1953.743 ±  
> 3.430  ns/op
> GetAndUpdateBench2.dflt:getAndUpdate1_dflt      avgt 20  1953.332 ±  
> 8.565  ns/op
> GetAndUpdateBench2.dflt:getAndUpdate2_dflt      avgt 20  1954.154 ±  
> 8.334  ns/op
> GetAndUpdateBench2.martin                       avgt 20  1955.770 ±  
> 6.025  ns/op
> GetAndUpdateBench2.martin:getAndUpdate1_martin  avgt 20  1956.913 ± 
> 10.290  ns/op
> GetAndUpdateBench2.martin:getAndUpdate2_martin  avgt 20  1954.626 ±  
> 6.957  ns/op
> GetAndUpdateBench2.shade                        avgt 20  1959.441 ±  
> 2.337  ns/op
> GetAndUpdateBench2.shade:getAndUpdate1_shade    avgt 20  1961.199 ±  
> 6.367  ns/op
> GetAndUpdateBench2.shade:getAndUpdate2_shade    avgt 20  1957.682 ±  
> 5.713  ns/op
> GetAndUpdateBench2.strong                       avgt 20  1960.668 ±  
> 8.764  ns/op
> GetAndUpdateBench2.strong:getAndUpdate1_strong  avgt 20  1959.773 ± 
> 10.804  ns/op
> GetAndUpdateBench2.strong:getAndUpdate2_strong  avgt 20  1961.564 ±  
> 8.086  ns/op
>
>
> Regards, Peter
>
>
> On 09/27/2016 03:07 AM, Paul Sandoz wrote:
>>> On 26 Sep 2016, at 16:32, Peter Levart<peter.levart at gmail.com>  wrote:
>>>
>>> Hi,
>>>
>>> I don't know about you, but on my i7 PC, I get consistently better results using strong CAS as opposed to weak CAS for the following benchmark:
>>>
>>> http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench.java
>>>
>>> The results:
>>>
>>> Benchmark                                      Mode  Cnt   Score   Error  Units
>>> GetAndUpdateBench.dflt                         avgt   20  53.947 ± 0.770  ns/op
>>> GetAndUpdateBench.dflt:getAndUpdate1_dflt      avgt   20  54.103 ± 1.083  ns/op
>>> GetAndUpdateBench.dflt:getAndUpdate2_dflt      avgt   20  53.791 ± 1.014  ns/op
>>> GetAndUpdateBench.martin                       avgt   20  52.724 ± 0.218  ns/op
>>> GetAndUpdateBench.martin:getAndUpdate1_martin  avgt   20  52.379 ± 0.972  ns/op
>>> GetAndUpdateBench.martin:getAndUpdate2_martin  avgt   20  53.070 ± 1.005  ns/op
>>> GetAndUpdateBench.shade                        avgt   20  53.437 ± 0.624  ns/op
>>> GetAndUpdateBench.shade:getAndUpdate1_shade    avgt   20  53.401 ± 0.615  ns/op
>>> GetAndUpdateBench.shade:getAndUpdate2_shade    avgt   20  53.474 ± 0.652  ns/op
>>> GetAndUpdateBench.strong                       avgt   20  38.090 ± 0.727  ns/op
>>> GetAndUpdateBench.strong:getAndUpdate1_strong  avgt   20  37.784 ± 1.633  ns/op
>>> GetAndUpdateBench.strong:getAndUpdate2_strong  avgt   20  38.397 ± 1.691  ns/op
>>>
>> For C2 on x86, the weak CAS intrinsic wires up in the same manner as the strong CAS intrinsic. IIRC for C1 on x86, weak CAS is not intrinsic.
>>
>> On platforms/compilers that do not support the weak CAS intrinsic, the Unsafe weak CAS implementation defers to the Unsafe strong CAS.
>>
>> I would be inclined to switch off tiered compilation and use parallel GC (not G1) and obtain results with a number of forks to see if that makes a difference. I am speculating, i really need to play with the benchmark and look at generated code/perfasm output.
>>
>> I believe a Raspberry Pi 2 is based on an ARM Cortex-A7 processor, which is 32-bit, and i am unsure of the state of affairs w.r.t. HotSpot on that platform.
>>
>> Paul.
>>
>>> The benchmark exhibits 2 threads that don't have data contention (each of them is modifying its own variable), but most probably those threads do modify the same cache line. The update function does have a non-zero cost.
>>>
>>> To complement above results, I also ran the same code on Raspbery PI 2 with Oracle build of JDK 9 (b137):
>>>
>>> Benchmark                                      Mode  Cnt     Score   Error  Units
>>> GetAndUpdateBench.dflt                         avgt   20  2347.811 ± 2.492  ns/op
>>> GetAndUpdateBench.dflt:getAndUpdate1_dflt      avgt   20  2347.328 ± 6.697  ns/op
>>> GetAndUpdateBench.dflt:getAndUpdate2_dflt      avgt   20  2348.294 ± 7.052  ns/op
>>> GetAndUpdateBench.martin                       avgt   20  2342.773 ± 5.618  ns/op
>>> GetAndUpdateBench.martin:getAndUpdate1_martin  avgt   20  2342.398 ± 8.662  ns/op
>>> GetAndUpdateBench.martin:getAndUpdate2_martin  avgt   20  2343.148 ± 9.514  ns/op
>>> GetAndUpdateBench.shade                        avgt   20  1844.426 ± 4.304  ns/op
>>> GetAndUpdateBench.shade:getAndUpdate1_shade    avgt   20  1841.747 ± 5.287  ns/op
>>> GetAndUpdateBench.shade:getAndUpdate2_shade    avgt   20  1847.105 ± 6.416  ns/op
>>> GetAndUpdateBench.strong                       avgt   20  1846.413 ± 1.155  ns/op
>>> GetAndUpdateBench.strong:getAndUpdate1_strong  avgt   20  1846.312 ± 8.095  ns/op
>>> GetAndUpdateBench.strong:getAndUpdate2_strong  avgt   20  1846.513 ± 8.110  ns/op
>>>
>>>
>>> Here we may be observing a strange phenomena. Reexecuting the non-zero-cost update function on spurious failure (shade) surprisingly gives better throughput than immediately retrying weak CAS (dflt, martin).
>>>
>>> Also, wasn't strong CAS on ARM supposed to be mapped to a weak CAS loop? Why the difference between dflt/martin and strong then?
>>>
>>> You've got something to study now... ;-)
>>>
>>> Regards, Peter
>>>
>>>
>>>
>>> On 09/26/2016 07:51 PM, Martin Buchholz wrote:
>>>> On Mon, Sep 26, 2016 at 10:36 AM, Paul Sandoz<paul.sandoz at oracle.com>  wrote:
>>>>
>>>>> On 26 Sep 2016, at 10:01, Martin Buchholz<martinrb at google.com>  wrote:
>>>>>
>>>>> Hi Aleksey,
>>>>>
>>>>> My goals were to
>>>>> - have the shortest code path if the CAS succeeds the first time, which is hopefully the common case
>>>>> - never call the update function multiple times in the case of spurious failure, only for real contention.  We don't know how expensive the update function is, and the number of times it is called is user-detectable.
>>>>> - yes, I have sort-of reconstructed the retry loop that strong CAS does with LL/SC, BUT we can save the value obtained from get(), which does not happen in the emulation below.  We don't have multiple value return in java!
>>>>>
>>>>>      public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>>>>>          for (;;) {
>>>>>              V prev = get();
>>>>>              V next = updateFunction.apply(prev);
>>>>>              if (strongCas(prev, next))
>>>>>                  return prev;
>>>>>          }
>>>>>      }
>>>>>      /** Implement strong CAS using weak CAS. */
>>>>>      boolean strongCas(V expectedValue, V newValue) {
>>>>>          while (get() == expectedValue) {
>>>>>              if (weakCompareAndSetVolatile(expectedValue, newValue))
>>>>>                  return true;
>>>>>          }
>>>>>          return false;
>>>>>      }
>>>>>
>>>> If you are gonna do why not just call “compareAndSet" instead of emulating with your own “strongCas” method? and then, i presume, we are back to where we started.
>>>>
>>>>
>>>> That code was just demonstrating how the return value from get() was getting lost, whereas if you inline a weak cas loop as I have done, you save the extra call to get().
>>>>
>>>> I don’t think the “updateFunction” can tell a spurious failure from another failure given the variable, under contention, can change from and back to the previous between the “strongCas” call.
>>>>
>>>> I'm imagining user code that counts all the calls to getAndUpdate, and all the calls to updateFunction.  If they differ, I would like that to be evidence of true contention.   True, we tell users updateFunction is supposed to be side-effect-free.  OTOH, we say that attempted updates may fail due to "contention among threads", suggesting we don't call updateFunction spuriously.
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>>
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160927/0c3e8862/attachment.html>

From aph at redhat.com  Tue Sep 27 08:56:43 2016
From: aph at redhat.com (Andrew Haley)
Date: Tue, 27 Sep 2016 13:56:43 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
Message-ID: <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>

On 27/09/16 11:44, Peter Levart wrote:

> What I was hoping to measure with the benchmark is some difference
> between using weak CAS and strong CAS in a getAndUpdate retry loop
> that always invokes the update function (even on spurious failures
> of weak CAS).

That's going to be very difficult.  In practice truly spurious
failures are rare enough that you won't see them unless you have false
sharing.  (In which case, of course, strong CAS slows down too.)  You
benefit from using weak CAS at times of high contention by not
spinning to retry a SC which is going to fail anyway.

> I can understand that this is not possible on x86_64 as both are
> mapped to the same strong CAS intrinsic. But I also haven't observed
> any meaningful difference on arm32. So is it possible that Oracle's
> JDK 9 b137 for arm32 also maps both weak and strong CAS to the same
> implementation which is a strong CAS simulation?

Certainly.  The only way to be enlightened is to look at the generated
source.

> Is there any platform where weak CAS is actually a weak CAS
> currently?

AArch64.

Andrew.

From peter.levart at gmail.com  Tue Sep 27 10:39:45 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 27 Sep 2016 16:39:45 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
Message-ID: <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>

Hi Andrew,


On 09/27/2016 02:56 PM, Andrew Haley wrote:
> On 27/09/16 11:44, Peter Levart wrote:
>
>> What I was hoping to measure with the benchmark is some difference
>> between using weak CAS and strong CAS in a getAndUpdate retry loop
>> that always invokes the update function (even on spurious failures
>> of weak CAS).
> That's going to be very difficult.  In practice truly spurious
> failures are rare enough that you won't see them unless you have false
> sharing.

And that's exactly what my benchmark tries to provoke. It uses CAS to 
concurrently update two distinct locations from two distinct threads (no 
data contention) with a common cache-line. I was trying to measure the 
worst case impact of re-invoking the update function on spurious failure 
of weak CAS vs. using strong CAS that would never fail in my benchmark 
(as it is strong), as a function of the update function execution time. 
But it appears there is no difference on Oracle's arm32 no matter how 
long the update function executes.

> (In which case, of course, strong CAS slows down too.)  You
> benefit from using weak CAS at times of high contention by not
> spinning to retry a SC which is going to fail anyway.
>
>> I can understand that this is not possible on x86_64 as both are
>> mapped to the same strong CAS intrinsic. But I also haven't observed
>> any meaningful difference on arm32. So is it possible that Oracle's
>> JDK 9 b137 for arm32 also maps both weak and strong CAS to the same
>> implementation which is a strong CAS simulation?
> Certainly.  The only way to be enlightened is to look at the generated
> source.
>
>> Is there any platform where weak CAS is actually a weak CAS
>> currently?
> AArch64.

Ah. I don't have any device for that (yet) ;-)

Thanks, Peter

> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160927/881f023f/attachment.html>

From aph at redhat.com  Tue Sep 27 11:21:32 2016
From: aph at redhat.com (Andrew Haley)
Date: Tue, 27 Sep 2016 16:21:32 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
Message-ID: <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>

On 27/09/16 15:39, Peter Levart wrote:
> Ah. I don't have any device for that (yet) ;-)

Well, I can do it if you tell me exactly what to do.


From martinrb at google.com  Tue Sep 27 14:18:59 2016
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 27 Sep 2016 11:18:59 -0700
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
Message-ID: <CA+kOe0-_7tAuw5=u8axhW55O+dmEnw9wQb6rN5fZTasN73jmwQ@mail.gmail.com>

My proposal produces strictly cleaner, more compact source code and
bytecode, while maintaining the property that updateFunction will never be
called just because of a spurious weak cas failure, without loss of
performance, so ... COMMIT IS YES?

(ambitious Dice-style cas optimizations are a future project)

(I would even be OK with *specifying* that updateFunction will never be
called spuriously, but that's also a future project)

(Paul/Doug: is there a VarHandle equivalent to Atomic*.getAndUpdate?
Another future project?)

On Sat, Sep 24, 2016 at 9:51 AM, Martin Buchholz <martinrb at google.com>
wrote:

> Discussion on CAS loops got me looking again at our own:
>
>     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>         V prev = get(), next = null;
>         for (boolean haveNext = false;;) {
>             if (!haveNext)
>                 next = updateFunction.apply(prev);
>             if (weakCompareAndSetVolatile(prev, next))
>                 return prev;
>             haveNext = (prev == (prev = get()));
>         }
>     }
>
> The haveNext boolean and useless initialization of next bothers me.  We
> can do better!
>
>     public final V getAndUpdate(UnaryOperator<V> updateFunction) {
>         for (V prev = get();;) {
>             V next = updateFunction.apply(prev);
>             do {
>                 if (weakCompareAndSetVolatile(prev, next))
>                     return prev;
>             } while (prev == (prev = get()));
>         }
>     }
>
> even though it probably saves more bytecodes than cycles.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160927/fad661ad/attachment.html>

From aph at redhat.com  Tue Sep 27 15:25:15 2016
From: aph at redhat.com (Andrew Haley)
Date: Tue, 27 Sep 2016 20:25:15 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0-_7tAuw5=u8axhW55O+dmEnw9wQb6rN5fZTasN73jmwQ@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <CA+kOe0-_7tAuw5=u8axhW55O+dmEnw9wQb6rN5fZTasN73jmwQ@mail.gmail.com>
Message-ID: <b1facd36-e0ab-fb1f-c5e8-72f5e48f4396@redhat.com>

On 27/09/16 19:18, Martin Buchholz wrote:

> My proposal produces strictly cleaner, more compact source code and
> bytecode, while maintaining the property that updateFunction will
> never be called just because of a spurious weak cas failure, without
> loss of performance, so ... COMMIT IS YES?

I don't think so, for reasons discussed.

Andrew.

From yaojingguo at gmail.com  Wed Sep 28 03:23:39 2016
From: yaojingguo at gmail.com (jingguo yao)
Date: Wed, 28 Sep 2016 15:23:39 +0800
Subject: [concurrency-interest] What can an incorrectly synchronized read
	see?
Message-ID: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>

Java code:

class Counter {
  private int val = 0;
  public synchronized int synchronizedGet() {
    return val;
  }
  public int unsynchronizedGet() {
    return val;
  }
  public synchronized void increment() {
    val++;
  }
}
class Writer extends Thread {
  private Counter c;
  public Writer(Counter c) {
    this.c = c;
  }
  @Override
  public void run() {
    c.increment();
  }
}
class Reader extends Thread {
  private Counter c;
  public Reader(Counter c) {
    this.c = c;
  }
  @Override
  public void run() {
    int unsynchronizedCount = c.unsynchronizedGet();
    int synchronizedCount = c.synchronizedGet();
    if (unsynchronizedCount > synchronizedCount)
      throw new RuntimeException("unsynchronized count is larger than
synchronized count");
  }
}
public class UnsynchronizedRead {
  public static void main(String[] args) {
    Counter c = new Counter();
    Writer w = new Writer(c);
    Reader r = new Reader(c);
    w.start();
    r.start();
  }
}


I think that unsynchronizedCount <= synchronizedCount is always true
for the above code provided that JVM implementations of read and write
operations for int type are atomic. Here is my reasoning.

When increment happens-before synchronizedGet, synchronizedGet sees 1.
And there is no happens-before order to prevent unsynchronizedGet from
seeing initial_write or increment. So unsynchronizedGet sees1 0 or 1.

Happens-before digram for this case:

  initial_write ---> increment ---> synchronizedGet
                                      ^
              unsynchronizedGet------/

When synchronizedGet happens-before increment, synchronizedGet sees 0. Since
unsynchronizedGet happens-before increment, it can't see 1 written
by increment. It can only see 0 written by initial_write.

Happens-before digram for this case:

  initial_write ---> synchronizedGet ---> increment
                        ^
  unsynchronizedGet----/

Is my reasoning correct?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/4ed5e090/attachment.html>

From shade at redhat.com  Wed Sep 28 04:22:56 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Wed, 28 Sep 2016 10:22:56 +0200
Subject: [concurrency-interest] What can an incorrectly synchronized
 read see?
In-Reply-To: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
Message-ID: <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>

Hi,

On 09/28/2016 09:23 AM, jingguo yao wrote:
> I think that unsynchronizedCount <= synchronizedCount is always true
> for the above code provided that JVM implementations of read and write
> operations for int type are atomic. 

They are atomic, mandated by JLS.


> When increment happens-before synchronizedGet, synchronizedGet sees 1.
> And there is no happens-before order to prevent unsynchronizedGet from
> seeing initial_write or increment. So unsynchronizedGet sees1 0 or 1.
>
> Happens-before digram for this case:
> 
>   initial_write ---> increment ---> synchronizedGet
>                                       ^
>               unsynchronizedGet------/
> 
> When synchronizedGet happens-before increment, synchronizedGet sees 0. Since
> unsynchronizedGet happens-before increment, it can't see 1 written
> by increment. It can only see 0 written by initial_write.
> 
> Happens-before digram for this case:
> 
>   initial_write ---> synchronizedGet ---> increment
>                         ^
>   unsynchronizedGet----/   
> 
> Is my reasoning correct?

Not quite, because I think it confuses the notion of program statements
and actions in JMM. The analysis should use JMM rules as stated, like below.

Happens-before consistency says that in valid executions, the read has
two relations with writes: a) with the writes that are tied in
happens-before with the read in question, the read can see only the
*latest* write in HB -- not the one before it, not the one after the
read in HB; b) with the writes that are not tied in happens-before with
the read in question, the read can see whatever such write.

So, these both are valid executions under JMM:

                               r(val):0 --po-\
                                             |
 w(val, 1) --po/hb--> unlck(m) --sw/hb--> lck(m) --po/hb--> r(val):1


                               r(val):1 --po-\
                                             |
 w(val, 1) --po/hb--> unlck(m) --sw/hb--> lck(m) --po/hb--> r(val):1

 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                               ^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^
                                 unsynch                 synch
     parts done by Writer              parts done by Reader


Where:
  w(F,V) is the write of value "V" to field "F"
  r(F):V is the read of value "V" from field "F"
  lck/unlck(M) are the lock and unlock of the monitor M
  --po/hb--> is the happens-before edge induced by program order
  --sw/hb--> is the happens-before edge induced by synchronizes-with
  --po--> is "just" the program order

Note that unsynchronized read can read either "0" or "1", regardless of
if synchronized sees "1" after it.

Can the unsynchronized see "1", but the synchronized see "0"? Now, this
is where it gets interesting. We cannot use the execution from above to
justify reading synchronized "0", because synchronizes-with mandates
seeing "1". Therefore, we need to reverse the order of Writer and Reader:


                  parts done by Reader
    unsync                                  synch
 vvvvvvvvvvvvvv       vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
 vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

 r(val):V1 --po/hb--> lck(m) --po/hb--> r(val):V2 --po/hb--> unlck(m)
                                                              /
      /-----------------------sw/hb--------------------------/
      v
    lck(m) --po/hb--> w(val, 1)

 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    parts done by Writer

Now, let's see what (V1, V2) combinations do not violate JMM:
 (0, 0) -- ok, both see some (default?) "0" write from before
 (1, 0) -- NOT OK, r(val):V1 cannot see the w(val,1) that's after in HB!
 (0, 1) and
 (1, 1) -- NOT OK for this particular execution, because r(val):V2 sees
the "future" write w(val,1). Notice, however, that both these results
are justified by the execution at the very beginning, when Writer and
Reader are sequenced in another order.

Hope this helps.

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/6bc7714b/attachment.sig>

From davidcholmes at aapt.net.au  Wed Sep 28 04:52:33 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 28 Sep 2016 18:52:33 +1000
Subject: [concurrency-interest] What can an incorrectly synchronized
	read see?
In-Reply-To: <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
Message-ID: <008d01d21965$a80050f0$f800f2d0$@aapt.net.au>

Is there a typo there Aleksey, the only OK outcome you listed was (0,0). ???

David

> -----Original Message-----
> From: Concurrency-interest [mailto:concurrency-interest-
> bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
> Sent: Wednesday, September 28, 2016 6:23 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] What can an incorrectly synchronized
> read see?
> 
> Hi,
> 
> On 09/28/2016 09:23 AM, jingguo yao wrote:
> > I think that unsynchronizedCount <= synchronizedCount is always true
> > for the above code provided that JVM implementations of read and write
> > operations for int type are atomic.
> 
> They are atomic, mandated by JLS.
> 
> 
> > When increment happens-before synchronizedGet, synchronizedGet sees
> 1.
> > And there is no happens-before order to prevent unsynchronizedGet from
> > seeing initial_write or increment. So unsynchronizedGet sees1 0 or 1.
> >
> > Happens-before digram for this case:
> >
> >   initial_write ---> increment ---> synchronizedGet
> >                                       ^
> >               unsynchronizedGet------/
> >
> > When synchronizedGet happens-before increment, synchronizedGet sees
> 0. Since
> > unsynchronizedGet happens-before increment, it can't see 1 written
> > by increment. It can only see 0 written by initial_write.
> >
> > Happens-before digram for this case:
> >
> >   initial_write ---> synchronizedGet ---> increment
> >                         ^
> >   unsynchronizedGet----/
> >
> > Is my reasoning correct?
> 
> Not quite, because I think it confuses the notion of program statements
> and actions in JMM. The analysis should use JMM rules as stated, like below.
> 
> Happens-before consistency says that in valid executions, the read has
> two relations with writes: a) with the writes that are tied in
> happens-before with the read in question, the read can see only the
> *latest* write in HB -- not the one before it, not the one after the
> read in HB; b) with the writes that are not tied in happens-before with
> the read in question, the read can see whatever such write.
> 
> So, these both are valid executions under JMM:
> 
>                                r(val):0 --po-\
>                                              |
>  w(val, 1) --po/hb--> unlck(m) --sw/hb--> lck(m) --po/hb--> r(val):1
> 
> 
>                                r(val):1 --po-\
>                                              |
>  w(val, 1) --po/hb--> unlck(m) --sw/hb--> lck(m) --po/hb--> r(val):1
> 
>  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>                                ^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^
>                                  unsynch                 synch
>      parts done by Writer              parts done by Reader
> 
> 
> Where:
>   w(F,V) is the write of value "V" to field "F"
>   r(F):V is the read of value "V" from field "F"
>   lck/unlck(M) are the lock and unlock of the monitor M
>   --po/hb--> is the happens-before edge induced by program order
>   --sw/hb--> is the happens-before edge induced by synchronizes-with
>   --po--> is "just" the program order
> 
> Note that unsynchronized read can read either "0" or "1", regardless of
> if synchronized sees "1" after it.
> 
> Can the unsynchronized see "1", but the synchronized see "0"? Now, this
> is where it gets interesting. We cannot use the execution from above to
> justify reading synchronized "0", because synchronizes-with mandates
> seeing "1". Therefore, we need to reverse the order of Writer and Reader:
> 
> 
>                   parts done by Reader
>     unsync                                  synch
>  vvvvvvvvvvvvvv       vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
> 
> vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
> v
> 
>  r(val):V1 --po/hb--> lck(m) --po/hb--> r(val):V2 --po/hb--> unlck(m)
>                                                               /
>       /-----------------------sw/hb--------------------------/
>       v
>     lck(m) --po/hb--> w(val, 1)
> 
>  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>     parts done by Writer
> 
> Now, let's see what (V1, V2) combinations do not violate JMM:
>  (0, 0) -- ok, both see some (default?) "0" write from before
>  (1, 0) -- NOT OK, r(val):V1 cannot see the w(val,1) that's after in HB!
>  (0, 1) and
>  (1, 1) -- NOT OK for this particular execution, because r(val):V2 sees
> the "future" write w(val,1). Notice, however, that both these results
> are justified by the execution at the very beginning, when Writer and
> Reader are sequenced in another order.
> 
> Hope this helps.
> 
> Thanks,
> -Aleksey



From shade at redhat.com  Wed Sep 28 05:01:04 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Wed, 28 Sep 2016 11:01:04 +0200
Subject: [concurrency-interest] What can an incorrectly synchronized
 read see?
In-Reply-To: <008d01d21965$a80050f0$f800f2d0$@aapt.net.au>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
 <008d01d21965$a80050f0$f800f2d0$@aapt.net.au>
Message-ID: <16fab73e-467c-d1ca-1743-adff0db7a9d5@redhat.com>

On 09/28/2016 10:52 AM, David Holmes wrote:
> Is there a typo there Aleksey, the only OK outcome you listed was (0,0). ???

Note that in my note, there are two classes of executions, depending on
the sequence of locks/unlocks in (R)eader and (W)riter.

(unsync, sync):
 (0, 0): justified by executions with R->W sequencing
 (0, 1): justified by executions with W->R sequencing
 (1, 0): is not justified by any executions
 (1, 1); justified by executions with W->R sequencing

>> -----Original Message-----
>> From: Concurrency-interest [mailto:concurrency-interest-
>> bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
>> Note that unsynchronized read can read either "0" or "1", regardless of
>> if synchronized sees "1" after it.

This is for W->R sequencing.

>> Now, let's see what (V1, V2) combinations do not violate JMM:
>>  (0, 0) -- ok, both see some (default?) "0" write from before
>>  (1, 0) -- NOT OK, r(val):V1 cannot see the w(val,1) that's after in HB!
>>  (0, 1) and
>>  (1, 1) -- NOT OK for this particular execution, because r(val):V2 sees
>> the "future" write w(val,1). Notice, however, that both these results
>> are justified by the execution at the very beginning, when Writer and
>> Reader are sequenced in another order.

This is for R->W sequencing.

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/60e16ead/attachment.sig>

From peter.levart at gmail.com  Wed Sep 28 06:33:08 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 28 Sep 2016 12:33:08 +0200
Subject: [concurrency-interest] Does JDK 9 String.hashCode() have a bug?
Message-ID: <bd741d53-4257-89d7-23f4-215c8d2d2fc2@gmail.com>

Hi,

I would like to discuss the implementation of String.hashCode() method 
in the light of JMM. Here it is (the recent one from JDK 9 which 
includes implementation of compact strings):


     public int hashCode() {
         if (hash == 0 && value.length > 0) {
             hash = isLatin1() ? StringLatin1.hashCode(value)
                               : StringUTF16.hashCode(value);
         }
         return hash;
     }


Here, 'hash' is a plain instance field in String. Suppose we have a 
freshly constructed non-zero length String object, whose reference 's' 
is accessible to two threads and each of them concurrently invokes 
hashCode() on it:


Thread 1:

int h1 = s.hashCode();


Thread 2:

int h2 = s.hashCode();


Suppose also that the isLatin1() ? StringLatin1.hashCode(value) : 
StringUTF16.hashCode(value) computation returns a non-zero value.


Is it possible that the outcome (h1, h2) is either (0, hc) or (hc, 0) 
where the hc is the non-zero result of above mentioned computation?

According to JMM, I think it is. Since there is no synchronization 
between a write w(hash, hc) by one thread and two reads r1(value):v1 and 
r2(value):v2 by the other thread that correspond to program lines:

     if (hash == 0 ....

and:

     return hash;


...the reads may or may not see the write independently. So there is a 
possibility that v1 == hc and v2 == 0.


Am I right and should this be fixed?


Note that JDK 8 code is different. It does not have this problem:


public int hashCode() {
         int h = hash;
         if (h == 0) {
             for (char v : value) {
                 h = 31 * h + v;
             }
             if (h != 0) {
                 hash = h;
             }
         }
         return h;
     }


Regards, Peter


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/38190e8b/attachment.html>

From shade at redhat.com  Wed Sep 28 06:38:29 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Wed, 28 Sep 2016 12:38:29 +0200
Subject: [concurrency-interest] Does JDK 9 String.hashCode() have a bug?
In-Reply-To: <bd741d53-4257-89d7-23f4-215c8d2d2fc2@gmail.com>
References: <bd741d53-4257-89d7-23f4-215c8d2d2fc2@gmail.com>
Message-ID: <9abf9037-9c80-af46-5458-62ad4bd14565@redhat.com>

Hi Peter,

On 09/28/2016 12:33 PM, Peter Levart wrote:
>     public int hashCode() {
>         if (hash == 0 && value.length > 0) {
>             hash = isLatin1() ? StringLatin1.hashCode(value)
>                               : StringUTF16.hashCode(value);
>         }
>         return hash;
>     }

Yes, this is a non-benign data race: should not do the second racy read
at "return hash". Embarrassing that we've missed this during code
reviews. Submit a bug/patch?

Thanks,
-Aleksey

P.S. Your explanation is similar to a more detailed of mine here:

https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-benign-is-resilient

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/48e704c6/attachment.sig>

From Maciej.Bobrowski at morganstanley.com  Wed Sep 28 06:47:37 2016
From: Maciej.Bobrowski at morganstanley.com (Bobrowski, Maciej)
Date: Wed, 28 Sep 2016 10:47:37 +0000
Subject: [concurrency-interest] What can an incorrectly synchronized
 read see?
In-Reply-To: <16fab73e-467c-d1ca-1743-adff0db7a9d5@redhat.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
 <008d01d21965$a80050f0$f800f2d0$@aapt.net.au>
 <16fab73e-467c-d1ca-1743-adff0db7a9d5@redhat.com>
Message-ID: <9838003A10254741BC6FFADE2ED02B457D5E7346@OZWEX0205N1.msad.ms.com>

﻿Right, so the only invalid execution in any possible sequencing is the (1, 0) which was the main question of this post, correct?

-----Original Message-----
From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
Sent: 28 September 2016 10:01
To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] What can an incorrectly synchronized read see?

On 09/28/2016 10:52 AM, David Holmes wrote:
> Is there a typo there Aleksey, the only OK outcome you listed was (0,0). ???

Note that in my note, there are two classes of executions, depending on the sequence of locks/unlocks in (R)eader and (W)riter.

(unsync, sync):
 (0, 0): justified by executions with R->W sequencing  (0, 1): justified by executions with W->R sequencing  (1, 0): is not justified by any executions  (1, 1); justified by executions with W->R sequencing

>> -----Original Message-----
>> From: Concurrency-interest [mailto:concurrency-interest- 
>> bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev Note that 
>> unsynchronized read can read either "0" or "1", regardless of if 
>> synchronized sees "1" after it.

This is for W->R sequencing.

>> Now, let's see what (V1, V2) combinations do not violate JMM:
>>  (0, 0) -- ok, both see some (default?) "0" write from before  (1, 0) 
>> -- NOT OK, r(val):V1 cannot see the w(val,1) that's after in HB!
>>  (0, 1) and
>>  (1, 1) -- NOT OK for this particular execution, because r(val):V2 
>> sees the "future" write w(val,1). Notice, however, that both these 
>> results are justified by the execution at the very beginning, when 
>> Writer and Reader are sequenced in another order.

This is for R->W sequencing.

Thanks,
-Aleksey



--------------------------------------------------------------------------------

NOTICE: Morgan Stanley is not acting as a municipal advisor and the opinions or views contained herein are not intended to be, and do not constitute, advice within the meaning of Section 975 of the Dodd-Frank Wall Street Reform and Consumer Protection Act. If you have received this communication in error, please destroy all electronic and paper copies and notify the sender immediately. Mistransmission is not intended to waive confidentiality or privilege. Morgan Stanley reserves the right, to the extent permitted under applicable law, to monitor electronic communications. This message is subject to terms available at the following link: http://www.morganstanley.com/disclaimers  If you cannot access these links, please notify us by reply message and we will send the contents to you. By communicating with Morgan Stanley you consent to the foregoing and to the voice recording of conversations with personnel of Morgan Stanley.

From shade at redhat.com  Wed Sep 28 06:55:20 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Wed, 28 Sep 2016 12:55:20 +0200
Subject: [concurrency-interest] What can an incorrectly synchronized
 read see?
In-Reply-To: <9838003A10254741BC6FFADE2ED02B457D5E7346@OZWEX0205N1.msad.ms.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
 <008d01d21965$a80050f0$f800f2d0$@aapt.net.au>
 <16fab73e-467c-d1ca-1743-adff0db7a9d5@redhat.com>
 <9838003A10254741BC6FFADE2ED02B457D5E7346@OZWEX0205N1.msad.ms.com>
Message-ID: <7a44ae1d-1473-101c-1b27-8dee701ca314@redhat.com>

On 09/28/2016 12:47 PM, Bobrowski, Maciej wrote:
> Right, so the only invalid execution in any possible sequencing is
> the (1, 0) which was the main question of this post, correct?

Correct. I wanted to show how to apply spec to arrive to this result,
and you get the collateral results for free too :)

NB: Mental trap: believing that arriving to the same result as
specification validates the alternative reasoning. (It might, though,
but you have to show that alternative reasoning gives correct answers
for all other cases too and/or has the clear boundaries where it applies)

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/b6a8359f/attachment-0001.sig>

From me at yawk.at  Wed Sep 28 07:28:05 2016
From: me at yawk.at (Jonas Konrad)
Date: Wed, 28 Sep 2016 13:28:05 +0200
Subject: [concurrency-interest] Does JDK 9 String.hashCode() have a bug?
In-Reply-To: <9abf9037-9c80-af46-5458-62ad4bd14565@redhat.com>
References: <bd741d53-4257-89d7-23f4-215c8d2d2fc2@gmail.com>
 <9abf9037-9c80-af46-5458-62ad4bd14565@redhat.com>
Message-ID: <dc529ee2-f50f-a4f5-93ec-bca63ebc0145@yawk.at>

Hey Aleksey,

On 09/28/2016 12:38 PM, Aleksey Shipilev wrote:
> P.S. Your explanation is similar to a more detailed of mine here:
>
> https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-benign-is-resilient

Wouldn't, according to that section in your blog, the old hashcode (and 
the suggested fix) also be unsafe? From what I can tell the hashcode in 
Java 8 already assumed (which it can because it's JDK code) that the VM 
inserts a memory barrier in the String constructor because of the final 
value field.

- Jonas

From davidcholmes at aapt.net.au  Wed Sep 28 07:29:27 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 28 Sep 2016 21:29:27 +1000
Subject: [concurrency-interest] Does JDK 9 String.hashCode() have a bug?
In-Reply-To: <bd741d53-4257-89d7-23f4-215c8d2d2fc2@gmail.com>
References: <bd741d53-4257-89d7-23f4-215c8d2d2fc2@gmail.com>
Message-ID: <00be01d2197b$92be42e0$b83ac8a0$@aapt.net.au>

I thought that one of the properties of the JMM is that a second read of a variable can not return an earlier write to that variable. Certainly it is a property I recall being discussed in JSR-133.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Peter Levart
Sent: Wednesday, September 28, 2016 8:33 PM
To: concurrency-interest <Concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] Does JDK 9 String.hashCode() have a bug?

 

Hi,

I would like to discuss the implementation of String.hashCode() method in the light of JMM. Here it is (the recent one from JDK 9 which includes implementation of compact strings):

 

    public int hashCode() {
        if (hash == 0 && value.length > 0) {
            hash = isLatin1() ? StringLatin1.hashCode(value)
                              : StringUTF16.hashCode(value);
        }
        return hash;
    }

 

Here, 'hash' is a plain instance field in String. Suppose we have a freshly constructed non-zero length String object, whose reference 's' is accessible to two threads and each of them concurrently invokes hashCode() on it:

 

Thread 1:

int h1 = s.hashCode();

 

Thread 2:

int h2 = s.hashCode();

 

Suppose also that the isLatin1() ? StringLatin1.hashCode(value) : StringUTF16.hashCode(value) computation returns a non-zero value.

 

Is it possible that the outcome (h1, h2) is either (0, hc) or (hc, 0) where the hc is the non-zero result of above mentioned computation?

According to JMM, I think it is. Since there is no synchronization between a write w(hash, hc) by one thread and two reads r1(value):v1 and r2(value):v2 by the other thread that correspond to program lines:

    if (hash == 0 ....

and:

    return hash;

 

...the reads may or may not see the write independently. So there is a possibility that v1 == hc and v2 == 0.

 

Am I right and should this be fixed?

 

Note that JDK 8 code is different. It does not have this problem:

 

public int hashCode() {
        int h = hash;
        if (h == 0) {
            for (char v : value) {
                h = 31 * h + v;
            }
            if (h != 0) {
                hash = h;
            }
        }
        return h;
    }

 

Regards, Peter

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/e35d33ce/attachment.html>

From shade at redhat.com  Wed Sep 28 07:37:25 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Wed, 28 Sep 2016 13:37:25 +0200
Subject: [concurrency-interest] Does JDK 9 String.hashCode() have a bug?
In-Reply-To: <dc529ee2-f50f-a4f5-93ec-bca63ebc0145@yawk.at>
References: <bd741d53-4257-89d7-23f4-215c8d2d2fc2@gmail.com>
 <9abf9037-9c80-af46-5458-62ad4bd14565@redhat.com>
 <dc529ee2-f50f-a4f5-93ec-bca63ebc0145@yawk.at>
Message-ID: <e5d04633-01b3-20a4-64c9-5be5e4f43c96@redhat.com>

On 09/28/2016 01:28 PM, Jonas Konrad wrote:
> On 09/28/2016 12:38 PM, Aleksey Shipilev wrote:
>> P.S. Your explanation is similar to a more detailed of mine here:
>>
>> https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-benign-is-resilient
> 
> Wouldn't, according to that section in your blog, the old hashcode (and
> the suggested fix) also be unsafe? 

Why unsafe? JDK 8 String.hashCode is a classic example of the benign
data race. It's benignity comes, as with other benign data races, from
two conditions:

 a) The object being published is safely constructed. In this particular
case, we publish the primitive value, and it is safe by definition.

 b) We read the field only once, which is exactly what JDK 8
String.hashCode does, but JDK 9 String.hashCode lacks.

JDK 8:

  public int hashCode() {
    int h = hash;  // <--- read once
    if (h == 0) {  // <--- not 0? proceed to return
      for (char v : value) {
        h = 31 * h + v;
      }
      if (h != 0) {
        hash = h;
      }
    }
    return h;  // <--- return the verified value, DO NOT racy read again
  }

> From what I can tell the hashcode in Java 8 already assumed (which it
> can because it's JDK code) that the VM inserts a memory barrier in
> the String constructor because of the final value field.

Note this is *within* the String instance already, no "final value field
init in String" applies here.

Thanks,
-Aleksey




-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/192e19ab/attachment.sig>

From me at yawk.at  Wed Sep 28 07:55:19 2016
From: me at yawk.at (Jonas Konrad)
Date: Wed, 28 Sep 2016 13:55:19 +0200
Subject: [concurrency-interest] Does JDK 9 String.hashCode() have a bug?
In-Reply-To: <e5d04633-01b3-20a4-64c9-5be5e4f43c96@redhat.com>
References: <bd741d53-4257-89d7-23f4-215c8d2d2fc2@gmail.com>
 <9abf9037-9c80-af46-5458-62ad4bd14565@redhat.com>
 <dc529ee2-f50f-a4f5-93ec-bca63ebc0145@yawk.at>
 <e5d04633-01b3-20a4-64c9-5be5e4f43c96@redhat.com>
Message-ID: <9f8cbd2b-51ec-c732-a0f1-84216ddea986@yawk.at>

Take this example:

class Race {
     String s = "a";
     void writer() { s = "b"; }

     void reader() {
         String tmp = s;
         assert s.hashCode() == s.hashCode();
     }
}

class String {
     final char[] value; int hash;

     int hashCode() {
         int h = hash;
         if (h == 0) {
             hash = h = computeHashCode(value);
         }
         return h;
     }
}

In the JMM, can't that assertion fail? Since the 'hash' field is not
final and the String is not safely published, The reader could observe
the 'old' hashCode of "a" on the first read, not do any computation
because h != 0, and then see the 'new' hashCode with h == 0 in the
second read.

I see how the two reads are a good example of your other
"NonBenignRace2" though.

Thanks,
- Jonas

On 09/28/2016 01:37 PM, Aleksey Shipilev wrote:
> On 09/28/2016 01:28 PM, Jonas Konrad wrote:
>> On 09/28/2016 12:38 PM, Aleksey Shipilev wrote:
>>> P.S. Your explanation is similar to a more detailed of mine here:
>>>
>>> https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-benign-is-resilient
>>
>> Wouldn't, according to that section in your blog, the old hashcode (and
>> the suggested fix) also be unsafe?
>
> Why unsafe? JDK 8 String.hashCode is a classic example of the benign
> data race. It's benignity comes, as with other benign data races, from
> two conditions:
>
>  a) The object being published is safely constructed. In this particular
> case, we publish the primitive value, and it is safe by definition.
>
>  b) We read the field only once, which is exactly what JDK 8
> String.hashCode does, but JDK 9 String.hashCode lacks.
>
> JDK 8:
>
>   public int hashCode() {
>     int h = hash;  // <--- read once
>     if (h == 0) {  // <--- not 0? proceed to return
>       for (char v : value) {
>         h = 31 * h + v;
>       }
>       if (h != 0) {
>         hash = h;
>       }
>     }
>     return h;  // <--- return the verified value, DO NOT racy read again
>   }
>
>> From what I can tell the hashcode in Java 8 already assumed (which it
>> can because it's JDK code) that the VM inserts a memory barrier in
>> the String constructor because of the final value field.
>
> Note this is *within* the String instance already, no "final value field
> init in String" applies here.
>
> Thanks,
> -Aleksey
>
>
>
>

From shade at redhat.com  Wed Sep 28 08:07:01 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Wed, 28 Sep 2016 14:07:01 +0200
Subject: [concurrency-interest] Does JDK 9 String.hashCode() have a bug?
In-Reply-To: <9f8cbd2b-51ec-c732-a0f1-84216ddea986@yawk.at>
References: <bd741d53-4257-89d7-23f4-215c8d2d2fc2@gmail.com>
 <9abf9037-9c80-af46-5458-62ad4bd14565@redhat.com>
 <dc529ee2-f50f-a4f5-93ec-bca63ebc0145@yawk.at>
 <e5d04633-01b3-20a4-64c9-5be5e4f43c96@redhat.com>
 <9f8cbd2b-51ec-c732-a0f1-84216ddea986@yawk.at>
Message-ID: <b15b0182-97f0-4b5c-aad3-2060cbb74a2e@redhat.com>

On 09/28/2016 01:55 PM, Jonas Konrad wrote:
> Take this example:
> 
> class Race {
>     String s = "a";
>     void writer() { s = "b"; }
> 
>     void reader() {
>         String tmp = s;
>         assert s.hashCode() == s.hashCode();
>     }
> }
> 
> class String {
>     final char[] value; int hash;
> 
>     int hashCode() {
>         int h = hash;
>         if (h == 0) {
>             hash = h = computeHashCode(value);
>         }
>         return h;
>     }
> }
> 
> In the JMM, can't that assertion fail? 

Well, it can fail, because you have the prior race on Race.s, and you
might be comparing the hashcodes from two different Strings to begin
with. Obviously, this cannot be recovered in String.hashCode:

     void reader() {
         String tmp = s;
         assert s.hashCode() == s.hashCode();
     }

...but if you were to write:

     void reader() {
         String tmp = s;
         assert tmp.hashCode() == tmp.hashCode();
     }

...then it cannot fail, because String.hashCode() as stated cannot
return 0. (This is also one of many reasons why compilers cannot
generally replace local variables with memory reads, exposing users to
races). String.hashCode above will take a corrective action it if
happens to read (hash == 0), and return the recomputed one.

Thanks,
-Aleksey


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/2f2b9412/attachment-0001.sig>

From aph at redhat.com  Wed Sep 28 09:08:02 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 28 Sep 2016 14:08:02 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
Message-ID: <144054f8-ad80-a973-0594-799d716a575e@redhat.com>

I did a few experiments, with simpler code.

The version with the direct byte buffer was not very efficient: it
does many reads of memory unrelated to the actual thing we're trying
to test.  This is a shame, given that one of the goals of JEP 193 is
that the API should be at least as good as Unsafe.  Right now we'e a
long way from that goal.

The test code using the DBB VarHandle does this:

        spill
        safepoint poll
        DirectByteBuffer.limit
        DirectByteBuffer.hb
        DirectByteBuffer.address
        {our int value}
        spill
        getAndUpdate_shade.updateFn
        getAndUpdate_shade.updateFn.getClass()
        ... call updateFn
        Blackhole
        .. call consumeCPU
        spill
        DirectByteBuffer.hb
        DirectByteBuffer.address
        DirectByteBuffer.isReadOnly

        ... and finally

        CAS {our int value}

I used GetAndUpdateBench (which uses simple VarHandles, no
ByteBuffers) and replaced the call to Blackhole.consumeCPU(20L) with

        for (int i = 0; i < 10; i++) {
            // Marsaglia xor-shift generator
            n ^= (n << 21);
            n ^= (n >>> 35);
            n ^= (n << 4);
        }

to try to get a better idea of the real performance of getAndUpdate
with a (somewhat) time-consuming update function.  I did this because
Blackhole.consumeCPU() causes all registers to be spilled to the
stack.

The results, AArch64:

GetAndUpdateBench0.dflt                         avgt   20  160.454 ?  8.667  ns/op
GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  160.459 ?  8.555  ns/op
GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  160.450 ?  8.787  ns/op
GetAndUpdateBench0.martin                       avgt   20  159.229 ? 10.803  ns/op
GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  159.279 ? 10.774  ns/op
GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  159.180 ? 10.833  ns/op
GetAndUpdateBench0.shade                        avgt   20  136.533 ?  4.101  ns/op
GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  136.811 ?  4.042  ns/op
GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  136.255 ?  4.246  ns/op
GetAndUpdateBench0.strong                       avgt   20  136.746 ?  4.992  ns/op
GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  136.737 ?  5.007  ns/op
GetAndUpdateBench0.strong:getAndUpdate2_strong  avgt   20  136.755 ?  4.985  ns/op

strong and shade are the fastest, as I'd expect, because they do the
least work.  dflt and martin have a lot of jitter, perhaps because of
branch prediction.  I think shade would be better at times of high
contention, but that is very hard to measure.

Re the profile data for martin: I'm seeing "spurious" CAS failures in
the profile data of 87018:270, i.e. 0.3%.

I conclude that string and shade are equally good unless there is high
contention.  I haven't measured that.  I also haven't measured an
expensive update function with high jitter.  I suspect that with high
contention shade would win.  With no contention and no false sharing
there is nothing to choose between them.

If the update function was very expensive the rate of CAS failures
might well rise and it would be worth going to heroic attempts to
avoid calling it, but I suspect that such functions are unusual.

Andrew.


NB:

Looking at the profile data, "spurious" CAS failures are pretty
rare.  I do not believe that this is simply because we "got lucky" and
the variables were on different cache lines, but it's possible.  One
one run I saw times spectacularly better than these, and I ignored it.
I added some padding to prevent the two variables sharing the same
cache line, and:

Benchmark                                       Mode  Cnt   Score   Error  Units
GetAndUpdateBench0.dflt                         avgt   20  77.116 ? 0.120  ns/op
GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  77.142 ? 0.159  ns/op
GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  77.090 ? 0.112  ns/op
GetAndUpdateBench0.martin                       avgt   20  76.865 ? 0.222  ns/op
GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  76.840 ? 0.260  ns/op
GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  76.889 ? 0.241  ns/op
GetAndUpdateBench0.shade                        avgt   20  76.324 ? 0.096  ns/op
GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  76.286 ? 0.015  ns/op
GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  76.362 ? 0.183  ns/op
GetAndUpdateBench0.strong                       avgt   20  76.303 ? 0.080  ns/op
GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  76.330 ? 0.154  ns/op
GetAndUpdateBench0.strong:getAndUpdate2_strong  avgt   20  76.277 ? 0.011  ns/op

QED, I think.


Appendix

With trivial update function, no false sharing:

Benchmark                                       Mode  Cnt   Score   Error  Units
GetAndUpdateBench0.dflt                         avgt   20  44.842 ? 0.189  ns/op
GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  44.830 ? 0.188  ns/op
GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  44.855 ? 0.202  ns/op
GetAndUpdateBench0.martin                       avgt   20  44.900 ? 0.182  ns/op
GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  44.874 ? 0.338  ns/op
GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  44.926 ? 0.342  ns/op
GetAndUpdateBench0.shade                        avgt   20  44.626 ? 0.037  ns/op
GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  44.622 ? 0.022  ns/op
GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  44.630 ? 0.071  ns/op
GetAndUpdateBench0.strong                       avgt   20  44.208 ? 0.051  ns/op
GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  44.229 ? 0.097  ns/op
GetAndUpdateBench0.strong:getAndUpdate2_strong  avgt   20  44.188 ? 0.014  ns/op

With trivial update function and false sharing:

# Run complete. Total time: 00:03:35

Benchmark                                       Mode  Cnt    Score    Error  Units
GetAndUpdateBench0.dflt                         avgt   20  145.885 ?  3.751  ns/op
GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  149.685 ?  9.260  ns/op
GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  142.085 ?  8.205  ns/op
GetAndUpdateBench0.martin                       avgt   20  155.300 ? 10.353  ns/op
GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  153.278 ? 32.496  ns/op
GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  157.323 ? 35.911  ns/op
GetAndUpdateBench0.shade                        avgt   20  140.776 ?  8.517  ns/op
GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  140.115 ?  8.897  ns/op
GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  141.438 ?  9.049  ns/op
GetAndUpdateBench0.strong                       avgt   20  108.120 ?  3.737  ns/op
GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  108.126 ?  3.735  ns/op

From peter.levart at gmail.com  Wed Sep 28 10:17:28 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 28 Sep 2016 16:17:28 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
Message-ID: <29b4f32a-380b-bd2c-9e3f-99dd6bf0e2d4@gmail.com>

Hi Andrew,

If you look at what the GetAndUpdateBench actually does, then you'll see 
that *all* weak CAS failures (100 %) should be spurious and that there 
should be *no* strong CAS failures. The benchmark exhibits 2 threads and 
each of them accesses its own variable.

The problem with this benchmark is, I think, that all platforms tried so 
far either natively implement strong CAS or weak CAS, but not both (I 
might be wrong, that's just what I have observed so far). On Intel, weak 
CAS delegates to strong CAS and on AArch64, strong CAS is emulated with 
a weak CAS / re-read / compare loop. So this benchmark that tries to 
show the difference between using weak and strong CAS on one platform 
just compares same content with two different envelopes. The results 
should be comparable.

The difference you get between dflt/martin and shade/strong are 
therefore probably just caused by false-sharing / no-false-sharing 
between the fields of the allocated Vars object. Remember, each of the 
two threads accesses a different field. There's no data race (not strong 
CAS failures), just false-sharing or not resulting in spurious weak CAS 
failures or cache contention with strong CASes or not.

To verify my claims, I replaced VarHandle view over direct ByteBuffer 
with Unsafe accesses to buffer's native memory in the following benchmark:

http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench3.java

compiled here:

http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/benchmarks.jar

Can you try it on AArch64. You can replace BlackHole.consumeCPU with you 
xhorshift loop, but I don't think you'll get much different results.

On Intel (i7), they are comparable and just about 5% faster than 
VarHandle variant, so I think even VarHandle variant is dominated by CAS 
and updateFn and not by indirections or checks performed by VarHandle 
view over direct ByteBuffer:

Benchmark                                       (updateFnCpu) Mode  
Cnt    Score   Error  Units
GetAndUpdateBench3.dflt 1  avgt   20   44.269 ± 5.233  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt 1  avgt   20   44.661 ± 
5.533  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt 1  avgt   20   43.877 ± 
5.128  ns/op
GetAndUpdateBench3.dflt 10  avgt   20   48.481 ± 1.235  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt 10  avgt   20   48.021 ± 
1.156  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt 10  avgt   20   48.942 ± 
1.773  ns/op
GetAndUpdateBench3.dflt 20  avgt   20   54.645 ± 0.647  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt 20  avgt   20   54.546 ± 
2.042  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt 20  avgt   20   54.744 ± 
2.250  ns/op
GetAndUpdateBench3.dflt 50  avgt   20  111.442 ± 0.236  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt 50  avgt   20  111.143 ± 
1.168  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt 50  avgt   20  111.740 ± 
0.944  ns/op
GetAndUpdateBench3.dflt 100  avgt   20  202.829 ± 3.689  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt 100  avgt   20  202.541 ± 
4.309  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt 100  avgt   20  203.117 ± 
4.126  ns/op
GetAndUpdateBench3.martin 1  avgt   20   44.245 ± 0.295  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin 1  avgt   20   44.254 ± 
0.310  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin 1  avgt   20   44.235 ± 
0.289  ns/op
GetAndUpdateBench3.martin 10  avgt   20   46.241 ± 3.474  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin 10  avgt   20   46.213 ± 
3.682  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin 10  avgt   20   46.270 ± 
3.692  ns/op
GetAndUpdateBench3.martin 20  avgt   20   55.344 ± 1.307  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin 20  avgt   20   56.396 ± 
2.557  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin 20  avgt   20   54.291 ± 
1.630  ns/op
GetAndUpdateBench3.martin 50  avgt   20  111.164 ± 1.533  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin 50  avgt   20  111.110 ± 
2.465  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin 50  avgt   20  111.217 ± 
2.781  ns/op
GetAndUpdateBench3.martin 100  avgt   20  202.234 ± 0.912  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin 100  avgt   20  201.866 ± 
2.174  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin 100  avgt   20  202.601 ± 
2.502  ns/op
GetAndUpdateBench3.shade 1  avgt   20   43.170 ± 6.922  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade 1  avgt   20   43.200 ± 
6.937  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade 1  avgt   20   43.140 ± 
6.915  ns/op
GetAndUpdateBench3.shade 10  avgt   20   47.756 ± 1.122  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade 10  avgt   20   47.364 ± 
1.209  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade 10  avgt   20   48.147 ± 
1.329  ns/op
GetAndUpdateBench3.shade 20  avgt   20   54.876 ± 0.732  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade 20  avgt   20   54.705 ± 
1.152  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade 20  avgt   20   55.047 ± 
1.423  ns/op
GetAndUpdateBench3.shade 50  avgt   20  110.661 ± 0.177  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade 50  avgt   20  111.847 ± 
2.226  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade 50  avgt   20  109.476 ± 
2.115  ns/op
GetAndUpdateBench3.shade 100  avgt   20  203.791 ± 2.609  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade 100  avgt   20  204.484 ± 
4.302  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade 100  avgt   20  203.097 ± 
4.031  ns/op
GetAndUpdateBench3.strong 1  avgt   20   44.953 ± 0.785  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong 1  avgt   20   44.989 ± 
0.722  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong 1  avgt   20   44.917 ± 
0.926  ns/op
GetAndUpdateBench3.strong 10  avgt   20   46.965 ± 0.620  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong 10  avgt   20   46.799 ± 
1.144  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong 10  avgt   20   47.130 ± 
1.289  ns/op
GetAndUpdateBench3.strong 20  avgt   20   54.904 ± 0.956  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong 20  avgt   20   55.006 ± 
2.855  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong 20  avgt   20   54.802 ± 
2.413  ns/op
GetAndUpdateBench3.strong 50  avgt   20  109.759 ± 0.941  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong 50  avgt   20  109.110 ± 
2.792  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong 50  avgt   20  110.408 ± 
2.905  ns/op
GetAndUpdateBench3.strong 100  avgt   20  201.612 ± 0.881  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong 100  avgt   20  199.593 ± 
2.319  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong 100  avgt   20  203.630 ± 
3.172  ns/op


Regards, Peter


On 09/28/2016 03:08 PM, Andrew Haley wrote:
> I did a few experiments, with simpler code.
>
> The version with the direct byte buffer was not very efficient: it
> does many reads of memory unrelated to the actual thing we're trying
> to test.  This is a shame, given that one of the goals of JEP 193 is
> that the API should be at least as good as Unsafe.  Right now we'e a
> long way from that goal.
>
> The test code using the DBB VarHandle does this:
>
>          spill
>          safepoint poll
>          DirectByteBuffer.limit
>          DirectByteBuffer.hb
>          DirectByteBuffer.address
>          {our int value}
>          spill
>          getAndUpdate_shade.updateFn
>          getAndUpdate_shade.updateFn.getClass()
>          ... call updateFn
>          Blackhole
>          .. call consumeCPU
>          spill
>          DirectByteBuffer.hb
>          DirectByteBuffer.address
>          DirectByteBuffer.isReadOnly
>
>          ... and finally
>
>          CAS {our int value}
>
> I used GetAndUpdateBench (which uses simple VarHandles, no
> ByteBuffers) and replaced the call to Blackhole.consumeCPU(20L) with
>
>          for (int i = 0; i < 10; i++) {
>              // Marsaglia xor-shift generator
>              n ^= (n << 21);
>              n ^= (n >>> 35);
>              n ^= (n << 4);
>          }
>
> to try to get a better idea of the real performance of getAndUpdate
> with a (somewhat) time-consuming update function.  I did this because
> Blackhole.consumeCPU() causes all registers to be spilled to the
> stack.
>
> The results, AArch64:
>
> GetAndUpdateBench0.dflt                         avgt   20  160.454 ?  8.667  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  160.459 ?  8.555  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  160.450 ?  8.787  ns/op
> GetAndUpdateBench0.martin                       avgt   20  159.229 ? 10.803  ns/op
> GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  159.279 ? 10.774  ns/op
> GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  159.180 ? 10.833  ns/op
> GetAndUpdateBench0.shade                        avgt   20  136.533 ?  4.101  ns/op
> GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  136.811 ?  4.042  ns/op
> GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  136.255 ?  4.246  ns/op
> GetAndUpdateBench0.strong                       avgt   20  136.746 ?  4.992  ns/op
> GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  136.737 ?  5.007  ns/op
> GetAndUpdateBench0.strong:getAndUpdate2_strong  avgt   20  136.755 ?  4.985  ns/op
>
> strong and shade are the fastest, as I'd expect, because they do the
> least work.  dflt and martin have a lot of jitter, perhaps because of
> branch prediction.  I think shade would be better at times of high
> contention, but that is very hard to measure.
>
> Re the profile data for martin: I'm seeing "spurious" CAS failures in
> the profile data of 87018:270, i.e. 0.3%.
>
> I conclude that string and shade are equally good unless there is high
> contention.  I haven't measured that.  I also haven't measured an
> expensive update function with high jitter.  I suspect that with high
> contention shade would win.  With no contention and no false sharing
> there is nothing to choose between them.
>
> If the update function was very expensive the rate of CAS failures
> might well rise and it would be worth going to heroic attempts to
> avoid calling it, but I suspect that such functions are unusual.
>
> Andrew.
>
>
> NB:
>
> Looking at the profile data, "spurious" CAS failures are pretty
> rare.  I do not believe that this is simply because we "got lucky" and
> the variables were on different cache lines, but it's possible.  One
> one run I saw times spectacularly better than these, and I ignored it.
> I added some padding to prevent the two variables sharing the same
> cache line, and:
>
> Benchmark                                       Mode  Cnt   Score   Error  Units
> GetAndUpdateBench0.dflt                         avgt   20  77.116 ? 0.120  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  77.142 ? 0.159  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  77.090 ? 0.112  ns/op
> GetAndUpdateBench0.martin                       avgt   20  76.865 ? 0.222  ns/op
> GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  76.840 ? 0.260  ns/op
> GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  76.889 ? 0.241  ns/op
> GetAndUpdateBench0.shade                        avgt   20  76.324 ? 0.096  ns/op
> GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  76.286 ? 0.015  ns/op
> GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  76.362 ? 0.183  ns/op
> GetAndUpdateBench0.strong                       avgt   20  76.303 ? 0.080  ns/op
> GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  76.330 ? 0.154  ns/op
> GetAndUpdateBench0.strong:getAndUpdate2_strong  avgt   20  76.277 ? 0.011  ns/op
>
> QED, I think.
>
>
> Appendix
>
> With trivial update function, no false sharing:
>
> Benchmark                                       Mode  Cnt   Score   Error  Units
> GetAndUpdateBench0.dflt                         avgt   20  44.842 ? 0.189  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  44.830 ? 0.188  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  44.855 ? 0.202  ns/op
> GetAndUpdateBench0.martin                       avgt   20  44.900 ? 0.182  ns/op
> GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  44.874 ? 0.338  ns/op
> GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  44.926 ? 0.342  ns/op
> GetAndUpdateBench0.shade                        avgt   20  44.626 ? 0.037  ns/op
> GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  44.622 ? 0.022  ns/op
> GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  44.630 ? 0.071  ns/op
> GetAndUpdateBench0.strong                       avgt   20  44.208 ? 0.051  ns/op
> GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  44.229 ? 0.097  ns/op
> GetAndUpdateBench0.strong:getAndUpdate2_strong  avgt   20  44.188 ? 0.014  ns/op
>
> With trivial update function and false sharing:
>
> # Run complete. Total time: 00:03:35
>
> Benchmark                                       Mode  Cnt    Score    Error  Units
> GetAndUpdateBench0.dflt                         avgt   20  145.885 ?  3.751  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  149.685 ?  9.260  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  142.085 ?  8.205  ns/op
> GetAndUpdateBench0.martin                       avgt   20  155.300 ? 10.353  ns/op
> GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  153.278 ? 32.496  ns/op
> GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  157.323 ? 35.911  ns/op
> GetAndUpdateBench0.shade                        avgt   20  140.776 ?  8.517  ns/op
> GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  140.115 ?  8.897  ns/op
> GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  141.438 ?  9.049  ns/op
> GetAndUpdateBench0.strong                       avgt   20  108.120 ?  3.737  ns/op
> GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  108.126 ?  3.735  ns/op
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/2736dbde/attachment-0001.html>

From peter.levart at gmail.com  Wed Sep 28 10:29:37 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 28 Sep 2016 16:29:37 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
Message-ID: <8cbcbf07-a2d3-68a9-aabf-112303382d62@gmail.com>



On 09/28/2016 03:08 PM, Andrew Haley wrote:
> Looking at the profile data, "spurious" CAS failures are pretty
> rare.  I do not believe that this is simply because we "got lucky" and
> the variables were on different cache lines, but it's possible.  One
> one run I saw times spectacularly better than these, and I ignored it.
> I added some padding to prevent the two variables sharing the same
> cache line, and:
>
> Benchmark                                       Mode  Cnt   Score   Error  Units
> GetAndUpdateBench0.dflt                         avgt   20  77.116 ? 0.120  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  77.142 ? 0.159  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  77.090 ? 0.112  ns/op
> GetAndUpdateBench0.martin                       avgt   20  76.865 ? 0.222  ns/op
> GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  76.840 ? 0.260  ns/op
> GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  76.889 ? 0.241  ns/op
> GetAndUpdateBench0.shade                        avgt   20  76.324 ? 0.096  ns/op
> GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  76.286 ? 0.015  ns/op
> GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  76.362 ? 0.183  ns/op
> GetAndUpdateBench0.strong                       avgt   20  76.303 ? 0.080  ns/op
> GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  76.330 ? 0.154  ns/op
> GetAndUpdateBench0.strong:getAndUpdate2_strong  avgt   20  76.277 ? 0.011  ns/op
>
> QED, I think.

Yes, but that's an entirely different benchmark. This just tests raw 
speed of a single thread in its own cache-line. GetAndUpdateBench was 
meant to test the effect of false-sharing on different chosen strategy 
(mainly martin vs. shade). But there seems to be no measurable 
difference so far. GetAndUpdateBench failed because it did not guarantee 
false-sharing. Thus native ByteBuffer variant GetAndUpdateBench2,3 was 
made where the test has control over alignment.

Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/b4809556/attachment.html>

From vitalyd at gmail.com  Wed Sep 28 10:39:31 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Sep 2016 10:39:31 -0400
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <8cbcbf07-a2d3-68a9-aabf-112303382d62@gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <8cbcbf07-a2d3-68a9-aabf-112303382d62@gmail.com>
Message-ID: <CAHjP37EB-ZKceCKDx-YWtPCZi=sxM1hWt9cJJY5W+mUW0iswQA@mail.gmail.com>

On Wednesday, September 28, 2016, Peter Levart <peter.levart at gmail.com>
wrote:

>
>
> On 09/28/2016 03:08 PM, Andrew Haley wrote:
>
> Looking at the profile data, "spurious" CAS failures are pretty
> rare.  I do not believe that this is simply because we "got lucky" and
> the variables were on different cache lines, but it's possible.  One
> one run I saw times spectacularly better than these, and I ignored it.
> I added some padding to prevent the two variables sharing the same
> cache line, and:
>
> Benchmark                                       Mode  Cnt   Score   Error  Units
> GetAndUpdateBench0.dflt                         avgt   20  77.116 ? 0.120  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate1_dflt      avgt   20  77.142 ? 0.159  ns/op
> GetAndUpdateBench0.dflt:getAndUpdate2_dflt      avgt   20  77.090 ? 0.112  ns/op
> GetAndUpdateBench0.martin                       avgt   20  76.865 ? 0.222  ns/op
> GetAndUpdateBench0.martin:getAndUpdate1_martin  avgt   20  76.840 ? 0.260  ns/op
> GetAndUpdateBench0.martin:getAndUpdate2_martin  avgt   20  76.889 ? 0.241  ns/op
> GetAndUpdateBench0.shade                        avgt   20  76.324 ? 0.096  ns/op
> GetAndUpdateBench0.shade:getAndUpdate1_shade    avgt   20  76.286 ? 0.015  ns/op
> GetAndUpdateBench0.shade:getAndUpdate2_shade    avgt   20  76.362 ? 0.183  ns/op
> GetAndUpdateBench0.strong                       avgt   20  76.303 ? 0.080  ns/op
> GetAndUpdateBench0.strong:getAndUpdate1_strong  avgt   20  76.330 ? 0.154  ns/op
> GetAndUpdateBench0.strong:getAndUpdate2_strong  avgt   20  76.277 ? 0.011  ns/op
>
> QED, I think.
>
>
> Yes, but that's an entirely different benchmark. This just tests raw speed
> of a single thread in its own cache-line. GetAndUpdateBench was meant to
> test the effect of false-sharing on different chosen strategy (mainly
> martin vs. shade). But there seems to be no measurable difference so far.
> GetAndUpdateBench failed because it did not guarantee false-sharing. Thus
> native ByteBuffer variant GetAndUpdateBench2,3 was made where the test has
> control over alignment.
>
A better way to test the different versions is to run jmh under perf and
look at PMU counters.  In addition, probably makes sense to use a multi
socket machine and spread the threads across them to see worst case of
cachelines
bouncing across the sockets and not within a single socket via L2 or LLC.

>
>
> Regards, Peter
>
>


-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/0e9eb237/attachment.html>

From peter.levart at gmail.com  Wed Sep 28 10:44:06 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 28 Sep 2016 16:44:06 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <29b4f32a-380b-bd2c-9e3f-99dd6bf0e2d4@gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <29b4f32a-380b-bd2c-9e3f-99dd6bf0e2d4@gmail.com>
Message-ID: <0051b71f-398f-60a2-2c55-09404e9fff05@gmail.com>



On 09/28/2016 04:17 PM, Peter Levart wrote:
>
> To verify my claims, I replaced VarHandle view over direct ByteBuffer 
> with Unsafe accesses to buffer's native memory in the following benchmark:
>
> http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench3.java
>
>
> Can you try it on AArch64. You can replace BlackHole.consumeCPU with 
> you xhorshift loop, but I don't think you'll get much different results.

Ah, yes. To compile the bench, you must put something like the following 
into pom.xml:

             <plugin>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-compiler-plugin</artifactId>
                 <version>3.1</version>
                 <configuration>
<compilerVersion>${javac.target}</compilerVersion>
<source>${javac.target}</source>
<target>${javac.target}</target>
*                    <compilerArgs>**
**<arg>--add-exports</arg>**
**<arg>java.base/jdk.internal.misc=ALL-UNNAMED</arg>**
**                    </compilerArgs>*
                 </configuration>
             </plugin>

(Danger: don't do that at home, or Oracle might remove these options! ;-)...

Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/eadf8f0a/attachment.html>

From gergg at cox.net  Wed Sep 28 11:05:35 2016
From: gergg at cox.net (Gregg Wonderly)
Date: Wed, 28 Sep 2016 10:05:35 -0500
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <mMVz1t01302hR0p01MW2vM>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <mMVz1t01302hR0p01MW2vM>
Message-ID: <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>

The completely less thread consuming alternative is call backs.   VMS used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It was a very performant and friendly way to allow small software modules to be written to do one thing and used as the call back for asynchronous system behaviors.  I wrote several symbionts which did serial connection scripting for connecting to printers remotely, process management for process reuse on PMDF to speed up mail flow through the queue, and other things interactive with the kernel to alter process priorities to provide longer term scheduling on a heavily loaded 750.  It was like heaven to have such a small amount of code (lots of functions, some global data).   I am just suggesting that this kind of thing was a major part of more than one programming environment.

Having 100’s of threads blocked on various locks adds considerably to the overhead of scheduling but also complicates cache use and burdens the developer with looping and waiting in ways that increase bugs in code that should not exist.  I really feel that the kind of thing that CompletionStage provides to the developer is something that should of been more prevalent from the start.  It inverts programatic logic in some cases, but call backs really are the best way to react to asynchronous eventing in software.  

Gregg Wonderly

> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com> wrote:
> 
> My limited understanding is that the original API was only CompletableFuture and that CompletionStage introduced as a compromise. It did not appear to be an attempt to strictly follow an interface-implementation separation, e.g. collections. As you said #toCompletableFuture() may throw an UOE, which means some use-cases can't rely on CompletionState which limits its usefulness. In my case that would be an AsyncLoadingCache with a synchronous LoadingCache view. I think having to code that the resulting implementation would be worse if it called toCompletableFuture, caught the exception, and then adapted as you said. 
> 
> When the new future class was introduced it was stated,
> 
> "In other words, we (j.u.c) are not now in a position to dictate a common interface for all SettableFuture, FutureValue, Promise, ListenableFuture, etc like APIs. And as we've seen, different audiences want/need different subsets of this API exposed as interfaces for their usages, and are in any case unlikely to want change all their existing interfaces. However, what we can do is provide a common underlying implementation that is as fast, scalable, space-conserving, carefully-specified, and reliable as possible. It should then be easy and attractive for others creating or reworking higher-level APIs to relay all functionality to the CompletableFuture implementation."  - Doug Lea, '12
> 
> I've gradually come to terms using CF as part of an API and haven't experienced a downside yet.
> 
> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com <mailto:martinrb at google.com>> wrote:
> (Sorry to re-open this discussion)
> 
> The separation of a read-only CompletionStage from CompletableFuture is great.  I'm a fan of the scala style Promise/Future split as described in http://docs.scala-lang.org/overviews/core/futures.html <http://docs.scala-lang.org/overviews/core/futures.html>, but: we need to re-add (safe, read-only) blocking methods like join.  Java is not Node.js, where there are no threads but there is a universal event loop.  Java programmers are used to Future, where the *only* way to use a future's value is to block waiting for it.  The existing CompletionStage methods are a better scaling alternative to blocking all the time, but blocking is almost always eventually necessary in Java.  For example, junit test methods that start any asynchronous computation need to block until the computation is done, before returning.
> 
> As Viktor has pointed out, users can always implement blocking themselves by writing
> 
>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T> stage) {
>         CompletableFuture<T> f = new CompletableFuture<>();
>         stage.handle((T t, Throwable ex) -> {
>                          if (ex != null) f.completeExceptionally(ex);
>                          else f.complete(t);
>                          return null;
>                      });
>         return f;
>     }
> 
>     static <T> T join(CompletionStage<T> stage) {
>         return toCompletableFuture(stage).join();
>     }
> 
> but unlike Viktor, I think it's unreasonable to not provide this for users (especially when we can do so more efficiently).  What is happening instead is API providers not using CompletionStage as return values in public APIs because of the lack of convenient blocking, and instead returning CompletableFuture, which is a tragic software engineering failure.
> 
> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture from throwing UnsupportedOperationException, and implement join as:
> 
>     public default T join() { return toCompletableFuture().join(); }
> 
> There is a risk of multiple-inheritance conflict with Future if we add e.g. isDone(), but there are no current plans to turn those Future methods into default methods, and even if we did in some future release, it would be only a source, not binary incompatibility, so far less serious.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/91e420e3/attachment-0001.html>

From kirk at kodewerk.com  Wed Sep 28 11:15:43 2016
From: kirk at kodewerk.com (kirk at kodewerk.com)
Date: Wed, 28 Sep 2016 17:15:43 +0200
Subject: [concurrency-interest] We need to add blocking methods to
 CompletionStage!
In-Reply-To: <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <mMVz1t01302hR0p01MW2vM> <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
Message-ID: <CE300227-1E8E-4609-BABF-72117BB14937@kodewerk.com>


> On Sep 28, 2016, at 5:05 PM, Gregg Wonderly <gergg at cox.net> wrote:
> 
> The completely less thread consuming alternative is call backs.  

This is a technique that we commonly used in a system that a group of use built in the 90s. The system had many variants on a number of different data flows through the system. It could take days for a number of the services to complete so synchronous calls simply were not a practical solution. async turned out to be so simple that it was used all over.

Regards,
Kirk


From aph at redhat.com  Wed Sep 28 11:19:47 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 28 Sep 2016 16:19:47 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <8cbcbf07-a2d3-68a9-aabf-112303382d62@gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <8cbcbf07-a2d3-68a9-aabf-112303382d62@gmail.com>
Message-ID: <74fba78f-7138-8a51-4acf-6281e794764a@redhat.com>

On 28/09/16 15:29, Peter Levart wrote:
> Yes, but that's an entirely different benchmark. This just tests raw 
> speed of a single thread in its own cache-line.

Well, exactly, that's the idea.  I wanted to make sure that we really
had false sharing in the test case; I can do that by making sure that
when there is no false sharing things really are much faster.

Andrew.


From peter.levart at gmail.com  Wed Sep 28 12:34:36 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 28 Sep 2016 18:34:36 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <74fba78f-7138-8a51-4acf-6281e794764a@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <8cbcbf07-a2d3-68a9-aabf-112303382d62@gmail.com>
 <74fba78f-7138-8a51-4acf-6281e794764a@redhat.com>
Message-ID: <14348620-8b94-b43d-308c-ca3a9c482e04@gmail.com>



On 09/28/2016 05:19 PM, Andrew Haley wrote:
> On 28/09/16 15:29, Peter Levart wrote:
>> Yes, but that's an entirely different benchmark. This just tests raw
>> speed of a single thread in its own cache-line.
> Well, exactly, that's the idea.  I wanted to make sure that we really
> had false sharing in the test case; I can do that by making sure that
> when there is no false sharing things really are much faster.
>
> Andrew.
>

So the updateFn was the same? Hm, then I really must see the results of 
this on your AArch64:

http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench3.java
http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/benchmarks.jar

Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/7255eec9/attachment.html>

From aph at redhat.com  Wed Sep 28 12:58:45 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 28 Sep 2016 17:58:45 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <29b4f32a-380b-bd2c-9e3f-99dd6bf0e2d4@gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <29b4f32a-380b-bd2c-9e3f-99dd6bf0e2d4@gmail.com>
Message-ID: <b3cb9c46-bfe8-7381-d3f5-989e9091919e@redhat.com>

On 28/09/16 15:17, Peter Levart wrote:
> If you look at what the GetAndUpdateBench actually does, then you'll see 
> that *all* weak CAS failures (100 %) should be spurious and that there 
> should be *no* strong CAS failures. The benchmark exhibits 2 threads and 
> each of them accesses its own variable.

Yes.  Got that.

> The problem with this benchmark is, I think, that all platforms tried so 
> far either natively implement strong CAS or weak CAS, but not both (I 
> might be wrong, that's just what I have observed so far).

Mine does both.  I know because I implemented these functions!

> The difference you get between dflt/martin and shade/strong are 
> therefore probably just caused by false-sharing / no-false-sharing 
> between the fields of the allocated Vars object.

I don't understand why you say that.  I'm pretty certain that the
false-sharing / no-false-sharing doesn't differ between these.  It's
a real performance difference between the implementations.

> Remember, each of the two threads accesses a different
> field. There's no data race (not strong CAS failures), just
> false-sharing or not resulting in spurious weak CAS failures or
> cache contention with strong CASes or not.

Yes.  Again, we don't differ on this.

> To verify my claims, I replaced VarHandle view over direct ByteBuffer 
> with Unsafe accesses to buffer's native memory in the following benchmark:
> 
> http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/GetAndUpdateBench3.java
> 
> compiled here:
> 
> http://cr.openjdk.java.net/~plevart/misc/GetAndUpdate/benchmarks.jar
> 
> Can you try it on AArch64. You can replace BlackHole.consumeCPU with you 
> xhorshift loop, but I don't think you'll get much different results.
> 
> On Intel (i7), they are comparable and just about 5% faster than 
> VarHandle variant, so I think even VarHandle variant is dominated by CAS 
> and updateFn and not by indirections or checks performed by VarHandle 
> view over direct ByteBuffer:

Your mailer totally mangled this table.  Please fix your mailer so
that it doesn't wrap outgoing lines.

Results appended.  The different reults for each implementation look
much more similar than when I did the measurement, but there's more
fixed overhead.  The lowest-overhead versions give much the same
results:

GetAndUpdateBench3.dflt                                     1  avgt   20  132.744 ? 2.497  ns/op
GetAndUpdateBench3.martin                                   1  avgt   20  141.096 ? 9.840  ns/op
GetAndUpdateBench3.shade                                    1  avgt   20  132.816 ? 0.809  ns/op
GetAndUpdateBench3.strong                                   1  avgt   20  130.283 ? 3.070  ns/op

Andrew.


Benchmark                                       (updateFnCpu)  Mode  Cnt    Score   Error  Units
GetAndUpdateBench3.dflt                                     1  avgt   20  132.744 ? 2.497  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt                  1  avgt   20  132.869 ? 2.444  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt                  1  avgt   20  132.619 ? 2.576  ns/op
GetAndUpdateBench3.dflt                                    10  avgt   20  151.319 ? 3.903  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt                 10  avgt   20  151.331 ? 3.953  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt                 10  avgt   20  151.306 ? 3.856  ns/op
GetAndUpdateBench3.dflt                                    20  avgt   20  185.814 ? 1.673  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt                 20  avgt   20  185.758 ? 1.711  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt                 20  avgt   20  185.870 ? 1.667  ns/op
GetAndUpdateBench3.dflt                                    50  avgt   20  305.686 ? 1.639  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt                 50  avgt   20  305.685 ? 1.718  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt                 50  avgt   20  305.687 ? 1.576  ns/op
GetAndUpdateBench3.dflt                                   100  avgt   20  492.770 ? 1.362  ns/op
GetAndUpdateBench3.dflt:getAndUpdate1_dflt                100  avgt   20  492.933 ? 1.392  ns/op
GetAndUpdateBench3.dflt:getAndUpdate2_dflt                100  avgt   20  492.607 ? 1.540  ns/op
GetAndUpdateBench3.martin                                   1  avgt   20  141.096 ? 9.840  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin              1  avgt   20  141.042 ? 9.827  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin              1  avgt   20  141.150 ? 9.855  ns/op
GetAndUpdateBench3.martin                                  10  avgt   20  147.714 ? 3.361  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin             10  avgt   20  147.723 ? 3.351  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin             10  avgt   20  147.705 ? 3.375  ns/op
GetAndUpdateBench3.martin                                  20  avgt   20  185.620 ? 2.114  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin             20  avgt   20  185.583 ? 2.151  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin             20  avgt   20  185.657 ? 2.082  ns/op
GetAndUpdateBench3.martin                                  50  avgt   20  293.495 ? 1.739  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin             50  avgt   20  293.524 ? 1.775  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin             50  avgt   20  293.466 ? 1.724  ns/op
GetAndUpdateBench3.martin                                 100  avgt   20  487.543 ? 2.683  ns/op
GetAndUpdateBench3.martin:getAndUpdate1_martin            100  avgt   20  487.456 ? 2.865  ns/op
GetAndUpdateBench3.martin:getAndUpdate2_martin            100  avgt   20  487.629 ? 2.580  ns/op
GetAndUpdateBench3.shade                                    1  avgt   20  132.816 ? 0.809  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade                1  avgt   20  132.780 ? 0.839  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade                1  avgt   20  132.852 ? 0.796  ns/op
GetAndUpdateBench3.shade                                   10  avgt   20  159.925 ? 9.468  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade               10  avgt   20  160.100 ? 9.645  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade               10  avgt   20  159.749 ? 9.318  ns/op
GetAndUpdateBench3.shade                                   20  avgt   20  205.441 ? 5.715  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade               20  avgt   20  206.646 ? 8.524  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade               20  avgt   20  204.235 ? 7.383  ns/op
GetAndUpdateBench3.shade                                   50  avgt   20  314.961 ? 5.568  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade               50  avgt   20  314.340 ? 5.699  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade               50  avgt   20  315.582 ? 5.878  ns/op
GetAndUpdateBench3.shade                                  100  avgt   20  495.603 ? 4.379  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade              100  avgt   20  495.755 ? 4.923  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade              100  avgt   20  495.450 ? 3.985  ns/op
GetAndUpdateBench3.strong                                   1  avgt   20  130.283 ? 3.070  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong              1  avgt   20  130.259 ? 3.081  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong              1  avgt   20  130.308 ? 3.067  ns/op
GetAndUpdateBench3.strong                                  10  avgt   20  146.124 ? 3.120  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong             10  avgt   20  146.176 ? 3.064  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong             10  avgt   20  146.072 ? 3.192  ns/op
GetAndUpdateBench3.strong                                  20  avgt   20  186.345 ? 1.518  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong             20  avgt   20  186.319 ? 1.529  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong             20  avgt   20  186.371 ? 1.564  ns/op
GetAndUpdateBench3.strong                                  50  avgt   20  292.982 ? 1.855  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong             50  avgt   20  292.991 ? 1.820  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong             50  avgt   20  292.973 ? 1.902  ns/op
GetAndUpdateBench3.strong                                 100  avgt   20  480.361 ? 1.097  ns/op
GetAndUpdateBench3.strong:getAndUpdate1_strong            100  avgt   20  480.359 ? 1.189  ns/op
GetAndUpdateBench3.strong:getAndUpdate2_strong            100  avgt   20  480.363 ? 1.248  ns/op

From aph at redhat.com  Wed Sep 28 13:55:09 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 28 Sep 2016 18:55:09 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <74fba78f-7138-8a51-4acf-6281e794764a@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <8cbcbf07-a2d3-68a9-aabf-112303382d62@gmail.com>
 <74fba78f-7138-8a51-4acf-6281e794764a@redhat.com>
Message-ID: <9bc73165-a3f6-3d02-a674-32fcc8e9f251@redhat.com>

One important thing to realize: even when we have false sharing and
the benchmark runs much more slowly, there is still not a significant
percentage of weak CAS fails.  The benchmark is slower because the
cache line is pinging back-and-forth between L1 caches so the L1 cache
misses, not because of a high weak CAS failure rate.  I suspect this
is typical: once the line has been fetched by LL and is in exclusive
state, there is only a tiny window before SC.

The few weak CAS failures I am seeing correspond with delays of
extremly long duration, and sometimes as long as a few milliseconds.
The only really likely explanation for that is that the thread is
descheduled or otherwise interrupted by the OS kernel and this
obviously would cause a weak CAS to fail.

I've used the AuxCounters feature to record when weak CAS fails.
cr.openjdk.java.net:~aph/test/GetAndUpdateBench3

P.S.: shade, please print out the number of instances of case1 and
case2, not just their average time.  Thanks.

Andrew.


Benchmark                                     (updateFnCpu)  Mode  Cnt        Score         Error  Units
GetAndUpdateBench3.shade                                  1  avgt   20      135.842 ?      12.581  ns/op
GetAndUpdateBench3.shade:case1                            1  avgt   20      135.646 ?      12.689  ns/op
GetAndUpdateBench3.shade:case2                            1  avgt   20    14738.834 ?   15183.891  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade              1  avgt   20      140.626 ?      22.241  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade              1  avgt   20      131.058 ?      12.259  ns/op
GetAndUpdateBench3.shade:total                            1  avgt   20      113.007 ?       5.217  ns/op
GetAndUpdateBench3.shade                                 10  avgt   20      146.594 ?       0.575  ns/op
GetAndUpdateBench3.shade:case1                           10  avgt   20      146.465 ?       0.586  ns/op
GetAndUpdateBench3.shade:case2                           10  avgt   20  1248338.412 ? 1145549.327  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade             10  avgt   20      146.543 ?       0.662  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade             10  avgt   20      146.644 ?       0.562  ns/op
GetAndUpdateBench3.shade:total                           10  avgt   20      145.162 ?       2.839  ns/op
GetAndUpdateBench3.shade                                 20  avgt   20      173.579 ?       0.901  ns/op
GetAndUpdateBench3.shade:case1                           20  avgt   20      173.500 ?       0.842  ns/op
GetAndUpdateBench3.shade:case2                           20  avgt   20   916952.445 ? 1147957.020  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade             20  avgt   20      173.498 ?       1.133  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade             20  avgt   20      173.659 ?       0.728  ns/op
GetAndUpdateBench3.shade:total                           20  avgt   20      172.134 ?       2.710  ns/op
GetAndUpdateBench3.shade                                 50  avgt   20      300.589 ?      23.422  ns/op
GetAndUpdateBench3.shade:case1                           50  avgt   20      300.537 ?      23.457  ns/op
GetAndUpdateBench3.shade:case2                           50  avgt   20  6912133.566 ? 4212701.775  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade             50  avgt   20      300.559 ?      23.396  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade             50  avgt   20      300.618 ?      23.452  ns/op
GetAndUpdateBench3.shade:total                           50  avgt   20      292.863 ?      15.469  ns/op
GetAndUpdateBench3.shade                                100  avgt   20      453.153 ?       7.168  ns/op
GetAndUpdateBench3.shade:case1                          100  avgt   20      452.858 ?       6.940  ns/op
GetAndUpdateBench3.shade:case2                          100  avgt   20  1877949.365 ? 2351908.185  ns/op
GetAndUpdateBench3.shade:getAndUpdate1_shade            100  avgt   20      452.429 ?       6.266  ns/op
GetAndUpdateBench3.shade:getAndUpdate2_shade            100  avgt   20      453.877 ?       8.503  ns/op
GetAndUpdateBench3.shade:total                          100  avgt   20      446.519 ?       3.611  ns/op

From oleksandr.otenko at gmail.com  Wed Sep 28 14:38:35 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 28 Sep 2016 19:38:35 +0100
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <mMVz1t01302hR0p01MW2vM> <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
Message-ID: <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>

100s of threads blocked on various locks is no different from 100s of callbacks waiting in the queue.

Callbacks require a different expression of the same problem. For example, local variables are no longer local variables - they become instance variables. Without language support for closures it becomes a nightmare.

But most importantly, such common algorithmic constructs as loops and return from function, no longer look like that. A sequence of calls to potentially blocking methods turn into a pyramid of doom - just take a look at node.js projects.

It’s a tradeoff.

Alex

> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net> wrote:
> 
> The completely less thread consuming alternative is call backs.   VMS used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It was a very performant and friendly way to allow small software modules to be written to do one thing and used as the call back for asynchronous system behaviors.  I wrote several symbionts which did serial connection scripting for connecting to printers remotely, process management for process reuse on PMDF to speed up mail flow through the queue, and other things interactive with the kernel to alter process priorities to provide longer term scheduling on a heavily loaded 750.  It was like heaven to have such a small amount of code (lots of functions, some global data).   I am just suggesting that this kind of thing was a major part of more than one programming environment.
> 
> Having 100’s of threads blocked on various locks adds considerably to the overhead of scheduling but also complicates cache use and burdens the developer with looping and waiting in ways that increase bugs in code that should not exist.  I really feel that the kind of thing that CompletionStage provides to the developer is something that should of been more prevalent from the start.  It inverts programatic logic in some cases, but call backs really are the best way to react to asynchronous eventing in software.  
> 
> Gregg Wonderly
> 
>> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com <mailto:ben.manes at gmail.com>> wrote:
>> 
>> My limited understanding is that the original API was only CompletableFuture and that CompletionStage introduced as a compromise. It did not appear to be an attempt to strictly follow an interface-implementation separation, e.g. collections. As you said #toCompletableFuture() may throw an UOE, which means some use-cases can't rely on CompletionState which limits its usefulness. In my case that would be an AsyncLoadingCache with a synchronous LoadingCache view. I think having to code that the resulting implementation would be worse if it called toCompletableFuture, caught the exception, and then adapted as you said. 
>> 
>> When the new future class was introduced it was stated,
>> 
>> "In other words, we (j.u.c) are not now in a position to dictate a common interface for all SettableFuture, FutureValue, Promise, ListenableFuture, etc like APIs. And as we've seen, different audiences want/need different subsets of this API exposed as interfaces for their usages, and are in any case unlikely to want change all their existing interfaces. However, what we can do is provide a common underlying implementation that is as fast, scalable, space-conserving, carefully-specified, and reliable as possible. It should then be easy and attractive for others creating or reworking higher-level APIs to relay all functionality to the CompletableFuture implementation."  - Doug Lea, '12
>> 
>> I've gradually come to terms using CF as part of an API and haven't experienced a downside yet.
>> 
>> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com <mailto:martinrb at google.com>> wrote:
>> (Sorry to re-open this discussion)
>> 
>> The separation of a read-only CompletionStage from CompletableFuture is great.  I'm a fan of the scala style Promise/Future split as described in http://docs.scala-lang.org/overviews/core/futures.html <http://docs.scala-lang.org/overviews/core/futures.html>, but: we need to re-add (safe, read-only) blocking methods like join.  Java is not Node.js, where there are no threads but there is a universal event loop.  Java programmers are used to Future, where the *only* way to use a future's value is to block waiting for it.  The existing CompletionStage methods are a better scaling alternative to blocking all the time, but blocking is almost always eventually necessary in Java.  For example, junit test methods that start any asynchronous computation need to block until the computation is done, before returning.
>> 
>> As Viktor has pointed out, users can always implement blocking themselves by writing
>> 
>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T> stage) {
>>         CompletableFuture<T> f = new CompletableFuture<>();
>>         stage.handle((T t, Throwable ex) -> {
>>                          if (ex != null) f.completeExceptionally(ex);
>>                          else f.complete(t);
>>                          return null;
>>                      });
>>         return f;
>>     }
>> 
>>     static <T> T join(CompletionStage<T> stage) {
>>         return toCompletableFuture(stage).join();
>>     }
>> 
>> but unlike Viktor, I think it's unreasonable to not provide this for users (especially when we can do so more efficiently).  What is happening instead is API providers not using CompletionStage as return values in public APIs because of the lack of convenient blocking, and instead returning CompletableFuture, which is a tragic software engineering failure.
>> 
>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture from throwing UnsupportedOperationException, and implement join as:
>> 
>>     public default T join() { return toCompletableFuture().join(); }
>> 
>> There is a risk of multiple-inheritance conflict with Future if we add e.g. isDone(), but there are no current plans to turn those Future methods into default methods, and even if we did in some future release, it would be only a source, not binary incompatibility, so far less serious.
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/206adf33/attachment-0001.html>

From oleksandr.otenko at gmail.com  Wed Sep 28 15:23:24 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 28 Sep 2016 20:23:24 +0100
Subject: [concurrency-interest] What can an incorrectly synchronized
	read see?
In-Reply-To: <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
Message-ID: <81B68E94-4ED5-4388-83D4-DBDC527C9A9F@gmail.com>

This reasoning is actually quite complicated, and the original reasoning is ok, it just needs statements justifying the claims about happens-before between unsynchronizedGet and increments.

1. All lock acquires and lock releases are in total order and establish happens-before edges between lock releases and subsequent lock acquires (definition of synchronization order)

2. All statements in program order before a lock release happen-before any statements that are in program order after the lock acquires following that lock release (definition of transitive closure)

This is justification enough for the existence of happens-before between unsynchronizedGet and some increments - ie there are writes unsynchronizedGet cannot observe.


3. unsynchronizedGet cannot observe the writes that are after subsequent synchronizedGet in total synchronization order (slightly simplified statement, since synchronization order will only contain lock acquires/releases, not the whole operations).

4. Therefore, unsynchronizedGet observes one of the writes that precede the synchronizedGet in total synchronization order.

5. unsynchronizedGet either observes the same write that the synchronizedGet observes, or some preceding write. Trivially, the values seen by unsynchronizedGet are less or equal the values seen by synchronizedGet.


Alex

> On 28 Sep 2016, at 09:22, Aleksey Shipilev <shade at redhat.com> wrote:
> 
> Hi,
> 
> On 09/28/2016 09:23 AM, jingguo yao wrote:
>> I think that unsynchronizedCount <= synchronizedCount is always true
>> for the above code provided that JVM implementations of read and write
>> operations for int type are atomic. 
> 
> They are atomic, mandated by JLS.
> 
> 
>> When increment happens-before synchronizedGet, synchronizedGet sees 1.
>> And there is no happens-before order to prevent unsynchronizedGet from
>> seeing initial_write or increment. So unsynchronizedGet sees1 0 or 1.
>> 
>> Happens-before digram for this case:
>> 
>>  initial_write ---> increment ---> synchronizedGet
>>                                      ^
>>              unsynchronizedGet------/
>> 
>> When synchronizedGet happens-before increment, synchronizedGet sees 0. Since
>> unsynchronizedGet happens-before increment, it can't see 1 written
>> by increment. It can only see 0 written by initial_write.
>> 
>> Happens-before digram for this case:
>> 
>>  initial_write ---> synchronizedGet ---> increment
>>                        ^
>>  unsynchronizedGet----/   
>> 
>> Is my reasoning correct?
> 
> Not quite, because I think it confuses the notion of program statements
> and actions in JMM. The analysis should use JMM rules as stated, like below.
> 
> Happens-before consistency says that in valid executions, the read has
> two relations with writes: a) with the writes that are tied in
> happens-before with the read in question, the read can see only the
> *latest* write in HB -- not the one before it, not the one after the
> read in HB; b) with the writes that are not tied in happens-before with
> the read in question, the read can see whatever such write.
> 
> So, these both are valid executions under JMM:
> 
>                               r(val):0 --po-\
>                                             |
> w(val, 1) --po/hb--> unlck(m) --sw/hb--> lck(m) --po/hb--> r(val):1
> 
> 
>                               r(val):1 --po-\
>                                             |
> w(val, 1) --po/hb--> unlck(m) --sw/hb--> lck(m) --po/hb--> r(val):1
> 
> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>                               ^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^
>                                 unsynch                 synch
>     parts done by Writer              parts done by Reader
> 
> 
> Where:
>  w(F,V) is the write of value "V" to field "F"
>  r(F):V is the read of value "V" from field "F"
>  lck/unlck(M) are the lock and unlock of the monitor M
>  --po/hb--> is the happens-before edge induced by program order
>  --sw/hb--> is the happens-before edge induced by synchronizes-with
>  --po--> is "just" the program order
> 
> Note that unsynchronized read can read either "0" or "1", regardless of
> if synchronized sees "1" after it.
> 
> Can the unsynchronized see "1", but the synchronized see "0"? Now, this
> is where it gets interesting. We cannot use the execution from above to
> justify reading synchronized "0", because synchronizes-with mandates
> seeing "1". Therefore, we need to reverse the order of Writer and Reader:
> 
> 
>                  parts done by Reader
>    unsync                                  synch
> vvvvvvvvvvvvvv       vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
> vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
> 
> r(val):V1 --po/hb--> lck(m) --po/hb--> r(val):V2 --po/hb--> unlck(m)
>                                                              /
>      /-----------------------sw/hb--------------------------/
>      v
>    lck(m) --po/hb--> w(val, 1)
> 
> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>    parts done by Writer
> 
> Now, let's see what (V1, V2) combinations do not violate JMM:
> (0, 0) -- ok, both see some (default?) "0" write from before
> (1, 0) -- NOT OK, r(val):V1 cannot see the w(val,1) that's after in HB!
> (0, 1) and
> (1, 1) -- NOT OK for this particular execution, because r(val):V2 sees
> the "future" write w(val,1). Notice, however, that both these results
> are justified by the execution at the very beginning, when Writer and
> Reader are sequenced in another order.
> 
> Hope this helps.
> 
> Thanks,
> -Aleksey
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From peter.levart at gmail.com  Wed Sep 28 15:45:50 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 28 Sep 2016 21:45:50 +0200
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <9bc73165-a3f6-3d02-a674-32fcc8e9f251@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <8cbcbf07-a2d3-68a9-aabf-112303382d62@gmail.com>
 <74fba78f-7138-8a51-4acf-6281e794764a@redhat.com>
 <9bc73165-a3f6-3d02-a674-32fcc8e9f251@redhat.com>
Message-ID: <e13c46bb-3b9d-af53-5332-46f7fa4eb4c5@gmail.com>

Hi Andrew,


On 09/28/2016 07:55 PM, Andrew Haley wrote:
> One important thing to realize: even when we have false sharing and
> the benchmark runs much more slowly, there is still not a significant
> percentage of weak CAS fails.  The benchmark is slower because the
> cache line is pinging back-and-forth between L1 caches so the L1 cache
> misses, not because of a high weak CAS failure rate.  I suspect this
> is typical: once the line has been fetched by LL and is in exclusive
> state, there is only a tiny window before SC.
>
> The few weak CAS failures I am seeing correspond with delays of
> extremly long duration, and sometimes as long as a few milliseconds.
> The only really likely explanation for that is that the thread is
> descheduled or otherwise interrupted by the OS kernel and this
> obviously would cause a weak CAS to fail.

I see. So false sharing can only provoke spurious LL/SC failure when the 
cache line is stolen between LL and SC? This makes sense. That's why 
results between strong and weak CAS are comparable even on AArch64. Weak 
CAS is not spuriously failing much if cache-line contention is not 
extremely high. Do you believe other HW platforms that implement weak 
CAS have similar spurious failure characteristics?

Thanks for helping understand the mechanics of new weak CAS operations.

Regards, Peter

> I've used the AuxCounters feature to record when weak CAS fails.
> cr.openjdk.java.net:~aph/test/GetAndUpdateBench3
>
> P.S.: shade, please print out the number of instances of case1 and
> case2, not just their average time.  Thanks.
>
> Andrew.
>
>
> Benchmark                                     (updateFnCpu)  Mode  Cnt        Score         Error  Units
> GetAndUpdateBench3.shade                                  1  avgt   20      135.842 ?      12.581  ns/op
> GetAndUpdateBench3.shade:case1                            1  avgt   20      135.646 ?      12.689  ns/op
> GetAndUpdateBench3.shade:case2                            1  avgt   20    14738.834 ?   15183.891  ns/op
> GetAndUpdateBench3.shade:getAndUpdate1_shade              1  avgt   20      140.626 ?      22.241  ns/op
> GetAndUpdateBench3.shade:getAndUpdate2_shade              1  avgt   20      131.058 ?      12.259  ns/op
> GetAndUpdateBench3.shade:total                            1  avgt   20      113.007 ?       5.217  ns/op
> GetAndUpdateBench3.shade                                 10  avgt   20      146.594 ?       0.575  ns/op
> GetAndUpdateBench3.shade:case1                           10  avgt   20      146.465 ?       0.586  ns/op
> GetAndUpdateBench3.shade:case2                           10  avgt   20  1248338.412 ? 1145549.327  ns/op
> GetAndUpdateBench3.shade:getAndUpdate1_shade             10  avgt   20      146.543 ?       0.662  ns/op
> GetAndUpdateBench3.shade:getAndUpdate2_shade             10  avgt   20      146.644 ?       0.562  ns/op
> GetAndUpdateBench3.shade:total                           10  avgt   20      145.162 ?       2.839  ns/op
> GetAndUpdateBench3.shade                                 20  avgt   20      173.579 ?       0.901  ns/op
> GetAndUpdateBench3.shade:case1                           20  avgt   20      173.500 ?       0.842  ns/op
> GetAndUpdateBench3.shade:case2                           20  avgt   20   916952.445 ? 1147957.020  ns/op
> GetAndUpdateBench3.shade:getAndUpdate1_shade             20  avgt   20      173.498 ?       1.133  ns/op
> GetAndUpdateBench3.shade:getAndUpdate2_shade             20  avgt   20      173.659 ?       0.728  ns/op
> GetAndUpdateBench3.shade:total                           20  avgt   20      172.134 ?       2.710  ns/op
> GetAndUpdateBench3.shade                                 50  avgt   20      300.589 ?      23.422  ns/op
> GetAndUpdateBench3.shade:case1                           50  avgt   20      300.537 ?      23.457  ns/op
> GetAndUpdateBench3.shade:case2                           50  avgt   20  6912133.566 ? 4212701.775  ns/op
> GetAndUpdateBench3.shade:getAndUpdate1_shade             50  avgt   20      300.559 ?      23.396  ns/op
> GetAndUpdateBench3.shade:getAndUpdate2_shade             50  avgt   20      300.618 ?      23.452  ns/op
> GetAndUpdateBench3.shade:total                           50  avgt   20      292.863 ?      15.469  ns/op
> GetAndUpdateBench3.shade                                100  avgt   20      453.153 ?       7.168  ns/op
> GetAndUpdateBench3.shade:case1                          100  avgt   20      452.858 ?       6.940  ns/op
> GetAndUpdateBench3.shade:case2                          100  avgt   20  1877949.365 ? 2351908.185  ns/op
> GetAndUpdateBench3.shade:getAndUpdate1_shade            100  avgt   20      452.429 ?       6.266  ns/op
> GetAndUpdateBench3.shade:getAndUpdate2_shade            100  avgt   20      453.877 ?       8.503  ns/op
> GetAndUpdateBench3.shade:total                          100  avgt   20      446.519 ?       3.611  ns/op

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/95ddb135/attachment.html>

From paul.sandoz at oracle.com  Wed Sep 28 16:02:40 2016
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Wed, 28 Sep 2016 13:02:40 -0700
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
Message-ID: <971EC089-7BC5-44A5-A9DC-C9869D2E58B4@oracle.com>


> On 28 Sep 2016, at 06:08, Andrew Haley <aph at redhat.com> wrote:
> 
> I did a few experiments, with simpler code.
> 
> The version with the direct byte buffer was not very efficient: it
> does many reads of memory unrelated to the actual thing we're trying
> to test.  This is a shame, given that one of the goals of JEP 193 is
> that the API should be at least as good as Unsafe.  Right now we'e a
> long way from that goal.
> 

Alas, it’s tricky to do better while retaining safe access, there are:

- bounds checks;
- read only checks;
- alignment checks; and
- that the buffer is effectively a box of the address (base & offset, where base == null for off-heap).

When looping the checks can be hoisted, and unrolling should result in efficient addressing. So, e.g. for plain access, a the generated hot-loop is similar to that as if Unsafe was directly used.

Project Panama is likely to run into similar challenges with it’s Pointer and (Bounded)MemoryRegion.

Paul.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/e9c88a67/attachment-0001.sig>

From david.holmes at oracle.com  Wed Sep 28 20:42:04 2016
From: david.holmes at oracle.com (David Holmes)
Date: Thu, 29 Sep 2016 10:42:04 +1000
Subject: [concurrency-interest] RFR: 8166842: String.hashCode() has a
	non-benign data race
In-Reply-To: <4d006383-f1ba-8c04-37a5-5436f43aaa78@gmail.com>
References: <308e3b89-7de5-ea24-2df4-a4c529de36a5@gmail.com>
 <091c3433-c878-91b4-5568-04f8105e698c@oracle.com>
 <4d006383-f1ba-8c04-37a5-5436f43aaa78@gmail.com>
Message-ID: <c8ea0b63-3b3d-283b-8822-d472ac24103a@oracle.com>

On 28/09/2016 11:24 PM, Peter Levart wrote:
>
> On 09/28/2016 03:05 PM, David Holmes wrote:
>> On 28/09/2016 10:44 PM, Peter Levart wrote:
>>> Hi,
>>>
>>> According to discussion here:
>>>
>>> http://cs.oswego.edu/pipermail/concurrency-interest/2016-September/015414.html
>>>
>>> it seems compact strings introduced (at least theoretical) non-benign
>>> data race into String.hasCode() method.
>>>
>>> Here is a proposed patch:
>>>
>>> http://cr.openjdk.java.net/~plevart/jdk9-dev/8166842_String.hashCode/webrev.01/
>>>
>>
>> I'm far from convinced that the bug exists - theoretical or otherwise
>> - but the "fix" is harmless.
>>
>> When we were working on JSR-133 one of the conceptual models is that
>> every write to a variable goes into the set of values that a read may
>> potentially return (so no out-of-thin-air for example). happens-before
>> establishes constraints on which value can legally be returned - the
>> most recent. An additional property was that once a value was
>> returned, a later read could not return an earlier value - in essence
>> once a read returns a given value, all earlier written values are
>> removed from the set of potential values that can be read.
>
> That would be a nice property, yes.
>
>>
>> Your bug requires that the code act as-if written:
>>
>> int temp = hash;
>> if (temp == 0) {
>>    hash = ...
>> }
>> return temp;
>>
>> and I do not believe that is allowed.
>>
>> David
>
> Well, I can't find anything like that in JMM description. Could you
> point me to it? Above example only reads once from hash. The code in
> question is this:
>
> if (hash == 0) { // 1st read
>     hash = ...
> }
> return hash; // 2nd read
>
> And the bug requires the code to act like:
>
> int temp2 = hash; // 2nd read
> int temp1 = hash; // 1st read
> if (temp1 == 0) {
>     return (hash = ...);
> }
> return temp2;

Given hash is not a volatile field there is nothing that requires the 
compiler to emit code that reads it twice - of course javac will issue 
getfields as per the source code.

So the compiler is of course allowed to read it twice, but you then want 
it to reorder those reads such that the second use uses the value of the 
first read, and the first use uses the value of the second read.

I recall discussions of such perverse compiler behaviour in the past. :(

> Is this allowed?

Sadly yes. What we ended up with in JLS Ch17 permits this - it is a 
variation of the aliasing example in 17.4.

There are places where the formalization of the JMM "threw the baby out 
with the bath water" IMHO. Some of the less formal descriptions made far 
more sense - like the notion of there being a set of values a read may 
return, and how happens-before constrains that, and how once a value is 
returned you cant then "read" an earlier value. <sigh>

Thanks,
David

> Regards, Peter
>
>>
>>>
>>> For the bug:
>>>
>>>     https://bugs.openjdk.java.net/browse/JDK-8166842
>>>
>>>
>>>
>>> JDK 8 did not have this problem, so no back-porting necessary.
>>>
>>> Regards, Peter
>>>
>

From vitalyd at gmail.com  Wed Sep 28 21:05:32 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Sep 2016 21:05:32 -0400
Subject: [concurrency-interest] RFR: 8166842: String.hashCode() has a
	non-benign data race
In-Reply-To: <c8ea0b63-3b3d-283b-8822-d472ac24103a@oracle.com>
References: <308e3b89-7de5-ea24-2df4-a4c529de36a5@gmail.com>
 <091c3433-c878-91b4-5568-04f8105e698c@oracle.com>
 <4d006383-f1ba-8c04-37a5-5436f43aaa78@gmail.com>
 <c8ea0b63-3b3d-283b-8822-d472ac24103a@oracle.com>
Message-ID: <CAHjP37GEGgE28DbcywfhzJdvkcchaXwxz1h2O2Pv1h1c2hQwSg@mail.gmail.com>

On Wednesday, September 28, 2016, David Holmes <david.holmes at oracle.com>
wrote:

> On 28/09/2016 11:24 PM, Peter Levart wrote:
>
>>
>> On 09/28/2016 03:05 PM, David Holmes wrote:
>>
>>> On 28/09/2016 10:44 PM, Peter Levart wrote:
>>>
>>>> Hi,
>>>>
>>>> According to discussion here:
>>>>
>>>> http://cs.oswego.edu/pipermail/concurrency-interest/2016-
>>>> September/015414.html
>>>>
>>>> it seems compact strings introduced (at least theoretical) non-benign
>>>> data race into String.hasCode() method.
>>>>
>>>> Here is a proposed patch:
>>>>
>>>> http://cr.openjdk.java.net/~plevart/jdk9-dev/8166842_String.
>>>> hashCode/webrev.01/
>>>>
>>>>
>>> I'm far from convinced that the bug exists - theoretical or otherwise
>>> - but the "fix" is harmless.
>>>
>>> When we were working on JSR-133 one of the conceptual models is that
>>> every write to a variable goes into the set of values that a read may
>>> potentially return (so no out-of-thin-air for example). happens-before
>>> establishes constraints on which value can legally be returned - the
>>> most recent. An additional property was that once a value was
>>> returned, a later read could not return an earlier value - in essence
>>> once a read returns a given value, all earlier written values are
>>> removed from the set of potential values that can be read.
>>>
>>
>> That would be a nice property, yes.
>>
>>
>>> Your bug requires that the code act as-if written:
>>>
>>> int temp = hash;
>>> if (temp == 0) {
>>>    hash = ...
>>> }
>>> return temp;
>>>
>>> and I do not believe that is allowed.
>>>
>>> David
>>>
>>
>> Well, I can't find anything like that in JMM description. Could you
>> point me to it? Above example only reads once from hash. The code in
>> question is this:
>>
>> if (hash == 0) { // 1st read
>>     hash = ...
>> }
>> return hash; // 2nd read
>>
>> And the bug requires the code to act like:
>>
>> int temp2 = hash; // 2nd read
>> int temp1 = hash; // 1st read
>> if (temp1 == 0) {
>>     return (hash = ...);
>> }
>> return temp2;
>>
>
> Given hash is not a volatile field there is nothing that requires the
> compiler to emit code that reads it twice - of course javac will issue
> getfields as per the source code.

I don't think there's anything preventing it from reading it any number of
times so long as intra-thread semantics are preserved.

>
> So the compiler is of course allowed to read it twice, but you then want
> it to reorder those reads such that the second use uses the value of the
> first read, and the first use uses the value of the second read.

I gave a pseudo example upthread.  It would be weird codegen but allowed.

>
> I recall discussions of such perverse compiler behaviour in the past. :(

Be thankful that at least an explicit read into a local is preserved - C++
compilers are allowed to reload in that case too unless the local is marked
volatile.

None of this is an issue when not using data races.  If you choose to
engage in a data race, gotta play by the rules :).

>
> Is this allowed?
>>
>
> Sadly yes. What we ended up with in JLS Ch17 permits this - it is a
> variation of the aliasing example in 17.4.
>
> There are places where the formalization of the JMM "threw the baby out
> with the bath water" IMHO. Some of the less formal descriptions made far
> more sense - like the notion of there being a set of values a read may
> return, and how happens-before constrains that, and how once a value is
> returned you cant then "read" an earlier value. <sigh>
>
> Thanks,
> David
>
> Regards, Peter
>>
>>
>>>
>>>> For the bug:
>>>>
>>>>     https://bugs.openjdk.java.net/browse/JDK-8166842
>>>>
>>>>
>>>>
>>>> JDK 8 did not have this problem, so no back-porting necessary.
>>>>
>>>> Regards, Peter
>>>>
>>>>
>>

-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/9e557ece/attachment.html>

From vitalyd at gmail.com  Wed Sep 28 21:26:03 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Sep 2016 21:26:03 -0400
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <971EC089-7BC5-44A5-A9DC-C9869D2E58B4@oracle.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <971EC089-7BC5-44A5-A9DC-C9869D2E58B4@oracle.com>
Message-ID: <CAHjP37EkwTzwtQL7SbeO7AVYUvOeZts4=+nUpNMtQbAYq3pJmw@mail.gmail.com>

On Wednesday, September 28, 2016, Paul Sandoz <paul.sandoz at oracle.com>
wrote:

>
> > On 28 Sep 2016, at 06:08, Andrew Haley <aph at redhat.com <javascript:;>>
> wrote:
> >
> > I did a few experiments, with simpler code.
> >
> > The version with the direct byte buffer was not very efficient: it
> > does many reads of memory unrelated to the actual thing we're trying
> > to test.  This is a shame, given that one of the goals of JEP 193 is
> > that the API should be at least as good as Unsafe.  Right now we'e a
> > long way from that goal.
> >
>
> Alas, it’s tricky to do better while retaining safe access, there are:
>
> - bounds checks;
> - read only checks;
> - alignment checks; and
> - that the buffer is effectively a box of the address (base & offset,
> where base == null for off-heap).
>
> When looping the checks can be hoisted, and unrolling should result in
> efficient addressing. So, e.g. for plain access, a the generated hot-loop
> is similar to that as if Unsafe was directly used.

So maybe we do need Unsafe after all? :) I mean it's fairly clear that
safety checks can only be amortized when compiler is dealing with them in
bulk - should work well for loops (assuming inlining doesn't fail),
although OSR wouldn't I think.

But what about non-loop cases or when compiler's compilation horizon
doesn't see the loop? Other languages have unsafe escape hatches for when
you really want to subvert the system because "you know better".  .NET is a
close analog of a managed safe runtime, and it has it.  Perhaps something
needs to be available for those cases? Unsafe has kind of been it thus far.

>
> Project Panama is likely to run into similar challenges with it’s Pointer
> and (Bounded)MemoryRegion.
>
> Paul.
>


-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160928/5d8a7829/attachment.html>

From yaojingguo at gmail.com  Wed Sep 28 21:55:43 2016
From: yaojingguo at gmail.com (jingguo yao)
Date: Thu, 29 Sep 2016 09:55:43 +0800
Subject: [concurrency-interest] What can an incorrectly synchronized
	read see?
In-Reply-To: <81B68E94-4ED5-4388-83D4-DBDC527C9A9F@gmail.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
 <81B68E94-4ED5-4388-83D4-DBDC527C9A9F@gmail.com>
Message-ID: <CANPB7a7ypD4JUBMKBuRcbx1F1CtqK5we6X0HB3OUjUcT3x=DwQ@mail.gmail.com>

Alex:


I like your explanation. Thanks.

2016-09-29 3:23 GMT+08:00 Alex Otenko <oleksandr.otenko at gmail.com>:

> This reasoning is actually quite complicated, and the original reasoning
> is ok, it just needs statements justifying the claims about happens-before
> between unsynchronizedGet and increments.
>
> 1. All lock acquires and lock releases are in total order and establish
> happens-before edges between lock releases and subsequent lock acquires
> (definition of synchronization order)
>
> 2. All statements in program order before a lock release happen-before any
> statements that are in program order after the lock acquires following that
> lock release (definition of transitive closure)
>
> This is justification enough for the existence of happens-before between
> unsynchronizedGet and some increments - ie there are writes
> unsynchronizedGet cannot observe.
>
>
> 3. unsynchronizedGet cannot observe the writes that are after subsequent
> synchronizedGet in total synchronization order (slightly simplified
> statement, since synchronization order will only contain lock
> acquires/releases, not the whole operations).
>
> 4. Therefore, unsynchronizedGet observes one of the writes that precede
> the synchronizedGet in total synchronization order.
>
> 5. unsynchronizedGet either observes the same write that the
> synchronizedGet observes, or some preceding write. Trivially, the values
> seen by unsynchronizedGet are less or equal the values seen by
> synchronizedGet.
>
>
> Alex
>
> > On 28 Sep 2016, at 09:22, Aleksey Shipilev <shade at redhat.com> wrote:
> >
> > Hi,
> >
> > On 09/28/2016 09:23 AM, jingguo yao wrote:
> >> I think that unsynchronizedCount <= synchronizedCount is always true
> >> for the above code provided that JVM implementations of read and write
> >> operations for int type are atomic.
> >
> > They are atomic, mandated by JLS.
> >
> >
> >> When increment happens-before synchronizedGet, synchronizedGet sees 1.
> >> And there is no happens-before order to prevent unsynchronizedGet from
> >> seeing initial_write or increment. So unsynchronizedGet sees1 0 or 1.
> >>
> >> Happens-before digram for this case:
> >>
> >>  initial_write ---> increment ---> synchronizedGet
> >>                                      ^
> >>              unsynchronizedGet------/
> >>
> >> When synchronizedGet happens-before increment, synchronizedGet sees 0.
> Since
> >> unsynchronizedGet happens-before increment, it can't see 1 written
> >> by increment. It can only see 0 written by initial_write.
> >>
> >> Happens-before digram for this case:
> >>
> >>  initial_write ---> synchronizedGet ---> increment
> >>                        ^
> >>  unsynchronizedGet----/
> >>
> >> Is my reasoning correct?
> >
> > Not quite, because I think it confuses the notion of program statements
> > and actions in JMM. The analysis should use JMM rules as stated, like
> below.
> >
> > Happens-before consistency says that in valid executions, the read has
> > two relations with writes: a) with the writes that are tied in
> > happens-before with the read in question, the read can see only the
> > *latest* write in HB -- not the one before it, not the one after the
> > read in HB; b) with the writes that are not tied in happens-before with
> > the read in question, the read can see whatever such write.
> >
> > So, these both are valid executions under JMM:
> >
> >                               r(val):0 --po-\
> >                                             |
> > w(val, 1) --po/hb--> unlck(m) --sw/hb--> lck(m) --po/hb--> r(val):1
> >
> >
> >                               r(val):1 --po-\
> >                                             |
> > w(val, 1) --po/hb--> unlck(m) --sw/hb--> lck(m) --po/hb--> r(val):1
> >
> > ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> >                               ^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^
> >                                 unsynch                 synch
> >     parts done by Writer              parts done by Reader
> >
> >
> > Where:
> >  w(F,V) is the write of value "V" to field "F"
> >  r(F):V is the read of value "V" from field "F"
> >  lck/unlck(M) are the lock and unlock of the monitor M
> >  --po/hb--> is the happens-before edge induced by program order
> >  --sw/hb--> is the happens-before edge induced by synchronizes-with
> >  --po--> is "just" the program order
> >
> > Note that unsynchronized read can read either "0" or "1", regardless of
> > if synchronized sees "1" after it.
> >
> > Can the unsynchronized see "1", but the synchronized see "0"? Now, this
> > is where it gets interesting. We cannot use the execution from above to
> > justify reading synchronized "0", because synchronizes-with mandates
> > seeing "1". Therefore, we need to reverse the order of Writer and Reader:
> >
> >
> >                  parts done by Reader
> >    unsync                                  synch
> > vvvvvvvvvvvvvv       vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
> > vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
> >
> > r(val):V1 --po/hb--> lck(m) --po/hb--> r(val):V2 --po/hb--> unlck(m)
> >                                                              /
> >      /-----------------------sw/hb--------------------------/
> >      v
> >    lck(m) --po/hb--> w(val, 1)
> >
> > ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> >    parts done by Writer
> >
> > Now, let's see what (V1, V2) combinations do not violate JMM:
> > (0, 0) -- ok, both see some (default?) "0" write from before
> > (1, 0) -- NOT OK, r(val):V1 cannot see the w(val,1) that's after in HB!
> > (0, 1) and
> > (1, 1) -- NOT OK for this particular execution, because r(val):V2 sees
> > the "future" write w(val,1). Notice, however, that both these results
> > are justified by the execution at the very beginning, when Writer and
> > Reader are sequenced in another order.
> >
> > Hope this helps.
> >
> > Thanks,
> > -Aleksey
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Jingguo
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160929/ff873bab/attachment-0001.html>

From yaojingguo at gmail.com  Wed Sep 28 22:05:02 2016
From: yaojingguo at gmail.com (jingguo yao)
Date: Thu, 29 Sep 2016 10:05:02 +0800
Subject: [concurrency-interest] What can an incorrectly synchronized
	read see?
In-Reply-To: <7a44ae1d-1473-101c-1b27-8dee701ca314@redhat.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
 <008d01d21965$a80050f0$f800f2d0$@aapt.net.au>
 <16fab73e-467c-d1ca-1743-adff0db7a9d5@redhat.com>
 <9838003A10254741BC6FFADE2ED02B457D5E7346@OZWEX0205N1.msad.ms.com>
 <7a44ae1d-1473-101c-1b27-8dee701ca314@redhat.com>
Message-ID: <CANPB7a53DSvM2jJxc-1FAk=2ogGZm+n_2mjnhbcDzpaZFWX-XA@mail.gmail.com>

Aleksey:

Your reasoning is much more formal. Thanks.

2016-09-28 18:55 GMT+08:00 Aleksey Shipilev <shade at redhat.com>:

> On 09/28/2016 12:47 PM, Bobrowski, Maciej wrote:
> > Right, so the only invalid execution in any possible sequencing is
> > the (1, 0) which was the main question of this post, correct?
>
> Correct. I wanted to show how to apply spec to arrive to this result,
> and you get the collateral results for free too :)
>
> NB: Mental trap: believing that arriving to the same result as
> specification validates the alternative reasoning. (It might, though,
> but you have to show that alternative reasoning gives correct answers
> for all other cases too and/or has the clear boundaries where it applies)
>
> Thanks,
> -Aleksey
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Jingguo
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160929/3ecb9f5f/attachment.html>

From oleksandr.otenko at gmail.com  Thu Sep 29 03:13:03 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 29 Sep 2016 08:13:03 +0100
Subject: [concurrency-interest] What can an incorrectly synchronized
	read see?
In-Reply-To: <CANPB7a53DSvM2jJxc-1FAk=2ogGZm+n_2mjnhbcDzpaZFWX-XA@mail.gmail.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
 <008d01d21965$a80050f0$f800f2d0$@aapt.net.au>
 <16fab73e-467c-d1ca-1743-adff0db7a9d5@redhat.com>
 <9838003A10254741BC6FFADE2ED02B457D5E7346@OZWEX0205N1.msad.ms.com>
 <7a44ae1d-1473-101c-1b27-8dee701ca314@redhat.com>
 <CANPB7a53DSvM2jJxc-1FAk=2ogGZm+n_2mjnhbcDzpaZFWX-XA@mail.gmail.com>
Message-ID: <015D9FB5-355C-411F-9D74-AA4CCD9ADF82@gmail.com>

You need to be careful about using a “proof by example” as a proof. It is ok to use it to demonstrate what happens to two reads of two writes, but it is not a proof about all reads and all writes.

It is not as formal as it looks. How do you conclude it is sufficient to consider just two diagrams? How do you conclude it is sufficient to consider just four outcomes?

Is (500100, 100500) an allowed outcome? No? Why not?

You need to start reasoning how to arrive at the conclusion about that outcome from one of the four outcomes considered. You may discover then that you are using mechanisms that do not care about the four outcomes at all.


Alex

> On 29 Sep 2016, at 03:05, jingguo yao <yaojingguo at gmail.com> wrote:
> 
> Aleksey:
> 
> Your reasoning is much more formal. Thanks.
> 
> 2016-09-28 18:55 GMT+08:00 Aleksey Shipilev <shade at redhat.com <mailto:shade at redhat.com>>:
> On 09/28/2016 12:47 PM, Bobrowski, Maciej wrote:
> > Right, so the only invalid execution in any possible sequencing is
> > the (1, 0) which was the main question of this post, correct?
> 
> Correct. I wanted to show how to apply spec to arrive to this result,
> and you get the collateral results for free too :)
> 
> NB: Mental trap: believing that arriving to the same result as
> specification validates the alternative reasoning. (It might, though,
> but you have to show that alternative reasoning gives correct answers
> for all other cases too and/or has the clear boundaries where it applies)
> 
> Thanks,
> -Aleksey
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> 
> 
> 
> -- 
> Jingguo
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160929/0c5ccd40/attachment.html>

From shade at redhat.com  Thu Sep 29 04:05:10 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Thu, 29 Sep 2016 10:05:10 +0200
Subject: [concurrency-interest] What can an incorrectly synchronized
 read see?
In-Reply-To: <015D9FB5-355C-411F-9D74-AA4CCD9ADF82@gmail.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
 <008d01d21965$a80050f0$f800f2d0$@aapt.net.au>
 <16fab73e-467c-d1ca-1743-adff0db7a9d5@redhat.com>
 <9838003A10254741BC6FFADE2ED02B457D5E7346@OZWEX0205N1.msad.ms.com>
 <7a44ae1d-1473-101c-1b27-8dee701ca314@redhat.com>
 <CANPB7a53DSvM2jJxc-1FAk=2ogGZm+n_2mjnhbcDzpaZFWX-XA@mail.gmail.com>
 <015D9FB5-355C-411F-9D74-AA4CCD9ADF82@gmail.com>
Message-ID: <365fa874-6684-fca3-aceb-971aeb1381bb@redhat.com>

On 09/29/2016 09:13 AM, Alex Otenko wrote:
> You need to be careful about using a “proof by example” as a proof. It
> is ok to use it to demonstrate what happens to two reads of two writes,
> but it is not a proof about *all* reads and *all* writes.
> 
> It is not as formal as it looks. How do you conclude it is sufficient to
> consider just two diagrams? How do you conclude it is sufficient to
> consider just four outcomes?

If you look closely, my proof basically shows the same properties you
picked: actions that happened-before lock acquisition in one thread
cannot see the updates happening in after lock acquisition in another
thread. For the same reason your interpretation uses -- the total
ordering of locks/unlocks, plus transitivity -- we are free to only look
for two cases of relative sequencing of Reader and Writer, either R->W,
or W->R, and everything else is the extension of these two basic patterns.

But I would be very, very careful with interpretations that talk about
statements, and not the program actions with the actual values observed.
Because this is subtly broken: "All statements in program order before a
lock release happen-before any statements that are in program order
after the lock acquires following that lock release"

Statements do not form a program order. Program order is total, and
statements are not forming total order (easy example: control flow split
at conditionals). As stated, in this example:

Thread 1:
  if (false) // more complicated predicate in real-life case, of course
    y = 1
  sync(this) {
    w = 1
  }

Thread 2:
  sync(this) {
    r1 = w;
    r2 = y;
  }


...your interpretation says the "statement" y=1 happens-before r2=y,
which mandates seeing (r1, r2) = (1, 1), which is obviously incorrect.
This is an easy mistake that everyone makes when talking about JMM [1],
and it does not help that we keep perpetuating it with inaccurate claims
on public lists.

Thanks,
-Aleksey

[1]
https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-all-my-hb

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160929/7609305d/attachment.sig>

From aph at redhat.com  Thu Sep 29 04:32:51 2016
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Sep 2016 09:32:51 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <971EC089-7BC5-44A5-A9DC-C9869D2E58B4@oracle.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <971EC089-7BC5-44A5-A9DC-C9869D2E58B4@oracle.com>
Message-ID: <b7d70c0d-8ef3-40d2-73f3-938a17dcd5e5@redhat.com>

On 28/09/16 21:02, Paul Sandoz wrote:
> 
> Alas, it’s tricky to do better while retaining safe access, there are:
> 
> - bounds checks;
> - read only checks;
> - alignment checks; and
> - that the buffer is effectively a box of the address (base &
> offset, where base == null for off-heap).
> 
> When looping the checks can be hoisted, and unrolling should result
> in efficient addressing. So, e.g. for plain access, a the generated
> hot-loop is similar to that as if Unsafe was directly used.

Right, I get that.  So this is a slightly artificial case: if we'd been
doing a few thousand accesses to the same buffer we'd perhaps see
something which better represents real-life Java.

Andrew.

From aph at redhat.com  Thu Sep 29 04:36:32 2016
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Sep 2016 09:36:32 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CAHjP37EkwTzwtQL7SbeO7AVYUvOeZts4=+nUpNMtQbAYq3pJmw@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <971EC089-7BC5-44A5-A9DC-C9869D2E58B4@oracle.com>
 <CAHjP37EkwTzwtQL7SbeO7AVYUvOeZts4=+nUpNMtQbAYq3pJmw@mail.gmail.com>
Message-ID: <ead5c647-2efa-3aa6-29a7-65fa965d7d65@redhat.com>

On 29/09/16 02:26, Vitaly Davidovich wrote:
> 
>     Alas, it’s tricky to do better while retaining safe access, there are:
> 
>     - bounds checks;
>     - read only checks;
>     - alignment checks; and
>     - that the buffer is effectively a box of the address (base &
>     offset, where base == null for off-heap).
> 
>     When looping the checks can be hoisted, and unrolling should
>     result in efficient addressing. So, e.g. for plain access, a the
>     generated hot-loop is similar to that as if Unsafe was directly
>     used.
> 
> So maybe we do need Unsafe after all? :) I mean it's fairly clear
> that safety checks can only be amortized when compiler is dealing
> with them in bulk - should work well for loops (assuming inlining
> doesn't fail), although OSR wouldn't I think.
> 
> But what about non-loop cases or when compiler's compilation horizon
> doesn't see the loop? Other languages have unsafe escape hatches for
> when you really want to subvert the system because "you know
> better".

Well, some do.  I'd argue that you need high-speed access to raw
memory in the cases when you have a lot of bulk data.  And in those
cases a skilled programmer can work with JVM to make sure that the
compiler gets what it needs to do a good job.

Andrew.

From aph at redhat.com  Thu Sep 29 04:36:53 2016
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Sep 2016 09:36:53 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <9bc73165-a3f6-3d02-a674-32fcc8e9f251@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <8cbcbf07-a2d3-68a9-aabf-112303382d62@gmail.com>
 <74fba78f-7138-8a51-4acf-6281e794764a@redhat.com>
 <9bc73165-a3f6-3d02-a674-32fcc8e9f251@redhat.com>
Message-ID: <8b55ff04-0e70-10d5-f57d-2ddb4225968f@redhat.com>

On 28/09/16 18:55, Andrew Haley wrote:
> The only really likely explanation for that is that the thread is
> descheduled or otherwise interrupted by the OS kernel and this
> obviously would cause a weak CAS to fail.

Oh, silly me!  This is a deoptimization trap, of course.

Andrew.


From oleksandr.otenko at gmail.com  Thu Sep 29 08:16:00 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 29 Sep 2016 13:16:00 +0100
Subject: [concurrency-interest] What can an incorrectly synchronized
	read see?
In-Reply-To: <365fa874-6684-fca3-aceb-971aeb1381bb@redhat.com>
References: <CANPB7a4-FVO0iu0WL+XxYAjDZT8nnDqAmzVt7_8mwzSLyv6HDQ@mail.gmail.com>
 <a71d804b-f3b2-3be9-a52c-138d6a011241@redhat.com>
 <008d01d21965$a80050f0$f800f2d0$@aapt.net.au>
 <16fab73e-467c-d1ca-1743-adff0db7a9d5@redhat.com>
 <9838003A10254741BC6FFADE2ED02B457D5E7346@OZWEX0205N1.msad.ms.com>
 <7a44ae1d-1473-101c-1b27-8dee701ca314@redhat.com>
 <CANPB7a53DSvM2jJxc-1FAk=2ogGZm+n_2mjnhbcDzpaZFWX-XA@mail.gmail.com>
 <015D9FB5-355C-411F-9D74-AA4CCD9ADF82@gmail.com>
 <365fa874-6684-fca3-aceb-971aeb1381bb@redhat.com>
Message-ID: <E9A453BF-E1FF-454F-AF7E-FD0D625BFC45@gmail.com>

Sure. I do mean “statements that got executed”.

The “enumerate all possible outcomes” approach can work for other cases - CAS succeeded or not; lock acquired or not; pointer set or not (even then you’d need a method to model ABA). But integer counters are more complex than that, and need a sound method of extending the enumeration of a few outcomes to all integers.


> On 29 Sep 2016, at 09:05, Aleksey Shipilev <shade at redhat.com> wrote:
> 
> On 09/29/2016 09:13 AM, Alex Otenko wrote:
>> You need to be careful about using a “proof by example” as a proof. It
>> is ok to use it to demonstrate what happens to two reads of two writes,
>> but it is not a proof about *all* reads and *all* writes.
>> 
>> It is not as formal as it looks. How do you conclude it is sufficient to
>> consider just two diagrams? How do you conclude it is sufficient to
>> consider just four outcomes?
> 
> If you look closely, my proof basically shows the same properties you
> picked: actions that happened-before lock acquisition in one thread
> cannot see the updates happening in after lock acquisition in another
> thread. For the same reason your interpretation uses -- the total
> ordering of locks/unlocks, plus transitivity -- we are free to only look
> for two cases of relative sequencing of Reader and Writer, either R->W,
> or W->R, and everything else is the extension of these two basic patterns.

Sure, in the end it will have to rely on the same postulates to extend the conclusion to all integers - that’s all the postulates we can use, no magic there. I am just pointing out that the method does not tell us how we can jump from “this works for 0 and 1” to “this works for all integers”.

“pattern” is not part of JMM, so something else is needed.

Say, is (3,1) allowed? Now you need to look at R->W->W->W, W->R->W->W, W->W->R->W, W->W->W->R, provide a method of determining these are the only cases we need to consider (say, why W->W->W->R->W is irrelevant), and provide a method of reducing them to the previously considered cases before we can conclude that “everything else is the extension”.


I am not trying to say there is no way to do that (although I wouldn’t approach the problem that way), or that “enumerate all possible outcomes” is not practical - after all, that’s the only automated validation we have - I am merely pointing out that it is not a rigorous proof for the general question posed, and sweeps important details under the carpet.


Alex


> But I would be very, very careful with interpretations that talk about
> statements, and not the program actions with the actual values observed.
> Because this is subtly broken: "All statements in program order before a
> lock release happen-before any statements that are in program order
> after the lock acquires following that lock release"
> 
> Statements do not form a program order. Program order is total, and
> statements are not forming total order (easy example: control flow split
> at conditionals). As stated, in this example:
> 
> Thread 1:
>  if (false) // more complicated predicate in real-life case, of course
>    y = 1
>  sync(this) {
>    w = 1
>  }
> 
> Thread 2:
>  sync(this) {
>    r1 = w;
>    r2 = y;
>  }
> 
> 
> ...your interpretation says the "statement" y=1 happens-before r2=y,
> which mandates seeing (r1, r2) = (1, 1), which is obviously incorrect.
> This is an easy mistake that everyone makes when talking about JMM [1],
> and it does not help that we keep perpetuating it with inaccurate claims
> on public lists.
> 
> Thanks,
> -Aleksey
> 
> [1]
> https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-all-my-hb
> 


From vitalyd at gmail.com  Thu Sep 29 09:53:54 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Sep 2016 09:53:54 -0400
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <ead5c647-2efa-3aa6-29a7-65fa965d7d65@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <971EC089-7BC5-44A5-A9DC-C9869D2E58B4@oracle.com>
 <CAHjP37EkwTzwtQL7SbeO7AVYUvOeZts4=+nUpNMtQbAYq3pJmw@mail.gmail.com>
 <ead5c647-2efa-3aa6-29a7-65fa965d7d65@redhat.com>
Message-ID: <CAHjP37EtxwX2qqG0OdQw-T_OdjW4Er4QBtvPLpOipUbPGvWvfA@mail.gmail.com>

On Thursday, September 29, 2016, Andrew Haley <aph at redhat.com> wrote:

> On 29/09/16 02:26, Vitaly Davidovich wrote:
> >
> >     Alas, it’s tricky to do better while retaining safe access, there
> are:
> >
> >     - bounds checks;
> >     - read only checks;
> >     - alignment checks; and
> >     - that the buffer is effectively a box of the address (base &
> >     offset, where base == null for off-heap).
> >
> >     When looping the checks can be hoisted, and unrolling should
> >     result in efficient addressing. So, e.g. for plain access, a the
> >     generated hot-loop is similar to that as if Unsafe was directly
> >     used.
> >
> > So maybe we do need Unsafe after all? :) I mean it's fairly clear
> > that safety checks can only be amortized when compiler is dealing
> > with them in bulk - should work well for loops (assuming inlining
> > doesn't fail), although OSR wouldn't I think.
> >
> > But what about non-loop cases or when compiler's compilation horizon
> > doesn't see the loop? Other languages have unsafe escape hatches for
> > when you really want to subvert the system because "you know
> > better".
>
> Well, some do.  I'd argue that you need high-speed access to raw
> memory in the cases when you have a lot of bulk data.  And in those
> cases a skilled programmer can work with JVM to make sure that the
> compiler gets what it needs to do a good job.

This argument leans heavily on the Sufficiently Smart Compiler fallacy.  In
particular, "bulk data" or looping might be hidden from compiler's
optimization horizon.

>
> Andrew.
>


-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160929/c407289e/attachment.html>

From aph at redhat.com  Thu Sep 29 10:07:59 2016
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Sep 2016 15:07:59 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CAHjP37EtxwX2qqG0OdQw-T_OdjW4Er4QBtvPLpOipUbPGvWvfA@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <971EC089-7BC5-44A5-A9DC-C9869D2E58B4@oracle.com>
 <CAHjP37EkwTzwtQL7SbeO7AVYUvOeZts4=+nUpNMtQbAYq3pJmw@mail.gmail.com>
 <ead5c647-2efa-3aa6-29a7-65fa965d7d65@redhat.com>
 <CAHjP37EtxwX2qqG0OdQw-T_OdjW4Er4QBtvPLpOipUbPGvWvfA@mail.gmail.com>
Message-ID: <0b15deb7-9c80-5599-efa8-54d48d615f8e@redhat.com>

On 29/09/16 14:53, Vitaly Davidovich wrote:
> On Thursday, September 29, 2016, Andrew Haley <aph at redhat.com> wrote:
> 
>> On 29/09/16 02:26, Vitaly Davidovich wrote:
>>>
>>>     Alas, it’s tricky to do better while retaining safe access, there
>> are:
>>>
>>>     - bounds checks;
>>>     - read only checks;
>>>     - alignment checks; and
>>>     - that the buffer is effectively a box of the address (base &
>>>     offset, where base == null for off-heap).
>>>
>>>     When looping the checks can be hoisted, and unrolling should
>>>     result in efficient addressing. So, e.g. for plain access, a the
>>>     generated hot-loop is similar to that as if Unsafe was directly
>>>     used.
>>>
>>> So maybe we do need Unsafe after all? :) I mean it's fairly clear
>>> that safety checks can only be amortized when compiler is dealing
>>> with them in bulk - should work well for loops (assuming inlining
>>> doesn't fail), although OSR wouldn't I think.
>>>
>>> But what about non-loop cases or when compiler's compilation horizon
>>> doesn't see the loop? Other languages have unsafe escape hatches for
>>> when you really want to subvert the system because "you know
>>> better".
>>
>> Well, some do.  I'd argue that you need high-speed access to raw
>> memory in the cases when you have a lot of bulk data.  And in those
>> cases a skilled programmer can work with JVM to make sure that the
>> compiler gets what it needs to do a good job.
> 
> This argument leans heavily on the Sufficiently Smart Compiler fallacy.  In
> particular, "bulk data" or looping might be hidden from compiler's
> optimization horizon.

Perhaps, but if so I would argue that getting good performance out of
HotSpot for anything relies on the Sufficiently Smart Compiler
fallacy.

Andrew.

From vitalyd at gmail.com  Thu Sep 29 10:28:28 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Sep 2016 10:28:28 -0400
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <0b15deb7-9c80-5599-efa8-54d48d615f8e@redhat.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <958a9533-b4cd-2dfc-8ede-c21c5b93ddf2@redhat.com>
 <CA+kOe0_DvSR1EoO1XzNp4-wrcUAa4vKxfSdT82dZ9_zysxMLHQ@mail.gmail.com>
 <A20729D9-6DAF-49E1-9E20-99106E62937B@oracle.com>
 <CA+kOe0-YmkX8_LnHx2xrjybDePvDt6Za0u7+zoh6D5aES86F5Q@mail.gmail.com>
 <d7c50804-0b09-7073-44db-792b6191d91b@gmail.com>
 <CDF3F1D7-4CBA-4713-B8CC-BC52A935E3B3@oracle.com>
 <00ffe52f-4a99-3784-1fdf-814d968e8794@gmail.com>
 <74b5e633-f6a8-ef93-44ae-4de1ff181614@gmail.com>
 <64d04e43-9262-802a-a7e2-64a46e45a169@redhat.com>
 <7c5b2596-bb0c-c5a8-2826-e9c8494b8a33@gmail.com>
 <4e942ba3-9a5e-e31d-d2d1-be107d0f7d7b@redhat.com>
 <144054f8-ad80-a973-0594-799d716a575e@redhat.com>
 <971EC089-7BC5-44A5-A9DC-C9869D2E58B4@oracle.com>
 <CAHjP37EkwTzwtQL7SbeO7AVYUvOeZts4=+nUpNMtQbAYq3pJmw@mail.gmail.com>
 <ead5c647-2efa-3aa6-29a7-65fa965d7d65@redhat.com>
 <CAHjP37EtxwX2qqG0OdQw-T_OdjW4Er4QBtvPLpOipUbPGvWvfA@mail.gmail.com>
 <0b15deb7-9c80-5599-efa8-54d48d615f8e@redhat.com>
Message-ID: <CAHjP37FB=UAXhZiZQK2y-Yoo4QKrO-jUn+-61U-L_duSoU3ZAw@mail.gmail.com>

On Thursday, September 29, 2016, Andrew Haley <aph at redhat.com> wrote:

> On 29/09/16 14:53, Vitaly Davidovich wrote:
> > On Thursday, September 29, 2016, Andrew Haley <aph at redhat.com
> <javascript:;>> wrote:
> >
> >> On 29/09/16 02:26, Vitaly Davidovich wrote:
> >>>
> >>>     Alas, it’s tricky to do better while retaining safe access, there
> >> are:
> >>>
> >>>     - bounds checks;
> >>>     - read only checks;
> >>>     - alignment checks; and
> >>>     - that the buffer is effectively a box of the address (base &
> >>>     offset, where base == null for off-heap).
> >>>
> >>>     When looping the checks can be hoisted, and unrolling should
> >>>     result in efficient addressing. So, e.g. for plain access, a the
> >>>     generated hot-loop is similar to that as if Unsafe was directly
> >>>     used.
> >>>
> >>> So maybe we do need Unsafe after all? :) I mean it's fairly clear
> >>> that safety checks can only be amortized when compiler is dealing
> >>> with them in bulk - should work well for loops (assuming inlining
> >>> doesn't fail), although OSR wouldn't I think.
> >>>
> >>> But what about non-loop cases or when compiler's compilation horizon
> >>> doesn't see the loop? Other languages have unsafe escape hatches for
> >>> when you really want to subvert the system because "you know
> >>> better".
> >>
> >> Well, some do.  I'd argue that you need high-speed access to raw
> >> memory in the cases when you have a lot of bulk data.  And in those
> >> cases a skilled programmer can work with JVM to make sure that the
> >> compiler gets what it needs to do a good job.
> >
> > This argument leans heavily on the Sufficiently Smart Compiler fallacy.
> In
> > particular, "bulk data" or looping might be hidden from compiler's
> > optimization horizon.
>
> Perhaps, but if so I would argue that getting good performance out of
> HotSpot for anything relies on the Sufficiently Smart Compiler
> fallacy.

That's exactly right.  But that's why escape hatches are needed to ensure
you get the codegen you expect, always.  Let's also not forget that due to
PGO and the associated heuristics in the compiler, codegen can differ run
to run.  This is typically not seen in microbenchmarks because they run
with a "clean" profile, have a fairly narrow set of CFGs, methods don't
typically fail to inline because they were inlined elsewhere and native
code is too big (i.e. InlineSmallCode cutoffs), etc.

Don't get me wrong - I think Hotspot is tremendous piece of engineering,
and it does a great job optimizing an otherwise performance-anemic
execution model.  But inevitably you need ways to take matters into your
own hands.

>
> Andrew.
>


-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160929/c01a8b87/attachment-0001.html>

From heinz at javaspecialists.eu  Thu Sep 29 15:54:58 2016
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 29 Sep 2016 22:54:58 +0300
Subject: [concurrency-interest] Concurrency Puzzle
Message-ID: <57ED7192.5020206@javaspecialists.eu>

Hi everyone,

I've written a newsletter with a little concurrency puzzle that you 
might find amusing to try.  It is not difficult to solve, but please 
don't post your solution for now.  The code is very obviously broken, 
but it fails a bit too predictably.  Enough said ...

http://www.javaspecialists.eu/archive/Issue241.html

Once you've found the reason why it fails so consistently, the next step 
is to fix the code using Java 8 StampedLock.

This is just a thought exercise, so please don't go putting this class 
into your code base :-)

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun/Oracle Java Champion since 2005
JavaOne Rock Star Speaker 2012
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz


From peter.levart at gmail.com  Fri Sep 30 11:42:45 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 30 Sep 2016 17:42:45 +0200
Subject: [concurrency-interest] RFR: 8166842: String.hashCode() has a
	non-benign data race
In-Reply-To: <CA+kOe0-JcTzWxqz+_uirxLwFftVFPgJSxAHE14-hU6Zg5yxsRA@mail.gmail.com>
References: <308e3b89-7de5-ea24-2df4-a4c529de36a5@gmail.com>
 <091c3433-c878-91b4-5568-04f8105e698c@oracle.com>
 <CAHjP37F5VFYQzMKwKn+sTjeoUCkZ7Cqu1Br5xUPB21B+qwbtJg@mail.gmail.com>
 <CANhc0PUcHLxiq+d_=eqgiVvbcHRjrQ92wwq38t+Mupts-=H=Cw@mail.gmail.com>
 <ca66a386-6f17-c58b-5007-aaf69ad9d5bc@oracle.com>
 <CANhc0PXVgYfguLBZkqvaF5-A_TAJrCuoSuP2o=xmwW_zK3iXLw@mail.gmail.com>
 <CAHjP37HhJwff5n330h9Tbg=qmeSYfrqUxuiEZ1=Zjs4gRieyQw@mail.gmail.com>
 <dffcf644-6ce6-d7f2-500a-b7ff2e12b684@gmail.com>
 <CA+kOe0-JcTzWxqz+_uirxLwFftVFPgJSxAHE14-hU6Zg5yxsRA@mail.gmail.com>
Message-ID: <81dc6019-344f-24a2-c121-00979259a52f@gmail.com>

Hi,

Thank you all for reviews and comments, especially instructive was Hans 
Boehm's explanation about why JMM must allow reordering of plain reads 
from the same location.

The fix is now pushed.

Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160930/9a2a7015/attachment.html>

