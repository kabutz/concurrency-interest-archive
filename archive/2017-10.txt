From ionutb83 at yahoo.com  Mon Oct  2 13:02:16 2017
From: ionutb83 at yahoo.com (Ionut)
Date: Mon, 2 Oct 2017 17:02:16 +0000 (UTC)
Subject: [concurrency-interest] Asynchronous event processing in Java
In-Reply-To: <512056112.167871.1506752466848@mail.yahoo.com>
References: <512056112.167871.1506752466848.ref@mail.yahoo.com>
 <512056112.167871.1506752466848@mail.yahoo.com>
Message-ID: <1548131330.1276922.1506963736706@mail.yahoo.com>

Hi All,
  Any thoughts on this? Btw, I am refering to standard Java API, not any 3rd party lib.
Thanks 
 
  On Sat, Sep 30, 2017 at 8:21, Ionut<ionutb83 at yahoo.com> wrote:   Hello All,
   I am searching for asynchronous event processing APIs in Java. Up to this point I found/used the CompletableFuture / Future and they seem a good fit, but I am wondering if there might be other alternatives I am not aware of.
Can you pls share other Java APIs in regards to asynch event processing ?
ThanksIonut  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171002/f9f52837/attachment.html>

From shevek at anarres.org  Tue Oct  3 07:36:07 2017
From: shevek at anarres.org (Shevek)
Date: Tue, 3 Oct 2017 12:36:07 +0100
Subject: [concurrency-interest] Backpressure in ExecutorService with uneven
	length jobs
Message-ID: <ec466a91-6b46-d608-c622-179e9c1b562c@anarres.org>

Dear list, I cannot be the first to ask...

In a previous discussion, it was suggested that using CallerRunsPolicy 
with a queue of (say) ncpus * 10 is the canonical way to achieve 
backpressure. However, when the jobs are of particularly uneven sizes, 
sometimes the caller gets a job which runs for so long that all the 
worker threads exhaust the queue and sit idle. I don't have a good bound 
for the unevenness of jobs, and I have a 32-way Xeon.

Is there a good solution to this? Perhaps blocking the caller in 
submit() until space is available in the queue? Would I better write my 
own queue implementation, or wrap the executor in a semaphore?

It's not obviously amenable to an FJP: It's 3 million independent tasks, 
one per node in a graph, although if the answer is "Yes, make an FJP, 
and this is roughly how to make task-stealing do re-balancing", I'll be 
equally happy.

Thank you.

S.

From viktor.klang at gmail.com  Tue Oct  3 10:35:37 2017
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 3 Oct 2017 16:35:37 +0200
Subject: [concurrency-interest] Backpressure in ExecutorService with
 uneven length jobs
In-Reply-To: <ec466a91-6b46-d608-c622-179e9c1b562c@anarres.org>
References: <ec466a91-6b46-d608-c622-179e9c1b562c@anarres.org>
Message-ID: <CANPzfU9PZw2A-gtSbJLv=u4yLFeZEnoz+EGYd3LB5HYbAr8DRA@mail.gmail.com>

Have you considered using ReactiveStreams/j.u.c.Flow?

-- 
Cheers,
√

On Oct 3, 2017 14:55, "Shevek" <shevek at anarres.org> wrote:

> Dear list, I cannot be the first to ask...
>
> In a previous discussion, it was suggested that using CallerRunsPolicy
> with a queue of (say) ncpus * 10 is the canonical way to achieve
> backpressure. However, when the jobs are of particularly uneven sizes,
> sometimes the caller gets a job which runs for so long that all the worker
> threads exhaust the queue and sit idle. I don't have a good bound for the
> unevenness of jobs, and I have a 32-way Xeon.
>
> Is there a good solution to this? Perhaps blocking the caller in submit()
> until space is available in the queue? Would I better write my own queue
> implementation, or wrap the executor in a semaphore?
>
> It's not obviously amenable to an FJP: It's 3 million independent tasks,
> one per node in a graph, although if the answer is "Yes, make an FJP, and
> this is roughly how to make task-stealing do re-balancing", I'll be equally
> happy.
>
> Thank you.
>
> S.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171003/e1182aaa/attachment.html>

From henri.tremblay at gmail.com  Tue Oct  3 11:04:51 2017
From: henri.tremblay at gmail.com (Henri Tremblay)
Date: Tue, 3 Oct 2017 08:04:51 -0700
Subject: [concurrency-interest] Backpressure in ExecutorService with
 uneven length jobs
In-Reply-To: <ec466a91-6b46-d608-c622-179e9c1b562c@anarres.org>
References: <ec466a91-6b46-d608-c622-179e9c1b562c@anarres.org>
Message-ID: <CADZL2=t5VTQ7bdgo8gio_KeragSFK5YswU_GgpOOv=BKzfrzDw@mail.gmail.com>

Not going reactive, I would see two ways to go.

One way to indeed to manage the back pressure some other way. With a
blocking queue for instance. But there might be a better solution. I'm not
an expert on the topic. But you seem to already do that so it would juste
mean to remove the CallerRunsPolicy (and lose a bit of throughput)

The other way would be to pause during long running tasks. So if a task is
ran by the caller thread, make pauses after a moment to dispatch some other
tasks to the worker threads. To prevent them from starving.

Henri


On 3 October 2017 at 04:36, Shevek <shevek at anarres.org> wrote:

> Dear list, I cannot be the first to ask...
>
> In a previous discussion, it was suggested that using CallerRunsPolicy
> with a queue of (say) ncpus * 10 is the canonical way to achieve
> backpressure. However, when the jobs are of particularly uneven sizes,
> sometimes the caller gets a job which runs for so long that all the worker
> threads exhaust the queue and sit idle. I don't have a good bound for the
> unevenness of jobs, and I have a 32-way Xeon.
>
> Is there a good solution to this? Perhaps blocking the caller in submit()
> until space is available in the queue? Would I better write my own queue
> implementation, or wrap the executor in a semaphore?
>
> It's not obviously amenable to an FJP: It's 3 million independent tasks,
> one per node in a graph, although if the answer is "Yes, make an FJP, and
> this is roughly how to make task-stealing do re-balancing", I'll be equally
> happy.
>
> Thank you.
>
> S.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171003/ae600880/attachment.html>

From thurston at nomagicsoftware.com  Sat Oct  7 10:14:46 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 7 Oct 2017 07:14:46 -0700 (MST)
Subject: [concurrency-interest] Why is happens-before order a partial order?
Message-ID: <1507385686003-0.post@n7.nabble.com>

I'm hoping for some clarification on the distinction between synchronization
order (described as a total order over all synchronization actions) and
happens-before order (described as a partial order).  I believe it is just a
semantical distinction without a difference, but I want to make sure there
isn't a subtle difference that I'm missing)
Note:  I am *not* asking about the difference between synchronization order
and happens-before order, just the characterization as one as "total" and
the other as "partial".

A simple program to illustrate (letters are just labels, and all
instructions are plain memory accesses unless specified; order is source
program order in the usual way). For simplicity, assume that the program
just executes the 3 threads one time each (concurrently) and then exits.
<pre>
volatile int x;

Thread 1:                                    Thread 2:                                             
Thread 3:
A:                                              D:  r1 = x // volatile read
"sees" 10              G:
B:                                              E:                                                         
H:  
C: x = 10 //volatile write                F:
</pre>

Ok, so the synchronization order is (C < D), i.e. over {C, D}
Given the rules of  JLS 17.4.5. Happens-before Order,
we have:
hb(A, B), hb(B, C) and due to transitivity hb(A, C)
hb(C, D) all synchronizes-with relations are also hb
hb(D, E), hb(E, F) and due to transitivity hb(D, F)
and
hb(G, H)

so summarizing, we have *over* {A-F}, 
A <= B <= C <= D <= E <= F.
Now, isn't that enumeration above a "happens-before" order?  And if so, how
is it not a total order (over {A-F})?
Sure, it would be appropriate to say it is a partial order over the set of
*all* memory actions {A-H}, but
wouldn't we say the same about the synchronization order, i.e. it's a
partial order over {A-H} (only relating C and D)?

Or is there a subtle difference, between the ordering/visibility guarantees
of SO C < D, than the {A-F} happens before order?





--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From thurston at nomagicsoftware.com  Sat Oct  7 10:24:13 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 7 Oct 2017 07:24:13 -0700 (MST)
Subject: [concurrency-interest] Why is happens-before order a partial
	order?
In-Reply-To: <1507385686003-0.post@n7.nabble.com>
References: <1507385686003-0.post@n7.nabble.com>
Message-ID: <1507386253403-0.post@n7.nabble.com>

Drat.  I can't seem to format the example "program" appropriately, (I use
nabble to post) - it looked fine in the preview.
volatile int x;Thread 1:                                    Thread 2:                                             
Thread 3:A:                                              D:  r1 = x //
volatile read "sees" 10              G:B:                                             
E:                                                          H:C: x = 10
//volatile write                F:




--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171007/40546e88/attachment.html>

From shade at redhat.com  Sat Oct  7 10:27:02 2017
From: shade at redhat.com (Aleksey Shipilev)
Date: Sat, 7 Oct 2017 16:27:02 +0200
Subject: [concurrency-interest] Why is happens-before order a partial
 order?
In-Reply-To: <1507385686003-0.post@n7.nabble.com>
References: <1507385686003-0.post@n7.nabble.com>
Message-ID: <d0ccd97f-ff50-7d14-d307-68b002bfd914@redhat.com>

Reformatting the example:

Thread 1:
 A:
 B:
 C: x = 10 //volatile write

Thread 2:
 D: r1 = x // volatile read "sees" 10
 E:
 F:

Thread 3:
 G:
 H:

On 10/07/2017 04:14 PM, thurstonn wrote:
> so summarizing, we have *over* {A-F}, A <= B <= C <= D <= E <= F. Now, isn't that enumeration
> above a "happens-before" order?  And if so, how is it not a total order (over {A-F})?
It is, but the partiality comes it when you consider any pair of actions. In spec language, "two
actions can be ordered by a happens-before relationship". Notice "can", not "should".

In your example, G or H  are not hb-ordered with any other action (except for G and H), and this is
where partiality gets you. In fact, if synchronizes-with was not present between C and D, no
inter-thread HB edges would be present at all. Of course, you can select the subset of actions, and
claim totality there, but this is not the order JMM talks about.

> Sure, it would be appropriate to say it is a partial order over the set of *all* memory actions
> {A-H}, but wouldn't we say the same about the synchronization order, i.e. it's a partial order
> over {A-H} (only relating C and D)?
No. By definition, "A synchronization order is a total order over *all of the synchronization
actions* of an execution". Notice "all synchronization actions". In your example only C and D are
synchronization actions, and SO is total over them.

-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171007/270df26d/attachment.sig>

From thurston at nomagicsoftware.com  Sat Oct  7 10:36:17 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 7 Oct 2017 07:36:17 -0700 (MST)
Subject: [concurrency-interest] Why is happens-before order a partial
	order?
In-Reply-To: <d0ccd97f-ff50-7d14-d307-68b002bfd914@redhat.com>
References: <1507385686003-0.post@n7.nabble.com>
 <d0ccd97f-ff50-7d14-d307-68b002bfd914@redhat.com>
Message-ID: <1507386977173-0.post@n7.nabble.com>

Aleksey Shipilev-3 wrote
> Reformatting the example:
> 
> Thread 1:
>  A:
>  B:
>  C: x = 10 //volatile write
> 
> Thread 2:
>  D: r1 = x // volatile read "sees" 10
>  E:
>  F:
> 
> Thread 3:
>  G:
>  H:
> 
> On 10/07/2017 04:14 PM, thurstonn wrote:
>> so summarizing, we have *over* {A-F}, A <= B <= C <= D <= E <= F. Now,
>> isn't that enumeration
>> above a "happens-before" order?  And if so, how is it not a total order
>> (over {A-F})?
> It is, but the partiality comes it when you consider any pair of actions.
> In spec language, "two
> actions can be ordered by a happens-before relationship". Notice "can",
> not "should".
> 
> In your example, G or H  are not hb-ordered with any other action (except
> for G and H), and this is
> where partiality gets you. In fact, if synchronizes-with was not present
> between C and D, no
> inter-thread HB edges would be present at all. Of course, you can select
> the subset of actions, and
> claim totality there, but this is not the order JMM talks about.
> 
>> Sure, it would be appropriate to say it is a partial order over the set
>> of *all* memory actions
>> {A-H}, but wouldn't we say the same about the synchronization order, i.e.
>> it's a partial order
>> over {A-H} (only relating C and D)?
> No. By definition, "A synchronization order is a total order over *all of
> the synchronization
> actions* of an execution". Notice "all synchronization actions". In your
> example only C and D are
> synchronization actions, and SO is total over them.
> 
> -Aleksey
> 
> 
> _______________________________________________

Well, yes that's my understanding (as is clear from the OP).
So, just for clearing up the semantics, would it be correct to say that the
JMM is saying there is exactly *one* happens-before order (for a given
execution), just as there is only one synchronization order?

And if so, the single happens-before order of the above execution would be:
A <= B <= C <= D <= E <= F, G <= H




--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From oleksandr.otenko at gmail.com  Sat Oct  7 10:39:48 2017
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Sat, 7 Oct 2017 15:39:48 +0100
Subject: [concurrency-interest] Why is happens-before order a partial
	order?
In-Reply-To: <1507385686003-0.post@n7.nabble.com>
References: <1507385686003-0.post@n7.nabble.com>
Message-ID: <BDEEC076-8393-42F6-891F-0DC2A7657777@gmail.com>

A total order is also a partial order.

So (not using your example)

A -> B -> C -> D -> E
       /        \
F -> G           H

could be a representation of happens-before for two threads:

A < B < D < E is a total order (program order)

F < G < C < H is a total order (program order)

B < C < D < H is a total order (synchronization order)


Now, if B is a write, and C, D, H are reads, then all of them synchronize-with B.

Alex

> On 7 Oct 2017, at 15:14, thurstonn <thurston at nomagicsoftware.com> wrote:
> 
> I'm hoping for some clarification on the distinction between synchronization
> order (described as a total order over all synchronization actions) and
> happens-before order (described as a partial order).  I believe it is just a
> semantical distinction without a difference, but I want to make sure there
> isn't a subtle difference that I'm missing)
> Note:  I am *not* asking about the difference between synchronization order
> and happens-before order, just the characterization as one as "total" and
> the other as "partial".
> 
> A simple program to illustrate (letters are just labels, and all
> instructions are plain memory accesses unless specified; order is source
> program order in the usual way). For simplicity, assume that the program
> just executes the 3 threads one time each (concurrently) and then exits.
> <pre>
> volatile int x;
> 
> Thread 1:                                    Thread 2:                                             
> Thread 3:
> A:                                              D:  r1 = x // volatile read
> "sees" 10              G:
> B:                                              E:                                                         
> H:  
> C: x = 10 //volatile write                F:
> </pre>
> 
> Ok, so the synchronization order is (C < D), i.e. over {C, D}
> Given the rules of  JLS 17.4.5. Happens-before Order,
> we have:
> hb(A, B), hb(B, C) and due to transitivity hb(A, C)
> hb(C, D) all synchronizes-with relations are also hb
> hb(D, E), hb(E, F) and due to transitivity hb(D, F)
> and
> hb(G, H)
> 
> so summarizing, we have *over* {A-F}, 
> A <= B <= C <= D <= E <= F.
> Now, isn't that enumeration above a "happens-before" order?  And if so, how
> is it not a total order (over {A-F})?
> Sure, it would be appropriate to say it is a partial order over the set of
> *all* memory actions {A-H}, but
> wouldn't we say the same about the synchronization order, i.e. it's a
> partial order over {A-H} (only relating C and D)?
> 
> Or is there a subtle difference, between the ordering/visibility guarantees
> of SO C < D, than the {A-F} happens before order?
> 
> 
> 
> 
> 
> --
> Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171007/e2fcd9c6/attachment.html>

From shade at redhat.com  Sat Oct  7 10:45:06 2017
From: shade at redhat.com (Aleksey Shipilev)
Date: Sat, 7 Oct 2017 16:45:06 +0200
Subject: [concurrency-interest] Why is happens-before order a partial
 order?
In-Reply-To: <1507386977173-0.post@n7.nabble.com>
References: <1507385686003-0.post@n7.nabble.com>
 <d0ccd97f-ff50-7d14-d307-68b002bfd914@redhat.com>
 <1507386977173-0.post@n7.nabble.com>
Message-ID: <5c8dc3c3-e434-7948-e156-feb5ce42d02d@redhat.com>

On 10/07/2017 04:36 PM, thurstonn wrote:
> Well, yes that's my understanding (as is clear from the OP).
> So, just for clearing up the semantics, would it be correct to say that the
> JMM is saying there is exactly *one* happens-before order (for a given
> execution), just as there is only one synchronization order?

By definition in 17.4.6, execution is the tuple that contains exactly one synchronization order, and
exactly one happens-before order, among other things. I.e. these are the orders over the set of
actions present in that execution.

But this does not say how many executions can relate to a particular program.

> And if so, the single happens-before order of the above execution would be:
> A <= B <= C <= D <= E <= F, G <= H

Yes, assuming no other actions are synchronization actions, except for C and D. The missing
relations to G and H forbid us from calling that partial order total.

-Aleksey



-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171007/25740c1b/attachment-0001.sig>

From oleksandr.otenko at gmail.com  Sat Oct  7 10:52:00 2017
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Sat, 7 Oct 2017 15:52:00 +0100
Subject: [concurrency-interest] Why is happens-before order a partial
	order?
In-Reply-To: <1507385686003-0.post@n7.nabble.com>
References: <1507385686003-0.post@n7.nabble.com>
Message-ID: <7592ECE3-BB4B-4730-896C-52B74033DB21@gmail.com>

Now, about your example specifically:

> On 7 Oct 2017, at 15:14, thurstonn <thurston at nomagicsoftware.com> wrote:
> 
> ...A simple program to illustrate (letters are just labels, and all
> instructions are plain memory accesses unless specified; order is source
> program order in the usual way). For simplicity, assume that the program
> just executes the 3 threads one time each (concurrently) and then exits.
> <pre>
> volatile int x;
> 
> Thread 1:                                    Thread 2:                                             
> Thread 3:
> A:                                              D:  r1 = x // volatile read
> "sees" 10              G:
> B:                                              E:                                                         
> H:  
> C: x = 10 //volatile write                F:
> </pre>
> 
> Ok, so the synchronization order is (C < D), i.e. over {C, D}
> Given the rules of  JLS 17.4.5. Happens-before Order,
> we have:
> hb(A, B), hb(B, C) and due to transitivity hb(A, C)
> hb(C, D) all synchronizes-with relations are also hb
> hb(D, E), hb(E, F) and due to transitivity hb(D, F)
> and
> hb(G, H)
> 
> so summarizing, we have *over* {A-F}, 
> A <= B <= C <= D <= E <= F.
> Now, isn't that enumeration above a "happens-before" order?  And if so, how
> is it not a total order (over {A-F})?

It is. It just happens to be a total order this time. (Every total order is also a partial order)


> Sure, it would be appropriate to say it is a partial order over the set of
> *all* memory actions {A-H}, but
> wouldn't we say the same about the synchronization order, i.e. it's a
> partial order over {A-H} (only relating C and D)?

No, not every partial order is total. So synchronization order is required to always be total. (vs happens-before being total order in this particular execution - it won’t be in a different execution, eg D: r1 = x // volatile read “does not see” 10 is allowed. The synchronization order then is D <= C - still total; but happens-before then is no longer total, it will be partial.


> Or is there a subtle difference, between the ordering/visibility guarantees
> of SO C < D, than the {A-F} happens before order?

Well… the subtle difference is in the name: one has to be a total order, the other does not have to be. That is, all synchronization actions are lined up in a sequence (so the order between all reads and writes in this sequence exists - synchronizes-with can be established). All other reads and writes are not necessarily lined up in a sequence - the outcome is that you cannot tell which one is before which. This is only a model. This is a representation of the fact that the normal writes may be observed by different CPU cores in different order. (The spec is even weaker than that, and accounts for other things, too, but just to give a concrete example of what effect this model relates to)


Alex

> --
> Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171007/b0f1fba8/attachment.html>

From thurston at nomagicsoftware.com  Sat Oct  7 10:52:13 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 7 Oct 2017 07:52:13 -0700 (MST)
Subject: [concurrency-interest] Why is happens-before order a partial
	order?
In-Reply-To: <5c8dc3c3-e434-7948-e156-feb5ce42d02d@redhat.com>
References: <1507385686003-0.post@n7.nabble.com>
 <d0ccd97f-ff50-7d14-d307-68b002bfd914@redhat.com>
 <1507386977173-0.post@n7.nabble.com>
 <5c8dc3c3-e434-7948-e156-feb5ce42d02d@redhat.com>
Message-ID: <1507387933365-0.post@n7.nabble.com>

Ok, thanks.

For those following along, it's probably covered by:

17.4.7. Well-Formed Executions 
"The happens-before order is given by the transitive closure of
synchronizes-with edges and program order"







--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From fw at deneb.enyo.de  Sun Oct 15 04:38:51 2017
From: fw at deneb.enyo.de (Florian Weimer)
Date: Sun, 15 Oct 2017 10:38:51 +0200
Subject: [concurrency-interest] Difference between
	AtomicReference.getPlain() and getOpaque()
Message-ID: <87h8v0ls9w.fsf@mid.deneb.enyo.de>

Are these methods essentially the same?  I think so, based on JLS §17.7:

| Writes to and reads of references are always atomic, regardless of
| whether they are implemented as 32-bit or 64-bit values.

I assume there is a difference between getPlain() and getOpaque() for
AtomicLong because of potentially non-atomic access: I assume
getOpaque() rules that out (but that is not very clear), but
getPlain() obviously does not.

Whatabout AtomicInteger and AtomicBoolean?  Do these methods show a
difference there?

From aph at redhat.com  Sun Oct 15 04:52:01 2017
From: aph at redhat.com (Andrew Haley)
Date: Sun, 15 Oct 2017 09:52:01 +0100
Subject: [concurrency-interest] Difference between
 AtomicReference.getPlain() and getOpaque()
In-Reply-To: <87h8v0ls9w.fsf@mid.deneb.enyo.de>
References: <87h8v0ls9w.fsf@mid.deneb.enyo.de>
Message-ID: <db681a26-1b62-c0a6-77ab-2d6d1e612f03@redhat.com>

On 15/10/17 09:38, Florian Weimer wrote:
> Are these methods essentially the same?  I think so, based on JLS §17.7:
> 
> | Writes to and reads of references are always atomic, regardless of
> | whether they are implemented as 32-bit or 64-bit values.
> 
> I assume there is a difference between getPlain() and getOpaque() for
> AtomicLong because of potentially non-atomic access: I assume
> getOpaque() rules that out (but that is not very clear), but
> getPlain() obviously does not.

Consider a reference field x, and a loop:

while (x == null) { }

and another thread

x = new Object();

If x is not volatile, Java compiler can hoist the load of x out of a loop
and turn that loop into

for (;;) { }

... so it loops forever.

However, it cannot do the same with getOpaque().  Eventually the read of
x will return non-null, although it may be delayed for a long time and it
won't be ordered with any other accesses.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From marnikitta at gmail.com  Sun Oct 15 04:54:04 2017
From: marnikitta at gmail.com (=?UTF-8?B?0J3QuNC60LjRgtCwINCc0LDRgNGI0LDQu9C60LjQvQ==?=)
Date: Sun, 15 Oct 2017 11:54:04 +0300
Subject: [concurrency-interest] Difference between
 AtomicReference.getPlain() and getOpaque()
In-Reply-To: <87h8v0ls9w.fsf@mid.deneb.enyo.de>
References: <87h8v0ls9w.fsf@mid.deneb.enyo.de>
Message-ID: <CAPP3q214ifk4eikVdGc4kinu44PWusF12Owth8gmpbn69_Ru9g@mail.gmail.com>

Here's note by Doug Lea on memory orderings in JDK 9.

http://gee.cs.oswego.edu/dl/html/j9mm.html

On Sun, Oct 15, 2017 at 11:38 AM, Florian Weimer <fw at deneb.enyo.de> wrote:

> Are these methods essentially the same?  I think so, based on JLS §17.7:
>
> | Writes to and reads of references are always atomic, regardless of
> | whether they are implemented as 32-bit or 64-bit values.
>
> I assume there is a difference between getPlain() and getOpaque() for
> AtomicLong because of potentially non-atomic access: I assume
> getOpaque() rules that out (but that is not very clear), but
> getPlain() obviously does not.
>
> Whatabout AtomicInteger and AtomicBoolean?  Do these methods show a
> difference there?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Yours sincerely,
Nikita Marshalkin.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171015/f8eeb8fb/attachment.html>

From shade at redhat.com  Sun Oct 15 05:17:15 2017
From: shade at redhat.com (Aleksey Shipilev)
Date: Sun, 15 Oct 2017 11:17:15 +0200
Subject: [concurrency-interest] Difference between
 AtomicReference.getPlain() and getOpaque()
In-Reply-To: <CAPP3q214ifk4eikVdGc4kinu44PWusF12Owth8gmpbn69_Ru9g@mail.gmail.com>
References: <87h8v0ls9w.fsf@mid.deneb.enyo.de>
 <CAPP3q214ifk4eikVdGc4kinu44PWusF12Owth8gmpbn69_Ru9g@mail.gmail.com>
Message-ID: <eac1e78c-4360-2557-6a78-c8a14ba102af@redhat.com>

On 10/15/2017 10:54 AM, Никита Маршалкин wrote:
> Here's note by Doug Lea on memory orderings in JDK 9. 
> 
> http://gee.cs.oswego.edu/dl/html/j9mm.html <http://gee.cs.oswego.edu/dl/html/j9mm.html>

TL;DR: "opaque" = "plain" + access atomicity + coherence + eventual progress

Access atomicity:
 http://hg.openjdk.java.net/code-tools/jcstress/file/6a9ce3b8ccc4/jcstress-samples/src/main/java/org/openjdk/jcstress/samples/JMMSample_01_AccessAtomicity.java

Coherence:
 http://hg.openjdk.java.net/code-tools/jcstress/file/6a9ce3b8ccc4/jcstress-samples/src/main/java/org/openjdk/jcstress/samples/JMMSample_03_Coherence.java

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171015/25382b02/attachment.sig>

From fw at deneb.enyo.de  Sun Oct 15 05:23:09 2017
From: fw at deneb.enyo.de (Florian Weimer)
Date: Sun, 15 Oct 2017 11:23:09 +0200
Subject: [concurrency-interest] Difference between
	AtomicReference.getPlain() and getOpaque()
In-Reply-To: <db681a26-1b62-c0a6-77ab-2d6d1e612f03@redhat.com> (Andrew Haley's
 message of "Sun, 15 Oct 2017 09:52:01 +0100")
References: <87h8v0ls9w.fsf@mid.deneb.enyo.de>
 <db681a26-1b62-c0a6-77ab-2d6d1e612f03@redhat.com>
Message-ID: <87a80slq82.fsf@mid.deneb.enyo.de>

* Andrew Haley:

> On 15/10/17 09:38, Florian Weimer wrote:
>> Are these methods essentially the same?  I think so, based on JLS §17.7:
>> 
>> | Writes to and reads of references are always atomic, regardless of
>> | whether they are implemented as 32-bit or 64-bit values.
>> 
>> I assume there is a difference between getPlain() and getOpaque() for
>> AtomicLong because of potentially non-atomic access: I assume
>> getOpaque() rules that out (but that is not very clear), but
>> getPlain() obviously does not.
>
> Consider a reference field x, and a loop:
>
> while (x == null) { }
>
> and another thread
>
> x = new Object();
>
> If x is not volatile, Java compiler can hoist the load of x out of a loop
> and turn that loop into
>
> for (;;) { }
>
> ... so it loops forever.
>
> However, it cannot do the same with getOpaque().  Eventually the read of
> x will return non-null, although it may be delayed for a long time and it
> won't be ordered with any other accesses.

Thanks, with this and (the reference Никита provided), it's clear that
they are not the same.  It's really not obvious based on JLS 9 and the
JDK 9 API documentation that there are additional constraints on the
implementation here.  The description of the memory model in the JLS
could at least say that it's not been updated for the introduction of
opaque accesses in JDK 9. 8-/

From aph at redhat.com  Sun Oct 15 09:28:29 2017
From: aph at redhat.com (Andrew Haley)
Date: Sun, 15 Oct 2017 14:28:29 +0100
Subject: [concurrency-interest] Difference between
 AtomicReference.getPlain() and getOpaque()
In-Reply-To: <87a80slq82.fsf@mid.deneb.enyo.de>
References: <87h8v0ls9w.fsf@mid.deneb.enyo.de>
 <db681a26-1b62-c0a6-77ab-2d6d1e612f03@redhat.com>
 <87a80slq82.fsf@mid.deneb.enyo.de>
Message-ID: <2e187d26-4a1e-0e27-06a0-cf5f692cfb93@redhat.com>

On 15/10/17 10:23, Florian Weimer wrote:
> Thanks, with this and (the reference Никита provided), it's clear that
> they are not the same.  It's really not obvious based on JLS 9 and the
> JDK 9 API documentation that there are additional constraints on the
> implementation here.  The description of the memory model in the JLS
> could at least say that it's not been updated for the introduction of
> opaque accesses in JDK 9.

"Obvious" is perhaps not the word to use in this context.  It seems to
be extraordinarily difficult to specify extensions to the JMM (and,
indeed, the JMM itself) in a way that makes sense to most Java
programmers.  But the behaviour I described is implied by the spec:

getPlain()

     * Returns the current value, with memory semantics of reading as
     * if the variable was declared non-{@code volatile}.

getOpaque()

     * Returns the value of a variable, accessed in program order, but
     * with no assurance of memory ordering effects with respect to
     * other threads.

The key phrase here is "accessed in program order," but most
programmers won't notice its significance.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From maaartinus at gmail.com  Tue Oct 17 17:02:01 2017
From: maaartinus at gmail.com (Martin Grajcar)
Date: Tue, 17 Oct 2017 23:02:01 +0200
Subject: [concurrency-interest] The parts of the ReentrantReadWriteLock
	hold no reference to it
In-Reply-To: <CAGsWfGjbP2ra1R6MGKifbGo4G4v5-NNTjG6GZi3+h9oJvh_ArA@mail.gmail.com>
References: <CAGsWfGjbP2ra1R6MGKifbGo4G4v5-NNTjG6GZi3+h9oJvh_ArA@mail.gmail.com>
Message-ID: <CAGsWfGgK5kbgbQ=SZX5Wy=ZvdJpoxdbTLGzyuGmM10pcTARzCA@mail.gmail.com>

The parts of the ReentrantReadWriteLock holding no reference to it may lead
to a situation where the ReentrantReadWriteLock gets GC'ed while its read
and write locks are still in use. This is alright unless the
ReentrantReadWriteLock is weakly cached as e.g., in Guava Striped, where it
has lead to the bug https://github.com/google/guava/issues/2477.

I'd suggest to add a reference to the enclosing class, e.g., by removing
the static modifier. As all these objects are lightweight and not meant to
have millions of instances, the cost is negligible. The advantage is
decreased chance of buggy user code and avoiding lengthy workarounds like
https://github.com/google/guava/commit/957c1a5455508120d224f6d0d8f3bf
8afa3630f0.

The cost benefit ratio seems to be good.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171017/463cc56d/attachment.html>

From martinrb at google.com  Tue Oct 17 17:11:09 2017
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 17 Oct 2017 14:11:09 -0700
Subject: [concurrency-interest] The parts of the ReentrantReadWriteLock
 hold no reference to it
In-Reply-To: <CAGsWfGgK5kbgbQ=SZX5Wy=ZvdJpoxdbTLGzyuGmM10pcTARzCA@mail.gmail.com>
References: <CAGsWfGjbP2ra1R6MGKifbGo4G4v5-NNTjG6GZi3+h9oJvh_ArA@mail.gmail.com>
 <CAGsWfGgK5kbgbQ=SZX5Wy=ZvdJpoxdbTLGzyuGmM10pcTARzCA@mail.gmail.com>
Message-ID: <CA+kOe08_N6PbQOZMRcccVpcYUnoQ=GfekErEGr-JPkuVKd0UhQ@mail.gmail.com>

Thanks!  I filed bug
https://bugs.openjdk.java.net/browse/JDK-8189598

On Tue, Oct 17, 2017 at 2:02 PM, Martin Grajcar <maaartinus at gmail.com>
wrote:

> The parts of the ReentrantReadWriteLock holding no reference to it may
> lead to a situation where the ReentrantReadWriteLock gets GC'ed while its
> read and write locks are still in use. This is alright unless the
> ReentrantReadWriteLock is weakly cached as e.g., in Guava Striped, where
> it has lead to the bug https://github.com/google/guava/issues/2477.
>
> I'd suggest to add a reference to the enclosing class, e.g., by removing
> the static modifier. As all these objects are lightweight and not meant to
> have millions of instances, the cost is negligible. The advantage is
> decreased chance of buggy user code and avoiding lengthy workarounds like
> https://github.com/google/guava/commit/957c1a5455508120
> d224f6d0d8f3bf8afa3630f0.
>
> The cost benefit ratio seems to be good.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171017/28452a6f/attachment.html>

From dl at cs.oswego.edu  Tue Oct 17 19:53:12 2017
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Oct 2017 19:53:12 -0400
Subject: [concurrency-interest] The parts of the ReentrantReadWriteLock
 hold no reference to it
In-Reply-To: <CAGsWfGgK5kbgbQ=SZX5Wy=ZvdJpoxdbTLGzyuGmM10pcTARzCA@mail.gmail.com>
References: <CAGsWfGjbP2ra1R6MGKifbGo4G4v5-NNTjG6GZi3+h9oJvh_ArA@mail.gmail.com>
 <CAGsWfGgK5kbgbQ=SZX5Wy=ZvdJpoxdbTLGzyuGmM10pcTARzCA@mail.gmail.com>
Message-ID: <e658a0f2-39c9-ca79-b99e-47b40ef6aebc@cs.oswego.edu>

On 10/17/2017 05:02 PM, Martin Grajcar wrote:
> The parts of the ReentrantReadWriteLock holding no reference to it may
> lead to a situation where the ReentrantReadWriteLock gets GC'ed while
> its read and write locks are still in use. This is alright unless
> the ReentrantReadWriteLock is weakly cached as e.g., in Guava Striped,
> where it has lead to the bug https://github.com/google/guava/issues/2477
> <https://github.com/google/guava/issues/2477>.

> 
> I'd suggest to add a reference to the enclosing class, e.g., by removing
> the static modifier. As all these objects are lightweight and not meant
> to have millions of instances, the cost is negligible. The advantage is

Some program out there probably has more ReentrantReadWriteLocks than
any of use can imagine, so will be hurt by this.
Still, I suppose we have done worse things to help solve
not-our-problem problems in even more crazy uses than yours,
so we probably ought to do it.

Maybe the added overhead will drive more people to use StampedLocks,
as we keep hoping.

-Doug

> decreased chance of buggy user code and avoiding lengthy workarounds
> like https://github.com/google/guava/commit/957c1a5455508120d224f6d0d8f3bf8afa3630f0
> <https://github.com/google/guava/commit/957c1a5455508120d224f6d0d8f3bf8afa3630f0>.
> 
> The cost benefit ratio seems to be good.
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From dimitar.georgiev.bg at gmail.com  Fri Oct 20 10:39:06 2017
From: dimitar.georgiev.bg at gmail.com (Dimitar Georgiev)
Date: Fri, 20 Oct 2017 17:39:06 +0300
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with strict
	threading semantics
Message-ID: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>

Sorry if this has already been discussed.

I need to implement the following function:

public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
upstream, Function<A, B> f);

It's pretty apparent what the function does. However, it has the
requirement that f() be always executed in the thread where upstream
is completed. (That is, if upstream is completed non-exceptionally,
since if it was not, f will not be called so there is no threading
semantics concerning f)

Is this possible with j.u.c.CompletableFuture?

Regards, Dimitar

From viktor.klang at gmail.com  Fri Oct 20 10:52:43 2017
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 20 Oct 2017 16:52:43 +0200
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with
 strict threading semantics
In-Reply-To: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
References: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
Message-ID: <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>

Hi Dimitar,

In general this is not possible, nor desirable, since completion is likely
to happen *before* a transformation is added.

However, there is nothing technical which prevents you to create an
implementation of CompletionStage which stores a reference to the Executor
to be used, and runs transformations which are applied to it on that
Executor unless some other Executor is specified.

-- 
Cheers,
√


On Oct 20, 2017 09:41, "Dimitar Georgiev" <dimitar.georgiev.bg at gmail.com>
wrote:

Sorry if this has already been discussed.

I need to implement the following function:

public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
upstream, Function<A, B> f);

It's pretty apparent what the function does. However, it has the
requirement that f() be always executed in the thread where upstream
is completed. (That is, if upstream is completed non-exceptionally,
since if it was not, f will not be called so there is no threading
semantics concerning f)

Is this possible with j.u.c.CompletableFuture?

Regards, Dimitar
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171020/64ef482f/attachment.html>

From dimitar.georgiev.bg at gmail.com  Fri Oct 20 11:32:46 2017
From: dimitar.georgiev.bg at gmail.com (Dimitar Georgiev)
Date: Fri, 20 Oct 2017 18:32:46 +0300
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with
 strict threading semantics
In-Reply-To: <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>
References: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
 <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>
Message-ID: <CAKnQuOH2Jv8eLTuzPUKZ_M5mORMDg7mpCN0XyFqDk+Lo7K7SUw@mail.gmail.com>

Thanks Viktor! I had not thought of it this way, and you are absolutely right.

I hope it is okay to expand the discussion a little bit and go a bit
off-topic from j.u.c. Viktor probably already sees where this is
going. Next is flatMap:

public static <A, B> CompletableFuture<B> flatMap(CompletableFuture<A>
future, Function<A, CompletableFuture<B>> f);

Semantics my app requires here: if the result of f() is completed on a
new threading context, I want subsequent map / flatMap operations to
use that context. If not (if the result of f() is pure, i.e. of the
form new CompletableFuture<>().complete(value)), continue in the same
threading context.

 Viktor, your remark leads me to think there is no sane way to achieve
this semantics with an eager future implementation such as
CompletableFuture. I would need something lazy which is a description,
creating the thing be referentially transparent, and
submission/execution be decoupled from creating the thing.

To finish the off-topic with the actual question:
- Does such a thing exist in the Java ecosystem at all? I know it does
in Scala, but I need a Java alternative.

P.S. I found a couple of months ago https://github.com/traneio/future.
A claim is made that the threading semantics are what I just
described, but it is an eager future. I will look into the
implementation in the coming days; I expect it to be something along
the "workaround" Viktor described. Myself, I don't feel I am smart
enough or have the testing and formal verification tools to go down
that path...






On 20 October 2017 at 17:52, Viktor Klang <viktor.klang at gmail.com> wrote:
> Hi Dimitar,
>
> In general this is not possible, nor desirable, since completion is likely
> to happen *before* a transformation is added.
>
> However, there is nothing technical which prevents you to create an
> implementation of CompletionStage which stores a reference to the Executor
> to be used, and runs transformations which are applied to it on that
> Executor unless some other Executor is specified.
>
> --
> Cheers,
> √
>
>
> On Oct 20, 2017 09:41, "Dimitar Georgiev" <dimitar.georgiev.bg at gmail.com>
> wrote:
>
> Sorry if this has already been discussed.
>
> I need to implement the following function:
>
> public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
> upstream, Function<A, B> f);
>
> It's pretty apparent what the function does. However, it has the
> requirement that f() be always executed in the thread where upstream
> is completed. (That is, if upstream is completed non-exceptionally,
> since if it was not, f will not be called so there is no threading
> semantics concerning f)
>
> Is this possible with j.u.c.CompletableFuture?
>
> Regards, Dimitar
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From viktor.klang at gmail.com  Fri Oct 20 14:06:16 2017
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 20 Oct 2017 20:06:16 +0200
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with
 strict threading semantics
In-Reply-To: <CAKnQuOH2Jv8eLTuzPUKZ_M5mORMDg7mpCN0XyFqDk+Lo7K7SUw@mail.gmail.com>
References: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
 <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>
 <CAKnQuOH2Jv8eLTuzPUKZ_M5mORMDg7mpCN0XyFqDk+Lo7K7SUw@mail.gmail.com>
Message-ID: <CANPzfU_bctspebBeSGc3nf37GCP3AsP4ftuEfai0RrHNsJDdnw@mail.gmail.com>

Dimitar,

Just a note: I realized that you can probably get to what you want by
controlling injecting a "valve"—CompletableFuture, and only complete that
once you want the transformations to be applied:

Pseudocode (since I'm having lunch):
enum Trigger { On; }
CompletableFuture<X> upstream = …
CompletableFuture<Trigger> upstreamTrigger = new
CompletableFuture<Trigger>()

CompletableFuture<Y> result = upstream.thenCombineAsync(upstreamTrigger,
(x, discard) -> x, YOUR_INTENDED_EXECUTOR).thenApply(…).thenCompose(…)…

upstreamTrigger.complete(Trigger.On); // this will trigger the execution of
the transformations on YOUR_INTENDED_EXECUTOR regardless of what
transformations are added to `result` as long as they HAPPEN-BEFORE the
execution of this line.

YMMV,

√



On Fri, Oct 20, 2017 at 5:32 PM, Dimitar Georgiev <
dimitar.georgiev.bg at gmail.com> wrote:

> Thanks Viktor! I had not thought of it this way, and you are absolutely
> right.
>
> I hope it is okay to expand the discussion a little bit and go a bit
> off-topic from j.u.c. Viktor probably already sees where this is
> going. Next is flatMap:
>
> public static <A, B> CompletableFuture<B> flatMap(CompletableFuture<A>
> future, Function<A, CompletableFuture<B>> f);
>
> Semantics my app requires here: if the result of f() is completed on a
> new threading context, I want subsequent map / flatMap operations to
> use that context. If not (if the result of f() is pure, i.e. of the
> form new CompletableFuture<>().complete(value)), continue in the same
> threading context.
>
>  Viktor, your remark leads me to think there is no sane way to achieve
> this semantics with an eager future implementation such as
> CompletableFuture. I would need something lazy which is a description,
> creating the thing be referentially transparent, and
> submission/execution be decoupled from creating the thing.
>
> To finish the off-topic with the actual question:
> - Does such a thing exist in the Java ecosystem at all? I know it does
> in Scala, but I need a Java alternative.
>
> P.S. I found a couple of months ago https://github.com/traneio/future.
> A claim is made that the threading semantics are what I just
> described, but it is an eager future. I will look into the
> implementation in the coming days; I expect it to be something along
> the "workaround" Viktor described. Myself, I don't feel I am smart
> enough or have the testing and formal verification tools to go down
> that path...
>
>
>
>
>
>
> On 20 October 2017 at 17:52, Viktor Klang <viktor.klang at gmail.com> wrote:
> > Hi Dimitar,
> >
> > In general this is not possible, nor desirable, since completion is
> likely
> > to happen *before* a transformation is added.
> >
> > However, there is nothing technical which prevents you to create an
> > implementation of CompletionStage which stores a reference to the
> Executor
> > to be used, and runs transformations which are applied to it on that
> > Executor unless some other Executor is specified.
> >
> > --
> > Cheers,
> > √
> >
> >
> > On Oct 20, 2017 09:41, "Dimitar Georgiev" <dimitar.georgiev.bg at gmail.com
> >
> > wrote:
> >
> > Sorry if this has already been discussed.
> >
> > I need to implement the following function:
> >
> > public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
> > upstream, Function<A, B> f);
> >
> > It's pretty apparent what the function does. However, it has the
> > requirement that f() be always executed in the thread where upstream
> > is completed. (That is, if upstream is completed non-exceptionally,
> > since if it was not, f will not be called so there is no threading
> > semantics concerning f)
> >
> > Is this possible with j.u.c.CompletableFuture?
> >
> > Regards, Dimitar
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171020/f5fcacdb/attachment.html>

From Sebastian.Millies at softwareag.com  Sat Oct 21 08:01:44 2017
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Sat, 21 Oct 2017 12:01:44 +0000
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with
 strict threading semantics
In-Reply-To: <CANPzfU_bctspebBeSGc3nf37GCP3AsP4ftuEfai0RrHNsJDdnw@mail.gmail.com>
References: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
 <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>
 <CAKnQuOH2Jv8eLTuzPUKZ_M5mORMDg7mpCN0XyFqDk+Lo7K7SUw@mail.gmail.com>
 <CANPzfU_bctspebBeSGc3nf37GCP3AsP4ftuEfai0RrHNsJDdnw@mail.gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E49001178061C5@daeexmbx1.eur.ad.sag>

Hi,

I’m sorry to be so dense, but can you explain what the difference is to simply upstream.thenApplyAsync(f, YOUR_INTENDED_EXECUTOR) ?


n  SebaSTIAN

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Viktor Klang
Sent: Friday, October 20, 2017 8:06 PM
To: Dimitar Georgiev
Cc: concurrency-interest
Subject: Re: [concurrency-interest] map over j.u.c.CompletableFuture with strict threading semantics

Dimitar,

Just a note: I realized that you can probably get to what you want by controlling injecting a "valve"—CompletableFuture, and only complete that once you want the transformations to be applied:

Pseudocode (since I'm having lunch):
enum Trigger { On; }
CompletableFuture<X> upstream = …
CompletableFuture<Trigger> upstreamTrigger = new CompletableFuture<Trigger>()

CompletableFuture<Y> result = upstream.thenCombineAsync(upstreamTrigger, (x, discard) -> x, YOUR_INTENDED_EXECUTOR).thenApply(…).thenCompose(…)…

upstreamTrigger.complete(Trigger.On); // this will trigger the execution of the transformations on YOUR_INTENDED_EXECUTOR regardless of what transformations are added to `result` as long as they HAPPEN-BEFORE the execution of this line.

YMMV,

√



On Fri, Oct 20, 2017 at 5:32 PM, Dimitar Georgiev <dimitar.georgiev.bg at gmail.com<mailto:dimitar.georgiev.bg at gmail.com>> wrote:
Thanks Viktor! I had not thought of it this way, and you are absolutely right.

I hope it is okay to expand the discussion a little bit and go a bit
off-topic from j.u.c. Viktor probably already sees where this is
going. Next is flatMap:

public static <A, B> CompletableFuture<B> flatMap(CompletableFuture<A>
future, Function<A, CompletableFuture<B>> f);

Semantics my app requires here: if the result of f() is completed on a
new threading context, I want subsequent map / flatMap operations to
use that context. If not (if the result of f() is pure, i.e. of the
form new CompletableFuture<>().complete(value)), continue in the same
threading context.

 Viktor, your remark leads me to think there is no sane way to achieve
this semantics with an eager future implementation such as
CompletableFuture. I would need something lazy which is a description,
creating the thing be referentially transparent, and
submission/execution be decoupled from creating the thing.

To finish the off-topic with the actual question:
- Does such a thing exist in the Java ecosystem at all? I know it does
in Scala, but I need a Java alternative.

P.S. I found a couple of months ago https://github.com/traneio/future.
A claim is made that the threading semantics are what I just
described, but it is an eager future. I will look into the
implementation in the coming days; I expect it to be something along
the "workaround" Viktor described. Myself, I don't feel I am smart
enough or have the testing and formal verification tools to go down
that path...






On 20 October 2017 at 17:52, Viktor Klang <viktor.klang at gmail.com<mailto:viktor.klang at gmail.com>> wrote:
> Hi Dimitar,
>
> In general this is not possible, nor desirable, since completion is likely
> to happen *before* a transformation is added.
>
> However, there is nothing technical which prevents you to create an
> implementation of CompletionStage which stores a reference to the Executor
> to be used, and runs transformations which are applied to it on that
> Executor unless some other Executor is specified.
>
> --
> Cheers,
> √
>
>
> On Oct 20, 2017 09:41, "Dimitar Georgiev" <dimitar.georgiev.bg at gmail.com<mailto:dimitar.georgiev.bg at gmail.com>>
> wrote:
>
> Sorry if this has already been discussed.
>
> I need to implement the following function:
>
> public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
> upstream, Function<A, B> f);
>
> It's pretty apparent what the function does. However, it has the
> requirement that f() be always executed in the thread where upstream
> is completed. (That is, if upstream is completed non-exceptionally,
> since if it was not, f will not be called so there is no threading
> semantics concerning f)
>
> Is this possible with j.u.c.CompletableFuture?
>
> Regards, Dimitar
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>



--
Cheers,
√

Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt, Germany – Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt, Dr. Stefan Sigg; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171021/05739375/attachment-0001.html>

From viktor.klang at gmail.com  Sat Oct 21 12:06:20 2017
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sat, 21 Oct 2017 18:06:20 +0200
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with
 strict threading semantics
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E49001178061C5@daeexmbx1.eur.ad.sag>
References: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
 <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>
 <CAKnQuOH2Jv8eLTuzPUKZ_M5mORMDg7mpCN0XyFqDk+Lo7K7SUw@mail.gmail.com>
 <CANPzfU_bctspebBeSGc3nf37GCP3AsP4ftuEfai0RrHNsJDdnw@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E49001178061C5@daeexmbx1.eur.ad.sag>
Message-ID: <CANPzfU9nTk2B03g3nfJh6NOe4wYOdfo4vNoMi_DbaKGCLnWh0w@mail.gmail.com>

Hi Sebastian,

On Sat, Oct 21, 2017 at 2:01 PM, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:

> Hi,
>
>
>
> I’m sorry to be so dense, but can you explain what the difference is to
> simply upstream.thenApplyAsync(f, YOUR_INTENDED_EXECUTOR) ?
>

Sorry, on my way to the airport, and I might have misunderstood your
question, but think about "Which thread adds the transformations" vs "Which
thread executes the transformations". (There needs to be a happens-before
relationship between "all transformations already added" and "intended
executor executes the value which produces the first result"—so that all
the transformations are piggybacked on that executor and not the thread
which adds the transformation.)

Cheers,
√


>
>
> n  SebaSTIAN
>
>
>
> *From:* Concurrency-interest [mailto:concurrency-interest-
> bounces at cs.oswego.edu] *On Behalf Of *Viktor Klang
> *Sent:* Friday, October 20, 2017 8:06 PM
> *To:* Dimitar Georgiev
> *Cc:* concurrency-interest
> *Subject:* Re: [concurrency-interest] map over j.u.c.CompletableFuture
> with strict threading semantics
>
>
>
> Dimitar,
>
>
>
> Just a note: I realized that you can probably get to what you want by
> controlling injecting a "valve"—CompletableFuture, and only complete that
> once you want the transformations to be applied:
>
>
>
> Pseudocode (since I'm having lunch):
>
> enum Trigger { On; }
>
> CompletableFuture<X> upstream = …
>
> CompletableFuture<Trigger> upstreamTrigger = new
> CompletableFuture<Trigger>()
>
>
>
> CompletableFuture<Y> result = upstream.thenCombineAsync(upstreamTrigger,
> (x, discard) -> x, YOUR_INTENDED_EXECUTOR).thenApply(…).thenCompose(…)…
>
>
>
> upstreamTrigger.complete(Trigger.On); // this will trigger the execution
> of the transformations on YOUR_INTENDED_EXECUTOR regardless of what
> transformations are added to `result` as long as they HAPPEN-BEFORE the
> execution of this line.
>
>
>
> YMMV,
>
>
>
> √
>
>
>
>
>
>
>
> On Fri, Oct 20, 2017 at 5:32 PM, Dimitar Georgiev <
> dimitar.georgiev.bg at gmail.com> wrote:
>
> Thanks Viktor! I had not thought of it this way, and you are absolutely
> right.
>
> I hope it is okay to expand the discussion a little bit and go a bit
> off-topic from j.u.c. Viktor probably already sees where this is
> going. Next is flatMap:
>
> public static <A, B> CompletableFuture<B> flatMap(CompletableFuture<A>
> future, Function<A, CompletableFuture<B>> f);
>
> Semantics my app requires here: if the result of f() is completed on a
> new threading context, I want subsequent map / flatMap operations to
> use that context. If not (if the result of f() is pure, i.e. of the
> form new CompletableFuture<>().complete(value)), continue in the same
> threading context.
>
>  Viktor, your remark leads me to think there is no sane way to achieve
> this semantics with an eager future implementation such as
> CompletableFuture. I would need something lazy which is a description,
> creating the thing be referentially transparent, and
> submission/execution be decoupled from creating the thing.
>
> To finish the off-topic with the actual question:
> - Does such a thing exist in the Java ecosystem at all? I know it does
> in Scala, but I need a Java alternative.
>
> P.S. I found a couple of months ago https://github.com/traneio/future.
> A claim is made that the threading semantics are what I just
> described, but it is an eager future. I will look into the
> implementation in the coming days; I expect it to be something along
> the "workaround" Viktor described. Myself, I don't feel I am smart
> enough or have the testing and formal verification tools to go down
> that path...
>
>
>
>
>
>
>
> On 20 October 2017 at 17:52, Viktor Klang <viktor.klang at gmail.com> wrote:
> > Hi Dimitar,
> >
> > In general this is not possible, nor desirable, since completion is
> likely
> > to happen *before* a transformation is added.
> >
> > However, there is nothing technical which prevents you to create an
> > implementation of CompletionStage which stores a reference to the
> Executor
> > to be used, and runs transformations which are applied to it on that
> > Executor unless some other Executor is specified.
> >
> > --
> > Cheers,
> > √
> >
> >
> > On Oct 20, 2017 09:41, "Dimitar Georgiev" <dimitar.georgiev.bg at gmail.com
> >
> > wrote:
> >
> > Sorry if this has already been discussed.
> >
> > I need to implement the following function:
> >
> > public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
> > upstream, Function<A, B> f);
> >
> > It's pretty apparent what the function does. However, it has the
> > requirement that f() be always executed in the thread where upstream
> > is completed. (That is, if upstream is completed non-exceptionally,
> > since if it was not, f will not be called so there is no threading
> > semantics concerning f)
> >
> > Is this possible with j.u.c.CompletableFuture?
> >
> > Regards, Dimitar
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
>
>
>
>
> --
>
> Cheers,
>
> √
>
> Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt,
> Germany – Registergericht/Commercial register: Darmstadt HRB 1562 -
> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt, Dr. Stefan Sigg; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171021/d7783aeb/attachment.html>

From Sebastian.Millies at softwareag.com  Sun Oct 22 08:08:05 2017
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Sun, 22 Oct 2017 12:08:05 +0000
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with
 strict threading semantics
In-Reply-To: <CANPzfU9nTk2B03g3nfJh6NOe4wYOdfo4vNoMi_DbaKGCLnWh0w@mail.gmail.com>
References: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
 <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>
 <CAKnQuOH2Jv8eLTuzPUKZ_M5mORMDg7mpCN0XyFqDk+Lo7K7SUw@mail.gmail.com>
 <CANPzfU_bctspebBeSGc3nf37GCP3AsP4ftuEfai0RrHNsJDdnw@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E49001178061C5@daeexmbx1.eur.ad.sag>
 <CANPzfU9nTk2B03g3nfJh6NOe4wYOdfo4vNoMi_DbaKGCLnWh0w@mail.gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E4900117826C2F@daeexmbx1.eur.ad.sag>

I see. What I meant was, why do you need to establish that happens-before relationship in such a complicated way if all you want to do is to execute a transformation in some given executor known in advance? Instead, can’t you just as well supply that executor to one of the async methods in CF in each of the downstream transformations?

From: Viktor Klang [mailto:viktor.klang at gmail.com]
Sent: Saturday, October 21, 2017 6:06 PM
To: Millies, Sebastian
Cc: Dimitar Georgiev; concurrency-interest
Subject: Re: [concurrency-interest] map over j.u.c.CompletableFuture with strict threading semantics

Hi Sebastian,

On Sat, Oct 21, 2017 at 2:01 PM, Millies, Sebastian <Sebastian.Millies at softwareag.com<mailto:Sebastian.Millies at softwareag.com>> wrote:
Hi,

I’m sorry to be so dense, but can you explain what the difference is to simply upstream.thenApplyAsync(f, YOUR_INTENDED_EXECUTOR) ?

Sorry, on my way to the airport, and I might have misunderstood your question, but think about "Which thread adds the transformations" vs "Which thread executes the transformations". (There needs to be a happens-before relationship between "all transformations already added" and "intended executor executes the value which produces the first result"—so that all the transformations are piggybacked on that executor and not the thread which adds the transformation.)

Cheers,
√



•  SebaSTIAN

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu>] On Behalf Of Viktor Klang
Sent: Friday, October 20, 2017 8:06 PM
To: Dimitar Georgiev
Cc: concurrency-interest
Subject: Re: [concurrency-interest] map over j.u.c.CompletableFuture with strict threading semantics

Dimitar,

Just a note: I realized that you can probably get to what you want by controlling injecting a "valve"—CompletableFuture, and only complete that once you want the transformations to be applied:

Pseudocode (since I'm having lunch):
enum Trigger { On; }
CompletableFuture<X> upstream = …
CompletableFuture<Trigger> upstreamTrigger = new CompletableFuture<Trigger>()

CompletableFuture<Y> result = upstream.thenCombineAsync(upstreamTrigger, (x, discard) -> x, YOUR_INTENDED_EXECUTOR).thenApply(…).thenCompose(…)…

upstreamTrigger.complete(Trigger.On); // this will trigger the execution of the transformations on YOUR_INTENDED_EXECUTOR regardless of what transformations are added to `result` as long as they HAPPEN-BEFORE the execution of this line.

YMMV,

√



On Fri, Oct 20, 2017 at 5:32 PM, Dimitar Georgiev <dimitar.georgiev.bg at gmail.com<mailto:dimitar.georgiev.bg at gmail.com>> wrote:
Thanks Viktor! I had not thought of it this way, and you are absolutely right.

I hope it is okay to expand the discussion a little bit and go a bit
off-topic from j.u.c. Viktor probably already sees where this is
going. Next is flatMap:

public static <A, B> CompletableFuture<B> flatMap(CompletableFuture<A>
future, Function<A, CompletableFuture<B>> f);

Semantics my app requires here: if the result of f() is completed on a
new threading context, I want subsequent map / flatMap operations to
use that context. If not (if the result of f() is pure, i.e. of the
form new CompletableFuture<>().complete(value)), continue in the same
threading context.

 Viktor, your remark leads me to think there is no sane way to achieve
this semantics with an eager future implementation such as
CompletableFuture. I would need something lazy which is a description,
creating the thing be referentially transparent, and
submission/execution be decoupled from creating the thing.

To finish the off-topic with the actual question:
- Does such a thing exist in the Java ecosystem at all? I know it does
in Scala, but I need a Java alternative.

P.S. I found a couple of months ago https://github.com/traneio/future.
A claim is made that the threading semantics are what I just
described, but it is an eager future. I will look into the
implementation in the coming days; I expect it to be something along
the "workaround" Viktor described. Myself, I don't feel I am smart
enough or have the testing and formal verification tools to go down
that path...






On 20 October 2017 at 17:52, Viktor Klang <viktor.klang at gmail.com<mailto:viktor.klang at gmail.com>> wrote:
> Hi Dimitar,
>
> In general this is not possible, nor desirable, since completion is likely
> to happen *before* a transformation is added.
>
> However, there is nothing technical which prevents you to create an
> implementation of CompletionStage which stores a reference to the Executor
> to be used, and runs transformations which are applied to it on that
> Executor unless some other Executor is specified.
>
> --
> Cheers,
> √
>
>
> On Oct 20, 2017 09:41, "Dimitar Georgiev" <dimitar.georgiev.bg at gmail.com<mailto:dimitar.georgiev.bg at gmail.com>>
> wrote:
>
> Sorry if this has already been discussed.
>
> I need to implement the following function:
>
> public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
> upstream, Function<A, B> f);
>
> It's pretty apparent what the function does. However, it has the
> requirement that f() be always executed in the thread where upstream
> is completed. (That is, if upstream is completed non-exceptionally,
> since if it was not, f will not be called so there is no threading
> semantics concerning f)
>
> Is this possible with j.u.c.CompletableFuture?
>
> Regards, Dimitar
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>



--
Cheers,
√

Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt, Germany – Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt, Dr. Stefan Sigg; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com




--
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171022/94c3c344/attachment-0001.html>

From viktor.klang at gmail.com  Sun Oct 22 08:31:38 2017
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 22 Oct 2017 14:31:38 +0200
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with
 strict threading semantics
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E4900117826C2F@daeexmbx1.eur.ad.sag>
References: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
 <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>
 <CAKnQuOH2Jv8eLTuzPUKZ_M5mORMDg7mpCN0XyFqDk+Lo7K7SUw@mail.gmail.com>
 <CANPzfU_bctspebBeSGc3nf37GCP3AsP4ftuEfai0RrHNsJDdnw@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E49001178061C5@daeexmbx1.eur.ad.sag>
 <CANPzfU9nTk2B03g3nfJh6NOe4wYOdfo4vNoMi_DbaKGCLnWh0w@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E4900117826C2F@daeexmbx1.eur.ad.sag>
Message-ID: <CANPzfU_tUWdB3KCqCK47xpoQFhtuuyTubku4OZDVHizVmsCvzQ@mail.gmail.com>

You can of course put the executor explicitly everywhere you want, but that
wasn't the OPs question. 😊

-- 
Cheers,
√

On Oct 22, 2017 14:08, "Millies, Sebastian" <
Sebastian.Millies at softwareag.com> wrote:

> I see. What I meant was, why do you need to establish that happens-before
> relationship in such a complicated way if all you want to do is to execute
> a transformation in some given executor known in advance? Instead, can’t
> you just as well supply that executor to one of the async methods in CF in
> each of the downstream transformations?
>
>
>
> *From:* Viktor Klang [mailto:viktor.klang at gmail.com]
> *Sent:* Saturday, October 21, 2017 6:06 PM
> *To:* Millies, Sebastian
> *Cc:* Dimitar Georgiev; concurrency-interest
> *Subject:* Re: [concurrency-interest] map over j.u.c.CompletableFuture
> with strict threading semantics
>
>
>
> Hi Sebastian,
>
>
>
> On Sat, Oct 21, 2017 at 2:01 PM, Millies, Sebastian <
> Sebastian.Millies at softwareag.com> wrote:
>
> Hi,
>
>
>
> I’m sorry to be so dense, but can you explain what the difference is to
> simply upstream.thenApplyAsync(f, YOUR_INTENDED_EXECUTOR) ?
>
>
>
> Sorry, on my way to the airport, and I might have misunderstood your
> question, but think about "Which thread adds the transformations" vs "Which
> thread executes the transformations". (There needs to be a happens-before
> relationship between "all transformations already added" and "intended
> executor executes the value which produces the first result"—so that all
> the transformations are piggybacked on that executor and not the thread
> which adds the transformation.)
>
>
>
> Cheers,
>
> √
>
>
>
>
>
> n  SebaSTIAN
>
>
>
> *From:* Concurrency-interest [mailto:concurrency-interest-
> bounces at cs.oswego.edu] *On Behalf Of *Viktor Klang
> *Sent:* Friday, October 20, 2017 8:06 PM
> *To:* Dimitar Georgiev
> *Cc:* concurrency-interest
> *Subject:* Re: [concurrency-interest] map over j.u.c.CompletableFuture
> with strict threading semantics
>
>
>
> Dimitar,
>
>
>
> Just a note: I realized that you can probably get to what you want by
> controlling injecting a "valve"—CompletableFuture, and only complete that
> once you want the transformations to be applied:
>
>
>
> Pseudocode (since I'm having lunch):
>
> enum Trigger { On; }
>
> CompletableFuture<X> upstream = …
>
> CompletableFuture<Trigger> upstreamTrigger = new
> CompletableFuture<Trigger>()
>
>
>
> CompletableFuture<Y> result = upstream.thenCombineAsync(upstreamTrigger,
> (x, discard) -> x, YOUR_INTENDED_EXECUTOR).thenApply(…).thenCompose(…)…
>
>
>
> upstreamTrigger.complete(Trigger.On); // this will trigger the execution
> of the transformations on YOUR_INTENDED_EXECUTOR regardless of what
> transformations are added to `result` as long as they HAPPEN-BEFORE the
> execution of this line.
>
>
>
> YMMV,
>
>
>
> √
>
>
>
>
>
>
>
> On Fri, Oct 20, 2017 at 5:32 PM, Dimitar Georgiev <
> dimitar.georgiev.bg at gmail.com> wrote:
>
> Thanks Viktor! I had not thought of it this way, and you are absolutely
> right.
>
> I hope it is okay to expand the discussion a little bit and go a bit
> off-topic from j.u.c. Viktor probably already sees where this is
> going. Next is flatMap:
>
> public static <A, B> CompletableFuture<B> flatMap(CompletableFuture<A>
> future, Function<A, CompletableFuture<B>> f);
>
> Semantics my app requires here: if the result of f() is completed on a
> new threading context, I want subsequent map / flatMap operations to
> use that context. If not (if the result of f() is pure, i.e. of the
> form new CompletableFuture<>().complete(value)), continue in the same
> threading context.
>
>  Viktor, your remark leads me to think there is no sane way to achieve
> this semantics with an eager future implementation such as
> CompletableFuture. I would need something lazy which is a description,
> creating the thing be referentially transparent, and
> submission/execution be decoupled from creating the thing.
>
> To finish the off-topic with the actual question:
> - Does such a thing exist in the Java ecosystem at all? I know it does
> in Scala, but I need a Java alternative.
>
> P.S. I found a couple of months ago https://github.com/traneio/future.
> A claim is made that the threading semantics are what I just
> described, but it is an eager future. I will look into the
> implementation in the coming days; I expect it to be something along
> the "workaround" Viktor described. Myself, I don't feel I am smart
> enough or have the testing and formal verification tools to go down
> that path...
>
>
>
>
>
>
>
> On 20 October 2017 at 17:52, Viktor Klang <viktor.klang at gmail.com> wrote:
> > Hi Dimitar,
> >
> > In general this is not possible, nor desirable, since completion is
> likely
> > to happen *before* a transformation is added.
> >
> > However, there is nothing technical which prevents you to create an
> > implementation of CompletionStage which stores a reference to the
> Executor
> > to be used, and runs transformations which are applied to it on that
> > Executor unless some other Executor is specified.
> >
> > --
> > Cheers,
> > √
> >
> >
> > On Oct 20, 2017 09:41, "Dimitar Georgiev" <dimitar.georgiev.bg at gmail.com
> >
> > wrote:
> >
> > Sorry if this has already been discussed.
> >
> > I need to implement the following function:
> >
> > public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
> > upstream, Function<A, B> f);
> >
> > It's pretty apparent what the function does. However, it has the
> > requirement that f() be always executed in the thread where upstream
> > is completed. (That is, if upstream is completed non-exceptionally,
> > since if it was not, f will not be called so there is no threading
> > semantics concerning f)
> >
> > Is this possible with j.u.c.CompletableFuture?
> >
> > Regards, Dimitar
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
>
>
>
>
> --
>
> Cheers,
>
> √
>
>
>
> Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt,
> Germany – Registergericht/Commercial register: Darmstadt HRB 1562 -
> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt, Dr. Stefan Sigg; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>
>
>
>
>
> --
>
> Cheers,
>
> √
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171022/3038b3fd/attachment.html>

From dimitar.georgiev.bg at gmail.com  Sun Oct 22 12:13:34 2017
From: dimitar.georgiev.bg at gmail.com (Dimitar Georgiev)
Date: Sun, 22 Oct 2017 19:13:34 +0300
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with
 strict threading semantics
In-Reply-To: <CANPzfU_tUWdB3KCqCK47xpoQFhtuuyTubku4OZDVHizVmsCvzQ@mail.gmail.com>
References: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
 <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>
 <CAKnQuOH2Jv8eLTuzPUKZ_M5mORMDg7mpCN0XyFqDk+Lo7K7SUw@mail.gmail.com>
 <CANPzfU_bctspebBeSGc3nf37GCP3AsP4ftuEfai0RrHNsJDdnw@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E49001178061C5@daeexmbx1.eur.ad.sag>
 <CANPzfU9nTk2B03g3nfJh6NOe4wYOdfo4vNoMi_DbaKGCLnWh0w@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E4900117826C2F@daeexmbx1.eur.ad.sag>
 <CANPzfU_tUWdB3KCqCK47xpoQFhtuuyTubku4OZDVHizVmsCvzQ@mail.gmail.com>
Message-ID: <CAKnQuOGhxZD8wWYBOgGFZ7j+r6PYLqCq9CSgJNzmpT3ZH2YGxQ@mail.gmail.com>

The point is to have an API where forks only ever happen explicitly, i.e.
when the programmer returns a future that will complete in a new thread via
flatMap, or when explicitly invoking a fork() operator.

In all other cases map() and flatMap() will run in upstream's thread.

This model of threading is, I think, roughly equivalent to
scalaz.concurrent.Task, if you are familiar with that.

 I will push a testcase sometime this week to show more explicitly what I
expect, and try the trick Viktor suggested. Thanks everyone!

On 22 October 2017 at 15:31, Viktor Klang <viktor.klang at gmail.com> wrote:

> You can of course put the executor explicitly everywhere you want, but
> that wasn't the OPs question. 😊
>
> --
> Cheers,
> √
>
> On Oct 22, 2017 14:08, "Millies, Sebastian" <Sebastian.Millies at softwareag.
> com> wrote:
>
>> I see. What I meant was, why do you need to establish that happens-before
>> relationship in such a complicated way if all you want to do is to execute
>> a transformation in some given executor known in advance? Instead, can’t
>> you just as well supply that executor to one of the async methods in CF in
>> each of the downstream transformations?
>>
>>
>>
>> *From:* Viktor Klang [mailto:viktor.klang at gmail.com]
>> *Sent:* Saturday, October 21, 2017 6:06 PM
>> *To:* Millies, Sebastian
>> *Cc:* Dimitar Georgiev; concurrency-interest
>> *Subject:* Re: [concurrency-interest] map over j.u.c.CompletableFuture
>> with strict threading semantics
>>
>>
>>
>> Hi Sebastian,
>>
>>
>>
>> On Sat, Oct 21, 2017 at 2:01 PM, Millies, Sebastian <
>> Sebastian.Millies at softwareag.com> wrote:
>>
>> Hi,
>>
>>
>>
>> I’m sorry to be so dense, but can you explain what the difference is to
>> simply upstream.thenApplyAsync(f, YOUR_INTENDED_EXECUTOR) ?
>>
>>
>>
>> Sorry, on my way to the airport, and I might have misunderstood your
>> question, but think about "Which thread adds the transformations" vs "Which
>> thread executes the transformations". (There needs to be a happens-before
>> relationship between "all transformations already added" and "intended
>> executor executes the value which produces the first result"—so that all
>> the transformations are piggybacked on that executor and not the thread
>> which adds the transformation.)
>>
>>
>>
>> Cheers,
>>
>> √
>>
>>
>>
>>
>>
>> n  SebaSTIAN
>>
>>
>>
>> *From:* Concurrency-interest [mailto:concurrency-interest-b
>> ounces at cs.oswego.edu] *On Behalf Of *Viktor Klang
>> *Sent:* Friday, October 20, 2017 8:06 PM
>> *To:* Dimitar Georgiev
>> *Cc:* concurrency-interest
>> *Subject:* Re: [concurrency-interest] map over j.u.c.CompletableFuture
>> with strict threading semantics
>>
>>
>>
>> Dimitar,
>>
>>
>>
>> Just a note: I realized that you can probably get to what you want by
>> controlling injecting a "valve"—CompletableFuture, and only complete that
>> once you want the transformations to be applied:
>>
>>
>>
>> Pseudocode (since I'm having lunch):
>>
>> enum Trigger { On; }
>>
>> CompletableFuture<X> upstream = …
>>
>> CompletableFuture<Trigger> upstreamTrigger = new
>> CompletableFuture<Trigger>()
>>
>>
>>
>> CompletableFuture<Y> result = upstream.thenCombineAsync(upstreamTrigger,
>> (x, discard) -> x, YOUR_INTENDED_EXECUTOR).thenApply(…).thenCompose(…)…
>>
>>
>>
>> upstreamTrigger.complete(Trigger.On); // this will trigger the execution
>> of the transformations on YOUR_INTENDED_EXECUTOR regardless of what
>> transformations are added to `result` as long as they HAPPEN-BEFORE the
>> execution of this line.
>>
>>
>>
>> YMMV,
>>
>>
>>
>> √
>>
>>
>>
>>
>>
>>
>>
>> On Fri, Oct 20, 2017 at 5:32 PM, Dimitar Georgiev <
>> dimitar.georgiev.bg at gmail.com> wrote:
>>
>> Thanks Viktor! I had not thought of it this way, and you are absolutely
>> right.
>>
>> I hope it is okay to expand the discussion a little bit and go a bit
>> off-topic from j.u.c. Viktor probably already sees where this is
>> going. Next is flatMap:
>>
>> public static <A, B> CompletableFuture<B> flatMap(CompletableFuture<A>
>> future, Function<A, CompletableFuture<B>> f);
>>
>> Semantics my app requires here: if the result of f() is completed on a
>> new threading context, I want subsequent map / flatMap operations to
>> use that context. If not (if the result of f() is pure, i.e. of the
>> form new CompletableFuture<>().complete(value)), continue in the same
>> threading context.
>>
>>  Viktor, your remark leads me to think there is no sane way to achieve
>> this semantics with an eager future implementation such as
>> CompletableFuture. I would need something lazy which is a description,
>> creating the thing be referentially transparent, and
>> submission/execution be decoupled from creating the thing.
>>
>> To finish the off-topic with the actual question:
>> - Does such a thing exist in the Java ecosystem at all? I know it does
>> in Scala, but I need a Java alternative.
>>
>> P.S. I found a couple of months ago https://github.com/traneio/future.
>> A claim is made that the threading semantics are what I just
>> described, but it is an eager future. I will look into the
>> implementation in the coming days; I expect it to be something along
>> the "workaround" Viktor described. Myself, I don't feel I am smart
>> enough or have the testing and formal verification tools to go down
>> that path...
>>
>>
>>
>>
>>
>>
>>
>> On 20 October 2017 at 17:52, Viktor Klang <viktor.klang at gmail.com> wrote:
>> > Hi Dimitar,
>> >
>> > In general this is not possible, nor desirable, since completion is
>> likely
>> > to happen *before* a transformation is added.
>> >
>> > However, there is nothing technical which prevents you to create an
>> > implementation of CompletionStage which stores a reference to the
>> Executor
>> > to be used, and runs transformations which are applied to it on that
>> > Executor unless some other Executor is specified.
>> >
>> > --
>> > Cheers,
>> > √
>> >
>> >
>> > On Oct 20, 2017 09:41, "Dimitar Georgiev" <
>> dimitar.georgiev.bg at gmail.com>
>> > wrote:
>> >
>> > Sorry if this has already been discussed.
>> >
>> > I need to implement the following function:
>> >
>> > public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
>> > upstream, Function<A, B> f);
>> >
>> > It's pretty apparent what the function does. However, it has the
>> > requirement that f() be always executed in the thread where upstream
>> > is completed. (That is, if upstream is completed non-exceptionally,
>> > since if it was not, f will not be called so there is no threading
>> > semantics concerning f)
>> >
>> > Is this possible with j.u.c.CompletableFuture?
>> >
>> > Regards, Dimitar
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>>
>>
>>
>>
>>
>> --
>>
>> Cheers,
>>
>> √
>>
>>
>>
>> Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt,
>> Germany – Registergericht/Commercial register: Darmstadt HRB 1562 -
>> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
>> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt, Dr. Stefan Sigg; -
>> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
>> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>>
>>
>>
>>
>>
>> --
>>
>> Cheers,
>>
>> √
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171022/5b54e2bc/attachment.html>

From viktor.klang at gmail.com  Sun Oct 22 13:39:47 2017
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 22 Oct 2017 19:39:47 +0200
Subject: [concurrency-interest] map over j.u.c.CompletableFuture with
 strict threading semantics
In-Reply-To: <CANPzfU_o1uhA2J5jT3wHO_j1LYxnOAj7XHfxMT=NJhp0WgXD9w@mail.gmail.com>
References: <CAKnQuOF58BNTLMtPdRpd-HOcLM5y6XcGRkmD=iURUbWA_rCEKQ@mail.gmail.com>
 <CANPzfU-dR9hzgDfYzuAseOoka7=RBJhsw7nzqwX3cJ1qb_yzTQ@mail.gmail.com>
 <CAKnQuOH2Jv8eLTuzPUKZ_M5mORMDg7mpCN0XyFqDk+Lo7K7SUw@mail.gmail.com>
 <CANPzfU_bctspebBeSGc3nf37GCP3AsP4ftuEfai0RrHNsJDdnw@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E49001178061C5@daeexmbx1.eur.ad.sag>
 <CANPzfU9nTk2B03g3nfJh6NOe4wYOdfo4vNoMi_DbaKGCLnWh0w@mail.gmail.com>
 <32F15738E8E5524DA4F01A0FA4A8E4900117826C2F@daeexmbx1.eur.ad.sag>
 <CANPzfU_tUWdB3KCqCK47xpoQFhtuuyTubku4OZDVHizVmsCvzQ@mail.gmail.com>
 <CAKnQuOGhxZD8wWYBOgGFZ7j+r6PYLqCq9CSgJNzmpT3ZH2YGxQ@mail.gmail.com>
 <CANPzfU-iNGUshaWkrM25SciJqn_oTFLiPk19RH96WmbXECS8kQ@mail.gmail.com>
 <CANPzfU_o1uhA2J5jT3wHO_j1LYxnOAj7XHfxMT=NJhp0WgXD9w@mail.gmail.com>
Message-ID: <CANPzfU_N+D2PLCb73373v0FPfQXUhLhjzxVU1KfLnzVHuZqCeQ@mail.gmail.com>

Dimitar,

Worth noting is that Task is a completely different construct. (Not saying
Task is not useful, just saying it is a different construct.)

Also, for context, Scala Futures does not support "run it on the completing
thread OR the attaching thread, whichever happens first during execution",
as explained here by Havoc Pennington:
https://blog.ometer.com/2011/07/24/callbacks-synchronous-and-asynchronous/

Most times running something on the completing thread is a questionable
idea since now it is difficult to reason about which Executor performs what
workloads, and when it is safe to shut down the ExecutorService from a
lifecycle/ownership perspective as it is being used to execute unknown
workloads.

-- 
Cheers,
√

On Oct 22, 2017 18:13, "Dimitar Georgiev" <dimitar.georgiev.bg at gmail.com>
wrote:

The point is to have an API where forks only ever happen explicitly, i.e.
when the programmer returns a future that will complete in a new thread via
flatMap, or when explicitly invoking a fork() operator.

In all other cases map() and flatMap() will run in upstream's thread.

This model of threading is, I think, roughly equivalent to
scalaz.concurrent.Task, if you are familiar with that.

 I will push a testcase sometime this week to show more explicitly what I
expect, and try the trick Viktor suggested. Thanks everyone!

On 22 October 2017 at 15:31, Viktor Klang <viktor.klang at gmail.com> wrote:

> You can of course put the executor explicitly everywhere you want, but
> that wasn't the OPs question. 😊
>
> --
> Cheers,
> √
>
> On Oct 22, 2017 14:08, "Millies, Sebastian" <Sebastian.Millies at softwareag.
> com> wrote:
>
>> I see. What I meant was, why do you need to establish that happens-before
>> relationship in such a complicated way if all you want to do is to execute
>> a transformation in some given executor known in advance? Instead, can’t
>> you just as well supply that executor to one of the async methods in CF in
>> each of the downstream transformations?
>>
>>
>>
>> *From:* Viktor Klang [mailto:viktor.klang at gmail.com]
>> *Sent:* Saturday, October 21, 2017 6:06 PM
>> *To:* Millies, Sebastian
>> *Cc:* Dimitar Georgiev; concurrency-interest
>> *Subject:* Re: [concurrency-interest] map over j.u.c.CompletableFuture
>> with strict threading semantics
>>
>>
>>
>> Hi Sebastian,
>>
>>
>>
>> On Sat, Oct 21, 2017 at 2:01 PM, Millies, Sebastian <
>> Sebastian.Millies at softwareag.com> wrote:
>>
>> Hi,
>>
>>
>>
>> I’m sorry to be so dense, but can you explain what the difference is to
>> simply upstream.thenApplyAsync(f, YOUR_INTENDED_EXECUTOR) ?
>>
>>
>>
>> Sorry, on my way to the airport, and I might have misunderstood your
>> question, but think about "Which thread adds the transformations" vs "Which
>> thread executes the transformations". (There needs to be a happens-before
>> relationship between "all transformations already added" and "intended
>> executor executes the value which produces the first result"—so that all
>> the transformations are piggybacked on that executor and not the thread
>> which adds the transformation.)
>>
>>
>>
>> Cheers,
>>
>> √
>>
>>
>>
>>
>>
>> n  SebaSTIAN
>>
>>
>>
>> *From:* Concurrency-interest [mailto:concurrency-interest-b
>> ounces at cs.oswego.edu] *On Behalf Of *Viktor Klang
>> *Sent:* Friday, October 20, 2017 8:06 PM
>> *To:* Dimitar Georgiev
>> *Cc:* concurrency-interest
>> *Subject:* Re: [concurrency-interest] map over j.u.c.CompletableFuture
>> with strict threading semantics
>>
>>
>>
>> Dimitar,
>>
>>
>>
>> Just a note: I realized that you can probably get to what you want by
>> controlling injecting a "valve"—CompletableFuture, and only complete that
>> once you want the transformations to be applied:
>>
>>
>>
>> Pseudocode (since I'm having lunch):
>>
>> enum Trigger { On; }
>>
>> CompletableFuture<X> upstream = …
>>
>> CompletableFuture<Trigger> upstreamTrigger = new
>> CompletableFuture<Trigger>()
>>
>>
>>
>> CompletableFuture<Y> result = upstream.thenCombineAsync(upstreamTrigger,
>> (x, discard) -> x, YOUR_INTENDED_EXECUTOR).thenApply(…).thenCompose(…)…
>>
>>
>>
>> upstreamTrigger.complete(Trigger.On); // this will trigger the execution
>> of the transformations on YOUR_INTENDED_EXECUTOR regardless of what
>> transformations are added to `result` as long as they HAPPEN-BEFORE the
>> execution of this line.
>>
>>
>>
>> YMMV,
>>
>>
>>
>> √
>>
>>
>>
>>
>>
>>
>>
>> On Fri, Oct 20, 2017 at 5:32 PM, Dimitar Georgiev <
>> dimitar.georgiev.bg at gmail.com> wrote:
>>
>> Thanks Viktor! I had not thought of it this way, and you are absolutely
>> right.
>>
>> I hope it is okay to expand the discussion a little bit and go a bit
>> off-topic from j.u.c. Viktor probably already sees where this is
>> going. Next is flatMap:
>>
>> public static <A, B> CompletableFuture<B> flatMap(CompletableFuture<A>
>> future, Function<A, CompletableFuture<B>> f);
>>
>> Semantics my app requires here: if the result of f() is completed on a
>> new threading context, I want subsequent map / flatMap operations to
>> use that context. If not (if the result of f() is pure, i.e. of the
>> form new CompletableFuture<>().complete(value)), continue in the same
>> threading context.
>>
>>  Viktor, your remark leads me to think there is no sane way to achieve
>> this semantics with an eager future implementation such as
>> CompletableFuture. I would need something lazy which is a description,
>> creating the thing be referentially transparent, and
>> submission/execution be decoupled from creating the thing.
>>
>> To finish the off-topic with the actual question:
>> - Does such a thing exist in the Java ecosystem at all? I know it does
>> in Scala, but I need a Java alternative.
>>
>> P.S. I found a couple of months ago https://github.com/traneio/future.
>> A claim is made that the threading semantics are what I just
>> described, but it is an eager future. I will look into the
>> implementation in the coming days; I expect it to be something along
>> the "workaround" Viktor described. Myself, I don't feel I am smart
>> enough or have the testing and formal verification tools to go down
>> that path...
>>
>>
>>
>>
>>
>>
>>
>> On 20 October 2017 at 17:52, Viktor Klang <viktor.klang at gmail.com> wrote:
>> > Hi Dimitar,
>> >
>> > In general this is not possible, nor desirable, since completion is
>> likely
>> > to happen *before* a transformation is added.
>> >
>> > However, there is nothing technical which prevents you to create an
>> > implementation of CompletionStage which stores a reference to the
>> Executor
>> > to be used, and runs transformations which are applied to it on that
>> > Executor unless some other Executor is specified.
>> >
>> > --
>> > Cheers,
>> > √
>> >
>> >
>> > On Oct 20, 2017 09:41, "Dimitar Georgiev" <
>> dimitar.georgiev.bg at gmail.com>
>> > wrote:
>> >
>> > Sorry if this has already been discussed.
>> >
>> > I need to implement the following function:
>> >
>> > public static <A, B> CompletableFuture<B> map(CompletableFuture<A>
>> > upstream, Function<A, B> f);
>> >
>> > It's pretty apparent what the function does. However, it has the
>> > requirement that f() be always executed in the thread where upstream
>> > is completed. (That is, if upstream is completed non-exceptionally,
>> > since if it was not, f will not be called so there is no threading
>> > semantics concerning f)
>> >
>> > Is this possible with j.u.c.CompletableFuture?
>> >
>> > Regards, Dimitar
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>>
>>
>>
>>
>>
>> --
>>
>> Cheers,
>>
>> √
>>
>>
>>
>> Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt,
>> Germany – Registergericht/Commercial register: Darmstadt HRB 1562 -
>> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
>> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt, Dr. Stefan Sigg; -
>> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
>> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>>
>>
>>
>>
>>
>> --
>>
>> Cheers,
>>
>> √
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171022/c2241f52/attachment-0001.html>

From rl.stpuu at gmail.com  Sun Oct 22 14:43:57 2017
From: rl.stpuu at gmail.com (Roussanka Loukanova)
Date: Sun, 22 Oct 2017 20:43:57 +0200
Subject: [concurrency-interest] CfP: Natural Language Processing in
	Artificial Intelligence - NLPinAI 2018
Message-ID: <CACAe74hGDxfQMNxvVUox3Q35uNWGyV20btLBaEbBcb04siOLzg@mail.gmail.com>

CALL FOR PAPERS

Special Session on
Natural Language Processing in Artificial Intelligence - NLPinAI 2018

16-18 January, 2018 - Funchal, Madeira, Portugal

http://www.icaart.org/NLPinAI.aspx

Within the 10th International Conference on Agents and Artificial
Intelligence - ICAART 2018

-------------------------------------------------------------
SCOPE:
Computational and technological developments that incorporate natural
language are proliferating. Adequate coverage encounters difficult problems
related to partiality, underspecification, and context-dependency, which
are signature features of information in nature and natural languages.
Furthermore, agents (humans or computational systems) are information
conveyors, interpreters, or participate as components of informational
content. Generally, language processing depends on agents' knowledge,
reasoning, perspectives, and interactions.

The session covers theoretical work, advanced applications, approaches, and
techniques for computational models of information and its presentation by
language (artificial, human, or natural in other ways). The goal is to
promote intelligent natural language processing and related models of
thought, mental states, reasoning, and other cognitive processes.

TOPICS:
We invite contributions relevant to the following topics, without limiting
to them:

- Type theories for applications to language and information processing
- Computational grammar
- Computational syntax
- Computational semantics of natural languages
- Computational syntax-semantics interface
- Interfaces between morphology, lexicon, syntax, semantics, speech, text,
pragmatics
- Parsing
- Multilingual processing
- Large-scale grammars of natural languages
- Interfaces between morphology, lexicon, syntax, semantics, speech, text,
pragmatics
- Models of computation and algorithms for natural language processing
- Computational models of partiality, underspecification, and
context-dependency
- Models of situations, contexts, and agents, for applications to language
processing
- Information about space and time in language models and processing
- Models of computation and algorithms for linguistics
- Data science in language processing
- Machine learning of language
- Interdisciplinary methods
- Integration of formal, computational, model theoretic, graphical,
diagrammatic, statistical, and other related methods
- Logic for information extraction or expression in written and spoken
language
- Language processing based on biological fundamentals of information and
languages
- Computational neuroscience of language

IMPORTANT DATES:
Paper Submission: November 7, 2017
Authors Notification: November 21, 2017
Camera Ready and Registration: November 29, 2017

PAPER SUBMISSION:
Authors can submit their work in the form of a Regular Paper, representing
completed and validated research, or as a Position Paper, for preliminary
work in progress.

Regular Papers
- Submission:
It is recommended that Regular Papers are submitted for review with around
8 to 10 pages
- Acceptance:
After a double-blind peer review, qualifying Regular Papers may be accepted
as either Full Papers or Short Papers
- Publication:
Regular Papers classified as Full Papers will be assigned a 12-page limit
in the Conference Proceedings, while Regular Papers classified as Short
Papers have an 8-page limit

Position Papers
- Submission:
Position Papers should be submitted for review with around 6 or 7 pages
- Acceptance:
After a double-blind peer review, qualifying Position Papers will be
accepted as Short Papers
- Publication: Position Papers will be assigned a 8-page limit in the
Conference Proceedings

Instructions for preparing the manuscript (in Word and Latex formats) are
available at the page with paper Templates:

http://www.icaart.org/Templates.aspx

Please also check the Guidelines:

http://www.icaart.org/Guidelines.aspx

Papers must be submitted electronically via the web-based submission system
using the appropriated button Submit Paper on the pages of NLPinAI 2018.

The Conference Proceedings will be published under an ISBN number by
SCITEPRESS and include final versions of all accepted papers, adjusted to
satisfy reviewers' recommendations. They will be obtainable on paper and
CD-Rom support, and made available for online consultation at the
SCITEPRESS Digital Library. Online publication is exclusive to papers which
have been both published and presented at the event.

Indexation: The proceedings will be submitted to Thomson Reuters Conference
Proceedings Citation Index (CPCI/ISI), INSPEC, DBLP, EI (Elsevier
Engineering Village Index) and Scopus for indexation.

-------------------------------------------------------------
CHAIRS:
Roussanka Loukanova
Stockholm University, Sweden

Aarne Ranta
University of Gothenburg
Sweden

CONTACT:
Roussanka Loukanova (rloukanova at gmail.com)
-------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171022/0bdc3e7c/attachment.html>

From thurston at nomagicsoftware.com  Mon Oct 23 16:11:41 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 23 Oct 2017 13:11:41 -0700 (MST)
Subject: [concurrency-interest] Memory Semantics of CompletableFuture
Message-ID: <1508789501713-0.post@n7.nabble.com>

Hello,

I wanted to inquire about the memory visibility semantics/guarantees of 
CompletableFuture#runAfterBothAsync
and 
CompetableFuture#thenRunAsync

I have a dependency graph that looks like the following:
3 initial tasks (A, B, C) which run independently, then afterwards (looping
through some rows)
C depends on (prior) C and prior B
B depends on (prior) B and prior A
A depends on prior A,

so code looks like (in a loop):
C = C.runAfterBothAsync(B, lambda)
B = B.runAfterBothAsync(A, lambda)
A = A.thenRunAsync(lambda)

the lambdas write to a shared array, but there is no clobbering (a single
write to each cell), my understanding is that each "new" CF, is *guaranteed*
to see the writes of any of its dependencies before it executes.  Surely
that is correct, although CF's javadoc is strangely silent on any memory
guarantees.
Also I'm assuming the MV guarantees extend to the master thread (the one
invoking the above code). i.e. any writes it has made happen before any
reads/writes in the CF's

Anyway, In my tests I get inconsistent results, surely due to some memory
visibility issues; I'm not 100% sure it's not a problem on my end, but I
wanted to get confirmation on what the memory visibility guarantees there
are or are not;


*I could instead, of course, join on all 3 CF's before "submitting" new
ones, creating any new ones in each iteration, but I really like the above
style and prefer not to have the master thread block at all (until all rows
have been processed)



--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From martinrb at google.com  Mon Oct 23 16:23:08 2017
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 23 Oct 2017 13:23:08 -0700
Subject: [concurrency-interest] Memory Semantics of CompletableFuture
In-Reply-To: <1508789501713-0.post@n7.nabble.com>
References: <1508789501713-0.post@n7.nabble.com>
Message-ID: <CA+kOe08MV1KH=djrUB3V+hJ5SWQrP2dfK=tZOw2cN-GN2vEShA@mail.gmail.com>

Are you not expecting the lambdas to run concurrently?

On Mon, Oct 23, 2017 at 1:11 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> Hello,
>
> I wanted to inquire about the memory visibility semantics/guarantees of
> CompletableFuture#runAfterBothAsync
> and
> CompetableFuture#thenRunAsync
>
> I have a dependency graph that looks like the following:
> 3 initial tasks (A, B, C) which run independently, then afterwards (looping
> through some rows)
> C depends on (prior) C and prior B
> B depends on (prior) B and prior A
> A depends on prior A,
>
> so code looks like (in a loop):
> C = C.runAfterBothAsync(B, lambda)
> B = B.runAfterBothAsync(A, lambda)
> A = A.thenRunAsync(lambda)
>
> the lambdas write to a shared array, but there is no clobbering (a single
> write to each cell), my understanding is that each "new" CF, is
> *guaranteed*
> to see the writes of any of its dependencies before it executes.  Surely
> that is correct, although CF's javadoc is strangely silent on any memory
> guarantees.
> Also I'm assuming the MV guarantees extend to the master thread (the one
> invoking the above code). i.e. any writes it has made happen before any
> reads/writes in the CF's
>
> Anyway, In my tests I get inconsistent results, surely due to some memory
> visibility issues; I'm not 100% sure it's not a problem on my end, but I
> wanted to get confirmation on what the memory visibility guarantees there
> are or are not;
>
>
> *I could instead, of course, join on all 3 CF's before "submitting" new
> ones, creating any new ones in each iteration, but I really like the above
> style and prefer not to have the master thread block at all (until all rows
> have been processed)
>
>
>
> --
> Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171023/68e0df27/attachment.html>

From thurston at nomagicsoftware.com  Mon Oct 23 16:33:17 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 23 Oct 2017 13:33:17 -0700 (MST)
Subject: [concurrency-interest] Memory Semantics of CompletableFuture
In-Reply-To: <CA+kOe08MV1KH=djrUB3V+hJ5SWQrP2dfK=tZOw2cN-GN2vEShA@mail.gmail.com>
References: <1508789501713-0.post@n7.nabble.com>
 <CA+kOe08MV1KH=djrUB3V+hJ5SWQrP2dfK=tZOw2cN-GN2vEShA@mail.gmail.com>
Message-ID: <1508790797916-0.post@n7.nabble.com>

I'm not sure I understand; of course A, B, C can run concurrently.

It's just the "new" C, has to wait for the "old" C, and "old" B, etc.
But at most (A, B, C) are running at the same time.

So, e.g. if the initial A fell behind, the new B would wait



--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From martinrb at google.com  Mon Oct 23 16:38:23 2017
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 23 Oct 2017 13:38:23 -0700
Subject: [concurrency-interest] Memory Semantics of CompletableFuture
In-Reply-To: <1508790797916-0.post@n7.nabble.com>
References: <1508789501713-0.post@n7.nabble.com>
 <CA+kOe08MV1KH=djrUB3V+hJ5SWQrP2dfK=tZOw2cN-GN2vEShA@mail.gmail.com>
 <1508790797916-0.post@n7.nabble.com>
Message-ID: <CA+kOe08X7c+K30TjHzn0fHAq3GCV0c8Ub8cPJh76dEx+Jqy2mA@mail.gmail.com>

Are you confusing the lambdas with their dependencies (A, B, C)?

Again I ask: Are you not expecting the lambdas to run concurrently?

On Mon, Oct 23, 2017 at 1:33 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> I'm not sure I understand; of course A, B, C can run concurrently.
>
> It's just the "new" C, has to wait for the "old" C, and "old" B, etc.
> But at most (A, B, C) are running at the same time.
>
> So, e.g. if the initial A fell behind, the new B would wait
>
>
>
> --
> Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171023/6f0029e4/attachment.html>

From thurston at nomagicsoftware.com  Mon Oct 23 16:44:42 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 23 Oct 2017 13:44:42 -0700 (MST)
Subject: [concurrency-interest] Memory Semantics of CompletableFuture
In-Reply-To: <CA+kOe08X7c+K30TjHzn0fHAq3GCV0c8Ub8cPJh76dEx+Jqy2mA@mail.gmail.com>
References: <1508789501713-0.post@n7.nabble.com>
 <CA+kOe08MV1KH=djrUB3V+hJ5SWQrP2dfK=tZOw2cN-GN2vEShA@mail.gmail.com>
 <1508790797916-0.post@n7.nabble.com>
 <CA+kOe08X7c+K30TjHzn0fHAq3GCV0c8Ub8cPJh76dEx+Jqy2mA@mail.gmail.com>
Message-ID: <1508791482840-0.post@n7.nabble.com>

Again, I answer.
I expect the lambdas to run concurrently, and *they do* - this is certain;
the lambdas are the CFs, or to be more precise the CFs are just wrappers
around the lambdas.

I was asking about the memory visibility guarantees of the respective
methods



--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From ionutb83 at yahoo.com  Tue Oct 24 04:32:22 2017
From: ionutb83 at yahoo.com (Ionut)
Date: Tue, 24 Oct 2017 08:32:22 +0000 (UTC)
Subject: [concurrency-interest] [JDK-8129920] Vectorized loop unrolling for
	x64 ?
References: <1598356757.3225648.1508833942659.ref@mail.yahoo.com>
Message-ID: <1598356757.3225648.1508833942659@mail.yahoo.com>

Hello All,
    I want to ask you about https://bugs.openjdk.java.net/browse/JDK-8129920 - Vectorized loop unrolling which says it is applicable only for x86 targets. Do you plan to fix this for x64 as well? Or I miss something here?
RegardsIonut
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171024/05004838/attachment.html>

From davidcholmes at aapt.net.au  Tue Oct 24 04:43:05 2017
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 24 Oct 2017 18:43:05 +1000
Subject: [concurrency-interest] [JDK-8129920] Vectorized loop unrolling
	for	x64 ?
In-Reply-To: <1598356757.3225648.1508833942659@mail.yahoo.com>
References: <1598356757.3225648.1508833942659.ref@mail.yahoo.com>
 <1598356757.3225648.1508833942659@mail.yahoo.com>
Message-ID: <006601d34ca4$1c94fcf0$55bef6d0$@aapt.net.au>

Wrong mailing list 😊

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Ionut
Sent: Tuesday, October 24, 2017 6:32 PM
To: Concurrency-interest <concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] [JDK-8129920] Vectorized loop unrolling for x64 ?

 

Hello All,

 

    I want to ask you about https://bugs.openjdk.java.net/browse/JDK-8129920 - Vectorized loop unrolling which says it is applicable only for x86 targets. Do you plan to fix this for x64 as well? Or I miss something here?

 

Regards

Ionut

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171024/a36b3759/attachment-0001.html>

From ionutb83 at yahoo.com  Tue Oct 24 04:45:09 2017
From: ionutb83 at yahoo.com (Ionut)
Date: Tue, 24 Oct 2017 08:45:09 +0000 (UTC)
Subject: [concurrency-interest] []JDK-8129920] Vectorized loop unrolling on
	x64?
References: <1351814659.3210894.1508834709497.ref@mail.yahoo.com>
Message-ID: <1351814659.3210894.1508834709497@mail.yahoo.com>

Hello All,
    I want to ask you about https://bugs.openjdk.java.net/browse/JDK-8129920 - Vectorized loop unrolling which says it is applicable only for x86 targets. Do you plan to fix this for x64 as well? Or I miss something here?
RegardsIonut
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171024/6cc6f10c/attachment.html>

From martinrb at google.com  Wed Oct 25 02:47:10 2017
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 24 Oct 2017 23:47:10 -0700
Subject: [concurrency-interest] Memory Semantics of CompletableFuture
In-Reply-To: <1508791482840-0.post@n7.nabble.com>
References: <1508789501713-0.post@n7.nabble.com>
 <CA+kOe08MV1KH=djrUB3V+hJ5SWQrP2dfK=tZOw2cN-GN2vEShA@mail.gmail.com>
 <1508790797916-0.post@n7.nabble.com>
 <CA+kOe08X7c+K30TjHzn0fHAq3GCV0c8Ub8cPJh76dEx+Jqy2mA@mail.gmail.com>
 <1508791482840-0.post@n7.nabble.com>
Message-ID: <CA+kOe0-MjnZsPUr4tZHSRpPP4i2o06iC-1H+gBGAOLjd13e5sw@mail.gmail.com>

On Mon, Oct 23, 2017 at 1:44 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> Again, I answer.
> I expect the lambdas to run concurrently, and *they do* - this is certain;
> the lambdas are the CFs, or to be more precise the CFs are just wrappers
> around the lambdas.
>
> I was asking about the memory visibility guarantees of the respective
> methods
>
>
I agree we should say more about memory visibility guarantees.

But common sense suggests that completion/start of dependent actions has
the semantics of volatile variable write/read ("total order of
synchronization actions")
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171024/bb4d3eca/attachment.html>

From dholmes at ieee.org  Wed Oct 25 09:03:34 2017
From: dholmes at ieee.org (dholmes at ieee.org)
Date: Wed, 25 Oct 2017 23:03:34 +1000
Subject: [concurrency-interest] OpenJDK Project Loom - lightweight threads
	for Java
Message-ID: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>

This should be of interest to the followers of this mailing list.

http://mail.openjdk.java.net/pipermail/discuss/2017-September/004390.html

http://cr.openjdk.java.net/~rpressler/loom/Loom-Proposal.html

David

From thurston at nomagicsoftware.com  Wed Oct 25 13:28:24 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 25 Oct 2017 10:28:24 -0700 (MST)
Subject: [concurrency-interest] Memory Semantics of CompletableFuture
In-Reply-To: <CA+kOe0-MjnZsPUr4tZHSRpPP4i2o06iC-1H+gBGAOLjd13e5sw@mail.gmail.com>
References: <1508789501713-0.post@n7.nabble.com>
 <CA+kOe08MV1KH=djrUB3V+hJ5SWQrP2dfK=tZOw2cN-GN2vEShA@mail.gmail.com>
 <1508790797916-0.post@n7.nabble.com>
 <CA+kOe08X7c+K30TjHzn0fHAq3GCV0c8Ub8cPJh76dEx+Jqy2mA@mail.gmail.com>
 <1508791482840-0.post@n7.nabble.com>
 <CA+kOe0-MjnZsPUr4tZHSRpPP4i2o06iC-1H+gBGAOLjd13e5sw@mail.gmail.com>
Message-ID: <1508952504446-0.post@n7.nabble.com>

I completely agree with that, in fact I would suggest that "common sanity"
demands it;
anyway, the problem turned out to be, not surprisingly, on my end



--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From thurston at nomagicsoftware.com  Wed Oct 25 13:31:18 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 25 Oct 2017 10:31:18 -0700 (MST)
Subject: [concurrency-interest] OpenJDK Project Loom - lightweight
	threads for Java
In-Reply-To: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
References: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
Message-ID: <1508952678128-0.post@n7.nabble.com>

Love it.

BTW, for those with institutional memory, isn't that what "green threads"
more or less were meant to be, back in the earlier days of Java?  Before my
time, and I never used them . . .



--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From ariel at weisberg.ws  Wed Oct 25 14:11:45 2017
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Wed, 25 Oct 2017 14:11:45 -0400
Subject: [concurrency-interest] OpenJDK Project Loom - lightweight
 threads for Java
In-Reply-To: <1508952678128-0.post@n7.nabble.com>
References: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
 <1508952678128-0.post@n7.nabble.com>
Message-ID: <1508955105.2307841.1150876216.788FB35C@webmail.messagingengine.com>

Hi,

I am very excited for this. This is the #1 ergonomic enhancement for me.

Ariel

On Wed, Oct 25, 2017, at 01:31 PM, thurstonn wrote:
> Love it.
> 
> BTW, for those with institutional memory, isn't that what "green threads"
> more or less were meant to be, back in the earlier days of Java?  Before
> my
> time, and I never used them . . .
> 
> 
> 
> --
> Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From anmiller at redhat.com  Wed Oct 25 14:42:03 2017
From: anmiller at redhat.com (Andrig Miller)
Date: Wed, 25 Oct 2017 12:42:03 -0600
Subject: [concurrency-interest] OpenJDK Project Loom - lightweight
 threads for Java
In-Reply-To: <1508952678128-0.post@n7.nabble.com>
References: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
 <1508952678128-0.post@n7.nabble.com>
Message-ID: <CAF1-sp7mVD14tP+G24TTwYS5nfZbpfy8iMVQCzp95wAw7BPsnQ@mail.gmail.com>

On Wed, Oct 25, 2017 at 11:31 AM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> Love it.
>
> BTW, for those with institutional memory, isn't that what "green threads"
> more or less were meant to be, back in the earlier days of Java?  Before my
> time, and I never used them . . .
>

​If memory serves me correctly, green threads were only present for
platforms that didn't have threads in their kernel.

Andy
​

>
>
>
> --
> Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Andrig (Andy) T. Miller
Global Platform Director, Middleware
Red Hat, Inc.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171025/999d1abc/attachment.html>

From jbloch at gmail.com  Wed Oct 25 15:34:28 2017
From: jbloch at gmail.com (Joshua Bloch)
Date: Wed, 25 Oct 2017 12:34:28 -0700
Subject: [concurrency-interest] OpenJDK Project Loom - lightweight
 threads for Java
In-Reply-To: <CAF1-sp7mVD14tP+G24TTwYS5nfZbpfy8iMVQCzp95wAw7BPsnQ@mail.gmail.com>
References: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
 <1508952678128-0.post@n7.nabble.com>
 <CAF1-sp7mVD14tP+G24TTwYS5nfZbpfy8iMVQCzp95wAw7BPsnQ@mail.gmail.com>
Message-ID: <CAP0L=UR6-m7ex-rR1gokAA5aVWmYOHaL9su2SOQuggYBkk4d9Q@mail.gmail.com>

This is very similar in spirit to the "many-to-few" threading model that
was popular in the '90s. You can read about it in Butenhof's *Programming
with POSIX Threads (Addison-Wesley, 1997)*, §5.6.3. The concept pretty much
died on the vine when kernel threads got faster and cheaper, but everything
old is new again.

BTW, I believe that by far the most important thing this project could do
for the average Java programmer is to provide yield iterators, à la C#.
Writing your own iterator is difficult, and writing one for collections
whose traversal is inherently recursive is extremely difficult: you have to
translate the recursive algorithm into an iterative one by maintaining an
explicit stack instead of using the thread's stack. Giving people yield
iterators makes it trivial to write iterators. This should be a tier 1 use
case: If a proposed solution can't handle it, the solution should be
repaired or discarded.

Josh

On Wed, Oct 25, 2017 at 11:42 AM, Andrig Miller <anmiller at redhat.com> wrote:

>
>
> On Wed, Oct 25, 2017 at 11:31 AM, thurstonn <thurston at nomagicsoftware.com>
> wrote:
>
>> Love it.
>>
>> BTW, for those with institutional memory, isn't that what "green threads"
>> more or less were meant to be, back in the earlier days of Java?  Before
>> my
>> time, and I never used them . . .
>>
>
> ​If memory serves me correctly, green threads were only present for
> platforms that didn't have threads in their kernel.
>
> Andy
> ​
>
>>
>>
>>
>> --
>> Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Andrig (Andy) T. Miller
> Global Platform Director, Middleware
> Red Hat, Inc.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171025/509c4226/attachment.html>

From thurston at nomagicsoftware.com  Wed Oct 25 15:40:40 2017
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 25 Oct 2017 12:40:40 -0700 (MST)
Subject: [concurrency-interest] OpenJDK Project Loom - lightweight
	threads for Java
In-Reply-To: <CAP0L=UR6-m7ex-rR1gokAA5aVWmYOHaL9su2SOQuggYBkk4d9Q@mail.gmail.com>
References: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
 <1508952678128-0.post@n7.nabble.com>
 <CAF1-sp7mVD14tP+G24TTwYS5nfZbpfy8iMVQCzp95wAw7BPsnQ@mail.gmail.com>
 <CAP0L=UR6-m7ex-rR1gokAA5aVWmYOHaL9su2SOQuggYBkk4d9Q@mail.gmail.com>
Message-ID: <1508960440858-0.post@n7.nabble.com>

Joshua Bloch wrote
> This is very similar in spirit to the "many-to-few" threading model that
> was popular in the '90s. You can read about it in Butenhof's *Programming
> with POSIX Threads (Addison-Wesley, 1997)*, §5.6.3. The concept pretty
> much
> died on the vine when kernel threads got faster and cheaper, but
> everything
> old is new again.

Uh, Erlang




--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From anmiller at redhat.com  Wed Oct 25 15:45:51 2017
From: anmiller at redhat.com (Andrig Miller)
Date: Wed, 25 Oct 2017 13:45:51 -0600
Subject: [concurrency-interest] OpenJDK Project Loom - lightweight
 threads for Java
In-Reply-To: <CAP0L=UR6-m7ex-rR1gokAA5aVWmYOHaL9su2SOQuggYBkk4d9Q@mail.gmail.com>
References: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
 <1508952678128-0.post@n7.nabble.com>
 <CAF1-sp7mVD14tP+G24TTwYS5nfZbpfy8iMVQCzp95wAw7BPsnQ@mail.gmail.com>
 <CAP0L=UR6-m7ex-rR1gokAA5aVWmYOHaL9su2SOQuggYBkk4d9Q@mail.gmail.com>
Message-ID: <CAF1-sp68xGpEpMBCLxk=w6EtWYPH3=CbZkjoN3OszHUCcL67YQ@mail.gmail.com>

On Wed, Oct 25, 2017 at 1:34 PM, Joshua Bloch <jbloch at gmail.com> wrote:

> This is very similar in spirit to the "many-to-few" threading model that
> was popular in the '90s. You can read about it in Butenhof's *Programming
> with POSIX Threads (Addison-Wesley, 1997)*, §5.6.3. The concept pretty
> much died on the vine when kernel threads got faster and cheaper, but
> everything old is new again.
>

​Solaris had a M-to-N threading model, at least at one point, but I'm not
sure if they kept that in the latest versions.

Andy
​

>
> BTW, I believe that by far the most important thing this project could do
> for the average Java programmer is to provide yield iterators, à la C#.
> Writing your own iterator is difficult, and writing one for collections
> whose traversal is inherently recursive is extremely difficult: you have to
> translate the recursive algorithm into an iterative one by maintaining an
> explicit stack instead of using the thread's stack. Giving people yield
> iterators makes it trivial to write iterators. This should be a tier 1 use
> case: If a proposed solution can't handle it, the solution should be
> repaired or discarded.
>
> Josh
>
> On Wed, Oct 25, 2017 at 11:42 AM, Andrig Miller <anmiller at redhat.com>
> wrote:
>
>>
>>
>> On Wed, Oct 25, 2017 at 11:31 AM, thurstonn <thurston at nomagicsoftware.com
>> > wrote:
>>
>>> Love it.
>>>
>>> BTW, for those with institutional memory, isn't that what "green threads"
>>> more or less were meant to be, back in the earlier days of Java?  Before
>>> my
>>> time, and I never used them . . .
>>>
>>
>> ​If memory serves me correctly, green threads were only present for
>> platforms that didn't have threads in their kernel.
>>
>> Andy
>> ​
>>
>>>
>>>
>>>
>>> --
>>> Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>> --
>> Andrig (Andy) T. Miller
>> Global Platform Director, Middleware
>> Red Hat, Inc.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


-- 
Andrig (Andy) T. Miller
Global Platform Director, Middleware
Red Hat, Inc.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171025/650b5778/attachment-0001.html>

From akarnokd at gmail.com  Wed Oct 25 15:58:18 2017
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Wed, 25 Oct 2017 21:58:18 +0200
Subject: [concurrency-interest] OpenJDK Project Loom - lightweight
 threads for Java
In-Reply-To: <1508960440858-0.post@n7.nabble.com>
References: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
 <1508952678128-0.post@n7.nabble.com>
 <CAF1-sp7mVD14tP+G24TTwYS5nfZbpfy8iMVQCzp95wAw7BPsnQ@mail.gmail.com>
 <CAP0L=UR6-m7ex-rR1gokAA5aVWmYOHaL9su2SOQuggYBkk4d9Q@mail.gmail.com>
 <1508960440858-0.post@n7.nabble.com>
Message-ID: <CAAWwtm-+n9+wxhdzFyMkmiOjLdeDuyjz8x70pfdPD2RkpMW9eQ@mail.gmail.com>

I'm a bit worried about the performance implications in exchange for the
promise of imperative looking code.

For example, having a bounded blocking queue suspend on being full/empty.
If the producer runs too fast, it gets suspended all the time with what it
seems to be quite a lot of state to be saved and restored in a thread-safe
manner. Similarly, a fast running consumer would get suspended all the time.


2017-10-25 21:40 GMT+02:00 thurstonn <thurston at nomagicsoftware.com>:

> Joshua Bloch wrote
> > This is very similar in spirit to the "many-to-few" threading model that
> > was popular in the '90s. You can read about it in Butenhof's *Programming
> > with POSIX Threads (Addison-Wesley, 1997)*, §5.6.3. The concept pretty
> > much
> > died on the vine when kernel threads got faster and cheaper, but
> > everything
> > old is new again.
>
> Uh, Erlang
>
>
>
>
> --
> Sent from: http://jsr166-concurrency.10961.n7.nabble.com/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171025/3aafb611/attachment.html>

From alexei.kaigorodov at gmail.com  Thu Oct 26 09:50:34 2017
From: alexei.kaigorodov at gmail.com (Alexei Kaigorodov)
Date: Thu, 26 Oct 2017 06:50:34 -0700 (MST)
Subject: [concurrency-interest] OpenJDK Project Loom - threads are not ideal
	mental tool
In-Reply-To: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
References: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
Message-ID: <1509025834232-0.post@n7.nabble.com>

>From Loom Proposals:

This is a sad case of a good and natural abstraction being abandoned in
favor of a less natural one [asynchronous task], which is overall worse in
many respects, merely because of the runtime performance characteristics of
the abstraction.

Well, "natural" may have at least two meanings. First, natural for the
brains of human beings, and natural from mathematical point of view, and
that meanings do not always agree. Say, Roman digits are more natural for
low-skilled persons than Hindu–Arabic numerals, while the last is more
natural mathematically. And being more natural mathematically always results
in being more performant. Can you imagine a computer working in Roman
numerals? Its cost and performance?

So, if an abstraction is less performant, it should be examined against
mathematical naturality. The main drawback of thread is, of course, the size
of stack memory, and the proposed project is going to fix it. But there are
other performance problems, which the project is not going to solve.
Consider a program for copying a file:

for (;;) {
   buff = input.read();
   if (buff == null) break;
   output.write(buff);
}

Reading and writing of each buffer is done sequentially (which is natural
for human brain), while they could be done in parallel. But expressing this
parallelism using threads would make thread-based program as complex as
asynchronous implementation.

The program representation which allows maximal parallelism is a dataflow
graph. A thread represents a sequence in that graph, usually a cycle. Of
course, each graph can be represented as a set of sequences (threads), but
the thread representation has (at least) two problems. First is that
elements of the sequence are executed sequentially, as pointed above.
Second, decomposition of the graph into threads can be done in many
variants. And the variant chosen by a programmer could be suboptimal, or
become sо during the evolution of the program.

So in order to support both brain and mathematics naturalities, I'd propose
to develop following tools:
 - a compiler from thread  representation to asynchronous (dataflow)
representation
 - graphical editor and debugger who works with graph representation
 - a decompiler to convert graph representation into thread representation,
which accepts user's advices how to split graph into threads.






--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

From ron.pressler at oracle.com  Thu Oct 26 14:19:20 2017
From: ron.pressler at oracle.com (Ron Pressler)
Date: Thu, 26 Oct 2017 21:19:20 +0300
Subject: [concurrency-interest] OpenJDK Project Loom - threads are not
 ideal mental tool
In-Reply-To: <1509025834232-0.post@n7.nabble.com>
References: <09b3fc11-9f36-0ebc-32b6-1e9aa155aeb1@oracle.com>
 <1509025834232-0.post@n7.nabble.com>
Message-ID: <etPan.59f22728.91c9501.392@oracle.com>

Hi. I wrote the Loom proposal.

Concurrency and parallelism are very different things. Concurrency is the problem of scheduling multiple _competing_ domain problems (e.g., servicing a transaction request from a user) onto some bounded set of computational resources; parallelism is the problem of optimally employing a set of computational resource to _cooperate_ on solving a single domain problem. Project Loom is first and foremost concerned with addressing the first (concurrency) and not the second (for which Java streams are likely a better solution). Concurrency is an important problem that is worth addressing, as motivated by the issues expressed in the beginning of the proposal; parallelism is simply not within the scope of this particular project. Having said that, the availability of delimited continuations may assist parallelism as well, and they can certainly help implementing dataflow graphs — those, however, will be left to third-parties, at least in the initial delivery of Project Loom.

Ron


On October 26, 2017 at 4:52:06 PM, Alexei Kaigorodov (alexei.kaigorodov at gmail.com) wrote:

From Loom Proposals:  

This is a sad case of a good and natural abstraction being abandoned in  
favor of a less natural one [asynchronous task], which is overall worse in  
many respects, merely because of the runtime performance characteristics of  
the abstraction.  

Well, "natural" may have at least two meanings. First, natural for the  
brains of human beings, and natural from mathematical point of view, and  
that meanings do not always agree. Say, Roman digits are more natural for  
low-skilled persons than Hindu–Arabic numerals, while the last is more  
natural mathematically. And being more natural mathematically always results  
in being more performant. Can you imagine a computer working in Roman  
numerals? Its cost and performance?  

So, if an abstraction is less performant, it should be examined against  
mathematical naturality. The main drawback of thread is, of course, the size  
of stack memory, and the proposed project is going to fix it. But there are  
other performance problems, which the project is not going to solve.  
Consider a program for copying a file:  

for (;;) {  
buff = input.read();  
if (buff == null) break;  
output.write(buff);  
}  

Reading and writing of each buffer is done sequentially (which is natural  
for human brain), while they could be done in parallel. But expressing this  
parallelism using threads would make thread-based program as complex as  
asynchronous implementation.  

The program representation which allows maximal parallelism is a dataflow  
graph. A thread represents a sequence in that graph, usually a cycle. Of  
course, each graph can be represented as a set of sequences (threads), but  
the thread representation has (at least) two problems. First is that  
elements of the sequence are executed sequentially, as pointed above.  
Second, decomposition of the graph into threads can be done in many  
variants. And the variant chosen by a programmer could be suboptimal, or  
become sо during the evolution of the program.  

So in order to support both brain and mathematics naturalities, I'd propose  
to develop following tools:  
- a compiler from thread representation to asynchronous (dataflow)  
representation  
- graphical editor and debugger who works with graph representation  
- a decompiler to convert graph representation into thread representation,  
which accepts user's advices how to split graph into threads.  






--  
Sent from: https://urldefense.proofpoint.com/v2/url?u=http-3A__jsr166-2Dconcurrency.10961.n7.nabble.com_&d=DwIGaQ&c=RoP1YumCXCgaWHvlZYR8PQcxBKCX5YTpkKY057SbK10&r=-kUDHxC1IVB_ypN88uP5I0zEdrgC5UAi06w1pnrnunw&m=PweXYYVC_WnNu_wy-6J-6huh6rsBxDKguogCD-V57eU&s=lzVWOXrM-0mON4FCZKKZokTu-SdHL0UUNRU5VmVnYtU&e=  
_______________________________________________  
Concurrency-interest mailing list  
Concurrency-interest at cs.oswego.edu  
https://urldefense.proofpoint.com/v2/url?u=http-3A__cs.oswego.edu_mailman_listinfo_concurrency-2Dinterest&d=DwIGaQ&c=RoP1YumCXCgaWHvlZYR8PQcxBKCX5YTpkKY057SbK10&r=-kUDHxC1IVB_ypN88uP5I0zEdrgC5UAi06w1pnrnunw&m=PweXYYVC_WnNu_wy-6J-6huh6rsBxDKguogCD-V57eU&s=O-EwZSIf6TwIoY3qXtXFmLMe8V2_aZjkzwpGVmzLEqQ&e=  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20171026/38bf325b/attachment.html>

