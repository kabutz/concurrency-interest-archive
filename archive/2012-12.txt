From kirk at kodewerk.com  Sat Dec  1 00:15:21 2012
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Sat, 1 Dec 2012 06:15:21 +0100
Subject: [concurrency-interest] Java hardware counter profiler tools
In-Reply-To: <CAHjP37H8pMtR18xdneYoEPs2-9stq0WgXa+M=XRSOACy6HSXog@mail.gmail.com>
References: <CAHjP37HQ7O8G31Cgj3D-rZ_aeDwd52pj1jOH-DHZM2ns40=yRA@mail.gmail.com>
	<CAHjP37EeBtgAhx3Ye8DSJCx-cKPo6_H-j8HcuOy94ALJaSkucA@mail.gmail.com>
	<CAHjP37H8pMtR18xdneYoEPs2-9stq0WgXa+M=XRSOACy6HSXog@mail.gmail.com>
Message-ID: <37850E6D-B3D8-45B1-9D4F-F08E332EEFC7@kodewerk.com>

I have some tooling that interacts with MSR's on LInux.. we have a correlation between MSR activity and Java code. Be happy to talk to you off list.

-- Kirk

On 2012-12-01, at 1:26 AM, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Hi guys,
> 
> This isn't a strictly concurrency question, but I figure this group is as good an audience as any for this question.
> 
> What are folks using for doing hardware counter based profiling of java code? Specifically, I'm interested in being able to correlate generated assembly with java source and in turn have hardware counters associated with it (e.g. LLC miss, branch mispredict, etc).  Linux intel x64 is the target.
> 
> I understand this is a tall order for a profiler (with JIT in the middle), but hoping something halfway decent exists.
> 
> I've briefly toyed with Intel's amplifier 13.1 but haven't quite got it working to this level.
> 
> Does anyone have any recommendations and possibly links for more detailed info?
> 
> Thanks
> 
> Sent from my phone
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121201/06972786/attachment-0001.html>

From alarmnummer at gmail.com  Sat Dec  1 03:49:08 2012
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sat, 1 Dec 2012 10:49:08 +0200
Subject: [concurrency-interest] Java hardware counter profiler tools
In-Reply-To: <37850E6D-B3D8-45B1-9D4F-F08E332EEFC7@kodewerk.com>
References: <CAHjP37HQ7O8G31Cgj3D-rZ_aeDwd52pj1jOH-DHZM2ns40=yRA@mail.gmail.com>
	<CAHjP37EeBtgAhx3Ye8DSJCx-cKPo6_H-j8HcuOy94ALJaSkucA@mail.gmail.com>
	<CAHjP37H8pMtR18xdneYoEPs2-9stq0WgXa+M=XRSOACy6HSXog@mail.gmail.com>
	<37850E6D-B3D8-45B1-9D4F-F08E332EEFC7@kodewerk.com>
Message-ID: <CAGuAWdANf-5B+-ujCEtkqBUcERsBxLuqT1vob3Y2HYx9=5jLFg@mail.gmail.com>

I have had very good experiences with Intel VTune 9. Within a single
environment you can
see the java code, the assembly and you can see which java/machine instructions
caused a slowdown.

Unfortunately 10/XE doesn't support Java anymore. But AFAIK vtune 9
can be used with 1.6.. perhaps
even 1.7. It can be used on Linux and Windows.

On Sat, Dec 1, 2012 at 7:15 AM, Kirk Pepperdine <kirk at kodewerk.com> wrote:
> I have some tooling that interacts with MSR's on LInux.. we have a
> correlation between MSR activity and Java code. Be happy to talk to you off
> list.
>
> -- Kirk
>
> On 2012-12-01, at 1:26 AM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
> Hi guys,
>
> This isn't a strictly concurrency question, but I figure this group is as
> good an audience as any for this question.
>
> What are folks using for doing hardware counter based profiling of java
> code? Specifically, I'm interested in being able to correlate generated
> assembly with java source and in turn have hardware counters associated with
> it (e.g. LLC miss, branch mispredict, etc).  Linux intel x64 is the target.
>
> I understand this is a tall order for a profiler (with JIT in the middle),
> but hoping something halfway decent exists.
>
> I've briefly toyed with Intel's amplifier 13.1 but haven't quite got it
> working to this level.
>
> Does anyone have any recommendations and possibly links for more detailed
> info?
>
> Thanks
>
> Sent from my phone
>
> _______________________________________________
>
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From mr.chrisvest at gmail.com  Sat Dec  1 05:18:06 2012
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Sat, 1 Dec 2012 11:18:06 +0100
Subject: [concurrency-interest] Java hardware counter profiler tools
In-Reply-To: <CAGuAWdANf-5B+-ujCEtkqBUcERsBxLuqT1vob3Y2HYx9=5jLFg@mail.gmail.com>
References: <CAHjP37HQ7O8G31Cgj3D-rZ_aeDwd52pj1jOH-DHZM2ns40=yRA@mail.gmail.com>
	<CAHjP37EeBtgAhx3Ye8DSJCx-cKPo6_H-j8HcuOy94ALJaSkucA@mail.gmail.com>
	<CAHjP37H8pMtR18xdneYoEPs2-9stq0WgXa+M=XRSOACy6HSXog@mail.gmail.com>
	<37850E6D-B3D8-45B1-9D4F-F08E332EEFC7@kodewerk.com>
	<CAGuAWdANf-5B+-ujCEtkqBUcERsBxLuqT1vob3Y2HYx9=5jLFg@mail.gmail.com>
Message-ID: <CAHXi_0fhXtQ3UY5K7+Se5sRmZJG46Xv1Eq1QY0syqPkm_xcxqQ@mail.gmail.com>

Probably a crude last resort compared to the other tools already mentioned,
but Martin Thompson talked about a Linux `perf` tool [1] on his blog [2].
[1] https://perf.wiki.kernel.org/index.php/Tutorial
[2]
http://mechanical-sympathy.blogspot.dk/2012/08/memory-access-patterns-are-important.html

Personally I'd be interested in hearing about a tool that works on Intel
Macs.


On 1 December 2012 09:49, Peter Veentjer <alarmnummer at gmail.com> wrote:

> I have had very good experiences with Intel VTune 9. Within a single
> environment you can
> see the java code, the assembly and you can see which java/machine
> instructions
> caused a slowdown.
>
> Unfortunately 10/XE doesn't support Java anymore. But AFAIK vtune 9
> can be used with 1.6.. perhaps
> even 1.7. It can be used on Linux and Windows.
>
> On Sat, Dec 1, 2012 at 7:15 AM, Kirk Pepperdine <kirk at kodewerk.com> wrote:
> > I have some tooling that interacts with MSR's on LInux.. we have a
> > correlation between MSR activity and Java code. Be happy to talk to you
> off
> > list.
> >
> > -- Kirk
> >
> > On 2012-12-01, at 1:26 AM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> >
> > Hi guys,
> >
> > This isn't a strictly concurrency question, but I figure this group is as
> > good an audience as any for this question.
> >
> > What are folks using for doing hardware counter based profiling of java
> > code? Specifically, I'm interested in being able to correlate generated
> > assembly with java source and in turn have hardware counters associated
> with
> > it (e.g. LLC miss, branch mispredict, etc).  Linux intel x64 is the
> target.
> >
> > I understand this is a tall order for a profiler (with JIT in the
> middle),
> > but hoping something halfway decent exists.
> >
> > I've briefly toyed with Intel's amplifier 13.1 but haven't quite got it
> > working to this level.
> >
> > Does anyone have any recommendations and possibly links for more detailed
> > info?
> >
> > Thanks
> >
> > Sent from my phone
> >
> > _______________________________________________
> >
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121201/dd388969/attachment.html>

From mikeb01 at gmail.com  Sat Dec  1 16:34:28 2012
From: mikeb01 at gmail.com (Michael Barker)
Date: Sat, 1 Dec 2012 21:34:28 +0000
Subject: [concurrency-interest] Java hardware counter profiler tools
In-Reply-To: <CAHXi_0fhXtQ3UY5K7+Se5sRmZJG46Xv1Eq1QY0syqPkm_xcxqQ@mail.gmail.com>
References: <CAHjP37HQ7O8G31Cgj3D-rZ_aeDwd52pj1jOH-DHZM2ns40=yRA@mail.gmail.com>
	<CAHjP37EeBtgAhx3Ye8DSJCx-cKPo6_H-j8HcuOy94ALJaSkucA@mail.gmail.com>
	<CAHjP37H8pMtR18xdneYoEPs2-9stq0WgXa+M=XRSOACy6HSXog@mail.gmail.com>
	<37850E6D-B3D8-45B1-9D4F-F08E332EEFC7@kodewerk.com>
	<CAGuAWdANf-5B+-ujCEtkqBUcERsBxLuqT1vob3Y2HYx9=5jLFg@mail.gmail.com>
	<CAHXi_0fhXtQ3UY5K7+Se5sRmZJG46Xv1Eq1QY0syqPkm_xcxqQ@mail.gmail.com>
Message-ID: <CALwNKeSmjjj+oivXts7RcT7Ad_y86Z756-aMJw5MzujC6jQkjA@mail.gmail.com>

> Probably a crude last resort compared to the other tools already mentioned,
> but Martin Thompson talked about a Linux `perf` tool [1] on his blog [2].
> [1] https://perf.wiki.kernel.org/index.php/Tutorial
> [2]
> http://mechanical-sympathy.blogspot.dk/2012/08/memory-access-patterns-are-important.html

The 'perf' tool is a command line based on the kernel perf event API
and is part of the 'linux-tools' package on most distributions.  There
is are couple of other tools e.g. likwid[1] that can do similar
things, however most of them read MSRs directly (well not quite
directly, typically through a kernel driver or the /dev/cpu/xx/msr
filesystem).  One of the drawbacks of this approach is that the
counters are specific to a CPU not to a process.  This means that you
need CPU affinity in order to get accurate counters for a specific
process or thread.  As I understand it, the Linux kernel perf
counters[3] support per task stats and get around this restriction by
capturing the counters each time the task context switches.

> Personally I'd be interested in hearing about a tool that works on Intel Macs

I hacked up a version of DirectHW from the core boot project that I
used to collect MSRs on my Mac[2].  However, as Mac OS does not really
support CPU affinity[4], it's use is limited.

Intel's VTune 11 adds limited support for Java.

Mike.

[1] http://code.google.com/p/likwid/
[2] http://bit.ly/UyOQjF
[3] http://lxr.free-electrons.com/source/tools/perf/design.txt
[4] https://developer.apple.com/library/mac/#releasenotes/Performance/RN-AffinityAPI/_index.html

From vitalyd at gmail.com  Mon Dec  3 07:28:36 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 3 Dec 2012 07:28:36 -0500
Subject: [concurrency-interest] Java hardware counter profiler tools
In-Reply-To: <CAHjP37H8pMtR18xdneYoEPs2-9stq0WgXa+M=XRSOACy6HSXog@mail.gmail.com>
References: <CAHjP37HQ7O8G31Cgj3D-rZ_aeDwd52pj1jOH-DHZM2ns40=yRA@mail.gmail.com>
	<CAHjP37EeBtgAhx3Ye8DSJCx-cKPo6_H-j8HcuOy94ALJaSkucA@mail.gmail.com>
	<CAHjP37H8pMtR18xdneYoEPs2-9stq0WgXa+M=XRSOACy6HSXog@mail.gmail.com>
Message-ID: <CAHjP37Fbr9xKJ=tfS+RTPe6u0w22C68hmMdOPSEdMkxyHpvwhQ@mail.gmail.com>

Thanks for all suggestions - I'll look into them.

Cheers,

Vitaly

Sent from my phone
On Nov 30, 2012 7:26 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> Hi guys,
>
> This isn't a strictly concurrency question, but I figure this group is as
> good an audience as any for this question.
>
> What are folks using for doing hardware counter based profiling of java
> code? Specifically, I'm interested in being able to correlate generated
> assembly with java source and in turn have hardware counters associated
> with it (e.g. LLC miss, branch mispredict, etc).  Linux intel x64 is the
> target.
>
> I understand this is a tall order for a profiler (with JIT in the middle),
> but hoping something halfway decent exists.
>
> I've briefly toyed with Intel's amplifier 13.1 but haven't quite got it
> working to this level.
>
> Does anyone have any recommendations and possibly links for more detailed
> info?
>
> Thanks
>
> Sent from my phone
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121203/1f4bac8c/attachment.html>

From dl at cs.oswego.edu  Tue Dec  4 15:12:10 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 04 Dec 2012 15:12:10 -0500
Subject: [concurrency-interest] JDK8 transition plans
Message-ID: <50BE591A.6050908@cs.oswego.edu>


Starting soon, we will be transitioning the main java.util.concurrent
repository to be JDK8-only. You will only be able to build and run
with preview builds of openjdk8 (and eventually preview builds from
other JVM vendors as well, and even more eventually, JDK8 releases).
We need to do this to use lambdas, as well as upcoming JVM intrinsics.

Because some people depend on our j.u.c sources
(in addition to jsr166e and jsr166y packagings)
being  available for use under released versions of Java,
we'll first create a src/jdk7/java/util/... repository that
contains current, JDK7- (actually JDK6-) compilable versions of
only those things in JDK7 releases (although possibly with
extensions/features that weren't in the initial versions).
The src/main/java/util/... branch will as always contain
things that we expect to see in j.u.c in Java/JDK releases. (Our
expectations have not been wrong yet.)

These should take effect within the next few days.  Some of the
JDK8-only-ness might be hard to cope with, possibly including times
when the only people who can compile/run are those of us building
openjdk from source branches, as we wait for public openjdk preview
releases to catch up. Sorry in advance.

The most observable aspect of "JDK8-only" will be lambdas. We'll use
as much of the JSR335 java.util lambdadification as we can, but still
with some twists/extensions, like ConcurrentHashMap support for
parallel bulk operations over "live" data. Less observably, internal
implementations will be able to use the upcoming @Contended
annotations, and intrinsics-only (no java.* APIs) fence methods and
direct access to getAndAdd-based and getAndSet-based operations
(emulated with VM-assist on processors not nateively supporting them.)

One small deviation from our normal practices is that a few things
will be put out in straight-to-j.u.c form (no preview jsr166*
packagings), since they will be too entangled with JDK8isms to package
separately.  Among these will be, finally, CompletableFuture, a class
discussed off and on here over the years under the names
SettableFuture, FutureValue, and Promise, that needs lambdas as well
as other JDK8-only-j.u.c support for the sake of defining a nice
API. Stay tuned for details on this, as well as a few minor changes
in jsr166e classes as they transition into JDK8 j.u.c.

-Doug




From aleksey.shipilev at oracle.com  Tue Dec  4 15:35:34 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 05 Dec 2012 00:35:34 +0400
Subject: [concurrency-interest] JDK8 transition plans
In-Reply-To: <50BE591A.6050908@cs.oswego.edu>
References: <50BE591A.6050908@cs.oswego.edu>
Message-ID: <50BE5E96.8000803@oracle.com>

On 12/05/2012 12:12 AM, Doug Lea wrote:
> Starting soon, we will be transitioning the main java.util.concurrent
> repository to be JDK8-only. You will only be able to build and run
> with preview builds of openjdk8 (and eventually preview builds from
> other JVM vendors as well, and even more eventually, JDK8 releases).
> We need to do this to use lambdas, as well as upcoming JVM intrinsics.

Good stuff.

Does that effectively mean jsr166e will merge into "main" at this point
and cease to exist?

-Aleksey.

From dl at cs.oswego.edu  Tue Dec  4 16:03:08 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 04 Dec 2012 16:03:08 -0500
Subject: [concurrency-interest] JDK8 transition plans
In-Reply-To: <50BE5E96.8000803@oracle.com>
References: <50BE591A.6050908@cs.oswego.edu> <50BE5E96.8000803@oracle.com>
Message-ID: <50BE650C.9000705@cs.oswego.edu>

On 12/04/12 15:35, Aleksey Shipilev wrote:

> Does that effectively mean jsr166e will merge into "main" at this point
> and cease to exist?
>

No, jsr166e will remain the JDK7-compilable set of those
classes targeted to be added JDK8, but without JDK packaging.

Similarly, jsr166y remains as the JDK6-compilable set of
additions released in JDK7.

(And we even still keep jsr166x around, with JDK5-compilable
JDK6 additions.)

In all cases, we keep the differences between j.u.c
and non-JDK forms as small as we can. Some are necessary
due to packaging and naming, but usually it is easy to
move from preview release to full release.

-Doug






From forax at univ-mlv.fr  Tue Dec  4 16:04:05 2012
From: forax at univ-mlv.fr (Remi Forax)
Date: Tue, 04 Dec 2012 22:04:05 +0100
Subject: [concurrency-interest] JDK8 transition plans
In-Reply-To: <50BE591A.6050908@cs.oswego.edu>
References: <50BE591A.6050908@cs.oswego.edu>
Message-ID: <50BE6545.9090901@univ-mlv.fr>

On 12/04/2012 09:12 PM, Doug Lea wrote:
>
> Starting soon, we will be transitioning the main java.util.concurrent
> repository to be JDK8-only. You will only be able to build and run
> with preview builds of openjdk8 (and eventually preview builds from
> other JVM vendors as well, and even more eventually, JDK8 releases).
> We need to do this to use lambdas, as well as upcoming JVM intrinsics.
>
> Because some people depend on our j.u.c sources
> (in addition to jsr166e and jsr166y packagings)
> being  available for use under released versions of Java,
> we'll first create a src/jdk7/java/util/... repository that
> contains current, JDK7- (actually JDK6-) compilable versions of
> only those things in JDK7 releases (although possibly with
> extensions/features that weren't in the initial versions).
> The src/main/java/util/... branch will as always contain
> things that we expect to see in j.u.c in Java/JDK releases. (Our
> expectations have not been wrong yet.)
>
> These should take effect within the next few days.  Some of the
> JDK8-only-ness might be hard to cope with, possibly including times
> when the only people who can compile/run are those of us building
> openjdk from source branches, as we wait for public openjdk preview
> releases to catch up. Sorry in advance.
>
> The most observable aspect of "JDK8-only" will be lambdas. We'll use
> as much of the JSR335 java.util lambdadification as we can, but still
> with some twists/extensions, like ConcurrentHashMap support for
> parallel bulk operations over "live" data. Less observably, internal
> implementations will be able to use the upcoming @Contended
> annotations, and intrinsics-only (no java.* APIs) fence methods and
> direct access to getAndAdd-based and getAndSet-based operations
> (emulated with VM-assist on processors not nateively supporting them.)
>
> One small deviation from our normal practices is that a few things
> will be put out in straight-to-j.u.c form (no preview jsr166*
> packagings), since they will be too entangled with JDK8isms to package
> separately.  Among these will be, finally, CompletableFuture, a class
> discussed off and on here over the years under the names
> SettableFuture, FutureValue, and Promise, that needs lambdas as well
> as other JDK8-only-j.u.c support for the sake of defining a nice
> API. Stay tuned for details on this, as well as a few minor changes
> in jsr166e classes as they transition into JDK8 j.u.c.

I think it's possible to backport source codes that uses lambdas to 
generate jdk6 compatible code using a javac annotation processor.
It's also easy to rewrite the bytecode to downgrade the source to be 1.6 
compatible.
For the later, if nobody had written that code next year, I will write 
it because I will need it :)

>
> -Doug

R?mi

R?mi


From kedar.mhaswade at gmail.com  Thu Dec  6 08:22:37 2012
From: kedar.mhaswade at gmail.com (kedar mhaswade)
Date: Thu, 6 Dec 2012 05:22:37 -0800
Subject: [concurrency-interest] Uniqueness and Reference Immutability for
	Safe Parallelism
Message-ID: <CABzSAw_ur1hPp=-_is+En9E_JfZ2EpkqAtfP_iNe5C5EVHo9nw@mail.gmail.com>

Ran into this recent research paper from Microsoft Research [1] that I
thought was (somewhat) related to the discussion thread on @Contended.

1: http://research.microsoft.com/pubs/170528/msr-tr-2012-79.pdf
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121206/b071eea3/attachment.html>

From pyiapa at gmail.com  Wed Dec 12 06:53:03 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Wed, 12 Dec 2012 11:53:03 +0000
Subject: [concurrency-interest] Memory Alignment in Java
Message-ID: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>

Hello,

I would like to ask if anybody knows what is the memory alignment
boundaries in Java.
Also if the VM is doing any padding automatically for you based on the
running architecture.
I am using the HotSpot JVM on an UltraSparc 2 machine.


Thank you in advance,

Paris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121212/3973df3b/attachment.html>

From aleksey.shipilev at oracle.com  Wed Dec 12 07:08:11 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 12 Dec 2012 16:08:11 +0400
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
Message-ID: <50C873AB.70401@oracle.com>

On 12/12/2012 03:53 PM, Paris Yiapanis wrote:
> I would like to ask if anybody knows what is the memory alignment 
> boundaries in Java. Also if the VM is doing any padding automatically
> for you based on the running architecture. I am using the HotSpot JVM
> on an UltraSparc 2 machine.

No padding except to meet the alignments required for target
architecture (either for performance or correctness reasons). Objects
are aligned at either 4 or 8 bytes, depending on platform bitness,
longs/doubles are aligned for 8 bytes on specific platforms, etc.

Here are the few hints for HotSpot land:

$ java -XX:+PrintFlagsFinal 2>&1 | grep ObjectAlignment
    intx ObjectAlignmentInBytes     = 8  {lp64_product}

...also, you might ask Unsafe for exact field offsets, or use this:
  https://github.com/shipilev/java-field-layout

-Aleksey.



From pyiapa at gmail.com  Wed Dec 12 07:26:11 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Wed, 12 Dec 2012 12:26:11 +0000
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <50C873AB.70401@oracle.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
Message-ID: <CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>

Thank you very much. Very useful information.

So, if I understand correctly, the only thing I you can do as a programmer
is to try and make your data structures divisible by 4 bytes (assuming 4
byte alignment) and the JVM will sort out the rest based in the specific
hardware. Is this correct?

- Paris

On Wed, Dec 12, 2012 at 12:08 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 12/12/2012 03:53 PM, Paris Yiapanis wrote:
> > I would like to ask if anybody knows what is the memory alignment
> > boundaries in Java. Also if the VM is doing any padding automatically
> > for you based on the running architecture. I am using the HotSpot JVM
> > on an UltraSparc 2 machine.
>
> No padding except to meet the alignments required for target
> architecture (either for performance or correctness reasons). Objects
> are aligned at either 4 or 8 bytes, depending on platform bitness,
> longs/doubles are aligned for 8 bytes on specific platforms, etc.
>
> Here are the few hints for HotSpot land:
>
> $ java -XX:+PrintFlagsFinal 2>&1 | grep ObjectAlignment
>     intx ObjectAlignmentInBytes     = 8  {lp64_product}
>
> ...also, you might ask Unsafe for exact field offsets, or use this:
>   https://github.com/shipilev/java-field-layout
>
> -Aleksey.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121212/6495f26a/attachment.html>

From aleksey.shipilev at oracle.com  Wed Dec 12 07:38:06 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 12 Dec 2012 16:38:06 +0400
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
Message-ID: <50C87AAE.5080905@oracle.com>

On 12/12/2012 04:26 PM, Paris Yiapanis wrote:
> So, if I understand correctly, the only thing I you can do as a
> programmer is to try and make your data structures divisible by 4 bytes
> (assuming 4 byte alignment) and the JVM will sort out the rest based in
> the specific hardware. Is this correct?

Not sure what you mean by "divisible".

You should not think about alignment, and just use the usual Java types,
JVM will align those appropriately as required for target hardware.

-Aleksey.


From pyiapa at gmail.com  Wed Dec 12 07:56:07 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Wed, 12 Dec 2012 12:56:07 +0000
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <50C87AAE.5080905@oracle.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
Message-ID: <CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>

Sorry, I meant to say when creating an object to make the fields to be a
number multiple of 4 bytes.
For instance if I create an object with instance fields a byte and an
integer that's 40 bits. So its 5 bytes. Will JVM pad it to make it 8
(assuming 4 byte boundary hardware)?

Basically I wanted to know if the JVM takes care of the alignment or if I
could help by placing the data in a certain way (Not sure but I think in
C++ you could optimize and the make data aligned with certain assembly
instructions).
>From what you say the JVM does take care of it so that's perfect.

Thank you very much again.

- Paris

On Wed, Dec 12, 2012 at 12:38 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 12/12/2012 04:26 PM, Paris Yiapanis wrote:
> > So, if I understand correctly, the only thing I you can do as a
> > programmer is to try and make your data structures divisible by 4 bytes
> > (assuming 4 byte alignment) and the JVM will sort out the rest based in
> > the specific hardware. Is this correct?
>
> Not sure what you mean by "divisible".
>
> You should not think about alignment, and just use the usual Java types,
> JVM will align those appropriately as required for target hardware.
>
> -Aleksey.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121212/af7c796e/attachment.html>

From aleksandar.prokopec at gmail.com  Wed Dec 12 08:11:23 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Wed, 12 Dec 2012 14:11:23 +0100
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
Message-ID: <50C8827B.9040506@gmail.com>

Here's a nice blog post on the subject that might help:

http://www.codeinstructions.com/2008/12/java-objects-memory-structure.html

--
Aleksandar Prokopec
Doctoral Assistant
LAMP, IC, EPFL
http://people.epfl.ch/aleksandar.prokopec

On 12/12/12 1:56 PM, Paris Yiapanis wrote:
> Sorry, I meant to say when creating an object to make the fields to be 
> a number multiple of 4 bytes.
> For instance if I create an object with instance fields a byte and an 
> integer that's 40 bits. So its 5 bytes. Will JVM pad it to make it 8 
> (assuming 4 byte boundary hardware)?
>
> Basically I wanted to know if the JVM takes care of the alignment or 
> if I could help by placing the data in a certain way (Not sure but I 
> think in C++ you could optimize and the make data aligned with certain 
> assembly instructions).
> >From what you say the JVM does take care of it so that's perfect.
>
> Thank you very much again.
>
> - Paris
>
> On Wed, Dec 12, 2012 at 12:38 PM, Aleksey Shipilev 
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
>
>     On 12/12/2012 04:26 PM, Paris Yiapanis wrote:
>     > So, if I understand correctly, the only thing I you can do as a
>     > programmer is to try and make your data structures divisible by
>     4 bytes
>     > (assuming 4 byte alignment) and the JVM will sort out the rest
>     based in
>     > the specific hardware. Is this correct?
>
>     Not sure what you mean by "divisible".
>
>     You should not think about alignment, and just use the usual Java
>     types,
>     JVM will align those appropriately as required for target hardware.
>
>     -Aleksey.
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121212/03754c3a/attachment.html>

From vitalyd at gmail.com  Wed Dec 12 08:13:03 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 12 Dec 2012 08:13:03 -0500
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
Message-ID: <CAHjP37FVpZCFuTmugz7TKBGXmze-Jp+LgE9aAzV8OMdZmB_h6A@mail.gmail.com>

JVM takes care of adding padding to meet alignment requirements.  It also
tries to lay out the class in an optimal manner (reduce holes and thus
wasted space).  There's nothing you can do to in java code to alter that
(as compared to c/c++ where you can).

Vitaly

Sent from my phone
On Dec 12, 2012 7:59 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

> Sorry, I meant to say when creating an object to make the fields to be a
> number multiple of 4 bytes.
> For instance if I create an object with instance fields a byte and an
> integer that's 40 bits. So its 5 bytes. Will JVM pad it to make it 8
> (assuming 4 byte boundary hardware)?
>
> Basically I wanted to know if the JVM takes care of the alignment or if I
> could help by placing the data in a certain way (Not sure but I think in
> C++ you could optimize and the make data aligned with certain assembly
> instructions).
> From what you say the JVM does take care of it so that's perfect.
>
> Thank you very much again.
>
> - Paris
>
> On Wed, Dec 12, 2012 at 12:38 PM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> On 12/12/2012 04:26 PM, Paris Yiapanis wrote:
>> > So, if I understand correctly, the only thing I you can do as a
>> > programmer is to try and make your data structures divisible by 4 bytes
>> > (assuming 4 byte alignment) and the JVM will sort out the rest based in
>> > the specific hardware. Is this correct?
>>
>> Not sure what you mean by "divisible".
>>
>> You should not think about alignment, and just use the usual Java types,
>> JVM will align those appropriately as required for target hardware.
>>
>> -Aleksey.
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121212/df061877/attachment-0001.html>

From aleksey.shipilev at oracle.com  Wed Dec 12 08:12:51 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 12 Dec 2012 17:12:51 +0400
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
Message-ID: <50C882D3.1020508@oracle.com>

On 12/12/2012 04:56 PM, Paris Yiapanis wrote:
> Sorry, I meant to say when creating an object to make the fields to be a
> number multiple of 4 bytes.
> For instance if I create an object with instance fields a byte and an
> integer that's 40 bits. So its 5 bytes. Will JVM pad it to make it 8
> (assuming 4 byte boundary hardware)?

The answer to your question is basically "yes", with one complication:
object header. On 32-bit platform, it will probably do something like:

a = object address [aligned at 4 bytes]
  a+0: (header)
 a+12: integer field [fits perfectly for 4-byte alignment]
 a+13: byte field
 a+14: (end of fields, gap)

 a+16: (here the next object lies, due to 4-byte alignment)

> From what you say the JVM does take care of it so that's perfect.

Yes, it does that for correctness (to make this non-offtopic on this
list, one of the reasons is that the values crossing the cache lines are
not guaranteed to be atomically read/written at least on x86) and
performance reasons automatically.

-Aleksey.


From dawid.weiss at gmail.com  Wed Dec 12 08:27:39 2012
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Wed, 12 Dec 2012 14:27:39 +0100
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAHjP37FVpZCFuTmugz7TKBGXmze-Jp+LgE9aAzV8OMdZmB_h6A@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<CAHjP37FVpZCFuTmugz7TKBGXmze-Jp+LgE9aAzV8OMdZmB_h6A@mail.gmail.com>
Message-ID: <CAM21Rt-7LT-cndeB6DLb1tryM2u7XLa5RMeiCJZCg49JMt4psA@mail.gmail.com>

If it helps anyhow -- here's a slideshow I gave a while back about a
sizeof() estimator implementation for Apache Lucene. It contains some
examples that should highlight the points already made by Aleksey --

http://www.slideshare.net/DawidWeiss/sizeofobject-how-much-memory-objects-take-on-jvms-and-when-this-may-matter

Dawid

From pyiapa at gmail.com  Wed Dec 12 09:01:38 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Wed, 12 Dec 2012 14:01:38 +0000
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAM21Rt-7LT-cndeB6DLb1tryM2u7XLa5RMeiCJZCg49JMt4psA@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<CAHjP37FVpZCFuTmugz7TKBGXmze-Jp+LgE9aAzV8OMdZmB_h6A@mail.gmail.com>
	<CAM21Rt-7LT-cndeB6DLb1tryM2u7XLa5RMeiCJZCg49JMt4psA@mail.gmail.com>
Message-ID: <CAPUTfNkLiv_WNrfSHzRDtwQdf1Ci4q+RDfA-y4sO-jnhAuvxVQ@mail.gmail.com>

Thank you very much guys, for all the information and the prompt
responsiveness.
It is all very nice and useful information.

- Paris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121212/adbc137c/attachment.html>

From alex.snaps at gmail.com  Wed Dec 12 09:04:38 2012
From: alex.snaps at gmail.com (Alex Snaps)
Date: Wed, 12 Dec 2012 09:04:38 -0500
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAM21Rt-7LT-cndeB6DLb1tryM2u7XLa5RMeiCJZCg49JMt4psA@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<CAHjP37FVpZCFuTmugz7TKBGXmze-Jp+LgE9aAzV8OMdZmB_h6A@mail.gmail.com>
	<CAM21Rt-7LT-cndeB6DLb1tryM2u7XLa5RMeiCJZCg49JMt4psA@mail.gmail.com>
Message-ID: <CAKux6pYhHrd6uJsGafpt6Xbt-S4N3fJi5mwvKO1aNM5bDe1xdg@mail.gmail.com>

Hey Dawid,
We've been doing some efforts on that front too as part of ehcache -
thought you might be interested in it. Not available as a separate module
though, but all is well contained here:
http://svn.terracotta.org/svn/ehcache/trunk/ehcache/ehcache-core/src/main/java/net/sf/ehcache/pool/sizeof/
Interestingly, we found that CMS imposes a minimum size an object will
use... Anyways, slightly off topic here maybe, sharing nonetheless ;)

On Wed, Dec 12, 2012 at 8:27 AM, Dawid Weiss <dawid.weiss at gmail.com> wrote:

> If it helps anyhow -- here's a slideshow I gave a while back about a
> sizeof() estimator implementation for Apache Lucene. It contains some
> examples that should highlight the points already made by Aleksey --
>
>
> http://www.slideshare.net/DawidWeiss/sizeofobject-how-much-memory-objects-take-on-jvms-and-when-this-may-matter
>
> Dawid
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Alex Snaps
Senior Software Engineer - Terracotta
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121212/81670741/attachment.html>

From dawid.weiss at gmail.com  Wed Dec 12 12:04:54 2012
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Wed, 12 Dec 2012 18:04:54 +0100
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAKux6pYhHrd6uJsGafpt6Xbt-S4N3fJi5mwvKO1aNM5bDe1xdg@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<CAHjP37FVpZCFuTmugz7TKBGXmze-Jp+LgE9aAzV8OMdZmB_h6A@mail.gmail.com>
	<CAM21Rt-7LT-cndeB6DLb1tryM2u7XLa5RMeiCJZCg49JMt4psA@mail.gmail.com>
	<CAKux6pYhHrd6uJsGafpt6Xbt-S4N3fJi5mwvKO1aNM5bDe1xdg@mail.gmail.com>
Message-ID: <CAM21Rt-RYXa-5LfdwxVBtFGo29k0sxLp3bMx05mNrOocE8f_Fw@mail.gmail.com>

Hi Alex,

Thanks a lot for the pointer -- I'll add it to my todo list to check
it out. Funny that there are so many existing implementations of this
stuff -- it's more than a hint that an API to investigate the memory
layout of Java objects should be part of the standard (or even
non-standard) library, don't you agree? :)

Dawid

On Wed, Dec 12, 2012 at 3:04 PM, Alex Snaps <alex.snaps at gmail.com> wrote:
> Hey Dawid,
> We've been doing some efforts on that front too as part of ehcache - thought
> you might be interested in it. Not available as a separate module though,
> but all is well contained here:
> http://svn.terracotta.org/svn/ehcache/trunk/ehcache/ehcache-core/src/main/java/net/sf/ehcache/pool/sizeof/
> Interestingly, we found that CMS imposes a minimum size an object will
> use... Anyways, slightly off topic here maybe, sharing nonetheless ;)
>
> On Wed, Dec 12, 2012 at 8:27 AM, Dawid Weiss <dawid.weiss at gmail.com> wrote:
>>
>> If it helps anyhow -- here's a slideshow I gave a while back about a
>> sizeof() estimator implementation for Apache Lucene. It contains some
>> examples that should highlight the points already made by Aleksey --
>>
>>
>> http://www.slideshare.net/DawidWeiss/sizeofobject-how-much-memory-objects-take-on-jvms-and-when-this-may-matter
>>
>> Dawid
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> --
> Alex Snaps
> Senior Software Engineer - Terracotta

From pyiapa at gmail.com  Thu Dec 13 09:01:29 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Thu, 13 Dec 2012 14:01:29 +0000
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <50C882D3.1020508@oracle.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<50C882D3.1020508@oracle.com>
Message-ID: <CAPUTfNm=FLo0uO-UwdvbpwkfirzLLW_Te96X8aL094r6Jma5Sw@mail.gmail.com>

Aleksey,

You mention that values exceeding cache line are not guaranteed to be
atomic (in x86 at least).
I have two questions here:

Just to see if I get this correct:
Assuming a cache line of 8 Bytes.
Assume also my hardware supports 32-bit and 64-bit CAS instructions.
If I issue an AtomicInteger CAS (32-bits) and an AtomicLong CAS (64-bit),
will the JVM pad the first 32-bit CAS with additional 32-bits to fit the
cache line, so the next 64-bit CAS fits in the next cache line?
-----------------------

Also a second question; while reading the x86 manual, I saw (among other
operations) that following operations are guaranteed to be atomic:

- Reading or writing a word aligned on a 16-bit boundary.
?- Reading or writing a doubleword aligned on a 32-bit boundary.

I presume this applies only if programmed in C/C++ since in Java, alignment
is out of programmers control. Is this correct?

Thanks,

Paris



On Wed, Dec 12, 2012 at 1:12 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 12/12/2012 04:56 PM, Paris Yiapanis wrote:
> > Sorry, I meant to say when creating an object to make the fields to be a
> > number multiple of 4 bytes.
> > For instance if I create an object with instance fields a byte and an
> > integer that's 40 bits. So its 5 bytes. Will JVM pad it to make it 8
> > (assuming 4 byte boundary hardware)?
>
> The answer to your question is basically "yes", with one complication:
> object header. On 32-bit platform, it will probably do something like:
>
> a = object address [aligned at 4 bytes]
>   a+0: (header)
>  a+12: integer field [fits perfectly for 4-byte alignment]
>  a+13: byte field
>  a+14: (end of fields, gap)
>
>  a+16: (here the next object lies, due to 4-byte alignment)
>
> > From what you say the JVM does take care of it so that's perfect.
>
> Yes, it does that for correctness (to make this non-offtopic on this
> list, one of the reasons is that the values crossing the cache lines are
> not guaranteed to be atomically read/written at least on x86) and
> performance reasons automatically.
>
> -Aleksey.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/7fdb18b4/attachment.html>

From vitalyd at gmail.com  Thu Dec 13 09:30:49 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 13 Dec 2012 09:30:49 -0500
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAPUTfNm=FLo0uO-UwdvbpwkfirzLLW_Te96X8aL094r6Jma5Sw@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<50C882D3.1020508@oracle.com>
	<CAPUTfNm=FLo0uO-UwdvbpwkfirzLLW_Te96X8aL094r6Jma5Sw@mail.gmail.com>
Message-ID: <CAHjP37FxbDtJ3kXBbFO7kfntpb3-8JCVP_kD774aEnWoFxsDig@mail.gmail.com>

On x86(64) CAS generates machine instruction for this (cmpxchg) with a lock
prefix.  This instruction cannot cas values bigger than machine word size
(so 32 or 64 bits).  The cache line on x86/64 is typically 64 bytes (or
more).  When a processor issues a locked cmpxchg what happens depends on
the state of the cache line (I.e. MESI protocol) - it can be a purely local
operation, for example.  But there's no padding per say since you can't cas
more than word size at a time.

The cache line splitting that Aleksey talks about refers to data spanning
multiple lines - the non atomic nature comes in because the processor only
deals in cache lines.  If you read a cache line split value it requires two
line fetches which is not atomic (this is assuming the CPU and/or OS even
allow unaligned reads).

The x86 manual talks about this for general purpose but compiler writers
(c/c++ or java or ...) take advantage/abide by the rules.  C/c++ compilers
also align/pad structs and classes to ensure perf and safety of mem
accesses on the target CPU.

I'm on my phone so above is a bit of a dump - let us know if you need more
info.

Vitaly

Sent from my phone
On Dec 13, 2012 9:07 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

>
> Aleksey,
>
> You mention that values exceeding cache line are not guaranteed to be
> atomic (in x86 at least).
> I have two questions here:
>
> Just to see if I get this correct:
> Assuming a cache line of 8 Bytes.
> Assume also my hardware supports 32-bit and 64-bit CAS instructions.
> If I issue an AtomicInteger CAS (32-bits) and an AtomicLong CAS (64-bit),
> will the JVM pad the first 32-bit CAS with additional 32-bits to fit the
> cache line, so the next 64-bit CAS fits in the next cache line?
> -----------------------
>
> Also a second question; while reading the x86 manual, I saw (among other
> operations) that following operations are guaranteed to be atomic:
>
> - Reading or writing a word aligned on a 16-bit boundary.
> ?- Reading or writing a doubleword aligned on a 32-bit boundary.
>
> I presume this applies only if programmed in C/C++ since in Java,
> alignment is out of programmers control. Is this correct?
>
> Thanks,
>
> Paris
>
>
>
> On Wed, Dec 12, 2012 at 1:12 PM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> On 12/12/2012 04:56 PM, Paris Yiapanis wrote:
>> > Sorry, I meant to say when creating an object to make the fields to be a
>> > number multiple of 4 bytes.
>> > For instance if I create an object with instance fields a byte and an
>> > integer that's 40 bits. So its 5 bytes. Will JVM pad it to make it 8
>> > (assuming 4 byte boundary hardware)?
>>
>> The answer to your question is basically "yes", with one complication:
>> object header. On 32-bit platform, it will probably do something like:
>>
>> a = object address [aligned at 4 bytes]
>>   a+0: (header)
>>  a+12: integer field [fits perfectly for 4-byte alignment]
>>  a+13: byte field
>>  a+14: (end of fields, gap)
>>
>>  a+16: (here the next object lies, due to 4-byte alignment)
>>
>> > From what you say the JVM does take care of it so that's perfect.
>>
>> Yes, it does that for correctness (to make this non-offtopic on this
>> list, one of the reasons is that the values crossing the cache lines are
>> not guaranteed to be atomically read/written at least on x86) and
>> performance reasons automatically.
>>
>> -Aleksey.
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/db2f0460/attachment.html>

From pyiapa at gmail.com  Thu Dec 13 09:51:28 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Thu, 13 Dec 2012 14:51:28 +0000
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAHjP37FxbDtJ3kXBbFO7kfntpb3-8JCVP_kD774aEnWoFxsDig@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<50C882D3.1020508@oracle.com>
	<CAPUTfNm=FLo0uO-UwdvbpwkfirzLLW_Te96X8aL094r6Jma5Sw@mail.gmail.com>
	<CAHjP37FxbDtJ3kXBbFO7kfntpb3-8JCVP_kD774aEnWoFxsDig@mail.gmail.com>
Message-ID: <CAPUTfN=hd8XuwyY4kjb2Q_kpeL=mbBpVsdR372_xfW4ef_VkPA@mail.gmail.com>

Vitaly,

Thanks for your response. I am just a bit confused about the first sentence
of your last paragraph.
So, do you mean that  essentially the Java compiler as it is will ignore
those two rules below (for example)? Thus, the compiler writer has to
explicitly write code (with knowledge of the specific hardware) to make
this happen?

-- Reading or writing a word aligned on a 16-bit boundary.
-- Reading or writing a doubleword aligned on a 32-bit boundary.



- Paris


On Thu, Dec 13, 2012 at 2:30 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> On x86(64) CAS generates machine instruction for this (cmpxchg) with a
> lock prefix.  This instruction cannot cas values bigger than machine word
> size (so 32 or 64 bits).  The cache line on x86/64 is typically 64 bytes
> (or more).  When a processor issues a locked cmpxchg what happens depends
> on the state of the cache line (I.e. MESI protocol) - it can be a purely
> local operation, for example.  But there's no padding per say since you
> can't cas more than word size at a time.
>
> The cache line splitting that Aleksey talks about refers to data spanning
> multiple lines - the non atomic nature comes in because the processor only
> deals in cache lines.  If you read a cache line split value it requires two
> line fetches which is not atomic (this is assuming the CPU and/or OS even
> allow unaligned reads).
>
> The x86 manual talks about this for general purpose but compiler writers
> (c/c++ or java or ...) take advantage/abide by the rules.  C/c++ compilers
> also align/pad structs and classes to ensure perf and safety of mem
> accesses on the target CPU.
>
> I'm on my phone so above is a bit of a dump - let us know if you need more
> info.
>
> Vitaly
>
> Sent from my phone
> On Dec 13, 2012 9:07 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>
>>
>> Aleksey,
>>
>> You mention that values exceeding cache line are not guaranteed to be
>> atomic (in x86 at least).
>> I have two questions here:
>>
>> Just to see if I get this correct:
>> Assuming a cache line of 8 Bytes.
>> Assume also my hardware supports 32-bit and 64-bit CAS instructions.
>> If I issue an AtomicInteger CAS (32-bits) and an AtomicLong CAS (64-bit),
>> will the JVM pad the first 32-bit CAS with additional 32-bits to fit the
>> cache line, so the next 64-bit CAS fits in the next cache line?
>> -----------------------
>>
>> Also a second question; while reading the x86 manual, I saw (among other
>> operations) that following operations are guaranteed to be atomic:
>>
>> - Reading or writing a word aligned on a 16-bit boundary.
>> ?- Reading or writing a doubleword aligned on a 32-bit boundary.
>>
>> I presume this applies only if programmed in C/C++ since in Java,
>> alignment is out of programmers control. Is this correct?
>>
>> Thanks,
>>
>> Paris
>>
>>
>>
>> On Wed, Dec 12, 2012 at 1:12 PM, Aleksey Shipilev <
>> aleksey.shipilev at oracle.com> wrote:
>>
>>> On 12/12/2012 04:56 PM, Paris Yiapanis wrote:
>>> > Sorry, I meant to say when creating an object to make the fields to be
>>> a
>>> > number multiple of 4 bytes.
>>> > For instance if I create an object with instance fields a byte and an
>>> > integer that's 40 bits. So its 5 bytes. Will JVM pad it to make it 8
>>> > (assuming 4 byte boundary hardware)?
>>>
>>> The answer to your question is basically "yes", with one complication:
>>> object header. On 32-bit platform, it will probably do something like:
>>>
>>> a = object address [aligned at 4 bytes]
>>>   a+0: (header)
>>>  a+12: integer field [fits perfectly for 4-byte alignment]
>>>  a+13: byte field
>>>  a+14: (end of fields, gap)
>>>
>>>  a+16: (here the next object lies, due to 4-byte alignment)
>>>
>>> > From what you say the JVM does take care of it so that's perfect.
>>>
>>> Yes, it does that for correctness (to make this non-offtopic on this
>>> list, one of the reasons is that the values crossing the cache lines are
>>> not guaranteed to be atomically read/written at least on x86) and
>>> performance reasons automatically.
>>>
>>> -Aleksey.
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/b1872cac/attachment-0001.html>

From vitalyd at gmail.com  Thu Dec 13 10:25:24 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 13 Dec 2012 10:25:24 -0500
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAPUTfN=hd8XuwyY4kjb2Q_kpeL=mbBpVsdR372_xfW4ef_VkPA@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<50C882D3.1020508@oracle.com>
	<CAPUTfNm=FLo0uO-UwdvbpwkfirzLLW_Te96X8aL094r6Jma5Sw@mail.gmail.com>
	<CAHjP37FxbDtJ3kXBbFO7kfntpb3-8JCVP_kD774aEnWoFxsDig@mail.gmail.com>
	<CAPUTfN=hd8XuwyY4kjb2Q_kpeL=mbBpVsdR372_xfW4ef_VkPA@mail.gmail.com>
Message-ID: <CAHjP37G-vfyDhxh-eh0fC3x6162Vqac+pP7LQ0_inwTgfZeNOA@mail.gmail.com>

So in that parlance, a word = 2 bytes (16 bits), double word = 4 bytes (32
bits), and though not mentioned in your snippet, you can also have quad
word = 8 bytes (64 bits).

What the above Intel manual is basically saying is that a read/write of a
piece of data aligned to its "natural" size is atomic.  If you're familiar
with c/c++ compilers, they tend to do this natural alignment as well
(unless developer explicitly requests something different).

In a nutshell, if your access is split across cache line, bus width, or
page boundary, read/write is not atomic.  If you read further in the Intel
manual, I believe it says that on Pentium and newer processors, unaligned
16, 32, and 64 bit access to memory that *fits within cache line* is
atomic.  So modern CPUs just require the bus width/cache line/page size
alignment requirement for (cacheable) memory access.  If any of these
constraints are violated, the machine will have to issue multiple memory
operations (read or write) to read/write the data and that's where
atomicity breaks down.

Compiler (and relatedly, assembly) writers do need to know alignment
(amongst plethora of other hardware specifics) requirements and what
guarantees are made in order to generate correct (and/or performant)
assembly.

Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/3d68f263/attachment.html>

From david.dice at gmail.com  Thu Dec 13 10:32:35 2012
From: david.dice at gmail.com (David Dice)
Date: Thu, 13 Dec 2012 10:32:35 -0500
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
Message-ID: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>

On x86, hotspot ensures all objects start on a 8-byte aligned address.
 Furthermore the placement mechanism is supposed to ensure "natural"
alignment for fields.   An 8-byte volatile long should have an offset
that's a multiple of 8, and given the object alignment it should then
should have an virtual address that's a multiple of 8.   Natural alignment
is best for performance and also ensures you don't have situations where a
volatile long might split a cache line.   Loads that split a line won't
necessarily be atomic.   Atomic instructions that split a line _will be
atomic, but that's handled through a rather bizarre legacy mechanism.  It's
best not to depend on this in the future.   Modern Intel implementations
basically quiesce the whole system to make the operation atomic.    It's
anything but a local operation and the whole system suffers.  There's some
discussion on the technique buried in the following :

 https://blogs.oracle.com/dave/entry/qpi_quiescence

Regards, -Dave
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/19f431bd/attachment.html>

From pyiapa at gmail.com  Thu Dec 13 11:46:16 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Thu, 13 Dec 2012 16:46:16 +0000
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAHjP37G-vfyDhxh-eh0fC3x6162Vqac+pP7LQ0_inwTgfZeNOA@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<50C882D3.1020508@oracle.com>
	<CAPUTfNm=FLo0uO-UwdvbpwkfirzLLW_Te96X8aL094r6Jma5Sw@mail.gmail.com>
	<CAHjP37FxbDtJ3kXBbFO7kfntpb3-8JCVP_kD774aEnWoFxsDig@mail.gmail.com>
	<CAPUTfN=hd8XuwyY4kjb2Q_kpeL=mbBpVsdR372_xfW4ef_VkPA@mail.gmail.com>
	<CAHjP37G-vfyDhxh-eh0fC3x6162Vqac+pP7LQ0_inwTgfZeNOA@mail.gmail.com>
Message-ID: <CAPUTfNmZ_NhP=ha+EsZO1o8SK0ZpMRQjTvnxS7-odnJzYCfFsw@mail.gmail.com>

Vitaly,

How about the Java compiler? Does it also do this natural alignment if the
piece of data is aligned to its "natural size"?
What I am trying to say in essence is: Can I rely on the Java compiler to
do the natural alignment so a non-CAS load/store will be atomic?

- Paris

On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> So in that parlance, a word = 2 bytes (16 bits), double word = 4 bytes (32
> bits), and though not mentioned in your snippet, you can also have quad
> word = 8 bytes (64 bits).
>
> What the above Intel manual is basically saying is that a read/write of a
> piece of data aligned to its "natural" size is atomic.  If you're familiar
> with c/c++ compilers, they tend to do this natural alignment as well
> (unless developer explicitly requests something different).
>
> In a nutshell, if your access is split across cache line, bus width, or
> page boundary, read/write is not atomic.  If you read further in the Intel
> manual, I believe it says that on Pentium and newer processors, unaligned
> 16, 32, and 64 bit access to memory that *fits within cache line* is
> atomic.  So modern CPUs just require the bus width/cache line/page size
> alignment requirement for (cacheable) memory access.  If any of these
> constraints are violated, the machine will have to issue multiple memory
> operations (read or write) to read/write the data and that's where
> atomicity breaks down.
>
> Compiler (and relatedly, assembly) writers do need to know alignment
> (amongst plethora of other hardware specifics) requirements and what
> guarantees are made in order to generate correct (and/or performant)
> assembly.
>
> Sent from my phone
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/86e28650/attachment.html>

From pyiapa at gmail.com  Thu Dec 13 12:00:27 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Thu, 13 Dec 2012 17:00:27 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
Message-ID: <CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>

Dave,

When you say 'the placement mechanism is supposed to ensure natural
alignment' I presume that it is not enforced.
Basically, I read a paper where they align four 16-bit subwords inside a
64-bit aligned word in order to guarantee atomic access (to each of those
16-bit subwords) without using CAS instructions (on an intel x86 machine).
However that was implemented in C++.
I was wondering if I could do the same with Java?


On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com> wrote:

>
> On x86, hotspot ensures all objects start on a 8-byte aligned address.
>  Furthermore the placement mechanism is supposed to ensure "natural"
> alignment for fields.   An 8-byte volatile long should have an offset
> that's a multiple of 8, and given the object alignment it should then
> should have an virtual address that's a multiple of 8.   Natural alignment
> is best for performance and also ensures you don't have situations where a
> volatile long might split a cache line.   Loads that split a line won't
> necessarily be atomic.   Atomic instructions that split a line _will be
> atomic, but that's handled through a rather bizarre legacy mechanism.  It's
> best not to depend on this in the future.   Modern Intel implementations
> basically quiesce the whole system to make the operation atomic.    It's
> anything but a local operation and the whole system suffers.  There's some
> discussion on the technique buried in the following :
>
>  https://blogs.oracle.com/dave/entry/qpi_quiescence
>
> Regards, -Dave
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/c0407052/attachment.html>

From gergg at cox.net  Thu Dec 13 12:07:25 2012
From: gergg at cox.net (Gregg Wonderly)
Date: Thu, 13 Dec 2012 11:07:25 -0600
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
Message-ID: <01AD9A19-AE72-4FAA-9CF2-B673B4DA2319@cox.net>

The JLS and JMM and code provided in java.util.concurrent all work together to make it unnecessary to worry about this issue for correctness.  There may be some kind of subtle, yet important performance issue that you could encounter, because some types of packing of data may not align such structures into a single cache-line.  If you are that much against the wall on performance, it may be that Java is not the right choice for "coding" that part of your application.  Moving some things into JNI could allow you to manage your own structures then.  

I personally try really hard to stay away from the precipice because I use Java to completely eliminate time wasted on such things that are largely unnecessary complications and create much more difficult debugging situations.

Gregg Wonderly

On Dec 13, 2012, at 11:00 AM, Paris Yiapanis <pyiapa at gmail.com> wrote:

> 
> Dave,
> 
> When you say 'the placement mechanism is supposed to ensure natural alignment' I presume that it is not enforced.
> Basically, I read a paper where they align four 16-bit subwords inside a 64-bit aligned word in order to guarantee atomic access (to each of those 16-bit subwords) without using CAS instructions (on an intel x86 machine). However that was implemented in C++. 
> I was wondering if I could do the same with Java?
> 
> 
> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com> wrote:
> 
> On x86, hotspot ensures all objects start on a 8-byte aligned address.    Furthermore the placement mechanism is supposed to ensure "natural" alignment for fields.   An 8-byte volatile long should have an offset that's a multiple of 8, and given the object alignment it should then should have an virtual address that's a multiple of 8.   Natural alignment is best for performance and also ensures you don't have situations where a volatile long might split a cache line.   Loads that split a line won't necessarily be atomic.   Atomic instructions that split a line _will be atomic, but that's handled through a rather bizarre legacy mechanism.  It's best not to depend on this in the future.   Modern Intel implementations basically quiesce the whole system to make the operation atomic.    It's anything but a local operation and the whole system suffers.  There's some discussion on the technique buried in the following :
> 
>  https://blogs.oracle.com/dave/entry/qpi_quiescence
> 
> Regards, -Dave
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/e890962e/attachment.html>

From vitalyd at gmail.com  Thu Dec 13 12:12:24 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 13 Dec 2012 12:12:24 -0500
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAHjP37GBugBgypqM=4L9ZJrns=kVbb8YoBAwB8x3y0qSVoJKSw@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<50C882D3.1020508@oracle.com>
	<CAPUTfNm=FLo0uO-UwdvbpwkfirzLLW_Te96X8aL094r6Jma5Sw@mail.gmail.com>
	<CAHjP37FxbDtJ3kXBbFO7kfntpb3-8JCVP_kD774aEnWoFxsDig@mail.gmail.com>
	<CAPUTfN=hd8XuwyY4kjb2Q_kpeL=mbBpVsdR372_xfW4ef_VkPA@mail.gmail.com>
	<CAHjP37G-vfyDhxh-eh0fC3x6162Vqac+pP7LQ0_inwTgfZeNOA@mail.gmail.com>
	<CAPUTfNmZ_NhP=ha+EsZO1o8SK0ZpMRQjTvnxS7-odnJzYCfFsw@mail.gmail.com>
	<CAHjP37GBugBgypqM=4L9ZJrns=kVbb8YoBAwB8x3y0qSVoJKSw@mail.gmail.com>
Message-ID: <CAHjP37H_gbwZ7CQzykMCKApNTwkEhkoPMUuiCcWMNDvd3w5zNg@mail.gmail.com>

You should read the Java spec, particularly the memory model section - that
will tell you what JVMs will provide; how they do it is implementation
specific.

To answer your question though, references and everything up to int are
atomic.  Longs are atomic on x64 but not x86.

Also, if you do plain load/store which is atomic in size, you don't get any
further guarantees in terms of visibility/ordering of instructions in
multithreaded scenarios; I'm assuming you know that and we're only talking
about this stuff in terms of cacheline/bus width/page boundary aspects.

Sent from my phone
On Dec 13, 2012 11:49 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

Vitaly,

How about the Java compiler? Does it also do this natural alignment if the
piece of data is aligned to its "natural size"?
What I am trying to say in essence is: Can I rely on the Java compiler to
do the natural alignment so a non-CAS load/store will be atomic?

- Paris

On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> So in that parlance, a word = 2 bytes (16 bits), double word = 4 bytes (32
> bits), and though not mentioned in your snippet, you can also have quad
> word = 8 bytes (64 bits).
>
> What the above Intel manual is basically saying is that a read/write of a
> piece of data aligned to its "natural" size is atomic.  If you're familiar
> with c/c++ compilers, they tend to do this natural alignment as well
> (unless developer explicitly requests something different).
>
> In a nutshell, if your access is split across cache line, bus width, or
> page boundary, read/write is not atomic.  If you read further in the Intel
> manual, I believe it says that on Pentium and newer processors, unaligned
> 16, 32, and 64 bit access to memory that *fits within cache line* is
> atomic.  So modern CPUs just require the bus width/cache line/page size
> alignment requirement for (cacheable) memory access.  If any of these
> constraints are violated, the machine will have to issue multiple memory
> operations (read or write) to read/write the data and that's where
> atomicity breaks down.
>
> Compiler (and relatedly, assembly) writers do need to know alignment
> (amongst plethora of other hardware specifics) requirements and what
> guarantees are made in order to generate correct (and/or performant)
> assembly.
>
> Sent from my phone
>


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/a2d66c4f/attachment.html>

From pyiapa at gmail.com  Thu Dec 13 12:20:13 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Thu, 13 Dec 2012 17:20:13 +0000
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAHjP37H_gbwZ7CQzykMCKApNTwkEhkoPMUuiCcWMNDvd3w5zNg@mail.gmail.com>
References: <CAPUTfNmocHacWeUxDgReHrRNwk=0VbaGj-TPxMj41q8Tykwkdw@mail.gmail.com>
	<50C873AB.70401@oracle.com>
	<CAPUTfNnqU3FZOnxOeGuxNQybb0VwY3aBWcppGirvAJjj0C886g@mail.gmail.com>
	<50C87AAE.5080905@oracle.com>
	<CAPUTfN=8mx+Sa8qixyaW6QGLC00rx6HX7cJe=cMuR7X_xVm3NA@mail.gmail.com>
	<50C882D3.1020508@oracle.com>
	<CAPUTfNm=FLo0uO-UwdvbpwkfirzLLW_Te96X8aL094r6Jma5Sw@mail.gmail.com>
	<CAHjP37FxbDtJ3kXBbFO7kfntpb3-8JCVP_kD774aEnWoFxsDig@mail.gmail.com>
	<CAPUTfN=hd8XuwyY4kjb2Q_kpeL=mbBpVsdR372_xfW4ef_VkPA@mail.gmail.com>
	<CAHjP37G-vfyDhxh-eh0fC3x6162Vqac+pP7LQ0_inwTgfZeNOA@mail.gmail.com>
	<CAPUTfNmZ_NhP=ha+EsZO1o8SK0ZpMRQjTvnxS7-odnJzYCfFsw@mail.gmail.com>
	<CAHjP37GBugBgypqM=4L9ZJrns=kVbb8YoBAwB8x3y0qSVoJKSw@mail.gmail.com>
	<CAHjP37H_gbwZ7CQzykMCKApNTwkEhkoPMUuiCcWMNDvd3w5zNg@mail.gmail.com>
Message-ID: <CAPUTfNkv1XQMF7zrt1FU6K9z2f+=CJJK4PR__tUDeJS1YiY++Q@mail.gmail.com>

I was concerned more on the atomicity part but I didn't realize that
visibility is not guaranteed.
Thank you very much for the information.

- Paris

On Thu, Dec 13, 2012 at 5:12 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> You should read the Java spec, particularly the memory model section -
> that will tell you what JVMs will provide; how they do it is implementation
> specific.
>
> To answer your question though, references and everything up to int are
> atomic.  Longs are atomic on x64 but not x86.
>
> Also, if you do plain load/store which is atomic in size, you don't get
> any further guarantees in terms of visibility/ordering of instructions in
> multithreaded scenarios; I'm assuming you know that and we're only talking
> about this stuff in terms of cacheline/bus width/page boundary aspects.
>
> Sent from my phone
> On Dec 13, 2012 11:49 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>
> Vitaly,
>
> How about the Java compiler? Does it also do this natural alignment if the
> piece of data is aligned to its "natural size"?
> What I am trying to say in essence is: Can I rely on the Java compiler to
> do the natural alignment so a non-CAS load/store will be atomic?
>
> - Paris
>
> On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> So in that parlance, a word = 2 bytes (16 bits), double word = 4 bytes
>> (32 bits), and though not mentioned in your snippet, you can also have quad
>> word = 8 bytes (64 bits).
>>
>> What the above Intel manual is basically saying is that a read/write of a
>> piece of data aligned to its "natural" size is atomic.  If you're familiar
>> with c/c++ compilers, they tend to do this natural alignment as well
>> (unless developer explicitly requests something different).
>>
>> In a nutshell, if your access is split across cache line, bus width, or
>> page boundary, read/write is not atomic.  If you read further in the Intel
>> manual, I believe it says that on Pentium and newer processors, unaligned
>> 16, 32, and 64 bit access to memory that *fits within cache line* is
>> atomic.  So modern CPUs just require the bus width/cache line/page size
>> alignment requirement for (cacheable) memory access.  If any of these
>> constraints are violated, the machine will have to issue multiple memory
>> operations (read or write) to read/write the data and that's where
>> atomicity breaks down.
>>
>> Compiler (and relatedly, assembly) writers do need to know alignment
>> (amongst plethora of other hardware specifics) requirements and what
>> guarantees are made in order to generate correct (and/or performant)
>> assembly.
>>
>> Sent from my phone
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/a74064f1/attachment-0001.html>

From vitalyd at gmail.com  Thu Dec 13 12:45:51 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 13 Dec 2012 12:45:51 -0500
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
Message-ID: <CAHjP37HKTTda_CVYZUL3My=pY-CaJaz13zfZK7KsQOy5rjFMww@mail.gmail.com>

You can pack data into the processor's word size and get atomic ops out of
it, but in multithreaded scenarios, you're on your own :).  For example,
compiler will generate a full word load even though you're only going to
modify some subword.  When you write this value back (thinking you're only
modifying your subword), you can trash the other subwords written by other
CPUs.

As for the c++ example, this isn't quite the same, but read this - you may
find it interesting: http://lwn.net/Articles/478657/.

Sent from my phone
On Dec 13, 2012 12:03 PM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

>
> Dave,
>
> When you say 'the placement mechanism is supposed to ensure natural
> alignment' I presume that it is not enforced.
> Basically, I read a paper where they align four 16-bit subwords inside a
> 64-bit aligned word in order to guarantee atomic access (to each of those
> 16-bit subwords) without using CAS instructions (on an intel x86 machine).
> However that was implemented in C++.
> I was wondering if I could do the same with Java?
>
>
> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com> wrote:
>
>>
>> On x86, hotspot ensures all objects start on a 8-byte aligned address.
>>  Furthermore the placement mechanism is supposed to ensure "natural"
>> alignment for fields.   An 8-byte volatile long should have an offset
>> that's a multiple of 8, and given the object alignment it should then
>> should have an virtual address that's a multiple of 8.   Natural alignment
>> is best for performance and also ensures you don't have situations where a
>> volatile long might split a cache line.   Loads that split a line won't
>> necessarily be atomic.   Atomic instructions that split a line _will be
>> atomic, but that's handled through a rather bizarre legacy mechanism.  It's
>> best not to depend on this in the future.   Modern Intel implementations
>> basically quiesce the whole system to make the operation atomic.    It's
>> anything but a local operation and the whole system suffers.  There's some
>> discussion on the technique buried in the following :
>>
>>  https://blogs.oracle.com/dave/entry/qpi_quiescence
>>
>> Regards, -Dave
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/9640d97e/attachment.html>

From pyiapa at gmail.com  Thu Dec 13 13:01:41 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Thu, 13 Dec 2012 18:01:41 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAHjP37HKTTda_CVYZUL3My=pY-CaJaz13zfZK7KsQOy5rjFMww@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<CAHjP37HKTTda_CVYZUL3My=pY-CaJaz13zfZK7KsQOy5rjFMww@mail.gmail.com>
Message-ID: <CAPUTfNkJ4uvMJtArOjk8rDUkgkfN0RYNkgnL+pn8vGEjQZKr8A@mail.gmail.com>

Very interesting articles.

Thank you very much guys.

- Paris

On Thu, Dec 13, 2012 at 5:45 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> You can pack data into the processor's word size and get atomic ops out of
> it, but in multithreaded scenarios, you're on your own :).  For example,
> compiler will generate a full word load even though you're only going to
> modify some subword.  When you write this value back (thinking you're only
> modifying your subword), you can trash the other subwords written by other
> CPUs.
>
> As for the c++ example, this isn't quite the same, but read this - you may
> find it interesting: http://lwn.net/Articles/478657/.
>
> Sent from my phone
> On Dec 13, 2012 12:03 PM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>
>>
>> Dave,
>>
>> When you say 'the placement mechanism is supposed to ensure natural
>> alignment' I presume that it is not enforced.
>> Basically, I read a paper where they align four 16-bit subwords inside a
>> 64-bit aligned word in order to guarantee atomic access (to each of those
>> 16-bit subwords) without using CAS instructions (on an intel x86 machine).
>> However that was implemented in C++.
>> I was wondering if I could do the same with Java?
>>
>>
>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com> wrote:
>>
>>>
>>> On x86, hotspot ensures all objects start on a 8-byte aligned address.
>>>  Furthermore the placement mechanism is supposed to ensure "natural"
>>> alignment for fields.   An 8-byte volatile long should have an offset
>>> that's a multiple of 8, and given the object alignment it should then
>>> should have an virtual address that's a multiple of 8.   Natural alignment
>>> is best for performance and also ensures you don't have situations where a
>>> volatile long might split a cache line.   Loads that split a line won't
>>> necessarily be atomic.   Atomic instructions that split a line _will be
>>> atomic, but that's handled through a rather bizarre legacy mechanism.  It's
>>> best not to depend on this in the future.   Modern Intel implementations
>>> basically quiesce the whole system to make the operation atomic.    It's
>>> anything but a local operation and the whole system suffers.  There's some
>>> discussion on the technique buried in the following :
>>>
>>>  https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>
>>> Regards, -Dave
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/d3ae3c70/attachment.html>

From dl at cs.oswego.edu  Thu Dec 13 16:01:57 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 13 Dec 2012 16:01:57 -0500
Subject: [concurrency-interest] jsr166e updates
Message-ID: <50CA4245.30403@cs.oswego.edu>


While stabilizing package jsr166e (still in preparation for
destablilizing JDK8 java.util.concurrent), I committed some
improvements to ForkJoin*, and especially ConcurrentHashMapV8,
which now supports cooperative resizing. Thanks to several of
you for reports, suggestions, and helpful prods along the way.

Classes in the jsr166e package probably will not change much
in the near future. In particular, ConcurrentHashMapV8 will continue to
have embedded declarations of interfaces for functions etc that
are similar to those expected for JDK8, but remain usable in JDK7.
The java.util.concurrent.ConcurrentHashMap version is at the
moment (nearly) identical, but will soon be usable only in JDK8
because it will import other JDK8 stuff.

Please try out the updates. Here are the usual links:

API specs: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/

jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar (compiled using 
Java7 javac).

Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/

-Doug

From davidcholmes at aapt.net.au  Thu Dec 13 16:44:09 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 14 Dec 2012 07:44:09 +1000
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAHjP37H_gbwZ7CQzykMCKApNTwkEhkoPMUuiCcWMNDvd3w5zNg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCELKJIAA.davidcholmes@aapt.net.au>

Just to expand a little, the atomicity guarantee (for load/store) is
extended to volatile long and volatile double.

Just to nit pick, long accesses are not required to be atomic even on
64-bit. Hotspot will implement them that way of course.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
  Sent: Friday, 14 December 2012 3:12 AM
  To: Paris Yiapanis
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Memory Alignment in Java


  You should read the Java spec, particularly the memory model section -
that will tell you what JVMs will provide; how they do it is implementation
specific.

  To answer your question though, references and everything up to int are
atomic.  Longs are atomic on x64 but not x86.

  Also, if you do plain load/store which is atomic in size, you don't get
any further guarantees in terms of visibility/ordering of instructions in
multithreaded scenarios; I'm assuming you know that and we're only talking
about this stuff in terms of cacheline/bus width/page boundary aspects.

  Sent from my phone

  On Dec 13, 2012 11:49 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

    Vitaly,

    How about the Java compiler? Does it also do this natural alignment if
the piece of data is aligned to its "natural size"?
    What I am trying to say in essence is: Can I rely on the Java compiler
to do the natural alignment so a non-CAS load/store will be atomic?

    - Paris



    On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

      So in that parlance, a word = 2 bytes (16 bits), double word = 4 bytes
(32 bits), and though not mentioned in your snippet, you can also have quad
word = 8 bytes (64 bits).

      What the above Intel manual is basically saying is that a read/write
of a piece of data aligned to its "natural" size is atomic.  If you're
familiar with c/c++ compilers, they tend to do this natural alignment as
well (unless developer explicitly requests something different).

      In a nutshell, if your access is split across cache line, bus width,
or page boundary, read/write is not atomic.  If you read further in the
Intel manual, I believe it says that on Pentium and newer processors,
unaligned 16, 32, and 64 bit access to memory that *fits within cache line*
is atomic.  So modern CPUs just require the bus width/cache line/page size
alignment requirement for (cacheable) memory access.  If any of these
constraints are violated, the machine will have to issue multiple memory
operations (read or write) to read/write the data and that's where atomicity
breaks down.

      Compiler (and relatedly, assembly) writers do need to know alignment
(amongst plethora of other hardware specifics) requirements and what
guarantees are made in order to generate correct (and/or performant)
assembly.

      Sent from my phone




    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/5fda3cf8/attachment-0001.html>

From vitalyd at gmail.com  Thu Dec 13 17:33:31 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 13 Dec 2012 17:33:31 -0500
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCELKJIAA.davidcholmes@aapt.net.au>
References: <CAHjP37H_gbwZ7CQzykMCKApNTwkEhkoPMUuiCcWMNDvd3w5zNg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCELKJIAA.davidcholmes@aapt.net.au>
Message-ID: <CAHjP37EkQO7Wrmq4gOWZUNPruhWF=+b_gRhk-vSdeo+4FBEDig@mail.gmail.com>

Yes, good point about long access - I forgot to say that I was answering in
the context of hotspot (only JVM I have experience with).  I'd be surprised
if other mainstream JVMs weren't the same way on x64 though ...

Sent from my phone
On Dec 13, 2012 4:44 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

> **
> Just to expand a little, the atomicity guarantee (for load/store) is
> extended to volatile long and volatile double.
>
> Just to nit pick, long accesses are not required to be atomic even on
> 64-bit. Hotspot will implement them that way of course.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Vitaly
> Davidovich
> *Sent:* Friday, 14 December 2012 3:12 AM
> *To:* Paris Yiapanis
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Memory Alignment in Java
>
> You should read the Java spec, particularly the memory model section -
> that will tell you what JVMs will provide; how they do it is implementation
> specific.
>
> To answer your question though, references and everything up to int are
> atomic.  Longs are atomic on x64 but not x86.
>
> Also, if you do plain load/store which is atomic in size, you don't get
> any further guarantees in terms of visibility/ordering of instructions in
> multithreaded scenarios; I'm assuming you know that and we're only talking
> about this stuff in terms of cacheline/bus width/page boundary aspects.
>
> Sent from my phone
> On Dec 13, 2012 11:49 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>
> Vitaly,
>
> How about the Java compiler? Does it also do this natural alignment if the
> piece of data is aligned to its "natural size"?
> What I am trying to say in essence is: Can I rely on the Java compiler to
> do the natural alignment so a non-CAS load/store will be atomic?
>
> - Paris
>
> On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> So in that parlance, a word = 2 bytes (16 bits), double word = 4 bytes
>> (32 bits), and though not mentioned in your snippet, you can also have quad
>> word = 8 bytes (64 bits).
>>
>> What the above Intel manual is basically saying is that a read/write of a
>> piece of data aligned to its "natural" size is atomic.  If you're familiar
>> with c/c++ compilers, they tend to do this natural alignment as well
>> (unless developer explicitly requests something different).
>>
>> In a nutshell, if your access is split across cache line, bus width, or
>> page boundary, read/write is not atomic.  If you read further in the Intel
>> manual, I believe it says that on Pentium and newer processors, unaligned
>> 16, 32, and 64 bit access to memory that *fits within cache line* is
>> atomic.  So modern CPUs just require the bus width/cache line/page size
>> alignment requirement for (cacheable) memory access.  If any of these
>> constraints are violated, the machine will have to issue multiple memory
>> operations (read or write) to read/write the data and that's where
>> atomicity breaks down.
>>
>> Compiler (and relatedly, assembly) writers do need to know alignment
>> (amongst plethora of other hardware specifics) requirements and what
>> guarantees are made in order to generate correct (and/or performant)
>> assembly.
>>
>> Sent from my phone
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/5a5aeccf/attachment.html>

From nitsanw at yahoo.com  Thu Dec 13 17:34:13 2012
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Thu, 13 Dec 2012 14:34:13 -0800 (PST)
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCELKJIAA.davidcholmes@aapt.net.au>
References: <CAHjP37H_gbwZ7CQzykMCKApNTwkEhkoPMUuiCcWMNDvd3w5zNg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCELKJIAA.davidcholmes@aapt.net.au>
Message-ID: <1355438053.32859.YahooMailNeo@web120703.mail.ne1.yahoo.com>

Looking at Unsafe.allocateMemory - it guarantees the memory is aligned to the largest type, so 8b aligned? Thought I read somewhere that it is actually 16b?
What happens if I write a type across the cache line using Unsafe.put*()?
Example:
long address = Unsafe.allocateMemory (128); //returns 48 which is 16b aligned
Unsafe.putLong(address, 1L);

Unsafe.putInt(address+8, 1L);

Unsafe.putLong(address+12, 1L); --> split




________________________________
 From: David Holmes <davidcholmes at aapt.net.au>
To: Vitaly Davidovich <vitalyd at gmail.com>; Paris Yiapanis <pyiapa at gmail.com> 
Cc: concurrency-interest at cs.oswego.edu 
Sent: Thursday, December 13, 2012 9:44 PM
Subject: Re: [concurrency-interest] Memory Alignment in Java
 

Just 
to expand a little, the atomicity guarantee (for load/store) is extended to 
volatile long and volatile double.
?
Just 
to nit pick, long accesses are not required to be atomic even on 64-bit. Hotspot 
will implement them that way of course.
?
David
-----Original Message-----
>From: concurrency-interest-bounces at cs.oswego.edu  [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly  Davidovich
>Sent: Friday, 14 December 2012 3:12 AM
>To: Paris Yiapanis
>Cc: concurrency-interest at cs.oswego.edu
>Subject: Re:  [concurrency-interest] Memory Alignment in Java
>
>
>You should read the Java spec, particularly the memory model  section - that will tell you what JVMs will provide; how they do it is  implementation specific.
>To answer your question though, references and everything up to int  are atomic.? Longs are atomic on x64 but not x86.
>Also, if you do plain load/store which is atomic in size, you don't  get any further guarantees in terms of visibility/ordering of instructions in  multithreaded scenarios; I'm assuming you know that and we're only talking  about this stuff in terms of cacheline/bus width/page boundary aspects.
>Sent from my phone
>On Dec 13, 2012 11:49 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>
>Vitaly,
>>
>>How about the Java compiler? Does it also do this 
    natural alignment if the piece of data is aligned to its "natural 
    size"?
>>What I am trying to say in essence is: Can I rely on the Java 
    compiler to do the natural alignment so a non-CAS load/store will be 
    atomic?
>>
>>- Paris 
>> 
>>
>>
>>On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>>So in that parlance, a word = 2 bytes (16 bits), double word =  4 bytes (32 bits), and though not mentioned in your snippet, you can also  have quad word = 8 bytes (64 bits).
>>>What the above Intel manual is basically saying is that a  read/write of a piece of data aligned to its "natural" size is  atomic.? If you're familiar with c/c++ compilers, they tend to do  this natural alignment as well (unless developer explicitly requests  something different).
>>>In a nutshell, if your access is split across cache line, bus  width, or page boundary, read/write is not atomic.? If you read  further in the Intel manual, I believe it says that on Pentium and newer  processors, unaligned 16, 32, and 64 bit access to memory that *fits  within cache line* is atomic.? So modern CPUs just require the bus  width/cache line/page size alignment requirement for (cacheable) memory  access.? If any of these constraints are violated, the machine will  have to issue multiple memory operations (read or write) to read/write the  data and that's where atomicity breaks down.
>>>Compiler (and relatedly, assembly) writers do need to know  alignment (amongst plethora of other hardware specifics) requirements and  what guarantees are made in order to generate correct (and/or performant)  assembly.
>>>Sent from my  phone
>>
>>_______________________________________________
>>Concurrency-interest 
    mailing list
>>Concurrency-interest at cs.oswego.edu
>>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/1af15917/attachment.html>

From davidcholmes at aapt.net.au  Thu Dec 13 17:45:16 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 14 Dec 2012 08:45:16 +1000
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <1355438053.32859.YahooMailNeo@web120703.mail.ne1.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMELLJIAA.davidcholmes@aapt.net.au>

If you force an unaligned value then you get an unaligned value. For x86
this just results in a slower access (and non-atomic of course). For other
platforms you might get a SIGBUS.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nitsan
Wakart
  Sent: Friday, 14 December 2012 8:34 AM
  To: dholmes at ieee.org; Vitaly Davidovich; Paris Yiapanis
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Memory Alignment in Java


  Looking at Unsafe.allocateMemory - it guarantees the memory is aligned to
the largest type, so 8b aligned? Thought I read somewhere that it is
actually 16b?
  What happens if I write a type across the cache line using Unsafe.put*()?
  Example:
  long address = Unsafe.allocateMemory (128); //returns 48 which is 16b
aligned
  Unsafe.putLong(address, 1L);

  Unsafe.putInt(address+8, 1L);

  Unsafe.putLong(address+12, 1L); --> split






----------------------------------------------------------------------------
--
  From: David Holmes <davidcholmes at aapt.net.au>
  To: Vitaly Davidovich <vitalyd at gmail.com>; Paris Yiapanis
<pyiapa at gmail.com>
  Cc: concurrency-interest at cs.oswego.edu
  Sent: Thursday, December 13, 2012 9:44 PM
  Subject: Re: [concurrency-interest] Memory Alignment in Java



  Just to expand a little, the atomicity guarantee (for load/store) is
extended to volatile long and volatile double.

  Just to nit pick, long accesses are not required to be atomic even on
64-bit. Hotspot will implement them that way of course.

  David
    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
    Sent: Friday, 14 December 2012 3:12 AM
    To: Paris Yiapanis
    Cc: concurrency-interest at cs.oswego.edu
    Subject: Re: [concurrency-interest] Memory Alignment in Java


    You should read the Java spec, particularly the memory model section -
that will tell you what JVMs will provide; how they do it is implementation
specific.
    To answer your question though, references and everything up to int are
atomic.  Longs are atomic on x64 but not x86.
    Also, if you do plain load/store which is atomic in size, you don't get
any further guarantees in terms of visibility/ordering of instructions in
multithreaded scenarios; I'm assuming you know that and we're only talking
about this stuff in terms of cacheline/bus width/page boundary aspects.
    Sent from my phone
    On Dec 13, 2012 11:49 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

      Vitaly,

      How about the Java compiler? Does it also do this natural alignment if
the piece of data is aligned to its "natural size"?
      What I am trying to say in essence is: Can I rely on the Java compiler
to do the natural alignment so a non-CAS load/store will be atomic?

      - Paris



      On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

        So in that parlance, a word = 2 bytes (16 bits), double word = 4
bytes (32 bits), and though not mentioned in your snippet, you can also have
quad word = 8 bytes (64 bits).
        What the above Intel manual is basically saying is that a read/write
of a piece of data aligned to its "natural" size is atomic.  If you're
familiar with c/c++ compilers, they tend to do this natural alignment as
well (unless developer explicitly requests something different).
        In a nutshell, if your access is split across cache line, bus width,
or page boundary, read/write is not atomic.  If you read further in the
Intel manual, I believe it says that on Pentium and newer processors,
unaligned 16, 32, and 64 bit access to memory that *fits within cache line*
is atomic.  So modern CPUs just require the bus width/cache line/page size
alignment requirement for (cacheable) memory access.  If any of these
constraints are violated, the machine will have to issue multiple memory
operations (read or write) to read/write the data and that's where atomicity
breaks down.
        Compiler (and relatedly, assembly) writers do need to know alignment
(amongst plethora of other hardware specifics) requirements and what
guarantees are made in order to generate correct (and/or performant)
assembly.
        Sent from my phone



      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest



  _______________________________________________
  Concurrency-interest mailing list
  Concurrency-interest at cs.oswego.edu
  http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/08d9ffae/attachment-0001.html>

From vitalyd at gmail.com  Thu Dec 13 17:50:10 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 13 Dec 2012 17:50:10 -0500
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAHjP37FxX-qxCc5QXk5ZCSwuf60AEXoapQcBo7ju=T+_N5tgoA@mail.gmail.com>
References: <1355438053.32859.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCMELLJIAA.davidcholmes@aapt.net.au>
	<CAHjP37FxX-qxCc5QXk5ZCSwuf60AEXoapQcBo7ju=T+_N5tgoA@mail.gmail.com>
Message-ID: <CAHjP37Fx6eRung7J3uW4+XpqwOi7v7ocRwDmLb=zR=1Pd8tecg@mail.gmail.com>

Right, to expand a bit:

- if the CPU supports unaligned reads, it does it - typically slower than
normal because it has to load multiple cache lines and then mask out the
bytes you didn't request

- CPU generates sigbus, operating system traps it and then performs aligned
reads + masking on your behalf (also slow)

- CPU sigbus and OS doesn't handle it (don't know offhand what combo this
is out in the wild - maybe some embedded stuff)

Sent from my phone
On Dec 13, 2012 5:45 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

**
If you force an unaligned value then you get an unaligned value. For x86
this just results in a slower access (and non-atomic of course). For other
platforms you might get a SIGBUS.

David

-----Original Message-----
*From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Nitsan Wakart
*Sent:* Friday, 14 December 2012 8:34 AM
*To:* dholmes at ieee.org; Vitaly Davidovich; Paris Yiapanis
*Cc:* concurrency-interest at cs.oswego.edu
*Subject:* Re: [concurrency-interest] Memory Alignment in Java

Looking at Unsafe.allocateMemory - it guarantees the memory is aligned to
the largest type, so 8b aligned? Thought I read somewhere that it is
actually 16b?
What happens if I write a type across the cache line using Unsafe.put*()?
Example:
long address = Unsafe.allocateMemory (128); //returns 48 which is 16b
aligned
Unsafe.putLong(address, 1L);
Unsafe.putInt(address+8, 1L);
Unsafe.putLong(address+12, 1L); --> split


  ------------------------------
*From:* David Holmes <davidcholmes at aapt.net.au>
*To:* Vitaly Davidovich <vitalyd at gmail.com>; Paris Yiapanis <
pyiapa at gmail.com>
*Cc:* concurrency-interest at cs.oswego.edu
*Sent:* Thursday, December 13, 2012 9:44 PM
*Subject:* Re: [concurrency-interest] Memory Alignment in Java

 Just to expand a little, the atomicity guarantee (for load/store) is
extended to volatile long and volatile double.

Just to nit pick, long accesses are not required to be atomic even on
64-bit. Hotspot will implement them that way of course.

David

-----Original Message-----
*From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Vitaly Davidovich
*Sent:* Friday, 14 December 2012 3:12 AM
*To:* Paris Yiapanis
*Cc:* concurrency-interest at cs.oswego.edu
*Subject:* Re: [concurrency-interest] Memory Alignment in Java

You should read the Java spec, particularly the memory model section - that
will tell you what JVMs will provide; how they do it is implementation
specific.
To answer your question though, references and everything up to int are
atomic.  Longs are atomic on x64 but not x86.
Also, if you do plain load/store which is atomic in size, you don't get any
further guarantees in terms of visibility/ordering of instructions in
multithreaded scenarios; I'm assuming you know that and we're only talking
about this stuff in terms of cacheline/bus width/page boundary aspects.
Sent from my phone
On Dec 13, 2012 11:49 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

Vitaly,

How about the Java compiler? Does it also do this natural alignment if the
piece of data is aligned to its "natural size"?
What I am trying to say in essence is: Can I rely on the Java compiler to
do the natural alignment so a non-CAS load/store will be atomic?

- Paris

On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

So in that parlance, a word = 2 bytes (16 bits), double word = 4 bytes (32
bits), and though not mentioned in your snippet, you can also have quad
word = 8 bytes (64 bits).
What the above Intel manual is basically saying is that a read/write of a
piece of data aligned to its "natural" size is atomic.  If you're familiar
with c/c++ compilers, they tend to do this natural alignment as well
(unless developer explicitly requests something different).
In a nutshell, if your access is split across cache line, bus width, or
page boundary, read/write is not atomic.  If you read further in the Intel
manual, I believe it says that on Pentium and newer processors, unaligned
16, 32, and 64 bit access to memory that *fits within cache line* is
atomic.  So modern CPUs just require the bus width/cache line/page size
alignment requirement for (cacheable) memory access.  If any of these
constraints are violated, the machine will have to issue multiple memory
operations (read or write) to read/write the data and that's where
atomicity breaks down.
Compiler (and relatedly, assembly) writers do need to know alignment
(amongst plethora of other hardware specifics) requirements and what
guarantees are made in order to generate correct (and/or performant)
assembly.
Sent from my phone



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/bb922b11/attachment.html>

From vitalyd at gmail.com  Thu Dec 13 17:54:20 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 13 Dec 2012 17:54:20 -0500
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <1355438053.32859.YahooMailNeo@web120703.mail.ne1.yahoo.com>
References: <CAHjP37H_gbwZ7CQzykMCKApNTwkEhkoPMUuiCcWMNDvd3w5zNg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCELKJIAA.davidcholmes@aapt.net.au>
	<1355438053.32859.YahooMailNeo@web120703.mail.ne1.yahoo.com>
Message-ID: <CAHjP37EJgS1zmyv-Lb6xFtfDELmTKXTmGh5mscg=98Wj0sZYig@mail.gmail.com>

It aligns to HeapWordSize, which is the same value used for managed object
allocation alignment; this is 8 bytes, as Dave Dice mentioned earlier.

Sent from my phone
On Dec 13, 2012 5:34 PM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:

> Looking at Unsafe.allocateMemory - it guarantees the memory is aligned to
> the largest type, so 8b aligned? Thought I read somewhere that it is
> actually 16b?
> What happens if I write a type across the cache line using Unsafe.put*()?
> Example:
> long address = Unsafe.allocateMemory (128); //returns 48 which is 16b
> aligned
> Unsafe.putLong(address, 1L);
> Unsafe.putInt(address+8, 1L);
> Unsafe.putLong(address+12, 1L); --> split
>
>
>   ------------------------------
> *From:* David Holmes <davidcholmes at aapt.net.au>
> *To:* Vitaly Davidovich <vitalyd at gmail.com>; Paris Yiapanis <
> pyiapa at gmail.com>
> *Cc:* concurrency-interest at cs.oswego.edu
> *Sent:* Thursday, December 13, 2012 9:44 PM
> *Subject:* Re: [concurrency-interest] Memory Alignment in Java
>
>  Just to expand a little, the atomicity guarantee (for load/store) is
> extended to volatile long and volatile double.
>
> Just to nit pick, long accesses are not required to be atomic even on
> 64-bit. Hotspot will implement them that way of course.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Vitaly
> Davidovich
> *Sent:* Friday, 14 December 2012 3:12 AM
> *To:* Paris Yiapanis
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Memory Alignment in Java
>
> You should read the Java spec, particularly the memory model section -
> that will tell you what JVMs will provide; how they do it is implementation
> specific.
> To answer your question though, references and everything up to int are
> atomic.  Longs are atomic on x64 but not x86.
> Also, if you do plain load/store which is atomic in size, you don't get
> any further guarantees in terms of visibility/ordering of instructions in
> multithreaded scenarios; I'm assuming you know that and we're only talking
> about this stuff in terms of cacheline/bus width/page boundary aspects.
> Sent from my phone
> On Dec 13, 2012 11:49 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>
> Vitaly,
>
> How about the Java compiler? Does it also do this natural alignment if the
> piece of data is aligned to its "natural size"?
> What I am trying to say in essence is: Can I rely on the Java compiler to
> do the natural alignment so a non-CAS load/store will be atomic?
>
> - Paris
>
> On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
> So in that parlance, a word = 2 bytes (16 bits), double word = 4 bytes (32
> bits), and though not mentioned in your snippet, you can also have quad
> word = 8 bytes (64 bits).
> What the above Intel manual is basically saying is that a read/write of a
> piece of data aligned to its "natural" size is atomic.  If you're familiar
> with c/c++ compilers, they tend to do this natural alignment as well
> (unless developer explicitly requests something different).
> In a nutshell, if your access is split across cache line, bus width, or
> page boundary, read/write is not atomic.  If you read further in the Intel
> manual, I believe it says that on Pentium and newer processors, unaligned
> 16, 32, and 64 bit access to memory that *fits within cache line* is
> atomic.  So modern CPUs just require the bus width/cache line/page size
> alignment requirement for (cacheable) memory access.  If any of these
> constraints are violated, the machine will have to issue multiple memory
> operations (read or write) to read/write the data and that's where
> atomicity breaks down.
> Compiler (and relatedly, assembly) writers do need to know alignment
> (amongst plethora of other hardware specifics) requirements and what
> guarantees are made in order to generate correct (and/or performant)
> assembly.
> Sent from my phone
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/4470fd15/attachment-0001.html>

From vitalyd at gmail.com  Thu Dec 13 18:03:32 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 13 Dec 2012 18:03:32 -0500
Subject: [concurrency-interest] Memory Alignment in Java
In-Reply-To: <CAHjP37EJgS1zmyv-Lb6xFtfDELmTKXTmGh5mscg=98Wj0sZYig@mail.gmail.com>
References: <CAHjP37H_gbwZ7CQzykMCKApNTwkEhkoPMUuiCcWMNDvd3w5zNg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCELKJIAA.davidcholmes@aapt.net.au>
	<1355438053.32859.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CAHjP37EJgS1zmyv-Lb6xFtfDELmTKXTmGh5mscg=98Wj0sZYig@mail.gmail.com>
Message-ID: <CAHjP37GfwUgwX29YrrwO0B8dAGct+epsOYbuviHURhb3xwvJow@mail.gmail.com>

By the way, you can install a different allocator which
Unsafe.allocateMemory will end up calling (it calls JVM's os::malloc()
which calls the platform malloc, which you can hook into or override).  I
think Cassandra does something like this (using jemalloc) for their off
heap store.

The point here is the underlying platform allocator can add its own
alignment requirements.  So I guess the answer is you'll get *at least* 8
byte alignment.

Sent from my phone
On Dec 13, 2012 5:54 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> It aligns to HeapWordSize, which is the same value used for managed object
> allocation alignment; this is 8 bytes, as Dave Dice mentioned earlier.
>
> Sent from my phone
> On Dec 13, 2012 5:34 PM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:
>
>> Looking at Unsafe.allocateMemory - it guarantees the memory is aligned to
>> the largest type, so 8b aligned? Thought I read somewhere that it is
>> actually 16b?
>> What happens if I write a type across the cache line using Unsafe.put*()?
>> Example:
>> long address = Unsafe.allocateMemory (128); //returns 48 which is 16b
>> aligned
>> Unsafe.putLong(address, 1L);
>> Unsafe.putInt(address+8, 1L);
>> Unsafe.putLong(address+12, 1L); --> split
>>
>>
>>   ------------------------------
>> *From:* David Holmes <davidcholmes at aapt.net.au>
>> *To:* Vitaly Davidovich <vitalyd at gmail.com>; Paris Yiapanis <
>> pyiapa at gmail.com>
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Sent:* Thursday, December 13, 2012 9:44 PM
>> *Subject:* Re: [concurrency-interest] Memory Alignment in Java
>>
>>  Just to expand a little, the atomicity guarantee (for load/store) is
>> extended to volatile long and volatile double.
>>
>> Just to nit pick, long accesses are not required to be atomic even on
>> 64-bit. Hotspot will implement them that way of course.
>>
>> David
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Vitaly
>> Davidovich
>> *Sent:* Friday, 14 December 2012 3:12 AM
>> *To:* Paris Yiapanis
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Memory Alignment in Java
>>
>> You should read the Java spec, particularly the memory model section -
>> that will tell you what JVMs will provide; how they do it is implementation
>> specific.
>> To answer your question though, references and everything up to int are
>> atomic.  Longs are atomic on x64 but not x86.
>> Also, if you do plain load/store which is atomic in size, you don't get
>> any further guarantees in terms of visibility/ordering of instructions in
>> multithreaded scenarios; I'm assuming you know that and we're only talking
>> about this stuff in terms of cacheline/bus width/page boundary aspects.
>> Sent from my phone
>> On Dec 13, 2012 11:49 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>>
>> Vitaly,
>>
>> How about the Java compiler? Does it also do this natural alignment if
>> the piece of data is aligned to its "natural size"?
>> What I am trying to say in essence is: Can I rely on the Java compiler to
>> do the natural alignment so a non-CAS load/store will be atomic?
>>
>> - Paris
>>
>> On Thu, Dec 13, 2012 at 3:25 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>
>> So in that parlance, a word = 2 bytes (16 bits), double word = 4 bytes
>> (32 bits), and though not mentioned in your snippet, you can also have quad
>> word = 8 bytes (64 bits).
>> What the above Intel manual is basically saying is that a read/write of a
>> piece of data aligned to its "natural" size is atomic.  If you're familiar
>> with c/c++ compilers, they tend to do this natural alignment as well
>> (unless developer explicitly requests something different).
>> In a nutshell, if your access is split across cache line, bus width, or
>> page boundary, read/write is not atomic.  If you read further in the Intel
>> manual, I believe it says that on Pentium and newer processors, unaligned
>> 16, 32, and 64 bit access to memory that *fits within cache line* is
>> atomic.  So modern CPUs just require the bus width/cache line/page size
>> alignment requirement for (cacheable) memory access.  If any of these
>> constraints are violated, the machine will have to issue multiple memory
>> operations (read or write) to read/write the data and that's where
>> atomicity breaks down.
>> Compiler (and relatedly, assembly) writers do need to know alignment
>> (amongst plethora of other hardware specifics) requirements and what
>> guarantees are made in order to generate correct (and/or performant)
>> assembly.
>> Sent from my phone
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121213/251bb340/attachment.html>

From oleksandr.otenko at oracle.com  Fri Dec 14 06:41:10 2012
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Fri, 14 Dec 2012 11:41:10 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
Message-ID: <50CB1056.8010909@oracle.com>

What are you trying to do?

Declare the 16-bit quantities volatile, and you get atomic access to 
them "without a CAS". (Not even sure how would CAS help otherwise)

Alex

On 13/12/2012 17:00, Paris Yiapanis wrote:
>
> Dave,
>
> When you say 'the placement mechanism is supposed to ensure natural 
> alignment' I presume that it is not enforced.
> Basically, I read a paper where they align four 16-bit subwords inside 
> a 64-bit aligned word in order to guarantee atomic access (to each of 
> those 16-bit subwords) without using CAS instructions (on an intel x86 
> machine). However that was implemented in C++.
> I was wondering if I could do the same with Java?
>
>
> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com 
> <mailto:david.dice at gmail.com>> wrote:
>
>
>     On x86, hotspot ensures all objects start on a 8-byte aligned
>     address.    Furthermore the placement mechanism is supposed to
>     ensure "natural" alignment for fields.   An 8-byte volatile long
>     should have an offset that's a multiple of 8, and given the object
>     alignment it should then should have an virtual address that's a
>     multiple of 8.   Natural alignment is best for performance and
>     also ensures you don't have situations where a volatile long might
>     split a cache line.   Loads that split a line won't necessarily be
>     atomic.   Atomic instructions that split a line _will be atomic,
>     but that's handled through a rather bizarre legacy mechanism.
>      It's best not to depend on this in the future.   Modern Intel
>     implementations basically quiesce the whole system to make the
>     operation atomic.    It's anything but a local operation and the
>     whole system suffers.  There's some discussion on the technique
>     buried in the following :
>
>     https://blogs.oracle.com/dave/entry/qpi_quiescence
>
>     Regards, -Dave
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/55c3701a/attachment.html>

From pyiapa at gmail.com  Fri Dec 14 06:53:03 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Fri, 14 Dec 2012 11:53:03 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <50CB1056.8010909@oracle.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
Message-ID: <CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>

Not exactly. According to intel x86 specification if you place four 16-bit
subwords inside a 64-bit aligned word, then an access to each of those
16-bit subwords is guaranteed to be atomic. I think (maybe I am wrong) the
reason is because such an access to one of those subwords will cause the
entire cache line to flush.
So how about in Java? If you put together four consecutive Short primitives
(that are 16-bits each) will the same scenario occur? Not volatile shorts,
just plain shorts.
But if I understand correct from the discussion, this is guaranteed for
C/C++ that you can specify explicitly the alignment but it is not
guaranteed for Java.

- Paris

On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  What are you trying to do?
>
> Declare the 16-bit quantities volatile, and you get atomic access to them
> "without a CAS". (Not even sure how would CAS help otherwise)
>
> Alex
>
> On 13/12/2012 17:00, Paris Yiapanis wrote:
>
>
> Dave,
>
> When you say 'the placement mechanism is supposed to ensure natural
> alignment' I presume that it is not enforced.
> Basically, I read a paper where they align four 16-bit subwords inside a
> 64-bit aligned word in order to guarantee atomic access (to each of those
> 16-bit subwords) without using CAS instructions (on an intel x86 machine).
> However that was implemented in C++.
> I was wondering if I could do the same with Java?
>
>
> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com> wrote:
>
>>
>> On x86, hotspot ensures all objects start on a 8-byte aligned address.
>>  Furthermore the placement mechanism is supposed to ensure "natural"
>> alignment for fields.   An 8-byte volatile long should have an offset
>> that's a multiple of 8, and given the object alignment it should then
>> should have an virtual address that's a multiple of 8.   Natural alignment
>> is best for performance and also ensures you don't have situations where a
>> volatile long might split a cache line.   Loads that split a line won't
>> necessarily be atomic.   Atomic instructions that split a line _will be
>> atomic, but that's handled through a rather bizarre legacy mechanism.  It's
>> best not to depend on this in the future.   Modern Intel implementations
>> basically quiesce the whole system to make the operation atomic.    It's
>> anything but a local operation and the whole system suffers.  There's some
>> discussion on the technique buried in the following :
>>
>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>
>>  Regards, -Dave
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/404713df/attachment-0001.html>

From pyiapa at gmail.com  Fri Dec 14 07:19:17 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Fri, 14 Dec 2012 12:19:17 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
Message-ID: <CAPUTfNnMXXKKBgU9uHxxddg7Gv3Nv6mLLXVr+JZubmwTAMz8Og@mail.gmail.com>

Sorry, forgot to mention that the access to each of those 16-bit subwords
is guaranteed to be atomic if any of those are accessed concurrently by
different processors because the entire cache line is flushed so it
invalidates the entire 64-bit word. I was trying to avoid volatiles since
that would force every access to each subword to go through memory. But
taking advantage of that alignment property will only be costly if there is
a concurrent access by multiple processors on those subwords. But again If
I understand correctly there is no 100% guaranty for that in Java?

- Paris

On Fri, Dec 14, 2012 at 11:53 AM, Paris Yiapanis <pyiapa at gmail.com> wrote:

> Not exactly. According to intel x86 specification if you place four 16-bit
> subwords inside a 64-bit aligned word, then an access to each of those
> 16-bit subwords is guaranteed to be atomic. I think (maybe I am wrong) the
> reason is because such an access to one of those subwords will cause the
> entire cache line to flush.
> So how about in Java? If you put together four consecutive Short
> primitives (that are 16-bits each) will the same scenario occur? Not
> volatile shorts, just plain shorts.
> But if I understand correct from the discussion, this is guaranteed for
> C/C++ that you can specify explicitly the alignment but it is not
> guaranteed for Java.
>
> - Paris
>
>
> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  What are you trying to do?
>>
>> Declare the 16-bit quantities volatile, and you get atomic access to them
>> "without a CAS". (Not even sure how would CAS help otherwise)
>>
>> Alex
>>
>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>
>>
>> Dave,
>>
>> When you say 'the placement mechanism is supposed to ensure natural
>> alignment' I presume that it is not enforced.
>> Basically, I read a paper where they align four 16-bit subwords inside a
>> 64-bit aligned word in order to guarantee atomic access (to each of those
>> 16-bit subwords) without using CAS instructions (on an intel x86 machine).
>> However that was implemented in C++.
>> I was wondering if I could do the same with Java?
>>
>>
>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com> wrote:
>>
>>>
>>> On x86, hotspot ensures all objects start on a 8-byte aligned address.
>>>  Furthermore the placement mechanism is supposed to ensure "natural"
>>> alignment for fields.   An 8-byte volatile long should have an offset
>>> that's a multiple of 8, and given the object alignment it should then
>>> should have an virtual address that's a multiple of 8.   Natural alignment
>>> is best for performance and also ensures you don't have situations where a
>>> volatile long might split a cache line.   Loads that split a line won't
>>> necessarily be atomic.   Atomic instructions that split a line _will be
>>> atomic, but that's handled through a rather bizarre legacy mechanism.  It's
>>> best not to depend on this in the future.   Modern Intel implementations
>>> basically quiesce the whole system to make the operation atomic.    It's
>>> anything but a local operation and the whole system suffers.  There's some
>>> discussion on the technique buried in the following :
>>>
>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>
>>>  Regards, -Dave
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/098af068/attachment.html>

From oleksandr.otenko at oracle.com  Fri Dec 14 07:49:55 2012
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Fri, 14 Dec 2012 12:49:55 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
Message-ID: <50CB2073.2080203@oracle.com>

There is no difference between volatile access and non-volatile access, 
if you look at just loads / stores on x86. The difference is in 
additional serialization between last volatile store and next volatile 
load. Without this the non-volatile stores, if they are done, will 
appear atomically, too; there's just no guarantee on ordering with 
respect to other loads (stores are still ordered).

Alex

On 14/12/2012 11:53, Paris Yiapanis wrote:
> Not exactly. According to intel x86 specification if you place four 
> 16-bit subwords inside a 64-bit aligned word, then an access to each 
> of those 16-bit subwords is guaranteed to be atomic. I think (maybe I 
> am wrong) the reason is because such an access to one of those 
> subwords will cause the entire cache line to flush.
> So how about in Java? If you put together four consecutive Short 
> primitives (that are 16-bits each) will the same scenario occur? Not 
> volatile shorts, just plain shorts.
> But if I understand correct from the discussion, this is guaranteed 
> for C/C++ that you can specify explicitly the alignment but it is not 
> guaranteed for Java.
>
> - Paris
>
> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     What are you trying to do?
>
>     Declare the 16-bit quantities volatile, and you get atomic access
>     to them "without a CAS". (Not even sure how would CAS help otherwise)
>
>     Alex
>
>     On 13/12/2012 17:00, Paris Yiapanis wrote:
>>
>>     Dave,
>>
>>     When you say 'the placement mechanism is supposed to ensure
>>     natural alignment' I presume that it is not enforced.
>>     Basically, I read a paper where they align four 16-bit subwords
>>     inside a 64-bit aligned word in order to guarantee atomic access
>>     (to each of those 16-bit subwords) without using CAS instructions
>>     (on an intel x86 machine). However that was implemented in C++.
>>     I was wondering if I could do the same with Java?
>>
>>
>>     On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com
>>     <mailto:david.dice at gmail.com>> wrote:
>>
>>
>>         On x86, hotspot ensures all objects start on a 8-byte aligned
>>         address.    Furthermore the placement mechanism is supposed
>>         to ensure "natural" alignment for fields.   An 8-byte
>>         volatile long should have an offset that's a multiple of 8,
>>         and given the object alignment it should then should have an
>>         virtual address that's a multiple of 8.   Natural alignment
>>         is best for performance and also ensures you don't have
>>         situations where a volatile long might split a cache line.  
>>         Loads that split a line won't necessarily be atomic.   Atomic
>>         instructions that split a line _will be atomic, but that's
>>         handled through a rather bizarre legacy mechanism.  It's best
>>         not to depend on this in the future.   Modern Intel
>>         implementations basically quiesce the whole system to make
>>         the operation atomic.    It's anything but a local operation
>>         and the whole system suffers.  There's some discussion on the
>>         technique buried in the following :
>>
>>         https://blogs.oracle.com/dave/entry/qpi_quiescence
>>
>>         Regards, -Dave
>>
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/c279a249/attachment.html>

From pyiapa at gmail.com  Fri Dec 14 08:30:25 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Fri, 14 Dec 2012 13:30:25 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <50CB2073.2080203@oracle.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<50CB2073.2080203@oracle.com>
Message-ID: <CAPUTfNkUU-T7fn5gMwKYSs48O56=DosTP4D1sQ14b2j4+Xrx4A@mail.gmail.com>

I agree with you in terms of ordering. Also, as you say, individual
loads/stores in any of these 16-bit (non-volatile) subwords is atomic as of
itself. But maybe I wasn't very clear on what I need.

If four of these 16-bit subwords are aligned as part of the 64-bit word, a
concurrent access to one of those 16-bit subwords will atomically update
the full 64-bit word (that means all four 16-bit subwords together). Thus,
no two threads can modify concurrently two subwords of the same word (given
that they are aligned as the intel spec mentions). So, if I understand well
it acts like you atomically update the entire 64-bit word when you update
one of its aligned subwords. But again this is in C++, so it seems not
guaranteed for Java (again if I understand well).

- Paris

On Fri, Dec 14, 2012 at 12:49 PM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  There is no difference between volatile access and non-volatile access,
> if you look at just loads / stores on x86. The difference is in additional
> serialization between last volatile store and next volatile load. Without
> this the non-volatile stores, if they are done, will appear atomically,
> too; there's just no guarantee on ordering with respect to other loads
> (stores are still ordered).
>
> Alex
>
> On 14/12/2012 11:53, Paris Yiapanis wrote:
>
> Not exactly. According to intel x86 specification if you place four 16-bit
> subwords inside a 64-bit aligned word, then an access to each of those
> 16-bit subwords is guaranteed to be atomic. I think (maybe I am wrong) the
> reason is because such an access to one of those subwords will cause the
> entire cache line to flush.
> So how about in Java? If you put together four consecutive Short
> primitives (that are 16-bits each) will the same scenario occur? Not
> volatile shorts, just plain shorts.
> But if I understand correct from the discussion, this is guaranteed for
> C/C++ that you can specify explicitly the alignment but it is not
> guaranteed for Java.
>
> - Paris
>
> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  What are you trying to do?
>>
>> Declare the 16-bit quantities volatile, and you get atomic access to them
>> "without a CAS". (Not even sure how would CAS help otherwise)
>>
>> Alex
>>
>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>
>>
>> Dave,
>>
>> When you say 'the placement mechanism is supposed to ensure natural
>> alignment' I presume that it is not enforced.
>> Basically, I read a paper where they align four 16-bit subwords inside a
>> 64-bit aligned word in order to guarantee atomic access (to each of those
>> 16-bit subwords) without using CAS instructions (on an intel x86 machine).
>> However that was implemented in C++.
>> I was wondering if I could do the same with Java?
>>
>>
>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com> wrote:
>>
>>>
>>> On x86, hotspot ensures all objects start on a 8-byte aligned address.
>>>  Furthermore the placement mechanism is supposed to ensure "natural"
>>> alignment for fields.   An 8-byte volatile long should have an offset
>>> that's a multiple of 8, and given the object alignment it should then
>>> should have an virtual address that's a multiple of 8.   Natural alignment
>>> is best for performance and also ensures you don't have situations where a
>>> volatile long might split a cache line.   Loads that split a line won't
>>> necessarily be atomic.   Atomic instructions that split a line _will be
>>> atomic, but that's handled through a rather bizarre legacy mechanism.  It's
>>> best not to depend on this in the future.   Modern Intel implementations
>>> basically quiesce the whole system to make the operation atomic.    It's
>>> anything but a local operation and the whole system suffers.  There's some
>>> discussion on the technique buried in the following :
>>>
>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>
>>>  Regards, -Dave
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/e77c7bd0/attachment-0001.html>

From vitalyd at gmail.com  Fri Dec 14 08:40:15 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 14 Dec 2012 08:40:15 -0500
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNnMXXKKBgU9uHxxddg7Gv3Nv6mLLXVr+JZubmwTAMz8Og@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<CAPUTfNnMXXKKBgU9uHxxddg7Gv3Nv6mLLXVr+JZubmwTAMz8Og@mail.gmail.com>
Message-ID: <CAHjP37GjF-S_-_ytv3=Spu6F+_qs7CixFhvKHOz5Czrwn04J9A@mail.gmail.com>

Yes, the coherence system invalidates the entire cacheline; the wrinkle
here is the store buffer - it doesn't participate in coherence; once the
buffer is drained to L1, the coherence system ensures, well, coherence :).
So, you might write to a subword of a word.  The coherence system knows the
state of the cacheline; if the line is missing from L1, it issues a load to
bring it in.  In the meantime, the CPU stores the line into the store
buffer (this is to prevent a stall waiting for memory to respond).  Well,
at this point, no other CPU can "see" the store you just dropped into
buffer - it's only after the line drains from the store buffer does
coherence request exclusive access to the line and issues invalidate
requests to other CPU caches that have it.  The problem, of course, is that
there's no order guarantee if multiple CPUs are doing this concurrently.

As for volatile loads, they don't necessarily go through memory; if the
line is already (best case) in L1 then access time is just a few cycles
anyway.  For CAS or volatile stores, they also don't have to be much worse
than normal writes - the overwhelming cost in either case is going to be
all the coherence traffic between the CPUs to invalidate the line and grab
exclusive access.  I think Dave Dice had a blog post a while ago comparing
CAS vs normal writes when many threads are doing this concurrently -
performance was abysmal in both cases (and pretty close in relative terms).

This is a very good paper on how caches interact:
https://www.google.com/url?sa=t&source=web&cd=2&ved=0CDUQFjAB&url=http%3A%2F%2Fwww.rdrop.com%2Fusers%2Fpaulmck%2Fscalability%2Fpaper%2Fwhymb.2009.04.05a.pdf&ei=fSvLULjMIIW40AGql4GIBw&usg=AFQjCNG4cHx3vsWSJO7yhbi2xghwsegpDA

Sent from my phone
On Dec 14, 2012 7:28 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

> Sorry, forgot to mention that the access to each of those 16-bit subwords
> is guaranteed to be atomic if any of those are accessed concurrently by
> different processors because the entire cache line is flushed so it
> invalidates the entire 64-bit word. I was trying to avoid volatiles since
> that would force every access to each subword to go through memory. But
> taking advantage of that alignment property will only be costly if there is
> a concurrent access by multiple processors on those subwords. But again If
> I understand correctly there is no 100% guaranty for that in Java?
>
> - Paris
>
> On Fri, Dec 14, 2012 at 11:53 AM, Paris Yiapanis <pyiapa at gmail.com> wrote:
>
>> Not exactly. According to intel x86 specification if you place four
>> 16-bit subwords inside a 64-bit aligned word, then an access to each of
>> those 16-bit subwords is guaranteed to be atomic. I think (maybe I am
>> wrong) the reason is because such an access to one of those subwords will
>> cause the entire cache line to flush.
>> So how about in Java? If you put together four consecutive Short
>> primitives (that are 16-bits each) will the same scenario occur? Not
>> volatile shorts, just plain shorts.
>> But if I understand correct from the discussion, this is guaranteed for
>> C/C++ that you can specify explicitly the alignment but it is not
>> guaranteed for Java.
>>
>> - Paris
>>
>>
>> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>>  What are you trying to do?
>>>
>>> Declare the 16-bit quantities volatile, and you get atomic access to
>>> them "without a CAS". (Not even sure how would CAS help otherwise)
>>>
>>> Alex
>>>
>>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>>
>>>
>>> Dave,
>>>
>>> When you say 'the placement mechanism is supposed to ensure natural
>>> alignment' I presume that it is not enforced.
>>> Basically, I read a paper where they align four 16-bit subwords inside a
>>> 64-bit aligned word in order to guarantee atomic access (to each of those
>>> 16-bit subwords) without using CAS instructions (on an intel x86 machine).
>>> However that was implemented in C++.
>>> I was wondering if I could do the same with Java?
>>>
>>>
>>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com>wrote:
>>>
>>>>
>>>> On x86, hotspot ensures all objects start on a 8-byte aligned address.
>>>>    Furthermore the placement mechanism is supposed to ensure "natural"
>>>> alignment for fields.   An 8-byte volatile long should have an offset
>>>> that's a multiple of 8, and given the object alignment it should then
>>>> should have an virtual address that's a multiple of 8.   Natural alignment
>>>> is best for performance and also ensures you don't have situations where a
>>>> volatile long might split a cache line.   Loads that split a line won't
>>>> necessarily be atomic.   Atomic instructions that split a line _will be
>>>> atomic, but that's handled through a rather bizarre legacy mechanism.  It's
>>>> best not to depend on this in the future.   Modern Intel implementations
>>>> basically quiesce the whole system to make the operation atomic.    It's
>>>> anything but a local operation and the whole system suffers.  There's some
>>>> discussion on the technique buried in the following :
>>>>
>>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>>
>>>>  Regards, -Dave
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/3c594b3d/attachment.html>

From pyiapa at gmail.com  Fri Dec 14 08:54:42 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Fri, 14 Dec 2012 13:54:42 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAHjP37GjF-S_-_ytv3=Spu6F+_qs7CixFhvKHOz5Czrwn04J9A@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<CAPUTfNnMXXKKBgU9uHxxddg7Gv3Nv6mLLXVr+JZubmwTAMz8Og@mail.gmail.com>
	<CAHjP37GjF-S_-_ytv3=Spu6F+_qs7CixFhvKHOz5Czrwn04J9A@mail.gmail.com>
Message-ID: <CAPUTfNnKFtx8jeeXWmKUrKheEQc2GP0Fpd3JRw=r8+fGXnH3+A@mail.gmail.com>

Good to know. Many Thanks.

Is this the blog post you mentioned?
https://blogs.oracle.com/dave/entry/cas_and_cache_trivia_invalidate

On Fri, Dec 14, 2012 at 1:40 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Yes, the coherence system invalidates the entire cacheline; the wrinkle
> here is the store buffer - it doesn't participate in coherence; once the
> buffer is drained to L1, the coherence system ensures, well, coherence :).
> So, you might write to a subword of a word.  The coherence system knows the
> state of the cacheline; if the line is missing from L1, it issues a load to
> bring it in.  In the meantime, the CPU stores the line into the store
> buffer (this is to prevent a stall waiting for memory to respond).  Well,
> at this point, no other CPU can "see" the store you just dropped into
> buffer - it's only after the line drains from the store buffer does
> coherence request exclusive access to the line and issues invalidate
> requests to other CPU caches that have it.  The problem, of course, is that
> there's no order guarantee if multiple CPUs are doing this concurrently.
>
> As for volatile loads, they don't necessarily go through memory; if the
> line is already (best case) in L1 then access time is just a few cycles
> anyway.  For CAS or volatile stores, they also don't have to be much worse
> than normal writes - the overwhelming cost in either case is going to be
> all the coherence traffic between the CPUs to invalidate the line and grab
> exclusive access.  I think Dave Dice had a blog post a while ago comparing
> CAS vs normal writes when many threads are doing this concurrently -
> performance was abysmal in both cases (and pretty close in relative terms).
>
> This is a very good paper on how caches interact:
> https://www.google.com/url?sa=t&source=web&cd=2&ved=0CDUQFjAB&url=http%3A%2F%2Fwww.rdrop.com%2Fusers%2Fpaulmck%2Fscalability%2Fpaper%2Fwhymb.2009.04.05a.pdf&ei=fSvLULjMIIW40AGql4GIBw&usg=AFQjCNG4cHx3vsWSJO7yhbi2xghwsegpDA
>
> Sent from my phone
> On Dec 14, 2012 7:28 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>
>> Sorry, forgot to mention that the access to each of those 16-bit subwords
>> is guaranteed to be atomic if any of those are accessed concurrently by
>> different processors because the entire cache line is flushed so it
>> invalidates the entire 64-bit word. I was trying to avoid volatiles since
>> that would force every access to each subword to go through memory. But
>> taking advantage of that alignment property will only be costly if there is
>> a concurrent access by multiple processors on those subwords. But again If
>> I understand correctly there is no 100% guaranty for that in Java?
>>
>> - Paris
>>
>> On Fri, Dec 14, 2012 at 11:53 AM, Paris Yiapanis <pyiapa at gmail.com>wrote:
>>
>>> Not exactly. According to intel x86 specification if you place four
>>> 16-bit subwords inside a 64-bit aligned word, then an access to each of
>>> those 16-bit subwords is guaranteed to be atomic. I think (maybe I am
>>> wrong) the reason is because such an access to one of those subwords will
>>> cause the entire cache line to flush.
>>> So how about in Java? If you put together four consecutive Short
>>> primitives (that are 16-bits each) will the same scenario occur? Not
>>> volatile shorts, just plain shorts.
>>> But if I understand correct from the discussion, this is guaranteed for
>>> C/C++ that you can specify explicitly the alignment but it is not
>>> guaranteed for Java.
>>>
>>> - Paris
>>>
>>>
>>> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
>>> oleksandr.otenko at oracle.com> wrote:
>>>
>>>>  What are you trying to do?
>>>>
>>>> Declare the 16-bit quantities volatile, and you get atomic access to
>>>> them "without a CAS". (Not even sure how would CAS help otherwise)
>>>>
>>>> Alex
>>>>
>>>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>>>
>>>>
>>>> Dave,
>>>>
>>>> When you say 'the placement mechanism is supposed to ensure natural
>>>> alignment' I presume that it is not enforced.
>>>> Basically, I read a paper where they align four 16-bit subwords inside
>>>> a 64-bit aligned word in order to guarantee atomic access (to each of those
>>>> 16-bit subwords) without using CAS instructions (on an intel x86 machine).
>>>> However that was implemented in C++.
>>>> I was wondering if I could do the same with Java?
>>>>
>>>>
>>>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com>wrote:
>>>>
>>>>>
>>>>> On x86, hotspot ensures all objects start on a 8-byte aligned address.
>>>>>    Furthermore the placement mechanism is supposed to ensure "natural"
>>>>> alignment for fields.   An 8-byte volatile long should have an offset
>>>>> that's a multiple of 8, and given the object alignment it should then
>>>>> should have an virtual address that's a multiple of 8.   Natural alignment
>>>>> is best for performance and also ensures you don't have situations where a
>>>>> volatile long might split a cache line.   Loads that split a line won't
>>>>> necessarily be atomic.   Atomic instructions that split a line _will be
>>>>> atomic, but that's handled through a rather bizarre legacy mechanism.  It's
>>>>> best not to depend on this in the future.   Modern Intel implementations
>>>>> basically quiesce the whole system to make the operation atomic.    It's
>>>>> anything but a local operation and the whole system suffers.  There's some
>>>>> discussion on the technique buried in the following :
>>>>>
>>>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>>>
>>>>>  Regards, -Dave
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/4e6449cd/attachment-0001.html>

From vitalyd at gmail.com  Fri Dec 14 09:12:06 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 14 Dec 2012 09:12:06 -0500
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNnKFtx8jeeXWmKUrKheEQc2GP0Fpd3JRw=r8+fGXnH3+A@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<CAPUTfNnMXXKKBgU9uHxxddg7Gv3Nv6mLLXVr+JZubmwTAMz8Og@mail.gmail.com>
	<CAHjP37GjF-S_-_ytv3=Spu6F+_qs7CixFhvKHOz5Czrwn04J9A@mail.gmail.com>
	<CAPUTfNnKFtx8jeeXWmKUrKheEQc2GP0Fpd3JRw=r8+fGXnH3+A@mail.gmail.com>
Message-ID: <CAHjP37HejSP6pv=iff37q7aTD1jz_RugwGWMdNKT3Sv35KZOPw@mail.gmail.com>

I think it's the one he links to in this post, but the link 404s so can't
make sure.  Dave is on this list so maybe he'll see this and add his
wisdom/insight.

By the way, I see I wrote "exclusive" access in a few places in previous
email in a generic sense - in MESI terms, I actually mean Modified state.
I realize this can be confusing so just wanted to clarify.

Sent from my phone
On Dec 14, 2012 8:54 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

> Good to know. Many Thanks.
>
> Is this the blog post you mentioned?
> https://blogs.oracle.com/dave/entry/cas_and_cache_trivia_invalidate
>
> On Fri, Dec 14, 2012 at 1:40 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> Yes, the coherence system invalidates the entire cacheline; the wrinkle
>> here is the store buffer - it doesn't participate in coherence; once the
>> buffer is drained to L1, the coherence system ensures, well, coherence :).
>> So, you might write to a subword of a word.  The coherence system knows the
>> state of the cacheline; if the line is missing from L1, it issues a load to
>> bring it in.  In the meantime, the CPU stores the line into the store
>> buffer (this is to prevent a stall waiting for memory to respond).  Well,
>> at this point, no other CPU can "see" the store you just dropped into
>> buffer - it's only after the line drains from the store buffer does
>> coherence request exclusive access to the line and issues invalidate
>> requests to other CPU caches that have it.  The problem, of course, is that
>> there's no order guarantee if multiple CPUs are doing this concurrently.
>>
>> As for volatile loads, they don't necessarily go through memory; if the
>> line is already (best case) in L1 then access time is just a few cycles
>> anyway.  For CAS or volatile stores, they also don't have to be much worse
>> than normal writes - the overwhelming cost in either case is going to be
>> all the coherence traffic between the CPUs to invalidate the line and grab
>> exclusive access.  I think Dave Dice had a blog post a while ago comparing
>> CAS vs normal writes when many threads are doing this concurrently -
>> performance was abysmal in both cases (and pretty close in relative terms).
>>
>> This is a very good paper on how caches interact:
>> https://www.google.com/url?sa=t&source=web&cd=2&ved=0CDUQFjAB&url=http%3A%2F%2Fwww.rdrop.com%2Fusers%2Fpaulmck%2Fscalability%2Fpaper%2Fwhymb.2009.04.05a.pdf&ei=fSvLULjMIIW40AGql4GIBw&usg=AFQjCNG4cHx3vsWSJO7yhbi2xghwsegpDA
>>
>> Sent from my phone
>> On Dec 14, 2012 7:28 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>>
>>> Sorry, forgot to mention that the access to each of those 16-bit
>>> subwords is guaranteed to be atomic if any of those are accessed
>>> concurrently by different processors because the entire cache line is
>>> flushed so it invalidates the entire 64-bit word. I was trying to avoid
>>> volatiles since that would force every access to each subword to go through
>>> memory. But taking advantage of that alignment property will only be costly
>>> if there is a concurrent access by multiple processors on those subwords.
>>> But again If I understand correctly there is no 100% guaranty for that in
>>> Java?
>>>
>>> - Paris
>>>
>>> On Fri, Dec 14, 2012 at 11:53 AM, Paris Yiapanis <pyiapa at gmail.com>wrote:
>>>
>>>> Not exactly. According to intel x86 specification if you place four
>>>> 16-bit subwords inside a 64-bit aligned word, then an access to each of
>>>> those 16-bit subwords is guaranteed to be atomic. I think (maybe I am
>>>> wrong) the reason is because such an access to one of those subwords will
>>>> cause the entire cache line to flush.
>>>> So how about in Java? If you put together four consecutive Short
>>>> primitives (that are 16-bits each) will the same scenario occur? Not
>>>> volatile shorts, just plain shorts.
>>>> But if I understand correct from the discussion, this is guaranteed for
>>>> C/C++ that you can specify explicitly the alignment but it is not
>>>> guaranteed for Java.
>>>>
>>>> - Paris
>>>>
>>>>
>>>> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
>>>> oleksandr.otenko at oracle.com> wrote:
>>>>
>>>>>  What are you trying to do?
>>>>>
>>>>> Declare the 16-bit quantities volatile, and you get atomic access to
>>>>> them "without a CAS". (Not even sure how would CAS help otherwise)
>>>>>
>>>>> Alex
>>>>>
>>>>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>>>>
>>>>>
>>>>> Dave,
>>>>>
>>>>> When you say 'the placement mechanism is supposed to ensure natural
>>>>> alignment' I presume that it is not enforced.
>>>>> Basically, I read a paper where they align four 16-bit subwords inside
>>>>> a 64-bit aligned word in order to guarantee atomic access (to each of those
>>>>> 16-bit subwords) without using CAS instructions (on an intel x86 machine).
>>>>> However that was implemented in C++.
>>>>> I was wondering if I could do the same with Java?
>>>>>
>>>>>
>>>>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com>wrote:
>>>>>
>>>>>>
>>>>>> On x86, hotspot ensures all objects start on a 8-byte aligned
>>>>>> address.    Furthermore the placement mechanism is supposed to ensure
>>>>>> "natural" alignment for fields.   An 8-byte volatile long should have an
>>>>>> offset that's a multiple of 8, and given the object alignment it should
>>>>>> then should have an virtual address that's a multiple of 8.   Natural
>>>>>> alignment is best for performance and also ensures you don't have
>>>>>> situations where a volatile long might split a cache line.   Loads that
>>>>>> split a line won't necessarily be atomic.   Atomic instructions that split
>>>>>> a line _will be atomic, but that's handled through a rather bizarre legacy
>>>>>> mechanism.  It's best not to depend on this in the future.   Modern Intel
>>>>>> implementations basically quiesce the whole system to make the operation
>>>>>> atomic.    It's anything but a local operation and the whole system
>>>>>> suffers.  There's some discussion on the technique buried in the following :
>>>>>>
>>>>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>>>>
>>>>>>  Regards, -Dave
>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/65d8e528/attachment.html>

From pyiapa at gmail.com  Fri Dec 14 09:17:55 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Fri, 14 Dec 2012 14:17:55 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAHjP37HejSP6pv=iff37q7aTD1jz_RugwGWMdNKT3Sv35KZOPw@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<CAPUTfNnMXXKKBgU9uHxxddg7Gv3Nv6mLLXVr+JZubmwTAMz8Og@mail.gmail.com>
	<CAHjP37GjF-S_-_ytv3=Spu6F+_qs7CixFhvKHOz5Czrwn04J9A@mail.gmail.com>
	<CAPUTfNnKFtx8jeeXWmKUrKheEQc2GP0Fpd3JRw=r8+fGXnH3+A@mail.gmail.com>
	<CAHjP37HejSP6pv=iff37q7aTD1jz_RugwGWMdNKT3Sv35KZOPw@mail.gmail.com>
Message-ID: <CAPUTfNnn8V869+4bjfgT4kkyK8hk2kzKLo7tOB07HaBU91FH=w@mail.gmail.com>

Thanks Vitaly.

I think the link that 404s is moved here:
https://blogs.oracle.com/dave/entry/biased_locking_in_hotspot

- Paris

On Fri, Dec 14, 2012 at 2:12 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> I think it's the one he links to in this post, but the link 404s so can't
> make sure.  Dave is on this list so maybe he'll see this and add his
> wisdom/insight.
>
> By the way, I see I wrote "exclusive" access in a few places in previous
> email in a generic sense - in MESI terms, I actually mean Modified state.
> I realize this can be confusing so just wanted to clarify.
>
> Sent from my phone
> On Dec 14, 2012 8:54 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>
>> Good to know. Many Thanks.
>>
>> Is this the blog post you mentioned?
>> https://blogs.oracle.com/dave/entry/cas_and_cache_trivia_invalidate
>>
>> On Fri, Dec 14, 2012 at 1:40 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>
>>> Yes, the coherence system invalidates the entire cacheline; the wrinkle
>>> here is the store buffer - it doesn't participate in coherence; once the
>>> buffer is drained to L1, the coherence system ensures, well, coherence :).
>>> So, you might write to a subword of a word.  The coherence system knows the
>>> state of the cacheline; if the line is missing from L1, it issues a load to
>>> bring it in.  In the meantime, the CPU stores the line into the store
>>> buffer (this is to prevent a stall waiting for memory to respond).  Well,
>>> at this point, no other CPU can "see" the store you just dropped into
>>> buffer - it's only after the line drains from the store buffer does
>>> coherence request exclusive access to the line and issues invalidate
>>> requests to other CPU caches that have it.  The problem, of course, is that
>>> there's no order guarantee if multiple CPUs are doing this concurrently.
>>>
>>> As for volatile loads, they don't necessarily go through memory; if the
>>> line is already (best case) in L1 then access time is just a few cycles
>>> anyway.  For CAS or volatile stores, they also don't have to be much worse
>>> than normal writes - the overwhelming cost in either case is going to be
>>> all the coherence traffic between the CPUs to invalidate the line and grab
>>> exclusive access.  I think Dave Dice had a blog post a while ago comparing
>>> CAS vs normal writes when many threads are doing this concurrently -
>>> performance was abysmal in both cases (and pretty close in relative terms).
>>>
>>> This is a very good paper on how caches interact:
>>> https://www.google.com/url?sa=t&source=web&cd=2&ved=0CDUQFjAB&url=http%3A%2F%2Fwww.rdrop.com%2Fusers%2Fpaulmck%2Fscalability%2Fpaper%2Fwhymb.2009.04.05a.pdf&ei=fSvLULjMIIW40AGql4GIBw&usg=AFQjCNG4cHx3vsWSJO7yhbi2xghwsegpDA
>>>
>>> Sent from my phone
>>> On Dec 14, 2012 7:28 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>>>
>>>> Sorry, forgot to mention that the access to each of those 16-bit
>>>> subwords is guaranteed to be atomic if any of those are accessed
>>>> concurrently by different processors because the entire cache line is
>>>> flushed so it invalidates the entire 64-bit word. I was trying to avoid
>>>> volatiles since that would force every access to each subword to go through
>>>> memory. But taking advantage of that alignment property will only be costly
>>>> if there is a concurrent access by multiple processors on those subwords.
>>>> But again If I understand correctly there is no 100% guaranty for that in
>>>> Java?
>>>>
>>>> - Paris
>>>>
>>>> On Fri, Dec 14, 2012 at 11:53 AM, Paris Yiapanis <pyiapa at gmail.com>wrote:
>>>>
>>>>> Not exactly. According to intel x86 specification if you place four
>>>>> 16-bit subwords inside a 64-bit aligned word, then an access to each of
>>>>> those 16-bit subwords is guaranteed to be atomic. I think (maybe I am
>>>>> wrong) the reason is because such an access to one of those subwords will
>>>>> cause the entire cache line to flush.
>>>>> So how about in Java? If you put together four consecutive Short
>>>>> primitives (that are 16-bits each) will the same scenario occur? Not
>>>>> volatile shorts, just plain shorts.
>>>>> But if I understand correct from the discussion, this is guaranteed
>>>>> for C/C++ that you can specify explicitly the alignment but it is not
>>>>> guaranteed for Java.
>>>>>
>>>>> - Paris
>>>>>
>>>>>
>>>>> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>
>>>>>>  What are you trying to do?
>>>>>>
>>>>>> Declare the 16-bit quantities volatile, and you get atomic access to
>>>>>> them "without a CAS". (Not even sure how would CAS help otherwise)
>>>>>>
>>>>>> Alex
>>>>>>
>>>>>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>>>>>
>>>>>>
>>>>>> Dave,
>>>>>>
>>>>>> When you say 'the placement mechanism is supposed to ensure natural
>>>>>> alignment' I presume that it is not enforced.
>>>>>> Basically, I read a paper where they align four 16-bit subwords
>>>>>> inside a 64-bit aligned word in order to guarantee atomic access (to each
>>>>>> of those 16-bit subwords) without using CAS instructions (on an intel x86
>>>>>> machine). However that was implemented in C++.
>>>>>> I was wondering if I could do the same with Java?
>>>>>>
>>>>>>
>>>>>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com>wrote:
>>>>>>
>>>>>>>
>>>>>>> On x86, hotspot ensures all objects start on a 8-byte aligned
>>>>>>> address.    Furthermore the placement mechanism is supposed to ensure
>>>>>>> "natural" alignment for fields.   An 8-byte volatile long should have an
>>>>>>> offset that's a multiple of 8, and given the object alignment it should
>>>>>>> then should have an virtual address that's a multiple of 8.   Natural
>>>>>>> alignment is best for performance and also ensures you don't have
>>>>>>> situations where a volatile long might split a cache line.   Loads that
>>>>>>> split a line won't necessarily be atomic.   Atomic instructions that split
>>>>>>> a line _will be atomic, but that's handled through a rather bizarre legacy
>>>>>>> mechanism.  It's best not to depend on this in the future.   Modern Intel
>>>>>>> implementations basically quiesce the whole system to make the operation
>>>>>>> atomic.    It's anything but a local operation and the whole system
>>>>>>> suffers.  There's some discussion on the technique buried in the following :
>>>>>>>
>>>>>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>>>>>
>>>>>>>  Regards, -Dave
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/7c5db3db/attachment-0001.html>

From vitalyd at gmail.com  Fri Dec 14 09:23:09 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 14 Dec 2012 09:23:09 -0500
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNnn8V869+4bjfgT4kkyK8hk2kzKLo7tOB07HaBU91FH=w@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<CAPUTfNnMXXKKBgU9uHxxddg7Gv3Nv6mLLXVr+JZubmwTAMz8Og@mail.gmail.com>
	<CAHjP37GjF-S_-_ytv3=Spu6F+_qs7CixFhvKHOz5Czrwn04J9A@mail.gmail.com>
	<CAPUTfNnKFtx8jeeXWmKUrKheEQc2GP0Fpd3JRw=r8+fGXnH3+A@mail.gmail.com>
	<CAHjP37HejSP6pv=iff37q7aTD1jz_RugwGWMdNKT3Sv35KZOPw@mail.gmail.com>
	<CAPUTfNnn8V869+4bjfgT4kkyK8hk2kzKLo7tOB07HaBU91FH=w@mail.gmail.com>
Message-ID: <CAHjP37GUPvvN3ZMO=jkVhqtn4d00bVHmUYkczMaW9vuhN4ZnbQ@mail.gmail.com>

Yup that's the one - thanks for digging it up.

Sent from my phone
On Dec 14, 2012 9:17 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

> Thanks Vitaly.
>
> I think the link that 404s is moved here:
> https://blogs.oracle.com/dave/entry/biased_locking_in_hotspot
>
> - Paris
>
> On Fri, Dec 14, 2012 at 2:12 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> I think it's the one he links to in this post, but the link 404s so can't
>> make sure.  Dave is on this list so maybe he'll see this and add his
>> wisdom/insight.
>>
>> By the way, I see I wrote "exclusive" access in a few places in previous
>> email in a generic sense - in MESI terms, I actually mean Modified state.
>> I realize this can be confusing so just wanted to clarify.
>>
>> Sent from my phone
>> On Dec 14, 2012 8:54 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>>
>>> Good to know. Many Thanks.
>>>
>>> Is this the blog post you mentioned?
>>> https://blogs.oracle.com/dave/entry/cas_and_cache_trivia_invalidate
>>>
>>> On Fri, Dec 14, 2012 at 1:40 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>
>>>> Yes, the coherence system invalidates the entire cacheline; the wrinkle
>>>> here is the store buffer - it doesn't participate in coherence; once the
>>>> buffer is drained to L1, the coherence system ensures, well, coherence :).
>>>> So, you might write to a subword of a word.  The coherence system knows the
>>>> state of the cacheline; if the line is missing from L1, it issues a load to
>>>> bring it in.  In the meantime, the CPU stores the line into the store
>>>> buffer (this is to prevent a stall waiting for memory to respond).  Well,
>>>> at this point, no other CPU can "see" the store you just dropped into
>>>> buffer - it's only after the line drains from the store buffer does
>>>> coherence request exclusive access to the line and issues invalidate
>>>> requests to other CPU caches that have it.  The problem, of course, is that
>>>> there's no order guarantee if multiple CPUs are doing this concurrently.
>>>>
>>>> As for volatile loads, they don't necessarily go through memory; if the
>>>> line is already (best case) in L1 then access time is just a few cycles
>>>> anyway.  For CAS or volatile stores, they also don't have to be much worse
>>>> than normal writes - the overwhelming cost in either case is going to be
>>>> all the coherence traffic between the CPUs to invalidate the line and grab
>>>> exclusive access.  I think Dave Dice had a blog post a while ago comparing
>>>> CAS vs normal writes when many threads are doing this concurrently -
>>>> performance was abysmal in both cases (and pretty close in relative terms).
>>>>
>>>> This is a very good paper on how caches interact:
>>>> https://www.google.com/url?sa=t&source=web&cd=2&ved=0CDUQFjAB&url=http%3A%2F%2Fwww.rdrop.com%2Fusers%2Fpaulmck%2Fscalability%2Fpaper%2Fwhymb.2009.04.05a.pdf&ei=fSvLULjMIIW40AGql4GIBw&usg=AFQjCNG4cHx3vsWSJO7yhbi2xghwsegpDA
>>>>
>>>> Sent from my phone
>>>> On Dec 14, 2012 7:28 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>>>>
>>>>> Sorry, forgot to mention that the access to each of those 16-bit
>>>>> subwords is guaranteed to be atomic if any of those are accessed
>>>>> concurrently by different processors because the entire cache line is
>>>>> flushed so it invalidates the entire 64-bit word. I was trying to avoid
>>>>> volatiles since that would force every access to each subword to go through
>>>>> memory. But taking advantage of that alignment property will only be costly
>>>>> if there is a concurrent access by multiple processors on those subwords.
>>>>> But again If I understand correctly there is no 100% guaranty for that in
>>>>> Java?
>>>>>
>>>>> - Paris
>>>>>
>>>>> On Fri, Dec 14, 2012 at 11:53 AM, Paris Yiapanis <pyiapa at gmail.com>wrote:
>>>>>
>>>>>> Not exactly. According to intel x86 specification if you place four
>>>>>> 16-bit subwords inside a 64-bit aligned word, then an access to each of
>>>>>> those 16-bit subwords is guaranteed to be atomic. I think (maybe I am
>>>>>> wrong) the reason is because such an access to one of those subwords will
>>>>>> cause the entire cache line to flush.
>>>>>> So how about in Java? If you put together four consecutive Short
>>>>>> primitives (that are 16-bits each) will the same scenario occur? Not
>>>>>> volatile shorts, just plain shorts.
>>>>>> But if I understand correct from the discussion, this is guaranteed
>>>>>> for C/C++ that you can specify explicitly the alignment but it is not
>>>>>> guaranteed for Java.
>>>>>>
>>>>>> - Paris
>>>>>>
>>>>>>
>>>>>> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>
>>>>>>>  What are you trying to do?
>>>>>>>
>>>>>>> Declare the 16-bit quantities volatile, and you get atomic access to
>>>>>>> them "without a CAS". (Not even sure how would CAS help otherwise)
>>>>>>>
>>>>>>> Alex
>>>>>>>
>>>>>>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>>>>>>
>>>>>>>
>>>>>>> Dave,
>>>>>>>
>>>>>>> When you say 'the placement mechanism is supposed to ensure natural
>>>>>>> alignment' I presume that it is not enforced.
>>>>>>> Basically, I read a paper where they align four 16-bit subwords
>>>>>>> inside a 64-bit aligned word in order to guarantee atomic access (to each
>>>>>>> of those 16-bit subwords) without using CAS instructions (on an intel x86
>>>>>>> machine). However that was implemented in C++.
>>>>>>> I was wondering if I could do the same with Java?
>>>>>>>
>>>>>>>
>>>>>>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com>wrote:
>>>>>>>
>>>>>>>>
>>>>>>>> On x86, hotspot ensures all objects start on a 8-byte aligned
>>>>>>>> address.    Furthermore the placement mechanism is supposed to ensure
>>>>>>>> "natural" alignment for fields.   An 8-byte volatile long should have an
>>>>>>>> offset that's a multiple of 8, and given the object alignment it should
>>>>>>>> then should have an virtual address that's a multiple of 8.   Natural
>>>>>>>> alignment is best for performance and also ensures you don't have
>>>>>>>> situations where a volatile long might split a cache line.   Loads that
>>>>>>>> split a line won't necessarily be atomic.   Atomic instructions that split
>>>>>>>> a line _will be atomic, but that's handled through a rather bizarre legacy
>>>>>>>> mechanism.  It's best not to depend on this in the future.   Modern Intel
>>>>>>>> implementations basically quiesce the whole system to make the operation
>>>>>>>> atomic.    It's anything but a local operation and the whole system
>>>>>>>> suffers.  There's some discussion on the technique buried in the following :
>>>>>>>>
>>>>>>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>>>>>>
>>>>>>>>  Regards, -Dave
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/908815c6/attachment.html>

From vitalyd at gmail.com  Fri Dec 14 09:29:02 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 14 Dec 2012 09:29:02 -0500
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNnn8V869+4bjfgT4kkyK8hk2kzKLo7tOB07HaBU91FH=w@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<CAPUTfNnMXXKKBgU9uHxxddg7Gv3Nv6mLLXVr+JZubmwTAMz8Og@mail.gmail.com>
	<CAHjP37GjF-S_-_ytv3=Spu6F+_qs7CixFhvKHOz5Czrwn04J9A@mail.gmail.com>
	<CAPUTfNnKFtx8jeeXWmKUrKheEQc2GP0Fpd3JRw=r8+fGXnH3+A@mail.gmail.com>
	<CAHjP37HejSP6pv=iff37q7aTD1jz_RugwGWMdNKT3Sv35KZOPw@mail.gmail.com>
	<CAPUTfNnn8V869+4bjfgT4kkyK8hk2kzKLo7tOB07HaBU91FH=w@mail.gmail.com>
Message-ID: <CAHjP37H2+TrjswsJzQxSzP9dpDDQQ01M01gjKreYQyTYL47ruQ@mail.gmail.com>

One more good blog entry on CAS by the Azul guys:
http://www.azulsystems.com/blog/cliff/2011-11-16-a-short-conversation-on-biased-locking

I think the main takeaway from these things is multiple concurrent shared
memory writers doesn't scale - period; CAS or plain store, doesn't matter.
At least that's how I think of it.
Sent from my phone
On Dec 14, 2012 9:17 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:

> Thanks Vitaly.
>
> I think the link that 404s is moved here:
> https://blogs.oracle.com/dave/entry/biased_locking_in_hotspot
>
> - Paris
>
> On Fri, Dec 14, 2012 at 2:12 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> I think it's the one he links to in this post, but the link 404s so can't
>> make sure.  Dave is on this list so maybe he'll see this and add his
>> wisdom/insight.
>>
>> By the way, I see I wrote "exclusive" access in a few places in previous
>> email in a generic sense - in MESI terms, I actually mean Modified state.
>> I realize this can be confusing so just wanted to clarify.
>>
>> Sent from my phone
>> On Dec 14, 2012 8:54 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>>
>>> Good to know. Many Thanks.
>>>
>>> Is this the blog post you mentioned?
>>> https://blogs.oracle.com/dave/entry/cas_and_cache_trivia_invalidate
>>>
>>> On Fri, Dec 14, 2012 at 1:40 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>
>>>> Yes, the coherence system invalidates the entire cacheline; the wrinkle
>>>> here is the store buffer - it doesn't participate in coherence; once the
>>>> buffer is drained to L1, the coherence system ensures, well, coherence :).
>>>> So, you might write to a subword of a word.  The coherence system knows the
>>>> state of the cacheline; if the line is missing from L1, it issues a load to
>>>> bring it in.  In the meantime, the CPU stores the line into the store
>>>> buffer (this is to prevent a stall waiting for memory to respond).  Well,
>>>> at this point, no other CPU can "see" the store you just dropped into
>>>> buffer - it's only after the line drains from the store buffer does
>>>> coherence request exclusive access to the line and issues invalidate
>>>> requests to other CPU caches that have it.  The problem, of course, is that
>>>> there's no order guarantee if multiple CPUs are doing this concurrently.
>>>>
>>>> As for volatile loads, they don't necessarily go through memory; if the
>>>> line is already (best case) in L1 then access time is just a few cycles
>>>> anyway.  For CAS or volatile stores, they also don't have to be much worse
>>>> than normal writes - the overwhelming cost in either case is going to be
>>>> all the coherence traffic between the CPUs to invalidate the line and grab
>>>> exclusive access.  I think Dave Dice had a blog post a while ago comparing
>>>> CAS vs normal writes when many threads are doing this concurrently -
>>>> performance was abysmal in both cases (and pretty close in relative terms).
>>>>
>>>> This is a very good paper on how caches interact:
>>>> https://www.google.com/url?sa=t&source=web&cd=2&ved=0CDUQFjAB&url=http%3A%2F%2Fwww.rdrop.com%2Fusers%2Fpaulmck%2Fscalability%2Fpaper%2Fwhymb.2009.04.05a.pdf&ei=fSvLULjMIIW40AGql4GIBw&usg=AFQjCNG4cHx3vsWSJO7yhbi2xghwsegpDA
>>>>
>>>> Sent from my phone
>>>> On Dec 14, 2012 7:28 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>>>>
>>>>> Sorry, forgot to mention that the access to each of those 16-bit
>>>>> subwords is guaranteed to be atomic if any of those are accessed
>>>>> concurrently by different processors because the entire cache line is
>>>>> flushed so it invalidates the entire 64-bit word. I was trying to avoid
>>>>> volatiles since that would force every access to each subword to go through
>>>>> memory. But taking advantage of that alignment property will only be costly
>>>>> if there is a concurrent access by multiple processors on those subwords.
>>>>> But again If I understand correctly there is no 100% guaranty for that in
>>>>> Java?
>>>>>
>>>>> - Paris
>>>>>
>>>>> On Fri, Dec 14, 2012 at 11:53 AM, Paris Yiapanis <pyiapa at gmail.com>wrote:
>>>>>
>>>>>> Not exactly. According to intel x86 specification if you place four
>>>>>> 16-bit subwords inside a 64-bit aligned word, then an access to each of
>>>>>> those 16-bit subwords is guaranteed to be atomic. I think (maybe I am
>>>>>> wrong) the reason is because such an access to one of those subwords will
>>>>>> cause the entire cache line to flush.
>>>>>> So how about in Java? If you put together four consecutive Short
>>>>>> primitives (that are 16-bits each) will the same scenario occur? Not
>>>>>> volatile shorts, just plain shorts.
>>>>>> But if I understand correct from the discussion, this is guaranteed
>>>>>> for C/C++ that you can specify explicitly the alignment but it is not
>>>>>> guaranteed for Java.
>>>>>>
>>>>>> - Paris
>>>>>>
>>>>>>
>>>>>> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>
>>>>>>>  What are you trying to do?
>>>>>>>
>>>>>>> Declare the 16-bit quantities volatile, and you get atomic access to
>>>>>>> them "without a CAS". (Not even sure how would CAS help otherwise)
>>>>>>>
>>>>>>> Alex
>>>>>>>
>>>>>>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>>>>>>
>>>>>>>
>>>>>>> Dave,
>>>>>>>
>>>>>>> When you say 'the placement mechanism is supposed to ensure natural
>>>>>>> alignment' I presume that it is not enforced.
>>>>>>> Basically, I read a paper where they align four 16-bit subwords
>>>>>>> inside a 64-bit aligned word in order to guarantee atomic access (to each
>>>>>>> of those 16-bit subwords) without using CAS instructions (on an intel x86
>>>>>>> machine). However that was implemented in C++.
>>>>>>> I was wondering if I could do the same with Java?
>>>>>>>
>>>>>>>
>>>>>>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com>wrote:
>>>>>>>
>>>>>>>>
>>>>>>>> On x86, hotspot ensures all objects start on a 8-byte aligned
>>>>>>>> address.    Furthermore the placement mechanism is supposed to ensure
>>>>>>>> "natural" alignment for fields.   An 8-byte volatile long should have an
>>>>>>>> offset that's a multiple of 8, and given the object alignment it should
>>>>>>>> then should have an virtual address that's a multiple of 8.   Natural
>>>>>>>> alignment is best for performance and also ensures you don't have
>>>>>>>> situations where a volatile long might split a cache line.   Loads that
>>>>>>>> split a line won't necessarily be atomic.   Atomic instructions that split
>>>>>>>> a line _will be atomic, but that's handled through a rather bizarre legacy
>>>>>>>> mechanism.  It's best not to depend on this in the future.   Modern Intel
>>>>>>>> implementations basically quiesce the whole system to make the operation
>>>>>>>> atomic.    It's anything but a local operation and the whole system
>>>>>>>> suffers.  There's some discussion on the technique buried in the following :
>>>>>>>>
>>>>>>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>>>>>>
>>>>>>>>  Regards, -Dave
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/af98dbc9/attachment-0001.html>

From jbellis at gmail.com  Fri Dec 14 11:40:09 2012
From: jbellis at gmail.com (Jonathan Ellis)
Date: Fri, 14 Dec 2012 10:40:09 -0600
Subject: [concurrency-interest] LinkedTransferQueue performance
	substantially worse than LinkedBlockingQueue
Message-ID: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>

Hi all,

Cassandra database requests spend a lot of time (100s of microseconds)
moving from one stage (ThreadPoolExecutor) to another.  Each stage is
based on an unbounded LinkedBlockingQueue.  Typically each stage has
multiple producers and consumers, although for some (such as for
network traffic to a specific peer) there is only a single consumer.

I tried updating Cassandra to use LTQ [1] instead using the
jsr166y.jar for JDK 1.6 and saw performance go down by over 15% on
both reads and writes [2].  Am I correct that this is unexpected?  How
could I proceed with troubleshooting?

[1] https://github.com/jbellis/cassandra/branches/ltq
[2] https://issues.apache.org/jira/browse/CASSANDRA-4718

-- 
Jonathan Ellis
Project Chair, Apache Cassandra
co-founder, http://www.datastax.com
@spyced

From viktor.klang at gmail.com  Fri Dec 14 12:11:04 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 14 Dec 2012 18:11:04 +0100
Subject: [concurrency-interest] LinkedTransferQueue performance
 substantially worse than LinkedBlockingQueue
In-Reply-To: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
References: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
Message-ID: <CANPzfU-ASLDixDouuo+D=857fS4fZRDMMJuh0p3RwN_rZvJ84Q@mail.gmail.com>

What are your requirements?

Cheers,
?


On Fri, Dec 14, 2012 at 5:40 PM, Jonathan Ellis <jbellis at gmail.com> wrote:

> Hi all,
>
> Cassandra database requests spend a lot of time (100s of microseconds)
> moving from one stage (ThreadPoolExecutor) to another.  Each stage is
> based on an unbounded LinkedBlockingQueue.  Typically each stage has
> multiple producers and consumers, although for some (such as for
> network traffic to a specific peer) there is only a single consumer.
>
> I tried updating Cassandra to use LTQ [1] instead using the
> jsr166y.jar for JDK 1.6 and saw performance go down by over 15% on
> both reads and writes [2].  Am I correct that this is unexpected?  How
> could I proceed with troubleshooting?
>
> [1] https://github.com/jbellis/cassandra/branches/ltq
> [2] https://issues.apache.org/jira/browse/CASSANDRA-4718
>
> --
> Jonathan Ellis
> Project Chair, Apache Cassandra
> co-founder, http://www.datastax.com
> @spyced
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Viktor Klang

Director of Engineering
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/73159332/attachment.html>

From jbellis at gmail.com  Fri Dec 14 12:59:53 2012
From: jbellis at gmail.com (Jonathan Ellis)
Date: Fri, 14 Dec 2012 11:59:53 -0600
Subject: [concurrency-interest] LinkedTransferQueue performance
 substantially worse than LinkedBlockingQueue
In-Reply-To: <CANPzfU-ASLDixDouuo+D=857fS4fZRDMMJuh0p3RwN_rZvJ84Q@mail.gmail.com>
References: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
	<CANPzfU-ASLDixDouuo+D=857fS4fZRDMMJuh0p3RwN_rZvJ84Q@mail.gmail.com>
Message-ID: <CALdd-zgF6HxLVeX5CZJqQmC7ybKA8PpsX2=4nRRiGVtzz0ci6g@mail.gmail.com>

To start with I'd settle for "Not be worse than LBQ."

What do you mean more specifically?

On Fri, Dec 14, 2012 at 11:11 AM, ?iktor ?lang <viktor.klang at gmail.com> wrote:
> What are your requirements?
>
> Cheers,
> ?
>
>
> On Fri, Dec 14, 2012 at 5:40 PM, Jonathan Ellis <jbellis at gmail.com> wrote:
>>
>> Hi all,
>>
>> Cassandra database requests spend a lot of time (100s of microseconds)
>> moving from one stage (ThreadPoolExecutor) to another.  Each stage is
>> based on an unbounded LinkedBlockingQueue.  Typically each stage has
>> multiple producers and consumers, although for some (such as for
>> network traffic to a specific peer) there is only a single consumer.
>>
>> I tried updating Cassandra to use LTQ [1] instead using the
>> jsr166y.jar for JDK 1.6 and saw performance go down by over 15% on
>> both reads and writes [2].  Am I correct that this is unexpected?  How
>> could I proceed with troubleshooting?
>>
>> [1] https://github.com/jbellis/cassandra/branches/ltq
>> [2] https://issues.apache.org/jira/browse/CASSANDRA-4718
>>
>> --
>> Jonathan Ellis
>> Project Chair, Apache Cassandra
>> co-founder, http://www.datastax.com
>> @spyced
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> --
> Viktor Klang
>
> Director of Engineering
> Typesafe - The software stack for applications that scale
>
> Twitter: @viktorklang
>



-- 
Jonathan Ellis
Project Chair, Apache Cassandra
co-founder, http://www.datastax.com
@spyced


From nathan.reynolds at oracle.com  Fri Dec 14 13:15:20 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 14 Dec 2012 11:15:20 -0700
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAHjP37HKTTda_CVYZUL3My=pY-CaJaz13zfZK7KsQOy5rjFMww@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<CAHjP37HKTTda_CVYZUL3My=pY-CaJaz13zfZK7KsQOy5rjFMww@mail.gmail.com>
Message-ID: <50CB6CB8.6010503@oracle.com>

In C++, we hit a problem where the compiler did exactly that.  We 
thought we were loading and storing a 32-bit bitfield.  The compiler 
decided to extend this to 64-bits and trashed a nearby 32-bit reference 
counter.  We stared at a lot of code a long time before deciding to look 
at the assembly.  Once we did that, it became very apparent what was 
happening.  We now don't like bitfields and pad to 64-bits all of our 
32-bit variables used in atomic operations.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/13/2012 10:45 AM, Vitaly Davidovich wrote:
>
> You can pack data into the processor's word size and get atomic ops 
> out of it, but in multithreaded scenarios, you're on your own :).  For 
> example, compiler will generate a full word load even though you're 
> only going to modify some subword.  When you write this value back 
> (thinking you're only modifying your subword), you can trash the other 
> subwords written by other CPUs.
>
> As for the c++ example, this isn't quite the same, but read this - you 
> may find it interesting: http://lwn.net/Articles/478657/.
>
> Sent from my phone
>
> On Dec 13, 2012 12:03 PM, "Paris Yiapanis" <pyiapa at gmail.com 
> <mailto:pyiapa at gmail.com>> wrote:
>
>
>     Dave,
>
>     When you say 'the placement mechanism is supposed to ensure
>     natural alignment' I presume that it is not enforced.
>     Basically, I read a paper where they align four 16-bit subwords
>     inside a 64-bit aligned word in order to guarantee atomic access
>     (to each of those 16-bit subwords) without using CAS instructions
>     (on an intel x86 machine). However that was implemented in C++.
>     I was wondering if I could do the same with Java?
>
>
>     On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com
>     <mailto:david.dice at gmail.com>> wrote:
>
>
>         On x86, hotspot ensures all objects start on a 8-byte aligned
>         address.    Furthermore the placement mechanism is supposed to
>         ensure "natural" alignment for fields. An 8-byte volatile long
>         should have an offset that's a multiple of 8, and given the
>         object alignment it should then should have an virtual address
>         that's a multiple of 8.   Natural alignment is best for
>         performance and also ensures you don't have situations where a
>         volatile long might split a cache line.   Loads that split a
>         line won't necessarily be atomic.   Atomic instructions that
>         split a line _will be atomic, but that's handled through a
>         rather bizarre legacy mechanism.  It's best not to depend on
>         this in the future.   Modern Intel implementations basically
>         quiesce the whole system to make the operation atomic.    It's
>         anything but a local operation and the whole system suffers.
>          There's some discussion on the technique buried in the
>         following :
>
>         https://blogs.oracle.com/dave/entry/qpi_quiescence
>
>         Regards, -Dave
>
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/f5b54bf6/attachment.html>

From nathan.reynolds at oracle.com  Fri Dec 14 13:17:25 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 14 Dec 2012 11:17:25 -0700
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
Message-ID: <50CB6D35.4040401@oracle.com>

The guarantee isn't because the cache line flushes.  It is guaranteed 
because the processor will load or store only 2 bytes... if the compiler 
uses the 2-byte mov instruction.

In Java, you have no control over the layout of the fields. However, if 
JIT uses the 2-byte mov instruction then you won't see any word tearing 
or corruption of nearby fields.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/14/2012 4:53 AM, Paris Yiapanis wrote:
> Not exactly. According to intel x86 specification if you place four 
> 16-bit subwords inside a 64-bit aligned word, then an access to each 
> of those 16-bit subwords is guaranteed to be atomic. I think (maybe I 
> am wrong) the reason is because such an access to one of those 
> subwords will cause the entire cache line to flush.
> So how about in Java? If you put together four consecutive Short 
> primitives (that are 16-bits each) will the same scenario occur? Not 
> volatile shorts, just plain shorts.
> But if I understand correct from the discussion, this is guaranteed 
> for C/C++ that you can specify explicitly the alignment but it is not 
> guaranteed for Java.
>
> - Paris
>
> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     What are you trying to do?
>
>     Declare the 16-bit quantities volatile, and you get atomic access
>     to them "without a CAS". (Not even sure how would CAS help otherwise)
>
>     Alex
>
>     On 13/12/2012 17:00, Paris Yiapanis wrote:
>>
>>     Dave,
>>
>>     When you say 'the placement mechanism is supposed to ensure
>>     natural alignment' I presume that it is not enforced.
>>     Basically, I read a paper where they align four 16-bit subwords
>>     inside a 64-bit aligned word in order to guarantee atomic access
>>     (to each of those 16-bit subwords) without using CAS instructions
>>     (on an intel x86 machine). However that was implemented in C++.
>>     I was wondering if I could do the same with Java?
>>
>>
>>     On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com
>>     <mailto:david.dice at gmail.com>> wrote:
>>
>>
>>         On x86, hotspot ensures all objects start on a 8-byte aligned
>>         address.    Furthermore the placement mechanism is supposed
>>         to ensure "natural" alignment for fields.   An 8-byte
>>         volatile long should have an offset that's a multiple of 8,
>>         and given the object alignment it should then should have an
>>         virtual address that's a multiple of 8.   Natural alignment
>>         is best for performance and also ensures you don't have
>>         situations where a volatile long might split a cache line.  
>>         Loads that split a line won't necessarily be atomic.   Atomic
>>         instructions that split a line _will be atomic, but that's
>>         handled through a rather bizarre legacy mechanism.  It's best
>>         not to depend on this in the future.   Modern Intel
>>         implementations basically quiesce the whole system to make
>>         the operation atomic.    It's anything but a local operation
>>         and the whole system suffers.  There's some discussion on the
>>         technique buried in the following :
>>
>>         https://blogs.oracle.com/dave/entry/qpi_quiescence
>>
>>         Regards, -Dave
>>
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/a812e6ed/attachment-0001.html>

From nathan.reynolds at oracle.com  Fri Dec 14 13:20:46 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 14 Dec 2012 11:20:46 -0700
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAHjP37H2+TrjswsJzQxSzP9dpDDQQ01M01gjKreYQyTYL47ruQ@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<CAPUTfNnMXXKKBgU9uHxxddg7Gv3Nv6mLLXVr+JZubmwTAMz8Og@mail.gmail.com>
	<CAHjP37GjF-S_-_ytv3=Spu6F+_qs7CixFhvKHOz5Czrwn04J9A@mail.gmail.com>
	<CAPUTfNnKFtx8jeeXWmKUrKheEQc2GP0Fpd3JRw=r8+fGXnH3+A@mail.gmail.com>
	<CAHjP37HejSP6pv=iff37q7aTD1jz_RugwGWMdNKT3Sv35KZOPw@mail.gmail.com>
	<CAPUTfNnn8V869+4bjfgT4kkyK8hk2kzKLo7tOB07HaBU91FH=w@mail.gmail.com>
	<CAHjP37H2+TrjswsJzQxSzP9dpDDQQ01M01gjKreYQyTYL47ruQ@mail.gmail.com>
Message-ID: <50CB6DFE.2000601@oracle.com>

 > multiple concurrent shared memory writers doesn't scale

This would be called false sharing.  It doesn't scale at all. 
Furthermore, if each store is wrapped in a transactional memory, then a 
lot of aborts are going to happen due to so many threads/cores fighting 
to update the same cache line.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/14/2012 7:29 AM, Vitaly Davidovich wrote:
>
> One more good blog entry on CAS by the Azul guys:
> http://www.azulsystems.com/blog/cliff/2011-11-16-a-short-conversation-on-biased-locking
>
> I think the main takeaway from these things is multiple concurrent 
> shared memory writers doesn't scale - period; CAS or plain store, 
> doesn't matter.  At least that's how I think of it.
> Sent from my phone
>
> On Dec 14, 2012 9:17 AM, "Paris Yiapanis" <pyiapa at gmail.com 
> <mailto:pyiapa at gmail.com>> wrote:
>
>     Thanks Vitaly.
>
>     I think the link that 404s is moved here:
>     https://blogs.oracle.com/dave/entry/biased_locking_in_hotspot
>
>     - Paris
>
>     On Fri, Dec 14, 2012 at 2:12 PM, Vitaly Davidovich
>     <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>
>         I think it's the one he links to in this post, but the link
>         404s so can't make sure.  Dave is on this list so maybe he'll
>         see this and add his wisdom/insight.
>
>         By the way, I see I wrote "exclusive" access in a few places
>         in previous email in a generic sense - in MESI terms, I
>         actually mean Modified state.  I realize this can be confusing
>         so just wanted to clarify.
>
>         Sent from my phone
>
>         On Dec 14, 2012 8:54 AM, "Paris Yiapanis" <pyiapa at gmail.com
>         <mailto:pyiapa at gmail.com>> wrote:
>
>             Good to know. Many Thanks.
>
>             Is this the blog post you mentioned?
>             https://blogs.oracle.com/dave/entry/cas_and_cache_trivia_invalidate
>
>             On Fri, Dec 14, 2012 at 1:40 PM, Vitaly Davidovich
>             <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>
>                 Yes, the coherence system invalidates the entire
>                 cacheline; the wrinkle here is the store buffer - it
>                 doesn't participate in coherence; once the buffer is
>                 drained to L1, the coherence system ensures, well,
>                 coherence :).  So, you might write to a subword of a
>                 word.  The coherence system knows the state of the
>                 cacheline; if the line is missing from L1, it issues a
>                 load to bring it in.  In the meantime, the CPU stores
>                 the line into the store buffer (this is to prevent a
>                 stall waiting for memory to respond).  Well, at this
>                 point, no other CPU can "see" the store you just
>                 dropped into buffer - it's only after the line drains
>                 from the store buffer does coherence request exclusive
>                 access to the line and issues invalidate requests to
>                 other CPU caches that have it.  The problem, of
>                 course, is that there's no order guarantee if multiple
>                 CPUs are doing this concurrently.
>
>                 As for volatile loads, they don't necessarily go
>                 through memory; if the line is already (best case) in
>                 L1 then access time is just a few cycles anyway.  For
>                 CAS or volatile stores, they also don't have to be
>                 much worse than normal writes - the overwhelming cost
>                 in either case is going to be all the coherence
>                 traffic between the CPUs to invalidate the line and
>                 grab exclusive access.  I think Dave Dice had a blog
>                 post a while ago comparing CAS vs normal writes when
>                 many threads are doing this concurrently - performance
>                 was abysmal in both cases (and pretty close in
>                 relative terms).
>
>                 This is a very good paper on how caches interact:
>                 https://www.google.com/url?sa=t&source=web&cd=2&ved=0CDUQFjAB&url=http%3A%2F%2Fwww.rdrop.com%2Fusers%2Fpaulmck%2Fscalability%2Fpaper%2Fwhymb.2009.04.05a.pdf&ei=fSvLULjMIIW40AGql4GIBw&usg=AFQjCNG4cHx3vsWSJO7yhbi2xghwsegpDA
>
>                 Sent from my phone
>
>                 On Dec 14, 2012 7:28 AM, "Paris Yiapanis"
>                 <pyiapa at gmail.com <mailto:pyiapa at gmail.com>> wrote:
>
>                     Sorry, forgot to mention that the access to each
>                     of those 16-bit subwords is guaranteed to be
>                     atomic if any of those are accessed concurrently
>                     by different processors because the entire cache
>                     line is flushed so it invalidates the entire
>                     64-bit word. I was trying to avoid volatiles since
>                     that would force every access to each subword to
>                     go through memory. But taking advantage of that
>                     alignment property will only be costly if there is
>                     a concurrent access by multiple processors on
>                     those subwords. But again If I understand
>                     correctly there is no 100% guaranty for that in Java?
>
>                     - Paris
>
>                     On Fri, Dec 14, 2012 at 11:53 AM, Paris Yiapanis
>                     <pyiapa at gmail.com <mailto:pyiapa at gmail.com>> wrote:
>
>                         Not exactly. According to intel x86
>                         specification if you place four 16-bit
>                         subwords inside a 64-bit aligned word, then an
>                         access to each of those 16-bit subwords is
>                         guaranteed to be atomic. I think (maybe I am
>                         wrong) the reason is because such an access to
>                         one of those subwords will cause the entire
>                         cache line to flush.
>                         So how about in Java? If you put together four
>                         consecutive Short primitives (that are 16-bits
>                         each) will the same scenario occur? Not
>                         volatile shorts, just plain shorts.
>                         But if I understand correct from the
>                         discussion, this is guaranteed for C/C++ that
>                         you can specify explicitly the alignment but
>                         it is not guaranteed for Java.
>
>                         - Paris
>
>
>                         On Fri, Dec 14, 2012 at 11:41 AM, oleksandr
>                         otenko <oleksandr.otenko at oracle.com
>                         <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>                             What are you trying to do?
>
>                             Declare the 16-bit quantities volatile,
>                             and you get atomic access to them "without
>                             a CAS". (Not even sure how would CAS help
>                             otherwise)
>
>                             Alex
>
>                             On 13/12/2012 17:00, Paris Yiapanis wrote:
>>
>>                             Dave,
>>
>>                             When you say 'the placement mechanism is
>>                             supposed to ensure natural alignment' I
>>                             presume that it is not enforced.
>>                             Basically, I read a paper where they
>>                             align four 16-bit subwords inside a
>>                             64-bit aligned word in order to guarantee
>>                             atomic access (to each of those 16-bit
>>                             subwords) without using CAS instructions
>>                             (on an intel x86 machine). However that
>>                             was implemented in C++.
>>                             I was wondering if I could do the same
>>                             with Java?
>>
>>
>>                             On Thu, Dec 13, 2012 at 3:32 PM, David
>>                             Dice <david.dice at gmail.com
>>                             <mailto:david.dice at gmail.com>> wrote:
>>
>>
>>                                 On x86, hotspot ensures all objects
>>                                 start on a 8-byte aligned address.
>>                                  Furthermore the placement mechanism
>>                                 is supposed to ensure "natural"
>>                                 alignment for fields.   An 8-byte
>>                                 volatile long should have an offset
>>                                 that's a multiple of 8, and given the
>>                                 object alignment it should then
>>                                 should have an virtual address that's
>>                                 a multiple of 8.   Natural alignment
>>                                 is best for performance and also
>>                                 ensures you don't have situations
>>                                 where a volatile long might split a
>>                                 cache line. Loads that split a line
>>                                 won't necessarily be atomic. Atomic
>>                                 instructions that split a line _will
>>                                 be atomic, but that's handled through
>>                                 a rather bizarre legacy mechanism.
>>                                  It's best not to depend on this in
>>                                 the future. Modern Intel
>>                                 implementations basically quiesce the
>>                                 whole system to make the operation
>>                                 atomic.  It's anything but a local
>>                                 operation and the whole system
>>                                 suffers.  There's some discussion on
>>                                 the technique buried in the following :
>>
>>                                 https://blogs.oracle.com/dave/entry/qpi_quiescence
>>
>>                                 Regards, -Dave
>>
>>
>>
>>                                 _______________________________________________
>>                                 Concurrency-interest mailing list
>>                                 Concurrency-interest at cs.oswego.edu
>>                                 <mailto:Concurrency-interest at cs.oswego.edu>
>>                                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>                             _______________________________________________
>>                             Concurrency-interest mailing list
>>                             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>                     Concurrency-interest at cs.oswego.edu
>                     <mailto:Concurrency-interest at cs.oswego.edu>
>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/48c23d8f/attachment-0001.html>

From vitalyd at gmail.com  Fri Dec 14 13:20:57 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 14 Dec 2012 13:20:57 -0500
Subject: [concurrency-interest] LinkedTransferQueue performance
 substantially worse than LinkedBlockingQueue
In-Reply-To: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
References: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
Message-ID: <CAHjP37F5Bk9FhBBzkUa29y+LAsqPGssJYdMfzRJokNZr0zCJyg@mail.gmail.com>

Jon,

Sorry to take a step back and not answer your question, but have you
considered using the Disruptor lib to model each stage as a ring buffer?
You're after reduced latency in handoff (as I understand it) so perhaps
it'll work better for you than executorservice + queue?

As for LTQ, have you guys run this under a profiler to get an idea of where
the delay is coming from? In the places where you're seeing regression,
what's the typical interaction between producers/consumers? That is, when
producer puts something is there already a consumer waiting there? Does one
side outpace the other?

Vitaly

Sent from my phone
On Dec 14, 2012 11:45 AM, "Jonathan Ellis" <jbellis at gmail.com> wrote:

> Hi all,
>
> Cassandra database requests spend a lot of time (100s of microseconds)
> moving from one stage (ThreadPoolExecutor) to another.  Each stage is
> based on an unbounded LinkedBlockingQueue.  Typically each stage has
> multiple producers and consumers, although for some (such as for
> network traffic to a specific peer) there is only a single consumer.
>
> I tried updating Cassandra to use LTQ [1] instead using the
> jsr166y.jar for JDK 1.6 and saw performance go down by over 15% on
> both reads and writes [2].  Am I correct that this is unexpected?  How
> could I proceed with troubleshooting?
>
> [1] https://github.com/jbellis/cassandra/branches/ltq
> [2] https://issues.apache.org/jira/browse/CASSANDRA-4718
>
> --
> Jonathan Ellis
> Project Chair, Apache Cassandra
> co-founder, http://www.datastax.com
> @spyced
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/4f8e2cab/attachment.html>

From viktor.klang at gmail.com  Fri Dec 14 13:24:27 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 14 Dec 2012 19:24:27 +0100
Subject: [concurrency-interest] LinkedTransferQueue performance
 substantially worse than LinkedBlockingQueue
In-Reply-To: <CALdd-zgF6HxLVeX5CZJqQmC7ybKA8PpsX2=4nRRiGVtzz0ci6g@mail.gmail.com>
References: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
	<CANPzfU-ASLDixDouuo+D=857fS4fZRDMMJuh0p3RwN_rZvJ84Q@mail.gmail.com>
	<CALdd-zgF6HxLVeX5CZJqQmC7ybKA8PpsX2=4nRRiGVtzz0ci6g@mail.gmail.com>
Message-ID: <CANPzfU-ZT-ebq6Jig2HJNMA=kskN_RU6DjwUzK_sEDmu3RUiPg@mail.gmail.com>

Yeah, what operations do you use, what are the average queue sizes, where
is the most CPU time spent?


On Fri, Dec 14, 2012 at 6:59 PM, Jonathan Ellis <jbellis at gmail.com> wrote:

> To start with I'd settle for "Not be worse than LBQ."
>
> What do you mean more specifically?
>
> On Fri, Dec 14, 2012 at 11:11 AM, ?iktor ?lang <viktor.klang at gmail.com>
> wrote:
> > What are your requirements?
> >
> > Cheers,
> > ?
> >
> >
> > On Fri, Dec 14, 2012 at 5:40 PM, Jonathan Ellis <jbellis at gmail.com>
> wrote:
> >>
> >> Hi all,
> >>
> >> Cassandra database requests spend a lot of time (100s of microseconds)
> >> moving from one stage (ThreadPoolExecutor) to another.  Each stage is
> >> based on an unbounded LinkedBlockingQueue.  Typically each stage has
> >> multiple producers and consumers, although for some (such as for
> >> network traffic to a specific peer) there is only a single consumer.
> >>
> >> I tried updating Cassandra to use LTQ [1] instead using the
> >> jsr166y.jar for JDK 1.6 and saw performance go down by over 15% on
> >> both reads and writes [2].  Am I correct that this is unexpected?  How
> >> could I proceed with troubleshooting?
> >>
> >> [1] https://github.com/jbellis/cassandra/branches/ltq
> >> [2] https://issues.apache.org/jira/browse/CASSANDRA-4718
> >>
> >> --
> >> Jonathan Ellis
> >> Project Chair, Apache Cassandra
> >> co-founder, http://www.datastax.com
> >> @spyced
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
> >
> > --
> > Viktor Klang
> >
> > Director of Engineering
> > Typesafe - The software stack for applications that scale
> >
> > Twitter: @viktorklang
> >
>
>
>
> --
> Jonathan Ellis
> Project Chair, Apache Cassandra
> co-founder, http://www.datastax.com
> @spyced
>



-- 
Viktor Klang

Director of Engineering
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/f5f2b40a/attachment.html>

From nathan.reynolds at oracle.com  Fri Dec 14 13:28:53 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 14 Dec 2012 11:28:53 -0700
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <CAPUTfNkUU-T7fn5gMwKYSs48O56=DosTP4D1sQ14b2j4+Xrx4A@mail.gmail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<50CB2073.2080203@oracle.com>
	<CAPUTfNkUU-T7fn5gMwKYSs48O56=DosTP4D1sQ14b2j4+Xrx4A@mail.gmail.com>
Message-ID: <50CB6FE5.803@oracle.com>

The program issues a 2-byte store.  The address and data are put into 
the load/store buffer inside the core.  The cache issues a 
read/invalidate for the 64-byte cache line.  The line arrives into L1 
and no other cache has a copy. The 2 bytes of data is updated in the 
cache line and the rest of the bytes in the entire cache line are not 
modified.  The address and data are removed from the load/store buffer.  
This change will appear to be atomic to the rest of the system.  The 
rest of the system can't see part of the store's value and part of the 
old value.  However, a 2^(n)^(d) store to the same cache will not happen 
atomically with the first.  So, another core could observe the 1^(s)^(t) 
store without the second.

It is guaranteed in Java and C++ so long as JIT/compiler uses the 2-byte 
mov instruction.  If it uses a 4-byte mov instruction, then nearby 
fields could end up being corrupted.  If JIT/compiler uses a 4-byte or 
8-byte mov instruction, then the nearby fields will be corrupted.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/14/2012 6:30 AM, Paris Yiapanis wrote:
> I agree with you in terms of ordering. Also, as you say, individual 
> loads/stores in any of these 16-bit (non-volatile) subwords is atomic 
> as of itself. But maybe I wasn't very clear on what I need.
>
> If four of these 16-bit subwords are aligned as part of the 64-bit 
> word, a concurrent access to one of those 16-bit subwords will 
> atomically update the full 64-bit word (that means all four 16-bit 
> subwords together). Thus, no two threads can modify concurrently two 
> subwords of the same word (given that they are aligned as the intel 
> spec mentions). So, if I understand well it acts like you atomically 
> update the entire 64-bit word when you update one of its aligned 
> subwords. But again this is in C++, so it seems not guaranteed for 
> Java (again if I understand well).
>
> - Paris
>
> On Fri, Dec 14, 2012 at 12:49 PM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     There is no difference between volatile access and non-volatile
>     access, if you look at just loads / stores on x86. The difference
>     is in additional serialization between last volatile store and
>     next volatile load. Without this the non-volatile stores, if they
>     are done, will appear atomically, too; there's just no guarantee
>     on ordering with respect to other loads (stores are still ordered).
>
>     Alex
>
>     On 14/12/2012 11:53, Paris Yiapanis wrote:
>>     Not exactly. According to intel x86 specification if you place
>>     four 16-bit subwords inside a 64-bit aligned word, then an access
>>     to each of those 16-bit subwords is guaranteed to be atomic. I
>>     think (maybe I am wrong) the reason is because such an access to
>>     one of those subwords will cause the entire cache line to flush.
>>     So how about in Java? If you put together four consecutive Short
>>     primitives (that are 16-bits each) will the same scenario occur?
>>     Not volatile shorts, just plain shorts.
>>     But if I understand correct from the discussion, this is
>>     guaranteed for C/C++ that you can specify explicitly the
>>     alignment but it is not guaranteed for Java.
>>
>>     - Paris
>>
>>     On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         What are you trying to do?
>>
>>         Declare the 16-bit quantities volatile, and you get atomic
>>         access to them "without a CAS". (Not even sure how would CAS
>>         help otherwise)
>>
>>         Alex
>>
>>         On 13/12/2012 17:00, Paris Yiapanis wrote:
>>>
>>>         Dave,
>>>
>>>         When you say 'the placement mechanism is supposed to ensure
>>>         natural alignment' I presume that it is not enforced.
>>>         Basically, I read a paper where they align four 16-bit
>>>         subwords inside a 64-bit aligned word in order to guarantee
>>>         atomic access (to each of those 16-bit subwords) without
>>>         using CAS instructions (on an intel x86 machine). However
>>>         that was implemented in C++.
>>>         I was wondering if I could do the same with Java?
>>>
>>>
>>>         On Thu, Dec 13, 2012 at 3:32 PM, David Dice
>>>         <david.dice at gmail.com <mailto:david.dice at gmail.com>> wrote:
>>>
>>>
>>>             On x86, hotspot ensures all objects start on a 8-byte
>>>             aligned address.    Furthermore the placement mechanism
>>>             is supposed to ensure "natural" alignment for fields.  
>>>             An 8-byte volatile long should have an offset that's a
>>>             multiple of 8, and given the object alignment it should
>>>             then should have an virtual address that's a multiple of
>>>             8.   Natural alignment is best for performance and also
>>>             ensures you don't have situations where a volatile long
>>>             might split a cache line.   Loads that split a line
>>>             won't necessarily be atomic. Atomic instructions that
>>>             split a line _will be atomic, but that's handled through
>>>             a rather bizarre legacy mechanism.  It's best not to
>>>             depend on this in the future. Modern Intel
>>>             implementations basically quiesce the whole system to
>>>             make the operation atomic.  It's anything but a local
>>>             operation and the whole system suffers.  There's some
>>>             discussion on the technique buried in the following :
>>>
>>>             https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>
>>>             Regards, -Dave
>>>
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/e00db02e/attachment-0001.html>

From holger.hoffstaette at googlemail.com  Fri Dec 14 13:29:24 2012
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Fri, 14 Dec 2012 19:29:24 +0100
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <50CB6CB8.6010503@oracle.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<CAHjP37HKTTda_CVYZUL3My=pY-CaJaz13zfZK7KsQOy5rjFMww@mail.gmail.com>
	<50CB6CB8.6010503@oracle.com>
Message-ID: <50CB7004.9040805@googlemail.com>

On 12/14/12 19:15, Nathan Reynolds wrote:
> In C++, we hit a problem where the compiler did exactly that.  We
> thought we were loading and storing a 32-bit bitfield.  The compiler
> decided to extend this to 64-bits and trashed a nearby 32-bit reference
> counter. [...]

For those who think this is a C++ bug or an isolated incident:
http://lwn.net/Articles/478657/

Nothing like half-baked language/compiler specs..

enjoy
-h


From vitalyd at gmail.com  Fri Dec 14 13:32:52 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 14 Dec 2012 13:32:52 -0500
Subject: [concurrency-interest] Memory Alignment in Java
Message-ID: <CAHjP37E83NQtfXeqyWZCGED3AtavY1+r3gyROqn4Znha=jVw2w@mail.gmail.com>

I was talking about true/intended sharing.  False sharing is actually
someone trying to not share but getting bit by how hardware actually works
with memory. :)

Sent from my phone
On Dec 14, 2012 1:29 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  > multiple concurrent shared memory writers doesn't scale
>
> This would be called false sharing.  It doesn't scale at all.
> Furthermore, if each store is wrapped in a transactional memory, then a lot
> of aborts are going to happen due to so many threads/cores fighting to
> update the same cache line.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 12/14/2012 7:29 AM, Vitaly Davidovich wrote:
>
> One more good blog entry on CAS by the Azul guys:
>
> http://www.azulsystems.com/blog/cliff/2011-11-16-a-short-conversation-on-biased-locking
>
> I think the main takeaway from these things is multiple concurrent shared
> memory writers doesn't scale - period; CAS or plain store, doesn't matter.
> At least that's how I think of it.
> Sent from my phone
> On Dec 14, 2012 9:17 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>
>> Thanks Vitaly.
>>
>> I think the link that 404s is moved here:
>> https://blogs.oracle.com/dave/entry/biased_locking_in_hotspot
>>
>> - Paris
>>
>> On Fri, Dec 14, 2012 at 2:12 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>
>>> I think it's the one he links to in this post, but the link 404s so
>>> can't make sure.  Dave is on this list so maybe he'll see this and add his
>>> wisdom/insight.
>>>
>>> By the way, I see I wrote "exclusive" access in a few places in previous
>>> email in a generic sense - in MESI terms, I actually mean Modified state.
>>> I realize this can be confusing so just wanted to clarify.
>>>
>>> Sent from my phone
>>>  On Dec 14, 2012 8:54 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>>>
>>>> Good to know. Many Thanks.
>>>>
>>>> Is this the blog post you mentioned?
>>>> https://blogs.oracle.com/dave/entry/cas_and_cache_trivia_invalidate
>>>>
>>>> On Fri, Dec 14, 2012 at 1:40 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>>
>>>>> Yes, the coherence system invalidates the entire cacheline; the
>>>>> wrinkle here is the store buffer - it doesn't participate in coherence;
>>>>> once the buffer is drained to L1, the coherence system ensures, well,
>>>>> coherence :).  So, you might write to a subword of a word.  The coherence
>>>>> system knows the state of the cacheline; if the line is missing from L1, it
>>>>> issues a load to bring it in.  In the meantime, the CPU stores the line
>>>>> into the store buffer (this is to prevent a stall waiting for memory to
>>>>> respond).  Well, at this point, no other CPU can "see" the store you just
>>>>> dropped into buffer - it's only after the line drains from the store buffer
>>>>> does coherence request exclusive access to the line and issues invalidate
>>>>> requests to other CPU caches that have it.  The problem, of course, is that
>>>>> there's no order guarantee if multiple CPUs are doing this concurrently.
>>>>>
>>>>> As for volatile loads, they don't necessarily go through memory; if
>>>>> the line is already (best case) in L1 then access time is just a few cycles
>>>>> anyway.  For CAS or volatile stores, they also don't have to be much worse
>>>>> than normal writes - the overwhelming cost in either case is going to be
>>>>> all the coherence traffic between the CPUs to invalidate the line and grab
>>>>> exclusive access.  I think Dave Dice had a blog post a while ago comparing
>>>>> CAS vs normal writes when many threads are doing this concurrently -
>>>>> performance was abysmal in both cases (and pretty close in relative terms).
>>>>>
>>>>> This is a very good paper on how caches interact:
>>>>> https://www.google.com/url?sa=t&source=web&cd=2&ved=0CDUQFjAB&url=http%3A%2F%2Fwww.rdrop.com%2Fusers%2Fpaulmck%2Fscalability%2Fpaper%2Fwhymb.2009.04.05a.pdf&ei=fSvLULjMIIW40AGql4GIBw&usg=AFQjCNG4cHx3vsWSJO7yhbi2xghwsegpDA
>>>>>
>>>>> Sent from my phone
>>>>>  On Dec 14, 2012 7:28 AM, "Paris Yiapanis" <pyiapa at gmail.com> wrote:
>>>>>
>>>>>> Sorry, forgot to mention that the access to each of those 16-bit
>>>>>> subwords is guaranteed to be atomic if any of those are accessed
>>>>>> concurrently by different processors because the entire cache line is
>>>>>> flushed so it invalidates the entire 64-bit word. I was trying to avoid
>>>>>> volatiles since that would force every access to each subword to go through
>>>>>> memory. But taking advantage of that alignment property will only be costly
>>>>>> if there is a concurrent access by multiple processors on those subwords.
>>>>>> But again If I understand correctly there is no 100% guaranty for that in
>>>>>> Java?
>>>>>>
>>>>>> - Paris
>>>>>>
>>>>>> On Fri, Dec 14, 2012 at 11:53 AM, Paris Yiapanis <pyiapa at gmail.com>wrote:
>>>>>>
>>>>>>> Not exactly. According to intel x86 specification if you place four
>>>>>>> 16-bit subwords inside a 64-bit aligned word, then an access to each of
>>>>>>> those 16-bit subwords is guaranteed to be atomic. I think (maybe I am
>>>>>>> wrong) the reason is because such an access to one of those subwords will
>>>>>>> cause the entire cache line to flush.
>>>>>>> So how about in Java? If you put together four consecutive Short
>>>>>>> primitives (that are 16-bits each) will the same scenario occur? Not
>>>>>>> volatile shorts, just plain shorts.
>>>>>>> But if I understand correct from the discussion, this is guaranteed
>>>>>>> for C/C++ that you can specify explicitly the alignment but it is not
>>>>>>> guaranteed for Java.
>>>>>>>
>>>>>>> - Paris
>>>>>>>
>>>>>>>
>>>>>>> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
>>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>>
>>>>>>>>  What are you trying to do?
>>>>>>>>
>>>>>>>> Declare the 16-bit quantities volatile, and you get atomic access
>>>>>>>> to them "without a CAS". (Not even sure how would CAS help otherwise)
>>>>>>>>
>>>>>>>> Alex
>>>>>>>>
>>>>>>>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>>>>>>>
>>>>>>>>
>>>>>>>> Dave,
>>>>>>>>
>>>>>>>> When you say 'the placement mechanism is supposed to ensure natural
>>>>>>>> alignment' I presume that it is not enforced.
>>>>>>>> Basically, I read a paper where they align four 16-bit subwords
>>>>>>>> inside a 64-bit aligned word in order to guarantee atomic access (to each
>>>>>>>> of those 16-bit subwords) without using CAS instructions (on an intel x86
>>>>>>>> machine). However that was implemented in C++.
>>>>>>>> I was wondering if I could do the same with Java?
>>>>>>>>
>>>>>>>>
>>>>>>>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com>wrote:
>>>>>>>>
>>>>>>>>>
>>>>>>>>> On x86, hotspot ensures all objects start on a 8-byte aligned
>>>>>>>>> address.    Furthermore the placement mechanism is supposed to ensure
>>>>>>>>> "natural" alignment for fields.   An 8-byte volatile long should have an
>>>>>>>>> offset that's a multiple of 8, and given the object alignment it should
>>>>>>>>> then should have an virtual address that's a multiple of 8.   Natural
>>>>>>>>> alignment is best for performance and also ensures you don't have
>>>>>>>>> situations where a volatile long might split a cache line.   Loads that
>>>>>>>>> split a line won't necessarily be atomic.   Atomic instructions that split
>>>>>>>>> a line _will be atomic, but that's handled through a rather bizarre legacy
>>>>>>>>> mechanism.  It's best not to depend on this in the future.   Modern Intel
>>>>>>>>> implementations basically quiesce the whole system to make the operation
>>>>>>>>> atomic.    It's anything but a local operation and the whole system
>>>>>>>>> suffers.  There's some discussion on the technique buried in the following :
>>>>>>>>>
>>>>>>>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>>>>>>>
>>>>>>>>>  Regards, -Dave
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/80646eee/attachment-0001.html>

From jacyg at alumni.rice.edu  Fri Dec 14 13:37:11 2012
From: jacyg at alumni.rice.edu (Jacy Odin Grannis)
Date: Fri, 14 Dec 2012 12:37:11 -0600
Subject: [concurrency-interest] LinkedTransferQueue performance
 substantially worse than LinkedBlockingQueue
In-Reply-To: <CALdd-zgF6HxLVeX5CZJqQmC7ybKA8PpsX2=4nRRiGVtzz0ci6g@mail.gmail.com>
References: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
	<CANPzfU-ASLDixDouuo+D=857fS4fZRDMMJuh0p3RwN_rZvJ84Q@mail.gmail.com>
	<CALdd-zgF6HxLVeX5CZJqQmC7ybKA8PpsX2=4nRRiGVtzz0ci6g@mail.gmail.com>
Message-ID: <CAESiqEpBt4xabb7Y243OuHit756A8ghPjBXcPKkwyBRR4JmMWw@mail.gmail.com>

I'm not sure why you'd expect it to be better.  Even from just reading the
descriptions, it should be clear that LTQ is accomplishing something more
complex than LBQ.  As for why it would be slower, if you look into the
source, in addition to there being extra code to execute to determine if a
transfer is available or if it should put into the queue, the Node object
is larger, which, while likely short-lived, will still tie up more memory
and make any GC operations (such as young generation collections) more
frequent.


On Fri, Dec 14, 2012 at 11:59 AM, Jonathan Ellis <jbellis at gmail.com> wrote:

> To start with I'd settle for "Not be worse than LBQ."
>
> What do you mean more specifically?
>
> On Fri, Dec 14, 2012 at 11:11 AM, ?iktor ?lang <viktor.klang at gmail.com>
> wrote:
> > What are your requirements?
> >
> > Cheers,
> > ?
> >
> >
> > On Fri, Dec 14, 2012 at 5:40 PM, Jonathan Ellis <jbellis at gmail.com>
> wrote:
> >>
> >> Hi all,
> >>
> >> Cassandra database requests spend a lot of time (100s of microseconds)
> >> moving from one stage (ThreadPoolExecutor) to another.  Each stage is
> >> based on an unbounded LinkedBlockingQueue.  Typically each stage has
> >> multiple producers and consumers, although for some (such as for
> >> network traffic to a specific peer) there is only a single consumer.
> >>
> >> I tried updating Cassandra to use LTQ [1] instead using the
> >> jsr166y.jar for JDK 1.6 and saw performance go down by over 15% on
> >> both reads and writes [2].  Am I correct that this is unexpected?  How
> >> could I proceed with troubleshooting?
> >>
> >> [1] https://github.com/jbellis/cassandra/branches/ltq
> >> [2] https://issues.apache.org/jira/browse/CASSANDRA-4718
> >>
> >> --
> >> Jonathan Ellis
> >> Project Chair, Apache Cassandra
> >> co-founder, http://www.datastax.com
> >> @spyced
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
> >
> > --
> > Viktor Klang
> >
> > Director of Engineering
> > Typesafe - The software stack for applications that scale
> >
> > Twitter: @viktorklang
> >
>
>
>
> --
> Jonathan Ellis
> Project Chair, Apache Cassandra
> co-founder, http://www.datastax.com
> @spyced
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/aabf3651/attachment.html>

From jbellis at gmail.com  Fri Dec 14 13:41:49 2012
From: jbellis at gmail.com (Jonathan Ellis)
Date: Fri, 14 Dec 2012 12:41:49 -0600
Subject: [concurrency-interest] LinkedTransferQueue performance
 substantially worse than LinkedBlockingQueue
In-Reply-To: <CAHjP37F5Bk9FhBBzkUa29y+LAsqPGssJYdMfzRJokNZr0zCJyg@mail.gmail.com>
References: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
	<CAHjP37F5Bk9FhBBzkUa29y+LAsqPGssJYdMfzRJokNZr0zCJyg@mail.gmail.com>
Message-ID: <CALdd-ziRR=tHy0aTe0wqaeDusFrYivrcNdOEfQRz6WAtOj=LZg@mail.gmail.com>

I have, but with Distruptor I'd be stuck with worst-case queue (ring)
size for each threadpool, which at the least is a much bigger rewrite.
 Besides the obvious API changes we'd have to move to NIO for the
network stack, at a minimum.  (Which from our experiments client-side
consistently has worse latency characteristics than blocking i/o.)

Also, last I checked Disruptor gives you the choice between "always
spin" and "always block for signal."  The former is a nonstarter for
us (sysadmins have a reasonable preference to see cpu usage correlate
with workload) and I'm not sure the latter is going to give us that
much of an improvement.

On Fri, Dec 14, 2012 at 12:20 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> Jon,
>
> Sorry to take a step back and not answer your question, but have you
> considered using the Disruptor lib to model each stage as a ring buffer?
> You're after reduced latency in handoff (as I understand it) so perhaps
> it'll work better for you than executorservice + queue?
>
> As for LTQ, have you guys run this under a profiler to get an idea of where
> the delay is coming from? In the places where you're seeing regression,
> what's the typical interaction between producers/consumers? That is, when
> producer puts something is there already a consumer waiting there? Does one
> side outpace the other?
>
> Vitaly
>
> Sent from my phone
>
> On Dec 14, 2012 11:45 AM, "Jonathan Ellis" <jbellis at gmail.com> wrote:
>>
>> Hi all,
>>
>> Cassandra database requests spend a lot of time (100s of microseconds)
>> moving from one stage (ThreadPoolExecutor) to another.  Each stage is
>> based on an unbounded LinkedBlockingQueue.  Typically each stage has
>> multiple producers and consumers, although for some (such as for
>> network traffic to a specific peer) there is only a single consumer.
>>
>> I tried updating Cassandra to use LTQ [1] instead using the
>> jsr166y.jar for JDK 1.6 and saw performance go down by over 15% on
>> both reads and writes [2].  Am I correct that this is unexpected?  How
>> could I proceed with troubleshooting?
>>
>> [1] https://github.com/jbellis/cassandra/branches/ltq
>> [2] https://issues.apache.org/jira/browse/CASSANDRA-4718
>>
>> --
>> Jonathan Ellis
>> Project Chair, Apache Cassandra
>> co-founder, http://www.datastax.com
>> @spyced
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-- 
Jonathan Ellis
Project Chair, Apache Cassandra
co-founder, http://www.datastax.com
@spyced

From nathan.reynolds at oracle.com  Fri Dec 14 14:04:56 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 14 Dec 2012 12:04:56 -0700
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <50CB7004.9040805@googlemail.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<CAHjP37HKTTda_CVYZUL3My=pY-CaJaz13zfZK7KsQOy5rjFMww@mail.gmail.com>
	<50CB6CB8.6010503@oracle.com> <50CB7004.9040805@googlemail.com>
Message-ID: <50CB7858.1050606@oracle.com>

This is exactly what I saw but on a different compiler on a different 
platform.  So, it would seem the problem is more wide spread than I 
originally thought.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/14/2012 11:29 AM, Holger Hoffst?tte wrote:
> On 12/14/12 19:15, Nathan Reynolds wrote:
>> In C++, we hit a problem where the compiler did exactly that.  We
>> thought we were loading and storing a 32-bit bitfield.  The compiler
>> decided to extend this to 64-bits and trashed a nearby 32-bit reference
>> counter. [...]
> For those who think this is a C++ bug or an isolated incident:
> http://lwn.net/Articles/478657/
>
> Nothing like half-baked language/compiler specs..
>
> enjoy
> -h
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/0f6bd26f/attachment.html>

From hans.boehm at hp.com  Fri Dec 14 15:00:00 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 14 Dec 2012 20:00:00 +0000
Subject: [concurrency-interest] Memory Alignment in Java
	(Vitaly	Davidovich)
In-Reply-To: <50CB6CB8.6010503@oracle.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<CAHjP37HKTTda_CVYZUL3My=pY-CaJaz13zfZK7KsQOy5rjFMww@mail.gmail.com>
	<50CB6CB8.6010503@oracle.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD23695A6DC@G9W0725.americas.hpqcorp.net>

[C,C++ aside:]
As is pointed out in the article, the bit-field problem is a near-term issue.  The standards (C11, C++11) that fix it were published a few months before the article.  But gcc top-of-trunk didn't start doing the right thing until very recently.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nathan Reynolds
Sent: Friday, December 14, 2012 10:15 AM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Memory Alignment in Java (Vitaly Davidovich)

In C++, we hit a problem where the compiler did exactly that.  We thought we were loading and storing a 32-bit bitfield.  The compiler decided to extend this to 64-bits and trashed a nearby 32-bit reference counter.  We stared at a lot of code a long time before deciding to look at the assembly.  Once we did that, it became very apparent what was happening.  We now don't like bitfields and pad to 64-bits all of our 32-bit variables used in atomic operations.
Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | Architect | 602.333.9091
Oracle PSR Engineering<http://psr.us.oracle.com/> | Server Technology
On 12/13/2012 10:45 AM, Vitaly Davidovich wrote:

You can pack data into the processor's word size and get atomic ops out of it, but in multithreaded scenarios, you're on your own :).  For example, compiler will generate a full word load even though you're only going to modify some subword.  When you write this value back (thinking you're only modifying your subword), you can trash the other subwords written by other CPUs.

As for the c++ example, this isn't quite the same, but read this - you may find it interesting: http://lwn.net/Articles/478657/.

Sent from my phone
On Dec 13, 2012 12:03 PM, "Paris Yiapanis" <pyiapa at gmail.com<mailto:pyiapa at gmail.com>> wrote:

Dave,

When you say 'the placement mechanism is supposed to ensure natural alignment' I presume that it is not enforced.
Basically, I read a paper where they align four 16-bit subwords inside a 64-bit aligned word in order to guarantee atomic access (to each of those 16-bit subwords) without using CAS instructions (on an intel x86 machine). However that was implemented in C++.
I was wondering if I could do the same with Java?

On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com<mailto:david.dice at gmail.com>> wrote:

On x86, hotspot ensures all objects start on a 8-byte aligned address.    Furthermore the placement mechanism is supposed to ensure "natural" alignment for fields.   An 8-byte volatile long should have an offset that's a multiple of 8, and given the object alignment it should then should have an virtual address that's a multiple of 8.   Natural alignment is best for performance and also ensures you don't have situations where a volatile long might split a cache line.   Loads that split a line won't necessarily be atomic.   Atomic instructions that split a line _will be atomic, but that's handled through a rather bizarre legacy mechanism.  It's best not to depend on this in the future.   Modern Intel implementations basically quiesce the whole system to make the operation atomic.    It's anything but a local operation and the whole system suffers.  There's some discussion on the technique buried in the following :

 https://blogs.oracle.com/dave/entry/qpi_quiescence

Regards, -Dave



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121214/46409b7b/attachment-0001.html>

From holger.hoffstaette at googlemail.com  Fri Dec 14 16:51:55 2012
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Fri, 14 Dec 2012 22:51:55 +0100
Subject: [concurrency-interest] LinkedTransferQueue performance
 substantially worse than LinkedBlockingQueue
In-Reply-To: <CALdd-zgF6HxLVeX5CZJqQmC7ybKA8PpsX2=4nRRiGVtzz0ci6g@mail.gmail.com>
References: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
	<CANPzfU-ASLDixDouuo+D=857fS4fZRDMMJuh0p3RwN_rZvJ84Q@mail.gmail.com>
	<CALdd-zgF6HxLVeX5CZJqQmC7ybKA8PpsX2=4nRRiGVtzz0ci6g@mail.gmail.com>
Message-ID: <CAHji153YpezHjW189ySCAqcFe+FgHisMJs5_8wGVPy5c_YUEyQ@mail.gmail.com>

Jonathan,

Did you not get my other email about the CLBQ that I sent directly to you?
Check your spam..

Holger

On Fri, Dec 14, 2012 at 6:59 PM, Jonathan Ellis <jbellis at gmail.com> wrote:
> To start with I'd settle for "Not be worse than LBQ."

From davidcholmes at aapt.net.au  Sat Dec 15 01:31:38 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 15 Dec 2012 16:31:38 +1000
Subject: [concurrency-interest] LinkedTransferQueue
	performancesubstantially worse than LinkedBlockingQueue
In-Reply-To: <CALdd-zh=EFrYECZjB8vwhw3qb7_X3iyoDz7HmaUz8G8oH2xLrw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEMEJIAA.davidcholmes@aapt.net.au>

Jonathon,

Transfer queues employ a handshake between producer and consumer. If your
LBQ behaviour was mostly bursty (queue fills up, queue drains, queue fills
up, ...) then forcing the producers and consumers to synchronize using a
transfer queue will naturally slow things down. A blocking queue is like
dropping something into an in-tray and walking away; a transfer queue means
you have to wait till someone is ready to pick it up. The numbers of
producers and consumers and the production/consumption rates will determine
whether you waste a lot of time waiting or not.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jonathan
> Ellis
> Sent: Saturday, 15 December 2012 2:40 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] LinkedTransferQueue
> performancesubstantially worse than LinkedBlockingQueue
>
>
> Hi all,
>
> Cassandra database requests spend a lot of time (100s of microseconds)
> moving from one stage (ThreadPoolExecutor) to another.  Each stage is
> based on an unbounded LinkedBlockingQueue.  Typically each stage has
> multiple producers and consumers, although for some (such as for
> network traffic to a specific peer) there is only a single consumer.
>
> I tried updating Cassandra to use LTQ [1] instead using the
> jsr166y.jar for JDK 1.6 and saw performance go down by over 15% on
> both reads and writes [2].  Am I correct that this is unexpected?  How
> could I proceed with troubleshooting?
>
> [1] https://github.com/jbellis/cassandra/branches/ltq
> [2] https://issues.apache.org/jira/browse/CASSANDRA-4718
>
> --
> Jonathan Ellis
> Project Chair, Apache Cassandra
> co-founder, http://www.datastax.com
> @spyced
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From pyiapa at gmail.com  Sat Dec 15 05:49:26 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Sat, 15 Dec 2012 10:49:26 +0000
Subject: [concurrency-interest] Memory Alignment in Java (Vitaly
	Davidovich)
In-Reply-To: <50CB6D35.4040401@oracle.com>
References: <CANbRUcjjw3QXcqd+Wd-AwsA8nNtUg=iaTLZR=uG=FXJEO=sB-w@mail.gmail.com>
	<CAPUTfNkV+LiGcwLgngUkX+q6=gajTeCWHZrHP_mfdVbPQymv7g@mail.gmail.com>
	<50CB1056.8010909@oracle.com>
	<CAPUTfNm93C2y-Lrkc0=71w6htSKU-vp1GgSrcBGC8YhSqQn5mw@mail.gmail.com>
	<50CB6D35.4040401@oracle.com>
Message-ID: <CAPUTfNkPcqyyHLqazP6Hv012NqDL6OLndiUBNGwJngvSbzXW-w@mail.gmail.com>

Thanks for the correction.

On Fri, Dec 14, 2012 at 6:17 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

>  The guarantee isn't because the cache line flushes.  It is guaranteed
> because the processor will load or store only 2 bytes... if the compiler
> uses the 2-byte mov instruction.
>
> In Java, you have no control over the layout of the fields.  However, if
> JIT uses the 2-byte mov instruction then you won't see any word tearing or
> corruption of nearby fields.
>
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 12/14/2012 4:53 AM, Paris Yiapanis wrote:
>
> Not exactly. According to intel x86 specification if you place four 16-bit
> subwords inside a 64-bit aligned word, then an access to each of those
> 16-bit subwords is guaranteed to be atomic. I think (maybe I am wrong) the
> reason is because such an access to one of those subwords will cause the
> entire cache line to flush.
> So how about in Java? If you put together four consecutive Short
> primitives (that are 16-bits each) will the same scenario occur? Not
> volatile shorts, just plain shorts.
> But if I understand correct from the discussion, this is guaranteed for
> C/C++ that you can specify explicitly the alignment but it is not
> guaranteed for Java.
>
> - Paris
>
> On Fri, Dec 14, 2012 at 11:41 AM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  What are you trying to do?
>>
>> Declare the 16-bit quantities volatile, and you get atomic access to them
>> "without a CAS". (Not even sure how would CAS help otherwise)
>>
>> Alex
>>
>> On 13/12/2012 17:00, Paris Yiapanis wrote:
>>
>>
>> Dave,
>>
>> When you say 'the placement mechanism is supposed to ensure natural
>> alignment' I presume that it is not enforced.
>> Basically, I read a paper where they align four 16-bit subwords inside a
>> 64-bit aligned word in order to guarantee atomic access (to each of those
>> 16-bit subwords) without using CAS instructions (on an intel x86 machine).
>> However that was implemented in C++.
>> I was wondering if I could do the same with Java?
>>
>>
>> On Thu, Dec 13, 2012 at 3:32 PM, David Dice <david.dice at gmail.com> wrote:
>>
>>>
>>> On x86, hotspot ensures all objects start on a 8-byte aligned address.
>>>  Furthermore the placement mechanism is supposed to ensure "natural"
>>> alignment for fields.   An 8-byte volatile long should have an offset
>>> that's a multiple of 8, and given the object alignment it should then
>>> should have an virtual address that's a multiple of 8.   Natural alignment
>>> is best for performance and also ensures you don't have situations where a
>>> volatile long might split a cache line.   Loads that split a line won't
>>> necessarily be atomic.   Atomic instructions that split a line _will be
>>> atomic, but that's handled through a rather bizarre legacy mechanism.  It's
>>> best not to depend on this in the future.   Modern Intel implementations
>>> basically quiesce the whole system to make the operation atomic.    It's
>>> anything but a local operation and the whole system suffers.  There's some
>>> discussion on the technique buried in the following :
>>>
>>>   https://blogs.oracle.com/dave/entry/qpi_quiescence
>>>
>>>  Regards, -Dave
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121215/7ae6f0a1/attachment.html>

From dl at cs.oswego.edu  Sat Dec 15 15:28:11 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 15 Dec 2012 15:28:11 -0500
Subject: [concurrency-interest] jsr166e updates
In-Reply-To: <50CA4245.30403@cs.oswego.edu>
References: <50CA4245.30403@cs.oswego.edu>
Message-ID: <50CCDD5B.10809@cs.oswego.edu>

On 12/13/12 16:01, Doug Lea wrote:
> Classes in the jsr166e package probably will not change much
> in the near future.

... I might have said this too soon :-)
Hopefully among the last API changes for ConcurrentHashMapV8
is that the bulk operations now have "InParallel" or
"Sequentially" as parts of their names. This helps
isolate from potential upcoming JDK incompatibilities.
Sorry to those of you using these methods that placement and
naming have changed several times now.

See updates in the usual places:

API specs: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/

jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar (compiled using 
Java7 javac).

Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/

-Doug


From thundza at gmail.com  Sun Dec 16 10:44:23 2012
From: thundza at gmail.com (Devin Smith)
Date: Sun, 16 Dec 2012 09:44:23 -0600
Subject: [concurrency-interest] Another volatile bug?
Message-ID: <CAHnhnEOa4+T=_bu68VNrOGiy9OL6-CXBEO+Ag0EEhsKc4m56dA@mail.gmail.com>

Seems like there may be another bug similar to
http://cs.oswego.edu/pipermail/concurrency-interest/2012-May/009440.html

I'm not very versed w/ the JMM, but I suspect that my program below should
exit cleanly?

I looked at the compiled code, and it seems that the compiler is
reorganizing:

localGuard1 = guard1;
localData = data;
localGuard2 = guard2;

to
localGuard1 = guard1;
localGuard2 = guard2;
localData = data;

Is this valid given the JMM?

Triggered w/ -server or -client, but not java.compiler=none or -Xint using:
java version "1.7.0_10"
Java(TM) SE Runtime Environment (build 1.7.0_10-b18)
Java HotSpot(TM) 64-Bit Server VM (build 23.6-b04, mixed mode)

package com.research.dsmith.volatiles;

/**
 * User: dsmith
 * Date: 12/13/12
 */
public class VolatileBug {
    volatile int guard1 = 0;
    int data = 0;
    volatile int guard2 = 0;

    public void increment() {
        guard2++;
        data++;
        guard1++;
    }

    public boolean isValid() {
        int localGuard1;
        int localData;
        int localGuard2;
        do {
            localGuard1 = guard1;
            localData = data;
            localGuard2 = guard2;
        } while (localGuard1 != localGuard2);
        return localGuard1 == localData;
    }

    public static void main(String[] args) throws InterruptedException {
        final long testTime = 5000; //Long.parseLong(args[0]);
        final VolatileBug bug = new VolatileBug();
        final Thread reader = new Thread(new Runnable() {
            @Override
            public void run() {
                while (bug.isValid()) {}
                System.out.println("BUG HIT");
                System.exit(1);
            }
        });
        final Thread writer = new Thread(new Runnable() {
            @Override
            public void run() {
                while (true) {
                    bug.increment();
                }
            }
        });
        reader.start();
        writer.start();
        reader.join(testTime);
        System.exit(0);
    }
}


Note: I believe the comments are slightly off below, I think %r10d is
guard1, %r8d is guard2, and %r9d is data (as verified by the cmp %r8d,%r10d
followed by cmp %r9d,%r10d). (%r10d, %r11d, seem to be correctly commented
as guard1 & guard2 though)

Decoding compiled method 0x00007f889106e910:
Code:
[Entry Point]
[Constants]
  # {method} 'isValid' '()Z' in 'com/research/dsmith/volatiles/VolatileBug'
  #           [sp+0x20]  (sp of caller)
  0x00007f889106ea40: mov    0x8(%rsi),%r10d
  0x00007f889106ea44: cmp    %r10,%rax
  0x00007f889106ea47: jne    0x00007f8891037a60  ;   {runtime_call}
  0x00007f889106ea4d: xchg   %ax,%ax
[Verified Entry Point]
  0x00007f889106ea50: sub    $0x18,%rsp
  0x00007f889106ea57: mov    %rbp,0x10(%rsp)    ;*synchronization entry
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at -1 (line 23)
  0x00007f889106ea5c: mov    0xc(%rsi),%r10d    ;*getfield guard1
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 1 (line 23)
  0x00007f889106ea60: mov    0x14(%rsi),%r8d
  0x00007f889106ea64: mov    0x10(%rsi),%r9d    ;*getfield guard2
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 11 (line 25)
  0x00007f889106ea68: cmp    %r8d,%r10d
  0x00007f889106ea6b: je     0x00007f889106ea87  ;*if_icmpne
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 17 (line 26)
  0x00007f889106ea6d: xchg   %ax,%ax            ;*aload_0
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 0 (line 23)
  0x00007f889106ea70: mov    0xc(%rsi),%r10d    ;*getfield guard1
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 1 (line 23)
  0x00007f889106ea74: mov    0x14(%rsi),%r11d   ;*getfield guard2
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 11 (line 25)
  0x00007f889106ea78: mov    0x10(%rsi),%r9d    ; OopMap{rsi=Oop off=60}
                                                ;*if_icmpne
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 17 (line 26)
  0x00007f889106ea7c: test   %eax,0xab2457e(%rip)        #
0x00007f889bb93000
                                                ;   {poll}
  0x00007f889106ea82: cmp    %r11d,%r10d
  0x00007f889106ea85: jne    0x00007f889106ea70  ;*iload_1
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 20 (line 27)
  0x00007f889106ea87: cmp    %r9d,%r10d
  0x00007f889106ea8a: jne    0x00007f889106ea9d  ;*if_icmpne
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 22 (line 27)
  0x00007f889106ea8c: mov    $0x1,%eax          ;*ireturn
                                                ; -
com.research.dsmith.volatiles.VolatileBug::isValid at 30 (line 27)
  0x00007f889106ea91: add    $0x10,%rsp
  0x00007f889106ea95: pop    %rbp
  0x00007f889106ea96: test   %eax,0xab24564(%rip)        #
0x00007f889bb93000
                                                ;   {poll_return}
  0x00007f889106ea9c: retq
  0x00007f889106ea9d: xor    %eax,%eax
  0x00007f889106ea9f: jmp    0x00007f889106ea91
  0x00007f889106eaa1: hlt
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121216/dd20932e/attachment.html>

From jan at kotek.net  Sun Dec 16 11:02:23 2012
From: jan at kotek.net (Jan Kotek)
Date: Sun, 16 Dec 2012 16:02:23 +0000
Subject: [concurrency-interest] Stress tests for ConcurrentMaps?
Message-ID: <20121216160223.c5087b14356624864cb41651@kotek.net>

Hi,

I am writing an implementation of Map backed up by disk storage. I already have pretty decent implementation of ConcurrentNavigableMap (B-Linked-Tree) and  ConcurrentMap (segmented HTree) . However it is not ready yet, it needs  more documentation and polishing [1].

I need to tests my implementations. I took basic unit tests from Google Collections and Apache Harmony. 
But I have troubles finding tests which could actually verify thread safety of ConcurrentMaps. Something which would start several threads and hammer Map with random data for a week.

Is there some stress test suite for concurrent collections?

Regards,
Jan Kotek


[1] http://www.github.com/jankotek/mapdb

From zhong.j.yu at gmail.com  Sun Dec 16 11:52:54 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sun, 16 Dec 2012 10:52:54 -0600
Subject: [concurrency-interest] Another volatile bug?
In-Reply-To: <CAHnhnEOa4+T=_bu68VNrOGiy9OL6-CXBEO+Ag0EEhsKc4m56dA@mail.gmail.com>
References: <CAHnhnEOa4+T=_bu68VNrOGiy9OL6-CXBEO+Ag0EEhsKc4m56dA@mail.gmail.com>
Message-ID: <CACuKZqGua3Mt93xGpJv_SF0neN1Bz0_nM7+wgf-8tctKVr8JtQ@mail.gmail.com>

I don't think there's a bug here; the execution is correct per JMM.

Suppose write_i(guard1) synchronizes with read_j(guard1), we have
write_i(data) happens-before read_j(data). However there's no
constraints between read_j(data) and write_i+1(data); JMM does not
prevent read_j from observing write_i+1, write_i+2 etc. JMM does
forbid read_j to observe write_i-1, write-i-2 etc. So we are only
guaranteed that `localGuard1<=localData`.

The execution is not sequentially consistent, since the program
contains data races.

Zhong Yu

On Sun, Dec 16, 2012 at 9:44 AM, Devin Smith <thundza at gmail.com> wrote:
> Seems like there may be another bug similar to
> http://cs.oswego.edu/pipermail/concurrency-interest/2012-May/009440.html
>
> I'm not very versed w/ the JMM, but I suspect that my program below should
> exit cleanly?
>
> I looked at the compiled code, and it seems that the compiler is
> reorganizing:
>
> localGuard1 = guard1;
> localData = data;
> localGuard2 = guard2;
>
> to
> localGuard1 = guard1;
> localGuard2 = guard2;
> localData = data;
>
> Is this valid given the JMM?
>
> Triggered w/ -server or -client, but not java.compiler=none or -Xint using:
> java version "1.7.0_10"
> Java(TM) SE Runtime Environment (build 1.7.0_10-b18)
> Java HotSpot(TM) 64-Bit Server VM (build 23.6-b04, mixed mode)
>
> package com.research.dsmith.volatiles;
>
> /**
>  * User: dsmith
>  * Date: 12/13/12
>  */
> public class VolatileBug {
>     volatile int guard1 = 0;
>     int data = 0;
>     volatile int guard2 = 0;
>
>     public void increment() {
>         guard2++;
>         data++;
>         guard1++;
>     }
>
>     public boolean isValid() {
>         int localGuard1;
>         int localData;
>         int localGuard2;
>         do {
>             localGuard1 = guard1;
>             localData = data;
>             localGuard2 = guard2;
>         } while (localGuard1 != localGuard2);
>         return localGuard1 == localData;
>     }
>
>     public static void main(String[] args) throws InterruptedException {
>         final long testTime = 5000; //Long.parseLong(args[0]);
>         final VolatileBug bug = new VolatileBug();
>         final Thread reader = new Thread(new Runnable() {
>             @Override
>             public void run() {
>                 while (bug.isValid()) {}
>                 System.out.println("BUG HIT");
>                 System.exit(1);
>             }
>         });
>         final Thread writer = new Thread(new Runnable() {
>             @Override
>             public void run() {
>                 while (true) {
>                     bug.increment();
>                 }
>             }
>         });
>         reader.start();
>         writer.start();
>         reader.join(testTime);
>         System.exit(0);
>     }
> }
>
>
> Note: I believe the comments are slightly off below, I think %r10d is
> guard1, %r8d is guard2, and %r9d is data (as verified by the cmp %r8d,%r10d
> followed by cmp %r9d,%r10d). (%r10d, %r11d, seem to be correctly commented
> as guard1 & guard2 though)
>
> Decoding compiled method 0x00007f889106e910:
> Code:
> [Entry Point]
> [Constants]
>   # {method} 'isValid' '()Z' in 'com/research/dsmith/volatiles/VolatileBug'
>   #           [sp+0x20]  (sp of caller)
>   0x00007f889106ea40: mov    0x8(%rsi),%r10d
>   0x00007f889106ea44: cmp    %r10,%rax
>   0x00007f889106ea47: jne    0x00007f8891037a60  ;   {runtime_call}
>   0x00007f889106ea4d: xchg   %ax,%ax
> [Verified Entry Point]
>   0x00007f889106ea50: sub    $0x18,%rsp
>   0x00007f889106ea57: mov    %rbp,0x10(%rsp)    ;*synchronization entry
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at -1 (line 23)
>   0x00007f889106ea5c: mov    0xc(%rsi),%r10d    ;*getfield guard1
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 1 (line 23)
>   0x00007f889106ea60: mov    0x14(%rsi),%r8d
>   0x00007f889106ea64: mov    0x10(%rsi),%r9d    ;*getfield guard2
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 11 (line 25)
>   0x00007f889106ea68: cmp    %r8d,%r10d
>   0x00007f889106ea6b: je     0x00007f889106ea87  ;*if_icmpne
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 17 (line 26)
>   0x00007f889106ea6d: xchg   %ax,%ax            ;*aload_0
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 0 (line 23)
>   0x00007f889106ea70: mov    0xc(%rsi),%r10d    ;*getfield guard1
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 1 (line 23)
>   0x00007f889106ea74: mov    0x14(%rsi),%r11d   ;*getfield guard2
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 11 (line 25)
>   0x00007f889106ea78: mov    0x10(%rsi),%r9d    ; OopMap{rsi=Oop off=60}
>                                                 ;*if_icmpne
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 17 (line 26)
>   0x00007f889106ea7c: test   %eax,0xab2457e(%rip)        #
> 0x00007f889bb93000
>                                                 ;   {poll}
>   0x00007f889106ea82: cmp    %r11d,%r10d
>   0x00007f889106ea85: jne    0x00007f889106ea70  ;*iload_1
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 20 (line 27)
>   0x00007f889106ea87: cmp    %r9d,%r10d
>   0x00007f889106ea8a: jne    0x00007f889106ea9d  ;*if_icmpne
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 22 (line 27)
>   0x00007f889106ea8c: mov    $0x1,%eax          ;*ireturn
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 30 (line 27)
>   0x00007f889106ea91: add    $0x10,%rsp
>   0x00007f889106ea95: pop    %rbp
>   0x00007f889106ea96: test   %eax,0xab24564(%rip)        #
> 0x00007f889bb93000
>                                                 ;   {poll_return}
>   0x00007f889106ea9c: retq
>   0x00007f889106ea9d: xor    %eax,%eax
>   0x00007f889106ea9f: jmp    0x00007f889106ea91
>   0x00007f889106eaa1: hlt
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From aleksey.shipilev at oracle.com  Sun Dec 16 12:08:48 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sun, 16 Dec 2012 21:08:48 +0400
Subject: [concurrency-interest] Another volatile bug?
In-Reply-To: <CAHnhnEOa4+T=_bu68VNrOGiy9OL6-CXBEO+Ag0EEhsKc4m56dA@mail.gmail.com>
References: <CAHnhnEOa4+T=_bu68VNrOGiy9OL6-CXBEO+Ag0EEhsKc4m56dA@mail.gmail.com>
Message-ID: <50CE0020.7090809@oracle.com>

I don't think this is a bug. Nobody guarantees you can't actually see
the updated $data before seeing the updates for $localGuard1 and
$localGuard2. So then this execution is valid under JMM:

 Thread 1:                    Thread 2:
--------------------------------------------
 read(guard2 = 0)
 write(guard2 = 1)
 read(data = 0)
 write(data = 1)
                              read(guard1 = 0)
 read(guard1 = 0)
                              read(data = 1)
 write(guard1 = 1)
                              read(guard2 = 0) // valid to read 0 here

This is the apparent break of sequential consistency, but still valid
under JMM.

In other words, JMM can only make sure that once you _see_ the update to
$localGuard1, you are guaranteed to see the corresponding update to
$data. In short, if you count the end states, like this [1]:

    @Override
    public void observe(State state, int[] result) {
        result[0] = state.guard1;
        result[1] = state.data;
        result[2] = state.guard2;
    }

...then you end up with these states:

Running org.openjdk.concurrent.torture.tests.volatiles.DoubleVolatileTest
Iterations .....
            Observed state  Occurrences          Expectation
                 [0, 0, 0] (  14490200)           ACCEPTABLE
                 [1, 0, 0] (         0)            FORBIDDEN
                 [1, 0, 1] (         0)            FORBIDDEN
                 [0, 0, 1] (     18285)           ACCEPTABLE
                 [0, 1, 0] (     10456)           ACCEPTABLE
                 [0, 1, 1] (    227246)           ACCEPTABLE
                 [1, 1, 1] (  47733883)           ACCEPTABLE

-Aleksey.

[1]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/volatiles/DoubleVolatileTest.java

On 12/16/2012 07:44 PM, Devin Smith wrote:
> Seems like there may be another bug similar to
> http://cs.oswego.edu/pipermail/concurrency-interest/2012-May/009440.html
> 
> I'm not very versed w/ the JMM, but I suspect that my program below
> should exit cleanly?
> 
> I looked at the compiled code, and it seems that the compiler is
> reorganizing:
> 
> localGuard1 = guard1;
> localData = data;
> localGuard2 = guard2;
> 
> to
> localGuard1 = guard1;
> localGuard2 = guard2;
> localData = data;
> 
> Is this valid given the JMM?
> 
> Triggered w/ -server or -client, but not java.compiler=none or -Xint using:
> java version "1.7.0_10"
> Java(TM) SE Runtime Environment (build 1.7.0_10-b18)
> Java HotSpot(TM) 64-Bit Server VM (build 23.6-b04, mixed mode)
> 
> package com.research.dsmith.volatiles;
> 
> /**
>  * User: dsmith
>  * Date: 12/13/12
>  */
> public class VolatileBug {
>     volatile int guard1 = 0;
>     int data = 0;
>     volatile int guard2 = 0;
> 
>     public void increment() {
>         guard2++;
>         data++;
>         guard1++;
>     }
> 
>     public boolean isValid() {
>         int localGuard1;
>         int localData;
>         int localGuard2;
>         do {
>             localGuard1 = guard1;
>             localData = data;
>             localGuard2 = guard2;
>         } while (localGuard1 != localGuard2);
>         return localGuard1 == localData;
>     }
>     
>     public static void main(String[] args) throws InterruptedException {
>         final long testTime = 5000; //Long.parseLong(args[0]);
>         final VolatileBug bug = new VolatileBug();
>         final Thread reader = new Thread(new Runnable() {
>             @Override
>             public void run() {
>                 while (bug.isValid()) {}
>                 System.out.println("BUG HIT");
>                 System.exit(1);
>             }
>         });
>         final Thread writer = new Thread(new Runnable() {
>             @Override
>             public void run() {
>                 while (true) {
>                     bug.increment();
>                 }
>             }
>         });
>         reader.start();
>         writer.start();
>         reader.join(testTime);
>         System.exit(0);
>     }
> }
> 
> 
> Note: I believe the comments are slightly off below, I think %r10d is
> guard1, %r8d is guard2, and %r9d is data (as verified by the cmp
> %r8d,%r10d followed by cmp %r9d,%r10d). (%r10d, %r11d, seem to be
> correctly commented as guard1 & guard2 though)
> 
> Decoding compiled method 0x00007f889106e910:
> Code:
> [Entry Point]
> [Constants]
>   # {method} 'isValid' '()Z' in 'com/research/dsmith/volatiles/VolatileBug'
>   #           [sp+0x20]  (sp of caller)
>   0x00007f889106ea40: mov    0x8(%rsi),%r10d
>   0x00007f889106ea44: cmp    %r10,%rax
>   0x00007f889106ea47: jne    0x00007f8891037a60  ;   {runtime_call}
>   0x00007f889106ea4d: xchg   %ax,%ax
> [Verified Entry Point]
>   0x00007f889106ea50: sub    $0x18,%rsp
>   0x00007f889106ea57: mov    %rbp,0x10(%rsp)    ;*synchronization entry
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at -1 (line 23)
>   0x00007f889106ea5c: mov    0xc(%rsi),%r10d    ;*getfield guard1
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 1 (line 23)
>   0x00007f889106ea60: mov    0x14(%rsi),%r8d
>   0x00007f889106ea64: mov    0x10(%rsi),%r9d    ;*getfield guard2
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 11 (line 25)
>   0x00007f889106ea68: cmp    %r8d,%r10d
>   0x00007f889106ea6b: je     0x00007f889106ea87  ;*if_icmpne
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 17 (line 26)
>   0x00007f889106ea6d: xchg   %ax,%ax            ;*aload_0
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 0 (line 23)
>   0x00007f889106ea70: mov    0xc(%rsi),%r10d    ;*getfield guard1
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 1 (line 23)
>   0x00007f889106ea74: mov    0x14(%rsi),%r11d   ;*getfield guard2
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 11 (line 25)
>   0x00007f889106ea78: mov    0x10(%rsi),%r9d    ; OopMap{rsi=Oop off=60}
>                                                 ;*if_icmpne
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 17 (line 26)
>   0x00007f889106ea7c: test   %eax,0xab2457e(%rip)        #
> 0x00007f889bb93000
>                                                 ;   {poll}
>   0x00007f889106ea82: cmp    %r11d,%r10d
>   0x00007f889106ea85: jne    0x00007f889106ea70  ;*iload_1
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 20 (line 27)
>   0x00007f889106ea87: cmp    %r9d,%r10d
>   0x00007f889106ea8a: jne    0x00007f889106ea9d  ;*if_icmpne
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 22 (line 27)
>   0x00007f889106ea8c: mov    $0x1,%eax          ;*ireturn
>                                                 ; -
> com.research.dsmith.volatiles.VolatileBug::isValid at 30 (line 27)
>   0x00007f889106ea91: add    $0x10,%rsp
>   0x00007f889106ea95: pop    %rbp
>   0x00007f889106ea96: test   %eax,0xab24564(%rip)        #
> 0x00007f889bb93000
>                                                 ;   {poll_return}
>   0x00007f889106ea9c: retq   
>   0x00007f889106ea9d: xor    %eax,%eax
>   0x00007f889106ea9f: jmp    0x00007f889106ea91
>   0x00007f889106eaa1: hlt
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From aleksey.shipilev at oracle.com  Sun Dec 16 12:19:07 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sun, 16 Dec 2012 21:19:07 +0400
Subject: [concurrency-interest] Another volatile bug?
In-Reply-To: <CAHnhnEOa4+T=_bu68VNrOGiy9OL6-CXBEO+Ag0EEhsKc4m56dA@mail.gmail.com>
References: <CAHnhnEOa4+T=_bu68VNrOGiy9OL6-CXBEO+Ag0EEhsKc4m56dA@mail.gmail.com>
Message-ID: <50CE028B.2030502@oracle.com>

On 12/16/2012 07:44 PM, Devin Smith wrote:
> I looked at the compiled code, and it seems that the compiler is
> reorganizing:
> 
> localGuard1 = guard1;
> localData = data;
> localGuard2 = guard2;
> 
> to
> localGuard1 = guard1;
> localGuard2 = guard2;
> localData = data;
> 
> Is this valid given the JMM?

...and answering this particular question, yes, this is a valid compiler
transform: you can push the non-volatile read after the volatile read
without breaking JMM, see "roach motel ordering", or [1]

-Aleksey.

[1] http://g.oswego.edu/dl/jmm/cookbook.html, "Reorderings" / "Volatiles
and Monitors", 1st op = Normal Load, 2nd op = Volatile Load.


From aleksey.shipilev at oracle.com  Sun Dec 16 12:34:19 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sun, 16 Dec 2012 21:34:19 +0400
Subject: [concurrency-interest] Stress tests for ConcurrentMaps?
In-Reply-To: <20121216160223.c5087b14356624864cb41651@kotek.net>
References: <20121216160223.c5087b14356624864cb41651@kotek.net>
Message-ID: <50CE061B.6070505@oracle.com>

On 12/16/2012 08:02 PM, Jan Kotek wrote:
> Is there some stress test suite for concurrent collections?

Two things pops into my mind, first one is Doug's tests, which contain
many TCK tests against CHM and friends:
  http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/tck/

...the other (shamelessly promoted) option would be extending
concurrency-torture suite to test ConcurrentMaps, possibly easier to
contribute the tests against 3rd party components:
  https://github.com/shipilev/java-concurrency-torture

-Aleksey.

From dl at cs.oswego.edu  Sun Dec 16 15:21:54 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 16 Dec 2012 15:21:54 -0500
Subject: [concurrency-interest] Another volatile bug?
In-Reply-To: <CAHnhnEOa4+T=_bu68VNrOGiy9OL6-CXBEO+Ag0EEhsKc4m56dA@mail.gmail.com>
References: <CAHnhnEOa4+T=_bu68VNrOGiy9OL6-CXBEO+Ag0EEhsKc4m56dA@mail.gmail.com>
Message-ID: <50CE2D62.1000704@cs.oswego.edu>

On 12/16/12 10:44, Devin Smith wrote:

> I'm not very versed w/ the JMM, but I suspect that my program below should exit
> cleanly?
>
> I looked at the compiled code, and it seems that the compiler is reorganizing:
>
> localGuard1 = guard1;
> localData = data;
> localGuard2 = guard2;
>
> to
> localGuard1 = guard1;
> localGuard2 = guard2;
> localData = data;
>
> Is this valid given the JMM?

Yes it is valid. See archives for discussions surrounding
SequenceLocks, that rely on a similar reordering
constraint. Or Hans's nice MSCP paper:
http://www.hpl.hp.com/techreports/2012/HPL-2012-68.html

Hopefully we will have access to explicit fences someday
so that there will be some advertised way to get this effect
when you need it. (Until then there are only some
JVM-specific hacks.)

-Doug



From dl at cs.oswego.edu  Sun Dec 16 15:24:16 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 16 Dec 2012 15:24:16 -0500
Subject: [concurrency-interest] Stress tests for ConcurrentMaps?
In-Reply-To: <50CE061B.6070505@oracle.com>
References: <20121216160223.c5087b14356624864cb41651@kotek.net>
	<50CE061B.6070505@oracle.com>
Message-ID: <50CE2DF0.2060705@cs.oswego.edu>

On 12/16/12 12:34, Aleksey Shipilev wrote:
> On 12/16/2012 08:02 PM, Jan Kotek wrote:
>> Is there some stress test suite for concurrent collections?
>
> Two things pops into my mind, first one is Doug's tests, which contain
> many TCK tests against CHM and friends:
>    http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/tck/

And others mainly geared toward performance checks but with
some embedded correctness checks in our "loops" CVS
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/

-Doug



From jan at kotek.net  Sun Dec 16 15:34:15 2012
From: jan at kotek.net (Jan Kotek)
Date: Sun, 16 Dec 2012 20:34:15 +0000
Subject: [concurrency-interest] Stress tests for ConcurrentMaps?
In-Reply-To: <50CE2DF0.2060705@cs.oswego.edu>
References: <20121216160223.c5087b14356624864cb41651@kotek.net>
	<50CE061B.6070505@oracle.com> <50CE2DF0.2060705@cs.oswego.edu>
Message-ID: <20121216203415.2804821d5bc4ce9e3fa2686e@kotek.net>

> Doug's tests, which contain many TCK tests against CHM and friends
I already use (and pass) those. Most of them are part of Apache Harmony.



> extending concurrency-torture suite to test ConcurrentMaps,

That project looks very good, and it would make good sense to contribute.
However I preffer to write my testing code in Kotlin (new JVM language), it is way more faster.
So question is would you accept Kotlin code?



> And others mainly geared toward performance checks but with
> some embedded correctness checks in our "loops" CVS
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/

Not exactly what I had on mind. But it will do as starting point.

Thanks,
Jan Kotek

From aleksey.shipilev at oracle.com  Sun Dec 16 16:06:06 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Mon, 17 Dec 2012 01:06:06 +0400
Subject: [concurrency-interest] Stress tests for ConcurrentMaps?
In-Reply-To: <20121216203415.2804821d5bc4ce9e3fa2686e@kotek.net>
References: <20121216160223.c5087b14356624864cb41651@kotek.net>
	<50CE061B.6070505@oracle.com> <50CE2DF0.2060705@cs.oswego.edu>
	<20121216203415.2804821d5bc4ce9e3fa2686e@kotek.net>
Message-ID: <2920201C-7658-47F4-B4D5-F034D8800519@oracle.com>



On 17.12.2012, at 0:34, Jan Kotek <jan at kotek.net> wrote:
>> extending concurrency-torture suite to test ConcurrentMaps,
> 
> That project looks very good, and it would make good sense to contribute.
> However I preffer to write my testing code in Kotlin (new JVM language), it is way more faster.
> So question is would you accept Kotlin code?

I think I'll have to in the nearest future. This seems to be the nice way to try out JVM lang interop. Maven plugins are there, lang interop seems simple enough, should work. Can you draft up the simple test and send it back?

-Aleksey.

From jan at kotek.net  Sun Dec 16 16:52:29 2012
From: jan at kotek.net (Jan Kotek)
Date: Sun, 16 Dec 2012 21:52:29 +0000
Subject: [concurrency-interest] Stress tests for ConcurrentMaps?
In-Reply-To: <2920201C-7658-47F4-B4D5-F034D8800519@oracle.com>
References: <20121216160223.c5087b14356624864cb41651@kotek.net>
	<50CE061B.6070505@oracle.com> <50CE2DF0.2060705@cs.oswego.edu>
	<20121216203415.2804821d5bc4ce9e3fa2686e@kotek.net>
	<2920201C-7658-47F4-B4D5-F034D8800519@oracle.com>
Message-ID: <20121216215229.f7d8746b56778219b90294c8@kotek.net>

I will write it as part of my project (MapDB) and merge it to yours when it will be finished. 
Right now I need to stabilise and verify my code and it is good oportunity to write such test.
I will send links when I will have first usable version.

j.

On Mon, 17 Dec 2012 01:06:06 +0400
Aleksey Shipilev <aleksey.shipilev at oracle.com> wrote:

> 
> 
> On 17.12.2012, at 0:34, Jan Kotek <jan at kotek.net> wrote:
> >> extending concurrency-torture suite to test ConcurrentMaps,
> > 
> > That project looks very good, and it would make good sense to contribute.
> > However I preffer to write my testing code in Kotlin (new JVM language), it is way more faster.
> > So question is would you accept Kotlin code?
> 
> I think I'll have to in the nearest future. This seems to be the nice way to try out JVM lang interop. Maven plugins are there, lang interop seems simple enough, should work. Can you draft up the simple test and send it back?
> 
> -Aleksey.

-- 
Jan Kotek <jan at kotek.net>

From agoncharuk at gridgain.com  Tue Dec 18 03:57:52 2012
From: agoncharuk at gridgain.com (Alexey Goncharuk)
Date: Tue, 18 Dec 2012 12:57:52 +0400
Subject: [concurrency-interest] Concurrent copy to array
Message-ID: <CANbUDgX4L=rOXjkw3gp0QBymsuSPbiQb6vcuOrTJ_FXyVxo0XA@mail.gmail.com>

Hi,

Bounced by a recent discussion on memory alignment in java and atomic reads
and writes, I have the following question:

Is it guaranteed to have consistent write result if two threads are writing
to disjoint adjacent parts of some array? In particular, how would simple
index-based access, System.arraycopy() and Unsafe.copyMemory() (which is
utilized by DirectByteBuffer) interfere?

Thanks,
-- 
Alexey Goncharuk
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/f32deca0/attachment.html>

From aleksey.shipilev at oracle.com  Tue Dec 18 04:12:51 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 18 Dec 2012 13:12:51 +0400
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <CANbUDgX4L=rOXjkw3gp0QBymsuSPbiQb6vcuOrTJ_FXyVxo0XA@mail.gmail.com>
References: <CANbUDgX4L=rOXjkw3gp0QBymsuSPbiQb6vcuOrTJ_FXyVxo0XA@mail.gmail.com>
Message-ID: <50D03393.4060506@oracle.com>

On 12/18/2012 12:57 PM, Alexey Goncharuk wrote:
> Is it guaranteed to have consistent write result if two threads are
> writing to disjoint adjacent parts of some array? In particular, how
> would simple index-based access, System.arraycopy() and
> Unsafe.copyMemory() (which is utilized by DirectByteBuffer) interfere?

http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.6

tl;dr: Writing disjoint elements does not break integrity. Writing
overlapping elements is a data race, and (virtually) all bets are off.

-Aleksey.

From davidcholmes at aapt.net.au  Tue Dec 18 04:15:49 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 18 Dec 2012 19:15:49 +1000
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <CANbUDgX4L=rOXjkw3gp0QBymsuSPbiQb6vcuOrTJ_FXyVxo0XA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEALJJAA.davidcholmes@aapt.net.au>

The Java language prohibits word-tearing, so adjacent array access must not
interfere. This extends to System.arraycopy (modulo bugs of which there have
been some).

Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic - so it
too should be immune to word-tearing.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alexey
Goncharuk
  Sent: Tuesday, 18 December 2012 6:58 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Concurrent copy to array


  Hi,


  Bounced by a recent discussion on memory alignment in java and atomic
reads and writes, I have the following question:


  Is it guaranteed to have consistent write result if two threads are
writing to disjoint adjacent parts of some array? In particular, how would
simple index-based access, System.arraycopy() and Unsafe.copyMemory() (which
is utilized by DirectByteBuffer) interfere?



  Thanks,
  --
  Alexey Goncharuk

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/b2685993/attachment.html>

From nitsanw at yahoo.com  Tue Dec 18 04:39:39 2012
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Tue, 18 Dec 2012 01:39:39 -0800 (PST)
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEALJJAA.davidcholmes@aapt.net.au>
References: <CANbUDgX4L=rOXjkw3gp0QBymsuSPbiQb6vcuOrTJ_FXyVxo0XA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOEALJJAA.davidcholmes@aapt.net.au>
Message-ID: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>

What about unsafe.putByte(address,value) and unsafe.putByte(address+1,value) from 2 threads?
Also how would this interact with cross cache line writes, i.e. assume address is -4 from cache boundary and I do unsafe.putLong(address,value) and unsafe.putLong(address+8,value) from 2 threads?



________________________________
 From: David Holmes <davidcholmes at aapt.net.au>
To: Alexey Goncharuk <agoncharuk at gridgain.com>; concurrency-interest at cs.oswego.edu 
Sent: Tuesday, December 18, 2012 9:15 AM
Subject: Re: [concurrency-interest] Concurrent copy to array
 

The 
Java language prohibits word-tearing, so adjacent array access must not 
interfere. This extends to System.arraycopy (modulo bugs of which there have 
been some).
?
Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic - 
so it too should be immune to word-tearing.
?
David
-----Original Message-----
>From: concurrency-interest-bounces at cs.oswego.edu  [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alexey  Goncharuk
>Sent: Tuesday, 18 December 2012 6:58 PM
>To: concurrency-interest at cs.oswego.edu
>Subject: [concurrency-interest]  Concurrent copy to array
>
>Hi, 
>
>
>Bounced by a recent discussion on memory alignment in java and atomic  reads and writes, I have the following question:
>
>
>Is it guaranteed to have consistent write result if two threads are  writing to disjoint adjacent parts of some array? In particular, how would  simple index-based access, System.arraycopy() and Unsafe.copyMemory() (which  is utilized by DirectByteBuffer) interfere?
>
>
>
>Thanks,-- 
>Alexey  Goncharuk
>
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/96004ae1/attachment.html>

From davidcholmes at aapt.net.au  Tue Dec 18 05:03:07 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 18 Dec 2012 20:03:07 +1000
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>

Byte accesses are atomic on all supported architectures.

I don't quite understand what you mean about the cache line example. An
unaligned long access need not be atomic but it I don't think the cache line
positioning has any bearing on whether "word tearing" can occur - but I
don't know cache operation to that low-level of detail.

David
  -----Original Message-----
  From: Nitsan Wakart [mailto:nitsanw at yahoo.com]
  Sent: Tuesday, 18 December 2012 7:40 PM
  To: dholmes at ieee.org; Alexey Goncharuk; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Concurrent copy to array


  What about unsafe.putByte(address,value) and
unsafe.putByte(address+1,value) from 2 threads?
  Also how would this interact with cross cache line writes, i.e. assume
address is -4 from cache boundary and I do unsafe.putLong(address,value) and
unsafe.putLong(address+8,value) from 2 threads?





----------------------------------------------------------------------------
--
  From: David Holmes <davidcholmes at aapt.net.au>
  To: Alexey Goncharuk <agoncharuk at gridgain.com>;
concurrency-interest at cs.oswego.edu
  Sent: Tuesday, December 18, 2012 9:15 AM
  Subject: Re: [concurrency-interest] Concurrent copy to array



  The Java language prohibits word-tearing, so adjacent array access must
not interfere. This extends to System.arraycopy (modulo bugs of which there
have been some).

  Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic - so
it too should be immune to word-tearing.

  David
    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alexey
Goncharuk
    Sent: Tuesday, 18 December 2012 6:58 PM
    To: concurrency-interest at cs.oswego.edu
    Subject: [concurrency-interest] Concurrent copy to array


    Hi,


    Bounced by a recent discussion on memory alignment in java and atomic
reads and writes, I have the following question:


    Is it guaranteed to have consistent write result if two threads are
writing to disjoint adjacent parts of some array? In particular, how would
simple index-based access, System.arraycopy() and Unsafe.copyMemory() (which
is utilized by DirectByteBuffer) interfere?



    Thanks,
    --
    Alexey Goncharuk



  _______________________________________________
  Concurrency-interest mailing list
  Concurrency-interest at cs.oswego.edu
  http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/c84c54d3/attachment-0001.html>

From nitsanw at yahoo.com  Tue Dec 18 06:00:31 2012
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Tue, 18 Dec 2012 03:00:31 -0800 (PST)
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
References: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
Message-ID: <1355828431.84673.YahooMailNeo@web120704.mail.ne1.yahoo.com>

I'm wondering if the loss of atomicity will in turn cause lose of word tearing guarantee as the cache line writes are somehow separate. My expectation is that this will maitain correctness but at a price to performance.




________________________________
 From: David Holmes <davidcholmes at aapt.net.au>
To: Nitsan Wakart <nitsanw at yahoo.com>; Alexey Goncharuk <agoncharuk at gridgain.com>; concurrency-interest at cs.oswego.edu 
Sent: Tuesday, December 18, 2012 10:03 AM
Subject: RE: [concurrency-interest] Concurrent copy to array
 

Byte 
accesses are atomic on all supported architectures.
?
I 
don't quite understand what you mean about the cache line example. An unaligned 
long access need not be atomic but it I don't think the cache line positioning 
has any bearing on whether "word tearing" can occur - but I don't know cache 
operation to that low-level of detail.
?
David
-----Original Message-----
>From: Nitsan Wakart  [mailto:nitsanw at yahoo.com]
>Sent: Tuesday, 18 December 2012 7:40  PM
>To: dholmes at ieee.org; Alexey Goncharuk;  concurrency-interest at cs.oswego.edu
>Subject: Re:  [concurrency-interest] Concurrent copy to array
>
>
>What  about unsafe.putByte(address,value) and unsafe.putByte(address+1,value) from 2  threads?
>Also how would this interact with cross cache line writes, i.e. 
  assume address is -4 from cache boundary and I do 
  unsafe.putLong(address,value) and unsafe.putLong(address+8,value) from 2 
  threads? 
>
>
>
>
>
>________________________________
> From: David Holmes  <davidcholmes at aapt.net.au>
>To: Alexey Goncharuk  <agoncharuk at gridgain.com>; concurrency-interest at cs.oswego.edu 
>Sent: Tuesday, December 18,  2012 9:15 AM
>Subject: Re:  [concurrency-interest] Concurrent copy to array
>
>
>The Java language prohibits word-tearing, so adjacent array access  must not interfere. This extends to System.arraycopy (modulo bugs of which  there have been some).
>?
>Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic  - so it too should be immune to word-tearing.
>?
>David
>-----Original Message-----
>>From: concurrency-interest-bounces at cs.oswego.edu  [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alexey Goncharuk
>>Sent: Tuesday, 18 December 2012 6:58  PM
>>To: concurrency-interest at cs.oswego.edu
>>Subject: [concurrency-interest] Concurrent copy to array
>>
>>Hi, 
>>
>>
>>Bounced by a recent discussion on memory alignment in java and atomic  reads and writes, I have the following question:
>>
>>
>>Is it guaranteed to have consistent write result if two threads are  writing to disjoint adjacent parts of some array? In particular, how would  simple index-based access, System.arraycopy() and Unsafe.copyMemory() (which  is utilized by DirectByteBuffer) interfere?
>>
>>
>>
>>Thanks,-- 
>>Alexey  Goncharuk
>>
>>
>_______________________________________________
>Concurrency-interest 
  mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/76e301f5/attachment.html>

From nitsanw at yahoo.com  Tue Dec 18 06:00:33 2012
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Tue, 18 Dec 2012 03:00:33 -0800 (PST)
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
References: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
Message-ID: <1355828433.18166.YahooMailNeo@web120706.mail.ne1.yahoo.com>

I'm wondering if the loss of atomicity will in turn cause lose of word tearing guarantee as the cache line writes are somehow separate. My expectation is that this will maitain correctness but at a price to performance.




________________________________
 From: David Holmes <davidcholmes at aapt.net.au>
To: Nitsan Wakart <nitsanw at yahoo.com>; Alexey Goncharuk <agoncharuk at gridgain.com>; concurrency-interest at cs.oswego.edu 
Sent: Tuesday, December 18, 2012 10:03 AM
Subject: RE: [concurrency-interest] Concurrent copy to array
 

Byte 
accesses are atomic on all supported architectures.
?
I 
don't quite understand what you mean about the cache line example. An unaligned 
long access need not be atomic but it I don't think the cache line positioning 
has any bearing on whether "word tearing" can occur - but I don't know cache 
operation to that low-level of detail.
?
David
-----Original Message-----
>From: Nitsan Wakart  [mailto:nitsanw at yahoo.com]
>Sent: Tuesday, 18 December 2012 7:40  PM
>To: dholmes at ieee.org; Alexey Goncharuk;  concurrency-interest at cs.oswego.edu
>Subject: Re:  [concurrency-interest] Concurrent copy to array
>
>
>What  about unsafe.putByte(address,value) and unsafe.putByte(address+1,value) from 2  threads?
>Also how would this interact with cross cache line writes, i.e. 
  assume address is -4 from cache boundary and I do 
  unsafe.putLong(address,value) and unsafe.putLong(address+8,value) from 2 
  threads? 
>
>
>
>
>
>________________________________
> From: David Holmes  <davidcholmes at aapt.net.au>
>To: Alexey Goncharuk  <agoncharuk at gridgain.com>; concurrency-interest at cs.oswego.edu 
>Sent: Tuesday, December 18,  2012 9:15 AM
>Subject: Re:  [concurrency-interest] Concurrent copy to array
>
>
>The Java language prohibits word-tearing, so adjacent array access  must not interfere. This extends to System.arraycopy (modulo bugs of which  there have been some).
>?
>Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic  - so it too should be immune to word-tearing.
>?
>David
>-----Original Message-----
>>From: concurrency-interest-bounces at cs.oswego.edu  [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alexey Goncharuk
>>Sent: Tuesday, 18 December 2012 6:58  PM
>>To: concurrency-interest at cs.oswego.edu
>>Subject: [concurrency-interest] Concurrent copy to array
>>
>>Hi, 
>>
>>
>>Bounced by a recent discussion on memory alignment in java and atomic  reads and writes, I have the following question:
>>
>>
>>Is it guaranteed to have consistent write result if two threads are  writing to disjoint adjacent parts of some array? In particular, how would  simple index-based access, System.arraycopy() and Unsafe.copyMemory() (which  is utilized by DirectByteBuffer) interfere?
>>
>>
>>
>>Thanks,-- 
>>Alexey  Goncharuk
>>
>>
>_______________________________________________
>Concurrency-interest 
  mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/a39fcfed/attachment.html>

From pyiapa at gmail.com  Tue Dec 18 07:31:43 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Tue, 18 Dec 2012 12:31:43 +0000
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
References: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUTfNn2mBbNX35Rnk4p6WVQo_M9zS2wt2JO+DEH-_P=P3CQAw@mail.gmail.com>

Since you mention atomic bytes, I notice that in SPARC V9 there is a
special instruction LDSTUB for loading and storing unsigned bytes
atomically.
Is there a way using the standard Java libraries to actually generate that
instruction?

- Paris

On Tue, Dec 18, 2012 at 10:03 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> Byte accesses are atomic on all supported architectures.
>
> I don't quite understand what you mean about the cache line example. An
> unaligned long access need not be atomic but it I don't think the cache
> line positioning has any bearing on whether "word tearing" can occur - but
> I don't know cache operation to that low-level of detail.
>
> David
>
> -----Original Message-----
> *From:* Nitsan Wakart [mailto:nitsanw at yahoo.com]
> *Sent:* Tuesday, 18 December 2012 7:40 PM
> *To:* dholmes at ieee.org; Alexey Goncharuk;
> concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>
> What about unsafe.putByte(address,value) and
> unsafe.putByte(address+1,value) from 2 threads?
> Also how would this interact with cross cache line writes, i.e. assume
> address is -4 from cache boundary and I do unsafe.putLong(address,value)
> and unsafe.putLong(address+8,value) from 2 threads?
>
>
>   ------------------------------
> *From:* David Holmes <davidcholmes at aapt.net.au>
> *To:* Alexey Goncharuk <agoncharuk at gridgain.com>;
> concurrency-interest at cs.oswego.edu
> *Sent:* Tuesday, December 18, 2012 9:15 AM
> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>
>  The Java language prohibits word-tearing, so adjacent array access must
> not interfere. This extends to System.arraycopy (modulo bugs of which there
> have been some).
>
> Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic - so
> it too should be immune to word-tearing.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Alexey Goncharuk
> *Sent:* Tuesday, 18 December 2012 6:58 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Concurrent copy to array
>
> Hi,
>
> Bounced by a recent discussion on memory alignment in java and atomic
> reads and writes, I have the following question:
>
> Is it guaranteed to have consistent write result if two threads are
> writing to disjoint adjacent parts of some array? In particular, how would
> simple index-based access, System.arraycopy() and Unsafe.copyMemory()
> (which is utilized by DirectByteBuffer) interfere?
>
> Thanks,
> --
> Alexey Goncharuk
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/77b6a728/attachment-0001.html>

From aleksey.shipilev at oracle.com  Tue Dec 18 07:52:18 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 18 Dec 2012 16:52:18 +0400
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <1355828431.84673.YahooMailNeo@web120704.mail.ne1.yahoo.com>
References: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
	<1355828431.84673.YahooMailNeo@web120704.mail.ne1.yahoo.com>
Message-ID: <50D06702.1060207@oracle.com>

On 12/18/2012 03:00 PM, Nitsan Wakart wrote:
> I'm wondering if the loss of atomicity will in turn cause lose of word
> tearing guarantee as the cache line writes are somehow separate. My
> expectation is that this will maitain correctness but at a price to
> performance.

JLS concerns itself with Java language, which will force all data types
to reside on the single cache line, because of the alignment constraints
(it plays nicely since the longest datatype we have is 8 bytes long, and
the cache lines are way longer).

Unsafe is not a Java language (even though it expresses the Java
bindings for low-level concepts), hence it does not follow that
misaligned Unsafe accesses will guarantee the same degree of atomicity,
rather exposing ourselves to particular HW behavior; if x86 does not
guarantee atomicity for cross-cacheline ops, neither Unsafe does; if
SPARC SIGBUS'es on misaligned reads, so the Unsafe does; etc. etc.

It can be best articulated by this interesting test [1], which does the
Unsafe.get/putInt to/from byte array at random offsets. The test works
perfectly well with -Dalign=4:

org.openjdk.concurrent.torture.tests.unsorted.UnsafeAtomicityTest
               Observed state  Occurrences          Expectation
                 [0, 0, 0, 0] (   7447706)           ACCEPTABLE
             [-1, -1, -1, -1] (  36080104)           ACCEPTABLE

...which means we see either the full 0xFFFFFFFF write or the default
value. However, the atomicity goes out of the window on my x86 laptop
with -Dalign=1:

org.openjdk.concurrent.torture.tests.unsorted.UnsafeAtomicityTest
               Observed state  Occurrences          Expectation
                 [0, 0, 0, 0] (   7040542)           ACCEPTABLE
             [-1, -1, -1, -1] (  32473147)           ACCEPTABLE
              [0, -1, -1, -1] (      1324)            FORBIDDEN
                [-1, 0, 0, 0] (       229)            FORBIDDEN
               [0, 0, -1, -1] (      1327)            FORBIDDEN
              [-1, -1, -1, 0] (       216)            FORBIDDEN
               [-1, -1, 0, 0] (       215)            FORBIDDEN
                [0, 0, 0, -1] (      1340)            FORBIDDEN

Q.E.D.

Once you stepped on Unsafe lands, you are on your own, may God have
mercy on your soul.

-Aleksey.

[1]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/unsorted/UnsafeAtomicityTest.java

From vitalyd at gmail.com  Tue Dec 18 08:42:10 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 18 Dec 2012 08:42:10 -0500
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <1355828433.18166.YahooMailNeo@web120706.mail.ne1.yahoo.com>
References: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
	<1355828433.18166.YahooMailNeo@web120706.mail.ne1.yahoo.com>
Message-ID: <CAHjP37HaPGDbvrkLGMw8rWqANPajqe1XKhu_c-eaEBB2708FJA@mail.gmail.com>

Recall that cacheable memory is cacheline-at-a-time in the CPU.  Cache
coherence ensures only one CPU can have the line in modified state.  Recall
also that aligned access is when you read a value at an address divisible
by (I.e. aligned to) its natural size.

In the byte example, modern mainstream CPUs support byte mov instructions
so given their 1 byte size, they're always aligned.  In your case, two CPUs
writing different bytes on same line will induce a false sharing penalty,
but otherwise won't tear.

In the long example, you're not writing the long at a 8 byte divisible
address if it's -4 from a cache line boundary.  This will count as
unaligned access and we discussed on the other thread what happens here:
CPU supports unaligned access, OS does unaligned access, or sigbus.  The
former two are expensive.  Since the write straddles two lines you can
experience word tearing.  Intel manual is pretty clear that writes spanning
cache lines are not guaranteed atomic.

Sent from my phone
On Dec 18, 2012 6:04 AM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:

> I'm wondering if the loss of atomicity will in turn cause lose of word
> tearing guarantee as the cache line writes are somehow separate. My
> expectation is that this will maitain correctness but at a price to
> performance.
>
>
>   ------------------------------
> *From:* David Holmes <davidcholmes at aapt.net.au>
> *To:* Nitsan Wakart <nitsanw at yahoo.com>; Alexey Goncharuk <
> agoncharuk at gridgain.com>; concurrency-interest at cs.oswego.edu
> *Sent:* Tuesday, December 18, 2012 10:03 AM
> *Subject:* RE: [concurrency-interest] Concurrent copy to array
>
>  Byte accesses are atomic on all supported architectures.
>
> I don't quite understand what you mean about the cache line example. An
> unaligned long access need not be atomic but it I don't think the cache
> line positioning has any bearing on whether "word tearing" can occur - but
> I don't know cache operation to that low-level of detail.
>
> David
>
> -----Original Message-----
> *From:* Nitsan Wakart [mailto:nitsanw at yahoo.com]
> *Sent:* Tuesday, 18 December 2012 7:40 PM
> *To:* dholmes at ieee.org; Alexey Goncharuk;
> concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>
> What about unsafe.putByte(address,value) and
> unsafe.putByte(address+1,value) from 2 threads?
> Also how would this interact with cross cache line writes, i.e. assume
> address is -4 from cache boundary and I do unsafe.putLong(address,value)
> and unsafe.putLong(address+8,value) from 2 threads?
>
>
>   ------------------------------
> *From:* David Holmes <davidcholmes at aapt.net.au>
> *To:* Alexey Goncharuk <agoncharuk at gridgain.com>;
> concurrency-interest at cs.oswego.edu
> *Sent:* Tuesday, December 18, 2012 9:15 AM
> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>
>  The Java language prohibits word-tearing, so adjacent array access must
> not interfere. This extends to System.arraycopy (modulo bugs of which there
> have been some).
>
> Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic - so
> it too should be immune to word-tearing.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Alexey Goncharuk
> *Sent:* Tuesday, 18 December 2012 6:58 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Concurrent copy to array
>
> Hi,
>
> Bounced by a recent discussion on memory alignment in java and atomic
> reads and writes, I have the following question:
>
> Is it guaranteed to have consistent write result if two threads are
> writing to disjoint adjacent parts of some array? In particular, how would
> simple index-based access, System.arraycopy() and Unsafe.copyMemory()
> (which is utilized by DirectByteBuffer) interfere?
>
> Thanks,
> --
> Alexey Goncharuk
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/99004ce9/attachment.html>

From nathan.reynolds at oracle.com  Tue Dec 18 11:29:25 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 18 Dec 2012 09:29:25 -0700
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <CAPUTfNn2mBbNX35Rnk4p6WVQo_M9zS2wt2JO+DEH-_P=P3CQAw@mail.gmail.com>
References: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
	<CAPUTfNn2mBbNX35Rnk4p6WVQo_M9zS2wt2JO+DEH-_P=P3CQAw@mail.gmail.com>
Message-ID: <50D099E5.9050507@oracle.com>

There doesn't exist an AtomicByte class so you can't call 
AtomicByte.getAndSet().  There doesn't exist a method in Unsafe to do a 
getAndSet(byte).  So, it doesn't appear that there is a way to 
atomically load and store a byte.

As a work around, you can use the following code.  It doesn't use byte 
atomic instructions.  Instead, it uses an int atomic instruction.  The 
code ensures the update is atomic and doesn't tear or corrupt the 
surrounding values.

At first I thought this code would cause extra contention since multiple 
threads could be trying to update adjacent byte values. However, since 
the processor deals with cache lines, then there would be any additional 
contention since adjacent byte values are going to be on the same cache 
line (except in the 2 edge cases).

The ideal solution would use an atomic byte instruction.  This would get 
rid of the CAS loop and the problems associated with such.  (See 
https://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs).

    private final AtomicIntegerArray m_data = new AtomicIntegerArray(10);

    public byte getAndSet(int index, byte value)
    {
       int offset, shift, mask, set;
       int expect, update, result;

       offset = index / 4;
       shift  = 8 * (index % 4);
       mask   = 0xFF << shift;
       set    = (0x00FF & value) << shift;

       do
       {
          expect = m_data.get(offset);
          update = (expect & ~mask) | set;
       }
       while (!m_data.compareAndSet(offset, expect, update));

       result = expect >> shift;

       return((byte) result);
    }

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/18/2012 5:31 AM, Paris Yiapanis wrote:
> Since you mention atomic bytes, I notice that in SPARC V9 there is a 
> special instruction LDSTUB for loading and storing unsigned bytes 
> atomically.
> Is there a way using the standard Java libraries to actually generate 
> that instruction?
>
> - Paris
>
> On Tue, Dec 18, 2012 at 10:03 AM, David Holmes 
> <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>
>     Byte accesses are atomic on all supported architectures.
>     I don't quite understand what you mean about the cache line
>     example. An unaligned long access need not be atomic but it I
>     don't think the cache line positioning has any bearing on whether
>     "word tearing" can occur - but I don't know cache operation to
>     that low-level of detail.
>     David
>
>         -----Original Message-----
>         *From:* Nitsan Wakart [mailto:nitsanw at yahoo.com
>         <mailto:nitsanw at yahoo.com>]
>         *Sent:* Tuesday, 18 December 2012 7:40 PM
>         *To:* dholmes at ieee.org <mailto:dholmes at ieee.org>; Alexey
>         Goncharuk; concurrency-interest at cs.oswego.edu
>         <mailto:concurrency-interest at cs.oswego.edu>
>         *Subject:* Re: [concurrency-interest] Concurrent copy to array
>
>         What about unsafe.putByte(address,value) and
>         unsafe.putByte(address+1,value) from 2 threads?
>         Also how would this interact with cross cache line writes,
>         i.e. assume address is -4 from cache boundary and I do
>         unsafe.putLong(address,value) and
>         unsafe.putLong(address+8,value) from 2 threads?
>
>
>         ------------------------------------------------------------------------
>         *From:* David Holmes <davidcholmes at aapt.net.au
>         <mailto:davidcholmes at aapt.net.au>>
>         *To:* Alexey Goncharuk <agoncharuk at gridgain.com
>         <mailto:agoncharuk at gridgain.com>>;
>         concurrency-interest at cs.oswego.edu
>         <mailto:concurrency-interest at cs.oswego.edu>
>         *Sent:* Tuesday, December 18, 2012 9:15 AM
>         *Subject:* Re: [concurrency-interest] Concurrent copy to array
>
>         The Java language prohibits word-tearing, so adjacent array
>         access must not interfere. This extends to System.arraycopy
>         (modulo bugs of which there have been some).
>         Unsafe.copyMemory is implemented using
>         Copy::conjoint_memory_atomic - so it too should be immune to
>         word-tearing.
>         David
>
>             -----Original Message-----
>             *From:* concurrency-interest-bounces at cs.oswego.edu
>             <mailto:concurrency-interest-bounces at cs.oswego.edu>
>             [mailto:concurrency-interest-bounces at cs.oswego.edu
>             <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On
>             Behalf Of *Alexey Goncharuk
>             *Sent:* Tuesday, 18 December 2012 6:58 PM
>             *To:* concurrency-interest at cs.oswego.edu
>             <mailto:concurrency-interest at cs.oswego.edu>
>             *Subject:* [concurrency-interest] Concurrent copy to array
>
>             Hi,
>
>             Bounced by a recent discussion on memory alignment in java
>             and atomic reads and writes, I have the following question:
>
>             Is it guaranteed to have consistent write result if two
>             threads are writing to disjoint adjacent parts of some
>             array? In particular, how would simple index-based access,
>             System.arraycopy() and Unsafe.copyMemory() (which is
>             utilized by DirectByteBuffer) interfere?
>
>             Thanks,
>             -- 
>             Alexey Goncharuk
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/70e7913f/attachment-0001.html>

From vitalyd at gmail.com  Tue Dec 18 11:47:34 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 18 Dec 2012 11:47:34 -0500
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <50D099E5.9050507@oracle.com>
References: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
	<CAPUTfNn2mBbNX35Rnk4p6WVQo_M9zS2wt2JO+DEH-_P=P3CQAw@mail.gmail.com>
	<50D099E5.9050507@oracle.com>
Message-ID: <CAHjP37ENakJa3YL9usQOqujO-0Caa_fCbuAhd9XmOZuCULQOxA@mail.gmail.com>

I think we should also be careful to differentiate atomic CPU instructions
vs atomic memory writes.  When Intel manual, e.g., calls out what memory
writes are atomic, they're not talking about atomic/fenced/locked
instructions - they're only talking about the tearing part with respect to
crossing bus width, cache line, and page size.  That's of course quite
different from what the atomic instructions provide.

Just thought I'd mention that so nobody is confused.

Sent from my phone
On Dec 18, 2012 11:33 AM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  There doesn't exist an AtomicByte class so you can't call
> AtomicByte.getAndSet().  There doesn't exist a method in Unsafe to do a
> getAndSet(byte).  So, it doesn't appear that there is a way to atomically
> load and store a byte.
>
> As a work around, you can use the following code.  It doesn't use byte
> atomic instructions.  Instead, it uses an int atomic instruction.  The code
> ensures the update is atomic and doesn't tear or corrupt the surrounding
> values.
>
> At first I thought this code would cause extra contention since multiple
> threads could be trying to update adjacent byte values.  However, since the
> processor deals with cache lines, then there would be any additional
> contention since adjacent byte values are going to be on the same cache
> line (except in the 2 edge cases).
>
> The ideal solution would use an atomic byte instruction.  This would get
> rid of the CAS loop and the problems associated with such.  (See
> https://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs).
>
>    private final AtomicIntegerArray m_data = new AtomicIntegerArray(10);
>
>    public byte getAndSet(int index, byte value)
>    {
>       int offset, shift, mask, set;
>       int expect, update, result;
>
>       offset = index / 4;
>       shift  = 8 * (index % 4);
>       mask   = 0xFF << shift;
>       set    = (0x00FF & value) << shift;
>
>       do
>       {
>          expect = m_data.get(offset);
>          update = (expect & ~mask) | set;
>       }
>       while (!m_data.compareAndSet(offset, expect, update));
>
>       result = expect >> shift;
>
>       return((byte) result);
>    }
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
>  Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 12/18/2012 5:31 AM, Paris Yiapanis wrote:
>
> Since you mention atomic bytes, I notice that in SPARC V9 there is a
> special instruction LDSTUB for loading and storing unsigned bytes
> atomically.
> Is there a way using the standard Java libraries to actually generate that
> instruction?
>
> - Paris
>
> On Tue, Dec 18, 2012 at 10:03 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>>  Byte accesses are atomic on all supported architectures.
>>
>> I don't quite understand what you mean about the cache line example. An
>> unaligned long access need not be atomic but it I don't think the cache
>> line positioning has any bearing on whether "word tearing" can occur - but
>> I don't know cache operation to that low-level of detail.
>>
>> David
>>
>> -----Original Message-----
>> *From:* Nitsan Wakart [mailto:nitsanw at yahoo.com]
>> *Sent:* Tuesday, 18 December 2012 7:40 PM
>> *To:* dholmes at ieee.org; Alexey Goncharuk;
>> concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>>
>>  What about unsafe.putByte(address,value) and
>> unsafe.putByte(address+1,value) from 2 threads?
>> Also how would this interact with cross cache line writes, i.e. assume
>> address is -4 from cache boundary and I do unsafe.putLong(address,value)
>> and unsafe.putLong(address+8,value) from 2 threads?
>>
>>
>>   ------------------------------
>> *From:* David Holmes <davidcholmes at aapt.net.au>
>> *To:* Alexey Goncharuk <agoncharuk at gridgain.com>;
>> concurrency-interest at cs.oswego.edu
>> *Sent:* Tuesday, December 18, 2012 9:15 AM
>> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>>
>>  The Java language prohibits word-tearing, so adjacent array access must
>> not interfere. This extends to System.arraycopy (modulo bugs of which there
>> have been some).
>>
>> Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic - so
>> it too should be immune to word-tearing.
>>
>> David
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Alexey
>> Goncharuk
>> *Sent:* Tuesday, 18 December 2012 6:58 PM
>> *To:* concurrency-interest at cs.oswego.edu
>> *Subject:* [concurrency-interest] Concurrent copy to array
>>
>>  Hi,
>>
>>  Bounced by a recent discussion on memory alignment in java and atomic
>> reads and writes, I have the following question:
>>
>>  Is it guaranteed to have consistent write result if two threads are
>> writing to disjoint adjacent parts of some array? In particular, how would
>> simple index-based access, System.arraycopy() and Unsafe.copyMemory()
>> (which is utilized by DirectByteBuffer) interfere?
>>
>>  Thanks,
>> --
>> Alexey Goncharuk
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/b58d549e/attachment.html>

From davidcholmes at aapt.net.au  Tue Dec 18 15:06:42 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 19 Dec 2012 06:06:42 +1000
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <CAHjP37HaPGDbvrkLGMw8rWqANPajqe1XKhu_c-eaEBB2708FJA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEAPJJAA.davidcholmes@aapt.net.au>

Vitaly,

There is a difference between atomicity and word-tearing. The atomicity
constraint determines whether any other observer might see a partially
updated value. The word-tearing aspect concerns whether any adjacent,
sub-word, memory locations (not part of the current access) can be modified
without actually being the target of a store.

I'm surprised if this cache-line alignment issue can lead to word-tearing.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
  Sent: Tuesday, 18 December 2012 11:42 PM
  To: Nitsan Wakart
  Cc: concurrency-interest at cs.oswego.edu; dholmes at ieee.org
  Subject: Re: [concurrency-interest] Concurrent copy to array


  Recall that cacheable memory is cacheline-at-a-time in the CPU.  Cache
coherence ensures only one CPU can have the line in modified state.  Recall
also that aligned access is when you read a value at an address divisible by
(I.e. aligned to) its natural size.

  In the byte example, modern mainstream CPUs support byte mov instructions
so given their 1 byte size, they're always aligned.  In your case, two CPUs
writing different bytes on same line will induce a false sharing penalty,
but otherwise won't tear.

  In the long example, you're not writing the long at a 8 byte divisible
address if it's -4 from a cache line boundary.  This will count as unaligned
access and we discussed on the other thread what happens here: CPU supports
unaligned access, OS does unaligned access, or sigbus.  The former two are
expensive.  Since the write straddles two lines you can experience word
tearing.  Intel manual is pretty clear that writes spanning cache lines are
not guaranteed atomic.

  Sent from my phone

  On Dec 18, 2012 6:04 AM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:

    I'm wondering if the loss of atomicity will in turn cause lose of word
tearing guarantee as the cache line writes are somehow separate. My
expectation is that this will maitain correctness but at a price to
performance.






----------------------------------------------------------------------------
    From: David Holmes <davidcholmes at aapt.net.au>
    To: Nitsan Wakart <nitsanw at yahoo.com>; Alexey Goncharuk
<agoncharuk at gridgain.com>; concurrency-interest at cs.oswego.edu
    Sent: Tuesday, December 18, 2012 10:03 AM
    Subject: RE: [concurrency-interest] Concurrent copy to array



    Byte accesses are atomic on all supported architectures.

    I don't quite understand what you mean about the cache line example. An
unaligned long access need not be atomic but it I don't think the cache line
positioning has any bearing on whether "word tearing" can occur - but I
don't know cache operation to that low-level of detail.

    David
      -----Original Message-----
      From: Nitsan Wakart [mailto:nitsanw at yahoo.com]
      Sent: Tuesday, 18 December 2012 7:40 PM
      To: dholmes at ieee.org; Alexey Goncharuk;
concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Concurrent copy to array


      What about unsafe.putByte(address,value) and
unsafe.putByte(address+1,value) from 2 threads?
      Also how would this interact with cross cache line writes, i.e. assume
address is -4 from cache boundary and I do unsafe.putLong(address,value) and
unsafe.putLong(address+8,value) from 2 threads?





--------------------------------------------------------------------------
      From: David Holmes <davidcholmes at aapt.net.au>
      To: Alexey Goncharuk <agoncharuk at gridgain.com>;
concurrency-interest at cs.oswego.edu
      Sent: Tuesday, December 18, 2012 9:15 AM
      Subject: Re: [concurrency-interest] Concurrent copy to array



      The Java language prohibits word-tearing, so adjacent array access
must not interfere. This extends to System.arraycopy (modulo bugs of which
there have been some).

      Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic -
so it too should be immune to word-tearing.

      David
        -----Original Message-----
        From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alexey
Goncharuk
        Sent: Tuesday, 18 December 2012 6:58 PM
        To: concurrency-interest at cs.oswego.edu
        Subject: [concurrency-interest] Concurrent copy to array


        Hi,


        Bounced by a recent discussion on memory alignment in java and
atomic reads and writes, I have the following question:


        Is it guaranteed to have consistent write result if two threads are
writing to disjoint adjacent parts of some array? In particular, how would
simple index-based access, System.arraycopy() and Unsafe.copyMemory() (which
is utilized by DirectByteBuffer) interfere?



        Thanks,
        --
        Alexey Goncharuk



      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest







    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121219/4c50e216/attachment.html>

From davidcholmes at aapt.net.au  Tue Dec 18 15:10:20 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 19 Dec 2012 06:10:20 +1000
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <50D099E5.9050507@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEAPJJAA.davidcholmes@aapt.net.au>

All byte accesses (load/store) are atomic, no need for AtomicByte for that.

ldstub is emitted by the hotspot JIT but I don't know for what.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nathan
Reynolds
  Sent: Wednesday, 19 December 2012 2:29 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Concurrent copy to array


  There doesn't exist an AtomicByte class so you can't call
AtomicByte.getAndSet().  There doesn't exist a method in Unsafe to do a
getAndSet(byte).  So, it doesn't appear that there is a way to atomically
load and store a byte.

  As a work around, you can use the following code.  It doesn't use byte
atomic instructions.  Instead, it uses an int atomic instruction.  The code
ensures the update is atomic and doesn't tear or corrupt the surrounding
values.

  At first I thought this code would cause extra contention since multiple
threads could be trying to update adjacent byte values.  However, since the
processor deals with cache lines, then there would be any additional
contention since adjacent byte values are going to be on the same cache line
(except in the 2 edge cases).

  The ideal solution would use an atomic byte instruction.  This would get
rid of the CAS loop and the problems associated with such.  (See
https://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs).

     private final AtomicIntegerArray m_data = new AtomicIntegerArray(10);

     public byte getAndSet(int index, byte value)
     {
        int offset, shift, mask, set;
        int expect, update, result;

        offset = index / 4;
        shift  = 8 * (index % 4);
        mask   = 0xFF << shift;
        set    = (0x00FF & value) << shift;

        do
        {
           expect = m_data.get(offset);
           update = (expect & ~mask) | set;
        }
        while (!m_data.compareAndSet(offset, expect, update));

        result = expect >> shift;

        return((byte) result);
     }

  Nathan Reynolds | Architect | 602.333.9091

  Oracle PSR Engineering | Server Technology

  On 12/18/2012 5:31 AM, Paris Yiapanis wrote:

    Since you mention atomic bytes, I notice that in SPARC V9 there is a
special instruction LDSTUB for loading and storing unsigned bytes
atomically.
    Is there a way using the standard Java libraries to actually generate
that instruction?

    - Paris


    On Tue, Dec 18, 2012 at 10:03 AM, David Holmes
<davidcholmes at aapt.net.au> wrote:

      Byte accesses are atomic on all supported architectures.

      I don't quite understand what you mean about the cache line example.
An unaligned long access need not be atomic but it I don't think the cache
line positioning has any bearing on whether "word tearing" can occur - but I
don't know cache operation to that low-level of detail.

      David
        -----Original Message-----
        From: Nitsan Wakart [mailto:nitsanw at yahoo.com]
        Sent: Tuesday, 18 December 2012 7:40 PM
        To: dholmes at ieee.org; Alexey Goncharuk;
concurrency-interest at cs.oswego.edu
        Subject: Re: [concurrency-interest] Concurrent copy to array


        What about unsafe.putByte(address,value) and
unsafe.putByte(address+1,value) from 2 threads?
        Also how would this interact with cross cache line writes, i.e.
assume address is -4 from cache boundary and I do
unsafe.putLong(address,value) and unsafe.putLong(address+8,value) from 2
threads?





------------------------------------------------------------------------
        From: David Holmes <davidcholmes at aapt.net.au>
        To: Alexey Goncharuk <agoncharuk at gridgain.com>;
concurrency-interest at cs.oswego.edu
        Sent: Tuesday, December 18, 2012 9:15 AM
        Subject: Re: [concurrency-interest] Concurrent copy to array



        The Java language prohibits word-tearing, so adjacent array access
must not interfere. This extends to System.arraycopy (modulo bugs of which
there have been some).

        Unsafe.copyMemory is implemented using
Copy::conjoint_memory_atomic - so it too should be immune to word-tearing.

        David
          -----Original Message-----
          From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alexey
Goncharuk
          Sent: Tuesday, 18 December 2012 6:58 PM
          To: concurrency-interest at cs.oswego.edu
          Subject: [concurrency-interest] Concurrent copy to array


          Hi,


          Bounced by a recent discussion on memory alignment in java and
atomic reads and writes, I have the following question:


          Is it guaranteed to have consistent write result if two threads
are writing to disjoint adjacent parts of some array? In particular, how
would simple index-based access, System.arraycopy() and Unsafe.copyMemory()
(which is utilized by DirectByteBuffer) interfere?



          Thanks,
          --
          Alexey Goncharuk



        _______________________________________________
        Concurrency-interest mailing list
        Concurrency-interest at cs.oswego.edu
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest




      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121219/b4818a63/attachment-0001.html>

From vitalyd at gmail.com  Tue Dec 18 16:06:41 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 18 Dec 2012 16:06:41 -0500
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEAPJJAA.davidcholmes@aapt.net.au>
References: <CAHjP37HaPGDbvrkLGMw8rWqANPajqe1XKhu_c-eaEBB2708FJA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEAPJJAA.davidcholmes@aapt.net.au>
Message-ID: <CAHjP37GCj-=pu2J3_zBtNpVWfrFhCVyAgm-1HbweP28JTeAJPQ@mail.gmail.com>

David,

Section 8.1.1 of the Intel 64 and IA32 Architectures Software Developer's
manual talks about which memory ops are guaranteed to be carried out
atomically.  These ops have nothing to do with atomic instructions, which
is the main distinction I wanted to draw.

As far as I know, "word tearing" is a general term for non atomic memory
ops - whether it's because a subword access was turned into a larger access
or the load/store was split across coherence units, it's called tearing
since it's non-atomic.

Perhaps there's some official definition of tearing and I'm wrong, but I
don't think it only means subword access trashing adjacent locations.

Cheers

Sent from my phone
On Dec 18, 2012 3:06 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

> **
> Vitaly,
>
> There is a difference between atomicity and word-tearing. The atomicity
> constraint determines whether any other observer might see a partially
> updated value. The word-tearing aspect concerns whether any adjacent,
> sub-word, memory locations (not part of the current access) can be modified
> without actually being the target of a store.
>
> I'm surprised if this cache-line alignment issue can lead to word-tearing.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Vitaly
> Davidovich
> *Sent:* Tuesday, 18 December 2012 11:42 PM
> *To:* Nitsan Wakart
> *Cc:* concurrency-interest at cs.oswego.edu; dholmes at ieee.org
> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>
> Recall that cacheable memory is cacheline-at-a-time in the CPU.  Cache
> coherence ensures only one CPU can have the line in modified state.  Recall
> also that aligned access is when you read a value at an address divisible
> by (I.e. aligned to) its natural size.
>
> In the byte example, modern mainstream CPUs support byte mov instructions
> so given their 1 byte size, they're always aligned.  In your case, two CPUs
> writing different bytes on same line will induce a false sharing penalty,
> but otherwise won't tear.
>
> In the long example, you're not writing the long at a 8 byte divisible
> address if it's -4 from a cache line boundary.  This will count as
> unaligned access and we discussed on the other thread what happens here:
> CPU supports unaligned access, OS does unaligned access, or sigbus.  The
> former two are expensive.  Since the write straddles two lines you can
> experience word tearing.  Intel manual is pretty clear that writes spanning
> cache lines are not guaranteed atomic.
>
> Sent from my phone
> On Dec 18, 2012 6:04 AM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:
>
>>  I'm wondering if the loss of atomicity will in turn cause lose of word
>> tearing guarantee as the cache line writes are somehow separate. My
>> expectation is that this will maitain correctness but at a price to
>> performance.
>>
>>
>>   ------------------------------
>> *From:* David Holmes <davidcholmes at aapt.net.au>
>> *To:* Nitsan Wakart <nitsanw at yahoo.com>; Alexey Goncharuk <
>> agoncharuk at gridgain.com>; concurrency-interest at cs.oswego.edu
>> *Sent:* Tuesday, December 18, 2012 10:03 AM
>> *Subject:* RE: [concurrency-interest] Concurrent copy to array
>>
>>  Byte accesses are atomic on all supported architectures.
>>
>> I don't quite understand what you mean about the cache line example. An
>> unaligned long access need not be atomic but it I don't think the cache
>> line positioning has any bearing on whether "word tearing" can occur - but
>> I don't know cache operation to that low-level of detail.
>>
>> David
>>
>> -----Original Message-----
>> *From:* Nitsan Wakart [mailto:nitsanw at yahoo.com]
>> *Sent:* Tuesday, 18 December 2012 7:40 PM
>> *To:* dholmes at ieee.org; Alexey Goncharuk;
>> concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>>
>> What about unsafe.putByte(address,value) and
>> unsafe.putByte(address+1,value) from 2 threads?
>> Also how would this interact with cross cache line writes, i.e. assume
>> address is -4 from cache boundary and I do unsafe.putLong(address,value)
>> and unsafe.putLong(address+8,value) from 2 threads?
>>
>>
>>   ------------------------------
>> *From:* David Holmes <davidcholmes at aapt.net.au>
>> *To:* Alexey Goncharuk <agoncharuk at gridgain.com>;
>> concurrency-interest at cs.oswego.edu
>> *Sent:* Tuesday, December 18, 2012 9:15 AM
>> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>>
>>  The Java language prohibits word-tearing, so adjacent array access must
>> not interfere. This extends to System.arraycopy (modulo bugs of which there
>> have been some).
>>
>> Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic - so
>> it too should be immune to word-tearing.
>>
>> David
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Alexey
>> Goncharuk
>> *Sent:* Tuesday, 18 December 2012 6:58 PM
>> *To:* concurrency-interest at cs.oswego.edu
>> *Subject:* [concurrency-interest] Concurrent copy to array
>>
>> Hi,
>>
>> Bounced by a recent discussion on memory alignment in java and atomic
>> reads and writes, I have the following question:
>>
>> Is it guaranteed to have consistent write result if two threads are
>> writing to disjoint adjacent parts of some array? In particular, how would
>> simple index-based access, System.arraycopy() and Unsafe.copyMemory()
>> (which is utilized by DirectByteBuffer) interfere?
>>
>> Thanks,
>> --
>> Alexey Goncharuk
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/33a6811a/attachment.html>

From nathan.reynolds at oracle.com  Tue Dec 18 16:07:24 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 18 Dec 2012 14:07:24 -0700
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEAPJJAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEAPJJAA.davidcholmes@aapt.net.au>
Message-ID: <50D0DB0C.8020105@oracle.com>

Loading a byte is done as an atomic operation.  Storing a byte is done 
as an atomic operation. However, ldstub will load and store a byte 
atomically.  It is analogous to aa non-existent method called 
AtomicByte.getAndSet() where the method stores a byte but returns the 
byte that was replaced.  This can not be done with 2 instructions (i.e. 
load then store).

I grep'ed the code base and I see mechanisms to decode and encode the 
machine code for ldstub.  I don't see it being used.  Maybe I missed it.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/18/2012 1:10 PM, David Holmes wrote:
> All byte accesses (load/store) are atomic, no need for AtomicByte for 
> that.
> ldstub is emitted by the hotspot JIT but I don't know for what.
> David
>
>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Nathan Reynolds
>     *Sent:* Wednesday, 19 December 2012 2:29 AM
>     *To:* concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] Concurrent copy to array
>
>     There doesn't exist an AtomicByte class so you can't call
>     AtomicByte.getAndSet().  There doesn't exist a method in Unsafe to
>     do a getAndSet(byte).  So, it doesn't appear that there is a way
>     to atomically load and store a byte.
>
>     As a work around, you can use the following code.  It doesn't use
>     byte atomic instructions.  Instead, it uses an int atomic
>     instruction.  The code ensures the update is atomic and doesn't
>     tear or corrupt the surrounding values.
>
>     At first I thought this code would cause extra contention since
>     multiple threads could be trying to update adjacent byte values. 
>     However, since the processor deals with cache lines, then there
>     would be any additional contention since adjacent byte values are
>     going to be on the same cache line (except in the 2 edge cases).
>
>     The ideal solution would use an atomic byte instruction.  This
>     would get rid of the CAS loop and the problems associated with
>     such.  (See
>     https://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs).
>
>        private final AtomicIntegerArray m_data = new
>     AtomicIntegerArray(10);
>
>        public byte getAndSet(int index, byte value)
>        {
>           int offset, shift, mask, set;
>           int expect, update, result;
>
>           offset = index / 4;
>           shift  = 8 * (index % 4);
>           mask   = 0xFF << shift;
>           set    = (0x00FF & value) << shift;
>
>           do
>           {
>              expect = m_data.get(offset);
>              update = (expect & ~mask) | set;
>           }
>           while (!m_data.compareAndSet(offset, expect, update));
>
>           result = expect >> shift;
>
>           return((byte) result);
>        }
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 12/18/2012 5:31 AM, Paris Yiapanis wrote:
>>     Since you mention atomic bytes, I notice that in SPARC V9 there
>>     is a special instruction LDSTUB for loading and storing unsigned
>>     bytes atomically.
>>     Is there a way using the standard Java libraries to actually
>>     generate that instruction?
>>
>>     - Paris
>>
>>     On Tue, Dec 18, 2012 at 10:03 AM, David Holmes
>>     <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>>
>>         Byte accesses are atomic on all supported architectures.
>>         I don't quite understand what you mean about the cache line
>>         example. An unaligned long access need not be atomic but it I
>>         don't think the cache line positioning has any bearing on
>>         whether "word tearing" can occur - but I don't know cache
>>         operation to that low-level of detail.
>>         David
>>
>>             -----Original Message-----
>>             *From:* Nitsan Wakart [mailto:nitsanw at yahoo.com
>>             <mailto:nitsanw at yahoo.com>]
>>             *Sent:* Tuesday, 18 December 2012 7:40 PM
>>             *To:* dholmes at ieee.org <mailto:dholmes at ieee.org>; Alexey
>>             Goncharuk; concurrency-interest at cs.oswego.edu
>>             <mailto:concurrency-interest at cs.oswego.edu>
>>             *Subject:* Re: [concurrency-interest] Concurrent copy to
>>             array
>>
>>             What about unsafe.putByte(address,value) and
>>             unsafe.putByte(address+1,value) from 2 threads?
>>             Also how would this interact with cross cache line
>>             writes, i.e. assume address is -4 from cache boundary and
>>             I do unsafe.putLong(address,value) and
>>             unsafe.putLong(address+8,value) from 2 threads?
>>
>>
>>             ------------------------------------------------------------------------
>>             *From:* David Holmes <davidcholmes at aapt.net.au
>>             <mailto:davidcholmes at aapt.net.au>>
>>             *To:* Alexey Goncharuk <agoncharuk at gridgain.com
>>             <mailto:agoncharuk at gridgain.com>>;
>>             concurrency-interest at cs.oswego.edu
>>             <mailto:concurrency-interest at cs.oswego.edu>
>>             *Sent:* Tuesday, December 18, 2012 9:15 AM
>>             *Subject:* Re: [concurrency-interest] Concurrent copy to
>>             array
>>
>>             The Java language prohibits word-tearing, so adjacent
>>             array access must not interfere. This extends to
>>             System.arraycopy (modulo bugs of which there have been some).
>>             Unsafe.copyMemory is implemented using
>>             Copy::conjoint_memory_atomic - so it too should be immune
>>             to word-tearing.
>>             David
>>
>>                 -----Original Message-----
>>                 *From:* concurrency-interest-bounces at cs.oswego.edu
>>                 <mailto:concurrency-interest-bounces at cs.oswego.edu>
>>                 [mailto:concurrency-interest-bounces at cs.oswego.edu
>>                 <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On
>>                 Behalf Of *Alexey Goncharuk
>>                 *Sent:* Tuesday, 18 December 2012 6:58 PM
>>                 *To:* concurrency-interest at cs.oswego.edu
>>                 <mailto:concurrency-interest at cs.oswego.edu>
>>                 *Subject:* [concurrency-interest] Concurrent copy to
>>                 array
>>
>>                 Hi,
>>
>>                 Bounced by a recent discussion on memory alignment in
>>                 java and atomic reads and writes, I have the
>>                 following question:
>>
>>                 Is it guaranteed to have consistent write result if
>>                 two threads are writing to disjoint adjacent parts of
>>                 some array? In particular, how would simple
>>                 index-based access, System.arraycopy() and
>>                 Unsafe.copyMemory() (which is utilized by
>>                 DirectByteBuffer) interfere?
>>
>>                 Thanks,
>>                 -- 
>>                 Alexey Goncharuk
>>
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121218/b320d010/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Dec 18 16:14:06 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 19 Dec 2012 07:14:06 +1000
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <CAHjP37GCj-=pu2J3_zBtNpVWfrFhCVyAgm-1HbweP28JTeAJPQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEBAJJAA.davidcholmes@aapt.net.au>

Okay I think we need a different term to describe "unintentional updates to
adjacent memory locations" as in the array example. :)

I've always used word-tearing in the sense of "tearing out" subwords from a
word not as a general term for non-atomic.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
  Sent: Wednesday, 19 December 2012 7:07 AM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Concurrent copy to array


  David,

  Section 8.1.1 of the Intel 64 and IA32 Architectures Software Developer's
manual talks about which memory ops are guaranteed to be carried out
atomically.  These ops have nothing to do with atomic instructions, which is
the main distinction I wanted to draw.

  As far as I know, "word tearing" is a general term for non atomic memory
ops - whether it's because a subword access was turned into a larger access
or the load/store was split across coherence units, it's called tearing
since it's non-atomic.

  Perhaps there's some official definition of tearing and I'm wrong, but I
don't think it only means subword access trashing adjacent locations.

  Cheers

  Sent from my phone

  On Dec 18, 2012 3:06 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

    Vitaly,

    There is a difference between atomicity and word-tearing. The atomicity
constraint determines whether any other observer might see a partially
updated value. The word-tearing aspect concerns whether any adjacent,
sub-word, memory locations (not part of the current access) can be modified
without actually being the target of a store.

    I'm surprised if this cache-line alignment issue can lead to
word-tearing.

    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
      Sent: Tuesday, 18 December 2012 11:42 PM
      To: Nitsan Wakart
      Cc: concurrency-interest at cs.oswego.edu; dholmes at ieee.org
      Subject: Re: [concurrency-interest] Concurrent copy to array


      Recall that cacheable memory is cacheline-at-a-time in the CPU.  Cache
coherence ensures only one CPU can have the line in modified state.  Recall
also that aligned access is when you read a value at an address divisible by
(I.e. aligned to) its natural size.

      In the byte example, modern mainstream CPUs support byte mov
instructions so given their 1 byte size, they're always aligned.  In your
case, two CPUs writing different bytes on same line will induce a false
sharing penalty, but otherwise won't tear.

      In the long example, you're not writing the long at a 8 byte divisible
address if it's -4 from a cache line boundary.  This will count as unaligned
access and we discussed on the other thread what happens here: CPU supports
unaligned access, OS does unaligned access, or sigbus.  The former two are
expensive.  Since the write straddles two lines you can experience word
tearing.  Intel manual is pretty clear that writes spanning cache lines are
not guaranteed atomic.

      Sent from my phone

      On Dec 18, 2012 6:04 AM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:

        I'm wondering if the loss of atomicity will in turn cause lose of
word tearing guarantee as the cache line writes are somehow separate. My
expectation is that this will maitain correctness but at a price to
performance.






------------------------------------------------------------------------
        From: David Holmes <davidcholmes at aapt.net.au>
        To: Nitsan Wakart <nitsanw at yahoo.com>; Alexey Goncharuk
<agoncharuk at gridgain.com>; concurrency-interest at cs.oswego.edu
        Sent: Tuesday, December 18, 2012 10:03 AM
        Subject: RE: [concurrency-interest] Concurrent copy to array



        Byte accesses are atomic on all supported architectures.

        I don't quite understand what you mean about the cache line example.
An unaligned long access need not be atomic but it I don't think the cache
line positioning has any bearing on whether "word tearing" can occur - but I
don't know cache operation to that low-level of detail.

        David
          -----Original Message-----
          From: Nitsan Wakart [mailto:nitsanw at yahoo.com]
          Sent: Tuesday, 18 December 2012 7:40 PM
          To: dholmes at ieee.org; Alexey Goncharuk;
concurrency-interest at cs.oswego.edu
          Subject: Re: [concurrency-interest] Concurrent copy to array


          What about unsafe.putByte(address,value) and
unsafe.putByte(address+1,value) from 2 threads?
          Also how would this interact with cross cache line writes, i.e.
assume address is -4 from cache boundary and I do
unsafe.putLong(address,value) and unsafe.putLong(address+8,value) from 2
threads?





----------------------------------------------------------------------
          From: David Holmes <davidcholmes at aapt.net.au>
          To: Alexey Goncharuk <agoncharuk at gridgain.com>;
concurrency-interest at cs.oswego.edu
          Sent: Tuesday, December 18, 2012 9:15 AM
          Subject: Re: [concurrency-interest] Concurrent copy to array



          The Java language prohibits word-tearing, so adjacent array access
must not interfere. This extends to System.arraycopy (modulo bugs of which
there have been some).

          Unsafe.copyMemory is implemented using
Copy::conjoint_memory_atomic - so it too should be immune to word-tearing.

          David
            -----Original Message-----
            From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alexey
Goncharuk
            Sent: Tuesday, 18 December 2012 6:58 PM
            To: concurrency-interest at cs.oswego.edu
            Subject: [concurrency-interest] Concurrent copy to array


            Hi,


            Bounced by a recent discussion on memory alignment in java and
atomic reads and writes, I have the following question:


            Is it guaranteed to have consistent write result if two threads
are writing to disjoint adjacent parts of some array? In particular, how
would simple index-based access, System.arraycopy() and Unsafe.copyMemory()
(which is utilized by DirectByteBuffer) interfere?



            Thanks,
            --
            Alexey Goncharuk



          _______________________________________________
          Concurrency-interest mailing list
          Concurrency-interest at cs.oswego.edu
          http://cs.oswego.edu/mailman/listinfo/concurrency-interest







        _______________________________________________
        Concurrency-interest mailing list
        Concurrency-interest at cs.oswego.edu
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121219/767201c8/attachment.html>

From davidcholmes at aapt.net.au  Tue Dec 18 16:15:35 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 19 Dec 2012 07:15:35 +1000
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <50D0DB0C.8020105@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEBAJJAA.davidcholmes@aapt.net.au>

Thanks for clarifying that Nathan - obviously I'm not fluent in sparcv9 :)

As we don't define any atomic ops for bytes I'm not surprised to not see
ldstub being used.

David
  -----Original Message-----
  From: Nathan Reynolds [mailto:nathan.reynolds at oracle.com]
  Sent: Wednesday, 19 December 2012 7:07 AM
  To: dholmes at ieee.org
  Cc: David Holmes; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Concurrent copy to array


  Loading a byte is done as an atomic operation.  Storing a byte is done as
an atomic operation.  However, ldstub will load and store a byte atomically.
It is analogous to aa non-existent method called AtomicByte.getAndSet()
where the method stores a byte but returns the byte that was replaced.  This
can not be done with 2 instructions (i.e. load then store).

  I grep'ed the code base and I see mechanisms to decode and encode the
machine code for ldstub.  I don't see it being used.  Maybe I missed it.


  Nathan Reynolds | Architect | 602.333.9091
  Oracle PSR Engineering | Server Technology

  On 12/18/2012 1:10 PM, David Holmes wrote:

    All byte accesses (load/store) are atomic, no need for AtomicByte for
that.

    ldstub is emitted by the hotspot JIT but I don't know for what.

    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nathan
Reynolds
      Sent: Wednesday, 19 December 2012 2:29 AM
      To: concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Concurrent copy to array


      There doesn't exist an AtomicByte class so you can't call
AtomicByte.getAndSet().  There doesn't exist a method in Unsafe to do a
getAndSet(byte).  So, it doesn't appear that there is a way to atomically
load and store a byte.

      As a work around, you can use the following code.  It doesn't use byte
atomic instructions.  Instead, it uses an int atomic instruction.  The code
ensures the update is atomic and doesn't tear or corrupt the surrounding
values.

      At first I thought this code would cause extra contention since
multiple threads could be trying to update adjacent byte values.  However,
since the processor deals with cache lines, then there would be any
additional contention since adjacent byte values are going to be on the same
cache line (except in the 2 edge cases).

      The ideal solution would use an atomic byte instruction.  This would
get rid of the CAS loop and the problems associated with such.  (See
https://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs).

         private final AtomicIntegerArray m_data = new
AtomicIntegerArray(10);

         public byte getAndSet(int index, byte value)
         {
            int offset, shift, mask, set;
            int expect, update, result;

            offset = index / 4;
            shift  = 8 * (index % 4);
            mask   = 0xFF << shift;
            set    = (0x00FF & value) << shift;

            do
            {
               expect = m_data.get(offset);
               update = (expect & ~mask) | set;
            }
            while (!m_data.compareAndSet(offset, expect, update));

            result = expect >> shift;

            return((byte) result);
         }

      Nathan Reynolds | Architect | 602.333.9091

      Oracle PSR Engineering | Server Technology

      On 12/18/2012 5:31 AM, Paris Yiapanis wrote:

        Since you mention atomic bytes, I notice that in SPARC V9 there is a
special instruction LDSTUB for loading and storing unsigned bytes
atomically.
        Is there a way using the standard Java libraries to actually
generate that instruction?

        - Paris


        On Tue, Dec 18, 2012 at 10:03 AM, David Holmes
<davidcholmes at aapt.net.au> wrote:

          Byte accesses are atomic on all supported architectures.

          I don't quite understand what you mean about the cache line
example. An unaligned long access need not be atomic but it I don't think
the cache line positioning has any bearing on whether "word tearing" can
occur - but I don't know cache operation to that low-level of detail.

          David
            -----Original Message-----
            From: Nitsan Wakart [mailto:nitsanw at yahoo.com]
            Sent: Tuesday, 18 December 2012 7:40 PM
            To: dholmes at ieee.org; Alexey Goncharuk;
concurrency-interest at cs.oswego.edu
            Subject: Re: [concurrency-interest] Concurrent copy to array


            What about unsafe.putByte(address,value) and
unsafe.putByte(address+1,value) from 2 threads?
            Also how would this interact with cross cache line writes, i.e.
assume address is -4 from cache boundary and I do
unsafe.putLong(address,value) and unsafe.putLong(address+8,value) from 2
threads?





--------------------------------------------------------------------
            From: David Holmes <davidcholmes at aapt.net.au>
            To: Alexey Goncharuk <agoncharuk at gridgain.com>;
concurrency-interest at cs.oswego.edu
            Sent: Tuesday, December 18, 2012 9:15 AM
            Subject: Re: [concurrency-interest] Concurrent copy to array



            The Java language prohibits word-tearing, so adjacent array
access must not interfere. This extends to System.arraycopy (modulo bugs of
which there have been some).

            Unsafe.copyMemory is implemented using
Copy::conjoint_memory_atomic - so it too should be immune to word-tearing.

            David
              -----Original Message-----
              From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alexey
Goncharuk
              Sent: Tuesday, 18 December 2012 6:58 PM
              To: concurrency-interest at cs.oswego.edu
              Subject: [concurrency-interest] Concurrent copy to array


              Hi,


              Bounced by a recent discussion on memory alignment in java and
atomic reads and writes, I have the following question:


              Is it guaranteed to have consistent write result if two
threads are writing to disjoint adjacent parts of some array? In particular,
how would simple index-based access, System.arraycopy() and
Unsafe.copyMemory() (which is utilized by DirectByteBuffer) interfere?



              Thanks,
              --
              Alexey Goncharuk



            _______________________________________________
            Concurrency-interest mailing list
            Concurrency-interest at cs.oswego.edu
            http://cs.oswego.edu/mailman/listinfo/concurrency-interest




          _______________________________________________
          Concurrency-interest mailing list
          Concurrency-interest at cs.oswego.edu
          http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121219/ac1e490e/attachment-0001.html>

From aleksey.shipilev at oracle.com  Wed Dec 19 04:54:54 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 19 Dec 2012 13:54:54 +0400
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <CAHjP37GCj-=pu2J3_zBtNpVWfrFhCVyAgm-1HbweP28JTeAJPQ@mail.gmail.com>
References: <CAHjP37HaPGDbvrkLGMw8rWqANPajqe1XKhu_c-eaEBB2708FJA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEAPJJAA.davidcholmes@aapt.net.au>
	<CAHjP37GCj-=pu2J3_zBtNpVWfrFhCVyAgm-1HbweP28JTeAJPQ@mail.gmail.com>
Message-ID: <50D18EEE.6000501@oracle.com>

On 12/19/2012 01:06 AM, Vitaly Davidovich wrote:
> As far as I know, "word tearing" is a general term for non atomic memory
> ops - whether it's because a subword access was turned into a larger
> access or the load/store was split across coherence units, it's called
> tearing since it's non-atomic.

Tearing has a specific meaning in JLS, which is coherent with David's
definition: the action on field/array element that is affecting other
fields/array elements. Let's stick with that.

It's very easy to confuse the read/write atomicity, and read/write
granularity: atomicity is about seeing the complete state all at once,
indivisibly; while granularity specifies if low-granular op can be
emulated by high-granular op.

That said, you can theoretically have the atomicity of the update under
tearing, e.g. setting single byte rewriting the whole word will cause
the observer will see the complete and instantaneous word-wide
corruption atomically.

IIRC, the provision for word tearing in JLS is much more about the
compilers that can (not) use the larger reads/writes while updating
packed data under the race, i.e. beefing up op granularity. Most of the
commodity hardware is byte-granular, and as such does not experience
word tearing. On the platforms which are not byte-granular, VMs would
have to provide sparse array/class implementations to meet JMM requirements.

On the practical note, if you extend my previous test [1] to update the
adjacent array substrides, then even though we are crossing the cache
line boundary on x86 and exhibit the lack of atomicity, word tearing
guarantees still hold (i.e. "no tearing") [2]. Also, if we update the
same array under the race with two threads, one setting even elements,
and another one setting odd, then word tearing guarantees are still
there with either with the plain array ops [3], or even with the Unsafe [4]:

org.openjdk.concurrent.torture.tests.unsorted.ArrayInterleaveTest
                 Observed state  Occurrences          Expectation
                  [0, 128, 128] (   3323967)             REQUIRED

org.openjdk.concurrent.torture.tests.unsorted.UnsafeArrayInterleaveTest
                 Observed state  Occurrences          Expectation
                  [0, 128, 128] (   3162050)             REQUIRED

Hope this summarizes the thread well enough :)

-Aleksey.

[1]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/unsorted/UnsafeCrossCacheLineAtomicityTest.java
[2]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/unsorted/UnsafeCrossCacheLineTearingTest.java
[3]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/unsorted/ArrayInterleaveTest.java
[4]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/unsorted/UnsafeArrayInterleaveTest.java

From pyiapa at gmail.com  Wed Dec 19 06:11:09 2012
From: pyiapa at gmail.com (Paris Yiapanis)
Date: Wed, 19 Dec 2012 11:11:09 +0000
Subject: [concurrency-interest] Concurrent copy to array
In-Reply-To: <50D099E5.9050507@oracle.com>
References: <1355823579.42925.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCOEAMJJAA.davidcholmes@aapt.net.au>
	<CAPUTfNn2mBbNX35Rnk4p6WVQo_M9zS2wt2JO+DEH-_P=P3CQAw@mail.gmail.com>
	<50D099E5.9050507@oracle.com>
Message-ID: <CAPUTfNnL83kiJ76zTGpuK_BCrr1O9hyLoefjLH3_ZJyyaz-o+A@mail.gmail.com>

Thank you very much Nathan.
In one application,  I also used AtomicIntegerArray by treating each atomic
int as a bitstring. I thought the same like you regarding contention but it
turned out to be fine (I presume for the reason you explained - adjacent
values in same cache line).

- Paris

On Tue, Dec 18, 2012 at 4:29 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

>  There doesn't exist an AtomicByte class so you can't call
> AtomicByte.getAndSet().  There doesn't exist a method in Unsafe to do a
> getAndSet(byte).  So, it doesn't appear that there is a way to atomically
> load and store a byte.
>
> As a work around, you can use the following code.  It doesn't use byte
> atomic instructions.  Instead, it uses an int atomic instruction.  The code
> ensures the update is atomic and doesn't tear or corrupt the surrounding
> values.
>
> At first I thought this code would cause extra contention since multiple
> threads could be trying to update adjacent byte values.  However, since the
> processor deals with cache lines, then there would be any additional
> contention since adjacent byte values are going to be on the same cache
> line (except in the 2 edge cases).
>
> The ideal solution would use an atomic byte instruction.  This would get
> rid of the CAS loop and the problems associated with such.  (See
> https://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs).
>
>    private final AtomicIntegerArray m_data = new AtomicIntegerArray(10);
>
>    public byte getAndSet(int index, byte value)
>    {
>       int offset, shift, mask, set;
>       int expect, update, result;
>
>       offset = index / 4;
>       shift  = 8 * (index % 4);
>       mask   = 0xFF << shift;
>       set    = (0x00FF & value) << shift;
>
>       do
>       {
>          expect = m_data.get(offset);
>          update = (expect & ~mask) | set;
>       }
>       while (!m_data.compareAndSet(offset, expect, update));
>
>       result = expect >> shift;
>
>       return((byte) result);
>    }
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
>  Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 12/18/2012 5:31 AM, Paris Yiapanis wrote:
>
> Since you mention atomic bytes, I notice that in SPARC V9 there is a
> special instruction LDSTUB for loading and storing unsigned bytes
> atomically.
> Is there a way using the standard Java libraries to actually generate that
> instruction?
>
> - Paris
>
> On Tue, Dec 18, 2012 at 10:03 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>>  Byte accesses are atomic on all supported architectures.
>>
>> I don't quite understand what you mean about the cache line example. An
>> unaligned long access need not be atomic but it I don't think the cache
>> line positioning has any bearing on whether "word tearing" can occur - but
>> I don't know cache operation to that low-level of detail.
>>
>> David
>>
>> -----Original Message-----
>> *From:* Nitsan Wakart [mailto:nitsanw at yahoo.com]
>> *Sent:* Tuesday, 18 December 2012 7:40 PM
>> *To:* dholmes at ieee.org; Alexey Goncharuk;
>> concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>>
>>  What about unsafe.putByte(address,value) and
>> unsafe.putByte(address+1,value) from 2 threads?
>> Also how would this interact with cross cache line writes, i.e. assume
>> address is -4 from cache boundary and I do unsafe.putLong(address,value)
>> and unsafe.putLong(address+8,value) from 2 threads?
>>
>>
>>   ------------------------------
>> *From:* David Holmes <davidcholmes at aapt.net.au>
>> *To:* Alexey Goncharuk <agoncharuk at gridgain.com>;
>> concurrency-interest at cs.oswego.edu
>> *Sent:* Tuesday, December 18, 2012 9:15 AM
>> *Subject:* Re: [concurrency-interest] Concurrent copy to array
>>
>>  The Java language prohibits word-tearing, so adjacent array access must
>> not interfere. This extends to System.arraycopy (modulo bugs of which there
>> have been some).
>>
>> Unsafe.copyMemory is implemented using Copy::conjoint_memory_atomic - so
>> it too should be immune to word-tearing.
>>
>> David
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Alexey
>> Goncharuk
>> *Sent:* Tuesday, 18 December 2012 6:58 PM
>> *To:* concurrency-interest at cs.oswego.edu
>> *Subject:* [concurrency-interest] Concurrent copy to array
>>
>>  Hi,
>>
>>  Bounced by a recent discussion on memory alignment in java and atomic
>> reads and writes, I have the following question:
>>
>>  Is it guaranteed to have consistent write result if two threads are
>> writing to disjoint adjacent parts of some array? In particular, how would
>> simple index-based access, System.arraycopy() and Unsafe.copyMemory()
>> (which is utilized by DirectByteBuffer) interfere?
>>
>>  Thanks,
>> --
>> Alexey Goncharuk
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121219/173a3695/attachment.html>

From m_sarmad_asgher at yahoo.com  Wed Dec 19 11:46:57 2012
From: m_sarmad_asgher at yahoo.com (Sarmad Asgher)
Date: Wed, 19 Dec 2012 08:46:57 -0800 (PST)
Subject: [concurrency-interest] Coauthor a paper on lock free doubly linked
	list
Message-ID: <1355935617.10673.YahooMailClassic@web140704.mail.bf1.yahoo.com>

Dear All,
I have done my Masters in Computer Science from FAST-NU Pakistan. My research interests include Lock-Free Programming. The topic of my master?s thesis was Lock-Free Data Structures.??My article titled "Practical Lock-Free Buffers" has appeared in the Dr. Dobbs Journal dated Aug 26, 2009 by Sarmad Asghar
URL:http://www.ddj.com/go-parallel/article/showArticle.jhtml?articleID=219500200?
This paper on "A lock free doubly linked list" is a continuation of my interest in Lock-Freedom. Can any one expert on this topic coauthor this paper with me and my teacher? It would be a great honor for me. I am anxiously waiting for your response.
Best Regards,Muhammad Sarmad Asghar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121219/8a9b4f5e/attachment-0001.html>

From martinrb at google.com  Wed Dec 19 19:58:25 2012
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 19 Dec 2012 16:58:25 -0800
Subject: [concurrency-interest] Coauthor a paper on lock free doubly
 linked list
In-Reply-To: <1355935617.10673.YahooMailClassic@web140704.mail.bf1.yahoo.com>
References: <1355935617.10673.YahooMailClassic@web140704.mail.bf1.yahoo.com>
Message-ID: <CA+kOe0-H9VLQfF+iOvb1WRRS0T2jw4MDUP-KFHgCU8vedzXh2g@mail.gmail.com>

Hi Samad,

I've also done a lot of thinking about this topic.
Our work in
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166y/ConcurrentLinkedDeque.java?view=markup
is largely implementing a lock-free doubly-linked list.
The long implementation comments in that file is begging to be turned into
an academic paper.  If you'd like to go down that road, I'd like to be your
co-author.

Martin

On Wed, Dec 19, 2012 at 8:46 AM, Sarmad Asgher <m_sarmad_asgher at yahoo.com>wrote:

> Dear All,
>
> I have done my Masters in Computer Science from FAST-NU Pakistan. My
> research interests include Lock-Free Programming. The topic of my master?s
> thesis was Lock-Free Data Structures.
>
> My article titled "Practical Lock-Free Buffers" has appeared in the Dr.
> Dobbs Journal dated Aug 26, 2009 by Sarmad Asghar
>
> URL:
> http://www.ddj.com/go-parallel/article/showArticle.jhtml?articleID=219500200
>
>
> This paper on "A lock free doubly linked list" is a continuation of my
> interest in Lock-Freedom. Can any one expert on this topic coauthor this
> paper with me and my teacher? It would be a great honor for me. I am
> anxiously waiting for your response.
>
> Best Regards,
> Muhammad Sarmad Asghar
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121219/230ec38e/attachment.html>

From aleksey.shipilev at oracle.com  Thu Dec 20 06:42:11 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 20 Dec 2012 15:42:11 +0400
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
Message-ID: <50D2F993.4030901@oracle.com>

Hi guys,

Following up on recent discussion, I want to get the opinions on the
rather tricky exercise.

java.nio.ByteBuffer follows the general contract for java.nio.Buffer,
which states that buffers are not safe for use by multiple concurrent
threads. However, what are the reasonable expectations about this
behavior under the race?

For the specific example:

   ByteBuffer buf = ByteBuffer.allocate(100);
   (publish $buf to both threads properly)
   (start both threads)

   Thread 1:
    buf.putInt(0, 42); // at position 0

   Thread 2:
    int i = buf.getInt(0); // at position 0
    Assert.assertTrue( (i == 0) || (i == 42) )

Should this assert be always true?

The answer is tricky since as of now, spec does not guarantee the
atomicity of reads/writes if performed without the synchronization.

Our tests [1], however, show that on x86:
 a) Char/Short/Int/Long/Double/FloatBuffers are atomic
 b) direct ByteBuffer is atomic for all width ops
 c) direct ByteBuffer views are atomic for all width ops
 d) heap ByteBuffer is atomic for byte ops
 e) heap ByteBuffer is *NOT* atomic for other width op
 f) heap ByteBuffer views are *NOT* atomic

e) and f) are the manifestations of the same concrete implementation
with backing on-heap BB writing the large datatypes by striping them
into the bytes first.

The big question is: do we need Buffer to be spec'ed more carefully
about the atomicity? Does anyone has the scenario in which this relaxed
spec would be profitable? How do people generally feel about this issue
(or rather, is this an issue at all)?

Would appreciate your inputs.

Thanks,
-Aleksey.

P.S. I would appreciate if any non-Hotspot/OpenJDK folks can run these
tests against their classlib.

[1]
https://github.com/shipilev/java-concurrency-torture/tree/master/src/main/java/org/openjdk/concurrent/torture/tests/atomicity/buffers

From elizarov at devexperts.com  Thu Dec 20 06:59:32 2012
From: elizarov at devexperts.com (Roman Elizarov)
Date: Thu, 20 Dec 2012 11:59:32 +0000
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D2F993.4030901@oracle.com>
References: <50D2F993.4030901@oracle.com>
Message-ID: <C248BCD79E2CBC4B93C0AE3B1E77E9A81E76C2C6@RAVEN.office.devexperts.com>

Yes. ByteBuffer is a powerful and fast inter-process communication mechanism via mapped view of file. As an IPC mechanism it needs explicit guarantees with respect to atomicity and concurrent access in general. It also badly needs a class like ConcurrentByteBuffer with CAS support. This class will address a whole group of use-cases where you currently have to resort to sun.misc.Unsafe for your IPC.

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
Sent: Thursday, December 20, 2012 3:42 PM
To: Concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity

Hi guys,

Following up on recent discussion, I want to get the opinions on the rather tricky exercise.

java.nio.ByteBuffer follows the general contract for java.nio.Buffer, which states that buffers are not safe for use by multiple concurrent threads. However, what are the reasonable expectations about this behavior under the race?

For the specific example:

   ByteBuffer buf = ByteBuffer.allocate(100);
   (publish $buf to both threads properly)
   (start both threads)

   Thread 1:
    buf.putInt(0, 42); // at position 0

   Thread 2:
    int i = buf.getInt(0); // at position 0
    Assert.assertTrue( (i == 0) || (i == 42) )

Should this assert be always true?

The answer is tricky since as of now, spec does not guarantee the atomicity of reads/writes if performed without the synchronization.

Our tests [1], however, show that on x86:
 a) Char/Short/Int/Long/Double/FloatBuffers are atomic
 b) direct ByteBuffer is atomic for all width ops
 c) direct ByteBuffer views are atomic for all width ops
 d) heap ByteBuffer is atomic for byte ops
 e) heap ByteBuffer is *NOT* atomic for other width op
 f) heap ByteBuffer views are *NOT* atomic

e) and f) are the manifestations of the same concrete implementation with backing on-heap BB writing the large datatypes by striping them into the bytes first.

The big question is: do we need Buffer to be spec'ed more carefully about the atomicity? Does anyone has the scenario in which this relaxed spec would be profitable? How do people generally feel about this issue (or rather, is this an issue at all)?

Would appreciate your inputs.

Thanks,
-Aleksey.

P.S. I would appreciate if any non-Hotspot/OpenJDK folks can run these tests against their classlib.

[1]
https://github.com/shipilev/java-concurrency-torture/tree/master/src/main/java/org/openjdk/concurrent/torture/tests/atomicity/buffers
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From stanimir at riflexo.com  Thu Dec 20 07:18:10 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 20 Dec 2012 14:18:10 +0200
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D2F993.4030901@oracle.com>
References: <50D2F993.4030901@oracle.com>
Message-ID: <CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>

  >> ByteBuffer buf = ByteBuffer.allocate(100);
This creates a heap byte buffer that's just a wrapped byte[]. So all the
rules for accessing byte[] apply there. Unless the Buffer is readonly you
can always get a reference to the underlying array. IMO, heap Buffers are
not interesting.

Direct (byte)Buffers basically follow no specification and I can't think of
a case to take benefit from racy usage within the same process. CAS would
be good for IPC via shared memory and really low latency, though... however
that crosses the boundaries what Java can define.

Direct ByteBufffers are always allocated to be properly aligned.

Stanimir

On Thu, Dec 20, 2012 at 1:42 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

>    ByteBuffer buf = ByteBuffer.allocate(100);
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121220/7006c95b/attachment.html>

From aleksey.shipilev at oracle.com  Thu Dec 20 07:21:44 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 20 Dec 2012 16:21:44 +0400
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <C248BCD79E2CBC4B93C0AE3B1E77E9A81E76C2C6@RAVEN.office.devexperts.com>
References: <50D2F993.4030901@oracle.com>
	<C248BCD79E2CBC4B93C0AE3B1E77E9A81E76C2C6@RAVEN.office.devexperts.com>
Message-ID: <50D302D8.3080200@oracle.com>

On 12/20/2012 03:59 PM, Roman Elizarov wrote:
> Yes. ByteBuffer is a powerful and fast inter-process communication
> mechanism via mapped view of file. As an IPC mechanism it needs
> explicit guarantees with respect to atomicity and concurrent access
> in general. It also badly needs a class like ConcurrentByteBuffer
> with CAS support. This class will address a whole group of use-cases
> where you currently have to resort to sun.misc.Unsafe for your IPC.

Yeah, I remember you we telling me something about this in person.

Can you be a little more concrete on this? What are the reasonable
expectations for ByteBuffer for you? Will the guarantee for read/write
atomicity (but without CAS) fit your bill? Will the absence of
volatile-like visibility ruin anything?

-Aleksey.

From aleksey.shipilev at oracle.com  Thu Dec 20 07:24:48 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 20 Dec 2012 16:24:48 +0400
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
Message-ID: <50D30390.4040002@oracle.com>

On 12/20/2012 04:18 PM, Stanimir Simeonoff wrote:
>   >> ByteBuffer buf = ByteBuffer.allocate(100);
> This creates a heap byte buffer that's just a wrapped byte[]. So all the
> rules for accessing byte[] apply there. Unless the Buffer is readonly
> you can always get a reference to the underlying array. IMO, heap
> Buffers are not interesting.

Understood. Now, is the "putInt" op defined on byte[]? Nope. The only
semantically-equivalent operation is Unsafe.putInt() to backing array,
and that is surprising that even though Unsafe call would buy me the
perceived atomicity, the public BB.putInt() will not.

So the opinion I hear from you is along the lines of "everything is OK
now, no need for change", right?

-Aleksey.

From stanimir at riflexo.com  Thu Dec 20 07:37:23 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 20 Dec 2012 14:37:23 +0200
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D30390.4040002@oracle.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
Message-ID: <CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>

On Thu, Dec 20, 2012 at 2:24 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 12/20/2012 04:18 PM, Stanimir Simeonoff wrote:
> >   >> ByteBuffer buf = ByteBuffer.allocate(100);
> > This creates a heap byte buffer that's just a wrapped byte[]. So all the
> > rules for accessing byte[] apply there. Unless the Buffer is readonly
> > you can always get a reference to the underlying array. IMO, heap
> > Buffers are not interesting.
>
> Understood. Now, is the "putInt" op defined on byte[]? Nope. The only
> semantically-equivalent operation is Unsafe.putInt() to backing array,
> and that is surprising that even though Unsafe call would buy me the
> perceived atomicity, the public BB.putInt() will not.
>
> This is almost true - since it would work only if there is no tearing and
the endianess of the Bufffer have to be taken care of prior to the write
and you need probably happens before along side w/ the endianess, etc. Then
there come the views and the slicing.
But again, I have not come up w/ a case where atomically accessing within
the case process would matter.


> So the opinion I hear from you is along the lines of "everything is OK
> now, no need for change", right?
>
> Well, "unmap" for Mapped Buffers would be lovely. The hacking via the
cleaner is very dangerous now. 32bit systems can literally run out of
virtual memory.
Faster/deterministic reclaim of the direct buffers would be great, since
the direct buffers take very little heap memory the GC is not so keen on
collecting them but they eat tons of native memory to the point oom_killer
can kick in.

I just do not see need to define atomically operations if they span only
within the java process.

Stanimir


> -Aleksey.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121220/4de4315b/attachment-0001.html>

From nitsanw at yahoo.com  Thu Dec 20 07:36:29 2012
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Thu, 20 Dec 2012 04:36:29 -0800 (PST)
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
Message-ID: <1356006989.44388.YahooMailNeo@web120704.mail.ne1.yahoo.com>

--> Direct ByteBufffers are always allocated to be properly aligned.
As of JDK7 not true, or at least not the same.
ByteBuffers are 8b aligned, but no longer page size aligned unless specific JVM flag used.



________________________________
 From: Stanimir Simeonoff <stanimir at riflexo.com>
To: Aleksey Shipilev <aleksey.shipilev at oracle.com> 
Cc: "Concurrency-interest at cs.oswego.edu" <Concurrency-interest at cs.oswego.edu> 
Sent: Thursday, December 20, 2012 12:18 PM
Subject: Re: [concurrency-interest] java.nio.ByteBuffer atomicity
 

? >> ByteBuffer buf = ByteBuffer.allocate(100);
This creates a heap byte buffer that's just a wrapped byte[]. So all the rules for accessing byte[] apply there. Unless the Buffer is readonly you can always get a reference to the underlying array. IMO, heap Buffers are not interesting.

Direct (byte)Buffers basically follow no specification and I can't think of a case to take benefit from racy usage within the same process. CAS would be good for IPC via shared memory and really low latency, though... however that crosses the boundaries what Java can define.

Direct ByteBufffers are always allocated to be properly aligned.

Stanimir


On Thu, Dec 20, 2012 at 1:42 PM, Aleksey Shipilev <aleksey.shipilev at oracle.com> wrote:

? ?ByteBuffer buf = ByteBuffer.allocate(100);

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121220/855f5d89/attachment.html>

From oleksandr.otenko at oracle.com  Thu Dec 20 07:41:40 2012
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 20 Dec 2012 12:41:40 +0000
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D302D8.3080200@oracle.com>
References: <50D2F993.4030901@oracle.com>
	<C248BCD79E2CBC4B93C0AE3B1E77E9A81E76C2C6@RAVEN.office.devexperts.com>
	<50D302D8.3080200@oracle.com>
Message-ID: <50D30784.6090408@oracle.com>

I need the buffers to not enforce volatile access. But I need an 
interface with volatile guarantees and atomic operations. At this moment 
I am fine with limiting this to just DirectByteBuffers.

I am fine with the interface telling me what alignment to choose and 
enforce volatile guarantees only at those alignments (ie no need for the 
interface to enforce volatile semantics for any offset).

Alex

On 20/12/2012 12:21, Aleksey Shipilev wrote:
> On 12/20/2012 03:59 PM, Roman Elizarov wrote:
>> Yes. ByteBuffer is a powerful and fast inter-process communication
>> mechanism via mapped view of file. As an IPC mechanism it needs
>> explicit guarantees with respect to atomicity and concurrent access
>> in general. It also badly needs a class like ConcurrentByteBuffer
>> with CAS support. This class will address a whole group of use-cases
>> where you currently have to resort to sun.misc.Unsafe for your IPC.
> Yeah, I remember you we telling me something about this in person.
>
> Can you be a little more concrete on this? What are the reasonable
> expectations for ByteBuffer for you? Will the guarantee for read/write
> atomicity (but without CAS) fit your bill? Will the absence of
> volatile-like visibility ruin anything?
>
> -Aleksey.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121220/c1c44ef8/attachment.html>

From elizarov at devexperts.com  Thu Dec 20 07:50:34 2012
From: elizarov at devexperts.com (Roman Elizarov)
Date: Thu, 20 Dec 2012 12:50:34 +0000
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D302D8.3080200@oracle.com>
References: <50D2F993.4030901@oracle.com>
	<C248BCD79E2CBC4B93C0AE3B1E77E9A81E76C2C6@RAVEN.office.devexperts.com>
	<50D302D8.3080200@oracle.com>
Message-ID: <C248BCD79E2CBC4B93C0AE3B1E77E9A81E76C402@RAVEN.office.devexperts.com>

It depends. My use case really needs everything: non-atomic, atomic, volatilie, and cas, so that I can access data structures that are shared between processes in the fastest possible way with a full control over guarantees I need for each particular memory access. To address it in its entirety, the best thing I can think of, is to leave ByteBuffer as is (without any guarantees even to atomicity), but introduce a ConcurrentByteArray class with explicit get/setAtomicXXX, get/setVolatileXXX, and CAS. The obvious problem with all these methods is that they can be efficiently implemented only when you respect the memory alignment. It means, that from the performance standpoint, it does not make sense to require regular ByteBuffer operations to be atomic, as it will negatively affect their performance. However, new ConcurrentByteBuffer operations can require proper alignment by design, and thus be efficient on all supported architectures.

-----Original Message-----
From: Aleksey Shipilev [mailto:aleksey.shipilev at oracle.com] 
Sent: Thursday, December 20, 2012 4:22 PM
To: Roman Elizarov
Cc: Concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] java.nio.ByteBuffer atomicity

On 12/20/2012 03:59 PM, Roman Elizarov wrote:
> Yes. ByteBuffer is a powerful and fast inter-process communication 
> mechanism via mapped view of file. As an IPC mechanism it needs 
> explicit guarantees with respect to atomicity and concurrent access in 
> general. It also badly needs a class like ConcurrentByteBuffer with 
> CAS support. This class will address a whole group of use-cases where 
> you currently have to resort to sun.misc.Unsafe for your IPC.

Yeah, I remember you we telling me something about this in person.

Can you be a little more concrete on this? What are the reasonable expectations for ByteBuffer for you? Will the guarantee for read/write atomicity (but without CAS) fit your bill? Will the absence of volatile-like visibility ruin anything?

-Aleksey.


From nitsanw at yahoo.com  Thu Dec 20 08:05:59 2012
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Thu, 20 Dec 2012 05:05:59 -0800 (PST)
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
Message-ID: <1356008759.67679.YahooMailNeo@web120701.mail.ne1.yahoo.com>

Personal opinion: 
- I think we can just get on with a DirectMemory class to manipulate memory locations(same as Unsafe.putXXX(address,value)). I'd be happy with a combination of that and direct construction of ByteBuffers from address. 
- A utility for allocating aligned memory would be nice, as in ByteBuffer.allocateDirect(capacity,alignment)
- As for cross process happens before guarantees, my understanding is that x86 sorts you out there i.e. if you putOrdered/Volatile you get the same happens before across processes as you would with threads. I would not expect the JVM to give me further guarantees on all platforms.

I agree with Aleksey that the difference in putLong atomicity between heap and direct is disturbing, should probably be documented.



________________________________
 From: Stanimir Simeonoff <stanimir at riflexo.com>
To: Aleksey Shipilev <aleksey.shipilev at oracle.com> 
Cc: "Concurrency-interest at cs.oswego.edu" <Concurrency-interest at cs.oswego.edu> 
Sent: Thursday, December 20, 2012 12:37 PM
Subject: Re: [concurrency-interest] java.nio.ByteBuffer atomicity
 




On Thu, Dec 20, 2012 at 2:24 PM, Aleksey Shipilev <aleksey.shipilev at oracle.com> wrote:

On 12/20/2012 04:18 PM, Stanimir Simeonoff wrote:
>> ? >> ByteBuffer buf = ByteBuffer.allocate(100);
>
>> This creates a heap byte buffer that's just a wrapped byte[]. So all the
>> rules for accessing byte[] apply there. Unless the Buffer is readonly
>> you can always get a reference to the underlying array. IMO, heap
>> Buffers are not interesting.
>
>Understood. Now, is the "putInt" op defined on byte[]? Nope. The only
>semantically-equivalent operation is Unsafe.putInt() to backing array,
>and that is surprising that even though Unsafe call would buy me the
>perceived atomicity, the public BB.putInt() will not.
>
>
This is almost true - since it would work only if there is no tearing and the endianess of the Bufffer have to be taken care of prior to the write and you need probably happens before along side w/ the endianess, etc. Then there come the views and the slicing.
But again, I have not come up w/ a case where atomically accessing within the case process would matter. 
?

So the opinion I hear from you is along the lines of "everything is OK
>now, no need for change", right?
>
>
Well, "unmap" for Mapped Buffers would be lovely. The hacking via the cleaner is very dangerous now. 32bit systems can literally run out of virtual memory. 
Faster/deterministic reclaim of the direct buffers would be great, since the direct buffers take very little heap memory the GC is not so keen on collecting them but they eat tons of native memory to the point oom_killer can kick in.

I just do not see need to define atomically operations if they span only within the java process.

Stanimir 
?? 

-Aleksey.
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121220/335b9ff7/attachment-0001.html>

From zhong.j.yu at gmail.com  Thu Dec 20 11:16:05 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 20 Dec 2012 10:16:05 -0600
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
Message-ID: <CACuKZqE+dVJP61CcrFjbGupAbHYNuTpb4s2_YOyrch1xtdJN8g@mail.gmail.com>

On Thu, Dec 20, 2012 at 6:37 AM, Stanimir Simeonoff
<stanimir at riflexo.com> wrote:
>
>
> On Thu, Dec 20, 2012 at 2:24 PM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com> wrote:
>>
>> On 12/20/2012 04:18 PM, Stanimir Simeonoff wrote:
>> >   >> ByteBuffer buf = ByteBuffer.allocate(100);
>> > This creates a heap byte buffer that's just a wrapped byte[]. So all the
>> > rules for accessing byte[] apply there. Unless the Buffer is readonly
>> > you can always get a reference to the underlying array. IMO, heap
>> > Buffers are not interesting.
>>
>> Understood. Now, is the "putInt" op defined on byte[]? Nope. The only
>> semantically-equivalent operation is Unsafe.putInt() to backing array,
>> and that is surprising that even though Unsafe call would buy me the
>> perceived atomicity, the public BB.putInt() will not.
>>
> This is almost true - since it would work only if there is no tearing and
> the endianess of the Bufffer have to be taken care of prior to the write and
> you need probably happens before along side w/ the endianess, etc. Then
> there come the views and the slicing.
> But again, I have not come up w/ a case where atomically accessing within
> the case process would matter.
>
>>
>> So the opinion I hear from you is along the lines of "everything is OK
>> now, no need for change", right?
>>
> Well, "unmap" for Mapped Buffers would be lovely. The hacking via the

+1.

http://bugs.sun.com/view_bug.do?bug_id=4724038

> cleaner is very dangerous now. 32bit systems can literally run out of
> virtual memory.
> Faster/deterministic reclaim of the direct buffers would be great, since the
> direct buffers take very little heap memory the GC is not so keen on
> collecting them but they eat tons of native memory to the point oom_killer
> can kick in.

From mthornton at optrak.com  Thu Dec 20 11:40:22 2012
From: mthornton at optrak.com (Mark Thornton)
Date: Thu, 20 Dec 2012 16:40:22 +0000
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
Message-ID: <50D33F76.8020604@optrak.com>

On 20/12/12 12:37, Stanimir Simeonoff wrote:
> Well, "unmap" for Mapped Buffers would be lovely. The hacking via the 
> cleaner is very dangerous now. 32bit systems can literally run out of 
> virtual memory.
> Faster/deterministic reclaim of the direct buffers would be great, 
> since the direct buffers take very little heap memory the GC is not so 
> keen on collecting them but they eat tons of native memory to the 
> point oom_killer can kick in.
unmap has been looked at many times and no satisfactory way of 
implementing it has yet been found. It is easy to do in a single 
threaded context, but that isn't Java.

Mark


From elizarov at devexperts.com  Thu Dec 20 13:10:28 2012
From: elizarov at devexperts.com (Roman Elizarov)
Date: Thu, 20 Dec 2012 18:10:28 +0000
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D33F76.8020604@optrak.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>,
	<50D33F76.8020604@optrak.com>
Message-ID: <B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>

It has to be solved. And direct ByteBuffers are not the only problem. Basically, all native resources need a determenisctic GC. You have to have a restricted set of Java classes that GCed via auto ref counting, so that you are guaranteed that they are immediately reclaimed as soon as the last reference to them becomes null. Finalizers were a mistake. Let's admit it, provide a replacement, and deprecate them.

I'd live with JVM-defined set of such classes, but I wish to be able to define my own classes as such with with custom auto release pools, subject to certain automatically-checked restrictions (you have to prove that you cannot have reference cycles). It'd give me so much power in defining more memory-efficient data structures (that is my constant struggle).

On 20.12.2012, at 20:42, "Mark Thornton" <mthornton at optrak.com> wrote:

> On 20/12/12 12:37, Stanimir Simeonoff wrote:
>> Well, "unmap" for Mapped Buffers would be lovely. The hacking via the cleaner is very dangerous now. 32bit systems can literally run out of virtual memory.
>> Faster/deterministic reclaim of the direct buffers would be great, since the direct buffers take very little heap memory the GC is not so keen on collecting them but they eat tons of native memory to the point oom_killer can kick in.
> unmap has been looked at many times and no satisfactory way of implementing it has yet been found. It is easy to do in a single threaded context, but that isn't Java.
> 
> Mark
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From stanimir at riflexo.com  Thu Dec 20 14:07:58 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 20 Dec 2012 21:07:58 +0200
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D33F76.8020604@optrak.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
Message-ID: <CAEJX8oqsRM9ogF=FSynQrWs4UAPL8KTVJEunmifnLDFkN+h0cw@mail.gmail.com>

Of course, unmap is a very hard nut to crack, esp. mutiplatform. A single
threaded version of the MappedByteBuffer was my idea. Before use the JVM
has to prove it's the originating thread that dereferences the Buffer, the
compiler then coalesces the checks so, it's not expensive.
Slices/duplicates/views do inherit the thread owner and follow suit. Then
unmap can be a safe operation.
Presently I can do the same via the cleaner (even the hotspot calls cleaner
directly) but it's a hackery that shall be left behind.

Stanimir







On Thu, Dec 20, 2012 at 6:40 PM, Mark Thornton <mthornton at optrak.com> wrote:

> On 20/12/12 12:37, Stanimir Simeonoff wrote:
>
>> Well, "unmap" for Mapped Buffers would be lovely. The hacking via the
>> cleaner is very dangerous now. 32bit systems can literally run out of
>> virtual memory.
>> Faster/deterministic reclaim of the direct buffers would be great, since
>> the direct buffers take very little heap memory the GC is not so keen on
>> collecting them but they eat tons of native memory to the point oom_killer
>> can kick in.
>>
> unmap has been looked at many times and no satisfactory way of
> implementing it has yet been found. It is easy to do in a single threaded
> context, but that isn't Java.
>
> Mark
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121220/df963abb/attachment.html>

From nathan.reynolds at oracle.com  Thu Dec 20 15:03:01 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 20 Dec 2012 13:03:01 -0700
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>,
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
Message-ID: <50D36EF5.4090205@oracle.com>

 > It has to be solved. And direct ByteBuffers are not the only problem. 
Basically, all native resources need a determenisctic GC.

I have bumped into OutOfMemoryErrors because threads are being created 
and terminating faster than the call stacks are cleaned up.  The thread 
call stacks can't be cleaned up except in a full GC.  Of course, 
creating so many short-lived threads is a problem of the program.  
However, I have hit this problem across several different programs 
including Eclipse IDE yesterday.  It seems programmers think thread 
pools aren't worth the effort, won't be needed or are for only 
performance sensitive code.  "new Thread" should be flagged as a problem 
and thread pools should always be used.

If I remember right, Java guarantees that an OutOfMemoryError won't be 
thrown until after a GC.  This seems to apply only for the Java heap.  
It should also apply for the native heap as well. Before an 
OutOfMemoryError is thrown from native code, a full GC should happen and 
ByteBuffers and thread call stacks should be cleaned up.  If there still 
isn't enough native memory, then throw the OutOfMemoryError.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/20/2012 11:10 AM, Roman Elizarov wrote:
> It has to be solved. And direct ByteBuffers are not the only problem. Basically, all native resources need a determenisctic GC. You have to have a restricted set of Java classes that GCed via auto ref counting, so that you are guaranteed that they are immediately reclaimed as soon as the last reference to them becomes null. Finalizers were a mistake. Let's admit it, provide a replacement, and deprecate them.
>
> I'd live with JVM-defined set of such classes, but I wish to be able to define my own classes as such with with custom auto release pools, subject to certain automatically-checked restrictions (you have to prove that you cannot have reference cycles). It'd give me so much power in defining more memory-efficient data structures (that is my constant struggle).
>
> On 20.12.2012, at 20:42, "Mark Thornton" <mthornton at optrak.com> wrote:
>
>> On 20/12/12 12:37, Stanimir Simeonoff wrote:
>>> Well, "unmap" for Mapped Buffers would be lovely. The hacking via the cleaner is very dangerous now. 32bit systems can literally run out of virtual memory.
>>> Faster/deterministic reclaim of the direct buffers would be great, since the direct buffers take very little heap memory the GC is not so keen on collecting them but they eat tons of native memory to the point oom_killer can kick in.
>> unmap has been looked at many times and no satisfactory way of implementing it has yet been found. It is easy to do in a single threaded context, but that isn't Java.
>>
>> Mark
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121220/c17d9e19/attachment.html>

From stanimir at riflexo.com  Thu Dec 20 16:02:05 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 20 Dec 2012 23:02:05 +0200
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D36EF5.4090205@oracle.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com>
Message-ID: <CAEJX8oq6VWdpW7Z7XSqnDwA7CVyOSCdUgbW744mamLgWYxfE2g@mail.gmail.com>

> However, I have hit this problem across several different programs
> including Eclipse IDE yesterday.  It seems programmers think thread pools
> aren't worth the effort, won't be needed or are for only performance
> sensitive code.  "new Thread" should be flagged as a problem and thread
> pools should always be used.
>
> A not-so-visible issue is using a lot of ThreadLocals, they die trivially
w/ the threads however long-running threads can create leaks and being
lazy, not using thread-pools is way easier. Also not using threads
orthogonally to the tasks is not widely accepted practice.


> If I remember right, Java guarantees that an OutOfMemoryError won't be
> thrown until after a GC.  This seems to apply only for the Java heap.
>
Yes unfortunately, it's only the java heap.

It should also apply for the native heap as well.  Before an
> OutOfMemoryError is thrown from native code, a full GC should happen and
> ByteBuffers and thread call stacks should be cleaned up.
>
The main main issue here is that OOM from native code usually kills the jvm
itself, it has to be checked on each malloc and on linux oom_killer
terminates the process w/o any further notice. Direct bufffers are sort of
easy to track, though. The GC should probably monitor as the creation of a
direct buffer requires process-wide sync, so the allocated memory for
direct buffers is always known.


>   If there still isn't enough native memory, then throw the
> OutOfMemoryError.
>
>
OOM in native code is basically process end as it can occur in any possible
location and the host OS might just kill the process instead returning NULL
to malloc.

Stanimir


> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 12/20/2012 11:10 AM, Roman Elizarov wrote:
>
> It has to be solved. And direct ByteBuffers are not the only problem. Basically, all native resources need a determenisctic GC. You have to have a restricted set of Java classes that GCed via auto ref counting, so that you are guaranteed that they are immediately reclaimed as soon as the last reference to them becomes null. Finalizers were a mistake. Let's admit it, provide a replacement, and deprecate them.
>
> I'd live with JVM-defined set of such classes, but I wish to be able to define my own classes as such with with custom auto release pools, subject to certain automatically-checked restrictions (you have to prove that you cannot have reference cycles). It'd give me so much power in defining more memory-efficient data structures (that is my constant struggle).
>
> On 20.12.2012, at 20:42, "Mark Thornton" <mthornton at optrak.com> <mthornton at optrak.com> wrote:
>
>
>  On 20/12/12 12:37, Stanimir Simeonoff wrote:
>
>  Well, "unmap" for Mapped Buffers would be lovely. The hacking via the cleaner is very dangerous now. 32bit systems can literally run out of virtual memory.
> Faster/deterministic reclaim of the direct buffers would be great, since the direct buffers take very little heap memory the GC is not so keen on collecting them but they eat tons of native memory to the point oom_killer can kick in.
>
>  unmap has been looked at many times and no satisfactory way of implementing it has yet been found. It is easy to do in a single threaded context, but that isn't Java.
>
> Mark
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121220/f3283396/attachment-0001.html>

From elizarov at devexperts.com  Thu Dec 20 16:24:04 2012
From: elizarov at devexperts.com (Roman Elizarov)
Date: Thu, 20 Dec 2012 21:24:04 +0000
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D36EF5.4090205@oracle.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>,
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com>
Message-ID: <C248BCD79E2CBC4B93C0AE3B1E77E9A81E76CC18@RAVEN.office.devexperts.com>

Yes. Full GC on running out of native memory would help a lot as an interim solution. But memory is not the only problem. I've been running out of file handles, out of socket handles, out of graphic resources. You name it! It would be great to have JVM hooks in all those places and force Full GC (and a run of finalizers ;), then retry to acquire the corresponding native resource. It is still a kludge, though, and I'm not sure it can be made to work. It's not clear how you are going to run finalizers while you our running out of resources. JVM has to automagically know that, for example, malloc'd direct ByteBuffers without Java references to them have a pointer to native memory inside should be free'd when running out of native memory, without having to invoke its finalizer.

Forget about native resources. I've found myself multiple times writing data structures that need a strict ownership tracking and resource reclamation in pure Java. In C++ I would have used constructors and destructors to make all the ref count tracking for me transparently and encapsulate everything inside my objects, so that a user of my code just cannot get it wrong (it can lead only to Java-style memory-leaks, when you add objects to an ever-growing list, for example)

In Java I have to manually inspect my code to make sure it follows all the tracking conventions of calling something like "addRefCounter" and "releaseRefCounter" in the right places. I've spent days of my life doing that, only to found that the code is correct and the problems we were seeing in production were likely caused by JIT/HotSpot miscomplication of our code.

What's worse, is that I cannot create a framework that exposes these methods, because I cannot trust other code to follow the conventions. It's too easy to make a mistake. It's like having to remember to "removeListener" in pair with every "addListnener". Everybody forgets it. That's why almost everybody who does large-scale Java GUI development employs some kind of a framework that makes "removeListener" automatic, so that you can to write your code only in one place.

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nathan Reynolds
Sent: Friday, December 21, 2012 12:03 AM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] java.nio.ByteBuffer atomicity

> It has to be solved. And direct ByteBuffers are not the only problem. Basically, all native resources need a determenisctic GC.

I have bumped into OutOfMemoryErrors because threads are being created and terminating faster than the call stacks are cleaned up.  The thread call stacks can't be cleaned up except in a full GC.  Of course, creating so many short-lived threads is a problem of the program.  However, I have hit this problem across several different programs including Eclipse IDE yesterday.  It seems programmers think thread pools aren't worth the effort, won't be needed or are for only performance sensitive code.  "new Thread" should be flagged as a problem and thread pools should always be used.

If I remember right, Java guarantees that an OutOfMemoryError won't be thrown until after a GC.  This seems to apply only for the Java heap.  It should also apply for the native heap as well.  Before an OutOfMemoryError is thrown from native code, a full GC should happen and ByteBuffers and thread call stacks should be cleaned up.  If there still isn't enough native memory, then throw the OutOfMemoryError.
Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | Architect | 602.333.9091
Oracle PSR Engineering<http://psr.us.oracle.com/> | Server Technology
On 12/20/2012 11:10 AM, Roman Elizarov wrote:

It has to be solved. And direct ByteBuffers are not the only problem. Basically, all native resources need a determenisctic GC. You have to have a restricted set of Java classes that GCed via auto ref counting, so that you are guaranteed that they are immediately reclaimed as soon as the last reference to them becomes null. Finalizers were a mistake. Let's admit it, provide a replacement, and deprecate them.



I'd live with JVM-defined set of such classes, but I wish to be able to define my own classes as such with with custom auto release pools, subject to certain automatically-checked restrictions (you have to prove that you cannot have reference cycles). It'd give me so much power in defining more memory-efficient data structures (that is my constant struggle).



On 20.12.2012, at 20:42, "Mark Thornton" <mthornton at optrak.com><mailto:mthornton at optrak.com> wrote:



On 20/12/12 12:37, Stanimir Simeonoff wrote:

Well, "unmap" for Mapped Buffers would be lovely. The hacking via the cleaner is very dangerous now. 32bit systems can literally run out of virtual memory.

Faster/deterministic reclaim of the direct buffers would be great, since the direct buffers take very little heap memory the GC is not so keen on collecting them but they eat tons of native memory to the point oom_killer can kick in.

unmap has been looked at many times and no satisfactory way of implementing it has yet been found. It is easy to do in a single threaded context, but that isn't Java.



Mark



_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121220/bc241f5b/attachment.html>

From mthornton at optrak.com  Fri Dec 21 03:29:45 2012
From: mthornton at optrak.com (Mark Thornton)
Date: Fri, 21 Dec 2012 08:29:45 +0000
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D36EF5.4090205@oracle.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>,
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com>
Message-ID: <50D41DF9.7060908@optrak.com>

On 20/12/12 20:03, Nathan Reynolds wrote:
> If I remember right, Java guarantees that an OutOfMemoryError won't be 
> thrown until after a GC.  This seems to apply only for the Java heap.  
> It should also apply for the native heap as well.
I believe the JVM was changed so that a failing request for a direct 
ByteBuffer would trigger gc in an attempt to release any that were no 
longer reachable.

Mark Thornton


From stanimir at riflexo.com  Fri Dec 21 04:06:42 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Fri, 21 Dec 2012 11:06:42 +0200
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D41DF9.7060908@optrak.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
Message-ID: <CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>

On Fri, Dec 21, 2012 at 10:29 AM, Mark Thornton <mthornton at optrak.com>wrote:

> On 20/12/12 20:03, Nathan Reynolds wrote:
>
>> If I remember right, Java guarantees that an OutOfMemoryError won't be
>> thrown until after a GC.  This seems to apply only for the Java heap.  It
>> should also apply for the native heap as well.
>>
> I believe the JVM was changed so that a failing request for a direct
> ByteBuffer would trigger gc in an attempt to release any that were no
> longer reachable.
>
> That works only when -XX:MaxDirectMemorySize is specified properly
(java.nio.Bits::reserveMemory) so there is a known hard limit on the max
memory for direct buffers. If the setting is left unattended it well may
end w/ process termination rather than an OOM.

Stanimir



>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121221/3e9b7dc8/attachment.html>

From Alan.Bateman at oracle.com  Fri Dec 21 06:05:03 2012
From: Alan.Bateman at oracle.com (Alan Bateman)
Date: Fri, 21 Dec 2012 11:05:03 +0000
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
Message-ID: <50D4425F.1080803@oracle.com>

On 21/12/2012 09:06, Stanimir Simeonoff wrote:
>
>
> On Fri, Dec 21, 2012 at 10:29 AM, Mark Thornton <mthornton at optrak.com 
> <mailto:mthornton at optrak.com>> wrote:
>
>     On 20/12/12 20:03, Nathan Reynolds wrote:
>
>         If I remember right, Java guarantees that an OutOfMemoryError
>         won't be thrown until after a GC.  This seems to apply only
>         for the Java heap.  It should also apply for the native heap
>         as well.
>
>     I believe the JVM was changed so that a failing request for a
>     direct ByteBuffer would trigger gc in an attempt to release any
>     that were no longer reachable.
>
> That works only when -XX:MaxDirectMemorySize is specified properly 
> (java.nio.Bits::reserveMemory) so there is a known hard limit on the 
> max memory for direct buffers. If the setting is left unattended it 
> well may end w/ process termination rather than an OOM.
If MaxDirectMemorySize isn't specified then it defaults to the same size 
as the heap. One useful thing to know is that we added instrumentation 
in jdk7 so you can monitor the number of direct buffers in use and the 
memory with any JMX based monitoring tool. This can be useful for tuning 
MaxDirectMemorySize where needed.

On Mark Thornton's comment, then it does do a GC when the limit is 
reached but it lacks feedback so it is still possible to get OOME even 
though there may be reference queued that would free enough memory to 
satisfy the request.

Someone else mentioned short-lived threads and ThreadLocal caches, this 
is also something that could be improved as these cached buffers could 
be eagerly released when threads terminate.

In any case, I guess this is all a bit removed from the original topic.

-Alan.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121221/a5e1dc99/attachment-0001.html>

From zhong.j.yu at gmail.com  Fri Dec 21 11:44:45 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Fri, 21 Dec 2012 10:44:45 -0600
Subject: [concurrency-interest] java.nio.ByteBuffer atomicity
In-Reply-To: <50D4425F.1080803@oracle.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
Message-ID: <CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>

On Fri, Dec 21, 2012 at 5:05 AM, Alan Bateman <Alan.Bateman at oracle.com> wrote:
> On 21/12/2012 09:06, Stanimir Simeonoff wrote:
>
>
>
> On Fri, Dec 21, 2012 at 10:29 AM, Mark Thornton <mthornton at optrak.com>
> wrote:
>>
>> On 20/12/12 20:03, Nathan Reynolds wrote:
>>>
>>> If I remember right, Java guarantees that an OutOfMemoryError won't be
>>> thrown until after a GC.  This seems to apply only for the Java heap.  It
>>> should also apply for the native heap as well.
>>
>> I believe the JVM was changed so that a failing request for a direct
>> ByteBuffer would trigger gc in an attempt to release any that were no longer
>> reachable.
>>
> That works only when -XX:MaxDirectMemorySize is specified properly
> (java.nio.Bits::reserveMemory) so there is a known hard limit on the max
> memory for direct buffers. If the setting is left unattended it well may end
> w/ process termination rather than an OOM.
>
> If MaxDirectMemorySize isn't specified then it defaults to the same size as
> the heap. One useful thing to know is that we added instrumentation in jdk7
> so you can monitor the number of direct buffers in use and the memory with
> any JMX based monitoring tool. This can be useful for tuning
> MaxDirectMemorySize where needed.
>
> On Mark Thornton's comment, then it does do a GC when the limit is reached
> but it lacks feedback so it is still possible to get OOME even though there
> may be reference queued that would free enough memory to satisfy the
> request.
>
> Someone else mentioned short-lived threads and ThreadLocal caches, this is
> also something that could be improved as these cached buffers could be
> eagerly released when threads terminate.
>
> In any case, I guess this is all a bit removed from the original topic.

It would be really really nice to be able to properly unmap/dealloc a
direct buffer. If the solution is to have to externally lock the
buffer before access, it's a very small price to pay. Not being able
to unmap/dealloc is a crippling deficiency, it's like file channels
have no close() method. It is killing me in my work (where I can't use
the Cleaner hack since buffers are exposed to 3rd party).

As others have said, we could introduce a new/simpler class for direct
memory access, with features like alignment, atomicity and dealloc.
It's probably limited to a single thread at any time. It doesn't need
position/limit/clones/views, but it can be converted to ByteBuffer.

Zhong Yu

From david.lloyd at redhat.com  Fri Dec 21 13:08:35 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Fri, 21 Dec 2012 12:08:35 -0600
Subject: [concurrency-interest] Unmapping byte buffers (was:
 java.nio.ByteBuffer atomicity)
In-Reply-To: <CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
	<CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
Message-ID: <50D4A5A3.1040103@redhat.com>

On 12/21/2012 10:44 AM, Zhong Yu wrote:
> It would be really really nice to be able to properly unmap/dealloc a
> direct buffer. If the solution is to have to externally lock the
> buffer before access, it's a very small price to pay. Not being able
> to unmap/dealloc is a crippling deficiency, it's like file channels
> have no close() method. It is killing me in my work (where I can't use
> the Cleaner hack since buffers are exposed to 3rd party).
>
> As others have said, we could introduce a new/simpler class for direct
> memory access, with features like alignment, atomicity and dealloc.
> It's probably limited to a single thread at any time. It doesn't need
> position/limit/clones/views, but it can be converted to ByteBuffer.

It seems to me that supporting unmapping is only possible if the process 
has control over its own address space.  For example on Linux we could 
choose the address of our buffer using MAP_FIXED, so that deallocation 
can simply unmap the buffers.  Any SIGSEGV with a corresponding fault 
address can be mapped to a sensible exception type.  When the buffer is 
GCed, the corresponding address range can be made available to the 
buffer allocator again.

When address space is limited (e.g. 32-bit) the same GC mechanism that 
is currently used for unmapping could conceivably be used for freeing 
reserved address ranges.

Unfortunately this all falls apart on platforms without the ability to 
control mapping addresses.

-- 
- DML

From david.lloyd at redhat.com  Fri Dec 21 13:16:52 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Fri, 21 Dec 2012 12:16:52 -0600
Subject: [concurrency-interest] Unmapping byte buffers
In-Reply-To: <50D4A5A3.1040103@redhat.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
	<CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
	<50D4A5A3.1040103@redhat.com>
Message-ID: <50D4A794.3090004@redhat.com>

On 12/21/2012 12:08 PM, David M. Lloyd wrote:
> On 12/21/2012 10:44 AM, Zhong Yu wrote:
>> It would be really really nice to be able to properly unmap/dealloc a
>> direct buffer. If the solution is to have to externally lock the
>> buffer before access, it's a very small price to pay. Not being able
>> to unmap/dealloc is a crippling deficiency, it's like file channels
>> have no close() method. It is killing me in my work (where I can't use
>> the Cleaner hack since buffers are exposed to 3rd party).
>>
>> As others have said, we could introduce a new/simpler class for direct
>> memory access, with features like alignment, atomicity and dealloc.
>> It's probably limited to a single thread at any time. It doesn't need
>> position/limit/clones/views, but it can be converted to ByteBuffer.
>
> It seems to me that supporting unmapping is only possible if the process
> has control over its own address space.  For example on Linux we could
> choose the address of our buffer using MAP_FIXED, so that deallocation
> can simply unmap the buffers.  Any SIGSEGV with a corresponding fault
> address can be mapped to a sensible exception type.  When the buffer is
> GCed, the corresponding address range can be made available to the
> buffer allocator again.
>
> When address space is limited (e.g. 32-bit) the same GC mechanism that
> is currently used for unmapping could conceivably be used for freeing
> reserved address ranges.
>
> Unfortunately this all falls apart on platforms without the ability to
> control mapping addresses.

Though a bit of quick research suggests that MAP_FIXED or a direct 
equivalent exists for Mac OS X, Linux, BSD, Solaris, Windows, and z/OS, 
so maybe it's a viable idea...

-- 
- DML

From mthornton at optrak.com  Fri Dec 21 13:28:03 2012
From: mthornton at optrak.com (Mark Thornton)
Date: Fri, 21 Dec 2012 18:28:03 +0000
Subject: [concurrency-interest] Unmapping byte buffers
In-Reply-To: <50D4A5A3.1040103@redhat.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
	<CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
	<50D4A5A3.1040103@redhat.com>
Message-ID: <50D4AA33.4070807@optrak.com>

On 21/12/12 18:08, David M. Lloyd wrote:
> On 12/21/2012 10:44 AM, Zhong Yu wrote:
>> It would be really really nice to be able to properly unmap/dealloc a
>> direct buffer. If the solution is to have to externally lock the
>> buffer before access, it's a very small price to pay. Not being able
>> to unmap/dealloc is a crippling deficiency, it's like file channels
>> have no close() method. It is killing me in my work (where I can't use
>> the Cleaner hack since buffers are exposed to 3rd party).
>>
>> As others have said, we could introduce a new/simpler class for direct
>> memory access, with features like alignment, atomicity and dealloc.
>> It's probably limited to a single thread at any time. It doesn't need
>> position/limit/clones/views, but it can be converted to ByteBuffer.
>
> It seems to me that supporting unmapping is only possible if the 
> process has control over its own address space.  For example on Linux 
> we could choose the address of our buffer using MAP_FIXED, so that 
> deallocation can simply unmap the buffers.  Any SIGSEGV with a 
> corresponding fault address can be mapped to a sensible exception 
> type.  When the buffer is GCed, the corresponding address range can be 
> made available to the buffer allocator again.
>
> When address space is limited (e.g. 32-bit) the same GC mechanism that 
> is currently used for unmapping could conceivably be used for freeing 
> reserved address ranges.
>
> Unfortunately this all falls apart on platforms without the ability to 
> control mapping addresses.
>

You can't be sure that access to an unmapped buffer will result in an 
exception --- other code in the process (not necessarily Java) may have 
subsequently mapped the address. The 'unmap' operation needs to keep the 
address range reserved until all buffers referencing it are unreachable. 
This would help with unlocking files (a mapped file can't be deleted on 
Windows), but not with address space shortage.

Mark



From hans.boehm at hp.com  Fri Dec 21 13:37:21 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 21 Dec 2012 18:37:21 +0000
Subject: [concurrency-interest] Unmapping byte buffers
In-Reply-To: <50D4A794.3090004@redhat.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
	<CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
	<50D4A5A3.1040103@redhat.com> <50D4A794.3090004@redhat.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD236965B4C@G9W0725.americas.hpqcorp.net>

> From: David M. Lloyd
> 
> On 12/21/2012 12:08 PM, David M. Lloyd wrote:
> > On 12/21/2012 10:44 AM, Zhong Yu wrote:
> >> It would be really really nice to be able to properly unmap/dealloc
> a
> >> direct buffer. If the solution is to have to externally lock the
> >> buffer before access, it's a very small price to pay. Not being able
> >> to unmap/dealloc is a crippling deficiency, it's like file channels
> >> have no close() method. It is killing me in my work (where I can't
> use
> >> the Cleaner hack since buffers are exposed to 3rd party).
> >>
> >> As others have said, we could introduce a new/simpler class for
> direct
> >> memory access, with features like alignment, atomicity and dealloc.
> >> It's probably limited to a single thread at any time. It doesn't
> need
> >> position/limit/clones/views, but it can be converted to ByteBuffer.
> >
> > It seems to me that supporting unmapping is only possible if the
> process
> > has control over its own address space.  For example on Linux we
> could
> > choose the address of our buffer using MAP_FIXED, so that
> deallocation
> > can simply unmap the buffers.  Any SIGSEGV with a corresponding fault
> > address can be mapped to a sensible exception type.  When the buffer
> is
> > GCed, the corresponding address range can be made available to the
> > buffer allocator again.
> >
> > When address space is limited (e.g. 32-bit) the same GC mechanism
> that
> > is currently used for unmapping could conceivably be used for freeing
> > reserved address ranges.
> >
> > Unfortunately this all falls apart on platforms without the ability
> to
> > control mapping addresses.
> 
> Though a bit of quick research suggests that MAP_FIXED or a direct
> equivalent exists for Mac OS X, Linux, BSD, Solaris, Windows, and z/OS,
> so maybe it's a viable idea...
> 
On Linux et al, you probably don't actually want to unmap the space, since that opens it up for reuse by native code, e.g. to map a newly loaded dynamic library.  You instead want to remap it with no access against /dev/zero or the like, to keep it reserved without using memory.  But that still has all the same issues you point out above; in particular you need MAP_FIXED, and you need enough address space.

Hans


From elizarov at devexperts.com  Fri Dec 21 13:50:45 2012
From: elizarov at devexperts.com (Roman Elizarov)
Date: Fri, 21 Dec 2012 18:50:45 +0000
Subject: [concurrency-interest] Unmapping byte buffers (was:
 java.nio.ByteBuffer atomicity)
In-Reply-To: <50D4A5A3.1040103@redhat.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
	<CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
	<50D4A5A3.1040103@redhat.com>
Message-ID: <C248BCD79E2CBC4B93C0AE3B1E77E9A81E76D62E@RAVEN.office.devexperts.com>

Unmap/free support is totally possible in a platform-independent way with automatic runtime-enforced reference counting to all native memory locations. References to native memory areas (like DirectByteBuffers) shall be special objects recognized by JVM, that count the number of direct references to them (with RT&JIT support of such counting, of course). Unmap/free of native memory shall happen immediately as the last direct reference disappears. That's it. "unmap" or "free" methods just cannot exist in the managed world of safe Java code. There is no safe way to make them work (of course, you can do it in Unsafe way, but there is no safe way to do it, that is what I'm saying).

"Finalizers" for native memory references should stay as a stop-gap measure in cases when the user of direct memory had lost them inside their object tree. Finalizer shall complain as loudly as possible that the native memory was "lost" to alert developers to go and fix their memory-management bugs. Yes, if you work with native memory, then it's your responsibility to manage it, just like you manage it in C or C++, but instead of calling "unmap" or "free", you go and clear all references to it (it is easy just to keep a single reference that is encapsulated in one of your own objects). Runtime can only catch some of your memory management bugs, but it cannot manage your native memory for you. Naturally, managed runtime can only manage managed memory (pun intended). You manage unmanaged memory yourself.

I've spent a lot of time thinking about it, and that is the only platform-independent and safe approach to unmap/free of native memory that I can think of.

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David M. Lloyd
Sent: Friday, December 21, 2012 10:09 PM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Unmapping byte buffers (was: java.nio.ByteBuffer atomicity)

On 12/21/2012 10:44 AM, Zhong Yu wrote:
> It would be really really nice to be able to properly unmap/dealloc a 
> direct buffer. If the solution is to have to externally lock the 
> buffer before access, it's a very small price to pay. Not being able 
> to unmap/dealloc is a crippling deficiency, it's like file channels 
> have no close() method. It is killing me in my work (where I can't use 
> the Cleaner hack since buffers are exposed to 3rd party).
>
> As others have said, we could introduce a new/simpler class for direct 
> memory access, with features like alignment, atomicity and dealloc.
> It's probably limited to a single thread at any time. It doesn't need 
> position/limit/clones/views, but it can be converted to ByteBuffer.

It seems to me that supporting unmapping is only possible if the process has control over its own address space.  For example on Linux we could choose the address of our buffer using MAP_FIXED, so that deallocation can simply unmap the buffers.  Any SIGSEGV with a corresponding fault address can be mapped to a sensible exception type.  When the buffer is GCed, the corresponding address range can be made available to the buffer allocator again.

When address space is limited (e.g. 32-bit) the same GC mechanism that is currently used for unmapping could conceivably be used for freeing reserved address ranges.

Unfortunately this all falls apart on platforms without the ability to control mapping addresses.

--
- DML
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From zhong.j.yu at gmail.com  Fri Dec 21 15:46:21 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Fri, 21 Dec 2012 14:46:21 -0600
Subject: [concurrency-interest] Unmapping byte buffers
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236965B4C@G9W0725.americas.hpqcorp.net>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
	<CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
	<50D4A5A3.1040103@redhat.com> <50D4A794.3090004@redhat.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236965B4C@G9W0725.americas.hpqcorp.net>
Message-ID: <CACuKZqHGf8taAMuNRqK8AmvwDgq6Mbv6oX3h=x8NaSGs_itZVw@mail.gmail.com>

My understanding is that, if all methods are synchronized, unmap() is trivial:

    synchronized
    byte get(index)
        assertValid();
        ...

    synchronized
    void unmap()
        markInvalid();
        ...

The problem is get() would be very slow.

What if the lock is externalized, so user can arrange lock coarsening
over many get() calls. Within get() we only check whether the current
thread holds the lock, which is a very cheap operation.

Zhong Yu

From elizarov at devexperts.com  Fri Dec 21 15:59:09 2012
From: elizarov at devexperts.com (Roman Elizarov)
Date: Fri, 21 Dec 2012 20:59:09 +0000
Subject: [concurrency-interest] Unmapping byte buffers
In-Reply-To: <CACuKZqHGf8taAMuNRqK8AmvwDgq6Mbv6oX3h=x8NaSGs_itZVw@mail.gmail.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
	<CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
	<50D4A5A3.1040103@redhat.com> <50D4A794.3090004@redhat.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236965B4C@G9W0725.americas.hpqcorp.net>,
	<CACuKZqHGf8taAMuNRqK8AmvwDgq6Mbv6oX3h=x8NaSGs_itZVw@mail.gmail.com>
Message-ID: <A4A24F7E-4273-41B9-BDC7-A67105E2DBB2@devexperts.com>

The only problem is that we want concurrent access to native memory to be allowed, because native memory is a key to many memory-efficient and concurrent data structures. 

On 22.12.2012, at 0:50, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:

> My understanding is that, if all methods are synchronized, unmap() is trivial:
> 
>    synchronized
>    byte get(index)
>        assertValid();
>        ...
> 
>    synchronized
>    void unmap()
>        markInvalid();
>        ...
> 
> The problem is get() would be very slow.
> 
> What if the lock is externalized, so user can arrange lock coarsening
> over many get() calls. Within get() we only check whether the current
> thread holds the lock, which is a very cheap operation.
> 
> Zhong Yu
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathan.reynolds at oracle.com  Fri Dec 21 16:02:46 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 21 Dec 2012 14:02:46 -0700
Subject: [concurrency-interest] Unmapping byte buffers
In-Reply-To: <CACuKZqHGf8taAMuNRqK8AmvwDgq6Mbv6oX3h=x8NaSGs_itZVw@mail.gmail.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
	<CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
	<50D4A5A3.1040103@redhat.com> <50D4A794.3090004@redhat.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236965B4C@G9W0725.americas.hpqcorp.net>
	<CACuKZqHGf8taAMuNRqK8AmvwDgq6Mbv6oX3h=x8NaSGs_itZVw@mail.gmail.com>
Message-ID: <50D4CE76.7070806@oracle.com>

If the ByteBuffer doesn't escape, then the lock can be elided completely 
from the instruction stream. Otherwise, biased locking can remove much 
of the cost of the lock.  This is where the thread acquires the lock 
once and then never releases.  If another thread wants the lock, it has 
to suspend the owner thread, scan its stack and set the lock to the 
proper state.  So, biased locking should be pretty cheep just 10s of 
cycles to verify that the current thread is the lock owner.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/21/2012 1:46 PM, Zhong Yu wrote:
> My understanding is that, if all methods are synchronized, unmap() is trivial:
>
>      synchronized
>      byte get(index)
>          assertValid();
>          ...
>
>      synchronized
>      void unmap()
>          markInvalid();
>          ...
>
> The problem is get() would be very slow.
>
> What if the lock is externalized, so user can arrange lock coarsening
> over many get() calls. Within get() we only check whether the current
> thread holds the lock, which is a very cheap operation.
>
> Zhong Yu
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121221/89bb11a3/attachment.html>

From zhong.j.yu at gmail.com  Fri Dec 21 16:21:30 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Fri, 21 Dec 2012 15:21:30 -0600
Subject: [concurrency-interest] Unmapping byte buffers
In-Reply-To: <A4A24F7E-4273-41B9-BDC7-A67105E2DBB2@devexperts.com>
References: <50D2F993.4030901@oracle.com>
	<CAEJX8ooA0xjA5Q0PG513gouJROfQ9JCcSh8WRBgP2nPnG0+pAQ@mail.gmail.com>
	<50D30390.4040002@oracle.com>
	<CAEJX8opnb6iQ6Lm4gC6q5h7MC47+GRZQpCykKkrULGHzXhOJyg@mail.gmail.com>
	<50D33F76.8020604@optrak.com>
	<B7180537-7A82-4966-99FC-2FAE4CB2918B@devexperts.com>
	<50D36EF5.4090205@oracle.com> <50D41DF9.7060908@optrak.com>
	<CAEJX8orqPPXy2G5ij508W-4UmxhFNmmmrz8fdqCOpudZg3EHSQ@mail.gmail.com>
	<50D4425F.1080803@oracle.com>
	<CACuKZqH_iDt7TUZTf-eB8f+m3s3WCiQMUGzHjTDNm-bnJbEukA@mail.gmail.com>
	<50D4A5A3.1040103@redhat.com> <50D4A794.3090004@redhat.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236965B4C@G9W0725.americas.hpqcorp.net>
	<CACuKZqHGf8taAMuNRqK8AmvwDgq6Mbv6oX3h=x8NaSGs_itZVw@mail.gmail.com>
	<A4A24F7E-4273-41B9-BDC7-A67105E2DBB2@devexperts.com>
Message-ID: <CACuKZqGPaYjn9Cw4b5AP9xZHxfKunsVFR1V0FBPYxB9aZhcwoQ@mail.gmail.com>

The lock for unmap is exclusive. The lock for read/write can be shared
- but then I don't know if there's a cheap way to check lock
ownership. A solution I can think of is for each thread to have its
own view; the view enforces that it's only accessed by the owner
thread.

On Fri, Dec 21, 2012 at 2:59 PM, Roman Elizarov <elizarov at devexperts.com> wrote:
> The only problem is that we want concurrent access to native memory to be allowed, because native memory is a key to many memory-efficient and concurrent data structures.
>
> On 22.12.2012, at 0:50, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
>
>> My understanding is that, if all methods are synchronized, unmap() is trivial:
>>
>>    synchronized
>>    byte get(index)
>>        assertValid();
>>        ...
>>
>>    synchronized
>>    void unmap()
>>        markInvalid();
>>        ...
>>
>> The problem is get() would be very slow.
>>
>> What if the lock is externalized, so user can arrange lock coarsening
>> over many get() calls. Within get() we only check whether the current
>> thread holds the lock, which is a very cheap operation.
>>
>> Zhong Yu
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From zhong.j.yu at gmail.com  Fri Dec 21 16:48:53 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Fri, 21 Dec 2012 15:48:53 -0600
Subject: [concurrency-interest] Unmapping byte buffers
Message-ID: <CACuKZqGr2oxyE3GzmVaQwPR_Cah2YAP+bde9=8sNn4AGC+kNzw@mail.gmail.com>

On Fri, Dec 21, 2012 at 12:50 PM, Roman Elizarov
<elizarov at devexperts.com> wrote:
> Unmap/free support is totally possible in a platform-independent way with automatic runtime-enforced reference counting to all native memory locations. References to native memory areas (like DirectByteBuffers) shall be special objects recognized by JVM, that count the number of direct references to them (with RT&JIT support of such counting, of course).

Performance-wise, a thread obtaining/discarding such a reference is
similar to acquiring/releasing a lock, correct?

Unmap/free of native memory shall happen immediately as the last
direct reference disappears. That's it. "unmap" or "free" methods just
cannot exist in the managed world of safe Java code. There is no safe
way to make them work (of course, you can do it in Unsafe way, but
there is no safe way to do it, that is what I'm saying).
>
> "Finalizers" for native memory references should stay as a stop-gap measure in cases when the user of direct memory had lost them inside their object tree. Finalizer shall complain as loudly as possible that the native memory was "lost" to alert developers to go and fix their memory-management bugs. Yes, if you work with native memory, then it's your responsibility to manage it, just like you manage it in C or C++, but instead of calling "unmap" or "free", you go and clear all references to it (it is easy just to keep a single reference that is encapsulated in one of your own objects). Runtime can only catch some of your memory management bugs, but it cannot manage your native memory for you. Naturally, managed runtime can only manage managed memory (pun intended). You manage unmanaged memory yourself.
>
> I've spent a lot of time thinking about it, and that is the only platform-independent and safe approach to unmap/free of native memory that I can think of.
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David M. Lloyd
> Sent: Friday, December 21, 2012 10:09 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Unmapping byte buffers (was: java.nio.ByteBuffer atomicity)
>
> On 12/21/2012 10:44 AM, Zhong Yu wrote:
>> It would be really really nice to be able to properly unmap/dealloc a
>> direct buffer. If the solution is to have to externally lock the
>> buffer before access, it's a very small price to pay. Not being able
>> to unmap/dealloc is a crippling deficiency, it's like file channels
>> have no close() method. It is killing me in my work (where I can't use
>> the Cleaner hack since buffers are exposed to 3rd party).
>>
>> As others have said, we could introduce a new/simpler class for direct
>> memory access, with features like alignment, atomicity and dealloc.
>> It's probably limited to a single thread at any time. It doesn't need
>> position/limit/clones/views, but it can be converted to ByteBuffer.
>
> It seems to me that supporting unmapping is only possible if the process has control over its own address space.  For example on Linux we could choose the address of our buffer using MAP_FIXED, so that deallocation can simply unmap the buffers.  Any SIGSEGV with a corresponding fault address can be mapped to a sensible exception type.  When the buffer is GCed, the corresponding address range can be made available to the buffer allocator again.
>
> When address space is limited (e.g. 32-bit) the same GC mechanism that is currently used for unmapping could conceivably be used for freeing reserved address ranges.
>
> Unfortunately this all falls apart on platforms without the ability to control mapping addresses.
>
> --
> - DML
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From david.lloyd at redhat.com  Fri Dec 21 17:41:53 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Fri, 21 Dec 2012 16:41:53 -0600
Subject: [concurrency-interest] Unmapping byte buffers
In-Reply-To: <CACuKZqGr2oxyE3GzmVaQwPR_Cah2YAP+bde9=8sNn4AGC+kNzw@mail.gmail.com>
References: <CACuKZqGr2oxyE3GzmVaQwPR_Cah2YAP+bde9=8sNn4AGC+kNzw@mail.gmail.com>
Message-ID: <50D4E5B1.80608@redhat.com>

On 12/21/2012 03:48 PM, Zhong Yu wrote:
> On Fri, Dec 21, 2012 at 12:50 PM, Roman Elizarov
> <elizarov at devexperts.com> wrote:
>> Unmap/free support is totally possible in a platform-independent way with automatic runtime-enforced reference counting to all native memory locations. References to native memory areas (like DirectByteBuffers) shall be special objects recognized by JVM, that count the number of direct references to them (with RT&JIT support of such counting, of course).
>
> Performance-wise, a thread obtaining/discarding such a reference is
> similar to acquiring/releasing a lock, correct?
>
> Unmap/free of native memory shall happen immediately as the last
> direct reference disappears. That's it. "unmap" or "free" methods just
> cannot exist in the managed world of safe Java code. There is no safe
> way to make them work (of course, you can do it in Unsafe way, but
> there is no safe way to do it, that is what I'm saying).

It depends on what you are most interested in reclaiming.  If you just 
want the memory back, then remapping that address space with 
non-accessible pages will do the trick (as Mark Thornton points out, 
it's not enough just to unmap the memory because something else in the 
process might remap it when you're not looking).  If you want to reclaim 
the address space then you have to track the references and unmap the 
memory once they're all (provably) gone.  I think aggressive address 
space reclamation is probably more important on systems with constrained 
address spaces.  On a typical modern 64-bit system, using a finalizer to 
free up mappings is probably sufficient (and would mean that no 
reference counting is necessary).  You'd need a finalizer in any case to 
ensure that memory areas aren't leaked.

-- 
- DML

From elizarov at devexperts.com  Sat Dec 22 00:19:07 2012
From: elizarov at devexperts.com (Roman Elizarov)
Date: Sat, 22 Dec 2012 05:19:07 +0000
Subject: [concurrency-interest] Unmapping byte buffers
In-Reply-To: <CACuKZqGr2oxyE3GzmVaQwPR_Cah2YAP+bde9=8sNn4AGC+kNzw@mail.gmail.com>
References: <CACuKZqGr2oxyE3GzmVaQwPR_Cah2YAP+bde9=8sNn4AGC+kNzw@mail.gmail.com>
Message-ID: <77E20664-F32B-4970-A7FA-AA5056CB2C31@devexperts.com>

It is implemented with "lock inc" and "lock dec" on x86. It is conceptually a kind of read lock, but without a paired write lock and without all the wait-lists infrastructure (since you never wait) and without corresponding check, so it is faster and simpler. It should be optimized in a way similar to lock, with coarsening by JIT being a key optimization to make performance-critical code in inner loops work at a native speed.

On 22.12.2012, at 1:49, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:

> On Fri, Dec 21, 2012 at 12:50 PM, Roman Elizarov
> <elizarov at devexperts.com> wrote:
>> Unmap/free support is totally possible in a platform-independent way with automatic runtime-enforced reference counting to all native memory locations. References to native memory areas (like DirectByteBuffers) shall be special objects recognized by JVM, that count the number of direct references to them (with RT&JIT support of such counting, of course).
> 
> Performance-wise, a thread obtaining/discarding such a reference is
> similar to acquiring/releasing a lock, correct?
> 
> Unmap/free of native memory shall happen immediately as the last
> direct reference disappears. That's it. "unmap" or "free" methods just
> cannot exist in the managed world of safe Java code. There is no safe
> way to make them work (of course, you can do it in Unsafe way, but
> there is no safe way to do it, that is what I'm saying).
>> 
>> "Finalizers" for native memory references should stay as a stop-gap measure in cases when the user of direct memory had lost them inside their object tree. Finalizer shall complain as loudly as possible that the native memory was "lost" to alert developers to go and fix their memory-management bugs. Yes, if you work with native memory, then it's your responsibility to manage it, just like you manage it in C or C++, but instead of calling "unmap" or "free", you go and clear all references to it (it is easy just to keep a single reference that is encapsulated in one of your own objects). Runtime can only catch some of your memory management bugs, but it cannot manage your native memory for you. Naturally, managed runtime can only manage managed memory (pun intended). You manage unmanaged memory yourself.
>> 
>> I've spent a lot of time thinking about it, and that is the only platform-independent and safe approach to unmap/free of native memory that I can think of.
>> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David M. Lloyd
>> Sent: Friday, December 21, 2012 10:09 PM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Unmapping byte buffers (was: java.nio.ByteBuffer atomicity)
>> 
>> On 12/21/2012 10:44 AM, Zhong Yu wrote:
>>> It would be really really nice to be able to properly unmap/dealloc a
>>> direct buffer. If the solution is to have to externally lock the
>>> buffer before access, it's a very small price to pay. Not being able
>>> to unmap/dealloc is a crippling deficiency, it's like file channels
>>> have no close() method. It is killing me in my work (where I can't use
>>> the Cleaner hack since buffers are exposed to 3rd party).
>>> 
>>> As others have said, we could introduce a new/simpler class for direct
>>> memory access, with features like alignment, atomicity and dealloc.
>>> It's probably limited to a single thread at any time. It doesn't need
>>> position/limit/clones/views, but it can be converted to ByteBuffer.
>> 
>> It seems to me that supporting unmapping is only possible if the process has control over its own address space.  For example on Linux we could choose the address of our buffer using MAP_FIXED, so that deallocation can simply unmap the buffers.  Any SIGSEGV with a corresponding fault address can be mapped to a sensible exception type.  When the buffer is GCed, the corresponding address range can be made available to the buffer allocator again.
>> 
>> When address space is limited (e.g. 32-bit) the same GC mechanism that is currently used for unmapping could conceivably be used for freeing reserved address ranges.
>> 
>> Unfortunately this all falls apart on platforms without the ability to control mapping addresses.
>> 
>> --
>> - DML
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Thu Dec 27 13:34:54 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 27 Dec 2012 13:34:54 -0500
Subject: [concurrency-interest] CompletableFuture
Message-ID: <50DC94CE.5050407@cs.oswego.edu>


An initial version of JDK8 java.util.concurrent.CompletableFuture
is now available. See javadocs at:
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
and source at
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log

This class implements functionality that has variously been
discussed under names SettableFuture, FutureValue, Promise, and
probably others.

Because of heavy use of function-types, this version is only
compilable/usable using recent JDK8-lambda builds (I think
including the current one at http://jdk8.java.net/lambda).
It would be possible to create a jsr166e version that defined
nested interface types to be usable stand-alone under JDK7,
but it would be at best unpleasant to use without lambda support.

The basic design is a semi-fluent API in which you can arrange:
  (sequential or async)
  (functions or actions)
triggered on completion of
   one ("then") or two ("andThen" and "orThen")
others. As in:
   CompletableFuture<String> f = ...; g = ...
   f.then((s -> aStringFunction(s)).thenAsync(s -> ...);
or
   f.andThen(g, (s, t) -> combineStrings).or(CompletableFuture.async(()->...)....

Comments and suggestions (as always, especially, experience-driven
ones) would be welcome.

-Doug






From aleksey.shipilev at oracle.com  Thu Dec 27 14:54:09 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 27 Dec 2012 23:54:09 +0400
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DC94CE.5050407@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
Message-ID: <5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>

I have a few immediate reactions:

a. on using Runnable as functional interface around the place, e.g. in then(). Isn't that the "function with zero arguments", i.e. Block? Or, is this just the symmetry against async*()?

b. can all these andThen, orThen, etc. be actually folded into then*() with two and()/or() combinators?

-Aleksey

On 27.12.2012, at 22:34, Doug Lea <dl at cs.oswego.edu> wrote:

> 
> An initial version of JDK8 java.util.concurrent.CompletableFuture
> is now available. See javadocs at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
> and source at
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
> 
> This class implements functionality that has variously been
> discussed under names SettableFuture, FutureValue, Promise, and
> probably others.
> 
> Because of heavy use of function-types, this version is only
> compilable/usable using recent JDK8-lambda builds (I think
> including the current one at http://jdk8.java.net/lambda).
> It would be possible to create a jsr166e version that defined
> nested interface types to be usable stand-alone under JDK7,
> but it would be at best unpleasant to use without lambda support.
> 
> The basic design is a semi-fluent API in which you can arrange:
> (sequential or async)
> (functions or actions)
> triggered on completion of
>  one ("then") or two ("andThen" and "orThen")
> others. As in:
>  CompletableFuture<String> f = ...; g = ...
>  f.then((s -> aStringFunction(s)).thenAsync(s -> ...);
> or
>  f.andThen(g, (s, t) -> combineStrings).or(CompletableFuture.async(()->...)....
> 
> Comments and suggestions (as always, especially, experience-driven
> ones) would be welcome.
> 
> -Doug
> 
> 
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Thu Dec 27 15:06:58 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 27 Dec 2012 15:06:58 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
Message-ID: <50DCAA62.8090804@cs.oswego.edu>

On 12/27/12 14:54, Aleksey Shipilev wrote:
> I have a few immediate reactions:

Thanks!

>
> a. on using Runnable as functional interface around the place, e.g. in
> then(). Isn't that the "function with zero arguments", i.e. Block? Or, is
> this just the symmetry against async*()?

It would be possible to have forms for Block<? super T> as well,
but (1) the Runnable (void->void) forms are designed for pure
triggering, which is the most common form for non-functional
actions; and (2) the current/projected lambda syntax support
would often require a cast to distinguish from Function form,
so it would not be very pleasant to use. So the middle ground here
is that if you want an in-between form, use Function<T, Void>.


>
> b. can all these andThen, orThen, etc. be actually folded into then*() with
> two and()/or() combinators?

Not very elegantly, mainly because of exception trapping differences.
(Plus there is already enough internal plumbing to not want to
add a few more layers.)

-Doug


>
> -Aleksey
>
> On 27.12.2012, at 22:34, Doug Lea <dl at cs.oswego.edu> wrote:
>
>>
>> An initial version of JDK8 java.util.concurrent.CompletableFuture is now
>> available. See javadocs at:
>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
>>
>>
and source at
>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>>
>>
>>
This class implements functionality that has variously been
>> discussed under names SettableFuture, FutureValue, Promise, and probably
>> others.
>>
>> Because of heavy use of function-types, this version is only
>> compilable/usable using recent JDK8-lambda builds (I think including the
>> current one at http://jdk8.java.net/lambda). It would be possible to create
>> a jsr166e version that defined nested interface types to be usable
>> stand-alone under JDK7, but it would be at best unpleasant to use without
>> lambda support.
>>
>> The basic design is a semi-fluent API in which you can arrange: (sequential
>> or async) (functions or actions) triggered on completion of one ("then") or
>> two ("andThen" and "orThen") others. As in: CompletableFuture<String> f =
>> ...; g = ... f.then((s -> aStringFunction(s)).thenAsync(s -> ...); or
>> f.andThen(g, (s, t) ->
>> combineStrings).or(CompletableFuture.async(()->...)....
>>
>> Comments and suggestions (as always, especially, experience-driven ones)
>> would be welcome.
>>
>> -Doug
>>
>>
>>
>>
>>
>> _______________________________________________ Concurrency-interest
>> mailing list Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From viktor.klang at gmail.com  Thu Dec 27 15:46:39 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 27 Dec 2012 21:46:39 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DCAA62.8090804@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
Message-ID: <CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>

Haven't had time to digest it yet, but the "force"-method gives me the
creeps.


On Thu, Dec 27, 2012 at 9:06 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/27/12 14:54, Aleksey Shipilev wrote:
>
>> I have a few immediate reactions:
>>
>
> Thanks!
>
>
>
>> a. on using Runnable as functional interface around the place, e.g. in
>> then(). Isn't that the "function with zero arguments", i.e. Block? Or, is
>> this just the symmetry against async*()?
>>
>
> It would be possible to have forms for Block<? super T> as well,
> but (1) the Runnable (void->void) forms are designed for pure
> triggering, which is the most common form for non-functional
> actions; and (2) the current/projected lambda syntax support
> would often require a cast to distinguish from Function form,
> so it would not be very pleasant to use. So the middle ground here
> is that if you want an in-between form, use Function<T, Void>.
>
>
>
>
>> b. can all these andThen, orThen, etc. be actually folded into then*()
>> with
>> two and()/or() combinators?
>>
>
> Not very elegantly, mainly because of exception trapping differences.
> (Plus there is already enough internal plumbing to not want to
> add a few more layers.)
>
> -Doug
>
>
>
>
>> -Aleksey
>>
>> On 27.12.2012, at 22:34, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>>
>>> An initial version of JDK8 java.util.concurrent.**CompletableFuture is
>>> now
>>> available. See javadocs at:
>>> http://gee.cs.oswego.edu/dl/**jsr166/dist/docs/java/util/**
>>> concurrent/CompletableFuture.**html<http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html>
>>>
>>>
>>>  and source at
>
>> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**
>>> main/java/util/concurrent/**CompletableFuture.java?view=**log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log>
>>>
>>>
>>>
>>>  This class implements functionality that has variously been
>
>> discussed under names SettableFuture, FutureValue, Promise, and probably
>>> others.
>>>
>>> Because of heavy use of function-types, this version is only
>>> compilable/usable using recent JDK8-lambda builds (I think including the
>>> current one at http://jdk8.java.net/lambda). It would be possible to
>>> create
>>> a jsr166e version that defined nested interface types to be usable
>>> stand-alone under JDK7, but it would be at best unpleasant to use without
>>> lambda support.
>>>
>>> The basic design is a semi-fluent API in which you can arrange:
>>> (sequential
>>> or async) (functions or actions) triggered on completion of one ("then")
>>> or
>>> two ("andThen" and "orThen") others. As in: CompletableFuture<String> f =
>>> ...; g = ... f.then((s -> aStringFunction(s)).thenAsync(**s -> ...); or
>>> f.andThen(g, (s, t) ->
>>> combineStrings).or(**CompletableFuture.async(()->..**.)....
>>>
>>> Comments and suggestions (as always, especially, experience-driven ones)
>>> would be welcome.
>>>
>>> -Doug
>>>
>>>
>>>
>>>
>>>
>>> ______________________________**_________________ Concurrency-interest
>>> mailing list Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Director of Engineering
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121227/bece32e5/attachment.html>

From dl at cs.oswego.edu  Thu Dec 27 15:56:55 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 27 Dec 2012 15:56:55 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
	<CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
Message-ID: <50DCB617.7030403@cs.oswego.edu>

On 12/27/12 15:46, ?iktor ?lang wrote:
> Haven't had time to digest it yet, but the "force"-method gives me the creeps.

It's the same genre as Phaser.forceTermination, ForkJoinTask.reset and others.
You don't want people to use them in normal processing, but if you don't
supply mechanisms for error recovery, people who need them
will eventually find ways to get these effects in even creepier ways :-)

-Doug





From viktor.klang at gmail.com  Thu Dec 27 16:00:15 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 27 Dec 2012 22:00:15 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DCB617.7030403@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
	<CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
	<50DCB617.7030403@cs.oswego.edu>
Message-ID: <CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>

There are way to do error recovery that does not involve multiple
assignment :-)

defrecover[U >: T](pf:
PartialFunction<http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/scala/PartialFunction.html>
[Throwable<http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/scala/package.html#Throwable=Throwable>
, U])(implicit executor:
ExecutionContext<http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/scala/concurrent/ExecutionContext.html>
): Future<http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/scala/concurrent/Future.html>
[U]

Creates a new future that will handle any matching throwable that this
future might contain.


We spent quite some time getting async Futures "right" (and I think the
result is great):
http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/index.html#scala.concurrent.Future


On Thu, Dec 27, 2012 at 9:56 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/27/12 15:46, ?iktor ?lang wrote:
>
>> Haven't had time to digest it yet, but the "force"-method gives me the
>> creeps.
>>
>
> It's the same genre as Phaser.forceTermination, ForkJoinTask.reset and
> others.
> You don't want people to use them in normal processing, but if you don't
> supply mechanisms for error recovery, people who need them
> will eventually find ways to get these effects in even creepier ways :-)
>
> -Doug
>
>
>
>


-- 
Viktor Klang

Director of Engineering
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121227/d2b79849/attachment-0001.html>

From zhong.j.yu at gmail.com  Thu Dec 27 16:04:54 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 27 Dec 2012 15:04:54 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DC94CE.5050407@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
Message-ID: <CACuKZqGeJE4dkYqfNNWK5k75NOyEAHCk98ax=tj3+2aS5sV8bQ@mail.gmail.com>

- complete() returns false if already completed

In my applications I expect that complete() is invoked once and only
once; a 2nd complete() call must be a bug, so it would be better for
my taste if the 2nd complete() throws IllegalStateException, instead
of returning false.

On Thu, Dec 27, 2012 at 12:34 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
> An initial version of JDK8 java.util.concurrent.CompletableFuture
> is now available. See javadocs at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
> and source at
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>
> This class implements functionality that has variously been
> discussed under names SettableFuture, FutureValue, Promise, and
> probably others.
>
> Because of heavy use of function-types, this version is only
> compilable/usable using recent JDK8-lambda builds (I think
> including the current one at http://jdk8.java.net/lambda).
> It would be possible to create a jsr166e version that defined
> nested interface types to be usable stand-alone under JDK7,
> but it would be at best unpleasant to use without lambda support.
>
> The basic design is a semi-fluent API in which you can arrange:
>  (sequential or async)
>  (functions or actions)
> triggered on completion of
>   one ("then") or two ("andThen" and "orThen")
> others. As in:
>   CompletableFuture<String> f = ...; g = ...
>   f.then((s -> aStringFunction(s)).thenAsync(s -> ...);
> or
>   f.andThen(g, (s, t) ->
> combineStrings).or(CompletableFuture.async(()->...)....
>
> Comments and suggestions (as always, especially, experience-driven
> ones) would be welcome.
>
> -Doug
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From viktor.klang at gmail.com  Thu Dec 27 16:06:27 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 27 Dec 2012 22:06:27 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
	<CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
	<50DCB617.7030403@cs.oswego.edu>
	<CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>
Message-ID: <CANPzfU8wzzRBsz5MTW4UeiDSto7aiFgwmEULYmvX2zyG=2CkBw@mail.gmail.com>

The problem I see is that _users_ will have CompletableFuture in their
signatures, which exposes the "force" method to the world, which means that
you effectively turn CompletableFuture into AtomicReference, which is way
harder to reason about than a singel-assignment cell, which leads to
defensive copying which leads back into the whole
"shared-reassignable-state-concurrency" is hard. My strong recommendation
is to keep it single-assignment, I know I wouldn't trade that away for
anything.

Cheers,
?


On Thu, Dec 27, 2012 at 10:00 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:

> There are way to do error recovery that does not involve multiple
> assignment :-)
>
> defrecover[U >: T](pf: PartialFunction<http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/scala/PartialFunction.html>
> [Throwable<http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/scala/package.html#Throwable=Throwable>
> , U])(implicit executor: ExecutionContext<http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/scala/concurrent/ExecutionContext.html>
> ): Future<http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/scala/concurrent/Future.html>
> [U]
>
> Creates a new future that will handle any matching throwable that this
> future might contain.
>
>
> We spent quite some time getting async Futures "right" (and I think the
> result is great):
> http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/index.html#scala.concurrent.Future
>
>
>
> On Thu, Dec 27, 2012 at 9:56 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 12/27/12 15:46, ?iktor ?lang wrote:
>>
>>> Haven't had time to digest it yet, but the "force"-method gives me the
>>> creeps.
>>>
>>
>> It's the same genre as Phaser.forceTermination, ForkJoinTask.reset and
>> others.
>> You don't want people to use them in normal processing, but if you don't
>> supply mechanisms for error recovery, people who need them
>> will eventually find ways to get these effects in even creepier ways :-)
>>
>> -Doug
>>
>>
>>
>>
>
>
> --
> Viktor Klang
>
> Director of Engineering
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
>
> Twitter: @viktorklang
>
>


-- 
Viktor Klang

Director of Engineering
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121227/78a267cd/attachment.html>

From jan at kotek.net  Thu Dec 27 16:06:21 2012
From: jan at kotek.net (Jan Kotek)
Date: Thu, 27 Dec 2012 21:06:21 +0000
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DC94CE.5050407@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
Message-ID: <20121227210621.70c483a8f5c2ce08074294b0@kotek.net>

Hi,

> It would be possible to create a jsr166e version that defined
> nested interface types to be usable stand-alone under JDK7,
> but it would be at best unpleasant to use without lambda support.

Maybe backport to JDK7 would make a sense. 

Java is not the only language running on JDK7.
 Scala, Kotlin or Groovy could use this class the same way as Java8. 

Regards
Jan Kotek

From viktor.klang at gmail.com  Thu Dec 27 16:09:16 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 27 Dec 2012 22:09:16 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CACuKZqGeJE4dkYqfNNWK5k75NOyEAHCk98ax=tj3+2aS5sV8bQ@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu>
	<CACuKZqGeJE4dkYqfNNWK5k75NOyEAHCk98ax=tj3+2aS5sV8bQ@mail.gmail.com>
Message-ID: <CANPzfU-oR+zrrGniuZyH0HefjW8SFUm2tHGK-YOsJpxz--N0CA@mail.gmail.com>

On Thu, Dec 27, 2012 at 10:04 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> - complete() returns false if already completed
>
> In my applications I expect that complete() is invoked once and only
> once; a 2nd complete() call must be a bug, so it would be better for
> my taste if the 2nd complete() throws IllegalStateException, instead
> of returning false.
>
>
Good catch, I agree with your conclusion as well, we originally ignored the
completes after the first, which led to subtle bugs being swept under the
carpet, which is why we now have "complete" and "tryComplete" with the
latter returning a boolean and the former throwing an exception if already
completed.

Cheers,
?



> On Thu, Dec 27, 2012 at 12:34 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> >
> > An initial version of JDK8 java.util.concurrent.CompletableFuture
> > is now available. See javadocs at:
> >
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
> > and source at
> >
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
> >
> > This class implements functionality that has variously been
> > discussed under names SettableFuture, FutureValue, Promise, and
> > probably others.
> >
> > Because of heavy use of function-types, this version is only
> > compilable/usable using recent JDK8-lambda builds (I think
> > including the current one at http://jdk8.java.net/lambda).
> > It would be possible to create a jsr166e version that defined
> > nested interface types to be usable stand-alone under JDK7,
> > but it would be at best unpleasant to use without lambda support.
> >
> > The basic design is a semi-fluent API in which you can arrange:
> >  (sequential or async)
> >  (functions or actions)
> > triggered on completion of
> >   one ("then") or two ("andThen" and "orThen")
> > others. As in:
> >   CompletableFuture<String> f = ...; g = ...
> >   f.then((s -> aStringFunction(s)).thenAsync(s -> ...);
> > or
> >   f.andThen(g, (s, t) ->
> > combineStrings).or(CompletableFuture.async(()->...)....
> >
> > Comments and suggestions (as always, especially, experience-driven
> > ones) would be welcome.
> >
> > -Doug
> >
> >
> >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Viktor Klang

Director of Engineering
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121227/9f5a646b/attachment-0001.html>

From dl at cs.oswego.edu  Thu Dec 27 16:10:40 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 27 Dec 2012 16:10:40 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
	<CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
	<50DCB617.7030403@cs.oswego.edu>
	<CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>
Message-ID: <50DCB950.4050706@cs.oswego.edu>

On 12/27/12 16:00, ?iktor ?lang wrote:
> There are way to do error recovery that does not involve multiple assignment
> :-)

And there ways that do :-)
j.u.c tries to remain stubbornly non-religious about concurrent programming.
If you don't like a method, don't use it, or subclass to
disable it ...

Relatedly...

On 12/27/12 16:04, Zhong Yu wrote:
> - complete() returns false if already completed
>
> In my applications I expect that complete() is invoked once and only once; a
> 2nd complete() call must be a bug, so it would be better for my taste if the
> 2nd complete() throws IllegalStateException, instead of returning false.

Almost the same answer. Except that there are inherent races if
you use asyncs with method orThen in particular.

-Doug




From zhong.j.yu at gmail.com  Thu Dec 27 16:23:18 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 27 Dec 2012 15:23:18 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU8wzzRBsz5MTW4UeiDSto7aiFgwmEULYmvX2zyG=2CkBw@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
	<CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
	<50DCB617.7030403@cs.oswego.edu>
	<CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>
	<CANPzfU8wzzRBsz5MTW4UeiDSto7aiFgwmEULYmvX2zyG=2CkBw@mail.gmail.com>
Message-ID: <CACuKZqH5pZ9Xu4r8Jnos8cQRPJOC3r084Lr_JmxmZ_SfXUdCug@mail.gmail.com>

It would be great to have a simple read-only interface in JDK,
otherwise people are forced to invent their own.

Is it possible to add a simple method to Future

    public default void uponCompletion(Runnable callback)

that would satisfy many async applications.

Zhong Yu

On Thu, Dec 27, 2012 at 3:06 PM, ?iktor ?lang <viktor.klang at gmail.com> wrote:
> The problem I see is that _users_ will have CompletableFuture in their
> signatures, which exposes the "force" method to the world, which means that
> you effectively turn CompletableFuture into AtomicReference, which is way
> harder to reason about than a singel-assignment cell, which leads to
> defensive copying which leads back into the whole
> "shared-reassignable-state-concurrency" is hard. My strong recommendation is
> to keep it single-assignment, I know I wouldn't trade that away for
> anything.
>
> Cheers,
> ?
>
>
> On Thu, Dec 27, 2012 at 10:00 PM, ?iktor ?lang <viktor.klang at gmail.com>
> wrote:
>>
>> There are way to do error recovery that does not involve multiple
>> assignment :-)
>>
>> defrecover[U >: T](pf: PartialFunction[Throwable, U])(implicit executor:
>> ExecutionContext): Future[U]
>>
>> Creates a new future that will handle any matching throwable that this
>> future might contain.
>>
>>
>>
>> We spent quite some time getting async Futures "right" (and I think the
>> result is great):
>> http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/index.html#scala.concurrent.Future
>>
>>
>>
>> On Thu, Dec 27, 2012 at 9:56 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>
>>> On 12/27/12 15:46, ?iktor ?lang wrote:
>>>>
>>>> Haven't had time to digest it yet, but the "force"-method gives me the
>>>> creeps.
>>>
>>>
>>> It's the same genre as Phaser.forceTermination, ForkJoinTask.reset and
>>> others.
>>> You don't want people to use them in normal processing, but if you don't
>>> supply mechanisms for error recovery, people who need them
>>> will eventually find ways to get these effects in even creepier ways :-)
>>>
>>> -Doug
>>>
>>>
>>>
>>
>>
>>
>> --
>> Viktor Klang
>>
>> Director of Engineering
>> Typesafe - The software stack for applications that scale
>>
>> Twitter: @viktorklang
>>
>
>
>
> --
> Viktor Klang
>
> Director of Engineering
> Typesafe - The software stack for applications that scale
>
> Twitter: @viktorklang
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From viktor.klang at gmail.com  Thu Dec 27 17:29:13 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 27 Dec 2012 23:29:13 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DCB950.4050706@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
	<CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
	<50DCB617.7030403@cs.oswego.edu>
	<CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>
	<50DCB950.4050706@cs.oswego.edu>
Message-ID: <CANPzfU9Ai9wNGBKaYbdeXQU9j9dWa_TQtU6zXBFeKdxQ1+x7Bw@mail.gmail.com>

On Thu, Dec 27, 2012 at 10:10 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/27/12 16:00, ?iktor ?lang wrote:
>
>> There are way to do error recovery that does not involve multiple
>> assignment
>> :-)
>>
>
> And there ways that do :-)
> j.u.c tries to remain stubbornly non-religious about concurrent
> programming.
> If you don't like a method, don't use it, or subclass to
> disable it ...
>

Yes, but that's the problem, I get a CompletableFuture in as a parameter,
if I am to make sure that it doesn't get tampered with, I'll have to do
defensive copying all the time, which is counterproductive.

This is all about years of experience in writing async Futures and using
them. :(


>
> Relatedly...
>
>
> On 12/27/12 16:04, Zhong Yu wrote:
>
>> - complete() returns false if already completed
>>
>> In my applications I expect that complete() is invoked once and only
>> once; a
>> 2nd complete() call must be a bug, so it would be better for my taste if
>> the
>> 2nd complete() throws IllegalStateException, instead of returning false.
>>
>
> Almost the same answer. Except that there are inherent races if
> you use asyncs with method orThen in particular.


Yep, orThen (which we used to call "either" before we removed it) is a
race-creating method, we found that it was more harmful than useful. :/

Cheers,
?


>
>
> -Doug
>
>
>


-- 
Viktor Klang

Director of Engineering
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121227/2f884bfb/attachment.html>

From viktor.klang at gmail.com  Thu Dec 27 17:31:47 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 27 Dec 2012 23:31:47 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CACuKZqH5pZ9Xu4r8Jnos8cQRPJOC3r084Lr_JmxmZ_SfXUdCug@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
	<CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
	<50DCB617.7030403@cs.oswego.edu>
	<CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>
	<CANPzfU8wzzRBsz5MTW4UeiDSto7aiFgwmEULYmvX2zyG=2CkBw@mail.gmail.com>
	<CACuKZqH5pZ9Xu4r8Jnos8cQRPJOC3r084Lr_JmxmZ_SfXUdCug@mail.gmail.com>
Message-ID: <CANPzfU--b9jZTCH5bnvJS9gdcjkCUdqFX9g_y3nvhZ=OTJh==Q@mail.gmail.com>

On Thu, Dec 27, 2012 at 10:23 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> It would be great to have a simple read-only interface in JDK,
> otherwise people are forced to invent their own.
>
> Is it possible to add a simple method to Future
>
>     public default void uponCompletion(Runnable callback)
>
> that would satisfy many async applications.
>

Yes, "Don't call us, we'll call you" is the minimum for doing async
futures, however, in my experience these callback-style methods quite
quickly lead users into problems with both error reporting and closing over
too much, i.e. not having a directed flow of data. "uponCompletion" is
great for plumbing, but dangerous for end users, which is why we now
recommend using async transformations instead of callbacks (i.e. map,
flatMap, filter etc)

Cheers,
?


>
> Zhong Yu
>
> On Thu, Dec 27, 2012 at 3:06 PM, ?iktor ?lang <viktor.klang at gmail.com>
> wrote:
> > The problem I see is that _users_ will have CompletableFuture in their
> > signatures, which exposes the "force" method to the world, which means
> that
> > you effectively turn CompletableFuture into AtomicReference, which is way
> > harder to reason about than a singel-assignment cell, which leads to
> > defensive copying which leads back into the whole
> > "shared-reassignable-state-concurrency" is hard. My strong
> recommendation is
> > to keep it single-assignment, I know I wouldn't trade that away for
> > anything.
> >
> > Cheers,
> > ?
> >
> >
> > On Thu, Dec 27, 2012 at 10:00 PM, ?iktor ?lang <viktor.klang at gmail.com>
> > wrote:
> >>
> >> There are way to do error recovery that does not involve multiple
> >> assignment :-)
> >>
> >> defrecover[U >: T](pf: PartialFunction[Throwable, U])(implicit executor:
> >> ExecutionContext): Future[U]
> >>
> >> Creates a new future that will handle any matching throwable that this
> >> future might contain.
> >>
> >>
> >>
> >> We spent quite some time getting async Futures "right" (and I think the
> >> result is great):
> >>
> http://www.scala-lang.org/archives/downloads/distrib/files/nightly/docs/library/index.html#scala.concurrent.Future
> >>
> >>
> >>
> >> On Thu, Dec 27, 2012 at 9:56 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> >>>
> >>> On 12/27/12 15:46, ?iktor ?lang wrote:
> >>>>
> >>>> Haven't had time to digest it yet, but the "force"-method gives me the
> >>>> creeps.
> >>>
> >>>
> >>> It's the same genre as Phaser.forceTermination, ForkJoinTask.reset and
> >>> others.
> >>> You don't want people to use them in normal processing, but if you
> don't
> >>> supply mechanisms for error recovery, people who need them
> >>> will eventually find ways to get these effects in even creepier ways
> :-)
> >>>
> >>> -Doug
> >>>
> >>>
> >>>
> >>
> >>
> >>
> >> --
> >> Viktor Klang
> >>
> >> Director of Engineering
> >> Typesafe - The software stack for applications that scale
> >>
> >> Twitter: @viktorklang
> >>
> >
> >
> >
> > --
> > Viktor Klang
> >
> > Director of Engineering
> > Typesafe - The software stack for applications that scale
> >
> > Twitter: @viktorklang
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>



-- 
Viktor Klang

Director of Engineering
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121227/aaeb8d7c/attachment.html>

From aleksey.shipilev at oracle.com  Fri Dec 28 05:37:17 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 14:37:17 +0400
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DC94CE.5050407@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
Message-ID: <50DD765D.2000502@oracle.com>

On 12/27/2012 10:34 PM, Doug Lea wrote:
> An initial version of JDK8 java.util.concurrent.CompletableFuture
> is now available. See javadocs at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html

Covering the CompletableFuture with concurrency-torture tests.

This guy:

    @Override
    public void actor1(CompletableFuture<Integer> future) {
        future.completeExceptionally(new MyThrowable());
    }

    public class MyThrowable extends Throwable {}

...fails on get() with:

java.lang.ClassCastException:
org.openjdk.concurrent.torture.tests.future.completable.MyThrowable
cannot be cast to java.lang.RuntimeException
	at
java.util.concurrent.CompletableFuture.rethrow(CompletableFuture.java:1611)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:209)

Seems to assume we can only complete with runtime exceptions?

-Aleksey.

From dl at cs.oswego.edu  Fri Dec 28 06:47:40 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 28 Dec 2012 06:47:40 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU9Ai9wNGBKaYbdeXQU9j9dWa_TQtU6zXBFeKdxQ1+x7Bw@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
	<CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
	<50DCB617.7030403@cs.oswego.edu>
	<CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>
	<50DCB950.4050706@cs.oswego.edu>
	<CANPzfU9Ai9wNGBKaYbdeXQU9j9dWa_TQtU6zXBFeKdxQ1+x7Bw@mail.gmail.com>
Message-ID: <50DD86DC.5090208@cs.oswego.edu>

On 12/27/12 17:29, ?iktor ?lang wrote:
> On Thu, Dec 27, 2012 at 10:10 PM, Doug Lea <dl at cs.oswego.edu
> <mailto:dl at cs.oswego.edu>> wrote:
>
>     On 12/27/12 16:00, ?iktor ?lang wrote:
>
>         There are way to do error recovery that does not involve multiple assignment
>         :-)
>
>
>     And there ways that do :-)
>     j.u.c tries to remain stubbornly non-religious about concurrent programming.
>     If you don't like a method, don't use it, or subclass to
>     disable it ...
>
>
> Yes, but that's the problem, I get a CompletableFuture in as a parameter, if I
> am to make sure that it doesn't get tampered with, I'll have to do defensive
> copying all the time, which is counterproductive.


... or make a subclass that you uniformly use. Or use a static checker.
Or whatever.

Sometimes I'm tempted to open up a concurrency-morality list :-)
For every j.u.c class/method that one set of people need, there's
another set of people who say they shouldn't have it. This results
in stalls that can last years. Think of for example, Fences, that
you need inside Akka but haven't been able to use because of
controversies surrounding policy/spec questions.

-Doug




From dl at cs.oswego.edu  Fri Dec 28 06:58:59 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 28 Dec 2012 06:58:59 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DD765D.2000502@oracle.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
Message-ID: <50DD8983.5020404@cs.oswego.edu>

On 12/28/12 05:37, Aleksey Shipilev wrote:

> Covering the CompletableFuture with concurrency-torture tests.
>
> This guy:
>
>      @Override
>      public void actor1(CompletableFuture<Integer> future) {
>          future.completeExceptionally(new MyThrowable());
>      }
>
>      public class MyThrowable extends Throwable {}
>
> ...fails on get() with:
>
> java.lang.ClassCastException:
> org.openjdk.concurrent.torture.tests.future.completable.MyThrowable
> cannot be cast to java.lang.RuntimeException

Thanks! I now see that one set of internal constructions evade
the "sneakyThrow" approach to translation/tunneling. Until
this is redone (hopefully very soon) you should be able to evade by:

   public class MyThrowable extends Error {}

(Background for others in case you are curious. The lack of a
base class/interface for unchecked vs checked exceptions
leads to some delicate goopiness inside this and other classes.)


-Doug


From viktor.klang at gmail.com  Fri Dec 28 07:02:04 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 28 Dec 2012 13:02:04 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DD86DC.5090208@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
	<5898AC53-EAF2-4BA5-895D-967764C1755D@oracle.com>
	<50DCAA62.8090804@cs.oswego.edu>
	<CANPzfU_hcavHH4bhWz0g0BoqeaD7Du5EtaWeu3TPG_cGqQQr4g@mail.gmail.com>
	<50DCB617.7030403@cs.oswego.edu>
	<CANPzfU_bkE2RoQ=XBHFD4eLAVCEG_9PscsY9GrOW4YQ==1-u-g@mail.gmail.com>
	<50DCB950.4050706@cs.oswego.edu>
	<CANPzfU9Ai9wNGBKaYbdeXQU9j9dWa_TQtU6zXBFeKdxQ1+x7Bw@mail.gmail.com>
	<50DD86DC.5090208@cs.oswego.edu>
Message-ID: <CANPzfU9suiimObF_Fvm2+NqG9p6Xp_X1u_Dp6UXZUwqBJyoZjg@mail.gmail.com>

On Fri, Dec 28, 2012 at 12:47 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/27/12 17:29, ?iktor ?lang wrote:
>
>> On Thu, Dec 27, 2012 at 10:10 PM, Doug Lea <dl at cs.oswego.edu
>> <mailto:dl at cs.oswego.edu>> wrote:
>>
>>     On 12/27/12 16:00, ?iktor ?lang wrote:
>>
>>         There are way to do error recovery that does not involve multiple
>> assignment
>>         :-)
>>
>>
>>     And there ways that do :-)
>>     j.u.c tries to remain stubbornly non-religious about concurrent
>> programming.
>>     If you don't like a method, don't use it, or subclass to
>>     disable it ...
>>
>>
>> Yes, but that's the problem, I get a CompletableFuture in as a parameter,
>> if I
>> am to make sure that it doesn't get tampered with, I'll have to do
>> defensive
>> copying all the time, which is counterproductive.
>>
>
>
> ... or make a subclass that you uniformly use. Or use a static checker.
> Or whatever.
>
> Sometimes I'm tempted to open up a concurrency-morality list :-)
>

Haha, I'm sure we'd have tons of fun there :-)


> For every j.u.c class/method that one set of people need, there's
> another set of people who say they shouldn't have it. This results
> in stalls that can last years. Think of for example, Fences, that
> you need inside Akka but haven't been able to use because of
> controversies surrounding policy/spec questions.


Yeah, I agree there, but I think the cause is that there's no difference
between library-builders and end-users API wise.
I like the "Make the simple things easy and the hard things possible"
philosophy, which is sort of different from "Make all things simple" which
I find lead to a lot of misuse :)

Cheers,
?


>
>
> -Doug
>
>
>


-- 
Viktor Klang

Director of Engineering
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121228/012b5f4b/attachment.html>

From aleksey.shipilev at oracle.com  Fri Dec 28 07:07:28 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 16:07:28 +0400
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DD8983.5020404@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu>
Message-ID: <50DD8B80.3090304@oracle.com>

On 12/28/2012 03:58 PM, Doug Lea wrote:
> On 12/28/12 05:37, Aleksey Shipilev wrote:
> 
>> Covering the CompletableFuture with concurrency-torture tests.
>>
>> This guy:
>>
>>      @Override
>>      public void actor1(CompletableFuture<Integer> future) {
>>          future.completeExceptionally(new MyThrowable());
>>      }
>>
>>      public class MyThrowable extends Throwable {}
>>
>> ...fails on get() with:
>>
>> java.lang.ClassCastException:
>> org.openjdk.concurrent.torture.tests.future.completable.MyThrowable
>> cannot be cast to java.lang.RuntimeException
> 
> Thanks! I now see that one set of internal constructions evade
> the "sneakyThrow" approach to translation/tunneling. Until
> this is redone (hopefully very soon) you should be able to evade by:
> 
>   public class MyThrowable extends Error {}
> 
> (Background for others in case you are curious. The lack of a
> base class/interface for unchecked vs checked exceptions
> leads to some delicate goopiness inside this and other classes.)

Note to Viktor: this is reproducible on JDK 7u4-7u12, if you pull out
the sneaky throw thing alone:

public class SneakyThrow {

    public static final void main(String[] args) {
        rethrow(new MyThrowable());
    }

    /**
     * A version of "sneaky throw" to relay exceptions
     */
    static void rethrow(final Throwable ex) {
        if (ex != null) {
            if (ex instanceof Error)
                throw (Error)ex;
            if (ex instanceof RuntimeException)
                throw (RuntimeException)ex;
            throw uncheckedThrowable(ex, RuntimeException.class);
        }
    }

    /**
     * The sneaky part of sneaky throw, relying on generics
     * limitations to evade compiler complaints about rethrowing
     * unchecked exceptions
     */
    @SuppressWarnings("unchecked") static <T extends Throwable>
        T uncheckedThrowable(final Throwable t, final Class<T> c) {
        return (T)t; // rely on vacuous cast
    }

    public static class MyThrowable extends Throwable {}

}

-Aleksey.


From viktor.klang at gmail.com  Fri Dec 28 08:53:21 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 28 Dec 2012 14:53:21 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DD8B80.3090304@oracle.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
Message-ID: <CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>

On Fri, Dec 28, 2012 at 1:07 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 12/28/2012 03:58 PM, Doug Lea wrote:
> > On 12/28/12 05:37, Aleksey Shipilev wrote:
> >
> >> Covering the CompletableFuture with concurrency-torture tests.
> >>
> >> This guy:
> >>
> >>      @Override
> >>      public void actor1(CompletableFuture<Integer> future) {
> >>          future.completeExceptionally(new MyThrowable());
> >>      }
> >>
> >>      public class MyThrowable extends Throwable {}
> >>
> >> ...fails on get() with:
> >>
> >> java.lang.ClassCastException:
> >> org.openjdk.concurrent.torture.tests.future.completable.MyThrowable
> >> cannot be cast to java.lang.RuntimeException
> >
> > Thanks! I now see that one set of internal constructions evade
> > the "sneakyThrow" approach to translation/tunneling. Until
> > this is redone (hopefully very soon) you should be able to evade by:
> >
> >   public class MyThrowable extends Error {}
> >
> > (Background for others in case you are curious. The lack of a
> > base class/interface for unchecked vs checked exceptions
> > leads to some delicate goopiness inside this and other classes.)
>
> Note to Viktor: this is reproducible on JDK 7u4-7u12, if you pull out
> the sneaky throw thing alone:
>

Even with -target 1.6 ?


>
> public class SneakyThrow {
>
>     public static final void main(String[] args) {
>         rethrow(new MyThrowable());
>     }
>
>     /**
>      * A version of "sneaky throw" to relay exceptions
>      */
>     static void rethrow(final Throwable ex) {
>         if (ex != null) {
>             if (ex instanceof Error)
>                 throw (Error)ex;
>             if (ex instanceof RuntimeException)
>                 throw (RuntimeException)ex;
>             throw uncheckedThrowable(ex, RuntimeException.class);
>         }
>     }
>
>     /**
>      * The sneaky part of sneaky throw, relying on generics
>      * limitations to evade compiler complaints about rethrowing
>      * unchecked exceptions
>      */
>     @SuppressWarnings("unchecked") static <T extends Throwable>
>         T uncheckedThrowable(final Throwable t, final Class<T> c) {
>         return (T)t; // rely on vacuous cast
>     }
>
>     public static class MyThrowable extends Throwable {}
>
> }
>
> -Aleksey.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121228/57e6bfb2/attachment.html>

From aleksey.shipilev at oracle.com  Fri Dec 28 08:57:39 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 17:57:39 +0400
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
Message-ID: <50DDA553.60904@oracle.com>

On 12/28/2012 05:53 PM, ?iktor ?lang wrote:
>     Note to Viktor: this is reproducible on JDK 7u4-7u12, if you pull out
>     the sneaky throw thing alone:
> 
> Even with -target 1.6 ?

Yes, try it yourself:

$ ~/Install/jdk6u34/bin/javac -target 1.6 SneakyThrow.java
$ ~/Install/jdk6u34/bin/java SneakyThrow
Exception in thread "main" java.lang.ClassCastException:
SneakyThrow$MyThrowable cannot be cast to java.lang.RuntimeException
	at SneakyThrow.rethrow(SneakyThrow.java:16)
	at SneakyThrow.main(SneakyThrow.java:4)

-Aleksey.

From viktor.klang at gmail.com  Fri Dec 28 08:58:17 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 28 Dec 2012 14:58:17 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
Message-ID: <CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>

Reinier Zwitserloot's Project Lombok has an alternate implementation, does
that work better?

https://github.com/rzwitserloot/lombok/blob/master/src/core/lombok/Lombok.java#L50

Cheers,
?


On Fri, Dec 28, 2012 at 2:53 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:

>
>
>
> On Fri, Dec 28, 2012 at 1:07 PM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> On 12/28/2012 03:58 PM, Doug Lea wrote:
>> > On 12/28/12 05:37, Aleksey Shipilev wrote:
>> >
>> >> Covering the CompletableFuture with concurrency-torture tests.
>> >>
>> >> This guy:
>> >>
>> >>      @Override
>> >>      public void actor1(CompletableFuture<Integer> future) {
>> >>          future.completeExceptionally(new MyThrowable());
>> >>      }
>> >>
>> >>      public class MyThrowable extends Throwable {}
>> >>
>> >> ...fails on get() with:
>> >>
>> >> java.lang.ClassCastException:
>> >> org.openjdk.concurrent.torture.tests.future.completable.MyThrowable
>> >> cannot be cast to java.lang.RuntimeException
>> >
>> > Thanks! I now see that one set of internal constructions evade
>> > the "sneakyThrow" approach to translation/tunneling. Until
>> > this is redone (hopefully very soon) you should be able to evade by:
>> >
>> >   public class MyThrowable extends Error {}
>> >
>> > (Background for others in case you are curious. The lack of a
>> > base class/interface for unchecked vs checked exceptions
>> > leads to some delicate goopiness inside this and other classes.)
>>
>> Note to Viktor: this is reproducible on JDK 7u4-7u12, if you pull out
>> the sneaky throw thing alone:
>>
>
> Even with -target 1.6 ?
>
>
>>
>> public class SneakyThrow {
>>
>>     public static final void main(String[] args) {
>>         rethrow(new MyThrowable());
>>     }
>>
>>     /**
>>      * A version of "sneaky throw" to relay exceptions
>>      */
>>     static void rethrow(final Throwable ex) {
>>         if (ex != null) {
>>             if (ex instanceof Error)
>>                 throw (Error)ex;
>>             if (ex instanceof RuntimeException)
>>                 throw (RuntimeException)ex;
>>             throw uncheckedThrowable(ex, RuntimeException.class);
>>         }
>>     }
>>
>>     /**
>>      * The sneaky part of sneaky throw, relying on generics
>>      * limitations to evade compiler complaints about rethrowing
>>      * unchecked exceptions
>>      */
>>     @SuppressWarnings("unchecked") static <T extends Throwable>
>>         T uncheckedThrowable(final Throwable t, final Class<T> c) {
>>         return (T)t; // rely on vacuous cast
>>     }
>>
>>     public static class MyThrowable extends Throwable {}
>>
>> }
>>
>> -Aleksey.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> *
> *
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @viktorklang
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121228/1fd3282b/attachment.html>

From viktor.klang at gmail.com  Fri Dec 28 08:59:12 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 28 Dec 2012 14:59:12 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDA553.60904@oracle.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<50DDA553.60904@oracle.com>
Message-ID: <CANPzfU_YhWkNHxhhCxb7+SQSyv=LF07VOHANDaW25qix1sR0Zg@mail.gmail.com>

On Fri, Dec 28, 2012 at 2:57 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 12/28/2012 05:53 PM, ?iktor ?lang wrote:
> >     Note to Viktor: this is reproducible on JDK 7u4-7u12, if you pull out
> >     the sneaky throw thing alone:
> >
> > Even with -target 1.6 ?
>
> Yes, try it yourself:
>

Alright, will have to try when I'm back at my workstation.

See my other email.

Cheers,
?


>
> $ ~/Install/jdk6u34/bin/javac -target 1.6 SneakyThrow.java
> $ ~/Install/jdk6u34/bin/java SneakyThrow
> Exception in thread "main" java.lang.ClassCastException:
> SneakyThrow$MyThrowable cannot be cast to java.lang.RuntimeException
>         at SneakyThrow.rethrow(SneakyThrow.java:16)
>         at SneakyThrow.main(SneakyThrow.java:4)
>
> -Aleksey.
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121228/c572f9cb/attachment.html>

From aleksey.shipilev at oracle.com  Fri Dec 28 09:05:32 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 18:05:32 +0400
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
Message-ID: <50DDA72C.7050102@oracle.com>

Yes, this one works on JDK6-JDK8.

-Aleksey.

On 12/28/2012 05:58 PM, ?iktor ?lang wrote:
> Reinier Zwitserloot's Project Lombok has an alternate implementation,
> does that work better?
> 
> https://github.com/rzwitserloot/lombok/blob/master/src/core/lombok/Lombok.java#L50
> 
> Cheers,
> ?
> 
> 
> On Fri, Dec 28, 2012 at 2:53 PM, ?iktor ?lang <viktor.klang at gmail.com
> <mailto:viktor.klang at gmail.com>> wrote:
> 
> 
> 
> 
>     On Fri, Dec 28, 2012 at 1:07 PM, Aleksey Shipilev
>     <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>>
>     wrote:
> 
>         On 12/28/2012 03:58 PM, Doug Lea wrote:
>         > On 12/28/12 05:37, Aleksey Shipilev wrote:
>         >
>         >> Covering the CompletableFuture with concurrency-torture tests.
>         >>
>         >> This guy:
>         >>
>         >>      @Override
>         >>      public void actor1(CompletableFuture<Integer> future) {
>         >>          future.completeExceptionally(new MyThrowable());
>         >>      }
>         >>
>         >>      public class MyThrowable extends Throwable {}
>         >>
>         >> ...fails on get() with:
>         >>
>         >> java.lang.ClassCastException:
>         >>
>         org.openjdk.concurrent.torture.tests.future.completable.MyThrowable
>         >> cannot be cast to java.lang.RuntimeException
>         >
>         > Thanks! I now see that one set of internal constructions evade
>         > the "sneakyThrow" approach to translation/tunneling. Until
>         > this is redone (hopefully very soon) you should be able to
>         evade by:
>         >
>         >   public class MyThrowable extends Error {}
>         >
>         > (Background for others in case you are curious. The lack of a
>         > base class/interface for unchecked vs checked exceptions
>         > leads to some delicate goopiness inside this and other classes.)
> 
>         Note to Viktor: this is reproducible on JDK 7u4-7u12, if you
>         pull out
>         the sneaky throw thing alone:
> 
> 
>     Even with -target 1.6 ?
>      
> 
> 
>         public class SneakyThrow {
> 
>             public static final void main(String[] args) {
>                 rethrow(new MyThrowable());
>             }
> 
>             /**
>              * A version of "sneaky throw" to relay exceptions
>              */
>             static void rethrow(final Throwable ex) {
>                 if (ex != null) {
>                     if (ex instanceof Error)
>                         throw (Error)ex;
>                     if (ex instanceof RuntimeException)
>                         throw (RuntimeException)ex;
>                     throw uncheckedThrowable(ex, RuntimeException.class);
>                 }
>             }
> 
>             /**
>              * The sneaky part of sneaky throw, relying on generics
>              * limitations to evade compiler complaints about rethrowing
>              * unchecked exceptions
>              */
>             @SuppressWarnings("unchecked") static <T extends Throwable>
>                 T uncheckedThrowable(final Throwable t, final Class<T> c) {
>                 return (T)t; // rely on vacuous cast
>             }
> 
>             public static class MyThrowable extends Throwable {}
> 
>         }
> 
>         -Aleksey.
> 
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
> 
>     -- 
>     *Viktor Klang*
>     /Director of Engineering/
>     /
>     /
>     Typesafe <http://www.typesafe.com/> - The software stack for
>     applications that scale
>     Twitter: @viktorklang
> 
> 
> 
> 
> -- 
> *Viktor Klang*
> /Director of Engineering/
> /
> /
> Typesafe <http://www.typesafe.com/> - The software stack for
> applications that scale
> Twitter: @viktorklang


From dl at cs.oswego.edu  Fri Dec 28 09:10:11 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 28 Dec 2012 09:10:11 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDA72C.7050102@oracle.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com>
Message-ID: <50DDA843.7010106@cs.oswego.edu>

On 12/28/12 09:05, Aleksey Shipilev wrote:
> Yes, this one works on JDK6-JDK8.
>

Thanks. I bypassed the need for it in updated CompletableFuture,
but some analogous constructs elsewhere could use this.

-Doug


> -Aleksey.
>
> On 12/28/2012 05:58 PM, ?iktor ?lang wrote:
>> Reinier Zwitserloot's Project Lombok has an alternate implementation,
>> does that work better?
>>
>> https://github.com/rzwitserloot/lombok/blob/master/src/core/lombok/Lombok.java#L50
>>
>> Cheers,
>> ?
>>
>>
>> On Fri, Dec 28, 2012 at 2:53 PM, ?iktor ?lang <viktor.klang at gmail.com
>> <mailto:viktor.klang at gmail.com>> wrote:
>>
>>
>>
>>
>>      On Fri, Dec 28, 2012 at 1:07 PM, Aleksey Shipilev
>>      <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>>
>>      wrote:
>>
>>          On 12/28/2012 03:58 PM, Doug Lea wrote:
>>          > On 12/28/12 05:37, Aleksey Shipilev wrote:
>>          >
>>          >> Covering the CompletableFuture with concurrency-torture tests.
>>          >>
>>          >> This guy:
>>          >>
>>          >>      @Override
>>          >>      public void actor1(CompletableFuture<Integer> future) {
>>          >>          future.completeExceptionally(new MyThrowable());
>>          >>      }
>>          >>
>>          >>      public class MyThrowable extends Throwable {}
>>          >>
>>          >> ...fails on get() with:
>>          >>
>>          >> java.lang.ClassCastException:
>>          >>
>>          org.openjdk.concurrent.torture.tests.future.completable.MyThrowable
>>          >> cannot be cast to java.lang.RuntimeException
>>          >
>>          > Thanks! I now see that one set of internal constructions evade
>>          > the "sneakyThrow" approach to translation/tunneling. Until
>>          > this is redone (hopefully very soon) you should be able to
>>          evade by:
>>          >
>>          >   public class MyThrowable extends Error {}
>>          >
>>          > (Background for others in case you are curious. The lack of a
>>          > base class/interface for unchecked vs checked exceptions
>>          > leads to some delicate goopiness inside this and other classes.)
>>
>>          Note to Viktor: this is reproducible on JDK 7u4-7u12, if you
>>          pull out
>>          the sneaky throw thing alone:
>>
>>
>>      Even with -target 1.6 ?
>>
>>
>>
>>          public class SneakyThrow {
>>
>>              public static final void main(String[] args) {
>>                  rethrow(new MyThrowable());
>>              }
>>
>>              /**
>>               * A version of "sneaky throw" to relay exceptions
>>               */
>>              static void rethrow(final Throwable ex) {
>>                  if (ex != null) {
>>                      if (ex instanceof Error)
>>                          throw (Error)ex;
>>                      if (ex instanceof RuntimeException)
>>                          throw (RuntimeException)ex;
>>                      throw uncheckedThrowable(ex, RuntimeException.class);
>>                  }
>>              }
>>
>>              /**
>>               * The sneaky part of sneaky throw, relying on generics
>>               * limitations to evade compiler complaints about rethrowing
>>               * unchecked exceptions
>>               */
>>              @SuppressWarnings("unchecked") static <T extends Throwable>
>>                  T uncheckedThrowable(final Throwable t, final Class<T> c) {
>>                  return (T)t; // rely on vacuous cast
>>              }
>>
>>              public static class MyThrowable extends Throwable {}
>>
>>          }
>>
>>          -Aleksey.
>>
>>          _______________________________________________
>>          Concurrency-interest mailing list
>>          Concurrency-interest at cs.oswego.edu
>>          <mailto:Concurrency-interest at cs.oswego.edu>
>>          http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>      --
>>      *Viktor Klang*
>>      /Director of Engineering/
>>      /
>>      /
>>      Typesafe <http://www.typesafe.com/> - The software stack for
>>      applications that scale
>>      Twitter: @viktorklang
>>
>>
>>
>>
>> --
>> *Viktor Klang*
>> /Director of Engineering/
>> /
>> /
>> Typesafe <http://www.typesafe.com/> - The software stack for
>> applications that scale
>> Twitter: @viktorklang
>




From forax at univ-mlv.fr  Fri Dec 28 09:24:23 2012
From: forax at univ-mlv.fr (Remi Forax)
Date: Fri, 28 Dec 2012 15:24:23 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDA843.7010106@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
Message-ID: <50DDAB97.8020604@univ-mlv.fr>

On 12/28/2012 03:10 PM, Doug Lea wrote:
> On 12/28/12 09:05, Aleksey Shipilev wrote:
>> Yes, this one works on JDK6-JDK8.
>>
>
> Thanks. I bypassed the need for it in updated CompletableFuture,
> but some analogous constructs elsewhere could use this.
>
> -Doug
>

you can also use Unsafe.throwException().

R?mi

>
>> -Aleksey.
>>
>> On 12/28/2012 05:58 PM, ?iktor ?lang wrote:
>>> Reinier Zwitserloot's Project Lombok has an alternate implementation,
>>> does that work better?
>>>
>>> https://github.com/rzwitserloot/lombok/blob/master/src/core/lombok/Lombok.java#L50 
>>>
>>>
>>> Cheers,
>>> ?
>>>
>>>
>>> On Fri, Dec 28, 2012 at 2:53 PM, ?iktor ?lang <viktor.klang at gmail.com
>>> <mailto:viktor.klang at gmail.com>> wrote:
>>>
>>>
>>>
>>>
>>>      On Fri, Dec 28, 2012 at 1:07 PM, Aleksey Shipilev
>>>      <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>>
>>>      wrote:
>>>
>>>          On 12/28/2012 03:58 PM, Doug Lea wrote:
>>>          > On 12/28/12 05:37, Aleksey Shipilev wrote:
>>>          >
>>>          >> Covering the CompletableFuture with concurrency-torture 
>>> tests.
>>>          >>
>>>          >> This guy:
>>>          >>
>>>          >>      @Override
>>>          >>      public void actor1(CompletableFuture<Integer> 
>>> future) {
>>>          >>          future.completeExceptionally(new MyThrowable());
>>>          >>      }
>>>          >>
>>>          >>      public class MyThrowable extends Throwable {}
>>>          >>
>>>          >> ...fails on get() with:
>>>          >>
>>>          >> java.lang.ClassCastException:
>>>          >>
>>> org.openjdk.concurrent.torture.tests.future.completable.MyThrowable
>>>          >> cannot be cast to java.lang.RuntimeException
>>>          >
>>>          > Thanks! I now see that one set of internal constructions 
>>> evade
>>>          > the "sneakyThrow" approach to translation/tunneling. Until
>>>          > this is redone (hopefully very soon) you should be able to
>>>          evade by:
>>>          >
>>>          >   public class MyThrowable extends Error {}
>>>          >
>>>          > (Background for others in case you are curious. The lack 
>>> of a
>>>          > base class/interface for unchecked vs checked exceptions
>>>          > leads to some delicate goopiness inside this and other 
>>> classes.)
>>>
>>>          Note to Viktor: this is reproducible on JDK 7u4-7u12, if you
>>>          pull out
>>>          the sneaky throw thing alone:
>>>
>>>
>>>      Even with -target 1.6 ?
>>>
>>>
>>>
>>>          public class SneakyThrow {
>>>
>>>              public static final void main(String[] args) {
>>>                  rethrow(new MyThrowable());
>>>              }
>>>
>>>              /**
>>>               * A version of "sneaky throw" to relay exceptions
>>>               */
>>>              static void rethrow(final Throwable ex) {
>>>                  if (ex != null) {
>>>                      if (ex instanceof Error)
>>>                          throw (Error)ex;
>>>                      if (ex instanceof RuntimeException)
>>>                          throw (RuntimeException)ex;
>>>                      throw uncheckedThrowable(ex, 
>>> RuntimeException.class);
>>>                  }
>>>              }
>>>
>>>              /**
>>>               * The sneaky part of sneaky throw, relying on generics
>>>               * limitations to evade compiler complaints about 
>>> rethrowing
>>>               * unchecked exceptions
>>>               */
>>>              @SuppressWarnings("unchecked") static <T extends 
>>> Throwable>
>>>                  T uncheckedThrowable(final Throwable t, final 
>>> Class<T> c) {
>>>                  return (T)t; // rely on vacuous cast
>>>              }
>>>
>>>              public static class MyThrowable extends Throwable {}
>>>
>>>          }
>>>
>>>          -Aleksey.
>>>
>>>          _______________________________________________
>>>          Concurrency-interest mailing list
>>>          Concurrency-interest at cs.oswego.edu
>>>          <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>      --
>>>      *Viktor Klang*
>>>      /Director of Engineering/
>>>      /
>>>      /
>>>      Typesafe <http://www.typesafe.com/> - The software stack for
>>>      applications that scale
>>>      Twitter: @viktorklang
>>>
>>>
>>>
>>>
>>> -- 
>>> *Viktor Klang*
>>> /Director of Engineering/
>>> /
>>> /
>>> Typesafe <http://www.typesafe.com/> - The software stack for
>>> applications that scale
>>> Twitter: @viktorklang
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From viktor.klang at gmail.com  Fri Dec 28 09:31:26 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 28 Dec 2012 15:31:26 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDAB97.8020604@univ-mlv.fr>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAB97.8020604@univ-mlv.fr>
Message-ID: <CANPzfU_yE2dtZ3zo0=gajhYh76TYE6SPvUtE23VqUX-Ej4vESg@mail.gmail.com>

On Fri, Dec 28, 2012 at 3:24 PM, Remi Forax <forax at univ-mlv.fr> wrote:

> On 12/28/2012 03:10 PM, Doug Lea wrote:
>
>> On 12/28/12 09:05, Aleksey Shipilev wrote:
>>
>>> Yes, this one works on JDK6-JDK8.
>>>
>>>
>> Thanks. I bypassed the need for it in updated CompletableFuture,
>> but some analogous constructs elsewhere could use this.
>>
>> -Doug
>>
>>
> you can also use Unsafe.throwException().
>

In theory: yes, in practive, it doesn't work on Android.

Cheers,
?


>
> R?mi
>
>
>>  -Aleksey.
>>>
>>> On 12/28/2012 05:58 PM, ?iktor ?lang wrote:
>>>
>>>> Reinier Zwitserloot's Project Lombok has an alternate implementation,
>>>> does that work better?
>>>>
>>>> https://github.com/**rzwitserloot/lombok/blob/**
>>>> master/src/core/lombok/Lombok.**java#L50<https://github.com/rzwitserloot/lombok/blob/master/src/core/lombok/Lombok.java#L50>
>>>>
>>>> Cheers,
>>>> ?
>>>>
>>>>
>>>> On Fri, Dec 28, 2012 at 2:53 PM, ?iktor ?lang <viktor.klang at gmail.com
>>>> <mailto:viktor.klang at gmail.com**>> wrote:
>>>>
>>>>
>>>>
>>>>
>>>>      On Fri, Dec 28, 2012 at 1:07 PM, Aleksey Shipilev
>>>>      <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev@**oracle.com<aleksey.shipilev at oracle.com>
>>>> >>
>>>>      wrote:
>>>>
>>>>          On 12/28/2012 03:58 PM, Doug Lea wrote:
>>>>          > On 12/28/12 05:37, Aleksey Shipilev wrote:
>>>>          >
>>>>          >> Covering the CompletableFuture with concurrency-torture
>>>> tests.
>>>>          >>
>>>>          >> This guy:
>>>>          >>
>>>>          >>      @Override
>>>>          >>      public void actor1(CompletableFuture<**Integer>
>>>> future) {
>>>>          >>          future.completeExceptionally(**new MyThrowable());
>>>>          >>      }
>>>>          >>
>>>>          >>      public class MyThrowable extends Throwable {}
>>>>          >>
>>>>          >> ...fails on get() with:
>>>>          >>
>>>>          >> java.lang.ClassCastException:
>>>>          >>
>>>> org.openjdk.concurrent.**torture.tests.future.**completable.MyThrowable
>>>>          >> cannot be cast to java.lang.RuntimeException
>>>>          >
>>>>          > Thanks! I now see that one set of internal constructions
>>>> evade
>>>>          > the "sneakyThrow" approach to translation/tunneling. Until
>>>>          > this is redone (hopefully very soon) you should be able to
>>>>          evade by:
>>>>          >
>>>>          >   public class MyThrowable extends Error {}
>>>>          >
>>>>          > (Background for others in case you are curious. The lack of a
>>>>          > base class/interface for unchecked vs checked exceptions
>>>>          > leads to some delicate goopiness inside this and other
>>>> classes.)
>>>>
>>>>          Note to Viktor: this is reproducible on JDK 7u4-7u12, if you
>>>>          pull out
>>>>          the sneaky throw thing alone:
>>>>
>>>>
>>>>      Even with -target 1.6 ?
>>>>
>>>>
>>>>
>>>>          public class SneakyThrow {
>>>>
>>>>              public static final void main(String[] args) {
>>>>                  rethrow(new MyThrowable());
>>>>              }
>>>>
>>>>              /**
>>>>               * A version of "sneaky throw" to relay exceptions
>>>>               */
>>>>              static void rethrow(final Throwable ex) {
>>>>                  if (ex != null) {
>>>>                      if (ex instanceof Error)
>>>>                          throw (Error)ex;
>>>>                      if (ex instanceof RuntimeException)
>>>>                          throw (RuntimeException)ex;
>>>>                      throw uncheckedThrowable(ex,
>>>> RuntimeException.class);
>>>>                  }
>>>>              }
>>>>
>>>>              /**
>>>>               * The sneaky part of sneaky throw, relying on generics
>>>>               * limitations to evade compiler complaints about
>>>> rethrowing
>>>>               * unchecked exceptions
>>>>               */
>>>>              @SuppressWarnings("unchecked") static <T extends Throwable>
>>>>                  T uncheckedThrowable(final Throwable t, final Class<T>
>>>> c) {
>>>>                  return (T)t; // rely on vacuous cast
>>>>              }
>>>>
>>>>              public static class MyThrowable extends Throwable {}
>>>>
>>>>          }
>>>>
>>>>          -Aleksey.
>>>>
>>>>          ______________________________**_________________
>>>>          Concurrency-interest mailing list
>>>>          Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>>          <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>> >
>>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>>
>>>>
>>>>
>>>>
>>>>      --
>>>>      *Viktor Klang*
>>>>      /Director of Engineering/
>>>>      /
>>>>      /
>>>>      Typesafe <http://www.typesafe.com/> - The software stack for
>>>>      applications that scale
>>>>      Twitter: @viktorklang
>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> *Viktor Klang*
>>>> /Director of Engineering/
>>>> /
>>>> /
>>>> Typesafe <http://www.typesafe.com/> - The software stack for
>>>> applications that scale
>>>> Twitter: @viktorklang
>>>>
>>>
>>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121228/ea4380cf/attachment.html>

From aleksey.shipilev at oracle.com  Fri Dec 28 09:32:29 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 18:32:29 +0400
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDA843.7010106@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
Message-ID: <50DDAD7D.6060308@oracle.com>

On 12/28/2012 06:10 PM, Doug Lea wrote:
> On 12/28/12 09:05, Aleksey Shipilev wrote:
>> Yes, this one works on JDK6-JDK8.
>>
> 
> Thanks. I bypassed the need for it in updated CompletableFuture,
> but some analogous constructs elsewhere could use this.

Thanks Doug.

I had pushed the first batch of tests into concurrency-torture [1]. You
have to have JDK8 (Lambda) build and jsr166 bootclasspath'ed to run
these tests, otherwise the harness will gracefully fail.

Tests results look good (at very least, no dumb
FutureTask-but-really-the-SettableFuture-like races, at last).

However, I'm at the Viktor's camp at this moment, having a really
terrible feeling about force(). The troubles we have in tests:
 a) it does not care about previous state of CompletableFuture; so it
can erase the result, and also can erase the exception.
 b) it races with complete(), and the subsequent get() can return either
completed value, or the forced one, obliterating consistency
 c) the same applies for race vs. another force()

Even if we go for force(), can we at least hide it under "protected"?
Users requiring forced updates should unlock this on their own risk.
Also, there is no symmetrical forceExceptional(), which can rewrite the
CF state to the exception

Thanks,
-Aleksey.

[1]
https://github.com/shipilev/java-concurrency-torture/tree/master/src/main/java/org/openjdk/concurrent/torture/tests/future/completable


From dl at cs.oswego.edu  Fri Dec 28 09:48:04 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 28 Dec 2012 09:48:04 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDAD7D.6060308@oracle.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com>
Message-ID: <50DDB124.7020909@cs.oswego.edu>

On 12/28/12 09:32, Aleksey Shipilev wrote:

> However, I'm at the Viktor's camp at this moment, having a really
> terrible feeling about force().

The canonical use case is a pick-your-poison desperation scenario:
Something unexpectedly went wrong causing a bad result value or
exception. Other cleanup/recovery code will keep failing until a
usable value is in place. Putting one in place will encounter
races, and might not always succeed, but is better than outright
failure.

And a few more along these lines.

>
> Even if we go for force(), can we at least hide it under "protected"?

We used to do things like this for methods that people shouldn't
normally use. It makes everyone feel better about releasing it,
but makes it even worse for the people who need it in some corner case
because no one except them knew to use a subclass allowing it.

(Maybe we could instead require a non-null instance of
Unsafe as a required argument :-)

In any case, the specs do need even more disclaimers.

-Doug


From viktor.klang at gmail.com  Fri Dec 28 09:56:05 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 28 Dec 2012 15:56:05 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDB124.7020909@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
Message-ID: <CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>

On Fri, Dec 28, 2012 at 3:48 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/28/12 09:32, Aleksey Shipilev wrote:
>
>  However, I'm at the Viktor's camp at this moment, having a really
>> terrible feeling about force().
>>
>
> The canonical use case is a pick-your-poison desperation scenario:
> Something unexpectedly went wrong causing a bad result value or
> exception. Other cleanup/recovery code will keep failing until a
> usable value is in place. Putting one in place will encounter
> races, and might not always succeed, but is better than outright
> failure.
>
> And a few more along these lines.


What is your thought on "recover"?


>
>
>
>> Even if we go for force(), can we at least hide it under "protected"?
>>
>
> We used to do things like this for methods that people shouldn't
> normally use. It makes everyone feel better about releasing it,
> but makes it even worse for the people who need it in some corner case
> because no one except them knew to use a subclass allowing it.
>
> (Maybe we could instead require a non-null instance of
> Unsafe as a required argument :-)
>

Lol, that's a good one :)

Cheers,
?


>
> In any case, the specs do need even more disclaimers.
>
> -Doug
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121228/a92f9362/attachment.html>

From aleksey.shipilev at oracle.com  Fri Dec 28 09:57:55 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 18:57:55 +0400
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDB124.7020909@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
Message-ID: <50DDB373.1000509@oracle.com>

On 12/28/2012 06:48 PM, Doug Lea wrote:
>> Even if we go for force(), can we at least hide it under "protected"?
> 
> We used to do things like this for methods that people shouldn't
> normally use. It makes everyone feel better about releasing it,
> but makes it even worse for the people who need it in some corner case
> because no one except them knew to use a subclass allowing it.

Yeah, tough choice. The trouble I envision is users picking up force()
from the IDE autocompletion without even realizing what can go wrong. If
only we had the way to mark this method special. Maybe static trampoline
method is enough at marking this action as a very special case?

> (Maybe we could instead require a non-null instance of
> Unsafe as a required argument :-)

LOL.

-Aleksey.

From dl at cs.oswego.edu  Fri Dec 28 13:19:16 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 28 Dec 2012 13:19:16 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
Message-ID: <50DDE2A4.2090903@cs.oswego.edu>

On 12/28/12 09:56, ?iktor ?lang wrote:
> What is your thought on "recover"?

It forms a good argument to rework CF.exceptionally in a mostly
similar way; to produce a result rather than always acting as a
terminal stage. Thanks for the prod!

But still doesn't address the unexpected-error problem.

On 12/28/12 09:57, Aleksey Shipilev wrote:
> Yeah, tough choice. The trouble I envision is users picking up force() from
> the IDE autocompletion without even realizing what can go wrong. If

Picking a better name would help. Scanning around thesauruses,
I'm going for the pleasantly obscure "obtrude"
(def: "to force or impose without warrant or request" ).
If anyone ever selects this by accident, then they are clearly
trying to write random code :-)

     /**
      * Forcibly sets or resets the value subsequently returned by
      * method get() and related methods, whether or not already
      * completed. This method is designed for use only in error
      * recovery actions, and even in such situations may result in
      * ongoing dependent completions using established versus overwritten
      * values.
      *
      * @param value the completion value
      */
     public void obtrudeValue(T value) {


-Doug



From aleksey.shipilev at oracle.com  Fri Dec 28 13:36:10 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 22:36:10 +0400
Subject: [concurrency-interest] IRIW and sequential consistency
Message-ID: <50DDE69A.4020101@oracle.com>

Hi guys,

I wanted to cross-check what I know about IRIW and sequential
consistency. The whole topic is another can of worms, but I have the
specific question, given:

   Thread 1 |  Thread 2  |  Thread 3  |  Thread 4
  ----------+------------+------------+------------
     x = 1  |    y = 1   |   r1 = x   |  r4 = y
            |            |   r2 = y   |  r3 = x

...is that true that the violation of sequential consistency can only be
detected with <r1,r2,r3,r4> = <1, 0, 0, 1>? That's what my simple Python
script [1] exhausting all the SC executions tells me.

Context: java-concurrency-torture finally does IRIW tests [2][3], and I
need to stick in some grading. It seems to me that under JMM, if $x and
$y are volatile, then operations on them are SC, and then IRIW should
observe only SC executions, right [4]?

-Aleksey.

P.S. The interesting gig is that <r1,r2,r3,r4> = <0, 1, 1, 0> case on my
x86 laptop is extremely rare (while being allowed by SC). I wonder
what's the reasonable explanation for this.


[1] http://shipilev.net/pub/stuff/iriw.py
[2]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/volatiles/VolatileIRIWTest.java
[3]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/volatiles/IRIWTest.java
[4]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/resources/org/openjdk/concurrent/torture/desc/volatiles-iriw.xml

From aleksey.shipilev at oracle.com  Fri Dec 28 13:56:52 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 22:56:52 +0400
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDE2A4.2090903@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu>
Message-ID: <50DDEB74.8090306@oracle.com>

On 12/28/2012 10:19 PM, Doug Lea wrote:
> On 12/28/12 09:57, Aleksey Shipilev wrote:
>> Yeah, tough choice. The trouble I envision is users picking up force()
>> from
>> the IDE autocompletion without even realizing what can go wrong. If
> 
> Picking a better name would help. Scanning around thesauruses,
> I'm going for the pleasantly obscure "obtrude"
> (def: "to force or impose without warrant or request" ).
> If anyone ever selects this by accident, then they are clearly
> trying to write random code :-)

Well, Java allows Unicode in the identifiers, why don't we pick
something from simplified Chinese? (Expert programmers can always
copy-paste from Javadoc) :]

On the serious note, obtrudeValue() works. I thought about something
more consistent, e.g. destructivelyComplete(), in the same ballpark as
exceptionallyComplete().

-Aleksey.


From dl at cs.oswego.edu  Fri Dec 28 14:02:57 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 28 Dec 2012 14:02:57 -0500
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DDE69A.4020101@oracle.com>
References: <50DDE69A.4020101@oracle.com>
Message-ID: <50DDECE1.8080409@cs.oswego.edu>

On 12/28/12 13:36, Aleksey Shipilev wrote:
> I have the
> specific question, given:
>
>     Thread 1 |  Thread 2  |  Thread 3  |  Thread 4
>    ----------+------------+------------+------------
>       x = 1  |    y = 1   |   r1 = x   |  r4 = y
>              |            |   r2 = y   |  r3 = x
>
> ...is that true that the violation of sequential consistency can only be
> detected with <r1,r2,r3,r4> = <1, 0, 0, 1>?

Yes. You might want to cross-check some of these with
Peter Sewell et al's instruction-level tests.
See links on "Software and Testing Tools" at
http://www.cl.cam.ac.uk/~pes20/weakmemory/index.html

> Context: java-concurrency-torture finally does IRIW tests [2][3], and I
> need to stick in some grading. It seems to me that under JMM, if $x and
> $y are volatile, then operations on them are SC, and then IRIW should
> observe only SC executions, right [4]?

Yes.

> P.S. The interesting gig is that <r1,r2,r3,r4> = <0, 1, 1, 0> case on my
> x86 laptop is extremely rare (while being allowed by SC). I wonder
> what's the reasonable explanation for this.
>

Most hardware is a bit more consistent than it is required to be,
because they have some of the same issues we do in software,
and so force some cases to be impossible. You can't depend on this though.

-Doug



From aleksey.shipilev at oracle.com  Fri Dec 28 14:12:25 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 23:12:25 +0400
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DDECE1.8080409@cs.oswego.edu>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
Message-ID: <50DDEF19.3010804@oracle.com>

On 12/28/2012 11:02 PM, Doug Lea wrote:
> On 12/28/12 13:36, Aleksey Shipilev wrote:
>> I have the
>> specific question, given:
>>
>>     Thread 1 |  Thread 2  |  Thread 3  |  Thread 4
>>    ----------+------------+------------+------------
>>       x = 1  |    y = 1   |   r1 = x   |  r4 = y
>>              |            |   r2 = y   |  r3 = x
>>
>> ...is that true that the violation of sequential consistency can only be
>> detected with <r1,r2,r3,r4> = <1, 0, 0, 1>?
> 
> Yes. You might want to cross-check some of these with
> Peter Sewell et al's instruction-level tests.
> See links on "Software and Testing Tools" at
> http://www.cl.cam.ac.uk/~pes20/weakmemory/index.html

Yeah, I was lazy to do that.

Thanks, Doug. Phew, this is one of the few cases dealing with memory
models where I can get the 100% correct answer from the start! :)

-Aleksey.


From aleksey.shipilev at oracle.com  Fri Dec 28 14:17:10 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 23:17:10 +0400
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DDECE1.8080409@cs.oswego.edu>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
Message-ID: <50DDF036.7000900@oracle.com>

On 12/28/2012 11:02 PM, Doug Lea wrote:
>> P.S. The interesting gig is that <r1,r2,r3,r4> = <0, 1, 1, 0> case on my
>> x86 laptop is extremely rare (while being allowed by SC). I wonder
>> what's the reasonable explanation for this.
> 
> Most hardware is a bit more consistent than it is required to be,
> because they have some of the same issues we do in software,
> and so force some cases to be impossible. You can't depend on this though.

This is unrelated, so in the separate thread. The thing is not about
being impossible, it is observed, but several orders of magnitude (well,
~10^8) less frequently than other cases, which makes it interestingly
special. I'd speculate the explanation is somewhere in the thread
scheduling realm. Maybe someone on this list would feel intrigued enough
to untangle this ;)

-Aleksey.


From vitalyd at gmail.com  Fri Dec 28 14:56:37 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 28 Dec 2012 14:56:37 -0500
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DDF036.7000900@oracle.com>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
	<50DDF036.7000900@oracle.com>
Message-ID: <CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>

How many cores does your laptop have? Is the system otherwise quiet while
running your tests? No high interrupt frequency? I'd also guess that it's
related to OS scheduling.

Sent from my phone
On Dec 28, 2012 2:19 PM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com>
wrote:

> On 12/28/2012 11:02 PM, Doug Lea wrote:
> >> P.S. The interesting gig is that <r1,r2,r3,r4> = <0, 1, 1, 0> case on my
> >> x86 laptop is extremely rare (while being allowed by SC). I wonder
> >> what's the reasonable explanation for this.
> >
> > Most hardware is a bit more consistent than it is required to be,
> > because they have some of the same issues we do in software,
> > and so force some cases to be impossible. You can't depend on this
> though.
>
> This is unrelated, so in the separate thread. The thing is not about
> being impossible, it is observed, but several orders of magnitude (well,
> ~10^8) less frequently than other cases, which makes it interestingly
> special. I'd speculate the explanation is somewhere in the thread
> scheduling realm. Maybe someone on this list would feel intrigued enough
> to untangle this ;)
>
> -Aleksey.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121228/8c84e804/attachment.html>

From aleksey.shipilev at oracle.com  Fri Dec 28 14:59:13 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 28 Dec 2012 23:59:13 +0400
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
	<50DDF036.7000900@oracle.com>
	<CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>
Message-ID: <50DDFA11.1050602@oracle.com>

Yes, my laptop has 1 package, 2 cores, 4 hardware threads; and the test
requires 5 threads. Harness forces yields on the safest point in the
workload, and only 4 application threads are in need to be active at
every single point.

You are welcome to try on larger server :)

-Aleksey.

On 12/28/2012 11:56 PM, Vitaly Davidovich wrote:
> How many cores does your laptop have? Is the system otherwise quiet
> while running your tests? No high interrupt frequency? I'd also guess
> that it's related to OS scheduling.
> 
> Sent from my phone
> 
> On Dec 28, 2012 2:19 PM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com
> <mailto:aleksey.shipilev at oracle.com>> wrote:
> 
>     On 12/28/2012 11:02 PM, Doug Lea wrote:
>     >> P.S. The interesting gig is that <r1,r2,r3,r4> = <0, 1, 1, 0>
>     case on my
>     >> x86 laptop is extremely rare (while being allowed by SC). I wonder
>     >> what's the reasonable explanation for this.
>     >
>     > Most hardware is a bit more consistent than it is required to be,
>     > because they have some of the same issues we do in software,
>     > and so force some cases to be impossible. You can't depend on this
>     though.
> 
>     This is unrelated, so in the separate thread. The thing is not about
>     being impossible, it is observed, but several orders of magnitude (well,
>     ~10^8) less frequently than other cases, which makes it interestingly
>     special. I'd speculate the explanation is somewhere in the thread
>     scheduling realm. Maybe someone on this list would feel intrigued enough
>     to untangle this ;)
> 
>     -Aleksey.
> 
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From jed at atlassian.com  Fri Dec 28 15:35:07 2012
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Sat, 29 Dec 2012 07:35:07 +1100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DDEB74.8090306@oracle.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu> <50DDEB74.8090306@oracle.com>
Message-ID: <CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>

Being firmly in the functional camp I agree with the creepiness
feeling of Viktor and Aleksey regarding the force/obtrudeValue. Once
completed it would na?vely seem the horse has already bolted.

We have been using a Promise quite successfully internally, and
although it unfortunately extends guava's ListenableFuture and all its
complication, the essential uses are quite similar to the Scala
Future:

https://bitbucket.org/atlassian/atlassian-util-concurrent/src/master/src/main/java/com/atlassian/util/concurrent/Promise.java

The API is deliberately quite simple. Of particular value is the
flatMap operation as it allows you to sequence multiple Future
producing operations together into one. The closest I can see on
CompletableFuture is andThen/andThenAsync which seem quite a bit more
complicated to use.

cheers,
jed.


From nathan.reynolds at oracle.com  Fri Dec 28 15:46:07 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 28 Dec 2012 13:46:07 -0700
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DDFA11.1050602@oracle.com>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
	<50DDF036.7000900@oracle.com>
	<CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>
	<50DDFA11.1050602@oracle.com>
Message-ID: <50DE050F.8070202@oracle.com>

How do you signal the threads to start?  Do you sequentially unpark the 
threads?  Do you use notifyAll?  Or, do you have a global volatile 
boolean which one thread sets while the other 4 threads are spinning 
(without yield) on it?

Running 5 software threads on 4 hardware threads is a bit troublesome.  
Have you tried using CPU affinity to force the threads to their own 
hardware thread?  What happens if you change where each thread runs?  
You might consider going through all of the permutations to see if that 
has an impact on the results.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/28/2012 12:59 PM, Aleksey Shipilev wrote:
> Yes, my laptop has 1 package, 2 cores, 4 hardware threads; and the test
> requires 5 threads. Harness forces yields on the safest point in the
> workload, and only 4 application threads are in need to be active at
> every single point.
>
> You are welcome to try on larger server :)
>
> -Aleksey.
>
> On 12/28/2012 11:56 PM, Vitaly Davidovich wrote:
>> How many cores does your laptop have? Is the system otherwise quiet
>> while running your tests? No high interrupt frequency? I'd also guess
>> that it's related to OS scheduling.
>>
>> Sent from my phone
>>
>> On Dec 28, 2012 2:19 PM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com
>> <mailto:aleksey.shipilev at oracle.com>> wrote:
>>
>>      On 12/28/2012 11:02 PM, Doug Lea wrote:
>>      >> P.S. The interesting gig is that <r1,r2,r3,r4> = <0, 1, 1, 0>
>>      case on my
>>      >> x86 laptop is extremely rare (while being allowed by SC). I wonder
>>      >> what's the reasonable explanation for this.
>>      >
>>      > Most hardware is a bit more consistent than it is required to be,
>>      > because they have some of the same issues we do in software,
>>      > and so force some cases to be impossible. You can't depend on this
>>      though.
>>
>>      This is unrelated, so in the separate thread. The thing is not about
>>      being impossible, it is observed, but several orders of magnitude (well,
>>      ~10^8) less frequently than other cases, which makes it interestingly
>>      special. I'd speculate the explanation is somewhere in the thread
>>      scheduling realm. Maybe someone on this list would feel intrigued enough
>>      to untangle this ;)
>>
>>      -Aleksey.
>>
>>      _______________________________________________
>>      Concurrency-interest mailing list
>>      Concurrency-interest at cs.oswego.edu
>>      <mailto:Concurrency-interest at cs.oswego.edu>
>>      http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121228/56a92c84/attachment.html>

From aleksey.shipilev at oracle.com  Fri Dec 28 16:00:19 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sat, 29 Dec 2012 01:00:19 +0400
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DE050F.8070202@oracle.com>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
	<50DDF036.7000900@oracle.com>
	<CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>
	<50DDFA11.1050602@oracle.com> <50DE050F.8070202@oracle.com>
Message-ID: <50DE0863.3010402@oracle.com>

On 12/29/2012 12:46 AM, Nathan Reynolds wrote:
> How do you signal the threads to start?  Do you sequentially unpark the
> threads?  Do you use notifyAll?  Or, do you have a global volatile
> boolean which one thread sets while the other 4 threads are spinning
> (without yield) on it?

Short answer: all threads busy-wait on volatile field with yielding.

Longer answer: in active phase, 4 threads are doing the work without
yields, and 1 thread is busy-waiting with yield; in work-injecting
phase, 4 threads are busy-waiting with yields, 1 thread is doing the
work without the yield.

Longest answer:
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/infra/FourActorsRunner.java

> Running 5 software threads on 4 hardware threads is a bit troublesome. 
> Have you tried using CPU affinity to force the threads to their own
> hardware thread?  What happens if you change where each thread runs? 
> You might consider going through all of the permutations to see if that
> has an impact on the results.

Nope, I did not try that. FWIW, I think it's better to just try and run
it on the large server (but I'm lazy enough to skip that part until
better time).

-Aleksey.

From nathan.reynolds at oracle.com  Fri Dec 28 16:15:18 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 28 Dec 2012 14:15:18 -0700
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DE0863.3010402@oracle.com>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
	<50DDF036.7000900@oracle.com>
	<CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>
	<50DDFA11.1050602@oracle.com> <50DE050F.8070202@oracle.com>
	<50DE0863.3010402@oracle.com>
Message-ID: <50DE0BE6.9020109@oracle.com>

I looked at the longest answer.  Do the results change if you get rid of 
the yield?

Each time yield is called, the caller goes into the run queue and the 1 
thread in the run queue starts running.  Depending upon which thread is 
in the run queue when the go signal happens, that thread will naturally 
fall behind since it has to exit the kernel.  The other threads are 
already in user land.  I join your suspicions that the delta is due to 
thread scheduling.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/28/2012 2:00 PM, Aleksey Shipilev wrote:
> On 12/29/2012 12:46 AM, Nathan Reynolds wrote:
>> How do you signal the threads to start?  Do you sequentially unpark the
>> threads?  Do you use notifyAll?  Or, do you have a global volatile
>> boolean which one thread sets while the other 4 threads are spinning
>> (without yield) on it?
> Short answer: all threads busy-wait on volatile field with yielding.
>
> Longer answer: in active phase, 4 threads are doing the work without
> yields, and 1 thread is busy-waiting with yield; in work-injecting
> phase, 4 threads are busy-waiting with yields, 1 thread is doing the
> work without the yield.
>
> Longest answer:
> https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/infra/FourActorsRunner.java
>
>> Running 5 software threads on 4 hardware threads is a bit troublesome.
>> Have you tried using CPU affinity to force the threads to their own
>> hardware thread?  What happens if you change where each thread runs?
>> You might consider going through all of the permutations to see if that
>> has an impact on the results.
> Nope, I did not try that. FWIW, I think it's better to just try and run
> it on the large server (but I'm lazy enough to skip that part until
> better time).
>
> -Aleksey.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121228/d1721fa2/attachment.html>

From aleksey.shipilev at oracle.com  Sat Dec 29 05:14:46 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sat, 29 Dec 2012 14:14:46 +0400
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DE0BE6.9020109@oracle.com>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
	<50DDF036.7000900@oracle.com>
	<CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>
	<50DDFA11.1050602@oracle.com> <50DE050F.8070202@oracle.com>
	<50DE0863.3010402@oracle.com> <50DE0BE6.9020109@oracle.com>
Message-ID: <50DEC296.70207@oracle.com>

On 12/29/2012 01:15 AM, Nathan Reynolds wrote:
> I looked at the longest answer.  Do the results change if you get rid of
> the yield?

Disabling yielding will severely limit the throughput (because actors
cannibalize the CPUs not letting the injector to push the new set of
states to work on).

> I join your suspicions that the delta is due to thread scheduling.

Yeah, and the interesting part is that only this state is rare. All
others are orders of magnitude more frequent. Go figure.

-Aleksey.


From viktor.klang at gmail.com  Sat Dec 29 07:57:09 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sat, 29 Dec 2012 13:57:09 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu> <50DDEB74.8090306@oracle.com>
	<CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>
Message-ID: <CANPzfU9pwHOgG6v6WLATPFFT3Ex+nMBqU4v5UHST7ZPZ397RJw@mail.gmail.com>

I feel that if you need to have to ability to "recover" from a
catasttrophic event, you return a different CompletableFuture from the one
that is going to get completed with the result of the possibly catastrophic
event, and then you are free to populate the actual return value
independently from the dangerous operation result.

Cheers,
?


On Fri, Dec 28, 2012 at 9:35 PM, Jed Wesley-Smith <jed at atlassian.com> wrote:

> Being firmly in the functional camp I agree with the creepiness
> feeling of Viktor and Aleksey regarding the force/obtrudeValue. Once
> completed it would na?vely seem the horse has already bolted.
>
> We have been using a Promise quite successfully internally, and
> although it unfortunately extends guava's ListenableFuture and all its
> complication, the essential uses are quite similar to the Scala
> Future:
>
>
> https://bitbucket.org/atlassian/atlassian-util-concurrent/src/master/src/main/java/com/atlassian/util/concurrent/Promise.java
>
> The API is deliberately quite simple. Of particular value is the
> flatMap operation as it allows you to sequence multiple Future
> producing operations together into one. The closest I can see on
> CompletableFuture is andThen/andThenAsync which seem quite a bit more
> complicated to use.
>
> cheers,
> jed.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121229/c28600c6/attachment-0001.html>

From dl at cs.oswego.edu  Sat Dec 29 08:14:40 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 29 Dec 2012 08:14:40 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU9pwHOgG6v6WLATPFFT3Ex+nMBqU4v5UHST7ZPZ397RJw@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu> <50DDEB74.8090306@oracle.com>
	<CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>
	<CANPzfU9pwHOgG6v6WLATPFFT3Ex+nMBqU4v5UHST7ZPZ397RJw@mail.gmail.com>
Message-ID: <50DEECC0.9080103@cs.oswego.edu>

On 12/29/12 07:57, ?iktor ?lang wrote:
> I feel that if you need to have to ability to "recover" from a catasttrophic
> event, you return a different CompletableFuture from the one that is going to
> get completed with the result of the possibly catastrophic event, and then you
> are free to populate the actual return value independently from the dangerous
> operation result.
>

Just to restate this hopefully one last time: We have a consistent
story in j.u.c that we provide *some* way to recover from unexpected
events; sometimes ordinary cases, sometimes last resort options: 
ExecutorService.shutdownNow, Future.cancel, Phaser.forceTermination,
ForkJoinTask.reset, and so on. All of them are racy. All of them
may fail to lead to full recovery. We hope you never need to use any
of them, but are sure that someday you will.

-Doug





From viktor.klang at gmail.com  Sat Dec 29 08:31:49 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sat, 29 Dec 2012 14:31:49 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DEECC0.9080103@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu> <50DDEB74.8090306@oracle.com>
	<CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>
	<CANPzfU9pwHOgG6v6WLATPFFT3Ex+nMBqU4v5UHST7ZPZ397RJw@mail.gmail.com>
	<50DEECC0.9080103@cs.oswego.edu>
Message-ID: <CANPzfU9nMyfwDCjAgi=DsiWWnXWuB30_TdYhkP92qbt-8CViqA@mail.gmail.com>

On Sat, Dec 29, 2012 at 2:14 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/29/12 07:57, ?iktor ?lang wrote:
>
>> I feel that if you need to have to ability to "recover" from a
>> catasttrophic
>> event, you return a different CompletableFuture from the one that is
>> going to
>> get completed with the result of the possibly catastrophic event, and
>> then you
>> are free to populate the actual return value independently from the
>> dangerous
>> operation result.
>>
>>
> Just to restate this hopefully one last time: We have a consistent
> story in j.u.c that we provide *some* way to recover from unexpected
> events; sometimes ordinary cases, sometimes last resort options:


I still unconvinced that the "recover" method is insufficient, but am happy
to be proven otherwise.


> ExecutorService.shutdownNow,


This is one of the reasons I never expose anything but Executor as the user
API, according to "The creator is the destroyer".
So I view ExecutorService as the producer-side of the API and Executor as
the consumer-side of the API.


> Future.cancel,


juc.Future already sadly has the cancel method so there's not much to do
about that.
I fought hard to keep similar methods out of the Scala Future library as
this method make it impossible to reason about your code as you do not know
who has access to mess with your Future. (Especially in a non-closed-world
scenario where you get passed Futures and return Futures).


> Phaser.forceTermination,
>

Never used, so cannot comment.


> ForkJoinTask.reset,


Wasn't usable for me since, correct me if I'm wrong, it couldn't be called
within the task itself, leading to having to wrap tasks in tasks leading to
bad cache locality and excessive GCing.


> and so on. All of them are racy. All of them
> may fail to lead to full recovery. We hope you never need to use any
> of them, but are sure that someday you will.


>From my perspective it doesn't really matter to me whether there's a force
method or not, since I'd just create a thin integration and only ever
expose Scala Futures, but knowing what problems people run into I just
wanted to share my experiences in hope that the users of the
CompletableFuture API doesn't get it delivered with the barrel footwards.
:-)

Have you considered splitting out things in a manner similar to
Promise/Future with one side having the write-operations and the other the
read-operations, so you as a user can choose what capabilities you expose?

Cheers,
?


>
> -Doug
>
>
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121229/b6011ae1/attachment.html>

From viktor.klang at gmail.com  Sat Dec 29 08:44:34 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sat, 29 Dec 2012 14:44:34 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU9nMyfwDCjAgi=DsiWWnXWuB30_TdYhkP92qbt-8CViqA@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu> <50DDEB74.8090306@oracle.com>
	<CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>
	<CANPzfU9pwHOgG6v6WLATPFFT3Ex+nMBqU4v5UHST7ZPZ397RJw@mail.gmail.com>
	<50DEECC0.9080103@cs.oswego.edu>
	<CANPzfU9nMyfwDCjAgi=DsiWWnXWuB30_TdYhkP92qbt-8CViqA@mail.gmail.com>
Message-ID: <CANPzfU_-7HaOafBtyHNAOBOvYywojAC6y1MM=3zcQnjygvWX8w@mail.gmail.com>

This might just be solved by having a CompletableFutureTask (extends
FutureTask) that has the obtrude method, and then have it extend
CompletableFuture, which does not have the obtrude method?

Cheers,
?


On Sat, Dec 29, 2012 at 2:31 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:

>
>
>
> On Sat, Dec 29, 2012 at 2:14 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 12/29/12 07:57, ?iktor ?lang wrote:
>>
>>> I feel that if you need to have to ability to "recover" from a
>>> catasttrophic
>>> event, you return a different CompletableFuture from the one that is
>>> going to
>>> get completed with the result of the possibly catastrophic event, and
>>> then you
>>> are free to populate the actual return value independently from the
>>> dangerous
>>> operation result.
>>>
>>>
>> Just to restate this hopefully one last time: We have a consistent
>> story in j.u.c that we provide *some* way to recover from unexpected
>> events; sometimes ordinary cases, sometimes last resort options:
>
>
> I still unconvinced that the "recover" method is insufficient, but am
> happy to be proven otherwise.
>
>
>> ExecutorService.shutdownNow,
>
>
> This is one of the reasons I never expose anything but Executor as the
> user API, according to "The creator is the destroyer".
> So I view ExecutorService as the producer-side of the API and Executor as
> the consumer-side of the API.
>
>
>> Future.cancel,
>
>
> juc.Future already sadly has the cancel method so there's not much to do
> about that.
> I fought hard to keep similar methods out of the Scala Future library as
> this method make it impossible to reason about your code as you do not know
> who has access to mess with your Future. (Especially in a non-closed-world
> scenario where you get passed Futures and return Futures).
>
>
>> Phaser.forceTermination,
>>
>
> Never used, so cannot comment.
>
>
>> ForkJoinTask.reset,
>
>
> Wasn't usable for me since, correct me if I'm wrong, it couldn't be called
> within the task itself, leading to having to wrap tasks in tasks leading to
> bad cache locality and excessive GCing.
>
>
>> and so on. All of them are racy. All of them
>> may fail to lead to full recovery. We hope you never need to use any
>> of them, but are sure that someday you will.
>
>
> From my perspective it doesn't really matter to me whether there's a force
> method or not, since I'd just create a thin integration and only ever
> expose Scala Futures, but knowing what problems people run into I just
> wanted to share my experiences in hope that the users of the
> CompletableFuture API doesn't get it delivered with the barrel footwards.
> :-)
>
> Have you considered splitting out things in a manner similar to
> Promise/Future with one side having the write-operations and the other the
> read-operations, so you as a user can choose what capabilities you expose?
>
> Cheers,
> ?
>
>
>>
>> -Doug
>>
>>
>>
>>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> *
> *
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @viktorklang
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121229/76dfc6f1/attachment.html>

From dl at cs.oswego.edu  Sat Dec 29 09:49:05 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 29 Dec 2012 09:49:05 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_-7HaOafBtyHNAOBOvYywojAC6y1MM=3zcQnjygvWX8w@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu> <50DDEB74.8090306@oracle.com>
	<CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>
	<CANPzfU9pwHOgG6v6WLATPFFT3Ex+nMBqU4v5UHST7ZPZ397RJw@mail.gmail.com>
	<50DEECC0.9080103@cs.oswego.edu>
	<CANPzfU9nMyfwDCjAgi=DsiWWnXWuB30_TdYhkP92qbt-8CViqA@mail.gmail.com>
	<CANPzfU_-7HaOafBtyHNAOBOvYywojAC6y1MM=3zcQnjygvWX8w@mail.gmail.com>
Message-ID: <50DF02E1.7030808@cs.oswego.edu>

On 12/29/12 08:44, ?iktor ?lang wrote:
> This might just be solved by having a CompletableFutureTask (extends FutureTask)

Please no!! The main motivation for CompletableFuture is
to overcome the JDK5 decision to give up on any form of
non-task-based settable Future because we couldn't agree on API.
Which has led to all sorts of incompatible third-party versions,
many of which abuse FutureTask by bypassing the Task aspects.
The only good thing about waiting so many years is that the existence
of lambdas makes the basic shape of this API less controversial, as
witnessed by the similarities to other recent similar APIs people
have mentioned. The main goal for j.u.c version is enough generality
to serve these needs, along with the best possible performance
and reliability.

Also, while I'm at it:


>
>         ForkJoinTask.reset,
>
>
>     Wasn't usable for me since, correct me if I'm wrong, it couldn't be called
>     within the task itself, leading to having to wrap tasks in tasks leading to
>     bad cache locality and excessive GCing.

No, you shouldn't/can't call from an active task. But you can
call when known quiescent. (See the Jacobi demos in loops CVS
for an example of use in a non-recovery situation; to re-traverse
a computation tree after a full stop.) You can also cache old ones
in, say, a thread-local list, although in many cases the overhead
of doing this wouldn't outweigh allocation/construction costs.

>
>      From my perspective it doesn't really matter to me whether there's a force
>     method or not, since I'd just create a thin integration and only ever expose
>     Scala Futures,

And people like you are our main audience. Basic concurrency control support
components are rarely the best things to expose at application levels anyway.

-Doug





From viktor.klang at gmail.com  Sat Dec 29 10:03:21 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sat, 29 Dec 2012 16:03:21 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DF02E1.7030808@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu> <50DDEB74.8090306@oracle.com>
	<CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>
	<CANPzfU9pwHOgG6v6WLATPFFT3Ex+nMBqU4v5UHST7ZPZ397RJw@mail.gmail.com>
	<50DEECC0.9080103@cs.oswego.edu>
	<CANPzfU9nMyfwDCjAgi=DsiWWnXWuB30_TdYhkP92qbt-8CViqA@mail.gmail.com>
	<CANPzfU_-7HaOafBtyHNAOBOvYywojAC6y1MM=3zcQnjygvWX8w@mail.gmail.com>
	<50DF02E1.7030808@cs.oswego.edu>
Message-ID: <CANPzfU8B0xKT0CmC8ow=KjZCpcDSdnaHJW33a6h6LH5eekiRMA@mail.gmail.com>

On Sat, Dec 29, 2012 at 3:49 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/29/12 08:44, ?iktor ?lang wrote:
>
>> This might just be solved by having a CompletableFutureTask (extends
>> FutureTask)
>>
>
> Please no!! The main motivation for CompletableFuture is
> to overcome the JDK5 decision to give up on any form of
> non-task-based settable Future because we couldn't agree on API.
>

Ah, this was definitely news to me!


> Which has led to all sorts of incompatible third-party versions,
> many of which abuse FutureTask by bypassing the Task aspects.
>

Sounds bad


> The only good thing about waiting so many years is that the existence
> of lambdas makes the basic shape of this API less controversial, as
> witnessed by the similarities to other recent similar APIs people
> have mentioned. The main goal for j.u.c version is enough generality
> to serve these needs, along with the best possible performance
> and reliability.
>

Alright, thanks for the recap, it's nice to be able to put your proposal
into context.


>
> Also, while I'm at it:
>
>
>
>
>>         ForkJoinTask.reset,
>>
>>
>>     Wasn't usable for me since, correct me if I'm wrong, it couldn't be
>> called
>>     within the task itself, leading to having to wrap tasks in tasks
>> leading to
>>     bad cache locality and excessive GCing.
>>
>
> No, you shouldn't/can't call from an active task. But you can
> call when known quiescent. (See the Jacobi demos in loops CVS
> for an example of use in a non-recovery situation; to re-traverse
> a computation tree after a full stop.)


I unfortunately cannot do that, I need an async tailcall of sorts, i.e. at
the end of the task, set that the task is ready to be resubmitted.


> You can also cache old ones
> in, say, a thread-local list, although in many cases the overhead
> of doing this wouldn't outweigh allocation/construction costs.


Yes, this definitely aligns with my experience.


>
>
>
>>      From my perspective it doesn't really matter to me whether there's a
>> force
>>     method or not, since I'd just create a thin integration and only ever
>> expose
>>     Scala Futures,
>>
>
> And people like you are our main audience. Basic concurrency control
> support
> components are rarely the best things to expose at application levels
> anyway.


Ok, so CompletableFuture is intended as an integration of sorts? In that
case it makes more sense, but the problem will be how to reason about
semantics, i.e. how does cancellation cascade, how does obtrude trickle
down etc.

Cheers,
?


>
>
> -Doug
>
>
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121229/1b814e17/attachment.html>

From gregg at cytetech.com  Sat Dec 29 11:20:48 2012
From: gregg at cytetech.com (Gregg Wonderly)
Date: Sat, 29 Dec 2012 10:20:48 -0600
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DEC296.70207@oracle.com>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
	<50DDF036.7000900@oracle.com>
	<CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>
	<50DDFA11.1050602@oracle.com> <50DE050F.8070202@oracle.com>
	<50DE0863.3010402@oracle.com> <50DE0BE6.9020109@oracle.com>
	<50DEC296.70207@oracle.com>
Message-ID: <50DF1860.1050009@cytetech.com>

Maybe instead of yield you can call to the injector's push activity to put 
values back into play?  If you do that with some "randomness" and some "specific 
count" both, you might see whether it's order of execution/scheduling or what?

Gregg Wonderly

On 12/29/2012 4:14 AM, Aleksey Shipilev wrote:
> On 12/29/2012 01:15 AM, Nathan Reynolds wrote:
>> I looked at the longest answer.  Do the results change if you get rid of
>> the yield?
>
> Disabling yielding will severely limit the throughput (because actors
> cannibalize the CPUs not letting the injector to push the new set of
> states to work on).
>
>> I join your suspicions that the delta is due to thread scheduling.
>
> Yeah, and the interesting part is that only this state is rare. All
> others are orders of magnitude more frequent. Go figure.
>
> -Aleksey.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From gregg at cytetech.com  Sat Dec 29 11:43:53 2012
From: gregg at cytetech.com (Gregg Wonderly)
Date: Sat, 29 Dec 2012 10:43:53 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_-7HaOafBtyHNAOBOvYywojAC6y1MM=3zcQnjygvWX8w@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu> <50DDEB74.8090306@oracle.com>
	<CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>
	<CANPzfU9pwHOgG6v6WLATPFFT3Ex+nMBqU4v5UHST7ZPZ397RJw@mail.gmail.com>
	<50DEECC0.9080103@cs.oswego.edu>
	<CANPzfU9nMyfwDCjAgi=DsiWWnXWuB30_TdYhkP92qbt-8CViqA@mail.gmail.com>
	<CANPzfU_-7HaOafBtyHNAOBOvYywojAC6y1MM=3zcQnjygvWX8w@mail.gmail.com>
Message-ID: <50DF1DC9.8060802@cytetech.com>

I was also thinking along these lines.  I am not sure I have a concrete example 
API, but what I was thinking about, is something that would allow the user to 
provide the complete "object" without having to wrap.  That would imply 
something different from Runnable, or perhaps an extension.

But, in the end, the Runnable is going to have to call out to say failed(), or 
failedWithException() anyway, so how is a layer between the Runnable and 
obtrudeValue() and obtrudeException() etc., really going to be more "helpful"?

Gregg

On 12/29/2012 7:44 AM, ?iktor ?lang wrote:
> This might just be solved by having a CompletableFutureTask (extends FutureTask)
> that has the obtrude method, and then have it extend CompletableFuture, which
> does not have the obtrude method?
>
> Cheers,
> ?
>
>
> On Sat, Dec 29, 2012 at 2:31 PM, ?iktor ?lang <viktor.klang at gmail.com
> <mailto:viktor.klang at gmail.com>> wrote:
>
>
>
>
>     On Sat, Dec 29, 2012 at 2:14 PM, Doug Lea <dl at cs.oswego.edu
>     <mailto:dl at cs.oswego.edu>> wrote:
>
>         On 12/29/12 07:57, ?iktor ?lang wrote:
>
>             I feel that if you need to have to ability to "recover" from a
>             catasttrophic
>             event, you return a different CompletableFuture from the one that is
>             going to
>             get completed with the result of the possibly catastrophic event,
>             and then you
>             are free to populate the actual return value independently from the
>             dangerous
>             operation result.
>
>
>         Just to restate this hopefully one last time: We have a consistent
>         story in j.u.c that we provide *some* way to recover from unexpected
>         events; sometimes ordinary cases, sometimes last resort options:
>
>
>     I still unconvinced that the "recover" method is insufficient, but am happy
>     to be proven otherwise.
>
>         ExecutorService.shutdownNow,
>
>
>     This is one of the reasons I never expose anything but Executor as the user
>     API, according to "The creator is the destroyer".
>     So I view ExecutorService as the producer-side of the API and Executor as
>     the consumer-side of the API.
>
>         Future.cancel,
>
>
>     juc.Future already sadly has the cancel method so there's not much to do
>     about that.
>     I fought hard to keep similar methods out of the Scala Future library as
>     this method make it impossible to reason about your code as you do not know
>     who has access to mess with your Future. (Especially in a non-closed-world
>     scenario where you get passed Futures and return Futures).
>
>         Phaser.forceTermination,
>
>
>     Never used, so cannot comment.
>
>         ForkJoinTask.reset,
>
>
>     Wasn't usable for me since, correct me if I'm wrong, it couldn't be called
>     within the task itself, leading to having to wrap tasks in tasks leading to
>     bad cache locality and excessive GCing.
>
>         and so on. All of them are racy. All of them
>         may fail to lead to full recovery. We hope you never need to use any
>         of them, but are sure that someday you will.
>
>
>      From my perspective it doesn't really matter to me whether there's a force
>     method or not, since I'd just create a thin integration and only ever expose
>     Scala Futures, but knowing what problems people run into I just wanted to
>     share my experiences in hope that the users of the CompletableFuture API
>     doesn't get it delivered with the barrel footwards. :-)
>
>     Have you considered splitting out things in a manner similar to
>     Promise/Future with one side having the write-operations and the other the
>     read-operations, so you as a user can choose what capabilities you expose?
>     Cheers,
>     ?
>
>
>
>         -Doug
>
>
>
>
>
>
>     --
>     *Viktor Klang*
>     /Director of Engineering/
>     /
>     /
>     Typesafe <http://www.typesafe.com/>- The software stack for applications
>     that scale
>     Twitter: @viktorklang
>
>
>
>
> --
> *Viktor Klang*
> /Director of Engineering/
> /
> /
> Typesafe <http://www.typesafe.com/>- The software stack for applications that scale
> Twitter: @viktorklang
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From stanimir at riflexo.com  Sat Dec 29 12:59:32 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Sat, 29 Dec 2012 19:59:32 +0200
Subject: [concurrency-interest] IRIW and sequential consistency
In-Reply-To: <50DEC296.70207@oracle.com>
References: <50DDE69A.4020101@oracle.com> <50DDECE1.8080409@cs.oswego.edu>
	<50DDF036.7000900@oracle.com>
	<CAHjP37HTWPRedsNtMadqos=yf38_NyD9gzpFM9f7Y_Q8AU2RXg@mail.gmail.com>
	<50DDFA11.1050602@oracle.com> <50DE050F.8070202@oracle.com>
	<50DE0863.3010402@oracle.com> <50DE0BE6.9020109@oracle.com>
	<50DEC296.70207@oracle.com>
Message-ID: <CAEJX8opxcWQNUfqJd2Kwa_QJO=JjKYGyvT8C6_zzgnYiVr6P7g@mail.gmail.com>

Aleksey,
Perhaps you can disable the hyperthreading and check the results again, it
could be that cache misses doesn't favor one of settings?

Stanimir


On Sat, Dec 29, 2012 at 12:14 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 12/29/2012 01:15 AM, Nathan Reynolds wrote:
> > I looked at the longest answer.  Do the results change if you get rid of
> > the yield?
>
> Disabling yielding will severely limit the throughput (because actors
> cannibalize the CPUs not letting the injector to push the new set of
> states to work on).
>
> > I join your suspicions that the delta is due to thread scheduling.
>
> Yeah, and the interesting part is that only this state is rare. All
> others are orders of magnitude more frequent. Go figure.
>
> -Aleksey.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121229/2335a530/attachment.html>

From dl at cs.oswego.edu  Sat Dec 29 13:22:08 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 29 Dec 2012 13:22:08 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DC94CE.5050407@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu>
Message-ID: <50DF34D0.3070309@cs.oswego.edu>

To avoid usage snags surrounding lambda arguments, CompletableFuture
methods are no longer overloaded, but instead indicate the expected
functional type. This turns out to be a bit clearer all around.
For example, the previously multiple forms of cf.then are now
   cf.thenApply(function)
   cf.thenAccept(block)
   cf.thenRun(runnable)
and so on.

See javadocs at:
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html

and source:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log

-Doug



From kasperni at gmail.com  Sat Dec 29 13:49:01 2012
From: kasperni at gmail.com (Kasper Nielsen)
Date: Sat, 29 Dec 2012 19:49:01 +0100
Subject: [concurrency-interest] DoubleAccumulator/LongAccumulator
Message-ID: <CAPs6153C7LLmLrShF=CtfgmMchHm69swVw-MM3bA+GE=0sx4pw@mail.gmail.com>

Hi,

I was wondering if it would make sense to use a more interface-like
approach for DoubleAccumulator and LongAccumulator.

For example, for maintaining min/max counts I think most of the time
AtomicLong would be a better "backend" than Striped64 because you will only
see write contention, at least for most usage, in the beginning as we get
closer and closer to minimum/maximum. Reading would just be a volatile
read. People could also trade scalability for memory overhead by allowing
multiple implementations. Off course, I could just use AtomicLong in these
places but it would be nice to be able to use the same API.

Also add some static methods such as
static LongAccumulator minAccumulator(/* Long.MAX_VALUE */);
I don't if it would be possible to create optimized methods at some later
point for some of these methods. For example, if future cpus supported new
concurrent operations.

Sorry if I'm turning this into a metrics library;)

- Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121229/0c10735b/attachment.html>

From dl at cs.oswego.edu  Sat Dec 29 14:40:27 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 29 Dec 2012 14:40:27 -0500
Subject: [concurrency-interest] DoubleAccumulator/LongAccumulator
In-Reply-To: <CAPs6153C7LLmLrShF=CtfgmMchHm69swVw-MM3bA+GE=0sx4pw@mail.gmail.com>
References: <CAPs6153C7LLmLrShF=CtfgmMchHm69swVw-MM3bA+GE=0sx4pw@mail.gmail.com>
Message-ID: <50DF472B.60500@cs.oswego.edu>

On 12/29/12 13:49, Kasper Nielsen wrote:
> For example, for maintaining min/max counts I think most of the time
> AtomicLong would be a better "backend" than Striped64 because you will only see
> write contention, at least for most usage, in the beginning as we get closer and
> closer to minimum/maximum. Reading would just be a volatile read. People could
> also trade scalability for memory overhead by allowing multiple implementations.
> Off course, I could just use AtomicLong in these places but it would be nice to
> be able to use the same API.

The (JDK8-only, not jsr166e) {Long,Double}Accumulator implementations
do take this into account by doing a recheck before attempted CAS.
This could slow them down just-barely-noticeably in some cases. But
because many applications that don't specifically use LongAdder
will probably be more max/min-like than add-like, this seems like the best
way to reduce API sprawl without giving up much if any performance.


>
> Also add some static methods such as
> static LongAccumulator minAccumulator(/* Long.MAX_VALUE */);

Yes; coming soon. Do you have any suggestions besides min and max?

-Doug


From zhong.j.yu at gmail.com  Sat Dec 29 15:20:00 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sat, 29 Dec 2012 14:20:00 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DF34D0.3070309@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
Message-ID: <CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>

About exception propagation: so the general principle is that any
exception anywhere on a chain will immediately cause the last node to
complete exceptionally; the exception will not be intercepted/handled
by intermediaries; unless the exceptionally() method is used. Is that
correct?

Zhong Yu


On Sat, Dec 29, 2012 at 12:22 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> To avoid usage snags surrounding lambda arguments, CompletableFuture
> methods are no longer overloaded, but instead indicate the expected
> functional type. This turns out to be a bit clearer all around.
> For example, the previously multiple forms of cf.then are now
>   cf.thenApply(function)
>   cf.thenAccept(block)
>   cf.thenRun(runnable)
> and so on.
>
>
> See javadocs at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
>
> and source:
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From dl at cs.oswego.edu  Sat Dec 29 15:50:03 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 29 Dec 2012 15:50:03 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
Message-ID: <50DF577B.9040807@cs.oswego.edu>

On 12/29/12 15:20, Zhong Yu wrote:
> About exception propagation: so the general principle is that any
> exception anywhere on a chain will immediately cause the last node to
> complete exceptionally; the exception will not be intercepted/handled
> by intermediaries; unless the exceptionally() method is used. Is that
> correct?

That's one way to phrase it.
If you have a CF dependent on another that failed and was not handled,
then it must fail as well. (An alternative policy/design would be to
allow these dependent CFs to remain uncompleted for eternity, but this
would not be a very useful choice.)

-Doug


From zhong.j.yu at gmail.com  Sun Dec 30 00:40:17 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sat, 29 Dec 2012 23:40:17 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DF577B.9040807@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
Message-ID: <CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>

On Sat, Dec 29, 2012 at 2:50 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 12/29/12 15:20, Zhong Yu wrote:
>>
>> About exception propagation: so the general principle is that any
>> exception anywhere on a chain will immediately cause the last node to
>> complete exceptionally; the exception will not be intercepted/handled
>> by intermediaries; unless the exceptionally() method is used. Is that
>> correct?
>
>
> That's one way to phrase it.
> If you have a CF dependent on another that failed and was not handled,
> then it must fail as well. (An alternative policy/design would be to
> allow these dependent CFs to remain uncompleted for eternity, but this
> would not be a very useful choice.)

Apparently missing is a transformation that is triggered by any
completion, regardless if the completion is normal or exceptional. Is
that considered not very useful?

ComputableFuture seems to treat exceptions as something really
"exceptional", but many applications have exceptions that are more
like alternative results instead of devastating events. So
ComputableFuture isn't appropriate for these applications?

Zhong Yu

From jed at atlassian.com  Sun Dec 30 01:43:02 2012
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Sun, 30 Dec 2012 17:43:02 +1100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
Message-ID: <CAKh+yi8iyROCLvqmNyTvXMfO-pANteunFvuqw_rZu7wy1bWiug@mail.gmail.com>

There is the form that accepts a Function of something extending
Throwable and returning the contained T type. If you use something
like a disjoint union* type you can capture the exception as the
failure side of that and the result type on the success side.

cheers,
jed.

* For instance see our Either type:
https://bitbucket.org/atlassian/fugue/src/master/src/main/java/com/atlassian/fugue/Either.java

On 30 December 2012 16:40, Zhong Yu <zhong.j.yu at gmail.com> wrote:
> On Sat, Dec 29, 2012 at 2:50 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>> On 12/29/12 15:20, Zhong Yu wrote:
>>>
>>> About exception propagation: so the general principle is that any
>>> exception anywhere on a chain will immediately cause the last node to
>>> complete exceptionally; the exception will not be intercepted/handled
>>> by intermediaries; unless the exceptionally() method is used. Is that
>>> correct?
>>
>>
>> That's one way to phrase it.
>> If you have a CF dependent on another that failed and was not handled,
>> then it must fail as well. (An alternative policy/design would be to
>> allow these dependent CFs to remain uncompleted for eternity, but this
>> would not be a very useful choice.)
>
> Apparently missing is a transformation that is triggered by any
> completion, regardless if the completion is normal or exceptional. Is
> that considered not very useful?
>
> ComputableFuture seems to treat exceptions as something really
> "exceptional", but many applications have exceptions that are more
> like alternative results instead of devastating events. So
> ComputableFuture isn't appropriate for these applications?
>
> Zhong Yu
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From dl at cs.oswego.edu  Sun Dec 30 09:48:01 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 30 Dec 2012 09:48:01 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
Message-ID: <50E05421.5060902@cs.oswego.edu>

On 12/30/12 00:40, Zhong Yu wrote:

> Apparently missing is a transformation that is triggered by any
> completion, regardless if the completion is normal or exceptional. Is
> that considered not very useful?

Since we are proliferating methods based on different function
shapes/types to meet known usages anyway :-), I think it is
reasonable to add this.
As well as one suggested by Jed that simplifies composition
with other CompletableFuture-returning functions.
Pasted below. Full javadocs/source for these (plus another
naming regularization pass) are in the usual places:

http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log


     /**
      * Creates and returns a CompletableFuture that is completed with
      * the result of the given function of the result and exception of
      * this CompletableFuture's completion when it completes.  The
      * given function is invoked with the result (or {@code null} if
      * none) and the exception (or {@code null} if none) of this
      * CompletableFuture when complete.
      *
      * @param fn the function to use to compute the value of the
      * returned CompletableFuture

      * @return the new CompletableFuture
      */
     public <U> CompletableFuture<U> handle(BiFunction<? super T, Throwable, ? 
extends U> fn)

     /**
      * Returns a CompletableFuture equal or equivalent to that
      * produced by the given function of the result of this
      * CompletableFuture when completed.  If this CompletableFuture
      * completes exceptionally, then the returned CompletableFuture
      * also does so, with a RuntimeException having this exception as
      * its cause.
      *
      * @param fn the function returning a new CompletableFuture.
      * @return the CompletableFuture, that {@code isDone()} upon
      * return if completed by the given function, or an exception
      * occurs.
      */
     public <U> CompletableFuture<U> thenCompose(Function<? super T,
                                                 CompletableFuture<U>> fn)

From dl at cs.oswego.edu  Sun Dec 30 10:16:44 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 30 Dec 2012 10:16:44 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50DF02E1.7030808@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DD765D.2000502@oracle.com>
	<50DD8983.5020404@cs.oswego.edu> <50DD8B80.3090304@oracle.com>
	<CANPzfU_ow5LMX+dutx8GrrUNzHzG9m3R7nvj2eCyftyYWDZC8A@mail.gmail.com>
	<CANPzfU_b2JDCC=NyDiR_tP=ObiRx8s-mTv_a38Esi=Evh8Ko+g@mail.gmail.com>
	<50DDA72C.7050102@oracle.com> <50DDA843.7010106@cs.oswego.edu>
	<50DDAD7D.6060308@oracle.com> <50DDB124.7020909@cs.oswego.edu>
	<CANPzfU9Cwcoe-tOAwizOt0dxzdz5mu_HbZGK__J_aa-mjeOV2Q@mail.gmail.com>
	<50DDE2A4.2090903@cs.oswego.edu> <50DDEB74.8090306@oracle.com>
	<CAKh+yi9H8-CH-2js2XR_ksMfyZ8pJUSXHNPC_eKQYfX6iY=vkg@mail.gmail.com>
	<CANPzfU9pwHOgG6v6WLATPFFT3Ex+nMBqU4v5UHST7ZPZ397RJw@mail.gmail.com>
	<50DEECC0.9080103@cs.oswego.edu>
	<CANPzfU9nMyfwDCjAgi=DsiWWnXWuB30_TdYhkP92qbt-8CViqA@mail.gmail.com>
	<CANPzfU_-7HaOafBtyHNAOBOvYywojAC6y1MM=3zcQnjygvWX8w@mail.gmail.com>
	<50DF02E1.7030808@cs.oswego.edu>
Message-ID: <50E05ADC.6040306@cs.oswego.edu>

To expand a bit on this, also recapping some off-list discussions:

On 12/29/12 09:49, Doug Lea wrote:
> The main motivation for CompletableFuture is
> to overcome the JDK5 decision to give up on any form of
> non-task-based settable Future because we couldn't agree on API.
> Which has led to all sorts of incompatible third-party versions,
> many of which abuse FutureTask by bypassing the Task aspects.
> The only good thing about waiting so many years is that the existence
> of lambdas makes the basic shape of this API less controversial, as
> witnessed by the similarities to other recent similar APIs people
> have mentioned. The main goal for j.u.c version is enough generality
> to serve these needs, along with the best possible performance
> and reliability.
>

In other words, we (j.u.c) are not now in a position to
dictate a common interface for all SettableFuture, FutureValue,
Promise, ListenableFuture, etc like APIs.
And as we've seen, different audiences want/need different
subsets of this API exposed as interfaces for
their usages, and are in any case unlikely to want change
all their existing interfaces. However, what we can do is
provide a common underlying implementation that is as
fast, scalable, space-conserving, carefully-specified, and
reliable as possible.
It should then be easy and attractive for others creating
or reworking higher-level APIs to relay all functionality
to the CompletableFuture implementation.
I was initially tempted to declare this as a "final"
class, that would mandate only this kind of usage, but
simple extensions remain possible, so "final" would be overkill.

-Doug


From zhong.j.yu at gmail.com  Sun Dec 30 10:35:22 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sun, 30 Dec 2012 09:35:22 -0600
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E05421.5060902@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
Message-ID: <CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>

On Sun, Dec 30, 2012 at 8:48 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 12/30/12 00:40, Zhong Yu wrote:
>
>> Apparently missing is a transformation that is triggered by any
>> completion, regardless if the completion is normal or exceptional. Is
>> that considered not very useful?
>
>
> Since we are proliferating methods based on different function
> shapes/types to meet known usages anyway :-), I think it is
> reasonable to add this.
> As well as one suggested by Jed that simplifies composition
> with other CompletableFuture-returning functions.

Great.

> Pasted below. Full javadocs/source for these (plus another
> naming regularization pass) are in the usual places:
>
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>
>
>     /**
>      * Creates and returns a CompletableFuture that is completed with
>      * the result of the given function of the result and exception of
>      * this CompletableFuture's completion when it completes.  The
>      * given function is invoked with the result (or {@code null} if
>      * none) and the exception (or {@code null} if none) of this
>      * CompletableFuture when complete.
>      *
>      * @param fn the function to use to compute the value of the
>      * returned CompletableFuture
>
>      * @return the new CompletableFuture
>      */
>     public <U> CompletableFuture<U> handle(BiFunction<? super T, Throwable,
> ? extends U> fn)

Since null can be a valid result, but not a valid exception, fn should
test the null-ness of the exception parameter to learn the completion
mode. In my taste I'd make the exception the 1st parameter.

    (exception, result) -> { if(exception!=null) { ... } else { ... } }

Also I wish `fn` can throw checked exception that is delivered to the
destination future unwrapped. If we smuggle a check exception in a
RuntimeException, it's a bit of work to uncover it later without
ambiguity; I wished there's a dedicated java.lang.SneakedException for
the sole purpose of smuggling checked exceptions.

Or we can have a new CF-specific functional interface that throws
Exception. I feel that introducing special purpose functional
interfaces is probably not a big deal in java 8, since user code
rarely mentions them by name.

>
>     /**
>      * Returns a CompletableFuture equal or equivalent to that
>      * produced by the given function of the result of this
>      * CompletableFuture when completed.  If this CompletableFuture
>      * completes exceptionally, then the returned CompletableFuture
>      * also does so, with a RuntimeException having this exception as
>      * its cause.
>      *
>      * @param fn the function returning a new CompletableFuture.
>      * @return the CompletableFuture, that {@code isDone()} upon
>      * return if completed by the given function, or an exception
>      * occurs.
>      */
>     public <U> CompletableFuture<U> thenCompose(Function<? super T,
>                                                 CompletableFuture<U>> fn)

there is no

    thenCompose(BiFunction<T, Throwable, CF<U>> fn)

? too much?

Zhong Yu

From dl at cs.oswego.edu  Sun Dec 30 14:30:49 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 30 Dec 2012 14:30:49 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
Message-ID: <50E09669.9000209@cs.oswego.edu>

On 12/30/12 10:35, Zhong Yu wrote:

> Also I wish `fn` can throw checked exception that is delivered to the
> destination future unwrapped.

Sorry, mixing checked exceptions and lambdas is too nightmarish.
It is a sure bet that as lambdas become more commonly used, checked
exceptions will become less commonly used.

> If we smuggle a check exception in a
> RuntimeException, it's a bit of work to uncover it later without
> ambiguity; I wished there's a dedicated java.lang.SneakedException for
> the sole purpose of smuggling checked exceptions.

It might be a good idea to institute a JDK-wide convention
about a specific RuntimeException to use in these situations.
I'll explore this. For now though, just RuntimeException
itself is the only reasonable choice.

-Doug


From viktor.klang at gmail.com  Sun Dec 30 15:43:16 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sun, 30 Dec 2012 21:43:16 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E09669.9000209@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu>
Message-ID: <CANPzfU9dC6_9hzQORgu4zbiqzA9QPq7DQfgwdMcWhjwRvfYVFw@mail.gmail.com>

If a common bridge is the end goal then I suggest:

interface CompletableFuture<T> {
  public void whenResult(T result);
  public void whenException(Throwable exception);
}

//ComposableFuture is what you now call CompletableFuture.
class ComposableFuture<T> implements CompletableFuture<T>, Future<T> {

  Here you want the following method shapes:

  (T -> U) -> ComposableFuture<U> // More commonly known as "map"
  (T -> ComposableFuture<U>) -> ComposableFuture<U> // More commonly known
as "bind" or "flatMap"
  (Throwable -> U) -> ComposableFuture<U> // More commonly known as
"recover"
  (Throwable -> ComposableFuture<U>) -> ComposableFuture<U> // More
commonly known as "recoverWith"

  ...
}

Why? Because no other methods than whenResult & whenException are needed,
also, j.u.c.Future is, in my humble opinion, a very very bad interface, and
I'd vote for not involving juv.Future into CompletableFuture, but instead
add that to ComposableFuture.

Cheers,
?


On Sun, Dec 30, 2012 at 8:30 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/30/12 10:35, Zhong Yu wrote:
>
>  Also I wish `fn` can throw checked exception that is delivered to the
>> destination future unwrapped.
>>
>
> Sorry, mixing checked exceptions and lambdas is too nightmarish.
> It is a sure bet that as lambdas become more commonly used, checked
> exceptions will become less commonly used.
>
>
>  If we smuggle a check exception in a
>> RuntimeException, it's a bit of work to uncover it later without
>> ambiguity; I wished there's a dedicated java.lang.SneakedException for
>> the sole purpose of smuggling checked exceptions.
>>
>
> It might be a good idea to institute a JDK-wide convention
> about a specific RuntimeException to use in these situations.
> I'll explore this. For now though, just RuntimeException
> itself is the only reasonable choice.
>
> -Doug
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121230/1dd17d84/attachment.html>

From viktor.klang at gmail.com  Sun Dec 30 15:49:28 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sun, 30 Dec 2012 21:49:28 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU9dC6_9hzQORgu4zbiqzA9QPq7DQfgwdMcWhjwRvfYVFw@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu>
	<CANPzfU9dC6_9hzQORgu4zbiqzA9QPq7DQfgwdMcWhjwRvfYVFw@mail.gmail.com>
Message-ID: <CANPzfU_n0=zBQAM__+9_W5uBf_WAN8zMJxYq0FCU4w9taBRSOg@mail.gmail.com>

Since essentially all methods on ComposableFuture can be implemented in
terms of whenResult and whenException, this enables interaction with
essentially all existing async Futures out there, and then the
ComposableFuture class is the "end-user" level API.

Cheers,
?


On Sun, Dec 30, 2012 at 9:43 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:

> If a common bridge is the end goal then I suggest:
>
> interface CompletableFuture<T> {
>   public void whenResult(T result);
>   public void whenException(Throwable exception);
> }
>
> //ComposableFuture is what you now call CompletableFuture.
> class ComposableFuture<T> implements CompletableFuture<T>, Future<T> {
>
>   Here you want the following method shapes:
>
>   (T -> U) -> ComposableFuture<U> // More commonly known as "map"
>   (T -> ComposableFuture<U>) -> ComposableFuture<U> // More commonly known
> as "bind" or "flatMap"
>   (Throwable -> U) -> ComposableFuture<U> // More commonly known as
> "recover"
>   (Throwable -> ComposableFuture<U>) -> ComposableFuture<U> // More
> commonly known as "recoverWith"
>
>   ...
> }
>
> Why? Because no other methods than whenResult & whenException are needed,
> also, j.u.c.Future is, in my humble opinion, a very very bad interface, and
> I'd vote for not involving juv.Future into CompletableFuture, but instead
> add that to ComposableFuture.
>
> Cheers,
> ?
>
>
> On Sun, Dec 30, 2012 at 8:30 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 12/30/12 10:35, Zhong Yu wrote:
>>
>>  Also I wish `fn` can throw checked exception that is delivered to the
>>> destination future unwrapped.
>>>
>>
>> Sorry, mixing checked exceptions and lambdas is too nightmarish.
>> It is a sure bet that as lambdas become more commonly used, checked
>> exceptions will become less commonly used.
>>
>>
>>  If we smuggle a check exception in a
>>> RuntimeException, it's a bit of work to uncover it later without
>>> ambiguity; I wished there's a dedicated java.lang.SneakedException for
>>> the sole purpose of smuggling checked exceptions.
>>>
>>
>> It might be a good idea to institute a JDK-wide convention
>> about a specific RuntimeException to use in these situations.
>> I'll explore this. For now though, just RuntimeException
>> itself is the only reasonable choice.
>>
>> -Doug
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> *
> *
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @viktorklang
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121230/814091c8/attachment.html>

From kasperni at gmail.com  Sun Dec 30 16:51:03 2012
From: kasperni at gmail.com (Kasper Nielsen)
Date: Sun, 30 Dec 2012 22:51:03 +0100
Subject: [concurrency-interest] A simple predicate based synchronizer
Message-ID: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>

Hi,

Just an idea for a real simple general purpose synchronizer that I think
would make a nice addition to the once already in JUC. While Phaser is
really useful
I think it is too complex to ever be widely used. My intuition is based on
a google search for "import java.util.concurrent.Phaser" filetype:java

The synchronizer is based on supplying a Predicate in the constructor of
the synchronizer. And then having await methods
that will wait until the supplied predicate accepts its argument.

public class SimpleSynchronizer<T> {
    //Trieber stack to keep track of waiting threads. keeps t, next, thread
    SimpleSynchronizer(Predicate<T> checker)
    void await(T t) throws InterruptedException;
    boolean await(T t, long time, TimeUnit unit) throws
InterruptedException;
    void checkAll(); //checks all waiting threads to see if they should be
released
    //probably some more methods
}

A simple example would be one that just awaited on some state to be reached.

SimpleSynchronized<Void> ss=new SimpleSynchronized(-> service.isStarted());

ss.await(null) <- awaits for the service to start


A better example would be one where we want to wait on a specific
(monotonically increasing) state
SimpleSynchronized<Void> ss=new SimpleSynchronized( s -> s >=
service.getState());

to wait on a particular state you would call:
ss.await(4); // or
ss.await(9, 10, TimeUnit.SECONDS);

Whenever the services state changed you would call:
service.state = newState;
ss.recheckAll();

An alternative definition of the synchronizer would be one that maintained
some kind of state that would also be returned to the waiting threads once
they where released

public interface SimpleStatebasedSynchronizer<S, T> {
    SimpleSynchronizer(BinaryPredicate<S, T> checker, S initialState)
    S await(T t) throws InterruptedException;
    S await(T t, long time, TimeUnit unit) throws InterruptedException,
TimeoutException;
    S getState(); //returns the current state
    void setState(S newState); //checks All waiting threads to see if they
should be released
}

- Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121230/f6e73b71/attachment-0001.html>

From dl at cs.oswego.edu  Sun Dec 30 18:17:55 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 30 Dec 2012 18:17:55 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E09669.9000209@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu>
Message-ID: <50E0CBA3.5000304@cs.oswego.edu>

On 12/30/12 14:30, Doug Lea wrote:
> Sorry, mixing checked exceptions and lambdas is too nightmarish.

One reason here is that dependent chains can become
very long, so we'd like to automate exception/completion
propagation rather than force user code to selectively catch/rethrow.
But this can be done (and is done in current update) without
forcing so many mismatches with Future.get/timed-get API. A new
(unchecked) j.u.c.CompletionException is now used in
the same way as ExecutionException, but is internally
collapsed upon propagation.
For example if, in  ...thenRun(a).thenRun(b).thenRun(c);
if the CF associated with a completes exceptionally, then the
CF for c has a CompletionException holding a's exception
as cause rather than the chain of c with cause b with cause a.

Most usages of CFs that use functional forms will probably
want to use the added getValue() method, that reports these
as CompletionExceptions, instead of get(), that must throw
them as ExecutionExceptions.

-Doug


From dl at cs.oswego.edu  Sun Dec 30 18:37:47 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 30 Dec 2012 18:37:47 -0500
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
References: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
Message-ID: <50E0D04B.4030900@cs.oswego.edu>

On 12/30/12 16:51, Kasper Nielsen wrote:
> Hi,
>
> Just an idea for a real simple general purpose synchronizer that I think
> would make a nice addition to the once already in JUC. While Phaser is really useful
> I think it is too complex to ever be widely used. My intuition is based on
> a google search for "import java.util.concurrent.Phaser" filetype:java
>
> The synchronizer is based on supplying a Predicate in the constructor of the
> synchronizer. And then having await methods
> that will wait until the supplied predicate accepts its argument.

I vaguely remember writing this once :-) Or a minor variant anyway:
http://gee.cs.oswego.edu/dl/classes/EDU/oswego/cs/dl/util/concurrent/WaitableBoolean.html

It never made it past the triage for inclusion in j.u.c back in JDK5.
But maybe worth reconsideration given its lambda-ish API that
was once considered a liability...

-Doug



From zhong.j.yu at gmail.com  Sun Dec 30 18:46:22 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sun, 30 Dec 2012 17:46:22 -0600
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
References: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
Message-ID: <CACuKZqFcnNr5vTibgJ+NS7ysBCDfCsV+Vo7d+=2eU1WurWbcGA@mail.gmail.com>

How about moving predicate to await() site

    class Synchronizer<T>
        void signal(T value);
        void await( Predicate<T> filter );

    Synchronizer<Integer> s = new Synchronizer<>();

    ss.await( x-> (x>2) );

    ss.signal( 3 );


On Sun, Dec 30, 2012 at 3:51 PM, Kasper Nielsen <kasperni at gmail.com> wrote:
> Hi,
>
> Just an idea for a real simple general purpose synchronizer that I think
> would make a nice addition to the once already in JUC. While Phaser is
> really useful
> I think it is too complex to ever be widely used. My intuition is based on a
> google search for "import java.util.concurrent.Phaser" filetype:java
>
> The synchronizer is based on supplying a Predicate in the constructor of the
> synchronizer. And then having await methods
> that will wait until the supplied predicate accepts its argument.
>
> public class SimpleSynchronizer<T> {
>     //Trieber stack to keep track of waiting threads. keeps t, next, thread
>     SimpleSynchronizer(Predicate<T> checker)
>     void await(T t) throws InterruptedException;
>     boolean await(T t, long time, TimeUnit unit) throws
> InterruptedException;
>     void checkAll(); //checks all waiting threads to see if they should be
> released
>     //probably some more methods
> }
>
> A simple example would be one that just awaited on some state to be reached.
>
> SimpleSynchronized<Void> ss=new SimpleSynchronized(-> service.isStarted());
>
> ss.await(null) <- awaits for the service to start
>
>
> A better example would be one where we want to wait on a specific
> (monotonically increasing) state
> SimpleSynchronized<Void> ss=new SimpleSynchronized( s -> s >=
> service.getState());
>
> to wait on a particular state you would call:
> ss.await(4); // or
> ss.await(9, 10, TimeUnit.SECONDS);
>
> Whenever the services state changed you would call:
> service.state = newState;
> ss.recheckAll();
>
> An alternative definition of the synchronizer would be one that maintained
> some kind of state that would also be returned to the waiting threads once
> they where released
>
> public interface SimpleStatebasedSynchronizer<S, T> {
>     SimpleSynchronizer(BinaryPredicate<S, T> checker, S initialState)
>     S await(T t) throws InterruptedException;
>     S await(T t, long time, TimeUnit unit) throws InterruptedException,
> TimeoutException;
>     S getState(); //returns the current state
>     void setState(S newState); //checks All waiting threads to see if they
> should be released
> }
>
> - Kasper
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From jim.andreou at gmail.com  Sun Dec 30 19:00:20 2012
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Sun, 30 Dec 2012 16:00:20 -0800
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
References: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
Message-ID: <CADJdpByGQ99sDONB8Z++3ON++3DntgLbjTQa8oJp9WB-N04fbw@mail.gmail.com>

http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/util/concurrent/Monitor.html


On Sun, Dec 30, 2012 at 1:51 PM, Kasper Nielsen <kasperni at gmail.com> wrote:

> Hi,
>
> Just an idea for a real simple general purpose synchronizer that I think
> would make a nice addition to the once already in JUC. While Phaser is
> really useful
> I think it is too complex to ever be widely used. My intuition is based on
> a google search for "import java.util.concurrent.Phaser" filetype:java
>
> The synchronizer is based on supplying a Predicate in the constructor of
> the synchronizer. And then having await methods
> that will wait until the supplied predicate accepts its argument.
>
> public class SimpleSynchronizer<T> {
>     //Trieber stack to keep track of waiting threads. keeps t, next, thread
>     SimpleSynchronizer(Predicate<T> checker)
>     void await(T t) throws InterruptedException;
>     boolean await(T t, long time, TimeUnit unit) throws
> InterruptedException;
>     void checkAll(); //checks all waiting threads to see if they should be
> released
>     //probably some more methods
> }
>
> A simple example would be one that just awaited on some state to be
> reached.
>
> SimpleSynchronized<Void> ss=new SimpleSynchronized(-> service.isStarted());
>
> ss.await(null) <- awaits for the service to start
>
>
> A better example would be one where we want to wait on a specific
> (monotonically increasing) state
> SimpleSynchronized<Void> ss=new SimpleSynchronized( s -> s >=
> service.getState());
>
> to wait on a particular state you would call:
> ss.await(4); // or
> ss.await(9, 10, TimeUnit.SECONDS);
>
> Whenever the services state changed you would call:
> service.state = newState;
> ss.recheckAll();
>
> An alternative definition of the synchronizer would be one that maintained
> some kind of state that would also be returned to the waiting threads once
> they where released
>
> public interface SimpleStatebasedSynchronizer<S, T> {
>     SimpleSynchronizer(BinaryPredicate<S, T> checker, S initialState)
>     S await(T t) throws InterruptedException;
>     S await(T t, long time, TimeUnit unit) throws InterruptedException,
> TimeoutException;
>     S getState(); //returns the current state
>     void setState(S newState); //checks All waiting threads to see if they
> should be released
> }
>
> - Kasper
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121230/c6774e18/attachment.html>

From kasperni at gmail.com  Sun Dec 30 19:24:56 2012
From: kasperni at gmail.com (Kasper Nielsen)
Date: Mon, 31 Dec 2012 01:24:56 +0100
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CADJdpByGQ99sDONB8Z++3ON++3DntgLbjTQa8oJp9WB-N04fbw@mail.gmail.com>
References: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
	<CADJdpByGQ99sDONB8Z++3ON++3DntgLbjTQa8oJp9WB-N04fbw@mail.gmail.com>
Message-ID: <CAPs6153ha4E1ngeAtn+DK0N_yr=vaGZN=+OH0XJRN0K==deVJA@mail.gmail.com>

A Monitor is about as applicable as a Lock here. I "just" need to
synchronize some threads. No need for critical sections it would only slow
down things.

On Mon, Dec 31, 2012 at 1:00 AM, Dimitris Andreou <jim.andreou at gmail.com>wrote:

>
> http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/util/concurrent/Monitor.html
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/7a2a5ab0/attachment.html>

From davidcholmes at aapt.net.au  Sun Dec 30 22:04:44 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 31 Dec 2012 13:04:44 +1000
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEDEJJAA.davidcholmes@aapt.net.au>

Separating the state upon which you need to await from the mechanics of
waiting and notification is generally problematic because you need to
provide thread-safe mutation of the state and/or you need "level-triggers"
rather than "edge-triggers" where the state upon which you were waiting is
known not to change until you have completed your action - for which you
need Lock+Condition. General edge-triggers are normally accommodated through
the existing synchronizers: semaphores, latches, barriers, gates. Your
"await start" example is just a latch.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Kasper
Nielsen
  Sent: Monday, 31 December 2012 7:51 AM
  To: Concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] A simple predicate based synchronizer



  Hi,


  Just an idea for a real simple general purpose synchronizer that I think
  would make a nice addition to the once already in JUC. While Phaser is
really useful
  I think it is too complex to ever be widely used. My intuition is based on
a google search for "import java.util.concurrent.Phaser" filetype:java


  The synchronizer is based on supplying a Predicate in the constructor of
the synchronizer. And then having await methods

  that will wait until the supplied predicate accepts its argument.


  public class SimpleSynchronizer<T> {
      //Trieber stack to keep track of waiting threads. keeps t, next,
thread

      SimpleSynchronizer(Predicate<T> checker)

      void await(T t) throws InterruptedException;

      boolean await(T t, long time, TimeUnit unit) throws
InterruptedException;

      void checkAll(); //checks all waiting threads to see if they should be
released

      //probably some more methods

  }



  A simple example would be one that just awaited on some state to be
reached.


  SimpleSynchronized<Void> ss=new SimpleSynchronized(->
service.isStarted());


  ss.await(null) <- awaits for the service to start




  A better example would be one where we want to wait on a specific
(monotonically increasing) state

  SimpleSynchronized<Void> ss=new SimpleSynchronized( s -> s >=
service.getState());



  to wait on a particular state you would call:
  ss.await(4); // or
  ss.await(9, 10, TimeUnit.SECONDS);


  Whenever the services state changed you would call:
  service.state = newState;
  ss.recheckAll();


  An alternative definition of the synchronizer would be one that maintained
some kind of state that would also be returned to the waiting threads once
they where released


  public interface SimpleStatebasedSynchronizer<S, T> {
      SimpleSynchronizer(BinaryPredicate<S, T> checker, S initialState)

      S await(T t) throws InterruptedException;

      S await(T t, long time, TimeUnit unit) throws InterruptedException,
TimeoutException;

      S getState(); //returns the current state

      void setState(S newState); //checks All waiting threads to see if they
should be released

  }



  - Kasper

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/0efa1ff9/attachment-0001.html>

From viktor.klang at gmail.com  Mon Dec 31 03:57:04 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 31 Dec 2012 09:57:04 +0100
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <50E0CBA3.5000304@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu> <50E0CBA3.5000304@cs.oswego.edu>
Message-ID: <CANPzfU_-gruW=4a_+8_AeaKCRgOu0CsiRGjJ=anAx6mkPps1qA@mail.gmail.com>

On Dec 31, 2012 12:20 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:
>
> On 12/30/12 14:30, Doug Lea wrote:
>>
>> Sorry, mixing checked exceptions and lambdas is too nightmarish.
>
>
> One reason here is that dependent chains can become
> very long, so we'd like to automate exception/completion
> propagation rather than force user code to selectively catch/rethrow.
> But this can be done (and is done in current update) without
> forcing so many mismatches with Future.get/timed-get API. A new
> (unchecked) j.u.c.CompletionException is now used in
> the same way as ExecutionException, but is internally
> collapsed upon propagation.
> For example if, in  ...thenRun(a).thenRun(b).thenRun(c);
> if the CF associated with a completes exceptionally, then the
> CF for c has a CompletionException holding a's exception
> as cause rather than the chain of c with cause b with cause a.

Sounds great! How are non-recoverable Errors handled? (VMError, etc)?

>
> Most usages of CFs that use functional forms will probably
> want to use the added getValue() method, that reports these
> as CompletionExceptions, instead of get(), that must throw
> them as ExecutionExceptions.

That's a very important distinction.

Cheers,
V
>
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/77c55e34/attachment.html>

From kasperni at gmail.com  Mon Dec 31 04:25:57 2012
From: kasperni at gmail.com (Kasper Nielsen)
Date: Mon, 31 Dec 2012 10:25:57 +0100
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEDEJJAA.davidcholmes@aapt.net.au>
References: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDEJJAA.davidcholmes@aapt.net.au>
Message-ID: <CAPs6152Rfc+O3Ge4LoNJ0wx7Aw0DJ2_LUG6=RFTXkPNGEo+UOQ@mail.gmail.com>

On Mon, Dec 31, 2012 at 4:04 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> General edge-triggers are normally accommodated through the existing
> synchronizers: semaphores, latches, barriers, gates. Your "await start"
> example is just a latch.
>
I don't really see you point? Everything can be build using
AbstractQueuedSynchronizer as
well. But that doesn't necessarily make it easy for end users.

One area where I really miss something like this is when writing unit
tests. Being able to do something like this
T1: signal("TestDone")
T2: await(s -> s.equals("TestDone"), 10, TimeUnit.SECONDS) or even
T2: awaitEqualsTo("TestDone", 10, TimeUnit.SECONDS)
would make my life much easier.

While I could use CountdownLatch in places like this. I usually end up with
a couple of them whenever multiple states are involved. Which make it much
harder to read at a later time.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/d73ac238/attachment.html>

From holger.hoffstaette at googlemail.com  Mon Dec 31 07:03:51 2012
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Mon, 31 Dec 2012 13:03:51 +0100
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs6152Rfc+O3Ge4LoNJ0wx7Aw0DJ2_LUG6=RFTXkPNGEo+UOQ@mail.gmail.com>
References: <CAPs6153W8GCiD224ZQQ5zdVuHL+DbZQ8FwjadBUFzW0Y=ZoQ3w@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEDEJJAA.davidcholmes@aapt.net.au>
	<CAPs6152Rfc+O3Ge4LoNJ0wx7Aw0DJ2_LUG6=RFTXkPNGEo+UOQ@mail.gmail.com>
Message-ID: <50E17F27.4080607@googlemail.com>

On 31.12.2012 10:25, Kasper Nielsen wrote:
> One area where I really miss something like this is when writing unit
> tests. Being able to do something like this
> T1: signal("TestDone")
> T2: await(s -> s.equals("TestDone"), 10, TimeUnit.SECONDS) or even
> T2: awaitEqualsTo("TestDone", 10, TimeUnit.SECONDS)
> would make my life much easier. 

http://code.google.com/p/awaitility/

Also less intrusive & coupled than your example.

-h


From aaron.grunthal at infinite-source.de  Mon Dec 31 08:17:02 2012
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Mon, 31 Dec 2012 14:17:02 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a non-volatile
	field
Message-ID: <50E1904E.9030406@infinite-source.de>

I'm investigating potential performance improvements to JRuby and 
noticed that they store instance variables in an array where the field 
holding the array itself is volatile. Most of the time this reference 
only needs to be read and doesn't actually require volatile semantics 
for since the array access itself is non-volatile, too. The only reason 
it is marked as volatile is that the array needs to be grown on write 
access via a CAS operation through an atomic field updater.

Volatile reads themselves don't seem to be all that expensive, the main 
performance penalty is that they cause is that they prevent loop 
invariant code motion. Thus i'm trying to eliminate most volatile reads 
to enable LICM.

So I'm considering several options here

A) use Unsafe.getObject() for a non-volatile read.

This actually doesn't seem to work according to some microbenchmarks 
I've tried. The read does not get hoisted. My question here would be: Is 
this intentional/a limitation of hotspot or am I benchmarking wrong?


B) duplicate the field, one volatile, one non-volatile

This would increase the size of every single object in the jruby runtime.


C) make the field non-volatile and use Unsafe to do the CAS/volatile 
reads/writes when necessary and use direct field access for hoistable 
reads otherwise, basically emulating the atomic field updater on a 
non-volatile field.

Is this a safe thing to do? Is there a reason why we can't use the field 
updaters on non-volatile fields or is it just some 
don't-shoot-yourself-in-the-foot thing?



Oh yeah. ConcurrentHashMaps suffer from the same issues. They have no 
way to support loop-hoistable lazy gets. Even if they used Unsafe 
operations to do so it still wouldn't get hoisted (see A)


- Aaron

From stanimir at riflexo.com  Mon Dec 31 08:39:41 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 31 Dec 2012 15:39:41 +0200
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1904E.9030406@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de>
Message-ID: <CAEJX8oq-gVqbM8NsRO1J9L0V7oU56KVm408TyJfF305MZYdHGg@mail.gmail.com>

Using a CAS and not needing a volatile read smells like a race.

If you wish to skip the volatile access, assign to a local variable prior
to the loop.
About point A)
According Aleksey Shipilev on naked reads: *Unsafe intrinsics
gettranslated before register allocation, and so this is the usual
operation,
subject to the same rules as usual loads. Unless there is explicit barrier
forcing to re-read the field, there is no guarantees it acts the way I
need.
*So getObject should act like a normal load, i.e. what you need.

However, overall volatile reads on x86 are very cheap when hitting L1.

My advice, explore point A and C by examining the assembly code, there is
no better way to test. However, removing the volatile read may lead to
races.

Stanimir


On Mon, Dec 31, 2012 at 3:17 PM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:

> I'm investigating potential performance improvements to JRuby and noticed
> that they store instance variables in an array where the field holding the
> array itself is volatile. Most of the time this reference only needs to be
> read and doesn't actually require volatile semantics for since the array
> access itself is non-volatile, too. The only reason it is marked as
> volatile is that the array needs to be grown on write access via a CAS
> operation through an atomic field updater.
>
> Volatile reads themselves don't seem to be all that expensive, the main
> performance penalty is that they cause is that they prevent loop invariant
> code motion. Thus i'm trying to eliminate most volatile reads to enable
> LICM.
>
> So I'm considering several options here
>
> A) use Unsafe.getObject() for a non-volatile read.
>
> This actually doesn't seem to work according to some microbenchmarks I've
> tried. The read does not get hoisted. My question here would be: Is this
> intentional/a limitation of hotspot or am I benchmarking wrong?
>
>
> B) duplicate the field, one volatile, one non-volatile
>
> This would increase the size of every single object in the jruby runtime.
>
>
> C) make the field non-volatile and use Unsafe to do the CAS/volatile
> reads/writes when necessary and use direct field access for hoistable reads
> otherwise, basically emulating the atomic field updater on a non-volatile
> field.
>
> Is this a safe thing to do? Is there a reason why we can't use the field
> updaters on non-volatile fields or is it just some
> don't-shoot-yourself-in-the-**foot thing?
>
>
>
> Oh yeah. ConcurrentHashMaps suffer from the same issues. They have no way
> to support loop-hoistable lazy gets. Even if they used Unsafe operations to
> do so it still wouldn't get hoisted (see A)
>
>
> - Aaron
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/2d27110b/attachment.html>

From aleksey.shipilev at oracle.com  Mon Dec 31 09:42:21 2012
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Mon, 31 Dec 2012 18:42:21 +0400
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1904E.9030406@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de>
Message-ID: <50E1A44D.1090509@oracle.com>

Hi Aaron,

Sorry for very brief response, have no time to dive deeply into the
issue this year.

On 12/31/2012 05:17 PM, Aaron Grunthal wrote:
> I'm investigating potential performance improvements to JRuby and
> noticed that they store instance variables in an array where the field
> holding the array itself is volatile. Most of the time this reference
> only needs to be read and doesn't actually require volatile semantics
> for since the array access itself is non-volatile, too. The only reason
> it is marked as volatile is that the array needs to be grown on write
> access via a CAS operation through an atomic field updater.

This looks like something already discussed on this list before, brought
by Charlie himself, for this exact use case in JRuby, see
http://markmail.org/message/o2cejsnrwtdg7ead

> A) use Unsafe.getObject() for a non-volatile read.
> This actually doesn't seem to work according to some microbenchmarks
> I've tried. The read does not get hoisted. My question here would be: Is
> this intentional/a limitation of hotspot or am I benchmarking wrong?

Maybe both, can you post the test, and the relevant disassembly?
Unsafe.getObject() against non-volatile field should not entail any sort
of ordering in the best case (this is somewhat complicated by alias
analysis, etc).

> Is this a safe thing to do? Is there a reason why we can't use the field
> updaters on non-volatile fields or is it just some
> don't-shoot-yourself-in-the-foot thing?

Generally, the reasoning about the program state under the races is very
complicated. Doing the ordinary write / volatile read (this is the
relaxation possible by lazySet) or volatile write / ordinary read kicks
you off the safe harbor of happens-before consistency.

The motto of this list is "Don't use the races". If you post the exact
scenarios in which you are about to use racy operations, we can
collectively have the fun of telling you exactly how it would break (I
figured that's what the people on this list like to do anyway :])

Happy New Year!

Cheers,
-Aleksey.

From davidcholmes at aapt.net.au  Mon Dec 31 09:54:03 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 1 Jan 2013 00:54:03 +1000
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <CAPs6152Rfc+O3Ge4LoNJ0wx7Aw0DJ2_LUG6=RFTXkPNGEo+UOQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>

A latch is a thousand times simpler than using AQS. Any "wait for event" can
be handled using a simple latch.

The problem with your predicate based solution is that, as I said generally
you need to to ensure that:
a) the predicate is evaluated in a thread-safe way
b) the predicate remains valid while you act on it.

If you don;t need the above then a simple existing synchronizer is likely to
suffice.

I simply don't see what you suggest as being a generally useful tool - and I
do see it as one easily misused because you do in fact need thread-safety
and atomicity.

YMMV.

David

  -----Original Message-----
  From: Kasper Nielsen [mailto:kasperni at gmail.com]
  Sent: Monday, 31 December 2012 7:26 PM
  To: dholmes at ieee.org
  Cc: Concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] A simple predicate based synchronizer


  On Mon, Dec 31, 2012 at 4:04 AM, David Holmes <davidcholmes at aapt.net.au>
wrote:

    General edge-triggers are normally accommodated through the existing
synchronizers: semaphores, latches, barriers, gates. Your "await start"
example is just a latch.

  I don't really see you point? Everything can be build using
AbstractQueuedSynchronizer as well. But that doesn't necessarily make it
easy for end users.


  One area where I really miss something like this is when writing unit
tests. Being able to do something like this
  T1: signal("TestDone")
  T2: await(s -> s.equals("TestDone"), 10, TimeUnit.SECONDS) or even

  T2: awaitEqualsTo("TestDone", 10, TimeUnit.SECONDS)

  would make my life much easier.


  While I could use CountdownLatch in places like this. I usually end up
with a couple of them whenever multiple states are involved. Which make it
much harder to read at a later time.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/1086d4ac/attachment.html>

From dl at cs.oswego.edu  Mon Dec 31 10:17:47 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 31 Dec 2012 10:17:47 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <CANPzfU_-gruW=4a_+8_AeaKCRgOu0CsiRGjJ=anAx6mkPps1qA@mail.gmail.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu> <50E0CBA3.5000304@cs.oswego.edu>
	<CANPzfU_-gruW=4a_+8_AeaKCRgOu0CsiRGjJ=anAx6mkPps1qA@mail.gmail.com>
Message-ID: <50E1AC9B.6040509@cs.oswego.edu>

On 12/31/12 03:57, ?iktor ?lang wrote:
>
> On Dec 31, 2012 12:20 AM, "Doug Lea" <dl at cs.oswego.edu
>  > forcing so many mismatches with Future.get/timed-get API. A new
>  > (unchecked) j.u.c.CompletionException is now used in
>  > the same way as ExecutionException, but is internally
>  > collapsed upon propagation.

>
> Sounds great! How are non-recoverable Errors handled? (VMError, etc)?

RuntimeExceptions and Errors are treated in the same way.
If people think they can recover from an Error (normally
caught as CompletionException with the Error as cause), they are
free to try.

>
>  >
>  > Most usages of CFs that use functional forms will probably
>  > want to use the added getValue() method, that reports these
>  > as CompletionExceptions, instead of get(), that must throw
>  > them as ExecutionExceptions.
>
> That's a very important distinction.
>

Yes. CF now uses exactly the same approach used in ForkJoin (which
also implements Future). For clarity I renamed the unchecked
CF version as "join()". In both CF and FJ,
we support the full checked versions of get/timed-get, but don't
expect people to use them much: If users leave the exception
handling/propagation to us, we can always do it right. Otherwise,
it is not always easy for people to figure out whether and how to
propagate with completeExceptionally, and/or rethrow and/or
exception-transforming rethrow. But when users must do any of
these things in custom situations, they can still do so.

With these changes, I'm claiming that CompletableFuture is for the
moment in a stable state, and urge others to read about and try
it out.

http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log

-Doug






From dl at cs.oswego.edu  Mon Dec 31 11:12:49 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 31 Dec 2012 11:12:49 -0500
Subject: [concurrency-interest] CompletableFuture
In-Reply-To: <20121227210621.70c483a8f5c2ce08074294b0@kotek.net>
References: <50DC94CE.5050407@cs.oswego.edu>
	<20121227210621.70c483a8f5c2ce08074294b0@kotek.net>
Message-ID: <50E1B981.5060602@cs.oswego.edu>

On 12/27/12 16:06, Jan Kotek wrote:
> Hi,
>
>> It would be possible to create a jsr166e version that defined
>> nested interface types to be usable stand-alone under JDK7,
>> but it would be at best unpleasant to use without lambda support.
>
> Maybe backport to JDK7 would make a sense.
>
> Java is not the only language running on JDK7.
>   Scala, Kotlin or Groovy could use this class the same way as Java8.
>

Now that it is stabilizing in j.u.c form (and, as it turns out,
not requiring any internal usages of lambdas) this is worth
contemplating.
I think the only reasonable tactic for doing this would be the same
as for jsr166e.ConcurrentHashMapV8: Include a set of nested
function-type interfaces that are similar to but different than
the ones planned for JDK8. Other languages would then need to
apply their own matching rules for literal lambda-like expressions.
This seemed to work OK for CHMV8, but other ideas are welcome.

-Doug



From aaron.grunthal at infinite-source.de  Mon Dec 31 12:07:29 2012
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Mon, 31 Dec 2012 18:07:29 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1A44D.1090509@oracle.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
Message-ID: <50E1C651.3010605@infinite-source.de>

On 31.12.2012 15:42, Aleksey Shipilev wrote:
> Hi Aaron,
>
> Sorry for very brief response, have no time to dive deeply into the
> issue this year.
>
> On 12/31/2012 05:17 PM, Aaron Grunthal wrote:
>> I'm investigating potential performance improvements to JRuby and
>> noticed that they store instance variables in an array where the field
>> holding the array itself is volatile. Most of the time this reference
>> only needs to be read and doesn't actually require volatile semantics
>> for since the array access itself is non-volatile, too. The only reason
>> it is marked as volatile is that the array needs to be grown on write
>> access via a CAS operation through an atomic field updater.
>
> This looks like something already discussed on this list before, brought
> by Charlie himself, for this exact use case in JRuby, see
> http://markmail.org/message/o2cejsnrwtdg7ead
>

Yes, it involves the same code.

>> A) use Unsafe.getObject() for a non-volatile read.
>> This actually doesn't seem to work according to some microbenchmarks
>> I've tried. The read does not get hoisted. My question here would be: Is
>> this intentional/a limitation of hotspot or am I benchmarking wrong?
>
> Maybe both, can you post the test, and the relevant disassembly?
> Unsafe.getObject() against non-volatile field should not entail any sort
> of ordering in the best case (this is somewhat complicated by alias
> analysis, etc).


The testcase including disassembly: 
https://gist.github.com/4420897#file-test-java

test1 is the volatile
test2 is the non-volatile
test3 is non-volatile access to volatile field through Unsafe

In case2 the field accesses and subsequently some other pre-calculations 
have been optimized through LICM, which is not the case in case 1 or 3.

>
>> Is this a safe thing to do? Is there a reason why we can't use the field
>> updaters on non-volatile fields or is it just some
>> don't-shoot-yourself-in-the-foot thing?
>
> Generally, the reasoning about the program state under the races is very
> complicated. Doing the ordinary write / volatile read (this is the
> relaxation possible by lazySet) or volatile write / ordinary read kicks
> you off the safe harbor of happens-before consistency.
>
> The motto of this list is "Don't use the races". If you post the exact
> scenarios in which you are about to use racy operations, we can
> collectively have the fun of telling you exactly how it would break (I
> figured that's what the people on this list like to do anyway :])
>

Ok. Each jruby object has a volatile Object[] varTable field that is 
used to store instance variables of ruby objects.

Read operations:

resolve name -> index
get current array (volatile read)
get value from array (non-volatile read)


Write operations:

resolve name -> index
get current array (volatile read)
if index >= size: new array + arraycopy + CAS
write value to current or new array (non-volatile write)


My idea was to eliminate the volatile for read operations to make the 
code more LICM-friendly while keeping the volatile/CAS for writes to 
avoid races while growing the table.

Since the array-read is non-volatile anyway i figured that the the only 
negative side-effect would be that a reader might get a stale value from 
an stale array instead of a stale value from a current array.

Now that I think about it there already might be a race that one write 
gets lost when the table gets grown by one thread and a different thread 
writes to the old instance... the write operation probably needs a 
validation when it's done.


- Aaron

From nathan.reynolds at oracle.com  Mon Dec 31 12:36:45 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 31 Dec 2012 10:36:45 -0700
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1C651.3010605@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
Message-ID: <50E1CD2D.2060602@oracle.com>

This isn't the point of this email chain...

 > Write operations:

resolve name -> index
get current array (volatile read)
if index >= size: new array + arraycopy + CAS
write value to current or new array (non-volatile write)


What if 2 threads are writing to 2 different variables?  Let's assume 
the first variable's index is < size and the second variable's index is 
? size.  Isn't there a data race here?  The update to the first variable 
could be lost if it happens between the arraycopy and the CAS operations 
for the second variable.  How is this solved?

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 12/31/2012 10:07 AM, Aaron Grunthal wrote:
> On 31.12.2012 15:42, Aleksey Shipilev wrote:
>> Hi Aaron,
>>
>> Sorry for very brief response, have no time to dive deeply into the
>> issue this year.
>>
>> On 12/31/2012 05:17 PM, Aaron Grunthal wrote:
>>> I'm investigating potential performance improvements to JRuby and
>>> noticed that they store instance variables in an array where the field
>>> holding the array itself is volatile. Most of the time this reference
>>> only needs to be read and doesn't actually require volatile semantics
>>> for since the array access itself is non-volatile, too. The only reason
>>> it is marked as volatile is that the array needs to be grown on write
>>> access via a CAS operation through an atomic field updater.
>>
>> This looks like something already discussed on this list before, brought
>> by Charlie himself, for this exact use case in JRuby, see
>> http://markmail.org/message/o2cejsnrwtdg7ead
>>
>
> Yes, it involves the same code.
>
>>> A) use Unsafe.getObject() for a non-volatile read.
>>> This actually doesn't seem to work according to some microbenchmarks
>>> I've tried. The read does not get hoisted. My question here would 
>>> be: Is
>>> this intentional/a limitation of hotspot or am I benchmarking wrong?
>>
>> Maybe both, can you post the test, and the relevant disassembly?
>> Unsafe.getObject() against non-volatile field should not entail any sort
>> of ordering in the best case (this is somewhat complicated by alias
>> analysis, etc).
>
>
> The testcase including disassembly: 
> https://gist.github.com/4420897#file-test-java
>
> test1 is the volatile
> test2 is the non-volatile
> test3 is non-volatile access to volatile field through Unsafe
>
> In case2 the field accesses and subsequently some other 
> pre-calculations have been optimized through LICM, which is not the 
> case in case 1 or 3.
>
>>
>>> Is this a safe thing to do? Is there a reason why we can't use the 
>>> field
>>> updaters on non-volatile fields or is it just some
>>> don't-shoot-yourself-in-the-foot thing?
>>
>> Generally, the reasoning about the program state under the races is very
>> complicated. Doing the ordinary write / volatile read (this is the
>> relaxation possible by lazySet) or volatile write / ordinary read kicks
>> you off the safe harbor of happens-before consistency.
>>
>> The motto of this list is "Don't use the races". If you post the exact
>> scenarios in which you are about to use racy operations, we can
>> collectively have the fun of telling you exactly how it would break (I
>> figured that's what the people on this list like to do anyway :])
>>
>
> Ok. Each jruby object has a volatile Object[] varTable field that is 
> used to store instance variables of ruby objects.
>
> Read operations:
>
> resolve name -> index
> get current array (volatile read)
> get value from array (non-volatile read)
>
>
> Write operations:
>
> resolve name -> index
> get current array (volatile read)
> if index >= size: new array + arraycopy + CAS
> write value to current or new array (non-volatile write)
>
>
> My idea was to eliminate the volatile for read operations to make the 
> code more LICM-friendly while keeping the volatile/CAS for writes to 
> avoid races while growing the table.
>
> Since the array-read is non-volatile anyway i figured that the the 
> only negative side-effect would be that a reader might get a stale 
> value from an stale array instead of a stale value from a current array.
>
> Now that I think about it there already might be a race that one write 
> gets lost when the table gets grown by one thread and a different 
> thread writes to the old instance... the write operation probably 
> needs a validation when it's done.
>
>
> - Aaron
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/2744d843/attachment.html>

From stanimir at riflexo.com  Mon Dec 31 12:45:17 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 31 Dec 2012 19:45:17 +0200
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1C651.3010605@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
Message-ID: <CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>

>> Now that I think about it there already might be a race that one write
gets lost when the table gets grown by one thread and a different thread
writes to the old instance... the write operation probably needs a
validation when it's done.
Yes, even with the existing volatile load the write might be lost.
Basically you can't have so cheap (race-free) put/get operations in
concurrent environment.

Stanimir




On Mon, Dec 31, 2012 at 7:07 PM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:

> Now that I think about it there already might be a race that one write
> gets lost when the table gets grown by one thread and a different thread
> writes to the old instance... the write operation probably needs a
> validation when it's done.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/30e83c18/attachment.html>

From aaron.grunthal at infinite-source.de  Mon Dec 31 13:02:19 2012
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Mon, 31 Dec 2012 19:02:19 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
Message-ID: <50E1D32B.1020302@infinite-source.de>

On 31.12.2012 18:45, Stanimir Simeonoff wrote:
>  >> Now that I think about it there already might be a race that one
> write gets lost when the table gets grown by one thread and a different
> thread writes to the old instance... the write operation probably needs
> a validation when it's done.
> Yes, even with the existing volatile load the write might be lost.
> Basically you can't have so cheap (race-free) put/get operations in
> concurrent environment.
>
> Stanimir
>

A race-free get is not the goal here. This code is only supposed to 
emulate non-volatile object field accesses with similar guarantees as 
those of java objects according to the JMM.

Stale reads are fine and that's why I think it should be possible to 
eliminate the volatile in the read code path.

Only the write code path is critical as it may have to copy the array, 
which can affect other, concurrent writes.

- Aaron

From stanimir at riflexo.com  Mon Dec 31 13:06:28 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 31 Dec 2012 20:06:28 +0200
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1D32B.1020302@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
Message-ID: <CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>

I meant race-free (and lock free) during the copy phase.

Stanimir

On Mon, Dec 31, 2012 at 8:02 PM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:

> A race-free get is not the goal here. This code is only supposed to
> emulate non-volatile object field accesses with similar guarantees as those
> of java objects according to the JMM.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/af10e2e5/attachment-0001.html>

From aaron.grunthal at infinite-source.de  Mon Dec 31 13:31:50 2012
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Mon, 31 Dec 2012 19:31:50 +0100
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
Message-ID: <50E1DA16.6050507@infinite-source.de>

On 31.12.2012 19:06, Stanimir Simeonoff wrote:
> I meant race-free (and lock free) during the copy phase.

Yes, that definitely needs to be fixed. So, would the following work?


private Object[] varTable; // not volatile!

Read OPs:

return varTable[index];


Write OPs:

1. local = Unsafe.getObjectVolatile(varTable) // volatile read
2. if index >= local.length
      local = allocate + arraycopy(local)  // grow array
3. local[index] = newValue // non-volatile write
4. Unsafe.compareAndSwapObject(varTable,local) // goto 1 on failure


If no copying happens the CAS would basically be a validation to ensure 
it's still the correct array that we've written to. Would it provide 
sufficient ordering or do we have to use ordered/volatile write for step 3?

Note that we do not need any ordering guarantees when it comes to writes 
to the same index. We just have to avoid losing writes when a different 
thread re-assigns the array.


- Aaron

From jeffhain at rocketmail.com  Mon Dec 31 14:16:34 2012
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Mon, 31 Dec 2012 19:16:34 +0000 (GMT)
Subject: [concurrency-interest] CompletableFuture - checked exceptions
In-Reply-To: <50E09669.9000209@cs.oswego.edu>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu>
Message-ID: <1356981394.83983.YahooMailNeo@web132104.mail.ird.yahoo.com>



Doug Lea wrote:

> Zhong Yu wrote:
>
>> If we smuggle a check exception in a
>> RuntimeException, it's a bit of work to uncover it later without
>> ambiguity; I wished there's a dedicated java.lang.SneakedException for
>> the sole purpose of smuggling checked exceptions.
>
>It might be a good idea to institute a JDK-wide convention
>about a specific RuntimeException to use in these situations.

I use a "RethrowException" for that, with (String,Throwable) and (Throwable)
constructors.
It is meant to be used either to "un-check" "annoyingly-checked" exceptions,
or to rethrow any throwable that would have been catched for local handling.

"RethrowException" can be understood as indicating a (re)throw-relative problem,
which checked exceptions are in a sense, or as being an exception-for-rethrow,
which it is.

-Jeff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/65be3f8b/attachment.html>

From zhong.j.yu at gmail.com  Mon Dec 31 14:24:54 2012
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Mon, 31 Dec 2012 13:24:54 -0600
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1DA16.6050507@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de>
Message-ID: <CACuKZqFUBr3g6vB3MkXKNT56otcWpLfA9r-qyfmCfMUNNB_3LQ@mail.gmail.com>

On Mon, Dec 31, 2012 at 12:31 PM, Aaron Grunthal
<aaron.grunthal at infinite-source.de> wrote:
> On 31.12.2012 19:06, Stanimir Simeonoff wrote:
>>
>> I meant race-free (and lock free) during the copy phase.
>
>
> Yes, that definitely needs to be fixed. So, would the following work?
>
>
> private Object[] varTable; // not volatile!
>
> Read OPs:
>
> return varTable[index];
>
>
> Write OPs:
>
> 1. local = Unsafe.getObjectVolatile(varTable) // volatile read
> 2. if index >= local.length
>      local = allocate + arraycopy(local)  // grow array
> 3. local[index] = newValue // non-volatile write
> 4. Unsafe.compareAndSwapObject(varTable,local) // goto 1 on failure

Question to experts: any chance that 3 and 4 appear out of order to
the read thread?

>
> If no copying happens the CAS would basically be a validation to ensure it's
> still the correct array that we've written to. Would it provide sufficient
> ordering or do we have to use ordered/volatile write for step 3?
>
> Note that we do not need any ordering guarantees when it comes to writes to
> the same index. We just have to avoid losing writes when a different thread
> re-assigns the array.
>
>
>
> - Aaron
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From viktor.klang at gmail.com  Mon Dec 31 15:05:16 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 31 Dec 2012 21:05:16 +0100
Subject: [concurrency-interest] CompletableFuture - checked exceptions
In-Reply-To: <1356981394.83983.YahooMailNeo@web132104.mail.ird.yahoo.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu>
	<1356981394.83983.YahooMailNeo@web132104.mail.ird.yahoo.com>
Message-ID: <CANPzfU_cUShNY08Tc-a12EsVGqtGzuMAv67xH1SexMd9m4LWOw@mail.gmail.com>

Of course there are a few edge-cases when it comes to propagating
Throwables across threads.
On Dec 31, 2012 8:19 PM, "Jeff Hain" <jeffhain at rocketmail.com> wrote:

>
> Doug Lea wrote:
>
> > Zhong Yu wrote:
> >
> >> If we smuggle a check exception in a
> >> RuntimeException, it's a bit of work to uncover it later without
> >> ambiguity; I wished there's a dedicated java.lang.SneakedException for
> >> the sole purpose of smuggling checked exceptions.
> >
> >It might be a good idea to institute a JDK-wide convention
> >about a specific RuntimeException to use in these situations.
>
> I use a "RethrowException" for that, with (String,Throwable) and
> (Throwable)
> constructors.
> It is meant to be used either to "un-check" "annoyingly-checked"
> exceptions,
> or to rethrow any throwable that would have been catched for local
> handling.
>
> "RethrowException" can be understood as indicating a (re)throw-relative
> problem,
> which checked exceptions are in a sense, or as being an
> exception-for-rethrow,
> which it is.
>
> -Jeff
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/4f9ab505/attachment.html>

From stanimir at riflexo.com  Mon Dec 31 16:26:58 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 31 Dec 2012 23:26:58 +0200
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <50E1DA16.6050507@infinite-source.de>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de>
Message-ID: <CAEJX8oozTYkkrS9bM3ger9DYUX6cYwv98Y_SNQyiMiyH3VF5aw@mail.gmail.com>

The code looks ok. The reading thread can get any varTable but the races on
the Ruby fields are given.

>>4. Unsafe.compareAndSwapObject(varTable,local) // goto 1 on failure
That's a naked CAS which would not be necessary in 99.99% of the cases
(i.e. varTable==local). Simple compare
if (Unsafe.getObjectVolatile(varTable)!=local && !CAS(varTable, local))
goto 1
should be better as "varTable" will be equal to "local" almost always,
hence the branch would be properly predicted by the hardware.

CAS is more expensive than volatile write and causes local latency, plus
some coherency traffic, so unless necessary (statically) it shall be
avoided.

Read OP is, of course, racy. Probably if the index is >=varTable.length,
you'd like a volatile read to 'refresh' instead of returning the default
value.

Stanimir

On Mon, Dec 31, 2012 at 8:31 PM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:

> On 31.12.2012 19:06, Stanimir Simeonoff wrote:
>
>> I meant race-free (and lock free) during the copy phase.
>>
>
> Yes, that definitely needs to be fixed. So, would the following work?
>
>
> private Object[] varTable; // not volatile!
>
> Read OPs:
>
> return varTable[index];
>
>
> Write OPs:
>
> 1. local = Unsafe.getObjectVolatile(**varTable) // volatile read
> 2. if index >= local.length
>      local = allocate + arraycopy(local)  // grow array
> 3. local[index] = newValue // non-volatile write
> 4. Unsafe.compareAndSwapObject(**varTable,local) // goto 1 on failure
>
>
> If no copying happens the CAS would basically be a validation to ensure
> it's still the correct array that we've written to. Would it provide
> sufficient ordering or do we have to use ordered/volatile write for step 3?
>
> Note that we do not need any ordering guarantees when it comes to writes
> to the same index. We just have to avoid losing writes when a different
> thread re-assigns the array.
>
>
> - Aaron
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/3e7cb860/attachment.html>

From martinrb at google.com  Mon Dec 31 16:30:06 2012
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 31 Dec 2012 13:30:06 -0800
Subject: [concurrency-interest] CompletableFuture - checked exceptions
In-Reply-To: <1356981394.83983.YahooMailNeo@web132104.mail.ird.yahoo.com>
References: <50DC94CE.5050407@cs.oswego.edu> <50DF34D0.3070309@cs.oswego.edu>
	<CACuKZqGF9WcOuhrwwrNsMpUGOPVVEHBiEp0TVkbaqXtkxo_SNQ@mail.gmail.com>
	<50DF577B.9040807@cs.oswego.edu>
	<CACuKZqG1oA6p-oqo75A1RykAeb7XJ1GFn_P9v2a=ZsYxDw6QGA@mail.gmail.com>
	<50E05421.5060902@cs.oswego.edu>
	<CACuKZqGR5S_f4uNy-uNEfQBJZ4LapU+YY6va6Gxp9kWPMD1njg@mail.gmail.com>
	<50E09669.9000209@cs.oswego.edu>
	<1356981394.83983.YahooMailNeo@web132104.mail.ird.yahoo.com>
Message-ID: <CA+kOe0-jnxd0v7CgwFro=VkTPB2nBpfbANEQO-QwLsUbd4WB5Q@mail.gmail.com>

So perhaps we should elide the constructors for CompletionException that do
not provide a Throwable cause.

On Mon, Dec 31, 2012 at 11:16 AM, Jeff Hain <jeffhain at rocketmail.com> wrote:

>
> Doug Lea wrote:
>
> > Zhong Yu wrote:
> >
> >> If we smuggle a check exception in a
> >> RuntimeException, it's a bit of work to uncover it later without
> >> ambiguity; I wished there's a dedicated java.lang.SneakedException for
> >> the sole purpose of smuggling checked exceptions.
> >
> >It might be a good idea to institute a JDK-wide convention
> >about a specific RuntimeException to use in these situations.
>
> I use a "RethrowException" for that, with (String,Throwable) and
> (Throwable)
> constructors.
> It is meant to be used either to "un-check" "annoyingly-checked"
> exceptions,
> or to rethrow any throwable that would have been catched for local
> handling.
>
> "RethrowException" can be understood as indicating a (re)throw-relative
> problem,
> which checked exceptions are in a sense, or as being an
> exception-for-rethrow,
> which it is.
>
> -Jeff
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/645bb04a/attachment-0001.html>

From stanimir at riflexo.com  Mon Dec 31 16:36:48 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 31 Dec 2012 23:36:48 +0200
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CACuKZqFUBr3g6vB3MkXKNT56otcWpLfA9r-qyfmCfMUNNB_3LQ@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de> <50E1A44D.1090509@oracle.com>
	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de>
	<CACuKZqFUBr3g6vB3MkXKNT56otcWpLfA9r-qyfmCfMUNNB_3LQ@mail.gmail.com>
Message-ID: <CAEJX8or6iKcQApBqUMEmHWSWc_9vOi3A+SawiXg14X4BiktOXw@mail.gmail.com>

> > 1. local = Unsafe.getObjectVolatile(varTable) // volatile read
> > 2. if index >= local.length
> >      local = allocate + arraycopy(local)  // grow array
> > 3. local[index] = newValue // non-volatile write
> > 4. Unsafe.compareAndSwapObject(varTable,local) // goto 1 on failure
>
> Question to experts: any chance that 3 and 4 appear out of order to
> the read thread?
>
>
Unsafe is not part of the JMM, so at that point you're at the
compiler+hardware mercy. Unsafe is not required to add fences if they are
necessary to comform JMM. AtomicXXX has explicit guarantees, so it may add
the fences on their right own.
Here is an article form Cliff Click on unsafe CAS:
http://www.azulsystems.com/blog/cliff/2010-07-24-unsafe-compareandswap
If Unsafe.CAS works as AtomicXXX CAS it shall be ok.


Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20121231/9fda48ac/attachment.html>

From davidcholmes at aapt.net.au  Mon Dec 31 17:16:51 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 1 Jan 2013 08:16:51 +1000
Subject: [concurrency-interest] A simple predicate based synchronizer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEDGJJAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEDJJJAA.davidcholmes@aapt.net.au>

Further without external state synchronization you would have to resort to
polling to avoid lost wakeups, or state changes would only be noticed at the
next state change if any eg:

   Thread 1:                                  Thread 2:
----------------------------------------------------------------
   while (!predicate.evaluate())
                                             updateState()
                                             sync.checkAll()
     park();

There is a lost wakeup here because the state change is not atomic with
respect to the synchronizers park/unpark logic. So we either can't park()
but must instead use a timed park(pollTime).

David

 -----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of David Holmes
Sent: Tuesday, 1 January 2013 12:54 AM
To: Kasper Nielsen
Cc: Concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] A simple predicate based synchronizer


  A latch is a thousand times simpler than using AQS. Any "wait for event"
can be handled using a simple latch.

  The problem with your predicate based solution is that, as I said
generally you need to to ensure that:
  a) the predicate is evaluated in a thread-safe way
  b) the predicate remains valid while you act on it.

  If you don;t need the above then a simple existing synchronizer is likely
to suffice.

  I simply don't see what you suggest as being a generally useful tool - and
I do see it as one easily misused because you do in fact need thread-safety
and atomicity.

  YMMV.

  David

    -----Original Message-----
    From: Kasper Nielsen [mailto:kasperni at gmail.com]
    Sent: Monday, 31 December 2012 7:26 PM
    To: dholmes at ieee.org
    Cc: Concurrency-interest at cs.oswego.edu
    Subject: Re: [concurrency-interest] A simple predicate based
synchronizer


    On Mon, Dec 31, 2012 at 4:04 AM, David Holmes <davidcholmes at aapt.net.au>
wrote:

      General edge-triggers are normally accommodated through the existing
synchronizers: semaphores, latches, barriers, gates. Your "await start"
example is just a latch.

    I don't really see you point? Everything can be build using
AbstractQueuedSynchronizer as well. But that doesn't necessarily make it
easy for end users.


    One area where I really miss something like this is when writing unit
tests. Being able to do something like this
    T1: signal("TestDone")
    T2: await(s -> s.equals("TestDone"), 10, TimeUnit.SECONDS) or even

    T2: awaitEqualsTo("TestDone", 10, TimeUnit.SECONDS)

    would make my life much easier.


    While I could use CountdownLatch in places like this. I usually end up
with a couple of them whenever multiple states are involved. Which make it
much harder to read at a later time.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130101/6c70aaaa/attachment.html>

From hans.boehm at hp.com  Mon Dec 31 21:58:32 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 1 Jan 2013 02:58:32 +0000
Subject: [concurrency-interest] Volatile/Atomic operations on a
 non-volatile field
In-Reply-To: <CACuKZqFUBr3g6vB3MkXKNT56otcWpLfA9r-qyfmCfMUNNB_3LQ@mail.gmail.com>
References: <50E1904E.9030406@infinite-source.de>
	<50E1A44D.1090509@oracle.com>	<50E1C651.3010605@infinite-source.de>
	<CAEJX8orY8WS4rdFT-fvqANZFtzGPgPP3Jndr3HpsA3aZrFgM=w@mail.gmail.com>
	<50E1D32B.1020302@infinite-source.de>
	<CAEJX8ornKDiOFWOsMLyFZ8jhnkhwFMs=2Ldiuef47WEe=KPx5Q@mail.gmail.com>
	<50E1DA16.6050507@infinite-source.de>
	<CACuKZqFUBr3g6vB3MkXKNT56otcWpLfA9r-qyfmCfMUNNB_3LQ@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD236967060@G9W0725.americas.hpqcorp.net>

> From: Zhong Yu
> On Mon, Dec 31, 2012 at 12:31 PM, Aaron Grunthal <aaron.grunthal at infinite-
> source.de> wrote:
> > ... So, would the following work?
> >
> >
> > private Object[] varTable; // not volatile!
> >
> > Read OPs:
> >
> > return varTable[index];
> >
I'm not sure I understand the setting here, but what prevents this from seeing an older, smaller version of varTable, with a newly allocated index, generating an out-of-bounds exception.  Is that acceptable?

There's also technically a problem, probably observed on no modern hardware, that the varTable[index] load may appear to complete before the varTable load, allowing the former to see a not-yet-copied value.  The Java memory model does not in general (aside from final fields) guarantee that visibility ordering respects data dependencies.

Hans


