From oleksandr.otenko at oracle.com  Thu May  1 08:52:02 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 01 May 2014 13:52:02 +0100
Subject: [concurrency-interest] Stricter read ordering
In-Reply-To: <d2b0d53e839c465da809ac725db7c045@exchmb01.office.devexperts.com>
References: <E7E7AFC7-646D-4C34-9C55-7F5F5F1BCD0E@gmail.com>	<5357B51D.9000206@oracle.com>
	<d2b0d53e839c465da809ac725db7c045@exchmb01.office.devexperts.com>
Message-ID: <53624372.5070703@oracle.com>

In terms of barriers, yes, only load-load barrier is needed. But the 
barriers have no mapping into JMM.

In terms of JMM, a sw is needed between the last volatile read and the 
volatile write following it in so. (so the write synchronizes-with the 
/earlier/ read, and then the normal reads preceding the volatile read hb 
the normal writes following the volatile write). This is not possible in 
current JMM, and it is not clear how to add it there - you'd need to 
somehow specify /which/ read and /which/ write to link like that, but 
Java language doesn't have means for that.


Alex


On 25/04/2014 08:02, Roman Elizarov wrote:
> I cannot not stop thinking that Java 8 solution with Unsafe.loadFence() in StampedLock.validate() method is unsatisfactory. Not that because it is "Unsafe", but because it explicitly invokes a fence, which, in the ideal view of the world, should be just one possible implementation mechanism, not the primitive that the developer uses. To me it does "appear exceptionally unnatural" as Hans Boehm notes in his 2012 paper on seqlocks.
>
> Reading back and forth that paper I don't see a consolation in its conclusions. On a surface of it, all that is needed to make seqlock idiom implementable in the Java-like (happens-before) memory model are two new kinds of synchronization operations that would establish a synchronizes-with (sw) relation between a read and a subsequent write in synchronization order (so), as opposed to write-release/read-acquire pair that establishes sw-relation between a write and so-subsequent read. There's no need for "read-don't-modify-write". Even if its actual write is optimized away, its memory model effect still looks too strong for seqlock needs. As Hans points out in his paper, the compiler could have safely optimized "read-don't-modify-write" into "fence+read", but seqlock only needs "loadFence+read" in its validate  method.
>
> In the validate method of seqlock we don't need any write (even if it does not modify). We don't need sw relation with so-previous write that "read-don't-modify-write" gives. We only need sw with so-subsequent write. In C++ terms, we just need to be able to memory_order_release on read in seqlock validate method (last read in the read path) and memory_order_acquire on a write in seqlock's first operation in a write path. For a multi-writer seqlock, the writer path uses read-modify-write operation anyway (to acquire lock), so it already has both acquire and release memory order. But what about read-release for validate? We don't have it now. Is there any existing research in that direction that might unify seqlocks with happens-before memory models in a satisfactory way?
>   
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Aleksey Shipilev
> Sent: Wednesday, April 23, 2014 4:42 PM
> To: Tobias Lindaaker; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Stricter read ordering
>
> On 04/16/2014 03:25 PM, Tobias Lindaaker wrote:
>> Is there any way I could introduce a fence that guarantees that the
>> data reads will not be moved to after the read of this.next?
> I think you are reinventing sequence locks. Prime Java example is StampedLock which deals with ordering reads with Unsafe.loadFence(), see StampedLock.validate().
>
> So, in your example, it amounts to:
>
>         public DataCarrier read() {
>           // I allow multiple readers, so this method is not ynchronized
>           int x, y;
>           long version;
>           do {
>             version = this.written;
>             x = this.x;
>             y = this.y;
>             Unsafe.loadFence();
>           } while ( version != this.next );
>           return new DataCarrier( x, y );
>         }
>       }
>
> -Aleksey.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140501/66bec3bc/attachment.html>

From dl at cs.oswego.edu  Fri May  2 13:59:10 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 02 May 2014 13:59:10 -0400
Subject: [concurrency-interest] Java 8 CompletableFuture.thenCombine()
 hangs
In-Reply-To: <534B14A4.1080209@cs.oswego.edu>
References: <533B48E6.6050700@sosnoski.com> <5341C9CC.4080104@cs.oswego.edu>
	<534B14A4.1080209@cs.oswego.edu>
Message-ID: <5363DCEE.2080600@cs.oswego.edu>

On 04/13/2014 06:50 PM, Doug Lea wrote:
> By further repacking actions, we can move completion lists
> around even in non-tail recursive cases without practical
> loss in potential parallelism, and, as it turns out, with
> a small gain in average throughput.

While testing sets of concurrent CompletableFutures with
millions of dependencies, I noticed that they could become
hostile to GC (mainly due to "floating" garbage).
Arranging to allow aggressive detachment led me
to undertake a long-needed internal refactoring.

-Doug

>
> Updated sources at
>
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>
>
> Also compiled into the jar at
>     http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
> that you can run with -Xbootclasspath
>
> Please try it out.
>

From ri.joel at gmail.com  Sun May  4 07:06:22 2014
From: ri.joel at gmail.com (Joel Richard)
Date: Sun, 4 May 2014 13:06:22 +0200
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
	simpler Async IO
Message-ID: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>

Hi,

Recently I have researched a little about how different languages implement
async IO because I wanted to implement a high scalability web server. When
I looked at Google Go, I realized that the common callback approach is
actually just a language-level workaround to fix a VM limitation.

So I wondered whether and how it could be possible to implement something
similar to goroutines for the JVM. Here are my thoughts: There should be an
ExecutorService implementation which executes tasks in a special mode. If
you use any (so far) blocking methods (like Thread.sleep, network/file
system IO, Object.wait and synchronization) in such a task it uses however
not the blocking native implementation, but an non-blocking version of it.
While it waits for the method to complete, it can continue with another
task in the same native thread. This means that all tasks can share a small
native thread pool. If longer computations have to be done, the developer
can either call Thread.yield() from time to time or just move the
computation to a classical thread.

With such an approach it would be much easier to write highly scalable
server software* and it would make it even possible to run most existing
libraries with minimal changes as asynchronous IO libraries. Basically,
they would just have to make sure that they call Thread.yield() in longer
computationally intensive loops and don't start additional normal threads.
Last but not least, the behavior of existing threads wouldn't be changed at
all.

* The advantage of async IO compared to blocking IO is that it doesn?t
require expensive thread context switches and that it needs less memory
(because fewer threads allocate less memory for stacks).

As mentioned above, this would require implementing an asynchronous version
of all blocking native methods. I am not that familiar with JNI nor C, but
I think this could by achieved by allowing the developer to define a second
C function with the _async suffix which calls a callback instead of
returning a value. If the Java native method is called in a normal thread,
the old C function is called and otherwise the _async version of it. Since
Java already supports async IO with NIO, I think it should be technically
possible to implement asynchronous versions for almost all of the currently
blocking native functions. If an operating system doesn't support certain
non-blocking operations, it could just block a special thread instead (as
far as I know Go does it similar).

I am aware that the required changes would be complicated, but I think it
is worth a discussion since concurrency, non-blocking IO and high
scalability will be even more important in the future. This feature would
make it much easier to fulfill such requirements. For example, it would not
be necessary anymore to create a second version of each so far blocking
API. Also all standardized APIs like JDBC or JPA would suddenly all have a
non-blocking implementation. The amount of work which could be saved would
be enormous.

What do you think about this idea? Has something like that been discussed
before?

Regards, Joel

PS I sent a similar email first to the jdk9-dev because I wasn't sure which
mailing list to use.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140504/ea4d0106/attachment.html>

From zhong.j.yu at gmail.com  Sun May  4 12:39:03 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sun, 4 May 2014 11:39:03 -0500
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
Message-ID: <CACuKZqFoxgPo61VdJ6E4YdXYphvTbDmxfe_uEX92Kw64qOjmAQ@mail.gmail.com>

See Ron Pressler's work

http://cs.oswego.edu/pipermail/concurrency-interest/2013-October/011900.html

http://cs.oswego.edu/pipermail/concurrency-interest/2014-February/012347.html

which I think is definitely the way to go. Other schemes - callback,
future/promise, generator, even C#'s async - are just DSLs trying to
simulate the threading model, imperfectly. If threads were lightweight
to begin with, these solutions would not have been invented.

Zhong Yu



On Sun, May 4, 2014 at 6:06 AM, Joel Richard <ri.joel at gmail.com> wrote:
> Hi,
>
> Recently I have researched a little about how different languages implement
> async IO because I wanted to implement a high scalability web server. When I
> looked at Google Go, I realized that the common callback approach is
> actually just a language-level workaround to fix a VM limitation.
>
> So I wondered whether and how it could be possible to implement something
> similar to goroutines for the JVM. Here are my thoughts: There should be an
> ExecutorService implementation which executes tasks in a special mode. If
> you use any (so far) blocking methods (like Thread.sleep, network/file
> system IO, Object.wait and synchronization) in such a task it uses however
> not the blocking native implementation, but an non-blocking version of it.
> While it waits for the method to complete, it can continue with another task
> in the same native thread. This means that all tasks can share a small
> native thread pool. If longer computations have to be done, the developer
> can either call Thread.yield() from time to time or just move the
> computation to a classical thread.
>
> With such an approach it would be much easier to write highly scalable
> server software* and it would make it even possible to run most existing
> libraries with minimal changes as asynchronous IO libraries. Basically, they
> would just have to make sure that they call Thread.yield() in longer
> computationally intensive loops and don't start additional normal threads.
> Last but not least, the behavior of existing threads wouldn't be changed at
> all.
>
> * The advantage of async IO compared to blocking IO is that it doesn?t
> require expensive thread context switches and that it needs less memory
> (because fewer threads allocate less memory for stacks).
>
> As mentioned above, this would require implementing an asynchronous version
> of all blocking native methods. I am not that familiar with JNI nor C, but I
> think this could by achieved by allowing the developer to define a second C
> function with the _async suffix which calls a callback instead of returning
> a value. If the Java native method is called in a normal thread, the old C
> function is called and otherwise the _async version of it. Since Java
> already supports async IO with NIO, I think it should be technically
> possible to implement asynchronous versions for almost all of the currently
> blocking native functions. If an operating system doesn't support certain
> non-blocking operations, it could just block a special thread instead (as
> far as I know Go does it similar).
>
> I am aware that the required changes would be complicated, but I think it is
> worth a discussion since concurrency, non-blocking IO and high scalability
> will be even more important in the future. This feature would make it much
> easier to fulfill such requirements. For example, it would not be necessary
> anymore to create a second version of each so far blocking API. Also all
> standardized APIs like JDBC or JPA would suddenly all have a non-blocking
> implementation. The amount of work which could be saved would be enormous.
>
> What do you think about this idea? Has something like that been discussed
> before?
>
> Regards, Joel
>
> PS I sent a similar email first to the jdk9-dev because I wasn't sure which
> mailing list to use.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From ri.joel at gmail.com  Sun May  4 17:03:51 2014
From: ri.joel at gmail.com (Joel Richard)
Date: Sun, 4 May 2014 23:03:51 +0200
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <CACuKZqFoxgPo61VdJ6E4YdXYphvTbDmxfe_uEX92Kw64qOjmAQ@mail.gmail.com>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
	<CACuKZqFoxgPo61VdJ6E4YdXYphvTbDmxfe_uEX92Kw64qOjmAQ@mail.gmail.com>
Message-ID: <CAOe+kYeUk+okQhrAKPBpymJrXY0PNnOBwFgkj0A-ynHtWY4uLw@mail.gmail.com>

That is a really interesting project. Thank you very much for mentioning it.

As far as I can see, the missing part is that it cannot transform thread
blocking IO into fiber blocking IO (which is asynchronous) yet. Maybe even
that could be implemented partially, if it replaced the most important
blocking classes of the Java standard library while loading with an
equivalent non-blocking version (based on NIO). This would be fantastic for
a proof of concept of my initial idea since it wouldn't require any changes
in the native C code.

@Ron, I hope it's ok for you that I have added you to this thread. Have you
thought about this before? Could you imagine that this would work?

Regards, Joel


On Sun, May 4, 2014 at 6:39 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> See Ron Pressler's work
>
>
> http://cs.oswego.edu/pipermail/concurrency-interest/2013-October/011900.html
>
>
> http://cs.oswego.edu/pipermail/concurrency-interest/2014-February/012347.html
>
> which I think is definitely the way to go. Other schemes - callback,
> future/promise, generator, even C#'s async - are just DSLs trying to
> simulate the threading model, imperfectly. If threads were lightweight
> to begin with, these solutions would not have been invented.
>
> Zhong Yu
>
>
>
> On Sun, May 4, 2014 at 6:06 AM, Joel Richard <ri.joel at gmail.com> wrote:
> > Hi,
> >
> > Recently I have researched a little about how different languages
> implement
> > async IO because I wanted to implement a high scalability web server.
> When I
> > looked at Google Go, I realized that the common callback approach is
> > actually just a language-level workaround to fix a VM limitation.
> >
> > So I wondered whether and how it could be possible to implement something
> > similar to goroutines for the JVM. Here are my thoughts: There should be
> an
> > ExecutorService implementation which executes tasks in a special mode. If
> > you use any (so far) blocking methods (like Thread.sleep, network/file
> > system IO, Object.wait and synchronization) in such a task it uses
> however
> > not the blocking native implementation, but an non-blocking version of
> it.
> > While it waits for the method to complete, it can continue with another
> task
> > in the same native thread. This means that all tasks can share a small
> > native thread pool. If longer computations have to be done, the developer
> > can either call Thread.yield() from time to time or just move the
> > computation to a classical thread.
> >
> > With such an approach it would be much easier to write highly scalable
> > server software* and it would make it even possible to run most existing
> > libraries with minimal changes as asynchronous IO libraries. Basically,
> they
> > would just have to make sure that they call Thread.yield() in longer
> > computationally intensive loops and don't start additional normal
> threads.
> > Last but not least, the behavior of existing threads wouldn't be changed
> at
> > all.
> >
> > * The advantage of async IO compared to blocking IO is that it doesn?t
> > require expensive thread context switches and that it needs less memory
> > (because fewer threads allocate less memory for stacks).
> >
> > As mentioned above, this would require implementing an asynchronous
> version
> > of all blocking native methods. I am not that familiar with JNI nor C,
> but I
> > think this could by achieved by allowing the developer to define a
> second C
> > function with the _async suffix which calls a callback instead of
> returning
> > a value. If the Java native method is called in a normal thread, the old
> C
> > function is called and otherwise the _async version of it. Since Java
> > already supports async IO with NIO, I think it should be technically
> > possible to implement asynchronous versions for almost all of the
> currently
> > blocking native functions. If an operating system doesn't support certain
> > non-blocking operations, it could just block a special thread instead (as
> > far as I know Go does it similar).
> >
> > I am aware that the required changes would be complicated, but I think
> it is
> > worth a discussion since concurrency, non-blocking IO and high
> scalability
> > will be even more important in the future. This feature would make it
> much
> > easier to fulfill such requirements. For example, it would not be
> necessary
> > anymore to create a second version of each so far blocking API. Also all
> > standardized APIs like JDBC or JPA would suddenly all have a non-blocking
> > implementation. The amount of work which could be saved would be
> enormous.
> >
> > What do you think about this idea? Has something like that been discussed
> > before?
> >
> > Regards, Joel
> >
> > PS I sent a similar email first to the jdk9-dev because I wasn't sure
> which
> > mailing list to use.
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140504/90b531b8/attachment.html>

From gregg at wonderly.org  Sun May  4 17:37:31 2014
From: gregg at wonderly.org (Gregg Wonderly)
Date: Sun, 4 May 2014 16:37:31 -0500
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
	simpler Async IO
In-Reply-To: <CAOe+kYeUk+okQhrAKPBpymJrXY0PNnOBwFgkj0A-ynHtWY4uLw@mail.gmail.com>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
	<CACuKZqFoxgPo61VdJ6E4YdXYphvTbDmxfe_uEX92Kw64qOjmAQ@mail.gmail.com>
	<CAOe+kYeUk+okQhrAKPBpymJrXY0PNnOBwFgkj0A-ynHtWY4uLw@mail.gmail.com>
Message-ID: <3813662F-0C97-4D21-9A14-BF987D25A7C1@wonderly.org>

The original version of Java used a runtime environment which used the term "Green Threads".  This was of course taken out when sun started trying to make Java more than an Applet engine.

There are two problems.  First is all the visibility and contention issues which the concurrency package has demonstrated to not be a simple issue due to rather arcane processor architecture issues.

What we actually need is infinitely fast cache and cache coherency algorithms, and much more sane processor designs which try to allow processors to work together rather than trying desperately to keep them from interfering with each other.

Gregg 

Sent from my iPhone

> On May 4, 2014, at 4:03 PM, Joel Richard <ri.joel at gmail.com> wrote:
> 
> That is a really interesting project. Thank you very much for mentioning it.
> 
> As far as I can see, the missing part is that it cannot transform thread blocking IO into fiber blocking IO (which is asynchronous) yet. Maybe even that could be implemented partially, if it replaced the most important blocking classes of the Java standard library while loading with an equivalent non-blocking version (based on NIO). This would be fantastic for a proof of concept of my initial idea since it wouldn't require any changes in the native C code.
> 
> @Ron, I hope it's ok for you that I have added you to this thread. Have you thought about this before? Could you imagine that this would work?
> 
> Regards, Joel
> 
> 
>> On Sun, May 4, 2014 at 6:39 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>> See Ron Pressler's work
>> 
>> http://cs.oswego.edu/pipermail/concurrency-interest/2013-October/011900.html
>> 
>> http://cs.oswego.edu/pipermail/concurrency-interest/2014-February/012347.html
>> 
>> which I think is definitely the way to go. Other schemes - callback,
>> future/promise, generator, even C#'s async - are just DSLs trying to
>> simulate the threading model, imperfectly. If threads were lightweight
>> to begin with, these solutions would not have been invented.
>> 
>> Zhong Yu
>> 
>> 
>> 
>> On Sun, May 4, 2014 at 6:06 AM, Joel Richard <ri.joel at gmail.com> wrote:
>> > Hi,
>> >
>> > Recently I have researched a little about how different languages implement
>> > async IO because I wanted to implement a high scalability web server. When I
>> > looked at Google Go, I realized that the common callback approach is
>> > actually just a language-level workaround to fix a VM limitation.
>> >
>> > So I wondered whether and how it could be possible to implement something
>> > similar to goroutines for the JVM. Here are my thoughts: There should be an
>> > ExecutorService implementation which executes tasks in a special mode. If
>> > you use any (so far) blocking methods (like Thread.sleep, network/file
>> > system IO, Object.wait and synchronization) in such a task it uses however
>> > not the blocking native implementation, but an non-blocking version of it.
>> > While it waits for the method to complete, it can continue with another task
>> > in the same native thread. This means that all tasks can share a small
>> > native thread pool. If longer computations have to be done, the developer
>> > can either call Thread.yield() from time to time or just move the
>> > computation to a classical thread.
>> >
>> > With such an approach it would be much easier to write highly scalable
>> > server software* and it would make it even possible to run most existing
>> > libraries with minimal changes as asynchronous IO libraries. Basically, they
>> > would just have to make sure that they call Thread.yield() in longer
>> > computationally intensive loops and don't start additional normal threads.
>> > Last but not least, the behavior of existing threads wouldn't be changed at
>> > all.
>> >
>> > * The advantage of async IO compared to blocking IO is that it doesn?t
>> > require expensive thread context switches and that it needs less memory
>> > (because fewer threads allocate less memory for stacks).
>> >
>> > As mentioned above, this would require implementing an asynchronous version
>> > of all blocking native methods. I am not that familiar with JNI nor C, but I
>> > think this could by achieved by allowing the developer to define a second C
>> > function with the _async suffix which calls a callback instead of returning
>> > a value. If the Java native method is called in a normal thread, the old C
>> > function is called and otherwise the _async version of it. Since Java
>> > already supports async IO with NIO, I think it should be technically
>> > possible to implement asynchronous versions for almost all of the currently
>> > blocking native functions. If an operating system doesn't support certain
>> > non-blocking operations, it could just block a special thread instead (as
>> > far as I know Go does it similar).
>> >
>> > I am aware that the required changes would be complicated, but I think it is
>> > worth a discussion since concurrency, non-blocking IO and high scalability
>> > will be even more important in the future. This feature would make it much
>> > easier to fulfill such requirements. For example, it would not be necessary
>> > anymore to create a second version of each so far blocking API. Also all
>> > standardized APIs like JDBC or JPA would suddenly all have a non-blocking
>> > implementation. The amount of work which could be saved would be enormous.
>> >
>> > What do you think about this idea? Has something like that been discussed
>> > before?
>> >
>> > Regards, Joel
>> >
>> > PS I sent a similar email first to the jdk9-dev because I wasn't sure which
>> > mailing list to use.
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140504/e7106073/attachment-0001.html>

From davidcholmes at aapt.net.au  Sun May  4 17:47:02 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 5 May 2014 07:47:02 +1000
Subject: [concurrency-interest] Proposal for Hybrid Threading Model
	andsimpler Async IO
In-Reply-To: <3813662F-0C97-4D21-9A14-BF987D25A7C1@wonderly.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCMECMKGAA.davidcholmes@aapt.net.au>

I think what Joel is considering is something more akin to the M:N scheduling model, than the M:1 model of green threads. It is a plausible threading model but not something that can be tacked on as a replacement to the existing 1:1 model - you are talking about a complete reimplementation of large parts of the VM and native library code. Further I don't think this is something that can just be switched in under the covers as the performance model would completely change - so frameworks and apps that are heavy I/O users and have been tuned accordingly would be completely undone by such a change.

A model that allows for full Threads and light-weight threads (ala Qasar) is the only realistic option, but not what Joel is actually looking for.

Just my 2c.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg Wonderly
  Sent: Monday, 5 May 2014 7:38 AM
  To: Joel Richard
  Cc: Concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Proposal for Hybrid Threading Model andsimpler Async IO


  The original version of Java used a runtime environment which used the term "Green Threads".  This was of course taken out when sun started trying to make Java more than an Applet engine.


  There are two problems.  First is all the visibility and contention issues which the concurrency package has demonstrated to not be a simple issue due to rather arcane processor architecture issues.


  What we actually need is infinitely fast cache and cache coherency algorithms, and much more sane processor designs which try to allow processors to work together rather than trying desperately to keep them from interfering with each other.


  Gregg 

  Sent from my iPhone

  On May 4, 2014, at 4:03 PM, Joel Richard <ri.joel at gmail.com> wrote:


    That is a really interesting project. Thank you very much for mentioning it.


    As far as I can see, the missing part is that it cannot transform thread blocking IO into fiber blocking IO (which is asynchronous) yet. Maybe even that could be implemented partially, if it replaced the most important blocking classes of the Java standard library while loading with an equivalent non-blocking version (based on NIO). This would be fantastic for a proof of concept of my initial idea since it wouldn't require any changes in the native C code.


    @Ron, I hope it's ok for you that I have added you to this thread. Have you thought about this before? Could you imagine that this would work?


    Regards, Joel



    On Sun, May 4, 2014 at 6:39 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:

      See Ron Pressler's work

      http://cs.oswego.edu/pipermail/concurrency-interest/2013-October/011900.html

      http://cs.oswego.edu/pipermail/concurrency-interest/2014-February/012347.html

      which I think is definitely the way to go. Other schemes - callback,
      future/promise, generator, even C#'s async - are just DSLs trying to
      simulate the threading model, imperfectly. If threads were lightweight
      to begin with, these solutions would not have been invented.

      Zhong Yu




      On Sun, May 4, 2014 at 6:06 AM, Joel Richard <ri.joel at gmail.com> wrote:
      > Hi,
      >
      > Recently I have researched a little about how different languages implement
      > async IO because I wanted to implement a high scalability web server. When I
      > looked at Google Go, I realized that the common callback approach is
      > actually just a language-level workaround to fix a VM limitation.
      >
      > So I wondered whether and how it could be possible to implement something
      > similar to goroutines for the JVM. Here are my thoughts: There should be an
      > ExecutorService implementation which executes tasks in a special mode. If
      > you use any (so far) blocking methods (like Thread.sleep, network/file
      > system IO, Object.wait and synchronization) in such a task it uses however
      > not the blocking native implementation, but an non-blocking version of it.
      > While it waits for the method to complete, it can continue with another task
      > in the same native thread. This means that all tasks can share a small
      > native thread pool. If longer computations have to be done, the developer
      > can either call Thread.yield() from time to time or just move the
      > computation to a classical thread.
      >
      > With such an approach it would be much easier to write highly scalable
      > server software* and it would make it even possible to run most existing
      > libraries with minimal changes as asynchronous IO libraries. Basically, they
      > would just have to make sure that they call Thread.yield() in longer
      > computationally intensive loops and don't start additional normal threads.
      > Last but not least, the behavior of existing threads wouldn't be changed at
      > all.
      >
      > * The advantage of async IO compared to blocking IO is that it doesn?t
      > require expensive thread context switches and that it needs less memory
      > (because fewer threads allocate less memory for stacks).
      >
      > As mentioned above, this would require implementing an asynchronous version
      > of all blocking native methods. I am not that familiar with JNI nor C, but I
      > think this could by achieved by allowing the developer to define a second C
      > function with the _async suffix which calls a callback instead of returning
      > a value. If the Java native method is called in a normal thread, the old C
      > function is called and otherwise the _async version of it. Since Java
      > already supports async IO with NIO, I think it should be technically
      > possible to implement asynchronous versions for almost all of the currently
      > blocking native functions. If an operating system doesn't support certain
      > non-blocking operations, it could just block a special thread instead (as
      > far as I know Go does it similar).
      >
      > I am aware that the required changes would be complicated, but I think it is
      > worth a discussion since concurrency, non-blocking IO and high scalability
      > will be even more important in the future. This feature would make it much
      > easier to fulfill such requirements. For example, it would not be necessary
      > anymore to create a second version of each so far blocking API. Also all
      > standardized APIs like JDBC or JPA would suddenly all have a non-blocking
      > implementation. The amount of work which could be saved would be enormous.
      >
      > What do you think about this idea? Has something like that been discussed
      > before?
      >
      > Regards, Joel
      >
      > PS I sent a similar email first to the jdk9-dev because I wasn't sure which
      > mailing list to use.
      >

      > _______________________________________________
      > Concurrency-interest mailing list
      > Concurrency-interest at cs.oswego.edu
      > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
      >



    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/a04f6441/attachment.html>

From kirk at kodewerk.com  Mon May  5 03:20:12 2014
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Mon, 5 May 2014 09:20:12 +0200
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
	simpler Async IO
In-Reply-To: <3813662F-0C97-4D21-9A14-BF987D25A7C1@wonderly.org>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
	<CACuKZqFoxgPo61VdJ6E4YdXYphvTbDmxfe_uEX92Kw64qOjmAQ@mail.gmail.com>
	<CAOe+kYeUk+okQhrAKPBpymJrXY0PNnOBwFgkj0A-ynHtWY4uLw@mail.gmail.com>
	<3813662F-0C97-4D21-9A14-BF987D25A7C1@wonderly.org>
Message-ID: <9E81873F-87C9-46F1-B80C-1BB272886E46@kodewerk.com>


On May 4, 2014, at 11:37 PM, Gregg Wonderly <gregg at wonderly.org> wrote:

> The original version of Java used a runtime environment which used the term "Green Threads".  This was of course taken out when sun started trying to make Java more than an Applet engine.

I believe the idea for Green Threads stems from lightweight threads in Solaris. There, a lightweight thread ran on top of a single kernel thread. The kernel thread time sliced between each of it?s LWP. I?m not sure it ever worked all that well as CPU?s at the time of inception weren?t quite what they are today and scheduling in Solaris always felt sluggish.
> 
> There are two problems.  First is all the visibility and contention issues which the concurrency package has demonstrated to not be a simple issue due to rather arcane processor architecture issues.
> 
> What we actually need is infinitely fast cache and cache coherency algorithms, and much more sane processor designs which try to allow processors to work together rather than trying desperately to keep them from interfering with each other.

Completely agree, copies are always going to be a problem and immutability can only be part of the answer.

Regards,
Kirk



From ri.joel at gmail.com  Mon May  5 07:55:29 2014
From: ri.joel at gmail.com (Joel Richard)
Date: Mon, 5 May 2014 13:55:29 +0200
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <9E81873F-87C9-46F1-B80C-1BB272886E46@kodewerk.com>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
	<CACuKZqFoxgPo61VdJ6E4YdXYphvTbDmxfe_uEX92Kw64qOjmAQ@mail.gmail.com>
	<CAOe+kYeUk+okQhrAKPBpymJrXY0PNnOBwFgkj0A-ynHtWY4uLw@mail.gmail.com>
	<3813662F-0C97-4D21-9A14-BF987D25A7C1@wonderly.org>
	<9E81873F-87C9-46F1-B80C-1BB272886E46@kodewerk.com>
Message-ID: <CAOe+kYdLejMnKNiNB7DD14ELuJVivpG95uGLcXn3O-1MNM8EJQ@mail.gmail.com>

Yes, my intent is to implement a M:N hybrid threading model; however, I
think it could co-exist with the existing 1:1 model. I would suggest that
the lightweight threads don't switch automatically until they block or call
Thread.yield (similar to Node.js). Therefore, the M:N model would be great
for servers while the 1:1 model might still be better suited for
computational tasks or precise timers. In order that library developers
don't have to know whether their code is running in a native or in a
lightweight thread, it would make sense to create some utilities, e.g. one
which executes computational tasks in a worker thread if the current thread
is just a lightweight thread. Moreover, some methods would change their
behavior (without breaking the API) if they are executed in lightweight
threads. For example, Thread.yield would rather just start/continue
executing another task instead of switching to another thread.

I agree, this would change the performance and even more the scalability
model, but I think that would be something positive and bring a lot of
opportunities - especially for application servers. If I am not mistaken,
the JVM doesn't guarantee a certain performance characteristic or thread
behavior. Therefore and since lightweight threads would be an optional
feature, I think this should be considered as a chance to make Java and the
JVM future proof without re-implementing hundreds of libraries. It would
make async IO and scalable servers easier than with Node.js and prevent
that companies switch from Java to Go (or similar languages) just because
they have a more appealing concurrency model right now.

Regards, Joel


On Mon, May 5, 2014 at 9:20 AM, Kirk Pepperdine <kirk at kodewerk.com> wrote:

>
> On May 4, 2014, at 11:37 PM, Gregg Wonderly <gregg at wonderly.org> wrote:
>
> > The original version of Java used a runtime environment which used the
> term "Green Threads".  This was of course taken out when sun started trying
> to make Java more than an Applet engine.
>
> I believe the idea for Green Threads stems from lightweight threads in
> Solaris. There, a lightweight thread ran on top of a single kernel thread.
> The kernel thread time sliced between each of it?s LWP. I?m not sure it
> ever worked all that well as CPU?s at the time of inception weren?t quite
> what they are today and scheduling in Solaris always felt sluggish.
> >
> > There are two problems.  First is all the visibility and contention
> issues which the concurrency package has demonstrated to not be a simple
> issue due to rather arcane processor architecture issues.
> >
> > What we actually need is infinitely fast cache and cache coherency
> algorithms, and much more sane processor designs which try to allow
> processors to work together rather than trying desperately to keep them
> from interfering with each other.
>
> Completely agree, copies are always going to be a problem and immutability
> can only be part of the answer.
>
> Regards,
> Kirk
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/97a676d9/attachment-0001.html>

From viktor.klang at gmail.com  Mon May  5 08:55:42 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 5 May 2014 14:55:42 +0200
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <CAOe+kYdLejMnKNiNB7DD14ELuJVivpG95uGLcXn3O-1MNM8EJQ@mail.gmail.com>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
	<CACuKZqFoxgPo61VdJ6E4YdXYphvTbDmxfe_uEX92Kw64qOjmAQ@mail.gmail.com>
	<CAOe+kYeUk+okQhrAKPBpymJrXY0PNnOBwFgkj0A-ynHtWY4uLw@mail.gmail.com>
	<3813662F-0C97-4D21-9A14-BF987D25A7C1@wonderly.org>
	<9E81873F-87C9-46F1-B80C-1BB272886E46@kodewerk.com>
	<CAOe+kYdLejMnKNiNB7DD14ELuJVivpG95uGLcXn3O-1MNM8EJQ@mail.gmail.com>
Message-ID: <CANPzfU9kuWQoJQ0xC7YvsDG7dvNR0e1fbrF_g4rMUien85dAHw@mail.gmail.com>

Hi Joel,

In case you haven't already, I recommend to have a look at Scheduler
Activations: http://en.wikipedia.org/wiki/Scheduler_activations


On Mon, May 5, 2014 at 1:55 PM, Joel Richard <ri.joel at gmail.com> wrote:

> Yes, my intent is to implement a M:N hybrid threading model; however, I
> think it could co-exist with the existing 1:1 model. I would suggest that
> the lightweight threads don't switch automatically until they block or call
> Thread.yield (similar to Node.js). Therefore, the M:N model would be great
> for servers while the 1:1 model might still be better suited for
> computational tasks or precise timers. In order that library developers
> don't have to know whether their code is running in a native or in a
> lightweight thread, it would make sense to create some utilities, e.g. one
> which executes computational tasks in a worker thread if the current thread
> is just a lightweight thread. Moreover, some methods would change their
> behavior (without breaking the API) if they are executed in lightweight
> threads. For example, Thread.yield would rather just start/continue
> executing another task instead of switching to another thread.
>
> I agree, this would change the performance and even more the scalability
> model, but I think that would be something positive and bring a lot of
> opportunities - especially for application servers. If I am not mistaken,
> the JVM doesn't guarantee a certain performance characteristic or thread
> behavior. Therefore and since lightweight threads would be an optional
> feature, I think this should be considered as a chance to make Java and the
> JVM future proof without re-implementing hundreds of libraries. It would
> make async IO and scalable servers easier than with Node.js and prevent
> that companies switch from Java to Go (or similar languages) just because
> they have a more appealing concurrency model right now.
>
> Regards, Joel
>
>
> On Mon, May 5, 2014 at 9:20 AM, Kirk Pepperdine <kirk at kodewerk.com> wrote:
>
>>
>> On May 4, 2014, at 11:37 PM, Gregg Wonderly <gregg at wonderly.org> wrote:
>>
>> > The original version of Java used a runtime environment which used the
>> term "Green Threads".  This was of course taken out when sun started trying
>> to make Java more than an Applet engine.
>>
>> I believe the idea for Green Threads stems from lightweight threads in
>> Solaris. There, a lightweight thread ran on top of a single kernel thread.
>> The kernel thread time sliced between each of it?s LWP. I?m not sure it
>> ever worked all that well as CPU?s at the time of inception weren?t quite
>> what they are today and scheduling in Solaris always felt sluggish.
>> >
>> > There are two problems.  First is all the visibility and contention
>> issues which the concurrency package has demonstrated to not be a simple
>> issue due to rather arcane processor architecture issues.
>> >
>> > What we actually need is infinitely fast cache and cache coherency
>> algorithms, and much more sane processor designs which try to allow
>> processors to work together rather than trying desperately to keep them
>> from interfering with each other.
>>
>> Completely agree, copies are always going to be a problem and
>> immutability can only be part of the answer.
>>
>> Regards,
>> Kirk
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/a49ffa77/attachment.html>

From valentin.male.kovalenko at gmail.com  Mon May  5 11:51:34 2014
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 5 May 2014 19:51:34 +0400
Subject: [concurrency-interest] back again to final fields semantics
Message-ID: <CAO-wXw+W7EuxBnqrc2Q9-WtvVha4R376Q4wbRuO7zGZwBJ1P6w@mail.gmail.com>

Consider the following simplest example where final fields semantics works:
class A {
   final int fx;
   A() {fx = 42;} // [w]
}
A a; //shared

//T1:
a = new A();

//T2:
A localA = a; // read a via data race
if(localA != null) {
        print(localA.fx); // [r]
} else {
        print("none");
}

We all know final semantics guarantees that in this example the only
possible outcomes are "42" and "none"; "0" (default value) is forbidden.

"JLS 17.5.1. Semantics of final Fields" introduces a new partial order hb*
(while refers to it as normal hb), which "does not transitively close with
other happens-before orderings" (this means that the introduced hb* is
definitely not the same hb defined in "JLS 17.4.5. Happens-before Order"
because partial order is transitive by definition).

We can formally show that there is a hb*(w, r) in the example above - no
questions here. But what conclusions can we derive from this fact? "JLS
17.5.1" specifies that hb*(w, r) is useful "when determining which values
can be seen by r", and that's all.

Here are the questions:
(Q1) How exactly it's supposed to "determine" values that can be seen by r?
(Q2) How can we formally show that final fields semantics forbids to see
the default value?

Please provide your answers to the questions Q1 and Q2
and/or confirm/refute the vision of the answers:


Let's assume that visibility rules for hb* are the same as for hb (i.e. in
the following statements we can use hb* and hb interchangeably):
- JLS only considers well-formed executions ("JLS 17.4.7. Well-Formed
Executions")
- well-formed execution is happens-before consistent ("JLS 17.4.7.
Well-Formed Executions"), which means that its set of actions is
happens-before consistent ("JLS 17.4.6. Executions")
-(1) in a happens-before consistent set of actions, each read sees a write
that it is allowed to see by the happens-before ordering ("JLS 17.4.5.
Happens-before Order")
- a read r of a variable fx is allowed to observe a write w to fx if, in
the happens-before partial order of the execution trace ("JLS 17.4.5.
Happens-before Order"):
--- r is not ordered before w (i.e., it is not the case that hb(r, w)), and
--- there is no intervening write w' to fx (i.e. no write w' to fx such
that hb(w, w') and hb(w', r))

In our example we have the following orderings:
(a) hb(wDefault, w) // because "the write of the default value (zero,
false, or null) to each variable synchronizes-with the first action in
EVERY thread" ("JLS 17.4.4. Synchronization Order"), so we have
so(wDefault, firstAction_T1), po(firstAction_T1, w) and hence hb(wDefault,
w)
(b) hb*(w, r) //because of final semantics
(c) hb(wDefault, r) // similarly to hb(wDefault, w), because so(wDefault,
firstAction_T2), po(firstAction_T2, r) and hence hb(wDefault, r)

Let's try to answer Q2.
According to the mentioned visibility rules r is not hb-ordered before
wDefault because of (c), but if w is an intervening write to fx then r
isn't allowed to observe wDefault. In order w to be an intervening write we
need hb(wDefault, w) and hb(w, r), but because we have assumed that we can
use hb* and hb interchangeably in visibility rules we should think that
hb(wDefault, w) and hb*(w, r) also means that w is an intervening write and
therefore r isn't allowed to observe wDefault.

Let's try to answer Q1.
According to the mentioned visibility rules r is not hb*-ordered before w
because of (b), and there is no intervening write to fx such that hb(w, w')
and hb(w', r) because the only candidate for such a write is wDefault which
isn't hb(w, wDefault) because of (a). So r is hb-ordered after w (actually
hb*-ordered, but remember the assumption is that we can use hb and hb*
interchangeably in the visibility rules) and there is no intervening write,
therefore r is allowed to see w and hence it sees w because of (1).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/d8aea5e2/attachment.html>

From discus at kotek.net  Mon May  5 11:58:51 2014
From: discus at kotek.net (Jan Kotek)
Date: Mon, 05 May 2014 18:58:51 +0300
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
	simpler Async IO
In-Reply-To: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
Message-ID: <2415474.gMg0GqRVoV@artemis>

For networking there are many projects for Async IO and lightweight threads. 
Checkout Akka, Verte.x, Actors...

I hope to implement async database one day. But for disk io there  is only 
AsynchronousFileChannel which has too much overhead.

Jan

On Sunday, May 04, 2014 13:06:22 Joel Richard wrote:


Hi,


Recently I have researched a little about how different languages implement async 
IO because I wanted to implement a high scalability web server. When I looked at 
Google Go, I realized that the common callback approach is actually just a language-
level workaround to fix a VM limitation. 


So I wondered whether and how it could be possible to implement something 
similar to goroutines for the JVM. Here are my thoughts: There should be an 
ExecutorService implementation which executes tasks in a special mode. If you use 
any (so far) blocking methods (like Thread.sleep, network/file system IO, 
Object.wait and synchronization) in such a task it uses however not the blocking 
native implementation, but an non-blocking version of it. While it waits for the 
method to complete, it can continue with another task in the same native thread. 
This means that all tasks can share a small native thread pool. If longer 
computations have to be done, the developer can either call Thread.yield() from 
time to time or just move the computation to a classical thread. 


With such an approach it would be much easier to write highly scalable server 
software* and it would make it even possible to run most existing libraries with 
minimal changes as asynchronous IO libraries. Basically, they would just have to 
make sure that they call Thread.yield() in longer computationally intensive loops and 
don't start additional normal threads. Last but not least, the behavior of existing 
threads wouldn't be changed at all. 


* The advantage of async IO compared to blocking IO is that it doesn?t require 
expensive thread context switches and that it needs less memory (because fewer 
threads allocate less memory for stacks). 


As mentioned above, this would require implementing an asynchronous version of 
all blocking native methods. I am not that familiar with JNI nor C, but I think this 
could by achieved by allowing the developer to define a second C function with the 
_async suffix which calls a callback instead of returning a value. If the Java native 
method is called in a normal thread, the old C function is called and otherwise the 
_async version of it. Since Java already supports async IO with NIO, I think it should 
be technically possible to implement asynchronous versions for almost all of the 
currently blocking native functions. If an operating system doesn't support certain 
non-blocking operations, it could just block a special thread instead (as far as I 
know Go does it similar). 


I am aware that the required changes would be complicated, but I think it is worth a 
discussion since concurrency, non-blocking IO and high scalability will be even more 
important in the future. This feature would make it much easier to fulfill such 
requirements. For example, it would not be necessary anymore to create a second 
version of each so far blocking API. Also all standardized APIs like JDBC or JPA would 
suddenly all have a non-blocking implementation. The amount of work which could 
be saved would be enormous. 


What do you think about this idea? Has something like that been discussed before?


Regards, Joel


PS I sent a similar email first to the jdk9-dev because I wasn't sure which mailing list 
to use. 


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/31681f45/attachment-0001.html>

From thurston at nomagicsoftware.com  Mon May  5 12:59:18 2014
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 5 May 2014 09:59:18 -0700 (PDT)
Subject: [concurrency-interest] JMM and reorderings
Message-ID: <1399309158454-10973.post@n7.nabble.com>

Given the following:

Initially, x == y == 0
Thread 1       Thread 2
r1 = x            r3 = y
y = 5             x = 5
r2 = x

Thinking about legal outcomes under the JMM:
r1 == r2 == r3 == 0 is a well-known legal outcome given allowed
reorderings/optimizations


But what about the following?
r2 == 0, r1 == 5
Which is equivalent to allowing:
r2 = x
r1 = x
y = 5

I want to say that that is a violation of intra-thread conistency guarantees
(or intra-thread semantics if you prefer), but it's not clear to me where
specifically the JMM would prohibit that.







--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/JMM-and-reorderings-tp10973.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From boehm at acm.org  Mon May  5 14:12:38 2014
From: boehm at acm.org (Hans Boehm)
Date: Mon, 5 May 2014 11:12:38 -0700
Subject: [concurrency-interest] JMM and reorderings
In-Reply-To: <1399309158454-10973.post@n7.nabble.com>
References: <1399309158454-10973.post@n7.nabble.com>
Message-ID: <CAPUmR1bqLSryRkTjxnkZCQtSHZ=n3zFoe2o9xWtfZZFhXfNm7Q@mail.gmail.com>

On Mon, May 5, 2014 at 9:59 AM, thurstonn <thurston at nomagicsoftware.com>wrote:

> Given the following:
>
> Initially, x == y == 0
> Thread 1       Thread 2
> r1 = x            r3 = y
> y = 5             x = 5
> r2 = x
>
> Thinking about legal outcomes under the JMM:
> r1 == r2 == r3 == 0 is a well-known legal outcome given allowed
> reorderings/optimizations
>
> Indeed, that is not a very interesting outcome, since it is the result of
a sequentially consistent/interleaving-based execution:

r3 = y; r1 = x; y = 5; r2 = x; x = 5;

Did you have something else in mind?


>
> But what about the following?
> r2 == 0, r1 == 5
> Which is equivalent to allowing:
> r2 = x
> r1 = x
> y = 5
>
> I want to say that that is a violation of intra-thread conistency
> guarantees
> (or intra-thread semantics if you prefer), but it's not clear to me where
> specifically the JMM would prohibit that.
>
>
Assuming nothing is declared as volatile, that is clearly allowed.  This
kind of thing is likely to happen if Thread 1 really looks like

r17 = a.x;
r1 = b.x;
y = 5;
r2 = a.x;

and a and b are really references to the same object.  The compiler is
likely to reuse the load to r17 to compute r2, effectively reversing the
loads into r1 and r2.

There are also several kinds of hardware that allow reordering of loads
from the same location.

Racing accesses to ordinary data have surprising and sometimes poorly
defined semantics in Java ...

Hans
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/7b7996dd/attachment.html>

From thurston at nomagicsoftware.com  Mon May  5 15:02:27 2014
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 5 May 2014 12:02:27 -0700 (PDT)
Subject: [concurrency-interest] JMM and reorderings
In-Reply-To: <CAPUmR1bqLSryRkTjxnkZCQtSHZ=n3zFoe2o9xWtfZZFhXfNm7Q@mail.gmail.com>
References: <1399309158454-10973.post@n7.nabble.com>
	<CAPUmR1bqLSryRkTjxnkZCQtSHZ=n3zFoe2o9xWtfZZFhXfNm7Q@mail.gmail.com>
Message-ID: <1399316547090-10975.post@n7.nabble.com>

Hans Boehm wrote
> 
> Did you have something else in mind?

Grrr, yes - the code should have been:
Initially, x == y == 0
Thread 1       Thread 2
r1 = x            x = 5
y = 5             r3 = y
r2 = x

But that's not the interesting part (r1 == r2 == r3 == 0)
>
> But what about the following?
> r2 == 0, r1 == 5
> Which is equivalent to allowing:
> r2 = x
> r1 = x
> y = 5
>
> I want to say that that is a violation of intra-thread conistency
> guarantees
> (or intra-thread semantics if you prefer), but it's not clear to me where
> specifically the JMM would prohibit that.
>
>
Assuming nothing is declared as volatile, that is clearly allowed.  This
kind of thing is likely to happen if Thread 1 really looks like

r17 = a.x;
r1 = b.x;
y = 5;
r2 = a.x;

and a and b are really references to the same object.  The compiler is
likely to reuse the load to r17 to compute r2, effectively reversing the
loads into r1 and r2.

There are also several kinds of hardware that allow reordering of loads
from the same location.

Racing accesses to ordinary data have surprising and sometimes poorly
defined semantics in Java ...

Hans


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at .oswego
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


That's surprising, i.e. that r2 = 0, r1 = 5 is allowed
I guess my thinking is that this violates the *intra-thread* semantics of
the Thread 1 code, viz.

program order is dictating that r1 can never "see a later value of x" than
r2 - ok that's not very formal I guess, but it seems to violate the
intra-thread guarantees;
yes, there is no happens-before and there is no explicit dependency between
r1 and r2 . . . so all bets are off?





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/JMM-and-reorderings-tp10973p10975.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From ri.joel at gmail.com  Mon May  5 15:42:11 2014
From: ri.joel at gmail.com (Joel Richard)
Date: Mon, 5 May 2014 21:42:11 +0200
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <2415474.gMg0GqRVoV@artemis>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
	<2415474.gMg0GqRVoV@artemis>
Message-ID: <CAOe+kYekRrQ4+DPs=Q1rSDu-uqB1LA5wbBPjBxOsWdnj2wvGww@mail.gmail.com>

My idea goes far behind such frameworks. First of all, it will provide a
new threading model and secondly, it should enable to run existing blocking
IO libraries asynchronously without changing code. Unfortunately, none of
the mentioned frameworks can achieve that. Right now I am thinking about
whether it would be possible to implement something with Java agents and
instrumentation, but I fear that the performance wouldn't be the same as if
it were implemented natively.


On Mon, May 5, 2014 at 5:58 PM, Jan Kotek <discus at kotek.net> wrote:

>  For networking there are many projects for Async IO and lightweight
> threads. Checkout Akka, Verte.x, Actors...
>
>
>
> I hope to implement async database one day. But for disk io there is only
> AsynchronousFileChannel which has too much overhead.
>
>
>
> Jan
>
>
>
> On Sunday, May 04, 2014 13:06:22 Joel Richard wrote:
>
> Hi,
>
>
> Recently I have researched a little about how different languages
> implement async IO because I wanted to implement a high scalability web
> server. When I looked at Google Go, I realized that the common callback
> approach is actually just a language-level workaround to fix a VM
> limitation.
>
>
> So I wondered whether and how it could be possible to implement something
> similar to goroutines for the JVM. Here are my thoughts: There should be an
> ExecutorService implementation which executes tasks in a special mode. If
> you use any (so far) blocking methods (like Thread.sleep, network/file
> system IO, Object.wait and synchronization) in such a task it uses however
> not the blocking native implementation, but an non-blocking version of it.
> While it waits for the method to complete, it can continue with another
> task in the same native thread. This means that all tasks can share a small
> native thread pool. If longer computations have to be done, the developer
> can either call Thread.yield() from time to time or just move the
> computation to a classical thread.
>
>
> With such an approach it would be much easier to write highly scalable
> server software* and it would make it even possible to run most existing
> libraries with minimal changes as asynchronous IO libraries. Basically,
> they would just have to make sure that they call Thread.yield() in longer
> computationally intensive loops and don't start additional normal threads.
> Last but not least, the behavior of existing threads wouldn't be changed at
> all.
>
>
> * The advantage of async IO compared to blocking IO is that it doesn?t
> require expensive thread context switches and that it needs less memory
> (because fewer threads allocate less memory for stacks).
>
>
> As mentioned above, this would require implementing an asynchronous
> version of all blocking native methods. I am not that familiar with JNI nor
> C, but I think this could by achieved by allowing the developer to define a
> second C function with the _async suffix which calls a callback instead of
> returning a value. If the Java native method is called in a normal thread,
> the old C function is called and otherwise the _async version of it. Since
> Java already supports async IO with NIO, I think it should be technically
> possible to implement asynchronous versions for almost all of the currently
> blocking native functions. If an operating system doesn't support certain
> non-blocking operations, it could just block a special thread instead (as
> far as I know Go does it similar).
>
>
> I am aware that the required changes would be complicated, but I think it
> is worth a discussion since concurrency, non-blocking IO and high
> scalability will be even more important in the future. This feature would
> make it much easier to fulfill such requirements. For example, it would not
> be necessary anymore to create a second version of each so far blocking
> API. Also all standardized APIs like JDBC or JPA would suddenly all have a
> non-blocking implementation. The amount of work which could be saved would
> be enormous.
>
>
> What do you think about this idea? Has something like that been discussed
> before?
>
>
> Regards, Joel
>
>
> PS I sent a similar email first to the jdk9-dev because I wasn't sure
> which mailing list to use.
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/273fe5fd/attachment.html>

From email at christian-fries.de  Mon May  5 15:58:44 2014
From: email at christian-fries.de (Christian Fries)
Date: Mon, 5 May 2014 21:58:44 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested Java 8
	streams.parallel().forEach( ... )
Message-ID: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>



Dear All.

I am new to this list, so please excuse, if this is not the place for the following topic (I did a bug report to Oracle and also posted this to Stackoverflow at http://stackoverflow.com/questions/23442183 - and it was suggested (at SO) that I post this here too).

Java 8 introduced parallel streams which use a common ForkJoinPool. I believe that the implementation of ForkJoinTask has a problem, which becomes relevant in a situation, which is likely to occur if Java 8 parallel streams are used a lot. Specifically the problem arises if one uses a nested Java 8 parallel stream foreach, e.g. as in

	// Outer loop
	IntStream.range(0,numberOfTasksInOuterLoop).parallel().forEach(i -> {
		// Inner loop
		IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
			// Work done here loop
		});

	});


In this situation the inner loop may be started on a workerThread of the common ForkJoinPool.

Now note that the implementation of ForkJoinTask check if the task is run on the parent thread (current thread) or on a forked worked thread via the line

((t = Thread.currentThread()) instanceof ForkJoinWorkerThread)

Hoever, for the case of the nested loop, it is not sufficient if the thread is ?instanceof ForkJoinWorkerThread?, because the parent thread itself was a ForkJoinWorkerThread. Further in the implementation of doInvoke (see http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/6be37bafb11a/src/share/classes/java/util/concurrent/ForkJoinTask.java#l393 ) this implied that we call awaitJoin on the workQueue - I believe this is wrong. Now, that workQueue belongs to the outer loop and this may result in a DEADLOCK.

For details see http://stackoverflow.com/questions/23442183/using-a-semaphore-inside-a-java-8-parallel-stream-action-may-deadlock-is-this-a
For a detailed test code run http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/ForkJoinPoolTest.java

That said, it appears as if the current Java implementation of ForkJoinPool was written without having ?Nesting? in mind (submitting tasks from a worker tasks to a common pool), but Java 8 streams makes that kind of nesting a natural thing.

I believe I know the fix for the problem (we have to store the parent thread and check if currentThread() equals the parentThread of a ForkJoinTask - in case you like me to get involved.

Please excuse if this is not the right mailing list for this. 

Best
Christian



________________________________________________
http://www.christian-fries.de






From ri.joel at gmail.com  Mon May  5 16:06:07 2014
From: ri.joel at gmail.com (Joel Richard)
Date: Mon, 5 May 2014 22:06:07 +0200
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <CANPzfU9kuWQoJQ0xC7YvsDG7dvNR0e1fbrF_g4rMUien85dAHw@mail.gmail.com>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
	<CACuKZqFoxgPo61VdJ6E4YdXYphvTbDmxfe_uEX92Kw64qOjmAQ@mail.gmail.com>
	<CAOe+kYeUk+okQhrAKPBpymJrXY0PNnOBwFgkj0A-ynHtWY4uLw@mail.gmail.com>
	<3813662F-0C97-4D21-9A14-BF987D25A7C1@wonderly.org>
	<9E81873F-87C9-46F1-B80C-1BB272886E46@kodewerk.com>
	<CAOe+kYdLejMnKNiNB7DD14ELuJVivpG95uGLcXn3O-1MNM8EJQ@mail.gmail.com>
	<CANPzfU9kuWQoJQ0xC7YvsDG7dvNR0e1fbrF_g4rMUien85dAHw@mail.gmail.com>
Message-ID: <CAOe+kYeb4zUgMMUxT0QGEZ7hdk+XUzw0-+iFam8uK_RKqJJqUA@mail.gmail.com>

Hi Viktor,

Thank you for the hint. The following article contains even more
interesting information: https://en.wikipedia.org/wiki/Thread_(computing)

With the terminology from there, I am just proposing to implement
coroutines in Java and execute all so far blocking calls asynchronously
(but only when executed in coroutines). That sounds already much better :-)

Regards, Joel

On Mon, May 5, 2014 at 2:55 PM, ?iktor ?lang <viktor.klang at gmail.com> wrote:

> Hi Joel,
>
> In case you haven't already, I recommend to have a look at Scheduler
> Activations: http://en.wikipedia.org/wiki/Scheduler_activations
>
>
> On Mon, May 5, 2014 at 1:55 PM, Joel Richard <ri.joel at gmail.com> wrote:
>
>> Yes, my intent is to implement a M:N hybrid threading model; however, I
>> think it could co-exist with the existing 1:1 model. I would suggest that
>> the lightweight threads don't switch automatically until they block or call
>> Thread.yield (similar to Node.js). Therefore, the M:N model would be great
>> for servers while the 1:1 model might still be better suited for
>> computational tasks or precise timers. In order that library developers
>> don't have to know whether their code is running in a native or in a
>> lightweight thread, it would make sense to create some utilities, e.g. one
>> which executes computational tasks in a worker thread if the current thread
>> is just a lightweight thread. Moreover, some methods would change their
>> behavior (without breaking the API) if they are executed in lightweight
>> threads. For example, Thread.yield would rather just start/continue
>> executing another task instead of switching to another thread.
>>
>> I agree, this would change the performance and even more the scalability
>> model, but I think that would be something positive and bring a lot of
>> opportunities - especially for application servers. If I am not mistaken,
>> the JVM doesn't guarantee a certain performance characteristic or thread
>> behavior. Therefore and since lightweight threads would be an optional
>> feature, I think this should be considered as a chance to make Java and the
>> JVM future proof without re-implementing hundreds of libraries. It would
>> make async IO and scalable servers easier than with Node.js and prevent
>> that companies switch from Java to Go (or similar languages) just because
>> they have a more appealing concurrency model right now.
>>
>> Regards, Joel
>>
>>
>> On Mon, May 5, 2014 at 9:20 AM, Kirk Pepperdine <kirk at kodewerk.com>wrote:
>>
>>>
>>> On May 4, 2014, at 11:37 PM, Gregg Wonderly <gregg at wonderly.org> wrote:
>>>
>>> > The original version of Java used a runtime environment which used the
>>> term "Green Threads".  This was of course taken out when sun started trying
>>> to make Java more than an Applet engine.
>>>
>>> I believe the idea for Green Threads stems from lightweight threads in
>>> Solaris. There, a lightweight thread ran on top of a single kernel thread.
>>> The kernel thread time sliced between each of it?s LWP. I?m not sure it
>>> ever worked all that well as CPU?s at the time of inception weren?t quite
>>> what they are today and scheduling in Solaris always felt sluggish.
>>> >
>>> > There are two problems.  First is all the visibility and contention
>>> issues which the concurrency package has demonstrated to not be a simple
>>> issue due to rather arcane processor architecture issues.
>>> >
>>> > What we actually need is infinitely fast cache and cache coherency
>>> algorithms, and much more sane processor designs which try to allow
>>> processors to work together rather than trying desperately to keep them
>>> from interfering with each other.
>>>
>>> Completely agree, copies are always going to be a problem and
>>> immutability can only be part of the answer.
>>>
>>> Regards,
>>> Kirk
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Cheers,
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/3c4aa155/attachment.html>

From davidcholmes at aapt.net.au  Mon May  5 16:08:36 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 6 May 2014 06:08:36 +1000
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
	simpler Async IO
In-Reply-To: <CAOe+kYdLejMnKNiNB7DD14ELuJVivpG95uGLcXn3O-1MNM8EJQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEDDKGAA.davidcholmes@aapt.net.au>

Joel Richard writes:
> 
> Yes, my intent is to implement a M:N hybrid threading model; 
> however, I think it could co-exist with the existing 1:1 model. I 
> would suggest that the lightweight threads don't switch 
> automatically until they block or call Thread.yield (similar to 
> Node.js). Therefore, the M:N model would be great for servers 
> while the 1:1 model might still be better suited for 
> computational tasks or precise timers. In order that library 
> developers don't have to know whether their code is running in a 
> native or in a lightweight thread, it would make sense to create 
> some utilities, e.g. one which executes computational tasks in a 
> worker thread if the current thread is just a lightweight thread. 
> Moreover, some methods would change their behavior (without 
> breaking the API) if they are executed in lightweight threads. 
> For example, Thread.yield would rather just start/continue 
> executing another task instead of switching to another thread.

Conceptually you may be able to have a system where a M:N scheduler co-exists with a 1:1 scheduler; but I do not think you can simply add that to hotspot without completely rewriting the existing code. Further, trying to generalize a M:N model to apply to all threads is extremely difficult - how exactly do you pick up another thread and when can you return to the original? Specialized approaches, like ForkJoin, define the computational chunks so that it is easy to do this. There are also issues with thread identity that would have to be handled somehow.
 
> 
> I agree, this would change the performance and even more the 
> scalability model, but I think that would be something positive 

Can you back that up with any numbers? "Words are wind ..." ;-)

> and bring a lot of opportunities - especially for application 
> servers. If I am not mistaken, the JVM doesn't guarantee a 
> certain performance characteristic or thread behavior.

It may not guarantee it but we go to a lot of trouble to try and not introduce discontinuities in the performance curves. People invest a lot of time and effort in tuning large apps, and in designing them for the way things actually work. You can't completely change the threading and performance model for these apps.

> and since lightweight threads would be an optional feature, I 
> think this should be considered as a chance to make Java and the 
> JVM future proof without re-implementing hundreds of libraries. 

Are you saying that the VM is either started in traditional mode, or else this new mode? That would address the compatability issue - provided you can implement the new mode without changing the existing performance model.

David
-----

> It would make async IO and scalable servers easier than with 
> Node.js and prevent that companies switch from Java to Go (or 
> similar languages) just because they have a more appealing 
> concurrency model right now.
> 
> 
> Regards, Joel
> 
> 
> 
> On Mon, May 5, 2014 at 9:20 AM, Kirk Pepperdine <kirk at kodewerk.com> wrote:
> 
> 
> On May 4, 2014, at 11:37 PM, Gregg Wonderly <gregg at wonderly.org> wrote:
> 
> > The original version of Java used a runtime environment which 
> used the term "Green Threads".  This was of course taken out when 
> sun started trying to make Java more than an Applet engine.
> 
> 
> I believe the idea for Green Threads stems from lightweight 
> threads in Solaris. There, a lightweight thread ran on top of a 
> single kernel thread. The kernel thread time sliced between each 
> of it?s LWP. I?m not sure it ever worked all that well as CPU?s 
> at the time of inception weren?t quite what they are today and 
> scheduling in Solaris always felt sluggish.
> 
> >
> > There are two problems.  First is all the visibility and 
> contention issues which the concurrency package has demonstrated 
> to not be a simple issue due to rather arcane processor 
> architecture issues.
> >
> > What we actually need is infinitely fast cache and cache 
> coherency algorithms, and much more sane processor designs which 
> try to allow processors to work together rather than trying 
> desperately to keep them from interfering with each other.
> 
> 
> Completely agree, copies are always going to be a problem and 
> immutability can only be part of the answer.
> 
> Regards,
> Kirk
> 



From boehm at acm.org  Mon May  5 16:12:21 2014
From: boehm at acm.org (Hans Boehm)
Date: Mon, 5 May 2014 13:12:21 -0700
Subject: [concurrency-interest] JMM and reorderings
In-Reply-To: <1399316547090-10975.post@n7.nabble.com>
References: <1399309158454-10973.post@n7.nabble.com>
	<CAPUmR1bqLSryRkTjxnkZCQtSHZ=n3zFoe2o9xWtfZZFhXfNm7Q@mail.gmail.com>
	<1399316547090-10975.post@n7.nabble.com>
Message-ID: <CAPUmR1aey6NcyAgGHNZbocmgPTOMoki3CZMPsXJ6nxUBE_28kQ@mail.gmail.com>

On Mon, May 5, 2014 at 12:02 PM, thurstonn <thurston at nomagicsoftware.com>wrote:

> Hans Boehm wrote
> >
> > Did you have something else in mind?
>
> Grrr, yes - the code should have been:
> Initially, x == y == 0
> Thread 1       Thread 2
> r1 = x            x = 5
> y = 5             r3 = y
> r2 = x
>
> But that's not the interesting part (r1 == r2 == r3 == 0)
> >
> > But what about the following?
> > r2 == 0, r1 == 5
> > Which is equivalent to allowing:
> > r2 = x
> > r1 = x
> > y = 5
> >
> > I want to say that that is a violation of intra-thread conistency
> > guarantees
> > (or intra-thread semantics if you prefer), but it's not clear to me where
> > specifically the JMM would prohibit that.
> >
> >
> Assuming nothing is declared as volatile, that is clearly allowed.  This
> kind of thing is likely to happen if Thread 1 really looks like
>
> r17 = a.x;
> r1 = b.x;
> y = 5;
> r2 = a.x;
>
> and a and b are really references to the same object.  The compiler is
> likely to reuse the load to r17 to compute r2, effectively reversing the
> loads into r1 and r2.
>
> There are also several kinds of hardware that allow reordering of loads
> from the same location.
>
> Racing accesses to ordinary data have surprising and sometimes poorly
> defined semantics in Java ...
>
> Hans
>
> That's surprising, i.e. that r2 = 0, r1 = 5 is allowed
> I guess my thinking is that this violates the *intra-thread* semantics of
> the Thread 1 code, viz.
>
> program order is dictating that r1 can never "see a later value of x" than
> r2 - ok that's not very formal I guess, but it seems to violate the
> intra-thread guarantees;
> yes, there is no happens-before and there is no explicit dependency between
> r1 and r2 . . . so all bets are off?
>
>
> Yes.  All bets are off.  And the alternative would disable some common
compiler optimizations and would require enforcement overhead on certain
kinds of hardware.

Hans
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/ecd0685e/attachment.html>

From davidcholmes at aapt.net.au  Mon May  5 16:15:52 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 6 May 2014 06:15:52 +1000
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8streams.parallel().forEach( ... )
In-Reply-To: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEDDKGAA.davidcholmes@aapt.net.au>

Christian,

You can not use arbitrary synchronization devices within a ForkJoinPool.
Such devices require/assume that you have control over the threads involved
in a computation which is not the case with FJ. The FJP docs state:

"A ForkJoinPool is constructed with a given target parallelism level; by
default, equal to the number of available processors. The pool attempts to
maintain enough active (or available) threads by dynamically adding,
suspending, or resuming internal worker threads, even if some tasks are
stalled waiting to join others. However, no such adjustments are guaranteed
in the face of blocked IO or other unmanaged synchronization. The nested
ForkJoinPool.ManagedBlocker interface enables extension of the kinds of
synchronization accommodated."

Phaser has been written to work with FJP but the other synchronization
constructs have not.

David Holmes
------------

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> Christian Fries
> Sent: Tuesday, 6 May 2014 5:59 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] ForkJoinPool not designed for nested
> Java 8streams.parallel().forEach( ... )
>
>
>
>
> Dear All.
>
> I am new to this list, so please excuse, if this is not the place
> for the following topic (I did a bug report to Oracle and also
> posted this to Stackoverflow at
> http://stackoverflow.com/questions/23442183 - and it was
> suggested (at SO) that I post this here too).
>
> Java 8 introduced parallel streams which use a common
> ForkJoinPool. I believe that the implementation of ForkJoinTask
> has a problem, which becomes relevant in a situation, which is
> likely to occur if Java 8 parallel streams are used a lot.
> Specifically the problem arises if one uses a nested Java 8
> parallel stream foreach, e.g. as in
>
> 	// Outer loop
>
> IntStream.range(0,numberOfTasksInOuterLoop).parallel().forEach(i -> {
> 		// Inner loop
>
> IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
> 			// Work done here loop
> 		});
>
> 	});
>
>
> In this situation the inner loop may be started on a workerThread
> of the common ForkJoinPool.
>
> Now note that the implementation of ForkJoinTask check if the
> task is run on the parent thread (current thread) or on a forked
> worked thread via the line
>
> ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread)
>
> Hoever, for the case of the nested loop, it is not sufficient if
> the thread is ?instanceof ForkJoinWorkerThread?, because the
> parent thread itself was a ForkJoinWorkerThread. Further in the
> implementation of doInvoke (see
> http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/6be37bafb11a/src/s
hare/classes/java/util/concurrent/ForkJoinTask.java#l393 ) this implied that
we call awaitJoin on the workQueue - I believe this is wrong. Now, that
workQueue belongs to the outer loop and this may result in a DEADLOCK.

For details see
http://stackoverflow.com/questions/23442183/using-a-semaphore-inside-a-java-
8-parallel-stream-action-may-deadlock-is-this-a
For a detailed test code run
http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experimen
ts/concurrency/ForkJoinPoolTest.java

That said, it appears as if the current Java implementation of ForkJoinPool
was written without having ?Nesting? in mind (submitting tasks from a worker
tasks to a common pool), but Java 8 streams makes that kind of nesting a
natural thing.

I believe I know the fix for the problem (we have to store the parent thread
and check if currentThread() equals the parentThread of a ForkJoinTask - in
case you like me to get involved.

Please excuse if this is not the right mailing list for this.

Best
Christian



________________________________________________
http://www.christian-fries.de





_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From email at christian-fries.de  Mon May  5 16:40:26 2014
From: email at christian-fries.de (Christian Fries)
Date: Mon, 5 May 2014 22:40:26 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8streams.parallel().forEach( ... )
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEDDKGAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOEDDKGAA.davidcholmes@aapt.net.au>
Message-ID: <6F1AEC0A-5273-42A1-9F90-C2787EC8D45D@christian-fries.de>


Dear David.

Thank you! I do understand that the use of that Semaphore is evil - I will refrain from it.

That said, just consider the Semaphore in my demo code as the litmus of a litmus test. (I am using that semaphore just to show that we have an pool.awaitJoin on the wrong thread).

It appears to me as if nested Java 8 parallel streams will create a very subtle ?implicit coupling? between the tasks submitted to the common pool, because tasks of an inner loop are joined with tasks on an outer loop.

So maybe the problem is to have a common FJP at all?

I still have the impression that the line 
	((t = Thread.currentThread()) instanceof ForkJoinWorkerThread)
is not correct in the case of a nested loop.

Thank you for your attention.

Best
Christian


Am 05.05.2014 um 22:15 schrieb David Holmes <davidcholmes at aapt.net.au>:

> Christian,
> 
> You can not use arbitrary synchronization devices within a ForkJoinPool.
> Such devices require/assume that you have control over the threads involved
> in a computation which is not the case with FJ. The FJP docs state:
> 
> "A ForkJoinPool is constructed with a given target parallelism level; by
> default, equal to the number of available processors. The pool attempts to
> maintain enough active (or available) threads by dynamically adding,
> suspending, or resuming internal worker threads, even if some tasks are
> stalled waiting to join others. However, no such adjustments are guaranteed
> in the face of blocked IO or other unmanaged synchronization. The nested
> ForkJoinPool.ManagedBlocker interface enables extension of the kinds of
> synchronization accommodated."
> 
> Phaser has been written to work with FJP but the other synchronization
> constructs have not.
> 
> David Holmes
> ------------
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> Christian Fries
>> Sent: Tuesday, 6 May 2014 5:59 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] ForkJoinPool not designed for nested
>> Java 8streams.parallel().forEach( ... )
>> 
>> 
>> 
>> 
>> Dear All.
>> 
>> I am new to this list, so please excuse, if this is not the place
>> for the following topic (I did a bug report to Oracle and also
>> posted this to Stackoverflow at
>> http://stackoverflow.com/questions/23442183 - and it was
>> suggested (at SO) that I post this here too).
>> 
>> Java 8 introduced parallel streams which use a common
>> ForkJoinPool. I believe that the implementation of ForkJoinTask
>> has a problem, which becomes relevant in a situation, which is
>> likely to occur if Java 8 parallel streams are used a lot.
>> Specifically the problem arises if one uses a nested Java 8
>> parallel stream foreach, e.g. as in
>> 
>> 	// Outer loop
>> 
>> IntStream.range(0,numberOfTasksInOuterLoop).parallel().forEach(i -> {
>> 		// Inner loop
>> 
>> IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
>> 			// Work done here loop
>> 		});
>> 
>> 	});
>> 
>> 
>> In this situation the inner loop may be started on a workerThread
>> of the common ForkJoinPool.
>> 
>> Now note that the implementation of ForkJoinTask check if the
>> task is run on the parent thread (current thread) or on a forked
>> worked thread via the line
>> 
>> ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread)
>> 
>> Hoever, for the case of the nested loop, it is not sufficient if
>> the thread is ?instanceof ForkJoinWorkerThread?, because the
>> parent thread itself was a ForkJoinWorkerThread. Further in the
>> implementation of doInvoke (see
>> http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/6be37bafb11a/src/s
> hare/classes/java/util/concurrent/ForkJoinTask.java#l393 ) this implied that
> we call awaitJoin on the workQueue - I believe this is wrong. Now, that
> workQueue belongs to the outer loop and this may result in a DEADLOCK.
> 
> For details see
> http://stackoverflow.com/questions/23442183/using-a-semaphore-inside-a-java-
> 8-parallel-stream-action-may-deadlock-is-this-a
> For a detailed test code run
> http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experimen
> ts/concurrency/ForkJoinPoolTest.java
> 
> That said, it appears as if the current Java implementation of ForkJoinPool
> was written without having ?Nesting? in mind (submitting tasks from a worker
> tasks to a common pool), but Java 8 streams makes that kind of nesting a
> natural thing.
> 
> I believe I know the fix for the problem (we have to store the parent thread
> and check if currentThread() equals the parentThread of a ForkJoinTask - in
> case you like me to get involved.
> 
> Please excuse if this is not the right mailing list for this.
> 
> Best
> Christian
> 
> 
> 
> ________________________________________________
> http://www.christian-fries.de
> 
> 
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From aleksey.shipilev at oracle.com  Mon May  5 16:50:42 2014
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 06 May 2014 00:50:42 +0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
Message-ID: <5367F9A2.4090507@oracle.com>

Hi Christian,

You are on the correct list.

On 05/05/2014 11:58 PM, Christian Fries wrote:
> Java 8 introduced parallel streams which use a common ForkJoinPool.
> I believe that the implementation of ForkJoinTask has a problem, 
> which becomes relevant in a situation, which is likely to occur if 
> Java 8 parallel streams are used a lot. Specifically the problem 
> arises if one uses a nested Java 8 parallel stream foreach, e.g. as
> in
> 	// Outer loop
> 	IntStream.range(0,numberOfTasksInOuterLoop).parallel().forEach(i -> {
> 		// Inner loop
> 		IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
> 			// Work done here loop
> 		});
> 
> 	});

To my knowledge of FJP mechanics, this should work perfectly well, since
submitters from the external loop will participate in the computation
and making progress. My throw-away tests, like:

    @GenerateMicroBenchmark
    public void test(BlackHole bh) {
        IntStream.range(0, TASKS).parallel().forEach(i -> {
            IntStream.range(0, TASKS).parallel().forEach(j -> {
                bh.consume(i + j);
            });
        });
    }

...do not manage to deadlock.


> For a detailed test code run http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/ForkJoinPoolTest.java

This test does not appear to deadlock with (isUseSemaphore = false)
either... Can you check again?

The (isUseSemaphore = true) seems to deplete the worker pool on my
machine, and the underlying FJP can not make progress. As others have
noted, you should use FJP.ManagedBlocker to tell the pool you are about
to block. If I wrap your Semaphore with:

this.concurrentExecutions = new ForkJoinSemaphore(new
Semaphore(concurrentExecusionsLimitInOuterLoop));


  private static class ForkJoinSemaphore implements
ForkJoinPool.ManagedBlocker {
        private final Semaphore s;

        public ForkJoinSemaphore(Semaphore s) {
            this.s = s;
        }

        @Override
        public boolean block() throws InterruptedException {
            s.acquire();
            return true;
        }

        @Override
        public boolean isReleasable() {
            return s.availablePermits() > 0;
        }

        public int availablePermits() {
            return s.availablePermits();
        }

        public void release() {
            s.release();
        }
    }

...and do ForkJoin.managedBlock(concurrentExecutions) instead of
acquire(), the deadlock is also gone.

> That said, it appears as if the current Java implementation of 
> ForkJoinPool was written without having ?Nesting? in mind
> (submitting tasks from a worker tasks to a common pool), but Java 8
> streams makes that kind of nesting a natural thing.

Looking very briefly at this, I think you just have a simple resource
starvation problem, not a FJP bug.

-Aleksey.

From aleksey.shipilev at oracle.com  Mon May  5 17:04:40 2014
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 06 May 2014 01:04:40 +0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8streams.parallel().forEach( ... )
In-Reply-To: <6F1AEC0A-5273-42A1-9F90-C2787EC8D45D@christian-fries.de>
References: <NFBBKALFDCPFIDBNKAPCOEDDKGAA.davidcholmes@aapt.net.au>
	<6F1AEC0A-5273-42A1-9F90-C2787EC8D45D@christian-fries.de>
Message-ID: <5367FCE8.6010902@oracle.com>

On 05/06/2014 12:40 AM, Christian Fries wrote:
> It appears to me as if nested Java 8 parallel streams will create a
> very subtle ?implicit coupling? between the tasks submitted to the
> common pool, because tasks of an inner loop are joined with tasks on
> an outer loop.

What coupling are you talking about now? In my mental model, starting
the parallel() computation from the parallel() computation is "just"
invoking the FJTask from the already executing one, and that is just
forking-joining on a new task. Joining on inner FJTask will assist
executing it. Which means that FJThreads executing the outer loop will
assist executing the inner loop if they are "joining" on inner loop
computations.

> So maybe the problem is to have a common FJP at all?
> 
> I still have the impression that the line ((t =
> Thread.currentThread()) instanceof ForkJoinWorkerThread) is not
> correct in the case of a nested loop.

Why? It checks if we are within the FJP and may proceed to awaitJoin to
*help* other tasks.

-Aleksey.

From email at christian-fries.de  Mon May  5 17:37:24 2014
From: email at christian-fries.de (Christian Fries)
Date: Mon, 5 May 2014 23:37:24 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8streams.parallel().forEach( ... )
In-Reply-To: <5367FCE8.6010902@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCOEDDKGAA.davidcholmes@aapt.net.au>
	<6F1AEC0A-5273-42A1-9F90-C2787EC8D45D@christian-fries.de>
	<5367FCE8.6010902@oracle.com>
Message-ID: <E74AA240-3AF1-416D-9731-FB0F95221A7B@christian-fries.de>


Hi Aleksey.

I can confirm that the use of your ForkJoinSemaphore fixes the deadlock situation. Thank you!

Note: I believe it should be
	        public boolean isReleasable() { return false; }
otherwise we are not blocking, hence never acquiring, 

But your code (then) works!

My code linked at 
	http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/ForkJoinPoolTest.java
only deadlocks with isUseSemaphore = true. But note that the deadlock vanishes if isWrappedInnerLoopThread = true (with the deadly Semaphore, without your ForkJoinSemaphore).

For the deadlock situation: there are 10 threads and the semaphore will block 5. The 5 others will execute the inner loop. When you check the DEADLOCK with a debugger then you find that 5 threads are blocked at the semaphore but the 5 others are blocked at the awaitJoin of an inner task, i.e. the inner foreach:
IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach
Why are we blocking at the awaitJoin of a task of the inner loop? That was puzzling.

Thanks again for the hint to ForkJoinPool.managedBlock - that was very valuable.

Best
Christian


Am 05.05.2014 um 23:04 schrieb Aleksey Shipilev <aleksey.shipilev at oracle.com>:

> On 05/06/2014 12:40 AM, Christian Fries wrote:
>> It appears to me as if nested Java 8 parallel streams will create a
>> very subtle ?implicit coupling? between the tasks submitted to the
>> common pool, because tasks of an inner loop are joined with tasks on
>> an outer loop.
> 
> What coupling are you talking about now? In my mental model, starting
> the parallel() computation from the parallel() computation is "just"
> invoking the FJTask from the already executing one, and that is just
> forking-joining on a new task. Joining on inner FJTask will assist
> executing it. Which means that FJThreads executing the outer loop will
> assist executing the inner loop if they are "joining" on inner loop
> computations.
> 
>> So maybe the problem is to have a common FJP at all?
>> 
>> I still have the impression that the line ((t =
>> Thread.currentThread()) instanceof ForkJoinWorkerThread) is not
>> correct in the case of a nested loop.
> 
> Why? It checks if we are within the FJP and may proceed to awaitJoin to
> *help* other tasks.
> 
> -Aleksey.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/4040d39b/attachment.html>

From email at christian-fries.de  Mon May  5 17:54:40 2014
From: email at christian-fries.de (Christian Fries)
Date: Mon, 5 May 2014 23:54:40 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8streams.parallel().forEach( ... )
In-Reply-To: <5367FCE8.6010902@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCOEDDKGAA.davidcholmes@aapt.net.au>
	<6F1AEC0A-5273-42A1-9F90-C2787EC8D45D@christian-fries.de>
	<5367FCE8.6010902@oracle.com>
Message-ID: <EB6C1FDD-E865-4F01-B669-AC58A567135F@christian-fries.de>


Dear Aleksey.

If run my code in the deadlock situation, wait for the deadlock, then check every thread (suspending all of them) you see that 5 threads are blocked at the Semaphore. And 5 others have reached the inner loop and are blocked at an awaitJoin of a ForkJoinTask of the inner loop.

My (maybe to light hearted) conclusion is that a task of the inner loop waits for a task of any blocked outer loop. Why and how is this dependence introduced?! (And given your solution: why is it solved by your code ;-)

Best
Christian


Am 05.05.2014 um 23:04 schrieb Aleksey Shipilev <aleksey.shipilev at oracle.com>:

> On 05/06/2014 12:40 AM, Christian Fries wrote:
>> It appears to me as if nested Java 8 parallel streams will create a
>> very subtle ?implicit coupling? between the tasks submitted to the
>> common pool, because tasks of an inner loop are joined with tasks on
>> an outer loop.
> 
> What coupling are you talking about now? In my mental model, starting
> the parallel() computation from the parallel() computation is "just"
> invoking the FJTask from the already executing one, and that is just
> forking-joining on a new task. Joining on inner FJTask will assist
> executing it. Which means that FJThreads executing the outer loop will
> assist executing the inner loop if they are "joining" on inner loop
> computations.
> 
>> So maybe the problem is to have a common FJP at all?
>> 
>> I still have the impression that the line ((t =
>> Thread.currentThread()) instanceof ForkJoinWorkerThread) is not
>> correct in the case of a nested loop.
> 
> Why? It checks if we are within the FJP and may proceed to awaitJoin to
> *help* other tasks.
> 
> -Aleksey.



From ri.joel at gmail.com  Mon May  5 17:59:43 2014
From: ri.joel at gmail.com (Joel Richard)
Date: Mon, 5 May 2014 23:59:43 +0200
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEDDKGAA.davidcholmes@aapt.net.au>
References: <CAOe+kYdLejMnKNiNB7DD14ELuJVivpG95uGLcXn3O-1MNM8EJQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEDDKGAA.davidcholmes@aapt.net.au>
Message-ID: <CAOe+kYcR7W7co3w0+287N35Azz5W+4nsSomu_=ACfngfHXuLhA@mail.gmail.com>

Hi David,

I have created a code example which illustrates the usage and behavior of
how I would implement coroutines* with simple async IO support in Java:
https://gist.github.com/joelrich/a97d7d364f69c86848ca

* Before I called them tasks or lightweight threads, but coroutine (or
fiber) seems to be the correct name: https://en.wikipedia.org/wiki/Coroutine

On Mon, May 5, 2014 at 10:08 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> Joel Richard writes:
> >
> > Yes, my intent is to implement a M:N hybrid threading model;
> > however, I think it could co-exist with the existing 1:1 model. I
> > would suggest that the lightweight threads don't switch
> > automatically until they block or call Thread.yield (similar to
> > Node.js). Therefore, the M:N model would be great for servers
> > while the 1:1 model might still be better suited for
> > computational tasks or precise timers. In order that library
> > developers don't have to know whether their code is running in a
> > native or in a lightweight thread, it would make sense to create
> > some utilities, e.g. one which executes computational tasks in a
> > worker thread if the current thread is just a lightweight thread.
> > Moreover, some methods would change their behavior (without
> > breaking the API) if they are executed in lightweight threads.
> > For example, Thread.yield would rather just start/continue
> > executing another task instead of switching to another thread.
>
> Conceptually you may be able to have a system where a M:N scheduler
> co-exists with a 1:1 scheduler; but I do not think you can simply add that
> to hotspot without completely rewriting the existing code.


It would definitely require a lot of work and adjustments - I don't deny
that. Apart from the already mentioned changes, we would have to add a
feature to HotSpot which allows to pause/continue code of a coroutine at an
arbitrary point without blocking the kernel thread. However, Quasar proofs
that this is even possible at a language level (with some limitations).


> Further, trying to generalize a M:N model to apply to all threads is
> extremely difficult - how exactly do you pick up another thread and when
> can you return to the original? Specialized approaches, like ForkJoin,
> define the computational chunks so that it is easy to do this.


I think it should use a cooperative scheduling similar to Node.js (however
with multiple kernel threads). This would be easy, effective and optimal
for IO servers.


> There are also issues with thread identity that would have to be handled
> somehow.


Where do you see the problem here?

 >
> > I agree, this would change the performance and even more the
> > scalability model, but I think that would be something positive
>
> Can you back that up with any numbers? "Words are wind ..." ;-)
>
> I think that is really hard before implementing a prototype. However, I
would say that it is general consensus that async socket IO scales better
than blocking socket IO. I don't think that coroutines would scale better
than callbacks, but probably comparable. The main advantage would be that
Java developers would suddenly have access to a huge number of libraries
which use asynchronous IO and existing software stacks could scale better.
Moreover, it would be easier to write async IO code.


> > and bring a lot of opportunities - especially for application
> > servers. If I am not mistaken, the JVM doesn't guarantee a
> > certain performance characteristic or thread behavior.
>
> It may not guarantee it but we go to a lot of trouble to try and not
> introduce discontinuities in the performance curves. People invest a lot of
> time and effort in tuning large apps, and in designing them for the way
> things actually work. You can't completely change the threading and
> performance model for these apps.
>
> > and since lightweight threads would be an optional feature, I
> > think this should be considered as a chance to make Java and the
> > JVM future proof without re-implementing hundreds of libraries.
>
> Are you saying that the VM is either started in traditional mode, or else
> this new mode? That would address the compatability issue - provided you
> can implement the new mode without changing the existing performance model.
>
> That would be an option, but I think it might be smarter to support both
modes at the same time as shown in my Gist example above. Application
servers could then use a coroutine executor service for the request
handling and all servlets would run in the new non-blocking mode. In
addition to that, it might make sense to add a command line option which
enables the coroutine mode by default.

Regards, Joel

David
> -----
>
> > It would make async IO and scalable servers easier than with
> > Node.js and prevent that companies switch from Java to Go (or
> > similar languages) just because they have a more appealing
> > concurrency model right now.
> >
> >
> > Regards, Joel
> >
> >
> >
> > On Mon, May 5, 2014 at 9:20 AM, Kirk Pepperdine <kirk at kodewerk.com>
> wrote:
> >
> >
> > On May 4, 2014, at 11:37 PM, Gregg Wonderly <gregg at wonderly.org> wrote:
> >
> > > The original version of Java used a runtime environment which
> > used the term "Green Threads".  This was of course taken out when
> > sun started trying to make Java more than an Applet engine.
> >
> >
> > I believe the idea for Green Threads stems from lightweight
> > threads in Solaris. There, a lightweight thread ran on top of a
> > single kernel thread. The kernel thread time sliced between each
> > of it?s LWP. I?m not sure it ever worked all that well as CPU?s
> > at the time of inception weren?t quite what they are today and
> > scheduling in Solaris always felt sluggish.
> >
> > >
> > > There are two problems.  First is all the visibility and
> > contention issues which the concurrency package has demonstrated
> > to not be a simple issue due to rather arcane processor
> > architecture issues.
> > >
> > > What we actually need is infinitely fast cache and cache
> > coherency algorithms, and much more sane processor designs which
> > try to allow processors to work together rather than trying
> > desperately to keep them from interfering with each other.
> >
> >
> > Completely agree, copies are always going to be a problem and
> > immutability can only be part of the answer.
> >
> > Regards,
> > Kirk
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140505/c2e13360/attachment-0001.html>

From davidcholmes at aapt.net.au  Mon May  5 18:37:23 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 6 May 2014 08:37:23 +1000
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
	simpler Async IO
In-Reply-To: <CAOe+kYcR7W7co3w0+287N35Azz5W+4nsSomu_=ACfngfHXuLhA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEDFKGAA.davidcholmes@aapt.net.au>

Hi Joel,

That's an example of how you would program with coroutines. I'm concerned about the actual implementation under covers. When your coroutine blocks on the I/O call where does the current thread go to execute some other co-routine? When the I/O is ready how does the blocked thread proceed? The programming models for these kinds of things are well established. But your proposal is to try and add this to the JDK so you need to consider how it can be implemented.

Cheers,
David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Joel Richard
  Sent: Tuesday, 6 May 2014 8:00 AM
  To: dholmes at ieee.org
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] Proposal for Hybrid Threading Model and simpler Async IO


  Hi David,


  I have created a code example which illustrates the usage and behavior of how I would implement coroutines* with simple async IO support in Java: https://gist.github.com/joelrich/a97d7d364f69c86848ca


  * Before I called them tasks or lightweight threads, but coroutine (or fiber) seems to be the correct name: https://en.wikipedia.org/wiki/Coroutine


  On Mon, May 5, 2014 at 10:08 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Joel Richard writes:
    >
    > Yes, my intent is to implement a M:N hybrid threading model;
    > however, I think it could co-exist with the existing 1:1 model. I
    > would suggest that the lightweight threads don't switch
    > automatically until they block or call Thread.yield (similar to
    > Node.js). Therefore, the M:N model would be great for servers
    > while the 1:1 model might still be better suited for
    > computational tasks or precise timers. In order that library
    > developers don't have to know whether their code is running in a
    > native or in a lightweight thread, it would make sense to create
    > some utilities, e.g. one which executes computational tasks in a
    > worker thread if the current thread is just a lightweight thread.
    > Moreover, some methods would change their behavior (without
    > breaking the API) if they are executed in lightweight threads.
    > For example, Thread.yield would rather just start/continue
    > executing another task instead of switching to another thread.


    Conceptually you may be able to have a system where a M:N scheduler co-exists with a 1:1 scheduler; but I do not think you can simply add that to hotspot without completely rewriting the existing code.


  It would definitely require a lot of work and adjustments - I don't deny that. Apart from the already mentioned changes, we would have to add a feature to HotSpot which allows to pause/continue code of a coroutine at an arbitrary point without blocking the kernel thread. However, Quasar proofs that this is even possible at a language level (with some limitations).

    Further, trying to generalize a M:N model to apply to all threads is extremely difficult - how exactly do you pick up another thread and when can you return to the original? Specialized approaches, like ForkJoin, define the computational chunks so that it is easy to do this.


  I think it should use a cooperative scheduling similar to Node.js (however with multiple kernel threads). This would be easy, effective and optimal for IO servers. 

    There are also issues with thread identity that would have to be handled somehow.


  Where do you see the problem here?


    >
    > I agree, this would change the performance and even more the
    > scalability model, but I think that would be something positive


    Can you back that up with any numbers? "Words are wind ..." ;-)



  I think that is really hard before implementing a prototype. However, I would say that it is general consensus that async socket IO scales better than blocking socket IO. I don't think that coroutines would scale better than callbacks, but probably comparable. The main advantage would be that Java developers would suddenly have access to a huge number of libraries which use asynchronous IO and existing software stacks could scale better. Moreover, it would be easier to write async IO code.

    > and bring a lot of opportunities - especially for application
    > servers. If I am not mistaken, the JVM doesn't guarantee a
    > certain performance characteristic or thread behavior.


    It may not guarantee it but we go to a lot of trouble to try and not introduce discontinuities in the performance curves. People invest a lot of time and effort in tuning large apps, and in designing them for the way things actually work. You can't completely change the threading and performance model for these apps.


    > and since lightweight threads would be an optional feature, I
    > think this should be considered as a chance to make Java and the
    > JVM future proof without re-implementing hundreds of libraries.


    Are you saying that the VM is either started in traditional mode, or else this new mode? That would address the compatability issue - provided you can implement the new mode without changing the existing performance model.


  That would be an option, but I think it might be smarter to support both modes at the same time as shown in my Gist example above. Application servers could then use a coroutine executor service for the request handling and all servlets would run in the new non-blocking mode. In addition to that, it might make sense to add a command line option which enables the coroutine mode by default.


  Regards, Joel


    David
    -----


    > It would make async IO and scalable servers easier than with
    > Node.js and prevent that companies switch from Java to Go (or
    > similar languages) just because they have a more appealing
    > concurrency model right now.
    >
    >
    > Regards, Joel
    >
    >
    >
    > On Mon, May 5, 2014 at 9:20 AM, Kirk Pepperdine <kirk at kodewerk.com> wrote:
    >
    >
    > On May 4, 2014, at 11:37 PM, Gregg Wonderly <gregg at wonderly.org> wrote:
    >
    > > The original version of Java used a runtime environment which
    > used the term "Green Threads".  This was of course taken out when
    > sun started trying to make Java more than an Applet engine.
    >
    >
    > I believe the idea for Green Threads stems from lightweight
    > threads in Solaris. There, a lightweight thread ran on top of a
    > single kernel thread. The kernel thread time sliced between each
    > of it?s LWP. I?m not sure it ever worked all that well as CPU?s
    > at the time of inception weren?t quite what they are today and
    > scheduling in Solaris always felt sluggish.
    >
    > >
    > > There are two problems.  First is all the visibility and
    > contention issues which the concurrency package has demonstrated
    > to not be a simple issue due to rather arcane processor
    > architecture issues.
    > >
    > > What we actually need is infinitely fast cache and cache
    > coherency algorithms, and much more sane processor designs which
    > try to allow processors to work together rather than trying
    > desperately to keep them from interfering with each other.
    >
    >
    > Completely agree, copies are always going to be a problem and
    > immutability can only be part of the answer.
    >
    > Regards,
    > Kirk
    >



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/0def5ab3/attachment.html>

From dl at cs.oswego.edu  Mon May  5 19:09:20 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 05 May 2014 19:09:20 -0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
Message-ID: <53681A20.7070805@cs.oswego.edu>

On 05/05/2014 03:58 PM, Christian Fries wrote:
>
> I am new to this list, so please excuse, if this is not the place for the
> following topic (I did a bug report to Oracle and also posted this to
> Stackoverflow at http://stackoverflow.com/questions/23442183 - and it was
> suggested (at SO) that I post this here too).

Sorry that you and a bunch of people at stackOverflow spent so much time
on this because you did not did not notice the java.util.stream documentation
(http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html)
about the use of independent functions/actions (especially not those including
synchronization). We should do something about this.

Thanks to David and Aleksey for explaining that the problem has nothing
to do with nested parallelism (except that in this case it triggered a
problem that otherwise happened not to occur).

I'm wondering if we should retrofit ManagedBlocker mechanics to all
j.u.c blocking synchronizers to avoid future problem reports. The
results of using streams/FJ in such cases will not always be very
sensible, but at least they won't starve. This would cover pretty
much all possibly user synchronization except for manual wait/notify
monitor constructions.

(And if we did this for IO as well, we'd have an implicit answer to
Joel Richards's queries...)

-Doug


From email at christian-fries.de  Tue May  6 04:09:44 2014
From: email at christian-fries.de (Christian Fries)
Date: Tue, 6 May 2014 10:09:44 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <53681A20.7070805@cs.oswego.edu>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
Message-ID: <BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>



Dear All.

Thank you for your replies.

At Stackoverflow people immediately reacted to the use of the semaphore as an obvious problem in my code. They are correct! However, I have the impression that the problem in the FJP is there and not related to this. So I created an example without that semaphore, you find it at:
http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/NestedParallelForEachTest.java
and I appended it below. Given that I would rephrase the problem as an unexpected performance issue.

Let me describe the setup:

We have a nested stream.parallel().forEach(). The inner loop is independent (stateless, no interference, etc. - except of the use of a common pool) and consumes 1 second in total in the worst case, namely if processed sequential. Half of the tasks of the outer loop consume 10 seconds prior that loop. Half consume 10 seconds after that loop. We have a boolean which allows to switch the inner loop from prallel() to sequential(). Hence every thread consumes 11 seconds (worst case) in total. Now: submitting 24 outer-loop-tasks to a pool of 8 we would expect 24/8 * 11 = 33 seconds at best (on an 8 core or better machine).

The result is:
- With inner loop sequential:	33 seconds.
- With inner loop parallel:		>80 seconds (I had 93 seconds).

Can you confirm this behavior on your machine? Mine is a Mid 2012 MBP 2.6 i7.

Darwin Vesper.local 13.1.0 Darwin Kernel Version 13.1.0: Wed Apr  2 23:52:02 PDT 2014; root:xnu-2422.92.1~2/RELEASE_X86_64 x86_64
java version "1.8.0_05"
Java(TM) SE Runtime Environment (build 1.8.0_05-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)

@Alexey: I believe the problem is induced by the awaitJoin called on the wrong queue due to that test I mentioned. This introduced a coupling where inner tasks wait on outer task. I have a workaround where you can nest parallel loops to the same cp and the problem goes away (wrap the inner loop in its own thread).

Best
Christian

 ? code starts here ? 

public class NestedParallelForEachTest {

	// The program uses 33 sec with this boolean to false and around 80+ with this boolean true:
	final boolean isInnerStreamParallel		= true;

	// Setup: Inner loop task 0.01 sec in worse case. Outer loop task: 10 sec + inner loop. This setup: (100 * 0.01 sec + 10 sec) * 20 = 22 sec.
	final int		numberOfTasksInOuterLoop = 24;				// In real applications this can be a large number (e.g. > 1000).
	final int		numberOfTasksInInnerLoop = 100;				// In real applications this can be a large number (e.g. > 1000).
	final int		concurrentExecutionsLimitForStreams	= 8;	// java.util.concurrent.ForkJoinPool.common.parallelism
	
	public static void main(String[] args) {
		(new NestedParallelForEachTest()).testNestedLoops();
	}

	public void testNestedLoops() {

		long start = System.currentTimeMillis();

		System.setProperty("java.util.concurrent.ForkJoinPool.common.parallelism",Integer.toString(concurrentExecutionsLimitForStreams));
		System.out.println("java.util.concurrent.ForkJoinPool.common.parallelism = " + System.getProperty("java.util.concurrent.ForkJoinPool.common.parallelism"));

		// Outer loop
		IntStream.range(0,numberOfTasksInOuterLoop).parallel().forEach(i -> {

			try {
				if(i < 10) Thread.sleep(10 * 1000);

				System.out.println(i + "\t" + Thread.currentThread());

				if(isInnerStreamParallel) {
					// Inner loop as parallel: worst case (sequential) it takes 10 * numberOfTasksInInnerLoop millis
					IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
						try { Thread.sleep(10); } catch (Exception e) { e.printStackTrace(); }
					});
						
				}
				else {
					// Inner loop as sequential
					IntStream.range(0,numberOfTasksInInnerLoop).sequential().forEach(j -> {
						try { Thread.sleep(10); } catch (Exception e) { e.printStackTrace(); }
					});
				}

				if(i >= 10) Thread.sleep(10 * 1000);
			} catch (Exception e) { e.printStackTrace(); }

		});

		long end = System.currentTimeMillis();

		System.out.println("Done in " + (end-start)/1000 + " sec.");
	}
}



Am 06.05.2014 um 01:09 schrieb Doug Lea <dl at cs.oswego.edu>:

> On 05/05/2014 03:58 PM, Christian Fries wrote:
>> 
>> I am new to this list, so please excuse, if this is not the place for the
>> following topic (I did a bug report to Oracle and also posted this to
>> Stackoverflow at http://stackoverflow.com/questions/23442183 - and it was
>> suggested (at SO) that I post this here too).
> 
> Sorry that you and a bunch of people at stackOverflow spent so much time
> on this because you did not did not notice the java.util.stream documentation
> (http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html)
> about the use of independent functions/actions (especially not those including
> synchronization). We should do something about this.
> 
> Thanks to David and Aleksey for explaining that the problem has nothing
> to do with nested parallelism (except that in this case it triggered a
> problem that otherwise happened not to occur).
> 
> I'm wondering if we should retrofit ManagedBlocker mechanics to all
> j.u.c blocking synchronizers to avoid future problem reports. The
> results of using streams/FJ in such cases will not always be very
> sensible, but at least they won't starve. This would cover pretty
> much all possibly user synchronization except for manual wait/notify
> monitor constructions.
> 
> (And if we did this for IO as well, we'd have an implicit answer to
> Joel Richards's queries...)
> 
> -Doug
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/3f51e7c1/attachment-0001.html>

From russel at winder.org.uk  Tue May  6 04:12:36 2014
From: russel at winder.org.uk (Russel Winder)
Date: Tue, 06 May 2014 09:12:36 +0100
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <5367F9A2.4090507@oracle.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<5367F9A2.4090507@oracle.com>
Message-ID: <1399363956.18559.18.camel@anglides.winder.org.uk>

On Tue, 2014-05-06 at 00:50 +0400, Aleksey Shipilev wrote:
[?]
> To my knowledge of FJP mechanics, this should work perfectly well, since
> submitters from the external loop will participate in the computation
> and making progress. My throw-away tests, like:
> 
>     @GenerateMicroBenchmark
>     public void test(BlackHole bh) {
>         IntStream.range(0, TASKS).parallel().forEach(i -> {
>             IntStream.range(0, TASKS).parallel().forEach(j -> {
>                 bh.consume(i + j);
>             });
>         });
>     }
> 
> ...do not manage to deadlock.
[?]

I am finding that whilst nested streams code works, it seems to have
dreadful performance. I need to do more work, and get proper benchmark
data, before making an serious claim on this, for now it is just an
anecdotal indicator. 
-- 
Russel.
=============================================================================
Dr Russel Winder      t: +44 20 7585 2200   voip: sip:russel.winder at ekiga.net
41 Buckmaster Road    m: +44 7770 465 077   xmpp: russel at winder.org.uk
London SW11 1EN, UK   w: www.russel.org.uk  skype: russel_winder


From email at christian-fries.de  Tue May  6 04:34:44 2014
From: email at christian-fries.de (Christian Fries)
Date: Tue, 6 May 2014 10:34:44 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <1399363956.18559.18.camel@anglides.winder.org.uk>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<5367F9A2.4090507@oracle.com>
	<1399363956.18559.18.camel@anglides.winder.org.uk>
Message-ID: <C14681D7-AE09-49A0-B694-3FEFCB71FA7D@christian-fries.de>


Dear Russel.

I see that too. Please also check my previous post and this code: http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/NestedParallelForEachTest.java

I can show you the line in the implementation of the ForkJoinTask that is responsible for this. It is this one:
http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/6be37bafb11a/src/share/classes/java/util/concurrent/ForkJoinTask.java#l386
This test if fine for any non-nested loop. For a nested loop the test is wrong and results in an awaitJoin called on a different workQueue (which then either results in a performance issue or in a deadlock).

Best
Christian

Am 06.05.2014 um 10:12 schrieb Russel Winder <russel at winder.org.uk>:

> On Tue, 2014-05-06 at 00:50 +0400, Aleksey Shipilev wrote:
> [?]
>> To my knowledge of FJP mechanics, this should work perfectly well, since
>> submitters from the external loop will participate in the computation
>> and making progress. My throw-away tests, like:
>> 
>>    @GenerateMicroBenchmark
>>    public void test(BlackHole bh) {
>>        IntStream.range(0, TASKS).parallel().forEach(i -> {
>>            IntStream.range(0, TASKS).parallel().forEach(j -> {
>>                bh.consume(i + j);
>>            });
>>        });
>>    }
>> 
>> ...do not manage to deadlock.
> [?]
> 
> I am finding that whilst nested streams code works, it seems to have
> dreadful performance. I need to do more work, and get proper benchmark
> data, before making an serious claim on this, for now it is just an
> anecdotal indicator. 
> -- 
> Russel.
> =============================================================================
> Dr Russel Winder      t: +44 20 7585 2200   voip: sip:russel.winder at ekiga.net
> 41 Buckmaster Road    m: +44 7770 465 077   xmpp: russel at winder.org.uk
> London SW11 1EN, UK   w: www.russel.org.uk  skype: russel_winder
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/c3594eab/attachment.html>

From paul.sandoz at oracle.com  Tue May  6 06:47:33 2014
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Tue, 6 May 2014 12:47:33 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
Message-ID: <8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>

Hi Chris,

I think the use of Thread.sleep is a little misleading, since it is a blocking operation. Try placing all those sleep statements in managed blocks and you will get different results e.g. with:

    interface Blocker {
        void block() throws InterruptedException;
    }

    ForkJoinPool.ManagedBlocker blocker(Blocker b) {
        return new ForkJoinPool.ManagedBlocker() {
            boolean finished = false;
            @Override
            public boolean block() throws InterruptedException {
                b.block();
                return finished = true;
            }

            @Override
            public boolean isReleasable() {
                return finished;
            }
        };
    }

  ForkJoinPool.managedBlock(blocker(() -> Thread.sleep(N)));


Assuming *non-blocking* operations an enclosed (nested) parallel stream will compete for the same common pool resources with the enclosing parallel stream, something i have advised against doing. I don't know if that would be significantly different from equivalent work performed by two independent parallel streams executed concurrently; need to measure as Russel says.

I think we could do a better job documenting the hazards of nested parallelism. Also pondering if it is appropriate and possible to disable the parallelism on a nested parallel stream.

Paul.


On May 6, 2014, at 10:09 AM, Christian Fries <email at christian-fries.de> wrote:

> 
> 
> Dear All.
> 
> Thank you for your replies.
> 
> At Stackoverflow people immediately reacted to the use of the semaphore as an obvious problem in my code. They are correct! However, I have the impression that the problem in the FJP is there and not related to this. So I created an example without that semaphore, you find it at:
> http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/NestedParallelForEachTest.java
> and I appended it below. Given that I would rephrase the problem as an unexpected performance issue.
> 
> Let me describe the setup:
> 
> We have a nested stream.parallel().forEach(). The inner loop is independent (stateless, no interference, etc. - except of the use of a common pool) and consumes 1 second in total in the worst case, namely if processed sequential. Half of the tasks of the outer loop consume 10 seconds prior that loop. Half consume 10 seconds after that loop. We have a boolean which allows to switch the inner loop from prallel() to sequential(). Hence every thread consumes 11 seconds (worst case) in total. Now: submitting 24 outer-loop-tasks to a pool of 8 we would expect 24/8 * 11 = 33 seconds at best (on an 8 core or better machine).
> 
> The result is:
> - With inner loop sequential:	33 seconds.
> - With inner loop parallel:		>80 seconds (I had 93 seconds).
> 
> Can you confirm this behavior on your machine? Mine is a Mid 2012 MBP 2.6 i7.
> 
> Darwin Vesper.local 13.1.0 Darwin Kernel Version 13.1.0: Wed Apr  2 23:52:02 PDT 2014; root:xnu-2422.92.1~2/RELEASE_X86_64 x86_64
> java version "1.8.0_05"
> Java(TM) SE Runtime Environment (build 1.8.0_05-b13)
> Java HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)
> 
> @Alexey: I believe the problem is induced by the awaitJoin called on the wrong queue due to that test I mentioned. This introduced a coupling where inner tasks wait on outer task. I have a workaround where you can nest parallel loops to the same cp and the problem goes away (wrap the inner loop in its own thread).
> 
> Best
> Christian

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/808a0e27/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/808a0e27/attachment.bin>

From dl at cs.oswego.edu  Tue May  6 07:17:35 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 06 May 2014 07:17:35 -0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
Message-ID: <5368C4CF.6090206@cs.oswego.edu>

On 05/06/2014 06:47 AM, Paul Sandoz wrote:

> I think the use of Thread.sleep is a little misleading, since it is a blocking
> operation. Try placing all those sleep statements in managed blocks and you will
> get different results e.g. with:

Yes. We ought to provide this. I suppose we could intercept 
java.lang.Thread.sleep to do it automatically,
but just adding ForkJoinTask.delay(timeout) would help.
Similarly for CompletableFuture.


> Assuming *non-blocking* operations an enclosed (nested) parallel stream will
> compete for the same common pool resources with the enclosing parallel stream,
> something i have advised against doing. I don't know if that would be
> significantly different from equivalent work performed by two independent
> parallel streams executed concurrently; need to measure as Russel says.

There's a fun story here. FJ is fully capable of keeping up with
nested parallelism, but some of it is internally disabled, so nested
forms will (only) sometimes run in a slow emulation of sequential
processing. In versions prior to JDK7 release, continuation-thread
compensation generated spare threads to maintain target parallelism.
This works great for correctly expressed nested parallelism but is
prone to positive feedback loops when people make small design errors
like inverting joins. When mixed with blocking IO, we saw cases where
this generated thousands of threads. So for the sake of JVM stability,
compensation is restricted to cases where the pool is saturated. This
could be re-explored.

-Doug



From viktor.klang at gmail.com  Tue May  6 08:34:35 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 6 May 2014 14:34:35 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <5368C4CF.6090206@cs.oswego.edu>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<5368C4CF.6090206@cs.oswego.edu>
Message-ID: <CANPzfU8uRPXS-D0Z4ebZ0gmhWg95SJU5UUiMGF-+AC6+ukm3BA@mail.gmail.com>

On Tue, May 6, 2014 at 1:17 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 05/06/2014 06:47 AM, Paul Sandoz wrote:
>
>  I think the use of Thread.sleep is a little misleading, since it is a
>> blocking
>> operation. Try placing all those sleep statements in managed blocks and
>> you will
>> get different results e.g. with:
>>
>
> Yes. We ought to provide this. I suppose we could intercept
> java.lang.Thread.sleep to do it automatically,
> but just adding ForkJoinTask.delay(timeout) would help.
> Similarly for CompletableFuture.
>
>
>
>  Assuming *non-blocking* operations an enclosed (nested) parallel stream
>> will
>> compete for the same common pool resources with the enclosing parallel
>> stream,
>> something i have advised against doing. I don't know if that would be
>> significantly different from equivalent work performed by two independent
>> parallel streams executed concurrently; need to measure as Russel says.
>>
>
> There's a fun story here. FJ is fully capable of keeping up with
> nested parallelism, but some of it is internally disabled, so nested
> forms will (only) sometimes run in a slow emulation of sequential
> processing. In versions prior to JDK7 release, continuation-thread
> compensation generated spare threads to maintain target parallelism.
> This works great for correctly expressed nested parallelism but is
> prone to positive feedback loops when people make small design errors
> like inverting joins. When mixed with blocking IO, we saw cases where
> this generated thousands of threads. So for the sake of JVM stability,
> compensation is restricted to cases where the pool is saturated. This
> could be re-explored.


Speaking of which?is there anything that prevents adding the capability to
specify the maximum number of threads allowed to be spawned due to managed
blocking? (The current default blows up about every JVM there is.)


>
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/2aa2eac0/attachment-0001.html>

From email at christian-fries.de  Tue May  6 09:01:14 2014
From: email at christian-fries.de (Christian Fries)
Date: Tue, 6 May 2014 15:01:14 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
Message-ID: <12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>


Hi Paul.

Oh. I expected that it would be countered that ?Thread.sleep()? is not be a legitimate test case? ;-)

I have update my demo and now I use a stupid for-loop summing up some cosine to burn real CPU time. The result is

Nested stream forEach: outer parallel, inner sequential:	34 sec
Nested stream forEach: outer parallel, inner parallel:		63 sec

In the first case my 8-core CPU ist working at 800% all the time. In the second case you can clearly see that its almost idle at certain times.

You find my updated test case here:
http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/NestedParallelForEachTest.java

If you like to not use Thread.sleep() set isCPUTimeBurned to true - you might need to calibrate that count used in the loop.

Best
Christian


Am 06.05.2014 um 12:47 schrieb Paul Sandoz <paul.sandoz at oracle.com>:

> Hi Chris,
> 
> I think the use of Thread.sleep is a little misleading, since it is a blocking operation. Try placing all those sleep statements in managed blocks and you will get different results e.g. with:
> 
>     interface Blocker {
>         void block() throws InterruptedException;
>     }
> 
>     ForkJoinPool.ManagedBlocker blocker(Blocker b) {
>         return new ForkJoinPool.ManagedBlocker() {
>             boolean finished = false;
>             @Override
>             public boolean block() throws InterruptedException {
>                 b.block();
>                 return finished = true;
>             }
> 
>             @Override
>             public boolean isReleasable() {
>                 return finished;
>             }
>         };
>     }
> 
>   ForkJoinPool.managedBlock(blocker(() -> Thread.sleep(N)));
> 
> 
> Assuming *non-blocking* operations an enclosed (nested) parallel stream will compete for the same common pool resources with the enclosing parallel stream, something i have advised against doing. I don't know if that would be significantly different from equivalent work performed by two independent parallel streams executed concurrently; need to measure as Russel says.
> 
> I think we could do a better job documenting the hazards of nested parallelism. Also pondering if it is appropriate and possible to disable the parallelism on a nested parallel stream.
> 
> Paul.
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/651cfa0c/attachment.html>

From paul.sandoz at oracle.com  Tue May  6 10:44:54 2014
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Tue, 6 May 2014 16:44:54 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
Message-ID: <A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>

Hi,

On May 6, 2014, at 3:01 PM, Christian Fries <email at christian-fries.de> wrote:

> 
> Hi Paul.
> 
> Oh. I expected that it would be countered that ?Thread.sleep()? is not be a legitimate test case? ;-)
> 

:-) I think what you have now with CPU burning is more relevant and also different from where you started. Have you tried writing a jmh benchmark instead? i would recommend doing so if you want to further explore the performance characteristics.


> I have update my demo and now I use a stupid for-loop summing up some cosine to burn real CPU time. The result is
> 
> Nested stream forEach: outer parallel, inner sequential:	34 sec
> Nested stream forEach: outer parallel, inner parallel:		63 sec
> 

That is not too surprising, to me at least, from my previous experience and esp. what Doug said about nested parallelism.


> In the first case my 8-core CPU ist working at 800% all the time. In the second case you can clearly see that its almost idle at certain times.
> 
> You find my updated test case here:
> http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/NestedParallelForEachTest.java
> 
> If you like to not use Thread.sleep() set isCPUTimeBurned to true - you might need to calibrate that count used in the loop.
> 

FWIW you can do: 

                    ForkJoinPool.managedBlock(blocker(() -> {
                        // Inner loop as parallel: worst case (sequential) it takes 10 * numberOfTasksInInnerLoop millis
                        IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
                            burnTime(10);
                        });
                    }));

which enables work to balance out a bit more, but could be regarded as somewhat of an abuse.

Paul.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/d6edba43/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/d6edba43/attachment.bin>

From email at christian-fries.de  Tue May  6 11:38:58 2014
From: email at christian-fries.de (Christian Fries)
Date: Tue, 6 May 2014 17:38:58 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
	<A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
Message-ID: <7235DA14-526C-4F93-A7CE-0417E8731844@christian-fries.de>


Hi Paul.
Hi All.

Thank you for the suggestion. However - I do not need any manage blocker to perfectly solve that performance issue. A simple workaround is the following:

Add the method
	private void wrapInThread(Runnable runnable) {
		Thread t = new Thread(runnable, "Wrapper Thread");
		try {
			t.start();
			t.join();
		} catch (InterruptedException e) { }
	}
(which obviously does not create any additional parallelism) and then wrap the inner loop in that ?Wrapper Thread? via
	wrapInThread( () ->
		IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
			burnTime(10);
		}));
(by which all inner loop tasks are still submitted to the common F/J pool). With this you get full performance (32 sec).

Why is this fixing the problem? I assume the explanation goes s.th. like this:

When you use a nested loop, it may be that the inner loop is started on a ForkJoinWorkerThread (and not on a non-worker thread - as a non-nested loop would do).
Now, the FJP distinguishes a worker thread from a non-worker thread via the line
  ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread)
i.e. it checks its type.
Now, since the inner loop is started on a ForkJoinWorkerThread (one of the outer, but that info is not present), he considers this thread as one of its worker threads and calls awaitJoin - actually joining with the outer loops work queue.
Starting the inner loop on a wrapper thread, the type of Thread.currentThread() will just be Thread and not ForkJoinWorkerThread, hence, we call an externalAwaitDone() instead.
So just wrap every parallel().forEach() in its own Thread (immediately joining) and nested loop work fine.

I am not completely sure that every detail is as described, but I assume that the problem is somewhat like that.

Best
Christian

PS: Sorry, I am new to this list. Is this the right list to discuss such low level code issues? The code I am referring to is at: http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/6be37bafb11a/src/share/classes/java/util/concurrent/ForkJoinTask.java#l401

Am 06.05.2014 um 16:44 schrieb Paul Sandoz <paul.sandoz at oracle.com>:

> Hi,
> 
> On May 6, 2014, at 3:01 PM, Christian Fries <email at christian-fries.de> wrote:
> 
>> 
>> Hi Paul.
>> 
>> Oh. I expected that it would be countered that ?Thread.sleep()? is not be a legitimate test case? ;-)
>> 
> 
> :-) I think what you have now with CPU burning is more relevant and also different from where you started. Have you tried writing a jmh benchmark instead? i would recommend doing so if you want to further explore the performance characteristics.
> 
> 
>> I have update my demo and now I use a stupid for-loop summing up some cosine to burn real CPU time. The result is
>> 
>> Nested stream forEach: outer parallel, inner sequential:	34 sec
>> Nested stream forEach: outer parallel, inner parallel:		63 sec
>> 
> 
> That is not too surprising, to me at least, from my previous experience and esp. what Doug said about nested parallelism.
> 
> 
>> In the first case my 8-core CPU ist working at 800% all the time. In the second case you can clearly see that its almost idle at certain times.
>> 
>> You find my updated test case here:
>> http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/NestedParallelForEachTest.java
>> 
>> If you like to not use Thread.sleep() set isCPUTimeBurned to true - you might need to calibrate that count used in the loop.
>> 
> 
> FWIW you can do: 
> 
>                     ForkJoinPool.managedBlock(blocker(() -> {
>                         // Inner loop as parallel: worst case (sequential) it takes 10 * numberOfTasksInInnerLoop millis
>                         IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
>                             burnTime(10);
>                         });
>                     }));
> 
> which enables work to balance out a bit more, but could be regarded as somewhat of an abuse.
> 
> Paul.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/aafcec80/attachment-0001.html>

From dl at cs.oswego.edu  Tue May  6 12:52:41 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 06 May 2014 12:52:41 -0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <CANPzfU8uRPXS-D0Z4ebZ0gmhWg95SJU5UUiMGF-+AC6+ukm3BA@mail.gmail.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>	<5368C4CF.6090206@cs.oswego.edu>
	<CANPzfU8uRPXS-D0Z4ebZ0gmhWg95SJU5UUiMGF-+AC6+ukm3BA@mail.gmail.com>
Message-ID: <53691359.1020509@cs.oswego.edu>

On 05/06/2014 08:34 AM, ?iktor ?lang wrote:
>
>   In versions prior to JDK7 release, continuation-thread
>     compensation generated spare threads to maintain target parallelism.
>     This works great for correctly expressed nested parallelism but is
>     prone to positive feedback loops when people make small design errors
>     like inverting joins. When mixed with blocking IO, we saw cases where
>     this generated thousands of threads. So for the sake of JVM stability,
>     compensation is restricted to cases where the pool is saturated. This
>     could be re-explored.
>
>
> Speaking of which?is there anything that prevents adding the capability to
> specify the maximum number of threads allowed to be spawned due to managed
> blocking? (The current default blows up about every JVM there is.)

The current default is to only generate a new thread if saturated,
which is the most conservative possible policy. If we allowed a
bound on compensations, then everything could freeze up if past
some arbitrary limit, which seems even less popular than risking
OutOfMemoryError (and RejectedExecutionException) thrown by
attempted Thread construction. Especially since you can use
ForkJoinWorkerThreadFactories and/or SecurityManagers or to bound
the total number of Threads in a program, whether by FJ or elsewhere.

-Doug




From dl at cs.oswego.edu  Tue May  6 13:43:09 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 06 May 2014 13:43:09 -0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <7235DA14-526C-4F93-A7CE-0417E8731844@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
	<A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
	<7235DA14-526C-4F93-A7CE-0417E8731844@christian-fries.de>
Message-ID: <53691F2D.3060708@cs.oswego.edu>

On 05/06/2014 11:38 AM, Christian Fries wrote:

> Thank you for the suggestion. However - I do not need any manage blocker to
> perfectly solve that performance issue. A simple workaround is the following:
>
> Add the method
> private void wrapInThread(Runnable runnable) ...

This does not in general do anything interesting beyond
making the task recursion stack shallower so that it becomes
less likely that nested joiners need compensation.
Which, again, is limited for resource reasons. This limit
causes your restricted parallelism. In particular, it has very
little to do with  instanceof ForkJoinWorkerThread tests.
Methods awaitJoin and externalAwaitJoin have similar implementations.
They mainly differ in that external joiners do not have guaranteed
ownership of queues.

-Doug


From viktor.klang at gmail.com  Tue May  6 13:54:03 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 6 May 2014 19:54:03 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <53691359.1020509@cs.oswego.edu>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<5368C4CF.6090206@cs.oswego.edu>
	<CANPzfU8uRPXS-D0Z4ebZ0gmhWg95SJU5UUiMGF-+AC6+ukm3BA@mail.gmail.com>
	<53691359.1020509@cs.oswego.edu>
Message-ID: <CANPzfU8we31DFPf_U_ULNCK1azMAhtKxDLPwUfq0Dq4yQzMX_w@mail.gmail.com>

On Tue, May 6, 2014 at 6:52 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 05/06/2014 08:34 AM, ?iktor ?lang wrote:
>
>>
>>   In versions prior to JDK7 release, continuation-thread
>>     compensation generated spare threads to maintain target parallelism.
>>     This works great for correctly expressed nested parallelism but is
>>     prone to positive feedback loops when people make small design errors
>>     like inverting joins. When mixed with blocking IO, we saw cases where
>>     this generated thousands of threads. So for the sake of JVM stability,
>>     compensation is restricted to cases where the pool is saturated. This
>>     could be re-explored.
>>
>>
>> Speaking of which?is there anything that prevents adding the capability to
>> specify the maximum number of threads allowed to be spawned due to managed
>> blocking? (The current default blows up about every JVM there is.)
>>
>
> The current default is to only generate a new thread if saturated,
> which is the most conservative possible policy. If we allowed a
> bound on compensations, then everything could freeze up if past
> some arbitrary limit, which seems even less popular than risking
> OutOfMemoryError (and RejectedExecutionException) thrown by
> attempted Thread construction. Especially since you can use
> ForkJoinWorkerThreadFactories and/or SecurityManagers or to bound
> the total number of Threads in a program, whether by FJ or elsewhere.


Assuming that there are other threads in the system than the pool, it'd be
perfectly fine to have all threads in the pool stalled when waiting for
external threads to complete/signal progress, right? The code that submits
the ManagedBlocker only does so to be polite about his/her blocking, and
won't have a full "global view" of what is currently running, and
definitely does not want to have an OOME due to too many native threads.

How can ForkJoinWorkerThreadFactory be used to cap the maximum number of
threads without blowing something up?


>
>
> -Doug
>
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/d26f2f97/attachment.html>

From dl at cs.oswego.edu  Tue May  6 14:18:43 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 06 May 2014 14:18:43 -0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <CANPzfU8we31DFPf_U_ULNCK1azMAhtKxDLPwUfq0Dq4yQzMX_w@mail.gmail.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>	<5368C4CF.6090206@cs.oswego.edu>	<CANPzfU8uRPXS-D0Z4ebZ0gmhWg95SJU5UUiMGF-+AC6+ukm3BA@mail.gmail.com>	<53691359.1020509@cs.oswego.edu>
	<CANPzfU8we31DFPf_U_ULNCK1azMAhtKxDLPwUfq0Dq4yQzMX_w@mail.gmail.com>
Message-ID: <53692783.3040407@cs.oswego.edu>

On 05/06/2014 01:54 PM, ?iktor ?lang wrote:
>     The current default is to only generate a new thread if saturated,
>     which is the most conservative possible policy. If we allowed a
>     bound on compensations, then everything could freeze up if past
>     some arbitrary limit, which seems even less popular than risking
>     OutOfMemoryError (and RejectedExecutionException) thrown by
>     attempted Thread construction. Especially since you can use
>     ForkJoinWorkerThreadFactories and/or SecurityManagers or to bound
>     the total number of Threads in a program, whether by FJ or elsewhere.
>
>
> Assuming that there are other threads in the system than the pool, it'd be
> perfectly fine to have all threads in the pool stalled when waiting for external
> threads to complete/signal progress, right?

This might hold in practice in some applications and frameworks,
but not in general: these other threads may be using FJ and blocking
on results.

>
> How can ForkJoinWorkerThreadFactory be used to cap the maximum number of threads
> without blowing something up?

Create a factory that hands out up to a maximum number of threads,
else null. So long as it provides at least as many threads as
the parallelism level, then nothing blows up when null is returned
thereafter. (You'd also need some bookkeeping in
ForkJoinWorkerThread.onTermination.)

-Doug








From viktor.klang at gmail.com  Tue May  6 15:24:18 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 6 May 2014 21:24:18 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <53692783.3040407@cs.oswego.edu>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<5368C4CF.6090206@cs.oswego.edu>
	<CANPzfU8uRPXS-D0Z4ebZ0gmhWg95SJU5UUiMGF-+AC6+ukm3BA@mail.gmail.com>
	<53691359.1020509@cs.oswego.edu>
	<CANPzfU8we31DFPf_U_ULNCK1azMAhtKxDLPwUfq0Dq4yQzMX_w@mail.gmail.com>
	<53692783.3040407@cs.oswego.edu>
Message-ID: <CANPzfU-P75rn0rJDvk9Ws0OVyBG9H3MTmiWVAu+1p9fJcY39qA@mail.gmail.com>

On Tue, May 6, 2014 at 8:18 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 05/06/2014 01:54 PM, ?iktor ?lang wrote:
>
>>     The current default is to only generate a new thread if saturated,
>>     which is the most conservative possible policy. If we allowed a
>>     bound on compensations, then everything could freeze up if past
>>     some arbitrary limit, which seems even less popular than risking
>>     OutOfMemoryError (and RejectedExecutionException) thrown by
>>     attempted Thread construction. Especially since you can use
>>     ForkJoinWorkerThreadFactories and/or SecurityManagers or to bound
>>     the total number of Threads in a program, whether by FJ or elsewhere.
>>
>>
>> Assuming that there are other threads in the system than the pool, it'd be
>> perfectly fine to have all threads in the pool stalled when waiting for
>> external
>> threads to complete/signal progress, right?
>>
>
> This might hold in practice in some applications and frameworks,
> but not in general: these other threads may be using FJ and blocking
> on results.


There's definitely the risk of reentrancy.


>
>
>
>> How can ForkJoinWorkerThreadFactory be used to cap the maximum number of
>> threads
>> without blowing something up?
>>
>
> Create a factory that hands out up to a maximum number of threads,
> else null. So long as it provides at least as many threads as
> the parallelism level, then nothing blows up when null is returned
> thereafter. (You'd also need some bookkeeping in
> ForkJoinWorkerThread.onTermination.)


That sounds fair enough, I'll give it a try. Thank you!


>
>
> -Doug
>
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140506/1c96b330/attachment.html>

From ri.joel at gmail.com  Wed May  7 11:54:09 2014
From: ri.joel at gmail.com (Joel Richard)
Date: Wed, 7 May 2014 17:54:09 +0200
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEDFKGAA.davidcholmes@aapt.net.au>
References: <CAOe+kYcR7W7co3w0+287N35Azz5W+4nsSomu_=ACfngfHXuLhA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEDFKGAA.davidcholmes@aapt.net.au>
Message-ID: <CAOe+kYeWoFKCZ0swDjQro0Lny+52k1-AhT9JS=gcfEKU1+Ei=g@mail.gmail.com>

Quick update: The original thread was resurrected and Jeremy Manson from
Google wrote that they are implementing something quite similar internally (
http://mail.openjdk.java.net/pipermail/jdk9-dev/2014-May/000670.html).
Before I reinvent the wheel, I am trying to find out if is interested to
share his research and concept. That would help a lot and could even decide
whether or not this will ever be implemented officially in the JVM.


On Tue, May 6, 2014 at 12:37 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

>  Hi Joel,
>
> That's an example of how you would program with coroutines. I'm concerned
> about the actual implementation under covers. When your coroutine blocks on
> the I/O call where does the current thread go to execute some other
> co-routine? When the I/O is ready how does the blocked thread proceed? The
> programming models for these kinds of things are well established. But your
> proposal is to try and add this to the JDK so you need to consider how it
> can be implemented.
>
> Cheers,
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Joel Richard
> *Sent:* Tuesday, 6 May 2014 8:00 AM
> *To:* dholmes at ieee.org
> *Cc:* concurrency-interest
> *Subject:* Re: [concurrency-interest] Proposal for Hybrid Threading Model
> and simpler Async IO
>
>  Hi David,
>
> I have created a code example which illustrates the usage and behavior of
> how I would implement coroutines* with simple async IO support in Java:
> https://gist.github.com/joelrich/a97d7d364f69c86848ca
>
> * Before I called them tasks or lightweight threads, but coroutine (or
> fiber) seems to be the correct name:
> https://en.wikipedia.org/wiki/Coroutine
>
> On Mon, May 5, 2014 at 10:08 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>> Joel Richard writes:
>> >
>> > Yes, my intent is to implement a M:N hybrid threading model;
>> > however, I think it could co-exist with the existing 1:1 model. I
>> > would suggest that the lightweight threads don't switch
>> > automatically until they block or call Thread.yield (similar to
>> > Node.js). Therefore, the M:N model would be great for servers
>> > while the 1:1 model might still be better suited for
>> > computational tasks or precise timers. In order that library
>> > developers don't have to know whether their code is running in a
>> > native or in a lightweight thread, it would make sense to create
>> > some utilities, e.g. one which executes computational tasks in a
>> > worker thread if the current thread is just a lightweight thread.
>> > Moreover, some methods would change their behavior (without
>> > breaking the API) if they are executed in lightweight threads.
>> > For example, Thread.yield would rather just start/continue
>> > executing another task instead of switching to another thread.
>>
>> Conceptually you may be able to have a system where a M:N scheduler
>> co-exists with a 1:1 scheduler; but I do not think you can simply add that
>> to hotspot without completely rewriting the existing code.
>
>
> It would definitely require a lot of work and adjustments - I don't deny
> that. Apart from the already mentioned changes, we would have to add a
> feature to HotSpot which allows to pause/continue code of a coroutine at an
> arbitrary point without blocking the kernel thread. However, Quasar proofs
> that this is even possible at a language level (with some limitations).
>
>
>> Further, trying to generalize a M:N model to apply to all threads is
>> extremely difficult - how exactly do you pick up another thread and when
>> can you return to the original? Specialized approaches, like ForkJoin,
>> define the computational chunks so that it is easy to do this.
>
>
> I think it should use a cooperative scheduling similar to Node.js (however
> with multiple kernel threads). This would be easy, effective and optimal
> for IO servers.
>
>
>> There are also issues with thread identity that would have to be handled
>> somehow.
>
>
> Where do you see the problem here?
>
>  >
>> > I agree, this would change the performance and even more the
>> > scalability model, but I think that would be something positive
>>
>> Can you back that up with any numbers? "Words are wind ..." ;-)
>>
>> I think that is really hard before implementing a prototype. However, I
> would say that it is general consensus that async socket IO scales better
> than blocking socket IO. I don't think that coroutines would scale better
> than callbacks, but probably comparable. The main advantage would be that
> Java developers would suddenly have access to a huge number of libraries
> which use asynchronous IO and existing software stacks could scale better.
> Moreover, it would be easier to write async IO code.
>
>
>> > and bring a lot of opportunities - especially for application
>> > servers. If I am not mistaken, the JVM doesn't guarantee a
>> > certain performance characteristic or thread behavior.
>>
>> It may not guarantee it but we go to a lot of trouble to try and not
>> introduce discontinuities in the performance curves. People invest a lot of
>> time and effort in tuning large apps, and in designing them for the way
>> things actually work. You can't completely change the threading and
>> performance model for these apps.
>>
>> > and since lightweight threads would be an optional feature, I
>> > think this should be considered as a chance to make Java and the
>> > JVM future proof without re-implementing hundreds of libraries.
>>
>> Are you saying that the VM is either started in traditional mode, or else
>> this new mode? That would address the compatability issue - provided you
>> can implement the new mode without changing the existing performance model.
>>
>> That would be an option, but I think it might be smarter to support both
> modes at the same time as shown in my Gist example above. Application
> servers could then use a coroutine executor service for the request
> handling and all servlets would run in the new non-blocking mode. In
> addition to that, it might make sense to add a command line option which
> enables the coroutine mode by default.
>
> Regards, Joel
>
> David
>> -----
>>
>> > It would make async IO and scalable servers easier than with
>> > Node.js and prevent that companies switch from Java to Go (or
>> > similar languages) just because they have a more appealing
>> > concurrency model right now.
>> >
>> >
>> > Regards, Joel
>> >
>> >
>> >
>> > On Mon, May 5, 2014 at 9:20 AM, Kirk Pepperdine <kirk at kodewerk.com>
>> wrote:
>> >
>> >
>> > On May 4, 2014, at 11:37 PM, Gregg Wonderly <gregg at wonderly.org> wrote:
>> >
>> > > The original version of Java used a runtime environment which
>> > used the term "Green Threads".  This was of course taken out when
>> > sun started trying to make Java more than an Applet engine.
>> >
>> >
>> > I believe the idea for Green Threads stems from lightweight
>> > threads in Solaris. There, a lightweight thread ran on top of a
>> > single kernel thread. The kernel thread time sliced between each
>> > of it?s LWP. I?m not sure it ever worked all that well as CPU?s
>> > at the time of inception weren?t quite what they are today and
>> > scheduling in Solaris always felt sluggish.
>> >
>> > >
>> > > There are two problems.  First is all the visibility and
>> > contention issues which the concurrency package has demonstrated
>> > to not be a simple issue due to rather arcane processor
>> > architecture issues.
>> > >
>> > > What we actually need is infinitely fast cache and cache
>> > coherency algorithms, and much more sane processor designs which
>> > try to allow processors to work together rather than trying
>> > desperately to keep them from interfering with each other.
>> >
>> >
>> > Completely agree, copies are always going to be a problem and
>> > immutability can only be part of the answer.
>> >
>> > Regards,
>> > Kirk
>> >
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140507/aa3da74c/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed May  7 18:00:33 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 07 May 2014 23:00:33 +0100
Subject: [concurrency-interest] JMM and reorderings
In-Reply-To: <1399316547090-10975.post@n7.nabble.com>
References: <1399309158454-10973.post@n7.nabble.com>	<CAPUmR1bqLSryRkTjxnkZCQtSHZ=n3zFoe2o9xWtfZZFhXfNm7Q@mail.gmail.com>
	<1399316547090-10975.post@n7.nabble.com>
Message-ID: <536AAD01.8010109@oracle.com>

Think of it this way:

*intra-*thread semantics isn't broken; that reordering is clearly 
allowed, because *intra-*thread there are no intervening writes to x.


Alex


On 05/05/2014 20:02, thurstonn wrote:
> ...
> But what about the following?
> r2 == 0, r1 == 5
> Which is equivalent to allowing:
> r2 = x
> r1 = x
> y = 5
>
> I want to say that that is a violation of intra-thread conistency
> guarantees
> (or intra-thread semantics if you prefer), but it's not clear to me where
> specifically the JMM would prohibit that.
> ...
> That's surprising, i.e. that r2 = 0, r1 = 5 is allowed
> I guess my thinking is that this violates the *intra-thread* semantics of
> the Thread 1 code, viz.
>
> program order is dictating that r1 can never "see a later value of x" than
> r2 - ok that's not very formal I guess, but it seems to violate the
> intra-thread guarantees;
> yes, there is no happens-before and there is no explicit dependency between
> r1 and r2 . . . so all bets are off?
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/JMM-and-reorderings-tp10973p10975.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140507/bdbd5697/attachment-0001.html>

From email at christian-fries.de  Thu May  8 04:10:56 2014
From: email at christian-fries.de (Christian Fries)
Date: Thu, 8 May 2014 10:10:56 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <53691F2D.3060708@cs.oswego.edu>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
	<A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
	<7235DA14-526C-4F93-A7CE-0417E8731844@christian-fries.de>
	<53691F2D.3060708@cs.oswego.edu>
Message-ID: <3B028602-3F31-434E-B92C-24AF381D4CDD@christian-fries.de>


Dear Doug.

Thank you for your reply. With respect to...

> This does not in general do anything interesting beyond
> making the task recursion stack shallower so that it becomes
> less likely that nested joiners need compensation.

...I believe that the performance issue I am seeing is hard to be attributed to ?less likely that nested joiners need compensation? (90 sec vs. 30 sec - I can provide you with more benchmarks on top of what is already on SO).

In addition I do not understand how this can be an explanation for the following behavior: The following code DEADLOCKS

	// Outer loop
	IntStream.range(0,numberOfTasksInOuterLoop).parallel().forEach(i -> {
		// do work here
		synchronized(this) {
			// Inner loop
			IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
				// do work here
			});
		}
	});

but the following workaround does no longer deadlock

	// Outer loop
	IntStream.range(0,numberOfTasksInOuterLoop).parallel().forEach(i -> {
		// do work here
		synchronized(this) {
			wrapInThread(() ->
				// Inner loop
				IntStream.range(0,numberOfTasksInInnerLoop).parallel().forEach(j -> {
					// do work here
				})
			);
		}
	});

here wrapInThread is just a simple method running its Runnable argument in a new Thread, immediately joining it with the caller.

A complete demo (just execute its main()) can be found here: http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/NestedParallelForEachAndSynchronization.java

The associated SO post is here: http://stackoverflow.com/questions/23489993/nested-java-8-parallel-foreach-loop-perform-poor-is-this-behavior-expected 

Best
Christian


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140508/375b134f/attachment-0001.html>

From email at christian-fries.de  Thu May  8 04:48:20 2014
From: email at christian-fries.de (Christian Fries)
Date: Thu, 8 May 2014 10:48:20 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <53681A20.7070805@cs.oswego.edu>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
Message-ID: <D3986428-0BC0-40E1-8E6F-3F3192FC72AC@christian-fries.de>


Dear All.

With respect to the improvement of the documentation suggested

Am 06.05.2014 um 01:09 schrieb Doug Lea <dl at cs.oswego.edu>:

> Sorry that you and a bunch of people at stackOverflow spent so much time
> on this because you did not did not notice the java.util.stream documentation
> (http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html)
> about the use of independent functions/actions (especially not those including
> synchronization). We should do something about this.

I would like to add two aspects from a user perspective:

First, the linked page ( http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html ) does not mention the ForkJoinPool or Managed Blocker, etc. at all. The words ?pool?, ?common?, ?fork?, ?managed? do not even occur in the document. It is just a discussion of parallel streams from the stream perspective suggesting that code like the one I have used (thread save, no interference, etc.) works fine. (This is not actually my observation, it was a comment on SO).
So a footnote that the backend of parallel streams is currently the FJP and that corresponding limitations apply (use of managed blockers) should be added (on the other hand - I would prefer to have the streams framework work without too much knowledge of the underlying implementation).

Second, I would like to comment on the idea to ?forbid? nested parallelism at all: From a user perspective I try to (or have to) use parallelism a lot. In my library many parts are functional, stateless, free of interference. If you use parallelism a lot it may be that you have parallelism encapsulated ?inside? many small parts and nested parallelism is a natural thing. It is also a good thing. If I have an outer parallel loop of 5 on our 64 core machine it would be a waste to forbid inner (nested) parallelism. Everyone of the 5 could work on its task with a parallelism of > 10 - so we are talking about a factor of 10. - I have this situation in my lib and it works great.

Of course, one aspect of the issue in my previous post is that we are submitting to a COMMON thread pool. A workaround would be to use a dedicated pool for each inner loop. But a common thread pool is an advantage, since it allows to globally control parallelism!

Best
Christian



From dl at cs.oswego.edu  Fri May  9 07:30:48 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 09 May 2014 07:30:48 -0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <3B028602-3F31-434E-B92C-24AF381D4CDD@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
	<A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
	<7235DA14-526C-4F93-A7CE-0417E8731844@christian-fries.de>
	<53691F2D.3060708@cs.oswego.edu>
	<3B028602-3F31-434E-B92C-24AF381D4CDD@christian-fries.de>
Message-ID: <536CBC68.7090409@cs.oswego.edu>

On 05/08/2014 04:10 AM, Christian Fries wrote:
> Thank you for your reply. With respect to...
>
>> This does not in general do anything interesting beyond
>> making the task recursion stack shallower so that it becomes
>> less likely that nested joiners need compensation.
>
> ...I believe that the performance issue I am seeing is hard to be attributed to
> ?less likely that nested joiners need compensation?

It is hard to believe but true. You might check some of this
(as I did) by running on machines with different numbers of
cores (and associated parallelism levels). The probability of
using uncompensated joins decreases with parallelism in this
(and some other) programs, because there are fewer ways that
workers can encounter joins but be unable to help process tasks.
In non-nested j.u.stream parallelism, streams use join-less
completion mechanics (the user-unfriendly CountedCompleter
form of ForkJoinTask that in part generalizes the much
friendlier CompletableFuture) such that they never hit this
problem. Currently, because nested parallelism uses two different
stream expressions, this technique is not used. Somehow
improving this state of affairs is on the todo list for JDK9.

In the mean time, if you need this usage to run fast,
you could try recasting it via the underlying CountedCompleter
mechanics. They are counter-intuitive to program, but we
put a lot of usage examples in the JavaDocs
(http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountedCompleter.html). 


(Plain FJ join techniques are a lot easier to understand and use,
which is why we did not even initially provide public completion
based APIs. In general, concurrent programming techniques
that avoid blocking in any form are often the most efficient
but hardest to use. Although we are lucky that some forms,
like the fluent CompletableFuture API used with lambdas,
are user-friendly enough to avoid internal support
with occasionally surprising limitations like those seen here.

-Doug




From dl at cs.oswego.edu  Fri May  9 08:43:54 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 09 May 2014 08:43:54 -0400
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <CAOe+kYeWoFKCZ0swDjQro0Lny+52k1-AhT9JS=gcfEKU1+Ei=g@mail.gmail.com>
References: <CAOe+kYcR7W7co3w0+287N35Azz5W+4nsSomu_=ACfngfHXuLhA@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCMEDFKGAA.davidcholmes@aapt.net.au>
	<CAOe+kYeWoFKCZ0swDjQro0Lny+52k1-AhT9JS=gcfEKU1+Ei=g@mail.gmail.com>
Message-ID: <536CCD8A.7040908@cs.oswego.edu>

On 05/07/2014 11:54 AM, Joel Richard wrote:
> Quick update: The original thread was resurrected and Jeremy Manson from Google
> wrote that they are implementing something quite similar internally
> (http://mail.openjdk.java.net/pipermail/jdk9-dev/2014-May/000670.html). Before I
> reinvent the wheel, I am trying to find out if is interested to share his
> research and concept. That would help a lot and could even decide whether or not
> this will ever be implemented officially in the JVM.
>

Sorry for delays in addressing this.

Backing up first: Providing lighter-weight thread-like constructions
has been a focus of attention in concurrency for a few decades.
The main idea of the JDK5 java.util.concurrent.Executor
framework was to decouple the notion of parallel execution
from contextual details that might vary (processes vs threads
vs tasks vs even FPGA or GPGPU strands). This works perfectly
if Executors can enforce their contextual usage constraints,
which they often either cannot do or choose not to do.
Usually, they just tell people to please not use Thread
methods or ThreadLocals, and in some cases like ForkJoin-related
classes tell people to please not block. This doesn't always
work out well because users do not want to change their
Thread-based code to use alternative constructions. And it
is hard to create alternatives for all possible ways to block
threads. (See the nested stream parallelism discussion on
this list for one case in point.)

There are (at least) two ways to try to address this. One is as
you suggest, to swap Thread stacks on blocking operations.
Another is to generate another thread to perform any available
work while the original thread blocks. ForkJoinPool uses
a form of this. It is in general a safer move, because
it preserves the relationship between threads, thread-locals,
and stacks assumed by other user and VM code. Plus it tolerates
cross-VM dependencies in which both a thread and its continuation
must make progress, which can occur when blocking is due to IO
representing message sends across cooperating processes on
different machines, that might otherwise lock up.

However, as we've found, even the "safe" strategy for doing this
is not a cure-all. Whether done via new threads of new stacks,
we have found that it is possible (and not rare) for users to
write code that leads to unchecked resource use -- positive
feedback-like loops in which an unbounded number of threads/stacks
must be created. For ForkJoinPool, the best practical alternative
appears to be to limit to constructing the minimal resources that
ensure liveness.

FWIW, I'm encouraged that the concurrency support community is
slowly but surely discovering ways of supporting usages that
avoid the underlying problems to begin with. For example,
as Jeremy Manson noted, most programmers tend not to like most
async and non-blocking IO APIs. But most programmers seem to like
the non-IO-oriented (CompletableFuture+lambda), and similar
(and layerable) Promise- Reactive /Rx-, async-actor-based extensions.
So we plan to further improve and integrate these for JDK9.

-Doug


From oleksandr.otenko at oracle.com  Fri May  9 08:46:52 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 09 May 2014 13:46:52 +0100
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <D3986428-0BC0-40E1-8E6F-3F3192FC72AC@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>
	<D3986428-0BC0-40E1-8E6F-3F3192FC72AC@christian-fries.de>
Message-ID: <536CCE3C.5030803@oracle.com>

To be honest, I don't understand why "nested" parallellism occurs at all.

1. flatten
2. recreate the shape

unless you are dealing with infinite streams; in the latter case the 
order of processing is extremely important, and it is no longer 
"uncontrolled" parallellism.

Alex

On 08/05/2014 09:48, Christian Fries wrote:
> Dear All.
>
> With respect to the improvement of the documentation suggested
>
> Am 06.05.2014 um 01:09 schrieb Doug Lea <dl at cs.oswego.edu>:
>
>> Sorry that you and a bunch of people at stackOverflow spent so much time
>> on this because you did not did not notice the java.util.stream documentation
>> (http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html)
>> about the use of independent functions/actions (especially not those including
>> synchronization). We should do something about this.
> I would like to add two aspects from a user perspective:
>
> First, the linked page ( http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html ) does not mention the ForkJoinPool or Managed Blocker, etc. at all. The words ?pool?, ?common?, ?fork?, ?managed? do not even occur in the document. It is just a discussion of parallel streams from the stream perspective suggesting that code like the one I have used (thread save, no interference, etc.) works fine. (This is not actually my observation, it was a comment on SO).
> So a footnote that the backend of parallel streams is currently the FJP and that corresponding limitations apply (use of managed blockers) should be added (on the other hand - I would prefer to have the streams framework work without too much knowledge of the underlying implementation).
>
> Second, I would like to comment on the idea to ?forbid? nested parallelism at all: From a user perspective I try to (or have to) use parallelism a lot. In my library many parts are functional, stateless, free of interference. If you use parallelism a lot it may be that you have parallelism encapsulated ?inside? many small parts and nested parallelism is a natural thing. It is also a good thing. If I have an outer parallel loop of 5 on our 64 core machine it would be a waste to forbid inner (nested) parallelism. Everyone of the 5 could work on its task with a parallelism of > 10 - so we are talking about a factor of 10. - I have this situation in my lib and it works great.
>
> Of course, one aspect of the issue in my previous post is that we are submitting to a COMMON thread pool. A workaround would be to use a dedicated pool for each inner loop. But a common thread pool is an advantage, since it allows to globally control parallelism!
>
> Best
> Christian
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Fri May  9 08:53:16 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 09 May 2014 08:53:16 -0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <536CCE3C.5030803@oracle.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>	<D3986428-0BC0-40E1-8E6F-3F3192FC72AC@christian-fries.de>
	<536CCE3C.5030803@oracle.com>
Message-ID: <536CCFBC.8010708@cs.oswego.edu>

On 05/09/2014 08:46 AM, Oleksandr Otenko wrote:
> To be honest, I don't understand why "nested" parallellism occurs at all.

Sorry, I should have led off with lt;dr version:

Nested parallelism issues occur because the stream framework
parallelizes one expression at a time. Currently, the only way you
can express nesting is to use more than one stream expression.

-Doug

> 1. flatten
> 2. recreate the shape
>
> unless you are dealing with infinite streams; in the latter case the order of
> processing is extremely important, and it is no longer "uncontrolled" parallellism.
>
> Alex
>
> On 08/05/2014 09:48, Christian Fries wrote:
>> Dear All.
>>
>> With respect to the improvement of the documentation suggested
>>
>> Am 06.05.2014 um 01:09 schrieb Doug Lea <dl at cs.oswego.edu>:
>>
>>> Sorry that you and a bunch of people at stackOverflow spent so much time
>>> on this because you did not did not notice the java.util.stream documentation
>>> (http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html)
>>> about the use of independent functions/actions (especially not those including
>>> synchronization). We should do something about this.
>> I would like to add two aspects from a user perspective:
>>
>> First, the linked page (
>> http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html
>> ) does not mention the ForkJoinPool or Managed Blocker, etc. at all. The words
>> ?pool?, ?common?, ?fork?, ?managed? do not even occur in the document. It is
>> just a discussion of parallel streams from the stream perspective suggesting
>> that code like the one I have used (thread save, no interference, etc.) works
>> fine. (This is not actually my observation, it was a comment on SO).
>> So a footnote that the backend of parallel streams is currently the FJP and
>> that corresponding limitations apply (use of managed blockers) should be added
>> (on the other hand - I would prefer to have the streams framework work without
>> too much knowledge of the underlying implementation).
>>
>> Second, I would like to comment on the idea to ?forbid? nested parallelism at
>> all: From a user perspective I try to (or have to) use parallelism a lot. In
>> my library many parts are functional, stateless, free of interference. If you
>> use parallelism a lot it may be that you have parallelism encapsulated
>> ?inside? many small parts and nested parallelism is a natural thing. It is
>> also a good thing. If I have an outer parallel loop of 5 on our 64 core
>> machine it would be a waste to forbid inner (nested) parallelism. Everyone of
>> the 5 could work on its task with a parallelism of > 10 - so we are talking
>> about a factor of 10. - I have this situation in my lib and it works great.
>>
>> Of course, one aspect of the issue in my previous post is that we are
>> submitting to a COMMON thread pool. A workaround would be to use a dedicated
>> pool for each inner loop. But a common thread pool is an advantage, since it
>> allows to globally control parallelism!
>>
>> Best
>> Christian
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>




From ron.pressler at gmail.com  Fri May  9 08:53:50 2014
From: ron.pressler at gmail.com (Ron Pressler)
Date: Fri, 9 May 2014 15:53:50 +0300
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <CAOe+kYeUk+okQhrAKPBpymJrXY0PNnOBwFgkj0A-ynHtWY4uLw@mail.gmail.com>
References: <CAOe+kYdfVngaF-9PEUYysQcOg_Pt-FQT-xkwFiJzR+OsyOv9PA@mail.gmail.com>
	<CACuKZqFoxgPo61VdJ6E4YdXYphvTbDmxfe_uEX92Kw64qOjmAQ@mail.gmail.com>
	<CAOe+kYeUk+okQhrAKPBpymJrXY0PNnOBwFgkj0A-ynHtWY4uLw@mail.gmail.com>
Message-ID: <CABg6-qiUbujC_i=6=N1TceBG39io-LHLyK7dE40QoKUgHW72wA@mail.gmail.com>

Quasar gives you fiber-blocking NIO by implementing the blocking NIO API
using async NIO under the covers.


On Mon, May 5, 2014 at 12:03 AM, Joel Richard <ri.joel at gmail.com> wrote:

> That is a really interesting project. Thank you very much for mentioning
> it.
>
> As far as I can see, the missing part is that it cannot transform thread
> blocking IO into fiber blocking IO (which is asynchronous) yet. Maybe even
> that could be implemented partially, if it replaced the most important
> blocking classes of the Java standard library while loading with an
> equivalent non-blocking version (based on NIO). This would be fantastic for
> a proof of concept of my initial idea since it wouldn't require any changes
> in the native C code.
>
> @Ron, I hope it's ok for you that I have added you to this thread. Have
> you thought about this before? Could you imagine that this would work?
>
> Regards, Joel
>
>
> On Sun, May 4, 2014 at 6:39 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>
>> See Ron Pressler's work
>>
>>
>> http://cs.oswego.edu/pipermail/concurrency-interest/2013-October/011900.html
>>
>>
>> http://cs.oswego.edu/pipermail/concurrency-interest/2014-February/012347.html
>>
>> which I think is definitely the way to go. Other schemes - callback,
>> future/promise, generator, even C#'s async - are just DSLs trying to
>> simulate the threading model, imperfectly. If threads were lightweight
>> to begin with, these solutions would not have been invented.
>>
>> Zhong Yu
>>
>>
>>
>> On Sun, May 4, 2014 at 6:06 AM, Joel Richard <ri.joel at gmail.com> wrote:
>> > Hi,
>> >
>> > Recently I have researched a little about how different languages
>> implement
>> > async IO because I wanted to implement a high scalability web server.
>> When I
>> > looked at Google Go, I realized that the common callback approach is
>> > actually just a language-level workaround to fix a VM limitation.
>> >
>> > So I wondered whether and how it could be possible to implement
>> something
>> > similar to goroutines for the JVM. Here are my thoughts: There should
>> be an
>> > ExecutorService implementation which executes tasks in a special mode.
>> If
>> > you use any (so far) blocking methods (like Thread.sleep, network/file
>> > system IO, Object.wait and synchronization) in such a task it uses
>> however
>> > not the blocking native implementation, but an non-blocking version of
>> it.
>> > While it waits for the method to complete, it can continue with another
>> task
>> > in the same native thread. This means that all tasks can share a small
>> > native thread pool. If longer computations have to be done, the
>> developer
>> > can either call Thread.yield() from time to time or just move the
>> > computation to a classical thread.
>> >
>> > With such an approach it would be much easier to write highly scalable
>> > server software* and it would make it even possible to run most existing
>> > libraries with minimal changes as asynchronous IO libraries. Basically,
>> they
>> > would just have to make sure that they call Thread.yield() in longer
>> > computationally intensive loops and don't start additional normal
>> threads.
>> > Last but not least, the behavior of existing threads wouldn't be
>> changed at
>> > all.
>> >
>> > * The advantage of async IO compared to blocking IO is that it doesn?t
>> > require expensive thread context switches and that it needs less memory
>> > (because fewer threads allocate less memory for stacks).
>> >
>> > As mentioned above, this would require implementing an asynchronous
>> version
>> > of all blocking native methods. I am not that familiar with JNI nor C,
>> but I
>> > think this could by achieved by allowing the developer to define a
>> second C
>> > function with the _async suffix which calls a callback instead of
>> returning
>> > a value. If the Java native method is called in a normal thread, the
>> old C
>> > function is called and otherwise the _async version of it. Since Java
>> > already supports async IO with NIO, I think it should be technically
>> > possible to implement asynchronous versions for almost all of the
>> currently
>> > blocking native functions. If an operating system doesn't support
>> certain
>> > non-blocking operations, it could just block a special thread instead
>> (as
>> > far as I know Go does it similar).
>> >
>> > I am aware that the required changes would be complicated, but I think
>> it is
>> > worth a discussion since concurrency, non-blocking IO and high
>> scalability
>> > will be even more important in the future. This feature would make it
>> much
>> > easier to fulfill such requirements. For example, it would not be
>> necessary
>> > anymore to create a second version of each so far blocking API. Also all
>> > standardized APIs like JDBC or JPA would suddenly all have a
>> non-blocking
>> > implementation. The amount of work which could be saved would be
>> enormous.
>> >
>> > What do you think about this idea? Has something like that been
>> discussed
>> > before?
>> >
>> > Regards, Joel
>> >
>> > PS I sent a similar email first to the jdk9-dev because I wasn't sure
>> which
>> > mailing list to use.
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140509/ac5067e4/attachment-0001.html>

From email at christian-fries.de  Fri May  9 09:05:05 2014
From: email at christian-fries.de (Christian Fries)
Date: Fri, 9 May 2014 15:05:05 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <536CCE3C.5030803@oracle.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>
	<D3986428-0BC0-40E1-8E6F-3F3192FC72AC@christian-fries.de>
	<536CCE3C.5030803@oracle.com>
Message-ID: <506E6E9C-94C6-42A8-BCBF-0F15FA5BF3D0@christian-fries.de>


Dear Alex.

Example: The Levenberg-Marquardt non-linear least square optimization can be parallelized easily on arguments of the objective function x -> f(x). Here f(x) is a function an the LM class performs parallel valuation on a stream of arguments. The function is provided as a lambda or abstract class. In many applications x is a vector and the function f is itself a composite of independent components which can be calculated in parallel.

Hence I have two classes: F (the objective function) and LM (the optimized) and f is provided to LM will introduce nested parallelism.

This is a real application. I already have this working perfectly in Java 6 with nested parallelism (without FJP).

Wouldn?t flattening mean loss of abstraction (no abstraction between F and LM), spaghetti code, etc.

Christian

PS: I am really surprised that there is so much discussion on this list in that direction. Modern frameworks are full of nested parallelism. Did anybody check the code that I have posted and confirm the behavior?

Am 09.05.2014 um 14:46 schrieb Oleksandr Otenko <oleksandr.otenko at oracle.com>:

> To be honest, I don't understand why "nested" parallellism occurs at all.
> 
> 1. flatten
> 2. recreate the shape
> 
> unless you are dealing with infinite streams; in the latter case the order of processing is extremely important, and it is no longer "uncontrolled" parallellism.
> 
> Alex
> 
> On 08/05/2014 09:48, Christian Fries wrote:
>> Dear All.
>> 
>> With respect to the improvement of the documentation suggested
>> 
>> Am 06.05.2014 um 01:09 schrieb Doug Lea <dl at cs.oswego.edu>:
>> 
>>> Sorry that you and a bunch of people at stackOverflow spent so much time
>>> on this because you did not did not notice the java.util.stream documentation
>>> (http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html)
>>> about the use of independent functions/actions (especially not those including
>>> synchronization). We should do something about this.
>> I would like to add two aspects from a user perspective:
>> 
>> First, the linked page ( http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html ) does not mention the ForkJoinPool or Managed Blocker, etc. at all. The words ?pool?, ?common?, ?fork?, ?managed? do not even occur in the document. It is just a discussion of parallel streams from the stream perspective suggesting that code like the one I have used (thread save, no interference, etc.) works fine. (This is not actually my observation, it was a comment on SO).
>> So a footnote that the backend of parallel streams is currently the FJP and that corresponding limitations apply (use of managed blockers) should be added (on the other hand - I would prefer to have the streams framework work without too much knowledge of the underlying implementation).
>> 
>> Second, I would like to comment on the idea to ?forbid? nested parallelism at all: From a user perspective I try to (or have to) use parallelism a lot. In my library many parts are functional, stateless, free of interference. If you use parallelism a lot it may be that you have parallelism encapsulated ?inside? many small parts and nested parallelism is a natural thing. It is also a good thing. If I have an outer parallel loop of 5 on our 64 core machine it would be a waste to forbid inner (nested) parallelism. Everyone of the 5 could work on its task with a parallelism of > 10 - so we are talking about a factor of 10. - I have this situation in my lib and it works great.
>> 
>> Of course, one aspect of the issue in my previous post is that we are submitting to a COMMON thread pool. A workaround would be to use a dedicated pool for each inner loop. But a common thread pool is an advantage, since it allows to globally control parallelism!
>> 
>> Best
>> Christian
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From ron.pressler at gmail.com  Fri May  9 09:06:47 2014
From: ron.pressler at gmail.com (Ron Pressler)
Date: Fri, 9 May 2014 16:06:47 +0300
Subject: [concurrency-interest] Proposal for Hybrid Threading Model and
 simpler Async IO
In-Reply-To: <536CCD8A.7040908@cs.oswego.edu>
References: <CAOe+kYcR7W7co3w0+287N35Azz5W+4nsSomu_=ACfngfHXuLhA@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEDFKGAA.davidcholmes@aapt.net.au>
	<CAOe+kYeWoFKCZ0swDjQro0Lny+52k1-AhT9JS=gcfEKU1+Ei=g@mail.gmail.com>
	<536CCD8A.7040908@cs.oswego.edu>
Message-ID: <CABg6-qjiXNbsAzr2t2ihPJU0Ufe2gh0Z20g4R6aRRk6reVbmvQ@mail.gmail.com>

Hi Doug, Joel, and all.
As others have mentioned, Quasar does indeed implement true fibers (just
like Go) on the JVM, and abstracts them, along with plain Java threads as
something called Strand, so that various constructs work equally well
whether run in a thread or a fiber. Coroutines are currently implemented at
the bytecode level with compile time/load time instrumentation, and fibers
are coroutines scheduled on an async FJPool. See this blog post:
http://blog.paralleluniverse.co/2014/02/06/fibers-threads-strands/ (which
I've linked to on this mailing list before).

Fibers preserve identity, and context (ThreadLocals). Also, we have a very
simple mechanism that turns any asynchronous, callback-based API, to a
fiber-blocking one (it's all open source).

With respect to Doug's concern over resource misuse, Quasar would warn you
if a fiber is taking too much CPU time, or if it's accidentally blocking
the underlying kernel thread.

While constructs like composable futures (and other monads) do make APIs a
easier to use, they essentially recreate the concept of the thread by
providing another way to say "block until this happens, and then do that".
We don't need this abstraction because we already have one. We just need a
different implementation. That's why people like Go and Quasar.

However, we would very much like to see fibers at the JVM level. Doug, I
realize that this is a huge undertaking, but is it something you would
consider as a JSR? What would be the timeframe for implementation in your
opinion?

Ron


On Fri, May 9, 2014 at 3:43 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 05/07/2014 11:54 AM, Joel Richard wrote:
>
>> Quick update: The original thread was resurrected and Jeremy Manson from
>> Google
>> wrote that they are implementing something quite similar internally
>> (http://mail.openjdk.java.net/pipermail/jdk9-dev/2014-May/000670.html).
>> Before I
>> reinvent the wheel, I am trying to find out if is interested to share his
>> research and concept. That would help a lot and could even decide whether
>> or not
>> this will ever be implemented officially in the JVM.
>>
>>
> Sorry for delays in addressing this.
>
> Backing up first: Providing lighter-weight thread-like constructions
> has been a focus of attention in concurrency for a few decades.
> The main idea of the JDK5 java.util.concurrent.Executor
> framework was to decouple the notion of parallel execution
> from contextual details that might vary (processes vs threads
> vs tasks vs even FPGA or GPGPU strands). This works perfectly
> if Executors can enforce their contextual usage constraints,
> which they often either cannot do or choose not to do.
> Usually, they just tell people to please not use Thread
> methods or ThreadLocals, and in some cases like ForkJoin-related
> classes tell people to please not block. This doesn't always
> work out well because users do not want to change their
> Thread-based code to use alternative constructions. And it
> is hard to create alternatives for all possible ways to block
> threads. (See the nested stream parallelism discussion on
> this list for one case in point.)
>
> There are (at least) two ways to try to address this. One is as
> you suggest, to swap Thread stacks on blocking operations.
> Another is to generate another thread to perform any available
> work while the original thread blocks. ForkJoinPool uses
> a form of this. It is in general a safer move, because
> it preserves the relationship between threads, thread-locals,
> and stacks assumed by other user and VM code. Plus it tolerates
> cross-VM dependencies in which both a thread and its continuation
> must make progress, which can occur when blocking is due to IO
> representing message sends across cooperating processes on
> different machines, that might otherwise lock up.
>
> However, as we've found, even the "safe" strategy for doing this
> is not a cure-all. Whether done via new threads of new stacks,
> we have found that it is possible (and not rare) for users to
> write code that leads to unchecked resource use -- positive
> feedback-like loops in which an unbounded number of threads/stacks
> must be created. For ForkJoinPool, the best practical alternative
> appears to be to limit to constructing the minimal resources that
> ensure liveness.
>
> FWIW, I'm encouraged that the concurrency support community is
> slowly but surely discovering ways of supporting usages that
> avoid the underlying problems to begin with. For example,
> as Jeremy Manson noted, most programmers tend not to like most
> async and non-blocking IO APIs. But most programmers seem to like
> the non-IO-oriented (CompletableFuture+lambda), and similar
> (and layerable) Promise- Reactive /Rx-, async-actor-based extensions.
> So we plan to further improve and integrate these for JDK9.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140509/3ccfb5a7/attachment.html>

From dl at cs.oswego.edu  Fri May  9 09:25:22 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 09 May 2014 09:25:22 -0400
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <506E6E9C-94C6-42A8-BCBF-0F15FA5BF3D0@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>	<D3986428-0BC0-40E1-8E6F-3F3192FC72AC@christian-fries.de>	<536CCE3C.5030803@oracle.com>
	<506E6E9C-94C6-42A8-BCBF-0F15FA5BF3D0@christian-fries.de>
Message-ID: <536CD742.50206@cs.oswego.edu>

On 05/09/2014 09:05 AM, Christian Fries wrote:
> Wouldn?t flattening mean loss of abstraction (no abstraction between F and
> LM), spaghetti code, etc.


Somehow we seem to be talking past each other. Here's a restart:

1. j.u.Stream.parallel() automates many forms of parallelism.

2. It doesn't do a great job for your usage on your computer.

3. Because of side issues (like blocking Semaphores), it took
some effort to discover underlying problems.

4. But now we know why, and will try to improve. For reasons you
probably don't care about, it is hard to improve this usage while
not hurting others.

5. In the mean time, you could if desired manually arrange
parallelism rather than relying on j.u.streams.

-Doug






From oleksandr.otenko at oracle.com  Fri May  9 10:51:29 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 09 May 2014 15:51:29 +0100
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <506E6E9C-94C6-42A8-BCBF-0F15FA5BF3D0@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>
	<D3986428-0BC0-40E1-8E6F-3F3192FC72AC@christian-fries.de>
	<536CCE3C.5030803@oracle.com>
	<506E6E9C-94C6-42A8-BCBF-0F15FA5BF3D0@christian-fries.de>
Message-ID: <536CEB71.1040103@oracle.com>

It never hurts to see if your abstraction meets the expectation of what 
FJP can do.

You have a tree of expressions that must be beta-reduced. The meaning of 
"nested" is undefined, and the order of evaluation is meaningless for 
"eager" evaluation strategies.

"Flattening" here means representing the tree as a list of expressions 
that need reducing. "Reconstructing" here means placing the reduction 
result back into the same structure.

If you don't expect FJP to work out arbitrary graph of expressions that 
needs reducing, then you need to construct the graph and reduce it yourself.

Alex


On 09/05/2014 14:05, Christian Fries wrote:
> Dear Alex.
>
> Example: The Levenberg-Marquardt non-linear least square optimization can be parallelized easily on arguments of the objective function x -> f(x). Here f(x) is a function an the LM class performs parallel valuation on a stream of arguments. The function is provided as a lambda or abstract class. In many applications x is a vector and the function f is itself a composite of independent components which can be calculated in parallel.
>
> Hence I have two classes: F (the objective function) and LM (the optimized) and f is provided to LM will introduce nested parallelism.
>
> This is a real application. I already have this working perfectly in Java 6 with nested parallelism (without FJP).
>
> Wouldn?t flattening mean loss of abstraction (no abstraction between F and LM), spaghetti code, etc.
>
> Christian
>
> PS: I am really surprised that there is so much discussion on this list in that direction. Modern frameworks are full of nested parallelism. Did anybody check the code that I have posted and confirm the behavior?
>
> Am 09.05.2014 um 14:46 schrieb Oleksandr Otenko <oleksandr.otenko at oracle.com>:
>
>> To be honest, I don't understand why "nested" parallellism occurs at all.
>>
>> 1. flatten
>> 2. recreate the shape
>>
>> unless you are dealing with infinite streams; in the latter case the order of processing is extremely important, and it is no longer "uncontrolled" parallellism.
>>
>> Alex
>>
>> On 08/05/2014 09:48, Christian Fries wrote:
>>> Dear All.
>>>
>>> With respect to the improvement of the documentation suggested
>>>
>>> Am 06.05.2014 um 01:09 schrieb Doug Lea <dl at cs.oswego.edu>:
>>>
>>>> Sorry that you and a bunch of people at stackOverflow spent so much time
>>>> on this because you did not did not notice the java.util.stream documentation
>>>> (http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html)
>>>> about the use of independent functions/actions (especially not those including
>>>> synchronization). We should do something about this.
>>> I would like to add two aspects from a user perspective:
>>>
>>> First, the linked page ( http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html ) does not mention the ForkJoinPool or Managed Blocker, etc. at all. The words ?pool?, ?common?, ?fork?, ?managed? do not even occur in the document. It is just a discussion of parallel streams from the stream perspective suggesting that code like the one I have used (thread save, no interference, etc.) works fine. (This is not actually my observation, it was a comment on SO).
>>> So a footnote that the backend of parallel streams is currently the FJP and that corresponding limitations apply (use of managed blockers) should be added (on the other hand - I would prefer to have the streams framework work without too much knowledge of the underlying implementation).
>>>
>>> Second, I would like to comment on the idea to ?forbid? nested parallelism at all: From a user perspective I try to (or have to) use parallelism a lot. In my library many parts are functional, stateless, free of interference. If you use parallelism a lot it may be that you have parallelism encapsulated ?inside? many small parts and nested parallelism is a natural thing. It is also a good thing. If I have an outer parallel loop of 5 on our 64 core machine it would be a waste to forbid inner (nested) parallelism. Everyone of the 5 could work on its task with a parallelism of > 10 - so we are talking about a factor of 10. - I have this situation in my lib and it works great.
>>>
>>> Of course, one aspect of the issue in my previous post is that we are submitting to a COMMON thread pool. A workaround would be to use a dedicated pool for each inner loop. But a common thread pool is an advantage, since it allows to globally control parallelism!
>>>
>>> Best
>>> Christian
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at oracle.com  Fri May  9 11:47:19 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 09 May 2014 16:47:19 +0100
Subject: [concurrency-interest] ForkJoinPool not designed for nested
 Java 8 streams.parallel().forEach( ... )
In-Reply-To: <536CEB71.1040103@oracle.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>
	<D3986428-0BC0-40E1-8E6F-3F3192FC72AC@christian-fries.de>
	<536CCE3C.5030803@oracle.com>
	<506E6E9C-94C6-42A8-BCBF-0F15FA5BF3D0@christian-fries.de>
	<536CEB71.1040103@oracle.com>
Message-ID: <536CF887.9070205@oracle.com>

To make my point clearer,

suppose, you have tasks A and B. Task A computes x+y, task B produces 
tasks C and D.

Without knowing that task B produces more work, there is no way to keep 
all threads busy in general case.

Alex

On 09/05/2014 15:51, Oleksandr Otenko wrote:
> It never hurts to see if your abstraction meets the expectation of 
> what FJP can do.
>
> You have a tree of expressions that must be beta-reduced. The meaning 
> of "nested" is undefined, and the order of evaluation is meaningless 
> for "eager" evaluation strategies.
>
> "Flattening" here means representing the tree as a list of expressions 
> that need reducing. "Reconstructing" here means placing the reduction 
> result back into the same structure.
>
> If you don't expect FJP to work out arbitrary graph of expressions 
> that needs reducing, then you need to construct the graph and reduce 
> it yourself.
>
> Alex
>
>
> On 09/05/2014 14:05, Christian Fries wrote:
>> Dear Alex.
>>
>> Example: The Levenberg-Marquardt non-linear least square optimization 
>> can be parallelized easily on arguments of the objective function x 
>> -> f(x). Here f(x) is a function an the LM class performs parallel 
>> valuation on a stream of arguments. The function is provided as a 
>> lambda or abstract class. In many applications x is a vector and the 
>> function f is itself a composite of independent components which can 
>> be calculated in parallel.
>>
>> Hence I have two classes: F (the objective function) and LM (the 
>> optimized) and f is provided to LM will introduce nested parallelism.
>>
>> This is a real application. I already have this working perfectly in 
>> Java 6 with nested parallelism (without FJP).
>>
>> Wouldn?t flattening mean loss of abstraction (no abstraction between 
>> F and LM), spaghetti code, etc.
>>
>> Christian
>>
>> PS: I am really surprised that there is so much discussion on this 
>> list in that direction. Modern frameworks are full of nested 
>> parallelism. Did anybody check the code that I have posted and 
>> confirm the behavior?
>>
>> Am 09.05.2014 um 14:46 schrieb Oleksandr Otenko 
>> <oleksandr.otenko at oracle.com>:
>>
>>> To be honest, I don't understand why "nested" parallellism occurs at 
>>> all.
>>>
>>> 1. flatten
>>> 2. recreate the shape
>>>
>>> unless you are dealing with infinite streams; in the latter case the 
>>> order of processing is extremely important, and it is no longer 
>>> "uncontrolled" parallellism.
>>>
>>> Alex
>>>
>>> On 08/05/2014 09:48, Christian Fries wrote:
>>>> Dear All.
>>>>
>>>> With respect to the improvement of the documentation suggested
>>>>
>>>> Am 06.05.2014 um 01:09 schrieb Doug Lea <dl at cs.oswego.edu>:
>>>>
>>>>> Sorry that you and a bunch of people at stackOverflow spent so 
>>>>> much time
>>>>> on this because you did not did not notice the java.util.stream 
>>>>> documentation
>>>>> (http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html) 
>>>>>
>>>>> about the use of independent functions/actions (especially not 
>>>>> those including
>>>>> synchronization). We should do something about this.
>>>> I would like to add two aspects from a user perspective:
>>>>
>>>> First, the linked page ( 
>>>> http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html 
>>>> ) does not mention the ForkJoinPool or Managed Blocker, etc. at 
>>>> all. The words ?pool?, ?common?, ?fork?, ?managed? do not even 
>>>> occur in the document. It is just a discussion of parallel streams 
>>>> from the stream perspective suggesting that code like the one I 
>>>> have used (thread save, no interference, etc.) works fine. (This is 
>>>> not actually my observation, it was a comment on SO).
>>>> So a footnote that the backend of parallel streams is currently the 
>>>> FJP and that corresponding limitations apply (use of managed 
>>>> blockers) should be added (on the other hand - I would prefer to 
>>>> have the streams framework work without too much knowledge of the 
>>>> underlying implementation).
>>>>
>>>> Second, I would like to comment on the idea to ?forbid? nested 
>>>> parallelism at all: From a user perspective I try to (or have to) 
>>>> use parallelism a lot. In my library many parts are functional, 
>>>> stateless, free of interference. If you use parallelism a lot it 
>>>> may be that you have parallelism encapsulated ?inside? many small 
>>>> parts and nested parallelism is a natural thing. It is also a good 
>>>> thing. If I have an outer parallel loop of 5 on our 64 core machine 
>>>> it would be a waste to forbid inner (nested) parallelism. Everyone 
>>>> of the 5 could work on its task with a parallelism of > 10 - so we 
>>>> are talking about a factor of 10. - I have this situation in my lib 
>>>> and it works great.
>>>>
>>>> Of course, one aspect of the issue in my previous post is that we 
>>>> are submitting to a COMMON thread pool. A workaround would be to 
>>>> use a dedicated pool for each inner loop. But a common thread pool 
>>>> is an advantage, since it allows to globally control parallelism!
>>>>
>>>> Best
>>>> Christian
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From robert.nicholson at gmail.com  Sun May 11 09:33:04 2014
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Sun, 11 May 2014 08:33:04 -0500
Subject: [concurrency-interest] Removing  from the Subject?
Message-ID: <9E4BA70B-FC8D-442D-B69E-152A70A7F9DD@gmail.com>

Does anybody have any workflow that allows them to store these emails in their cloud but remove the [concurrency-interest] from the Subject line? It takes up too much space on small devices so I?m interested in any approach that lets you automate it?s removal without needing to send thru an intermediary mail processor.

From email at christian-fries.de  Wed May 14 16:39:14 2014
From: email at christian-fries.de (Christian Fries)
Date: Wed, 14 May 2014 22:39:14 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <536CD742.50206@cs.oswego.edu>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>	<53681A20.7070805@cs.oswego.edu>	<D3986428-0BC0-40E1-8E6F-3F3192FC72AC@christian-fries.de>	<536CCE3C.5030803@oracle.com>
	<506E6E9C-94C6-42A8-BCBF-0F15FA5BF3D0@christian-fries.de>
	<536CD742.50206@cs.oswego.edu>
Message-ID: <F21B15C8-BE3B-4851-8C52-B870DCA18A1D@christian-fries.de>


Dear All.

Indeed, it appeared as if we were going a bit past each others in this discussions. I will stick to executor framework for nested parallelism, but since I did some further analysis, let me try to give a summary:

My concern was and is that there is a serious implementation bug in the 1.8u5 ForkJoinPool framework (specifically ForkJoinTask, likely line 401), rendering it potentially inappropriate for nested loops, threatening the versatility of Java parallel streams. I do not say that Fork/Join as a theoretical / academic concept is not appropriate for nested parallelism. In fact, I believe the opposite is true. Maybe that was a misunderstanding. - I am just concerned about the implementation.

The bug will lead to unexpected deadlocks and serious performance losses. I do have a workaround/bugfix.

For the performance: my test results are (reproducible, test cases without any blocking operation):

Timing for nested loop test cases, outer loop parallel (parallelism set to 2):
	inner loop sequential           = 27,61 +/- 0,50 (min: 26,62 , max: 28,61)	[CPU Time: 76,91]	(number of threads in F/J pool: 2)
	inner loop parallel             = 41,18 +/- 2,99 (min: 33,14 , max: 45,59)	[CPU Time: 106,00]	(number of threads in F/J pool: 8)
	inner loop parallel with bugfix = 26,99 +/- 0,81 (min: 25,75 , max: 28,77)	[CPU Time: 77,73]	(number of threads in F/J pool: 2)
(test cases were run independent with 20 warm up runs and 20 test runs on an Intel i7 at 2.6 GHz).

If you monitor the cpu usage of the jvm process (the above is just the bare getThreadCpuTime for the f/j threads and the main thread), then you will see that it is even worse: without the bug fix the jvm need over twice as much cpu time to be still slower by 30%.

For the deadlocks: The following code will result in a deadlock (surely):

	public static void nestedLoopDeadlockDemo() {
		System.setProperty("java.util.concurrent.ForkJoinPool.common.parallelism","8");
		Object lock = new Object();

		IntStream.range(0,24).parallel().forEach(i -> {
			// do some work here
			synchronized(lock) {
				IntStream.range(0,100).parallel().forEach(j -> {
					// do some work here
				});
			}
		});
		
		System.out.println("Done.");
	}

(and the problem is gone with that bugfix). Note: I really learned something from this discussion and discussing the design of my test case is nice, but that test case exists primarily to illustrate the bug. I do know that nesting can be avoided (though I believe nesting improves abstraction in some cases) and I do know that synchronization has to be done with care. With respect to some arguments which had appeared here: The deadlocking has nothing to do with compensation threads. I monitor the creation of threads and there are no compensation threads at all during the whole test. Also note, that the documentation of the backend ForkJoinPool states that
"The pool attempts to maintain enough active (or available) threads by dynamically adding, suspending, or resuming internal worker threads, even if some tasks are stalled waiting to join others. However, no such adjustments are guaranteed in the face of blocked IO or other unmanaged synchronization."
- but a deadlock would only be expected if the number of available threads is exhausted (and we are far below that number).

If you consider the same setup without the synchronized - i.e. plain nested parallel loops - then the bug will still results in a big performance loss in some cases.

More details for that bug:
Run the program referenced below in a debugger. It will hang in a deadlock in the last test case.
In the debugger suspend all threads and check where theses threads are waiting.
You will find out that all threads wait for the synchronize lock inside the outer loop and that lock is owned by one of the threads, lets call that owner of the lock ForkJoinPool.commonPool-worker-7.
If you check ForkJoinPool.commonPool-worker-7 (the owner of the lock) then you see that he waits for the synchronize lock, but he is already inside the inner loop. Now, lets check why a pice of code inside the synchronize waits for the lock: You see that the wait() is issued by the awaitJoin() in line 402 of ForkJoinTask. This is wrong, instead that task should have done an externalWaitDone(). Explanation: That task is the main task of the inner loop, i.e., a task of the outer loop that created the inner loop, but (due to a bug) that tasks considers itself as a forked worker (of the inner loop) and creates a join - effectively joining with the outer loop?s task. The problem is that if the inner loop is running on a forked worker of the outer loop, we cannot distinguish forked (inner loop?s) worker from main threads because line 401 is always true.
Double Checking: If the explanation in 2.2 would be correct, the problem would go away if we fix line 401 (and also all other corresponding tests). To fix this we change the type of the thread containing the inner loop from ForkJoinWorkerThread to Thread (by creating a wrapper). Indeed: this fixed that deadlock and greatly improved performance.
For a complete test demonstrating the deadlock and the workaround see: NestedParallelForEachAndSynchronization.java at
http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/NestedParallelForEachAndSynchronization.java

For the test producing the performance benchmarks see: NestedParallelForEachBenchmark.java at
http://svn.finmath.net/finmath%20experiments/trunk/src/net/finmath/experiments/concurrency/NestedParallelForEachBenchmark.java
If you run this test case for test case 1 and test case 3 you will note that without the bug fix the ForkJoinPool will spawn a lot (8) worker threads and burn a lot of cpu time (sometimes runs at 500% where the version with the bug fix will always stay around 200% with 2 worker threads)

I am not sure how easy it is to fix this, but since I already provide a workaround, it appears that a bug fix should be a minor correction to the F/J implementation. (If you like, I can comment a bit more on that might get changed in the ForkJoinTask).

Would you consider the deadlocking a bug too? Could anybody look at this? I believe this has to be fixed soon (cannot wait until JDK 9).

Best
Christian


Am 09.05.2014 um 15:25 schrieb Doug Lea <dl at cs.oswego.edu>:

> On 05/09/2014 09:05 AM, Christian Fries wrote:
>> Wouldn?t flattening mean loss of abstraction (no abstraction between F and
>> LM), spaghetti code, etc.
> 
> 
> Somehow we seem to be talking past each other. Here's a restart:
> 
> 1. j.u.Stream.parallel() automates many forms of parallelism.
> 
> 2. It doesn't do a great job for your usage on your computer.
> 
> 3. Because of side issues (like blocking Semaphores), it took
> some effort to discover underlying problems.
> 
> 4. But now we know why, and will try to improve. For reasons you
> probably don't care about, it is hard to improve this usage while
> not hurting others.
> 
> 5. In the mean time, you could if desired manually arrange
> parallelism rather than relying on j.u.streams.
> 
> -Doug
> 
> 
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140514/54921651/attachment.html>

From email at christian-fries.de  Wed May 14 16:53:07 2014
From: email at christian-fries.de (Christian Fries)
Date: Wed, 14 May 2014 22:53:07 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
	<A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
Message-ID: <D7A230D4-6B29-4CB6-87D1-810936327658@christian-fries.de>


Dear Paul.

> 
> :-) I think what you have now with CPU burning is more relevant and also different from where you started. Have you tried writing a jmh benchmark instead? i would recommend doing so if you want to further explore the performance characteristics.
> 

I wrote a JMH benchmark, but micro-benchmarking is not the right tool to show the impact of that bug, since the micro-benchmarks will capture ?microscopic? things like thread creation overhead, etc.
whereas the problem appears on the larger scale.

I have created a test case without any blocking operation and the results are that nested parallel loops are a factor 2 slower compared to fixing that bug - see my previous post to concurrency-interest.
For me, the deadlocking is still a more serious problem?

Best
Christian



From paul.sandoz at oracle.com  Thu May 15 03:50:20 2014
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Thu, 15 May 2014 09:50:20 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <D7A230D4-6B29-4CB6-87D1-810936327658@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
	<A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
	<D7A230D4-6B29-4CB6-87D1-810936327658@christian-fries.de>
Message-ID: <915F5FFE-D19C-4A97-9C82-D2FE141F5632@oracle.com>

Hi Christian,

On May 14, 2014, at 10:53 PM, Christian Fries <email at christian-fries.de> wrote:
> 
> Dear Paul.
> 
>> 
>> :-) I think what you have now with CPU burning is more relevant and also different from where you started. Have you tried writing a jmh benchmark instead? i would recommend doing so if you want to further explore the performance characteristics.
>> 
> 
> I wrote a JMH benchmark, but micro-benchmarking is not the right tool to show the impact of that bug, since the micro-benchmarks will capture ?microscopic? things like thread creation overhead, etc.
> whereas the problem appears on the larger scale.
> 

From http://openjdk.java.net/projects/code-tools/jmh/ :

  JMH is a Java harness for building, running, and analysing nano/micro/milli/macro benchmarks written in Java and other languages targetting the JVM.

i.e. M can be "Micro" | "Macro" or generally "Measurement"

It takes away the hassle of writing code for measurement/reporting/execution and makes it much easier for others to review the code/results and also execute.

Paul.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140515/4398e059/attachment-0001.bin>

From email at christian-fries.de  Thu May 15 17:55:03 2014
From: email at christian-fries.de (Christian Fries)
Date: Thu, 15 May 2014 23:55:03 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <915F5FFE-D19C-4A97-9C82-D2FE141F5632@oracle.com>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
	<A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
	<D7A230D4-6B29-4CB6-87D1-810936327658@christian-fries.de>
	<915F5FFE-D19C-4A97-9C82-D2FE141F5632@oracle.com>
Message-ID: <84C2FB72-C3AA-43F7-B034-61627486363D@christian-fries.de>


Dear Paul.

Am 15.05.2014 um 09:50 schrieb Paul Sandoz <paul.sandoz at oracle.com>:
>  JMH is a Java harness for building, running, and analysing nano/micro/milli/macro benchmarks written in Java and other languages targetting the JVM.
> 
> i.e. M can be "Micro" | "Macro" or generally "Measurement"

Thank you very much. I have overlooked that. JMH is a great tool. I have created the JMH benchmark (just added the annotations), they look the same.

Benchmark                                                                           Mode   Samples         Mean   Mean error    Units
o.s.NestedParallelForEachTest.timeNestedLoopWithInnerSequential                   sample        20       26,901        0,690     s/op	(<--- outer loop parallel, inner loop sequencial)
o.s.NestedParallelForEachTest.timeNestedLoopWithInnerParallel                     sample        20       35,120        2,852     s/op	(<--- outer loop parallel, inner loop parallel)
o.s.NestedParallelForEachTest.timeNestedLoopWithInnerParallelButWrappedInThread   sample        20       26,817        0,380     s/op	(<--- outer loop parallel, inner loop parallel + workaround fix)

Note: Something which I could not capture with JMH, but which my handmade test prints out is the number of threads involved and the CPU time used. The ?unfixed? version is 30% slower AND burnes 60%-100% more process cpu time. (Can JMH monitore cpu time? (I did not find that) - If not, this is maybe a nice feature request).

Best
Christian

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140515/5738c3e1/attachment.html>

From paul.sandoz at oracle.com  Fri May 16 04:43:47 2014
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Fri, 16 May 2014 10:43:47 +0200
Subject: [concurrency-interest] ForkJoinPool not designed for nested
	Java 8 streams.parallel().forEach( ... )
In-Reply-To: <84C2FB72-C3AA-43F7-B034-61627486363D@christian-fries.de>
References: <1C31A797-7E36-4C3B-BDFD-7C8988A5F490@christian-fries.de>
	<53681A20.7070805@cs.oswego.edu>
	<BC0401B3-37CD-46E2-B568-9669D7D5A4AC@christian-fries.de>
	<8E906D6F-D532-4AA4-AAEF-4A18F591C731@oracle.com>
	<12FFFBAA-B3D4-450C-A85A-9E2FF7F76D44@christian-fries.de>
	<A91459CA-01CE-4B17-9B8D-D9AF6F0640B2@oracle.com>
	<D7A230D4-6B29-4CB6-87D1-810936327658@christian-fries.de>
	<915F5FFE-D19C-4A97-9C82-D2FE141F5632@oracle.com>
	<84C2FB72-C3AA-43F7-B034-61627486363D@christian-fries.de>
Message-ID: <C716A462-C169-4373-941C-3B8D923F5FD8@oracle.com>

On May 15, 2014, at 11:55 PM, Christian Fries <email at christian-fries.de> wrote:
> (Can JMH monitore cpu time?

Not that i am aware off, but have not looked too closely. There are a bunch of profilers available, i dunno if it is possible to plug in additional ones separate from JMH itself:

$ java -jar target/microbenchmarks.jar -lprof
Supported profilers:
                  gc: GC profiling via standard MBeans
                comp: JIT compiler profiling via standard MBeans
                  cl: Classloader profiling via standard MBeans
               hs_rt: HotSpot (tm) runtime profiling via implementation-specific MBeans
               hs_cl: HotSpot (tm) classloader profiling via implementation-specific MBeans
             hs_comp: HotSpot (tm) JIT compiler profiling via implementation-specific MBeans
               hs_gc: HotSpot (tm) memory manager (GC) profiling via implementation-specific MBeans
              hs_thr: HotSpot (tm) threading subsystem via implementation-specific MBeans
               stack: Simple and naive Java stack profiler


> (I did not find that) - If not, this is maybe a nice feature request).
> 

Yes.

Paul.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140516/c76b3a9b/attachment.bin>

From aph at redhat.com  Thu May 22 09:55:09 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 22 May 2014 14:55:09 +0100
Subject: [concurrency-interest] Unsafe.getAndAddLong
Message-ID: <537E01BD.2090907@redhat.com>

More memory model mysteries:

Why do we use getLongVolatile here?  It'll be a load followed by an
acquire barrier.  But compareAndSwapLong is in effect a full barrier
anyway, so what's the point of using getLongVolatile?

    public final long getAndAddLong(Object o, long offset, long delta) {
        long v;
        do {
            v = getLongVolatile(o, offset);
        } while (!compareAndSwapLong(o, offset, v, v + delta));
        return v;
    }

Andrew.

From arcadiy at ivanov.biz  Thu May 22 10:12:06 2014
From: arcadiy at ivanov.biz (Arcadiy Ivanov)
Date: Thu, 22 May 2014 10:12:06 -0400
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E01BD.2090907@redhat.com>
References: <537E01BD.2090907@redhat.com>
Message-ID: <537E05B6.9070507@ivanov.biz>

(1)To increase the chance that the first CAS will succeed.
(2)To ensure that both 32bit words of a 64bit long are read atomically.

Otherwise there is a pretty good chance the first CAS will fail due to 
'v' either not being up-to-date or being nonsensical due to non-atomic 
read of non-volatile longs and doubles.

On 2014-05-22 09:55, Andrew Haley wrote:
> More memory model mysteries:
>
> Why do we use getLongVolatile here?  It'll be a load followed by an
> acquire barrier.  But compareAndSwapLong is in effect a full barrier
> anyway, so what's the point of using getLongVolatile?
>
>      public final long getAndAddLong(Object o, long offset, long delta) {
>          long v;
>          do {
>              v = getLongVolatile(o, offset);
>          } while (!compareAndSwapLong(o, offset, v, v + delta));
>          return v;
>      }
>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From aph at redhat.com  Thu May 22 10:14:03 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 22 May 2014 15:14:03 +0100
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E0552.30000@ivanov.biz>
References: <537E01BD.2090907@redhat.com> <537E0552.30000@ivanov.biz>
Message-ID: <537E062B.4080405@redhat.com>

How does getLongVolatile help the first read of 'v' to be up to date?
HotSpot emits an acquire barrier after reading 'v'.

On 05/22/2014 03:10 PM, Arcadiy Ivanov wrote:
> (1)To increase the chance that the first CAS will succeed.
> (2)To ensure that both 32bit words of a 64bit long are read atomically.
> 
> Otherwise there is a pretty good chance the first CAS will fail due to 
> 'v' either not being up-to-date or being nonsensical due to non-atomic 
> read of non-volatile longs and doubles.
> 
> On 2014-05-22 09:55, Andrew Haley wrote:
>> More memory model mysteries:
>>
>> Why do we use getLongVolatile here?  It'll be a load followed by an
>> acquire barrier.  But compareAndSwapLong is in effect a full barrier
>> anyway, so what's the point of using getLongVolatile?
>>
>>      public final long getAndAddLong(Object o, long offset, long delta) {
>>          long v;
>>          do {
>>              v = getLongVolatile(o, offset);
>>          } while (!compareAndSwapLong(o, offset, v, v + delta));
>>          return v;
>>      }
>>
>> Andrew.



From dl at cs.oswego.edu  Thu May 22 10:22:59 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 22 May 2014 10:22:59 -0400
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E05B6.9070507@ivanov.biz>
References: <537E01BD.2090907@redhat.com> <537E05B6.9070507@ivanov.biz>
Message-ID: <537E0843.3010305@cs.oswego.edu>

On 05/22/2014 10:12 AM, Arcadiy Ivanov wrote:
> (1)To increase the chance that the first CAS will succeed.
> (2)To ensure that both 32bit words of a 64bit long are read atomically.

Right. The second is the main reason. It could be relaxed at the
expense of more CAS misses. This might even be a good idea,
and worth contemplating, but seems marginal.

-Doug

>
> Otherwise there is a pretty good chance the first CAS will fail due to 'v'
> either not being up-to-date or being nonsensical due to non-atomic read of
> non-volatile longs and doubles.
>
> On 2014-05-22 09:55, Andrew Haley wrote:
>> More memory model mysteries:
>>
>> Why do we use getLongVolatile here?  It'll be a load followed by an
>> acquire barrier.  But compareAndSwapLong is in effect a full barrier
>> anyway, so what's the point of using getLongVolatile?
>>
>>      public final long getAndAddLong(Object o, long offset, long delta) {
>>          long v;
>>          do {
>>              v = getLongVolatile(o, offset);
>>          } while (!compareAndSwapLong(o, offset, v, v + delta));
>>          return v;
>>      }
>>
>> Andrew.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From arcadiy at ivanov.biz  Thu May 22 10:29:48 2014
From: arcadiy at ivanov.biz (Arcadiy Ivanov)
Date: Thu, 22 May 2014 10:29:48 -0400
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E062B.4080405@redhat.com>
References: <537E01BD.2090907@redhat.com> <537E0552.30000@ivanov.biz>
	<537E062B.4080405@redhat.com>
Message-ID: <537E09DC.8030404@ivanov.biz>

JLS 17.4.7

 1.

    Each read sees a write to the same variable in the execution.

    All reads and writes of volatile variables are volatile actions. For
    all reads/r/in/A/, we have/W(r)/in/A/and/W(r).v/=/r.v/. The
    variable/r.v/is volatile if and only if/r/is a volatile read, and
    the variable/w.v/is volatile if and only if/w/is a volatile write.

If you have a volatile write but no volatile read, there is no guarantee 
that there won't be a reordering occurring. As such, the compiler may 
optimize the first read away and you'll be left with a value that has 
been read quite a long time ago earlier potentially all the way to the 
previous CAS.

I would add another clause:
(3) Ensuring that the first read of v is not reordered.

But the worst case scenario you would have is that you'll have a failing 
CAS the first time so both reordering and stale cache are technically 
covered under my (1).

On 2014-05-22 10:14, Andrew Haley wrote:
> How does getLongVolatile help the first read of 'v' to be up to date?
> HotSpot emits an acquire barrier after reading 'v'.
>
> On 05/22/2014 03:10 PM, Arcadiy Ivanov wrote:
>> (1)To increase the chance that the first CAS will succeed.
>> (2)To ensure that both 32bit words of a 64bit long are read atomically.
>>
>> Otherwise there is a pretty good chance the first CAS will fail due to
>> 'v' either not being up-to-date or being nonsensical due to non-atomic
>> read of non-volatile longs and doubles.
>>
>> On 2014-05-22 09:55, Andrew Haley wrote:
>>> More memory model mysteries:
>>>
>>> Why do we use getLongVolatile here?  It'll be a load followed by an
>>> acquire barrier.  But compareAndSwapLong is in effect a full barrier
>>> anyway, so what's the point of using getLongVolatile?
>>>
>>>       public final long getAndAddLong(Object o, long offset, long delta) {
>>>           long v;
>>>           do {
>>>               v = getLongVolatile(o, offset);
>>>           } while (!compareAndSwapLong(o, offset, v, v + delta));
>>>           return v;
>>>       }
>>>
>>> Andrew.
>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140522/52994f5c/attachment.html>

From aph at redhat.com  Thu May 22 12:20:35 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 22 May 2014 17:20:35 +0100
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E09DC.8030404@ivanov.biz>
References: <537E01BD.2090907@redhat.com> <537E0552.30000@ivanov.biz>
	<537E062B.4080405@redhat.com> <537E09DC.8030404@ivanov.biz>
Message-ID: <537E23D3.8080202@redhat.com>

I don't think that word-tearing matters here: all it will do in the
odd chance that it occurs is cause a retry.

On 05/22/2014 03:29 PM, Arcadiy Ivanov wrote:
> JLS 17.4.7
> 
>  1.
> 
>     Each read sees a write to the same variable in the execution.
> 
>     All reads and writes of volatile variables are volatile actions. For
>     all reads/r/in/A/, we have/W(r)/in/A/and/W(r).v/=/r.v/. The
>     variable/r.v/is volatile if and only if/r/is a volatile read, and
>     the variable/w.v/is volatile if and only if/w/is a volatile write.
> 
> If you have a volatile write but no volatile read, there is no guarantee 
> that there won't be a reordering occurring. As such, the compiler may 
> optimize the first read away and you'll be left with a value that has 
> been read quite a long time ago earlier potentially all the way to the 
> previous CAS.
> 
> I would add another clause:
> (3) Ensuring that the first read of v is not reordered.
> 
> But the worst case scenario you would have is that you'll have a failing 
> CAS the first time so both reordering and stale cache are technically 
> covered under my (1).

This argument is a strange combination of pragmatic efficiency
concerns and abstract language semantics.  No-one is disputing
correctness, only efficiency.  And we can't talk about efficiency in
terms of abstract languages.  So, let's get real.

Here is a real machine with weakly-ordered memory, in this case ARMv8:

  0x0000007fa11df1b4: ldr	x12, [x10,#16]
  0x0000007fa11df1b8: dmb	ishld           ;*invokevirtual getLongVolatile
                                                ; - sun.misc.Unsafe::getAndAddLong at 3 (line 1050)
                                                ; - java.util.concurrent.atomic.AtomicLong::incrementAndGet at 8 (line 200)
                                                ; - Test1::run at 4 (line 6)

  0x0000007fa11df1bc: dmb	ish             ;*invokevirtual compareAndSwapLong
                                                ; - sun.misc.Unsafe::getAndAddLong at 18 (line 1051)
                                                ; - java.util.concurrent.atomic.AtomicLong::incrementAndGet at 8 (line 200)
                                                ; - Test1::run at 4 (line 6)

  0x0000007fa11df1c0: add	x13, x12, #0x1  ;*ladd
                                                ; - sun.misc.Unsafe::getAndAddLong at 17 (line 1051)
                                                ; - java.util.concurrent.atomic.AtomicLong::incrementAndGet at 8 (line 200)
                                                ; - Test1::run at 4 (line 6)

  0x0000007fa11df1c4: ldar	xscratch1, [x11]
  0x0000007fa11df1c8: cmp	xscratch1, x12
  0x0000007fa11df1cc: b.ne	0x0000007fa11df1d8
  0x0000007fa11df1d0: stlxr	wscratch1, x13, [x11]
  ...

Note that the LoadLoad|LoadStore barrier comes *after* the read of
getLongVolatile, so it can have no effect whatsoever on when v is
read.  It does not force a fresh copy of v to be read.  That value of
v could be very stale indeed.  I will grant you that it will, at
least, be read from memory, but that is no guarantee of any
timeliness.

I suppose that in theory you could be running on a machine where
volatile loads are implemented by issuing a StoreLoad barrier before
each volatile load instead of after each volatile store, but AFAIK
no-one does this.

I suggest that using getLongVolatile() does not help on any machine and
makes things worse on weakly-ordered machines.

Andrew.

From nathan.reynolds at oracle.com  Thu May 22 12:44:07 2014
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 22 May 2014 09:44:07 -0700
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E23D3.8080202@redhat.com>
References: <537E01BD.2090907@redhat.com>
	<537E0552.30000@ivanov.biz>	<537E062B.4080405@redhat.com>
	<537E09DC.8030404@ivanov.biz> <537E23D3.8080202@redhat.com>
Message-ID: <537E2957.6040601@oracle.com>

A CAS retry can be very costly.  See this blog entry for the details. 
https://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs

Basically, as CAS retries happen more often the branch predictor will 
predict that the loop will be re-executed.  So, when the CAS finally 
succeeds, the processor will stall because the branch was predicted the 
wrong way.  If CAS was used to acquire a lock, then the critical region 
of the lock is now longer and throughput will suffer.

Word tearing is probably not very common.  But, an already hot CAS loop 
wouldn't appreciate any more problems.

-Nathan

On 5/22/2014 9:20 AM, Andrew Haley wrote:
> I don't think that word-tearing matters here: all it will do in the
> odd chance that it occurs is cause a retry.
>
> On 05/22/2014 03:29 PM, Arcadiy Ivanov wrote:
>> JLS 17.4.7
>>
>>   1.
>>
>>      Each read sees a write to the same variable in the execution.
>>
>>      All reads and writes of volatile variables are volatile actions. For
>>      all reads/r/in/A/, we have/W(r)/in/A/and/W(r).v/=/r.v/. The
>>      variable/r.v/is volatile if and only if/r/is a volatile read, and
>>      the variable/w.v/is volatile if and only if/w/is a volatile write.
>>
>> If you have a volatile write but no volatile read, there is no guarantee
>> that there won't be a reordering occurring. As such, the compiler may
>> optimize the first read away and you'll be left with a value that has
>> been read quite a long time ago earlier potentially all the way to the
>> previous CAS.
>>
>> I would add another clause:
>> (3) Ensuring that the first read of v is not reordered.
>>
>> But the worst case scenario you would have is that you'll have a failing
>> CAS the first time so both reordering and stale cache are technically
>> covered under my (1).
> This argument is a strange combination of pragmatic efficiency
> concerns and abstract language semantics.  No-one is disputing
> correctness, only efficiency.  And we can't talk about efficiency in
> terms of abstract languages.  So, let's get real.
>
> Here is a real machine with weakly-ordered memory, in this case ARMv8:
>
>    0x0000007fa11df1b4: ldr	x12, [x10,#16]
>    0x0000007fa11df1b8: dmb	ishld           ;*invokevirtual getLongVolatile
>                                                  ; - sun.misc.Unsafe::getAndAddLong at 3 (line 1050)
>                                                  ; - java.util.concurrent.atomic.AtomicLong::incrementAndGet at 8 (line 200)
>                                                  ; - Test1::run at 4 (line 6)
>
>    0x0000007fa11df1bc: dmb	ish             ;*invokevirtual compareAndSwapLong
>                                                  ; - sun.misc.Unsafe::getAndAddLong at 18 (line 1051)
>                                                  ; - java.util.concurrent.atomic.AtomicLong::incrementAndGet at 8 (line 200)
>                                                  ; - Test1::run at 4 (line 6)
>
>    0x0000007fa11df1c0: add	x13, x12, #0x1  ;*ladd
>                                                  ; - sun.misc.Unsafe::getAndAddLong at 17 (line 1051)
>                                                  ; - java.util.concurrent.atomic.AtomicLong::incrementAndGet at 8 (line 200)
>                                                  ; - Test1::run at 4 (line 6)
>
>    0x0000007fa11df1c4: ldar	xscratch1, [x11]
>    0x0000007fa11df1c8: cmp	xscratch1, x12
>    0x0000007fa11df1cc: b.ne	0x0000007fa11df1d8
>    0x0000007fa11df1d0: stlxr	wscratch1, x13, [x11]
>    ...
>
> Note that the LoadLoad|LoadStore barrier comes *after* the read of
> getLongVolatile, so it can have no effect whatsoever on when v is
> read.  It does not force a fresh copy of v to be read.  That value of
> v could be very stale indeed.  I will grant you that it will, at
> least, be read from memory, but that is no guarantee of any
> timeliness.
>
> I suppose that in theory you could be running on a machine where
> volatile loads are implemented by issuing a StoreLoad barrier before
> each volatile load instead of after each volatile store, but AFAIK
> no-one does this.
>
> I suggest that using getLongVolatile() does not help on any machine and
> makes things worse on weakly-ordered machines.
>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140522/ba1eece5/attachment.html>

From arcadiy at ivanov.biz  Thu May 22 13:14:19 2014
From: arcadiy at ivanov.biz (Arcadiy Ivanov)
Date: Thu, 22 May 2014 13:14:19 -0400
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E23D3.8080202@redhat.com>
References: <537E01BD.2090907@redhat.com> <537E0552.30000@ivanov.biz>
	<537E062B.4080405@redhat.com> <537E09DC.8030404@ivanov.biz>
	<537E23D3.8080202@redhat.com>
Message-ID: <537E306B.1080303@ivanov.biz>


> This argument is a strange combination of pragmatic efficiency
> concerns and abstract language semantics.
It was not an argument, it was an answer. Your original question was 
"why is it there", not whether it's THE optimal implementation on all 
architectures. :)
Both Doug and myself provided you with several rationale in terms of JMM.
> I don't think that word-tearing matters here: all it will do in the
> odd chance that it occurs is cause a retry.
Hence my answer: no matter the three reasons, the worst case is your CAS 
will fail the first time (unless another CAS intervenes again, ad 
infinitum). How bad the CAS failure is depends on the architecture.

That said, I believe Unsafe.getAndAddLong is an intrinsic in Java 8 and 
9 and will, barring any -XX options to the contrary, result in an 
'inline_unsafe_load_store(T_LONG,   LS_xadd);' further resulting in LOCK 
XADDL on X86, so it's a non-issue to begin with for X86 architectures. 
You might be interested in a similar intrinsic for ARMv8 if one isn't 
already available and could be implemented.

- Arcadiy

On 2014-05-22 12:20, Andrew Haley wrote:
> I don't think that word-tearing matters here: all it will do in the
> odd chance that it occurs is cause a retry.


From aph at redhat.com  Thu May 22 13:40:06 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 22 May 2014 18:40:06 +0100
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E2957.6040601@oracle.com>
References: <537E01BD.2090907@redhat.com>	<537E0552.30000@ivanov.biz>	<537E062B.4080405@redhat.com>	<537E09DC.8030404@ivanov.biz>
	<537E23D3.8080202@redhat.com> <537E2957.6040601@oracle.com>
Message-ID: <537E3676.3060001@redhat.com>

On 05/22/2014 05:44 PM, Nathan Reynolds wrote:
> A CAS retry can be very costly.  See this blog entry for the details. 
> https://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs
> 
> Basically, as CAS retries happen more often the branch predictor will 
> predict that the loop will be re-executed.  So, when the CAS finally 
> succeeds, the processor will stall because the branch was predicted the 
> wrong way.  If CAS was used to acquire a lock, then the critical region 
> of the lock is now longer and throughput will suffer.
> 
> Word tearing is probably not very common.  But, an already hot CAS loop 
> wouldn't appreciate any more problems.

Sure, but the problem is that already hot CAS loop has an unnecessary
barrier, and that's not a rare case, that's every time.

Andrew.



From nathan.reynolds at oracle.com  Thu May 22 13:51:23 2014
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 22 May 2014 10:51:23 -0700
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E3676.3060001@redhat.com>
References: <537E01BD.2090907@redhat.com>	<537E0552.30000@ivanov.biz>	<537E062B.4080405@redhat.com>	<537E09DC.8030404@ivanov.biz>
	<537E23D3.8080202@redhat.com> <537E2957.6040601@oracle.com>
	<537E3676.3060001@redhat.com>
Message-ID: <537E391B.8080805@oracle.com>

I am curious.  How much does the load barrier cost?  The CAS can't 
execute until the load completes anyways.  In other words, there is a 
data dependency there.

-Nathan

On 5/22/2014 10:40 AM, Andrew Haley wrote:
> On 05/22/2014 05:44 PM, Nathan Reynolds wrote:
>> A CAS retry can be very costly.  See this blog entry for the details.
>> https://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs
>>
>> Basically, as CAS retries happen more often the branch predictor will
>> predict that the loop will be re-executed.  So, when the CAS finally
>> succeeds, the processor will stall because the branch was predicted the
>> wrong way.  If CAS was used to acquire a lock, then the critical region
>> of the lock is now longer and throughput will suffer.
>>
>> Word tearing is probably not very common.  But, an already hot CAS loop
>> wouldn't appreciate any more problems.
> Sure, but the problem is that already hot CAS loop has an unnecessary
> barrier, and that's not a rare case, that's every time.
>
> Andrew.
>
>
>
>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140522/646d3ca6/attachment.html>

From aph at redhat.com  Thu May 22 14:01:30 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 22 May 2014 19:01:30 +0100
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E306B.1080303@ivanov.biz>
References: <537E01BD.2090907@redhat.com> <537E0552.30000@ivanov.biz>
	<537E062B.4080405@redhat.com> <537E09DC.8030404@ivanov.biz>
	<537E23D3.8080202@redhat.com> <537E306B.1080303@ivanov.biz>
Message-ID: <537E3B7A.8030303@redhat.com>

On 05/22/2014 06:14 PM, Arcadiy Ivanov wrote:
> 
>> This argument is a strange combination of pragmatic efficiency
>> concerns and abstract language semantics.
> It was not an argument, it was an answer. 

OK, it's a strange answer, then.  :-)

> Your original question was "why is it there", not whether it's THE
> optimal implementation on all architectures. :)

I confess that the word-tearing argument never occurred to me.  I've
been working in the 64-bit world for so long that I'd forgotten about
that.  So, I can see why a volatile read may be a good idea on 32-bit
x86.

However, I had to question the explanation that volatile prevents a
stale value from being read because it doesn't do that on any
architecture of which I'm aware.

And it's worth pointing that out here, surely.

> Both Doug and myself provided you with several rationale in terms of JMM.
>> I don't think that word-tearing matters here: all it will do in the
>> odd chance that it occurs is cause a retry.
> Hence my answer: no matter the three reasons, the worst case is your CAS 
> will fail the first time (unless another CAS intervenes again, ad 
> infinitum). How bad the CAS failure is depends on the architecture.
> 
> That said, I believe Unsafe.getAndAddLong is an intrinsic in Java 8 and 
> 9 and will, barring any -XX options to the contrary, result in an 
> 'inline_unsafe_load_store(T_LONG,   LS_xadd);' further resulting in LOCK 
> XADDL on X86, so it's a non-issue to begin with for X86 architectures. 
> You might be interested in a similar intrinsic for ARMv8 if one isn't 
> already available and could be implemented.

Sure, I will do that.  I was just trying to understand why the Java
version is the way it is.

Andrew.

From aph at redhat.com  Thu May 22 14:15:48 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 22 May 2014 19:15:48 +0100
Subject: [concurrency-interest] Unsafe.getAndAddLong
In-Reply-To: <537E391B.8080805@oracle.com>
References: <537E01BD.2090907@redhat.com>	<537E0552.30000@ivanov.biz>	<537E062B.4080405@redhat.com>	<537E09DC.8030404@ivanov.biz>
	<537E23D3.8080202@redhat.com> <537E2957.6040601@oracle.com>
	<537E3676.3060001@redhat.com> <537E391B.8080805@oracle.com>
Message-ID: <537E3ED4.9010502@redhat.com>

On 05/22/2014 06:51 PM, Nathan Reynolds wrote:
> I am curious.  How much does the load barrier cost?  The CAS can't 
> execute until the load completes anyways.  In other words, there is a 
> data dependency there.

That's an excellent point.  I confess that I don't really know: I
assume that it might generate an extra bus transaction, but at best
it'll be completely free because it's in the shadow of the load.  It's
very hard for me to say for certain because there are (or will be)
several versions of this architecture, with different design goals.

Andrew.


From lukeisandberg at gmail.com  Fri May 23 20:05:40 2014
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 May 2014 17:05:40 -0700
Subject: [concurrency-interest] Proposal for a CallerRunsExecutor in
	j.u.c.Executors
Message-ID: <CAO9V1MKUEBD6Ea7yqHeqYW98JZX7cLv6zoyDgp7xu7051EHykg@mail.gmail.com>

Guava has an Executor instance called
MoreExecutors.sameThreadExecutor<http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/util/concurrent/MoreExecutors.html#sameThreadExecutor()>

For Guava users it is very popular for passing to APIs like
ListenableFuture.addListener and other Executor accepting utilities for
when the task will be quick enough that it isn't worth submitting to a
thread pool.

There was recently a performance bug reported against it:
https://code.google.com/p/guava-libraries/issues/detail?id=1734

This led to us reconsider the implementation in Guava and then thinking
that maybe this should even be a j.u.c.Executors feature.

We haven't decided exactly what we want to do (and we are somewhat limited
by needing to consider current MoreExecutors.sameThreadExecutor() users)
but the current proposals are:

* deprecate/remove sameThreadExecutor from guava and replace it with two
methods:

MoreExecutors.callerRunsExecutor():
  Returns a static final instance of
    new Executor() {
      public void execute(Runnable r) {
        r.run();
      }
    };
MoreExecutors.newCallerRunsExecutorService():
  which would be identical to the current sameThreadExecutor() (which
includes shutdown semantics)


* deprecate/remove sameThreadExecutor from guava and replace it with a
single method

MoreExecutors.callerRunsExecutorService()

Returns a shared ExecutorService (or really a ListeningExecutorService
since this is guava) instance, that cannot be shutdown and all the methods
return completed futures.


So there are still a lot of things up in the air, including the name!  But
we figured that the best place for a simple utility like this would
ultimately be in the JDK itself and we were interested in gathering the
opinions of the people on this list.

Our plan is still to make a change in Guava, but hopefully (with the input
from this list) it will be compatible with a later change to
j.u.c.Executors.

I think the main questions to answer would be
* Is it worth having an ExecutorService instance of this or just an
Executor (or both)?
     In the Google codebase, the vast majority of the time it only needs to
be an Executor.  The cases where this is useful as an ExecutorService are
more common in testing situations, though occasionally you see it used to
essentially disable concurrency.
* If we do want to have an ExecutorService should it be shutdown-able, how
useful would that be?  would it be useful at all beyond testing scenarios?
* What should we call it?

Thanks,
-Luke
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140523/e79f8ea9/attachment.html>

From dl at cs.oswego.edu  Sat May 24 13:09:21 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 24 May 2014 13:09:21 -0400
Subject: [concurrency-interest] More CompletableFuture internal improvements
Message-ID: <5380D241.6020409@cs.oswego.edu>


The first of various improvements catching up after a very busy semester ...

While addressing the recursion issues in CompletableFuture, I
had a deja vu prediction that someday, someone would create
zillions of useless completion stages for CompletableFuture that
never fired, and then complain about space usage. This can happen
for example with unused sources of anyOf and "either" constructions.
With a fair amount of restructuring, it's possible to clean these out.
Which the updated version does. Comments and experience reports welcome.
(For those familiar with internals: this required merging
"Async" with Completion classes, which turns out to be a good
idea anyway because it further reduces footprint.)

The deja vu comes from similar experiences dealing with
cleanup in cases of time-outs on queues etc every millecond
for days.

Updated sources at

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log

Also compiled into the jar at
    http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
that you can run with -Xbootclasspath

-Doug

From dl at cs.oswego.edu  Sat May 24 14:03:04 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 24 May 2014 14:03:04 -0400
Subject: [concurrency-interest] Proposal for a CallerRunsExecutor in
	j.u.c.Executors
In-Reply-To: <CAO9V1MKUEBD6Ea7yqHeqYW98JZX7cLv6zoyDgp7xu7051EHykg@mail.gmail.com>
References: <CAO9V1MKUEBD6Ea7yqHeqYW98JZX7cLv6zoyDgp7xu7051EHykg@mail.gmail.com>
Message-ID: <5380DED8.9060103@cs.oswego.edu>

On 05/23/2014 08:05 PM, Luke Sandberg wrote:
> Guava has an Executor instance called MoreExecutors.sameThreadExecutor
> <http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/util/concurrent/MoreExecutors.html#sameThreadExecutor()>
>
> For Guava users it is very popular for passing to APIs like
> ListenableFuture.addListener and other Executor accepting utilities for when the
> task will be quick enough that it isn't worth submitting to a thread pool.
>
> There was recently a performance bug reported against it:
> https://code.google.com/p/guava-libraries/issues/detail?id=1734
>
> This led to us reconsider the implementation in Guava and then thinking that
> maybe this should even be a j.u.c.Executors feature.

The class has been sitting there under the name DirectExecutor
since JDK5 as a code example in the Executors javadoc.
(See 
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Executor.html)
We even discussed putting it (as well as the other code example,
ThreadPerTaskExecutor) in Executors but for some reason decided
they weren't important enough. But they are important in that
standardizing their names and usage will save people work.
So I can't think of a reason not to do this, even if a decade
too late for most purposes. Any objections to using these names?

>     MoreExecutors.newCallerRunsExecutorService():
>        which would be identical to the current sameThreadExecutor() (which
>     includes shutdown semantics)


I gather that this one uses a Lock just to support the
awaitTermination method? (Otherwise it would need only
a volatile boolean shutdown bit.) It does seem like a niche usage,
but maybe someone could make a case for including it.

-Doug


From lukeisandberg at gmail.com  Sat May 24 18:15:43 2014
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Sat, 24 May 2014 15:15:43 -0700
Subject: [concurrency-interest] Proposal for a CallerRunsExecutor in
	j.u.c.Executors
In-Reply-To: <5380DED8.9060103@cs.oswego.edu>
References: <CAO9V1MKUEBD6Ea7yqHeqYW98JZX7cLv6zoyDgp7xu7051EHykg@mail.gmail.com>
	<5380DED8.9060103@cs.oswego.edu>
Message-ID: <CAO9V1MKzSg7W978rqYtP+v-fR0M3+MpjgjUtOpoNgvUeMhXGoA@mail.gmail.com>

On Sat, May 24, 2014 at 11:03 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 05/23/2014 08:05 PM, Luke Sandberg wrote:
>
>> Guava has an Executor instance called MoreExecutors.sameThreadExecutor
>> <http://docs.guava-libraries.googlecode.com/git/javadoc/
>> com/google/common/util/concurrent/MoreExecutors.html#sameThreadExecutor()
>> >
>>
>>
>> For Guava users it is very popular for passing to APIs like
>> ListenableFuture.addListener and other Executor accepting utilities for
>> when the
>> task will be quick enough that it isn't worth submitting to a thread pool.
>>
>> There was recently a performance bug reported against it:
>> https://code.google.com/p/guava-libraries/issues/detail?id=1734
>>
>> This led to us reconsider the implementation in Guava and then thinking
>> that
>> maybe this should even be a j.u.c.Executors feature.
>>
>
> The class has been sitting there under the name DirectExecutor
> since JDK5 as a code example in the Executors javadoc.
> (See http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/
> concurrent/Executor.html)
> We even discussed putting it (as well as the other code example,
> ThreadPerTaskExecutor) in Executors but for some reason decided
> they weren't important enough. But they are important in that
> standardizing their names and usage will save people work.
> So I can't think of a reason not to do this, even if a decade
> too late for most purposes. Any objections to using these names?


Yeah we noticed the DirectExecutor example too, but consensus appeared to
be against using that name.  Also it was noted that in JCIP Brian Goetz
calls the same thing 'WithinThreadExecutor'.  callerRunsExecutor was
suggested due to the existence of ThreadPoolExecutor.CallerRunsPolicy. (And
there were about 10 thousand other suggestions: immediateExecutor,
currentThreadExecutor, synchronousExecutor,...).

I have no objection to DirectExecutor though I do have a preference for
CallerRunsExecutor.

As for ThreadPerTaskExecutor, that name seems reasonable to me.


>
>
>      MoreExecutors.newCallerRunsExecutorService():
>>        which would be identical to the current sameThreadExecutor() (which
>>     includes shutdown semantics)
>>
>
>
> I gather that this one uses a Lock just to support the
> awaitTermination method? (Otherwise it would need only
> a volatile boolean shutdown bit.) It does seem like a niche usage,
> but maybe someone could make a case for including it.
>

That is correct.  I agree that this is less commonly useful (and it is
often most useful in testing situations).  So just leaving this in Guava
could make sense.  Martin Buchholz suggested just making the
CallerRunsExecutor be a unshutdownable shared ExecutorService instance as a
kind of compromise.


>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140524/c27f12bd/attachment.html>

From rblcin at gmail.com  Sat May 31 18:50:09 2014
From: rblcin at gmail.com (=?UTF-8?Q?Rafael_Brand=C3=A3o?=)
Date: Sat, 31 May 2014 19:50:09 -0300
Subject: [concurrency-interest] Potential threads getting stuck on WAITING
	(parking) in ReentrantLock
Message-ID: <CALHzM99GODVdy=3dqOXwhEE1nKRyW6xvjTxrrs7YmhYcf+Y2Xw@mail.gmail.com>

Hello,

I'm working on a few changes for ReentrantLock to make A/B tests between
the original implementation and my modifications, so my first step was to
copy the sources of ReentrantLock, AbstractQueuedSynchronizer (shortly AQS)
and AbstractOwnableSynchronizer and put them in a different package. After
resolving basic compilation issues, I've found the issue with Unsafe usage
(insecurity exception) and I've solved it based on the article found in
[1]. So basically my ReentrantLock has nothing different from the original
except for how I get the Unsafe in AQS.

After that, I've exhaustively conducted the following experiment: 200
threads try to increment 1000 times each some integer counter protected by
explicit locks. There are 10 counters, so each counter has 20 threads
concurrently trying to increment it. The main thread spawns all those
threads and then wait for them with join method. Once I've ran this
experiment many times (about 30000), it consistently get stuck in a join
for some thread when I use my almost unmodified ReentrantLock before 2000th
attempt.

I've got the thread stacks in [2] and what I understood from it is that the
remaining threads are all stuck waiting for the lock to be released.
However I've managed to print the owner of that lock and I see it's
unlocked and has no owner:

stuck thread state: WAITING
thread waiting lock: safe.ReentrantLock at 68a6a21a[Unlocked]
lock owner: null


I've been trying to reproduce it with the original ReentrantLock but I
didn't get stuck so far. So I'm here to ask you if there's a known issue on
ReentrantLock that could cause this situation (and also be extremely
unlikely to happen) or if there's something obvious that I should know
about the Unsafe usage given it's the only thing changed so far.

I suspect I have some disadvantage given that my nearly unmodified class is
not already compiled in the JVM (I could try to build the JDK later), but
this could only be a sign that the issue already exists in the code but can
only become more likely to happen in a slower implementation. I've also
searched for related bugs and the closest thing I've found was [3].

I'm using a Intel? Core? i7-3632QM notebook running Ubuntu 12.04. Java
version is "1.7.0_55" and I'm using OpenJDK Runtime Environment (IcedTea
2.4.7) (7u55-2.4.7-1ubuntu1~0.12.04.2) and OpenJDK 64-Bit Server VM (build
24.51-b03, mixed mode). I've experienced this issue after copying sources
of jdk7u, jdk8u and jdk9.


Best regards,
Rafael


[1] http://howtodoinjava.com/2013/10/19/usage-of-class-sun-misc-unsafe/
[2] https://gist.github.com/rafaelbrandao/4ec5f2cd272c4b8b183a
[3] https://bugs.openjdk.java.net/browse/JDK-8028686

-- 
Rafael Brand?o @ CIn - Center of Informatics
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140531/28816eac/attachment.html>

From davidcholmes at aapt.net.au  Sat May 31 19:55:18 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 1 Jun 2014 09:55:18 +1000
Subject: [concurrency-interest] Potential threads getting stuck on
	WAITING(parking) in ReentrantLock
In-Reply-To: <CALHzM99GODVdy=3dqOXwhEE1nKRyW6xvjTxrrs7YmhYcf+Y2Xw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEMDKGAA.davidcholmes@aapt.net.au>

Hi Rafael,

Does this reproduce with Oracle JDK or only the IcedTea distributions?

The most likely issue is the StackOverflowError. Have excluded that possibility?

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Rafael Brand?o
  Sent: Sunday, 1 June 2014 8:50 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Potential threads getting stuck on WAITING(parking) in ReentrantLock


  Hello,


  I'm working on a few changes for ReentrantLock to make A/B tests between the original implementation and my modifications, so my first step was to copy the sources of ReentrantLock, AbstractQueuedSynchronizer (shortly AQS) and AbstractOwnableSynchronizer and put them in a different package. After resolving basic compilation issues, I've found the issue with Unsafe usage (insecurity exception) and I've solved it based on the article found in [1]. So basically my ReentrantLock has nothing different from the original except for how I get the Unsafe in AQS.


  After that, I've exhaustively conducted the following experiment: 200 threads try to increment 1000 times each some integer counter protected by explicit locks. There are 10 counters, so each counter has 20 threads concurrently trying to increment it. The main thread spawns all those threads and then wait for them with join method. Once I've ran this experiment many times (about 30000), it consistently get stuck in a join for some thread when I use my almost unmodified ReentrantLock before 2000th attempt.


  I've got the thread stacks in [2] and what I understood from it is that the remaining threads are all stuck waiting for the lock to be released. However I've managed to print the owner of that lock and I see it's unlocked and has no owner:


    stuck thread state: WAITING
    thread waiting lock: safe.ReentrantLock at 68a6a21a[Unlocked]
    lock owner: null


  I've been trying to reproduce it with the original ReentrantLock but I didn't get stuck so far. So I'm here to ask you if there's a known issue on ReentrantLock that could cause this situation (and also be extremely unlikely to happen) or if there's something obvious that I should know about the Unsafe usage given it's the only thing changed so far.


  I suspect I have some disadvantage given that my nearly unmodified class is not already compiled in the JVM (I could try to build the JDK later), but this could only be a sign that the issue already exists in the code but can only become more likely to happen in a slower implementation. I've also searched for related bugs and the closest thing I've found was [3].


  I'm using a Intel? Core? i7-3632QM notebook running Ubuntu 12.04. Java version is "1.7.0_55" and I'm using OpenJDK Runtime Environment (IcedTea 2.4.7) (7u55-2.4.7-1ubuntu1~0.12.04.2) and OpenJDK 64-Bit Server VM (build 24.51-b03, mixed mode). I've experienced this issue after copying sources of jdk7u, jdk8u and jdk9.




  Best regards,
  Rafael




  [1] http://howtodoinjava.com/2013/10/19/usage-of-class-sun-misc-unsafe/
  [2] https://gist.github.com/rafaelbrandao/4ec5f2cd272c4b8b183a
  [3] https://bugs.openjdk.java.net/browse/JDK-8028686


  -- 
  Rafael Brand?o @ CIn - Center of Informatics 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140601/0f3fc3e3/attachment.html>

