From bryan at systap.com  Thu Oct  8 08:34:19 2009
From: bryan at systap.com (Bryan Thompson)
Date: Thu, 8 Oct 2009 07:34:19 -0500
Subject: [concurrency-interest] list of java concurrency and related package
	errors?
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED439A48A3E57@AUSP01VMBX06.collaborationhost.net>

Hello,

I found a list of issues and fixes by java release version a few months back but it seems to have retreated into the deep web.  Can anyone send me a list to this?  I am trying to track down a possible issue with 64-bit centos 5.

Thanks,

-bryan


From concurrency-interest at nitwit.de  Sat Oct 10 06:13:08 2009
From: concurrency-interest at nitwit.de (Timo Nentwig)
Date: Sat, 10 Oct 2009 12:13:08 +0200
Subject: [concurrency-interest] Resetable CountDownLatch
Message-ID: <200910101213.08814.concurrency-interest@nitwit.de>

Hi!

I'm looking for something like a resetable CountDownLatch. Here's what I want 
to do: n thread render an image in parallel (each calling countDown() when 
done) and when all threads are finished, some logic (i.e. drawing to screen) 
needs to be executed. And then start all over again.

I didn't find anything that fits in j.u.c. :-\

thx

From davidcholmes at aapt.net.au  Sat Oct 10 06:34:09 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 10 Oct 2009 20:34:09 +1000
Subject: [concurrency-interest] Resetable CountDownLatch
In-Reply-To: <200910101213.08814.concurrency-interest@nitwit.de>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEKGIDAA.davidcholmes@aapt.net.au>

Hi,

> I'm looking for something like a resetable CountDownLatch. Here's
> what I want to do: n thread render an image in parallel (each
> calling countDown() when done) and when all threads are finished,
> some logic (i.e. drawing  to screen) needs to be executed. And
> then start all over again.

You could just create a new CountDownLatch on each iteration.

Alternatively if you don't care about blocking the rendering threads across
iterations use a CyclicBarrier - the barrier action then makes it easy to do
the logic on completion.

David Holmes


From hoppithek at uni-muenster.de  Sat Oct 10 07:56:16 2009
From: hoppithek at uni-muenster.de (Armin Hopp)
Date: Sat, 10 Oct 2009 13:56:16 +0200
Subject: [concurrency-interest] Resetable CountDownLatch
In-Reply-To: <200910101213.08814.concurrency-interest@nitwit.de>
References: <200910101213.08814.concurrency-interest@nitwit.de>
Message-ID: <4AD07660.6000402@uni-muenster.de>

Hi Timo,

you should take a look at the Phaser class.

cheers
Armin

Timo Nentwig schrieb:
> Hi!
>
> I'm looking for something like a resetable CountDownLatch. Here's what I want 
> to do: n thread render an image in parallel (each calling countDown() when 
> done) and when all threads are finished, some logic (i.e. drawing to screen) 
> needs to be executed. And then start all over again.
>
> I didn't find anything that fits in j.u.c. :-\
>
> thx
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 312 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091010/7de5bd1f/attachment.bin>

From dvyukov at gmail.com  Sat Oct 10 10:17:00 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Sat, 10 Oct 2009 07:17:00 -0700
Subject: [concurrency-interest] Fences API and visibility/coherency
Message-ID: <9014da950910100717x4a33anb8068d73b1791db6@mail.gmail.com>

Hi,

I am new to this group and to the Java concurrency in general, so
probably I am just missing something obvious. Anyway I will appreciate
if someone will make following moment clear to me.

As far as I understand, Java does not provide any visibility/coherency
guarantees for accesses to plain (non-volatile) variables. I.e.
following code may run forever (right?):

public class StopThread {
  private static boolean stopRequested;
  public static void main(String[] args) throws InterruptedException {
    Thread backgroundThread = new Thread(new Runnable() {
      public void run() {
        int i = 0;
        while (!stopRequested)
          i++;
      }
    });
    backgroundThread.start();
    TimeUnit.SECONDS.sleep(1);
    stopRequested = true;
  }
}

I guess that for volatile variables there are visibility/coherency
guarantees, i.e. volatile stores performed in one thread must
eventually become visible to volatile loads performed in another
thread. (I do not see this in JLS and can not figure this from JVM
Spec Chapter 8, however I am sure that at least there is such
intention. Can somebody point to where such guarantees explicitly
stated for volatiles?)

So the question is: what about visibility/coherency guarantees
regarding Fences API? As far as I see Fences API deals solely with
ordering, but not with visibility. So if I use plain variables +
Fences API, then I get NO visibility/coherency. I.e. for the following
code:

class C {
    Object data;  // need volatile access but not volatile
    // ...
 }

 class App {
   Object getData(C c) {
      return Fences.orderReads(c).data;
   }

   void setData(C c) {
      Object newValue = ...;
      c.data = Fences.orderWrites(newValue);
      Fences.orderAccesses(c);
   }
   // ...
}

there is no guarantee that getData() will eventually see object stored
by setData(). And this renders the whole thing basically useless to
me.
What I am missing?
Thank you.

--
Dmitry Vyukov

From davidcholmes at aapt.net.au  Sat Oct 10 17:57:38 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 11 Oct 2009 07:57:38 +1000
Subject: [concurrency-interest] Resetable CountDownLatch
In-Reply-To: <4AD07660.6000402@uni-muenster.de>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEKGIDAA.davidcholmes@aapt.net.au>


Armin Hopp wrote
>
> you should take a look at the Phaser class.

Yes - good point! But as this is a new class for JDK7 it might not be
applicable yet.

David Holmes


> cheers
> Armin
>
> Timo Nentwig schrieb:
> > Hi!
> >
> > I'm looking for something like a resetable CountDownLatch.
> Here's what I want
> > to do: n thread render an image in parallel (each calling
> countDown() when
> > done) and when all threads are finished, some logic (i.e.
> drawing to screen)
> > needs to be executed. And then start all over again.
> >
> > I didn't find anything that fits in j.u.c. :-\
> >
> > thx
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>


From martinrb at google.com  Sat Oct 10 18:13:39 2009
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 10 Oct 2009 15:13:39 -0700
Subject: [concurrency-interest] Resetable CountDownLatch
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEKGIDAA.davidcholmes@aapt.net.au>
References: <4AD07660.6000402@uni-muenster.de>
	<NFBBKALFDCPFIDBNKAPCOEKGIDAA.davidcholmes@aapt.net.au>
Message-ID: <1ccfd1c10910101513i2e7b466o1adbfc856f0ce77f@mail.gmail.com>

On Sat, Oct 10, 2009 at 14:57, David Holmes <davidcholmes at aapt.net.au> wrote:
>
> Armin Hopp wrote
>>
>> you should take a look at the Phaser class.
>
> Yes - good point! But as this is a new class for JDK7 it might not be
> applicable yet.

Our intent is to support the version of Phaser in the jsr166y package
for the forseeable future, to be usable as a third-party add-on to jdk6.

Martin

From dl at cs.oswego.edu  Sun Oct 11 07:21:47 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 11 Oct 2009 07:21:47 -0400
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910100717x4a33anb8068d73b1791db6@mail.gmail.com>
References: <9014da950910100717x4a33anb8068d73b1791db6@mail.gmail.com>
Message-ID: <4AD1BFCB.8010304@cs.oswego.edu>

Dmitriy V'jukov wrote:

> As far as I understand, Java does not provide any visibility/coherency
> guarantees for accesses to plain (non-volatile) variables. I.e.
> following code may run forever (right?):

Yes. More generally, programs that include non-volatile,
non-final, non-fenced (see below) variables accessed
by multiple threads without locking have poorly defined
behavior. Modulo a few details surrounding thread
creation/termination, etc this is the definition of a
"race condition" for Java. Don't write programs with races.

> 
> public class StopThread {
>   private static boolean stopRequested;
>   public static void main(String[] args) throws InterruptedException {
>     Thread backgroundThread = new Thread(new Runnable() {
>       public void run() {
>         int i = 0;
>         while (!stopRequested)
>           i++;
>       }
>     });
>     backgroundThread.start();
>     TimeUnit.SECONDS.sleep(1);
>     stopRequested = true;
>   }
> }
> 
> I guess that for volatile variables there are visibility/coherency
> guarantees, i.e. volatile stores performed in one thread must
> eventually become visible to volatile loads performed in another
> thread. (I do not see this in JLS and can not figure this from JVM
> Spec Chapter 8, however I am sure that at least there is such
> intention. Can somebody point to where such guarantees explicitly
> stated for volatiles?)
> 

Hiding inside the ordering rules: If a given write must
happen-before a given read, then the results of that write must
be available to the read.

> So the question is: what about visibility/coherency guarantees
> regarding Fences API? As far as I see Fences API deals solely with
> ordering, but not with visibility. So if I use plain variables +
> Fences API, then I get NO visibility/coherency. I.e. for the following
> code:
> 

Fences operate at a lower level -- basically programming
directly to the happens-before rules.
A lot of people (including me) don't like this aspect of
Fences. Variables accessed using low-level fences
don't need to be specially marked, which makes correct
use even harder to verify than it might otherwise be.


> class C {
>     Object data;  // need volatile access but not volatile
>     // ...
>  }
> 
>  class App {
>    Object getData(C c) {
>       return Fences.orderReads(c).data;
>    }
> 
>    void setData(C c) {
>       Object newValue = ...;
>       c.data = Fences.orderWrites(newValue);
>       Fences.orderAccesses(c);
>    }
>    // ...
> }
> 
> there is no guarantee that getData() will eventually see object stored
> by setData(). And this renders the whole thing basically useless to
> me.
> What I am missing?

Any thread calling setData must eventually actually write "data",
thus making it  available to some subsequent call to getData
by another thread.

-Doug


From dvyukov at gmail.com  Sun Oct 11 09:20:06 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Sun, 11 Oct 2009 17:20:06 +0400
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <4AD1BFCB.8010304@cs.oswego.edu>
References: <9014da950910100717x4a33anb8068d73b1791db6@mail.gmail.com> 
	<4AD1BFCB.8010304@cs.oswego.edu>
Message-ID: <9014da950910110620j12759b3gc90e24b0d68ec33f@mail.gmail.com>

On Sun, Oct 11, 2009 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:

Hi Doug,

> Dmitriy V'jukov wrote:
>
>>
>> public class StopThread {
>> ?private static boolean stopRequested;
>> ?public static void main(String[] args) throws InterruptedException {
>> ? ?Thread backgroundThread = new Thread(new Runnable() {
>> ? ? ?public void run() {
>> ? ? ? ?int i = 0;
>> ? ? ? ?while (!stopRequested)
>> ? ? ? ? ?i++;
>> ? ? ?}
>> ? ?});
>> ? ?backgroundThread.start();
>> ? ?TimeUnit.SECONDS.sleep(1);
>> ? ?stopRequested = true;
>> ?}
>> }
>>
>> I guess that for volatile variables there are visibility/coherency
>> guarantees, i.e. volatile stores performed in one thread must
>> eventually become visible to volatile loads performed in another
>> thread. (I do not see this in JLS and can not figure this from JVM
>> Spec Chapter 8, however I am sure that at least there is such
>> intention. Can somebody point to where such guarantees explicitly
>> stated for volatiles?)
>>
> Hiding inside the ordering rules: If a given write must
> happen-before a given read, then the results of that write must
> be available to the read.

This is another thing, I mean visibility/coherency, not ordering. The
rule you described relates to both volatile and plain variables. BUT
it works IFF there is another volatile write that was already seen by
a thread and already established "happens-before edge" between
threads. My question is about that "top-level" volatile write, its
visibility is NOT guaranteed by any happens-before relations.

For example:

volatile int X = 0;

//thread 1:
X = 1;

// thread 2:
while (X == 0) {} // spin wait

Is there any guarantees regarding when thread 2 will see thread 1
write? You can't refer to ordering/happens-before here, because there
is NO happens-before edges between thread 1 and thread 2 yet.
Happens-before is what we are trying to establish with this write, but
do not have any yet.



>> So the question is: what about visibility/coherency guarantees
>> regarding Fences API? As far as I see Fences API deals solely with
>> ordering, but not with visibility. So if I use plain variables +
>> Fences API, then I get NO visibility/coherency. I.e. for the following
>> code:
>>
>
> Fences operate at a lower level -- basically programming
> directly to the happens-before rules.
> A lot of people (including me) don't like this aspect of
> Fences. Variables accessed using low-level fences
> don't need to be specially marked, which makes correct
> use even harder to verify than it might otherwise be.

Happens-before is about ordering, but what I am interested in is
visibility/coherence. What guarantees regarding visibility/coherence?


>> class C {
>> ? ?Object data; ?// need volatile access but not volatile
>> ? ?// ...
>> ?}
>>
>> ?class App {
>> ? Object getData(C c) {
>> ? ? ?return Fences.orderReads(c).data;
>> ? }
>>
>> ? void setData(C c) {
>> ? ? ?Object newValue = ...;
>> ? ? ?c.data = Fences.orderWrites(newValue);
>> ? ? ?Fences.orderAccesses(c);
>> ? }
>> ? // ...
>> }
>>
>> there is no guarantee that getData() will eventually see object stored
>> by setData(). And this renders the whole thing basically useless to
>> me.
>> What I am missing?
>
> Any thread calling setData must eventually actually write "data",
> thus making it ?available to some subsequent call to getData
> by another thread.

Ok, this is what I mean. You say that "thread must eventually actually
write", can you point to where it's described in the standard? For
example, for plain stores/loads there is no such guarantee, i.e.
thread perfectly allowed to not do actual writes of non-volatiles
variables (until it's required by ordering), or not do reads from main
memory of plain variables (until it's required by ordering). So it's
unclear to me what guarantees for volatiles and fences and
AtomicXXX.lazySet().


-- 
Dmitry Vyukov


From dl at cs.oswego.edu  Sun Oct 11 09:42:38 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 11 Oct 2009 09:42:38 -0400
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910110620j12759b3gc90e24b0d68ec33f@mail.gmail.com>
References: <9014da950910100717x4a33anb8068d73b1791db6@mail.gmail.com>
	<4AD1BFCB.8010304@cs.oswego.edu>
	<9014da950910110620j12759b3gc90e24b0d68ec33f@mail.gmail.com>
Message-ID: <4AD1E0CE.5020700@cs.oswego.edu>

Dmitriy V'jukov wrote:
>>> class C {
>>>    Object data;  // need volatile access but not volatile
>>>    // ...
>>>  }
>>>
>>>  class App {
>>>   Object getData(C c) {
>>>      return Fences.orderReads(c).data;
>>>   }
>>>
>>>   void setData(C c) {
>>>      Object newValue = ...;
>>>      c.data = Fences.orderWrites(newValue);
>>>      Fences.orderAccesses(c);
>>>   }
>>>   // ...
>>> }
>>>
> Ok, this is what I mean. You say that "thread must eventually actually
> write", can you point to where it's described in the standard? For
> example, for plain stores/loads there is no such guarantee, i.e.
> thread perfectly allowed to not do actual writes of non-volatiles
> variables (until it's required by ordering), or not do reads from main
> memory of plain variables (until it's required by ordering). So it's
> unclear to me what guarantees for volatiles and fences and
> AtomicXXX.lazySet().
> 

I agree that the answer is not obvious in the spec. But
you can reason as follows:
1. From JLS sec 17.4.4, the final action of a thread happens-before
(and synchronizes-with) that of any thread joining or waiting for it.
2. Because of the orderAccesses call above, the data write must
happen before the final action of the thread, say t, writing it,
and, inductively, before all other subsequent ordered accesses.
3. So, if t ever terminates and is joined, or performs
any other synchronization, the write will
occur and the read (in some other thread) must see it.

The only cases I know where this reasoning does not apply are for
"daemon" threads that are never joined and never synchronize
in any other way.
In which case they might as well have never run.

-Doug





From dvyukov at gmail.com  Sun Oct 11 11:02:03 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Sun, 11 Oct 2009 08:02:03 -0700
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <4AD1E0CE.5020700@cs.oswego.edu>
References: <9014da950910100717x4a33anb8068d73b1791db6@mail.gmail.com> 
	<4AD1BFCB.8010304@cs.oswego.edu>
	<9014da950910110620j12759b3gc90e24b0d68ec33f@mail.gmail.com> 
	<4AD1E0CE.5020700@cs.oswego.edu>
Message-ID: <9014da950910110802t5152fd28r7a2cee26763dd2f@mail.gmail.com>

On Sun, Oct 11, 2009 at 6:42 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> Dmitriy V'jukov wrote:
>>>>
>>>> class C {
>>>> ? Object data; ?// need volatile access but not volatile
>>>> ? // ...
>>>> ?}
>>>>
>>>> ?class App {
>>>> ?Object getData(C c) {
>>>> ? ? return Fences.orderReads(c).data;
>>>> ?}
>>>>
>>>> ?void setData(C c) {
>>>> ? ? Object newValue = ...;
>>>> ? ? c.data = Fences.orderWrites(newValue);
>>>> ? ? Fences.orderAccesses(c);
>>>> ?}
>>>> ?// ...
>>>> }
>>>>
>> Ok, this is what I mean. You say that "thread must eventually actually
>> write", can you point to where it's described in the standard? For
>> example, for plain stores/loads there is no such guarantee, i.e.
>> thread perfectly allowed to not do actual writes of non-volatiles
>> variables (until it's required by ordering), or not do reads from main
>> memory of plain variables (until it's required by ordering). So it's
>> unclear to me what guarantees for volatiles and fences and
>> AtomicXXX.lazySet().
>>
>
> I agree that the answer is not obvious in the spec. But
> you can reason as follows:
> 1. From JLS sec 17.4.4, the final action of a thread happens-before
> (and synchronizes-with) that of any thread joining or waiting for it.
> 2. Because of the orderAccesses call above, the data write must
> happen before the final action of the thread, say t, writing it,
> and, inductively, before all other subsequent ordered accesses.
> 3. So, if t ever terminates and is joined, or performs
> any other synchronization, the write will
> occur and the read (in some other thread) must see it.
>
> The only cases I know where this reasoning does not apply are for
> "daemon" threads that are never joined and never synchronize
> in any other way.
> In which case they might as well have never run.


Sorry, I am still missing something. You explain visibility in a
recursive way, i.e. this is visible if that is visible, and the
problem that I can not have 'that', just 'this'. What I expect to see
is some *inherent* visibility of volatiles (Fences.orderWrites(),
AtomicXXX.lazySet()), i.e. they have to be eventually visible just
because they have to eventually visible. Otherwise IMHO
synchronization primitive is dead broken. If I correctly understand
you, you are saying that if I write a producer-consumer queue it's
formal correctness depends on in what thread push() will be called
(elements can lost if enqueued from daemon thread). Wicked!

Here is more concrete example to consider:

public class StopThread {
  private static volatile int stopRequested;
  public static void main(String[] args) throws InterruptedException {
    Thread backgroundThread = new Thread(new Runnable() {
      public void run() {
        int i = 0;
        while (stopRequested == 0)
          i++;
      }
    });
    backgroundThread.start();
    TimeUnit.SECONDS.sleep(1);
    stopRequested = 1;
    backgroundThread.join();
  }
}

I guess that every sane programmer will expect the program to
terminate. And you state that the program may not terminate, because
volatile write is guaranteed to reach main memory only straight before
join, but join will not happen until main thread see volatile write.
So if both actions (join, volatile write) does not happen together no
rule is violated.

Another example:

class X
{
	Object data;
	void signal(Object o)
	{
		// basically store-release (w/o trailing #StoreLoad)
		data = Fences.orderWrites(o);
	}
	
	Object wait()
	{
		// basically load-acquire
		while (Fences.orderReads(data)) {}
		return Fences.orderReads(c);
	}
};

class App
{
	X x1;
	X x2;
	
	void thread1()
	{
		//...
		x1.signal(new Object ());
		//...
		x2.wait();
		//...
	}

	void thread2()
	{
		//...
		x2.signal(new Object ());
		//...
		x1.wait();
		//...
	}
}

If there are guarantees only regarding ordering, then this example
allowed to deadlock, because store-release is allowed to sink below
load-acquire. I.e. compiler may generate following code:

	void thread1()
	{
		//...
		x2.wait();
		x1.signal(new Object ());
		//...
	}

	void thread2()
	{
		//...
		x1.wait();
		x2.signal(new Object ());
		//...
	}

which is clearly deadlocks.
And if there are guarantees regarding visibility/coherence (i.e.
Fences.orderWrites()/volatile writes/AtomicXXX.lazySet() MUST become
visible to other threads in reasonable (finite) amount of time) then
compiler is disallowed to generate above deadlocking code, because
then it will violate coherence guarantee.

How Java can live w/o such guarantees? Please help me! I am totally lost.

--
Dmitry Vyukov


From dl at cs.oswego.edu  Sun Oct 11 16:47:21 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 11 Oct 2009 16:47:21 -0400
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910110802t5152fd28r7a2cee26763dd2f@mail.gmail.com>
References: <9014da950910100717x4a33anb8068d73b1791db6@mail.gmail.com>
	<4AD1BFCB.8010304@cs.oswego.edu>
	<9014da950910110620j12759b3gc90e24b0d68ec33f@mail.gmail.com>
	<4AD1E0CE.5020700@cs.oswego.edu>
	<9014da950910110802t5152fd28r7a2cee26763dd2f@mail.gmail.com>
Message-ID: <4AD24459.7060008@cs.oswego.edu>

Dmitriy V'jukov wrote:
> Sorry, I am still missing something. You explain visibility in a recursive
> way, i.e. this is visible if that is visible, and the problem that I can not
> have 'that', just 'this'. What I expect to see is some *inherent* visibility
> of volatiles 

A view more in keeping with the JLS is that a write is visible to a
read if the write happens-before/synchronizes-with the read.

Because Java (like most languages/platforms) has no strict
progress guarantees, the write could be stalled indefinitely,
but it will eventually happen before other actions that this
write in turn must happen-before.

Sorry that my further comment seems to have been confusing:

>> The only cases I know where this reasoning does not apply are for "daemon"
>> threads that are never joined and never synchronize in any other way. In
>> which case they might as well have never run.

This was just a side observation of an (uninteresting) possible gap
in the JLS wording -- it seems to officially allow the existence
of daemon threads without any distinguished end-event.
But it is not at all a practical issue. Among other reasons,
because the mere act of terminating involves a bunch of
synchronization.

-Doug




From davidcholmes at aapt.net.au  Sun Oct 11 18:18:41 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 12 Oct 2009 08:18:41 +1000
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910110802t5152fd28r7a2cee26763dd2f@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEKKIDAA.davidcholmes@aapt.net.au>


I think there are some tangled lines of communication here ...

Dmitriy V'jukov writes:
> Here is more concrete example to consider:
> 
> public class StopThread {
>   private static volatile int stopRequested;
>   public static void main(String[] args) throws InterruptedException {
>     Thread backgroundThread = new Thread(new Runnable() {
>       public void run() {
>         int i = 0;
>         while (stopRequested == 0)
>           i++;
>       }
>     });
>     backgroundThread.start();
>     TimeUnit.SECONDS.sleep(1);
>     stopRequested = 1;
>     backgroundThread.join();
>   }
> }
> 
> I guess that every sane programmer will expect the program to
> terminate. 

And it will terminate. The volatile write to stopRequested in main() will be seen by the new thread as soon as it happens because of the happens-before ordering established by volatile reads and writes. Because the volatile read of stopRequested exists, the volatile write to it _must_ be made visible before a subsequent read of that variable.

Other synchronization actions - like use of synchronized, and Thread.join() - can establish additional happens-before orderings that cover regular and volatile variables.

Happens-before is not just about ordering but also visibility. The happens-before relation is established by the program code not by runtime dynamics.**

One way happens-before used to be described was like this (consult Java Memory Model website/archives to search this out). In general, every write to a variable is placed into a set of writes for that variable, and a read is permitted to read any value in that set. So for example:

  int x = 0;

   // thread 1     // thread 2

   x = 1;
   x = 2;
                    int y = x;

y could have the value 0, 1 or 2, even if the read of x happened long after the write of 2 into x. By establishing happens-before relationships you remove values from the set of writes so that a subsequent read is constrained in what it returns ie:

  volatile int x = 0;

  //  thread 1    // thread 2
                  int y = x; // (a)  
  x = 1;
                  y = x;  // (b)
  x = 2;
                  y = x; // (c)

each read of x in thread 2 is now guaranteed to see specific values:
(a) y == 0
(b) y == 1
(c) y == 2

As I understand the Fences API, while they are expressed in a way that appears to be order-centric, their semantics are defined in terms of happens-before and so also determines visibility.

I hope this is of some assistance. The Java memory model is a very complex area and definitely not intuitive (for most people anyway - myself included).

** If a compiler/run-time system did an analysis that proved that there was no read of a given volatile variable then it could elide the write to the variable (keeping it in a thread-local register). But in the absence of such analysis the code that a JIT/runtime generates/executes for volatile writes, assumes that there will exist a volatile read somewhere and generates appropriate code: a store to main memory plus whatever memory-synchronization operations might be required for that platform.

Cheers,
David Holmes



From nnn6-twfe at spamex.com  Mon Oct 12 01:40:24 2009
From: nnn6-twfe at spamex.com (nnn6-twfe at spamex.com)
Date: Mon, 12 Oct 2009 01:40:24 -0400
Subject: [concurrency-interest] ConcurrentSkipListMap sort on values() not
	keys() ?
Message-ID: <200910120437.n9C4bAYC014902@cs.oswego.edu>

Hi,

We have a large J2EE enterprise application, and we make heavy use of the concurrency library. We use ConcurrentHashMap in many places.

One desire we have had was to have a ConcurrentHashMap but also have the values sorted in a default order so that we don't need to sort the results unless a custom sort is required.

We have looked at ConcurrentSkipListMap[K,V], and that seems to do exactly what we want, *except* it sorts based on the keys not the values.

Here is an simplified example of what we would like to do:
 
private class Group {
  int id;
  String name;
}

private final ConcurrentSkipListMap[Integer,Group] mGroups = new ConcurrentSkipListMap[Integer,Group]();

We have approximately 1000 to 10000 of these group objects and we have many threads accessing them concurrently almost 99.999% of time read only, so read/iteration access without locking is very important.
The code has two main code paths, either get a group by integer unique id, or iterating over the entire set.   When iterating over the entire set, we want them to be in a certain order, e.g. the String name of the group.

Group group = new Group(0,"global");
map.put(id,group)

So, the Key is the integer id, and the value is the Group object.

The ConcurrentSkipListMap specification says it sorts by key rather than value.
We could write a custom comparator, but to sort on a field in the Group object, we would have to use the map to look it up, which may cause a problem? I don't know if it's safe to call methods of the map from inside the comparator.

   class GroupComparator implements Comparator
    {

        public int compare(
            Object o1, Object o2
        ) {
            Integer id1 = (Integer) o1;
            Integer id2 = (Integer) o2;
            // referring back to the map while it is sorting the keys
            // may cause problem?
            Group group1 = (Group) map.get(id1);
            Group group2 = (Group) map.get(id2);
            return group1.getName().compareTo(group2.getName());
        }
    }

If anyone can suggest an alternative data structure or solution, or can confirm it is safe for the Comparator to call get() method on the map, I would appreciate it.

Thanks,

-Alex




From joe.bowbeer at gmail.com  Mon Oct 12 02:04:53 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 11 Oct 2009 23:04:53 -0700
Subject: [concurrency-interest] ConcurrentSkipListMap sort on values()
	not keys() ?
In-Reply-To: <200910120437.n9C4bAYC014902@cs.oswego.edu>
References: <200910120437.n9C4bAYC014902@cs.oswego.edu>
Message-ID: <31f2a7bd0910112304y5259803l552ac53f0e7ace8@mail.gmail.com>

ConcurrentSkipListMap is a SortedMap, so the comparator is used for all key
comparisons:

"... a sorted map performs all key comparisons using its compareTo (or
compare) method, so two keys that are deemed equal by this method are, from
the standpoint of the sorted map, equal."

A comparator that compares values instead of keys will not permit multiple
keys to be mapped to the same (or equal) values.

Joe

On Sun, Oct 11, 2009 at 10:40 PM, Alex wrote:

> Hi,
>
> We have a large J2EE enterprise application, and we make heavy use of the
> concurrency library. We use ConcurrentHashMap in many places.
>
> One desire we have had was to have a ConcurrentHashMap but also have the
> values sorted in a default order so that we don't need to sort the results
> unless a custom sort is required.
>
> We have looked at ConcurrentSkipListMap[K,V], and that seems to do exactly
> what we want, *except* it sorts based on the keys not the values.
>
> Here is an simplified example of what we would like to do:
>
> private class Group {
>  int id;
>  String name;
> }
>
> private final ConcurrentSkipListMap[Integer,Group] mGroups = new
> ConcurrentSkipListMap[Integer,Group]();
>
> We have approximately 1000 to 10000 of these group objects and we have many
> threads accessing them concurrently almost 99.999% of time read only, so
> read/iteration access without locking is very important.
> The code has two main code paths, either get a group by integer unique id,
> or iterating over the entire set.   When iterating over the entire set, we
> want them to be in a certain order, e.g. the String name of the group.
>
> Group group = new Group(0,"global");
> map.put(id,group)
>
> So, the Key is the integer id, and the value is the Group object.
>
> The ConcurrentSkipListMap specification says it sorts by key rather than
> value.
> We could write a custom comparator, but to sort on a field in the Group
> object, we would have to use the map to look it up, which may cause a
> problem? I don't know if it's safe to call methods of the map from inside
> the comparator.
>
>   class GroupComparator implements Comparator
>    {
>
>        public int compare(
>            Object o1, Object o2
>        ) {
>            Integer id1 = (Integer) o1;
>            Integer id2 = (Integer) o2;
>            // referring back to the map while it is sorting the keys
>            // may cause problem?
>            Group group1 = (Group) map.get(id1);
>            Group group2 = (Group) map.get(id2);
>            return group1.getName().compareTo(group2.getName());
>        }
>    }
>
> If anyone can suggest an alternative data structure or solution, or can
> confirm it is safe for the Comparator to call get() method on the map, I
> would appreciate it.
>
> Thanks,
>
> -Alex
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091011/aaf30010/attachment.html>

From ashley.williams at db.com  Mon Oct 12 03:51:17 2009
From: ashley.williams at db.com (Ashley Williams)
Date: Mon, 12 Oct 2009 08:51:17 +0100
Subject: [concurrency-interest] Immutability with own by reference
Message-ID: <OF4DEEE186.0E1A3A9A-ON8025764D.002A4E15-8025764D.002B25B1@db.com>

Hi,

Just a quick one: if my class contains some sort of modifiable list then 
usually it would be considered mutable. But is that the case if my class 
doesn't actually own the lifecycle of that list? So for example if that 
list has "owned by reference" semantics and was supplied externally by 
some parent of my class and the list that it writes its results to.

@Immutable??
class MyClass implements SomeCallback {
    List someList;

    MyClass(List someList) {
        this.someList = someList;
    }

    void onMessage(Message msg) {
        someList.add(msg);
    }

}

My hunch is that MyClass is immutable because the state is recorded in 
whichever class owns someList and we are just holding a handy reference to 
it. Is this reasoning sound?

Many thanks
- Ashley


---

This e-mail may contain confidential and/or privileged information. If you are not the intended recipient (or have received this e-mail in error) please notify the sender immediately and delete this e-mail. Any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly forbidden.

Please refer to http://www.db.com/en/content/eu_disclosures.htm for additional EU corporate and regulatory disclosures.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091012/c1d4e8ac/attachment.html>

From nnn6-twfe at spamex.com  Mon Oct 12 04:59:04 2009
From: nnn6-twfe at spamex.com (nnn6-twfe at spamex.com)
Date: Mon, 12 Oct 2009 04:59:04 -0400
Subject: [concurrency-interest] ConcurrentSkipListMap sort on values()
	not keys() ?
Message-ID: <200910120755.n9C7tQ6q023261@cs.oswego.edu>

Joe,

Thanks for the reply.

For our use case, each key (group unique integer id), there is one-to-one relationship between the ID and a Group object.  Each Group object is mapped to only one ID. However, what we are sorting on, the Group name, is not guaranteed to be unique. We can have multiple groups with the same name.

After thinking about this more, even if what I suggested could be made to work, the problem is we are iterating over the map.values() not the map.keys(), so it doesn't help that the keys are sorted based on custom comparator.  We would have to iterate over map.keys() and then lookup each value, which wouldn't be efficient.

Currently we just use ConcurrentHashMap so that concurrent lookups of the Group object by unique integer id is fast. Iteration over the group objects through map.values() is also fast.  But the inefficient part is that at a higher layer we end up having to sort the Group objects by name every where they are
displayed in list.  If somehow we could have a data structure that allowed for fast map lookups but also kept the values in sorted order (via Comparator or natural order), that would be the ultimate solution.    We don't care if updates or sorting is relatively slow, since 99.999% of the time there are only reader threads.

This is a generic problem we have throughout our application. We have many different objects like Group object which have the same use case. 

In the UI, we have paginated views, so we only show the first 50 rows by default, but in order to figure out which are the first 50 rows, you have to sort all the objects by the sort key, e.g. name.


-Alex

On Sun, 11 Oct 2009 23:04 -0700, "Joe Bowbeer"
<joe.bowbeer at gmail.com> wrote:

  ConcurrentSkipListMap is a SortedMap, so the comparator is
  used for all key comparisons:
  "... a sorted map performs all key comparisons using its
  compareTo (or compare) method, so two keys that are deemed
  equal by this method are, from the standpoint of the sorted
  map, equal."
  A comparator that compares values instead of keys will not
  permit multiple keys to be mapped to the same (or equal)
  values.
  Joe

On Sun, Oct 11, 2009 at 10:40 PM, Alex wrote:

  Hi,
  We have a large J2EE enterprise application, and we make heavy
  use of the concurrency library. We use ConcurrentHashMap in
  many places.
  One desire we have had was to have a ConcurrentHashMap but
  also have the values sorted in a default order so that we
  don't need to sort the results unless a custom sort is
  required.
  We have looked at ConcurrentSkipListMap[K,V], and that seems
  to do exactly what we want, *except* it sorts based on the
  keys not the values.
  Here is an simplified example of what we would like to do:
  private class Group {
   int id;
   String name;
  }
  private final ConcurrentSkipListMap[Integer,Group] mGroups =
  new ConcurrentSkipListMap[Integer,Group]();
  We have approximately 1000 to 10000 of these group objects and
  we have many threads accessing them concurrently almost
  99.999% of time read only, so read/iteration access without
  locking is very important.
  The code has two main code paths, either get a group by
  integer unique id, or iterating over the entire set.   When
  iterating over the entire set, we want them to be in a certain
  order, e.g. the String name of the group.
  Group group = new Group(0,"global");
  map.put(id,group)
  So, the Key is the integer id, and the value is the Group
  object.
  The ConcurrentSkipListMap specification says it sorts by key
  rather than value.
  We could write a custom comparator, but to sort on a field in
  the Group object, we would have to use the map to look it up,
  which may cause a problem? I don't know if it's safe to call
  methods of the map from inside the comparator.
    class GroupComparator implements Comparator
     {
         public int compare(
             Object o1, Object o2
         ) {
             Integer id1 = (Integer) o1;
             Integer id2 = (Integer) o2;
             // referring back to the map while it is sorting
  the keys
             // may cause problem?
             Group group1 = (Group) map.get(id1);
             Group group2 = (Group) map.get(id2);
             return
  group1.getName().compareTo(group2.getName());
         }
     }
  If anyone can suggest an alternative data structure or
  solution, or can confirm it is safe for the Comparator to call
  get() method on the map, I would appreciate it.
  Thanks,
  -Alex

From dvyukov at gmail.com  Mon Oct 12 04:17:40 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Mon, 12 Oct 2009 01:17:40 -0700
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEKKIDAA.davidcholmes@aapt.net.au>
References: <9014da950910110802t5152fd28r7a2cee26763dd2f@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCAEKKIDAA.davidcholmes@aapt.net.au>
Message-ID: <9014da950910120117l3adc19d8ifeb8b60a98fc204c@mail.gmail.com>

On Sun, Oct 11, 2009 at 3:18 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>
> I think there are some tangled lines of communication here ...
>
> Dmitriy V'jukov writes:
>> Here is more concrete example to consider:
>>
>> public class StopThread {
>> ? private static volatile int stopRequested;
>> ? public static void main(String[] args) throws InterruptedException {
>> ? ? Thread backgroundThread = new Thread(new Runnable() {
>> ? ? ? public void run() {
>> ? ? ? ? int i = 0;
>> ? ? ? ? while (stopRequested == 0)
>> ? ? ? ? ? i++;
>> ? ? ? }
>> ? ? });
>> ? ? backgroundThread.start();
>> ? ? TimeUnit.SECONDS.sleep(1);
>> ? ? stopRequested = 1;
>> ? ? backgroundThread.join();
>> ? }
>> }
>>
>> I guess that every sane programmer will expect the program to
>> terminate.
>
> And it will terminate. The volatile write to stopRequested in main() will be seen by the new thread as soon as it happens because of the happens-before ordering established by volatile reads and writes.

There is no happens-before relations yet. So you can't say that
something it this code happens because of the 'happens-before'. They
are established only when volatile read *by some other reason* returns
value stored by volatile write.


> Because the volatile read of stopRequested exists, the volatile write to it _must_ be made visible before a subsequent read of that variable.

David, as far as I understand, you misunderstand the specifications.
'subsequent' is defined in the specification not by means of wall
clock time. It's defined by synchronization order. Not vise versa. And
there are only weak consistency requirements for synchronization
order.
So, even if load happens after 1 hour by wall clock after write, it
may not be 'subsequent'. And it the case in my example - all (infinite
number of) loads of stopRequested perfectly maybe be NOT subsequent to
store to stopRequested. That's what I am talking about.


> Other synchronization actions - like use of synchronized, and Thread.join() - can establish additional happens-before orderings that cover regular and volatile variables.

I hope that I misunderstand you. Do you actually mean that volatiles
are useless for synchronization (i.e. may not propagate changes) if
not accompanied by periodic use of synchronized or thread joins?


> Happens-before is not just about ordering but also visibility. The happens-before relation is established by the program code not by runtime dynamics.**

As far as I understand happens-before, it's solely about ordering.
Yes, it provides visibility BUT only for dependent state (not
"top-level" accesses). I.e. if you have only 1 volatile store in a
program, there is no happens-before edges yet, so they can guarantee
NOTHING for that store.


> One way happens-before used to be described was like this (consult Java Memory Model website/archives to search this out). In general, every write to a variable is placed into a set of writes for that variable, and a read is permitted to read any value in that set. So for example:
>
> ?int x = 0;
>
> ? // thread 1 ? ? // thread 2
>
> ? x = 1;
> ? x = 2;
> ? ? ? ? ? ? ? ? ? ?int y = x;
>
> y could have the value 0, 1 or 2, even if the read of x happened long after the write of 2 into x. By establishing happens-before relationships you remove values from the set of writes so that a subsequent read is constrained in what it returns ie:
>
> ?volatile int x = 0;
>
> ?// ?thread 1 ? ?// thread 2
> ? ? ? ? ? ? ? ? ?int y = x; // (a)
> ?x = 1;
> ? ? ? ? ? ? ? ? ?y = x; ?// (b)
> ?x = 2;
> ? ? ? ? ? ? ? ? ?y = x; // (c)
>
> each read of x in thread 2 is now guaranteed to see specific values:
> (a) y == 0
> (b) y == 1
> (c) y == 2

Hummm... I'm not sure from what you draw any guarantees for this code.
>From mutual disposition of code for 2 threads? And what if I reformat
it in the following way:

volatile int x = 0;
//  thread 1    // thread 2
                 int y = x; // (a)
                 y = x;  // (b)
                 y = x; // (c)
x = 1;
x = 2;

All guarantees disappear now?
David, it's 'subsequent' that is defined by 'happens-before', not vise versa.


>
> As I understand the Fences API, while they are expressed in a way that appears to be order-centric, their semantics are defined in terms of happens-before and so also determines visibility.

Ordering and visibility is completely different things.
Roughly speaking, ordering deals with propagation *of dependent
state*. While visibility deals with first-class propagation of
changes.
For example for producer-consumer queue, ordering is charged with the
following thing: consumer will see correct user data associated with
the queue element. While visibility is charged with: consumer will see
queue element.

If synchronization primitive does not provide any visibility
guarantees it's basically useless, because allows a lot of code to not
take any effect (at least for a long-long time, basically infinity),
and makes some code deadlocking.


> I hope this is of some assistance. The Java memory model is a very complex area and definitely not intuitive (for most people anyway - myself included).
>
> ** If a compiler/run-time system did an analysis that proved that there was no read of a given volatile variable then it could elide the write to the variable (keeping it in a thread-local register). But in the absence of such analysis the code that a JIT/runtime generates/executes for volatile writes, assumes that there will exist a volatile read somewhere and generates appropriate code: a store to main memory plus whatever memory-synchronization operations might be required for that platform.

This is closer. Where is this guarantee in the specification? The
guarantee along the lines of: for volatile store compiler has to
generate write to main memory, and that write must not deviate too
much from program order (i.e. for example, write can't sink below a
loop with large number of iterations). And the same for loads. This
will be something like a guarantee of coherency. But will still not
cover Fences (they work with plain variables, not volatiles).

-- 
Dmitry Vyukov


From davidcholmes at aapt.net.au  Mon Oct 12 05:37:51 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 12 Oct 2009 19:37:51 +1000
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910120117l3adc19d8ifeb8b60a98fc204c@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEKMIDAA.davidcholmes@aapt.net.au>


Dmitriy,

The definition of "happens-before" from the JLS:

17.4.5 Happens-before Order
Two actions can be ordered by a happens-before relationship. If one action happens-
before another, then the first is visible to and ordered before the second.

---

As you can see _visibility_ is very much a part of it.

And happens-before is a relationship established by actions defined in the program source code. If an action A happens-before an action B, and at runtime B occurs subsequent to A, then because there is a happens-before relationship between them, we can deduce certain things from that. For example consider this class:

class VolatileValue {
  static volatile int x = 1;

  static int getX() { return x; }

  static void setX() { x = 42; }
}

The memory model rules tell us that:

"A write to a volatile variable (?8.3.1.4) v synchronizes-with all subsequent
reads of v by any thread (where subsequent is defined according to the synchronization
order)."

Or in simpler, informal, terms: a write to a volatile variable happens-before a subsequent read of that variable.

So suppose thread 1 calls VolatileValue.getX() and thread 2 calls VolatileValue.setX(). We don't know a-priori what order those calls will be made in but we do know the results of getX() for either ordering:
- 1 if the getX() occurs first; and
- 42 if the setX() occurs first

---

I suggest you take further discussion of Java Memory Model fundamentals to the JMM mailing list. My way of expressing things may not be precise enough to aid your understanding, so I'll leave it to one of the formalists there to express things more precisely.

https://mailman.cs.umd.edu/mailman/listinfo/javamemorymodel-discussion

Good luck.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dmitriy
> V'jukov
> Sent: Monday, 12 October 2009 6:18 PM
> To: dholmes at ieee.org
> Cc: Doug Lea; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Fences API and visibility/coherency
> 
> 
> On Sun, Oct 11, 2009 at 3:18 PM, David Holmes 
> <davidcholmes at aapt.net.au> wrote:
> >
> > I think there are some tangled lines of communication here ...
> >
> > Dmitriy V'jukov writes:
> >> Here is more concrete example to consider:
> >>
> >> public class StopThread {
> >>   private static volatile int stopRequested;
> >>   public static void main(String[] args) throws InterruptedException {
> >>     Thread backgroundThread = new Thread(new Runnable() {
> >>       public void run() {
> >>         int i = 0;
> >>         while (stopRequested == 0)
> >>           i++;
> >>       }
> >>     });
> >>     backgroundThread.start();
> >>     TimeUnit.SECONDS.sleep(1);
> >>     stopRequested = 1;
> >>     backgroundThread.join();
> >>   }
> >> }
> >>
> >> I guess that every sane programmer will expect the program to
> >> terminate.
> >
> > And it will terminate. The volatile write to stopRequested in 
> main() will be seen by the new thread as soon as it happens 
> because of the happens-before ordering established by volatile 
> reads and writes.
> 
> There is no happens-before relations yet. So you can't say that
> something it this code happens because of the 'happens-before'. They
> are established only when volatile read *by some other reason* returns
> value stored by volatile write.
> 
> 
> > Because the volatile read of stopRequested exists, the volatile 
> write to it _must_ be made visible before a subsequent read of 
> that variable.
> 
> David, as far as I understand, you misunderstand the specifications.
> 'subsequent' is defined in the specification not by means of wall
> clock time. It's defined by synchronization order. Not vise versa. And
> there are only weak consistency requirements for synchronization
> order.
> So, even if load happens after 1 hour by wall clock after write, it
> may not be 'subsequent'. And it the case in my example - all (infinite
> number of) loads of stopRequested perfectly maybe be NOT subsequent to
> store to stopRequested. That's what I am talking about.
> 
> 
> > Other synchronization actions - like use of synchronized, and 
> Thread.join() - can establish additional happens-before orderings 
> that cover regular and volatile variables.
> 
> I hope that I misunderstand you. Do you actually mean that volatiles
> are useless for synchronization (i.e. may not propagate changes) if
> not accompanied by periodic use of synchronized or thread joins?
> 
> 
> > Happens-before is not just about ordering but also visibility. 
> The happens-before relation is established by the program code 
> not by runtime dynamics.**
> 
> As far as I understand happens-before, it's solely about ordering.
> Yes, it provides visibility BUT only for dependent state (not
> "top-level" accesses). I.e. if you have only 1 volatile store in a
> program, there is no happens-before edges yet, so they can guarantee
> NOTHING for that store.
> 
> 
> > One way happens-before used to be described was like this 
> (consult Java Memory Model website/archives to search this out). 
> In general, every write to a variable is placed into a set of 
> writes for that variable, and a read is permitted to read any 
> value in that set. So for example:
> >
> >  int x = 0;
> >
> >   // thread 1     // thread 2
> >
> >   x = 1;
> >   x = 2;
> >                    int y = x;
> >
> > y could have the value 0, 1 or 2, even if the read of x 
> happened long after the write of 2 into x. By establishing 
> happens-before relationships you remove values from the set of 
> writes so that a subsequent read is constrained in what it returns ie:
> >
> >  volatile int x = 0;
> >
> >  //  thread 1    // thread 2
> >                  int y = x; // (a)
> >  x = 1;
> >                  y = x;  // (b)
> >  x = 2;
> >                  y = x; // (c)
> >
> > each read of x in thread 2 is now guaranteed to see specific values:
> > (a) y == 0
> > (b) y == 1
> > (c) y == 2
> 
> Hummm... I'm not sure from what you draw any guarantees for this code.
> From mutual disposition of code for 2 threads? And what if I reformat
> it in the following way:
> 
> volatile int x = 0;
> //  thread 1    // thread 2
>                  int y = x; // (a)
>                  y = x;  // (b)
>                  y = x; // (c)
> x = 1;
> x = 2;
> 
> All guarantees disappear now?
> David, it's 'subsequent' that is defined by 'happens-before', not 
> vise versa.
> 
> 
> >
> > As I understand the Fences API, while they are expressed in a 
> way that appears to be order-centric, their semantics are defined 
> in terms of happens-before and so also determines visibility.
> 
> Ordering and visibility is completely different things.
> Roughly speaking, ordering deals with propagation *of dependent
> state*. While visibility deals with first-class propagation of
> changes.
> For example for producer-consumer queue, ordering is charged with the
> following thing: consumer will see correct user data associated with
> the queue element. While visibility is charged with: consumer will see
> queue element.
> 
> If synchronization primitive does not provide any visibility
> guarantees it's basically useless, because allows a lot of code to not
> take any effect (at least for a long-long time, basically infinity),
> and makes some code deadlocking.
> 
> 
> > I hope this is of some assistance. The Java memory model is a 
> very complex area and definitely not intuitive (for most people 
> anyway - myself included).
> >
> > ** If a compiler/run-time system did an analysis that proved 
> that there was no read of a given volatile variable then it could 
> elide the write to the variable (keeping it in a thread-local 
> register). But in the absence of such analysis the code that a 
> JIT/runtime generates/executes for volatile writes, assumes that 
> there will exist a volatile read somewhere and generates 
> appropriate code: a store to main memory plus whatever 
> memory-synchronization operations might be required for that platform.
> 
> This is closer. Where is this guarantee in the specification? The
> guarantee along the lines of: for volatile store compiler has to
> generate write to main memory, and that write must not deviate too
> much from program order (i.e. for example, write can't sink below a
> loop with large number of iterations). And the same for loads. This
> will be something like a guarantee of coherency. But will still not
> cover Fences (they work with plain variables, not volatiles).
> 
> -- 
> Dmitry Vyukov
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From jim.andreou at gmail.com  Mon Oct 12 05:41:24 2009
From: jim.andreou at gmail.com (Jim Andreou)
Date: Mon, 12 Oct 2009 12:41:24 +0300
Subject: [concurrency-interest] ConcurrentSkipListMap sort on values()
	not keys() ?
In-Reply-To: <200910120755.n9C7tQ6q023261@cs.oswego.edu>
References: <200910120755.n9C7tQ6q023261@cs.oswego.edu>
Message-ID: <7d7138c10910120241q75ef48ebr24df7ae1831279f8@mail.gmail.com>

2009/10/12  <nnn6-twfe at spamex.com>:
> After thinking about this more, even if what I suggested could be made to work, the problem is we are iterating over the map.values() not the map.keys(), so it doesn't help that the keys are sorted based on custom comparator. ?We would have to iterate over map.keys() and then lookup each value, which wouldn't be efficient.

Iterators of map.values() and map.keySet() are in the same order (the
order of keys), quite naturally.

Dimitris


From dvyukov at gmail.com  Mon Oct 12 06:46:29 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Mon, 12 Oct 2009 14:46:29 +0400
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <4AD24459.7060008@cs.oswego.edu>
References: <9014da950910100717x4a33anb8068d73b1791db6@mail.gmail.com> 
	<4AD1BFCB.8010304@cs.oswego.edu>
	<9014da950910110620j12759b3gc90e24b0d68ec33f@mail.gmail.com> 
	<4AD1E0CE.5020700@cs.oswego.edu>
	<9014da950910110802t5152fd28r7a2cee26763dd2f@mail.gmail.com> 
	<4AD24459.7060008@cs.oswego.edu>
Message-ID: <9014da950910120346u173c36cdq8133e44ed808c060@mail.gmail.com>

On Sun, Oct 11, 2009 at 1:47 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> Dmitriy V'jukov wrote:
>>
>> Sorry, I am still missing something. You explain visibility in a recursive
>> way, i.e. this is visible if that is visible, and the problem that I can
>> not
>> have 'that', just 'this'. What I expect to see is some *inherent*
>> visibility
>> of volatiles
>
> A view more in keeping with the JLS is that a write is visible to a
> read if the write happens-before/synchronizes-with the read.

Arrrgggghhhh!
Please, see my reply to David. I understand that it's much more likely
that it's me who miss something than you and David. But happens-before
may only guarantee visibility of *dependent* state, and there are
always 'top-level' changes that are not covered by any happens-before
edges, they have to propagate on their own w/o any assistance.
It's called 'cache-coherency', or 'coherent system', i.e. changes once
done automatically propagate throughout the system on it's own. And
what I draw from what you and David say, JVM is an abstraction of not
cache coherent machine, i.e. changes do not have to propagate by
itself. And it's totally wicked for me, while programming distributed
cache-coherent machines is hard, programming distributed non
cache-coherent machines is brain-damaging. I thought that Java is
about simplicity in it's memory model...


> Because Java (like most languages/platforms) has no strict
> progress guarantees, the write could be stalled indefinitely,
> but it will eventually happen before other actions that this
> write in turn must happen-before.

I understand that such an abstract specification can't provide exact
timings, and that's not required.
I don't know regarding most languages. But C++ standard just says that:
"Implementations should make atomic stores visible to atomic loads
within a reasonable amount of time."
And it's enough for programs to not deadlock, and to be sure that
atomic stores will not sink below infinite loop.
The same applies to CLI/C#. You may see following discussion with Joe
Duffy (lead of Microsoft Parallel Team):
http://www.bluebytesoftware.com/blog/CommentView,guid,72aaa68e-4cbd-41db-a7ce-ddc64411eafa.aspx
(see comments section)
Basically, Joe said that the program may deadlock.
I said that if the program deadlocks, then the language does not
provide coherency guarantees and is not useful for concurrent
programming. The language must provide coherency guarantees.
Joe said that he consulted with compiler team and they conclude that
the aspect is underspecified and it's serious hole in the language,
and it's actually may lead to deadlocks in 'good' programs that every
sane programmer will expect to not deadlock. As far as I understand
they are going to patch specification and compiler.

Now as for practical aspect.
I believe that the things is not as you said, and every sane language
implementation provides inherent coherency guarantees for volatiles.
I.e. they becomes visible to other threads in reasonable amount of
time on their own. But it's not the case for plain variables. And the
concern is with Fences API because it uses plain variables. So it's
not clear as to whether 'good' program based on Fences allowed to
deadlock or not. For example:

class SingleProducerSingleConsumerQueue; // based on plain vars and Fences API

SingleProducerSingleConsumerQueue queue1;
SingleProducerSingleConsumerQueue queue2;

// thread 1:
queue1.push(new Object());
while (queue2.pop() == null) {}

// thread 2:
queue2.push(new Object());
while (queue1.pop() == null) {}

If methods will be inlined, push to queue may sink below pop, and the
program will deadlock. I do not see anything in the specification that
prevents that.

-- 
Dmitry Vyukov

From dvyukov at gmail.com  Mon Oct 12 08:15:16 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Mon, 12 Oct 2009 16:15:16 +0400
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEKMIDAA.davidcholmes@aapt.net.au>
References: <9014da950910120117l3adc19d8ifeb8b60a98fc204c@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCIEKMIDAA.davidcholmes@aapt.net.au>
Message-ID: <9014da950910120515x3cf069f2t72869828eb60fa0f@mail.gmail.com>

On Mon, Oct 12, 2009 at 1:37 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>
> Dmitriy,
>
> The definition of "happens-before" from the JLS:
>
> 17.4.5 Happens-before Order
> Two actions can be ordered by a happens-before relationship. If one action happens-
> before another, then the first is visible to and ordered before the second.
>
> ---
>
> As you can see _visibility_ is very much a part of it.
>
> And happens-before is a relationship established by actions defined in the program source code. If an action A happens-before an action B, and at runtime B occurs subsequent to A, then because there is a happens-before relationship between them, we can deduce certain things from that. For example consider this class:
>
> class VolatileValue {
>  static volatile int x = 1;
>
>  static int getX() { return x; }
>
>  static void setX() { x = 42; }
> }
>
> The memory model rules tell us that:
>
> "A write to a volatile variable (?8.3.1.4) v synchronizes-with all subsequent
> reads of v by any thread (where subsequent is defined according to the synchronization
> order)."
>
> Or in simpler, informal, terms: a write to a volatile variable happens-before a subsequent read of that variable.
>
> So suppose thread 1 calls VolatileValue.getX() and thread 2 calls VolatileValue.setX(). We don't know a-priori what order those calls will be made in but we do know the results of getX() for either ordering:
> - 1 if the getX() occurs first; and
> - 42 if the setX() occurs first


My question basically boils down to: if thread 1 periodically calls
VolatileValue.getX(), and we know by wall clock that thread 2 executed
VolatileValue.setX(), let's say, an hour ago, is there any guarantees
that thread 1 will eventually see 42?
Then, if we will replace volatile by plain var + Fences, what about
guarantees then?
Happens-before has nothing to do with this. Because it's
happens-before that will be determined by the synchronizes-with
relation, and not vise versa. Coherence is a kind of primary thing
here, and ordering is secondary, sort of.


> ---
>
> I suggest you take further discussion of Java Memory Model fundamentals to the JMM mailing list. My way of expressing things may not be precise enough to aid your understanding, so I'll leave it to one of the formalists there to express things more precisely.
>
> https://mailman.cs.umd.edu/mailman/listinfo/javamemorymodel-discussion

Thank you. I will ask my question there. I was just not aware of that
mailing, and seen here some discussions of Fences API.


--
Dmitry Vyukov


From gregg at cytetech.com  Mon Oct 12 09:29:01 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 12 Oct 2009 08:29:01 -0500
Subject: [concurrency-interest] ConcurrentSkipListMap sort on values()
 not keys() ?
In-Reply-To: <200910120755.n9C7tQ6q023261@cs.oswego.edu>
References: <200910120755.n9C7tQ6q023261@cs.oswego.edu>
Message-ID: <4AD32F1D.8030808@cytetech.com>

nnn6-twfe at spamex.com wrote:
> Joe,
> 
> Thanks for the reply.
> 
> For our use case, each key (group unique integer id), there is one-to-one relationship between the ID and a Group object.  Each Group object is mapped to only one ID. However, what we are sorting on, the Group name, is not guaranteed to be unique. We can have multiple groups with the same name.
> 
> After thinking about this more, even if what I suggested could be made to work, the problem is we are iterating over the map.values() not the map.keys(), so it doesn't help that the keys are sorted based on custom comparator.  We would have to iterate over map.keys() and then lookup each value, which wouldn't be efficient.
> 
> Currently we just use ConcurrentHashMap so that concurrent lookups of the Group object by unique integer id is fast. Iteration over the group objects through map.values() is also fast.  But the inefficient part is that at a higher layer we end up having to sort the Group objects by name every where they are
> displayed in list.  If somehow we could have a data structure that allowed for fast map lookups but also kept the values in sorted order (via Comparator or natural order), that would be the ultimate solution.    We don't care if updates or sorting is relatively slow, since 99.999% of the time there are only reader threads.
> 
> This is a generic problem we have throughout our application. We have many different objects like Group object which have the same use case. 
> 
> In the UI, we have paginated views, so we only show the first 50 rows by default, but in order to figure out which are the first 50 rows, you have to sort all the objects by the sort key, e.g. name.
> 
> 
> -Alex
> 
> On Sun, 11 Oct 2009 23:04 -0700, "Joe Bowbeer"
> <joe.bowbeer at gmail.com> wrote:
> 
>   ConcurrentSkipListMap is a SortedMap, so the comparator is
>   used for all key comparisons:
>   "... a sorted map performs all key comparisons using its
>   compareTo (or compare) method, so two keys that are deemed
>   equal by this method are, from the standpoint of the sorted
>   map, equal."
>   A comparator that compares values instead of keys will not
>   permit multiple keys to be mapped to the same (or equal)
>   values.
>   Joe
> 
> On Sun, Oct 11, 2009 at 10:40 PM, Alex wrote:
> 
>   Hi,
>   We have a large J2EE enterprise application, and we make heavy
>   use of the concurrency library. We use ConcurrentHashMap in
>   many places.
>   One desire we have had was to have a ConcurrentHashMap but
>   also have the values sorted in a default order so that we
>   don't need to sort the results unless a custom sort is
>   required.
>   We have looked at ConcurrentSkipListMap[K,V], and that seems
>   to do exactly what we want, *except* it sorts based on the
>   keys not the values.
>   Here is an simplified example of what we would like to do:
>   private class Group {
>    int id;
>    String name;
>   }
>   private final ConcurrentSkipListMap[Integer,Group] mGroups =
>   new ConcurrentSkipListMap[Integer,Group]();
>   We have approximately 1000 to 10000 of these group objects and
>   we have many threads accessing them concurrently almost
>   99.999% of time read only, so read/iteration access without
>   locking is very important.
>   The code has two main code paths, either get a group by
>   integer unique id, or iterating over the entire set.   When
>   iterating over the entire set, we want them to be in a certain
>   order, e.g. the String name of the group.
>   Group group = new Group(0,"global");
>   map.put(id,group)
>   So, the Key is the integer id, and the value is the Group
>   object.
>   The ConcurrentSkipListMap specification says it sorts by key
>   rather than value.
>   We could write a custom comparator, but to sort on a field in
>   the Group object, we would have to use the map to look it up,
>   which may cause a problem? I don't know if it's safe to call
>   methods of the map from inside the comparator.
>     class GroupComparator implements Comparator
>      {
>          public int compare(
>              Object o1, Object o2
>          ) {
>              Integer id1 = (Integer) o1;
>              Integer id2 = (Integer) o2;
>              // referring back to the map while it is sorting
>   the keys
>              // may cause problem?
>              Group group1 = (Group) map.get(id1);
>              Group group2 = (Group) map.get(id2);
>              return
>   group1.getName().compareTo(group2.getName());
>          }
>      }
>   If anyone can suggest an alternative data structure or
>   solution, or can confirm it is safe for the Comparator to call
>   get() method on the map, I would appreciate it.
>   Thanks,

I usually do something like that, and if you use Generics and put "Comparable" 
on classes involved you can reduce some of the visible code.

Map<Integer,String>map = new ConcurrentHashMap<Integer,String>();
List<Integer>sorted = new ArrayList<Integer>( map.keySet() );

Collections.sort( sorted, new Comparator<Integer>() {
	public int compare( Integer i1, Integer i2 ) {
		return map.get(i1).compareTo( map.get(i2) );
	}
});

This works quite well as long as you have appropriate locking in use.

Gregg Wonderly

From blanshlu at netscape.net  Mon Oct 12 10:22:21 2009
From: blanshlu at netscape.net (Luke Blanshard)
Date: Mon, 12 Oct 2009 09:22:21 -0500
Subject: [concurrency-interest] ConcurrentSkipListMap sort on values()
	not keys() ?
In-Reply-To: <200910120437.n9C4bAYC014902@cs.oswego.edu>
References: <200910120437.n9C4bAYC014902@cs.oswego.edu>
Message-ID: <ff6d37ad0910120722n6e4fbd64l167eacc0056f124e@mail.gmail.com>

On Mon, Oct 12, 2009 at 12:40 AM, <nnn6-twfe at spamex.com> wrote:

> ...
>
> If anyone can suggest an alternative data structure or solution, or can
> confirm it is safe for the Comparator to call get() method on the map, I
> would appreciate it.
>

 You can tell that it's not safe for the comparator to rely solely on the
map to locate the correct group, just consider how a new group is added to
the map.  The comparator needs to be invoked to determine the placement of
the group within the map, and since it isn't there yet the comparator will
blow up.

Likewise, consider how a lookup happens.  To find out where in the map to
look for the group, the map must invoke the comparator.  And the comparator
tries to get the group from the map to be able to tell it.  Stack overflow.

You could consider a hybrid structure, using both a hash map and a skip
list.  Use the hash map for lookup by id, and the skip list for iteration in
the correct order.  The skip list's comparator uses the hash map to pull out
the objects being compared.  So the "put" implementation for this hybrid map
first puts into the hash map, then into the skip list.  Because the
comparator relies only on the hash map, this will be safe.  (Though there is
a race condition between put and remove, unless you have some extra
synchronization.  Probably not a factor in practice, but you'd have to prove
that to yourself.)

Luke
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091012/2579dd39/attachment.html>

From ashley.williams at db.com  Mon Oct 12 10:27:21 2009
From: ashley.williams at db.com (Ashley Williams)
Date: Mon, 12 Oct 2009 15:27:21 +0100
Subject: [concurrency-interest] Immutability with own by reference
In-Reply-To: <4d22525f0910120451s700bcce0s5c3cc8278b40034a@mail.gmail.com>
Message-ID: <OFB7621F38.0A0BFEC5-ON8025764D.004EEEC2-8025764D.004F6858@db.com>

Hi Anton,

I'm aware of the pitfalls you mentioned, but the code I hand-typed is more 
for illustrative purposes -
you can assume the final keyword and also some better data structure such 
as ConcurrentHashMap.
It's the effects of the ownership semantics I'm more interested in, i.e. 
own by value versus own by reference.

- Ashley

Anton Maximov <amaximov at gmail.com> wrote on 12/10/2009 12:51:44:

> i would declare someList reference as final to ensure safe publication
> and initialization safety (JCIP 3.5.1 and 3.5.2).
> 
> also, you are mutating the structure of the list passed to you without
> any synchronization; unless your List implementation itself takes care
> of synchronization internally, you are asking for trouble.
> 
> -a
> 
> On Mon, Oct 12, 2009 at 3:51 AM, Ashley Williams <ashley.
> williams at db.com> wrote:
> >
> > Hi,
> >
> > Just a quick one: if my class contains some sort of modifiable list 
then
> > usually it would be considered mutable. But is that the case if my 
class
> > doesn't actually own the lifecycle of that list? So for example ifthat 
list
> > has "owned by reference" semantics and was supplied externally by some
> > parent of my class and the list that it writes its results to.
> >
> > @Immutable??
> > class MyClass implements SomeCallback {
> >     List someList;
> >
> >     MyClass(List someList) {
> >         this.someList = someList;
> >     }
> >
> >     void onMessage(Message msg) {
> >         someList.add(msg);
> >     }
> >
> > }
> >
> > My hunch is that MyClass is immutable because the state is recorded in
> > whichever class owns someList and we are just holding a handy 
reference to
> > it. Is this reasoning sound?
> >
> > Many thanks
> > - Ashley
> >
> > ---
> >
> > This e-mail may contain confidential and/or privileged information. If 
you
> > are not the intended recipient (or have received this e-mail in error)
> > please notify the sender immediately and delete this e-mail. Any
> > unauthorized copying, disclosure or distribution of the material in 
this
> > e-mail is strictly forbidden.
> >
> > Please refer to http://www.db.com/en/content/eu_disclosures.htm for
> > additional EU corporate and regulatory disclosures.
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >



---

This e-mail may contain confidential and/or privileged information. If you are not the intended recipient (or have received this e-mail in error) please notify the sender immediately and delete this e-mail. Any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly forbidden.

Please refer to http://www.db.com/en/content/eu_disclosures.htm for additional EU corporate and regulatory disclosures.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091012/16021d13/attachment.html>

From gilgamez at gmail.com  Mon Oct 12 17:05:18 2009
From: gilgamez at gmail.com (Gilgamesh Nootebos)
Date: Mon, 12 Oct 2009 23:05:18 +0200
Subject: [concurrency-interest] ConcurrentSkipListMap sort on values()
	not keys() ?
In-Reply-To: <ff6d37ad0910120722n6e4fbd64l167eacc0056f124e@mail.gmail.com>
References: <200910120437.n9C4bAYC014902@cs.oswego.edu>
	<ff6d37ad0910120722n6e4fbd64l167eacc0056f124e@mail.gmail.com>
Message-ID: <574ca9af0910121405m3488cc5aue353c31f86378683@mail.gmail.com>

Hi Alex,

It's also possible to create a custom map that uses the skiplistmap as
a backing structure and and wrap every key in an internal key that
defines a comparator on the value.

I've created a little example of this concept to show what I mean.
It's by no means complete or very robust but its only a demo after
all. I haven't given much thought on the performance properties of
reading values from the map, perhaps you can use a readonly view of
the internal map to bypass the abstractmap implementation.

Hope this brings you closer to a solution for your question.

Regards,

Gilgamesh Nootebos



On Mon, Oct 12, 2009 at 16:22, Luke Blanshard <blanshlu at netscape.net> wrote:
> On Mon, Oct 12, 2009 at 12:40 AM, <nnn6-twfe at spamex.com> wrote:
>>
>> ...
>>
>> If anyone can suggest an alternative data structure or solution, or can
>> confirm it is safe for the Comparator to call get() method on the map, I
>> would appreciate it.
>
> ?You can tell that it's not safe for the comparator to rely solely on the
> map to locate the correct group, just consider how a new group is added to
> the map.? The comparator needs to be invoked to determine the placement of
> the group within the map, and since it isn't there yet the comparator will
> blow up.
>
> Likewise, consider how a lookup happens.? To find out where in the map to
> look for the group, the map must invoke the comparator.? And the comparator
> tries to get the group from the map to be able to tell it.? Stack overflow.
>
> You could consider a hybrid structure, using both a hash map and a skip
> list.? Use the hash map for lookup by id, and the skip list for iteration in
> the correct order.? The skip list's comparator uses the hash map to pull out
> the objects being compared.? So the "put" implementation for this hybrid map
> first puts into the hash map, then into the skip list.? Because the
> comparator relies only on the hash map, this will be safe.? (Though there is
> a race condition between put and remove, unless you have some extra
> synchronization.? Probably not a factor in practice, but you'd have to prove
> that to yourself.)
>
> Luke
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ValueSortedConcurrentMap.java
Type: text/x-java
Size: 4313 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091012/3663dd58/attachment.bin>

From davidcholmes at aapt.net.au  Tue Oct 13 00:44:55 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 13 Oct 2009 14:44:55 +1000
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910120515x3cf069f2t72869828eb60fa0f@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEELCIDAA.davidcholmes@aapt.net.au>


Taking one last stab at clearing this up ...

Dmitriy V'jukov writes:
> My question basically boils down to: if thread 1 periodically calls
> VolatileValue.getX(), and we know by wall clock that thread 2 executed
> VolatileValue.setX(), let's say, an hour ago, is there any guarantees
> that thread 1 will eventually see 42?

Of course. The first getX() that occurs after setX() will return 42.

> Then, if we will replace volatile by plain var + Fences, what about
> guarantees then?

Assuming the correct Fence method is used - I believe so. The Fences API defines additional happens-before relationships and that will ensure visibility and ordering. 

(How this happens is another matter - one of the contention points with the Fences API is that it isn't always obvious which actions need to be handled specially because of a use of the Fences API elsewhere. And I'm not an expert on the Fences API - personally I think its not a good idea.)

> Happens-before has nothing to do with this. Because it's
> happens-before that will be determined by the synchronizes-with
> relation, and not vise versa. Coherence is a kind of primary thing
> here, and ordering is secondary, sort of.

I do not understand why you keep saying things like this - obviously there is some fundamental disconnect here (and I'm not sure what you are meaning by coherence in this context).

The happens-before ordering is defined by JLS 17.4.5. If there is a happens-before relationship between a write of a variable and a subsequent read of that variable, then the read must return the value written. A happens-before relationship can come about in a number of ways including:
a) the variable concerned is volatile
b) the write and read occur within synchronized regions of code that lock the same monitor

and now,

c) the Fences API has been correctly used to introduce a happens-before relationship.

The implementation is responsible for ensuring the memory model is obeyed. So for example, a simple implementation could always write a volatile variable to memory and add whatever platform specific memory synchronization instruction is needed. A hypothetical, sophisticated/complex implementation might only write variables into thread-local locations initially and upon a read of the variable deduce if a happens-before ordering exists that requires that the variable's value be retrieved from one of those thread-local locations.

> Thank you. I will ask my question there. I was just not aware of that
> mailing, and seen here some discussions of Fences API.

Sure. Discussing Fences API on either list is fine - but if there are issues to reconcile concerning the JMM fundamentals it's probably best to do that on the JMM list first.

Cheers,
David



From dvyukov at gmail.com  Tue Oct 13 03:47:57 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Tue, 13 Oct 2009 11:47:57 +0400
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEELCIDAA.davidcholmes@aapt.net.au>
References: <9014da950910120515x3cf069f2t72869828eb60fa0f@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCEELCIDAA.davidcholmes@aapt.net.au>
Message-ID: <9014da950910130047j3a465bc8wab390ce378d8c34a@mail.gmail.com>

On Tue, Oct 13, 2009 at 8:44 AM, David Holmes <davidcholmes at aapt.net.au> wrote:
>
> Taking one last stab at clearing this up ...
>
> Dmitriy V'jukov writes:
>> My question basically boils down to: if thread 1 periodically calls
>> VolatileValue.getX(), and we know by wall clock that thread 2 executed
>> VolatileValue.setX(), let's say, an hour ago, is there any guarantees
>> that thread 1 will eventually see 42?
>
> Of course. The first getX() that occurs after setX() will return 42.


Definitely. I agree with you. But the problem that "getX() that occurs
after setX()" may never occur. It's not wall-clock time, right?
"After" is defined by "synchronization-order", and
synchronization-order does not care about wall-clock time. So where
are any guarantees that "getX() that occurs after setX()" will ever
occur?


>> Then, if we will replace volatile by plain var + Fences, what about
>> guarantees then?
>
> Assuming the correct Fence method is used - I believe so. The Fences API defines additional happens-before relationships and that will ensure visibility and ordering.

Happens-before is not about visibility in any way, shape or form. It
basically guarantees that if/when you've seen X, then you will see
everything that happened before X. It's just ordering. It does not
guarantee that you will see X itself, and when you will see X.
And what I am talking about is that you may not see X. Ever. So the
guarantee that "if you will see X, then..." totally does not matter,
because it's not guaranteed that you will see X itself.



>
> (How this happens is another matter - one of the contention points with the Fences API is that it isn't always obvious which actions need to be handled specially because of a use of the Fences API elsewhere. And I'm not an expert on the Fences API - personally I think its not a good idea.)
>
>> Happens-before has nothing to do with this. Because it's
>> happens-before that will be determined by the synchronizes-with
>> relation, and not vise versa. Coherence is a kind of primary thing
>> here, and ordering is secondary, sort of.
>
> I do not understand why you keep saying things like this - obviously there is some fundamental disconnect here (and I'm not sure what you are meaning by coherence in this context).

Propagation of changes. Like in "cache coherent system".
If we have variable, and one thread writes to it, the changes have to
propagate to other threads.


> The happens-before ordering is defined by JLS 17.4.5. If there is a happens-before relationship between a write of a variable and a subsequent read of that variable, then the read must return the value written.

There is no happens-before relation between volatile write and
volatile read in my examples.
If you disagree, please prove opposite - you need to prove that read
occurs after write, after as defined by synchronization order, so you
need to prove that so(read, write).
If the things as you said, the proof must exist, right?

My proof is as follows:
There is no happens-before relation between volatile store and
volatile load. So the load always allowed to return initial value of
the variable.


> A happens-before relationship can come about in a number of ways including:
> a) the variable concerned is volatile
> b) the write and read occur within synchronized regions of code that lock the same monitor
>
> and now,
>
> c) the Fences API has been correctly used to introduce a happens-before relationship.
>
> The implementation is responsible for ensuring the memory model is obeyed. So for example, a simple implementation could always write a volatile variable to memory and add whatever platform specific memory synchronization instruction is needed. A hypothetical, sophisticated/complex implementation might only write variables into thread-local locations initially and upon a read of the variable deduce if a happens-before ordering exists that requires that the variable's value be retrieved from one of those thread-local locations.
>
>> Thank you. I will ask my question there. I was just not aware of that
>> mailing, and seen here some discussions of Fences API.
>
> Sure. Discussing Fences API on either list is fine - but if there are issues to reconcile concerning the JMM fundamentals it's probably best to do that on the JMM list first.

Here is my question on javamemorymodel-discussion list (archives
available only for members):
https://mailman.cs.umd.edu/mailman/private/javamemorymodel-discussion/2009-October/000357.html


-- 
Dmitry Vyukov


From davidcholmes at aapt.net.au  Tue Oct 13 04:09:36 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 13 Oct 2009 18:09:36 +1000
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910130047j3a465bc8wab390ce378d8c34a@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIELEIDAA.davidcholmes@aapt.net.au>


I think this is going nowhere, or in circles ...

> > Dmitriy V'jukov writes:
> >> My question basically boils down to: if thread 1 periodically calls
> >> VolatileValue.getX(), and we know by wall clock that thread 2 executed
> >> VolatileValue.setX(), let's say, an hour ago, is there any guarantees
> >> that thread 1 will eventually see 42?
> >
> > Of course. The first getX() that occurs after setX() will return 42.
> 
> 
> Definitely. I agree with you. But the problem that "getX() that occurs
> after setX()" may never occur. It's not wall-clock time, right?
> "After" is defined by "synchronization-order", and
> synchronization-order does not care about wall-clock time. So where
> are any guarantees that "getX() that occurs after setX()" will ever
> occur?

Are you talking about scheduling now? There may not be a guarantee that the thread calling setX() ever actually gets to do that while the other thread periodically calls getX(). But _if_ it does then the result of the getX() that follows that setX() is guaranteed to return 42. 

> Happens-before is not about visibility in any way, shape or form.

Again I refer you to the definition of the happens-before ordering in JLS 17.4.5:

"Two actions can be ordered by a happens-before relationship. If one action happens-
before another, then the first is visible to and ordered before the second."

See the word "visible" in there?

> It
> basically guarantees that if/when you've seen X, then you will see
> everything that happened before X. It's just ordering. It does not
> guarantee that you will see X itself, and when you will see X.
> And what I am talking about is that you may not see X. Ever. So the
> guarantee that "if you will see X, then..." totally does not matter,
> because it's not guaranteed that you will see X itself.

Incorrect. If a happens-before relationship exists then the memory model defines what values a read is required to return. 

> Propagation of changes. Like in "cache coherent system".
> If we have variable, and one thread writes to it, the changes have to
> propagate to other threads.

But that isn't what the Java Memory Model is trying to define. The old JMM was expressed in terms of working memory and main memory (ie a cache) but that model was inadequate. The new JMM is not expressed in those terms but more abstractly. To quote the JLS again:

"The memory model determines what values can be read at every point in the
program."
 
> > The happens-before ordering is defined by JLS 17.4.5. If there 
> is a happens-before relationship between a write of a variable 
> and a subsequent read of that variable, then the read must return 
> the value written.
> 
> There is no happens-before relation between volatile write and
> volatile read in my examples.
> If you disagree, please prove opposite - you need to prove that read
> occurs after write, after as defined by synchronization order, so you
> need to prove that so(read, write).
> If the things as you said, the proof must exist, right?

Of course there is a happens-before relationship in the example: x is a volatile; getX reads it and setX writes it:

"A write to a volatile variable (?8.3.1.4) v synchronizes-with all subsequent
reads of v by any thread (where subsequent is defined according to the synchronization
order)."

"If an action x synchronizes-with a following action y, then we also have hb(x,
y)."

Hence: there is a happens-before relationship between a write of a volatile and a subsequent read of that volatile.

So when getX is subsequent to setX the happens-before relation requires that the value 42 be returned.

QED.

Note I'm not saying when getX must be subsequent to setX, I'm saying _if it is_ subsequent. That is what the JMM defines.

I don't think there is anything else I say beyond continuing to repeat myself here.

David
-----
 
> My proof is as follows:
> There is no happens-before relation between volatile store and
> volatile load. So the load always allowed to return initial value of
> the variable.
> 
> 
> > A happens-before relationship can come about in a number of 
> ways including:
> > a) the variable concerned is volatile
> > b) the write and read occur within synchronized regions of code 
> that lock the same monitor
> >
> > and now,
> >
> > c) the Fences API has been correctly used to introduce a 
> happens-before relationship.
> >
> > The implementation is responsible for ensuring the memory model 
> is obeyed. So for example, a simple implementation could always 
> write a volatile variable to memory and add whatever platform 
> specific memory synchronization instruction is needed. A 
> hypothetical, sophisticated/complex implementation might only 
> write variables into thread-local locations initially and upon a 
> read of the variable deduce if a happens-before ordering exists 
> that requires that the variable's value be retrieved from one of 
> those thread-local locations.
> >
> >> Thank you. I will ask my question there. I was just not aware of that
> >> mailing, and seen here some discussions of Fences API.
> >
> > Sure. Discussing Fences API on either list is fine - but if 
> there are issues to reconcile concerning the JMM fundamentals 
> it's probably best to do that on the JMM list first.
> 
> Here is my question on javamemorymodel-discussion list (archives
> available only for members):
> https://mailman.cs.umd.edu/mailman/private/javamemorymodel-discuss
ion/2009-October/000357.html


-- 
Dmitry Vyukov



From dvyukov at gmail.com  Tue Oct 13 06:55:41 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Tue, 13 Oct 2009 14:55:41 +0400
Subject: [concurrency-interest] Reordering of plain vars/volatiles and
	benign data races
Message-ID: <9014da950910130355n57dcdc6ao6e23183d670b2347@mail.gmail.com>

On Sun, Oct 11, 2009 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> Dmitriy V'jukov wrote:
>
>> As far as I understand, Java does not provide any visibility/coherency
>> guarantees for accesses to plain (non-volatile) variables. I.e.
>> following code may run forever (right?):
>
> Yes. More generally, programs that include non-volatile,
> non-final, non-fenced (see below) variables accessed
> by multiple threads without locking have poorly defined
> behavior. Modulo a few details surrounding thread
> creation/termination, etc this is the definition of a
> "race condition" for Java. Don't write programs with races.


Btw, Doug,

I have an additional question and it will also provide a context for
the original question regarding visibility/coherence.

I've had a discussion on Russian forum (all in Russian, so links will
not make any sense) regarding possible reorderings of plain
vars/volatiles.
Here is it's essence:
him: Plain store can not sink below volatile store, but may hoist
above volatile store (referring to your "Cookbook for Compiler
Writers").
me: No, you must not write programs with data races, and for data race
free programs JMM guarantees sequential consistency, i.e. not
reorderings ever!
him: But there is a well-known data races in standard String class
regarding hash calculation.
me: Well, language standard does not operate in terms of 'good' and
'bad', it only defines semantics. So if you are 200% sure what you are
doing, and standard guarantees behaviour that you need for your racy
program, then, well, correct behaviour is guaranteed, what else to
add?
me: But then, if we are considering racy programs, you may observe
basically any reordering, not just "plain store may hoist above
volatile store". You even may "observe" (if you will reason from
"thread interleaving" point of view) that plain store sinked below
volatile store.

May you comment on this? Why you say "Don't write programs with
races"? What caveats here (besides extreme complexity)? In the end
"racy string hash calculation" is recognized as legal, right?

After some thinking I added:
If you use plain vars for synchronization, it's unclear for me
regarding coherence/visibility guarantees. Let's assume that ordering
is not required in particular situation. If you will use volatiles for
synchronization, updates will eventually propagate between threads.
But if you use plain vars then, I guess, there are no guarantees that
updates will propagate between threads. So if you "raise a signal" or
"enqueue an item into a queue", other threads may not ever see that.
This significantly reduces usefulness of plain vars for
synchronization.

After that I've thought: and what about Fences API? It provides only
ordering guarantees and uses plain vars, so probably there are no
guarantees of coherence/visibility for Fences API too (and I guess
AtomicXXX.lazySet(), because it's a kind of 'not volatile'). And this
makes them basically useless for synchronization (one may get
deadlocks, or changes may just effectively 'disappear').

Please, make my mind clear on this. I am not throlling.
I am more familiar with C++ memory model, and in C++ following program:

int main()
{
  std::atomic<bool> flag (false);
  std::thread th ([&]()
  {
	flag = true;
  });
  while (flag == false) {}
  th.join();
}

also allowed to deadlock IF one considers only ordering rules
(synchronizaes-with, happens-before relations). But the cornerstone is
the following rule:
"Implementations should make atomic stores visible to atomic loads
within a reasonable amount of time"
If we will take this rule into account, then we may conclude: store to
flag will be visible to load in a reasonable amount of time (note, no
ordering/happens-before involved here), after that reasonable amount
of time main thread will stop spinning, join worker thread, and exit.
QED.

So I am looking for similar rule for Java volatiles.
And then want to make my mind clear regarding Fences API (in what way
coherency guaranteed for Fences API, since rules for volatiles do not
applicable to it).

Thank you.

-- 
Dmitry Vyukov

From dvyukov at gmail.com  Tue Oct 13 06:59:29 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Tue, 13 Oct 2009 14:59:29 +0400
Subject: [concurrency-interest] Reordering of plain vars/volatiles and
	benign data races
In-Reply-To: <9014da950910130355n57dcdc6ao6e23183d670b2347@mail.gmail.com>
References: <9014da950910130355n57dcdc6ao6e23183d670b2347@mail.gmail.com>
Message-ID: <9014da950910130359l23d03359ifeb4a9378a84d7bc@mail.gmail.com>

On Sun, Oct 11, 2009 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> Dmitriy V'jukov wrote:
>
>> As far as I understand, Java does not provide any visibility/coherency
>> guarantees for accesses to plain (non-volatile) variables. I.e.
>> following code may run forever (right?):
>
> Yes. More generally, programs that include non-volatile,
> non-final, non-fenced (see below) variables accessed
> by multiple threads without locking have poorly defined
> behavior. Modulo a few details surrounding thread
> creation/termination, etc this is the definition of a
> "race condition" for Java. Don't write programs with races.


Btw, Doug,

I have an additional question and it will also provide a context for
the original question regarding visibility/coherence.

I've had a discussion on Russian forum (all in Russian, so links will
not make any sense) regarding possible reorderings of plain
vars/volatiles.
Here is it's essence:
him: Plain store can not sink below volatile store, but may hoist
above volatile store (referring to your "Cookbook for Compiler
Writers").
me: No, you must not write programs with data races, and for data race
free programs JMM guarantees sequential consistency, i.e. not
reorderings ever!
him: But there is a well-known data races in standard String class
regarding hash calculation.
me: Well, language standard does not operate in terms of 'good' and
'bad', it only defines semantics. So if you are 200% sure what you are
doing, and standard guarantees behaviour that you need for your racy
program, then, well, correct behaviour is guaranteed, what else to
add?
me: But then, if we are considering racy programs, you may observe
basically any reordering, not just "plain store may hoist above
volatile store". You even may "observe" (if you will reason from
"thread interleaving" point of view) that plain store sinked below
volatile store.

May you comment on this? Why you say "Don't write programs with
races"? What caveats here (besides extreme complexity)? In the end
"racy string hash calculation" is recognized as legal, right?

After some thinking I added:
If you use plain vars for synchronization, it's unclear for me
regarding coherence/visibility guarantees. Let's assume that ordering
is not required in particular situation. If you will use volatiles for
synchronization, updates will eventually propagate between threads.
But if you use plain vars then, I guess, there are no guarantees that
updates will propagate between threads. So if you "raise a signal" or
"enqueue an item into a queue", other threads may not ever see that.
This significantly reduces usefulness of plain vars for
synchronization.

After that I've thought: and what about Fences API? It provides only
ordering guarantees and uses plain vars, so probably there are no
guarantees of coherence/visibility for Fences API too (and I guess
AtomicXXX.lazySet(), because it's a kind of 'not volatile'). And this
makes them basically useless for synchronization (one may get
deadlocks, or changes may just effectively 'disappear').

Please, make my mind clear on this. I am not throlling.
I am more familiar with C++ memory model, and in C++ following program:

int main()
{
?std::atomic<bool> flag (false);
?std::thread th ([&]()
?{
? ? ? ?flag = true;
?});
?while (flag == false) {}
?th.join();
}

also allowed to deadlock IF one considers only ordering rules
(synchronizaes-with, happens-before relations). But the cornerstone is
the following rule:
"Implementations should make atomic stores visible to atomic loads
within a reasonable amount of time"
If we will take this rule into account, then we may conclude: store to
flag will be visible to load in a reasonable amount of time (note, no
ordering/happens-before involved here), after that reasonable amount
of time main thread will stop spinning, join worker thread, and exit.
QED.

So I am looking for similar rule for Java volatiles.
And then want to make my mind clear regarding Fences API (in what way
coherency guaranteed for Fences API, since rules for volatiles do not
applicable to it).

Thank you.

--
Dmitry Vyukov


From dvyukov at gmail.com  Tue Oct 13 08:31:51 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Tue, 13 Oct 2009 16:31:51 +0400
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIELEIDAA.davidcholmes@aapt.net.au>
References: <9014da950910130047j3a465bc8wab390ce378d8c34a@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCIELEIDAA.davidcholmes@aapt.net.au>
Message-ID: <9014da950910130531n393f7cfagc6800244518c2077@mail.gmail.com>

On Tue, Oct 13, 2009 at 12:09 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>
> I think this is going nowhere, or in circles ...

Yes :)
So I hope someone deeply involved in JMM will make things clear...


>
>> > Dmitriy V'jukov writes:
>> >> My question basically boils down to: if thread 1 periodically calls
>> >> VolatileValue.getX(), and we know by wall clock that thread 2 executed
>> >> VolatileValue.setX(), let's say, an hour ago, is there any guarantees
>> >> that thread 1 will eventually see 42?
>> >
>> > Of course. The first getX() that occurs after setX() will return 42.
>>
>>
>> Definitely. I agree with you. But the problem that "getX() that occurs
>> after setX()" may never occur. It's not wall-clock time, right?
>> "After" is defined by "synchronization-order", and
>> synchronization-order does not care about wall-clock time. So where
>> are any guarantees that "getX() that occurs after setX()" will ever
>> occur?
>
> Are you talking about scheduling now? There may not be a guarantee that the thread calling setX() ever actually gets to do that while the other thread periodically calls getX(). But _if_ it does then the result of the getX() that follows that setX() is guaranteed to return 42.

No, I do not mean scheduling, I assume fair scheduler that eventually
schedules ready to run threads.


>> Happens-before is not about visibility in any way, shape or form.
>
> Again I refer you to the definition of the happens-before ordering in JLS 17.4.5:
>
> "Two actions can be ordered by a happens-before relationship. If one action happens-
> before another, then the first is visible to and ordered before the second."
>
> See the word "visible" in there?


Yes, perfectly. But I am trying to get farther "words".
This definition itself does not guarantee visibility of changes to
other threads. The key is the word "if". You know it like "If you have
zillion of $$$, then may not work". Even if the statement is true, it
does not mean that every man on a planet may not work, because the
"if" part is usually not satisfied.
You take as unconditional precondition that there is a happens-before
relation, so "if" part is satisfied, so "then" part guarantees
visibility.
I say that there is no happens-before relations yet, so "if" is not
satisfied, so may not even read "then" part.
In order to establish a happens-before relation volatile read must
return value stored by volatile store. Why volatile load will return
value stored by volatile store? There must be some other reason for
that. You may say that that "other reason" is another happens-before
relation. I will say Ok, and how that other happens-before
established? You may say there is another happens-before relation...
Anyway, there must be some 'top-level' happens-before that is
established by completely different reason.
If you have a chicken, you may get an egg. If you have an egg, you may
get a chicken. But if you have neither chicken nor egg, you have to
get one by other means. For example buy one. And that's what I am
talking about - do Java volatiles allow you to buy your first
happens-before relation? After you have one, yes, you may get another,
but it will be only after.

Let's consider your example:

class VolatileValue {
 static volatile int x = 1;

 static int getX() { return x; }

 static void setX() { x = 42; }
}

As you said, thread 2 may get either 1 or 42. Both variants are
viable. The problem is that thread 2 may get 1 infinite number of
times. Since both variants are viable, implementations may choose
first variant always. Why not? No ordering rules broken here.
>From practical/implementation point of view, implementation may allow
the store to sink below infinite loop, or the load to never re-read
value from main memory. In both cases thread 2 will get 1 always.
And only if there are some additional requirements regarding
coherence/visibility (like "volatile stores must be visible to
volatile loads in a reasonable amount of time", just because they
"must" and not because of some ordering rules, ordering rules are not
applicable to this code, because no happens-before relations here yet,
the first happens-before relation will appear only when/if thread 2
will return 42).


>
>> It
>> basically guarantees that if/when you've seen X, then you will see
>> everything that happened before X. It's just ordering. It does not
>> guarantee that you will see X itself, and when you will see X.
>> And what I am talking about is that you may not see X. Ever. So the
>> guarantee that "if you will see X, then..." totally does not matter,
>> because it's not guaranteed that you will see X itself.
>
> Incorrect. If a happens-before relationship exists then the memory model defines what values a read is required to return.

Arrrrghhhh. Your statement is correct, but it is not applicable,
because "if' part is not satisfied.
Happens-before causes visibility of values.
Visibility of values causes happens-before.
It's cycle. And we are talking about situation when you have no
happens-before relations nor any visibility of values. You can't
cyclically refer to one or another. You have to buy any of them first.
Something must happen "out of the thin air", so to say. Happens-before
relations can't appear out of the thin air. So it's visibility of some
value that must happen out of the thin air (store in one thread must
just become visible to load in another thread). And after that, yes,
happens-before relation will be established, it will provide
visibility of other values, etc.


>> Propagation of changes. Like in "cache coherent system".
>> If we have variable, and one thread writes to it, the changes have to
>> propagate to other threads.
>
> But that isn't what the Java Memory Model is trying to define. The old JMM was expressed in terms of working memory and main memory (ie a cache) but that model was inadequate. The new JMM is not expressed in those terms but more abstractly. To quote the JLS again:
>
> "The memory model determines what values can be read at every point in the
> program."
>
>> > The happens-before ordering is defined by JLS 17.4.5. If there
>> is a happens-before relationship between a write of a variable
>> and a subsequent read of that variable, then the read must return
>> the value written.

It's not sufficient guarantees for the language to be useful. Because
if you have only such guarantees, programs may deadlock, or updates
disappear (actually deferred indefinitely, but it's the same from
practical point of view).
Once again, you example with 1 and 42. Ordering rules state that both
values are viable. Always.
The same situation is with C++ memory model. It also says that both
values are viable. But there are additional guarantees that changes
must propagate in a reasonable [wall-clock] time. So 1 and 42 are
viable only for reasonable time, and then only 42 is viable. So it's
guaranteed that a thread will see a signal in a reasonable time, and
then a program will terminate. And Java program is allowed to deadlock
here.



>> There is no happens-before relation between volatile write and
>> volatile read in my examples.
>> If you disagree, please prove opposite - you need to prove that read
>> occurs after write, after as defined by synchronization order, so you
>> need to prove that so(read, write).
>> If the things as you said, the proof must exist, right?
>
> Of course there is a happens-before relationship in the example: x is a volatile; getX reads it and setX writes it:
>
> "A write to a volatile variable (?8.3.1.4) v synchronizes-with all subsequent
> reads of v by any thread (where subsequent is defined according to the synchronization
> order)."
>
> "If an action x synchronizes-with a following action y, then we also have hb(x,
> y)."
>
> Hence: there is a happens-before relationship between a write of a volatile and a subsequent read of that volatile.
>
> So when getX is subsequent to setX the happens-before relation requires that the value 42 be returned.
>
> QED.
>
> Note I'm not saying when getX must be subsequent to setX, I'm saying _if it is_ subsequent. That is what the JMM defines.


Exactly. Now we are on the same page. If it is subsequent. But JMM
says "It's Ok, if they (all loads in the loop) will never become
subsequent to the store, and the program will deadlock".
The key here is that "subsequent" is defined not by wall-clock time,
but only by ordering rules (neither of them established yet). And by
ordering rules both 1 and 42 are always viable.

Yes, we all hope that after an hour of wall clock time volatile store
must be visible to volatile load. I.e. load must become 'subsequent'
to store.
But as far as I see there is not such guarantees in the standard. So
everything is basically held up by sanity of compiler writers.
As for volatiles I am sure that they have enough sanity to not allow
volatile store to sink below infinite loop. However my initial concern
was regarding Fences API. Since Fences API uses plain variables and
the aspect is underspecified, it may just happen so that different
parts of the system (compiler, libraries, virtual machine) will
combine is such a way that store will actually sink below infinite
loop or load will never re-read value from main memory. And then
'good' program will actually deadlock in real life.
It's what actually happen with C#/CLI:
http://www.bluebytesoftware.com/blog/CommentView,guid,72aaa68e-4cbd-41db-a7ce-ddc64411eafa.aspx
"I followed up with several compiler folks at Microsoft (VC++ and CLR
JIT), and the general consensus was: (1) we should explicitly prevent
this kind of reordering from happening for volatiles; (2) it is
insufficiently specified that, yes, it could happen in real life; and,
(3) we should explicitly allow this kind of reordering for
non-volatiles."

--
Dmitry Vyukov


From dvyukov at gmail.com  Tue Oct 13 10:00:49 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Tue, 13 Oct 2009 18:00:49 +0400
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences API
	and visibility/coherency
In-Reply-To: <8b82f1db0910130631p5f09902bt904dca15c6d46a68@mail.gmail.com>
References: <9014da950910120515x3cf069f2t72869828eb60fa0f@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCEELCIDAA.davidcholmes@aapt.net.au>
	<9014da950910130047j3a465bc8wab390ce378d8c34a@mail.gmail.com> 
	<8b82f1db0910130631p5f09902bt904dca15c6d46a68@mail.gmail.com>
Message-ID: <9014da950910130700p6ad1a7ccxfe34583110233d84@mail.gmail.com>

On Tue, Oct 13, 2009 at 5:31 PM, Adam Morrison <adamx at tau.ac.il> wrote:

>> Definitely. I agree with you. But the problem that "getX() that occurs
>> after setX()" may never occur. It's not wall-clock time, right?
>> "After" is defined by "synchronization-order", and
>> synchronization-order does not care about wall-clock time. So where
>> are any guarantees that "getX() that occurs after setX()" will ever
>> occur?
>
> You are worried about an infinite execution where for every getX(),
> so(getX, setX). ?Such an execution is not allowed by JLS 17.4.8.
> There must be a point where setX() is committed, so only a finite
> number of getX() preceed it in the synchronization order.

Hi Adam,

Thank you, at first glance it looks like what I am looking for.
However I have to re-read that piece a dozen of times or so in order
to understand it fully. I would appreciate if more people confirm or
deny as to whether it's what I am looking for or not (I hope that some
people get what I am looking for).

First concerns:
It's yet unclear as to whether the execution is terminating or not
(that's what we are trying to prove/disprove). So as a next committing
actions we may always choose getX. And then there will be no
contradiction, and we will conclude that it's a correct
NON-terminating program.

It's unclear as to whether volatile load must return value stored by
volatile load if the store was committed before load. Values returned
by volatile load are determined by synchronization-order, and not
committing order.

-- 
Dmitry Vyukov


From gregg at cytetech.com  Tue Oct 13 11:55:09 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 13 Oct 2009 10:55:09 -0500
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910130531n393f7cfagc6800244518c2077@mail.gmail.com>
References: <9014da950910130047j3a465bc8wab390ce378d8c34a@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIELEIDAA.davidcholmes@aapt.net.au>
	<9014da950910130531n393f7cfagc6800244518c2077@mail.gmail.com>
Message-ID: <4AD4A2DD.6050308@cytetech.com>

Dmitriy V'jukov wrote:
> On Tue, Oct 13, 2009 at 12:09 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>>>> Dmitriy V'jukov writes:
>>> Happens-before is not about visibility in any way, shape or form.
>> Again I refer you to the definition of the happens-before ordering in JLS 17.4.5:
>>
>> "Two actions can be ordered by a happens-before relationship. If one action happens-
>> before another, then the first is visible to and ordered before the second."
>>
>> See the word "visible" in there?
> 
> 
> Yes, perfectly. But I am trying to get farther "words".
> This definition itself does not guarantee visibility of changes to
> other threads. The key is the word "if". You know it like "If you have
> zillion of $$$, then may not work". Even if the statement is true, it
> does not mean that every man on a planet may not work, because the
> "if" part is usually not satisfied.

I think you are just not invoking useful thought here Dmitriy.  Just consider 
the example.

class VolatileValue {
  static volatile int x = 1;

  static int getX() { return x; }

  static void setX() { x = 42; }
}

If one thread does

VolatileValue a = new VolatileValue();
System.out.println( a.getX()+"" );
a.setX();
System.out.println( a.getX()+"" );

Would you not assert that the output would be the following?

1
42

Okay, lets look at why the output is 1 followed by 42.  Is it because getX() was 
called once before setX()?  No, the call to getX() before call to setX() has no 
effect on what setX() does, nor on what any future call of getX().

The setX() call then happens and the value 42 is written to x.  This is a 
happens before edge.  The following call of getX() now returns 42.

In this example, if x was not volatile, we'd get the same results.

Now if we instead write code that does the following:

final VolatileValue a = new VolatileValue();

new Thread( "Reader" ) {
	public void run() {
		while( true )
			System.out.println(a.getX());
	}
}.start();

new Thread( "Writer" ) {
	public void run() {
		a.setX();
	}
}.start();

Then what is the output of this code?  If the "Writer" thread is ever allowed to 
run, than the "moment" after setX() is called, the "Reader" thread's following 
getX() call will return 42 from that point forward.  This is a "wall clock" 
happens before because there is a moment that it happens.  But we don't talk 
about the "moment" because the  "moment" didn't make the change in value.  What 
made the change in value to x was the write to a volatile value which caused the 
JMM to have to "expose" that write to all subsequent reads.

At some point, there would be a "write" that happens before another "read".  It 
is that "event" that the JMM speaks of and is what guarantees the "JMM" part 
happens.

The scheduling of the OS, processing of exceptions, correctness of logic and all 
the other things on the side, are not part of this issue.

IFF setX() ever gets called, then after that "event" (which has a moment in 
time, but that time is not the "event"), all subsequent calls to getX() will 
return 42.

Gregg Wonderly

From joe.bowbeer at gmail.com  Tue Oct 13 13:53:45 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 13 Oct 2009 10:53:45 -0700
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910120346u173c36cdq8133e44ed808c060@mail.gmail.com>
References: <9014da950910100717x4a33anb8068d73b1791db6@mail.gmail.com>
	<4AD1BFCB.8010304@cs.oswego.edu>
	<9014da950910110620j12759b3gc90e24b0d68ec33f@mail.gmail.com>
	<4AD1E0CE.5020700@cs.oswego.edu>
	<9014da950910110802t5152fd28r7a2cee26763dd2f@mail.gmail.com>
	<4AD24459.7060008@cs.oswego.edu>
	<9014da950910120346u173c36cdq8133e44ed808c060@mail.gmail.com>
Message-ID: <31f2a7bd0910131053j362e5085h10c1ba9c3e286837@mail.gmail.com>

Was this question below about inlining and Fences addressed?

I'm assuming the answer is "no: it would not deadlock" -- because the
compiler will recognize Fences as akin to volatile, and as with volatile,
the compiler will not allow them to cross-paths during optimization.

I am enjoying this discussion.  There's a nebulous (and expanding) region
where optimizing compilers and relaxed JVM specifications compete.  If one
needs to impose more order on an execution, latches and phasers and even
custom thread schedulers are needed.  Though many Java programmers forget
this from time to time, underestimate the optimizer, and rely too naively on
the good intentions of the JVM.

Joe

On Mon, Oct 12, 2009 at 3:46 AM, Dmitriy V'jukov wrote:

>
> [...] Now as for practical aspect.
> I believe that the things is not as you said, and every sane language
> implementation provides inherent coherency guarantees for volatiles.
> I.e. they becomes visible to other threads in reasonable amount of
> time on their own. But it's not the case for plain variables. And the
> concern is with Fences API because it uses plain variables. So it's
> not clear as to whether 'good' program based on Fences allowed to
> deadlock or not. For example:
>
> class SingleProducerSingleConsumerQueue; // based on plain vars and Fences
> API
>
> SingleProducerSingleConsumerQueue queue1;
> SingleProducerSingleConsumerQueue queue2;
>
> // thread 1:
> queue1.push(new Object());
> while (queue2.pop() == null) {}
>
> // thread 2:
> queue2.push(new Object());
> while (queue1.pop() == null) {}
>
> If methods will be inlined, push to queue may sink below pop, and the
> program will deadlock. I do not see anything in the specification that
> prevents that.
>
> --
> Dmitry Vyukov
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091013/dda4e86d/attachment.html>

From dvyukov at gmail.com  Tue Oct 13 16:34:17 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Wed, 14 Oct 2009 00:34:17 +0400
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences API
	and visibility/coherency
In-Reply-To: <8b82f1db0910130939t12541e7dh5221d29197b27f53@mail.gmail.com>
References: <9014da950910120515x3cf069f2t72869828eb60fa0f@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCEELCIDAA.davidcholmes@aapt.net.au>
	<9014da950910130047j3a465bc8wab390ce378d8c34a@mail.gmail.com> 
	<8b82f1db0910130631p5f09902bt904dca15c6d46a68@mail.gmail.com> 
	<9014da950910130700p6ad1a7ccxfe34583110233d84@mail.gmail.com> 
	<8b82f1db0910130939t12541e7dh5221d29197b27f53@mail.gmail.com>
Message-ID: <9014da950910131334n1dad4badt3439bd034800b0ed@mail.gmail.com>

On Tue, Oct 13, 2009 at 8:39 PM, Adam Morrison <adamx at tau.ac.il> wrote:

>> First concerns:
>> It's yet unclear as to whether the execution is terminating or not
>> (that's what we are trying to prove/disprove). So as a next committing
>> actions we may always choose getX. And then there will be no
>> contradiction, and we will conclude that it's a correct
>> NON-terminating program.
>
> That won't work because then the union of all committed actions is not
> equal to all the actions in the execution.
>
> But there does seem to be a hole in the JLS. ?It says that the sequence
> of sets of commited actions for an infinite execution "may be infinite".
> So if it's finite, there must exist an infinite set of committed actions, and
> you can commit all the reads plus the write as a maximal element in the
> synchronization order.
>
> However, it looks like the JMM journal paper closes this hole. ?One of the
> conditions for a well-formed execution there is that "the synchronization
> order has an order less than or equal to omega. ?This means that for each
> synchronization action x, only a finite number of synchronization actions
> occur before x in the synchronization order."
>
> Link: ?http://unladen-swallow.googlecode.com/files/journal.pdf


Arrrggghhhhh. Thank you, Adam. This makes sense for me.

"the synchronization order has an order less than or equal to omega"
basically means that every synchronization action must either not
occur at all (situated after 'hang' action in the program order), or
occur in a finite number of steps. This disallows both volatile store
to sink below infinite loop and volatile load to hoist above infinite
loop. Propagation of changes may still take unreasonable but at least
finite amount of time :)
I guess it's a kind of have to be in the specification, but is absent
by some reason. It's sad.

Further proof is trivial:
"A write to a volatile variable v synchronizes-with all subsequent
reads of v by any thread (where subsequent is defined according to the
synchronization order)"

(side thought: JLS explicitly states that it does not require
underlying scheduler to be fair. But the above requirement is a kind
requires fair scheduler... at least for terminating threads. Because
every terminating thread contains final action, that have to occur in
a finite number of steps. So each other action of such thread has to
occur in a finite number of steps too)

As for JLS 17.4.8. As far as I get form "Discussion" section it's all
about prohibiting "out of the thin air" values. As you correctly
noted, there is no requirement for infinite A to be countable. So
volatile write may be committed after infinite number of volatile
loads.

Ok, now we may back to Fences API.
As far as I see, only Fences.orderAccesses() is a synchronization
action. And orderWrites() and orderReads are not.
I guess that the following:
X = Fences.orderWrites(O);
is equivalent to:
release_fence();
X = O;
Note that X is a plain var here.

So for the following code:

class SingleProducerSingleConsumerQueue; // based on plain vars and
Fences.orderWrites/orderReads

SingleProducerSingleConsumerQueue queue1;
SingleProducerSingleConsumerQueue queue2;

// thread 1:
queue1.push(new Object());
while (queue2.pop() == null) {}

// thread 2:
queue2.push(new Object());
while (queue1.pop() == null) {}

It's quite possible that store to X will sink below infinite loop, and
the program will deadlock.
What I am missing now? :)

-- 
Dmitry Vyukov


From davidcholmes at aapt.net.au  Tue Oct 13 19:45:42 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 14 Oct 2009 09:45:42 +1000
Subject: [concurrency-interest] Fences API and visibility/coherency
In-Reply-To: <9014da950910130531n393f7cfagc6800244518c2077@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOELIIDAA.davidcholmes@aapt.net.au>

Dmitriy,

As far as I am aware "subsequent" has its English-language meaning of
"occurring afterwards". It is not a special term in the JLS. The phrase
"(where subsequent is defined according to the synchronization order)"
simply means that if you examine the synchronization order (which is a total
order) than an action B is subsequent to an action A if B occurs after A in
the synchronization order. There is no trickery or subtlety here.

When a write to a volatile variable X occurs in one thread, the value
written does not have to become globally visible - it can be tucked into a
register as I've described before. But the next read of that variable, that
occurs after the write (ie subsequent to it) _must_ return the value that
was written _because_ there is a happens-before ordering between them.

David Holmes

> -----Original Message-----
> From: Dmitriy V'jukov [mailto:dvyukov at gmail.com]
> Sent: Tuesday, 13 October 2009 10:32 PM
> To: dholmes at ieee.org
> Cc: Doug Lea; concurrency-interest at cs.oswego.edu;
> javamemorymodel-discussion at mcfeely.cs.umd.edu
> Subject: Re: [concurrency-interest] Fences API and visibility/coherency
>
>
> On Tue, Oct 13, 2009 at 12:09 PM, David Holmes
> <davidcholmes at aapt.net.au> wrote:
> >
> > I think this is going nowhere, or in circles ...
>
> Yes :)
> So I hope someone deeply involved in JMM will make things clear...
>
>
> >
> >> > Dmitriy V'jukov writes:
> >> >> My question basically boils down to: if thread 1 periodically calls
> >> >> VolatileValue.getX(), and we know by wall clock that thread
> 2 executed
> >> >> VolatileValue.setX(), let's say, an hour ago, is there any
> guarantees
> >> >> that thread 1 will eventually see 42?
> >> >
> >> > Of course. The first getX() that occurs after setX() will return 42.
> >>
> >>
> >> Definitely. I agree with you. But the problem that "getX() that occurs
> >> after setX()" may never occur. It's not wall-clock time, right?
> >> "After" is defined by "synchronization-order", and
> >> synchronization-order does not care about wall-clock time. So where
> >> are any guarantees that "getX() that occurs after setX()" will ever
> >> occur?
> >
> > Are you talking about scheduling now? There may not be a
> guarantee that the thread calling setX() ever actually gets to do
> that while the other thread periodically calls getX(). But _if_
> it does then the result of the getX() that follows that setX() is
> guaranteed to return 42.
>
> No, I do not mean scheduling, I assume fair scheduler that eventually
> schedules ready to run threads.
>
>
> >> Happens-before is not about visibility in any way, shape or form.
> >
> > Again I refer you to the definition of the happens-before
> ordering in JLS 17.4.5:
> >
> > "Two actions can be ordered by a happens-before relationship.
> If one action happens-
> > before another, then the first is visible to and ordered before
> the second."
> >
> > See the word "visible" in there?
>
>
> Yes, perfectly. But I am trying to get farther "words".
> This definition itself does not guarantee visibility of changes to
> other threads. The key is the word "if". You know it like "If you have
> zillion of $$$, then may not work". Even if the statement is true, it
> does not mean that every man on a planet may not work, because the
> "if" part is usually not satisfied.
> You take as unconditional precondition that there is a happens-before
> relation, so "if" part is satisfied, so "then" part guarantees
> visibility.
> I say that there is no happens-before relations yet, so "if" is not
> satisfied, so may not even read "then" part.
> In order to establish a happens-before relation volatile read must
> return value stored by volatile store. Why volatile load will return
> value stored by volatile store? There must be some other reason for
> that. You may say that that "other reason" is another happens-before
> relation. I will say Ok, and how that other happens-before
> established? You may say there is another happens-before relation...
> Anyway, there must be some 'top-level' happens-before that is
> established by completely different reason.
> If you have a chicken, you may get an egg. If you have an egg, you may
> get a chicken. But if you have neither chicken nor egg, you have to
> get one by other means. For example buy one. And that's what I am
> talking about - do Java volatiles allow you to buy your first
> happens-before relation? After you have one, yes, you may get another,
> but it will be only after.
>
> Let's consider your example:
>
> class VolatileValue {
>  static volatile int x = 1;
>
>  static int getX() { return x; }
>
>  static void setX() { x = 42; }
> }
>
> As you said, thread 2 may get either 1 or 42. Both variants are
> viable. The problem is that thread 2 may get 1 infinite number of
> times. Since both variants are viable, implementations may choose
> first variant always. Why not? No ordering rules broken here.
> From practical/implementation point of view, implementation may allow
> the store to sink below infinite loop, or the load to never re-read
> value from main memory. In both cases thread 2 will get 1 always.
> And only if there are some additional requirements regarding
> coherence/visibility (like "volatile stores must be visible to
> volatile loads in a reasonable amount of time", just because they
> "must" and not because of some ordering rules, ordering rules are not
> applicable to this code, because no happens-before relations here yet,
> the first happens-before relation will appear only when/if thread 2
> will return 42).
>
>
> >
> >> It
> >> basically guarantees that if/when you've seen X, then you will see
> >> everything that happened before X. It's just ordering. It does not
> >> guarantee that you will see X itself, and when you will see X.
> >> And what I am talking about is that you may not see X. Ever. So the
> >> guarantee that "if you will see X, then..." totally does not matter,
> >> because it's not guaranteed that you will see X itself.
> >
> > Incorrect. If a happens-before relationship exists then the
> memory model defines what values a read is required to return.
>
> Arrrrghhhh. Your statement is correct, but it is not applicable,
> because "if' part is not satisfied.
> Happens-before causes visibility of values.
> Visibility of values causes happens-before.
> It's cycle. And we are talking about situation when you have no
> happens-before relations nor any visibility of values. You can't
> cyclically refer to one or another. You have to buy any of them first.
> Something must happen "out of the thin air", so to say. Happens-before
> relations can't appear out of the thin air. So it's visibility of some
> value that must happen out of the thin air (store in one thread must
> just become visible to load in another thread). And after that, yes,
> happens-before relation will be established, it will provide
> visibility of other values, etc.
>
>
> >> Propagation of changes. Like in "cache coherent system".
> >> If we have variable, and one thread writes to it, the changes have to
> >> propagate to other threads.
> >
> > But that isn't what the Java Memory Model is trying to define.
> The old JMM was expressed in terms of working memory and main
> memory (ie a cache) but that model was inadequate. The new JMM is
> not expressed in those terms but more abstractly. To quote the JLS again:
> >
> > "The memory model determines what values can be read at every
> point in the
> > program."
> >
> >> > The happens-before ordering is defined by JLS 17.4.5. If there
> >> is a happens-before relationship between a write of a variable
> >> and a subsequent read of that variable, then the read must return
> >> the value written.
>
> It's not sufficient guarantees for the language to be useful. Because
> if you have only such guarantees, programs may deadlock, or updates
> disappear (actually deferred indefinitely, but it's the same from
> practical point of view).
> Once again, you example with 1 and 42. Ordering rules state that both
> values are viable. Always.
> The same situation is with C++ memory model. It also says that both
> values are viable. But there are additional guarantees that changes
> must propagate in a reasonable [wall-clock] time. So 1 and 42 are
> viable only for reasonable time, and then only 42 is viable. So it's
> guaranteed that a thread will see a signal in a reasonable time, and
> then a program will terminate. And Java program is allowed to deadlock
> here.
>
>
>
> >> There is no happens-before relation between volatile write and
> >> volatile read in my examples.
> >> If you disagree, please prove opposite - you need to prove that read
> >> occurs after write, after as defined by synchronization order, so you
> >> need to prove that so(read, write).
> >> If the things as you said, the proof must exist, right?
> >
> > Of course there is a happens-before relationship in the
> example: x is a volatile; getX reads it and setX writes it:
> >
> > "A write to a volatile variable (?8.3.1.4) v synchronizes-with
> all subsequent
> > reads of v by any thread (where subsequent is defined according
> to the synchronization
> > order)."
> >
> > "If an action x synchronizes-with a following action y, then we
> also have hb(x,
> > y)."
> >
> > Hence: there is a happens-before relationship between a write
> of a volatile and a subsequent read of that volatile.
> >
> > So when getX is subsequent to setX the happens-before relation
> requires that the value 42 be returned.
> >
> > QED.
> >
> > Note I'm not saying when getX must be subsequent to setX, I'm
> saying _if it is_ subsequent. That is what the JMM defines.
>
>
> Exactly. Now we are on the same page. If it is subsequent. But JMM
> says "It's Ok, if they (all loads in the loop) will never become
> subsequent to the store, and the program will deadlock".
> The key here is that "subsequent" is defined not by wall-clock time,
> but only by ordering rules (neither of them established yet). And by
> ordering rules both 1 and 42 are always viable.
>
> Yes, we all hope that after an hour of wall clock time volatile store
> must be visible to volatile load. I.e. load must become 'subsequent'
> to store.
> But as far as I see there is not such guarantees in the standard. So
> everything is basically held up by sanity of compiler writers.
> As for volatiles I am sure that they have enough sanity to not allow
> volatile store to sink below infinite loop. However my initial concern
> was regarding Fences API. Since Fences API uses plain variables and
> the aspect is underspecified, it may just happen so that different
> parts of the system (compiler, libraries, virtual machine) will
> combine is such a way that store will actually sink below infinite
> loop or load will never re-read value from main memory. And then
> 'good' program will actually deadlock in real life.
> It's what actually happen with C#/CLI:
> http://www.bluebytesoftware.com/blog/CommentView,guid,72aaa68e-4cb
d-41db-a7ce-ddc64411eafa.aspx
"I followed up with several compiler folks at Microsoft (VC++ and CLR
JIT), and the general consensus was: (1) we should explicitly prevent
this kind of reordering from happening for volatiles; (2) it is
insufficiently specified that, yes, it could happen in real life; and,
(3) we should explicitly allow this kind of reordering for
non-volatiles."

--
Dmitry Vyukov



From davidcholmes at aapt.net.au  Tue Oct 13 20:00:16 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 14 Oct 2009 10:00:16 +1000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences API
	and visibility/coherency
In-Reply-To: <8b82f1db0910130631p5f09902bt904dca15c6d46a68@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIELJIDAA.davidcholmes@aapt.net.au>

Adam Morrison writes:
> On Tue, Oct 13, 2009 at 9:47 AM, Dmitriy V'jukov
> <dvyukov at gmail.com> wrote:
> >>> My question basically boils down to: if thread 1 periodically calls
> >>> VolatileValue.getX(), and we know by wall clock that thread 2 executed
> >>> VolatileValue.setX(), let's say, an hour ago, is there any guarantees
> >>> that thread 1 will eventually see 42?
> >>
> >> Of course. The first getX() that occurs after setX() will return 42.
> >
> >
> > Definitely. I agree with you. But the problem that "getX() that occurs
> > after setX()" may never occur. It's not wall-clock time, right?
> > "After" is defined by "synchronization-order", and
> > synchronization-order does not care about wall-clock time. So where
> > are any guarantees that "getX() that occurs after setX()" will ever
> > occur?
>
> You are worried about an infinite execution where for every getX(),
> so(getX, setX).  Such an execution is not allowed by JLS 17.4.8.
> There must be a point where setX() is committed, so only a finite
> number of getX() preceed it in the synchronization order.

I don't see where finite or infinite executions come into this - unless
we're just having some weird philosophical abstract discussion, in which
case count me out.

I'm assuming we have a VM that obeys the JVMS and executes the Java bytecode
as required for each thread in the application. If we've already dismissed
scheduling guarantees ie we take it as given that each thread will get a
chance to execute, then setX will execute and there will then be a getX that
is subsequent to that. End of story.

David


From davidcholmes at aapt.net.au  Wed Oct 14 05:48:12 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 14 Oct 2009 19:48:12 +1000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences API
	and visibility/coherency
In-Reply-To: <8b82f1db0910140221o1ee0b371l97a2e48146417fa@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCELOIDAA.davidcholmes@aapt.net.au>


Hi Adam,

> > I'm assuming we have a VM that obeys the JVMS and executes the
> > Java bytecode as required for each thread in the application. If
> > we've already dismissed scheduling guarantees ie we take it as
> > given that each thread will get a chance to execute, then setX
> > will execute and there will then be a getX that
> > is subsequent to that. End of story.
>
> Essentially the question was about a compiler that would inline a
> loop of getX() and then hoist the volatile load out of the loop,
> causing the program to deadlock.
> This would result in an infinite execution like the one we discussed.
>
> Strictly speaking, the JLS doesn't have any language forbidding such a
> transformation, but the text in the JMM paper makes such a transformation
> illegal.  For some reason, though, the text didn't make it into the JLS.

Hmmm I didn't extract this issue from the original postings and I'm somewhat
surprised that this is indeed the issue. The hoisting of the volatile out of
the loop must be prohibited or else it completely renders volatile useless -
as per the April 2004 discussion you referenced. If, after that, this was
somehow omitted from the spec then that is a major stuff up in my view. As I
stated back then I've never agreed with the principle of allowing the
compiler to do anything it can get away with as it requires you to enumerate
every bad transformation and that is impractical if not impossible. <sigh>

Thanks for clarifying that.

David Holmes



From dvyukov at gmail.com  Wed Oct 14 06:44:03 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Wed, 14 Oct 2009 14:44:03 +0400
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences API
	and visibility/coherency
In-Reply-To: <8b82f1db0910140221o1ee0b371l97a2e48146417fa@mail.gmail.com>
References: <8b82f1db0910130631p5f09902bt904dca15c6d46a68@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCIELJIDAA.davidcholmes@aapt.net.au>
	<8b82f1db0910140221o1ee0b371l97a2e48146417fa@mail.gmail.com>
Message-ID: <9014da950910140344j730b2f85xb1f6b34b0e37fff8@mail.gmail.com>

On Wed, Oct 14, 2009 at 1:21 PM, Adam Morrison <adamx at tau.ac.il> wrote:
>>> > synchronization-order does not care about wall-clock time. So where
>>> > are any guarantees that "getX() that occurs after setX()" will ever
>>> > occur?
>>>
>>> You are worried about an infinite execution where for every getX(),
>>> so(getX, setX). ?Such an execution is not allowed by JLS 17.4.8.
>>> There must be a point where setX() is committed, so only a finite
>>> number of getX() preceed it in the synchronization order.
>>
>> I don't see where finite or infinite executions come into this - unless
>> we're just having some weird philosophical abstract discussion, in which
>> case count me out.
>>
>> I'm assuming we have a VM that obeys the JVMS and executes the Java bytecode
>> as required for each thread in the application. If we've already dismissed
>> scheduling guarantees ie we take it as given that each thread will get a
>> chance to execute, then setX will execute and there will then be a getX that
>> is subsequent to that. End of story.
>
> Essentially the question was about a compiler that would inline a loop of getX()
> and then hoist the volatile load out of the loop, causing the program
> to deadlock.
> This would result in an infinite execution like the one we discussed.
>
> Strictly speaking, the JLS doesn't have any language forbidding such a
> transformation, but the text in the JMM paper makes such a transformation
> illegal. ?For some reason, though, the text didn't make it into the JLS.
>
> BTW, this problem was exactly the motivation for adding this requirement:
>
> ? ? ?http://www.cs.umd.edu/~pugh/java/memoryModel/archive/2150.html
>

Ah, Adam, you are a sight for my sore eyes. I thought my brain is too
damaged by all that stuff.
So now Bill Pugh is on my side too :)
Bill was saying exactly the same things "hoisting the volatile read
out of the loop". And it's only "order type omega" requirement for
synchronization-order and/or committing order that prevents that. This
requirement is not in the specification, though.

I just want to remind of potential problem with Fences API one more time.

-- 
Dmitry Vyukov


From hans.boehm at hp.com  Wed Oct 14 20:37:13 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 15 Oct 2009 00:37:13 +0000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
	API	and visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCELOIDAA.davidcholmes@aapt.net.au>
References: <8b82f1db0910140221o1ee0b371l97a2e48146417fa@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCELOIDAA.davidcholmes@aapt.net.au>
Message-ID: <238A96A773B3934685A7269CC8A8D042577D607AF1@GVW0436EXB.americas.hpqcorp.net>

[Some of these postings, especially from Dmitriy arrived here garbled, at least when viewed by Outlook.]

> From: David Holmes
> Sent: Wednesday, October 14, 2009 2:48 AM
> 
> 
> Hi Adam,
> 
> > > I'm assuming we have a VM that obeys the JVMS and 
> executes the Java 
> > > bytecode as required for each thread in the application. If we've 
> > > already dismissed scheduling guarantees ie we take it as 
> given that 
> > > each thread will get a chance to execute, then setX will 
> execute and 
> > > there will then be a getX that is subsequent to that. End 
> of story.
> >
> > Essentially the question was about a compiler that would 
> inline a loop 
> > of getX() and then hoist the volatile load out of the loop, causing 
> > the program to deadlock.
> > This would result in an infinite execution like the one we 
> discussed.
> >
> > Strictly speaking, the JLS doesn't have any language 
> forbidding such a 
> > transformation, but the text in the JMM paper makes such a 
> > transformation illegal.  For some reason, though, the text 
> didn't make it into the JLS.
> 
> Hmmm I didn't extract this issue from the original postings 
> and I'm somewhat surprised that this is indeed the issue. The 
> hoisting of the volatile out of the loop must be prohibited 
> or else it completely renders volatile useless - as per the 
> April 2004 discussion you referenced. If, after that, this 
> was somehow omitted from the spec then that is a major stuff 
> up in my view. As I stated back then I've never agreed with 
> the principle of allowing the compiler to do anything it can 
> get away with as it requires you to enumerate every bad 
> transformation and that is impractical if not impossible. <sigh>
> 
> Thanks for clarifying that.
> 
> David Holmes
> 
My recollection is that there was a very explicit decision during jmm discussions not to promise fairness, since we wanted a non-preemptive scheduler to be conforming.  This inherently means that if one thread is looping on a volatile (while (v == 0){}) while another sets it (v = 1), and no other actions are performed by the program (the original examples in this thread), the program should not be required to terminate.  And it isn't required to do so.  And in some VMs it probably won't.  If the second thread is observed to terminate (e.g. by visibly printing something after setting v to 1) then, I believe the intent of 17.4.9 was to require that the looping thread also terminate.

Note that this is also somewhat weasel-worded in the C++0x draft, since there is a difference between "should" and "shall".  The exact phrasing there is still under discussion.

Hans

From davidcholmes at aapt.net.au  Wed Oct 14 20:47:04 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 15 Oct 2009 10:47:04 +1000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
	APIand visibility/coherency
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577D607AF1@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEMAIDAA.davidcholmes@aapt.net.au>

Hi Hans,

> [Some of these postings, especially from Dmitriy arrived here
> garbled, at least when viewed by Outlook.]

Yes - faulty encoding it seems, but they are ok in the archive.

> My recollection is that there was a very explicit decision during
> jmm discussions not to promise fairness, since we wanted a
> non-preemptive scheduler to be conforming.  This inherently means
> that if one thread is looping on a volatile (while (v == 0){})
> while another sets it (v = 1), and no other actions are performed
> by the program (the original examples in this thread), the
> program should not be required to terminate.  And it isn't
> required to do so.

Yes _but_ it was decided (according to the archive) that it was ok for this
to occur due to unspecified scheduling behaviour, but it was _not_ ok for it
to happen because the volatile read got hoisted out of the loop. That was
the email thread Adam referenced:

      http://www.cs.umd.edu/~pugh/java/memoryModel/archive/2150.html

> And in some VMs it probably won't.  If the
> second thread is observed to terminate (e.g. by visibly printing
> something after setting v to 1) then, I believe the intent of
> 17.4.9 was to require that the looping thread also terminate.

It shouldn't matter if the other thread does something visible, if it
terminates then the write did occur and so a read should see it. Moving the
read out of the loop is simply invalid.

If the JMM allows this kind of "optimization" to remove happens-before edges
then it seems to me that the JMM is fundamentally broken, because any
argument as to allowed behaviour has to be suffixed with "assuming the
compiler didn't reorder things". And how do we then establish what the
compiler is and is not allowed to do?

David

> Note that this is also somewhat weasel-worded in the C++0x draft,
> since there is a difference between "should" and "shall".  The
> exact phrasing there is still under discussion.
>
> Hans=


From hans.boehm at hp.com  Wed Oct 14 21:31:08 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 15 Oct 2009 01:31:08 +0000
Subject: [concurrency-interest] Reordering of plain vars/volatiles
	and	benign data races
In-Reply-To: <9014da950910130355n57dcdc6ao6e23183d670b2347@mail.gmail.com>
References: <9014da950910130355n57dcdc6ao6e23183d670b2347@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042577D607B17@GVW0436EXB.americas.hpqcorp.net>

 

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Dmitriy V'jukov
> Sent: Tuesday, October 13, 2009 3:56 AM
> To: Doug Lea
> Cc: concurrency-interest at cs.oswego.edu; dholmes at ieee.org; 
> JavaMemoryModel at cs.umd.edu
> Subject: [concurrency-interest] Reordering of plain 
> vars/volatiles and benign data races
> 
> On Sun, Oct 11, 2009 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> > Dmitriy V'jukov wrote:
> >
> >> As far as I understand, Java does not provide any 
> >> visibility/coherency guarantees for accesses to plain 
> (non-volatile) variables. I.e.
> >> following code may run forever (right?):
> >
> > Yes. More generally, programs that include non-volatile, non-final, 
> > non-fenced (see below) variables accessed by multiple 
> threads without 
> > locking have poorly defined behavior. Modulo a few details 
> surrounding 
> > thread creation/termination, etc this is the definition of a "race 
> > condition" for Java. Don't write programs with races.
> 
> 
> Btw, Doug,
> 
> I have an additional question and it will also provide a 
> context for the original question regarding visibility/coherence.
> 
> I've had a discussion on Russian forum (all in Russian, so 
> links will not make any sense) regarding possible reorderings 
> of plain vars/volatiles.
> Here is it's essence:
> him: Plain store can not sink below volatile store, but may 
> hoist above volatile store (referring to your "Cookbook for 
> Compiler Writers").
> me: No, you must not write programs with data races, and for 
> data race free programs JMM guarantees sequential 
> consistency, i.e. not reorderings ever!
> him: But there is a well-known data races in standard String 
> class regarding hash calculation.
> me: Well, language standard does not operate in terms of 
> 'good' and 'bad', it only defines semantics. So if you are 
> 200% sure what you are doing, and standard guarantees 
> behaviour that you need for your racy program, then, well, 
> correct behaviour is guaranteed, what else to add?
> me: But then, if we are considering racy programs, you may 
> observe basically any reordering, not just "plain store may 
> hoist above volatile store". You even may "observe" (if you 
> will reason from "thread interleaving" point of view) that 
> plain store sinked below volatile store.
> 
> May you comment on this? Why you say "Don't write programs 
> with races"? What caveats here (besides extreme complexity)? 
> In the end "racy string hash calculation" is recognized as 
> legal, right?
> 
> After some thinking I added:
> If you use plain vars for synchronization, it's unclear for 
> me regarding coherence/visibility guarantees. Let's assume 
> that ordering is not required in particular situation. If you 
> will use volatiles for synchronization, updates will 
> eventually propagate between threads.
> But if you use plain vars then, I guess, there are no 
> guarantees that updates will propagate between threads. So if 
> you "raise a signal" or "enqueue an item into a queue", other 
> threads may not ever see that.
> This significantly reduces usefulness of plain vars for 
> synchronization.
> 
> After that I've thought: and what about Fences API? It 
> provides only ordering guarantees and uses plain vars, so 
> probably there are no guarantees of coherence/visibility for 
> Fences API too (and I guess AtomicXXX.lazySet(), because it's 
> a kind of 'not volatile'). And this makes them basically 
> useless for synchronization (one may get deadlocks, or 
> changes may just effectively 'disappear').
> 
> Please, make my mind clear on this. I am not throlling.
> I am more familiar with C++ memory model, and in C++ 
> following program:
> 
> int main()
> {
>   std::atomic<bool> flag (false);
>   std::thread th ([&]()
>   {
> 	flag = true;
>   });
>   while (flag == false) {}
>   th.join();
> }
> 
> also allowed to deadlock IF one considers only ordering rules 
> (synchronizaes-with, happens-before relations). But the 
> cornerstone is the following rule:
> "Implementations should make atomic stores visible to atomic 
> loads within a reasonable amount of time"
> If we will take this rule into account, then we may conclude: 
> store to flag will be visible to load in a reasonable amount 
> of time (note, no ordering/happens-before involved here), 
> after that reasonable amount of time main thread will stop 
> spinning, join worker thread, and exit.
> QED.
> 
> So I am looking for similar rule for Java volatiles.
> And then want to make my mind clear regarding Fences API (in 
> what way coherency guaranteed for Fences API, since rules for 
> volatiles do not applicable to it).
> 
> Thank you.
> 
> --
> Dmitry Vyukov

I'm not Doug, but ...

The Java memory model, unlike C++, does allow some races on ordinary variables.  It has to, since malicious sandboxed code may introduce races.  The problems with this are currently:

- Extreme complexity, both in writing the code, and in understanding the spec.

- People have recently discovered that the spec has some technical problems.  (See Sevcik and Aspinall, ECOOP 2008.)  It unexpectedly forbids some common optimizations.  (I.e. after you understand the complex spec, don't trust it 100% :-).  Fortunately, you are unlikely to encounter the problems.)  It is well understood that this is not good.  But there seem to be no painless and complete solutions.  Some of us are working on rather drastic ones.

The JMM guarantees sequential consistency for data-race-free programs.  This does not prohibt ordinary accesses from getting hoisted above volatile stores, for example.  A race-free program can't tell the difference.  This fact is important to get good performance.

If you do program with data races in Java the more complex rules you will have to deal with continue to allow hoisting above volatile stores.  The results no longer look anything like sequential consistency.

(Java ordinary variables behave a bit like memory_order_relaxed atomics in C++, only with even weaker properties, e.g. accesses to a single location may appear to be reordered, and long and double accesses are not indivisble.)

Hans

Hans

From hans.boehm at hp.com  Wed Oct 14 21:33:50 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 15 Oct 2009 01:33:50 +0000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
 APIand visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEMAIDAA.davidcholmes@aapt.net.au>
References: <238A96A773B3934685A7269CC8A8D042577D607AF1@GVW0436EXB.americas.hpqcorp.net>
	<NFBBKALFDCPFIDBNKAPCEEMAIDAA.davidcholmes@aapt.net.au>
Message-ID: <238A96A773B3934685A7269CC8A8D042577D607B19@GVW0436EXB.americas.hpqcorp.net>

 

> -----Original Message-----
> From: David Holmes [mailto:davidcholmes at aapt.net.au] 
> Sent: Wednesday, October 14, 2009 5:47 PM
> To: Boehm, Hans
> Cc: concurrency-interest at cs.oswego.edu; 
> javamemorymodel-discussion at cs.umd.edu
> Subject: RE: [Javamemorymodel-discussion] 
> [concurrency-interest] Fences APIand visibility/coherency
> 
> Hi Hans,
> 
> > [Some of these postings, especially from Dmitriy arrived 
> here garbled, 
> > at least when viewed by Outlook.]
> 
> Yes - faulty encoding it seems, but they are ok in the archive.
> 
> > My recollection is that there was a very explicit decision 
> during jmm 
> > discussions not to promise fairness, since we wanted a 
> non-preemptive 
> > scheduler to be conforming.  This inherently means that if 
> one thread 
> > is looping on a volatile (while (v == 0){}) while another 
> sets it (v = 
> > 1), and no other actions are performed by the program (the original 
> > examples in this thread), the program should not be required to 
> > terminate.  And it isn't required to do so.
> 
> Yes _but_ it was decided (according to the archive) that it 
> was ok for this to occur due to unspecified scheduling 
> behaviour, but it was _not_ ok for it to happen because the 
> volatile read got hoisted out of the loop. That was the email 
> thread Adam referenced:
> 
>       http://www.cs.umd.edu/~pugh/java/memoryModel/archive/2150.html
> 
> > And in some VMs it probably won't.  If the second thread is 
> observed 
> > to terminate (e.g. by visibly printing something after 
> setting v to 1) 
> > then, I believe the intent of
> > 17.4.9 was to require that the looping thread also terminate.
> 
> It shouldn't matter if the other thread does something 
> visible, if it terminates then the write did occur and so a 
> read should see it. Moving the read out of the loop is simply invalid.
> 
> If the JMM allows this kind of "optimization" to remove 
> happens-before edges then it seems to me that the JMM is 
> fundamentally broken, because any argument as to allowed 
> behaviour has to be suffixed with "assuming the compiler 
> didn't reorder things". And how do we then establish what the 
> compiler is and is not allowed to do?
It's not removing happens-before edges.  In the example in which the two threads do nothing else, a nonterminating result is 100% consistent with the execution in which only the looping thread ever runs and the assigning thread never makes progress.  This is possible with no optimization, it's purely a function of the scheduler.  Since that execution is allowed, the program is allowed not to terminate.  If a very clever compiler got to look at the whole program, it might be able to move the volatile out of the loop IN THIS SPECIFIC CASE, since the resulting execution has the same observable behavior as one in which no such transformation were made.  It would be cheating, but nobody could tell the difference.  In general it can't do so (except possibly by also conspiring with the scheduler) because some other thread might set the variable and then do something observable.  In any case, I know of no optimizer that comes close to trying to do this.

I think this is completely consistent with the message you cite

I do personally think that a general purpose VM that exhibits such unfair behavior is not a very good quality implementation.  But a lot of very early JVMs (way before Java 5) behaved that way.  And the issue before JSR133 was that this might be reasonable behavior for some special-purpose JVMs, e.g. to run database stored procedures. 

Hans

> 
> David
> 
> > Note that this is also somewhat weasel-worded in the C++0x draft, 
> > since there is a difference between "should" and "shall".  
> The exact 
> > phrasing there is still under discussion.
> >
> > Hans=
> 
> 

From davidcholmes at aapt.net.au  Wed Oct 14 22:09:54 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 15 Oct 2009 12:09:54 +1000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
	APIand visibility/coherency
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577D607B19@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEMBIDAA.davidcholmes@aapt.net.au>

Hans,

> > If the JMM allows this kind of "optimization" to remove
> > happens-before edges then it seems to me that the JMM is
> > fundamentally broken, because any argument as to allowed
> > behaviour has to be suffixed with "assuming the compiler
> > didn't reorder things". And how do we then establish what the
> > compiler is and is not allowed to do?
>
> It's not removing happens-before edges.

If the transformation does not take into account the scheduling behaviour
then it _is_ potentially removing happens-before edges because it precludes
a read from occurring subsequent to the write and so that HB edge ceases to
comes into existence. This is the point that was being made in 2004. It is
okay for the JMM to allow non-terminating executions for unfair schedulers
because in practice we simply don't employ them (or else we get what we
deserve). But it seems totally unacceptable for a transformation to make an
execution on a "fair" scheduler act as-if run on a hypothetical unfair one.

One of the primary volatile examples is the "stopRquested" example where we
have a thread doing:

  while(!stopRequested) {
     doWork();
  }

and we give the factual example that if stopRequested is not volatile then
the Hotspot server compiler will transform this into:

  if (!stopRequested) {
    while(true) {
      doWork();
    }
  }

but by making stopRequested volatile we say this can not happen and the code
is fully correct. But this discussion implies the transformation is still
legal even with volatile because it would emulate a system where the
scheduler never gave the thread setting stopRequested a chance to run!
Please tell me I'm wrong here and that the JMM does not allow this! Having a
memory model that allows this, even if no implementation employs it, is not
a service to anyone in the Java programming community. How can you reason
about code using volatiles if you have to consider this kind of
transformation? You now need to step outside the spec and understand the
details of a particular compiler! The discussion in 2004 indicated this
problem was to be addressed.

Goodness knows what the implications of this are for the Fences API.

I'd like to hear from Bill and Jeremy on this.

David
-----


In the example in which
> the two threads do nothing else, a nonterminating result is 100%
> consistent with the execution in which only the looping thread
> ever runs and the assigning thread never makes progress.  This is
> possible with no optimization, it's purely a function of the
> scheduler.  Since that execution is allowed, the program is
> allowed not to terminate.  If a very clever compiler got to look
> at the whole program, it might be able to move the volatile out
> of the loop IN THIS SPECIFIC CASE, since the resulting execution
> has the same observable behavior as one in which no such
> transformation were made.  It would be cheating, but nobody could
> tell the difference.  In general it can't do so (except possibly
> by also conspiring with the scheduler) because some other thread
> might set the variable and then do something observable.  In any
> case, I know of no optimizer that comes close to trying to do this.
>
> I think this is completely consistent with the message you cite
>
> I do personally think that a general purpose VM that exhibits
> such unfair behavior is not a very good quality implementation.
> But a lot of very early JVMs (way before Java 5) behaved that
> way.  And the issue before JSR133 was that this might be
> reasonable behavior for some special-purpose JVMs, e.g. to run
> database stored procedures.
>
> Hans
>
> >
> > David
> >
> > > Note that this is also somewhat weasel-worded in the C++0x draft,
> > > since there is a difference between "should" and "shall".
> > The exact
> > > phrasing there is still under discussion.
> > >
> > > Hans=
> >
> > =


From jed at atlassian.com  Thu Oct 15 03:36:32 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Thu, 15 Oct 2009 18:36:32 +1100
Subject: [concurrency-interest] Resetable CountDownLatch
In-Reply-To: <200910101213.08814.concurrency-interest@nitwit.de>
References: <200910101213.08814.concurrency-interest@nitwit.de>
Message-ID: <77467A50-16D5-4F8F-B082-27E7CE73C462@atlassian.com>

Forgive me if I am wrong, but might a better way to model this be  
using the forkJoin framework? The basic idea being that you submit a  
render action to a fork join executor that will devolve into the  
recurse actions that render each image part and when they all become  
available, draws the image or makes it available to the draw queue.

cheers,
jed.

On 10/10/2009, at 9:13 PM, Timo Nentwig <concurrency- 
interest at nitwit.de> wrote:

> Hi!
>
> I'm looking for something like a resetable CountDownLatch. Here's  
> what I want
> to do: n thread render an image in parallel (each calling countDown 
> () when
> done) and when all threads are finished, some logic (i.e. drawing to  
> screen)
> needs to be executed. And then start all over again.
>
> I didn't find anything that fits in j.u.c. :-\
>
> thx
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From hoppithek at uni-muenster.de  Thu Oct 15 05:11:49 2009
From: hoppithek at uni-muenster.de (Armin Hopp)
Date: Thu, 15 Oct 2009 11:11:49 +0200
Subject: [concurrency-interest] Resetable CountDownLatch
In-Reply-To: <77467A50-16D5-4F8F-B082-27E7CE73C462@atlassian.com>
References: <200910101213.08814.concurrency-interest@nitwit.de>
	<77467A50-16D5-4F8F-B082-27E7CE73C462@atlassian.com>
Message-ID: <4AD6E755.8070309@uni-muenster.de>

Hi Jed, Timo

I agree with you. I implemented a grayscaler using FJ once, though with
something fast like a grayscaler you might not get a speedup with small
to normal sized images. Also you have to work double buffered if you
need surrounding pixel to calculate the new pixel color. On large images
you probably only want to double buffer border pixels which blows up the
straight-forwardness of the implementation. But all in all I would still
try to fit it into an RecursiveAction.
As far as Timos explanations let as understand his problem, I say it
practically begs for a FJ implementation.

cheers
Armin

Jed Wesley-Smith schrieb:
> Forgive me if I am wrong, but might a better way to model this be
> using the forkJoin framework? The basic idea being that you submit a
> render action to a fork join executor that will devolve into the
> recurse actions that render each image part and when they all become
> available, draws the image or makes it available to the draw queue.
>
> cheers,
> jed.
>
> On 10/10/2009, at 9:13 PM, Timo Nentwig
> <concurrency-interest at nitwit.de> wrote:
>
>> Hi!
>>
>> I'm looking for something like a resetable CountDownLatch. Here's
>> what I want
>> to do: n thread render an image in parallel (each calling countDown()
>> when
>> done) and when all threads are finished, some logic (i.e. drawing to
>> screen)
>> needs to be executed. And then start all over again.
>>
>> I didn't find anything that fits in j.u.c. :-\
>>
>> thx
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 312 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091015/b17361a1/attachment.bin>

From bryan at systap.com  Thu Oct 15 10:24:27 2009
From: bryan at systap.com (Bryan Thompson)
Date: Thu, 15 Oct 2009 09:24:27 -0500
Subject: [concurrency-interest] Advice on running tasks with limited
 parallelism against a shared executor service?
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED439A4968688@AUSP01VMBX06.collaborationhost.net>

Hello,

I often have situations where I would like a set of tasks to execute with limited parallelism against a shared executor service with an unbounded thread pool.  I am interested in limited parallelism because there can be 1000s of subtasks for the designed operation that are decomposed onto tasks.  Typically, those subtasks are performing remote reads on different database shards.  I would like to use an unbounded thread pool to avoid the possibility of deadlock.  I would like to use a shared thread pool because some subtasks can be very light operations and I would like to avoid the overhead of establishing a new executor service and creating its threads and have better control over the executor service on which all of this is running, e.g., for monitoring task service rates.

So the "pattern" that I use is to submit the subtasks of a given task in chunks to the shared executor service, and then submit another chunk when the first one is finished.  This means that all subtasks in a chunk must complete before the next chunk of subtasks for a given caller can be processed.  Different high-level tasks may be submitting their own chunks concurrently, so the total concurrency of the task execution is not bounded by the #of subtasks submitted at once for any given high-level task.

Is there a better way to go about this without having to create an executor service for each set of tasks whose decomposed subtasks I want to run with limited parallelism?

Thanks,

-bryan


From holger.hoffstaette at googlemail.com  Thu Oct 15 11:07:51 2009
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Thu, 15 Oct 2009 17:07:51 +0200
Subject: [concurrency-interest] Advice on running tasks with limited
 parallelism against a shared executor service?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED439A4968688@AUSP01VMBX06.collaborationhost.net>
References: <DE10B00CCE0DC54883734F3060AC9ED439A4968688@AUSP01VMBX06.collaborationhost.net>
Message-ID: <4AD73AC7.3010601@googlemail.com>

Bryan Thompson wrote:
> Is there a better way to go about this without having to create an
> executor service for each set of tasks whose decomposed subtasks I want
> to run with limited parallelism?

Sounds like ExecutorCompletionService might fit the bill?

-h


From bryan at systap.com  Thu Oct 15 13:19:30 2009
From: bryan at systap.com (Bryan Thompson)
Date: Thu, 15 Oct 2009 12:19:30 -0500
Subject: [concurrency-interest] Advice on running tasks with limited
 parallelism against a shared executor service?
In-Reply-To: <4AD73AC7.3010601@googlemail.com>
References: <DE10B00CCE0DC54883734F3060AC9ED439A4968688@AUSP01VMBX06.collaborationhost.net>
	<4AD73AC7.3010601@googlemail.com>
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED439A4A3D336@AUSP01VMBX06.collaborationhost.net>

Holger,

It would appear that the ExecutionCompletionService does not limit the concurrency of the tasks accepted for processing except by the backing thread pool.  In my case, with an unbounded thread pool, it would accept all tasks and create new threads as necessary to execute those tasks.

Thanks,

-bryan 

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Holger Hoffst?tte
> Sent: Thursday, October 15, 2009 11:08 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Advice on running tasks 
> with limited parallelism against a shared executor service?
> 
> Bryan Thompson wrote:
> > Is there a better way to go about this without having to create an 
> > executor service for each set of tasks whose decomposed subtasks I 
> > want to run with limited parallelism?
> 
> Sounds like ExecutorCompletionService might fit the bill?
> 
> -h
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From hans.boehm at hp.com  Thu Oct 15 14:18:48 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 15 Oct 2009 18:18:48 +0000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
 APIand visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEMBIDAA.davidcholmes@aapt.net.au>
References: <238A96A773B3934685A7269CC8A8D042577D607B19@GVW0436EXB.americas.hpqcorp.net>
	<NFBBKALFDCPFIDBNKAPCEEMBIDAA.davidcholmes@aapt.net.au>
Message-ID: <238A96A773B3934685A7269CC8A8D042577D608025@GVW0436EXB.americas.hpqcorp.net>

> From: David Holmes [mailto:davidcholmes at aapt.net.au] 
> Sent: Wednesday, October 14, 2009 7:10 PM
> 
> Hans,
> 
> > > If the JMM allows this kind of "optimization" to remove 
> > > happens-before edges then it seems to me that the JMM is 
> > > fundamentally broken, because any argument as to allowed 
> behaviour 
> > > has to be suffixed with "assuming the compiler didn't reorder 
> > > things". And how do we then establish what the compiler is and is 
> > > not allowed to do?
> >
> > It's not removing happens-before edges.
> 
> If the transformation does not take into account the 
> scheduling behaviour then it _is_ potentially removing 
> happens-before edges because it precludes a read from 
> occurring subsequent to the write and so that HB edge ceases 
> to comes into existence. This is the point that was being 
> made in 2004. It is okay for the JMM to allow non-terminating 
> executions for unfair schedulers because in practice we 
> simply don't employ them (or else we get what we deserve). 
> But it seems totally unacceptable for a transformation to 
> make an execution on a "fair" scheduler act as-if run on a 
> hypothetical unfair one.
I'm not sure we have a real disagreement here about how implementations should behave.  But I think we're running into a limitation of language specifications in general:  They can only specify the allowable behavior of a program, not the details of the implementation.  Once we agree that our example program is allowed not to terminate (because we want to allow dumb schedulers), the implementation is allowed to produce that nonterminating behavior in any way it wants.

If your implementation also promises a fair scheduler (whatever that means in the presence of thread priorities, etc.), then it would almost certainly invalidate that claim if it ever moved volatiles out of loops.

In any case, the observable behavior rules mean that volatiles can bemoved out of loops only under very special circumstances, based on knowledge of the entire program, and no real implementation is likely to do so.  Why would a real implementation work really hard just to break user's code?

The problem is that there are aspects of the behavior here that are hard to specify precisely across all possible implementation contexts.  As a result, they are pinned down just enough to encourage implementors to do the right thing, but not enough to guard against devious implementors.

Hans
> 
> One of the primary volatile examples is the "stopRquested" 
> example where we have a thread doing:
> 
>   while(!stopRequested) {
>      doWork();
>   }
> 
> and we give the factual example that if stopRequested is not 
> volatile then the Hotspot server compiler will transform this into:
> 
>   if (!stopRequested) {
>     while(true) {
>       doWork();
>     }
>   }
> 
> but by making stopRequested volatile we say this can not 
> happen and the code is fully correct. But this discussion 
> implies the transformation is still legal even with volatile 
> because it would emulate a system where the scheduler never 
> gave the thread setting stopRequested a chance to run!
> Please tell me I'm wrong here and that the JMM does not allow 
> this! Having a memory model that allows this, even if no 
> implementation employs it, is not a service to anyone in the 
> Java programming community. How can you reason about code 
> using volatiles if you have to consider this kind of 
> transformation? You now need to step outside the spec and 
> understand the details of a particular compiler! The 
> discussion in 2004 indicated this problem was to be addressed.
> 
> Goodness knows what the implications of this are for the Fences API.
> 
> I'd like to hear from Bill and Jeremy on this.
> 
> David
> -----
> 
> 
> In the example in which
> > the two threads do nothing else, a nonterminating result is 100% 
> > consistent with the execution in which only the looping thread ever 
> > runs and the assigning thread never makes progress.  This 
> is possible 
> > with no optimization, it's purely a function of the 
> scheduler.  Since 
> > that execution is allowed, the program is allowed not to 
> terminate.  
> > If a very clever compiler got to look at the whole program, 
> it might 
> > be able to move the volatile out of the loop IN THIS SPECIFIC CASE, 
> > since the resulting execution has the same observable 
> behavior as one 
> > in which no such transformation were made.  It would be 
> cheating, but 
> > nobody could tell the difference.  In general it can't do 
> so (except 
> > possibly by also conspiring with the scheduler) because some other 
> > thread might set the variable and then do something observable.  In 
> > any case, I know of no optimizer that comes close to trying to do 
> > this.
> >
> > I think this is completely consistent with the message you cite
> >
> > I do personally think that a general purpose VM that exhibits such 
> > unfair behavior is not a very good quality implementation.
> > But a lot of very early JVMs (way before Java 5) behaved that way.  
> > And the issue before JSR133 was that this might be 
> reasonable behavior 
> > for some special-purpose JVMs, e.g. to run database stored 
> procedures.
> >
> > Hans
> >
> > >
> > > David
> > >
> > > > Note that this is also somewhat weasel-worded in the 
> C++0x draft, 
> > > > since there is a difference between "should" and "shall".
> > > The exact
> > > > phrasing there is still under discussion.
> > > >
> > > > Hans=
> > >
> > > =
> 
> 

From joe.bowbeer at gmail.com  Thu Oct 15 14:22:42 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 15 Oct 2009 11:22:42 -0700
Subject: [concurrency-interest] Advice on running tasks with limited
	parallelism against a shared executor service?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED439A4A3D336@AUSP01VMBX06.collaborationhost.net>
References: <DE10B00CCE0DC54883734F3060AC9ED439A4968688@AUSP01VMBX06.collaborationhost.net>
	<4AD73AC7.3010601@googlemail.com>
	<DE10B00CCE0DC54883734F3060AC9ED439A4A3D336@AUSP01VMBX06.collaborationhost.net>
Message-ID: <31f2a7bd0910151122m79f03898gb02ea80220f2bbd0@mail.gmail.com>

I'm imagining that a completion service was suggested as a way to
incrementally feed new tasks to the unbounded executor -- as existing tasks
complete.

Something along these lines has been discussed before, searching the
archives for "completion service" might find it.

A similar approach is to create something like a (lightweight)
SerialExecutor for each chunk of tasks.  See example in Executor javadoc:


http://java.sun.com/javase/6/docs/api/index.html?java/util/concurrent/Executor.html

The serial executor is similar to a completion service, in that it wraps
tasks in order to hook their done() method.

In this approach, each chunk of tasks would be gated by a serial executor.

Joe

On Thu, Oct 15, 2009 at 10:19 AM, Bryan Thompson wrote:

> Holger,
>
> It would appear that the ExecutionCompletionService does not limit the
> concurrency of the tasks accepted for processing except by the backing
> thread pool.  In my case, with an unbounded thread pool, it would accept all
> tasks and create new threads as necessary to execute those tasks.
>
> Thanks,
>
> -bryan
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091015/6d10145f/attachment.html>

From joe.bowbeer at gmail.com  Thu Oct 15 14:29:40 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 15 Oct 2009 11:29:40 -0700
Subject: [concurrency-interest] Advice on running tasks with limited
	parallelism against a shared executor service?
In-Reply-To: <31f2a7bd0910151122m79f03898gb02ea80220f2bbd0@mail.gmail.com>
References: <DE10B00CCE0DC54883734F3060AC9ED439A4968688@AUSP01VMBX06.collaborationhost.net>
	<4AD73AC7.3010601@googlemail.com>
	<DE10B00CCE0DC54883734F3060AC9ED439A4A3D336@AUSP01VMBX06.collaborationhost.net>
	<31f2a7bd0910151122m79f03898gb02ea80220f2bbd0@mail.gmail.com>
Message-ID: <31f2a7bd0910151129w8b941a3te96df5dd1bb94a49@mail.gmail.com>

Correction:

The SerialExecutor example doesn't hook the done() method (as
ExecutorCompletionService does); it wraps the Runnable task in a
try-finally.  Same difference...

--Joe

On Thu, Oct 15, 2009 at 11:22 AM, Joe Bowbeer wrote:

> I'm imagining that a completion service was suggested as a way to
> incrementally feed new tasks to the unbounded executor -- as existing tasks
> complete.
>
> Something along these lines has been discussed before, searching the
> archives for "completion service" might find it.
>
> A similar approach is to create something like a (lightweight)
> SerialExecutor for each chunk of tasks.  See example in Executor javadoc:
>
>
> http://java.sun.com/javase/6/docs/api/index.html?java/util/concurrent/Executor.html
>
> The serial executor is similar to a completion service, in that it wraps
> tasks in order to hook their done() method.
>
> In this approach, each chunk of tasks would be gated by a serial executor.
>
> Joe
>
>
> On Thu, Oct 15, 2009 at 10:19 AM, Bryan Thompson wrote:
>
>> Holger,
>>
>> It would appear that the ExecutionCompletionService does not limit the
>> concurrency of the tasks accepted for processing except by the backing
>> thread pool.  In my case, with an unbounded thread pool, it would accept all
>> tasks and create new threads as necessary to execute those tasks.
>>
>> Thanks,
>>
>> -bryan
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091015/689d8aba/attachment.html>

From phil.goodwin at gmail.com  Thu Oct 15 14:46:32 2009
From: phil.goodwin at gmail.com (Phil Goodwin)
Date: Thu, 15 Oct 2009 11:46:32 -0700
Subject: [concurrency-interest] Advice on running tasks with limited
	parallelism against a shared executor service?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED439A4968688@AUSP01VMBX06.collaborationhost.net>
References: <DE10B00CCE0DC54883734F3060AC9ED439A4968688@AUSP01VMBX06.collaborationhost.net>
Message-ID: <3dbbdf1d0910151146y43a56fcboa302aa994be261f3@mail.gmail.com>

Hi Bryan,

I have had success in the past with the following techniques:

To avoid deadlocks, tasks with dependencies are expressed as
continuations. When the gating task completes, it places all dependent
tasks into the work queue of the thread pool. That way the thread pool
contains only tasks that have hope of progressing. Placing timeouts on
remote operations prevents the system from becoming clogged with
threads waiting on hung operations.

To optimize parallelization of remote processes I dedicate a thread
pool of limited size (very often one) to each remote server and tune
the rate of requests to that server to optimize the load for
throughput. Then I dispatch incoming tasks according to the server
they will query so that I am keeping as many machines busy as
possible.

It would probably be better to make remote requests asynchronous, but
I don't have direct experience with that. My thinking is that sleeping
threads are a waste of system resources. I'd rather have a pointer to
a callback sitting around waiting for a response than the huge chunk
of ram that gets allocated as the stack for a thread.

I don't like the approach of using an unbounded thread pool. Threads
are expensive resources both in terms of the memory needed for their
stacks and the time it takes to schedule and context switch between
them, not to mention the hit taken by flushing CPU cache lines. Basing
their allocation strategy on the random demands produced by contention
between threads will lead to similarly random fluctuations on
performance. I think that it is better to allocate threads according
to the number of CPU cores available and the number of hardware
devices that require synchronous access, with some tuning to trade off
throughput for reduced latency.

If it is not feasible to anticipate the dependencies between tasks, it
is still possible to obtain the deadlock breaking effects of unlimited
threads by using a fair scheduler and putting timeouts on all
attempted resource acquisitions. If a task encounters a timeout it
responds by rolling back its work, placing itself at the back of the
work queue, and terminating. This is the moral equivalent of a thread
stalling and being replaced by a new thread, but with drastically
smaller overhead. You end up with the same benefit as "unlimited"
threads, but with a higher effective upper limit on tasks that can be
attempting to make forward progress at any given time.

Batching makes sense in that scenario (assuming that earlier batches
are not gated by tasks that are contained in later batches) because it
will limit the amount of churn that can be caused by task timeouts.
But in general I would prefer to manage dependencies and load
explicitly in order to get more predictable (and debuggable) results.

Nice to see you again, Bryan,

Phil

On Thu, Oct 15, 2009 at 7:24 AM, Bryan Thompson <bryan at systap.com> wrote:
> Hello,
>
> I often have situations where I would like a set of tasks to execute with limited parallelism against a shared executor service with an unbounded thread pool. ?I am interested in limited parallelism because there can be 1000s of subtasks for the designed operation that are decomposed onto tasks. ?Typically, those subtasks are performing remote reads on different database shards. ?I would like to use an unbounded thread pool to avoid the possibility of deadlock. ?I would like to use a shared thread pool because some subtasks can be very light operations and I would like to avoid the overhead of establishing a new executor service and creating its threads and have better control over the executor service on which all of this is running, e.g., for monitoring task service rates.
>
> So the "pattern" that I use is to submit the subtasks of a given task in chunks to the shared executor service, and then submit another chunk when the first one is finished. ?This means that all subtasks in a chunk must complete before the next chunk of subtasks for a given caller can be processed. ?Different high-level tasks may be submitting their own chunks concurrently, so the total concurrency of the task execution is not bounded by the #of subtasks submitted at once for any given high-level task.
>
> Is there a better way to go about this without having to create an executor service for each set of tasks whose decomposed subtasks I want to run with limited parallelism?
>
> Thanks,
>
> -bryan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From bryan at systap.com  Thu Oct 15 15:01:50 2009
From: bryan at systap.com (Bryan Thompson)
Date: Thu, 15 Oct 2009 14:01:50 -0500
Subject: [concurrency-interest] Advice on running tasks with
	limited	parallelism against a shared executor service?
In-Reply-To: <31f2a7bd0910151122m79f03898gb02ea80220f2bbd0@mail.gmail.com>
References: <DE10B00CCE0DC54883734F3060AC9ED439A4968688@AUSP01VMBX06.collaborationhost.net>
	<4AD73AC7.3010601@googlemail.com>
	<DE10B00CCE0DC54883734F3060AC9ED439A4A3D336@AUSP01VMBX06.collaborationhost.net>
	<31f2a7bd0910151122m79f03898gb02ea80220f2bbd0@mail.gmail.com>
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED439A4A3D413@AUSP01VMBX06.collaborationhost.net>

Joe,

Thanks - that makes sense.

-bryan

________________________________
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Joe Bowbeer
Sent: Thursday, October 15, 2009 2:23 PM
To: concurrency-interest
Subject: Re: [concurrency-interest] Advice on running tasks with limited parallelism against a shared executor service?

I'm imagining that a completion service was suggested as a way to incrementally feed new tasks to the unbounded executor -- as existing tasks complete.

Something along these lines has been discussed before, searching the archives for "completion service" might find it.

A similar approach is to create something like a (lightweight) SerialExecutor for each chunk of tasks.  See example in Executor javadoc:

  http://java.sun.com/javase/6/docs/api/index.html?java/util/concurrent/Executor.html

The serial executor is similar to a completion service, in that it wraps tasks in order to hook their done() method.

In this approach, each chunk of tasks would be gated by a serial executor.

Joe

On Thu, Oct 15, 2009 at 10:19 AM, Bryan Thompson wrote:
Holger,

It would appear that the ExecutionCompletionService does not limit the concurrency of the tasks accepted for processing except by the backing thread pool.  In my case, with an unbounded thread pool, it would accept all tasks and create new threads as necessary to execute those tasks.

Thanks,

-bryan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091015/f46f3757/attachment.html>

From sanders at cise.ufl.edu  Thu Oct 15 16:34:30 2009
From: sanders at cise.ufl.edu (Beverly Sanders)
Date: Thu, 15 Oct 2009 16:34:30 -0400
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
 APIand visibility/coherency
Message-ID: <d2ddb7251d3e7aaa9cd1a03e327834f5.squirrel@webmail.cise.ufl.edu>

> I'm not sure we have a real disagreement here about how implementations
should behave.  But I think we're running into a limitation of language
specifications in general:  They can only specify the allowable behavior
of a program, not the details of the implementation.  Once we agree that
our example program is allowed not to terminate (because we want to
allow
> dumb schedulers), the implementation is allowed to produce that
> nonterminating behavior in any way it wants.
>
> If your implementation also promises a fair scheduler (whatever that
means
> in the presence of thread priorities, etc.), then it would almost
certainly invalidate that claim if it ever moved volatiles out of loops.
>
> In any case, the observable behavior rules mean that volatiles can
bemoved
> out of loops only under very special circumstances, based on knowledge
of
> the entire program, and no real implementation is likely to do so.  Why
would a real implementation work really hard just to break user's code?
>
> The problem is that there are aspects of the behavior here that are hard
to specify precisely across all possible implementation contexts.  As a
result, they are pinned down just enough to encourage implementors to do
the right thing, but not enough to guard against devious implementors.
>
> Hans


Have the folks who do this stuff (JLS and JMM) ever considered creating
two separate specs--one for contexts that promise fair scheduling and one
for those that don't?

Trying to come up with a single specification that says exactly what you
want for both fair and unfair schedulers is pretty much impossible, I
think, even for issues a lot simpler than the memory model.

The clarity and expressiveness gained by doing this would outweigh the
disadvantage of having two specs and requiring JVM implementers to make
promises about scheduling.  Separate specs would be easier to understand
and allow people to make statements like "program A satisfies property P
when executed on a fair JVM" and have that statement follow directly from
the language specification rather than be based on arguments about how
(un)likely it is that a compiler would perform some type of optimization
in a real implementation.

Beverly



From davidcholmes at aapt.net.au  Thu Oct 15 19:07:08 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 16 Oct 2009 09:07:08 +1000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
	APIand visibility/coherency
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577D608025@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEMMIDAA.davidcholmes@aapt.net.au>


Hi Hans,

> I'm not sure we have a real disagreement here about how
> implementations should behave.  But I think we're running into a
> limitation of language specifications in general:  They can only
> specify the allowable behavior of a program, not the details of
> the implementation.  Once we agree that our example program is
> allowed not to terminate (because we want to allow dumb
> schedulers), the implementation is allowed to produce that
> nonterminating behavior in any way it wants.

I really don't follow the logic here that if the platform spec is weak in
its definitions regarding scheduling, that the compiler is allowed to
emulate pathological scheduling behaviour even if running on a system with
"friendly" scheduling (particularly when the actual scheduling details are
defined external to the JVM by virtue of the OS and the hardware
configuration!).

This is the part that I disagree with - and what Bill disagreed with back in
2004 and what appeared to be getting fixed back then! I should be able to
say given some code that "if this runs on a system where the two threads do
get scheduled then the JMM guarantees properties X, Y and Z". But if the JMM
allows the compiler to transform things in the way being discussed then that
doesn't hold.

> If your implementation also promises a fair scheduler (whatever
> that means in the presence of thread priorities, etc.), then it
> would almost certainly invalidate that claim if it ever moved
> volatiles out of loops.
>
> In any case, the observable behavior rules mean that volatiles
> can bemoved out of loops only under very special circumstances,
> based on knowledge of the entire program, and no real
> implementation is likely to do so.  Why would a real
> implementation work really hard just to break user's code?

It doesn't matter if no reasonable/useful implementation would do that, the
point is that the spec allows it when it shouldn't. I'm no formalist but it
seems strange to me that we have a (semi?) formal memory model where any
proof would have to start "assuming a benevolent compiler ...".

Now if there are further constraints on when the JMM allows this, then those
constraints are not at all clear - and that in itself is part of the
problem. The rules should be clear and obvious and easily discernible for
the programmer.

> The problem is that there are aspects of the behavior here that
> are hard to specify precisely across all possible implementation
> contexts.  As a result, they are pinned down just enough to
> encourage implementors to do the right thing, but not enough to
> guard against devious implementors.

The problem is that as a programmer using volatiles and now Fences I need to
be able to reason about how my code can be changed by the runtime compiler.
I thought the JMM gave me that ability - at least in regards to the
important variables - but now I have no confidence that it does.

David

> Hans
> >
> > One of the primary volatile examples is the "stopRquested"
> > example where we have a thread doing:
> >
> >   while(!stopRequested) {
> >      doWork();
> >   }
> >
> > and we give the factual example that if stopRequested is not
> > volatile then the Hotspot server compiler will transform this into:
> >
> >   if (!stopRequested) {
> >     while(true) {
> >       doWork();
> >     }
> >   }
> >
> > but by making stopRequested volatile we say this can not
> > happen and the code is fully correct. But this discussion
> > implies the transformation is still legal even with volatile
> > because it would emulate a system where the scheduler never
> > gave the thread setting stopRequested a chance to run!
> > Please tell me I'm wrong here and that the JMM does not allow
> > this! Having a memory model that allows this, even if no
> > implementation employs it, is not a service to anyone in the
> > Java programming community. How can you reason about code
> > using volatiles if you have to consider this kind of
> > transformation? You now need to step outside the spec and
> > understand the details of a particular compiler! The
> > discussion in 2004 indicated this problem was to be addressed.
> >
> > Goodness knows what the implications of this are for the Fences API.
> >
> > I'd like to hear from Bill and Jeremy on this.
> >
> > David
> > -----
> >
> >
> > In the example in which
> > > the two threads do nothing else, a nonterminating result is 100%
> > > consistent with the execution in which only the looping thread ever
> > > runs and the assigning thread never makes progress.  This
> > is possible
> > > with no optimization, it's purely a function of the
> > scheduler.  Since
> > > that execution is allowed, the program is allowed not to
> > terminate.
> > > If a very clever compiler got to look at the whole program,
> > it might
> > > be able to move the volatile out of the loop IN THIS SPECIFIC CASE,
> > > since the resulting execution has the same observable
> > behavior as one
> > > in which no such transformation were made.  It would be
> > cheating, but
> > > nobody could tell the difference.  In general it can't do
> > so (except
> > > possibly by also conspiring with the scheduler) because some other
> > > thread might set the variable and then do something observable.  In
> > > any case, I know of no optimizer that comes close to trying to do
> > > this.
> > >
> > > I think this is completely consistent with the message you cite
> > >
> > > I do personally think that a general purpose VM that exhibits such
> > > unfair behavior is not a very good quality implementation.
> > > But a lot of very early JVMs (way before Java 5) behaved that way.
> > > And the issue before JSR133 was that this might be
> > reasonable behavior
> > > for some special-purpose JVMs, e.g. to run database stored
> > procedures.
> > >
> > > Hans
> > >
> > > >
> > > > David
> > > >
> > > > > Note that this is also somewhat weasel-worded in the
> > C++0x draft,
> > > > > since there is a difference between "should" and "shall".
> > > > The exact
> > > > > phrasing there is still under discussion.
> > > > >
> > > > > Hans=
> > > >
> > > > =
> >
> > =


From hans.boehm at hp.com  Thu Oct 15 19:37:58 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 15 Oct 2009 23:37:58 +0000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
 APIand visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEMMIDAA.davidcholmes@aapt.net.au>
References: <238A96A773B3934685A7269CC8A8D042577D608025@GVW0436EXB.americas.hpqcorp.net>
	<NFBBKALFDCPFIDBNKAPCEEMMIDAA.davidcholmes@aapt.net.au>
Message-ID: <238A96A773B3934685A7269CC8A8D042577D6AC9E4@GVW0436EXB.americas.hpqcorp.net>

> From: David Holmes [mailto:davidcholmes at aapt.net.au] 
> 
> The problem is that as a programmer using volatiles and now 
> Fences I need to be able to reason about how my code can be 
> changed by the runtime compiler.
> I thought the JMM gave me that ability - at least in regards 
> to the important variables - but now I have no confidence 
> that it does.
> 
> David
> 
I think you need to be able to reason about the semantics of your program.  You don't want to reason about the transformations performed by the compiler; there might not even be a compiler, at least not a runtime one.  The problem might be a thread-local cache in the interpreter, for example.  The memory model should tell you what you need to know about allowable program behaviors, not about implementations.

It's a fair objection that in this case it doesn't since you really only care about the case of a "reasonable" scheduler.  But we curently have only one spec that covers all possible schedulers.

As Beverly suggested, eventually splitting this into two specs, one of which assumes some sort of fairness might not be a bad idea.  Many people really program to the latter one, which is not written down.  (In my experience, the notable exception is old Java code that contains hacks to deal with Green threads scheduling.  Those hacks aren't technically guaranteed to work either.)

The arguments against doing this are:

1) There are arguably bigger fish to fry.  The Sevcik-Aspinall issues worry me more.

2) This gets messy with priorities, etc.  Most people really don't expect the example to terminate if the looping thread runs at a really high priority.  Does that mean we have to describe priority inheritance schemes?

3) I think language specifications traditionally haven't said very precise things about liveness properties.  Why not stick with tradition? :-)

Hans

From davidcholmes at aapt.net.au  Thu Oct 15 20:18:52 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 16 Oct 2009 10:18:52 +1000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
	APIand visibility/coherency
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577D6AC9E4@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEMNIDAA.davidcholmes@aapt.net.au>


Hans,

> I think you need to be able to reason about the semantics of your
> program.  You don't want to reason about the transformations
> performed by the compiler; there might not even be a compiler, at
> least not a runtime one.  The problem might be a thread-local
> cache in the interpreter, for example.  The memory model should
> tell you what you need to know about allowable program behaviors,
> not about implementations.

Agreed, but in this case it doesn't. The semantics the memory model defines
stem from a combination of the program order and the eventual run-time order
(which establishes subsequent actions). When I look at the static code of
the program using volatiles I should be able to reason about the value that
can be read - that's the whole purpose of the memory model. But I can't
reason about that if the compiler/runtime/whatever is allowed to change the
program order in a way that invalidates the very reasoning I'm trying to
apply.

This is why to me it is completely wrong for a transformation to be able to
remove happens-before orderings, because they are the only thing I can
reason about within the JMM. If they are not stable/static/fixed (I don't
know the right term to use) then I can not do any reasoning.

> It's a fair objection that in this case it doesn't since you
> really only care about the case of a "reasonable" scheduler.  But
> we curently have only one spec that covers all possible schedulers.

The scheduler is a red-herring here in my view. I can write the example code
so that zero scheduling assumptions exist other than that a thread will
indeed be scheduled. The only reason the scheduler was raised is because it
is being used as an excuse to allow transformation that in my view should
not be allowed (and which the 2004 proposal disallowed!). ["Two wrongs don't
make a right" seems very applicable here.]

Bottom line, using the JMM I have to be able to determine when:

   while (!stopRequested) ...

can be transformed into:

   if (!stopRequested)
      while (true) ...

and my existing understanding, based on informal intentions of the JMM
combined with the 2004 attempt to close this particular hole formally, was
that it could not happen. But it seems that it can happen (even though it
would be "unreasonable"). So my question is: under what circumstances can it
happen and how can I prevent it?

My secondary concerns are then:
- well if that is allowed when I thought not, what else might be allowed
that I thought not?
- how does this bode for the Fences API?

David

> As Beverly suggested, eventually splitting this into two specs,
> one of which assumes some sort of fairness might not be a bad
> idea.  Many people really program to the latter one, which is not
> written down.  (In my experience, the notable exception is old
> Java code that contains hacks to deal with Green threads
> scheduling.  Those hacks aren't technically guaranteed to work either.)
>
> The arguments against doing this are:
>
> 1) There are arguably bigger fish to fry.  The Sevcik-Aspinall
> issues worry me more.
>
> 2) This gets messy with priorities, etc.  Most people really
> don't expect the example to terminate if the looping thread runs
> at a really high priority.  Does that mean we have to describe
> priority inheritance schemes?
>
> 3) I think language specifications traditionally haven't said
> very precise things about liveness properties.  Why not stick
> with tradition? :-)
>
> Hans=


From hans.boehm at hp.com  Thu Oct 15 20:57:20 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 16 Oct 2009 00:57:20 +0000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
 APIand visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEMNIDAA.davidcholmes@aapt.net.au>
References: <238A96A773B3934685A7269CC8A8D042577D6AC9E4@GVW0436EXB.americas.hpqcorp.net>
	<NFBBKALFDCPFIDBNKAPCKEMNIDAA.davidcholmes@aapt.net.au>
Message-ID: <238A96A773B3934685A7269CC8A8D042577D6ACA33@GVW0436EXB.americas.hpqcorp.net>

 

> -----Original Message-----
> From: David Holmes [mailto:davidcholmes at aapt.net.au] 
> Sent: Thursday, October 15, 2009 5:19 PM
> ...
> Bottom line, using the JMM I have to be able to determine when:
> 
>    while (!stopRequested) ...
> 
> can be transformed into:
> 
>    if (!stopRequested)
>       while (true) ...
> 
> and my existing understanding, based on informal intentions 
> of the JMM combined with the 2004 attempt to close this 
> particular hole formally, was that it could not happen. But 
> it seems that it can happen (even though it would be 
> "unreasonable"). So my question is: under what circumstances 
> can it happen and how can I prevent it?
Another way to look at it is that it can't happen in cases in which you can tell that it did.  That part is exactly the same as the compiler transforming i = 17 into i = 42.  The transformation is illegal in general, but it's fine in special cases, such as when i is dead.

Here the only complication is that for the simplest code examples, you can't actually tell whether the transformation happened, since the same output could have been produced by a weird, but legal, scheduler, since either case produces the same observable behavior.

> 
> My secondary concerns are then:
> - well if that is allowed when I thought not, what else might 
> be allowed that I thought not?
> - how does this bode for the Fences API?
I think this should be orthogonal to the fence API, if specified correctly.

A fence in the loop should prevent an old value from being observably reused exactly in the same sense that a volatile read shouldn't be observably reused.  But sometimes reuse won't be observable, so the compiler could technically cheat ...

Hans

> 
> David
> 


From davidcholmes at aapt.net.au  Thu Oct 15 22:40:24 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 16 Oct 2009 12:40:24 +1000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
	APIand visibility/coherency
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577D6ACA33@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEMOIDAA.davidcholmes@aapt.net.au>

Hans,

First, let me say that I really appreciate the continuing discourse.

> > Bottom line, using the JMM I have to be able to determine when:
> >
> >    while (!stopRequested) ...
> >
> > can be transformed into:
> >
> >    if (!stopRequested)
> >       while (true) ...
> >
> > and my existing understanding, based on informal intentions
> > of the JMM combined with the 2004 attempt to close this
> > particular hole formally, was that it could not happen. But
> > it seems that it can happen (even though it would be
> > "unreasonable"). So my question is: under what circumstances
> > can it happen and how can I prevent it?
>
> Another way to look at it is that it can't happen in cases in
> which you can tell that it did.  That part is exactly the same as
> the compiler transforming i = 17 into i = 42.  The transformation
> is illegal in general, but it's fine in special cases, such as
> when i is dead.
>
> Here the only complication is that for the simplest code
> examples, you can't actually tell whether the transformation
> happened, since the same output could have been produced by a
> weird, but legal, scheduler, since either case produces the same
> observable behavior.

I submit that I can tell because I don't "see" the thread exiting the loop.
But there's no requirement for the thread executing the loop to ever be
scheduled, hence the loop would never terminate anyway. Hence the
transformation seems free to take place regardless. :(

In fact taking the argument to the absurd. By allowing the compiler to do
anything that would be legal under the un-specified abstract platform
scheduler, it could replace any piece of code with "while(1);" and emulate
the thread never actually getting scheduled - which would seem to be
perfectly legal unless somewhere in the JVMS or JLS it states that if there
exists a runnable Java thread then the system must ensure some forward
progress on that thread is made? I don't think such a statement exists - if
it did then the loop would be required to terminate.

My point in all this is that allowing transformations that emulate a
pathological scheduler is not a sensible thing to do. This was recognized in
2004 but behind closed doors the public decision to address this was
silently (and erroneously in my view) reversed. So that now we have a JMM
where to reason about things concretely we first have to appeal to the
reasonableness of the implementation.

> >
> > My secondary concerns are then:
> > - well if that is allowed when I thought not, what else might
> > be allowed that I thought not?
> > - how does this bode for the Fences API?
> I think this should be orthogonal to the fence API, if specified
> correctly.
>
> A fence in the loop should prevent an old value from being
> observably reused exactly in the same sense that a volatile read
> shouldn't be observably reused.  But sometimes reuse won't be
> observable, so the compiler could technically cheat ...

Consider this. There is a section of code without volatiles or sync and
which the compiler performs a range of optimizations and transformation on.
Now you stick in a fence. How does that impact what the compiler is allowed
to do? I don't know. I sure hope it's balatantly obvious to the compiler
writers.

David

> Hans
>
> >
> > David
> >


From peter.kovacs.1.0rc at gmail.com  Fri Oct 16 04:09:48 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Fri, 16 Oct 2009 10:09:48 +0200
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
	APIand visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEMOIDAA.davidcholmes@aapt.net.au>
References: <238A96A773B3934685A7269CC8A8D042577D6ACA33@GVW0436EXB.americas.hpqcorp.net>
	<NFBBKALFDCPFIDBNKAPCMEMOIDAA.davidcholmes@aapt.net.au>
Message-ID: <fdeb32eb0910160109t31cdbb47wab8e23452736f88f@mail.gmail.com>

On Fri, Oct 16, 2009 at 4:40 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

> Hans,
>
> First, let me say that I really appreciate the continuing discourse.
>

I, as an amateur onlooker, find this dicussion very exciting.

One thing which might not be clear to the laymen (such as I am) is why there
was a major revision of the Java memory model required in 2004 -- at a time
when Java 5 was about to be (or had already been?) released, whereas the
synchronization primitives must have been there since, I guess, at least
about Java 1.1...? Please, David, could you briefly indicate the context in
which (why) the 2004 revision took place?

I hope (am sure) a brief aside doesn't derail the exciting discussion which
has been developing here.

Sorry for the interruption and many thanks
Peter


>
> > > Bottom line, using the JMM I have to be able to determine when:
> > >
> > >    while (!stopRequested) ...
> > >
> > > can be transformed into:
> > >
> > >    if (!stopRequested)
> > >       while (true) ...
> > >
> > > and my existing understanding, based on informal intentions
> > > of the JMM combined with the 2004 attempt to close this
> > > particular hole formally, was that it could not happen. But
> > > it seems that it can happen (even though it would be
> > > "unreasonable"). So my question is: under what circumstances
> > > can it happen and how can I prevent it?
> >
> > Another way to look at it is that it can't happen in cases in
> > which you can tell that it did.  That part is exactly the same as
> > the compiler transforming i = 17 into i = 42.  The transformation
> > is illegal in general, but it's fine in special cases, such as
> > when i is dead.
> >
> > Here the only complication is that for the simplest code
> > examples, you can't actually tell whether the transformation
> > happened, since the same output could have been produced by a
> > weird, but legal, scheduler, since either case produces the same
> > observable behavior.
>
> I submit that I can tell because I don't "see" the thread exiting the loop.
> But there's no requirement for the thread executing the loop to ever be
> scheduled, hence the loop would never terminate anyway. Hence the
> transformation seems free to take place regardless. :(
>
> In fact taking the argument to the absurd. By allowing the compiler to do
> anything that would be legal under the un-specified abstract platform
> scheduler, it could replace any piece of code with "while(1);" and emulate
> the thread never actually getting scheduled - which would seem to be
> perfectly legal unless somewhere in the JVMS or JLS it states that if there
> exists a runnable Java thread then the system must ensure some forward
> progress on that thread is made? I don't think such a statement exists - if
> it did then the loop would be required to terminate.
>
> My point in all this is that allowing transformations that emulate a
> pathological scheduler is not a sensible thing to do. This was recognized
> in
> 2004 but behind closed doors the public decision to address this was
> silently (and erroneously in my view) reversed. So that now we have a JMM
> where to reason about things concretely we first have to appeal to the
> reasonableness of the implementation.
>
> > >
> > > My secondary concerns are then:
> > > - well if that is allowed when I thought not, what else might
> > > be allowed that I thought not?
> > > - how does this bode for the Fences API?
> > I think this should be orthogonal to the fence API, if specified
> > correctly.
> >
> > A fence in the loop should prevent an old value from being
> > observably reused exactly in the same sense that a volatile read
> > shouldn't be observably reused.  But sometimes reuse won't be
> > observable, so the compiler could technically cheat ...
>
> Consider this. There is a section of code without volatiles or sync and
> which the compiler performs a range of optimizations and transformation on.
> Now you stick in a fence. How does that impact what the compiler is allowed
> to do? I don't know. I sure hope it's balatantly obvious to the compiler
> writers.
>
> David
>
> > Hans
> >
> > >
> > > David
> > >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091016/ae889973/attachment.html>

From davidcholmes at aapt.net.au  Fri Oct 16 04:15:52 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 16 Oct 2009 18:15:52 +1000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
	APIand visibility/coherency
In-Reply-To: <fdeb32eb0910160109t31cdbb47wab8e23452736f88f@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKENCIDAA.davidcholmes@aapt.net.au>

Peter,

Please see the JMM website:

http://www.cs.umd.edu/~pugh/java/memoryModel/

in particular Bill Pugh's paper:

http://www.cs.umd.edu/~pugh/java/broken.pdf

The old memory model was inadequate. It disallowed common compiler optimizations and didn't provide the right semantic guarantees. It was expressed in terms of a cache-like metaphor whereby each thread had a working memory and the memory-model defined when values had to be read/written from/to main memory.

David
  -----Original Message-----
  From: peter.dunay.kovacs at gmail.com [mailto:peter.dunay.kovacs at gmail.com]On Behalf Of P?ter Kov?cs
  Sent: Friday, 16 October 2009 6:10 PM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu; javamemorymodel-discussion at cs.umd.edu
  Subject: Re: [concurrency-interest] [Javamemorymodel-discussion] Fences APIand visibility/coherency


  On Fri, Oct 16, 2009 at 4:40 AM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Hans,

    First, let me say that I really appreciate the continuing discourse.


  I, as an amateur onlooker, find this dicussion very exciting.

  One thing which might not be clear to the laymen (such as I am) is why there was a major revision of the Java memory model required in 2004 -- at a time when Java 5 was about to be (or had already been?) released, whereas the synchronization primitives must have been there since, I guess, at least about Java 1.1...? Please, David, could you briefly indicate the context in which (why) the 2004 revision took place?

  I hope (am sure) a brief aside doesn't derail the exciting discussion which has been developing here.

  Sorry for the interruption and many thanks
  Peter
   

    > > Bottom line, using the JMM I have to be able to determine when:
    > >
    > >    while (!stopRequested) ...
    > >
    > > can be transformed into:
    > >
    > >    if (!stopRequested)
    > >       while (true) ...
    > >
    > > and my existing understanding, based on informal intentions
    > > of the JMM combined with the 2004 attempt to close this
    > > particular hole formally, was that it could not happen. But
    > > it seems that it can happen (even though it would be
    > > "unreasonable"). So my question is: under what circumstances
    > > can it happen and how can I prevent it?
    >
    > Another way to look at it is that it can't happen in cases in
    > which you can tell that it did.  That part is exactly the same as
    > the compiler transforming i = 17 into i = 42.  The transformation
    > is illegal in general, but it's fine in special cases, such as
    > when i is dead.
    >
    > Here the only complication is that for the simplest code
    > examples, you can't actually tell whether the transformation
    > happened, since the same output could have been produced by a
    > weird, but legal, scheduler, since either case produces the same
    > observable behavior.


    I submit that I can tell because I don't "see" the thread exiting the loop.
    But there's no requirement for the thread executing the loop to ever be
    scheduled, hence the loop would never terminate anyway. Hence the
    transformation seems free to take place regardless. :(

    In fact taking the argument to the absurd. By allowing the compiler to do
    anything that would be legal under the un-specified abstract platform
    scheduler, it could replace any piece of code with "while(1);" and emulate
    the thread never actually getting scheduled - which would seem to be
    perfectly legal unless somewhere in the JVMS or JLS it states that if there
    exists a runnable Java thread then the system must ensure some forward
    progress on that thread is made? I don't think such a statement exists - if
    it did then the loop would be required to terminate.

    My point in all this is that allowing transformations that emulate a
    pathological scheduler is not a sensible thing to do. This was recognized in
    2004 but behind closed doors the public decision to address this was
    silently (and erroneously in my view) reversed. So that now we have a JMM
    where to reason about things concretely we first have to appeal to the
    reasonableness of the implementation.


    > >
    > > My secondary concerns are then:
    > > - well if that is allowed when I thought not, what else might
    > > be allowed that I thought not?
    > > - how does this bode for the Fences API?
    > I think this should be orthogonal to the fence API, if specified
    > correctly.
    >
    > A fence in the loop should prevent an old value from being
    > observably reused exactly in the same sense that a volatile read
    > shouldn't be observably reused.  But sometimes reuse won't be
    > observable, so the compiler could technically cheat ...


    Consider this. There is a section of code without volatiles or sync and
    which the compiler performs a range of optimizations and transformation on.
    Now you stick in a fence. How does that impact what the compiler is allowed
    to do? I don't know. I sure hope it's balatantly obvious to the compiler
    writers.

    David


    > Hans
    >
    > >
    > > David
    > >

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091016/d5104561/attachment-0001.html>

From dvyukov at gmail.com  Fri Oct 16 08:28:25 2009
From: dvyukov at gmail.com (Dmitriy V'jukov)
Date: Fri, 16 Oct 2009 16:28:25 +0400
Subject: [concurrency-interest] Fences APIand visibility/coherency
Message-ID: <9014da950910160528l15ea68dsa419bf34965a3e03@mail.gmail.com>

> Date: Thu, 15 Oct 2009 18:18:48 +0000
> From: "Boehm, Hans" <hans.boehm at hp.com>
> Subject: Re: [concurrency-interest] [Javamemorymodel-discussion]
> ? ? ? ?Fences APIand visibility/coherency

> I'm not sure we have a real disagreement here about how implementations should behave. ?But I think we're running into a limitation of language specifications in general: ?They can only specify the allowable behavior of a program, not the details of the implementation. ?Once we agree that our example program is allowed not to terminate (because we want to allow dumb schedulers), the implementation is allowed to produce that nonterminating behavior in any way it wants.

Hans,

I understand what you are talking about, and this makes sense. However
here is some thoughts on this.

Is it why "synchronization order must be a countable set" removed in
favor of "Java does not require fair scheduler" (providing both
statements is contradicting)?

As far as I understand, what you are saying effectively implying that
volatile and plain vars are basically  the same (ordering aside). I.e.
assume I am in your team and write "signaling on a flag" with flag
declared as plain var. You say to me: Hey, Dmitry, you have to declare
flag as volatile. Me: But why, Hans? They both may work, and neither
guaranteed to work. So what's the distinction? What will you answer to
me?
Imho, there still may be some distinction factors for plain and
volatile vars regarding visibility/coherence (even if volatile is
still not guaranteed to work).
Or, for example, Fences API - is it "this not guaranteed to work" or
"that not guaranteed to work"? Are they "not guaranteed to work as
volatiles" or "not guaranteed to work as plain vars"? Currently you
can't answer (i.e. describe in Fences API specification), because
there is no distinction.

Then, as David Holmes said, it may be reasonable to make 2
specifications. Or at least specify implementations of type 1, for
which both practically fair scheduler and wicked optimizations
(allowing volatile store to sink below infinite loop) are prohibited.
And implementations of type 2, for which neither required (because
these 2 things are basically the same from observable behaviour point
of view).
Assume, I write a mission-critical program for health-care. Customer
legally asks me "Does your program guaranteed to work and not
deadlocks?". With current specification (equally applies to C++0x) I
may only answer "Ah, well, it may work or may deadlock. It's actually
not specified".
If there will be requirements for implementations of type 1 and type
2, then I will be able to answer "My program is guaranteed to not
deadlock on implementations of type1". And everybody knows that Sun
Java+Solaris  (or g++/Linux for C++) is implementation of type 1. Now
everything is in their right places, and I have some formal ground
under my feet.

-- 
Dmitry Vyukov


From hans.boehm at hp.com  Fri Oct 16 19:25:48 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 16 Oct 2009 23:25:48 +0000
Subject: [concurrency-interest] [Javamemorymodel-discussion] Fences
 APIand visibility/coherency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEMOIDAA.davidcholmes@aapt.net.au>
References: <238A96A773B3934685A7269CC8A8D042577D6ACA33@GVW0436EXB.americas.hpqcorp.net>
	<NFBBKALFDCPFIDBNKAPCMEMOIDAA.davidcholmes@aapt.net.au>
Message-ID: <238A96A773B3934685A7269CC8A8D042577D6AD1C0@GVW0436EXB.americas.hpqcorp.net>

> From: David Holmes [mailto:davidcholmes at aapt.net.au] 
> Sent: Thursday, October 15, 2009 7:40 PM
> 
> Hans,
> 
> First, let me say that I really appreciate the continuing discourse.
> 
> > > Bottom line, using the JMM I have to be able to determine when:
> > >
> > >    while (!stopRequested) ...
> > >
> > > can be transformed into:
> > >
> > >    if (!stopRequested)
> > >       while (true) ...
> > >
> > > and my existing understanding, based on informal 
> intentions of the 
> > > JMM combined with the 2004 attempt to close this particular hole 
> > > formally, was that it could not happen. But it seems that it can 
> > > happen (even though it would be "unreasonable"). So my 
> question is: 
> > > under what circumstances can it happen and how can I prevent it?
> >
> > Another way to look at it is that it can't happen in cases in which 
> > you can tell that it did.  That part is exactly the same as the 
> > compiler transforming i = 17 into i = 42.  The transformation is 
> > illegal in general, but it's fine in special cases, such as 
> when i is 
> > dead.
> >
> > Here the only complication is that for the simplest code 
> examples, you 
> > can't actually tell whether the transformation happened, since the 
> > same output could have been produced by a weird, but legal, 
> scheduler, 
> > since either case produces the same observable behavior.
> 
> I submit that I can tell because I don't "see" the thread 
> exiting the loop.
> But there's no requirement for the thread executing the loop 
> to ever be scheduled, hence the loop would never terminate 
> anyway. Hence the transformation seems free to take place 
> regardless. :(
> 
> In fact taking the argument to the absurd. By allowing the 
> compiler to do anything that would be legal under the 
> un-specified abstract platform scheduler, it could replace 
> any piece of code with "while(1);" and emulate the thread 
> never actually getting scheduled - which would seem to be 
> perfectly legal unless somewhere in the JVMS or JLS it states 
> that if there exists a runnable Java thread then the system 
> must ensure some forward progress on that thread is made? I 
> don't think such a statement exists - if it did then the loop 
> would be required to terminate.
I think 17.4.9 is intended to outlaw random hangs where there is no infinite loop.
> 
> My point in all this is that allowing transformations that 
> emulate a pathological scheduler is not a sensible thing to 
> do. This was recognized in
> 2004 but behind closed doors the public decision to address 
> this was silently (and erroneously in my view) reversed. So 
> that now we have a JMM where to reason about things 
> concretely we first have to appeal to the reasonableness of 
> the implementation.
My recollection is that nothing was reversed, it morphed into 17.4.9, which I think does imply that

Thread 1:
while (v) {}

Thread 2:
v = true; System.out.println("done");

can't print "done" and then continue to run.  Without constraining the scheduler, I think that's roughly all you can say.  And I think the decision not to constrain the scheduler is supported by perfectly rational arguments, though it's clearly a trade-off.

As you said, Bill and Jeremy may wish to comment here.

>From my perspective, the part of the process that didn't work particularly well here (aside from the fact that JSR 133 was trying to solve a technically very hard and in retrospect perhaps not completely solvable problem) is that the production of the actual JLS text for JSR 133 seemed to happen as too much of an afterthought, with not enough review of the actual text.  I suspect that with a bit more effort, 17.4.9 for example could have been clearer than it is.

> 
> > >
> > > My secondary concerns are then:
> > > - well if that is allowed when I thought not, what else might be 
> > > allowed that I thought not?
> > > - how does this bode for the Fences API?
> > I think this should be orthogonal to the fence API, if specified 
> > correctly.
> >
> > A fence in the loop should prevent an old value from being 
> observably 
> > reused exactly in the same sense that a volatile read shouldn't be 
> > observably reused.  But sometimes reuse won't be observable, so the 
> > compiler could technically cheat ...
> 
> Consider this. There is a section of code without volatiles 
> or sync and which the compiler performs a range of 
> optimizations and transformation on.
> Now you stick in a fence. How does that impact what the 
> compiler is allowed to do? I don't know. I sure hope it's 
> balatantly obvious to the compiler writers.
The right of fence would prevent a compiler from, for example, reusing a load from before the fence.  That means that you can't in general move a variable references outside a loop with a fence in it, since that amounts to reusing the value from before the loop.  This is not to argue that actually getting the details right here is easy; the C++ exercise has shown us that specifying fences correctly is hard, and specifying them in a way that satisfies everyone's intuition is impossible.

Hans

> 
> David
> 
> > Hans
> >
> > >
> > > David
> > >
> 
> 

From i30817 at gmail.com  Fri Oct 16 20:31:53 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Sat, 17 Oct 2009 01:31:53 +0100
Subject: [concurrency-interest] What happens when a ExecutorService Task...
Message-ID: <212322090910161731k92a6252lead0ee47c543d383@mail.gmail.com>

Calls interrupt (by shutdownnow or otherwise).
If the executor service caches the Threads does isInterrupted() still
return true in the next task or not?

From hans.boehm at hp.com  Fri Oct 16 20:46:38 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sat, 17 Oct 2009 00:46:38 +0000
Subject: [concurrency-interest] Fences APIand visibility/coherency
In-Reply-To: <9014da950910160528l15ea68dsa419bf34965a3e03@mail.gmail.com>
References: <9014da950910160528l15ea68dsa419bf34965a3e03@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042577D6AD21C@GVW0436EXB.americas.hpqcorp.net>

 

> -----Original Message-----
> From: Dmitriy V'jukov [mailto:dvyukov at gmail.com] 
> Sent: Friday, October 16, 2009 5:28 AM
> To: concurrency-interest at cs.oswego.edu; 
> javamemorymodel-discussion at cs.umd.edu
> Cc: Doug Lea; Boehm, Hans; davidcholmes at aapt.net.au; Adam Morrison
> Subject: Fences APIand visibility/coherency
> 
> > Date: Thu, 15 Oct 2009 18:18:48 +0000
> > From: "Boehm, Hans" <hans.boehm at hp.com>
> > Subject: Re: [concurrency-interest] [Javamemorymodel-discussion]
> > ? ? ? ?Fences APIand visibility/coherency
> 
> > I'm not sure we have a real disagreement here about how 
> implementations should behave. ?But I think we're running 
> into a limitation of language specifications in general: ?
> They can only specify the allowable behavior of a program, 
> not the details of the implementation. ?Once we agree that 
> our example program is allowed not to terminate (because we 
> want to allow dumb schedulers), the implementation is allowed 
> to produce that nonterminating behavior in any way it wants.
> 
> Hans,
> 
> I understand what you are talking about, and this makes 
> sense. However here is some thoughts on this.
> 
> Is it why "synchronization order must be a countable set" 
> removed in favor of "Java does not require fair scheduler" 
> (providing both statements is contradicting)?
As I said in my last message, I don't think the intent was to remove it, just to restate it.  See the section on observable behavior.
> 
> As far as I understand, what you are saying effectively 
> implying that volatile and plain vars are basically  the same 
> (ordering aside). I.e.
> assume I am in your team and write "signaling on a flag" with 
> flag declared as plain var. You say to me: Hey, Dmitry, you 
> have to declare flag as volatile. Me: But why, Hans? They 
> both may work, and neither guaranteed to work. So what's the 
> distinction? What will you answer to me?
The volatile flag will guarantee partial correctness; If you see the flag set, you will see prior state updates by the thread setting the flag.  The ordinary variable one won't guarantee that.

As a practical matter, the volatile flag will work as far as termination properties are concerned, the other one may well not.  I believe the volatile flag is guaranteed to do the right thing for applications that work no matter what the scheduler does.

> Imho, there still may be some distinction factors for plain 
> and volatile vars regarding visibility/coherence (even if 
> volatile is still not guaranteed to work).
I don't think your terminology here is very standard.  Volatile has the properties you want as far as partial correctness is concerned:  If the program terminates, it will behave as you expect.  Ordinary variables do not.  The whole language specification is weak in terms of progress guarantees, and that makes it difficult to guarantee progress for volatiles.  If you notify a thread with lock-protected flag, you have exactly the same issue, and perhaps even a more difficult one.

> Or, for example, Fences API - is it "this not guaranteed to 
> work" or "that not guaranteed to work"? Are they "not 
> guaranteed to work as volatiles" or "not guaranteed to work 
> as plain vars"? Currently you can't answer (i.e. describe in 
> Fences API specification), because there is no distinction.
I have to look back at the fence API.  Last I checked we were still trying to resolve other issues there.

> 
> Then, as David Holmes said, it may be reasonable to make 2 
> specifications. Or at least specify implementations of type 
> 1, for which both practically fair scheduler and wicked 
> optimizations (allowing volatile store to sink below infinite 
> loop) are prohibited.
Volatile stores can only sink below infinite loops in cases in which the infinite loop has no other observable result, so that it looks as though the loop is just getting all the cycles and nothing else is running.

> And implementations of type 2, for which neither required 
> (because these 2 things are basically the same from 
> observable behaviour point of view).
I do agree that there's something to be said for specifying both alternatives.  But again, the type 1 specification is nontrivial.  What happens to fairnes if thread priorities differ?

> Assume, I write a mission-critical program for health-care. 
> Customer legally asks me "Does your program guaranteed to 
> work and not deadlocks?". With current specification (equally 
> applies to C++0x) I may only answer "Ah, well, it may work or 
> may deadlock. It's actually not specified".
It won't deadlock.  But it may appear that a thread you don't care about gets all the cycles.

I'm actually not sure that this is realistically fixable, since that healthcare program probably needs real-time guarantees that are beyond this spec anyway.

> If there will be requirements for implementations of type 1 
> and type 2, then I will be able to answer "My program is 
> guaranteed to not deadlock on implementations of type1". And 
> everybody knows that Sun
> Java+Solaris  (or g++/Linux for C++) is implementation of type 1. Now
> everything is in their right places, and I have some formal 
> ground under my feet.
We agree that this would be good.  But I don't think it's a trivial oversight.  There are some hard issues there.

Hans
> 
> --
> Dmitry Vyukov
> 

From joe.bowbeer at gmail.com  Fri Oct 16 21:17:15 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 16 Oct 2009 18:17:15 -0700
Subject: [concurrency-interest] What happens when a ExecutorService
	Task...
In-Reply-To: <212322090910161731k92a6252lead0ee47c543d383@mail.gmail.com>
References: <212322090910161731k92a6252lead0ee47c543d383@mail.gmail.com>
Message-ID: <31f2a7bd0910161817o620cf847q4175da53fc430158@mail.gmail.com>

On Fri, Oct 16, 2009 at 5:31 PM, Paulo Levi <i30817 at gmail.com> wrote:

> Calls interrupt (by shutdownnow or otherwise).
> If the executor service caches the Threads does isInterrupted() still
> return true in the next task or not?
>
>

There's a long comment about this in the ThreadPoolExecutor source above the
private Worker class, and a shorter comment in Worker's runTask method:

Ensure that unless pool is stopping, this thread does not have its interrupt
> set.
>

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091016/7ad0b8f8/attachment.html>

From i30817 at gmail.com  Fri Oct 16 21:29:33 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Sat, 17 Oct 2009 02:29:33 +0100
Subject: [concurrency-interest] What happens when a ExecutorService
	Task...
In-Reply-To: <31f2a7bd0910161817o620cf847q4175da53fc430158@mail.gmail.com>
References: <212322090910161731k92a6252lead0ee47c543d383@mail.gmail.com> 
	<31f2a7bd0910161817o620cf847q4175da53fc430158@mail.gmail.com>
Message-ID: <212322090910161829y713f28a2w328c0d3d8ce2ab85@mail.gmail.com>

Ok, cool, all my custom ExecutorServices extend ThreadPoolExecutor.

From cowwoc at bbs.darktech.org  Mon Oct 19 15:26:46 2009
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Mon, 19 Oct 2009 12:26:46 -0700 (PDT)
Subject: [concurrency-interest] Moving CompletionHandler to
 java.util.concurrent
Message-ID: <25958440.post@talk.nabble.com>


Hi,

Hi,

I would like to propose moving CompletionHandler to package
java.util.concurrent. Its use is not limited to I/O operations. For example,
I use it for monitoring asynchronous Swing operations. CompletionHandler can
be used virtually anywhere that java.util.concurrent.Future can, so why not
place them in the same package? 

Orginal post:
http://n2.nabble.com/Moving-CompletionHandler-to-java-util-concurrent-tt3845440.html#a3848449

Thanks,
Gili
-- 
View this message in context: http://www.nabble.com/Moving-CompletionHandler-to-java.util.concurrent-tp25958440p25958440.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From sberlin at gmail.com  Mon Oct 19 15:50:28 2009
From: sberlin at gmail.com (Sam Berlin)
Date: Mon, 19 Oct 2009 15:50:28 -0400
Subject: [concurrency-interest] Moving CompletionHandler to
	java.util.concurrent
In-Reply-To: <25958440.post@talk.nabble.com>
References: <25958440.post@talk.nabble.com>
Message-ID: <19196d860910191250r4f417464o130e2e336e1bee12@mail.gmail.com>

There seem to be many inventions designed to workaround the fact that
Futures don't have a listening mechanism.  CompletionHandler is one
mechanism to asynchronously listen to the completion of a Future.  Google
recently open-sourced their "guava" library which adds methods for listening
to a Future.  There was a long discussion a while ago about different folks
implementing their own listeners to Future.

IMO, I'd rather really & truly add listeners to Future.  But
CompletionHandler is nice too.

Sam

On Mon, Oct 19, 2009 at 3:26 PM, cowwoc <cowwoc at bbs.darktech.org> wrote:

>
> Hi,
>
> Hi,
>
> I would like to propose moving CompletionHandler to package
> java.util.concurrent. Its use is not limited to I/O operations. For
> example,
> I use it for monitoring asynchronous Swing operations. CompletionHandler
> can
> be used virtually anywhere that java.util.concurrent.Future can, so why not
> place them in the same package?
>
> Orginal post:
>
> http://n2.nabble.com/Moving-CompletionHandler-to-java-util-concurrent-tt3845440.html#a3848449
>
> Thanks,
> Gili
> --
> View this message in context:
> http://www.nabble.com/Moving-CompletionHandler-to-java.util.concurrent-tp25958440p25958440.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091019/a7527ef5/attachment.html>

From joe.bowbeer at gmail.com  Mon Oct 19 15:51:49 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 19 Oct 2009 12:51:49 -0700
Subject: [concurrency-interest] Moving CompletionHandler to
	java.util.concurrent
In-Reply-To: <25958440.post@talk.nabble.com>
References: <25958440.post@talk.nabble.com>
Message-ID: <31f2a7bd0910191251ka608c0erb8fb02ca6efb3fd5@mail.gmail.com>

You are referring to the proposed java.nio.channels.CompletionHandler?

http://openjdk.java.net/projects/nio/javadoc/java/nio/channels/CompletionHandler.html

Joe

On Mon, Oct 19, 2009 at 12:26 PM, cowwoc wrote:

>
> Hi,
>
> Hi,
>
> I would like to propose moving CompletionHandler to package
> java.util.concurrent. Its use is not limited to I/O operations. For
> example,
> I use it for monitoring asynchronous Swing operations. CompletionHandler
> can
> be used virtually anywhere that java.util.concurrent.Future can, so why not
> place them in the same package?
>
> Orginal post:
>
> http://n2.nabble.com/Moving-CompletionHandler-to-java-util-concurrent-tt3845440.html#a3848449
>
> Thanks,
> Gili
> --
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091019/ff0d32c1/attachment.html>

From cowwoc at bbs.darktech.org  Mon Oct 19 16:36:13 2009
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Mon, 19 Oct 2009 13:36:13 -0700 (PDT)
Subject: [concurrency-interest] Moving CompletionHandler to
 java.util.concurrent
In-Reply-To: <31f2a7bd0910191251ka608c0erb8fb02ca6efb3fd5@mail.gmail.com>
References: <25958440.post@talk.nabble.com>
	<31f2a7bd0910191251ka608c0erb8fb02ca6efb3fd5@mail.gmail.com>
Message-ID: <25965204.post@talk.nabble.com>


Yes, I am.

Gili


Joe Bowbeer wrote:
> 
> You are referring to the proposed java.nio.channels.CompletionHandler?
> 
> http://openjdk.java.net/projects/nio/javadoc/java/nio/channels/CompletionHandler.html
> 
> Joe
> 
> On Mon, Oct 19, 2009 at 12:26 PM, cowwoc wrote:
> 
>>
>> Hi,
>>
>> Hi,
>>
>> I would like to propose moving CompletionHandler to package
>> java.util.concurrent. Its use is not limited to I/O operations. For
>> example,
>> I use it for monitoring asynchronous Swing operations. CompletionHandler
>> can
>> be used virtually anywhere that java.util.concurrent.Future can, so why
>> not
>> place them in the same package?
>>
>> Orginal post:
>>
>> http://n2.nabble.com/Moving-CompletionHandler-to-java-util-concurrent-tt3845440.html#a3848449
>>
>> Thanks,
>> Gili
>> --
>>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 

-- 
View this message in context: http://www.nabble.com/Moving-CompletionHandler-to-java.util.concurrent-tp25958440p25965204.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From i30817 at gmail.com  Mon Oct 19 21:09:27 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Tue, 20 Oct 2009 02:09:27 +0100
Subject: [concurrency-interest] What is can countdown latch do in case of a
	error?
Message-ID: <212322090910191809r496a994frdeca75c11bd802f4@mail.gmail.com>

So i have a master thread that has to await a serial runnable (one for
each word in a phrase) to end.
I was thinking of using a CountdownLatch instead of the wait/notify
i'm using now (and has a bug i'm trying to find),
But i don't see a way to notify the latch (and the master ofcourse) in
case of failure right?

From i30817 at gmail.com  Mon Oct 19 22:30:18 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Tue, 20 Oct 2009 03:30:18 +0100
Subject: [concurrency-interest] What is can countdown latch do in case
	of a error?
In-Reply-To: <212322090910191809r496a994frdeca75c11bd802f4@mail.gmail.com>
References: <212322090910191809r496a994frdeca75c11bd802f4@mail.gmail.com>
Message-ID: <212322090910191930u4eb89977r1dcbbe50b17bb199@mail.gmail.com>

Never mind, just realized a volatile field set before the latch is
counted down is enough

On Tue, Oct 20, 2009 at 2:09 AM, Paulo Levi <i30817 at gmail.com> wrote:
> So i have a master thread that has to await a serial runnable (one for
> each word in a phrase) to end.
> I was thinking of using a CountdownLatch instead of the wait/notify
> i'm using now (and has a bug i'm trying to find),
> But i don't see a way to notify the latch (and the master ofcourse) in
> case of failure right?
>

From karlthepagan at gmail.com  Tue Oct 20 22:59:28 2009
From: karlthepagan at gmail.com (karlthepagan)
Date: Tue, 20 Oct 2009 19:59:28 -0700
Subject: [concurrency-interest] Moving CompletionHandler to
	java.util.concurrent
In-Reply-To: <25965204.post@talk.nabble.com>
References: <25958440.post@talk.nabble.com>
	<31f2a7bd0910191251ka608c0erb8fb02ca6efb3fd5@mail.gmail.com>
	<25965204.post@talk.nabble.com>
Message-ID: <39008ef30910201959x59d0f47al46d804fcfc42f1a1@mail.gmail.com>

As much as CompletionService and CompletionHandler are complimentary it
would be nice to see APIs that incorporate both of the implementation
patterns that they promote. I am doing this with the next rework of my
MessageFuture lib.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091020/a6567e4d/attachment.html>

From i30817 at gmail.com  Wed Oct 21 15:55:01 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Wed, 21 Oct 2009 20:55:01 +0100
Subject: [concurrency-interest] What is the reason that synchronousQueue
	doesn't allow null elements?
Message-ID: <212322090910211255x2387cb0do4bbe11639c27796d@mail.gmail.com>

Would be really handy not to have to have a sentinel value hanging
around everywhere.

From i30817 at gmail.com  Wed Oct 21 15:57:56 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Wed, 21 Oct 2009 20:57:56 +0100
Subject: [concurrency-interest] What is the reason that synchronousQueue
	doesn't allow null elements?
In-Reply-To: <212322090910211255x2387cb0do4bbe11639c27796d@mail.gmail.com>
References: <212322090910211255x2387cb0do4bbe11639c27796d@mail.gmail.com>
Message-ID: <212322090910211257s51cc082fqe5c7072f554be9e6@mail.gmail.com>

In fact it leads to things like this if the result of the take() is
not used locally:
if(timerException != null && !(timerException instanceof NOP)) //ugly!!!

Not as ugly as wait / notify though.
On Wed, Oct 21, 2009 at 8:55 PM, Paulo Levi <i30817 at gmail.com> wrote:
> Would be really handy not to have to have a sentinel value hanging
> around everywhere.
>

From tim at peierls.net  Wed Oct 21 16:13:40 2009
From: tim at peierls.net (Tim Peierls)
Date: Wed, 21 Oct 2009 16:13:40 -0400
Subject: [concurrency-interest] What is the reason that synchronousQueue
	doesn't allow null elements?
In-Reply-To: <212322090910211257s51cc082fqe5c7072f554be9e6@mail.gmail.com>
References: <212322090910211255x2387cb0do4bbe11639c27796d@mail.gmail.com>
	<212322090910211257s51cc082fqe5c7072f554be9e6@mail.gmail.com>
Message-ID: <63b4e4050910211313nd62449dqd7ba8f4dcdfbc249@mail.gmail.com>

The BlockingQueue interface (not just SynchronousQueue) forbids null values
because null is used as a special return value by the timed and untimed
poll() methods to indicate that the queue has no element available; if nulls
were allowed, a null return value from poll() would be ambiguous: Does it
mean that a null value was available or that no value was available?

The javadocs for Queue also strongly discourage the use of null values, even
though it isn't actually forbidden (because some Queue implementations allow
nulls, e.g., LinkedList).

It should be easy, however, to write convenience methods that encapsulate
the use of a sentinel value so that client code doesn't have to deal with it
in repetitive and ugly ways.

--tim

On Wed, Oct 21, 2009 at 3:57 PM, Paulo Levi <i30817 at gmail.com> wrote:

> In fact it leads to things like this if the result of the take() is
> not used locally:
> if(timerException != null && !(timerException instanceof NOP)) //ugly!!!
>
> Not as ugly as wait / notify though.
> On Wed, Oct 21, 2009 at 8:55 PM, Paulo Levi <i30817 at gmail.com> wrote:
> > Would be really handy not to have to have a sentinel value hanging
> > around everywhere.
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091021/2a770d76/attachment.html>

From bryan at systap.com  Wed Oct 21 16:24:25 2009
From: bryan at systap.com (Bryan Thompson)
Date: Wed, 21 Oct 2009 15:24:25 -0500
Subject: [concurrency-interest] Concurrent approximate LRU?
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED439A4B1327E@AUSP01VMBX06.collaborationhost.net>

Hello,

We have a database with many different backing stores all of which compete to buffer ram in a shared LRU cache policy.  We have implemented this using an outer concurrent hash map providing indirection to a collection of inner hash maps (we use LinkedHashMap here for better performance on its iterator, but we do not use its ability to impose an LRU ordering because the order must be shared across all inner cache instances).  The keys are Long's.  The values are elements in a double-linked list.  If there is an LRU eviction then the record is evicted from the inner hash map corresponding to the store.  This spreads out the memory constraint across the backing stores based on access.

The problem with this approach is that the double-linked list is not thread-safe so we have to obtain a lock when testing the cache put adding to the cache and hold it across that operation.  This is about a 10% performance penalty involved.

Unfortunately, the ConcurrentHashMap does not allow us to impose an access policy, or we could use a single ConcurrentHashMap instance and use the store's identifier and the Long address of the record as a composite key and all would be good.

I am wondering if there is a way to maintain an approximate LRU ordering across the elements in these different caches?  

Thanks in advance,

-bryan

From i30817 at gmail.com  Wed Oct 21 16:30:15 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Wed, 21 Oct 2009 21:30:15 +0100
Subject: [concurrency-interest] What is the reason that synchronousQueue
	doesn't allow null elements?
In-Reply-To: <63b4e4050910211313nd62449dqd7ba8f4dcdfbc249@mail.gmail.com>
References: <212322090910211255x2387cb0do4bbe11639c27796d@mail.gmail.com> 
	<212322090910211257s51cc082fqe5c7072f554be9e6@mail.gmail.com> 
	<63b4e4050910211313nd62449dqd7ba8f4dcdfbc249@mail.gmail.com>
Message-ID: <212322090910211330h53c830fkf94b8218fb1063ed@mail.gmail.com>

That would be much appreciated.

From tim at peierls.net  Wed Oct 21 16:35:31 2009
From: tim at peierls.net (Tim Peierls)
Date: Wed, 21 Oct 2009 16:35:31 -0400
Subject: [concurrency-interest] What is the reason that synchronousQueue
	doesn't allow null elements?
In-Reply-To: <212322090910211330h53c830fkf94b8218fb1063ed@mail.gmail.com>
References: <212322090910211255x2387cb0do4bbe11639c27796d@mail.gmail.com>
	<212322090910211257s51cc082fqe5c7072f554be9e6@mail.gmail.com>
	<63b4e4050910211313nd62449dqd7ba8f4dcdfbc249@mail.gmail.com>
	<212322090910211330h53c830fkf94b8218fb1063ed@mail.gmail.com>
Message-ID: <63b4e4050910211335q19df18c4l9609bdd4bd56abb0@mail.gmail.com>

On Wed, Oct 21, 2009 at 4:30 PM, Paulo Levi <i30817 at gmail.com> wrote:

> That would be much appreciated.
>

Actually, I meant that it should be easy for *you* to write them. :-)  But
if you come up with something you think is generally useful, please share it
with us.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091021/2e60cf29/attachment.html>

From i30817 at gmail.com  Wed Oct 21 21:32:53 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Thu, 22 Oct 2009 02:32:53 +0100
Subject: [concurrency-interest] Daemon threads that don't die?
Message-ID: <212322090910211832g65774965s45b79afc18f38d68@mail.gmail.com>

Does ScheduledThreadPoolExecutor need allow core threads time out to
actually finish even if i set a thread factory that sets daemon true
by default?

From martinrb at google.com  Wed Oct 21 21:56:28 2009
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 21 Oct 2009 18:56:28 -0700
Subject: [concurrency-interest] Daemon threads that don't die?
In-Reply-To: <212322090910211832g65774965s45b79afc18f38d68@mail.gmail.com>
References: <212322090910211832g65774965s45b79afc18f38d68@mail.gmail.com>
Message-ID: <1ccfd1c10910211856rb3b11c1p44f449a3c0c74603@mail.gmail.com>

http://download.java.net/jdk7/docs/api/java/util/concurrent/ScheduledThreadPoolExecutor.html#getContinueExistingPeriodicTasksAfterShutdownPolicy()
http://download.java.net/jdk7/docs/api/java/util/concurrent/ScheduledThreadPoolExecutor.html#getExecuteExistingDelayedTasksAfterShutdownPolicy()

On Wed, Oct 21, 2009 at 18:32, Paulo Levi <i30817 at gmail.com> wrote:
> Does ScheduledThreadPoolExecutor need allow core threads time out to
> actually finish even if i set a thread factory that sets daemon true
> by default?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From davidcholmes at aapt.net.au  Thu Oct 22 00:07:15 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 22 Oct 2009 14:07:15 +1000
Subject: [concurrency-interest] Daemon threads that don't die?
In-Reply-To: <1ccfd1c10910211856rb3b11c1p44f449a3c0c74603@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEOHIDAA.davidcholmes@aapt.net.au>

But if the threads are daemons then the "after shutdown" policy won't even
come into play - the VM will simply terminate once only daemon threads exist
in the VM.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Martin
> Buchholz
> Sent: Thursday, 22 October 2009 11:56 AM
> To: Paulo Levi
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Daemon threads that don't die?
>
>
>
> http://download.java.net/jdk7/docs/api/java/util/concurrent/Schedu
> ledThreadPoolExecutor.html#getContinueExistingPeriodicTasksAfterSh
> utdownPolicy()
> http://download.java.net/jdk7/docs/api/java/util/concurrent/Schedu
ledThreadPoolExecutor.html#getExecuteExistingDelayedTasksAfterShutdownPolicy
()

On Wed, Oct 21, 2009 at 18:32, Paulo Levi <i30817 at gmail.com> wrote:
> Does ScheduledThreadPoolExecutor need allow core threads time out to
> actually finish even if i set a thread factory that sets daemon true
> by default?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From reachbach at gmail.com  Thu Oct 22 02:42:04 2009
From: reachbach at gmail.com (Bharath Ravi Kumar)
Date: Thu, 22 Oct 2009 12:12:04 +0530
Subject: [concurrency-interest] Happens before,
	thread start and preceding actions
Message-ID: <76b5ba080910212342r71ed4fc8rd0d75a544e0a319a@mail.gmail.com>

Hi,

The JSR 133 FAQ section on happens before (
http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#synchronization<http://www.cs.umd.edu/%7Epugh/java/memoryModel/jsr-133-faq.html#synchronization>)
states that there is a happens-before relationship between the start of a
thread and any action that happens within the thread. But what about the
relationship between actions that occur before the start of a thread and
those that occur after the start() of the thread? For instance, if there is
an assignment to an instance-level field or a local (final) variable, and a
runnable started off on a different thread within the same method following
the assignment, are there visibility guarantees? To illustrate the point:

class Foo{
    private Object bar;
    void doSomething(){
        bar = new Object();
        final Object baz = new Object();
        Thread thread = new Thread (new Runnable(){
             public void run(){
                 if(bar != null){
                     //do Something with baz
                     read(baz);
                 }
             }
        });
    }
}
Is it safe to assume that both bar and baz are visible to all actions within
the new thread?
 (I'd expect there to be visibility guarantees for baz, at the minimum)

Thanks,
Bharath
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091022/7a2fd166/attachment.html>

From alarmnummer at gmail.com  Thu Oct 22 04:55:55 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 22 Oct 2009 10:55:55 +0200
Subject: [concurrency-interest] Happens before,
	thread start and preceding 	actions
In-Reply-To: <76b5ba080910212342r71ed4fc8rd0d75a544e0a319a@mail.gmail.com>
References: <76b5ba080910212342r71ed4fc8rd0d75a544e0a319a@mail.gmail.com>
Message-ID: <1466c1d60910220155t53a1c518n60a5adf8f6433e00@mail.gmail.com>

Hi Bharath Ravi Kumar,


You have forgotten to start the thread, so there are no issues because
there is no multithreading, but lets
start the thread to give an answer to your question.

My knowledge is not perfect, so it could be that I'm making an error.

But AFAIK:

There is a happens before relation between the actions before the
thread.start and the thread.start (Program Order Rule)

There is a happens before relation between Thread.start and Thread.run
(Thread.start rule)

There is a happens before relation between Thread.run and the
instructions in the run (program order)

Since the happens before relation is transitive, you can say:

There is a happens before relation between the actions before the
thread.start and the instructions in the run method.

So I agree with you that it is save to assume that both bar and baz
are visible to all actions within the new thread.

PS:
If bar is going to change after the thread is started, all bets are off.


On Thu, Oct 22, 2009 at 8:42 AM, Bharath Ravi Kumar <reachbach at gmail.com> wrote:
> Hi,
>
> The JSR 133 FAQ section on happens before
> (http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#synchronization)
> states that there is a happens-before relationship between the start of a
> thread and any action that happens within the thread. But what about the
> relationship between actions that occur before the start of a thread and
> those that occur after the start() of the thread? For instance, if there is
> an assignment to an instance-level field or a local (final) variable, and a
> runnable started off on a different thread within the same method following
> the assignment, are there visibility guarantees? To illustrate the point:
>
> class Foo{
> ??? private Object bar;
> ??? void doSomething(){
> ??????? bar = new Object();
> ??????? final Object baz = new Object();
> ??????? Thread thread = new Thread (new Runnable(){
> ???????????? public void run(){
> ???????????????? if(bar != null){
> ???????????????????? //do Something with baz
> ???????????????????? read(baz);
> ???????????????? }
> ???????????? }
> ??????? });
> ??? }
> }
> Is it safe to assume that both bar and baz are visible to all actions within
> the new thread?
> ?(I'd expect there to be visibility guarantees for baz, at the minimum)
>
> Thanks,
> Bharath
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From i30817 at gmail.com  Thu Oct 22 05:15:35 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Thu, 22 Oct 2009 10:15:35 +0100
Subject: [concurrency-interest] What is the reason that synchronousQueue
	doesn't allow null elements?
In-Reply-To: <63b4e4050910211335q19df18c4l9609bdd4bd56abb0@mail.gmail.com>
References: <212322090910211255x2387cb0do4bbe11639c27796d@mail.gmail.com> 
	<212322090910211257s51cc082fqe5c7072f554be9e6@mail.gmail.com> 
	<63b4e4050910211313nd62449dqd7ba8f4dcdfbc249@mail.gmail.com> 
	<212322090910211330h53c830fkf94b8218fb1063ed@mail.gmail.com> 
	<63b4e4050910211335q19df18c4l9609bdd4bd56abb0@mail.gmail.com>
Message-ID: <212322090910220215p31fa565foecf9c25abd7d32c0@mail.gmail.com>

Well i found something strange... Is that good enough?
The shutdownnow() method doesn't appear to call interrupt only once.
I had to make this method:
        private RETURNSTATE unInterruptibleTake() {
            while (true) {
                try {
                    Thread.currentThread().interrupt();
                    return channel.take();
                } catch (InterruptedException ex1) {
                    //This should be cooperative?
                }
            }
        }

To make a SynchronousQueue inside a ThreadPoolExecutor, synchronizing
with another thread not flip out (not quite sure yet).




On Wed, Oct 21, 2009 at 9:35 PM, Tim Peierls <tim at peierls.net> wrote:
> On Wed, Oct 21, 2009 at 4:30 PM, Paulo Levi <i30817 at gmail.com> wrote:
>>
>> That would be much appreciated.
>
> Actually, I meant that it should be easy for *you* to write them. :-) ?But
> if you come up with something you think is generally useful, please share it
> with us.
> --tim


From reachbach at gmail.com  Thu Oct 22 07:30:42 2009
From: reachbach at gmail.com (Bharath Ravi Kumar)
Date: Thu, 22 Oct 2009 17:00:42 +0530
Subject: [concurrency-interest] Happens before,
	thread start and preceding 	actions
In-Reply-To: <1466c1d60910220155t53a1c518n60a5adf8f6433e00@mail.gmail.com>
References: <76b5ba080910212342r71ed4fc8rd0d75a544e0a319a@mail.gmail.com>
	<1466c1d60910220155t53a1c518n60a5adf8f6433e00@mail.gmail.com>
Message-ID: <76b5ba080910220430j2207397bp2615e09dbe3923b3@mail.gmail.com>

My apologies.  I meant to indicate that the thread was indeed being started.

-Bharath

On Thu, Oct 22, 2009 at 2:25 PM, Peter Veentjer <alarmnummer at gmail.com>wrote:

> Hi Bharath Ravi Kumar,
>
>
> You have forgotten to start the thread, so there are no issues because
> there is no multithreading, but lets
> start the thread to give an answer to your question.
>
> My knowledge is not perfect, so it could be that I'm making an error.
>
> But AFAIK:
>
> There is a happens before relation between the actions before the
> thread.start and the thread.start (Program Order Rule)
>
> There is a happens before relation between Thread.start and Thread.run
> (Thread.start rule)
>
> There is a happens before relation between Thread.run and the
> instructions in the run (program order)
>
> Since the happens before relation is transitive, you can say:
>
> There is a happens before relation between the actions before the
> thread.start and the instructions in the run method.
>
> So I agree with you that it is save to assume that both bar and baz
> are visible to all actions within the new thread.
>
> PS:
> If bar is going to change after the thread is started, all bets are off.
>
>
> On Thu, Oct 22, 2009 at 8:42 AM, Bharath Ravi Kumar <reachbach at gmail.com>
> wrote:
> > Hi,
> >
> > The JSR 133 FAQ section on happens before
> > (
> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#synchronization<http://www.cs.umd.edu/%7Epugh/java/memoryModel/jsr-133-faq.html#synchronization>
> )
> > states that there is a happens-before relationship between the start of a
> > thread and any action that happens within the thread. But what about the
> > relationship between actions that occur before the start of a thread and
> > those that occur after the start() of the thread? For instance, if there
> is
> > an assignment to an instance-level field or a local (final) variable, and
> a
> > runnable started off on a different thread within the same method
> following
> > the assignment, are there visibility guarantees? To illustrate the point:
> >
> > class Foo{
> >     private Object bar;
> >     void doSomething(){
> >         bar = new Object();
> >         final Object baz = new Object();
> >         Thread thread = new Thread (new Runnable(){
> >              public void run(){
> >                  if(bar != null){
> >                      //do Something with baz
> >                      read(baz);
> >                  }
> >              }
> >         });
> >     }
> > }
> > Is it safe to assume that both bar and baz are visible to all actions
> within
> > the new thread?
> >  (I'd expect there to be visibility guarantees for baz, at the minimum)
> >
> > Thanks,
> > Bharath
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091022/45444e39/attachment.html>

From reachbach at gmail.com  Thu Oct 22 07:41:38 2009
From: reachbach at gmail.com (Bharath Ravi Kumar)
Date: Thu, 22 Oct 2009 17:11:38 +0530
Subject: [concurrency-interest] Happens before,
	thread start and preceding 	actions
In-Reply-To: <1466c1d60910220155t53a1c518n60a5adf8f6433e00@mail.gmail.com>
References: <76b5ba080910212342r71ed4fc8rd0d75a544e0a319a@mail.gmail.com>
	<1466c1d60910220155t53a1c518n60a5adf8f6433e00@mail.gmail.com>
Message-ID: <76b5ba080910220441p383c9e98n80844e9192899049@mail.gmail.com>

Hi Peter,

Thanks for the clarification. I did expect that assignments made to bar and
baz before starting the thread would be visible to actions within run(). And
yes, subsequent assignments to bar aren't guaranteed to be visible to
actions in the new thread (in the absence of memory barrier instructions).

-Bharath

On Thu, Oct 22, 2009 at 2:25 PM, Peter Veentjer <alarmnummer at gmail.com>wrote:

> Hi Bharath Ravi Kumar,
>
>
> You have forgotten to start the thread, so there are no issues because
> there is no multithreading, but lets
> start the thread to give an answer to your question.
>
> My knowledge is not perfect, so it could be that I'm making an error.
>
> But AFAIK:
>
> There is a happens before relation between the actions before the
> thread.start and the thread.start (Program Order Rule)
>
> There is a happens before relation between Thread.start and Thread.run
> (Thread.start rule)
>
> There is a happens before relation between Thread.run and the
> instructions in the run (program order)
>
> Since the happens before relation is transitive, you can say:
>
> There is a happens before relation between the actions before the
> thread.start and the instructions in the run method.
>
> So I agree with you that it is save to assume that both bar and baz
> are visible to all actions within the new thread.
>
> PS:
> If bar is going to change after the thread is started, all bets are off.
>
>
> On Thu, Oct 22, 2009 at 8:42 AM, Bharath Ravi Kumar <reachbach at gmail.com>
> wrote:
> > Hi,
> >
> > The JSR 133 FAQ section on happens before
> > (
> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#synchronization<http://www.cs.umd.edu/%7Epugh/java/memoryModel/jsr-133-faq.html#synchronization>
> )
> > states that there is a happens-before relationship between the start of a
> > thread and any action that happens within the thread. But what about the
> > relationship between actions that occur before the start of a thread and
> > those that occur after the start() of the thread? For instance, if there
> is
> > an assignment to an instance-level field or a local (final) variable, and
> a
> > runnable started off on a different thread within the same method
> following
> > the assignment, are there visibility guarantees? To illustrate the point:
> >
> > class Foo{
> >     private Object bar;
> >     void doSomething(){
> >         bar = new Object();
> >         final Object baz = new Object();
> >         Thread thread = new Thread (new Runnable(){
> >              public void run(){
> >                  if(bar != null){
> >                      //do Something with baz
> >                      read(baz);
> >                  }
> >              }
> >         });
> >     }
> > }
> > Is it safe to assume that both bar and baz are visible to all actions
> within
> > the new thread?
> >  (I'd expect there to be visibility guarantees for baz, at the minimum)
> >
> > Thanks,
> > Bharath
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091022/a590d536/attachment-0001.html>

From tim at peierls.net  Thu Oct 22 10:01:18 2009
From: tim at peierls.net (Tim Peierls)
Date: Thu, 22 Oct 2009 10:01:18 -0400
Subject: [concurrency-interest] What is the reason that synchronousQueue
	doesn't allow null elements?
In-Reply-To: <212322090910220215p31fa565foecf9c25abd7d32c0@mail.gmail.com>
References: <212322090910211255x2387cb0do4bbe11639c27796d@mail.gmail.com>
	<212322090910211257s51cc082fqe5c7072f554be9e6@mail.gmail.com>
	<63b4e4050910211313nd62449dqd7ba8f4dcdfbc249@mail.gmail.com>
	<212322090910211330h53c830fkf94b8218fb1063ed@mail.gmail.com>
	<63b4e4050910211335q19df18c4l9609bdd4bd56abb0@mail.gmail.com>
	<212322090910220215p31fa565foecf9c25abd7d32c0@mail.gmail.com>
Message-ID: <63b4e4050910220701uf05bc10g57311a94b7c553ff@mail.gmail.com>

ThreadPoolExecutor.shutdownNow() calls Thread.interrupt() once on each pool
thread.

Are you trying to prevent the shutdownNow call from interrupting a
BlockingQueue.take() in a pool thread? The code snippet you gave looks
wrong, and the whole thing with sentinel values that you started with sounds
very fragile.

Can you step back from the current implementation details and describe what
you're trying to accomplish?

--tim

On Thu, Oct 22, 2009 at 5:15 AM, Paulo Levi <i30817 at gmail.com> wrote:

> Well i found something strange... Is that good enough?
> The shutdownnow() method doesn't appear to call interrupt only once.
> I had to make this method:
>        private RETURNSTATE unInterruptibleTake() {
>            while (true) {
>                try {
>                    Thread.currentThread().interrupt();
>                    return channel.take();
>                } catch (InterruptedException ex1) {
>                    //This should be cooperative?
>                }
>            }
>        }
>
> To make a SynchronousQueue inside a ThreadPoolExecutor, synchronizing
> with another thread not flip out (not quite sure yet).
>
>
>
>
> On Wed, Oct 21, 2009 at 9:35 PM, Tim Peierls <tim at peierls.net> wrote:
> > On Wed, Oct 21, 2009 at 4:30 PM, Paulo Levi <i30817 at gmail.com> wrote:
> >>
> >> That would be much appreciated.
> >
> > Actually, I meant that it should be easy for *you* to write them. :-)
>  But
> > if you come up with something you think is generally useful, please share
> it
> > with us.
> > --tim
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091022/4002f128/attachment.html>

From i30817 at gmail.com  Thu Oct 22 10:28:42 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Thu, 22 Oct 2009 15:28:42 +0100
Subject: [concurrency-interest] What is the reason that synchronousQueue
	doesn't allow null elements?
In-Reply-To: <63b4e4050910220701uf05bc10g57311a94b7c553ff@mail.gmail.com>
References: <212322090910211255x2387cb0do4bbe11639c27796d@mail.gmail.com> 
	<212322090910211257s51cc082fqe5c7072f554be9e6@mail.gmail.com> 
	<63b4e4050910211313nd62449dqd7ba8f4dcdfbc249@mail.gmail.com> 
	<212322090910211330h53c830fkf94b8218fb1063ed@mail.gmail.com> 
	<63b4e4050910211335q19df18c4l9609bdd4bd56abb0@mail.gmail.com> 
	<212322090910220215p31fa565foecf9c25abd7d32c0@mail.gmail.com> 
	<63b4e4050910220701uf05bc10g57311a94b7c553ff@mail.gmail.com>
Message-ID: <212322090910220728m617fa3dfv7299587cc83062c6@mail.gmail.com>

Err, i was trying to accomplish a bug. its supposed to be interrupt[ed]().

From robert.nicholson at gmail.com  Sun Oct 25 00:11:01 2009
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Sat, 24 Oct 2009 23:11:01 -0500
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
	this requirement?
Message-ID: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>

I have a need to process messages off a JMS queue in batches but yet  
still not wait around for messages to arrive before I can flush a  
batch. So occasionally I will flush a batch of records that is less  
than my batch size long. I do this by arranging for a one-time  
scheduled task to run every time I receive a message. The task will  
run in roughly 10 seconds from the time it's scheduled. If in the  
meantime another message arrives in less than 1 second I will cancel  
the previously scheduled task and arrange for another one. etc etc  
etc. So you can see the difference b/w when I scheduled the most  
recent task and the time of receipt of the most recent message  
determines whether or not I will cancel the most recently scheduled  
task. The idea being is that if I'm receiving a steady flow of  
incoming messages I don't need to rely on the scheduled tasks as I'll  
hit my batch size and flush the batch anyway.

Can somebody comment on the above logic and whether it's an  
appropriate use of scheduled executor.


From joe.bowbeer at gmail.com  Sun Oct 25 13:31:42 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 25 Oct 2009 10:31:42 -0700
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
	this requirement?
In-Reply-To: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
Message-ID: <31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>

Scheduled Executor is a thread-pool re-implementation of java.util.Timer.
If you need multiple threads servicing periodic tasks, that is, if your
periodic tasks should execute concurrently, then Scheduled Executor is
prescribed.

You can accomplish what you are describing with Timer or ScheduledExecutor.
Your code needs to maintain a current cleanup task that is scheduled to
execute in 10 seconds.  New messages would cancel the existing cleanup task
and schedule a new task.

Alternatively, you could schedule a single repeating task that executes
every 10 seconds, and have it do nothing if the last message was queued less
than, say, 1 second prior to its execution.

Joe

On Sat, Oct 24, 2009 at 9:11 PM, Robert Nicholson wrote:

> I have a need to process messages off a JMS queue in batches but yet still
> not wait around for messages to arrive before I can flush a batch. So
> occasionally I will flush a batch of records that is less than my batch size
> long. I do this by arranging for a one-time scheduled task to run every time
> I receive a message. The task will run in roughly 10 seconds from the time
> it's scheduled. If in the meantime another message arrives in less than 1
> second I will cancel the previously scheduled task and arrange for another
> one. etc etc etc. So you can see the difference b/w when I scheduled the
> most recent task and the time of receipt of the most recent message
> determines whether or not I will cancel the most recently scheduled task.
> The idea being is that if I'm receiving a steady flow of incoming messages I
> don't need to rely on the scheduled tasks as I'll hit my batch size and
> flush the batch anyway.
>
> Can somebody comment on the above logic and whether it's an appropriate use
> of scheduled executor.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091025/f590ed6e/attachment.html>

From gregg at cytetech.com  Mon Oct 26 09:27:16 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 26 Oct 2009 08:27:16 -0500
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
 this requirement?
In-Reply-To: <31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
	<31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>
Message-ID: <4AE5A3B4.1060803@cytetech.com>

Many times when I need to do this, and I don't otherwise have a STPE around, I 
just create a class that does this.  java.util.Timer was not designed for 
cancelled tasks at the beginning and it took several iterations (JDK versions) 
for it to get fixed to sorta work, and it still had a silly purge mechanism 
instead of taking care of this issue by itself.  I stopped using it because of 
the garbage it creates for mostly cancel situations like you described.

Gregg Wonderly

Joe Bowbeer wrote:
> Scheduled Executor is a thread-pool re-implementation of 
> java.util.Timer.  If you need multiple threads servicing periodic tasks, 
> that is, if your periodic tasks should execute concurrently, then 
> Scheduled Executor is prescribed.
> 
> You can accomplish what you are describing with Timer or 
> ScheduledExecutor.  Your code needs to maintain a current cleanup task 
> that is scheduled to execute in 10 seconds.  New messages would cancel 
> the existing cleanup task and schedule a new task.
> 
> Alternatively, you could schedule a single repeating task that executes 
> every 10 seconds, and have it do nothing if the last message was queued 
> less than, say, 1 second prior to its execution.
> 
> Joe
> 
> On Sat, Oct 24, 2009 at 9:11 PM, Robert Nicholson wrote:
> 
>     I have a need to process messages off a JMS queue in batches but yet
>     still not wait around for messages to arrive before I can flush a
>     batch. So occasionally I will flush a batch of records that is less
>     than my batch size long. I do this by arranging for a one-time
>     scheduled task to run every time I receive a message. The task will
>     run in roughly 10 seconds from the time it's scheduled. If in the
>     meantime another message arrives in less than 1 second I will cancel
>     the previously scheduled task and arrange for another one. etc etc
>     etc. So you can see the difference b/w when I scheduled the most
>     recent task and the time of receipt of the most recent message
>     determines whether or not I will cancel the most recently scheduled
>     task. The idea being is that if I'm receiving a steady flow of
>     incoming messages I don't need to rely on the scheduled tasks as
>     I'll hit my batch size and flush the batch anyway.
> 
>     Can somebody comment on the above logic and whether it's an
>     appropriate use of scheduled executor.
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From joe.bowbeer at gmail.com  Mon Oct 26 16:25:39 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 26 Oct 2009 13:25:39 -0700
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
	this requirement?
In-Reply-To: <4AE5A3B4.1060803@cytetech.com>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
	<31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>
	<4AE5A3B4.1060803@cytetech.com>
Message-ID: <31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com>

A purge() method was added to Timer in Java 5 -- at the same time that
(Scheduled)ThreadPoolExecutor with its purge() method was added.

The backlog of delayed tasks in this case will resemble the input queue
delayed by 10 secs.  Purging canceled tasks may be necessary, depending on
how many tasks are generated in 10 seconds, and how fast the timer thread
can sift through them on its own.

I like the simplicity of the single, repetitive task that only functions if
there are messages on the queue that have been sitting there for a while.

Joe


On Mon, Oct 26, 2009 at 6:27 AM, Gregg Wonderly wrote:

> Many times when I need to do this, and I don't otherwise have a STPE
> around, I just create a class that does this.  java.util.Timer was not
> designed for cancelled tasks at the beginning and it took several iterations
> (JDK versions) for it to get fixed to sorta work, and it still had a silly
> purge mechanism instead of taking care of this issue by itself.  I stopped
> using it because of the garbage it creates for mostly cancel situations like
> you described.
>
> Gregg Wonderly
>
> Joe Bowbeer wrote:
>
>> Scheduled Executor is a thread-pool re-implementation of java.util.Timer.
>>  If you need multiple threads servicing periodic tasks, that is, if your
>> periodic tasks should execute concurrently, then Scheduled Executor is
>> prescribed.
>>
>> You can accomplish what you are describing with Timer or
>> ScheduledExecutor.  Your code needs to maintain a current cleanup task that
>> is scheduled to execute in 10 seconds.  New messages would cancel the
>> existing cleanup task and schedule a new task.
>>
>> Alternatively, you could schedule a single repeating task that executes
>> every 10 seconds, and have it do nothing if the last message was queued less
>> than, say, 1 second prior to its execution.
>>
>> Joe
>>
>> On Sat, Oct 24, 2009 at 9:11 PM, Robert Nicholson wrote:
>>
>>    I have a need to process messages off a JMS queue in batches but yet
>>    still not wait around for messages to arrive before I can flush a
>>    batch. So occasionally I will flush a batch of records that is less
>>    than my batch size long. I do this by arranging for a one-time
>>    scheduled task to run every time I receive a message. The task will
>>    run in roughly 10 seconds from the time it's scheduled. If in the
>>    meantime another message arrives in less than 1 second I will cancel
>>    the previously scheduled task and arrange for another one. etc etc
>>    etc. So you can see the difference b/w when I scheduled the most
>>    recent task and the time of receipt of the most recent message
>>    determines whether or not I will cancel the most recently scheduled
>>    task. The idea being is that if I'm receiving a steady flow of
>>    incoming messages I don't need to rely on the scheduled tasks as
>>    I'll hit my batch size and flush the batch anyway.
>>
>>    Can somebody comment on the above logic and whether it's an
>>    appropriate use of scheduled executor.
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091026/39dc3baf/attachment.html>

From gregg at cytetech.com  Mon Oct 26 19:16:21 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 26 Oct 2009 18:16:21 -0500
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
 this requirement?
In-Reply-To: <31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
	<31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>
	<4AE5A3B4.1060803@cytetech.com>
	<31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com>
Message-ID: <4AE62DC5.8090404@cytetech.com>

Joe Bowbeer wrote:
> A purge() method was added to Timer in Java 5 -- at the same time that 
> (Scheduled)ThreadPoolExecutor with its purge() method was added.
> 
> The backlog of delayed tasks in this case will resemble the input queue 
> delayed by 10 secs.  Purging canceled tasks may be necessary, depending 
> on how many tasks are generated in 10 seconds, and how fast the timer 
> thread can sift through them on its own.
> 
> I like the simplicity of the single, repetitive task that only functions 
> if there are messages on the queue that have been sitting there for a while.

This is the bit of the API that I just don't understand the logic of design.  It 
takes the same amount of time to purge an entry from the list rather a 
background thread does it post cancel(), or the calling thread does it at 
cancel().  When cancel() is use heavily, purge's actions need to to be used 
heavily.  When cancel is used lightly, purge's actions can be used occasionally.

If purge's actions were performed each time cancel() was called, then in the 
occasional use, it would have limited impact on the calling thread.  When cancel 
is used heavily, then purge has to be used unconditionally, and this provides 
the needed resolution of garbage build up.

So, I don't understand how not performing the purge actions, every time cancel 
is called can really be of any benefit.

The data structure design of the "list" may be a factor, but it would seem to be 
to be more beneficial to fix that data structure and purge() on cancel(), no 
matter what, compared to the surprises that the API can create today when 
software designed for own use of the API changes in use, and the API then 
creates problems because it doesn't automatically account for that change.

Gregg Wonderly

From tim at peierls.net  Tue Oct 27 10:03:06 2009
From: tim at peierls.net (Tim Peierls)
Date: Tue, 27 Oct 2009 10:03:06 -0400
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
	this requirement?
In-Reply-To: <4AE62DC5.8090404@cytetech.com>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
	<31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>
	<4AE5A3B4.1060803@cytetech.com>
	<31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com>
	<4AE62DC5.8090404@cytetech.com>
Message-ID: <63b4e4050910270703u52c6a98ah4561560bfe708cbd@mail.gmail.com>

Looks like a setRemoveOnCancel() method is being added for Java 7. I think
it makes cancel() go from O(1) to O(log n). In contrast, purge() is linear
in the size of the queue when there is no interference, but potentially
quadratic if there's lots of interference. So you can make the time-memory
tradeoff decision per STPE instance.

--tim

On Mon, Oct 26, 2009 at 7:16 PM, Gregg Wonderly <gregg at cytetech.com> wrote:

> Joe Bowbeer wrote:
>
>> A purge() method was added to Timer in Java 5 -- at the same time that
>> (Scheduled)ThreadPoolExecutor with its purge() method was added.
>>
>> The backlog of delayed tasks in this case will resemble the input queue
>> delayed by 10 secs.  Purging canceled tasks may be necessary, depending on
>> how many tasks are generated in 10 seconds, and how fast the timer thread
>> can sift through them on its own.
>>
>> I like the simplicity of the single, repetitive task that only functions
>> if there are messages on the queue that have been sitting there for a while.
>>
>
> This is the bit of the API that I just don't understand the logic of
> design.  It takes the same amount of time to purge an entry from the list
> rather a background thread does it post cancel(), or the calling thread does
> it at cancel().  When cancel() is use heavily, purge's actions need to to be
> used heavily.  When cancel is used lightly, purge's actions can be used
> occasionally.
>
> If purge's actions were performed each time cancel() was called, then in
> the occasional use, it would have limited impact on the calling thread.
>  When cancel is used heavily, then purge has to be used unconditionally, and
> this provides the needed resolution of garbage build up.
>
> So, I don't understand how not performing the purge actions, every time
> cancel is called can really be of any benefit.
>
> The data structure design of the "list" may be a factor, but it would seem
> to be to be more beneficial to fix that data structure and purge() on
> cancel(), no matter what, compared to the surprises that the API can create
> today when software designed for own use of the API changes in use, and the
> API then creates problems because it doesn't automatically account for that
> change.
>
> Gregg Wonderly
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091027/6b45b21f/attachment.html>

From tim at peierls.net  Tue Oct 27 13:36:02 2009
From: tim at peierls.net (Tim Peierls)
Date: Tue, 27 Oct 2009 13:36:02 -0400
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
	this requirement?
In-Reply-To: <4AE719CF.7060208@cox.net>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
	<31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>
	<4AE5A3B4.1060803@cytetech.com>
	<31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com>
	<4AE62DC5.8090404@cytetech.com>
	<63b4e4050910270703u52c6a98ah4561560bfe708cbd@mail.gmail.com>
	<4AE719CF.7060208@cox.net>
Message-ID: <63b4e4050910271036t1c6cd939o30242ffc9c322719@mail.gmail.com>

Are you talking about changing the BlockingQueue implementation in STPE from
a heap-based structure to a doubly-linked list? Would that do much better in
practice than STPE.setRemoveOrCancel() for the kind of use you're talking
about (enqueuing lots of tasks only to cancel them most of the time)? It
would mean a performance and concurrency hit for what I think of as more
typical uses.

Adding setRemoveOrCancel() isn't "patching over issues" -- it's addressing
an issue involving a particular pattern of use without breaking or imposing
a performance/contention hit on other uses. Yes, it's an API change, but
it's the best kind: adding methods to a concrete class.

If you turn on setRemoveOnCancel() and you're canceling 99% of the tasks you
enqueue, the queue size n doesn't get huge, so O(log n) is likely to be
small. Have you tried this and determined that it isn't sufficiently fast
for your intended use? If so, why not create a new implementation of
ScheduledExecutorService, possibly extending ThreadPoolExecutor? If SES is
too constrained for your intended use, how about creating a new interface
(and implementation)? If you do, please share it with us.

--tim

On Tue, Oct 27, 2009 at 12:03 PM, Gregg Wonderly <gergg at cox.net> wrote:

> I've written various timer management things over the years, and I've
> almost always used a time sorted doubly linked list.  Insert is usually
> implemented by searching from the end of the list for the insertion point
> guessing that there would not be a random set of times.  Searching from the
> end thus has advantages compared to the front, either one being an O(n)
> operation.
>
> Historically this has made insertion happen as an append and thus a O(1)
> operation, and the cancel is an O(1) remove.  I understand that the API
> could have been developed for a specific use case where cancel was not an
> issue, but more and more cases of people doing things that are more like 99%
> cancel seem to pop up in discussions like this.  It seems like it would be a
> good idea to rethink the internals of this class rather than continuing to
> patch over the issues with more API name space.
>
> Gregg Wonderly
>
> Tim Peierls wrote:
>
>> Looks like a setRemoveOnCancel() method is being added for Java 7. I think
>> it makes cancel() go from O(1) to O(log n). In contrast, purge() is linear
>> in the size of the queue when there is no interference, but potentially
>> quadratic if there's lots of interference. So you can make the time-memory
>> tradeoff decision per STPE instance.
>>
>> --tim
>>
>> On Mon, Oct 26, 2009 at 7:16 PM, Gregg Wonderly <gregg at cytetech.com<mailto:
>> gregg at cytetech.com>> wrote:
>>
>>    Joe Bowbeer wrote:
>>
>>        A purge() method was added to Timer in Java 5 -- at the same
>>        time that (Scheduled)ThreadPoolExecutor with its purge() method
>>        was added.
>>
>>        The backlog of delayed tasks in this case will resemble the
>>        input queue delayed by 10 secs.  Purging canceled tasks may be
>>        necessary, depending on how many tasks are generated in 10
>>        seconds, and how fast the timer thread can sift through them on
>>        its own.
>>
>>        I like the simplicity of the single, repetitive task that only
>>        functions if there are messages on the queue that have been
>>        sitting there for a while.
>>
>>
>>    This is the bit of the API that I just don't understand the logic of
>>    design.  It takes the same amount of time to purge an entry from the
>>    list rather a background thread does it post cancel(), or the
>>    calling thread does it at cancel().  When cancel() is use heavily,
>>    purge's actions need to to be used heavily.  When cancel is used
>>    lightly, purge's actions can be used occasionally.
>>
>>    If purge's actions were performed each time cancel() was called,
>>    then in the occasional use, it would have limited impact on the
>>    calling thread.  When cancel is used heavily, then purge has to be
>>    used unconditionally, and this provides the needed resolution of
>>    garbage build up.
>>
>>    So, I don't understand how not performing the purge actions, every
>>    time cancel is called can really be of any benefit.
>>
>>    The data structure design of the "list" may be a factor, but it
>>    would seem to be to be more beneficial to fix that data structure
>>    and purge() on cancel(), no matter what, compared to the surprises
>>    that the API can create today when software designed for own use of
>>    the API changes in use, and the API then creates problems because it
>>    doesn't automatically account for that change.
>>
>>    Gregg Wonderly
>>
>>    _______________________________________________
>>    Concurrency-interest mailing list
>>    Concurrency-interest at cs.oswego.edu
>>    <mailto:Concurrency-interest at cs.oswego.edu>
>>
>>    http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091027/28f77620/attachment.html>

From gergg at cox.net  Tue Oct 27 15:02:18 2009
From: gergg at cox.net (Gregg Wonderly)
Date: Tue, 27 Oct 2009 14:02:18 -0500
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
 this requirement?
In-Reply-To: <63b4e4050910271036t1c6cd939o30242ffc9c322719@mail.gmail.com>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>	<31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>	<4AE5A3B4.1060803@cytetech.com>	<31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com>	<4AE62DC5.8090404@cytetech.com>	<63b4e4050910270703u52c6a98ah4561560bfe708cbd@mail.gmail.com>	<4AE719CF.7060208@cox.net>
	<63b4e4050910271036t1c6cd939o30242ffc9c322719@mail.gmail.com>
Message-ID: <4AE743BA.3060504@cox.net>

Doubly linked list was just an example, any kind of sorted structure which 
includes a partitioned structure that can reduce O(n) to a more manageable N is 
fine with me.

One traversal to delete 100 entries (for the setRemoveOnCancel(false) case) vs 
100 traversals to delete 100 entries (for the setRemoveOnCancel(true) case) is 
the issue that I'm trying to focus on.  That's where data structure design makes 
the difference.  Constant time access is what should be happening I think.

The reason why I said setRemoveOnCancel() was patching over the issue was that 
it implies that there are performance trade offs for one use case vs the other. 
   Deferring that activity doesn't reduce the computational impact of finding 
and removing a single entry, it only changes when that impact occurs.  If it 
does make an impact for deleting multiple entries, one at a time, then that 
implies a data structure design issue to me.

Canceling 10/minute is different than 200/sec.  In either case, you need to be 
able to get the work done.  If setRemoveOnCancel() is viable for 10/minute but 
not for 200/second, then it seems odd that the method would exist.  It it's not 
viable for all N/second (i.e. it scales linearly or worse), then adding it is 
just a patch to try and make some cases work out better till the point that 
performance degrades and it is no longer a viable solution.

Gregg Wonderly

Tim Peierls wrote:
> Are you talking about changing the BlockingQueue implementation in STPE 
> from a heap-based structure to a doubly-linked list? Would that do much 
> better in practice than STPE.setRemoveOrCancel() for the kind of use 
> you're talking about (enqueuing lots of tasks only to cancel them most 
> of the time)? It would mean a performance and concurrency hit for what I 
> think of as more typical uses.
> 
> Adding setRemoveOrCancel() isn't "patching over issues" -- it's 
> addressing an issue involving a particular pattern of use without 
> breaking or imposing a performance/contention hit on other uses. Yes, 
> it's an API change, but it's the best kind: adding methods to a concrete 
> class.
> 
> If you turn on setRemoveOnCancel() and you're canceling 99% of the tasks 
> you enqueue, the queue size n doesn't get huge, so O(log n) is likely to 
> be small. Have you tried this and determined that it isn't sufficiently 
> fast for your intended use? If so, why not create a new implementation 
> of ScheduledExecutorService, possibly extending ThreadPoolExecutor? If 
> SES is too constrained for your intended use, how about creating a new 
> interface (and implementation)? If you do, please share it with us.
> 
> --tim
> 
> On Tue, Oct 27, 2009 at 12:03 PM, Gregg Wonderly <gergg at cox.net 
> <mailto:gergg at cox.net>> wrote:
> 
>     I've written various timer management things over the years, and
>     I've almost always used a time sorted doubly linked list.  Insert is
>     usually implemented by searching from the end of the list for the
>     insertion point guessing that there would not be a random set of
>     times.  Searching from the end thus has advantages compared to the
>     front, either one being an O(n) operation.
> 
>     Historically this has made insertion happen as an append and thus a
>     O(1) operation, and the cancel is an O(1) remove.  I understand that
>     the API could have been developed for a specific use case where
>     cancel was not an issue, but more and more cases of people doing
>     things that are more like 99% cancel seem to pop up in discussions
>     like this.  It seems like it would be a good idea to rethink the
>     internals of this class rather than continuing to patch over the
>     issues with more API name space.
> 
>     Gregg Wonderly
> 
>     Tim Peierls wrote:
> 
>         Looks like a setRemoveOnCancel() method is being added for Java
>         7. I think it makes cancel() go from O(1) to O(log n). In
>         contrast, purge() is linear in the size of the queue when there
>         is no interference, but potentially quadratic if there's lots of
>         interference. So you can make the time-memory tradeoff decision
>         per STPE instance.
> 
>         --tim
> 
>         On Mon, Oct 26, 2009 at 7:16 PM, Gregg Wonderly
>         <gregg at cytetech.com <mailto:gregg at cytetech.com>
>         <mailto:gregg at cytetech.com <mailto:gregg at cytetech.com>>> wrote:
> 
>            Joe Bowbeer wrote:
> 
>                A purge() method was added to Timer in Java 5 -- at the same
>                time that (Scheduled)ThreadPoolExecutor with its purge()
>         method
>                was added.
> 
>                The backlog of delayed tasks in this case will resemble the
>                input queue delayed by 10 secs.  Purging canceled tasks
>         may be
>                necessary, depending on how many tasks are generated in 10
>                seconds, and how fast the timer thread can sift through
>         them on
>                its own.
> 
>                I like the simplicity of the single, repetitive task that
>         only
>                functions if there are messages on the queue that have been
>                sitting there for a while.
> 
> 
>            This is the bit of the API that I just don't understand the
>         logic of
>            design.  It takes the same amount of time to purge an entry
>         from the
>            list rather a background thread does it post cancel(), or the
>            calling thread does it at cancel().  When cancel() is use
>         heavily,
>            purge's actions need to to be used heavily.  When cancel is used
>            lightly, purge's actions can be used occasionally.
> 
>            If purge's actions were performed each time cancel() was called,
>            then in the occasional use, it would have limited impact on the
>            calling thread.  When cancel is used heavily, then purge has
>         to be
>            used unconditionally, and this provides the needed resolution of
>            garbage build up.
> 
>            So, I don't understand how not performing the purge actions,
>         every
>            time cancel is called can really be of any benefit.
> 
>            The data structure design of the "list" may be a factor, but it
>            would seem to be to be more beneficial to fix that data structure
>            and purge() on cancel(), no matter what, compared to the
>         surprises
>            that the API can create today when software designed for own
>         use of
>            the API changes in use, and the API then creates problems
>         because it
>            doesn't automatically account for that change.
> 
>            Gregg Wonderly
> 
>            _______________________________________________
>            Concurrency-interest mailing list
>            Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>            <mailto:Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
> 
>            http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
>         ------------------------------------------------------------------------
> 
> 
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From tim at peierls.net  Tue Oct 27 16:40:13 2009
From: tim at peierls.net (Tim Peierls)
Date: Tue, 27 Oct 2009 16:40:13 -0400
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
	this requirement?
In-Reply-To: <4AE743BA.3060504@cox.net>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
	<31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>
	<4AE5A3B4.1060803@cytetech.com>
	<31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com>
	<4AE62DC5.8090404@cytetech.com>
	<63b4e4050910270703u52c6a98ah4561560bfe708cbd@mail.gmail.com>
	<4AE719CF.7060208@cox.net>
	<63b4e4050910271036t1c6cd939o30242ffc9c322719@mail.gmail.com>
	<4AE743BA.3060504@cox.net>
Message-ID: <63b4e4050910271340s6e256396m8c6790f1489032bb@mail.gmail.com>

On Tue, Oct 27, 2009 at 3:02 PM, Gregg Wonderly <gergg at cox.net> wrote:

> Doubly linked list was just an example, any kind of sorted structure which
> includes a partitioned structure that can reduce O(n) to a more manageable N
> is fine with me.


How is doubly-linked list a partitioned structure?



> One traversal to delete 100 entries (for the setRemoveOnCancel(false) case)
> vs 100 traversals to delete 100 entries (for the setRemoveOnCancel(true)
> case) is the issue that I'm trying to focus on.  That's where data structure
> design makes the difference.  Constant time access is what should be
> happening I think.


Do you have a sorted BlockingQueue implementation in mind that exhibits
constant time put, take, and removal of internal nodes? That would be handy.



> The reason why I said setRemoveOnCancel() was patching over the issue was
> that it implies that there are performance trade offs for one use case vs
> the other.


There *are* trade-offs: setRemoveOnCancel(false) doesn't cause the lock on
the shared queue to be acquired for each cancel; setRemoveOnCancel(true)
does. If your main concern is avoiding contention on the queue, then you'll
probably prefer the former. If your main concern is to avoid filling up the
queue with canceled tasks, you'll probably prefer the latter. If both of
these are major concerns, then you have decide which approach will work
best, periodic purge() or remove-on-cancel or some combination of these.

I don't see a way around these competing concerns.



> Deferring that activity doesn't reduce the computational impact of finding
> and removing a single entry, it only changes when that impact occurs.  If it
> does make an impact for deleting multiple entries, one at a time, then that
> implies a data structure design issue to me.


Removing a canceled task from the middle of the queue in STPE means holding
the lock for O(log n). Taking a canceled task off the end of the queue and
discarding it is O(1). Purging is O(n). Are there concurrent data structures
that can do better?



> Canceling 10/minute is different than 200/sec.  In either case, you need to
> be able to get the work done.  If setRemoveOnCancel() is viable for
> 10/minute but not for 200/second, then it seems odd that the method would
> exist.  It it's not viable for all N/second (i.e. it scales linearly or
> worse), then adding it is just a patch to try and make some cases work out
> better till the point that performance degrades and it is no longer a viable
> solution.


Try setRemoveOnCancel(true) in practice. If it doesn't scale, the right
response is to come up with an implementation of SES (or to come up with
another approach entirely) that solves your problem, not to force all STPE
users to risk contention for the queue lock on every cancel.

Unless, as I've been hinting, you have a data structure that balances both
sets of concerns without recourse to a configuration property like
remove-on-cancel, in which case ... let's see it! :-)

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091027/b38c6d9c/attachment-0001.html>

From gregg at cytetech.com  Tue Oct 27 17:53:21 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 27 Oct 2009 16:53:21 -0500
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
 this requirement?
In-Reply-To: <63b4e4050910271340s6e256396m8c6790f1489032bb@mail.gmail.com>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
	<31f2a7bd0910251031h3f09c893x448c8dccb2f0e7ca@mail.gmail.com>
	<4AE5A3B4.1060803@cytetech.com>
	<31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com>
	<4AE62DC5.8090404@cytetech.com>
	<63b4e4050910270703u52c6a98ah4561560bfe708cbd@mail.gmail.com>
	<4AE719CF.7060208@cox.net>
	<63b4e4050910271036t1c6cd939o30242ffc9c322719@mail.gmail.com>
	<4AE743BA.3060504@cox.net>
	<63b4e4050910271340s6e256396m8c6790f1489032bb@mail.gmail.com>
Message-ID: <4AE76BD1.3050801@cytetech.com>

Tim Peierls wrote:
> On Tue, Oct 27, 2009 at 3:02 PM, Gregg Wonderly <gergg at cox.net 
> <mailto:gergg at cox.net>> wrote:
> 
>     Doubly linked list was just an example, any kind of sorted structure
>     which includes a partitioned structure that can reduce O(n) to a
>     more manageable N is fine with me.
>
> How is doubly-linked list a partitioned structure?

 From a locking perspective, you lock individual objects to traverse, not one 
global lock, so there is less contention on a single resource, over time.  If 
you then segment the list into groups of lists (as you'd do in a hash map), you 
get partitioning of space/time to search too.

>     One traversal to delete 100 entries (for the
>     setRemoveOnCancel(false) case) vs 100 traversals to delete 100
>     entries (for the setRemoveOnCancel(true) case) is the issue that I'm
>     trying to focus on.  That's where data structure design makes the
>     difference.  Constant time access is what should be happening I think.
>
> Do you have a sorted BlockingQueue implementation in mind that exhibits 
> constant time put, take, and removal of internal nodes? That would be handy.

No, I've not used BlockingQueue as the mechanism of implementation.  I have used 
paired map (key for search was not the timer entry itself) and a doubly linked 
list though.  Random times make the insertion more O(n) than constant, but if 
times have semi constant intervals, then it might be that most adds are appends, 
and so you can search from the back of a doubly linked list to find the 
insertion point.

>     The reason why I said setRemoveOnCancel() was patching over the
>     issue was that it implies that there are performance trade offs for
>     one use case vs the other.  
>
> There *are* trade-offs: setRemoveOnCancel(false) doesn't cause the lock 
> on the shared queue to be acquired for each cancel; 
> setRemoveOnCancel(true) does.

As I said this is an implementation detail.  I'm not suggesting that it is easy 
to create a data structure/model that makes all of this easy to code.  I'm just 
suggesting that some shortcuts are being taken because of the existing 
implementation details.

> I don't see a way around these competing concerns.

I guess you are saying that it's the number of times the lock is held that is 
the biggest issue in your mind?  I.e. if the lock is in place for every cancel, 
then every add (happening at the same rate) will likely contend for access?

>     Deferring that activity doesn't reduce the computational impact of
>     finding and removing a single entry, it only changes when that
>     impact occurs.  If it does make an impact for deleting multiple
>     entries, one at a time, then that implies a data structure design
>     issue to me.
> 
> Removing a canceled task from the middle of the queue in STPE means 
> holding the lock for O(log n). Taking a canceled task off the end of the 
> queue and discarding it is O(1). Purging is O(n). Are there concurrent 
> data structures that can do better?
>
>     Canceling 10/minute is different than 200/sec.  In either case, you
>     need to be able to get the work done.  If setRemoveOnCancel() is
>     viable for 10/minute but not for 200/second, then it seems odd that
>     the method would exist.  It it's not viable for all N/second (i.e.
>     it scales linearly or worse), then adding it is just a patch to try
>     and make some cases work out better till the point that performance
>     degrades and it is no longer a viable solution.
>
> Try setRemoveOnCancel(true) in practice. If it doesn't scale, the right 
> response is to come up with an implementation of SES (or to come up with 
> another approach entirely) that solves your problem, not to force all 
> STPE users to risk contention for the queue lock on every cancel.
> 
> Unless, as I've been hinting, you have a data structure that balances 
> both sets of concerns without recourse to a configuration property like 
> remove-on-cancel, in which case ... let's see it! :-)

I appreciate your excitement to see a better solution.  I don't have one now, 
but perhaps I'll play a bit with some of the things I have laying around to see 
if there is something there that I can gel into a BlockingQueue.

Gregg Wonderly

From tim at peierls.net  Tue Oct 27 19:20:05 2009
From: tim at peierls.net (Tim Peierls)
Date: Tue, 27 Oct 2009 19:20:05 -0400
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
	this requirement?
In-Reply-To: <4AE76BD1.3050801@cytetech.com>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com>
	<4AE5A3B4.1060803@cytetech.com>
	<31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com>
	<4AE62DC5.8090404@cytetech.com>
	<63b4e4050910270703u52c6a98ah4561560bfe708cbd@mail.gmail.com>
	<4AE719CF.7060208@cox.net>
	<63b4e4050910271036t1c6cd939o30242ffc9c322719@mail.gmail.com>
	<4AE743BA.3060504@cox.net>
	<63b4e4050910271340s6e256396m8c6790f1489032bb@mail.gmail.com>
	<4AE76BD1.3050801@cytetech.com>
Message-ID: <63b4e4050910271620g6f2fb24j1efc51cd46e2c61f@mail.gmail.com>

On Tue, Oct 27, 2009 at 5:53 PM, Gregg Wonderly <gregg at cytetech.com> wrote:

> How is doubly-linked list a partitioned structure?
>>
>
> From a locking perspective, you lock individual objects to traverse, not
> one global lock, so there is less contention on a single resource, over
> time.  If you then segment the list into groups of lists (as you'd do in a
> hash map), you get partitioning of space/time to search too.


Ah, OK -- not just a simple doubly-linked list, but something with
hand-over-hand or nested locking.



> I have used paired map (key for search was not the timer entry itself) and
>> a doubly linked list though.  Random times make the insertion more O(n) than
>> constant, but if times have semi constant intervals, then it might be that
>> most adds are appends, and so you can search from the back of a doubly
>> linked list to find the insertion point.
>
>
Not something you'd want to use for general-purpose scheduling, but
excellent for certain patterns of use.



> As I said this is an implementation detail.  I'm not suggesting that it is
>> easy to create a data structure/model that makes all of this easy to code.
>>  I'm just suggesting that some shortcuts are being taken because of the
>> existing implementation details.
>
>
I think it's unfair to suggest that setRemoveOnCancel() is a "shortcut"
without presenting a better general-purpose SES implementation that renders
it unnecessary. You'd have to demonstrate (1) that STPE has pathologically
bad behavior in certain practical contexts and (2) that the new impl has
good behavior in the same contexts and performance similar to (or better
than) STPE in all other practical contexts. I haven't seen (1) demonstrated
for the Java 7 STPE yet.

My perspective is that there are *some* uses of STPE -- I won't call them
niche uses, but I will say that they are by no means the only uses for STPE
-- for which the accumulation of canceled tasks is such a problem that it's
appropriate to risk a little contention by removing tasks on cancellation.
There was no way to achieve this in the past but as of Java 7 there will be.
That's not a patch or a shortcut; that's evolution.



> I don't see a way around these competing concerns.
>>
>
> I guess you are saying that it's the number of times the lock is held that
> is the biggest issue in your mind?  I.e. if the lock is in place for every
> cancel, then every add (happening at the same rate) will likely contend for
> access?


I'm saying that contention for the lock is one of several competing
concerns. In some contexts it might be irrelevant; it sounds like that's the
case for the folks who have argued for remove-on-cancel as the normal mode.



> Unless, as I've been hinting, you have a data structure that balances both
>> sets of concerns without recourse to a configuration property like
>> remove-on-cancel, in which case ... let's see it! :-)
>>
>
> I appreciate your excitement to see a better solution.  I don't have one
> now, but perhaps I'll play a bit with some of the things I have laying
> around to see if there is something there that I can gel into a
> BlockingQueue.
>

OK, and don't rule out the possibility that the best solution for the class
of problems that is motivating you does *not* involve a BlockingQueue (and
hence does not involve implementing ScheduledExecutorService); maybe you're
looking for a different beast entirely, a "ScheduledTaskExecutor", say. Not
having to implement BlockingQueue would remove some constraints on your
implementation.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091027/6eaefe61/attachment.html>

From cagatayk at acm.org  Wed Oct 28 23:32:51 2009
From: cagatayk at acm.org (Cagatay Kavukcuoglu)
Date: Wed, 28 Oct 2009 23:32:51 -0400
Subject: [concurrency-interest] Blocking peek/await in LBQ
Message-ID: <35b8f8000910282032p43f1e907n4a9d69e08e70e8e@mail.gmail.com>

Hi,

I realize this was discussed before
(http://cs.oswego.edu/pipermail/concurrency-interest/2009-June/006229.html),
but I wanted to see if folks think whether there are other ways to get
equivalent functionality. My particular use case is implementing a
concurrent data structure that would allow the usual tuple space
operations like read (blocks until a matching element exists and
returns without removing), take (like read, except the element is
removed) and put, where multiple values can be "queued" while being
associated with a key. An "await" operation on LinkedBlockingQueue
would have simplified the implementation significantly; in fact, I'm
duplicating LBQ code and adding this myself at the moment. Is there a
way to do this idiomatically in j.u.c that does not involve having to
resort to code duplication?

CK.

From i30817 at gmail.com  Thu Oct 29 08:13:41 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Thu, 29 Oct 2009 12:13:41 +0000
Subject: [concurrency-interest] Use of scheduledexecutor appropriate for
	this requirement?
In-Reply-To: <63b4e4050910271620g6f2fb24j1efc51cd46e2c61f@mail.gmail.com>
References: <0B8FFC40-7881-4131-BA38-ED545B1EE2D9@gmail.com> 
	<31f2a7bd0910261325m1baca02ej2974f7edf1fdbb7e@mail.gmail.com> 
	<4AE62DC5.8090404@cytetech.com>
	<63b4e4050910270703u52c6a98ah4561560bfe708cbd@mail.gmail.com> 
	<4AE719CF.7060208@cox.net>
	<63b4e4050910271036t1c6cd939o30242ffc9c322719@mail.gmail.com> 
	<4AE743BA.3060504@cox.net>
	<63b4e4050910271340s6e256396m8c6790f1489032bb@mail.gmail.com> 
	<4AE76BD1.3050801@cytetech.com>
	<63b4e4050910271620g6f2fb24j1efc51cd46e2c61f@mail.gmail.com>
Message-ID: <212322090910290513q32133ef7yee1db6e5423b289@mail.gmail.com>

Off topic, but still speaking of the scheduled executor,  the main
reason i changed from timer to scheduled executor was that timer
didn't allow reusing the TimerTask easily (if it is only going to be
used by one thread (resubmit) or is immutable). It was a pain to
recreate the object and add a long list of fields to another
constructor (to allow the required immutability that comes from that
design decision).

So i switched to ScheduledExecutor. However things here are missing.
Especially the scheduledExecutionTime from timer task, the fact that
there is no way to (easily) set it up to die after a time without
tasks (like for the others here :
http://www.kimchy.org/juc-executorservice-gotcha/) no way to set up a
sequence of executions with relative to last time (for ex:
schedule(long [] deltas, List<Runnable> s) - with error correction
from scheduledExecutionTime of course).

This is just whining so you can ignore it, it is already solved
(except the thread dieing when after a time without tasks).

From genman at noderunner.net  Thu Oct 29 11:27:23 2009
From: genman at noderunner.net (Elias Ross)
Date: Thu, 29 Oct 2009 08:27:23 -0700
Subject: [concurrency-interest] Speaking of ScheduledExecutor;
	JBoss Netty's timer
Message-ID: <b517768e0910290827r73300aebh9f62cbe3519fac23@mail.gmail.com>

JBoss Netty ships with (basically) a scheduled pooled executor that is
optimal for I/O cancellation tasks. Here's the Javadoc. It's worth reading
the paper it's based on:

http://www.jboss.org/file-access/default/members/netty/freezone/api/3.1/org/jboss/netty/util/HashedWheelTimer.html

The slides pointed to in the Javadoc link to a good explanation of the
time/space trade-offs per operation for this data structure:

http://www.cse.wustl.edu/~cdgill/courses/cs6874/TimingWheels.ppt

Since it's segmented, it seems much better designed for multi-threaded use.
Like ConcurrentHashMap, there is a separate bucket for each time interval.

Cancelation is a constant time operation.

Only unfortunate aspect of this class is that none of the interfaces from
java.util.concurrent are used, e.g. ScheduledFuture or
ScheduledExecutorService.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091029/abc898b1/attachment.html>

From trustin at gmail.com  Thu Oct 29 20:16:41 2009
From: trustin at gmail.com (=?UTF-8?B?VHJ1c3RpbiBMZWUgKOydtO2drOyKuSk=?=)
Date: Fri, 30 Oct 2009 09:16:41 +0900
Subject: [concurrency-interest] Speaking of ScheduledExecutor;
	JBoss 	Netty's timer
In-Reply-To: <b517768e0910290827r73300aebh9f62cbe3519fac23@mail.gmail.com>
References: <b517768e0910290827r73300aebh9f62cbe3519fac23@mail.gmail.com>
Message-ID: <aa4319810910291716j37cc1bb7k827a0301d6d1ec32@mail.gmail.com>

Hello Elias et al,

Here's the link to the source code:

    http://anonsvn.jboss.org/repos/netty/trunk/src/main/java/org/jboss/netty/util/HashedWheelTimer.java

and here's the link to the syntax-highlighted version:

    http://fisheye.jboss.org/browse/Netty/trunk/src/main/java/org/jboss/netty/util/HashedWheelTimer.java?r=1850

If someone is interested in picking up this implementation and
including it in j.u.concurrent, I would be happy to discuss with my
boss about relicensing it under the license that OpenJDK prefers.

The data structure is very simple, so it shouldn't be difficult to
write from scratch though if you really want.

HTH

? Trustin Lee, http://gleamynode.net/

On Fri, Oct 30, 2009 at 12:27 AM, Elias Ross <genman at noderunner.net> wrote:
> JBoss Netty ships with (basically) a scheduled pooled executor that is
> optimal for I/O cancellation tasks. Here's the Javadoc. It's worth reading
> the paper it's based on:
>
> http://www.jboss.org/file-access/default/members/netty/freezone/api/3.1/org/jboss/netty/util/HashedWheelTimer.html
>
> The slides pointed to in the Javadoc link to a good explanation of the
> time/space trade-offs per operation for this data structure:
>
> http://www.cse.wustl.edu/~cdgill/courses/cs6874/TimingWheels.ppt
>
> Since it's segmented, it seems much better designed for multi-threaded use.
> Like ConcurrentHashMap, there is a separate bucket for each time interval.
>
> Cancelation is a constant time operation.
>
> Only unfortunate aspect of this class is that none of the interfaces from
> java.util.concurrent are used, e.g. ScheduledFuture or
> ScheduledExecutorService.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From ashwin.jayaprakash at gmail.com  Fri Oct 30 19:26:04 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Fri, 30 Oct 2009 16:26:04 -0700
Subject: [concurrency-interest] Lazy fetching and volatile fields
Message-ID: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>

I'm trying to create a "lazy-retriever handle" for some expensive objects
and I'm looking for some validation on its design:

*1) *A handle can be "fetched" lazily by any thread that has a reference to
it. So, there are multiple threads that share a handle

*2) *Once a thread fetches the actual object, it gets stored in the volatile
field which will then become visible to all other threads using the handle

*3) *Based on my understanding of volatiles, I thought I could optimize it.
Pls tell me if this is valid:
*    a) *The handle has 2 fields - one that is volatile and another that is
not. Both will eventually point to the same actual "fetched" object

*    b) *Since the "getActual()" is called frequently, each thread first
checks its non-volatile field to avoid a volatile read

*    c) *If the non-volatile field is null, then it checks the volatile
field to see if any other thread fetched it recently

*    d) *If the volatile field is also null, then acquire a lock or use some
other implementation in the "fetch()" method and store it into the volatile
field. "fetch()" is abstract for now to avoid any diversions

*    e) *Now store the actual object into the non-volatile field. For this
thread at least we will not need any volatile reads in future because we
have read it into the non-volatile field

*Questions:*
1) Will this work?
2) Are the performance assumptions correct?
3) Is this really necessary? :) I do not have access to a super-multi-core
thingy to verify this
4) Should I pad the volatile into a different cache-line to avoid cache
ping-ponging?


 1 *package* temp;

 2

 3 *public* *abstract* *class* AbstractLazyHandle<T> {

 4     *protected* T *nonVolatileActual;*

 5

 6     *protected* *volatile* T *actual;*

 7

 8     *public* AbstractLazyHandle() {

 9     }

10

11     *public* T getActual() {

12         *if* (*nonVolatileActual *!= *null*) {

13             *return* *nonVolatileActual;*

14         }

15

16         *//--------------*

17

18         *//Fetch it into the volatile field.*

19         fetch();

20

21         *//Then cache it into a regular field.*

22         *nonVolatileActual *= *actual;*

23

24         *return* *nonVolatileActual;*

25     }

26

27     *protected* *abstract* T fetch();

28 }

29



Thanks!
Ashwin (http://www.ashwinjayaprakash.com)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091030/ea5004a0/attachment.html>

From hans.boehm at hp.com  Fri Oct 30 19:51:26 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 30 Oct 2009 23:51:26 +0000
Subject: [concurrency-interest] Lazy fetching and volatile fields
In-Reply-To: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>
References: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042577E54F22F@GVW0436EXB.americas.hpqcorp.net>



________________________________
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Ashwin Jayaprakash
Sent: Friday, October 30, 2009 4:26 PM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] Lazy fetching and volatile fields

I'm trying to create a "lazy-retriever handle" for some expensive objects and I'm looking for some validation on its design:

1) A handle can be "fetched" lazily by any thread that has a reference to it. So, there are multiple threads that share a handle

2) Once a thread fetches the actual object, it gets stored in the volatile field which will then become visible to all other threads using the handle

3) Based on my understanding of volatiles, I thought I could optimize it. Pls tell me if this is valid:
    a) The handle has 2 fields - one that is volatile and another that is not. Both will eventually point to the same actual "fetched" object

    b) Since the "getActual()" is called frequently, each thread first checks its non-volatile field to avoid a volatile read

    c) If the non-volatile field is null, then it checks the volatile field to see if any other thread fetched it recently

    d) If the volatile field is also null, then acquire a lock or use some other implementation in the "fetch()" method and store it into the volatile field. "fetch()" is abstract for now to avoid any diversions

    e) Now store the actual object into the non-volatile field. For this thread at least we will not need any volatile reads in future because we have read it into the non-volatile field

Questions:
1) Will this work?

Not really, though it may appear to.  At least if I understand correctly.  If you get the non-volatile copy of a non-null handle, there is no guarantee that the object it refers to has been completely constructed.  The handle may become visible before the object itself.

2) Are the performance assumptions correct?

Depends on the archiecture.  On X86, I believe the answer is no.  The cost of a volatile read shouldn't be much different from an ordinary one.  On other architectures that can be substantially different.

3) Is this really necessary? :) I do not have access to a super-multi-core thingy to verify this

I would just use a volatile for the frequently accessed version of the handle and be done with it.

4) Should I pad the volatile into a different cache-line to avoid cache ping-ponging?

A volatile read shouldn't require exclusive access to the cache line.  Once things are initialized, I don't see why it should matter.

Hans



 1 package temp;

 2

 3 public abstract class AbstractLazyHandle<T> {

 4     protected T nonVolatileActual;

 5

 6     protected volatile T actual;

 7

 8     public AbstractLazyHandle() {

 9     }

10

11     public T getActual() {

12         if (nonVolatileActual != null) {

13             return nonVolatileActual;

14         }

15

16         //--------------

17

18         //Fetch it into the volatile field.

19         fetch();

20

21         //Then cache it into a regular field.

22         nonVolatileActual = actual;

23

24         return nonVolatileActual;

25     }

26

27     protected abstract T fetch();

28 }

29


Thanks!
Ashwin (http://www.ashwinjayaprakash.com)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091030/6d6df79a/attachment-0001.html>

From joe.bowbeer at gmail.com  Fri Oct 30 20:02:58 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 30 Oct 2009 17:02:58 -0700
Subject: [concurrency-interest] Lazy fetching and volatile fields
In-Reply-To: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>
References: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>
Message-ID: <31f2a7bd0910301702u7c3455d1g2d9fe005b9ef3f18@mail.gmail.com>

On Fri, Oct 30, 2009 at 4:26 PM, Ashwin Jayaprakash wrote:

> I'm trying to create a "lazy-retriever handle" for some expensive objects
> and I'm looking for some validation on its design:
>


In addition to what Hans wrote:

The 'actual' volatile isn't assigned in the code snippet; I'm assuming the
code should read:

  actual = fetch();

The code as written will allow multiple fetch() invocations to execute
concurrently.  Is this a problem?

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091030/26b863e2/attachment.html>

From ashwin.jayaprakash at gmail.com  Fri Oct 30 20:30:03 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Fri, 30 Oct 2009 17:30:03 -0700
Subject: [concurrency-interest] Lazy fetching and volatile fields
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577E54F22F@GVW0436EXB.americas.hpqcorp.net>
References: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042577E54F22F@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <837c23d40910301730w13a49d0dy41b048df50612cba@mail.gmail.com>

Hans, you said:

1) Will this work?
>
> Not really, though it may appear to.  At least if I understand correctly.
> If you get the non-volatile copy of a non-null handle, there is no guarantee
> that the object it refers to has been completely constructed.  The handle
> may become visible before the object itself.
>
>
Since I copy the value from the volatile to the non-volatile field, why
would the volatile field have the incomplete object in the first place? If I
use locks to write to the volatile field I shouldn't have such problems,
right?


Thanks!
Ashwin (http://www.ashwinjayaprakash.com)

>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091030/124af177/attachment.html>

From hans.boehm at hp.com  Fri Oct 30 21:16:53 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sat, 31 Oct 2009 01:16:53 +0000
Subject: [concurrency-interest] Lazy fetching and volatile fields
In-Reply-To: <837c23d40910301730w13a49d0dy41b048df50612cba@mail.gmail.com>
References: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042577E54F22F@GVW0436EXB.americas.hpqcorp.net>
	<837c23d40910301730w13a49d0dy41b048df50612cba@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042577E54F261@GVW0436EXB.americas.hpqcorp.net>

I'm not sure I have a completely accurate picture of what you're doing.  But on the fast path, it's presumably something like:

1. Read non-volatile field containing reference to object x.
2. Access non-final field x.f

The memory model does not guarantee that these will happen in order, though it's difficult to construct scenarios in which they won't be, except on obsolete hardware.  A hypothetical clever compiler or hypothetical clever hardware could transform this to:

1. Make a good guess for what x will be.  Read x_predicted.f, and then x.
2. Verify that x_predicted == x.

This would read x and x.f in the wrong order if it guessed right.

Probably more realistically, I'm not at all sure that there's anything preventing the reordering of

- object initialization and assignment to the volatile field
- assignment to the non-volatile field

But I don't have a clear picture of your code in mind.

Hans

________________________________
From: Ashwin Jayaprakash [mailto:ashwin.jayaprakash at gmail.com]
Sent: Friday, October 30, 2009 5:30 PM
To: Boehm, Hans
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Lazy fetching and volatile fields

Hans, you said:

1) Will this work?

Not really, though it may appear to.  At least if I understand correctly.  If you get the non-volatile copy of a non-null handle, there is no guarantee that the object it refers to has been completely constructed.  The handle may become visible before the object itself.


Since I copy the value from the volatile to the non-volatile field, why would the volatile field have the incomplete object in the first place? If I use locks to write to the volatile field I shouldn't have such problems, right?


Thanks!
Ashwin (http://www.ashwinjayaprakash.com)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091031/739a174b/attachment.html>

From ashwin.jayaprakash at gmail.com  Sat Oct 31 03:31:40 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Sat, 31 Oct 2009 00:31:40 -0700
Subject: [concurrency-interest] Lazy fetching and volatile fields
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577E54F261@GVW0436EXB.americas.hpqcorp.net>
References: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042577E54F22F@GVW0436EXB.americas.hpqcorp.net>
	<837c23d40910301730w13a49d0dy41b048df50612cba@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042577E54F261@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <837c23d40910310031k2bc32db5tcf7b3c778a81fd68@mail.gmail.com>

All I'd do in the fetch() method would be to create the heavy object and set
it to the volatile field. So, why would the fields get re-ordered even with
the use of volatiles and a sync block? Or am I treading into the dreaded
double-checked-lock bug?

protected T fetch(){
   T newActual = null;

   synchronized(this){
     newActual = new T();
   }

   //Set to volatile field
   actual = newActual;

   return newActual;
}

public T getActual() {
   if (nonVolatileActual != null) {
      return nonVolatileActual;
   }

   //Fetch it into the volatile field.
   fetch();

   //Then cache it into a regular field. (Or, just do nonVolatileActual =
fetch() directly)
   nonVolatileActual = actual;

   return nonVolatileActual;
}



On Fri, Oct 30, 2009 at 6:16 PM, Boehm, Hans <hans.boehm at hp.com> wrote:

>  I'm not sure I have a completely accurate picture of what you're doing.
> But on the fast path, it's presumably something like:
>
> 1. Read non-volatile field containing reference to object x.
> 2. Access non-final field x.f
>
> The memory model does not guarantee that these will happen in order, though
> it's difficult to construct scenarios in which they won't be, except on
> obsolete hardware.  A hypothetical clever compiler or hypothetical clever
> hardware could transform this to:
>
> 1. Make a good guess for what x will be.  Read x_predicted.f, and then x.
> 2. Verify that x_predicted == x.
>
> This would read x and x.f in the wrong order if it guessed right.
>
> Probably more realistically, I'm not at all sure that there's anything
> preventing the reordering of
>
> - object initialization and assignment to the volatile field
> - assignment to the non-volatile field
>
> But I don't have a clear picture of your code in mind.
>
> Hans
>
>  ------------------------------
> *From:* Ashwin Jayaprakash [mailto:ashwin.jayaprakash at gmail.com]
> *Sent:* Friday, October 30, 2009 5:30 PM
> *To:* Boehm, Hans
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Lazy fetching and volatile fields
>
> Hans, you said:
>
> 1) Will this work?
>>
>> Not really, though it may appear to.  At least if I understand correctly.
>> If you get the non-volatile copy of a non-null handle, there is no guarantee
>> that the object it refers to has been completely constructed.  The handle
>> may become visible before the object itself.
>>
>>
> Since I copy the value from the volatile to the non-volatile field, why
> would the volatile field have the incomplete object in the first place? If I
> use locks to write to the volatile field I shouldn't have such problems,
> right?
>
>
> Thanks!
> Ashwin (http://www.ashwinjayaprakash.com)
>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091031/119f1ed6/attachment-0001.html>

From ashwin.jayaprakash at gmail.com  Sat Oct 31 03:42:16 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Sat, 31 Oct 2009 00:42:16 -0700
Subject: [concurrency-interest] Lazy fetching and volatile fields
In-Reply-To: <837c23d40910310031k2bc32db5tcf7b3c778a81fd68@mail.gmail.com>
References: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042577E54F22F@GVW0436EXB.americas.hpqcorp.net>
	<837c23d40910301730w13a49d0dy41b048df50612cba@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042577E54F261@GVW0436EXB.americas.hpqcorp.net>
	<837c23d40910310031k2bc32db5tcf7b3c778a81fd68@mail.gmail.com>
Message-ID: <837c23d40910310042j7635e6aarf152b1d9e4c9fad5@mail.gmail.com>

I was going to add that for now fetch() will get invoked multiple times. If
we can ignore that for the moment, would the volatile field still get read
before the sync block? I guess it would, but I missed what Hans was saying.
In which case the volatile will still be null and the code would get a null
assigned to nonVolatileActual...I guess. So, the only option would be to run
it in a loop until the field is not null.

Is all this worth it? :)


On Sat, Oct 31, 2009 at 12:31 AM, Ashwin Jayaprakash <
ashwin.jayaprakash at gmail.com> wrote:

> All I'd do in the fetch() method would be to create the heavy object and
> set it to the volatile field. So, why would the fields get re-ordered even
> with the use of volatiles and a sync block? Or am I treading into the
> dreaded double-checked-lock bug?
>
> protected T fetch(){
>    T newActual = null;
>
>    synchronized(this){
>      newActual = new T();
>    }
>
>    //Set to volatile field
>    actual = newActual;
>
>    return newActual;
> }
>
> public T getActual() {
>    if (nonVolatileActual != null) {
>       return nonVolatileActual;
>    }
>
>    //Fetch it into the volatile field.
>    fetch();
>
>    //Then cache it into a regular field. (Or, just do nonVolatileActual =
> fetch() directly)
>    nonVolatileActual = actual;
>
>    return nonVolatileActual;
>
> }
>
>
>
> On Fri, Oct 30, 2009 at 6:16 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
>
>>  I'm not sure I have a completely accurate picture of what you're doing.
>> But on the fast path, it's presumably something like:
>>
>> 1. Read non-volatile field containing reference to object x.
>> 2. Access non-final field x.f
>>
>> The memory model does not guarantee that these will happen in order,
>> though it's difficult to construct scenarios in which they won't be, except
>> on obsolete hardware.  A hypothetical clever compiler or hypothetical clever
>> hardware could transform this to:
>>
>> 1. Make a good guess for what x will be.  Read x_predicted.f, and then x.
>> 2. Verify that x_predicted == x.
>>
>> This would read x and x.f in the wrong order if it guessed right.
>>
>> Probably more realistically, I'm not at all sure that there's anything
>> preventing the reordering of
>>
>> - object initialization and assignment to the volatile field
>> - assignment to the non-volatile field
>>
>> But I don't have a clear picture of your code in mind.
>>
>> Hans
>>
>>  ------------------------------
>> *From:* Ashwin Jayaprakash [mailto:ashwin.jayaprakash at gmail.com]
>> *Sent:* Friday, October 30, 2009 5:30 PM
>> *To:* Boehm, Hans
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Lazy fetching and volatile fields
>>
>> Hans, you said:
>>
>> 1) Will this work?
>>>
>>> Not really, though it may appear to.  At least if I understand
>>> correctly.  If you get the non-volatile copy of a non-null handle, there is
>>> no guarantee that the object it refers to has been completely constructed.
>>> The handle may become visible before the object itself.
>>>
>>>
>> Since I copy the value from the volatile to the non-volatile field, why
>> would the volatile field have the incomplete object in the first place? If I
>> use locks to write to the volatile field I shouldn't have such problems,
>> right?
>>
>>
>> Thanks!
>> Ashwin (http://www.ashwinjayaprakash.com)
>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091031/9bcf7b83/attachment.html>

From dl at cs.oswego.edu  Sat Oct 31 09:34:35 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 31 Oct 2009 09:34:35 -0400
Subject: [concurrency-interest] Blocking peek/await in LBQ
In-Reply-To: <35b8f8000910282032p43f1e907n4a9d69e08e70e8e@mail.gmail.com>
References: <35b8f8000910282032p43f1e907n4a9d69e08e70e8e@mail.gmail.com>
Message-ID: <4AEC3CEB.7010501@cs.oswego.edu>

Cagatay Kavukcuoglu wrote:
> Hi,
> 
> I realize this was discussed before
> (http://cs.oswego.edu/pipermail/concurrency-interest/2009-June/006229.html),
> but I wanted to see if folks think whether there are other ways to get
> equivalent functionality. My particular use case is implementing a
> concurrent data structure that would allow the usual tuple space
> operations like read (blocks until a matching element exists and
> returns without removing), take (like read, except the element is
> removed) and put, where multiple values can be "queued" while being
> associated with a key. An "await" operation on LinkedBlockingQueue
> would have simplified the implementation significantly; in fact, I'm
> duplicating LBQ code and adding this myself at the moment. Is there a
> way to do this idiomatically in j.u.c that does not involve having to
> resort to code duplication?
> 

Basically, this amounts to supporting a change notification
scheme for the particular event of a collection (here, a queue)
becoming non-empty.

Our resistance to supplying such things stems from not wanting
to get into the business of adding to each concurrent collection
all the different state-change mechanics (listeners? callbacks?
blocking waits?) that you could apply to all their possible events
(non-empty? empty? too big? insertion or removal of particular
elements?)  (Other library folks are less stubborn about this.
I think some Apache and Google collections have some support
for some of these options.)

But the easy cases of doing this yourself are pretty
easy. For example you could use a synchronized wrapper class
around nearly any collection or queue, adding
a notifyAll on any insertion, along with an await
method along the lines of:
   while (q.isEmpty() == null) wait();
Although, if the single lock you'd need here for the wait/notify
becomes a serious scalability problem, you'd need a more
complicated solution.


-Doug

From hans.boehm at hp.com  Sat Oct 31 13:52:04 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sat, 31 Oct 2009 17:52:04 +0000
Subject: [concurrency-interest] Lazy fetching and volatile fields
In-Reply-To: <837c23d40910310042j7635e6aarf152b1d9e4c9fad5@mail.gmail.com>
References: <837c23d40910301626q500fc729t3fb0946c393a02b0@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042577E54F22F@GVW0436EXB.americas.hpqcorp.net>
	<837c23d40910301730w13a49d0dy41b048df50612cba@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042577E54F261@GVW0436EXB.americas.hpqcorp.net>
	<837c23d40910310031k2bc32db5tcf7b3c778a81fd68@mail.gmail.com>
	<837c23d40910310042j7635e6aarf152b1d9e4c9fad5@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042577E54F2D4@GVW0436EXB.americas.hpqcorp.net>

I suspect that if you assign to the volatile first, then reread it, and assign to the non-volatile, you're probably at the point at which conventional implementations won't break your code on conventional hardware.  However, the memory model still gives you no guarantees:  The problem is that a thread setting actual does not synchronize with a thread just accessing nonVolatileActual.  Thus there is no guarantee that the second thread will see a properly initialized T eventhough nonVolatileActual is nonzero.  Thus a clever implementation, or one on strange hardware, may still break your code.  And it's quite possible I'm overlooking a reason even a conventional implementation will break something.

In fact Java non-volatile semantics are so weak that you have yet another problem:

The code

if (nonVolatileActual != null) {
      return nonVolatileActual;
}
may return null, since the second load of nonVolatileActual may yield an "earlier" value.  This is fixable by loading it only once into a temporary.

Just say "no" to data races.   I definitely wouldn't consider anything along these lines unless you have a demonstrable performance problem.  Even then, I'd be inclined to first try caching in a thread-local, rather than a shared non-volatile variable.  Again, I think this makes no sense at all on something like X86 where volatile loads should be cheap.

Hans

________________________________
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Ashwin Jayaprakash
Sent: Saturday, October 31, 2009 12:42 AM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Lazy fetching and volatile fields

I was going to add that for now fetch() will get invoked multiple times. If we can ignore that for the moment, would the volatile field still get read before the sync block? I guess it would, but I missed what Hans was saying. In which case the volatile will still be null and the code would get a null assigned to nonVolatileActual...I guess. So, the only option would be to run it in a loop until the field is not null.

Is all this worth it? :)


On Sat, Oct 31, 2009 at 12:31 AM, Ashwin Jayaprakash <ashwin.jayaprakash at gmail.com<mailto:ashwin.jayaprakash at gmail.com>> wrote:
All I'd do in the fetch() method would be to create the heavy object and set it to the volatile field. So, why would the fields get re-ordered even with the use of volatiles and a sync block? Or am I treading into the dreaded double-checked-lock bug?

protected T fetch(){
   T newActual = null;

   synchronized(this){
     newActual = new T();
   }

   //Set to volatile field
   actual = newActual;

   return newActual;
}

public T getActual() {
   if (nonVolatileActual != null) {
      return nonVolatileActual;
   }

   //Fetch it into the volatile field.
   fetch();

   //Then cache it into a regular field. (Or, just do nonVolatileActual = fetch() directly)
   nonVolatileActual = actual;

   return nonVolatileActual;

}



On Fri, Oct 30, 2009 at 6:16 PM, Boehm, Hans <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:
I'm not sure I have a completely accurate picture of what you're doing.  But on the fast path, it's presumably something like:

1. Read non-volatile field containing reference to object x.
2. Access non-final field x.f

The memory model does not guarantee that these will happen in order, though it's difficult to construct scenarios in which they won't be, except on obsolete hardware.  A hypothetical clever compiler or hypothetical clever hardware could transform this to:

1. Make a good guess for what x will be.  Read x_predicted.f, and then x.
2. Verify that x_predicted == x.

This would read x and x.f in the wrong order if it guessed right.

Probably more realistically, I'm not at all sure that there's anything preventing the reordering of

- object initialization and assignment to the volatile field
- assignment to the non-volatile field

But I don't have a clear picture of your code in mind.

Hans

________________________________
From: Ashwin Jayaprakash [mailto:ashwin.jayaprakash at gmail.com<mailto:ashwin.jayaprakash at gmail.com>]
Sent: Friday, October 30, 2009 5:30 PM
To: Boehm, Hans
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Lazy fetching and volatile fields

Hans, you said:

1) Will this work?

Not really, though it may appear to.  At least if I understand correctly.  If you get the non-volatile copy of a non-null handle, there is no guarantee that the object it refers to has been completely constructed.  The handle may become visible before the object itself.


Since I copy the value from the volatile to the non-volatile field, why would the volatile field have the incomplete object in the first place? If I use locks to write to the volatile field I shouldn't have such problems, right?


Thanks!
Ashwin (http://www.ashwinjayaprakash.com)



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091031/6ff5b812/attachment.html>

