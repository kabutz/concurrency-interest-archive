From dl at cs.oswego.edu  Sat Aug  1 09:01:26 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 01 Aug 2009 09:01:26 -0400
Subject: [concurrency-interest] JDK7 integration progress
Message-ID: <4A743CA6.9080403@cs.oswego.edu>

We've completed most of the staging for JDK7 integration.
The classes in jsr166y, repackaged into java.util.concurrent,
should appear in openjdk builds within a month or two. They
are also now available in the main jsr166.jar as described at
updated:
   http://gee.cs.oswego.edu/dl/concurrency-interest/index.html

Given the continued uncertainties about Java7 release schedules,
we plan to maintain the jsr166y versions (requiring only JDK6
to run) into the indefinite future. We are extremely
lucky to have Martin Buchholz coordinating this to help
keep things in sync, among his other talents and contributions.
Package jsr166y does still rely on Java6, not Java5.
We don't have any immediate plans for a Java5 backport, since
it would entail some significant performance degradation to
work around lack of Java6 intrinsics.

The staged j.u.c.atomic.Fences class (that does not appear
in jsr166y) is functional, but full integration awaits
other upcoming openjdk VM support.
The spec/javadoc also still needs some work. And we'd
like to give those people who hate the method names
and usage one last chance to come up with alternatives.

In the course of integration efforts, we've added some new tests
(JUnit-based tck tests, "jtreg" developer tests, and performance
"loops" tests) and corrected a few spec and code problems.
We could still use your help in further code review and testing.

We triaged out plans for including a customizable
concurrent hash map for weak/soft/etc keys and values.
The main stall was uncertainty about supplying scalable
eviction policies for use in caches. To recap, the
usual implementations for policies such as LRU force
intrinsic bottlenecks. There are several alternatives,
but we aren't yet confident enough about their range
of applicability to commit to them. Given the availability
of other concurrent weak/soft maps out there, including
the ones provided by MapMaker in google collections --
http://google-collections.googlecode.com/svn/trunk/javadoc/com/google/common/collect/MapMaker.html
it doesn't seem worthwhile to include a new one until
we are sure it is an improvement over them.

Package extra166y still contains preliminary versions
of classes that we do not plan to integrate into JDK7.
This mainly includes ParallelArray classes, as well as
an eviction-less CustomConcurrentHashMap.

Someday there might be yet more new stuff in new
package jsr166z.

-Doug




From forax at univ-mlv.fr  Sat Aug  1 19:44:05 2009
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Sun, 02 Aug 2009 01:44:05 +0200
Subject: [concurrency-interest] JDK7 integration progress
In-Reply-To: <4A743CA6.9080403@cs.oswego.edu>
References: <4A743CA6.9080403@cs.oswego.edu>
Message-ID: <4A74D345.7060805@univ-mlv.fr>

Le 01/08/2009 15:01, Doug Lea a ?crit :
> We've completed most of the staging for JDK7 integration.
> The classes in jsr166y, repackaged into java.util.concurrent,
> should appear in openjdk builds within a month or two. They
> are also now available in the main jsr166.jar as described at
> updated:
>   http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
>
> Given the continued uncertainties about Java7 release schedules,
> we plan to maintain the jsr166y versions (requiring only JDK6
> to run) into the indefinite future. We are extremely
> lucky to have Martin Buchholz coordinating this to help
> keep things in sync, among his other talents and contributions.
> Package jsr166y does still rely on Java6, not Java5.
> We don't have any immediate plans for a Java5 backport, since
> it would entail some significant performance degradation to
> work around lack of Java6 intrinsics.
>
> The staged j.u.c.atomic.Fences class (that does not appear
> in jsr166y) is functional, but full integration awaits
> other upcoming openjdk VM support.
> The spec/javadoc also still needs some work. And we'd
> like to give those people who hate the method names
> and usage one last chance to come up with alternatives.
>
> In the course of integration efforts, we've added some new tests
> (JUnit-based tck tests, "jtreg" developer tests, and performance
> "loops" tests) and corrected a few spec and code problems.
> We could still use your help in further code review and testing.
>
> We triaged out plans for including a customizable
> concurrent hash map for weak/soft/etc keys and values.
> The main stall was uncertainty about supplying scalable
> eviction policies for use in caches. To recap, the
> usual implementations for policies such as LRU force
> intrinsic bottlenecks. There are several alternatives,
> but we aren't yet confident enough about their range
> of applicability to commit to them. Given the availability
> of other concurrent weak/soft maps out there, including
> the ones provided by MapMaker in google collections --
> http://google-collections.googlecode.com/svn/trunk/javadoc/com/google/common/collect/MapMaker.html 
>
> it doesn't seem worthwhile to include a new one until
> we are sure it is an improvement over them.
>
> Package extra166y still contains preliminary versions
> of classes that we do not plan to integrate into JDK7.
> This mainly includes ParallelArray classes, as well as
> an eviction-less CustomConcurrentHashMap.
>
> Someday there might be yet more new stuff in new
> package jsr166z.
>
> -Doug

Ok, my whishlist of corrections:

ForkJoinTask.adapt(Callable) should take a Callable<? extends T>.

ForkJoinTask.adapt(Runnable) should return a ForkJoinTask<?>
or better a <T> ForkJoinTask<T> (see below).

All methods "adapt" should be moved in a class ForkJoinTasks
like methods to adapt a callable are in Executors instead of being
in ThreadPoolExecutor.
In my opinion, all static methods declared in ForkJoinTasks
should be moved in ForkJoinTasks.
It will reduce the complexity when you want to understand how
it works.

overloaded methods "invoke" are not coherent:
ForkJoinTask.invokeAll(Collection) return the collection taken as
argument but invokeAll(ForkJoin...) returns void.
In my opinion, invokeAll(Collection) should returns void,
it will simplifies its signature:
void invokeAll(Collection<? extends ForkJoinTask<?>> tasks).

in ForkJoinTask.reinitialize() [Personally, I prefer reset]
"Effects under any other usage conditions are not guaranteed,
and are almost surely wrong".
I don't like the last part of the sentence, it seams that the
writer knows how all future usages of this method.

There is the same kind of wording also in complete():
"Its use in other situations is likely to be wrong".
I prefer the sentence of getRawResult()
"Its use in any other context is discouraged.".

ForkJoinTask.getPool() doc should have a reference to
ForkJoin.inForkJoinPool().
In general, there are not lot of @see or @link in
the current doc.

ForkJoinTask.getQueuedTaskCount()
and getSurplusQueuedTaskCount() doesn't explicitly says
what's happen if the current thread is not a fork join worker.


<T> ForkJoinPool.execute(ForkJoinTask<T>) should be
ForkJoinPool.execute(ForkJoinTask<?> task).
If T is needed to implement the method, you should do
a capture.

In ForkJoinPool:
<T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)
should be
<T> List<Future<T>> invokeAll(Collection<? extends Callable<? extends 
T>> tasks)
and the same transformation should be applied to
<T> RunnableFuture<T> newTaskFor(Callable<? extends T> callable) and
<T> ForkJoinTask<T> submit(Callable<T> task) .

Moreover, instead of
ForkJoinTask<?> submit(Runnable task)
because the method get() will return null, it signature can be written
<T> ForkJoinTask<T> submit(Runnable task) .
This signature allows more programs to compile, by example this one:

Runnable r =...
Callable<Integer> c=...
List<ForkJoinTask<Integer>> list = ...
ForkJoinTask<Integer> task1 = pool.submit(r);
list.add(task1);
list.add(pool.submit(c));

It currently doesn't compile even if it is a correct code.


RecursiveAction can be renamed to VoidRecursiveTask.

ThreadLocalRandom inherits from Random,
this design was already used in the past
(LinkedHashMap/HashMap, Properties/HashTable)
and this is not really future proof.

in extra:
CustomConcurrentHashMap.Strength,
enum values are not in upper case.

R?mi

From dl at cs.oswego.edu  Sun Aug  2 07:55:56 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 02 Aug 2009 07:55:56 -0400
Subject: [concurrency-interest] JDK7 integration progress
In-Reply-To: <4A74D345.7060805@univ-mlv.fr>
References: <4A743CA6.9080403@cs.oswego.edu> <4A74D345.7060805@univ-mlv.fr>
Message-ID: <4A757ECC.7030804@cs.oswego.edu>

Thanks very much!

R?mi Forax wrote:
> ForkJoinTask.adapt(Callable) should take a Callable<? extends T>.
> 
> ForkJoinTask.adapt(Runnable) should return a ForkJoinTask<?>

Thanks. Done.
(Note: during this transition, changes may show up first
in jsr166y versions
(docs at: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
before java.util.concurrent
(docs at http://gee.cs.oswego.edu/dl/jsr166/dist/docs/)

> 
> All methods "adapt" should be moved in a class ForkJoinTasks
> like methods to adapt a callable are in Executors instead of being
> in ThreadPoolExecutor.

I don't think this is worthwhile.

(The three adapt methods were previously non-public, but
people creating frameworks on top of FJ found they needed
to re-create them so they are now exported. The class-level
javadoc now includes a few sentences about intended usage.)

> overloaded methods "invoke" are not coherent:

Yes. Method fork() as well as the collection form of
invokeAll now return their arguments in response to
a request to do this to simplify usage. However, we
cannot do this for the other two forms of invokeAll
(pairwise and varadic) without introducing overloading
problems. The minor inconsistency seems worth the minor
usability improvement.

> 
> in ForkJoinTask.reinitialize() [Personally, I prefer reset]
> "Effects under any other usage conditions are not guaranteed,
> and are almost surely wrong".
> I don't like the last part of the sentence, it seams that the
> writer knows how all future usages of this method.

OK. You are right that merely saying usage is discouraged
is more in keeping with JDK style. (Although the reason usage
is discouraged is that it is usually wrong!)

> 
> ForkJoinTask.getQueuedTaskCount()
> and getSurplusQueuedTaskCount() doesn't explicitly says
> what's happen if the current thread is not a fork join worker.
> 

Thanks!

> In ForkJoinPool:
> <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)
> should be
> <T> List<Future<T>> invokeAll(Collection<? extends Callable<? extends 
> T>> tasks)
> ...

We cannot change these ForkJoinPool method signatures because
they are established by ExecutorService. And even though
it is binary-compatible under erasure, we cannot change
these without possibly causing existing code not to compile.
(We tried this once and learned our lesson not to try it again.)

> 
> RecursiveAction can be renamed to VoidRecursiveTask.

No way :-)

> ThreadLocalRandom inherits from Random,
> this design was already used in the past
> (LinkedHashMap/HashMap, Properties/HashTable)
> and this is not really future proof.

But this one works out OK. Random was designed
specifically to be subclassed
(see for example method next(bits)), and is elsewhere,
for example in SecureRandom.

-Doug




From davidcholmes at aapt.net.au  Sun Aug  2 17:49:13 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 3 Aug 2009 07:49:13 +1000
Subject: [concurrency-interest] JDK7 integration progress
In-Reply-To: <4A757ECC.7030804@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCAENEICAA.davidcholmes@aapt.net.au>


Doug lea writes:
> R?mi Forax wrote:
> > in ForkJoinTask.reinitialize() [Personally, I prefer reset]
> > "Effects under any other usage conditions are not guaranteed,
> > and are almost surely wrong".
> > I don't like the last part of the sentence, it seams that the
> > writer knows how all future usages of this method.
>
> OK. You are right that merely saying usage is discouraged
> is more in keeping with JDK style. (Although the reason usage
> is discouraged is that it is usually wrong!)

I disagree. If something is "discouraged" it is still implied that it will
"work correctly" (the test being would you accept a bug report on this?). My
take from the original wording is that there is no guarantee that it will
work as someone may think outside the intended contexts. So for the quoted
example it probably suffices to say "Effects under any other usage
conditions are not guaranteed."

The updated: "Effects under any other usage conditions are not guaranteed,
and are discouraged." doesn't read correctly as the subject of the sentence
is "Effects" so you are saying that the effects are discouraged ;-)

Cheers,
David Holmes



From dl at cs.oswego.edu  Sun Aug  2 18:28:27 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 02 Aug 2009 18:28:27 -0400
Subject: [concurrency-interest] JDK7 integration progress
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAENEICAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCAENEICAA.davidcholmes@aapt.net.au>
Message-ID: <4A76130B.6050800@cs.oswego.edu>

David Holmes wrote:
>
> work as someone may think outside the intended contexts. So for the quoted
> example it probably suffices to say "Effects under any other usage
> conditions are not guaranteed."
> 

Thanks! Done.

-Doug

From martinrb at google.com  Sun Aug  2 19:19:42 2009
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 2 Aug 2009 16:19:42 -0700
Subject: [concurrency-interest] JDK7 integration progress
In-Reply-To: <4A743CA6.9080403@cs.oswego.edu>
References: <4A743CA6.9080403@cs.oswego.edu>
Message-ID: <1ccfd1c10908021619r7c84dc94of38df76f2951eaf7@mail.gmail.com>

Here are some bug reports on ForkJoinTask.invokeAll.

---

Spec says

     * <p>Overloadings of this method exist for the special cases
     * of one to four arguments.

but in fact I only see one overloading, the one for two arguments.

  560:    public static void invokeAll(ForkJoinTask<?> t1, ForkJoinTask<?> t2) {
  585:    public static void invokeAll(ForkJoinTask<?>... tasks) {

---

In this snippet,

        if (!(tasks instanceof List<?>)) {
            invokeAll(tasks.toArray(new ForkJoinTask<?>[tasks.size()]));
            return tasks;
        }

it appears to require that the size of tasks is not changing,
else a null could sneak in.  Either the possibility of tasks being
a concurrent collection should be supported, or such support
should be explicitly denied, as is done elsewhere in j.u.c.

Also, perhaps you really mean to check for both
instanceof List *and* instanceof RandomAccess?
Or would no one be crazy enough to use a LinkedList?

Martin

From martinrb at google.com  Sun Aug  2 20:31:17 2009
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 2 Aug 2009 17:31:17 -0700
Subject: [concurrency-interest] JDK7 integration progress
In-Reply-To: <1ccfd1c10908021619r7c84dc94of38df76f2951eaf7@mail.gmail.com>
References: <4A743CA6.9080403@cs.oswego.edu>
	<1ccfd1c10908021619r7c84dc94of38df76f2951eaf7@mail.gmail.com>
Message-ID: <1ccfd1c10908021731r45a5788cp9cdd596848baa051@mail.gmail.com>

Should we s/Executor/ExecutorService/ below?

 * <p>{@code ForkJoinPool}s differ from other kinds of {@link
 * Executor}s mainly in that they provide <em>work-stealing</em>: all

From martinrb at google.com  Sun Aug  2 20:42:34 2009
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 2 Aug 2009 17:42:34 -0700
Subject: [concurrency-interest] ForkJoinPool.isTerminating
Message-ID: <1ccfd1c10908021742u6761d5dbu77d5d7eec5877ef5@mail.gmail.com>

this method

    /**
     * Returns {@code true} if the process of termination has
     * commenced but possibly not yet completed.
     *
     * @return {@code true} if terminating
     */
    public boolean isTerminating() {
        return runStateOf(runControl) >= TERMINATING;
    }

appears to have different semantics from TPE.isTerminating,
which returns false if termination is complete.
Also, it's not obvious to me from the above spec which is meant.
I suggest adopting semantics and wording from TPE.isTerminating

Martin

From dl at cs.oswego.edu  Mon Aug  3 09:06:19 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 03 Aug 2009 09:06:19 -0400
Subject: [concurrency-interest] ForkJoinPool.isTerminating
In-Reply-To: <1ccfd1c10908021742u6761d5dbu77d5d7eec5877ef5@mail.gmail.com>
References: <1ccfd1c10908021742u6761d5dbu77d5d7eec5877ef5@mail.gmail.com>
Message-ID: <4A76E0CB.7050204@cs.oswego.edu>

Martin Buchholz wrote:
> I suggest adopting semantics and wording from TPE.isTerminating
> 


Done; thanks!
Also the other wording cleanups in your other mail.

One downside of evolving in preliminary release for years
is that I haven't always checked that spec and code
changes stay in sync. Hopefully we are getting to the
last of mismatches though.

-Doug


From ganzhi at gmail.com  Wed Aug  5 01:48:48 2009
From: ganzhi at gmail.com (James Gan)
Date: Wed, 5 Aug 2009 13:48:48 +0800
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
Message-ID: <70c070d80908042248r6ea1e2fbo68f6ce072e98def9@mail.gmail.com>

Hi, All

During my recent test, stack component of Amino library works very well in
one of our product. The stack is used as a pool and its scalability is
better than lock-protected JDK stack. I was hoping that this component can
help others also.

Performance chart can be viewed in Amino Blog:
    1. http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html

The source code is already published under Apache License in SourceForge:
    2.
https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/amino/java/src/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java

-- 
Best Regards
James Gan
Current Project: Concurrent Building Block at
http://amino-cbbs.sourceforge.net/
Blog: http://ganzhi.blogspot.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090805/c08b2315/attachment.html>

From davidcholmes at aapt.net.au  Wed Aug  5 02:00:17 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 5 Aug 2009 16:00:17 +1000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <70c070d80908042248r6ea1e2fbo68f6ce072e98def9@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEOFICAA.davidcholmes@aapt.net.au>

James,

Is that a comparison again the ancient java.util.Stack? If so a better
comparison would be against a stack built over java.util.concurrent's
ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the blocking
semantics of your stack).

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of James Gan
  Sent: Wednesday, 5 August 2009 3:49 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack


  Hi, All

  During my recent test, stack component of Amino library works very well in
one of our product. The stack is used as a pool and its scalability is
better than lock-protected JDK stack. I was hoping that this component can
help others also.

  Performance chart can be viewed in Amino Blog:
      1.
http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html

  The source code is already published under Apache License in SourceForge:
      2.
https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/amino/java/s
rc/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java

  --
  Best Regards
  James Gan
  Current Project: Concurrent Building Block at
http://amino-cbbs.sourceforge.net/
  Blog: http://ganzhi.blogspot.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090805/43cf8bb4/attachment.html>

From ganzhi at gmail.com  Wed Aug  5 02:25:28 2009
From: ganzhi at gmail.com (ganzhi at gmail.com)
Date: Wed, 05 Aug 2009 06:25:28 +0000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEOFICAA.davidcholmes@aapt.net.au>
Message-ID: <00163628353ebbd41a04705f11f8@google.com>

David,

Thanks for the great suggestion! I'll do another comparison with  
ConcurrentLinkedQueue and post the result :)

On Aug 5, 2009 2:00pm, David Holmes <davidcholmes at aapt.net.au> wrote:

> James,



> Is that a comparison again the ancient java.util.Stack? If so a better
> comparison would be against a stack built over java.util.concurrent's
> ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the blocking
> semantics of your stack).



> Cheers,

> David Holmes


> -----Original Message-----
> From:
> concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of James
> Gan
> Sent: Wednesday, 5 August 2009 3:49 PM
> To:
> concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest]
> Project Amino: Introduce Lock-Free Stack


> Hi,
> All

> During my recent test, stack component of Amino library works very
> well in one of our product. The stack is used as a pool and its  
> scalability is
> better than lock-protected JDK stack. I was hoping that this component can
> help others also.

> Performance chart can be viewed in Amino Blog:

> 1. http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html

> The
> source code is already published under Apache License in
> SourceForge:
> 2.  
> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/amino/java/src/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java
> --
> Best Regards
> James Gan
> Current Project: Concurrent
> Building Block at http://amino-cbbs.sourceforge.net/
> Blog:
> http://ganzhi.blogspot.com


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090805/9ead0d7f/attachment.html>

From davidcholmes at aapt.net.au  Wed Aug  5 02:27:37 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 5 Aug 2009 16:27:37 +1000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEOFICAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEOGICAA.davidcholmes@aapt.net.au>

Correction: It was pointed out to me (thanks Joe!) that only
LinkedBlockingDeque supports the ability to add/remove at the same end and
so emulate a stack. I can't help but feel we have a gap in our concurrent
collections here ...

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of David Holmes
  Sent: Wednesday, 5 August 2009 4:00 PM
  To: James Gan; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Project Amino: Introduce Lock-Free
Stack


  James,

  Is that a comparison again the ancient java.util.Stack? If so a better
comparison would be against a stack built over java.util.concurrent's
ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the blocking
semantics of your stack).

  Cheers,
  David Holmes
    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of James Gan
    Sent: Wednesday, 5 August 2009 3:49 PM
    To: concurrency-interest at cs.oswego.edu
    Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack


    Hi, All

    During my recent test, stack component of Amino library works very well
in one of our product. The stack is used as a pool and its scalability is
better than lock-protected JDK stack. I was hoping that this component can
help others also.

    Performance chart can be viewed in Amino Blog:
        1.
http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html

    The source code is already published under Apache License in
SourceForge:
        2.
https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/amino/java/s
rc/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java

    --
    Best Regards
    James Gan
    Current Project: Concurrent Building Block at
http://amino-cbbs.sourceforge.net/
    Blog: http://ganzhi.blogspot.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090805/eb387943/attachment.html>

From ganzhi at gmail.com  Wed Aug  5 02:36:10 2009
From: ganzhi at gmail.com (ganzhi at gmail.com)
Date: Wed, 05 Aug 2009 06:36:10 +0000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEOGICAA.davidcholmes@aapt.net.au>
Message-ID: <001636163c5f06887104705f3843@google.com>

David,

Thanks for reminding me! I just realized this problem when I began to write  
a wrap class for ConcurrentLinkedQueue.

Implementation of Amino stack component is very straightforward. If it can  
help to mitigate the gap in jsr166 concurrent collections, I'd like to  
contribute this component to JSR166.

On Aug 5, 2009 2:27pm, David Holmes <davidcholmes at aapt.net.au> wrote:

> Correction: It was pointed out to me (thanks Joe!) that
> only LinkedBlockingDeque supports the ability to add/remove at the same  
> end and
> so emulate a stack. I can't help but feel we have a gap in our concurrent
> collections here ...



> David


> -----Original Message-----
> From:
> concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of David
> Holmes
> Sent: Wednesday, 5 August 2009 4:00 PM
> To: James
> Gan; concurrency-interest at cs.oswego.edu
> Subject: Re:
> [concurrency-interest] Project Amino: Introduce Lock-Free
> Stack



> James,



> Is that a comparison again the ancient java.util.Stack? If so a better
> comparison would be against a stack built over java.util.concurrent's
> ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the blocking
> semantics of your stack).



> Cheers,

> David Holmes


> -----Original Message-----
> From:
> concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of James
> Gan
> Sent: Wednesday, 5 August 2009 3:49 PM
> To:
> concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest]
> Project Amino: Introduce Lock-Free Stack


> Hi,
> All

> During my recent test, stack component of Amino library works
> very well in one of our product. The stack is used as a pool and its
> scalability is better than lock-protected JDK stack. I was hoping that  
> this
> component can help others also.

> Performance chart can be viewed in
> Amino Blog:
> 1. http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html

> The
> source code is already published under Apache License in
> SourceForge:
> 2.  
> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/amino/java/src/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java
> --
> Best Regards
> James Gan
> Current Project:
> Concurrent Building Block at http://amino-cbbs.sourceforge.net/
> Blog:
> http://ganzhi.blogspot.com


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090805/85b1a322/attachment-0001.html>

From martinrb at google.com  Wed Aug  5 02:41:28 2009
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 4 Aug 2009 23:41:28 -0700
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEOGICAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOEOFICAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCIEOGICAA.davidcholmes@aapt.net.au>
Message-ID: <1ccfd1c10908042341p178c65e9lb789ac0e1dcb2781@mail.gmail.com>

On Tue, Aug 4, 2009 at 23:27, David Holmes<davidcholmes at aapt.net.au> wrote:
> Correction: It was pointed out to me (thanks Joe!) that only
> LinkedBlockingDeque supports the ability to add/remove at the same end and
> so emulate a stack. I can't help but feel we have a gap in our concurrent
> collections here ...

I tend to feel the same.
The "Treiber stack" is the simplest useful lock-free data structure,
easier to implement than ConcurrentLinkedQueue.
And it should be able to match the same API.
ConcurrentLinkedStack?

Martin

>
> David
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of David Holmes
> Sent: Wednesday, 5 August 2009 4:00 PM
> To: James Gan; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
>
> James,
>
> Is that a comparison again the ancient java.util.Stack? If so a better
> comparison would be against a stack built over java.util.concurrent's
> ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the blocking
> semantics of your stack).
>
> Cheers,
> David Holmes
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of James Gan
> Sent: Wednesday, 5 August 2009 3:49 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
>
> Hi, All
>
> During my recent test, stack component of Amino library works very well in
> one of our product. The stack is used as a pool and its scalability is
> better than lock-protected JDK stack. I was hoping that this component can
> help others also.
>
> Performance chart can be viewed in Amino Blog:
> ??? 1. http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html
>
> The source code is already published under Apache License in SourceForge:
> ??? 2.
> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/amino/java/src/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java
>
> --
> Best Regards
> James Gan
> Current Project: Concurrent Building Block at
> http://amino-cbbs.sourceforge.net/
> Blog: http://ganzhi.blogspot.com
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From davidcholmes at aapt.net.au  Wed Aug  5 02:49:11 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 5 Aug 2009 16:49:11 +1000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <1ccfd1c10908042341p178c65e9lb789ac0e1dcb2781@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>


Martin Buchholz writes:
> On Tue, Aug 4, 2009 at 23:27, David
> Holmes<davidcholmes at aapt.net.au> wrote:
> > Correction: It was pointed out to me (thanks Joe!) that only
> > LinkedBlockingDeque supports the ability to add/remove at the
> > same end and so emulate a stack. I can't help but feel we have
> > a gap in our concurrent collections here ...
>
> I tend to feel the same.
> The "Treiber stack" is the simplest useful lock-free data structure,
> easier to implement than ConcurrentLinkedQueue.
> And it should be able to match the same API.
> ConcurrentLinkedStack?

Or perhaps a more general purpose ConcurrentLinkedList ?

David

> Martin
>
> >
> > David
> >
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> David Holmes
> > Sent: Wednesday, 5 August 2009 4:00 PM
> > To: James Gan; concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] Project Amino: Introduce
> Lock-Free Stack
> >
> > James,
> >
> > Is that a comparison again the ancient java.util.Stack? If so a better
> > comparison would be against a stack built over java.util.concurrent's
> > ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the blocking
> > semantics of your stack).
> >
> > Cheers,
> > David Holmes
> >
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> James Gan
> > Sent: Wednesday, 5 August 2009 3:49 PM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
> >
> > Hi, All
> >
> > During my recent test, stack component of Amino library works
> very well in
> > one of our product. The stack is used as a pool and its scalability is
> > better than lock-protected JDK stack. I was hoping that this
> component can
> > help others also.
> >
> > Performance chart can be viewed in Amino Blog:
> > ??? 1.
> http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html
> >
> > The source code is already published under Apache License in
> SourceForge:
> > ??? 2.
> >
> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/am
ino/java/src/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java
>
> --
> Best Regards
> James Gan
> Current Project: Concurrent Building Block at
> http://amino-cbbs.sourceforge.net/
> Blog: http://ganzhi.blogspot.com
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>



From jed at atlassian.com  Wed Aug  5 03:56:56 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Wed, 05 Aug 2009 17:56:56 +1000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>
Message-ID: <4A793B48.3050305@atlassian.com>

The Treiber stack is very simple, although it doesn't scale all that 
well. Implementing a concurrent List is a far more interesting 
proposition. Oh, oh, pick me!

I'll try and get something up for proposition.

cheers,
jed.

David Holmes wrote:
> Martin Buchholz writes:
>   
>> On Tue, Aug 4, 2009 at 23:27, David
>> Holmes<davidcholmes at aapt.net.au> wrote:
>>     
>>> Correction: It was pointed out to me (thanks Joe!) that only
>>> LinkedBlockingDeque supports the ability to add/remove at the
>>> same end and so emulate a stack. I can't help but feel we have
>>> a gap in our concurrent collections here ...
>>>       
>> I tend to feel the same.
>> The "Treiber stack" is the simplest useful lock-free data structure,
>> easier to implement than ConcurrentLinkedQueue.
>> And it should be able to match the same API.
>> ConcurrentLinkedStack?
>>     
>
> Or perhaps a more general purpose ConcurrentLinkedList ?
>
> David
>
>   
>> Martin
>>
>>     
>>> David
>>>
>>> -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu
>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>       
>> David Holmes
>>     
>>> Sent: Wednesday, 5 August 2009 4:00 PM
>>> To: James Gan; concurrency-interest at cs.oswego.edu
>>> Subject: Re: [concurrency-interest] Project Amino: Introduce
>>>       
>> Lock-Free Stack
>>     
>>> James,
>>>
>>> Is that a comparison again the ancient java.util.Stack? If so a better
>>> comparison would be against a stack built over java.util.concurrent's
>>> ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the blocking
>>> semantics of your stack).
>>>
>>> Cheers,
>>> David Holmes
>>>
>>> -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu
>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>       
>> James Gan
>>     
>>> Sent: Wednesday, 5 August 2009 3:49 PM
>>> To: concurrency-interest at cs.oswego.edu
>>> Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
>>>
>>> Hi, All
>>>
>>> During my recent test, stack component of Amino library works
>>>       
>> very well in
>>     
>>> one of our product. The stack is used as a pool and its scalability is
>>> better than lock-protected JDK stack. I was hoping that this
>>>       
>> component can
>>     
>>> help others also.
>>>
>>> Performance chart can be viewed in Amino Blog:
>>>     1.
>>>       
>> http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html
>>     
>>> The source code is already published under Apache License in
>>>       
>> SourceForge:
>>     
>>>     2.
>>>
>>>       
>> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/am
>>     
> ino/java/src/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java
>   
>> --
>> Best Regards
>> James Gan
>> Current Project: Concurrent Building Block at
>> http://amino-cbbs.sourceforge.net/
>> Blog: http://ganzhi.blogspot.com
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>     
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   


From ganzhi at gmail.com  Wed Aug  5 04:36:15 2009
From: ganzhi at gmail.com (James Gan)
Date: Wed, 5 Aug 2009 16:36:15 +0800
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <4A793B48.3050305@atlassian.com>
References: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>
	<4A793B48.3050305@atlassian.com>
Message-ID: <70c070d80908050136s6c26161rb45d63343b32f770@mail.gmail.com>

List is a very useful component indeed. Though if only push/pop operation
are needed, stack has  considerable performance&memory advantage.

There is an concurrent list in Amino project here:
https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/amino/java/src/main/java/org/amino/ds/lockfree/LockFreeList.java

On Wed, Aug 5, 2009 at 3:56 PM, Jed Wesley-Smith <jed at atlassian.com> wrote:

> The Treiber stack is very simple, although it doesn't scale all that well.
> Implementing a concurrent List is a far more interesting proposition. Oh,
> oh, pick me!
>
> I'll try and get something up for proposition.
>
> cheers,
> jed.
>
>
> David Holmes wrote:
>
>> Martin Buchholz writes:
>>
>>
>>> On Tue, Aug 4, 2009 at 23:27, David
>>> Holmes<davidcholmes at aapt.net.au> wrote:
>>>
>>>
>>>> Correction: It was pointed out to me (thanks Joe!) that only
>>>> LinkedBlockingDeque supports the ability to add/remove at the
>>>> same end and so emulate a stack. I can't help but feel we have
>>>> a gap in our concurrent collections here ...
>>>>
>>>>
>>> I tend to feel the same.
>>> The "Treiber stack" is the simplest useful lock-free data structure,
>>> easier to implement than ConcurrentLinkedQueue.
>>> And it should be able to match the same API.
>>> ConcurrentLinkedStack?
>>>
>>>
>>
>> Or perhaps a more general purpose ConcurrentLinkedList ?
>>
>> David
>>
>>
>>
>>> Martin
>>>
>>>
>>>
>>>> David
>>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>>
>>>>
>>> David Holmes
>>>
>>>
>>>> Sent: Wednesday, 5 August 2009 4:00 PM
>>>> To: James Gan; concurrency-interest at cs.oswego.edu
>>>> Subject: Re: [concurrency-interest] Project Amino: Introduce
>>>>
>>>>
>>> Lock-Free Stack
>>>
>>>
>>>> James,
>>>>
>>>> Is that a comparison again the ancient java.util.Stack? If so a better
>>>> comparison would be against a stack built over java.util.concurrent's
>>>> ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the blocking
>>>> semantics of your stack).
>>>>
>>>> Cheers,
>>>> David Holmes
>>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>>
>>>>
>>> James Gan
>>>
>>>
>>>> Sent: Wednesday, 5 August 2009 3:49 PM
>>>> To: concurrency-interest at cs.oswego.edu
>>>> Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
>>>>
>>>> Hi, All
>>>>
>>>> During my recent test, stack component of Amino library works
>>>>
>>>>
>>> very well in
>>>
>>>
>>>> one of our product. The stack is used as a pool and its scalability is
>>>> better than lock-protected JDK stack. I was hoping that this
>>>>
>>>>
>>> component can
>>>
>>>
>>>> help others also.
>>>>
>>>> Performance chart can be viewed in Amino Blog:
>>>>    1.
>>>>
>>>>
>>> http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html
>>>
>>>
>>>> The source code is already published under Apache License in
>>>>
>>>>
>>> SourceForge:
>>>
>>>
>>>>    2.
>>>>
>>>>
>>>>
>>> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/am
>>>
>>>
>> ino/java/src/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java
>>
>>
>>> --
>>> Best Regards
>>> James Gan
>>> Current Project: Concurrent Building Block at
>>> http://amino-cbbs.sourceforge.net/
>>> Blog: http://ganzhi.blogspot.com
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Best Regards
James Gan
Current Project: Concurrent Building Block at
http://amino-cbbs.sourceforge.net/
Blog: http://ganzhi.blogspot.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090805/340964fe/attachment.html>

From jed at atlassian.com  Thu Aug  6 22:47:39 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Fri, 07 Aug 2009 12:47:39 +1000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <4A793B48.3050305@atlassian.com>
References: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>
	<4A793B48.3050305@atlassian.com>
Message-ID: <4A7B95CB.4040406@atlassian.com>

OK, so I've been thinking a lot about it and have a working lock-free 
prototype (not prepared to call wait-free yet, but it has some wait-free 
characteristics). The thing is, there are a bunch of methods on the List 
interface that really don't make much sense in a concurrent context. 
Anything with an index is an obvious example, and of course size can 
only ever be approximate. A ConcurrentDeque makes somewhat more sense to 
implement, so I'll be giving that a go instead.

The whole project does pose some interesting questions as to relevancy. 
The following paper explains a few about why the author thinks a doubly 
linked list is not a concurrent data structure:

http://www.plannednonoperational.com/articles/public/concurrency_rabit_hole.pdf

Is there much point in continuing apart from my own enjoyment of the 
exercise?

cheers,
jed.

Jed Wesley-Smith wrote:
> The Treiber stack is very simple, although it doesn't scale all that 
> well. Implementing a concurrent List is a far more interesting 
> proposition. Oh, oh, pick me!
>
> I'll try and get something up for proposition.
>
> cheers,
> jed.
>
> David Holmes wrote:
>> Martin Buchholz writes:
>>  
>>> On Tue, Aug 4, 2009 at 23:27, David
>>> Holmes<davidcholmes at aapt.net.au> wrote:
>>>    
>>>> Correction: It was pointed out to me (thanks Joe!) that only
>>>> LinkedBlockingDeque supports the ability to add/remove at the
>>>> same end and so emulate a stack. I can't help but feel we have
>>>> a gap in our concurrent collections here ...
>>>>       
>>> I tend to feel the same.
>>> The "Treiber stack" is the simplest useful lock-free data structure,
>>> easier to implement than ConcurrentLinkedQueue.
>>> And it should be able to match the same API.
>>> ConcurrentLinkedStack?
>>>     
>>
>> Or perhaps a more general purpose ConcurrentLinkedList ?
>>
>> David
>>
>>  
>>> Martin
>>>
>>>    
>>>> David
>>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>>       
>>> David Holmes
>>>    
>>>> Sent: Wednesday, 5 August 2009 4:00 PM
>>>> To: James Gan; concurrency-interest at cs.oswego.edu
>>>> Subject: Re: [concurrency-interest] Project Amino: Introduce
>>>>       
>>> Lock-Free Stack
>>>    
>>>> James,
>>>>
>>>> Is that a comparison again the ancient java.util.Stack? If so a better
>>>> comparison would be against a stack built over java.util.concurrent's
>>>> ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the 
>>>> blocking
>>>> semantics of your stack).
>>>>
>>>> Cheers,
>>>> David Holmes
>>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>>       
>>> James Gan
>>>    
>>>> Sent: Wednesday, 5 August 2009 3:49 PM
>>>> To: concurrency-interest at cs.oswego.edu
>>>> Subject: [concurrency-interest] Project Amino: Introduce Lock-Free 
>>>> Stack
>>>>
>>>> Hi, All
>>>>
>>>> During my recent test, stack component of Amino library works
>>>>       
>>> very well in
>>>    
>>>> one of our product. The stack is used as a pool and its scalability is
>>>> better than lock-protected JDK stack. I was hoping that this
>>>>       
>>> component can
>>>    
>>>> help others also.
>>>>
>>>> Performance chart can be viewed in Amino Blog:
>>>>     1.
>>>>       
>>> http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html
>>>    
>>>> The source code is already published under Apache License in
>>>>       
>>> SourceForge:
>>>    
>>>>     2.
>>>>
>>>>       
>>> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/am
>>>     
>> ino/java/src/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java
>>  
>>> -- 
>>> Best Regards
>>> James Gan
>>> Current Project: Concurrent Building Block at
>>> http://amino-cbbs.sourceforge.net/
>>> Blog: http://ganzhi.blogspot.com
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>     
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>   
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Thu Aug  6 23:15:42 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 7 Aug 2009 13:15:42 +1000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <4A7B95CB.4040406@atlassian.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEPCICAA.davidcholmes@aapt.net.au>


Hi Jed,

That was an interesting read. It's quite bizarre that a slide containing a
picture of pieces of paper can be more engaging than a slide containing the
words on the papers :-)

The abstraction here is List, and doubly-linked-list is just a particular
implementation with particular properties/characteristics. Note I did say
ConcurrentLinkedList rather than ConcurrentDoublyLinkedList :) and I was
attempting to contrast with ConcurrentArrayList which would have some issues
with expanding.

But yes this does beg the question: does a concurrent List make sense, given
that a List implies some ordering/sequencing ? Perhaps not.

But for that matter does a concurrent stack make sense? What defines a
stack? The fact that items are removed in the opposite order to which they
are added. But in a concurrent environment how can you tell? :) Maybe all we
really need is an approximation of last-in-first-out, and a sequential stack
is a sufficient but not necessary solution to that problem.

Ditto for a ConcurrentQueue or ConcurrentDeque.

At this point I await someone who is really an expert on concurrent data
structures to chime in :-)

Cheers,
David Holmes

> Jed Wesley-Smith writes:
>
> OK, so I've been thinking a lot about it and have a working lock-free
> prototype (not prepared to call wait-free yet, but it has some wait-free
> characteristics). The thing is, there are a bunch of methods on the List
> interface that really don't make much sense in a concurrent context.
> Anything with an index is an obvious example, and of course size can
> only ever be approximate. A ConcurrentDeque makes somewhat more sense to
> implement, so I'll be giving that a go instead.
>
> The whole project does pose some interesting questions as to relevancy.
> The following paper explains a few about why the author thinks a doubly
> linked list is not a concurrent data structure:
>
> http://www.plannednonoperational.com/articles/public/concurrency_r
> abit_hole.pdf
>
> Is there much point in continuing apart from my own enjoyment of the
> exercise?
>
> cheers,
> jed.
>
> Jed Wesley-Smith wrote:
> > The Treiber stack is very simple, although it doesn't scale all that
> > well. Implementing a concurrent List is a far more interesting
> > proposition. Oh, oh, pick me!
> >
> > I'll try and get something up for proposition.
> >
> > cheers,
> > jed.
> >
> > David Holmes wrote:
> >> Martin Buchholz writes:
> >>
> >>> On Tue, Aug 4, 2009 at 23:27, David
> >>> Holmes<davidcholmes at aapt.net.au> wrote:
> >>>
> >>>> Correction: It was pointed out to me (thanks Joe!) that only
> >>>> LinkedBlockingDeque supports the ability to add/remove at the
> >>>> same end and so emulate a stack. I can't help but feel we have
> >>>> a gap in our concurrent collections here ...
> >>>>
> >>> I tend to feel the same.
> >>> The "Treiber stack" is the simplest useful lock-free data structure,
> >>> easier to implement than ConcurrentLinkedQueue.
> >>> And it should be able to match the same API.
> >>> ConcurrentLinkedStack?
> >>>
> >>
> >> Or perhaps a more general purpose ConcurrentLinkedList ?
> >>
> >> David
> >>
> >>
> >>> Martin
> >>>
> >>>
> >>>> David
> >>>>
> >>>> -----Original Message-----
> >>>> From: concurrency-interest-bounces at cs.oswego.edu
> >>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >>>>
> >>> David Holmes
> >>>
> >>>> Sent: Wednesday, 5 August 2009 4:00 PM
> >>>> To: James Gan; concurrency-interest at cs.oswego.edu
> >>>> Subject: Re: [concurrency-interest] Project Amino: Introduce
> >>>>
> >>> Lock-Free Stack
> >>>
> >>>> James,
> >>>>
> >>>> Is that a comparison again the ancient java.util.Stack? If
> so a better
> >>>> comparison would be against a stack built over java.util.concurrent's
> >>>> ConcurrentLinkedQueue, or LinkedBlockingQueue (depending on the
> >>>> blocking
> >>>> semantics of your stack).
> >>>>
> >>>> Cheers,
> >>>> David Holmes
> >>>>
> >>>> -----Original Message-----
> >>>> From: concurrency-interest-bounces at cs.oswego.edu
> >>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >>>>
> >>> James Gan
> >>>
> >>>> Sent: Wednesday, 5 August 2009 3:49 PM
> >>>> To: concurrency-interest at cs.oswego.edu
> >>>> Subject: [concurrency-interest] Project Amino: Introduce Lock-Free
> >>>> Stack
> >>>>
> >>>> Hi, All
> >>>>
> >>>> During my recent test, stack component of Amino library works
> >>>>
> >>> very well in
> >>>
> >>>> one of our product. The stack is used as a pool and its
> scalability is
> >>>> better than lock-protected JDK stack. I was hoping that this
> >>>>
> >>> component can
> >>>
> >>>> help others also.
> >>>>
> >>>> Performance chart can be viewed in Amino Blog:
> >>>>     1.
> >>>>
> >>> http://aminoprj.blogspot.com/2009/08/performance-of-amino-stack.html
> >>>
> >>>> The source code is already published under Apache License in
> >>>>
> >>> SourceForge:
> >>>
> >>>>     2.
> >>>>
> >>>>
> >>> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/am
> >>>
> >> ino/java/src/main/java/org/amino/ds/lockfree/LockFreeIndexedStack.java
> >>
> >>> --
> >>> Best Regards
> >>> James Gan
> >>> Current Project: Concurrent Building Block at
> >>> http://amino-cbbs.sourceforge.net/
> >>> Blog: http://ganzhi.blogspot.com
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>>
> >>>
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From jed at atlassian.com  Fri Aug  7 01:55:36 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Fri, 07 Aug 2009 15:55:36 +1000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEPCICAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEPCICAA.davidcholmes@aapt.net.au>
Message-ID: <4A7BC1D8.3070903@atlassian.com>

Hi David,

I don't necessarily think that these data structures make no sense in a 
concurrent environment, at least if you relax your requirement and idea 
of sequence*. There are plenty of techniques like elimination that will 
give you good throughput for a Stack, as long as your application 
doesn't need strict fair ordering (OFTOMH I can't think of one that 
would but there probably is one). Even thread-safe partially sequential 
algorithms are an improvement over fully sequential (synchronised) 
structures if they use techniques like lock striping (eg. 
ConcurrentHashMap).

The author points out that for real performance (read thoughput) 
relaxing the requirement for data to turn up immediately (delay) is 
actually beneficial ? which is basically what Doug's Fences API is 
hoping to achieve, more relaxed and weak happens-before relationships.

cheers,
jed.

* I always think that sequence as a fairly relativistic concept in a 
concurrent environment, at least until you require consensus anyway. As 
the referenced paper notes, anything that requires strict sequential 
consensus is not a concurrent structure anyway, just a thread-safe 
sequential one. A lock-free or even wait-free structure could similarly 
be argued to be sequential but with much finer grained happens-before 
guarantees and thus hopefully better throughput ? but I don't think 
that's all that useful for this discussion.

David Holmes wrote:
> Hi Jed,
>
> That was an interesting read. It's quite bizarre that a slide containing a
> picture of pieces of paper can be more engaging than a slide containing the
> words on the papers :-)
>
> The abstraction here is List, and doubly-linked-list is just a particular
> implementation with particular properties/characteristics. Note I did say
> ConcurrentLinkedList rather than ConcurrentDoublyLinkedList :) and I was
> attempting to contrast with ConcurrentArrayList which would have some issues
> with expanding.
>
> But yes this does beg the question: does a concurrent List make sense, given
> that a List implies some ordering/sequencing ? Perhaps not.
>
> But for that matter does a concurrent stack make sense? What defines a
> stack? The fact that items are removed in the opposite order to which they
> are added. But in a concurrent environment how can you tell? :) Maybe all we
> really need is an approximation of last-in-first-out, and a sequential stack
> is a sufficient but not necessary solution to that problem.
>
> Ditto for a ConcurrentQueue or ConcurrentDeque.
>
> At this point I await someone who is really an expert on concurrent data
> structures to chime in :-)
>
> Cheers,
> David Holmes
>
>   
>> Jed Wesley-Smith writes:
>>
>> OK, so I've been thinking a lot about it and have a working lock-free
>> prototype (not prepared to call wait-free yet, but it has some wait-free
>> characteristics). The thing is, there are a bunch of methods on the List
>> interface that really don't make much sense in a concurrent context.
>> Anything with an index is an obvious example, and of course size can
>> only ever be approximate. A ConcurrentDeque makes somewhat more sense to
>> implement, so I'll be giving that a go instead.
>>
>> The whole project does pose some interesting questions as to relevancy.
>> The following paper explains a few about why the author thinks a doubly
>> linked list is not a concurrent data structure:
>>
>> http://www.plannednonoperational.com/articles/public/concurrency_r
>> abit_hole.pdf
>>
>> Is there much point in continuing apart from my own enjoyment of the
>> exercise?
>>
>> cheers,
>> jed.


From jed at atlassian.com  Fri Aug  7 02:18:15 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Fri, 07 Aug 2009 16:18:15 +1000
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEPCICAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEPCICAA.davidcholmes@aapt.net.au>
Message-ID: <4A7BC727.80506@atlassian.com>

Hi David, my previous email didn't address your points directly:

David Holmes wrote:
> Hi Jed,
>
> That was an interesting read. It's quite bizarre that a slide containing a
> picture of pieces of paper can be more engaging than a slide containing the
> words on the papers :-)
>   

Yeah, it was a very effective device!

> The abstraction here is List, and doubly-linked-list is just a particular
> implementation with particular properties/characteristics. Note I did say
> ConcurrentLinkedList rather than ConcurrentDoublyLinkedList :) and I was
> attempting to contrast with ConcurrentArrayList which would have some issues
> with expanding.
>   

A single-link List is only half a List as it can only ever iterate 
(perfomantly) in one direction. Perfectly useful for a number of cases, 
but not a general purpose implementation. I'm trying to implement the 
whole interface, not just the easy bits :-)

> But yes this does beg the question: does a concurrent List make sense, given
> that a List implies some ordering/sequencing ? Perhaps not.
>   

That was where I ended up. Removing the index methods made life a lot 
easier and made a lot more sense.

> But for that matter does a concurrent stack make sense? What defines a
> stack? The fact that items are removed in the opposite order to which they
> are added. But in a concurrent environment how can you tell? :) Maybe all we
> really need is an approximation of last-in-first-out, and a sequential stack
> is a sufficient but not necessary solution to that problem.
>
> Ditto for a ConcurrentQueue or ConcurrentDeque.
>   

As I pointed out in my previous reply, they probably only don't make 
sense if strict fairness is required, besides you particularly need to 
know if you want to use them with single or multiple readers or writers.

> At this point I await someone who is really an expert on concurrent data
> structures to chime in :-)

Let's hope so!

cheers,
jed.

From dl at cs.oswego.edu  Fri Aug  7 06:47:23 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 07 Aug 2009 06:47:23 -0400
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <4A7B95CB.4040406@atlassian.com>
References: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>	<4A793B48.3050305@atlassian.com>
	<4A7B95CB.4040406@atlassian.com>
Message-ID: <4A7C063B.9070508@cs.oswego.edu>

Jed Wesley-Smith wrote:
> The whole project does pose some interesting questions as to relevancy. 
> The following paper explains a few about why the author thinks a doubly 
> linked list is not a concurrent data structure:
> 
> http://www.plannednonoperational.com/articles/public/concurrency_rabit_hole.pdf 
> 

You might also enjoy Guy Steele's recent talk:
"The future is parallel: What's a programmer to do?
Breaking sequential habits of thought" at
http://groups.csail.mit.edu/mac/users/gjs/6.945/readings/MITApril2009Steele.pdf
This makes some similar points, and has a nice
rationale for some of the ideas behind extra166y ParallelArray
classes and related follow-ons that we might someday
put into place in Java and/or other JVM-hosted languages.

While I'm at it: a historical note about lock-free Treiber
Stacks: I had originally planned to include these in initial
j.u.c release (Java5). The reasons for triaging them out
were (1) If you want them to support the Collection API, they
must implement method remove(Object x). This rarely
needed (but still sometimes defensibly needed) method
adds a lot of overhead and complexity (this also adds some
overhead/complexity to other Queue implementations but
proportionally less so). (2) Logically, they could/should
implement Queue, but using queue-based method names for
LIFO stacks is very confusing for users. (3) Basic
implementations supporting only push and pop are
simple enough to copy-paste-hack (which you can do from
the JCiP book examples at http://jcip.net/listings.html)


-Doug








From dl at cs.oswego.edu  Fri Aug  7 08:51:49 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 07 Aug 2009 08:51:49 -0400
Subject: [concurrency-interest] Fences
Message-ID: <4A7C2365.2020106@cs.oswego.edu>


One of the last tasks in JDK7 integration is to finalize
the Fences API in light of various unresolved discussions,
most of which surround the ugly names, awkward usages,
and unwanted implications about mappings to hardware.
(Thanks in particular to Jeremy Manson for not letting
up on this!)

I placed javadocs for one possible alternative, currently
just named "FencesV2" at:
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/FencesV2.html

Reactions would be welcome.

It differs in naming (for example "orderWrites" rather
than "preStoreFence") as well as typical usage -- by returning
the associated references, they may encourage a simpler
usage style. On the other hand, without those ugly
"pre"/"post" prefixes built into their names, they
might be more misusable.

This version also has more complete specs (although
still in need of further review) that are more clearly
tied to JLS chapter 17 terminology.

-Doug



From karmazilla at gmail.com  Fri Aug  7 09:48:22 2009
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Fri, 7 Aug 2009 15:48:22 +0200
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7C2365.2020106@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
Message-ID: <90622e530908070648r97a5857q358f94e4c365085a@mail.gmail.com>

I like the wording better in this one. Especially the informal
phrasings and the new method names. It also made the examples clearer.

I spotted some typos (minor stuff because this is what happens to jump
out at me):
   "r2 is contrained by w" in the orderReads doc is missing an 's'
   "nonnull" vs "non-null"
   missing space in "w such thatdereferences(ref, w)" in orderWrites doc
   should @return docs not end with a period?
   "--" vs "&mdash;" or "&ndash;"
   perhaps "c.getData()" (an the like) should be in a {@code } ?
   do the sentence "(In practice, among other changes, you would use
access methods instead of a public field)." really need that
parenthesis?



On Fri, Aug 7, 2009 at 2:51 PM, Doug Lea<dl at cs.oswego.edu> wrote:
>
> One of the last tasks in JDK7 integration is to finalize
> the Fences API in light of various unresolved discussions,
> most of which surround the ugly names, awkward usages,
> and unwanted implications about mappings to hardware.
> (Thanks in particular to Jeremy Manson for not letting
> up on this!)
>
> I placed javadocs for one possible alternative, currently
> just named "FencesV2" at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/FencesV2.html
>
> Reactions would be welcome.
>
> It differs in naming (for example "orderWrites" rather
> than "preStoreFence") as well as typical usage -- by returning
> the associated references, they may encourage a simpler
> usage style. On the other hand, without those ugly
> "pre"/"post" prefixes built into their names, they
> might be more misusable.
>
> This version also has more complete specs (although
> still in need of further review) that are more clearly
> tied to JLS chapter 17 terminology.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.

From gregg at cytetech.com  Fri Aug  7 10:10:27 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 07 Aug 2009 09:10:27 -0500
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <4A7BC727.80506@atlassian.com>
References: <NFBBKALFDCPFIDBNKAPCKEPCICAA.davidcholmes@aapt.net.au>
	<4A7BC727.80506@atlassian.com>
Message-ID: <4A7C35D3.1070004@cytetech.com>

Jed Wesley-Smith wrote:
> Hi David, my previous email didn't address your points directly:
> 
> David Holmes wrote:
>> But for that matter does a concurrent stack make sense? What defines a
>> stack? The fact that items are removed in the opposite order to which 
>> they
>> are added. But in a concurrent environment how can you tell? :) Maybe 
>> all we
>> really need is an approximation of last-in-first-out, and a sequential 
>> stack
>> is a sufficient but not necessary solution to that problem.
>>
>> Ditto for a ConcurrentQueue or ConcurrentDeque.
>>   
> 
> As I pointed out in my previous reply, they probably only don't make 
> sense if strict fairness is required, besides you particularly need to 
> know if you want to use them with single or multiple readers or writers.
> 
>> At this point I await someone who is really an expert on concurrent data
>> structures to chime in :-)
> 
> Let's hope so!

I think it's probably not a good idea to consider that all methods on a List 
will be used from all types of concurrency.  Rather, I'd like to suggest that 
perhaps a non-concurrent environment might use index methods to sort or 
otherwise rearrange a list contents, and then publish this list into a 
concurrent environment, to be consumed as a "work" queue or some other such 
strickly end-to-end traversal/consumption.

The lifecycle of data structues in a concurrent environment is hardly ever 
single phased.  There are often multiple phases, and each phase can present 
different requirements on the operation of the data structure right?

Gregg Wonderly

From dl at cs.oswego.edu  Fri Aug  7 10:18:24 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 07 Aug 2009 10:18:24 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <90622e530908070648r97a5857q358f94e4c365085a@mail.gmail.com>
References: <4A7C2365.2020106@cs.oswego.edu>
	<90622e530908070648r97a5857q358f94e4c365085a@mail.gmail.com>
Message-ID: <4A7C37B0.8020209@cs.oswego.edu>

Christian Vest Hansen wrote:
> I spotted some typos (minor stuff because this is what happens to jump
> out at me):

Thanks! These are fixed in the regenerated version
(http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/FencesV2.html)

-Doug





From Online at Stolsvik.com  Fri Aug  7 10:48:32 2009
From: Online at Stolsvik.com (=?UTF-8?Q?Endre_St=C3=B8lsvik?=)
Date: Fri, 7 Aug 2009 16:48:32 +0200
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7C2365.2020106@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
Message-ID: <1501fdf40908070748w552e8b2aq6a860bf1a83ddaf7@mail.gmail.com>

On Fri, Aug 7, 2009 at 14:51, Doug Lea <dl at cs.oswego.edu> wrote:

> It differs in naming (for example "orderWrites" rather
> than "preStoreFence") as well as typical usage -- by returning
> the associated references, they may encourage a simpler
> usage style. On the other hand, without those ugly
> "pre"/"post" prefixes built into their names, they
> might be more misusable.


What about sticking "Before" and "After" after the method names you suggest?
Unless one use these methods very often (which I understad most won't), I
believe those pre of postfixes will be helpful for ones memory.

Endre.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090807/40409be1/attachment.html>

From sberlin at gmail.com  Fri Aug  7 11:21:36 2009
From: sberlin at gmail.com (Sam Berlin)
Date: Fri, 7 Aug 2009 11:21:36 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <1501fdf40908070748w552e8b2aq6a860bf1a83ddaf7@mail.gmail.com>
References: <4A7C2365.2020106@cs.oswego.edu>
	<1501fdf40908070748w552e8b2aq6a860bf1a83ddaf7@mail.gmail.com>
Message-ID: <19196d860908070821q24c81185ndf8125ff657d4c2f@mail.gmail.com>

I do not profess any knowledge of the Fences API other than the most basic
cursory knowledge from reading posts on this list... but along the lines of
'before' & 'after', what about: orderEarlierWrites, orderLaterReads, etc..

Sam

2009/8/7 Endre St?lsvik <Online at stolsvik.com>

> On Fri, Aug 7, 2009 at 14:51, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> It differs in naming (for example "orderWrites" rather
>> than "preStoreFence") as well as typical usage -- by returning
>> the associated references, they may encourage a simpler
>> usage style. On the other hand, without those ugly
>> "pre"/"post" prefixes built into their names, they
>> might be more misusable.
>
>
> What about sticking "Before" and "After" after the method names you
> suggest? Unless one use these methods very often (which I understad most
> won't), I believe those pre of postfixes will be helpful for ones memory.
>
> Endre.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090807/ecad5218/attachment.html>

From dl at cs.oswego.edu  Fri Aug  7 12:06:00 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 07 Aug 2009 12:06:00 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <19196d860908070821q24c81185ndf8125ff657d4c2f@mail.gmail.com>
References: <4A7C2365.2020106@cs.oswego.edu>	
	<1501fdf40908070748w552e8b2aq6a860bf1a83ddaf7@mail.gmail.com>
	<19196d860908070821q24c81185ndf8125ff657d4c2f@mail.gmail.com>
Message-ID: <4A7C50E8.6050306@cs.oswego.edu>

Sam Berlin wrote:
> I do not profess any knowledge of the Fences API other than the most 
> basic cursory knowledge from reading posts on this list... but along the 
> lines of 'before' & 'after', what about: orderEarlierWrites, 
> orderLaterReads, etc..
> 

One of the challenges in specs associated with memory models
is that there are many ordering relations, none of them
exactly corresponding to everyday terms. The current
specs use "prior" and "subsequent" with respect to
program order, which is probably the most common.
But to make methods self-documenting, you'd need to
create method names like: orderPriorWritesBeforeSubsequentWrites.
It seems marginally better to omit the adjectives
(which I did in FencesV2). Or use names like "preStoreFence"
(as in initial version) that don't have much meaning in
themselves, but have names that tell you where to place them.

-Doug

From sberlin at gmail.com  Fri Aug  7 14:47:44 2009
From: sberlin at gmail.com (Sam Berlin)
Date: Fri, 7 Aug 2009 14:47:44 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7C50E8.6050306@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<1501fdf40908070748w552e8b2aq6a860bf1a83ddaf7@mail.gmail.com>
	<19196d860908070821q24c81185ndf8125ff657d4c2f@mail.gmail.com>
	<4A7C50E8.6050306@cs.oswego.edu>
Message-ID: <19196d860908071147i1aed85ei5e21f9d628ce6ee5@mail.gmail.com>

Longer names might actually be a good choice, if only because a name like
"orderWrites" sounds a lot more simple than a name like
"orderPriorWritesBeforeSubsequentWrites".  The long name gives some
explanation of what's it going to do as well as sounding imposing.
"orderWrites" by itself gives some explanation, but seems so simple that
people might mistake the way it's used or what it's supposed to do.
Conversely, "preStoreFence" sounds imposing, but doesn't give much
explanation by itself.  The long name seems like a good middle ground.

(Of course, I don't see myself ever actually using the API... so these
comments should be taken with a grain of salt.)

Sam

On Fri, Aug 7, 2009 at 12:06 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> Sam Berlin wrote:
>
>> I do not profess any knowledge of the Fences API other than the most basic
>> cursory knowledge from reading posts on this list... but along the lines of
>> 'before' & 'after', what about: orderEarlierWrites, orderLaterReads, etc..
>>
>>
> One of the challenges in specs associated with memory models
> is that there are many ordering relations, none of them
> exactly corresponding to everyday terms. The current
> specs use "prior" and "subsequent" with respect to
> program order, which is probably the most common.
> But to make methods self-documenting, you'd need to
> create method names like: orderPriorWritesBeforeSubsequentWrites.
> It seems marginally better to omit the adjectives
> (which I did in FencesV2). Or use names like "preStoreFence"
> (as in initial version) that don't have much meaning in
> themselves, but have names that tell you where to place them.
>
> -Doug
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090807/06ba50e4/attachment.html>

From hans.boehm at hp.com  Fri Aug  7 18:40:07 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 7 Aug 2009 22:40:07 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7C2365.2020106@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>

Doug -

[Copying Paul McKenney, who should look at the appended message first.]

This is an improvement, but I still have some serious issues:

0. I'm fine with reachabilityFence, except for the "undefined behavior" statement.
I don't think you can do that in Java, since it destroys any hope of proving
security properties, right?  Untrusted, sandboxed code can always execute
reachabilityFence(null), and we presumably have to not allow that to do anything bad.
Requiring it to be a no-op seems fine, and doesn't slow down the obvious implementation.

1. I'm really confused by the more formal specifications.  Some of the issues there are superficial:  the orderReads specification seems to introduce constraints on a variable r that's never used.  There seems to be at least one critical typo in 17.5.1 in the definition of a dereference chain (the spurious(?) r before "dereferences(r, a)").  I don't see any obvious way to correct the latter to turn this into a proper mathematical definition of a relation, though I can sort of guess what was intended.  (I thought the earlier version of this from Jeremey looked OK.)  If we're going to use this, I'd repeat a demangled version of the definition.  Maybe the correct version is:

For a read action r, and an action a that reads or writes object o in thread t, dereferences(r, a) holds iff t did not initialize o, and a accesses the object (o)
returned by r.

Did I get that approximately correct?

2.  I still don't see how you could possibly emulate final fields without also requiring a fence on the reader side, or effectively constraining the implementation of ordinary reads.  Java implementations are only required to preserve data-dependency ordering for final fields.  For other fields, I believe an access to x.a may still appear to happen before the access to x.  It may be that this freedom is not important for any up-to-date implementations.  But it would be a change in implementation requirements.  And it would probably further slow down any (are there likely to be any any?) Java 7 implementations on Alpha.  Not that that's likely to be a big deal.

If I read the "more formally" description correctly, you are constraining reads?
What's the impact of this on the rules for common subexpression elimination?  (I know there are issues there anyway, but I'm afraid of pontentially breaking things further.)

3. The volatile emulation seems like it's at best an approximation.  If you implement it with the obvious fences, I think you end up with a PowerPC implementation which is faster than what the PowerPC guys recommend.  This leads me to suspect it's not correct there.  It generally doesn't seem to guarantee the correct outcome for IRIW.  I'm not sure about more interesting examples.  It also depends on parts of the "more formal" definitions I don't yet understand.

I also suspect that the orderWrites fence you need for volatile emulation is different from the one you need for final field emulation.  For volatile emulation, the new relationships really do need to compose with happens-before.  For final-field emulation, I suspect it's critical that they don't.

4. I would strongly prefer fences that order all accesses with respect to a subsequent write or all prior accesses with respect to a previous read.  Otherwise you get very strange races, e.g. (taking syntactic liberties):

T1:
x = 0;
++x;
writefence;
done_with_x = true;

T2:
while (!done_with_x);
fullfence;
r1 = x;
x = 2;  // Race! not ordered with respect to read in ++x

In real cases, this seems to make the code senstive to how a particular object is initialized;  {x = 0; ++x; } is not equivalent to {x = 1;}

And there seems to be essentially no hardware (again Alpha excepted) on which read and write fences really buy you something substantial.

5. The "more formal" definition of orderAccesses doesn't make sense to me.  You normally need a pair of fences to establish a happens-before ordering.  What does "subsequent" mean?  We have w hb f .  If we also have w hb r, or f hb r, then the added
synchronizes with relationship is redundant anyway.

Overall:  Aside from reachabilityFence, which I strongly advocate, and I think is very well understood, this still seems like we're opening a really large can of worms.  I think the downside risk here is huge, since this implicitly constrains (already insufficiently understood) compiler optimization rules for ordinary field accesses.  I think I understand the motivation for doing this.  But I'm not sure fences are sufficiently definable, especially on a reasonable schedule, that you really want to commit all future implementations to supporting them.  Getting these right seems even harder than the other parts of JSR133, and we know we didn't get those all right.  Plus this has to interact correctly with the old pieces.  (Interactions with causality, anyone?)

Since this is intended to replace even worse approaches, I'm not sure what the right solution is.  Define this API as deprecated from the start?  (I'm not sure whether I'm joking ...)

Hans



> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Doug Lea
> Sent: Friday, August 07, 2009 5:52 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Fences
> 
> 
> One of the last tasks in JDK7 integration is to finalize the 
> Fences API in light of various unresolved discussions, most 
> of which surround the ugly names, awkward usages, and 
> unwanted implications about mappings to hardware.
> (Thanks in particular to Jeremy Manson for not letting up on this!)
> 
> I placed javadocs for one possible alternative, currently 
> just named "FencesV2" at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/FencesV2.html
> 
> Reactions would be welcome.
> 
> It differs in naming (for example "orderWrites" rather than 
> "preStoreFence") as well as typical usage -- by returning the 
> associated references, they may encourage a simpler usage 
> style. On the other hand, without those ugly "pre"/"post" 
> prefixes built into their names, they might be more misusable.
> 
> This version also has more complete specs (although still in 
> need of further review) that are more clearly tied to JLS 
> chapter 17 terminology.
> 
> -Doug
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From dl at cs.oswego.edu  Sat Aug  8 08:56:56 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 08 Aug 2009 08:56:56 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A7D7618.9050501@cs.oswego.edu>

Thanks for the helpful comments! I remain optimistic...

Boehm, Hans wrote:
> 0. I'm fine with reachabilityFence, except for the "undefined behavior" 
> statement. I don't think you can do that in Java, since it destroys any hope 
> of proving security properties, right?

Well, we do this all the time (for example,
disclaimers on effects of queue.drainTo(c) if c is
independently changing), but with the implicit understanding
that the range of "undefined behavior" does not impact
security. Maybe we should do something about
making this fact explicit across JDK specs.

> Requiring it to be a no-op seems fine, and doesn't slow down the obvious
> implementation.

But you are otherwise right that we might as well say
that null args have no effect for all these methods.

This and some other changes are in regenerated version
(http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/FencesV2.html)

> 
> the orderReads specification seems to introduce constraints on a variable r
> that's never used.

Oops, should be "r1"

> There seems to be at least one critical typo in 17.5.1 in the definition of a
> dereference chain (the spurious(?) r before "dereferences(r, a)").  I don't
> see any obvious way to correct the latter to turn this into a proper
> mathematical definition of a relation, though I can sort of guess what was
> intended.  (I thought the earlier version of this from Jeremey looked OK.)
> If we're going to use this, I'd repeat a demangled version of the definition.
> Maybe the correct version is:
> 
> For a read action r, and an action a that reads or writes object o in thread 
> t, dereferences(r, a) holds iff t did not initialize o, and a accesses the 
> object (o) returned by r.
> 
> Did I get that approximately correct?

I think so. We'll have to file a bug report for that JLS wording
(which like a few others, could have been mangled during
JLS integration of JSR133 spec). In the mean time, I copied
corrected version into class doc.


> 
> 2.  I still don't see how you could possibly emulate final fields without 
> also requiring a fence on the reader side, or effectively constraining the 
> implementation of ordinary reads.  Java implementations are only required to 
> preserve data-dependency ordering for final fields.  For other fields, I 
> believe an access to x.a may still appear to happen before the access to x. 
> It may be that this freedom is not important for any up-to-date 
> implementations.  But it would be a change in implementation requirements. 
> And it would probably further slow down any (are there likely to be any any?)
>  Java 7 implementations on Alpha.  Not that that's likely to be a big deal.

Background for other readers: the Alpha processor (used in "Tru64"
systems) has serious problems conforming to reasonable memory
models, because its spec allows things like speculating the value
of x.a  before reading x unless prohibited by a fence instruction.
(No other MP-able processor we know of has this mis-feature.)
As a minimal accommodation to this, the JSR133 spec contains
what amounts to a loophole that only strictly requires Alpha use
these fences for fields marked as "final". But since Alpha-based
systems have been been at end-of-lifetime for a few years now,
it seems crazy to let this get in the way of solving real
programming problems people have. On the other hand, I see that
there is a JSR133/Java5 JVM for Alpha (http://www.compaq.com/java/alpha/)
so it is conceivable that this would have some impact if they
continue post-EOL updates.

> 
> If I read the "more formally" description correctly, you are constraining 
> reads? What's the impact of this on the rules for common subexpression 
> elimination?  (I know there are issues there anyway, but I'm afraid of 
> pontentially breaking things further.)

I don't see any further breakage. If those JMM/JLS issues are fixed,
then it will be comparatively trivial to make any corresponding
adjustments in these specs.

> 
> 3. The volatile emulation seems like it's at best an approximation.  If you 
> implement it with the obvious fences, I think you end up with a PowerPC 
> implementation which is faster than what the PowerPC guys recommend.  This 
> leads me to suspect it's not correct there.  It generally doesn't seem to 
> guarantee the correct outcome for IRIW.

Sorry, I don't see this. Notice that orderAccesses ensures synchronizes-with.

(On PowerPC, I think both orderReads and orderWrites normally translate
to "lwsync" (plus compiler constraints), although can sometimes
be further optimized as in Paul and Raul's C++ mappings document
http://www.rdrop.com/users/paulmck/scalability/paper/N2745r.2009.02.22a.html
And orderAccesses to "hwsync". Using these seems to give heavier,
not lighter volatile implementation compared to best optimized forms
of volatiles, but not by much.

> 
> I also suspect that the orderWrites fence you need for volatile emulation is 
> different from the one you need for final field emulation.  For volatile 
> emulation, the new relationships really do need to compose with 
> happens-before.  For final-field emulation, I suspect it's critical that they
>  don't.

I don't think different ones are strictly needed, because of the
strengthening provided by the trailing orderAccesses. It may well
be that a few processors/implementations could optimize a little
better if they were separated though, but again, not by much.

> 
> 4. I would strongly prefer fences that order all accesses with respect to a 
> subsequent write or all prior accesses with respect to a previous read. 
> Otherwise you get very strange races, e.g. (taking syntactic liberties):
> 
> T1: x = 0; ++x; writefence; done_with_x = true;
> 
> T2: while (!done_with_x); fullfence; r1 = x; x = 2;  // Race! not ordered 
> with respect to read in ++x
> 
> In real cases, this seems to make the code senstive to how a particular 
> object is initialized;  {x = 0; ++x; } is not equivalent to {x = 1;}
> 
> And there seems to be essentially no hardware (again Alpha excepted) on which
>  read and write fences really buy you something substantial.

Right. There is no difference in implementation of
{Load|Store}StoreFence vs just StoreStoreFence on
any other processor (and similarly for load), and
no additional interesting compiler constraints.
The reason for limiting to just StoreStore
(for orderWrites) was just a usabilility/spec/API issue when
moving from xyFence to orderX style.
But it would not hurt to broaden them.

> 
> 5. The "more formal" definition of orderAccesses doesn't make sense to me. 
> You normally need a pair of fences to establish a happens-before ordering. 
> What does "subsequent" mean?  We have w hb f .  If we also have w hb r, or f 
> hb r, then the added synchronizes with relationship is redundant anyway.

This is basically just a rephrasing of bullet two of sec 17.4.4
(http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.4.4)
but I now see should also include its trailing "(where subsequent is defined
according to the synchronization order)." So much for using
"subsequent" here only  wrt program order. Sigh.

> 
> Since this is intended to replace even worse approaches, I'm not sure what 
> the right solution is.  Define this API as deprecated from the start?  (I'm 
> not sure whether I'm joking ...)

It still seems vastly preferable that we provide a mechanism
people can use to fix serious known bugs (unsafe publication
chief among them), even at the expense of introducing some more
minor challenges to efforts to fix underlying specs.
For an example of impact, the number of memory model
bugs out there might drop significantly if dependency injection
tools inserted appropriate calls to fence methods.

-Doug






From tim at peierls.net  Sat Aug  8 09:36:50 2009
From: tim at peierls.net (Tim Peierls)
Date: Sat, 8 Aug 2009 09:36:50 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7D7618.9050501@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
Message-ID: <63b4e4050908080636o7a1d5609r600a344e85156cd8@mail.gmail.com>

On Sat, Aug 8, 2009 at 8:56 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> For an example of impact, the number of memory model
> bugs out there might drop significantly if dependency injection
> tools inserted appropriate calls to fence methods.
>

I think this is a great selling point that we could push. (A little FUD,
perhaps, but whatever works.)

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090808/3bc8d13d/attachment-0001.html>

From davidcholmes at aapt.net.au  Sun Aug  9 01:28:22 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 9 Aug 2009 15:28:22 +1000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7D7618.9050501@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEPJICAA.davidcholmes@aapt.net.au>


Doug writes:
> It still seems vastly preferable that we provide a mechanism
> people can use to fix serious known bugs (unsafe publication
> chief among them)

This statement has me concerned. Of all the ways to fix unsafe publication,
expecting people to use the Fence API does not strike me as something we'd
want to advocate.

What kind of unsafe publication scenarios do you have in mind here? And
where would the fence be applied to fix them?

David


From peter.kovacs.1.0rc at gmail.com  Sun Aug  9 01:36:45 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Sun, 9 Aug 2009 07:36:45 +0200
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7D7618.9050501@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
Message-ID: <fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>

On Sat, Aug 8, 2009 at 2:56 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> Thanks for the helpful comments! I remain optimistic...
>
> Boehm, Hans wrote:
>

[snip]


> Since this is intended to replace even worse approaches, I'm not sure what
>> the right solution is.  Define this API as deprecated from the start?  (I'm
>> not sure whether I'm joking ...)
>>
>
> It still seems vastly preferable that we provide a mechanism
> people can use to fix serious known bugs (unsafe publication
> chief among them), even at the expense of introducing some more
> minor challenges to efforts to fix underlying specs.
> For an example of impact, the number of memory model
> bugs out there might drop significantly if dependency injection
> tools inserted appropriate caPlls to fence methods.


 Please, could someone give (point to) some background information? Which
dependency injection tools are specifically meant here? Are the issues with
these dependency injection tools unsolvable using currently available API?

Thanks
Peter


>
> -Doug
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090809/633db6dd/attachment.html>

From dl at cs.oswego.edu  Sun Aug  9 06:49:05 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 09 Aug 2009 06:49:05 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>
References: <4A7C2365.2020106@cs.oswego.edu>	
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	
	<4A7D7618.9050501@cs.oswego.edu>
	<fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>
Message-ID: <4A7EA9A1.8010902@cs.oswego.edu>

P?ter Kov?cs wrote:
>     For an example of impact, the number of memory model
>     bugs out there might drop significantly if dependency injection
>     tools inserted appropriate caPlls to fence methods.
> 
> 
>  Please, could someone give (point to) some background information? 
> Which dependency injection tools are specifically meant here? Are the 
> issues with these dependency injection tools unsolvable using currently 
> available API?
> 

Bob Lee and/or Tim Peierls could probably provide more
details about particular frameworks, but the main
cases are of the form illustrated in the "emulating final"
examples (pasted below).
A tool arranges that fields referencing components are bound
during an initialization phase. The references act as final
fields but sometimes (depending on the way initialization
is arranged by tools) cannot be declared as final.

As Tim cryptically noted, there is some fear, uncertainty
and doubt out there about the safety of some of this generated
initialization code across various usage contexts,
that would in most cases be settled by having tools
(not the application programmers) generate orderWrites
calls as appropriate.

...
[pasting from 
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/FencesV2.html]



Emulating final. With care, method orderWrites may be used to obtain the memory 
safety effects of final for a field that cannot be declared as final, because 
its primary initialization cannot be performed in a constructor, in turn because 
it is used in a framework requiring that all classes have a no-argument 
constructor; as in:

  class WidgetHolder {
    private Widget widget;
    public WidgetHolder() {}
    public static WidgetHolder newWidgetHolder(Params params) {
      WidgetHolder h = new WidgetHolder();
      h.widget = new Widget(params);
      return orderWrites(h);
   }
  }

Here, the invocation of orderWrites makes sure that the effects of the widget 
assignment are ordered before those of any (unknown) subsequent stores of h in 
other variables that make h available for use by other objects. Notice that this 
method observes the care required for final fields: It does not internally 
"leak" the reference by using it as an argument to a callback method or adding 
it to a static data structure. If such functionality were required, it may be 
possible to cope using more extensive sets of fences, or as a normally better 
choice, using synchronization (locking). Notice also that because final  could 
not be used here, the compiler and JVM cannot help you ensure that the field is 
set correctly across all usages.

An alternative approach is to place similar mechanics in the (sole) method that 
makes such objects available for use by others. Here is a stripped-down example 
illustrating the essentials. In practice, among other changes, you would use 
access methods instead of a public field.

  class AnotherWidgetHolder {
    public Widget widget;
    void publish(Widget w) {
      this.widget = orderWrites(w);
    }
    // ...
  }





From dl at cs.oswego.edu  Sun Aug  9 08:25:00 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 09 Aug 2009 08:25:00 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7C50E8.6050306@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>		<1501fdf40908070748w552e8b2aq6a860bf1a83ddaf7@mail.gmail.com>	<19196d860908070821q24c81185ndf8125ff657d4c2f@mail.gmail.com>
	<4A7C50E8.6050306@cs.oswego.edu>
Message-ID: <4A7EC01C.3030706@cs.oswego.edu>

Doug Lea wrote:
> But to make methods self-documenting, you'd need to
> create method names like: orderPriorWritesBeforeSubsequentWrites.

This naming style is even less comprehensible when
spec'ing the store fence case to also order load->store, as
it should (see exchange with Hans). So under this style,
the shorter names seem clearer.

Since no one has responded that they prefer the old
"preStoreFence" etc names, I committed FencesV2 as
Fences. See
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html

The "more formally" sections still need some work.

-Doug







From tim at peierls.net  Sun Aug  9 10:24:25 2009
From: tim at peierls.net (Tim Peierls)
Date: Sun, 9 Aug 2009 10:24:25 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7EC01C.3030706@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<1501fdf40908070748w552e8b2aq6a860bf1a83ddaf7@mail.gmail.com>
	<19196d860908070821q24c81185ndf8125ff657d4c2f@mail.gmail.com>
	<4A7C50E8.6050306@cs.oswego.edu> <4A7EC01C.3030706@cs.oswego.edu>
Message-ID: <63b4e4050908090724i49a17175mcfe0ca750dc0ae17@mail.gmail.com>

On Sun, Aug 9, 2009 at 8:25 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> The "more formally" sections still need some work.
>

I find it confusing that the JLS defines deferences(r, a) with r being a
read action, but the Fences javadocs say deferences(ref, a) with ref being
an object reference (while still quoting the JLS definition).

Femces -> Fences in AnotherWidgetHolder

extRes -> ext in the action2 example.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090809/100b0c67/attachment.html>

From peter.kovacs.1.0rc at gmail.com  Mon Aug 10 04:58:44 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Mon, 10 Aug 2009 10:58:44 +0200
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7EA9A1.8010902@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>
	<4A7EA9A1.8010902@cs.oswego.edu>
Message-ID: <fdeb32eb0908100158l5a2aff66o8dd866404b3407e7@mail.gmail.com>

Thank you, Doug, for the clarification!

Peter

2009/8/9 Doug Lea <dl at cs.oswego.edu>

> P?ter Kov?cs wrote:
>
>>    For an example of impact, the number of memory model
>>    bugs out there might drop significantly if dependency injection
>>    tools inserted appropriate caPlls to fence methods.
>>
>>
>>  Please, could someone give (point to) some background information? Which
>> dependency injection tools are specifically meant here? Are the issues with
>> these dependency injection tools unsolvable using currently available API?
>>
>>
> Bob Lee and/or Tim Peierls could probably provide more
> details about particular frameworks, but the main
> cases are of the form illustrated in the "emulating final"
> examples (pasted below).
> A tool arranges that fields referencing components are bound
> during an initialization phase. The references act as final
> fields but sometimes (depending on the way initialization
> is arranged by tools) cannot be declared as final.
>
> As Tim cryptically noted, there is some fear, uncertainty
> and doubt out there about the safety of some of this generated
> initialization code across various usage contexts,
> that would in most cases be settled by having tools
> (not the application programmers) generate orderWrites
> calls as appropriate.
>
> ...
> [pasting from
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/FencesV2.html
> ]
>
>
>
> Emulating final. With care, method orderWrites may be used to obtain the
> memory safety effects of final for a field that cannot be declared as final,
> because its primary initialization cannot be performed in a constructor, in
> turn because it is used in a framework requiring that all classes have a
> no-argument constructor; as in:
>
>  class WidgetHolder {
>   private Widget widget;
>   public WidgetHolder() {}
>   public static WidgetHolder newWidgetHolder(Params params) {
>     WidgetHolder h = new WidgetHolder();
>     h.widget = new Widget(params);
>     return orderWrites(h);
>  }
>  }
>
> Here, the invocation of orderWrites makes sure that the effects of the
> widget assignment are ordered before those of any (unknown) subsequent
> stores of h in other variables that make h available for use by other
> objects. Notice that this method observes the care required for final
> fields: It does not internally "leak" the reference by using it as an
> argument to a callback method or adding it to a static data structure. If
> such functionality were required, it may be possible to cope using more
> extensive sets of fences, or as a normally better choice, using
> synchronization (locking). Notice also that because final  could not be used
> here, the compiler and JVM cannot help you ensure that the field is set
> correctly across all usages.
>
> An alternative approach is to place similar mechanics in the (sole) method
> that makes such objects available for use by others. Here is a stripped-down
> example illustrating the essentials. In practice, among other changes, you
> would use access methods instead of a public field.
>
>  class AnotherWidgetHolder {
>   public Widget widget;
>   void publish(Widget w) {
>     this.widget = orderWrites(w);
>   }
>   // ...
>  }
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090810/d1320476/attachment.html>

From dl at cs.oswego.edu  Mon Aug 10 07:23:33 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 10 Aug 2009 07:23:33 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A800335.1080601@cs.oswego.edu>

Boehm, Hans wrote:
> If I read the "more formally" description correctly, you are constraining reads?

Yes, now I see what you mean...

> ...
> You normally need a pair of fences to establish a happens-before ordering. 

I had forgotten about a lingering spec problem here.
(Thanks also to Bill Pugh for pointing it out in a discussion.)
You are right that the non-initialization case only implicitly
required pairings of order{Writes,Accesses} with corresponding
orderReads. And because of this, orderReads spec was too strong,
implying orderings even when not paired.
The ground-rules here make this tricky to fix. You'd
like spec to refer to paired-up fences associated
with the actual reads/writes. But we cannot express
this because we don't have call-by-ref or address-of,
so people cannot say for example (C-ish) orderWrites(&r.x).
Another way out would be to require people to create
instances of Fence objects so they could be matched up
by identity. (As Bill and I sketched out, doing this
seems to lead to fairly pleasant and non-controversial specs.)
But this would require for example creating
entire arrays of them when used to control orderings
on other arrays, which would entail a lot of useless overhead.
The only way out that I can see is to implicitly identify
the pairings using scopes, which I'll try working out.

In the mean time, I did at least clarify the non-formal
descriptions.
(http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html)

-Doug


From dl at cs.oswego.edu  Mon Aug 10 07:51:59 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 10 Aug 2009 07:51:59 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <63b4e4050908090724i49a17175mcfe0ca750dc0ae17@mail.gmail.com>
References: <4A7C2365.2020106@cs.oswego.edu>	
	<1501fdf40908070748w552e8b2aq6a860bf1a83ddaf7@mail.gmail.com>	
	<19196d860908070821q24c81185ndf8125ff657d4c2f@mail.gmail.com>	
	<4A7C50E8.6050306@cs.oswego.edu> <4A7EC01C.3030706@cs.oswego.edu>
	<63b4e4050908090724i49a17175mcfe0ca750dc0ae17@mail.gmail.com>
Message-ID: <4A8009DF.5040903@cs.oswego.edu>

Tim Peierls wrote:

> I find it confusing that the JLS defines deferences(r, a) with r being a 
> read action, but the Fences javadocs say deferences(ref, a) with ref 
> being an object reference (while still quoting the JLS definition).
> 

The "ref" argument here is intrinsically the result of some read,
so this part works out, although suggested clarifications would be
welcome.

> Femces -> Fences in AnotherWidgetHolder
> 
> extRes -> ext in the action2 example.

Thanks!

-Doug


From hans.boehm at hp.com  Mon Aug 10 17:43:19 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 10 Aug 2009 21:43:19 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7D7618.9050501@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>

 

> -----Original Message-----
> From: Doug Lea [mailto:dl at cs.oswego.edu] 
> > 
> > 2.  I still don't see how you could possibly emulate final fields 
> > without also requiring a fence on the reader side, or effectively 
> > constraining the implementation of ordinary reads.  Java 
> > implementations are only required to preserve 
> data-dependency ordering 
> > for final fields.  For other fields, I believe an access to 
> x.a may still appear to happen before the access to x.
> > It may be that this freedom is not important for any up-to-date 
> > implementations.  But it would be a change in 
> implementation requirements.
> > And it would probably further slow down any (are there likely to be 
> > any any?)  Java 7 implementations on Alpha.  Not that 
> that's likely to be a big deal.
> 
> Background for other readers: the Alpha processor (used in "Tru64"
> systems) has serious problems conforming to reasonable memory 
> models, because its spec allows things like speculating the 
> value of x.a  before reading x unless prohibited by a fence 
> instruction.
> (No other MP-able processor we know of has this mis-feature.) 
> As a minimal accommodation to this, the JSR133 spec contains 
> what amounts to a loophole that only strictly requires Alpha 
> use these fences for fields marked as "final". But since 
> Alpha-based systems have been been at end-of-lifetime for a 
> few years now, it seems crazy to let this get in the way of 
> solving real programming problems people have. On the other 
> hand, I see that there is a JSR133/Java5 JVM for Alpha 
> (http://www.compaq.com/java/alpha/)
> so it is conceivable that this would have some impact if they 
> continue post-EOL updates.

I'm personally not so concerned about Alpha either.  It just serves as a useful illustration that we are imposing additional read-side constraints as well.
The real concerns I have are more along the lines of:

1) Other architectures do generally impose ordering based on data dependencies.
But the specific rules are often not that simple.  I don't understand, for example, whether PowerPC's rules for data dependency ordering are sufficiently strong to generally and cheaply provide the guarantees here.  Do they compose sufficiently with happens-before?  Currently I don't remember/understand either PowerPC rules nor the fence/final field rules well enough to answer that question.

2) My mental picture has always been that final field accesses are compiled under completely different rules than ordinary field accesses.  Final fields allow certain additional optimizations, like CSE across synchronization, but require that certain kinds of dependency ordering be maintained by the compiler.  I think Doug is implicitly claiming that the way normal fields are compiled in reality is also good enough for final fields.  He may be right because only very restricted data dependencies have to be preserved, and real compilers don't speculate on pointer values.  But I'm not sure.  If I understand the current spec correctly, if I write a[x.f] where f is a final field, the accesses to x.f and a[x.f] are already unordered?
> 
> > 
> > If I read the "more formally" description correctly, you are 
> > constraining reads? What's the impact of this on the rules 
> for common 
> > subexpression elimination?  (I know there are issues there 
> anyway, but 
> > I'm afraid of pontentially breaking things further.)
> 
> I don't see any further breakage. If those JMM/JLS issues are 
> fixed, then it will be comparatively trivial to make any 
> corresponding adjustments in these specs.
> 
> > 
> > 3. The volatile emulation seems like it's at best an 
> approximation.  
> > If you implement it with the obvious fences, I think you 
> end up with a 
> > PowerPC implementation which is faster than what the PowerPC guys 
> > recommend.  This leads me to suspect it's not correct there.  It 
> > generally doesn't seem to guarantee the correct outcome for IRIW.
> 
> Sorry, I don't see this. Notice that orderAccesses ensures 
> synchronizes-with.
> 
> (On PowerPC, I think both orderReads and orderWrites normally 
> translate to "lwsync" (plus compiler constraints), although 
> can sometimes be further optimized as in Paul and Raul's C++ 
> mappings document 
> http://www.rdrop.com/users/paulmck/scalability/paper/N2745r.2009.02.22a.html
> And orderAccesses to "hwsync". Using these seems to give 
> heavier, not lighter volatile implementation compared to best 
> optimized forms of volatiles, but not by much.
Paul and Raul's implementation uses a hwsync for Load Seq Cst, which is essentially a Java volatile store.  That's painful, since loads are presumably more frequent than stores, and you're adding a hwsync for accesses that you might expect to ususally hit in the cache.  If I understand correctly, the emulation with fences would only use an lwsync on the load side which, if correct, would solve this problem since it uses only an orderReads, which is presumably an lwsync, right?
...
> > 
> > Since this is intended to replace even worse approaches, 
> I'm not sure 
> > what the right solution is.  Define this API as deprecated from the 
> > start?  (I'm not sure whether I'm joking ...)
> 
> It still seems vastly preferable that we provide a mechanism 
> people can use to fix serious known bugs (unsafe publication 
> chief among them), even at the expense of introducing some 
> more minor challenges to efforts to fix underlying specs.
A bit more on that in another message ...

> For an example of impact, the number of memory model bugs out 
> there might drop significantly if dependency injection tools 
> inserted appropriate calls to fence methods.
As a practical matter, I agree.  Adding the fences would help code reliability.  Whether or not we can actually get the spec sufficiently bulletproof to actually eliminate the bugs as opposed to making them much less likely, I'm not sure.

Hans
> 
> -Doug
> 
 

From hans.boehm at hp.com  Mon Aug 10 17:53:23 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 10 Aug 2009 21:53:23 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A7EA9A1.8010902@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>
	<4A7EA9A1.8010902@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D0425779D4ECC0@GVW0436EXB.americas.hpqcorp.net>

> From: Doug Lea
> 
> P?ter Kov?cs wrote:
> >     For an example of impact, the number of memory model
> >     bugs out there might drop significantly if dependency injection
> >     tools inserted appropriate caPlls to fence methods.
> > 
> > 
> >  Please, could someone give (point to) some background information? 
> > Which dependency injection tools are specifically meant 
> here? Are the 
> > issues with these dependency injection tools unsolvable using 
> > currently available API?
> > 
> 
> Bob Lee and/or Tim Peierls could probably provide more 
> details about particular frameworks, but the main cases are 
> of the form illustrated in the "emulating final"
> examples (pasted below).
> A tool arranges that fields referencing components are bound 
> during an initialization phase. The references act as final 
> fields but sometimes (depending on the way initialization is 
> arranged by tools) cannot be declared as final.
> 
> As Tim cryptically noted, there is some fear, uncertainty and 
> doubt out there about the safety of some of this generated 
> initialization code across various usage contexts, that would 
> in most cases be settled by having tools (not the application 
> programmers) generate orderWrites calls as appropriate.
> 
The question in my mind here is whether there is some way to avoid the underlying data race on the reference r to the object O containing the emulated final fields.  If the reference were communicated without a data race (e.g. by declaring r volatile, or by protecting it with a lock), clearly the whole issue would go away.

I understand that you might not be able to guarantee this, because there might be a security risk if O is created by untrusted code, and it can see an inconsistent version of O by creating an unprotected reference to it.  It's also conceivable that declaring r to be volatile might be too expensive.

But I think the first line of defense here should be to avoid the underlying data races, without which none of this matters.

Hans

From dl at cs.oswego.edu  Tue Aug 11 07:27:25 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 11 Aug 2009 07:27:25 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A81559D.5000708@cs.oswego.edu>

Boehm, Hans wrote:
> 
> 2) My mental picture has always been that final field accesses are compiled 
> under completely different rules than ordinary field accesses.  Final fields 
> allow certain additional optimizations, like CSE across synchronization, but 
> require that certain kinds of dependency ordering be maintained by the 
> compiler.

The "emulation" of final fields is only wrt publication safety
(and only then under a bunch of constraints, but ones that often hold),
not any additional optimizations. Also, it remains the responsibility
of programmer to ensure nested publication safety for nested components.


> Paul and Raul's implementation uses a hwsync for Load Seq Cst, which is 
> essentially a Java volatile store.  That's painful, since loads are 
> presumably more frequent than stores, and you're adding a hwsync for accesses
>  that you might expect to ususally hit in the cache.  If I understand 
> correctly, the emulation with fences would only use an lwsync on the load 
> side which, if correct, would solve this problem since it uses only an 
> orderReads, which is presumably an lwsync, right? ...

I'm not sure either. Hopefully Paul can help figure out if
orderReads (and related volatile constructions) need to be hwsync.

Also, to follow up on my...

> You are right that the non-initialization case only implicitly required
> pairings of order{Writes,Accesses} with corresponding orderReads. ... The
> only way out that I can see is to implicitly identify the pairings ...

It occurred to me after writing this that you needed to do
something along these lines in C++0x fence specs too, so
I'm trying to adapt some of that approach here.
It would be nice if they were similar enough to make the
basically equivalent intent and usage clear to programmers;
modulo the final-field emulation aspect of Java version.
(For anyone interested in C++ specs, see
http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2009/n2914.pdf
mainly sec 29.8.)

-Doug









From dl at cs.oswego.edu  Tue Aug 11 07:48:55 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 11 Aug 2009 07:48:55 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D0425779D4ECC0@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>		<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>		<4A7D7618.9050501@cs.oswego.edu>	<fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>
	<4A7EA9A1.8010902@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECC0@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A815AA7.1050503@cs.oswego.edu>

Boehm, Hans wrote:
>> ... A tool arranges that fields referencing components are bound during an
>> initialization phase. The references act as final fields but sometimes
>> (depending on the way initialization is arranged by tools) cannot be
>> declared as final.
>> ...
> But I think the first line of defense here should be to avoid the underlying
> data races, without which none of this matters.
> 

The most common issues arise here just because of expressibility
snags. For example, it is nearly impossible to declare a
field "transient final" because there is no way (short of
reflection or Unsafe hacks that don't always apply)
of establishing value during deserialization. Plus,
tools that perform component binding don't know much
about their program context, but would like to at least
assist users in arranging the common case
where the components are immutable.

-Doug



From ganzhi at gmail.com  Tue Aug 11 11:57:56 2009
From: ganzhi at gmail.com (James Gan)
Date: Tue, 11 Aug 2009 23:57:56 +0800
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <70c070d80908110857j4d8e013dl2b4393ae9cc23829@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>
	<4A793B48.3050305@atlassian.com> <4A7B95CB.4040406@atlassian.com>
	<70c070d80908062202i3fc7b086ld40ca59340003a2d@mail.gmail.com>
	<4A80C45F.9070005@atlassian.com>
	<70c070d80908110857j4d8e013dl2b4393ae9cc23829@mail.gmail.com>
Message-ID: <70c070d80908110857t70778c9ew6a29a4fa7960a66a@mail.gmail.com>

Jed,

Components in Amino library are far from perfect. Some components works
pretty great, while some of them are not good. As performance is the 1st
priority, it sometimes doesn't implement all features of standard interface.
Thanks for your review!

I agree that toArray() can work as its javadoc described. But for lock-free
data structure, its return value looks a little strange if concurrent
modification is allowed. It's possible that result of toArray() can contain
removed elements and ignore newly added elements in one execution. I can't
think of any useful scenario of such toArray(). It will be very useful if
toArray() can return a snapshot.

The decision of not implementing iterator.remove() is very interesting. I
failed to find an approach for implementing a safe iterator.remove() method.
Though I noticed the defect of j.u.c.ConcurrentLinkedQueue.remove() method (
http://cs.oswego.edu/pipermail/concurrency-interest/2009-February/005880.html),
I don't know how to fix it. It seems that the newest version of
ConcurrentLinkedQueue in OpenJDK 7 has fixed this bug. I definitely want to
learn from it. :)



On Tue, Aug 11, 2009 at 9:07 AM, Jed Wesley-Smith <jed at atlassian.com> wrote:

> Hi James, seems I didn't hit send on this the other day...
>
> Fortunately, AbstractCollection's toArray() is perfectly good ? as long as
> your hasNext()/next() is stable!
>
> I agree about iterator removal, but at least remove is an "optional"
> method.
>
> I haven't had time to look through your Deque and Queue implementations
> thoroughly, although I have looked through some of the other
> implementations. There are some important design tradeoffs in some of them
> (like not implementing iterator.remove()) that I didn't allow myself to
> make, although as you point out they perhaps don't make much sense anyway
>
> cheers,
> jed.
>
> James Gan wrote:
>
>> I'm feeling the same1 Many methods of standard API interface are pretty
>> difficult to implement. The toArray() method is a good example. If
>> add()/remove() is lock-free, how we can generate a snapshot of the
>> List/Queue/Deque? And iterator() is another source of pain for Queue/Deque.
>> If it's allowed to remove elements by Queue.iterator().remove() method, is
>> it still a queue??
>>
>> Besides, there are LockFreeDeque/LockFreeQueue components in Amino
>> library:
>>
>> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/amino/java/src/main/java/org/amino/ds/lockfree/LockFreeDeque.java
>>
>> https://amino-cbbs.svn.sourceforge.net/svnroot/amino-cbbs/trunk/amino/java/src/main/java/org/amino/ds/lockfree/LockFreeQueue.java
>>
>> The deque component implements an algorithm from Maged Michael. And queue
>> component implement an algorithm from Edya Ladan-Mozes and Nir Shavit. In
>> our micro-benchmark, this queue is faster than ConcurrentLinkedQueue in JDK
>> v6.
>>
>> On Fri, Aug 7, 2009 at 10:47 AM, Jed Wesley-Smith <jed at atlassian.com<mailto:
>> jed at atlassian.com>> wrote:
>>
>>    OK, so I've been thinking a lot about it and have a working
>>    lock-free prototype (not prepared to call wait-free yet, but it
>>    has some wait-free characteristics). The thing is, there are a
>>    bunch of methods on the List interface that really don't make much
>>    sense in a concurrent context. Anything with an index is an
>>    obvious example, and of course size can only ever be approximate.
>>    A ConcurrentDeque makes somewhat more sense to implement, so I'll
>>    be giving that a go instead.
>>
>>    The whole project does pose some interesting questions as to
>>    relevancy. The following paper explains a few about why the author
>>    thinks a doubly linked list is not a concurrent data structure:
>>
>>
>> http://www.plannednonoperational.com/articles/public/concurrency_rabit_hole.pdf
>>
>>    Is there much point in continuing apart from my own enjoyment of
>>    the exercise?
>>
>>    cheers,
>>    jed.
>>
>>
>
>
>


-- 
Best Regards
James Gan
Current Project: Concurrent Building Block at
http://amino-cbbs.sourceforge.net/
Blog: http://ganzhi.blogspot.com



-- 
Best Regards
James Gan
Current Project: Concurrent Building Block at
http://amino-cbbs.sourceforge.net/
Blog: http://ganzhi.blogspot.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090811/45ac7591/attachment-0001.html>

From martinrb at google.com  Tue Aug 11 13:47:02 2009
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 11 Aug 2009 10:47:02 -0700
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <70c070d80908110857t70778c9ew6a29a4fa7960a66a@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>
	<4A793B48.3050305@atlassian.com> <4A7B95CB.4040406@atlassian.com>
	<70c070d80908062202i3fc7b086ld40ca59340003a2d@mail.gmail.com>
	<4A80C45F.9070005@atlassian.com>
	<70c070d80908110857j4d8e013dl2b4393ae9cc23829@mail.gmail.com>
	<70c070d80908110857t70778c9ew6a29a4fa7960a66a@mail.gmail.com>
Message-ID: <1ccfd1c10908111047l2a81b89bl52a43e3c9e87cdae@mail.gmail.com>

On Tue, Aug 11, 2009 at 08:57, James Gan<ganzhi at gmail.com> wrote:
> I agree that toArray() can work as its javadoc described. But for lock-free
> data structure, its return value looks a little strange if concurrent
> modification is allowed. It's possible that result of toArray() can contain
> removed elements and ignore newly added elements in one execution. I can't
> think of any useful scenario of such toArray(). It will be very useful if
> toArray() can return a snapshot.

For lock-free data structures (like ConcurrentLinkedQueue)
it cannot return a snapshot.

But toArray() can still be very useful, especially if it keeps its
implicit promise of returning all elements that were in the queue
during the lifetime of the method call.

> The decision of not implementing iterator.remove() is very interesting. I
> failed to find an approach for implementing a safe iterator.remove() method.
> Though I noticed the defect of j.u.c.ConcurrentLinkedQueue.remove() method
> (http://cs.oswego.edu/pipermail/concurrency-interest/2009-February/005880.html),
> I don't know how to fix it. It seems that the newest version of
> ConcurrentLinkedQueue in OpenJDK 7 has fixed this bug. I definitely want to
> learn from it. :)

The key is to have only one atomic action causing an element to be removed.
You can't use cas-ing head for removing nodes at head
because that won't
work for removal of interior nodes.  So you have to choose something
else, like cas-ing element to null.  Once you've abandoned the idea of
head giving a definitive answer to first element, might as well update it
in a lazier fashion, for performance reasons.

Martin

From opinali at gmail.com  Tue Aug 11 15:27:37 2009
From: opinali at gmail.com (Osvaldo Pinali Doederlein)
Date: Tue, 11 Aug 2009 16:27:37 -0300
Subject: [concurrency-interest] Fences
In-Reply-To: <4A815AA7.1050503@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>		<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>		<4A7D7618.9050501@cs.oswego.edu>	<fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>	<4A7EA9A1.8010902@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECC0@GVW0436EXB.americas.hpqcorp.net>
	<4A815AA7.1050503@cs.oswego.edu>
Message-ID: <4A81C629.9060503@gmail.com>

Doug Lea wrote:
> The most common issues arise here just because of expressibility
> snags. For example, it is nearly impossible to declare a
> field "transient final" because there is no way (short of
> reflection or Unsafe hacks that don't always apply)
> of establishing value during deserialization. Plus,
> tools that perform component binding don't know much
> about their program context, but would like to at least
> assist users in arranging the common case
> where the components are immutable.
This became one of my pet peeves with Java, years ago since I became a 
member of the Cult of Final Variables. Gosh, I totally hate when I have 
a beautiful immutable object with all fields final, and I'm forced to 
remove the "final" tag of some field when I have to customize its 
serialization. :-( I always though that Sun was just lazy because (a) a 
unserialized object is constructed in a controlled way by trusted code 
that can use any magic it wants, and (b) it's possible to determine 
definite assignment in readObject() just like it's done for 
constructors, isn't it?

And while we're at it, please let me declare a field like: "private 
serializable Map myMap", so many code validation tools (including EJB 
IDEs - it's specifically a EJB rule) won't complain that my Serializable 
class contains a non-Serializable, non-transient field. The problem here 
is that the collection interfaces like java.util.Map are not marked 
Serializable even though most implementations are; so, just let me say 
in the field declaration that it's supposed to be serializable. Java5's 
mixin bounds like X extends A&B could provide some basis for this but 
they don't. Serializable is important enough that it arguably deserves 
support from a modifier bit - we already have other language support 
like the transient token and type modifier, so why not just finishing 
the job.)

Another major nuisance is the fact that final variables are just syntax 
sugar, the final modifier is not preserved in bytecode - I'm not sure if 
this could deliver some advantage (easier work for the JIT compiler 
building SSA or something?), but I would have that warm fuzzy feling of 
knowing that my local variables are kept immutable even in the bytecode 
level. :-) If there is some advantage, perhaps the changes being done to 
support JSR-308 could accomodate the final modifer for locals? (Ugly 
hack and probably expensive, I'd prefer a single flag bit set like for 
final fields and not a pseudo-annotation, but...)

A+
Osvaldo

From ashpublic at mac.com  Wed Aug 12 07:45:32 2009
From: ashpublic at mac.com (Ashley Williams)
Date: Wed, 12 Aug 2009 12:45:32 +0100
Subject: [concurrency-interest] Support for persistent data structures
Message-ID: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>

Hi,

Just wondering if there are any popular java libraries that support  
persistent collections, in the sense of immutability rather than  
serializable. The only possible option I have found so far is to  
somehow use the clojure library from within java and I'm not certain  
if this can be done since I'm not familiar enough with it.

Cheers
- Ashley

From ashpublic at mac.com  Wed Aug 12 07:51:53 2009
From: ashpublic at mac.com (Ashley Williams)
Date: Wed, 12 Aug 2009 12:51:53 +0100
Subject: [concurrency-interest] Sizing thread pools
Message-ID: <35700FBB-2B75-484E-98F4-2581FFDBD502@mac.com>

Hi,

I have a question after reading up on sizing thread pool in the jcip  
book.

It recommends that for IO bound tasks, the thread pool can be much  
larger which makes sense since the cpu isn't the bottleneck. But say I  
have two threads executing on a single cpu where the first is  
executing a blocking io method and a context switch occurs to the  
second thread. Will the underlying operating system call that the  
first thread is waiting on still somehow still continue - eg data  
still gets buffered for example - so that when the first thread is  
rescheduled it has at least made some progress?

A related question - since there isn't any way to assign threads to  
specific cpu cores from java (is this true?), should one always assume  
that any two thread executions are time-sliced?

Cheers
- Ashley

From ghosh.debasish at gmail.com  Wed Aug 12 08:00:50 2009
From: ghosh.debasish at gmail.com (Debasish Ghosh)
Date: Wed, 12 Aug 2009 17:30:50 +0530
Subject: [concurrency-interest] Support for persistent data structures
In-Reply-To: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>
References: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>
Message-ID: <8b1c89270908120500p7fef9fd0m9fa307a29526abb4@mail.gmail.com>

You can have a look at Functional Java (http://functionaljava.org/). It
offers quite a few persistent data structures.

Thanks.
- Debasish

On Wed, Aug 12, 2009 at 5:15 PM, Ashley Williams <ashpublic at mac.com> wrote:

> Hi,
>
> Just wondering if there are any popular java libraries that support
> persistent collections, in the sense of immutability rather than
> serializable. The only possible option I have found so far is to somehow use
> the clojure library from within java and I'm not certain if this can be done
> since I'm not familiar enough with it.
>
> Cheers
> - Ashley
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090812/68341a36/attachment.html>

From karl.krukow at gmail.com  Wed Aug 12 08:26:03 2009
From: karl.krukow at gmail.com (Karl Krukow)
Date: Wed, 12 Aug 2009 14:26:03 +0200
Subject: [concurrency-interest] Support for persistent data structures
In-Reply-To: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>
References: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>
Message-ID: <C8261DD3-5A72-4FDE-AFF0-B0AE84AC5DA3@gmail.com>

  On 12/08/2009, at 13.45, Ashley Williams wrote:

> Hi,
>
> Just wondering if there are any popular java libraries that support  
> persistent collections, in the sense of immutability rather than  
> serializable. The only possible option I have found so far is to  
> somehow use the clojure library from within java and I'm not certain  
> if this can be done since I'm not familiar enough with it.

The Clojure library is certainly usable from Java - indeed, according  
to Rich Hickey, it was a java library before clojure. Just put  
clojure.jar on your classpath.

/Karl


From ashpublic at mac.com  Wed Aug 12 08:28:26 2009
From: ashpublic at mac.com (Ashley Williams)
Date: Wed, 12 Aug 2009 13:28:26 +0100
Subject: [concurrency-interest] Support for persistent data structures
In-Reply-To: <8b1c89270908120500p7fef9fd0m9fa307a29526abb4@mail.gmail.com>
References: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>
	<8b1c89270908120500p7fef9fd0m9fa307a29526abb4@mail.gmail.com>
Message-ID: <C037F912-2DC5-43FA-B54A-59BE2D92DAF1@mac.com>

Looks interesting, although I suspect I would make the jump to scala  
in preference. Purely as a library I would have more confidence if  
there were testimonials of its use in critical projects, but it  
certainly seems useful as a learning tool.

BTW, not sure about the mission statement "aims to prepare the Java  
programming language for the inclusion of closures" since the Java  
programming language isn't going to get closures ;)


On 12 Aug 2009, at 13:00, Debasish Ghosh wrote:

> You can have a look at Functional Java (http://functionaljava.org/).  
> It offers quite a few persistent data structures.
>
> Thanks.
> - Debasish
>
> On Wed, Aug 12, 2009 at 5:15 PM, Ashley Williams <ashpublic at mac.com>  
> wrote:
> Hi,
>
> Just wondering if there are any popular java libraries that support  
> persistent collections, in the sense of immutability rather than  
> serializable. The only possible option I have found so far is to  
> somehow use the clojure library from within java and I'm not certain  
> if this can be done since I'm not familiar enough with it.
>
> Cheers
> - Ashley
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090812/beecbfd4/attachment.html>

From Joe.Kearney at morganstanley.com  Wed Aug 12 08:41:44 2009
From: Joe.Kearney at morganstanley.com (Joe Kearney)
Date: Wed, 12 Aug 2009 13:41:44 +0100
Subject: [concurrency-interest] Support for persistent data structures
In-Reply-To: <C037F912-2DC5-43FA-B54A-59BE2D92DAF1@mac.com>
References: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com> 
	<8b1c89270908120500p7fef9fd0m9fa307a29526abb4@mail.gmail.com> 
	<C037F912-2DC5-43FA-B54A-59BE2D92DAF1@mac.com>
Message-ID: <ec295ec90908120541k44ae4104r65dd0fa440a22a56@mail.gmail.com>

See also the immutable support in the Google collections
library<http://code.google.com/p/google-collections/>
.
Joe

2009/8/12 Ashley Williams <ashpublic at mac.com>

> Looks interesting, although I suspect I would make the jump to scala in
> preference. Purely as a library I would have more confidence if there were
> testimonials of its use in critical projects, but it certainly seems useful
> as a learning tool.
>
> BTW, not sure about the mission statement "aims to prepare the Java
> programming language for the inclusion of closures" since the Java
> programming language isn't going to get closures ;)
>
>
> On 12 Aug 2009, at 13:00, Debasish Ghosh wrote:
>
> You can have a look at Functional Java (http://functionaljava.org/). It
> offers quite a few persistent data structures.
>
> Thanks.
> - Debasish
>
> On Wed, Aug 12, 2009 at 5:15 PM, Ashley Williams <ashpublic at mac.com>wrote:
>
>> Hi,
>>
>> Just wondering if there are any popular java libraries that support
>> persistent collections, in the sense of immutability rather than
>> serializable. The only possible option I have found so far is to somehow use
>> the clojure library from within java and I'm not certain if this can be done
>> since I'm not familiar enough with it.
>>
>> Cheers
>> - Ashley
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090812/0a75a697/attachment-0001.html>

From ashpublic at mac.com  Wed Aug 12 08:52:26 2009
From: ashpublic at mac.com (Ashley Williams)
Date: Wed, 12 Aug 2009 13:52:26 +0100
Subject: [concurrency-interest] Support for persistent data structures
In-Reply-To: <ec295ec90908120541k44ae4104r65dd0fa440a22a56@mail.gmail.com>
References: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>
	<8b1c89270908120500p7fef9fd0m9fa307a29526abb4@mail.gmail.com>
	<C037F912-2DC5-43FA-B54A-59BE2D92DAF1@mac.com>
	<ec295ec90908120541k44ae4104r65dd0fa440a22a56@mail.gmail.com>
Message-ID: <F837F6A9-E76E-45D1-B2F0-C97A87727D7F@mac.com>

This was the first place I looked and I believe I've double checked  
this but please correct me if I'm wrong.
They aren't persistent data structures at all, rather they just throw  
exceptions if you access their 'mutator' methods
rather than returning a new version.

On 12 Aug 2009, at 13:41, Joe Kearney wrote:

> See also the immutable support in the Google collections library.
> Joe
>
> 2009/8/12 Ashley Williams <ashpublic at mac.com>
> Looks interesting, although I suspect I would make the jump to scala  
> in preference. Purely as a library I would have more confidence if  
> there were testimonials of its use in critical projects, but it  
> certainly seems useful as a learning tool.
>
> BTW, not sure about the mission statement "aims to prepare the Java  
> programming language for the inclusion of closures" since the Java  
> programming language isn't going to get closures ;)
>
>
> On 12 Aug 2009, at 13:00, Debasish Ghosh wrote:
>
>> You can have a look at Functional Java (http:// 
>> functionaljava.org/). It offers quite a few persistent data  
>> structures.
>>
>> Thanks.
>> - Debasish
>>
>> On Wed, Aug 12, 2009 at 5:15 PM, Ashley Williams  
>> <ashpublic at mac.com> wrote:
>> Hi,
>>
>> Just wondering if there are any popular java libraries that support  
>> persistent collections, in the sense of immutability rather than  
>> serializable. The only possible option I have found so far is to  
>> somehow use the clojure library from within java and I'm not  
>> certain if this can be done since I'm not familiar enough with it.
>>
>> Cheers
>> - Ashley
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090812/1294d7ab/attachment.html>

From davidcholmes at aapt.net.au  Wed Aug 12 09:08:05 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 12 Aug 2009 23:08:05 +1000
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <35700FBB-2B75-484E-98F4-2581FFDBD502@mac.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEAGIDAA.davidcholmes@aapt.net.au>


> Ashley Williams writes:
> It recommends that for IO bound tasks, the thread pool can be much
> larger which makes sense since the cpu isn't the bottleneck. But say I
> have two threads executing on a single cpu where the first is
> executing a blocking io method and a context switch occurs to the
> second thread. Will the underlying operating system call that the
> first thread is waiting on still somehow still continue - eg data
> still gets buffered for example - so that when the first thread is
> rescheduled it has at least made some progress?

That's a rather general question - there is a lot of detail that determines
exactly what may happen. For example, if the first thread has initiated an
I/O request that requires it to block, say a read from a socket, and data
arrives on that socket, then the OS will process the arrival of that data in
some form. At a minimum the interrupt of the ethernet card will be
processed, but it could be that the thread itself is marked as no longer
blocked, and will be eligible to be switched to at the next scheduling
point. However if the I/O thread was preempted at some arbitrary point prior
to actually initiating the I/O request and prior to blocking, then it will -
if chosen for execution again - simply continue from that point with no
"progress" having been made. And anything on-between. It depends on the
details of the blocking operation and how it is handled by the OS.

> A related question - since there isn't any way to assign threads to
> specific cpu cores from java (is this true?),

It is true that there is no Java SE API for this (It will be in the
Real-Time Specification for Java 1.1). You can do it using native code.

> should one always assume that any two thread executions are time-sliced?

One should always strive to write portable Java programs and for portability
the golden rule with regards to scheduling is:
 - to assume that any two actions in two threads could be interleaved; but
 - never require that any two actions must be interleaved

In a nutshell: don't use scheduling decisions as an alternative to
synchronization. (Even in the real-time world, with strict priority
scheduling, you can only use scheduling as an alternative to synchronization
in very specific circumstances).

I'm unclear what connection you are making between this and the ability to
bind threads to cores? If you have two threads bound on different cores,
they can not preempt each other, but they can still execute code that is
effectively interleaved - the possible interleavings are more restricted,
but as the programmer you have no idea what the relative phasing is, nor how
long individual actions take to execute. It would be difficult, and very
context sensitive, to be able to take advantage of that situation I think.
But what were you thinking of?

Cheers,
David Holmes


From Joe.Kearney at morganstanley.com  Wed Aug 12 09:22:53 2009
From: Joe.Kearney at morganstanley.com (Joe Kearney)
Date: Wed, 12 Aug 2009 14:22:53 +0100
Subject: [concurrency-interest] Support for persistent data structures
In-Reply-To: <F837F6A9-E76E-45D1-B2F0-C97A87727D7F@mac.com>
References: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com> 
	<8b1c89270908120500p7fef9fd0m9fa307a29526abb4@mail.gmail.com> 
	<C037F912-2DC5-43FA-B54A-59BE2D92DAF1@mac.com>
	<ec295ec90908120541k44ae4104r65dd0fa440a22a56@mail.gmail.com> 
	<F837F6A9-E76E-45D1-B2F0-C97A87727D7F@mac.com>
Message-ID: <ec295ec90908120622o36fd1f6er3957bf81524d7166@mail.gmail.com>

Oh I see what you mean, I misunderstood. Yes, you're right. Note that the
Clojure implementations do this too, since of course you need new
definitions of the collections interfaces to support returning the new
updated collection after an add, say.

Thanks,
Joe

2009/8/12 Ashley Williams <ashpublic at mac.com>

> This was the first place I looked and I believe I've double checked this
> but please correct me if I'm wrong.They aren't persistent data structures
> at all, rather they just throw exceptions if you access their 'mutator'
> methods
> rather than returning a new version.
>
> On 12 Aug 2009, at 13:41, Joe Kearney wrote:
>
> See also the immutable support in the Google collections library<http://code.google.com/p/google-collections/>
> .
> Joe
>
> 2009/8/12 Ashley Williams <ashpublic at mac.com>
>
>> Looks interesting, although I suspect I would make the jump to scala in
>> preference. Purely as a library I would have more confidence if there were
>> testimonials of its use in critical projects, but it certainly seems useful
>> as a learning tool.
>>
>> BTW, not sure about the mission statement "aims to prepare the Java
>> programming language for the inclusion of closures" since the Java
>> programming language isn't going to get closures ;)
>>
>>
>> On 12 Aug 2009, at 13:00, Debasish Ghosh wrote:
>>
>> You can have a look at Functional Java (http://functionaljava.org/). It
>> offers quite a few persistent data structures.
>>
>> Thanks.
>> - Debasish
>>
>> On Wed, Aug 12, 2009 at 5:15 PM, Ashley Williams <ashpublic at mac.com>wrote:
>>
>>> Hi,
>>>
>>> Just wondering if there are any popular java libraries that support
>>> persistent collections, in the sense of immutability rather than
>>> serializable. The only possible option I have found so far is to somehow use
>>> the clojure library from within java and I'm not certain if this can be done
>>> since I'm not familiar enough with it.
>>>
>>> Cheers
>>> - Ashley
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090812/76ef6623/attachment.html>

From niko at alum.mit.edu  Wed Aug 12 09:33:26 2009
From: niko at alum.mit.edu (Niko Matsakis)
Date: Wed, 12 Aug 2009 15:33:26 +0200
Subject: [concurrency-interest] Support for persistent data structures
In-Reply-To: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>
References: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>
Message-ID: <849646A4-FF9D-40EE-9AF5-043749D52906@alum.mit.edu>

I haven't seen PCollections <http://code.google.com/p/pcollections/>  
mentioned.  I believe they are precisely what you want, though I can't  
comment on their efficiency.  Note that they extend the List interface  
to include methods like add() and minus() that return new collections;  
the original mutator methods simply throw exceptions.


Niko

On Aug 12, 2009, at 1:45 PM, Ashley Williams wrote:

> Hi,
>
> Just wondering if there are any popular java libraries that support  
> persistent collections, in the sense of immutability rather than  
> serializable. The only possible option I have found so far is to  
> somehow use the clojure library from within java and I'm not certain  
> if this can be done since I'm not familiar enough with it.
>
> Cheers
> - Ashley
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From ashpublic at mac.com  Wed Aug 12 10:13:34 2009
From: ashpublic at mac.com (Ashley Williams)
Date: Wed, 12 Aug 2009 15:13:34 +0100
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEAGIDAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEAGIDAA.davidcholmes@aapt.net.au>
Message-ID: <8BE0BCF5-A9C7-4A1E-BBB5-F72A0F0C43AB@mac.com>

Thanks for your detailed response. My assumption in all this, that I  
should have
mentioned, is that the tasks in the thread pools are self contained  
with no side
effects and so I'm not approaching this from a code safety angle -  
this is more
about performance and getting the best use out of the cpu configuration.

For IO bound tasks I am interested in finding out if the underlying OS  
io operation that
produces the data keeps going in some fashion even if the corresponding
thread that consumes it has not yet been reactivated. It would be  
great to
have confidence that even within a single core there is some sort of  
concurrency
that occurs when for example reading off a socket.

And cpu bound threads would ideally be assigned to separate cores so  
that
they can truly execute concurrently. But if the underlying scheduler  
decides to
assign them to the same core then there will be no improved performance.

So to somebody ignorant of the facts such as myself, it seems that the  
scheduler
would need some sort of clue as to how to schedule the threads otherwise
any effort to balance thread pools for size and type of task could  
well be in
vain.

On 12 Aug 2009, at 14:08, David Holmes wrote:

>
>> Ashley Williams writes:
>> It recommends that for IO bound tasks, the thread pool can be much
>> larger which makes sense since the cpu isn't the bottleneck. But  
>> say I
>> have two threads executing on a single cpu where the first is
>> executing a blocking io method and a context switch occurs to the
>> second thread. Will the underlying operating system call that the
>> first thread is waiting on still somehow still continue - eg data
>> still gets buffered for example - so that when the first thread is
>> rescheduled it has at least made some progress?
>
> That's a rather general question - there is a lot of detail that  
> determines
> exactly what may happen. For example, if the first thread has  
> initiated an
> I/O request that requires it to block, say a read from a socket, and  
> data
> arrives on that socket, then the OS will process the arrival of that  
> data in
> some form. At a minimum the interrupt of the ethernet card will be
> processed, but it could be that the thread itself is marked as no  
> longer
> blocked, and will be eligible to be switched to at the next scheduling
> point. However if the I/O thread was preempted at some arbitrary  
> point prior
> to actually initiating the I/O request and prior to blocking, then  
> it will -
> if chosen for execution again - simply continue from that point with  
> no
> "progress" having been made. And anything on-between. It depends on  
> the
> details of the blocking operation and how it is handled by the OS.
>
>> A related question - since there isn't any way to assign threads to
>> specific cpu cores from java (is this true?),
>
> It is true that there is no Java SE API for this (It will be in the
> Real-Time Specification for Java 1.1). You can do it using native  
> code.
>
>> should one always assume that any two thread executions are time- 
>> sliced?
>
> One should always strive to write portable Java programs and for  
> portability
> the golden rule with regards to scheduling is:
> - to assume that any two actions in two threads could be  
> interleaved; but
> - never require that any two actions must be interleaved
>
> In a nutshell: don't use scheduling decisions as an alternative to
> synchronization. (Even in the real-time world, with strict priority
> scheduling, you can only use scheduling as an alternative to  
> synchronization
> in very specific circumstances).
>
> I'm unclear what connection you are making between this and the  
> ability to
> bind threads to cores? If you have two threads bound on different  
> cores,
> they can not preempt each other, but they can still execute code  
> that is
> effectively interleaved - the possible interleavings are more  
> restricted,
> but as the programmer you have no idea what the relative phasing is,  
> nor how
> long individual actions take to execute. It would be difficult, and  
> very
> context sensitive, to be able to take advantage of that situation I  
> think.
> But what were you thinking of?
>
> Cheers,
> David Holmes
>


From michael.m.spiegel at gmail.com  Wed Aug 12 11:28:36 2009
From: michael.m.spiegel at gmail.com (Michael Spiegel)
Date: Wed, 12 Aug 2009 11:28:36 -0400
Subject: [concurrency-interest] Support for persistent data structures
In-Reply-To: <C8261DD3-5A72-4FDE-AFF0-B0AE84AC5DA3@gmail.com>
References: <DBB07CFB-4324-44AE-90B4-6FE060B45473@mac.com>
	<C8261DD3-5A72-4FDE-AFF0-B0AE84AC5DA3@gmail.com>
Message-ID: <1901f69e0908120828o831182codbcda42c8eed6d5d@mail.gmail.com>

The Clojure collections library are well-designed and worth some
attention.  They are also designed for a dynamically typed language
(Clojure), so consequently there's no parameterized types (generics)
on the containers, and the default .equals() method follows Clojure
semantics.  Although I believe you can specify your own comparator
(don't remember).  In any case, the libraries are worth your
attention, but keep in mind they were designed for Clojure not for
Java.

--Michael

On Wed, Aug 12, 2009 at 8:26 AM, Karl Krukow<karl.krukow at gmail.com> wrote:
> ?On 12/08/2009, at 13.45, Ashley Williams wrote:
>
>> Hi,
>>
>> Just wondering if there are any popular java libraries that support
>> persistent collections, in the sense of immutability rather than
>> serializable. The only possible option I have found so far is to somehow use
>> the clojure library from within java and I'm not certain if this can be done
>> since I'm not familiar enough with it.
>
> The Clojure library is certainly usable from Java - indeed, according to
> Rich Hickey, it was a java library before clojure. Just put clojure.jar on
> your classpath.
>
> /Karl
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From ganzhi at gmail.com  Wed Aug 12 11:50:35 2009
From: ganzhi at gmail.com (James Gan)
Date: Wed, 12 Aug 2009 23:50:35 +0800
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <1ccfd1c10908111047l2a81b89bl52a43e3c9e87cdae@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>
	<4A793B48.3050305@atlassian.com> <4A7B95CB.4040406@atlassian.com>
	<70c070d80908062202i3fc7b086ld40ca59340003a2d@mail.gmail.com>
	<4A80C45F.9070005@atlassian.com>
	<70c070d80908110857j4d8e013dl2b4393ae9cc23829@mail.gmail.com>
	<70c070d80908110857t70778c9ew6a29a4fa7960a66a@mail.gmail.com>
	<1ccfd1c10908111047l2a81b89bl52a43e3c9e87cdae@mail.gmail.com>
Message-ID: <70c070d80908120850y4b8d38e8ib33af8e4ec50368b@mail.gmail.com>

On Wed, Aug 12, 2009 at 1:47 AM, Martin Buchholz <martinrb at google.com>wrote:

> On Tue, Aug 11, 2009 at 08:57, James Gan<ganzhi at gmail.com> wrote:
> > I agree that toArray() can work as its javadoc described. But for
> lock-free
> > data structure, its return value looks a little strange if concurrent
> > modification is allowed. It's possible that result of toArray() can
> contain
> > removed elements and ignore newly added elements in one execution. I
> can't
> > think of any useful scenario of such toArray(). It will be very useful if
> > toArray() can return a snapshot.
>
> For lock-free data structures (like ConcurrentLinkedQueue)
> it cannot return a snapshot.

Agree. Though all algorithms can be implemented wait-free, it seems
impossible to create an efficient lock-free snapshot.

>
>
> But toArray() can still be very useful, especially if it keeps its
> implicit promise of returning all elements that were in the queue
> during the lifetime of the method call.
>

Yeah, I haven't noticed this before. Thanks!


> > The decision of not implementing iterator.remove() is very interesting. I
> > failed to find an approach for implementing a safe iterator.remove()
> method.
> > Though I noticed the defect of j.u.c.ConcurrentLinkedQueue.remove()
> method
> > (
> http://cs.oswego.edu/pipermail/concurrency-interest/2009-February/005880.html
> ),
> > I don't know how to fix it. It seems that the newest version of
> > ConcurrentLinkedQueue in OpenJDK 7 has fixed this bug. I definitely want
> to
> > learn from it. :)
>
> The key is to have only one atomic action causing an element to be removed.
> You can't use cas-ing head for removing nodes at head
> because that won't
> work for removal of interior nodes.  So you have to choose something
> else, like cas-ing element to null.  Once you've abandoned the idea of
> head giving a definitive answer to first element, might as well update it
> in a lazier fashion, for performance reasons.

It looks like there will be a performance downgrade since we'll need two CAS
per eacho queue operation.

>
>
> Martin
>



-- 
Best Regards
James Gan
Current Project: Concurrent Building Block at
http://amino-cbbs.sourceforge.net/
Blog: http://ganzhi.blogspot.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090812/3186240c/attachment.html>

From martinrb at google.com  Wed Aug 12 15:25:24 2009
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 12 Aug 2009 12:25:24 -0700
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <70c070d80908120850y4b8d38e8ib33af8e4ec50368b@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>
	<4A793B48.3050305@atlassian.com> <4A7B95CB.4040406@atlassian.com>
	<70c070d80908062202i3fc7b086ld40ca59340003a2d@mail.gmail.com>
	<4A80C45F.9070005@atlassian.com>
	<70c070d80908110857j4d8e013dl2b4393ae9cc23829@mail.gmail.com>
	<70c070d80908110857t70778c9ew6a29a4fa7960a66a@mail.gmail.com>
	<1ccfd1c10908111047l2a81b89bl52a43e3c9e87cdae@mail.gmail.com>
	<70c070d80908120850y4b8d38e8ib33af8e4ec50368b@mail.gmail.com>
Message-ID: <1ccfd1c10908121225s2498babfu5469f36d3f6106b9@mail.gmail.com>

On Wed, Aug 12, 2009 at 08:50, James Gan<ganzhi at gmail.com> wrote:

>> > (http://cs.oswego.edu/pipermail/concurrency-interest/2009-February/005880.html),
>> > I don't know how to fix it. It seems that the newest version of
>> > ConcurrentLinkedQueue in OpenJDK 7 has fixed this bug. I definitely want
>> > to
>> > learn from it. :)
>>
>> The key is to have only one atomic action causing an element to be
>> removed.
>> You can't use cas-ing head for removing nodes at head
>> because that won't
>> work for removal of interior nodes. ?So you have to choose something
>> else, like cas-ing element to null. ?Once you've abandoned the idea of
>> head giving a definitive answer to first element, might as well update it
>> in a lazier fashion, for performance reasons.
>
> It looks like there will be a performance downgrade since we'll need two CAS
> per eacho queue operation.

My "lazier" comment above was supposed to answer your question,
but it wasn't as clear as it could be.  I will elaborate.

Since head no longer needs to be updated for correctness,
we can consider deliberately not updating it, i.e. leaving it stale,
for performance reasons, if we believe that CAS is much more expensive
than volatile read, as is true on current x86 hardware.
ConcurrentLinkedQueue updates head/tail every other enqueue/dequeue,
making this what I think of as a "sesqui-CAS" algorithm.
This new word is obviously going to become an important part of our
collective vocabulary, so let me remove the hyphen now - "sesquicas" -
I like that word.  Of course, we could also set hop count to 4, making this
http://en.wiktionary.org/wiki/quasqui-
"quasquicas", which is even cooler.

Martin


From tim at peierls.net  Wed Aug 12 15:44:56 2009
From: tim at peierls.net (Tim Peierls)
Date: Wed, 12 Aug 2009 15:44:56 -0400
Subject: [concurrency-interest] Project Amino: Introduce Lock-Free Stack
In-Reply-To: <1ccfd1c10908121225s2498babfu5469f36d3f6106b9@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCGEOHICAA.davidcholmes@aapt.net.au>
	<4A793B48.3050305@atlassian.com> <4A7B95CB.4040406@atlassian.com>
	<70c070d80908062202i3fc7b086ld40ca59340003a2d@mail.gmail.com>
	<4A80C45F.9070005@atlassian.com>
	<70c070d80908110857j4d8e013dl2b4393ae9cc23829@mail.gmail.com>
	<70c070d80908110857t70778c9ew6a29a4fa7960a66a@mail.gmail.com>
	<1ccfd1c10908111047l2a81b89bl52a43e3c9e87cdae@mail.gmail.com>
	<70c070d80908120850y4b8d38e8ib33af8e4ec50368b@mail.gmail.com>
	<1ccfd1c10908121225s2498babfu5469f36d3f6106b9@mail.gmail.com>
Message-ID: <63b4e4050908121244r390ebc81nc1b57ca6d71f82ea@mail.gmail.com>

On Wed, Aug 12, 2009 at 3:25 PM, Martin Buchholz <martinrb at google.com>wrote:

> I like that word.  Of course, we could also set hop count to 4, making this
> http://en.wiktionary.org/wiki/quasqui-
> "quasquicas", which is even cooler.


If it becomes a standard approach that everyone agrees on, it could be
called "harmonicas". If it's universally distasteful, then ... "yuccas".

Don't just take it from me; write your own groaners:
http://www.morewords.com/ends-with/cas/

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090812/29b387eb/attachment.html>

From hans.boehm at hp.com  Wed Aug 12 17:53:09 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 12 Aug 2009 21:53:09 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A81C629.9060503@gmail.com>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>
	<4A7EA9A1.8010902@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECC0@GVW0436EXB.americas.hpqcorp.net>
	<4A815AA7.1050503@cs.oswego.edu> <4A81C629.9060503@gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042577A01E980@GVW0436EXB.americas.hpqcorp.net>

> From: Osvaldo Pinali Doederlein [mailto:opinali at gmail.com] 
>
> Another major nuisance is the fact that final variables are 
> just syntax sugar, the final modifier is not preserved in 
> bytecode - I'm not sure if this could deliver some advantage 
> (easier work for the JIT compiler building SSA or 
> something?), but I would have that warm fuzzy feling of 
> knowing that my local variables are kept immutable even in 
> the bytecode level. :-) If there is some advantage, perhaps 
> the changes being done to support JSR-308 could accomodate 
> the final modifer for locals? (Ugly hack and probably 
> expensive, I'd prefer a single flag bit set like for final 
> fields and not a pseudo-annotation, but...)
> 
> A+
> Osvaldo
> 
Just to clarify since this may be important to the fence semantics discussion: My reading of my (ancient copy of the) JVM spec is that final fields are identified as such (ACC_FINAL) at the JVM, .class file level, right?  So you're just referring to locals, right?

Since locals in Java can't be shared between threads, the field case seems to be the most important one for this discussion.

Hans

From martinrb at google.com  Wed Aug 12 18:11:03 2009
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 12 Aug 2009 15:11:03 -0700
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A01E980@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>
	<4A7EA9A1.8010902@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECC0@GVW0436EXB.americas.hpqcorp.net>
	<4A815AA7.1050503@cs.oswego.edu> <4A81C629.9060503@gmail.com>
	<238A96A773B3934685A7269CC8A8D042577A01E980@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <1ccfd1c10908121511y2658e984l221b09254111855f@mail.gmail.com>

FYI: get the latest greatest JVMS at
http://blogs.sun.com/abuckley/entry/draft_of_the_java_vm

> My reading of my (ancient copy of the) JVM spec

From davidcholmes at aapt.net.au  Wed Aug 12 18:51:07 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 13 Aug 2009 08:51:07 +1000
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <8BE0BCF5-A9C7-4A1E-BBB5-F72A0F0C43AB@mac.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEAIIDAA.davidcholmes@aapt.net.au>

Hi Ashley,

> For IO bound tasks I am interested in finding out if the underlying OS
> io operation that produces the data keeps going in some fashion even
> if the corresponding thread that consumes it has not yet been reactivated.

In general yes. Exactly how much depends on the OS and its structure. As
long as the I/O request has been made then the OS can service that request -
for example processing an incoming ethernet packet.
>
> And cpu bound threads would ideally be assigned to separate cores so
> that they can truly execute concurrently. But if the underlying scheduler
> decides to assign them to the same core then there will be no improved
performance.

Depends on how many threads you are trying to run on the system and how many
cores you have. If you give threads dedicated cores and the threads are
totally CPU bound then you effectively remove them from the scheduling
decisions: if nothing else can run on their core then they won't get
timesliced.

But to do this you require complete knowledge of everything on the system -
including OS services - to ensure that you don't "starve" your own threads
of services they need, and to ensure the system as a whole still functions
correctly.

> So to somebody ignorant of the facts such as myself, it seems that the
> scheduler would need some sort of clue as to how to schedule the threads
otherwise
> any effort to balance thread pools for size and type of task could
> well be in vain.

The scheduler only knows about scheduling policies and what properties
threads have under those policies. For "normal" time-sharing this means all
threads are initially equal, and the scheduler will schedule them as fairly
as it can, whilst still having regard for performance (eg. thread affinity
typically tries to run a thread on the same processor as it last ran on).
But CPU bound threads will consume their quanta and be switched out; while
I/O bound threads that block a lot tend to get some preferential treatment -
but it all depends on the OS and the scheduling policy.

Binding threads to specific cores makes the scheduler's job both harder and
easier - because there are less choices available.

It sounds to me that you may want to partition your available processors
into two sets: one for CPU bound and one for I/O bound tasks. That might
give your application better overall "performance" (as defined by your
application). But again there are no Java API's for doing this, and
processor-set/cpu-set management is a system level administrative task, not
something to be attempted from within the application.

But as always with these things the first step is to identify if there is
indeed a problem.

David Holmes


From opinali at gmail.com  Wed Aug 12 19:27:17 2009
From: opinali at gmail.com (Osvaldo Pinali Doederlein)
Date: Wed, 12 Aug 2009 20:27:17 -0300
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A01E980@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>		<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>		<4A7D7618.9050501@cs.oswego.edu>	<fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>	<4A7EA9A1.8010902@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECC0@GVW0436EXB.americas.hpqcorp.net>
	<4A815AA7.1050503@cs.oswego.edu> <4A81C629.9060503@gmail.com>
	<238A96A773B3934685A7269CC8A8D042577A01E980@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A834FD5.3000509@gmail.com>

 From Boehm, Hans:
>> From: Osvaldo Pinali Doederlein [mailto:opinali at gmail.com]
>>
>> Another major nuisance is the fact that final variables are
>> just syntax sugar, the final modifier is not preserved in
>> bytecode - I'm not sure if this could deliver some advantage
>> (easier work for the JIT compiler building SSA or
>> something?), but I would have that warm fuzzy feling of
>> knowing that my local variables are kept immutable even in
>> the bytecode level. :-) If there is some advantage, perhaps
>> the changes being done to support JSR-308 could accomodate
>> the final modifer for locals? (Ugly hack and probably
>> expensive, I'd prefer a single flag bit set like for final
>> fields and not a pseudo-annotation, but...)
>>      
> Just to clarify since this may be important to the fence semantics discussion: My reading of my (ancient copy of the) JVM spec is that final fields are identified as such (ACC_FINAL) at the JVM, .class file level, right?  So you're just referring to locals, right?
>
> Since locals in Java can't be shared between threads, the field case seems to be the most important one for this discussion.
>    
That's right, I was just discussing local variables (including method 
parameters). They don't have the ACC_FINAL modifier, so "final" is 
strictly source-level sugar. Right now the only usage for local's final 
tag is inner classes, that can only capture final locals. Preserving 
local var's final status at the bytecode level would indeed mean nothing 
for JMM purposes.

A+
Osvaldo

From robert.fischer at smokejumperit.com  Wed Aug 12 19:36:24 2009
From: robert.fischer at smokejumperit.com (Robert Fischer)
Date: Wed, 12 Aug 2009 19:36:24 -0400
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEAIIDAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEAIIDAA.davidcholmes@aapt.net.au>
Message-ID: <4A8351F8.3060709@smokejumperit.com>

My general guideline has been to shotgun the system: aim to be at the high end of the order of 
magnitude of the number of threads that can be processed at any point in time, and default to 
throwing off new threads to support a high water mark.  Yes, it creates some waste in terms of 
context switching and memory, but anything less than that seems to eventually hit throughput limits.

At the end of the day, though, it's all voodoo.

~~ Robert.

David Holmes wrote:
> Hi Ashley,
> 
>> For IO bound tasks I am interested in finding out if the underlying OS
>> io operation that produces the data keeps going in some fashion even
>> if the corresponding thread that consumes it has not yet been reactivated.
> 
> In general yes. Exactly how much depends on the OS and its structure. As
> long as the I/O request has been made then the OS can service that request -
> for example processing an incoming ethernet packet.
>> And cpu bound threads would ideally be assigned to separate cores so
>> that they can truly execute concurrently. But if the underlying scheduler
>> decides to assign them to the same core then there will be no improved
> performance.
> 
> Depends on how many threads you are trying to run on the system and how many
> cores you have. If you give threads dedicated cores and the threads are
> totally CPU bound then you effectively remove them from the scheduling
> decisions: if nothing else can run on their core then they won't get
> timesliced.
> 
> But to do this you require complete knowledge of everything on the system -
> including OS services - to ensure that you don't "starve" your own threads
> of services they need, and to ensure the system as a whole still functions
> correctly.
> 
>> So to somebody ignorant of the facts such as myself, it seems that the
>> scheduler would need some sort of clue as to how to schedule the threads
> otherwise
>> any effort to balance thread pools for size and type of task could
>> well be in vain.
> 
> The scheduler only knows about scheduling policies and what properties
> threads have under those policies. For "normal" time-sharing this means all
> threads are initially equal, and the scheduler will schedule them as fairly
> as it can, whilst still having regard for performance (eg. thread affinity
> typically tries to run a thread on the same processor as it last ran on).
> But CPU bound threads will consume their quanta and be switched out; while
> I/O bound threads that block a lot tend to get some preferential treatment -
> but it all depends on the OS and the scheduling policy.
> 
> Binding threads to specific cores makes the scheduler's job both harder and
> easier - because there are less choices available.
> 
> It sounds to me that you may want to partition your available processors
> into two sets: one for CPU bound and one for I/O bound tasks. That might
> give your application better overall "performance" (as defined by your
> application). But again there are no Java API's for doing this, and
> processor-set/cpu-set management is a system level administrative task, not
> something to be attempted from within the application.
> 
> But as always with these things the first step is to identify if there is
> indeed a problem.
> 
> David Holmes
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

-- 
~~ Robert Fischer, Smokejumper IT Consulting.
Enfranchised Mind Blog http://EnfranchisedMind.com/blog

Check out my book, "Grails Persistence with GORM and GSQL"!
http://www.smokejumperit.com/redirect.html

From dl at cs.oswego.edu  Wed Aug 12 20:08:25 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 12 Aug 2009 20:08:25 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <20090812220042.GS6779@linux.vnet.ibm.com>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>
	<20090812220042.GS6779@linux.vnet.ibm.com>
Message-ID: <4A835979.30702@cs.oswego.edu>

Paul E. McKenney wrote:
> OK.  I need you and Hans to come to agreement on this one.  ;-)
> 
> Single-level publication safety (Doug's position) is sufficient in many
> cases, but multi-level publication safely (Hans's position) really does
> have use cases in the Linux kernel.
> 

We do need to careful about over-promising.
For example, if some sub-component breaks
safety constraints by publishing a reference to itself, then
no guarantees can be made about safety of outer component.
My current belief is that the transitivity you get just by virtue
of defining in terms of happens-before and synchronizes-with
covers all the territory you can guarantee while still
conservatively minimizing impact on the JMM/JLS.
Which in turn dispenses with "memory chain"
and even "dereferences" relations.
I regenerated such a version,  now grouping formal
specs in a small section of class documentation rather than per-method.
(http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html)
The conciseness and relative simplicity (although not necessarily
immediate understandability :-) is in part due to a possibly
controversial move of positing ghost-fences upon first reads.
The general form is otherwise similar to C++ fence specs.
As always, all help making these better will be appreciated.

-Doug

From jeremy.manson at gmail.com  Wed Aug 12 21:46:03 2009
From: jeremy.manson at gmail.com (Jeremy Manson)
Date: Wed, 12 Aug 2009 18:46:03 -0700
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A01E980@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<fdeb32eb0908082236g2b426ca3ofb835c4d5fc25de0@mail.gmail.com>
	<4A7EA9A1.8010902@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECC0@GVW0436EXB.americas.hpqcorp.net>
	<4A815AA7.1050503@cs.oswego.edu> <4A81C629.9060503@gmail.com>
	<238A96A773B3934685A7269CC8A8D042577A01E980@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <1631da7d0908121846o747c54d1h4e25317d0914fec9@mail.gmail.com>

You are correct, Hans. (I'm sorry I've been bad about keeping up with
this thread).

Jeremy

On Wed, Aug 12, 2009 at 2:53 PM, Boehm, Hans<hans.boehm at hp.com> wrote:
>> From: Osvaldo Pinali Doederlein [mailto:opinali at gmail.com]
>>
>> Another major nuisance is the fact that final variables are
>> just syntax sugar, the final modifier is not preserved in
>> bytecode - I'm not sure if this could deliver some advantage
>> (easier work for the JIT compiler building SSA or
>> something?), but I would have that warm fuzzy feling of
>> knowing that my local variables are kept immutable even in
>> the bytecode level. :-) If there is some advantage, perhaps
>> the changes being done to support JSR-308 could accomodate
>> the final modifer for locals? (Ugly hack and probably
>> expensive, I'd prefer a single flag bit set like for final
>> fields and not a pseudo-annotation, but...)
>>
>> A+
>> Osvaldo
>>
> Just to clarify since this may be important to the fence semantics discussion: My reading of my (ancient copy of the) JVM spec is that final fields are identified as such (ACC_FINAL) at the JVM, .class file level, right? ?So you're just referring to locals, right?
>
> Since locals in Java can't be shared between threads, the field case seems to be the most important one for this discussion.
>
> Hans
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From jeremy.manson at gmail.com  Wed Aug 12 21:49:02 2009
From: jeremy.manson at gmail.com (Jeremy Manson)
Date: Wed, 12 Aug 2009 18:49:02 -0700
Subject: [concurrency-interest] Fences
In-Reply-To: <4A835979.30702@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>
	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
Message-ID: <1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>

I don't mind your formulating these fences in terms of happens-before
pairs (as long as the "pair" to which it is referring is relatively
obvious), but in that case, why are you still bothering to talk about
scope?

Jeremy

On Wed, Aug 12, 2009 at 5:08 PM, Doug Lea<dl at cs.oswego.edu> wrote:
> Paul E. McKenney wrote:
>>
>> OK. ?I need you and Hans to come to agreement on this one. ?;-)
>>
>> Single-level publication safety (Doug's position) is sufficient in many
>> cases, but multi-level publication safely (Hans's position) really does
>> have use cases in the Linux kernel.
>>
>
> We do need to careful about over-promising.
> For example, if some sub-component breaks
> safety constraints by publishing a reference to itself, then
> no guarantees can be made about safety of outer component.
> My current belief is that the transitivity you get just by virtue
> of defining in terms of happens-before and synchronizes-with
> covers all the territory you can guarantee while still
> conservatively minimizing impact on the JMM/JLS.
> Which in turn dispenses with "memory chain"
> and even "dereferences" relations.
> I regenerated such a version, ?now grouping formal
> specs in a small section of class documentation rather than per-method.
> (http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html)
> The conciseness and relative simplicity (although not necessarily
> immediate understandability :-) is in part due to a possibly
> controversial move of positing ghost-fences upon first reads.
> The general form is otherwise similar to C++ fence specs.
> As always, all help making these better will be appreciated.
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From dl at cs.oswego.edu  Thu Aug 13 08:32:48 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 13 Aug 2009 08:32:48 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
References: <4A7C2365.2020106@cs.oswego.edu>	
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	
	<4A7D7618.9050501@cs.oswego.edu>	
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	
	<4A81559D.5000708@cs.oswego.edu>	
	<20090812220042.GS6779@linux.vnet.ibm.com>	
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
Message-ID: <4A8407F0.7030708@cs.oswego.edu>

Jeremy Manson wrote:
>  but in that case, why are you still bothering to talk about
> scope?
>

Only because I wanted to let this settle a bit before
also simplifying the surrounding prose. But I guess a few hours
of settling is long enough :-), so I did a streamlining pass
through the javadocs, regenerated and checked in.  (As always, see
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html)

While I'm at it, a few other observations:

* I think the form of the specs makes it clearer that initial
publication is a specialized form of acquire/release, and
shares its context-dependent correctness, which accounts
for people being a little nervous about un-careful use.

* As Jeremy noticed (off-list), people will need more
care using orderWrites than they do with plain "final"
fields. For example, in:
   class C1 {
     final int[] a;
     C1() {
       a = new int[1];
       a[0] = 1;
     }
   }
People can get away with writing to the array after
assigning it to "a", and still obtain guarantees about
readers seeing a[0] as 1. But in the emulated version,
the assignment to "a" must follow initialization, as in:
   class C2 {
     int[] a; // emulated-final
     C2() {
       int[] tmp = new int[1];
       tmp[0] = 1;
       a = orderWrites(tmp);
     }
   }
Which is arguably better style anyway.

-Doug









From dl at cs.oswego.edu  Thu Aug 13 10:34:31 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 13 Aug 2009 10:34:31 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A8407F0.7030708@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>		<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>		<4A7D7618.9050501@cs.oswego.edu>		<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>		<4A81559D.5000708@cs.oswego.edu>		<20090812220042.GS6779@linux.vnet.ibm.com>		<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu>
Message-ID: <4A842477.3070205@cs.oswego.edu>

Doug Lea wrote:
> so I did a streamlining pass
> through the javadocs, regenerated and checked in.  (As always, see
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html) 

And another one, avoiding some further dereference-ese
in both specs and prose. Sorry for the churn.

-Doug



From dl at cs.oswego.edu  Fri Aug 14 08:17:43 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 14 Aug 2009 08:17:43 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <20090813234205.GO6744@linux.vnet.ibm.com>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>
	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
Message-ID: <4A8555E7.3070200@cs.oswego.edu>

Paul E. McKenney wrote:
>> ... (As always, see 
>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html)
>> 
> If I understand the intent, in "Ordering Semantics", the last sub-bullet of
> the last bullet of the "Given:" clause:
> 
> "a program point following the initial read obtaining value ref by some
> thread t, but prior to any other read by t."
> 
> needs to instead end with something like the following:
> 
> "but prior to any any other access by t that uses ref."

Thanks. The idea here is just to identify the location
of that ghost-fence, so any further implied constraints
were unintentional. At an implementation level,
there is no actual fence here for any processor
that does not speculate across the first read of a
reference by a thread, which holds for all
processors/platforms I know of except maybe Alpha.

Notice that in the formal specs, "ref" is basically
a tag to enable us to express semantics
in terms of pairings. While people could use arbitrary
object references in fence methods, the form of the API
now guides people to express constructions using
natural/sensible ones that are likely to match intent.

Thanks also to others pointing out other wording improvements.
Another update pass is now committed.

-Doug


From hans.boehm at hp.com  Fri Aug 14 19:57:52 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 14 Aug 2009 23:57:52 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A8555E7.3070200@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>

Doug -

Parts of this are clearly making progress, but I still have some basic issues with fence semantics.

Here's another set of comments on the current version:

- [Fundamental issue] I think there's still a question about whether orderReads orders the prior read with respect to subsequent writes, at least those using ref.  For the acquire/release example, I think it must also order those.  I think the current more formal description implies that writes are also ordered, but other sections say other things.

- [Presentation issue, but it might obscure a more fundamental problem]  Ordering semantics: The second bullet in the definition of rf remains very unclear.  Does the conclusion hold for all possible values of rf satisfying this condition, or one such program point, chosen by the system, so to speak.

- [Fundamental problem] In either case, I think the ordering semantics are MUCH too strong.  Consider:

Thread1:

x = 42;
orderWrites(flag);
flag = p;	// previously null


Thread2:

r1 = flag;
r2 = r1.a;
r3 = x;

If r1 == p, I believe we get x = 42 hb r3 = x, i.e. r3 == 42.  That's not going to hold on any weakly ordered architecture (PowerPC, ARM, Itanium, ...) without a fence (or ld.acq for Itanium) in thread 2.  But that would imply fences (or ordering constraints) everywhere, since none of the loads in thread 2 are identifiably special.

- Volatile emulation [presentation issue]:  I still don't think this is a faithful emulation, and we should point out that it's not.  At a minimum, it doesn't handle IRIW correctly.  I suspect there are other, more serious, issues.  Paul and I should discuss and are trying to find time.

- Acquire/release management [presentation issue]:
(I think Sarita also pointed this out previously in some related discussion.)  A major problem with this kind of usage is that it's visible to the client of such a class.  Calls to acquireItem() and releaseItem() don't look sequentially consistent to the caller; the non-sequentially-consistent behavior introduced by avoiding volatiles here is not hidden inside the implementation.  I would point this out explicitly, and strongly discourage this unless there are overriding critical performance considerations. Especially since I continue to suspect that the added overhead of volatile is an largely artifact of current implementations, I don't think we want to encourage this.  I understand that it may be hard to avoid in the short term.  (If ref is used only as a tag, I think the fences really outlaw a lot more potentially profitable hardware reordering than the volatile declaration.  However the fences are mostly a better match to current hardware ISAs.)

- reachabilityFence [presentation issue]: I'm not enthusiastic about the discussion following "Further continuing":

The next version of the example has a data race on the externalResourceArray access, and that should be avoided in the example.  It may be that real code would lock the array in any case, and thus this issue would go away, but the example becomes confusing.

I think we should make it clear that reachabilityFence is typically a better solution than synchronized(this).  I expect it to normally be MUCH cheaper (total cost on the order of a cycle or so, possibly zero, if implementations invest the effort).  And I think that for code that is not specifically intended for thread communication, synchronized(this) is typically not otherwise useful.  A more realistic version of the example would lock externalResourceArray, not this.

Hans

From dl at cs.oswego.edu  Sat Aug 15 09:34:30 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 15 Aug 2009 09:34:30 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>
	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A86B966.50406@cs.oswego.edu>

Boehm, Hans wrote:
> - [Fundamental issue] I think there's still a question about whether
> orderReads orders the prior read with respect to subsequent writes, at least
> those using ref.  For the acquire/release example, I think it must also order
> those.  I think the current more formal description implies that writes are
> also ordered, but other sections say other things.

I agree specs imply this.
I don't think the other wording currently says other things, but could
have missed something. For example, the introductory sentence says:
   "Method orderWrites is typically used to enforce order between two writes"
where the "typically" is a matter of usage guidance, but the method
declaration doc says "... accesses (reads or writes) prior ..."

> 
> The second bullet in the definition of rf remains very
> unclear.  Does the conclusion hold for all possible values of rf satisfying
> this condition, or one such program point, chosen by the system, so to speak.

The latter. I tried to clarify. (as always, see
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html)

> Consider:
> Thread1:
> x = 42; orderWrites(flag); flag = p;	// previously null
> Thread2:
> r1 = flag; r2 = r1.a; r3 = x;
> 
> If r1 == p, I believe we get x = 42 hb r3 = x, i.e. r3 == 42.  That's not
> going to hold on any weakly ordered architecture 

(It is challenging to balance the too-strong/too-weak dimension vs
avoiding reliance on those parts of JMM/JLS that are likely to
need fixing. I under-estimated this in a previous reply.)

It is possible to recast this, restricting guarantees to only writes
of ref and reads using (dereferencing) ref, which covers the vast
majority of practical cases anyway, although it would revive
need for introducing memory-chain-like terminology. If you
or anyone else would like to give this a shot, I'd appreciate
the help.

This would also require strengthening effect of orderAccess to
be independent of ref, which is a good idea anyway (see below)
so is now committed.

> 
> - Volatile emulation [presentation issue]:  I still don't think this is a
> faithful emulation, and we should point out that it's not.  At a minimum, it

Well, it IS a faithful emulation of how it is normally implemented,
so it is a matter of specs, not implementations, I hope!

(Most if not all JVMs internally use logical-fences,
that were initially mainly derived from
my Cookbook recommendations. They then at some point do a
secondary mapping to processor level, which may map to
fence instructions, or to related instruction sequences,
or to modes for reads or writes. Modulo a few details,
the Fences API corresponds to these logical fences,
so the VM implementors I've checked with are not
too concerned about implementability.)

> doesn't handle IRIW correctly.  
> I suspect there are other, more serious,
> issues.  Paul and I should discuss and are trying to find time.

I assume this is mainly the hwsync vs lwsync issue for load fences
on Power? Which would apply to all volatile constructions
as well; or, if not, would seem to imply a JMM spec problem?

In any case (also in keeping with the recent clarification
I noticed for similar aspects of C++ spec), the spec'ed
ordering effects of orderAccesses should be made independent
of ref argument to more readily support the inference that any
program in which each access is separated by orderAccesses is
sequentially consistent. (It would be nice to check if
this suffices without further tie-ins to the JLS sec 17.4
details we are trying to avoid relying on.)

> 
> - Acquire/release management [presentation issue]: (I think Sarita also
> pointed this out previously in some related discussion.)  A major problem
> with this kind of usage is that it's visible to the client of such a class.
> Calls to acquireItem() and releaseItem() don't look sequentially consistent
> to the caller

There is a limit to how many ways and places we can say "correctness is
context dependent" (as it currently does once in that paragraph)
without it overtaking the actual information provided.
Pointwise suggestions for additional ways/places to say this
where it would be helpful would be welcome though.
I did just now revive one disclaimer elsewhere from a previous version.


> - reachabilityFence [presentation issue]: I'm not enthusiastic about the
> discussion following "Further continuing":
> 
> The next version of the example has a data race on the externalResourceArray
> access, and that should be avoided in the example.  It may be that real code
> would lock the array in any case, and thus this issue would go away, but the
> example becomes confusing.

OK; I had adapted this from a posted suggestion from
discussions last year, but since it doesn't help
illustrate usage of reachabilityFence anyway (but instead one
way to avoid it), it is better to kill it if it is confusing.

> 
> I think we should make it clear that reachabilityFence is typically a better
> solution than synchronized(this).  

Thanks; I reworked a sentence to try to get this across
a little better.

-Doug

From dl at cs.oswego.edu  Sat Aug 15 12:02:46 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 15 Aug 2009 12:02:46 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>
	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A86DC26.1070905@cs.oswego.edu>

As an interlude, it is worth stepping back to remember
the motivation for this style of API. (The following is probably
mostly a repeat of postings from a year or two ago, but I don't
dare go back and look :-)

The main target audience seems to be three kinds of
angry customers:
  (1) People who encounter constructions that otherwise
      cannot be reasonably expressed (as in "transient final").
  (2) People fixing safety bugs, as in the emulation examples,
      These people are not sympathetic to suggestions that they
      instead overhaul their systems and/or relay bug reports
      to third-party component developers to fix immediate
      bugs. The use of such ad-hoc volatile/final/whatever
      emulation may not result in provably safe
      operation, but will surely not make things worse.
  (3) People encountering situations like those on the
      losing side of initial JSR133 "weak" vs "strong"
      volatile semantics debates. This usually amounts to
      acquire/release constructions for which the strong
      form buys you nothing, yet underlying fences cannot
      in general be optimized away because compilers and tools
      cannot verify the reasoning (usually some sort of ownership
      constraints) that would allow it.

Deciding whether to address this is a cost-benefit question:
Adding support for fine-grained ordering will alleviate existing
problems, but will also introduce new opportunities for error.
(I suppose that NOT adding support could also make some
existing problems go away, as frustrated developers eventually
decide to use other languages/platforms.)

What most people would ideally like for purposes
of fine-grained ordering control across these situations
is a way to obtain per-use overrides of volatile/final
qualifiers and/or AtomicX methods; perhaps
ideally supported via a bunch of methods like:
   void   setAsIfFinal(address, value)
   Object getAsIfVolatile(address);
   int    getIntAsIfRelaxed(address);

If we could create an API of this form, then it
would probably be easier to explain how this API fit
into the JLS/JMM, and also be easier for people to use
than alternatives. But (unlike the case with C++, where
a variant appears in the templated atomic
classes) we can't do this, because there is no legal
way in Java to express the "address" argument.
And we cannot introduce one.

To work around this, the growing number of people
desperate enough to do whatever it takes to fix safety
or performance problems requiring ordering control
have discovered ways to cheat by using the
internal underlying APIs that actually do
implement per-use volatile/final/ordered reads and writes,
but can do so only by employing an intrinsically unsafe
way of designating those addresses (as ref+offset).
It is very disconcerting to see people do this --
it is an indication of failure in providing enough
memory ordering constructs to meet actual use cases.

The next idea that usually comes to mind is that maybe
you could do this via reflection. As in adding a bunch
of methods to java.lang.reflect.Field, like:
   void setAsIfVolatile(value);
But you soon hit the problem that it would require
miraculously better implementations of reflection-related
support than is present even in those VMs optimizing
it to make this solution efficient enough
to contemplate. Performance of reflective invocation is
occasionally competitive; the main concern is its extremely
high variability in contexts where you need the predictable
performance you'd get with simple mappings to intrinsics.

Which leads to the Fences approach; which avoids using
inexpressible APIs, unsafe constructions, and reflection
overhead, and has a very simple mapping to the
internals of most JVMs. But at the expense of requiring specs
that rely on indirect matchups between the reads/writes
people actually care about, wrt the Fence invocations.
And with the challenge of avoiding making these specs
stronger, weaker, or incommensurate with the ones
you'd get with more direct approaches to this.
But since this is "only" a matter of specs, there
seems to be no reason in principle that this cannot
succeed.

-Doug

From dl at cs.oswego.edu  Sat Aug 15 14:28:00 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 15 Aug 2009 14:28:00 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>
	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A86FE30.2070802@cs.oswego.edu>

Back to...
Boehm, Hans wrote:
> Thread1:
> x = 42; orderWrites(flag); flag = p;	// previously null
> Thread2:
> r1 = flag; r2 = r1.a; r3 = x;
> 
> If r1 == p, I believe we get x = 42 hb r3 = x, i.e. r3 == 42.  

Here's some context I'm assuming is needed to make this
compile:

class Flag { int a; }
Flag flag = null;
Flag p = new Flag();
int x;

Thread 1:
   x = 42;
   orderWrites(flag); // == orderWrites(null) => no-op
   flag = p;

Thread 2:
   r1 = flag; // either null or p
   r2 = r1.a; // either NPE or 0
   r3 = x;

At which point I'm no longer positive about what you had in mind.

-Doug







From forax at univ-mlv.fr  Sat Aug 15 17:43:16 2009
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Sat, 15 Aug 2009 23:43:16 +0200
Subject: [concurrency-interest] Fences
In-Reply-To: <4A86DC26.1070905@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>	<4A8555E7.3070200@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
Message-ID: <4A872BF4.9010409@univ-mlv.fr>

Le 15/08/2009 18:02, Doug Lea a ?crit :
> As an interlude, it is worth stepping back to remember
> the motivation for this style of API. (The following is probably
> mostly a repeat of postings from a year or two ago, but I don't
> dare go back and look :-)
>
> The main target audience seems to be three kinds of
> angry customers:
>  (1) People who encounter constructions that otherwise
>      cannot be reasonably expressed (as in "transient final").
>  (2) People fixing safety bugs, as in the emulation examples,
>      These people are not sympathetic to suggestions that they
>      instead overhaul their systems and/or relay bug reports
>      to third-party component developers to fix immediate
>      bugs. The use of such ad-hoc volatile/final/whatever
>      emulation may not result in provably safe
>      operation, but will surely not make things worse.
>  (3) People encountering situations like those on the
>      losing side of initial JSR133 "weak" vs "strong"
>      volatile semantics debates. This usually amounts to
>      acquire/release constructions for which the strong
>      form buys you nothing, yet underlying fences cannot
>      in general be optimized away because compilers and tools
>      cannot verify the reasoning (usually some sort of ownership
>      constraints) that would allow it.
>
> Deciding whether to address this is a cost-benefit question:
> Adding support for fine-grained ordering will alleviate existing
> problems, but will also introduce new opportunities for error.
> (I suppose that NOT adding support could also make some
> existing problems go away, as frustrated developers eventually
> decide to use other languages/platforms.)
>
> What most people would ideally like for purposes
> of fine-grained ordering control across these situations
> is a way to obtain per-use overrides of volatile/final
> qualifiers and/or AtomicX methods; perhaps
> ideally supported via a bunch of methods like:
>   void   setAsIfFinal(address, value)
>   Object getAsIfVolatile(address);
>   int    getIntAsIfRelaxed(address);
>
> If we could create an API of this form, then it
> would probably be easier to explain how this API fit
> into the JLS/JMM, and also be easier for people to use
> than alternatives. But (unlike the case with C++, where
> a variant appears in the templated atomic
> classes) we can't do this, because there is no legal
> way in Java to express the "address" argument.
> And we cannot introduce one.

address = java.lang.reflect.Field + object

>
> To work around this, the growing number of people
> desperate enough to do whatever it takes to fix safety
> or performance problems requiring ordering control
> have discovered ways to cheat by using the
> internal underlying APIs that actually do
> implement per-use volatile/final/ordered reads and writes,
> but can do so only by employing an intrinsically unsafe
> way of designating those addresses (as ref+offset).
> It is very disconcerting to see people do this --
> it is an indication of failure in providing enough
> memory ordering constructs to meet actual use cases.
>
> The next idea that usually comes to mind is that maybe
> you could do this via reflection. As in adding a bunch
> of methods to java.lang.reflect.Field, like:
>   void setAsIfVolatile(value);
> But you soon hit the problem that it would require
> miraculously better implementations of reflection-related
> support than is present even in those VMs optimizing
> it to make this solution efficient enough
> to contemplate. Performance of reflective invocation is
> occasionally competitive; the main concern is its extremely
> high variability in contexts where you need the predictable
> performance you'd get with simple mappings to intrinsics.

field setter/getter call aren't that variable.
method invocation is hightly variable.

Why not something like Atomic*Updater ?

public class Fence|Integer or Long or Reference|FieldUpdater<T,V> {
   public static FenceFieldUpdater newUpdater(Class<T> declaringClass, 
Class<V> valueClass,  String fieldName)

   void   setAsIfFinal(T object, |int or long or Object| value)
   |int or long or Object| getAsIfVolatile(T object);
   |int or long or Object| getAsIfRelaxed(T object);
}


>
> Which leads to the Fences approach; which avoids using
> inexpressible APIs, unsafe constructions, and reflection
> overhead, and has a very simple mapping to the
> internals of most JVMs. But at the expense of requiring specs
> that rely on indirect matchups between the reads/writes
> people actually care about, wrt the Fence invocations.
> And with the challenge of avoiding making these specs
> stronger, weaker, or incommensurate with the ones
> you'd get with more direct approaches to this.
> But since this is "only" a matter of specs, there
> seems to be no reason in principle that this cannot
> succeed.
>
> -Doug

R?mi

From hans.boehm at hp.com  Sat Aug 15 22:29:28 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sun, 16 Aug 2009 02:29:28 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A86FE30.2070802@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86FE30.2070802@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D042577A0CF677@GVW0436EXB.americas.hpqcorp.net>

> From: Doug Lea [mailto:dl at cs.oswego.edu] 
> 
> Back to...
> Boehm, Hans wrote:
> > Thread1:
> > x = 42; orderWrites(flag); flag = p;	// previously null
> > Thread2:
> > r1 = flag; r2 = r1.a; r3 = x;
> > 
> > If r1 == p, I believe we get x = 42 hb r3 = x, i.e. r3 == 42.  
> 
> Here's some context I'm assuming is needed to make this
> compile:
> 
> class Flag { int a; }
> Flag flag = null;
> Flag p = new Flag();
> int x;
> 
> Thread 1:
>    x = 42;
>    orderWrites(flag); // == orderWrites(null) => no-op
>    flag = p;
> 
> Thread 2:
>    r1 = flag; // either null or p
>    r2 = r1.a; // either NPE or 0
>    r3 = x;
To clarify, assume thread 2 instead does

r1 = flag;
if (flag != null) {
   r2 = r1.a;  // sees 0
}
r3 = x;

Can we get r1 != null and r3 != 42?

If I understand correctly, the proposed fence semantics disallow this.  I expect that all current implementations would allow it for all reasonable implementations of the fence.  There is nothing to prevent compilers from performing the load from x early, for example.  The problem is that you are constraining the compilation of the code in thread 2, in spite of the fact that it doesn't mention fences.

Does that clarify things?

Hans

  
> 
> At which point I'm no longer positive about what you had in mind.
> 
> -Doug
> 


From dl at cs.oswego.edu  Sun Aug 16 07:22:00 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 16 Aug 2009 07:22:00 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A0CF677@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>
	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86FE30.2070802@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF677@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A87EBD8.3090007@cs.oswego.edu>

Boehm, Hans wrote:
> To clarify, assume thread 2 instead does
> 
> r1 = flag; if (flag != null) { r2 = r1.a;  // sees 0 } r3 = x;
> 
> Can we get r1 != null and r3 != 42?
> 
> If I understand correctly, the proposed fence semantics disallow this.  I
> expect that all current implementations would allow it for all reasonable
> implementations of the fence.  

OK; that's what I initially thought you meant (but the
null argument then led me to recheck).

I still do agree this must be weakened.
Doing so requires the introduction of wording about
dependencies, which may be hard to mesh
with JMM/JLS, so I'm still looking for help on this.
(I'll also give it a try.)

This might be a little easier to pull off if we
adopt a suggestion Bill Pugh made in discussions to
separate methods/specs for a separate form of
orderWritesForInitialPublication. This would still
entail adapting/fixing JLS sec 17.5 stuff.

In fact, taking this approach might be necessary to
achieve the goal of producing a spec with the property
that any program using the listed manual emulations of
finals, volatiles, and atomic methods conforms
to the JLS specs for the corresponding language constructs,
modulo little things like the fact that language-level
"final" allows user sloppiness in ordering.
This would have the side-benefit of more-or-less proving
the correctness of the usual internal implementations.

-Doug


From dl at cs.oswego.edu  Sun Aug 16 08:40:24 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 16 Aug 2009 08:40:24 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A872BF4.9010409@univ-mlv.fr>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>	<4A8555E7.3070200@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu> <4A872BF4.9010409@univ-mlv.fr>
Message-ID: <4A87FE38.3060304@cs.oswego.edu>

R?mi Forax wrote:
> Why not something like Atomic*Updater ?
> 
> public class Fence|Integer or Long or Reference|FieldUpdater<T,V> {
>   public static FenceFieldUpdater newUpdater(Class<T> declaringClass, 
> Class<V> valueClass,  String fieldName)
> 

Thanks for the reminder that I should have
also explained limitations of this approach;
especially in case anyone sees a miraculous
solution to them.

AtomicReferenceFieldUpdater is usually acceptable
for its primary use case, of enabling compareAndSet
(CAS) on a volatile field. It is OK there because
CAS is relatively expensive anyway, so the
overhead isn't usually bad enough to push you over to
the alternative of using distinct AtomicX objects in
data structures where you are trying to conserve
space, as well as avoiding fact that when you have
another object, then you have doubled the number of
reads/writes you need to think about.

But most other orderings are far cheaper to implement
than the overhead entailed in calling updater methods,
that entail dynamic checks that are hard to optimize away.
To illustrate:

class C {
   X aField; // emulated-final
   static ... u =  ...newUpdater(C.class, X.class, "afield");
   C() {
      X x = ...
      u.setAsIfFinal(this, x);
   }
}

Then the steps taken in setAsIfFinal(target, value),
which would be the same as current method lazySet,
are:
1. Check that target is instance of the first class
argument in the updater constructor.
2. Similarly for value as instance of second constructor argument
3. Check that the caller is allowed to access afield. Which,
    if  the field is "protected" is especially messy/costly.
4. Issue (logical) store fence
5. Issue write.

These checks are more expensive than those used in ordinary
internal JVM type checking/tracing because they match up
use-time types with the declare-time types held in updater
objects, as opposed to the usual case of known constant types.
(One reason these checks are needed, even though the generic
type parameterization ensures validity at compile-time,
is that a JVM must protect against use of bytecode versions
that did not go through javac so weren't statically checked.)

There are a few ways to make this a little faster,
for example using multiple internal classes that specialize
on some cheaper cases. And ideally, if escape analysis
were known to reliably detect that updaters
are never exported outside their classes (which they never
are in the usages they are intended for), then some of the
checks would be cheaper or optimized away.
But I don't see any realistic near/medium-term prospects
for avoiding updaters being at least a few times slower
than just issuing that store fence (which translates either
to a cheap kind of fence or a no-op on all platforms,
and imposes very few optimization constraints).

-Doug




From martinrb at google.com  Sun Aug 16 15:48:44 2009
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 16 Aug 2009 12:48:44 -0700
Subject: [concurrency-interest] Fences
In-Reply-To: <4A87FE38.3060304@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu> <4A872BF4.9010409@univ-mlv.fr>
	<4A87FE38.3060304@cs.oswego.edu>
Message-ID: <1ccfd1c10908161248y4a2b04a6yba7e8af0a920c7f2@mail.gmail.com>

On Sun, Aug 16, 2009 at 05:40, Doug Lea<dl at cs.oswego.edu> wrote:
> R?mi Forax wrote:
>>
>> Why not something like Atomic*Updater ?
>>
>> public class Fence|Integer or Long or Reference|FieldUpdater<T,V> {
>> ?public static FenceFieldUpdater newUpdater(Class<T> declaringClass,
>> Class<V> valueClass, ?String fieldName)
>
> AtomicReferenceFieldUpdater is usually acceptable
> for its primary use case, of enabling compareAndSet
> (CAS) on a volatile field. It is OK there because
> CAS is relatively expensive anyway, so the
> overhead isn't usually bad enough to push you over to
> the alternative of using distinct AtomicX objects in
> data structures where you are trying to conserve
> space, as well as avoiding fact that when you have
> another object, then you have doubled the number of
> reads/writes you need to think about.

It's worth noting that this is the cost model for x86.
As I've learned from Cliff Click, on Azul the cost of
uncontended hit-in-cache CAS is only one cycle.
But even Cliff recommends that Java programmers
program for the x86.  Still, we might think about
ways to make our use of the cost model more
explicit, e.g. to have a static final that represents
the ratio of cost of, e.g. CAS to volatile read,
and derive other values (e.g. CLQ hop count) from that.

> But most other orderings are far cheaper to implement
> than the overhead entailed in calling updater methods,
> that entail dynamic checks that are hard to optimize away.
> To illustrate:
>
> class C {
> ?X aField; // emulated-final
> ?static ... u = ?...newUpdater(C.class, X.class, "afield");
> ?C() {
> ? ? X x = ...
> ? ? u.setAsIfFinal(this, x);
> ?}
> }
>
> Then the steps taken in setAsIfFinal(target, value),
> which would be the same as current method lazySet,
> are:
> 1. Check that target is instance of the first class
> argument in the updater constructor.
> 2. Similarly for value as instance of second constructor argument
> 3. Check that the caller is allowed to access afield. Which,
> ? if ?the field is "protected" is especially messy/costly.
> 4. Issue (logical) store fence
> 5. Issue write.

It's not obvious to me that in the common cases all
of the checking overhead in atomic updater classes
cannot be optimized away, especially if the updater itself
is final.  The kinds of checking done by javac that
cannot be relied upon can instead be done by the
runtime compiler, which *can* be relied upon.

> These checks are more expensive than those used in ordinary
> internal JVM type checking/tracing because they match up
> use-time types with the declare-time types held in updater
> objects, as opposed to the usual case of known constant types.
> (One reason these checks are needed, even though the generic
> type parameterization ensures validity at compile-time,
> is that a JVM must protect against use of bytecode versions
> that did not go through javac so weren't statically checked.)
>
> There are a few ways to make this a little faster,
> for example using multiple internal classes that specialize
> on some cheaper cases. And ideally, if escape analysis
> were known to reliably detect that updaters
> are never exported outside their classes (which they never
> are in the usages they are intended for), then some of the
> checks would be cheaper or optimized away.

I'm imagining that the reference to the updater is static final,
and the call into the updater (e.g. setAsIfFinal) is inlined
into the caller, where the JIT can optimize away all the safety checks.
Hotspot could have special knowledge of these methods,
e.g. try extra hard to inline anything in java.util.concurrent.atomic.

Martin (not a hotspot engineer)

> But I don't see any realistic near/medium-term prospects
> for avoiding updaters being at least a few times slower
> than just issuing that store fence (which translates either
> to a cheap kind of fence or a no-op on all platforms,
> and imposes very few optimization constraints).
>
> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From hans.boehm at hp.com  Sun Aug 16 19:21:21 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sun, 16 Aug 2009 23:21:21 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A86DC26.1070905@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>

I agree that this appears to be by far the easiest solution to an immediate practical problem.

The reasons I remain uneasy about it have to do with longer term evolution of the language.  Stepping back even further:

I think we have some agreement that the current Java memory model works well in cases in which data races can be avoided.  In that case, the Java memory model basically reduces to sequential consistency.  And we don't have to worry about access granularity issues.  Synchronization-free library calls behave atomically (since there are no races that could observe otherwise), etc.  The world is reasonably simple and pleasant.  The one major problem is that  programmers typically get no warning when they leave this relatively simple world by accidentlly introducing a data race.

Once we introduce data races, I think we actually have a bit of a mess, as does every other major language in one way or another:

- The Java memory model isn't quite correct.  Or alternatively, current implementations aren't quite correct with respect to the model, at least in odd corner cases.  (See Sevcik and Aspinalls ECOOP 2008 paper.)  More importantly, reasoning about any of this seems much harder than we had thought.

- At least some of us have serious doubts about fixing this with minor tweaks.  It's possible that this may work, but there seem to be fundamental issues with specifying data race semantics in a way that doesn't disable essential compiler optimizations.

- I think the library's behavior is generally not very well specified in the presence of data races.  Technically, it probably should be, since malicious sandboxed code can introduce such races, and you can't really reason about security properties without understanding the consequences.

- Programmers have a notoriously hard time both in getting code with data race correct, and debugging the incorrect results.

At least some of us believe that it would be MUCH nicer if, in the possibly distant future, the implementation just prevented you from writing programs with data races, by statically or dynamically detecting them ahead of actually executing the race.   There are existing techniques to do this dynamicaally, but they are currently too slow to be practical in production code.

Especially if you believe that this is a plausible evolutionary path, but probably even if you don't, it seems important to evolve towards code that doesn't contain data races (or equivalently explicitly identifies racing operations as synchronization operations).  In the shorter term, this also has the advantage of making race detecting tools much more useful as debugging aids, in addition to sticking to a piece of the language spec we actually understand.

The problem with fences, as currently proposed, is that they are useful only for code that contains otherwise unannotated data races, which I would really prefer to see us eliminate.  Admittedly this is less of an issue if the fences are only used to ensure safety in the presence of malicious code, which can certainly be a legitimate  use.  (See Doug's (2) below.)

Admittedly, this is not an alternate solution for the short term problem.  The long term solution probably requires far more drastic changes than we are willing to make at the moment.

Hans

> -----Original Message-----
> From: Doug Lea [mailto:dl at cs.oswego.edu] 
> Sent: Saturday, August 15, 2009 9:03 AM
> To: Boehm, Hans
> Cc: paulmck at linux.vnet.ibm.com; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Fences
> 
> As an interlude, it is worth stepping back to remember the 
> motivation for this style of API. (The following is probably 
> mostly a repeat of postings from a year or two ago, but I 
> don't dare go back and look :-)
> 
> The main target audience seems to be three kinds of angry customers:
>   (1) People who encounter constructions that otherwise
>       cannot be reasonably expressed (as in "transient final").
>   (2) People fixing safety bugs, as in the emulation examples,
>       These people are not sympathetic to suggestions that they
>       instead overhaul their systems and/or relay bug reports
>       to third-party component developers to fix immediate
>       bugs. The use of such ad-hoc volatile/final/whatever
>       emulation may not result in provably safe
>       operation, but will surely not make things worse.
>   (3) People encountering situations like those on the
>       losing side of initial JSR133 "weak" vs "strong"
>       volatile semantics debates. This usually amounts to
>       acquire/release constructions for which the strong
>       form buys you nothing, yet underlying fences cannot
>       in general be optimized away because compilers and tools
>       cannot verify the reasoning (usually some sort of ownership
>       constraints) that would allow it.
> 
> Deciding whether to address this is a cost-benefit question:
> Adding support for fine-grained ordering will alleviate 
> existing problems, but will also introduce new opportunities 
> for error.
> (I suppose that NOT adding support could also make some 
> existing problems go away, as frustrated developers 
> eventually decide to use other languages/platforms.)
> 
> What most people would ideally like for purposes of 
> fine-grained ordering control across these situations is a 
> way to obtain per-use overrides of volatile/final qualifiers 
> and/or AtomicX methods; perhaps ideally supported via a bunch 
> of methods like:
>    void   setAsIfFinal(address, value)
>    Object getAsIfVolatile(address);
>    int    getIntAsIfRelaxed(address);
> 
> If we could create an API of this form, then it would 
> probably be easier to explain how this API fit into the 
> JLS/JMM, and also be easier for people to use than 
> alternatives. But (unlike the case with C++, where a variant 
> appears in the templated atomic
> classes) we can't do this, because there is no legal way in 
> Java to express the "address" argument.
> And we cannot introduce one.
> 
> To work around this, the growing number of people desperate 
> enough to do whatever it takes to fix safety or performance 
> problems requiring ordering control have discovered ways to 
> cheat by using the internal underlying APIs that actually do 
> implement per-use volatile/final/ordered reads and writes, 
> but can do so only by employing an intrinsically unsafe way 
> of designating those addresses (as ref+offset).
> It is very disconcerting to see people do this -- it is an 
> indication of failure in providing enough memory ordering 
> constructs to meet actual use cases.
> 
> The next idea that usually comes to mind is that maybe you 
> could do this via reflection. As in adding a bunch of methods 
> to java.lang.reflect.Field, like:
>    void setAsIfVolatile(value);
> But you soon hit the problem that it would require 
> miraculously better implementations of reflection-related 
> support than is present even in those VMs optimizing it to 
> make this solution efficient enough to contemplate. 
> Performance of reflective invocation is occasionally 
> competitive; the main concern is its extremely high 
> variability in contexts where you need the predictable 
> performance you'd get with simple mappings to intrinsics.
> 
> Which leads to the Fences approach; which avoids using 
> inexpressible APIs, unsafe constructions, and reflection 
> overhead, and has a very simple mapping to the internals of 
> most JVMs. But at the expense of requiring specs that rely on 
> indirect matchups between the reads/writes people actually 
> care about, wrt the Fence invocations.
> And with the challenge of avoiding making these specs 
> stronger, weaker, or incommensurate with the ones you'd get 
> with more direct approaches to this.
> But since this is "only" a matter of specs, there seems to be 
> no reason in principle that this cannot succeed.
> 
> -Doug
> 

From invite+zj4ocfjf_22y at facebookmail.com  Sun Aug 16 20:36:31 2009
From: invite+zj4ocfjf_22y at facebookmail.com (Kimo Crossman)
Date: Sun, 16 Aug 2009 17:36:31 -0700
Subject: [concurrency-interest] Check out my photos on Facebook
Message-ID: <4c551fd1f7beda1991cd0a8afb49432f@localhost.localdomain>

Hi concurrency-interest at cs.oswego.edu,

I set up a Facebook profile where I can post my pictures, videos and events and I want to add you as a friend so you can see it. First, you need to join Facebook! Once you join, you can also create your own profile.

Thanks,
Kimo

To sign up for Facebook, follow the link below:
http://www.facebook.com/p.php?i=766425653&k=Z4E33VSYT451UCD1QB64WSVZZRDA&r


concurrency-interest at cs.oswego.edu was invited to join Facebook by Kimo Crossman. If you do not wish to receive this type of email from Facebook in the future, please click on the link below to unsubscribe.
http://www.facebook.com/o.php?k=730475&u=100000167799772&mid=f2a33dG5af31a7aabdcG0G8
Facebook's offices are located at 1601 S. California Ave., Palo Alto, CA 94304

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090816/9b676224/attachment.html>

From dl at cs.oswego.edu  Mon Aug 17 07:32:57 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 17 Aug 2009 07:32:57 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A87EBD8.3090007@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>	<4A8555E7.3070200@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>	<4A86FE30.2070802@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF677@GVW0436EXB.americas.hpqcorp.net>
	<4A87EBD8.3090007@cs.oswego.edu>
Message-ID: <4A893FE9.4060208@cs.oswego.edu>

Doug Lea wrote:
> I still do agree this must be weakened.
> Doing so requires the introduction of wording about
> dependencies, 

As a place-holder for nailing down wording, I
split out the first-read rule with some
simple ad-hoc dependency wording that at least avoids
the full-fence implication.

As always,
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html

-Doug




From dl at cs.oswego.edu  Mon Aug 17 09:33:52 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 17 Aug 2009 09:33:52 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>
	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A895C40.5050207@cs.oswego.edu>

Boehm, Hans wrote:
> 
> At least some of us believe that it would be MUCH nicer if, in the possibly
> distant future, the implementation just prevented you from writing programs
> with data races, by statically or dynamically detecting them ahead of
> actually executing the race.  

Hoping not to let this meta-meta-level discussion get too
far off course...

As Hans knows, we agree on about 90% of this.
We might differ a bit on how to go about the
challenge of supporting enough constructions that
do not encounter errors (sometimes including those
that don't for non-obvious reasons)
so that people do not do the wrong thing because
they have no reasonable way to express the right thing.

There are more safe constructions than can be
readily expressed using the locks, volatiles, and finals
explicitly mentioned in the JMM. This is
one reason we added Atomics in j.u.c. -- mainly to
support CAS-based constructions. (Some might
recall the arguments against introducing Atomics
by those insisting that programmers should only express
CAS-like constructions using locks, that
super-smart JVMs would convert to CAS.)

And we need further support to fill in other known gaps.

All of these would be better with first-class language
and tool support to ensure that usages are correct.
Any language that does this is unlikely to be Java(tm),
but is likely to either an extension (like DPJ)
or a new one that runs on JVMs (like X10, Fortress, etc)
In which case they will still need something like Fences
to compile some of these constructs.

-Doug

From dl at cs.oswego.edu  Mon Aug 17 15:01:10 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 17 Aug 2009 15:01:10 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <20090817053603.GO6725@linux.vnet.ibm.com>
References: <4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>
	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<20090817053603.GO6725@linux.vnet.ibm.com>
Message-ID: <4A89A8F6.5030201@cs.oswego.edu>

Paul E. McKenney wrote:
> I believe that the best way for me
> to contribute is to show example code that I believe could benefit
> from weaker guarantees.  

Thanks! Very helpful.

> 1.	Split counters.  

Here's a shot at generalizing:
* You have a variable (like a counter) that is normally private
   to a thread, but sometimes needs inter-thread visibility.
* You don't want to penalize the normal private update.
* You do not need to achieve an accurate consensus/snapshot value
   when aggregating across per-thread values.

> 	In C++0x, one would use atomic variables with
> 	memory_order_relaxed.  I don't see the Java equivalent.

In C++, using atomic with memory_order_relaxed is sometimes in
part a permit for having a race without your program having
completely undefined semantics.
In Java, we ascribe (complicated and weak) meanings
even to programs with races, so on those grounds, you could just
declare the variables as normal variables and take what you get.
Without some further care though, you might not like what you get;
for example you may see all counters as zero.
Among the ways of avoiding this is to infrequently
use a fence in order to, conceptually, sometimes "promote"
the variable to being volatile-like.
Maybe just something of the form:
   if ((++stat.counter & 1023) == 0) Fences.orderAccesses(stat);

(There are better styles for doing this, but they don't fit
on one line :-). Plus a another single fence on reader side
before accumulation loop. You could use orderWrites here
instead, but it might require more care/thought, and if
they are infrequent enough, then it
won't matter much performance-wise. And of course you can use
some other infrequent trigger rather than count values.
(Also, while not guaranteed, having even a conditional
rare fence there is likely to force regular visible updates because
that would become the cheapest implementation.)

Not too relevant to Fences, but another line of attack here
is to let threads occasionally push values to a global atomic, as in:
   if ((++counter & 1023) == 0) {
      globalAtomicCounter.addAndget(counter);
      counter = 0;
   }
The CAS inside addAndGet will rarely contend, so is probably about as
cheap as a fence, and saves you from needing accumulation loop.
Or you can transfer counts only when the thread is known not to be doing
anything important, like before blocking. The ForkJoin framework does
this in a few places.

(And even less relevantly for present purposes, for the case where
aggregates must be accurate, there are the various techniques
described in the Herlihy & Shavit book.)

> 
> 2.	Publication with read-only subscription.  
> ...
> 
> 	The "[]" are supposed to map to the proposal.  If they do,
> 	I believe that this example is handled by the proposal.

Me too.

> 
> 3.	Publication with read-only subscription, but interspersed
> 	with reads from other variables.  
> ...
> 
> 	This appears to me to handled by the latest proposal, the one
> 	with three sub-bullets under "rf".  

(Or, more clearly, the revised version that separates out that third
case to clarify that this form only applies to dependent reads.)

> 	But if we add:
> 
> 		int offset = 0;
> 
> 	And suppose we then change subscribe_a() to read:
> 
> 	int subscribe_a(int *a)
> 	{
> 		int i;
> 		struct s1 *q; /* [ref] */
> 
> 		q = rcu_dereference(gp); /* [r] */
> 		/* [rf] */
> 		if (q == NULL)
> 			return 0;
> 		i = index;
> 		*a = q->a[i]; /* first read by t using ref. */
> 		*a += offset;
> 		return 1;
> 	}
> 
> 	The proposal would require a memory barrier to be emitted
> 	so as to force ordering of the fetch from "offset" on some
> 	architectures.  The weaker RCU semantics would require no
> 	such memory barrier, nor does C++0x memory_order_consume.

You are right that we don't have a version of fence/acquire
that ONLY governs dependent reads, regardless of whether they
are initial. So I think this example would need to use plain orderReads
fence but am not sure about details of "weaker RCU semantics" --
it might be that they are no weaker than minimal JMM guarantees
for plain (aka relaxed) variables?


> 
> 4.	But, strangely enough, RCU read-side critical sections
> 	(the stuff between an rcu_read_lock() and an rcu_read_unlock(),
> 	and the only place where an rcu_dereference() is legal, at
> 	least to a first order of approximation) can contain writes.
> 
> 	struct s1 {
> 		int a[2], b, c, accessed;
> 	};
> 
> 	struct s1 *gp = NULL;
> 	int index = 0;
> 
> 	void publish_new(a0, a1, b, c)
> 	{
> 		struct s1 *p;
> 
> 		p = malloc(sizeof(*p));
> 		if (p == NULL)
> 			handle_OOM();  /* Doesn't return. */
> 		p->a[0] = a0;
> 		p->a[1] = a1;
> 		p->b = b;
> 		p->c = c;
> 		p->accessed = 0;
> 		rcu_assign_pointer(gp, p);  /* similar to orderWrites() [wf] */
> 					    /*  followed by assignment */
> 					    /*  to gp [w] */
> 	}
> 
> 	int subscribe_a(int *a)
> 	{
> 		int i;
> 		struct s1 *q; /* [ref] */
> 
> 		q = rcu_dereference(gp); /* [r] */
> 		q->accessed = 1; /* not ordered by proposal!!! */
> 		/* [rf] */
> 		if (q == NULL)
> 			return 0;

Do you really mean to write through possibly null pointer?
Maybe you mean:
...
  		q = rcu_dereference(gp); /* [r] */
  		/* [rf] */
  		if (q == NULL)
  			return 0;
  		q->accessed = 1;

...

> 		i = index;
> 		*a = q->a[i]; /* first read by t using ref. */
> 		return 1;
> 	}
> 
> 	If I understand the proposal correctly, subscribe_a()'s
> 	write to ->accessed could be ordered before publish_new()'s
> 	write to ->accessed, leaving the value of this field equal
> 	to zero despite the fact that it has been accessed.

Hopefully my alternative form is what you actually want,
in which case "accessed = 0" is before wf and
"accessed = 1" after rf, so all is well.

All together, modulo the lack of pure-dependent-read
fence (which I don't think is supportable within JMM,
but maybe you don't need anyway?) I'm encouraged that
these look to work out, especially for RCU, which is
now a sort of canonical example of encapsulating
expert-level lightweight sync.

-Doug

From hans.boehm at hp.com  Mon Aug 17 17:43:02 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 17 Aug 2009 21:43:02 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A895C40.5050207@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>
	<4A895C40.5050207@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>

> From: Doug Lea [mailto:dl at cs.oswego.edu] 

> Hoping not to let this meta-meta-level discussion get too far 
> off course...
Right, but I still think this helps in making sure that we've explored all viable alternatives.

> ...
> All of these would be better with first-class language and 
> tool support to ensure that usages are correct.
> Any language that does this is unlikely to be Java(tm), but 
> is likely to either an extension (like DPJ) or a new one that 
> runs on JVMs (like X10, Fortress, etc) In which case they 
> will still need something like Fences to compile some of 
> these constructs.
> 
But this is where a lot of my concern comes from.  If we end up with more code that communicates between threads via races on ordinary variables, even if it uses sufficient fences to ensure correctness, we lose the ability to usefully detect races at the JVM level or below.  With correct uses of the proposed fence API, it becomes very difficult or impossible to distinguish accidental data races from those that were introduced by "optimizing" volatile accesses with explicit fences or the like.

I don't care whether we misidentify races introduced by untrusted code, but that were correctly handled by defensive use of fences to enforce sandboxing; having malicious code break sometime in the future is fine.  That use of fences seems to be both important and relatively uncontroversial.

On the other hand, I am bothered by intentional use of races for performance, if the program doesn't allow the implementation (or another human reader) to deduce which actions are intended for synchronization, i.e. are intended to be involved in a race.  And I think the current fence interface has that problem.

Perhaps this is fixable (modulo the small problem of actually specifying the semantics :-) ) by introducing fences along the lines of:

orderFieldRead(ref) orders the last preceding field access to ref with respect to subsequent (in program order) memory operations.  (If the last preceding field access is to a long or double, this is an error of some sort.)

orderFieldWrite(ref) orders prior memory accesses with respect to the next assignment to a field of ref.  (Similarly an error if the access is to a long or double.)

The above two are intended to match for acquire/release uses or volatile emulation.
Matching orderFieldWrite(ref) and orderFieldRead(ref) cause the corresponding access to synchronize.  Effectively this is sort of a way to add "volatile" after the fact, minus the StoreLoad ordering.  It's a blatant hack to get around the fact that we can't name fields, and a later hypothetical language extension coult conceivably add more reasonable syntax that no longer looks like a fence.

ensureWrites() orders prior memory accesses to fields x.f with respect to any subsequent writes of the corresponding reference x, but only with respect to dependent accesses (in the sense of mc()?) in another thread.  This is a no-op for properly synchronized code, and is intended only to prevent untrusted code from seeing incomplete objects.

We've effectively spit orderWrites into two, one for final field emulation, and one for everything else.  I'm not sure that we could avoid that anyway.  To me, they really don't have the same semantics.

We could leave orderAccesses as it is now, possibly removing the (meaningless?) argument, or make it more specific.  I think there's an inherent tension here between being able to identify all synchronization accesses (not an issue in C++) and supporting the kind of idioms that Paul likes to bring up.  I personally think that identifying the synchronization accesses is more important, at least given that Java doesn't have a legacy of fence-based code.

Does this make any sense?

Hans

From alarmnummer at gmail.com  Mon Aug 17 18:37:34 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 18 Aug 2009 00:37:34 +0200
Subject: [concurrency-interest] Fork/Join framework and object creation
Message-ID: <1466c1d60908171537u728959c8h97cc7f6e9db98534@mail.gmail.com>

Hi Guys,

I have a question about the influence of object creation when the
fork/join framework is used.

My experience so far is that object creation can influence performance
of code that is repeated very often (million times a second for
example).

For the fork/join framework you need to create at least a task object
and I assume that the deques also need to create additional objects to
store the task.

If object creation causes a slowdown, it essentially increases the
overhead. So you need to run larger tasks, to reduce the overhead.

I'm not criticizing the Fork/Join framework, but I have run into
performance problems caused by large numbers of objects being created
in (concurrent) code.
The performance improved when the number of objects being created was
reduced. So I would like to know how/if it influenced the design of
the fork/join
framework so I can reuse this knowledge on my own projects.

From dl at cs.oswego.edu  Mon Aug 17 19:14:05 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 17 Aug 2009 19:14:05 -0400
Subject: [concurrency-interest] Fork/Join framework and object creation
In-Reply-To: <1466c1d60908171537u728959c8h97cc7f6e9db98534@mail.gmail.com>
References: <1466c1d60908171537u728959c8h97cc7f6e9db98534@mail.gmail.com>
Message-ID: <4A89E43D.30003@cs.oswego.edu>

Peter Veentjer wrote:
> Hi Guys,
> 
> I have a question about the influence of object creation when the
> fork/join framework is used.
> 
> My experience so far is that object creation can influence performance
> of code that is repeated very often (million times a second for
> example).

FJ does generate tons of garbage, but is is "nice" garbage,
that tends to be very easy/cheap to collect by generational
collectors. My paper from 10(!) years ago 
(http://gee.cs.oswego.edu/dl/papers/fj.pdf)
included some graphs showing impact on one of first VMs
(pre-hotspot "EVM") using efficient generational GC.
GCs have gotten even better since then.

While, like most everything else, there are bad ways to use FJ
that will blow-up footprint and slow down GC, the most common
divide-and-conquer etc programs are extremely well-behaved,
so long as...

> 
> I'm not criticizing the Fork/Join framework, but I have run into
> performance problems caused by large numbers of objects being created
> in (concurrent) code.
> The performance improved when the number of objects being created was
> reduced. So I would like to know how/if it influenced the design of
> the fork/join
> framework so I can reuse this knowledge on my own projects.

You do still need to make reasonable choices about task
granularity. As it says in the Javadocs, as a rough guide,
more than 100 and less than 10,000 instructions per task
usually works well, but most people will need to experimentally
validate for different kinds of tasks.
In addition to the above paper, some slides at 
http://gee.cs.oswego.edu/dl/cpjslides/fj-mar09.pdf
include some granularity tradeoff curves.

-Doug


From hans.boehm at hp.com  Mon Aug 17 20:28:35 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 18 Aug 2009 00:28:35 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A86B966.50406@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86B966.50406@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D042577A1463CF@GVW0436EXB.americas.hpqcorp.net>

[I think I never replied to this.  I'll hold off on detailed suggestions for the other
fences for now, since those depend onwhat you think about my last message.]

> > - reachabilityFence [presentation issue]: I'm not 
> enthusiastic about 
> > the discussion following "Further continuing":
> > 
> > The next version of the example has a data race on the 
> > externalResourceArray access, and that should be avoided in the 
> > example.  It may be that real code would lock the array in 
> any case, 
> > and thus this issue would go away, but the example becomes 
> confusing.
> 
> OK; I had adapted this from a posted suggestion from 
> discussions last year, but since it doesn't help illustrate 
> usage of reachabilityFence anyway (but instead one way to 
> avoid it), it is better to kill it if it is confusing.
> 
> > 
> > I think we should make it clear that reachabilityFence is 
> typically a 
> > better solution than synchronized(this).
> 
> Thanks; I reworked a sentence to try to get this across a 
> little better.
> 
> -Doug
> 
Thanks.  That looks good.

It occurs to me that we probably also need an explicit statement that the reachabilityFence(ref) happens before (in the memory model sense) any finalizer invocations for ref, and before java.lang.ref erences are enqueued.

Hans

From hans.boehm at hp.com  Mon Aug 17 20:36:34 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 18 Aug 2009 00:36:34 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <20090818000052.GA24987@linux.vnet.ibm.com>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<20090818000052.GA24987@linux.vnet.ibm.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042577A1463DC@GVW0436EXB.americas.hpqcorp.net>

> From: Paul E. McKenney [mailto:paulmck at linux.vnet.ibm.com] 
> 
> On Fri, Aug 07, 2009 at 10:40:07PM +0000, Boehm, Hans wrote:
> > Doug -
> > 
> > [Copying Paul McKenney, who should look at the appended message 
> > first.]
> 
> [ . . . ]
> 
> > 3. The volatile emulation seems like it's at best an approximation.
> > If you implement it with the obvious fences, I think you 
> end up with a 
> > PowerPC implementation which is faster than what the PowerPC guys 
> > recommend.  This leads me to suspect it's not correct there.
> > It generally doesn't seem to guarantee the correct outcome for IRIW.
> > I'm not sure about more interesting examples.  It also depends on 
> > parts of the "more formal" definitions I don't yet understand.
> 
> Finally understood what Hans was getting at here...
> 
> PowerPC does indeed require hwsync between volatile loads if 
> you want IRIW to work.  The weaker/faster lwnsync instruction 
> does not guarantee that readers will see a consistent global 
> ordering for stores, due to its weaker cumulativity 
> guarantees.  Or, if you want a more hardware-centric 
> explanation, because lwsync does not flush the store buffer.
> 
> 							Thanx, Paul
> 
I think the right solution here would be to just describe this as an approximate volatile emulation.

On Itanium, it also won't get you a full emulation, I suspect.  For field stores that are implemented with an ordinary store, as opposed to an st.rel, I don't think there is any way to guarantee SC just by inserting fences.

Certainly C++ can't guarantee SC by adding fences between relaxed atomic operations.

Hans

From vijay at saraswat.org  Mon Aug 17 22:23:41 2009
From: vijay at saraswat.org (Vijay Saraswat)
Date: Mon, 17 Aug 2009 22:23:41 -0400
Subject: [concurrency-interest] CFP: First Workshop on Curricula in
 Concurrency and Parallelism OOPSLA 2009
Message-ID: <4A8A10AD.7080203@saraswat.org>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090817/da13e854/attachment-0001.html>

From karmazilla at gmail.com  Tue Aug 18 08:23:05 2009
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Tue, 18 Aug 2009 14:23:05 +0200
Subject: [concurrency-interest] Library with debugging tools for concurrent
	code?
Message-ID: <90622e530908180523i42c1afb6m12c416a8d28f83bb@mail.gmail.com>

Hi.

I'm finding that the current java.util.concurrent APIs a good for
writing unit tests that verify correct behavior of the multithreaded
code that I write - especially the CountDownLatch and Atomic* classes.
However, a couple of things are hard. One of those things is finding
the cause when things go wrong.

Here's a story: I was working on a unit test yesterday. I needed to
verify that some task scheduler executed the jobs submitted to it, and
I did this by submitting a job that would count up an AtomicInteger
and open a latch. The opened latch would let the junit thread proceed
to assert against the value of the AtomicInteger, expecting it to be
1. However, the count was *occasionally* 2. Looking through my
production code, I found no way for this to happen, yet it did. I
needed to figure out who was touching my counter, so I created a
ThreadMarkingAtomicInteger (bad name) that would record the thread,
new value and a stack trace, in a ConcurrentLinkedQueue on every
mutation. From this data, I could see that the mutations were coming
from threads from two different schedulers. The scheduler was
reconstructed prior to every unit test run, so the only logical
conclusion was, that I had a race between resetting it after one test
and reusing it in the next. This also explained why the bug only
surfaced in the second of two tests.

The TMAI was helpful, but a slight bother because my existing test
code was using AtomicIntegers which I could not extend because all of
the interesting methods are final.

The thing I wonder is, would it make sense with a library of parts
that can drop-in replace existing j.u.c parts and help with debugging?
This could be synchronizers that throws (or callbacks) when N parses
some threshold (no one must ever wait on this lock; no more than 3
threads are allowed to wait on this latch) or stuff like TMAI that
keep record of their use.


-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.

From dl at cs.oswego.edu  Tue Aug 18 08:49:21 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 18 Aug 2009 08:49:21 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>
	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>
	<4A895C40.5050207@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A8AA351.4020205@cs.oswego.edu>

Boehm, Hans wrote:
> 
> Perhaps this is fixable (modulo the small problem of actually specifying the
> semantics :-) ) by introducing fences along the lines of:

Most of this is very similar to the initial preliminary versions
of Fences I had put together a few years ago. (In fact,
the recently excised discussions of "scopes" were a
remnant of this.) I agree that it is closer to what we want.
After having taken a few stabs at specifying some
of the methods expressed in this style wrt JMM,
I'm not at all sure there is a way to do so
that doesn't introduce some mismatches with JLS sec 17.4.
But I'm happy to work with others to give it another shot:

> 
> orderFieldRead(ref) orders the last preceding field access to ref with
> respect to subsequent (in program order) memory operations.  
> 
> orderFieldWrite(ref) orders prior memory accesses with respect to the next
> assignment to a field of ref.  

The names for these methods could just be the current
orderReads and orderWrites. I hate to throw away
names that took so long for people to agree to. On the
other hand, having the term "Field" in the name might
be helpful.

> The above two are intended to match for acquire/release uses or volatile
> emulation. Matching orderFieldWrite(ref) and orderFieldRead(ref) cause the
> corresponding access to synchronize.  Effectively this is sort of a way to
> add "volatile" after the fact, minus the StoreLoad ordering.  It's a blatant
> hack to get around the fact that we can't name fields, and a later
> hypothetical language extension coult conceivably add more reasonable syntax
> that no longer looks like a fence.

My previous attempt to deal with this was to also define an
overloaded form allowing optional pinpointing of the field:
   orderReads(Object ref, java.lang.reflect.Field field);
I later omitted this because calling it would
usually require creating a Field object, that
is likely to be ignored by underlying implementations anyway.
And this doesn't work for arrays, although you could add
further even more awkward forms to deal with them.

Even still, it is worth contemplating reviving this
for people/tools who want extra precision, and who
can keep around static Field objects to be used
as arguments. Some guidance would need to be added
to tell people NOT to use this form unless they can
avoid the overhead.

> 
> ensureWrites() orders prior memory accesses to fields x.f with respect to any
(Here, it would be:  ensureWrites(x))
> subsequent writes of the corresponding reference x, but only with respect to
> dependent accesses (in the sense of mc()?) in another thread.  This is a
> no-op for properly synchronized code, and is intended only to prevent
> untrusted code from seeing incomplete objects.
> 
> We've effectively spit orderWrites into two, one for final field emulation,
> and one for everything else.  I'm not sure that we could avoid that anyway.
> To me, they really don't have the same semantics.

This does seem like the best dimension across which to split
them. Coming up with a name that helps more than hurts
seems challenging. Perhaps using the overly narrow name
orderInitialWrites(ref) would be OK -- it would
satisfy requests to highlight this case on usability grounds.
While the spec would just say what happens here, the name
might help people use it only in its primary intended use case.

Doing this also raises the question about whether to spec what
happens when ensureWrites/orderInitialWrites is matched
with orderFieldRead/orderReads, or when
orderFieldWrites/orderWrites isn't matched. I suppose these
cases could be intentionally unspec'ed. But I'm not
positive that the legitimate usage intersection of the
two is empty. On the other hand, as Bill noted,
even if not empty, there is not much of a penalty for people
to invoke both in any cases that arise.

> We could leave orderAccesses as it is now, possibly removing the
> (meaningless?) argument, or make it more specific.  I think there's an
> inherent tension here between being able to identify all synchronization
> accesses (not an issue in C++) and supporting the kind of idioms that Paul
> likes to bring up.  I personally think that identifying the synchronization
> accesses is more important, at least given that Java doesn't have a legacy of
> fence-based code.

The need for an argument also depends in part on whether
we strictly guarantee full volatile emulability.
More on that separately.

-Doug



From tim at peierls.net  Tue Aug 18 10:11:29 2009
From: tim at peierls.net (Tim Peierls)
Date: Tue, 18 Aug 2009 10:11:29 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A8AA351.4020205@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>
	<4A895C40.5050207@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
	<4A8AA351.4020205@cs.oswego.edu>
Message-ID: <63b4e4050908180711r22fc47bfh326d0ac439a0ef1f@mail.gmail.com>

On Tue, Aug 18, 2009 at 8:49 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> The names for these methods could just be the current
> orderReads and orderWrites. I hate to throw away
> names that took so long for people to agree to. On the
> other hand, having the term "Field" in the name might
> be helpful.


Um... If there was a point where everyone agreed to the
"orderReads/Writes/Accesses" names, I missed it. My silence was not a sign
of enthusiasm. I'm really not wild about "...ref...; orderReads(ref);
...ref..." -- the directionality and the fact that this is an infix
operation are not clear. The object of the verb "order" is the category of
things being put in order; you need prepositions to describe the ordering
itself.  "orderPriorReadsBeforeSubsequentReadsOf(ref)" is too verbose and I
suppose not even accurate, but it at least conveys the directionality and
infixedness.

But what was wrong with "fence" in the method name? It's a useful metaphor,
obviously, or it wouldn't have been enshrined in hardware. And "...ref...;
readFence(ref); ...ref..." just looks better to me. A stake in the ground,
as it were.

If there are people who are discouraged from using this API because the
names are unfamiliar, well ... great. If there are folks who think this is
an imprecise use of the term "fence", I would ask them to take one for the
team and think about the metaphor, not the specific application.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090818/07902364/attachment.html>

From dl at cs.oswego.edu  Tue Aug 18 10:25:54 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 18 Aug 2009 10:25:54 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A1463DC@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<20090818000052.GA24987@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A1463DC@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A8AB9F2.7000002@cs.oswego.edu>

Hans wrote:
>>> The volatile emulation seems like it's at best an approximation.

I had then changed orderAccesses spec, without changing
others much, as one step in removing the "approximation" aspect.
But that was probably not the best tactic...

Paul:
>> PowerPC does indeed require hwsync between volatile loads if you want IRIW 
>> to work.
Hans:
> On Itanium, it [using ld.acq/st.rel] also won't get you a full emulation, I
> suspect.

One approach is to instead say that the
only way to achieve accurate volatile emulation
is to use ONLY orderAccesses for both loads
and stores. We would then be stuck explaining
why people should almost always use orderReads
anyway for loads even in "volatile emulation".

This probably is the right way to go, but I will
need plenty of help writing prose that provides
the right usage guidance -- neither scaring people
into needlessly using slow full-fence modes, nor
ignoring the impact (especially wrt analysis tools)
of not actually achieving SC.

Here's some background for those not into
memory-model esoterica.

There are some subtle mismatches between some hardware
specs and the requirements of the JMM for volatiles
(and less so locks) that guarantee true sequential
consistency (SC). The main mismatches have become very
well-known inside memory-model circles, and have been
discussed to death; in part because they only show
up in usages that rarely arise in practice.
The most famous one is IRIW (below), in which independent reading
threads might see the writes of independent writing threads
in a different order, which turns out not to be allowed
in SC. (You might think that there must be some nice variant
of SC that doesn't impose this constraint,  but none of us
have found one that is anywhere near "nice".)

IRIW:
Initial: volatile x, y = 0. Can threads read values in parens?
T1          T2         T3                T4
x = 1      y = 1
                       r1 = x (1)    r3 = y (1)
                       r2 = y (0)    r4 = x (0)

On platforms in which IRIW violations are possible, processors
must sometimes use heavier kinds of fences on volatile loads
than they would otherwise need, and compilers must try hard
to minimize them by detecting special cases.
If nothing else, this messes up the otherwise desirable
mental model people have that volatile loads are always relatively
cheap (instead, on some platforms they are only sometimes so).

When you apply this to explicit Fence methods as opposed to
"volatile", then, arguably, users should have some say in this
matter: People can guarantee lower-cost loads if they are willing
to tolerate lack of full SC. As it turns out, most variants of
the more formal parts of Fences specs over the past few
weeks have (probably!) not even implied full SC. (The "probably"
here stems from the continuing problems of not getting these
quite right on other grounds as well.)

...

Somehow, the essence of the above would need to be transformed
into some simple usable guidance, probably mainly surrounding the
"emulating volatile" section. All ideas appreciated.

-Doug

From sanders at cise.ufl.edu  Tue Aug 18 13:52:23 2009
From: sanders at cise.ufl.edu (Beverly Sanders)
Date: Tue, 18 Aug 2009 13:52:23 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <mailman.3.1250611200.209.concurrency-interest@cs.oswego.edu>
References: <mailman.3.1250611200.209.concurrency-interest@cs.oswego.edu>
Message-ID: <783814efb4b590ce2cc1503578c4eed2.squirrel@webmail.cise.ufl.edu>


Greetings,
Here are a few comments and questions about the Fences specification and
documentation.  I haven?t had a chance to carefully read and absorb all
the discussion and I?m not an expert on the low level implementation
details so I am basing these comments on what I understand from the
reading the Fences API docs--I hope this perspective is useful.

===

In the ordering semantics, it isn't clear why the 4th bullet of 3 requires
r to be the first read by some thread that sees ref.

Example (using the WidgetHolder example)

Initially WidgetHolder x = null

T1
x = newWidgetHolder(something);

T2
WidgetHolder y=x;
Widget z = x.widget;   //could see widget field as null if x is reread
                       //but not otherwise

This would seem to admit different behavior depending on the details of
code generation.  Is this what is intended?

===

In the ordering semantics section, as written, the arguments of
orderAccesses are arbitrary.  This doesn?t correspond to the informal
documentation, which says that ?with respect to the given reference,
accesses (reads and writes) prior to 
?

===

In the informal docs, it isn't obvious exactly what the meaning of "with
respect to a given reference" is or what is being constrained.   Better to
say, for example, for orderWrites, something like "Ensures that reads or
writes of fields of the object referred to by ref occur before subsequent
writes to any field of that object."   Also, for orderWrites, shouldn?t
there be something about the matching orderReads?

===

I'm a little surprised that wf and rf are made part of the synchronization
order when they are orderWrites and orderReads invocations (item 2 in the
ordering semantics section).  The synchronization order is a total order
but if I understand correctly orderWrites and orderReads are only ordered
when they have a common argument and part of the reason for introducing
them is to allow that partial order.   The consequences of embedding them
in the synchronization order aren't at all obvious.

===

In the emulating volatile example,  couldn?t newValue in the setData
method be null?   This makes the effects of the orderWrites method
undefined.    (?Undefined? seems pretty loose, there must be some
reasonable constraints)

===

In the current JMM,  to prove that a program is sequentially consistent,
you only need to consider the SC executions and prove those are data-race
free.    What can you say about programs using the Fence API if you only
look at SC executions?   This is an important question for us as we have
developed a tool to precisely detect data races based on model checking
and, of course, model checking intrinsically relies on the assumption of
SC.   We could fairly easily (I think) extend it to maintain the
happens-before edges introduced by the Fences methods  (and with a little
more work) determine if a detected data race has been properly bracketed
by fence methods provided that looking at SC executions is sufficient.

Thanks,
Beverly


From hans.boehm at hp.com  Tue Aug 18 14:28:12 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 18 Aug 2009 18:28:12 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <20090818172801.GB6766@linux.vnet.ibm.com>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<20090818000052.GA24987@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A1463DC@GVW0436EXB.americas.hpqcorp.net>
	<4A8AB9F2.7000002@cs.oswego.edu>
	<20090818172801.GB6766@linux.vnet.ibm.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042577A146936@GVW0436EXB.americas.hpqcorp.net>

> From: Paul E. McKenney [mailto:paulmck at linux.vnet.ibm.com] 
> 
> On Tue, Aug 18, 2009 at 10:25:54AM -0400, Doug Lea wrote:
> > Hans wrote:
> >>>> The volatile emulation seems like it's at best an approximation.
> >
> > I had then changed orderAccesses spec, without changing 
> others much, 
> > as one step in removing the "approximation" aspect.
> > But that was probably not the best tactic...
> >
> > Paul:
> >>> PowerPC does indeed require hwsync between volatile loads if you 
> >>> want IRIW to work.
> > Hans:
> >> On Itanium, it [using ld.acq/st.rel] also won't get you a full 
> >> emulation, I suspect.
> >
> > One approach is to instead say that the only way to achieve 
> accurate 
> > volatile emulation is to use ONLY orderAccesses for both loads and 
> > stores. We would then be stuck explaining why people should almost 
> > always use orderReads anyway for loads even in "volatile emulation".
> >
[Anyone who hasn't been following the more esoteric memory model discussions may want to look at Doug's prior message decribing the IRIW test case before proceeding.  Thanks, Doug.]

I don't like this approach much at all:

- I'd have to check, but I don't believe it works on Itanium.  I think you only get a total store order with st.rel.  I don't think there is any way to add it after the fact with fences.
- I think any future architecture that tries to reflect programming language memory models in the hardware ISA is likely to run into issues similar to Itanium.  If you distinguish data and synchronization (volatile) accesses at the instruction set level, then I don't know of a good reason to enforce total store ordering for data accesses.  Nor is it obvious to me that fences should have any bearing on that.
- This approach doesn't make much sense to the programmer.  The fact that ordering more than loads for the IRIW example is counterintuitive, and really only makes sense if you look specifically at the PowerPC cumulativity rules.
- My recollection from the C++ discussion is that there are a bunch of other cases in which reasonable fence specifications will fail to imply sequential consistency, even if you put fences between every pair of instructions.  I know of one C++ open issue on this (http://www.open-std.org/jtc1/sc22/wg21/docs/lwg-active.html#926).  Others have been posted on newsgroups, and we decided that they were too esoteric to worry about, and trying to fix them was probably at odds with implementability on some platform (a concern which is also holding up the above issue).

I still think the right answer is that if you want SC, use volatile.  You can use fences to get close, but things will be much messier.  I think the only other practical way to really get SC would be a call that says "make the next access to ref volatile", but that strikes me as very intrusive into JVMs, and also quite ugly.

Hans

From dl at cs.oswego.edu  Tue Aug 18 15:46:34 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 18 Aug 2009 15:46:34 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <783814efb4b590ce2cc1503578c4eed2.squirrel@webmail.cise.ufl.edu>
References: <mailman.3.1250611200.209.concurrency-interest@cs.oswego.edu>
	<783814efb4b590ce2cc1503578c4eed2.squirrel@webmail.cise.ufl.edu>
Message-ID: <4A8B051A.8000602@cs.oswego.edu>

Beverly Sanders wrote:
> Greetings,
> Here are a few comments and questions about the Fences specification and
> documentation.  --I hope this perspective is useful.
> 

Yes; thanks very much! I'll try to improve/fix some of the
confusing wordings; although for a couple it might need to
wait until the ideas are less confused. In the mean time...

> ===
> 
> In the ordering semantics, it isn't clear why the 4th bullet of 3 requires
> r to be the first read by some thread that sees ref.
> 
> Example (using the WidgetHolder example)
> 
> Initially WidgetHolder x = null
> 
> T1
> x = newWidgetHolder(something);
> 
> T2
> WidgetHolder y=x;
> Widget z = x.widget;   //could see widget field as null if x is reread
>                        //but not otherwise
> 
> This would seem to admit different behavior depending on the details of
> code generation.  Is this what is intended?
> 

The idea is that the first read of reference x by a thread must
act as an "acquiring" read -- the thread is not allowed to
have any idea about the value of any of x's fields.
(It might help to think about it as one aspect of
"no out-of-thin-air" properties.) So using orderWrites
on the release side intrinsically gives you the benefit
that when another thread sees the ref, it will see
written field values and not speculate (and cannot reuse
previous values because there are none). This is all
a slightly different perspective on ideas behind
the JLS 17.5 final field rules.
After the first-write/first-read, you no longer
get any such guarantees unless you properly synchronize.

If you think they would help, please feel free to
suggest ways to get these kinds of
intuitions across in the intros or sample usages.

> 
> In the ordering semantics section, as written, the arguments of
> orderAccesses are arbitrary.  This doesn?t correspond to the informal
> documentation, which says that ?with respect to the given reference,
> accesses (reads and writes) prior to ??

Sorry that wordings have not kept pace with underlying specs.
The main underlying issue is that the ref arguments play
different roles (sometimes changing from one day to the next!) --
as scopes for effects and as tags for matching fences.
I hope we get our story straight on this soon.

> 
> I'm a little surprised that wf and rf are made part of the synchronization
> order when they are orderWrites and orderReads invocations (item 2 in the
> ordering semantics section).  The synchronization order is a total order
> but if I understand correctly orderWrites and orderReads are only ordered
> when they have a common argument and part of the reason for introducing
> them is to allow that partial order.   The consequences of embedding them
> in the synchronization order aren't at all obvious.

The technical ploy currently used here is that they are only
conditionally part of the synchronization order, if they match
up as required.

> 
> In the emulating volatile example,  couldn?t newValue in the setData
> method be null?   This makes the effects of the orderWrites method
> undefined.    (?Undefined? seems pretty loose, there must be some
> reasonable constraints)

Yes, the effect is now listed as a no-op with null argument.

> 
> ===
> 
> In the current JMM,  to prove that a program is sequentially consistent,
> you only need to consider the SC executions and prove those are data-race
> free.    What can you say about programs using the Fence API if you only
> look at SC executions?   This is an important question for us as we have
> developed a tool to precisely detect data races based on model checking
> and, of course, model checking intrinsically relies on the assumption of
> SC.   We could fairly easily (I think) extend it to maintain the
> happens-before edges introduced by the Fences methods  (and with a little
> more work) determine if a detected data race has been properly bracketed
> by fence methods provided that looking at SC executions is sufficient.

I'm afraid you will need to wade through the recent list traffic
on this. Whether and how fences can preserve pure SC is sadly a messy
issue; your input about it would be very welcome!

-Doug





From dl at cs.oswego.edu  Tue Aug 18 16:09:33 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 18 Aug 2009 16:09:33 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A146936@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<20090818000052.GA24987@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A1463DC@GVW0436EXB.americas.hpqcorp.net>
	<4A8AB9F2.7000002@cs.oswego.edu>
	<20090818172801.GB6766@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A146936@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A8B0A7D.9020901@cs.oswego.edu>

Boehm, Hans wrote:
>>> One approach is to instead say that the only way to achieve accurate 
>>> volatile emulation is to use ONLY orderAccesses for both loads and 
>>> stores.
> 
> I don't believe it works ...

Sigh. Sometimes I think some of those hardware folks must
scheme together to find new ways to annoy us.

> I still think the right answer is that if you want SC, use volatile.  You can
>  use fences to get close

It must be the right answer because it is the only answer.
This still leaves open the question in my previous
post of how to explain "close". Suggestions (especially
from non-memory-model-experts) still welcome.

-Doug








From dl at cs.oswego.edu  Tue Aug 18 16:31:17 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 18 Aug 2009 16:31:17 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <20090818012629.GR6760@linux.vnet.ibm.com>
References: <4A81559D.5000708@cs.oswego.edu>
	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<20090817053603.GO6725@linux.vnet.ibm.com>
	<4A89A8F6.5030201@cs.oswego.edu>
	<20090818012629.GR6760@linux.vnet.ibm.com>
Message-ID: <4A8B0F95.3000605@cs.oswego.edu>

Paul E. McKenney wrote:
> Hmmm...  The current document says:
> 
> 	"d, a read using r to access a field (or if array, an element)
> 	of the referenced object."
> 
> But "q->accessed = 1" is a write, not a read.  So, what am I missing?
> 

An accurate spec :-) For the time being it mentions only reads.
I think we are back to trying to define/spec most methods
primarily in terms of dependencies, so stay tuned...

-Doug

From hans.boehm at hp.com  Tue Aug 18 17:04:41 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 18 Aug 2009 21:04:41 +0000
Subject: [concurrency-interest] Fences
In-Reply-To: <4A8B0A7D.9020901@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<20090818000052.GA24987@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A1463DC@GVW0436EXB.americas.hpqcorp.net>
	<4A8AB9F2.7000002@cs.oswego.edu>
	<20090818172801.GB6766@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A146936@GVW0436EXB.americas.hpqcorp.net>
	<4A8B0A7D.9020901@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D042577A146AE9@GVW0436EXB.americas.hpqcorp.net>

 

> -----Original Message-----
> From: Doug Lea [mailto:dl at cs.oswego.edu] 
> Sent: Tuesday, August 18, 2009 1:10 PM
> To: Boehm, Hans
> Cc: paulmck at linux.vnet.ibm.com; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Fences
> 
> Boehm, Hans wrote:
> >>> One approach is to instead say that the only way to 
> achieve accurate 
> >>> volatile emulation is to use ONLY orderAccesses for both 
> loads and 
> >>> stores.
> > 
> > I don't believe it works ...
> 
> Sigh. Sometimes I think some of those hardware folks must 
> scheme together to find new ways to annoy us.
Sometimes.  But in this case I'm unconvinced they're doing the wrong thing.  In my view, fences are a reasonably natural way to control the order in which actions by one thread become visible.  They're an extremely weird way to impose something like a total order on stores by different threads.  Thus it seems to me that means they're just not the right way to enforce sequential consistency at either the source or ISA level.  Several of our problems seem to stem from attempts to do that.
> 
> > I still think the right answer is that if you want SC, use 
> volatile.  
> > You can  use fences to get close
> 
> It must be the right answer because it is the only answer.
> This still leaves open the question in my previous post of 
> how to explain "close". Suggestions (especially from 
> non-memory-model-experts) still welcome.
> 
But, do we have to? :-)

Actually I'm not really joking.  I think we do have to come up with a reasonably precise description of what fences so.  This will not imply sequential consistency even if you put all possible fences between every pair of memory accesses.  And I suspect the precise characterization of how it differs from SC will be complicated, at least if you try to be precise.  I think the answer is just that if you use fences, life will be complicated.  It always has been, except that a lot of authors of fence-based code don't look that closely and just hope that whatever rough intuition they have holds up.  (The fact that a bunch of papers in the research literature imply otherwise doesn't change things.  I also think they didn't look closely enough.)  I don't think we have much of a chance to change this fundamentally.

I would basically change the title "emulating volatile access" to "approximating volatile access" and add a vague disclaimer that this cannot be used to emulate full volatile semantics because there is no way to guarantee that operations performed by different threads are observed in the same order by all observer threads.

Hans

From jeremy.manson at gmail.com  Tue Aug 18 18:21:49 2009
From: jeremy.manson at gmail.com (Jeremy Manson)
Date: Tue, 18 Aug 2009 15:21:49 -0700
Subject: [concurrency-interest] Fences
In-Reply-To: <4A8B0A7D.9020901@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<20090818000052.GA24987@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A1463DC@GVW0436EXB.americas.hpqcorp.net>
	<4A8AB9F2.7000002@cs.oswego.edu>
	<20090818172801.GB6766@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A146936@GVW0436EXB.americas.hpqcorp.net>
	<4A8B0A7D.9020901@cs.oswego.edu>
Message-ID: <1631da7d0908181521u13332b9cgd7e5656609402d12@mail.gmail.com>

One observation to make is that happens-before does not imply SC.  For example:

For all unique references obj, there is a total order over
Fences.orderReads(obj) and Fences.orderWrites(obj).  There is a
happens-before relationship between Fences.orderWrites(obj) and
subsequent invocations of Fences.orderReads(obj).  The total order for
obj is independent of the total order for obj2, where obj != obj2.

This is a volatile formulation that gives you IRIW without giving you
SC.  I'd have to think harder before I believed that it was correct
for this use case, though.

Jeremy

On Tue, Aug 18, 2009 at 1:09 PM, Doug Lea<dl at cs.oswego.edu> wrote:
> Boehm, Hans wrote:
>>>>
>>>> One approach is to instead say that the only way to achieve accurate
>>>> volatile emulation is to use ONLY orderAccesses for both loads and stores.
>>
>> I don't believe it works ...
>
> Sigh. Sometimes I think some of those hardware folks must
> scheme together to find new ways to annoy us.
>
>> I still think the right answer is that if you want SC, use volatile. ?You
>> can
>> ?use fences to get close
>
> It must be the right answer because it is the only answer.
> This still leaves open the question in my previous
> post of how to explain "close". Suggestions (especially
> from non-memory-model-experts) still welcome.
>
> -Doug
>
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From dl at cs.oswego.edu  Wed Aug 19 08:45:06 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 19 Aug 2009 08:45:06 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A8AA351.4020205@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>	<4A8555E7.3070200@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>	<4A86DC26.1070905@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>	<4A895C40.5050207@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
	<4A8AA351.4020205@cs.oswego.edu>
Message-ID: <4A8BF3D2.9020907@cs.oswego.edu>

Doug Lea wrote:
> After having taken a few stabs at specifying some
> of the methods expressed in this style wrt JMM,
> I'm not at all sure there is a way to do so
> that doesn't introduce some mismatches with JLS sec 17.4.

I'm still not sure, but will move the JMM-ese
spec discussion off-list for a while.

In the mean time, I committed results of various
other discussions --
(http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html)

Also, are there any reactions to Tim's request to
reconsider using "Fence" versus "Order"-based methods names?
Those of us overly familiar with memory model
minutiae are among the worst people to decide on names,
so I'd rather choose by consensus.
Especially if you think you would use them for
some particular purpose you have in mind, it
might be helpful for you to sketch out what
the usages would look like under various names
and see if any of them strike you as particularly
good or bad.


-Doug


From dl at cs.oswego.edu  Wed Aug 19 09:03:29 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 19 Aug 2009 09:03:29 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A146AE9@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<20090818000052.GA24987@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A1463DC@GVW0436EXB.americas.hpqcorp.net>
	<4A8AB9F2.7000002@cs.oswego.edu>
	<20090818172801.GB6766@linux.vnet.ibm.com>
	<238A96A773B3934685A7269CC8A8D042577A146936@GVW0436EXB.americas.hpqcorp.net>
	<4A8B0A7D.9020901@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A146AE9@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A8BF821.5030107@cs.oswego.edu>

Boehm, Hans wrote:
>>>>> One approach is to instead say that the only way to achieve accurate
>>>>> volatile emulation is to use ONLY orderAccesses for both loads and
>>>>> stores.
>>> I don't believe it works ...

Not that it matters in terms of what we should do here,
(i.e., provide non-SC disclaimers, even for the extreme case of
full fences), but this still strikes
me as overly pessimistic. In principle, any platform capable of
implementing SC volatiles could surely do so for a
program in which each access were separated by orderAccesses.
Pseudo-proof: Even if the platform could only do this by
merging fence properties with access instructions, it could
simply to this for each such instruction. Of course
it would be silly to force unusual processors to do this
for all possible accesses just in case the program contains
any orderAccesses calls.

JVMs that use internal logical-fences must already
sometimes perform various forms of fence+access=>insn
matchings during code generation (for example .acq/.rel
on Itanium, and "locked" mode on x86).
So as a practical matter, implementations of orderAccesses
are very unlikely to interestingly differ from those for
volatile. But still I agree that there is no reason to
promise that they are equivalent.

-Doug


From normelton at gmail.com  Wed Aug 19 17:38:15 2009
From: normelton at gmail.com (Norman Elton)
Date: Wed, 19 Aug 2009 17:38:15 -0400
Subject: [concurrency-interest] Handling Exceptions in ThreadPoolExecutor
Message-ID: <6b3a7f010908191438n7fdb2531k3b1f33eb4d0d0df0@mail.gmail.com>

I've got a ThreadPoolExecutor that overrides beforeExecute &
afterExecute. After a job is run, my executor checks to see if an
exception was raised. If so, the database transaction is rolled back
and the job is re-queued to run again. If it fails three times, it is
discarded.

Problem is... if an exception reaches afterExecute, the thread dies!
This incurs some overhead and "thread churn" that I'd like to avoid.

Is it possible setup a ThreadPoolExecutor to allow the thread to
continue to live after an exception has been caught? Or some other way
to re-queue failed jobs?

Thanks,

Norman

From martinrb at google.com  Wed Aug 19 18:09:34 2009
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 19 Aug 2009 15:09:34 -0700
Subject: [concurrency-interest] Handling Exceptions in ThreadPoolExecutor
In-Reply-To: <6b3a7f010908191438n7fdb2531k3b1f33eb4d0d0df0@mail.gmail.com>
References: <6b3a7f010908191438n7fdb2531k3b1f33eb4d0d0df0@mail.gmail.com>
Message-ID: <1ccfd1c10908191509w406bbb55ndf698d573c0bfea2@mail.gmail.com>

TPE threads are not designed to recover from failing tasks.
But the TPE pool itself is (by spawning new threads).
afterExecute can be used for logging/monitoring,
but should probably not be used for execution control.

Another way is to wrap all tasks so that they simply
do not throw, perhaps on tasks submission,
as ScheduledThreadPoolExecutor does.

Martin

On Wed, Aug 19, 2009 at 14:38, Norman Elton <normelton at gmail.com> wrote:

> I've got a ThreadPoolExecutor that overrides beforeExecute &
> afterExecute. After a job is run, my executor checks to see if an
> exception was raised. If so, the database transaction is rolled back
> and the job is re-queued to run again. If it fails three times, it is
> discarded.
>
> Problem is... if an exception reaches afterExecute, the thread dies!
> This incurs some overhead and "thread churn" that I'd like to avoid.
>
> Is it possible setup a ThreadPoolExecutor to allow the thread to
> continue to live after an exception has been caught? Or some other way
> to re-queue failed jobs?
>
> Thanks,
>
> Norman
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090819/72cdc250/attachment.html>

From alarmnummer at gmail.com  Thu Aug 20 03:57:00 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 20 Aug 2009 09:57:00 +0200
Subject: [concurrency-interest] Handling Exceptions in ThreadPoolExecutor
In-Reply-To: <1ccfd1c10908191509w406bbb55ndf698d573c0bfea2@mail.gmail.com>
References: <6b3a7f010908191438n7fdb2531k3b1f33eb4d0d0df0@mail.gmail.com>
	<1ccfd1c10908191509w406bbb55ndf698d573c0bfea2@mail.gmail.com>
Message-ID: <1466c1d60908200057x7c01e80u15950b45b23c5500@mail.gmail.com>

I would also go for the wrapping solution.

When you are using some sort of transaction management aspect, it is
normal that the transaction is aborted when an exception is thrown:

So something like this:

Transaction t = connection.startTransaction();
boolean success = false;
try{
    ..do logic
   success = true;
}finally{
    if(success){
          t.commit();
    }else{
          t.abort();
    }
}

On Thu, Aug 20, 2009 at 12:09 AM, Martin Buchholz<martinrb at google.com> wrote:
> TPE threads are not designed to recover from failing tasks.
> But the TPE pool itself is (by spawning new threads).
> afterExecute can be used for logging/monitoring,
> but should probably not be used for execution control.
>
> Another way is to wrap all tasks so that they simply
> do not throw, perhaps on tasks submission,
> as ScheduledThreadPoolExecutor does.
>
> Martin
>
> On Wed, Aug 19, 2009 at 14:38, Norman Elton <normelton at gmail.com> wrote:
>>
>> I've got a ThreadPoolExecutor that overrides beforeExecute &
>> afterExecute. After a job is run, my executor checks to see if an
>> exception was raised. If so, the database transaction is rolled back
>> and the job is re-queued to run again. If it fails three times, it is
>> discarded.
>>
>> Problem is... if an exception reaches afterExecute, the thread dies!
>> This incurs some overhead and "thread churn" that I'd like to avoid.
>>
>> Is it possible setup a ThreadPoolExecutor to allow the thread to
>> continue to live after an exception has been caught? Or some other way
>> to re-queue failed jobs?
>>
>> Thanks,
>>
>> Norman
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From alarmnummer at gmail.com  Thu Aug 20 03:59:55 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 20 Aug 2009 09:59:55 +0200
Subject: [concurrency-interest] Handling Exceptions in ThreadPoolExecutor
In-Reply-To: <1466c1d60908200057x7c01e80u15950b45b23c5500@mail.gmail.com>
References: <6b3a7f010908191438n7fdb2531k3b1f33eb4d0d0df0@mail.gmail.com>
	<1ccfd1c10908191509w406bbb55ndf698d573c0bfea2@mail.gmail.com>
	<1466c1d60908200057x7c01e80u15950b45b23c5500@mail.gmail.com>
Message-ID: <1466c1d60908200059u39daa454k15365c5be8830af0@mail.gmail.com>

Another big advantage of doing the commit/abort directly, is they you
are less dependant on the Executor implementation.

On Thu, Aug 20, 2009 at 9:57 AM, Peter Veentjer<alarmnummer at gmail.com> wrote:
> I would also go for the wrapping solution.
>
> When you are using some sort of transaction management aspect, it is
> normal that the transaction is aborted when an exception is thrown:
>
> So something like this:
>
> Transaction t = connection.startTransaction();
> boolean success = false;
> try{
> ? ?..do logic
> ? success = true;
> }finally{
> ? ?if(success){
> ? ? ? ? ?t.commit();
> ? ?}else{
> ? ? ? ? ?t.abort();
> ? ?}
> }
>
> On Thu, Aug 20, 2009 at 12:09 AM, Martin Buchholz<martinrb at google.com> wrote:
>> TPE threads are not designed to recover from failing tasks.
>> But the TPE pool itself is (by spawning new threads).
>> afterExecute can be used for logging/monitoring,
>> but should probably not be used for execution control.
>>
>> Another way is to wrap all tasks so that they simply
>> do not throw, perhaps on tasks submission,
>> as ScheduledThreadPoolExecutor does.
>>
>> Martin
>>
>> On Wed, Aug 19, 2009 at 14:38, Norman Elton <normelton at gmail.com> wrote:
>>>
>>> I've got a ThreadPoolExecutor that overrides beforeExecute &
>>> afterExecute. After a job is run, my executor checks to see if an
>>> exception was raised. If so, the database transaction is rolled back
>>> and the job is re-queued to run again. If it fails three times, it is
>>> discarded.
>>>
>>> Problem is... if an exception reaches afterExecute, the thread dies!
>>> This incurs some overhead and "thread churn" that I'd like to avoid.
>>>
>>> Is it possible setup a ThreadPoolExecutor to allow the thread to
>>> continue to live after an exception has been caught? Or some other way
>>> to re-queue failed jobs?
>>>
>>> Thanks,
>>>
>>> Norman
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


From ashpublic at mac.com  Thu Aug 20 04:52:51 2009
From: ashpublic at mac.com (Ashley Williams)
Date: Thu, 20 Aug 2009 09:52:51 +0100
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <4A8351F8.3060709@smokejumperit.com>
References: <NFBBKALFDCPFIDBNKAPCEEAIIDAA.davidcholmes@aapt.net.au>
	<4A8351F8.3060709@smokejumperit.com>
Message-ID: <61AC43D3-0308-4F66-B5A2-D2CD58EED512@mac.com>

I'm starting to think this is the best approach too. The kicker for me  
is that you can't assign threads to specific cores at least not with  
standard java - but maybe the jvm is smart enough to notice when there  
are cores going spare and to shunt a cpu intensive thread that is  
currently time slicing with other threads onto one of them. I suppose  
in that case, adaptive scheduling techniques would do a far better job  
than my manual efforts would ever achieve.

The other fly in the ointment when trying to be scientific about these  
calculations is the sheer number of threads introduced by dependency  
libraries. For example dumping the stack for a running jboss  
application reveals scores of threads, making my own thread count a  
drop in the ocean. I suppose I could make a ham-fisted attempt to  
calculate the wait/compute time ratio but I wonder how representative  
I could ever hope to make my sample.

On 13 Aug 2009, at 00:36, Robert Fischer wrote:

> My general guideline has been to shotgun the system: aim to be at  
> the high end of the order of magnitude of the number of threads that  
> can be processed at any point in time, and default to throwing off  
> new threads to support a high water mark.  Yes, it creates some  
> waste in terms of context switching and memory, but anything less  
> than that seems to eventually hit throughput limits.
>
> At the end of the day, though, it's all voodoo.
>
> ~~ Robert.
>
> David Holmes wrote:
>> Hi Ashley,
>>> For IO bound tasks I am interested in finding out if the  
>>> underlying OS
>>> io operation that produces the data keeps going in some fashion even
>>> if the corresponding thread that consumes it has not yet been  
>>> reactivated.
>> In general yes. Exactly how much depends on the OS and its  
>> structure. As
>> long as the I/O request has been made then the OS can service that  
>> request -
>> for example processing an incoming ethernet packet.
>>> And cpu bound threads would ideally be assigned to separate cores so
>>> that they can truly execute concurrently. But if the underlying  
>>> scheduler
>>> decides to assign them to the same core then there will be no  
>>> improved
>> performance.
>> Depends on how many threads you are trying to run on the system and  
>> how many
>> cores you have. If you give threads dedicated cores and the threads  
>> are
>> totally CPU bound then you effectively remove them from the  
>> scheduling
>> decisions: if nothing else can run on their core then they won't get
>> timesliced.
>> But to do this you require complete knowledge of everything on the  
>> system -
>> including OS services - to ensure that you don't "starve" your own  
>> threads
>> of services they need, and to ensure the system as a whole still  
>> functions
>> correctly.
>>> So to somebody ignorant of the facts such as myself, it seems that  
>>> the
>>> scheduler would need some sort of clue as to how to schedule the  
>>> threads
>> otherwise
>>> any effort to balance thread pools for size and type of task could
>>> well be in vain.
>> The scheduler only knows about scheduling policies and what  
>> properties
>> threads have under those policies. For "normal" time-sharing this  
>> means all
>> threads are initially equal, and the scheduler will schedule them  
>> as fairly
>> as it can, whilst still having regard for performance (eg. thread  
>> affinity
>> typically tries to run a thread on the same processor as it last  
>> ran on).
>> But CPU bound threads will consume their quanta and be switched  
>> out; while
>> I/O bound threads that block a lot tend to get some preferential  
>> treatment -
>> but it all depends on the OS and the scheduling policy.
>> Binding threads to specific cores makes the scheduler's job both  
>> harder and
>> easier - because there are less choices available.
>> It sounds to me that you may want to partition your available  
>> processors
>> into two sets: one for CPU bound and one for I/O bound tasks. That  
>> might
>> give your application better overall "performance" (as defined by  
>> your
>> application). But again there are no Java API's for doing this, and
>> processor-set/cpu-set management is a system level administrative  
>> task, not
>> something to be attempted from within the application.
>> But as always with these things the first step is to identify if  
>> there is
>> indeed a problem.
>> David Holmes
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> -- 
> ~~ Robert Fischer, Smokejumper IT Consulting.
> Enfranchised Mind Blog http://EnfranchisedMind.com/blog
>
> Check out my book, "Grails Persistence with GORM and GSQL"!
> http://www.smokejumperit.com/redirect.html
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Thu Aug 20 05:06:07 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 20 Aug 2009 19:06:07 +1000
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <61AC43D3-0308-4F66-B5A2-D2CD58EED512@mac.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEECIIDAA.davidcholmes@aapt.net.au>

Ashley Williams writes:
> I'm starting to think this is the best approach too. The kicker for me
> is that you can't assign threads to specific cores at least not with
> standard java - but maybe the jvm is smart enough to notice when there
> are cores going spare and to shunt a cpu intensive thread that is
> currently time slicing with other threads onto one of them. I suppose
> in that case, adaptive scheduling techniques would do a far better job
> than my manual efforts would ever achieve.

The JVMs I'm familiar with do zero scheduling in general - it is all done by
the OS.

But unless you've estblished there is a problem how do you know what you are
trying to achieve?

Cheers,
David Holmes

> The other fly in the ointment when trying to be scientific about these
> calculations is the sheer number of threads introduced by dependency
> libraries. For example dumping the stack for a running jboss
> application reveals scores of threads, making my own thread count a
> drop in the ocean. I suppose I could make a ham-fisted attempt to
> calculate the wait/compute time ratio but I wonder how representative
> I could ever hope to make my sample.
>
> On 13 Aug 2009, at 00:36, Robert Fischer wrote:
>
> > My general guideline has been to shotgun the system: aim to be at
> > the high end of the order of magnitude of the number of threads that
> > can be processed at any point in time, and default to throwing off
> > new threads to support a high water mark.  Yes, it creates some
> > waste in terms of context switching and memory, but anything less
> > than that seems to eventually hit throughput limits.
> >
> > At the end of the day, though, it's all voodoo.
> >
> > ~~ Robert.
> >
> > David Holmes wrote:
> >> Hi Ashley,
> >>> For IO bound tasks I am interested in finding out if the
> >>> underlying OS
> >>> io operation that produces the data keeps going in some fashion even
> >>> if the corresponding thread that consumes it has not yet been
> >>> reactivated.
> >> In general yes. Exactly how much depends on the OS and its
> >> structure. As
> >> long as the I/O request has been made then the OS can service that
> >> request -
> >> for example processing an incoming ethernet packet.
> >>> And cpu bound threads would ideally be assigned to separate cores so
> >>> that they can truly execute concurrently. But if the underlying
> >>> scheduler
> >>> decides to assign them to the same core then there will be no
> >>> improved
> >> performance.
> >> Depends on how many threads you are trying to run on the system and
> >> how many
> >> cores you have. If you give threads dedicated cores and the threads
> >> are
> >> totally CPU bound then you effectively remove them from the
> >> scheduling
> >> decisions: if nothing else can run on their core then they won't get
> >> timesliced.
> >> But to do this you require complete knowledge of everything on the
> >> system -
> >> including OS services - to ensure that you don't "starve" your own
> >> threads
> >> of services they need, and to ensure the system as a whole still
> >> functions
> >> correctly.
> >>> So to somebody ignorant of the facts such as myself, it seems that
> >>> the
> >>> scheduler would need some sort of clue as to how to schedule the
> >>> threads
> >> otherwise
> >>> any effort to balance thread pools for size and type of task could
> >>> well be in vain.
> >> The scheduler only knows about scheduling policies and what
> >> properties
> >> threads have under those policies. For "normal" time-sharing this
> >> means all
> >> threads are initially equal, and the scheduler will schedule them
> >> as fairly
> >> as it can, whilst still having regard for performance (eg. thread
> >> affinity
> >> typically tries to run a thread on the same processor as it last
> >> ran on).
> >> But CPU bound threads will consume their quanta and be switched
> >> out; while
> >> I/O bound threads that block a lot tend to get some preferential
> >> treatment -
> >> but it all depends on the OS and the scheduling policy.
> >> Binding threads to specific cores makes the scheduler's job both
> >> harder and
> >> easier - because there are less choices available.
> >> It sounds to me that you may want to partition your available
> >> processors
> >> into two sets: one for CPU bound and one for I/O bound tasks. That
> >> might
> >> give your application better overall "performance" (as defined by
> >> your
> >> application). But again there are no Java API's for doing this, and
> >> processor-set/cpu-set management is a system level administrative
> >> task, not
> >> something to be attempted from within the application.
> >> But as always with these things the first step is to identify if
> >> there is
> >> indeed a problem.
> >> David Holmes
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> > --
> > ~~ Robert Fischer, Smokejumper IT Consulting.
> > Enfranchised Mind Blog http://EnfranchisedMind.com/blog
> >
> > Check out my book, "Grails Persistence with GORM and GSQL"!
> > http://www.smokejumperit.com/redirect.html
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From ashpublic at mac.com  Thu Aug 20 05:31:22 2009
From: ashpublic at mac.com (Ashley Williams)
Date: Thu, 20 Aug 2009 10:31:22 +0100
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEECIIDAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEECIIDAA.davidcholmes@aapt.net.au>
Message-ID: <1133C843-DC2C-4B85-81FE-C778E8C448B7@mac.com>

On 20 Aug 2009, at 10:06, David Holmes wrote:

> Ashley Williams writes:
>> I'm starting to think this is the best approach too. The kicker for  
>> me
>> is that you can't assign threads to specific cores at least not with
>> standard java - but maybe the jvm is smart enough to notice when  
>> there
>> are cores going spare and to shunt a cpu intensive thread that is
>> currently time slicing with other threads onto one of them. I suppose
>> in that case, adaptive scheduling techniques would do a far better  
>> job
>> than my manual efforts would ever achieve.
>
> The JVMs I'm familiar with do zero scheduling in general - it is all  
> done by
> the OS.
>
> But unless you've estblished there is a problem how do you know what  
> you are
> trying to achieve?

Hi David,

I'm not trying to solve a problem I'm trying to understand how to  
solve problems.

- Ashley

>
> Cheers,
> David Holmes
>
>> The other fly in the ointment when trying to be scientific about  
>> these
>> calculations is the sheer number of threads introduced by dependency
>> libraries. For example dumping the stack for a running jboss
>> application reveals scores of threads, making my own thread count a
>> drop in the ocean. I suppose I could make a ham-fisted attempt to
>> calculate the wait/compute time ratio but I wonder how representative
>> I could ever hope to make my sample.
>>
>> On 13 Aug 2009, at 00:36, Robert Fischer wrote:
>>
>>> My general guideline has been to shotgun the system: aim to be at
>>> the high end of the order of magnitude of the number of threads that
>>> can be processed at any point in time, and default to throwing off
>>> new threads to support a high water mark.  Yes, it creates some
>>> waste in terms of context switching and memory, but anything less
>>> than that seems to eventually hit throughput limits.
>>>
>>> At the end of the day, though, it's all voodoo.
>>>
>>> ~~ Robert.
>>>
>>> David Holmes wrote:
>>>> Hi Ashley,
>>>>> For IO bound tasks I am interested in finding out if the
>>>>> underlying OS
>>>>> io operation that produces the data keeps going in some fashion  
>>>>> even
>>>>> if the corresponding thread that consumes it has not yet been
>>>>> reactivated.
>>>> In general yes. Exactly how much depends on the OS and its
>>>> structure. As
>>>> long as the I/O request has been made then the OS can service that
>>>> request -
>>>> for example processing an incoming ethernet packet.
>>>>> And cpu bound threads would ideally be assigned to separate  
>>>>> cores so
>>>>> that they can truly execute concurrently. But if the underlying
>>>>> scheduler
>>>>> decides to assign them to the same core then there will be no
>>>>> improved
>>>> performance.
>>>> Depends on how many threads you are trying to run on the system and
>>>> how many
>>>> cores you have. If you give threads dedicated cores and the threads
>>>> are
>>>> totally CPU bound then you effectively remove them from the
>>>> scheduling
>>>> decisions: if nothing else can run on their core then they won't  
>>>> get
>>>> timesliced.
>>>> But to do this you require complete knowledge of everything on the
>>>> system -
>>>> including OS services - to ensure that you don't "starve" your own
>>>> threads
>>>> of services they need, and to ensure the system as a whole still
>>>> functions
>>>> correctly.
>>>>> So to somebody ignorant of the facts such as myself, it seems that
>>>>> the
>>>>> scheduler would need some sort of clue as to how to schedule the
>>>>> threads
>>>> otherwise
>>>>> any effort to balance thread pools for size and type of task could
>>>>> well be in vain.
>>>> The scheduler only knows about scheduling policies and what
>>>> properties
>>>> threads have under those policies. For "normal" time-sharing this
>>>> means all
>>>> threads are initially equal, and the scheduler will schedule them
>>>> as fairly
>>>> as it can, whilst still having regard for performance (eg. thread
>>>> affinity
>>>> typically tries to run a thread on the same processor as it last
>>>> ran on).
>>>> But CPU bound threads will consume their quanta and be switched
>>>> out; while
>>>> I/O bound threads that block a lot tend to get some preferential
>>>> treatment -
>>>> but it all depends on the OS and the scheduling policy.
>>>> Binding threads to specific cores makes the scheduler's job both
>>>> harder and
>>>> easier - because there are less choices available.
>>>> It sounds to me that you may want to partition your available
>>>> processors
>>>> into two sets: one for CPU bound and one for I/O bound tasks. That
>>>> might
>>>> give your application better overall "performance" (as defined by
>>>> your
>>>> application). But again there are no Java API's for doing this, and
>>>> processor-set/cpu-set management is a system level administrative
>>>> task, not
>>>> something to be attempted from within the application.
>>>> But as always with these things the first step is to identify if
>>>> there is
>>>> indeed a problem.
>>>> David Holmes
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>> --
>>> ~~ Robert Fischer, Smokejumper IT Consulting.
>>> Enfranchised Mind Blog http://EnfranchisedMind.com/blog
>>>
>>> Check out my book, "Grails Persistence with GORM and GSQL"!
>>> http://www.smokejumperit.com/redirect.html
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From dl at cs.oswego.edu  Thu Aug 20 07:48:59 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 20 Aug 2009 07:48:59 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <75a324d0908191532q60db8096y17b62b75eeba1c77@mail.gmail.com>
References: <4A7C2365.2020106@cs.oswego.edu>	
	<20090813234205.GO6744@linux.vnet.ibm.com>	
	<4A8555E7.3070200@cs.oswego.edu>	
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>	
	<4A86DC26.1070905@cs.oswego.edu>	
	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>	
	<4A895C40.5050207@cs.oswego.edu>	
	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>	
	<4A8AA351.4020205@cs.oswego.edu> <4A8BF3D2.9020907@cs.oswego.edu>
	<75a324d0908191532q60db8096y17b62b75eeba1c77@mail.gmail.com>
Message-ID: <4A8D382B.5040801@cs.oswego.edu>

Dariusz Kordonski wrote:
> At one place it says ' (...) Usages should be is restricted to the 
> control of strictly internal (...)' I believe the 'is' after 'should be' 
> should be removed.
> Also, the javadoc for orderWrites still says: 'If null, the effects of 
> the method are undefined.' I'm not sure, but it seems like a mistake?

Yes; thanks!

-Doug


From hanson.char at gmail.com  Thu Aug 20 13:21:40 2009
From: hanson.char at gmail.com (Hanson Char)
Date: Thu, 20 Aug 2009 10:21:40 -0700
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEECIIDAA.davidcholmes@aapt.net.au>
References: <61AC43D3-0308-4F66-B5A2-D2CD58EED512@mac.com>
	<NFBBKALFDCPFIDBNKAPCEECIIDAA.davidcholmes@aapt.net.au>
Message-ID: <ca53c8f80908201021j1ccf6296od20dd2ecb6ab6ffd@mail.gmail.com>

For scaling services (written in Java) in general, given the same box
equipped with multiple CPU's (and sufficient memory), can there be
performance advantages in running multiple (identical) JVM instances (each
with smaller thread pool) instead of running a single instance of JVM  but
with a larger thread pool ?

Thanks,
Hanson

On Thu, Aug 20, 2009 at 2:06 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

> The JVMs I'm familiar with do zero scheduling in general - it is all done
> by
> the OS.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090820/fd9b622f/attachment.html>

From davidcholmes at aapt.net.au  Thu Aug 20 18:05:47 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 21 Aug 2009 08:05:47 +1000
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <ca53c8f80908201021j1ccf6296od20dd2ecb6ab6ffd@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGECKIDAA.davidcholmes@aapt.net.au>


Hanson Char writes:
> For scaling services (written in Java) in general, given the same box
equipped with
> multiple CPU's (and sufficient memory), can there be performance
advantages in running
> multiple (identical) JVM instances (each with smaller thread pool) instead
of running
> a single instance of JVM  but with a larger thread pool ?

Generally I'd say that the additional VM overheads would count against you
too much. But it all depends on where your performance bottleneck is. In
theory if you're saturating a particular aspect of the VM then
horizontal-scaling might be a benefit (but usually that involves adding new
machines). The main issue is heap: what throughput can you maintain with 10
4OOMB heaps versus 1 4GB heap, for example ?

As always with these things, the proof is in the measurement. :)

Cheers,
David Holmes


From Darron_Shaffer at stercomm.com  Thu Aug 20 18:28:23 2009
From: Darron_Shaffer at stercomm.com (Shaffer, Darron)
Date: Thu, 20 Aug 2009 18:28:23 -0400
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGECKIDAA.davidcholmes@aapt.net.au>
References: <ca53c8f80908201021j1ccf6296od20dd2ecb6ab6ffd@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGECKIDAA.davidcholmes@aapt.net.au>
Message-ID: <FC30D8A2D3DEE64D93E8DA54A1DB349A0831F6C2@IWDUBCORMSG007.sci.local>

Some older (1.4) JVM's on Linux couldn't use more than 3 CPUs
effectively.  I wouldn't expect that to be the case anymore.

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David
Holmes
Sent: Thursday, August 20, 2009 5:06 PM
To: Hanson Char
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Sizing thread pools


Hanson Char writes:
> For scaling services (written in Java) in general, given the same box
equipped with
> multiple CPU's (and sufficient memory), can there be performance
advantages in running
> multiple (identical) JVM instances (each with smaller thread pool)
instead
of running
> a single instance of JVM  but with a larger thread pool ?

Generally I'd say that the additional VM overheads would count against
you
too much. But it all depends on where your performance bottleneck is. In
theory if you're saturating a particular aspect of the VM then
horizontal-scaling might be a benefit (but usually that involves adding
new
machines). The main issue is heap: what throughput can you maintain with
10
4OOMB heaps versus 1 4GB heap, for example ?

As always with these things, the proof is in the measurement. :)

Cheers,
David Holmes

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From hanson.char at gmail.com  Fri Aug 21 00:56:32 2009
From: hanson.char at gmail.com (Hanson Char)
Date: Thu, 20 Aug 2009 21:56:32 -0700
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGECKIDAA.davidcholmes@aapt.net.au>
References: <ca53c8f80908201021j1ccf6296od20dd2ecb6ab6ffd@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGECKIDAA.davidcholmes@aapt.net.au>
Message-ID: <ca53c8f80908202156m73379017n14d6cd27b51646d8@mail.gmail.com>

 >the proof is in the measurement

Totally agree.

Besides throughput, there is also the consideration of service response
time.  In particular, TP99.9.

My (naive?) understanding is the latency of performing a full GC is in
general linear (or at least in some way proportional) to the heap size, and
a full GC would eventually kicks in on a busy JVM regardless of the specific
collector (such as CMS) used.  So it will probably take (10 times?) longer
for the freeze when a full GC kicks in for a heap of 4GB than 0.4GB.  Having
10 JVM's with 0.4GB heap, the chances of all JVM performing a full GC at the
same time is low.  Wouldn't this translate to better TP99.9 if we have 10
small JVM's running on the same box than 1 big JVM (assuming we have say 10
(or more) CPU's on the box) ?

Thanks,
Hanson

On Thu, Aug 20, 2009 at 3:05 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

>
> Hanson Char writes:
> > For scaling services (written in Java) in general, given the same box
> equipped with
> > multiple CPU's (and sufficient memory), can there be performance
> advantages in running
> > multiple (identical) JVM instances (each with smaller thread pool)
> instead
> of running
> > a single instance of JVM  but with a larger thread pool ?
>
> Generally I'd say that the additional VM overheads would count against you
> too much. But it all depends on where your performance bottleneck is. In
> theory if you're saturating a particular aspect of the VM then
> horizontal-scaling might be a benefit (but usually that involves adding new
> machines). The main issue is heap: what throughput can you maintain with 10
> 4OOMB heaps versus 1 4GB heap, for example ?
>
> As always with these things, the proof is in the measurement. :)
>
> Cheers,
> David Holmes
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090820/59a72fa9/attachment.html>

From davidcholmes at aapt.net.au  Fri Aug 21 01:31:15 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 21 Aug 2009 15:31:15 +1000
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <ca53c8f80908202156m73379017n14d6cd27b51646d8@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGECMIDAA.davidcholmes@aapt.net.au>

Hanson,

You'd have to consult some GC experts for an answer to that. My guess is
that it all depends :)

Even looking at it simplistically though, you'd have to account for the full
GCs happening 10x more frequently.

Without thinking too hard about this I'd say that with 10 VMs you'd probably
reduce the worst-case latency but increase the median latency.

David

 -----Original Message-----
From: Hanson Char [mailto:hanson.char at gmail.com]
Sent: Friday, 21 August 2009 2:57 PM
To: dholmes at ieee.org
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Sizing thread pools


  >the proof is in the measurement

  Totally agree.

  Besides throughput, there is also the consideration of service response
time.  In particular, TP99.9.

  My (naive?) understanding is the latency of performing a full GC is in
general linear (or at least in some way proportional) to the heap size, and
a full GC would eventually kicks in on a busy JVM regardless of the specific
collector (such as CMS) used.  So it will probably take (10 times?) longer
for the freeze when a full GC kicks in for a heap of 4GB than 0.4GB.  Having
10 JVM's with 0.4GB heap, the chances of all JVM performing a full GC at the
same time is low.  Wouldn't this translate to better TP99.9 if we have 10
small JVM's running on the same box than 1 big JVM (assuming we have say 10
(or more) CPU's on the box) ?

  Thanks,
  Hanson


  On Thu, Aug 20, 2009 at 3:05 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:


    Hanson Char writes:
    > For scaling services (written in Java) in general, given the same box
    equipped with
    > multiple CPU's (and sufficient memory), can there be performance
    advantages in running
    > multiple (identical) JVM instances (each with smaller thread pool)
instead
    of running
    > a single instance of JVM  but with a larger thread pool ?


    Generally I'd say that the additional VM overheads would count against
you
    too much. But it all depends on where your performance bottleneck is. In
    theory if you're saturating a particular aspect of the VM then
    horizontal-scaling might be a benefit (but usually that involves adding
new
    machines). The main issue is heap: what throughput can you maintain with
10
    4OOMB heaps versus 1 4GB heap, for example ?

    As always with these things, the proof is in the measurement. :)

    Cheers,
    David Holmes



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090821/bfb17942/attachment.html>

From hanson.char at gmail.com  Fri Aug 21 01:46:42 2009
From: hanson.char at gmail.com (Hanson Char)
Date: Thu, 20 Aug 2009 22:46:42 -0700
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGECMIDAA.davidcholmes@aapt.net.au>
References: <ca53c8f80908202156m73379017n14d6cd27b51646d8@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGECMIDAA.davidcholmes@aapt.net.au>
Message-ID: <ca53c8f80908202246k75438821tddc9df0c93d94238@mail.gmail.com>

My simplistic thinking leads me to the same conclusion/expectation :)
Thanks, David.

Hanson

On Thu, Aug 20, 2009 at 10:31 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

>  Hanson,
>
> You'd have to consult some GC experts for an answer to that. My guess is
> that it all depends :)
>
> Even looking at it simplistically though, you'd have to account for the
> full GCs happening 10x more frequently.
>
> Without thinking too hard about this I'd say that with 10 VMs you'd
> probably reduce the worst-case latency but increase the median latency.
>
> David
>
>  -----Original Message-----
> *From:* Hanson Char [mailto:hanson.char at gmail.com]
> *Sent:* Friday, 21 August 2009 2:57 PM
> *To:* dholmes at ieee.org
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Sizing thread pools
>
> >the proof is in the measurement
>
> Totally agree.
>
> Besides throughput, there is also the consideration of service response
> time.  In particular, TP99.9.
>
> My (naive?) understanding is the latency of performing a full GC is in
> general linear (or at least in some way proportional) to the heap size, and
> a full GC would eventually kicks in on a busy JVM regardless of the specific
> collector (such as CMS) used.  So it will probably take (10 times?) longer
> for the freeze when a full GC kicks in for a heap of 4GB than 0.4GB.  Having
> 10 JVM's with 0.4GB heap, the chances of all JVM performing a full GC at the
> same time is low.  Wouldn't this translate to better TP99.9 if we have 10
> small JVM's running on the same box than 1 big JVM (assuming we have say 10
> (or more) CPU's on the box) ?
>
> Thanks,
> Hanson
>
> On Thu, Aug 20, 2009 at 3:05 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>>
>> Hanson Char writes:
>> > For scaling services (written in Java) in general, given the same box
>> equipped with
>> > multiple CPU's (and sufficient memory), can there be performance
>> advantages in running
>> > multiple (identical) JVM instances (each with smaller thread pool)
>> instead
>> of running
>> > a single instance of JVM  but with a larger thread pool ?
>>
>> Generally I'd say that the additional VM overheads would count against you
>> too much. But it all depends on where your performance bottleneck is. In
>> theory if you're saturating a particular aspect of the VM then
>> horizontal-scaling might be a benefit (but usually that involves adding
>> new
>> machines). The main issue is heap: what throughput can you maintain with
>> 10
>> 4OOMB heaps versus 1 4GB heap, for example ?
>>
>> As always with these things, the proof is in the measurement. :)
>>
>> Cheers,
>> David Holmes
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090820/00cc35c1/attachment-0001.html>

From markus.kohler at gmail.com  Fri Aug 21 04:50:50 2009
From: markus.kohler at gmail.com (Markus Kohler)
Date: Fri, 21 Aug 2009 10:50:50 +0200
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <ca53c8f80908201021j1ccf6296od20dd2ecb6ab6ffd@mail.gmail.com>
References: <61AC43D3-0308-4F66-B5A2-D2CD58EED512@mac.com>
	<NFBBKALFDCPFIDBNKAPCEECIIDAA.davidcholmes@aapt.net.au>
	<ca53c8f80908201021j1ccf6296od20dd2ecb6ab6ffd@mail.gmail.com>
Message-ID: <771905290908210150k7599210at5d8406dd4f66ccda@mail.gmail.com>

Hi Hanson,
This might be a bit off topic here, but it's an interesting topic, so
let my try to answer your question :)

Yes definitely running more than one JVM can be beneficial. There are
several reasons.
First you could run several  32bit JVM's on a 64 bit machine. The
32bit JVM's were usually faster and needed less memory because of
smaller references.  SAP for example used to recommend 2 nodes instead
of only one even on 64 bit machine. You also can get better fail over
support.

As others have said "it all depends", also IHMO today there aren't
many good reasons to do so.
CMS performs fairly well even with large heaps and there is compressed
references support in the latest SUN JDK. And with multiple JVM's you
also waste memory because some data will be kept within each JVM.

Regards,
Markus

On Thu, Aug 20, 2009 at 7:21 PM, Hanson Char<hanson.char at gmail.com> wrote:
> For scaling services (written in Java) in general, given the same box
> equipped with multiple CPU's (and sufficient memory), can there be
> performance advantages in running multiple (identical) JVM instances (each
> with smaller thread pool) instead of running a single instance of JVM? but
> with a larger thread pool ?
>
> Thanks,
> Hanson
>
> On Thu, Aug 20, 2009 at 2:06 AM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>>
>> The JVMs I'm familiar with do zero scheduling in general - it is all done
>> by
>> the OS.
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From gregg at cytetech.com  Fri Aug 21 12:57:05 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 21 Aug 2009 11:57:05 -0500
Subject: [concurrency-interest] Sizing thread pools
In-Reply-To: <ca53c8f80908202246k75438821tddc9df0c93d94238@mail.gmail.com>
References: <ca53c8f80908202156m73379017n14d6cd27b51646d8@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGECMIDAA.davidcholmes@aapt.net.au>
	<ca53c8f80908202246k75438821tddc9df0c93d94238@mail.gmail.com>
Message-ID: <4A8ED1E1.8060008@cytetech.com>

In the distant past, around 1986, I was responsible for a Vax-11/750 that was 
used by both the statistics department and the Math department.  The stats 
people ran simulations, all the time at their terminals, letting them run for 
hours, and just stopping by to look every once in a while to see if they were 
done.  The more terminals they had access to, the more simulations they'd run. 
The math people used a few math packages that used some CPU for a few minutes 
usually.  The majority of the people were writing papers and sending email, and 
with all the CPU bound stuff going on, the machine was dog slow in response to 
interactive requests.

I did some studies of the scheduler and found that it was actually pretty rigid, 
and so I wrote a program that ran in the background and periodically lowered 
processes' priorities the longer they stayed CPU bound, eventually putting them 
below the batch queue priorities even.  This made the interactive users feel 
like they were the only ones using the computer.  They started at prio=5, and 
got set down to 4 after 6 seconds of CPU.  At 30 seconds they went to 3 (batch 
queue priority), and after a couple of minutes or more, they went to 2 as I 
recall.  I finally would set them to priority 1 if they ran longer than an hour.

I mention this, because, for me, it points out, that the "larger" load on the 
machine you have the worse latency gets for any response (this is not news). 
With priorities, I was able to divide the machine in to pieces so that different 
users had different opportunities to use the machines resources more readily.

This was a form of horizontal expansion because the priority structure on VMS 
was fairly rigid.  Expanding horizontally with more machines/CPUs is almost 
always the easiest way to deal with latency in a non-real-time OS.  Having 10 
VMs running on the same machine, for me, is not horizontal expansion if you 
don't manage how each VM uses the machine so that things needing certain 
resources are in certain VMs all of which whose behavior needs to be 
purposefully partitioned.  I/O bandwidth to disk for swapping/paging and network 
contention for OS based scheduling bottle necks all need to be considered.

Gregg Wonderly

Hanson Char wrote:
> My simplistic thinking leads me to the same conclusion/expectation :)  
> Thanks, David.
> 
> Hanson
> 
> On Thu, Aug 20, 2009 at 10:31 PM, David Holmes <davidcholmes at aapt.net.au 
> <mailto:davidcholmes at aapt.net.au>> wrote:
> 
>     Hanson,
>      
>     You'd have to consult some GC experts for an answer to that. My
>     guess is that it all depends :)
>      
>     Even looking at it simplistically though, you'd have to account for
>     the full GCs happening 10x more frequently.
>      
>     Without thinking too hard about this I'd say that with 10 VMs you'd
>     probably reduce the worst-case latency but increase the median latency.
>      
>     David
>      
>      -----Original Message-----
>     *From:* Hanson Char [mailto:hanson.char at gmail.com
>     <mailto:hanson.char at gmail.com>]
>     *Sent:* Friday, 21 August 2009 2:57 PM
>     *To:* dholmes at ieee.org <mailto:dholmes at ieee.org>
>     *Cc:* concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>
>     *Subject:* Re: [concurrency-interest] Sizing thread pools
> 
>          >the proof is in the measurement
> 
>         Totally agree.
> 
>         Besides throughput, there is also the consideration of service
>         response time.  In particular, TP99.9.
> 
>         My (naive?) understanding is the latency of performing a full GC
>         is in general linear (or at least in some way proportional) to
>         the heap size, and a full GC would eventually kicks in on a busy
>         JVM regardless of the specific collector (such as CMS) used.  So
>         it will probably take (10 times?) longer for the freeze when a
>         full GC kicks in for a heap of 4GB than 0.4GB.  Having 10 JVM's
>         with 0.4GB heap, the chances of all JVM performing a full GC at
>         the same time is low.  Wouldn't this translate to better TP99.9
>         if we have 10 small JVM's running on the same box than 1 big JVM
>         (assuming we have say 10 (or more) CPU's on the box) ?
> 
>         Thanks,
>         Hanson
> 
>         On Thu, Aug 20, 2009 at 3:05 PM, David Holmes
>         <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
> 
> 
>             Hanson Char writes:
>              > For scaling services (written in Java) in general, given
>             the same box
>             equipped with
>              > multiple CPU's (and sufficient memory), can there be
>             performance
>             advantages in running
>              > multiple (identical) JVM instances (each with smaller
>             thread pool) instead
>             of running
>              > a single instance of JVM  but with a larger thread pool ?
> 
>             Generally I'd say that the additional VM overheads would
>             count against you
>             too much. But it all depends on where your performance
>             bottleneck is. In
>             theory if you're saturating a particular aspect of the VM then
>             horizontal-scaling might be a benefit (but usually that
>             involves adding new
>             machines). The main issue is heap: what throughput can you
>             maintain with 10
>             4OOMB heaps versus 1 4GB heap, for example ?
> 
>             As always with these things, the proof is in the measurement. :)
> 
>             Cheers,
>             David Holmes
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From ashwin.jayaprakash at gmail.com  Fri Aug 21 16:10:17 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Fri, 21 Aug 2009 13:10:17 -0700
Subject: [concurrency-interest] A Generic/Templatized ThreadPoolExecutor
Message-ID: <837c23d40908211310m3427a5f9j9c56064747a9ace@mail.gmail.com>

Hi, I was wondering if it would be a good idea to genericise the
j.u.c.ThreadPoolExecutor to accept a sub-type of Runnable. I mean:

*From:*
class ThreadPoolExecutor{
  ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long
keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue) {
  }
}

*To:*
class ThreadPoolExecutor<R extends Runnable>{
  ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long
keepAliveTime, TimeUnit unit, BlockingQueue<R> workQueue) {
  }
}

Several times, I've felt the need to create a specific class like
a.b.c.AsyncJob that implements Runnable which is scheduled on the
thread-pool. But I had to cast the return values of shutdownNow() or the
parameter for beforeExecute(R r, Throwable t) and afterExecute(R r,
Throwable t). Has anyone else faced this?

Thanks,
Ashwin (http://www.javaforu.blogspot.com)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090821/31fd782c/attachment.html>

From joe.bowbeer at gmail.com  Fri Aug 21 16:48:33 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 21 Aug 2009 13:48:33 -0700
Subject: [concurrency-interest] A Generic/Templatized ThreadPoolExecutor
In-Reply-To: <837c23d40908211310m3427a5f9j9c56064747a9ace@mail.gmail.com>
References: <837c23d40908211310m3427a5f9j9c56064747a9ace@mail.gmail.com>
Message-ID: <31f2a7bd0908211348s67c13012mada22555cab07fb@mail.gmail.com>

On Fri, Aug 21, 2009 at 1:10 PM, Ashwin Jayaprakash wrote:

> Hi, I was wondering if it would be a good idea to genericise the
> j.u.c.ThreadPoolExecutor to accept a sub-type of Runnable. I mean:
>
> *From:*
> class ThreadPoolExecutor{
>   ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long
> keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue) {
>   }
> }
>
> *To:*
> class ThreadPoolExecutor<R extends Runnable>{
>    ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long
> keepAliveTime, TimeUnit unit, BlockingQueue<R> workQueue) {
>   }
> }
>
> Several times, I've felt the need to create a specific class like
> a.b.c.AsyncJob that implements Runnable which is scheduled on the
> thread-pool. But I had to cast the return values of shutdownNow() or the
> parameter for beforeExecute(R r, Throwable t) and afterExecute(R r,
> Throwable t). Has anyone else faced this?
>


I see your point, however, note that if you submit Runnable tasks using any
of the submit or invoke methods, or via a CompletionService, then the items
on the work queue will be RunnableFutures created by the executorService's
newTaskFor method.

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090821/5eb38979/attachment.html>

From gregg at cytetech.com  Fri Aug 21 19:10:28 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 21 Aug 2009 18:10:28 -0500
Subject: [concurrency-interest] A Generic/Templatized ThreadPoolExecutor
In-Reply-To: <31f2a7bd0908211348s67c13012mada22555cab07fb@mail.gmail.com>
References: <837c23d40908211310m3427a5f9j9c56064747a9ace@mail.gmail.com>
	<31f2a7bd0908211348s67c13012mada22555cab07fb@mail.gmail.com>
Message-ID: <4A8F2964.1010508@cytetech.com>

Joe Bowbeer wrote:
> On Fri, Aug 21, 2009 at 1:10 PM, Ashwin Jayaprakash wrote:
> 
>     Hi, I was wondering if it would be a good idea to genericise the
>     j.u.c.ThreadPoolExecutor to accept a sub-type of Runnable. I mean:
>      
>     *From:*
>     class ThreadPoolExecutor{
>       ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long
>     keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue) {
>       }
>     }
> 
>     *To:*
>     class ThreadPoolExecutor<R extends Runnable>{
>       ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long
>     keepAliveTime, TimeUnit unit, BlockingQueue<R> workQueue) {
>       }
>     }
> 
>     Several times, I've felt the need to create a specific class like
>     a.b.c.AsyncJob that implements Runnable which is scheduled on the
>     thread-pool. But I had to cast the return values of shutdownNow() or
>     the parameter for beforeExecute(R r, Throwable t) and afterExecute(R
>     r, Throwable t). Has anyone else faced this?
> 
> 
> 
> I see your point, however, note that if you submit Runnable tasks using 
> any of the submit or invoke methods, or via a CompletionService, then 
> the items on the work queue will be RunnableFutures created by the 
> executorService's newTaskFor method.

This is one of those odd things to me.  The fact that the Runnable itself needs 
to be wrapped, and you can't deal with the wrapping class to find the "data" 
that you put into your "Runnable" is really a pain because you then have to 
create a map from Future to Runnable at the point of submission to get back to 
your Runnable's Data so that as Futures complete, you can handle any post run 
cleanup.

Gregg Wonderly

From joe.bowbeer at gmail.com  Fri Aug 21 20:40:36 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 21 Aug 2009 17:40:36 -0700
Subject: [concurrency-interest] A Generic/Templatized ThreadPoolExecutor
In-Reply-To: <4A8F2964.1010508@cytetech.com>
References: <837c23d40908211310m3427a5f9j9c56064747a9ace@mail.gmail.com>
	<31f2a7bd0908211348s67c13012mada22555cab07fb@mail.gmail.com>
	<4A8F2964.1010508@cytetech.com>
Message-ID: <31f2a7bd0908211740l56f7c7d8sac5ba534172ed609@mail.gmail.com>

I agree that it can be odd.

However, the Runnable doesn't "need" to be wrapped.  It can be executed
directly by any Executor.  OTOH, the Runnable "will" be wrapped if you
submit or invoke it.

I personally prefer to create my own Runnables (usually a subclass of
FutureTask) and execute them directly.  This requires a little more setup
initially, but it provides full control.

Note that the newTaskFor method was added later to help folks create their
own FutureTask specific variants even when they use the submit and/or invoke
methods.

Joe

On Fri, Aug 21, 2009 at 4:10 PM, Gregg Wonderly wrote:

> Joe Bowbeer wrote:
>
>> On Fri, Aug 21, 2009 at 1:10 PM, Ashwin Jayaprakash wrote:
>>
>>    Hi, I was wondering if it would be a good idea to genericise the
>>    j.u.c.ThreadPoolExecutor to accept a sub-type of Runnable. I mean:
>>        *From:*
>>    class ThreadPoolExecutor{
>>      ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long
>>    keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue) {
>>      }
>>    }
>>
>>    *To:*
>>    class ThreadPoolExecutor<R extends Runnable>{
>>      ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long
>>    keepAliveTime, TimeUnit unit, BlockingQueue<R> workQueue) {
>>      }
>>    }
>>
>>    Several times, I've felt the need to create a specific class like
>>    a.b.c.AsyncJob that implements Runnable which is scheduled on the
>>    thread-pool. But I had to cast the return values of shutdownNow() or
>>    the parameter for beforeExecute(R r, Throwable t) and afterExecute(R
>>    r, Throwable t). Has anyone else faced this?
>>
>>
>>
>> I see your point, however, note that if you submit Runnable tasks using
>> any of the submit or invoke methods, or via a CompletionService, then the
>> items on the work queue will be RunnableFutures created by the
>> executorService's newTaskFor method.
>>
>
> This is one of those odd things to me.  The fact that the Runnable itself
> needs to be wrapped, and you can't deal with the wrapping class to find the
> "data" that you put into your "Runnable" is really a pain because you then
> have to create a map from Future to Runnable at the point of submission to
> get back to your Runnable's Data so that as Futures complete, you can handle
> any post run cleanup.
>
> Gregg Wonderly
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090821/ed3c7e67/attachment-0001.html>

From ashwin.jayaprakash at gmail.com  Fri Aug 21 21:39:54 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Fri, 21 Aug 2009 18:39:54 -0700
Subject: [concurrency-interest] A Generic/Templatized ThreadPoolExecutor
Message-ID: <837c23d40908211839u5ad0b0cau4e5cc7f7a37b75a4@mail.gmail.com>

I guess newTaskFor() and RunnableFuture are some consolation - I hadn't
noticed these new classes. I'm still on 1.5 :-( This will definitely save
the expense of creating too many little wrapper objects - Future, Runnable..

However, RunnableFuture does not implement Callable. So, for such cases
you'd have to create and wrap the actual "job" with a Future. It still looks
a little weird to me - so many interfaces.

Still, I think it should've been templatized.

Thanks!
Ashwin.


---------- Forwarded message ----------
> From: Joe Bowbeer <joe.bowbeer at gmail.com>
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Date: Fri, 21 Aug 2009 17:40:36 -0700
> Subject: Re: [concurrency-interest] A Generic/Templatized
> ThreadPoolExecutor
> I agree that it can be odd.
>
> However, the Runnable doesn't "need" to be wrapped.  It can be executed
> directly by any Executor.  OTOH, the Runnable "will" be wrapped if you
> submit or invoke it.
>
> I personally prefer to create my own Runnables (usually a subclass of
> FutureTask) and execute them directly.  This requires a little more setup
> initially, but it provides full control.
>
> Note that the newTaskFor method was added later to help folks create their
> own FutureTask specific variants even when they use the submit and/or invoke
> methods.
>
> Joe
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090821/075ec3ff/attachment.html>

From davidcholmes at aapt.net.au  Fri Aug 21 21:46:21 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 22 Aug 2009 11:46:21 +1000
Subject: [concurrency-interest] A Generic/Templatized ThreadPoolExecutor
In-Reply-To: <837c23d40908211839u5ad0b0cau4e5cc7f7a37b75a4@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGECPIDAA.davidcholmes@aapt.net.au>

I think if you check the mailing list archives this has come up before and
there are reasons why generics can not be used.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ashwin
Jayaprakash
  Sent: Saturday, 22 August 2009 11:40 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] A Generic/Templatized
ThreadPoolExecutor


  I guess newTaskFor() and RunnableFuture are some consolation - I hadn't
noticed these new classes. I'm still on 1.5 :-( This will definitely save
the expense of creating too many little wrapper objects - Future, Runnable..

  However, RunnableFuture does not implement Callable. So, for such cases
you'd have to create and wrap the actual "job" with a Future. It still looks
a little weird to me - so many interfaces.

  Still, I think it should've been templatized.

  Thanks!
  Ashwin.



    ---------- Forwarded message ----------
    From: Joe Bowbeer <joe.bowbeer at gmail.com>
    To: concurrency-interest <concurrency-interest at cs.oswego.edu>
    Date: Fri, 21 Aug 2009 17:40:36 -0700
    Subject: Re: [concurrency-interest] A Generic/Templatized
ThreadPoolExecutor
    I agree that it can be odd.

    However, the Runnable doesn't "need" to be wrapped.  It can be executed
directly by any Executor.  OTOH, the Runnable "will" be wrapped if you
submit or invoke it.

    I personally prefer to create my own Runnables (usually a subclass of
FutureTask) and execute them directly.  This requires a little more setup
initially, but it provides full control.

    Note that the newTaskFor method was added later to help folks create
their own FutureTask specific variants even when they use the submit and/or
invoke methods.

    Joe


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090822/dd00d56b/attachment.html>

From ashwin.jayaprakash at gmail.com  Sat Aug 22 03:15:07 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Sat, 22 Aug 2009 00:15:07 -0700
Subject: [concurrency-interest] A Generic/Templatized ThreadPoolExecutor
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGECPIDAA.davidcholmes@aapt.net.au>
References: <837c23d40908211839u5ad0b0cau4e5cc7f7a37b75a4@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGECPIDAA.davidcholmes@aapt.net.au>
Message-ID: <837c23d40908220015n2e534aayae83ccc42db670ce@mail.gmail.com>

Yeah, I read it now. For others -
http://cs.oswego.edu/pipermail/concurrency-interest/2003-August/000515.html

Ashwin.



On Fri, Aug 21, 2009 at 6:46 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

>  I think if you check the mailing list archives this has come up before
> and there are reasons why generics can not be used.
>
> David Holmes
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090822/8c8aced2/attachment.html>

From dl at cs.oswego.edu  Sat Aug 22 13:58:34 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 22 Aug 2009 13:58:34 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>
	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>
	<4A895C40.5050207@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A9031CA.8010707@cs.oswego.edu>

Boehm, Hans wrote:
> Right, but I still think this helps in making sure that we've explored all
> viable alternatives.

Here's another exploration, that leads to an
approach that seems to better balance tradeoffs.
Basically, it amounts to refactoring fence methods
to be part of AtomicXFieldUpdater APIs. Here's the story:

> With correct uses of the
> proposed fence API, it becomes very difficult or impossible to distinguish
> accidental data races from those that were introduced by "optimizing"
> volatile accesses with explicit fences or the like.

To illustrate, here's a stripped-down version of the
acq/rel example:

  class Box {
    private Item item;
    Item get()       { return Fences.orderReads(item); }
    void set(Item x) { item = Fences.orderWrites(x); }
  }

It would be better to somehow indicate that Box.item is intended
to be accessed across multiple threads, which you cannot
otherwise figure out except by looking at all the code.
Further, labeling would improve prospects for tools that help
ensure that all code acts appropriately in the face of fields
like Box.item being accessed by multiple threads.

To explore solutions, first note that if AtomicXFieldUpdaters
were instead used here, accessibility across multiple threads
would be implied automatically -- why else would you create an
atomic updater? As in, for example (ignoring for the moment a
snag in how updater constructors are currently defined):

  class Box2 {
    private Item item;
    private static final AtomicReferenceFieldUpdater<Box2,Item>
      u = AtomicReferenceFieldUpdater.newUpdater
       (Box2.class, Item.class, "item");
    Item get()       { return u.get(this); }
    void set(Item x) { u.lazySet(this, x); }
  }

But, as we've discussed, updaters have overhead problems that
you cannot just wish away. We don't people avoiding solutions
(and so doing something worse) because they are too costly.

One way to approximate this effect using Fences is to recast
methods so that instead of having their arguments indicate the
values of the accesses, they indicate the fields being
accessed. Hans proposed using an argument that just indicates
the object containing such a field, which gives you only a rough
idea about target. But you can do better by adapting updater
approach, so that the argument pinpoints the field.

This idea immediately suggests a further refactoring: placing
fence methods in the updater classes themselves.  You need to
rename methods back to their old "fence-ese" form to mesh with
the resulting usage and semantics. As in:

  class Box3 {
    private Item item;
    static ... AtomicReferenceFieldUpdater<Box3,Item> u...;
    Item get()       {
      Item i = item;
      u.postLoadFence(this);
      return i;
    }
    void set(Item x) {
      u.preStoreFence(this);
      item = x;
    }
  }

There are several advantages to replacing the Fences API with
suitably enhanced updater classes:

* Tools etc can proceed under the interpretation that any field
with an updater is intended to be accessed by mulitple threads.

* Updaters are by far the ugliest and most imposing API I've
ever had anything to do with. Accidental casual or
hard-to-detect usages are not going to happen.

* Unlike the case for other updater methods, there is unlikely
to be intrinsic dynamic overhead involved in performing
underlying internal fences. For most usages, most
implementations can secretly ignore the argument so don't need
to check it. And while creating new updaters can be expensive,
it is a one-time cost per field per class, so unlikely to be
noticeable in practice.

* Many acq/rel-style usages entail some form of CAS, in which
case people need updaters anyway when the CAS involves the same
field as acq/rel. Having both fence and CAS defined as
operations on updater objects clarifies usage.

However, there are a few disadvantages:

* Updater constructors (actually factories) currently require
that updatable fields be declared as "volatile".  For the above
story to hold, this must be eliminated, which would require
approval of a JDK spec change request. (I don't think this is a
major obstacle since it has no impact on existing correct
usages.)  As a consequence, ALL usages of updaters will no
longer require volatile-ness, which might let a few new mistakes
out there to escape detection.  For internal consistency, doing
this will also entail a few other changes, like adding method
"relaxedGet".

* Goodbye to nicer method names and the "fluent" idioms enabled
by return of ref argument in the current Fences forms.  These
uglifications will themselves introduce opportunities for error,
and it is harder to informally check several strange-looking
lines of code vs a single strange-looking assignment+expression.
On the other hand, given that the main target audience includes
people making tools, developing low-level infrastructure, and
fixing bugs, the impact is limited.

* We currently support updaters only for int, long, ref.
Minimally, we'd need to add support for in-place array updates,
which is itself a challenge.

* This form does not support usages along the lines of "return
Fences.orderWrites(x)", where you don't know/care about the
assignment target of x upon return. Hans had suggested a
workaround for this by defining a separate method. But if the
Fences class goes away entirely, it is still possible
to support most underlying usages, For example,
recasting the safe-publication example:

   WidgetHolder newWidgetHolder(Params params) {
      WidgetHolder h = new WidgetHolder();
      Widget w = new Widget(params);
      widgetUpdater.preStoreFence(h); // assume declared as static
      h.widget = w;
      return h;
    }

* Similarly, the sorts of post-hoc/bug-fix/security usages
sketchily illustrated in the volatile-emulation example would
work only if the underlying field were accessible when
attempting to create an updater.  This holds in the
stripped-down example given, but not often in practice -- most
such classes use get/set accessor methods. In which case the
only response is "tough luck. Use a private lock". Which is
usually an OK response for bug-fix and security cases.  There
are some hacky options for people to do better in such cases if
they really nedd to though, like delicate constructions using a
field that you DO have access to as a proxy for purposes of
issuing fences.

* While it seems tolerable to also attach reachabilityFence to
updaters, the resulting usages are even more weird-looking and
awkward.

* I don't look forward to explaining in updater docs that
     fieldUpdater.preStoreFence(this); field = x;
  is almost surely faster than
     fieldUpdater.lazySet(this, x);

All in all though, as far as I can see, this solution addresses
the main reservations people have about unrestrained Fence
usages, at the expense of uglier and more limited application.

The only other Fence-hater reservations I can recall that are
not addressed by this surround the lack of any obvious hook to
help tools ensure that acquires match releases etc; which
probably requires language-level support.

I'll follow up (although probably not immediately) with some
more details about the various extensions to updater classes.
In the mean time, reactions to throwing out Fences and
adopting this approach would be welcome.

-Doug

From tim at peierls.net  Sun Aug 23 09:35:54 2009
From: tim at peierls.net (Tim Peierls)
Date: Sun, 23 Aug 2009 09:35:54 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A9031CA.8010707@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>
	<4A895C40.5050207@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
	<4A9031CA.8010707@cs.oswego.edu>
Message-ID: <63b4e4050908230635r334a759cy6338dd7737828be1@mail.gmail.com>

On Sat, Aug 22, 2009 at 1:58 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> * Goodbye to nicer method names and the "fluent" idioms enabled
>
by return of ref argument in the current Fences forms.  These
> uglifications will themselves introduce opportunities for error,
> and it is harder to informally check several strange-looking
> lines of code vs a single strange-looking assignment+expression.


I don't know about that. I scratched my head for a long time over some of
the "fluent" examples, but I read your Box3 example without any trouble.



> On the other hand, given that the main target audience includes
> people making tools, developing low-level infrastructure, and
> fixing bugs, the impact is limited.


Yeah, that, too.



> * While it seems tolerable to also attach reachabilityFence to
> updaters, the resulting usages are even more weird-looking and
> awkward.


But reachabilityFence could at least be packaged with the updater stuff, for
one stop shopping, right?



> * I don't look forward to explaining in updater docs that
>    fieldUpdater.preStoreFence(this); field = x;
>  is almost surely faster than
>    fieldUpdater.lazySet(this, x);


This bothers me not at all. Teachable moment.



> The only other Fence-hater reservations I can recall that are
> not addressed by this surround the lack of any obvious hook to
> help tools ensure that acquires match releases etc; which
> probably requires language-level support.


When we have tools that can intuit class concurrency policies without
benefit of author-added metadata, then we can worry about tools to help
check fence/updater uses.



> I'll follow up (although probably not immediately) with some
> more details about the various extensions to updater classes.
> In the mean time, reactions to throwing out Fences and
> adopting this approach would be welcome.


Ugly it up!

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090823/e4091fbb/attachment.html>

From alarmnummer at gmail.com  Sun Aug 23 16:50:53 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 23 Aug 2009 22:50:53 +0200
Subject: [concurrency-interest] STM and Spinning
Message-ID: <1466c1d60908231350o7d234d12ma02b566bbcade35e@mail.gmail.com>

Hi Guys,

I'm working on a software transactional memory implementation and one
of the things I want to pick up is spinning and I'm looking for
advice/ideas.

With my main STM implementation if can happen that a load is done for
a specific version (the data loaded should have a version equal or
smaller than the load version) even though the write has not happened
yet. When the load detects that it can't determine if the version the
transaction is looking for has been written, it aborts the transaction
and the transaction is retried. If this problem is not detected, the
STM could suffer from isolation problems (not seeing changes made by
transactions completed before it) and we don't want that. With
databases this particular problem is called the lost update.

The problem of aborting is that a lot of work has been executed for
nothing. To lower the chance that a transaction aborts with this
failure, I want to add spinning: just keep repeat reading until the
load can determine if the current version is the correct one (or not).
Since this ambiguous period is very short, spinning could be a light
weight solution.

The simplest approach that comes to mind is some form of bounded
spinning. But what kind of value should be used for the number of
iterations? 10, 100, 1000?

Or should it be more flexible like adaptive spinning used on the
intrinsic lock? If yes, how to design such a component? And how to
prevent it causing overhead?

And is there no better way? What about the (deprecated) Thread.yield
method? The advantage is that other transactions have more cpu-room,
so are more likely to complete sooner. And the sooner other
transactions complete, the sooner the load can complete.

ps:
using a lock in combination with a wait set is to expensive here.

From gregg at cytetech.com  Mon Aug 24 11:34:37 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 24 Aug 2009 10:34:37 -0500
Subject: [concurrency-interest] A Generic/Templatized ThreadPoolExecutor
In-Reply-To: <837c23d40908220015n2e534aayae83ccc42db670ce@mail.gmail.com>
References: <837c23d40908211839u5ad0b0cau4e5cc7f7a37b75a4@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGECPIDAA.davidcholmes@aapt.net.au>
	<837c23d40908220015n2e534aayae83ccc42db670ce@mail.gmail.com>
Message-ID: <4A92B30D.5010104@cytetech.com>

The bigger issue for me, is that ThreadPoolExecutor<T extends Runnable> actually 
allows me, as a developer to get very specific with typing so that I can make 
sure the right things are scheduled into the right queues.

Referring back to my commentary about the VAX VMS scheduling application, I 
sometimes use multiple executors to manage differing queues of differing run 
length objects, and new and old developers working on such software can often, 
by mistake, put the wrong type task into the wrong executor because typing is 
not "tight" enough to make sure that the correct tasks are going into the 
correct places.

Gregg Wonderly

Ashwin Jayaprakash wrote:
> Yeah, I read it now. For others - 
> http://cs.oswego.edu/pipermail/concurrency-interest/2003-August/000515.html
> 
> Ashwin.
> 
> 
> 
> On Fri, Aug 21, 2009 at 6:46 PM, David Holmes <davidcholmes at aapt.net.au 
> <mailto:davidcholmes at aapt.net.au>> wrote:
> 
>     I think if you check the mailing list archives this has come up
>     before and there are reasons why generics can not be used.
>      
>     David Holmes
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From gregg at cytetech.com  Mon Aug 24 12:06:32 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 24 Aug 2009 11:06:32 -0500
Subject: [concurrency-interest] Fences
In-Reply-To: <4A9031CA.8010707@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>
	<4A7D7618.9050501@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>
	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>
	<4A835979.30702@cs.oswego.edu>
	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>
	<4A8407F0.7030708@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>
	<4A895C40.5050207@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
	<4A9031CA.8010707@cs.oswego.edu>
Message-ID: <4A92BA88.5080205@cytetech.com>

Doug Lea wrote:
> Boehm, Hans wrote:
>> Right, but I still think this helps in making sure that we've explored 
>> all
>> viable alternatives.
> 
> Here's another exploration, that leads to an
> approach that seems to better balance tradeoffs.
> Basically, it amounts to refactoring fence methods
> to be part of AtomicXFieldUpdater APIs. Here's the story:

Last week, I sent Doug a private note about my thoughts regarding the exposure 
of this "facility" as an API of "tools" vs as solutions to specific problems.

In particular, I suggested specific annotations like @Atomic and @Sequential 
that might be put on fields or methods to suggest to the compiler and other 
external tools how the developer planned to use these parts of the software.

@Atomic, for example would say that all of this stuff needs to change 
simultaneously in the view of threads of execution.  An @Atomic method, might 
then be considered unsafe if it was not using synchronized, or if there was not 
some fence or synchronized usage implicit in the operations inside of it.

@Synchronized would be used for identifying things that needed to appear to 
change in the same order for example.

There are undoubtedly other more direct notions of management of concurrency 
that could be detailed.

I think that Doug's suggestion to put things into the Atomic/Updater apis is 
something that helps manage the "correctness" of the use and makes it much more 
probable that people will do things correctly.

Many people seem to be saying more and more, any field declaration that is not 
marked final or volatile, is wrong.  This, to me speaks about how unsure many 
developers are getting about how "correct" they can be about managing the 
lifecycle of their software so that future development doesn't "break" 
visibility or "correctness".

I think that the less "power" developers have to use this stuff "in-the-raw", 
the more opportunity we'll have of there being a solution that is recognizable 
and helpful in directing the success of concurrent software development in Java.

Gregg Wonderly

From gregg at cytetech.com  Mon Aug 24 12:19:12 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 24 Aug 2009 11:19:12 -0500
Subject: [concurrency-interest] Fences
In-Reply-To: <63b4e4050908230635r334a759cy6338dd7737828be1@mail.gmail.com>
References: <4A7C2365.2020106@cs.oswego.edu> <4A842477.3070205@cs.oswego.edu>
	<20090813234205.GO6744@linux.vnet.ibm.com>
	<4A8555E7.3070200@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>
	<4A86DC26.1070905@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>
	<4A895C40.5050207@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
	<4A9031CA.8010707@cs.oswego.edu>
	<63b4e4050908230635r334a759cy6338dd7737828be1@mail.gmail.com>
Message-ID: <4A92BD80.4060806@cytetech.com>

Tim Peierls wrote:
> On Sat, Aug 22, 2009 at 1:58 PM, Doug Lea <dl at cs.oswego.edu <mailto:dl at cs.oswego.edu>> wrote:
>     The only other Fence-hater reservations I can recall that are
>     not addressed by this surround the lack of any obvious hook to
>     help tools ensure that acquires match releases etc; which
>     probably requires language-level support.
>
> When we have tools that can intuit class concurrency policies without 
> benefit of author-added metadata, then we can worry about tools to help 
> check fence/updater uses.

What I'm hoping is that someday, field accesses will be wrapped by something 
that looks like the following, where we just "notice" that a new thread is in 
the mix, and a "fence" is asserted automatically.  Today, this is about all we 
can do.  The "@Atomic" nature of "which" and "val" here is another problem that 
needs a better solution than "synchronized".

public class FieldManager<T> {
     AtomicReference<Thread> which =
	new AtomicReference<Thread>(Thread.currentThread());
     private T val;
     public T get() {
         if( !which.compareAndSet( Thread.currentThread(),
				Thread.currentThread() ) ) {
             synchronized( this ) {
                 which.set( Thread.currentThread() );
                 return val;
             }
         }
         return val;
     }

     public void set( T val ) {
         if( !which.compareAndSet( Thread.currentThread(),
		 Thread.currentThread() ) ) {
             synchronized( this ) {
                 which.set( Thread.currentThread() );
                 this.val = val;
                 return;
             }

         }
         this.val = val;
     }
}

Gregg Wonderly


From forax at univ-mlv.fr  Mon Aug 24 13:17:53 2009
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Mon, 24 Aug 2009 19:17:53 +0200
Subject: [concurrency-interest] Fences
In-Reply-To: <4A9031CA.8010707@cs.oswego.edu>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>	<4A8555E7.3070200@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>	<4A86DC26.1070905@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>	<4A895C40.5050207@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
	<4A9031CA.8010707@cs.oswego.edu>
Message-ID: <4A92CB41.7010701@univ-mlv.fr>

Le 22/08/2009 19:58, Doug Lea a ?crit :

[...]

> However, there are a few disadvantages:
>
> * Updater constructors (actually factories) currently require
> that updatable fields be declared as "volatile".  For the above
> story to hold, this must be eliminated, which would require
> approval of a JDK spec change request. (I don't think this is a
> major obstacle since it has no impact on existing correct
> usages.)  As a consequence, ALL usages of updaters will no
> longer require volatile-ness, which might let a few new mistakes
> out there to escape detection.  For internal consistency, doing
> this will also entail a few other changes, like adding method
> "relaxedGet".

The fact that current updater requires volatile is very helpful,
As a teacher,  at least two students for each session make this error,
even if its written on the course document.
That why I have proposed another kind of updater.

Perhaps a new static factory is needed, something like
newUpdaterForNonVolatileField().

[...]

 From another mail:

> Then the steps taken in setAsIfFinal(target, value),
> which would be the same as current method lazySet,
> are:
> 1. Check that target is instance of the first class
> argument in the updater constructor.
> 2. Similarly for value as instance of second constructor argument
> 3. Check that the caller is allowed to access afield. Which,
>    if  the field is "protected" is especially messy/costly.
> 4. Issue (logical) store fence
> 5. Issue write.

about 1, 2, 3: as a part of JSR 292, the MethodHandle API allow
instead of checking access rights at each call, like reflection does,
to do that check once when the method handle is created.

// check is done here at creation of the MethodHandle
MethodHandle mh =MethodHandles.lookup().unreflectSetter(field);

// no check here
mk.<void>invoke(3);

see http://cr.openjdk.java.net/~jrose/pres/indy-javadoc-mlvm/
for more details.


I think something along that line can also be done for fences.

>
> -Doug

R?mi

From dl at cs.oswego.edu  Tue Aug 25 06:26:06 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 25 Aug 2009 06:26:06 -0400
Subject: [concurrency-interest] Fences
In-Reply-To: <4A92CB41.7010701@univ-mlv.fr>
References: <4A7C2365.2020106@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425778D0A955@GVW0436EXB.americas.hpqcorp.net>	<4A7D7618.9050501@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D0425779D4ECA9@GVW0436EXB.americas.hpqcorp.net>	<4A81559D.5000708@cs.oswego.edu>	<20090812220042.GS6779@linux.vnet.ibm.com>	<4A835979.30702@cs.oswego.edu>	<1631da7d0908121849s740e5ff6w5fcec8200b8d8bf9@mail.gmail.com>	<4A8407F0.7030708@cs.oswego.edu>	<4A842477.3070205@cs.oswego.edu>	<20090813234205.GO6744@linux.vnet.ibm.com>	<4A8555E7.3070200@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF5C9@GVW0436EXB.americas.hpqcorp.net>	<4A86DC26.1070905@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CF6D7@GVW0436EXB.americas.hpqcorp.net>	<4A895C40.5050207@cs.oswego.edu>	<238A96A773B3934685A7269CC8A8D042577A0CFCBA@GVW0436EXB.americas.hpqcorp.net>
	<4A9031CA.8010707@cs.oswego.edu> <4A92CB41.7010701@univ-mlv.fr>
Message-ID: <4A93BC3E.4010904@cs.oswego.edu>

R?mi Forax wrote:
> The fact that current updater requires volatile is very helpful,
> As a teacher,  at least two students for each session make this error,
> even if its written on the course document.
> That why I have proposed another kind of updater.

This is a a good idea; it also avoids spec-change issues.

> Perhaps a new static factory is needed, something like
> newUpdaterForNonVolatileField().

Or, "newRelaxedUpdater", borrowing the terminology
the C++0x folks adopted for these situations.

> about 1, 2, 3: as a part of JSR 292, the MethodHandle API allow
> instead of checking access rights at each call, like reflection does,
> to do that check once when the method handle is created.

There are still some low-level implementation snags here,
including the fact that compilers would need to
peek around internal logical-fences that may
otherwise disable optimized checks. But some improvements
do seem possible; I'll explore them.

The main concern remains that the actual underlying
effect of a store fence (by far the most important case)
is typically nearly a no-op, so any overhead at all hurts
enough to defeat the rationale for introducing constructs
along these lines -- people concerned about performance
will just revert to using Unsafe.

-Doug




From scottd.nerd at gmail.com  Tue Aug 25 20:55:21 2009
From: scottd.nerd at gmail.com (Scott Dixon)
Date: Tue, 25 Aug 2009 17:55:21 -0700
Subject: [concurrency-interest] checkShutdownAccess
Message-ID: <c14dbc830908251755s6f69efe9jda13957eb20814d9@mail.gmail.com>

I'm working on a system where there is a security manager in place that
denies the "modifyThread" permission globally but allows it for the thread
group my application runs within. When I use a ThreadPoolExecutor service
from 3.1 I get an a security exception:

java.security.AccessControlException: access denied
(java.lang.RuntimePermission modifyThread)
    at
java.security.AccessControlContext.checkPermission(AccessControlContext.java:278)
    at
java.security.AccessController.checkPermission(AccessController.java:466)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:537)
    at
edu.emory.mathcs.backport.java.util.concurrent.ThreadPoolExecutor.checkShutdownAccess(ThreadPoolExecutor.java:655)
    at
edu.emory.mathcs.backport.java.util.concurrent.ThreadPoolExecutor.shutdownNow(ThreadPoolExecutor.java:1330)

The documentation in the source says:

     * Permission required for callers of shutdown and shutdownNow.
     * We additionally require (see checkShutdownAccess) that callers
     * have permission to actually interrupt threads in the worker set
     * (as governed by Thread.interrupt, which relies on
     * ThreadGroup.checkAccess, which in turn relies on
     * SecurityManager.checkAccess). Shutdowns are attempted only if
     * these checks pass.

Since I do have permission to shutdown the threads that are in the
threadpool (they are all taken from my application's threadgroup) why does
the first check need to occur? In my case it is a false positive and if you
have permission to shutdown the worker threads I'm not sure why one would
need permission to shutdown any given thread on the system? Am I missing
something?

Thanks for the input.

cheers,
-scott
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090825/7e2d6f59/attachment.html>

From davidcholmes at aapt.net.au  Tue Aug 25 22:24:40 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 26 Aug 2009 12:24:40 +1000
Subject: [concurrency-interest] checkShutdownAccess
In-Reply-To: <c14dbc830908251755s6f69efe9jda13957eb20814d9@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEDMIDAA.davidcholmes@aapt.net.au>


Scott,

Here's my summary on this from early 2006. Sorry that it won't help solve your problem. The basic problem is that the security architecture is too coarse when it comes to thread permissions and we had to compromise on the side of being more strict than ideally necessary.

David Holmes
-------------

Here's my summary of the jsr-166 history on the security policy with 
regards to shutting down an executor.

We needed to be clear that shutdown might fail because the client 
performing the shutdown did not have the right permission(s). This arose 
from the context of using Executor implementations in J2EE environments.

Initially we thought perhaps an explicit new permission type was needed, 
but this would not have helped because shutdown may involve interrupting 
threads and Thread.interrupt has its own security requirements. We 
couldn't see a way to ensure that "shutdown" permission implied 
modifyThread permission or would allow checkAccess to pass for all the 
threads.

So we decided to simply tie shutdown permission to the modifyThread 
permission needed to perform a Thread.interrupt. This was far from ideal 
as depending on how the executors ThreadFactory produced threads, and 
the configured security manager, a client might have permission to 
interrupt some threads and not others. Given there was no way to account 
for all the possible permutations of behaviour we decided to work on the 
premise that you at least had to have modifyThread permission to do a 
shutdown.

So it was initially proposed that we simply perform for each worker 
thread t SecurityManager.checkAccess(t) if there is a security manager 
installed.

However, the default implementation of java.lang.SecurityManager only 
actually calls checkPermission if the thread concerned is part of the 
"rootGroup". As we were anticipating usage contexts where the threads 
most definitely would not be part of the root group, this would mean 
that the default SecurityManager would actually provide ZERO security 
and anyone could shutdown the executor. There was also no guarantee that 
a custom security manager would actually respect the modifyThread 
permission anyway when implementing checkAccess.

So the decision was made to check for a SecurityManager and if it was 
found to use that as an indication that security checks needed to be 
made but to perform a direct checkPermission with the AccessController 
to see if you had the modifyThread permission. We additionally call the 
security manager's checkAccess method for each thread to be interrupted, 
just in case we don't also have that permission (because a custom 
security manager might be stricter than the default security manager).

This led to the documented security policy:

@throws SecurityException - if a security manager exists and shutting 
down this ExecutorService may manipulate threads that the caller is not 
permitted to modify because it does not hold 
RuntimePermission("modifyThread"), or the security manager's checkAccess 
method denies access.

You must have the permission "modifyThread" AND the installed security 
manager must let checkAccess succeed. This prevents a security manager 
from ignoring the actual defined security policy.

Footnote: the fact that we called the AccessController directly was deemed to be a "bug" and so in JDK6 it was changed to always use the SecurityManager. This effectively loosened security as described above because we now have to trust the SecurityManager again.

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Scott Dixon
Sent: Wednesday, 26 August 2009 10:55 AM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] checkShutdownAccess


I'm working on a system where there is a security manager in place that denies the "modifyThread" permission globally but allows it for the thread group my application runs within. When I use a ThreadPoolExecutor service from 3.1 I get an a security exception:

java.security.AccessControlException: access denied (java.lang.RuntimePermission modifyThread)
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:278)
    at java.security.AccessController.checkPermission(AccessController.java:466)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:537)
    at edu.emory.mathcs.backport.java.util.concurrent.ThreadPoolExecutor.checkShutdownAccess(ThreadPoolExecutor.java:655)
    at edu.emory.mathcs.backport.java.util.concurrent.ThreadPoolExecutor.shutdownNow(ThreadPoolExecutor.java:1330)

The documentation in the source says: 

     * Permission required for callers of shutdown and shutdownNow.
     * We additionally require (see checkShutdownAccess) that callers
     * have permission to actually interrupt threads in the worker set
     * (as governed by Thread.interrupt, which relies on
     * ThreadGroup.checkAccess, which in turn relies on
     * SecurityManager.checkAccess). Shutdowns are attempted only if
     * these checks pass.

Since I do have permission to shutdown the threads that are in the threadpool (they are all taken from my application's threadgroup) why does the first check need to occur? In my case it is a false positive and if you have permission to shutdown the worker threads I'm not sure why one would need permission to shutdown any given thread on the system? Am I missing something?

Thanks for the input.

cheers,
-scott



From rreja2000 at yahoo.com  Fri Aug 28 09:32:23 2009
From: rreja2000 at yahoo.com (Rohit Reja)
Date: Fri, 28 Aug 2009 06:32:23 -0700 (PDT)
Subject: [concurrency-interest] Concurrent Puts on HashMap on different key
	sets.
In-Reply-To: <1466c1d60908231350o7d234d12ma02b566bbcade35e@mail.gmail.com>
Message-ID: <669838.24827.qm@web36204.mail.mud.yahoo.com>

Hello, 

Can I do concurrent puts on a HashMap safely if different threads operate on different key sets? What kind of issues can I expect to encounter?

Thanks,
Rohit



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090828/75b121f7/attachment.html>

From tim at peierls.net  Fri Aug 28 10:07:49 2009
From: tim at peierls.net (Tim Peierls)
Date: Fri, 28 Aug 2009 10:07:49 -0400
Subject: [concurrency-interest] Concurrent Puts on HashMap on different
	key sets.
In-Reply-To: <669838.24827.qm@web36204.mail.mud.yahoo.com>
References: <1466c1d60908231350o7d234d12ma02b566bbcade35e@mail.gmail.com>
	<669838.24827.qm@web36204.mail.mud.yahoo.com>
Message-ID: <63b4e4050908280707l1a59dccah5a581df10d6baa5f@mail.gmail.com>

No, you cannot. HashMap is not at all thread-safe. Examples of what might go
wrong, not in any way exhaustive: Two keys from different key sets might
hash to the same bucket, or the bucket array might be resized in one thread
while you're putting a value in another.
Just use ConcurrentHashMap.

--tim

On Fri, Aug 28, 2009 at 9:32 AM, Rohit Reja <rreja2000 at yahoo.com> wrote:

> Hello,
>
> Can I do concurrent puts on a HashMap safely if different threads operate
> on different key sets? What kind of issues can I expect to encounter?
>
> Thanks,
> Rohit
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090828/dc75f784/attachment.html>

From tim at peierls.net  Fri Aug 28 12:59:15 2009
From: tim at peierls.net (Tim Peierls)
Date: Fri, 28 Aug 2009 12:59:15 -0400
Subject: [concurrency-interest] (no subject)
In-Reply-To: <C6BDB8B6.28C1%youhumbleme@googlemail.com>
References: <63b4e4050908280707l1a59dccah5a581df10d6baa5f@mail.gmail.com>
	<C6BDB8B6.28C1%youhumbleme@googlemail.com>
Message-ID: <63b4e4050908280959w6bdc0045q92dcbdfe6c20f6c5@mail.gmail.com>

Did you mean to post this question to concurrency-interest?
--tim

On Fri, Aug 28, 2009 at 11:39 AM, George Kovoor
<youhumbleme at googlemail.com>wrote:

>
> Hi,
> I would like to know why we create 8 times more tasks than the number of
> worker threads in Parallel Array, apart from exploiting work-stealing
> technique does it provide any other benefits. Is there any results that can
> prove selection of 8 times more task would be beneficial .
>
> Thanks,
> George.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090828/19abb248/attachment.html>

From genman at noderunner.net  Fri Aug 28 13:57:20 2009
From: genman at noderunner.net (Elias Ross)
Date: Fri, 28 Aug 2009 10:57:20 -0700
Subject: [concurrency-interest] Concurrent Puts on HashMap on different
	key sets.
In-Reply-To: <63b4e4050908280707l1a59dccah5a581df10d6baa5f@mail.gmail.com>
References: <1466c1d60908231350o7d234d12ma02b566bbcade35e@mail.gmail.com>
	<669838.24827.qm@web36204.mail.mud.yahoo.com>
	<63b4e4050908280707l1a59dccah5a581df10d6baa5f@mail.gmail.com>
Message-ID: <b517768e0908281057y54ebc351o615d017122e10bfc@mail.gmail.com>

On Fri, Aug 28, 2009 at 7:07 AM, Tim Peierls <tim at peierls.net> wrote:

> No, you cannot. HashMap is not at all thread-safe. Examples of what might
> go wrong, not in any way exhaustive: Two keys from different key sets might
> hash to the same bucket, or the bucket array might be resized in one thread
> while you're putting a value in another.
>

I have also experienced one thread hanging in an infinite loop on a put. An
infinite loop is probably the worst possible thing to happen to your
application, as it not only hangs one thread but it will use most of the CPU
resources.

java.util.Hashtable is not safe to iterate over without locking. A college
said that it was, since you won't see ConcurrentModificationException thrown
from the Enumerator when iterating over the keys and values, but I can't
imagine that it is.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090828/0f5bbe8c/attachment.html>

From tim at peierls.net  Fri Aug 28 14:29:12 2009
From: tim at peierls.net (Tim Peierls)
Date: Fri, 28 Aug 2009 14:29:12 -0400
Subject: [concurrency-interest] (no subject)
In-Reply-To: <C6BDDF19.28D2%youhumbleme@googlemail.com>
References: <63b4e4050908281101i5db57875te60afc10aede72c4@mail.gmail.com>
	<C6BDDF19.28D2%youhumbleme@googlemail.com>
Message-ID: <63b4e4050908281129n1277757ck105db2130bbd04a6@mail.gmail.com>

I personally don't know the answer to your question, but I'm sure
someone in the concurrency-interest mailing list could give some kind
of answer.

Also, check the source code for ParallelArray to see if there any
implementation notes that might help.

--tim

On 8/28/09, George Kovoor <youhumbleme at googlemail.com> wrote:
> I am sorry, but if you get time would you please consider answering my
> question, I will be extremely grateful to you, this is for my MSc
> dissertation I have been doing an implementation based on Parallel Array so
> need to comment on the choice of selecting the multiplier as eight.
>
> Besides I have tried to post several questions to the group none showed up.I
> will post again.
>
> I sincerely apologies for the inconvenience caused.
>
> Thanks
> George
>
>
> On 28/08/2009 19:01, "Tim Peierls" <tim at peierls.net> wrote:
>
>> You aren't posting to a group at all. You are just sending to me, Tim
>> Peierls. I'm an individual, not a mailing list.
>>
>> --tim
>>
>> On 8/28/09, George Kovoor <youhumbleme at googlemail.com> wrote:
>>> I would really appreciate an answer to my question,I apologies if I had
>>> posted this question to the wrong group.  Parallel Array I am referring
>>> to
>>> is in extra 166y  package. Please advise me if I need to post my question
>>> in
>>> any other group.
>>> George.
>>>
>>>
>>> On 28/08/2009 17:59, "Tim Peierls" <tim at peierls.net> wrote:
>>>
>>>> Did you mean to post this question to concurrency-interest?
>>>>
>>>> --tim
>>>>
>>>> On Fri, Aug 28, 2009 at 11:39 AM, George Kovoor
>>>> <youhumbleme at googlemail.com>
>>>> wrote:
>>>>>
>>>>> Hi,
>>>>> I would like to know why we create 8 times more tasks than the number
>>>>> of
>>>>> worker threads in Parallel Array, apart from exploiting work-stealing
>>>>> technique does it provide any other benefits. Is there any results that
>>>>> can
>>>>> prove selection of 8 times more task would be beneficial .
>>>>>
>>>>> Thanks,
>>>>> George.
>>>>
>>>>
>>>
>>>
>
>
>

From dl at cs.oswego.edu  Sat Aug 29 09:44:09 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 29 Aug 2009 09:44:09 -0400
Subject: [concurrency-interest] Automating FJ task granularity control
In-Reply-To: <63b4e4050908281129n1277757ck105db2130bbd04a6@mail.gmail.com>
References: <63b4e4050908281101i5db57875te60afc10aede72c4@mail.gmail.com>	<C6BDDF19.28D2%youhumbleme@googlemail.com>
	<63b4e4050908281129n1277757ck105db2130bbd04a6@mail.gmail.com>
Message-ID: <4A9930A9.3020308@cs.oswego.edu>

George Kovoor <youhumbleme at googlemail.com> wrote:

[I think you may have initially sent this to the list
administration address (which discards such things).
It seems not have been posted.]

> I would like to know why we create 8 times more tasks than the number of
> worker threads in Parallel Array, 

Generating approximately 8 * #workers leaf tasks is one of a few
ways to balance expected mean throughput versus its expected
variance. Since this issue is likely to come up more often
as more people use FJ, here's a rundown of some of the
tradeoffs involved in automating granularity control.

In array-based processing, if you knew that all subdivided tasks
took exactly the same time to execute, that all
CPUS/cores/workers were continuously available (and not busy in
unrelated user or system processing) and that there were no
time costs in getting subtasks to the workers or joining them
upon completion, you could statically partition, generating only
one leaf task per worker (i.e., with each processing a subrange
of #elements/#workers). But in production settings, none of
these ideal assumptions are likely to hold.

Generating more (finer-granularity) tasks enables better load
balancing across violations of ideal assumptions, at the cost of
greater task overhead -- using extra tasks decreases expected
variance but also decreases expected mean throughput.  Within
reasonable bounds, task overhead is relatively low in FJ, and
the impact on throughput is less than the impact on variance.
So, using more tasks pays off as long as you stay within small
constant multiples of ideal loads, like 4, 8, or 16.  (Using
powers of two matches up with divide-and-conquer partitioning.)
On the other hand, using large multiples, or at the extreme
creating as many tasks as there are elements, will encounter
non-linearities in overhead for large arrays, mainly because of
the resulting need for more "full" garbage collections (which
among other effects may entail stopping all threads).

An alternative approach to automating decomposition thresholds is
queue-sensing.  It requires a bit more dynamic overhead but
tends to further reduce expected variance, and applies even to
those problems for which you do not know ahead of time how much
total work will be encountered.  The queue-sensing approach
starts off with three observations:

1. All other things being equal, throughput decreases with the
total time any worker is idle.

2. Workers are idle if they have no tasks of their own and
cannot steal any.

3. In a divide-and-conquer computation, initially and again upon
completion, only one worker has any tasks.

Given only these, you'd maximize throughput by ensuring that
those workers that do have any work generate enough subtasks for
others to steal. If that were the only constraint, then you
would decompose each task as finely as possible, providing the
maximal number to steal.  However, using more tasks takes more
time (and space). At the extreme, if task overhead costs were
great enough, then each worker should maintain on average,
only 1/#workers tasks (i.e., less than one), making available
only one stealable task in the entire pool.

So the optimum number of tasks that each worker should try to
make available for others to steal must lie between 1/#workers
and "as many as possible".  To narrow down, you could try
capturing the various non-ideal progress and overhead properties
mentioned above using probabilistic models, including those
accounting for the fact that workers cannot exactly maintain any
given target number of available tasks, but can only decide to
generate a task or not at particular points of their
execution. If you try modeling this, and/or perform systematic
empirical comparisons, the best answer is normally to choose a
small constant target value. Choosing 2, 3, or 4 more tasks than
there are workers that might try to steal them balances expected
mean and variance.  You can prove this more rigorously for
particular models and mean-vs-variance criteria, but this
doesn't seem very useful. The assumptions needed to model
program contexts are themselves too varied and messy to take the
resulting models very seriously.  The main observation is that
using a small fixed constant suffices across a range of such
choices.  So, in divide-and-conquer programs where you have no
further information, choosing to subdivide if
getSurplusQueuedTaskCount is greater than a small constant value
(like 3), is as good a criterion as any I know. (Although it
does seem likely that other good approximation strategies will
emerge as well, including those that take placement and memory
locality into account, which may in turn require future
enhancements in the FJ framework to efficiently support.)

On the other hand, while ForkJoinTask.getSurplusQueuedTaskCount
is pretty cheap (in part because it internally attempts
to only approximate reality), it is a bit
more expensive than just pre-determining cutoffs.  For
array-based tasks, where you do know max total work (via
#elements), it is a close call in practice whether to use
queue-sensing vs the constant-multiple-of-ideal strategy.  The
ParallelArray support classes have at various times used both,
or combinations of them, with little observable impact.

Analogous techniques apply to non-divide-and-conquer FJ
computations. For an application to graph algorithms (spanning
trees etc), see the paper we wrote about batching/aggregation
for a version of FJ underlying X10, at
   http://gee.cs.oswego.edu/dl/papers/icpp08.pdf

-Doug


From ganzhi at gmail.com  Mon Aug 31 20:31:48 2009
From: ganzhi at gmail.com (James Gan)
Date: Tue, 1 Sep 2009 08:31:48 +0800
Subject: [concurrency-interest] Concurrent Puts on HashMap on different
	key sets.
In-Reply-To: <b517768e0908281057y54ebc351o615d017122e10bfc@mail.gmail.com>
References: <1466c1d60908231350o7d234d12ma02b566bbcade35e@mail.gmail.com>
	<669838.24827.qm@web36204.mail.mud.yahoo.com>
	<63b4e4050908280707l1a59dccah5a581df10d6baa5f@mail.gmail.com>
	<b517768e0908281057y54ebc351o615d017122e10bfc@mail.gmail.com>
Message-ID: <70c070d80908311731r6a820cbeq288875af27d62523@mail.gmail.com>

The worst thing in my mind is data race. If it happens, your customer might
get wrong result without any prompt. It takes days to find such kind of bugs
without an adequate tool.

On Sat, Aug 29, 2009 at 1:57 AM, Elias Ross <genman at noderunner.net> wrote:

>
> On Fri, Aug 28, 2009 at 7:07 AM, Tim Peierls <tim at peierls.net> wrote:
>
>> No, you cannot. HashMap is not at all thread-safe. Examples of what might
>> go wrong, not in any way exhaustive: Two keys from different key sets might
>> hash to the same bucket, or the bucket array might be resized in one thread
>> while you're putting a value in another.
>>
>
> I have also experienced one thread hanging in an infinite loop on a put. An
> infinite loop is probably the worst possible thing to happen to your
> application, as it not only hangs one thread but it will use most of the CPU
> resources.
>
> java.util.Hashtable is not safe to iterate over without locking. A college
> said that it was, since you won't see ConcurrentModificationException thrown
> from the Enumerator when iterating over the keys and values, but I can't
> imagine that it is.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Best Regards
James Gan
Current Project: Concurrent Building Block at
http://amino-cbbs.sourceforge.net/
Blog: http://ganzhi.blogspot.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090901/bf3b0b2a/attachment.html>

