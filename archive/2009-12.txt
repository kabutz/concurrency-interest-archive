From dragonken at gmail.com  Wed Dec  2 21:32:06 2009
From: dragonken at gmail.com (Ken--@newsgroupstats.hk)
Date: Wed, 2 Dec 2009 18:32:06 -0800 (PST)
Subject: [concurrency-interest]  Task Queue ordering
Message-ID: <26585945.post@talk.nabble.com>


Hi All,

I have a Task Queue (ArrayBlockingQueue) with single thread for client's
task processing. For same client, I  have to guarantee the ordering of task
results as I need the result of previous task to perform next task of the
same client. It's work for ArrayBlockQueue with single thread as it's always
FIFO.

My question is, how to enable multi threads (like ThreadPoolExecutor) for
queue but maintaining the ordering of same client's tasks? I tried
PriorityBlockingQueue but it just ensure tasks are started in order.

Any solution? Please help.

Thanks and Best Regards,
Ken

-- 
View this message in context: http://old.nabble.com/Task-Queue-ordering-tp26585945p26585945.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From joe.bowbeer at gmail.com  Wed Dec  2 22:16:30 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 2 Dec 2009 19:16:30 -0800
Subject: [concurrency-interest] Task Queue ordering
In-Reply-To: <26585945.post@talk.nabble.com>
References: <26585945.post@talk.nabble.com>
Message-ID: <31f2a7bd0912021916v7712bd5btc72494f040a2b492@mail.gmail.com>

If each client's tasks are executed sequentially, you can use a
SerialExecutor per client.  See the "SerialExecutor" example in the Executor
javadoc.  Each client's executor would delegate to the shared thread pool
executor.

If all tasks can execute in concurrently, but you want to retire them in
order, I think there was a useful discussion about this in June '08.
Search for "Out-of-order execution, in-order retirement".

(The suggestion was to queue the Futures when the Callables are submitted,
and retire the tasks in the order they were enqueued.)

--Joe

On Wed, Dec 2, 2009 at 6:32 PM, Ken-- at newsgroupstats.hk wrote:

>
> Hi All,
>
> I have a Task Queue (ArrayBlockingQueue) with single thread for client's
> task processing. For same client, I  have to guarantee the ordering of task
> results as I need the result of previous task to perform next task of the
> same client. It's work for ArrayBlockQueue with single thread as it's
> always
> FIFO.
>
> My question is, how to enable multi threads (like ThreadPoolExecutor) for
> queue but maintaining the ordering of same client's tasks? I tried
> PriorityBlockingQueue but it just ensure tasks are started in order.
>
> Any solution? Please help.
>
> Thanks and Best Regards,
> Ken
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091202/c1d51341/attachment.html>

From gauravty at in.ibm.com  Thu Dec  3 17:31:16 2009
From: gauravty at in.ibm.com (Gaurav Kant Tyagi)
Date: Fri, 4 Dec 2009 04:01:16 +0530
Subject: [concurrency-interest] Gaurav Kant Tyagi is out of the office.
Message-ID: <OF59AAD68A.F563BFB7-ON65257681.007BB67F-65257681.007BB67F@in.ibm.com>

I will be out of the office starting  11/27/2009 and will not return until
12/07/2009.

I will respond to your message when I return.

CST coverage: Yogesh Kulkarni is my back-up for any project related
request.
IST coverage: Ashish Birla will be my back up for project related
activities.


From willmcqueen at yahoo.com  Fri Dec  4 14:22:48 2009
From: willmcqueen at yahoo.com (Will McQueen)
Date: Fri, 4 Dec 2009 11:22:48 -0800 (PST)
Subject: [concurrency-interest] Seeking Interruption Rule Clarification
Message-ID: <319857.41423.qm@web50101.mail.re2.yahoo.com>

Hi,

On p341 of JCIP, the Interruption Rule states:
"A thread calling interrupt on another thread happens-before the interrupted thread detects the interrupt (either by having InterruptedException thrown, or invoking isInterrupted or interrupted)"

I'm a little unclear on the specifics. Suppose that we have:
???? Threads T1, T2, and T3 are alive and the interrupt flag is clear. T3 is executing in a loop, where it does Thread.sleep(10000) and then calls T3.interrupted() to check+clear the flag status.

And suppose that we have the following global execution sequence:
1) T3 has just called Thread.sleep(10000)
2) 2 secs later, T1 calls T3.interrupt().
3) Another 2 secs later, T2 calls T3.interrupt().
4) Now T3's Thread.sleep(10000) call (from step #1) returns, and T3 calls T3.interrupted() and detects the interrupt.

In this case which of the following statements can we claim as true?
1) There's a HB edge from T1's T3.interrupt() call to T3's T3.interrupt() call.
2) There's a HB edge from T2's T3.interrupt() call to T3's T3.interrupt() call.

I would guess that we can only claim #1, because T1 is the thread that caused T3's interrupt flag state to change state from false to true. T2's call to T3.interrupt() did not cause a state change to T3's interrupt flag because it was already true before T2 called T3.interrupt(). My assumption can be summed-up as:

"The Interruption Rule results in a HB edge from T1's T3.interrupt() call to T3's T3.interrupted() call, only if T1 was the thread responsible for the state change to T3's interrupt flag".

But it gets even more complicated, because we could also have the following actual execution sequence (ie, assume that this is the "real" global execution sequence, after any reorderings) between just 2 threads, T1 and T3:

1) T3 has just called Thread.sleep(10000)

2) 2 secs later, T1 calls T3.interrupt().
3) 2 secs later, T1 executes the statement "x = 5" (stores a value into a mutable shared var)

4) 2 secs later, T1 again calls T3.interrupt()

5) Now T3's Thread.sleep(10000) call (from step #1) returns, and T3 calls T3.interrupted() and detects the interrupt.
6) T3 performs a read on var x

In this case, the same thread T1 made 2 calls to T3.interrupt() (in steps #2 & #4), but only the call from step #2 is the one that caused the actual state change to T3's interrupt flag, so supposedly this is the only state change that T3 "detected". Does that mean that there's only a HB left edge from the execution of T1's first T3.interrupt() call rather than from T1's second T3.interrupt() call? If that's true, then it seems that there can be no (transitive) HB edge between the write of var x in T1 and the read of var x in T3, in which case the value "5" might not be visible to T3's read, right?

Thank you. I appreciate your help in clarifying.

Cheers,
Will



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091204/5f5e39e1/attachment.html>

From joe.bowbeer at gmail.com  Fri Dec  4 14:56:22 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 4 Dec 2009 11:56:22 -0800
Subject: [concurrency-interest] Seeking Interruption Rule Clarification
In-Reply-To: <319857.41423.qm@web50101.mail.re2.yahoo.com>
References: <319857.41423.qm@web50101.mail.re2.yahoo.com>
Message-ID: <31f2a7bd0912041156g681d8266i5f1ada4ccf808c40@mail.gmail.com>

I think you can reason effectively about this by replacing the interrupted
flag with a volatile boolean flag.

T3: T3.flag = false
T1: T3.flag = true
T2: T3.flag = true
T3: if T3.flag then ...

The right answer, I think, is that the write to the volatile will always
"happen" (unless the compiler can deduce that a concurrent read will never
happen).  However, I'm not sure how or if the JMM specifies this.

--Joe

On Fri, Dec 4, 2009 at 11:22 AM, Will McQueen wrote:

> Hi,
>
> On p341 of JCIP, the Interruption Rule states:
> "A thread calling interrupt on another thread happens-before the
> interrupted thread detects the interrupt (either by having InterruptedException
> thrown, or invoking isInterrupted or interrupted)"
>
> I'm a little unclear on the specifics. Suppose that we have:
>      Threads T1, T2, and T3 are alive and the interrupt flag is clear. T3
> is executing in a loop, where it does Thread.sleep(10000) and then calls
> T3.interrupted() to check+clear the flag status.
>
> And suppose that we have the following global execution sequence:
> 1) T3 has just called Thread.sleep(10000)
> 2) 2 secs later, T1 calls T3.interrupt().
> 3) Another 2 secs later, T2 calls T3.interrupt().
> 4) Now T3's Thread.sleep(10000) call (from step #1) returns, and T3 calls
> T3.interrupted() and detects the interrupt.
>
> In this case which of the following statements can we claim as true?
> 1) There's a HB edge from T1's T3.interrupt() call to T3's T3.interrupt()
> call.
> 2) There's a HB edge from T2's T3.interrupt() call to T3's T3.interrupt()
> call.
>
> I would guess that we can only claim #1, because T1 is the thread that
> caused T3's interrupt flag state to change state from false to true. T2's
> call to T3.interrupt() did not cause a state change to T3's interrupt flag
> because it was already true before T2 called T3.interrupt(). My assumption
> can be summed-up as:
>
> "The Interruption Rule results in a HB edge from T1's T3.interrupt() call
> to T3's T3.interrupted() call, only if T1 was the thread responsible for the
> state change to T3's interrupt flag".
>
> But it gets even more complicated, because we could also have the following
> actual execution sequence (ie, assume that this is the "real" global
> execution sequence, after any reorderings) between just 2 threads, T1 and
> T3:
>
> 1) T3 has just called Thread.sleep(10000)
> 2) 2 secs later, T1 calls T3.interrupt().
> 3) 2 secs later, T1 executes the statement "x = 5" (stores a value into a
> mutable shared var)
> 4) 2 secs later, T1 again calls T3.interrupt()
> 5) Now T3's Thread.sleep(10000) call (from step #1) returns, and T3 calls
> T3.interrupted() and detects the interrupt.
> 6) T3 performs a read on var x
>
> In this case, the same thread T1 made 2 calls to T3.interrupt() (in steps
> #2 & #4), but only the call from step #2 is the one that caused the actual
> state change to T3's interrupt flag, so supposedly this is the only state
> change that T3 "detected". Does that mean that there's only a HB left edge
> from the execution of T1's first T3.interrupt() call rather than from T1's
> second T3.interrupt() call? If that's true, then it seems that there can be
> no (transitive) HB edge between the write of var x in T1 and the read of var
> x in T3, in which case the value "5" might not be visible to T3's read,
> right?
>
> Thank you. I appreciate your help in clarifying.
>
> Cheers,
> Will
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091204/d0bd560e/attachment.html>

From hans.boehm at hp.com  Fri Dec  4 15:53:31 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 4 Dec 2009 20:53:31 +0000
Subject: [concurrency-interest] Seeking Interruption Rule Clarification
In-Reply-To: <31f2a7bd0912041156g681d8266i5f1ada4ccf808c40@mail.gmail.com>
References: <319857.41423.qm@web50101.mail.re2.yahoo.com>
	<31f2a7bd0912041156g681d8266i5f1ada4ccf808c40@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D04257805BAC07@GVW0436EXB.americas.hpqcorp.net>

If you believe that the volatile case is equivalent, then the second bullet under 17.4.4 (JLS 3rd edition, http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.4.4) makes it clear that any prior (in a total synchronization order) write of a volatile flag synchronizes with any later read of  the synchronization variable.

In that case, both happens before  edges inthe original question exist.

I'm inclined to read the last bullet in that list as confirming this interpretation.

Hans

________________________________
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Joe Bowbeer
Sent: Friday, December 04, 2009 11:56 AM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Seeking Interruption Rule Clarification

I think you can reason effectively about this by replacing the interrupted flag with a volatile boolean flag.

T3: T3.flag = false
T1: T3.flag = true
T2: T3.flag = true
T3: if T3.flag then ...

The right answer, I think, is that the write to the volatile will always "happen" (unless the compiler can deduce that a concurrent read will never happen).  However, I'm not sure how or if the JMM specifies this.

--Joe

On Fri, Dec 4, 2009 at 11:22 AM, Will McQueen wrote:
Hi,

On p341 of JCIP, the Interruption Rule states:
"A thread calling interrupt on another thread happens-before the interrupted thread detects the interrupt (either by having InterruptedException thrown, or invoking isInterrupted or interrupted)"

I'm a little unclear on the specifics. Suppose that we have:
     Threads T1, T2, and T3 are alive and the interrupt flag is clear. T3 is executing in a loop, where it does Thread.sleep(10000) and then calls T3.interrupted() to check+clear the flag status.

And suppose that we have the following global execution sequence:
1) T3 has just called Thread.sleep(10000)
2) 2 secs later, T1 calls T3.interrupt().
3) Another 2 secs later, T2 calls T3.interrupt().
4) Now T3's Thread.sleep(10000) call (from step #1) returns, and T3 calls T3.interrupted() and detects the interrupt.

In this case which of the following statements can we claim as true?
1) There's a HB edge from T1's T3.interrupt() call to T3's T3.interrupt() call.
2) There's a HB edge from T2's T3.interrupt() call to T3's T3.interrupt() call.

I would guess that we can only claim #1, because T1 is the thread that caused T3's interrupt flag state to change state from false to true. T2's call to T3.interrupt() did not cause a state change to T3's interrupt flag because it was already true before T2 called T3.interrupt(). My assumption can be summed-up as:

"The Interruption Rule results in a HB edge from T1's T3.interrupt() call to T3's T3.interrupted() call, only if T1 was the thread responsible for the state change to T3's interrupt flag".

But it gets even more complicated, because we could also have the following actual execution sequence (ie, assume that this is the "real" global execution sequence, after any reorderings) between just 2 threads, T1 and T3:

1) T3 has just called Thread.sleep(10000)
2) 2 secs later, T1 calls T3.interrupt().
3) 2 secs later, T1 executes the statement "x = 5" (stores a value into a mutable shared var)
4) 2 secs later, T1 again calls T3.interrupt()
5) Now T3's Thread.sleep(10000) call (from step #1) returns, and T3 calls T3.interrupted() and detects the interrupt.
6) T3 performs a read on var x

In this case, the same thread T1 made 2 calls to T3.interrupt() (in steps #2 & #4), but only the call from step #2 is the one that caused the actual state change to T3's interrupt flag, so supposedly this is the only state change that T3 "detected". Does that mean that there's only a HB left edge from the execution of T1's first T3.interrupt() call rather than from T1's second T3.interrupt() call? If that's true, then it seems that there can be no (transitive) HB edge between the write of var x in T1 and the read of var x in T3, in which case the value "5" might not be visible to T3's read, right?

Thank you. I appreciate your help in clarifying.

Cheers,
Will


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091204/97d3946b/attachment-0001.html>

From dragonken at gmail.com  Sat Dec  5 00:51:23 2009
From: dragonken at gmail.com (Ken--@newsgroupstats.hk)
Date: Fri, 4 Dec 2009 21:51:23 -0800 (PST)
Subject: [concurrency-interest] Task Queue ordering
In-Reply-To: <31f2a7bd0912021916v7712bd5btc72494f040a2b492@mail.gmail.com>
References: <26585945.post@talk.nabble.com>
	<31f2a7bd0912021916v7712bd5btc72494f040a2b492@mail.gmail.com>
Message-ID: <26653201.post@talk.nabble.com>


Dear Joe,

Sorry I cannot under how to achieve it. I have tasks to be submitted in
order (task1 to task6) like this:

public class SerialTask implements Runnable {
    private String client = null;
    private long sleep = 0;
    public SerialTask(String client, long sleep){
        this.client = client;
        this.sleep = sleep;
    }
    public void run() {
        try {Thread.sleep(sleep);} catch (Exception e) {}
        String msg = "Task ID = " + client + ", sleep = " + sleep;
        System.out.println(msg);
    }
}

SerialTask task1 = new SerialTask("client-a", 10);
SerialTask task2 = new SerialTask("client-a", 10);
SerialTask task3 = new SerialTask("client-a", 10);
SerialTask task4 = new SerialTask("client-b", 2000);
SerialTask task5 = new SerialTask("client-b", 4000);
SerialTask task6 = new SerialTask("client-a", 10);

How can I make use of SerialExecutor to execute client's task in order? I
expected client-a's tasks can be finished in 40 mills and tasks are run in
serial. and after around (2000 + 4000 - 40) mills, all client-b tasks are
finished. 

Regards,
Ken




Joe Bowbeer wrote:
> 
> If each client's tasks are executed sequentially, you can use a
> SerialExecutor per client.  See the "SerialExecutor" example in the
> Executor
> javadoc.  Each client's executor would delegate to the shared thread pool
> executor.
> 
> If all tasks can execute in concurrently, but you want to retire them in
> order, I think there was a useful discussion about this in June '08.
> Search for "Out-of-order execution, in-order retirement".
> 
> (The suggestion was to queue the Futures when the Callables are submitted,
> and retire the tasks in the order they were enqueued.)
> 
> --Joe
> 
> On Wed, Dec 2, 2009 at 6:32 PM, Ken-- at newsgroupstats.hk wrote:
> 
>>
>> Hi All,
>>
>> I have a Task Queue (ArrayBlockingQueue) with single thread for client's
>> task processing. For same client, I  have to guarantee the ordering of
>> task
>> results as I need the result of previous task to perform next task of the
>> same client. It's work for ArrayBlockQueue with single thread as it's
>> always
>> FIFO.
>>
>> My question is, how to enable multi threads (like ThreadPoolExecutor) for
>> queue but maintaining the ordering of same client's tasks? I tried
>> PriorityBlockingQueue but it just ensure tasks are started in order.
>>
>> Any solution? Please help.
>>
>> Thanks and Best Regards,
>> Ken
>>
>>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 

-- 
View this message in context: http://old.nabble.com/Task-Queue-ordering-tp26585945p26653201.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From joe.bowbeer at gmail.com  Sat Dec  5 02:25:42 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 4 Dec 2009 23:25:42 -0800
Subject: [concurrency-interest] Task Queue ordering
In-Reply-To: <26653201.post@talk.nabble.com>
References: <26585945.post@talk.nabble.com>
	<31f2a7bd0912021916v7712bd5btc72494f040a2b492@mail.gmail.com>
	<26653201.post@talk.nabble.com>
Message-ID: <31f2a7bd0912042325o147169aeq25329cc01460fc4c@mail.gmail.com>

Ken,

I'm imagining a serial executor per client:

ExecutorService pool = Executors.newCachedThreadPool();

Executor execA = new SerialExecutor(pool);
Executor execB = new SerialExecutor(pool);

SerialTask task1 = new SerialTask("client-a", 10);
SerialTask task2 = new SerialTask("client-a", 10);
SerialTask task3 = new SerialTask("client-a", 10);
SerialTask task4 = new SerialTask("client-b", 2000);
SerialTask task5 = new SerialTask("client-b", 4000);
SerialTask task6 = new SerialTask("client-a", 10);

execA.execute(task1);
execA.execute(task2);
execA.execute(task3);
execB.execute(task4);
execB.execute(task5);
execA.execute(task6);

--Joe

On Fri, Dec 4, 2009 at 9:51 PM, Ken-- at newsgroupstats.hk wrote:

>
> Dear Joe,
>
> Sorry I cannot under how to achieve it. I have tasks to be submitted in
> order (task1 to task6) like this:
>
> public class SerialTask implements Runnable {
>    private String client = null;
>    private long sleep = 0;
>    public SerialTask(String client, long sleep){
>        this.client = client;
>        this.sleep = sleep;
>    }
>    public void run() {
>        try {Thread.sleep(sleep);} catch (Exception e) {}
>        String msg = "Task ID = " + client + ", sleep = " + sleep;
>        System.out.println(msg);
>    }
> }
>
> SerialTask task1 = new SerialTask("client-a", 10);
> SerialTask task2 = new SerialTask("client-a", 10);
> SerialTask task3 = new SerialTask("client-a", 10);
> SerialTask task4 = new SerialTask("client-b", 2000);
> SerialTask task5 = new SerialTask("client-b", 4000);
> SerialTask task6 = new SerialTask("client-a", 10);
>
> How can I make use of SerialExecutor to execute client's task in order? I
> expected client-a's tasks can be finished in 40 mills and tasks are run in
> serial. and after around (2000 + 4000 - 40) mills, all client-b tasks are
> finished.
>
> Regards,
> Ken
>
>
>
>
> Joe Bowbeer wrote:
> >
> > If each client's tasks are executed sequentially, you can use a
> > SerialExecutor per client.  See the "SerialExecutor" example in the
> > Executor
> > javadoc.  Each client's executor would delegate to the shared thread pool
> > executor.
> >
> > If all tasks can execute in concurrently, but you want to retire them in
> > order, I think there was a useful discussion about this in June '08.
> > Search for "Out-of-order execution, in-order retirement".
> >
> > (The suggestion was to queue the Futures when the Callables are
> submitted,
> > and retire the tasks in the order they were enqueued.)
> >
> > --Joe
> >
> > On Wed, Dec 2, 2009 at 6:32 PM, Ken-- at newsgroupstats.hk wrote:
> >
> >>
> >> Hi All,
> >>
> >> I have a Task Queue (ArrayBlockingQueue) with single thread for client's
> >> task processing. For same client, I  have to guarantee the ordering of
> >> task
> >> results as I need the result of previous task to perform next task of
> the
> >> same client. It's work for ArrayBlockQueue with single thread as it's
> >> always
> >> FIFO.
> >>
> >> My question is, how to enable multi threads (like ThreadPoolExecutor)
> for
> >> queue but maintaining the ordering of same client's tasks? I tried
> >> PriorityBlockingQueue but it just ensure tasks are started in order.
> >>
> >> Any solution? Please help.
> >>
> >> Thanks and Best Regards,
> >> Ken
> >>
> >>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091204/6c251c94/attachment.html>

From davidcholmes at aapt.net.au  Sun Dec  6 22:23:35 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 7 Dec 2009 13:23:35 +1000
Subject: [concurrency-interest] Seeking Interruption Rule Clarification
In-Reply-To: <319857.41423.qm@web50101.mail.re2.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEHDIEAA.davidcholmes@aapt.net.au>

Will,

> But it gets even more complicated, because we could also have the
> following actual execution sequence (ie, assume that this is the
> "real" global execution sequence, after any reorderings) between
> just 2 threads, T1 and T3:
>
> 1) T3 has just called Thread.sleep(10000)
> 2) 2 secs later, T1 calls T3.interrupt().
> 3) 2 secs later, T1 executes the statement "x = 5" (stores a
> value into a mutable shared var)
> 4) 2 secs later, T1 again calls T3.interrupt()
> 5) Now T3's Thread.sleep(10000) call (from step #1) returns, and
> T3 calls T3.interrupted() and detects the interrupt.
> 6) T3 performs a read on var x
>
> In this case, the same thread T1 made 2 calls to T3.interrupt()
> (in steps #2 & #4), but only the call from step #2 is the one
> that caused the actual state change to T3's interrupt flag, so
> supposedly this is the only state change that T3 "detected". Does
> that mean that there's only a HB left edge from the execution of
> T1's first T3.interrupt() call rather than from T1's second
> T3.interrupt() call? If that's true, then it seems that there can
> be no (transitive) HB edge between the write of var x in T1 and
> the read of var x in T3, in which case the value "5" might not be
> visible to T3's read, right?

Maybe. The problem is that JMM rule is really only considering the case
where the actual interrupt state changes. In the scenario given the second
interrupt call might be implemented as a no-op if the system can see that
the target is already in the interrupted state. In practice on Hotspot it
isn't a complete no-op so you're in fact likely to get the necessary memory
synchronization to see the value "5".

The JMM would have to specify the interaction here in a lot more detail to
be able to reason in the abstract about what is guaranteed in this case.
Even treating the interrupted state as a volatile does not specify it
precisely enough as you need to know if interrupt() is required to always
write to the volatile or can elide it if it sees the thread is already
interrupted.

Cheers,
David Holmes

> Thank you. I appreciate your help in clarifying.
>
> Cheers,
> Will


From raghuram.nidagal at gmail.com  Tue Dec  8 09:12:14 2009
From: raghuram.nidagal at gmail.com (raghuram nidagal)
Date: Tue, 8 Dec 2009 19:42:14 +0530
Subject: [concurrency-interest] out of order execution on uniprocessor
Message-ID: <7874b1f60912080612o167ef822h48423f70383a9c7a@mail.gmail.com>

Hi,
Is "out of order execution" relevant in case of a single processor. I see
the following extracts relating to single processors. Would appreciate if
somebody could clarify
what can be expected in a single processor case.

Discussing this in terms of caches, it may sound as if these issues only
affect multiprocessor machines. However, the reordering effects can be
easily seen on a single processor. It is not possible, for example, for the
compiler to move your code before an acquire or after a release. When we say
that acquires and releases act on caches, we are using shorthand for a
number of possible effects.
http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#reordering


If you are generating code that is guaranteed to only run on a uniprocessor,
then you can probably skip the rest of this section. Because uniprocessors
preserve apparent sequential consistency, you never need to issue barriers
unless object memory is somehow shared with asynchrononously accessible IO
memory. This might occur with specially mapped java.nio buffers, but
probably only in ways that affect internal JVM support code, not Java code.
Also, it is conceivable that some special barriers would be needed if
context switching doesn't entail sufficient synchronization.

http://gee.cs.oswego.edu/dl/jmm/cookbook.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091208/96f1bbaa/attachment.html>

From hans.boehm at hp.com  Tue Dec  8 13:37:36 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 8 Dec 2009 18:37:36 +0000
Subject: [concurrency-interest] out of order execution on uniprocessor
In-Reply-To: <7874b1f60912080612o167ef822h48423f70383a9c7a@mail.gmail.com>
References: <7874b1f60912080612o167ef822h48423f70383a9c7a@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D0425780628DC4@GVW0436EXB.americas.hpqcorp.net>

If you are writing Java code, it doesn't matter whether you're on a uniprocessor.  The hardware generally won't visibly reorder memory accesses on a uniprocessor, but the compiler still will.  And you'd be hard pressed to tell the difference.

If you are building a JVM, then it does matter.  Since the hardware doesn't reorder, you can take some shortcuts.

Both of the excerpts below are correct, but they're targeting a different audience.

Hans

P.S. I do think there are a few statements in the cookbook that are in need of an update.  The ppc and pa-risc situations are more complex than what's stated there.  (The PA_RISC recipe works for actual hardware, but I think the emulator on Itanium follows the rules inthe manual, which allow more reordering.  For PPC, I think the recipes for sequentially consistent operations in http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2745.html also apply to Java volatiles and are more accurate.)

________________________________
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of raghuram nidagal
Sent: Tuesday, December 08, 2009 6:12 AM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] out of order execution on uniprocessor

Hi,
Is "out of order execution" relevant in case of a single processor. I see the following extracts relating to single processors. Would appreciate if somebody could clarify
what can be expected in a single processor case.

Discussing this in terms of caches, it may sound as if these issues only affect multiprocessor machines. However, the reordering effects can be easily seen on a single processor. It is not possible, for example, for the compiler to move your code before an acquire or after a release. When we say that acquires and releases act on caches, we are using shorthand for a number of possible effects.
http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#reordering


If you are generating code that is guaranteed to only run on a uniprocessor, then you can probably skip the rest of this section. Because uniprocessors preserve apparent sequential consistency, you never need to issue barriers unless object memory is somehow shared with asynchrononously accessible IO memory. This might occur with specially mapped java.nio buffers, but probably only in ways that affect internal JVM support code, not Java code. Also, it is conceivable that some special barriers would be needed if context switching doesn't entail sufficient synchronization.

http://gee.cs.oswego.edu/dl/jmm/cookbook.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091208/f56f5590/attachment.html>

From alarmnummer at gmail.com  Thu Dec 10 08:55:31 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 10 Dec 2009 14:55:31 +0100
Subject: [concurrency-interest] out of order execution on uniprocessor
In-Reply-To: <238A96A773B3934685A7269CC8A8D0425780628DC4@GVW0436EXB.americas.hpqcorp.net>
References: <7874b1f60912080612o167ef822h48423f70383a9c7a@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D0425780628DC4@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <1466c1d60912100555p347485a7q4e0a96facfe67009@mail.gmail.com>

Some time ago I placed a blogpost about this topic:

http://pveentjer.wordpress.com/2008/10/27/jmm-single-processor-can-also-suffer-from-visibility-problems/

On Tue, Dec 8, 2009 at 7:37 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
> If you are writing Java code, it doesn't matter whether you're on a
> uniprocessor.? The hardware generally won't visibly reorder memory accesses
> on a uniprocessor, but the compiler still will.? And you'd be hard pressed
> to tell the difference.
>
> If you are building a JVM, then it does matter.? Since the hardware doesn't
> reorder, you can take some shortcuts.
>
> Both of the excerpts below are correct, but they're targeting a different
> audience.
>
> Hans
>
> P.S. I do think there are a few statements in the cookbook that are in need
> of an update.? The ppc and pa-risc situations?are more complex than what's
> stated there.? (The PA_RISC recipe works for actual hardware, but I think
> the emulator on Itanium follows the rules inthe manual, which allow more
> reordering.? For PPC, I think the recipes for sequentially consistent
> operations in
> http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2745.html?also
> apply to Java volatiles and are more accurate.)
>
> ________________________________
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of raghuram
> nidagal
> Sent: Tuesday, December 08, 2009 6:12 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] out of order execution on uniprocessor
>
> Hi,
> Is "out of order execution" relevant in case of a single processor. I see
> the following extracts relating to single processors. Would appreciate if
> somebody could clarify
> what can be expected in a single processor case.
> Discussing this in terms of caches, it may sound as if these issues only
> affect multiprocessor machines. However, the reordering effects can be
> easily seen on a single processor. It is not possible, for example, for the
> compiler to move your code before an acquire or after a release. When we say
> that acquires and releases act on caches, we are using shorthand for a
> number of possible effects.
> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#reordering
>
> If you are generating code that is guaranteed to only run on a uniprocessor,
> then you can probably skip the rest of this section. Because uniprocessors
> preserve apparent sequential consistency, you never need to issue barriers
> unless object memory is somehow shared with asynchrononously accessible IO
> memory. This might occur with specially mapped java.nio buffers, but
> probably only in ways that affect internal JVM support code, not Java code.
> Also, it is conceivable that some special barriers would be needed if
> context switching doesn't entail sufficient synchronization.
>
> http://gee.cs.oswego.edu/dl/jmm/cookbook.html
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From kus.bank at gmail.com  Sat Dec 12 15:21:36 2009
From: kus.bank at gmail.com (bank kus)
Date: Sat, 12 Dec 2009 12:21:36 -0800
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
Message-ID: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>

Is this guaranteed to complete in a finite number of steps. Asking
because am evaluating if I can implement ticket locking using this.

On a 16 virtual core system if all 16 vcores simultaneously had
threads invoke this operation ..
<a> are all vcores guaranteed to block context switching to any other
threads on their runqueues until this operation completes

<b> is there fair arbitration between cores. The pathological case
being vcore 2 gets to increment  then gets to do it again while other
vcores wait trying to do so.

I understand this would depend upon the hw instructions used but am
curious if any fair expectation can be made out of this API.

Regards
banks

From kus.bank at gmail.com  Sat Dec 12 19:03:43 2009
From: kus.bank at gmail.com (bank kus)
Date: Sat, 12 Dec 2009 16:03:43 -0800
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>
References: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>
Message-ID: <9a3ed5720912121603ic5617f8sa6ae3225b0d7ec37@mail.gmail.com>

Btw it seems I can avoid this whole dependence on fetch_and_increment by
using timestamp (there appears to be a lot of flame about how fast and
efficient gettimeofday is on various OSes need to catch up)

But the thing I realized is that in the strictest form, ticket locking has a
problem. It is really a spinlock where one thread can succeed and `n' others
are doomed to failure by virtue of ordering. yet the doomed threads will
occupy context timeslice on the processor by pause or continous spin cycles.

Instead if I did group ticketing (all threads with tickets less than 40 can
go for the spinlock now whoever wins wins ) ones above 40 please wait till
the current ticket is 40, this provides a bracket of fairness and doesnt
restrict performance.  If I replaced the fetchAndIncrement with gettimeofday
based counter then the two problems seem to gel well and I get to avoid the
CAS too.

<just random thoughts never mind .... :-)>
banks

On Sat, Dec 12, 2009 at 12:21 PM, bank kus <kus.bank at gmail.com> wrote:

> Is this guaranteed to complete in a finite number of steps. Asking
> because am evaluating if I can implement ticket locking using this.
>
> On a 16 virtual core system if all 16 vcores simultaneously had
> threads invoke this operation ..
> <a> are all vcores guaranteed to block context switching to any other
> threads on their runqueues until this operation completes
>
> <b> is there fair arbitration between cores. The pathological case
> being vcore 2 gets to increment  then gets to do it again while other
> vcores wait trying to do so.
>
> I understand this would depend upon the hw instructions used but am
> curious if any fair expectation can be made out of this API.
>
> Regards
> banks
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
banks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091212/0e153c0d/attachment.html>

From Hans.Boehm at hp.com  Sat Dec 12 21:46:14 2009
From: Hans.Boehm at hp.com (Hans Boehm)
Date: Sat, 12 Dec 2009 18:46:14 -0800 (PST)
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>
References: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>
Message-ID: <alpine.LNX.1.10.0912121840260.12500@192.168.2.2>



On Sat, 12 Dec 2009, bank kus wrote:

> Is this guaranteed to complete in a finite number of steps. Asking
> because am evaluating if I can implement ticket locking using this.
>
> On a 16 virtual core system if all 16 vcores simultaneously had
> threads invoke this operation ..
> <a> are all vcores guaranteed to block context switching to any other
> threads on their runqueues until this operation completes
>
> <b> is there fair arbitration between cores. The pathological case
> being vcore 2 gets to increment  then gets to do it again while other
> vcores wait trying to do so.
>
> I understand this would depend upon the hw instructions used but am
> curious if any fair expectation can be made out of this API.
>
Given that that the JLS is intentionally silent on any sort of fairness 
guarantee for thread scheduling, I doubt that you can guarantee anything 
of this kind without making some platform assumptions.  I expect that 
general purpose implementations and most hardware will generally do the 
right thing ...

Hans

From jed at atlassian.com  Sun Dec 13 18:35:40 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Mon, 14 Dec 2009 10:35:40 +1100
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <alpine.LNX.1.10.0912121840260.12500@192.168.2.2>
References: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>
	<alpine.LNX.1.10.0912121840260.12500@192.168.2.2>
Message-ID: <4B257A4C.7000206@atlassian.com>

This is definitely platform dependent, related to how the hardware 
implements the CAS instruction, and specifically how the cache coherence 
protocols work. Under heavy contention, CAS based algorithms tend to 
degrade exponentially (or at least, worse than linearly as failed set 
operations invalidate cache entries, so even an otherwise successful set 
will need to reload from main memory. I am pretty sure that on most 
current hardware designs this is not a wait-free algorithm.

For a thorough explanation, see The Art of Multiprocessor Programming 
(Herlihy & Shavit) for the performance problems of a TASLock (7.2, 7.3 
and App.B) which is a CAS based lock implementation.

cheers,
jed.

Hans Boehm wrote:
>
>
> On Sat, 12 Dec 2009, bank kus wrote:
>
>> Is this guaranteed to complete in a finite number of steps. Asking
>> because am evaluating if I can implement ticket locking using this.
>>
>> On a 16 virtual core system if all 16 vcores simultaneously had
>> threads invoke this operation ..
>> <a> are all vcores guaranteed to block context switching to any other
>> threads on their runqueues until this operation completes
>>
>> <b> is there fair arbitration between cores. The pathological case
>> being vcore 2 gets to increment  then gets to do it again while other
>> vcores wait trying to do so.
>>
>> I understand this would depend upon the hw instructions used but am
>> curious if any fair expectation can be made out of this API.
>>
> Given that that the JLS is intentionally silent on any sort of 
> fairness guarantee for thread scheduling, I doubt that you can 
> guarantee anything of this kind without making some platform 
> assumptions.  I expect that general purpose implementations and most 
> hardware will generally do the right thing ...
>
> Hans
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Sun Dec 13 19:19:32 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 14 Dec 2009 10:19:32 +1000
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEIGIEAA.davidcholmes@aapt.net.au>

> bank kus writes
> Is this guaranteed to complete in a finite number of steps. Asking
> because am evaluating if I can implement ticket locking using this.

You can not analyse a CAS use in isolation. You have to see the complete
code path that uses the CAS ie any looping   behaviour involving the code
that employs the CAS. You also can't analyze it without knowing how many
cores/processors are available and how many threads will execute code that
performs the CAS on the same location. You also need to know when preemption
may occur.

Simple answer is that there are no absolute guarantees. But you also don't
generally require such a guarantee.

> On a 16 virtual core system if all 16 vcores simultaneously had
> threads invoke this operation ..
> <a> are all vcores guaranteed to block context switching to any other
> threads on their runqueues until this operation completes
>
> <b> is there fair arbitration between cores. The pathological case
> being vcore 2 gets to increment  then gets to do it again while other
> vcores wait trying to do so.

I can't answer this but I don't think its the right question anyway. A CAS
is typically used in a loop:

while (true) {
  state i = read_initial_state();
  state n = compute_next_state(i);
  if (CAS(state, i, n))
     break;
}

So the semantics of the CAS itself if executed concurrently is not really
the issue. If all 16 cores execute a CAS on the same location, one will
succeed and 15 will fail. Your question is really: if 16 threads on 16 cores
execute the loop above is there any bound on the number of loop iterations
each thread may need to perform?

And that isn't a question that can be answered without doing a detailed
analysis of the system. A simple analysis says the worst case would be 16
iterations: 15 failures from interference on other cores plus success. But
what if during the 15 failures the first interfering thread can loop through
the higher-level code that performs the above CAS-loop such that it retries
the CAS-loop again? You can imagine pathological harmonic relationships that
could cause one thread to always fail in its CAS-loop. Is it likely? No. Can
it happen? Depends on the context. What's the worst case? Depends on the
context.

This is one reason why spin-locks often degrade to blocking locks after a
certain failure threshold is reached.

David Holmes


> I understand this would depend upon the hw instructions used but am
> curious if any fair expectation can be made out of this API.
>
> Regards
> banks
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From kus.bank at gmail.com  Sun Dec 13 19:39:38 2009
From: kus.bank at gmail.com (bank kus)
Date: Sun, 13 Dec 2009 16:39:38 -0800
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEIGIEAA.davidcholmes@aapt.net.au>
References: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEIGIEAA.davidcholmes@aapt.net.au>
Message-ID: <5f55507e0912131639w376069c4kc0580ea443e500c2@mail.gmail.com>

It is understood the algorithm is not wait-free if it does "loop / CAS
/ add"  the question is more
<a> if the hw version of lock:fetchAndAdd is wait-free
<b> if yes does such an instruction exist on all known platforms
<c> does the AtomicInteger.getandIncrement make any fairness guarantees.

Based on the responses I have received so far from different places
<a> x86 possibly yes
<b> NO!!
<c> Definitely NOT!!

Thanks everybody...

banks

On Sun, Dec 13, 2009 at 4:19 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>> bank kus writes
>> Is this guaranteed to complete in a finite number of steps. Asking
>> because am evaluating if I can implement ticket locking using this.
>
> You can not analyse a CAS use in isolation. You have to see the complete
> code path that uses the CAS ie any looping ? behaviour involving the code
> that employs the CAS. You also can't analyze it without knowing how many
> cores/processors are available and how many threads will execute code that
> performs the CAS on the same location. You also need to know when preemption
> may occur.
>
> Simple answer is that there are no absolute guarantees. But you also don't
> generally require such a guarantee.
>
>> On a 16 virtual core system if all 16 vcores simultaneously had
>> threads invoke this operation ..
>> <a> are all vcores guaranteed to block context switching to any other
>> threads on their runqueues until this operation completes
>>
>> <b> is there fair arbitration between cores. The pathological case
>> being vcore 2 gets to increment ?then gets to do it again while other
>> vcores wait trying to do so.
>
> I can't answer this but I don't think its the right question anyway. A CAS
> is typically used in a loop:
>
> while (true) {
> ?state i = read_initial_state();
> ?state n = compute_next_state(i);
> ?if (CAS(state, i, n))
> ? ? break;
> }
>
> So the semantics of the CAS itself if executed concurrently is not really
> the issue. If all 16 cores execute a CAS on the same location, one will
> succeed and 15 will fail. Your question is really: if 16 threads on 16 cores
> execute the loop above is there any bound on the number of loop iterations
> each thread may need to perform?
>
> And that isn't a question that can be answered without doing a detailed
> analysis of the system. A simple analysis says the worst case would be 16
> iterations: 15 failures from interference on other cores plus success. But
> what if during the 15 failures the first interfering thread can loop through
> the higher-level code that performs the above CAS-loop such that it retries
> the CAS-loop again? You can imagine pathological harmonic relationships that
> could cause one thread to always fail in its CAS-loop. Is it likely? No. Can
> it happen? Depends on the context. What's the worst case? Depends on the
> context.
>
> This is one reason why spin-locks often degrade to blocking locks after a
> certain failure threshold is reached.
>
> David Holmes
>
>
>> I understand this would depend upon the hw instructions used but am
>> curious if any fair expectation can be made out of this API.
>>
>> Regards
>> banks
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From davidcholmes at aapt.net.au  Sun Dec 13 19:45:39 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 14 Dec 2009 10:45:39 +1000
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <5f55507e0912131639w376069c4kc0580ea443e500c2@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEIGIEAA.davidcholmes@aapt.net.au>

Note that AtomicInteger.getandIncrement uses a CAS-loop, it does not use an
atomic fetch-and-add instruction even if it exists in the HW.

David

> -----Original Message-----
> From: kutty.banerjee at gmail.com [mailto:kutty.banerjee at gmail.com]On
> Behalf Of bank kus
> Sent: Monday, 14 December 2009 10:40 AM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] AtomicInteger.getAndIncrement()
>
>
> It is understood the algorithm is not wait-free if it does "loop / CAS
> / add"  the question is more
> <a> if the hw version of lock:fetchAndAdd is wait-free
> <b> if yes does such an instruction exist on all known platforms
> <c> does the AtomicInteger.getandIncrement make any fairness guarantees.
>
> Based on the responses I have received so far from different places
> <a> x86 possibly yes
> <b> NO!!
> <c> Definitely NOT!!
>
> Thanks everybody...
>
> banks
>
> On Sun, Dec 13, 2009 at 4:19 PM, David Holmes
> <davidcholmes at aapt.net.au> wrote:
> >> bank kus writes
> >> Is this guaranteed to complete in a finite number of steps. Asking
> >> because am evaluating if I can implement ticket locking using this.
> >
> > You can not analyse a CAS use in isolation. You have to see the complete
> > code path that uses the CAS ie any looping ? behaviour
> involving the code
> > that employs the CAS. You also can't analyze it without knowing how many
> > cores/processors are available and how many threads will
> execute code that
> > performs the CAS on the same location. You also need to know
> when preemption
> > may occur.
> >
> > Simple answer is that there are no absolute guarantees. But you
> also don't
> > generally require such a guarantee.
> >
> >> On a 16 virtual core system if all 16 vcores simultaneously had
> >> threads invoke this operation ..
> >> <a> are all vcores guaranteed to block context switching to any other
> >> threads on their runqueues until this operation completes
> >>
> >> <b> is there fair arbitration between cores. The pathological case
> >> being vcore 2 gets to increment ?then gets to do it again while other
> >> vcores wait trying to do so.
> >
> > I can't answer this but I don't think its the right question
> anyway. A CAS
> > is typically used in a loop:
> >
> > while (true) {
> > ?state i = read_initial_state();
> > ?state n = compute_next_state(i);
> > ?if (CAS(state, i, n))
> > ? ? break;
> > }
> >
> > So the semantics of the CAS itself if executed concurrently is
> not really
> > the issue. If all 16 cores execute a CAS on the same location, one will
> > succeed and 15 will fail. Your question is really: if 16
> threads on 16 cores
> > execute the loop above is there any bound on the number of loop
> iterations
> > each thread may need to perform?
> >
> > And that isn't a question that can be answered without doing a detailed
> > analysis of the system. A simple analysis says the worst case
> would be 16
> > iterations: 15 failures from interference on other cores plus
> success. But
> > what if during the 15 failures the first interfering thread can
> loop through
> > the higher-level code that performs the above CAS-loop such
> that it retries
> > the CAS-loop again? You can imagine pathological harmonic
> relationships that
> > could cause one thread to always fail in its CAS-loop. Is it
> likely? No. Can
> > it happen? Depends on the context. What's the worst case? Depends on the
> > context.
> >
> > This is one reason why spin-locks often degrade to blocking
> locks after a
> > certain failure threshold is reached.
> >
> > David Holmes
> >
> >
> >> I understand this would depend upon the hw instructions used but am
> >> curious if any fair expectation can be made out of this API.
> >>
> >> Regards
> >> banks
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >



From willmcqueen at yahoo.com  Mon Dec 14 03:34:59 2009
From: willmcqueen at yahoo.com (Will McQueen)
Date: Mon, 14 Dec 2009 00:34:59 -0800 (PST)
Subject: [concurrency-interest] Are synchronized methods safer than
	synchronized blocks?
Message-ID: <654412.59564.qm@web50105.mail.re2.yahoo.com>

Hi,

Are synchronized methods safer than synchronized blocks, in terms of guaranteeing that any monitors held by a thread will be released before that thread dies?


My understanding is this:

1) The monitor that is acquired when a thread enters a synchronized instance *method* or synchronized static *method* is guaranteed to be released when the method's frame is popped off that thread's stack (provided only that the JVM doesn't crash). That is, the monitor is absolutely guaranteed to be released when the method returns, whether it returned normally, or abnormally (via thrown exception). With synchronized methods, the JVM acquires/releases the monitor on behalf of the thread, since there are no corresponding monitorenter/monitorexit bytecodes for synchronized methods (only for synchronized blocks).

2) A synchronized *block* compiles into Java bytecodes in which a catch-all catch clause (inserted by the compiler) catches any thrown exception (a Throwable?). This compiler-generated catch clause calls
 'monitorexit' and then rethrows the exception with 'athrow'.



On p508 of "Inside the Java Virtual Machine, 2nd ed" [Venners], it says "No matter how the synchronized block is exited, the object lock acquired when the thread entered the block will definitely be released". I'm not so sure about that, but I'm hoping to be proven wrong. I believe that using a synchronized block results in a weaker guarantee of releasing a monitor than when using a synchronized method. For example:

Suppose I have an application with 2 threads, T1 and T2. Then suppose:
1) T1 enters a synchronized block:
??? public static final Object lock = new Object();
??? ...
??? public void foo() {
??? ??? synchronized(lock) {
??? ??? ??? ... //T1 is currently executing here
??? ??? }
???
 }
??? 
2) T2 calls foo(), and blocks waiting for T1 to exit the monitor associated with the object whose reference is stored in var 'lock'.

3) While T1 is executing within the synchronized block, the JVM causes an Error or RuntimeException to be thrown from T1 (or, we can think of some 3rd thread calls Thread.stop on T1, causing a ThreadDeath exception to be thrown from T1... I know Thread.stop is deprecated and unsafe, but I mention it as an analog to what I expect the JVM is doing). This causes execution in T1 to jump to the compiler-generated catch clause to begin execution there. But, just after the catch clause is entered, and before the monitorexit instr is executed, let's say that the JVM causes another Error or RuntimeException to be thrown from T1. So, this 2nd exception would cause immediate exit from the catch clause before the monitorexit instr has executed, correct? The exception continues propagating up the stack until thread T1 dies. In that case, I imagine that T2 would be deadlocked, waiting forever on a monitor that can never be released since the owner is now dead.


If the above is
 true, then I would conclude that:
1) I should favor synchronized methods over synchronized blocks, especially in critical systems where deadlock must absolutely be avoided.
2) By extension, I should favor synchronized methods over Java 1.5's Lock class (where the Lock object's unlock() method must be called in a finally clause that is also susceptible to the '2nd exception' issue).


Thoughts, comments, and corrections are greatly appreciated.

Thank you.

Cheers,
Will



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091214/365adda8/attachment-0001.html>

From davidcholmes at aapt.net.au  Mon Dec 14 03:55:46 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 14 Dec 2009 18:55:46 +1000
Subject: [concurrency-interest] Are synchronized methods safer than
	synchronized blocks?
In-Reply-To: <654412.59564.qm@web50105.mail.re2.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEIJIEAA.davidcholmes@aapt.net.au>

Hi Will,

The issue you raise with synchronized blocks is addressed through a somewhat
controversial mechanism whereby the catch-all block that does the
monitorExit acts as its own exception-handler, so if that second exception
occurs then the monitorExit will be attempted again. This has a consequence
that if monitorExit itself throws an exception then you tend to enter an
infinite loop - either continually throwing the exception, or else from the
second attempt throwing IllegalMonitorStateException because the monitor was
released prior to the first exception being thrown. But you are at least
guaranteed not to be able to leave the synchronized block without releasing
the monitor.

See
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4414101

and related CRs for some discussion.

In relation to use Lock/Condition vs inbuilt. Yes asynchronous exceptions
can break things.

But note that asynchronous exceptions can break just about any Java code. So
avoiding synchronized blocks and Locks/Conditions to get some perceived
protection from asynchronous exceptions is misguided and futile. If
asynchronous exceptions can happen in all but the most trivial of code then
you are up the proverbial creek without a paddle.

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Will McQueen
  Sent: Monday, 14 December 2009 6:35 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Are synchronized methods safer
thansynchronized blocks?


        Hi,

        Are synchronized methods safer than synchronized blocks, in terms of
guaranteeing that any monitors held by a thread will be released before that
thread dies?


        My understanding is this:

        1) The monitor that is acquired when a thread enters a synchronized
instance *method* or synchronized static *method* is guaranteed to be
released when the method's frame is popped off that thread's stack (provided
only that the JVM doesn't crash). That is, the monitor is absolutely
guaranteed to be released when the method returns, whether it returned
normally, or abnormally (via thrown exception). With synchronized methods,
the JVM acquires/releases the monitor on behalf of the thread, since there
are no corresponding monitorenter/monitorexit bytecodes for synchronized
methods (only for synchronized blocks).

        2) A synchronized *block* compiles into Java bytecodes in which a
catch-all catch clause (inserted by the compiler) catches any thrown
exception (a Throwable?). This compiler-generated catch clause calls
'monitorexit' and then rethrows the exception with 'athrow'.



        On p508 of "Inside the Java Virtual Machine, 2nd ed" [Venners], it
says "No matter how the synchronized block is exited, the object lock
acquired when the thread entered the block will definitely be released". I'm
not so sure about that, but I'm hoping to be proven wrong. I believe that
using a synchronized block results in a weaker guarantee of releasing a
monitor than when using a synchronized method. For example:

        Suppose I have an application with 2 threads, T1 and T2. Then
suppose:
        1) T1 enters a synchronized block:
            public static final Object lock = new Object();
            ...
            public void foo() {
                synchronized(lock) {
                    ... //T1 is currently executing here
                }
            }

        2) T2 calls foo(), and blocks waiting for T1 to exit the monitor
associated with the object whose reference is stored in var 'lock'.

        3) While T1 is executing within the synchronized block, the JVM
causes an Error or RuntimeException to be thrown from T1 (or, we can think
of some 3rd thread calls Thread.stop on T1, causing a ThreadDeath exception
to be thrown from T1... I know Thread.stop is deprecated and unsafe, but I
mention it as an analog to what I expect the JVM is doing). This causes
execution in T1 to jump to the compiler-generated catch clause to begin
execution there. But, just after the catch clause is entered, and before the
monitorexit instr is executed, let's say that the JVM causes another Error
or RuntimeException to be thrown from T1. So, this 2nd exception would cause
immediate exit from the catch clause before the monitorexit instr has
executed, correct? The exception continues propagating up the stack until
thread T1 dies. In that case, I imagine that T2 would be deadlocked, waiting
forever on a monitor that can never be released since the owner is now dead.


        If the above is true, then I would conclude that:
        1) I should favor synchronized methods over synchronized blocks,
especially in critical systems where deadlock must absolutely be avoided.
        2) By extension, I should favor synchronized methods over Java 1.5's
Lock class (where the Lock object's unlock() method must be called in a
finally clause that is also susceptible to the '2nd exception' issue).


        Thoughts, comments, and corrections are greatly appreciated.

        Thank you.

        Cheers,
        Will


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091214/81ce132e/attachment.html>

From aph at redhat.com  Mon Dec 14 05:20:21 2009
From: aph at redhat.com (Andrew Haley)
Date: Mon, 14 Dec 2009 10:20:21 +0000
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <5f55507e0912131639w376069c4kc0580ea443e500c2@mail.gmail.com>
References: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCCEIGIEAA.davidcholmes@aapt.net.au>
	<5f55507e0912131639w376069c4kc0580ea443e500c2@mail.gmail.com>
Message-ID: <4B261165.20409@redhat.com>

bank kus wrote:
> It is understood the algorithm is not wait-free if it does "loop / CAS
> / add"  the question is more
> <a> if the hw version of lock:fetchAndAdd is wait-free
> <b> if yes does such an instruction exist on all known platforms
> <c> does the AtomicInteger.getandIncrement make any fairness guarantees.
> 
> Based on the responses I have received so far from different places
> <a> x86 possibly yes
> <b> NO!!
> <c> Definitely NOT!!

I'm not sure that <a> is really true, even though x86 has an atomic add
instruction.  (Which Java doesn't use, but that's not really my point.)
On a core with hyperthreading, as I understand it, when one thread is blocked
waiting for a cache line the other core will proceed.  So even an atomic add
instruction is not wait free.

Andrew.


> On Sun, Dec 13, 2009 at 4:19 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>>> bank kus writes
>>> Is this guaranteed to complete in a finite number of steps. Asking
>>> because am evaluating if I can implement ticket locking using this.
>> You can not analyse a CAS use in isolation. You have to see the complete
>> code path that uses the CAS ie any looping   behaviour involving the code
>> that employs the CAS. You also can't analyze it without knowing how many
>> cores/processors are available and how many threads will execute code that
>> performs the CAS on the same location. You also need to know when preemption
>> may occur.
>>
>> Simple answer is that there are no absolute guarantees. But you also don't
>> generally require such a guarantee.
>>
>>> On a 16 virtual core system if all 16 vcores simultaneously had
>>> threads invoke this operation ..
>>> <a> are all vcores guaranteed to block context switching to any other
>>> threads on their runqueues until this operation completes
>>>
>>> <b> is there fair arbitration between cores. The pathological case
>>> being vcore 2 gets to increment  then gets to do it again while other
>>> vcores wait trying to do so.
>> I can't answer this but I don't think its the right question anyway. A CAS
>> is typically used in a loop:
>>
>> while (true) {
>>  state i = read_initial_state();
>>  state n = compute_next_state(i);
>>  if (CAS(state, i, n))
>>     break;
>> }
>>
>> So the semantics of the CAS itself if executed concurrently is not really
>> the issue. If all 16 cores execute a CAS on the same location, one will
>> succeed and 15 will fail. Your question is really: if 16 threads on 16 cores
>> execute the loop above is there any bound on the number of loop iterations
>> each thread may need to perform?
>>
>> And that isn't a question that can be answered without doing a detailed
>> analysis of the system. A simple analysis says the worst case would be 16
>> iterations: 15 failures from interference on other cores plus success. But
>> what if during the 15 failures the first interfering thread can loop through
>> the higher-level code that performs the above CAS-loop such that it retries
>> the CAS-loop again? You can imagine pathological harmonic relationships that
>> could cause one thread to always fail in its CAS-loop. Is it likely? No. Can
>> it happen? Depends on the context. What's the worst case? Depends on the
>> context.
>>
>> This is one reason why spin-locks often degrade to blocking locks after a
>> certain failure threshold is reached.
>>
>> David Holmes
>>
>>
>>> I understand this would depend upon the hw instructions used but am
>>> curious if any fair expectation can be made out of this API.
>>>
>>> Regards
>>> banks
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From kus.bank at gmail.com  Mon Dec 14 06:13:50 2009
From: kus.bank at gmail.com (bank kus)
Date: Mon, 14 Dec 2009 03:13:50 -0800
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <4B261165.20409@redhat.com>
References: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEIGIEAA.davidcholmes@aapt.net.au>
	<5f55507e0912131639w376069c4kc0580ea443e500c2@mail.gmail.com>
	<4B261165.20409@redhat.com>
Message-ID: <5f55507e0912140313h61be901fw21413bc06d0087cd@mail.gmail.com>

Hmm not sure I agree with that. From the hw designers perspective
treat each virtual core(that includes hyperthreading/CMT) as a client
of the lock make the instruction blocking (no ctxsw'ing until it
completes) now you have a bounded set of contenders in the worst case.

Now round robin over the contenders in any order and you should have a
starvation free algorithmor atleast thats what I d think w/o dwelving
too deep into this problem. Now whether x86 hw really does this or the
fact that they take locks out of their caches makes the arbitration
any worse is another question.

banks

On Mon, Dec 14, 2009 at 2:20 AM, Andrew Haley <aph at redhat.com> wrote:
> bank kus wrote:
>> It is understood the algorithm is not wait-free if it does "loop / CAS
>> / add" ?the question is more
>> <a> if the hw version of lock:fetchAndAdd is wait-free
>> <b> if yes does such an instruction exist on all known platforms
>> <c> does the AtomicInteger.getandIncrement make any fairness guarantees.
>>
>> Based on the responses I have received so far from different places
>> <a> x86 possibly yes
>> <b> NO!!
>> <c> Definitely NOT!!
>
> I'm not sure that <a> is really true, even though x86 has an atomic add
> instruction. ?(Which Java doesn't use, but that's not really my point.)
> On a core with hyperthreading, as I understand it, when one thread is blocked
> waiting for a cache line the other core will proceed. ?So even an atomic add
> instruction is not wait free.
>
> Andrew.
>
>
>> On Sun, Dec 13, 2009 at 4:19 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>>>> bank kus writes
>>>> Is this guaranteed to complete in a finite number of steps. Asking
>>>> because am evaluating if I can implement ticket locking using this.
>>> You can not analyse a CAS use in isolation. You have to see the complete
>>> code path that uses the CAS ie any looping ? behaviour involving the code
>>> that employs the CAS. You also can't analyze it without knowing how many
>>> cores/processors are available and how many threads will execute code that
>>> performs the CAS on the same location. You also need to know when preemption
>>> may occur.
>>>
>>> Simple answer is that there are no absolute guarantees. But you also don't
>>> generally require such a guarantee.
>>>
>>>> On a 16 virtual core system if all 16 vcores simultaneously had
>>>> threads invoke this operation ..
>>>> <a> are all vcores guaranteed to block context switching to any other
>>>> threads on their runqueues until this operation completes
>>>>
>>>> <b> is there fair arbitration between cores. The pathological case
>>>> being vcore 2 gets to increment ?then gets to do it again while other
>>>> vcores wait trying to do so.
>>> I can't answer this but I don't think its the right question anyway. A CAS
>>> is typically used in a loop:
>>>
>>> while (true) {
>>> ?state i = read_initial_state();
>>> ?state n = compute_next_state(i);
>>> ?if (CAS(state, i, n))
>>> ? ? break;
>>> }
>>>
>>> So the semantics of the CAS itself if executed concurrently is not really
>>> the issue. If all 16 cores execute a CAS on the same location, one will
>>> succeed and 15 will fail. Your question is really: if 16 threads on 16 cores
>>> execute the loop above is there any bound on the number of loop iterations
>>> each thread may need to perform?
>>>
>>> And that isn't a question that can be answered without doing a detailed
>>> analysis of the system. A simple analysis says the worst case would be 16
>>> iterations: 15 failures from interference on other cores plus success. But
>>> what if during the 15 failures the first interfering thread can loop through
>>> the higher-level code that performs the above CAS-loop such that it retries
>>> the CAS-loop again? You can imagine pathological harmonic relationships that
>>> could cause one thread to always fail in its CAS-loop. Is it likely? No. Can
>>> it happen? Depends on the context. What's the worst case? Depends on the
>>> context.
>>>
>>> This is one reason why spin-locks often degrade to blocking locks after a
>>> certain failure threshold is reached.
>>>
>>> David Holmes
>>>
>>>
>>>> I understand this would depend upon the hw instructions used but am
>>>> curious if any fair expectation can be made out of this API.
>>>>
>>>> Regards
>>>> banks
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From aph at redhat.com  Mon Dec 14 06:21:04 2009
From: aph at redhat.com (Andrew Haley)
Date: Mon, 14 Dec 2009 11:21:04 +0000
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <5f55507e0912140313h61be901fw21413bc06d0087cd@mail.gmail.com>
References: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>	
	<NFBBKALFDCPFIDBNKAPCCEIGIEAA.davidcholmes@aapt.net.au>	
	<5f55507e0912131639w376069c4kc0580ea443e500c2@mail.gmail.com>	
	<4B261165.20409@redhat.com>
	<5f55507e0912140313h61be901fw21413bc06d0087cd@mail.gmail.com>
Message-ID: <4B261FA0.7080705@redhat.com>

bank kus wrote:
> Hmm not sure I agree with that. From the hw designers perspective
> treat each virtual core(that includes hyperthreading/CMT) as a client
> of the lock make the instruction blocking (no ctxsw'ing until it
> completes) now you have a bounded set of contenders in the worst case.

That's right: the virtual core won't context switch, but the physical one
will.  So, it all comes down to precisely what you mean by "wait-free".
There is one important difference, which is that once you have obtained
the cache line the atomic increment will complete.  But the wait for the
cache line isn't bounded, AFAIAA.  Perhaps in practice the chipsets in
use use round robin, but I'm not sure that anything in the architecture
specification requires that, any more than the Java memory model does.

> Now round robin over the contenders in any order and you should have a
> starvation free algorithmor atleast thats what I d think w/o dwelving
> too deep into this problem. Now whether x86 hw really does this or the
> fact that they take locks out of their caches makes the arbitration
> any worse is another question.

Andrew.


> On Mon, Dec 14, 2009 at 2:20 AM, Andrew Haley <aph at redhat.com> wrote:
>> bank kus wrote:
>>> It is understood the algorithm is not wait-free if it does "loop / CAS
>>> / add"  the question is more
>>> <a> if the hw version of lock:fetchAndAdd is wait-free
>>> <b> if yes does such an instruction exist on all known platforms
>>> <c> does the AtomicInteger.getandIncrement make any fairness guarantees.
>>>
>>> Based on the responses I have received so far from different places
>>> <a> x86 possibly yes
>>> <b> NO!!
>>> <c> Definitely NOT!!
>> I'm not sure that <a> is really true, even though x86 has an atomic add
>> instruction.  (Which Java doesn't use, but that's not really my point.)
>> On a core with hyperthreading, as I understand it, when one thread is blocked
>> waiting for a cache line the other [thread] will proceed.  So even an atomic add
>> instruction is not wait free.
>>
>> Andrew.
>>
>>
>>> On Sun, Dec 13, 2009 at 4:19 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>>>>> bank kus writes
>>>>> Is this guaranteed to complete in a finite number of steps. Asking
>>>>> because am evaluating if I can implement ticket locking using this.
>>>> You can not analyse a CAS use in isolation. You have to see the complete
>>>> code path that uses the CAS ie any looping   behaviour involving the code
>>>> that employs the CAS. You also can't analyze it without knowing how many
>>>> cores/processors are available and how many threads will execute code that
>>>> performs the CAS on the same location. You also need to know when preemption
>>>> may occur.
>>>>
>>>> Simple answer is that there are no absolute guarantees. But you also don't
>>>> generally require such a guarantee.
>>>>
>>>>> On a 16 virtual core system if all 16 vcores simultaneously had
>>>>> threads invoke this operation ..
>>>>> <a> are all vcores guaranteed to block context switching to any other
>>>>> threads on their runqueues until this operation completes
>>>>>
>>>>> <b> is there fair arbitration between cores. The pathological case
>>>>> being vcore 2 gets to increment  then gets to do it again while other
>>>>> vcores wait trying to do so.
>>>> I can't answer this but I don't think its the right question anyway. A CAS
>>>> is typically used in a loop:
>>>>
>>>> while (true) {
>>>>  state i = read_initial_state();
>>>>  state n = compute_next_state(i);
>>>>  if (CAS(state, i, n))
>>>>     break;
>>>> }
>>>>
>>>> So the semantics of the CAS itself if executed concurrently is not really
>>>> the issue. If all 16 cores execute a CAS on the same location, one will
>>>> succeed and 15 will fail. Your question is really: if 16 threads on 16 cores
>>>> execute the loop above is there any bound on the number of loop iterations
>>>> each thread may need to perform?
>>>>
>>>> And that isn't a question that can be answered without doing a detailed
>>>> analysis of the system. A simple analysis says the worst case would be 16
>>>> iterations: 15 failures from interference on other cores plus success. But
>>>> what if during the 15 failures the first interfering thread can loop through
>>>> the higher-level code that performs the above CAS-loop such that it retries
>>>> the CAS-loop again? You can imagine pathological harmonic relationships that
>>>> could cause one thread to always fail in its CAS-loop. Is it likely? No. Can
>>>> it happen? Depends on the context. What's the worst case? Depends on the
>>>> context.
>>>>
>>>> This is one reason why spin-locks often degrade to blocking locks after a
>>>> certain failure threshold is reached.
>>>>
>>>> David Holmes
>>>>
>>>>
>>>>> I understand this would depend upon the hw instructions used but am
>>>>> curious if any fair expectation can be made out of this API.
>>>>>
>>>>> Regards
>>>>> banks
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>


From kus.bank at gmail.com  Mon Dec 14 06:32:05 2009
From: kus.bank at gmail.com (bank kus)
Date: Mon, 14 Dec 2009 03:32:05 -0800
Subject: [concurrency-interest] AtomicInteger.getAndIncrement()
In-Reply-To: <4B261FA0.7080705@redhat.com>
References: <5f55507e0912121221l7ac71fdla691d0f1d77e1d13@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEIGIEAA.davidcholmes@aapt.net.au>
	<5f55507e0912131639w376069c4kc0580ea443e500c2@mail.gmail.com>
	<4B261165.20409@redhat.com>
	<5f55507e0912140313h61be901fw21413bc06d0087cd@mail.gmail.com>
	<4B261FA0.7080705@redhat.com>
Message-ID: <5f55507e0912140332u6fd0bcb4p717235a3a50f339c@mail.gmail.com>

>> But the wait for the cache line isn't bounded,

Isnt that dangerous since it means lock:xxx aside even writes from
certain virtual cores could starve indefinitely? But then I m not sure
we can poll AMD/Intel for more details on their forums.

banks

On Mon, Dec 14, 2009 at 3:21 AM, Andrew Haley <aph at redhat.com> wrote:
> bank kus wrote:
>> Hmm not sure I agree with that. From the hw designers perspective
>> treat each virtual core(that includes hyperthreading/CMT) as a client
>> of the lock make the instruction blocking (no ctxsw'ing until it
>> completes) now you have a bounded set of contenders in the worst case.
>
> That's right: the virtual core won't context switch, but the physical one
> will. ?So, it all comes down to precisely what you mean by "wait-free".
> There is one important difference, which is that once you have obtained
> the cache line the atomic increment will complete. ?But the wait for the
> cache line isn't bounded, AFAIAA. ?Perhaps in practice the chipsets in
> use use round robin, but I'm not sure that anything in the architecture
> specification requires that, any more than the Java memory model does.
>
>> Now round robin over the contenders in any order and you should have a
>> starvation free algorithmor atleast thats what I d think w/o dwelving
>> too deep into this problem. Now whether x86 hw really does this or the
>> fact that they take locks out of their caches makes the arbitration
>> any worse is another question.
>
> Andrew.
>
>
>> On Mon, Dec 14, 2009 at 2:20 AM, Andrew Haley <aph at redhat.com> wrote:
>>> bank kus wrote:
>>>> It is understood the algorithm is not wait-free if it does "loop / CAS
>>>> / add" ?the question is more
>>>> <a> if the hw version of lock:fetchAndAdd is wait-free
>>>> <b> if yes does such an instruction exist on all known platforms
>>>> <c> does the AtomicInteger.getandIncrement make any fairness guarantees.
>>>>
>>>> Based on the responses I have received so far from different places
>>>> <a> x86 possibly yes
>>>> <b> NO!!
>>>> <c> Definitely NOT!!
>>> I'm not sure that <a> is really true, even though x86 has an atomic add
>>> instruction. ?(Which Java doesn't use, but that's not really my point.)
>>> On a core with hyperthreading, as I understand it, when one thread is blocked
>>> waiting for a cache line the other [thread] will proceed. ?So even an atomic add
>>> instruction is not wait free.
>>>
>>> Andrew.
>>>
>>>
>>>> On Sun, Dec 13, 2009 at 4:19 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>>>>>> bank kus writes
>>>>>> Is this guaranteed to complete in a finite number of steps. Asking
>>>>>> because am evaluating if I can implement ticket locking using this.
>>>>> You can not analyse a CAS use in isolation. You have to see the complete
>>>>> code path that uses the CAS ie any looping ? behaviour involving the code
>>>>> that employs the CAS. You also can't analyze it without knowing how many
>>>>> cores/processors are available and how many threads will execute code that
>>>>> performs the CAS on the same location. You also need to know when preemption
>>>>> may occur.
>>>>>
>>>>> Simple answer is that there are no absolute guarantees. But you also don't
>>>>> generally require such a guarantee.
>>>>>
>>>>>> On a 16 virtual core system if all 16 vcores simultaneously had
>>>>>> threads invoke this operation ..
>>>>>> <a> are all vcores guaranteed to block context switching to any other
>>>>>> threads on their runqueues until this operation completes
>>>>>>
>>>>>> <b> is there fair arbitration between cores. The pathological case
>>>>>> being vcore 2 gets to increment ?then gets to do it again while other
>>>>>> vcores wait trying to do so.
>>>>> I can't answer this but I don't think its the right question anyway. A CAS
>>>>> is typically used in a loop:
>>>>>
>>>>> while (true) {
>>>>> ?state i = read_initial_state();
>>>>> ?state n = compute_next_state(i);
>>>>> ?if (CAS(state, i, n))
>>>>> ? ? break;
>>>>> }
>>>>>
>>>>> So the semantics of the CAS itself if executed concurrently is not really
>>>>> the issue. If all 16 cores execute a CAS on the same location, one will
>>>>> succeed and 15 will fail. Your question is really: if 16 threads on 16 cores
>>>>> execute the loop above is there any bound on the number of loop iterations
>>>>> each thread may need to perform?
>>>>>
>>>>> And that isn't a question that can be answered without doing a detailed
>>>>> analysis of the system. A simple analysis says the worst case would be 16
>>>>> iterations: 15 failures from interference on other cores plus success. But
>>>>> what if during the 15 failures the first interfering thread can loop through
>>>>> the higher-level code that performs the above CAS-loop such that it retries
>>>>> the CAS-loop again? You can imagine pathological harmonic relationships that
>>>>> could cause one thread to always fail in its CAS-loop. Is it likely? No. Can
>>>>> it happen? Depends on the context. What's the worst case? Depends on the
>>>>> context.
>>>>>
>>>>> This is one reason why spin-locks often degrade to blocking locks after a
>>>>> certain failure threshold is reached.
>>>>>
>>>>> David Holmes
>>>>>
>>>>>
>>>>>> I understand this would depend upon the hw instructions used but am
>>>>>> curious if any fair expectation can be made out of this API.
>>>>>>
>>>>>> Regards
>>>>>> banks
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>
>


From dmytro_sheyko at hotmail.com  Mon Dec 14 09:42:00 2009
From: dmytro_sheyko at hotmail.com (Dmytro Sheyko)
Date: Mon, 14 Dec 2009 21:42:00 +0700
Subject: [concurrency-interest] Are synchronized methods safer
	than	synchronized blocks?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIJIEAA.davidcholmes@aapt.net.au>
References: <654412.59564.qm@web50105.mail.re2.yahoo.com>,
	<NFBBKALFDCPFIDBNKAPCEEIJIEAA.davidcholmes@aapt.net.au>
Message-ID: <SNT106-W28DF203CB0E3745B5321F58A890@phx.gbl>


Hi,

It seems that in practice infinite loop does not occur because monitorexit (for some reason) does not throw IllegalMonitorStateException.
The program proves this.

// test/Main.java
package test;

public class Main {

    public static void main(String... args) {
        try {
            new Main().useSynchronizedMethod();
            System.err.println("method ...\tok");
        } catch (Throwable exc) {
            System.err.println("method ...\t" + exc);
        }
        System.err.println();

        try {
            new Main().useSynchronizedBlock();
            System.err.println("block  ...\tok");
        } catch (Throwable exc) {
            System.err.println("block  ...\t" + exc);
        }
        System.err.println();
    }

    void useSynchronizedMethod() {
        System.err.println("before enter\t" + Thread.holdsLock(this));
        internalSynchronizedMethod();
        System.err.println("after  exit\t" + Thread.holdsLock(this));
    }

    private synchronized void internalSynchronizedMethod() {
        System.err.println("after  enter\t" + Thread.holdsLock(this));
        UnsafeMonitor.monitorExit(this);
        System.err.println("before exit\t" + Thread.holdsLock(this));
    }

    void useSynchronizedBlock() {
        System.err.println("before enter\t" + Thread.holdsLock(this));
        synchronized (this) {
            System.err.println("after  enter\t" + Thread.holdsLock(this));
            UnsafeMonitor.monitorExit(this);
            System.err.println("before exit\t" + Thread.holdsLock(this));
        }
        System.err.println("after  exit\t" + Thread.holdsLock(this));
    }

}

// test/UnsafeMonitor.java - it has to be in bootclasspath in order to access Unsafe object.
package test;

import sun.misc.Unsafe;

public class UnsafeMonitor {
    private static final Unsafe unsafe = Unsafe.getUnsafe();

    public static void monitorExit(Object obj) {
        System.err.println("unsafe exit");
        unsafe.monitorExit(obj);
    }

}

The output is following

before enter    false
after  enter    true
unsafe exit
before exit    false
after  exit    false
method ...    ok

before enter    false
after  enter    true
unsafe exit
before exit    false
after  exit    false
block  ...    ok

Regards,
Dmytro

From: davidcholmes at aapt.net.au
To: willmcqueen at yahoo.com; concurrency-interest at cs.oswego.edu
Date: Mon, 14 Dec 2009 18:55:46 +1000
Subject: Re: [concurrency-interest] Are synchronized methods safer than	synchronized blocks?








Hi Will,
 
The issue you raise with synchronized blocks is 
addressed through a somewhat controversial mechanism whereby the catch-all block 
that does the monitorExit acts as its own exception-handler, so if that second 
exception occurs then the monitorExit will be attempted again. This has a 
consequence that if monitorExit itself throws an exception then you tend to 
enter an infinite loop - either continually throwing the exception, or else from 
the second attempt throwing IllegalMonitorStateException because the monitor was 
released prior to the first exception being thrown. But you are at least 
guaranteed not to be able to leave the synchronized block without releasing the 
monitor.
 
See
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4414101
 
and related CRs for some discussion.
 
In relation to use Lock/Condition vs inbuilt. Yes asynchronous exceptions 
can break things.
 
But note that asynchronous exceptions can break just about any Java code. 
So avoiding synchronized blocks and Locks/Conditions to get some perceived 
protection from asynchronous exceptions is misguided and futile. If asynchronous 
exceptions can happen in all but the most trivial of code then you are up the 
proverbial creek without a paddle.
 
Cheers,
David Holmes

  -----Original Message-----
From: 
  concurrency-interest-bounces at cs.oswego.edu 
  [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Will 
  McQueen
Sent: Monday, 14 December 2009 6:35 PM
To: 
  concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] 
  Are synchronized methods safer thansynchronized blocks?


  
    
    
      
        Hi,

Are synchronized methods safer than 
        synchronized blocks, in terms of guaranteeing that any monitors held by 
        a thread will be released before that thread dies?


My 
        understanding is this:

1) The monitor that is acquired when a 
        thread enters a synchronized instance *method* or synchronized static 
        *method* is guaranteed to be released when the method's frame is popped 
        off that thread's stack (provided only that the JVM doesn't crash). That 
        is, the monitor is absolutely guaranteed to be released when the method 
        returns, whether it returned normally, or abnormally (via thrown 
        exception). With synchronized methods, the JVM acquires/releases the 
        monitor on behalf of the thread, since there are no corresponding 
        monitorenter/monitorexit bytecodes for synchronized methods (only for 
        synchronized blocks).

2) A synchronized *block* compiles into 
        Java bytecodes in which a catch-all catch clause (inserted by the 
        compiler) catches any thrown exception (a Throwable?). This 
        compiler-generated catch clause calls 'monitorexit' and then rethrows 
        the exception with 'athrow'.



On p508 of "Inside the Java 
        Virtual Machine, 2nd ed" [Venners], it says "No matter how the 
        synchronized block is exited, the object lock acquired when the thread 
        entered the block will definitely be released". I'm not so sure about 
        that, but I'm hoping to be proven wrong. I believe that using a 
        synchronized block results in a weaker guarantee of releasing a monitor 
        than when using a synchronized method. For example:

Suppose I 
        have an application with 2 threads, T1 and T2. Then suppose:
1) T1 
        enters a synchronized block:
    public static final 
        Object lock = new Object();
    
        ...
    public void foo() {
    
            synchronized(lock) {
    
                ... //T1 is currently executing 
        here
        }
    
        }
    
2) T2 calls foo(), and blocks waiting for T1 
        to exit the monitor associated with the object whose reference is stored 
        in var 'lock'.

3) While T1 is executing within the synchronized 
        block, the JVM causes an Error or RuntimeException to be thrown from T1 
        (or, we can think of some 3rd thread calls Thread.stop on T1, causing a 
        ThreadDeath exception to be thrown from T1... I know Thread.stop is 
        deprecated and unsafe, but I mention it as an analog to what I expect 
        the JVM is doing). This causes execution in T1 to jump to the 
        compiler-generated catch clause to begin execution there. But, just 
        after the catch clause is entered, and before the monitorexit instr is 
        executed, let's say that the JVM causes another Error or 
        RuntimeException to be thrown from T1. So, this 2nd exception would 
        cause immediate exit from the catch clause before the monitorexit instr 
        has executed, correct? The exception continues propagating up the stack 
        until thread T1 dies. In that case, I imagine that T2 would be 
        deadlocked, waiting forever on a monitor that can never be released 
        since the owner is now dead.


If the above is true, then I 
        would conclude that:
1) I should favor synchronized methods over 
        synchronized blocks, especially in critical systems where deadlock must 
        absolutely be avoided.
2) By extension, I should favor synchronized 
        methods over Java 1.5's Lock class (where the Lock object's unlock() 
        method must be called in a finally clause that is also susceptible to 
        the '2nd exception' issue).


Thoughts, comments, and 
        corrections are greatly appreciated.

Thank 
        you.

Cheers,
Will

 		 	   		  
_________________________________________________________________
Windows Live Hotmail: Your friends can get your Facebook updates, right from Hotmail?.
http://www.microsoft.com/middleeast/windows/windowslive/see-it-in-action/social-network-basics.aspx?ocid=PID23461::T:WLMTAGL:ON:WL:en-xm:SI_SB_4:092009
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091214/c355f368/attachment-0001.html>

From qiyaoltc at gmail.com  Mon Dec 14 21:22:52 2009
From: qiyaoltc at gmail.com (Yao Qi)
Date: Tue, 15 Dec 2009 10:22:52 +0800
Subject: [concurrency-interest] Are synchronized methods safer than
	synchronized blocks?
In-Reply-To: <654412.59564.qm@web50105.mail.re2.yahoo.com>
References: <654412.59564.qm@web50105.mail.re2.yahoo.com>
Message-ID: <dcbde71d0912141822geeb0d84nf4373b1776e5b77e@mail.gmail.com>

On Mon, Dec 14, 2009 at 4:34 PM, Will McQueen <willmcqueen at yahoo.com> wrote:
>

I agree with you on your understandings, but I have some different
thought on your conclusions....
>
>
> If the above is true, then I would conclude that:
> 1) I should favor synchronized methods over synchronized blocks, especially in critical systems where deadlock must absolutely be avoided.

I don't think deadlock can be avoided absolutely....
Here is an example (Deadlock.java in attachment) on two classes using
synchronized methods only however, deadlock is still there.  I run
Deadlock.java in MulticoreSDK
(http://www.alphaworks.ibm.com/tech/msdk), and find one deadlock in
it, shown in deadlock.png.

> 2) By extension, I should favor synchronized methods over Java 1.5's Lock class (where the Lock object's unlock() method must be called in a finally clause that is also susceptible to the '2nd exception' issue).

IMO, Java 5's Lock classes are introduced for performance and
flexibility.  JUC Lock is efficient than Object monitor in some cases,
and Lock is more and more used to replace Object monitor in order to
improve scalability....
I agree that there is a risk on "lock leak" problem, as you mentioned.
 Since there are some data flow tools can analyze "lock leak" problem
statically, it is not a big problem.

--
Yao Qi <qiyaoltc AT gmail DOT com> ? ?GNU/LinuxDeveloper
http://duewayqi.googlepages.com/
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Deadlock.java
Type: text/x-java
Size: 780 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091215/853f0d3e/attachment.bin>

From davidcholmes at aapt.net.au  Tue Dec 15 04:19:40 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 15 Dec 2009 19:19:40 +1000
Subject: [concurrency-interest] Are synchronized methods safer
	thansynchronized blocks?
In-Reply-To: <SNT106-W28DF203CB0E3745B5321F58A890@phx.gbl>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEIPIEAA.davidcholmes@aapt.net.au>

Dmytro,

I was quite surprised to discover this is the case. As always with the VM
there is a very complex story here once you get into the internals. I'm
currently trying to ascertain exactly what the situation is. You might (and
I hope you would) get a different result if you used a bytecode assembler to
issue a monitorexit bytecode rather than using Unsafe.

Thanks for the info.

David Holmes

 -----Original Message-----
From: Dmytro Sheyko [mailto:dmytro_sheyko at hotmail.com]
Sent: Tuesday, 15 December 2009 12:42 AM
To: dholmes at ieee.org; willmcqueen at yahoo.com;
concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] Are synchronized methods safer
thansynchronized blocks?


  Hi,

  It seems that in practice infinite loop does not occur because monitorexit
(for some reason) does not throw IllegalMonitorStateException.
  The program proves this.

  // test/Main.java
  package test;

  public class Main {

      public static void main(String... args) {
          try {
              new Main().useSynchronizedMethod();
              System.err.println("method ...\tok");
          } catch (Throwable exc) {
              System.err.println("method ...\t" + exc);
          }
          System.err.println();

          try {
              new Main().useSynchronizedBlock();
              System.err.println("block  ...\tok");
          } catch (Throwable exc) {
              System.err.println("block  ...\t" + exc);
          }
          System.err.println();
      }

      void useSynchronizedMethod() {
          System.err.println("before enter\t" + Thread.holdsLock(this));
          internalSynchronizedMethod();
          System.err.println("after  exit\t" + Thread.holdsLock(this));
      }

      private synchronized void internalSynchronizedMethod() {
          System.err.println("after  enter\t" + Thread.holdsLock(this));
          UnsafeMonitor.monitorExit(this);
          System.err.println("before exit\t" + Thread.holdsLock(this));
      }

      void useSynchronizedBlock() {
          System.err.println("before enter\t" + Thread.holdsLock(this));
          synchronized (this) {
              System.err.println("after  enter\t" + Thread.holdsLock(this));
              UnsafeMonitor.monitorExit(this);
              System.err.println("before exit\t" + Thread.holdsLock(this));
          }
          System.err.println("after  exit\t" + Thread.holdsLock(this));
      }

  }

  // test/UnsafeMonitor.java - it has to be in bootclasspath in order to
access Unsafe object.
  package test;

  import sun.misc.Unsafe;

  public class UnsafeMonitor {
      private static final Unsafe unsafe = Unsafe.getUnsafe();

      public static void monitorExit(Object obj) {
          System.err.println("unsafe exit");
          unsafe.monitorExit(obj);
      }

  }

  The output is following

  before enter    false
  after  enter    true
  unsafe exit
  before exit    false
  after  exit    false
  method ...    ok

  before enter    false
  after  enter    true
  unsafe exit
  before exit    false
  after  exit    false
  block  ...    ok

  Regards,
  Dmytro


----------------------------------------------------------------------------
--
  From: davidcholmes at aapt.net.au
  To: willmcqueen at yahoo.com; concurrency-interest at cs.oswego.edu
  Date: Mon, 14 Dec 2009 18:55:46 +1000
  Subject: Re: [concurrency-interest] Are synchronized methods safer than
synchronized blocks?


  Hi Will,

  The issue you raise with synchronized blocks is addressed through a
somewhat controversial mechanism whereby the catch-all block that does the
monitorExit acts as its own exception-handler, so if that second exception
occurs then the monitorExit will be attempted again. This has a consequence
that if monitorExit itself throws an exception then you tend to enter an
infinite loop - either continually throwing the exception, or else from the
second attempt throwing IllegalMonitorStateException because the monitor was
released prior to the first exception being thrown. But you are at least
guaranteed not to be able to leave the synchronized block without releasing
the monitor.

  See
  http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4414101

  and related CRs for some discussion.

  In relation to use Lock/Condition vs inbuilt. Yes asynchronous exceptions
can break things.

  But note that asynchronous exceptions can break just about any Java code.
So avoiding synchronized blocks and Locks/Conditions to get some perceived
protection from asynchronous exceptions is misguided and futile. If
asynchronous exceptions can happen in all but the most trivial of code then
you are up the proverbial creek without a paddle.

  Cheers,
  David Holmes
    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Will McQueen
    Sent: Monday, 14 December 2009 6:35 PM
    To: concurrency-interest at cs.oswego.edu
    Subject: [concurrency-interest] Are synchronized methods safer
thansynchronized blocks?


          Hi,

          Are synchronized methods safer than synchronized blocks, in terms
of guaranteeing that any monitors held by a thread will be released before
that thread dies?


          My understanding is this:

          1) The monitor that is acquired when a thread enters a
synchronized instance *method* or synchronized static *method* is guaranteed
to be released when the method's frame is popped off that thread's stack
(provided only that the JVM doesn't crash). That is, the monitor is
absolutely guaranteed to be released when the method returns, whether it
returned normally, or abnormally (via thrown exception). With synchronized
methods, the JVM acquires/releases the monitor on behalf of the thread,
since there are no corresponding monitorenter/monitorexit bytecodes for
synchronized methods (only for synchronized blocks).

          2) A synchronized *block* compiles into Java bytecodes in which a
catch-all catch clause (inserted by the compiler) catches any thrown
exception (a Throwable?). This compiler-generated catch clause calls
'monitorexit' and then rethrows the exception with 'athrow'.



          On p508 of "Inside the Java Virtual Machine, 2nd ed" [Venners], it
says "No matter how the synchronized block is exited, the object lock
acquired when the thread entered the block will definitely be released". I'm
not so sure about that, but I'm hoping to be proven wrong. I believe that
using a synchronized block results in a weaker guarantee of releasing a
monitor than when using a synchronized method. For example:

          Suppose I have an application with 2 threads, T1 and T2. Then
suppose:
          1) T1 enters a synchronized block:
              public static final Object lock = new Object();
              ...
              public void foo() {
                  synchronized(lock) {
                      ... //T1 is currently executing here
                  }
              }

          2) T2 calls foo(), and blocks waiting for T1 to exit the monitor
associated with the object whose reference is stored in var 'lock'.

          3) While T1 is executing within the synchronized block, the JVM
causes an Error or RuntimeException to be thrown from T1 (or, we can think
of some 3rd thread calls Thread.stop on T1, causing a ThreadDeath exception
to be thrown from T1... I know Thread.stop is deprecated and unsafe, but I
mention it as an analog to what I expect the JVM is doing). This causes
execution in T1 to jump to the compiler-generated catch clause to begin
execution there. But, just after the catch clause is entered, and before the
monitorexit instr is executed, let's say that the JVM causes another Error
or RuntimeException to be thrown from T1. So, this 2nd exception would cause
immediate exit from the catch clause before the monitorexit instr has
executed, correct? The exception continues propagating up the stack until
thread T1 dies. In that case, I imagine that T2 would be deadlocked, waiting
forever on a monitor that can never be released since the owner is now dead.


          If the above is true, then I would conclude that:
          1) I should favor synchronized methods over synchronized blocks,
especially in critical systems where deadlock must absolutely be avoided.
          2) By extension, I should favor synchronized methods over Java
1.5's Lock class (where the Lock object's unlock() method must be called in
a finally clause that is also susceptible to the '2nd exception' issue).


          Thoughts, comments, and corrections are greatly appreciated.

          Thank you.

          Cheers,
          Will





----------------------------------------------------------------------------
--
  Windows Live Hotmail: Your friends can get your Facebook updates, right
from Hotmail?.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091215/f44473f8/attachment-0001.html>

From dhanji at gmail.com  Tue Dec 15 19:00:34 2009
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Wed, 16 Dec 2009 11:00:34 +1100
Subject: [concurrency-interest] Latches and safe handoff
Message-ID: <aa067ea10912151600lf789acej372763483df4f4b4@mail.gmail.com>

Here is a pattern that I am curious about:

public void doStuff() {

    MyException[] holder = new MyException[1];
    otherThreadPerformer.perform(new Runnable() {
        ... run() {
           try {
           ...
           } catch (MyException e) {
              holder[0] = e;
           } finally {
              latch.go(); //can't recall what the method is called
           }
       }
   }

   latch.await();
   // read holder[0]
}


Is this handoff safe? Or am I better off using an AtomicReference? I imagine
that it is not, as the holder array could still be null (there is no
synchronized write to it) after the latch is opened.

Dhanji.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091216/0857e1ac/attachment.html>

From jim.andreou at gmail.com  Tue Dec 15 19:13:46 2009
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Wed, 16 Dec 2009 02:13:46 +0200
Subject: [concurrency-interest] Latches and safe handoff
In-Reply-To: <aa067ea10912151600lf789acej372763483df4f4b4@mail.gmail.com>
References: <aa067ea10912151600lf789acej372763483df4f4b4@mail.gmail.com>
Message-ID: <7d7138c10912151613y4ad6f5abx93655932959e9d4e@mail.gmail.com>

Hi Dhanji,

If you are talking about a j.u.c.CountDownLatch or friends, then it's
safe. Quoting javadocs:

Memory consistency effects: Actions in a thread prior to calling
countDown() happen-before actions following a successful return from a
corresponding await() in another thread.

Dimitris

2009/12/16 Dhanji R. Prasanna <dhanji at gmail.com>:
> Here is a pattern that I am curious about:
> public void doStuff() {
> ?? ?MyException[] holder = new MyException[1];
> ?? ?otherThreadPerformer.perform(new Runnable() {
> ?? ? ? ?... run() {
> ?? ? ? ? ? try {
> ?? ? ? ? ? ...
> ?? ? ? ? ? } catch (MyException e) {
> ?? ? ? ? ? ? ?holder[0] = e;
> ?? ? ? ? ? } finally {
> ?? ? ? ? ? ? ?latch.go(); //can't recall what the method is called
> ?? ? ? ? ? }
> ?? ? ? }
> ?? }
> ?? latch.await();
> ?? // read holder[0]
> }
>
> Is this handoff safe? Or am I better off using an AtomicReference? I imagine
> that it is not, as the holder array could still be null (there is no
> synchronized write to it) after the latch is opened.
> Dhanji.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From tim at peierls.net  Tue Dec 15 19:20:07 2009
From: tim at peierls.net (Tim Peierls)
Date: Tue, 15 Dec 2009 19:20:07 -0500
Subject: [concurrency-interest] Latches and safe handoff
In-Reply-To: <aa067ea10912151600lf789acej372763483df4f4b4@mail.gmail.com>
References: <aa067ea10912151600lf789acej372763483df4f4b4@mail.gmail.com>
Message-ID: <63b4e4050912151620w3289c68apab37ad2dcd49c516@mail.gmail.com>

It is safe for the obvious otherThreadPerformer suspects, i.e., something
involving Thread.start/join, Executor.execute, or things like that. (You'll
need holder to be final.)

Seems like you're working very hard to avoid Future/Callable, though.

--tim

On Tue, Dec 15, 2009 at 7:00 PM, Dhanji R. Prasanna <dhanji at gmail.com>wrote:

> Here is a pattern that I am curious about:
>
> public void doStuff() {
>
>     MyException[] holder = new MyException[1];
>     otherThreadPerformer.perform(new Runnable() {
>         ... run() {
>            try {
>            ...
>            } catch (MyException e) {
>               holder[0] = e;
>            } finally {
>               latch.go(); //can't recall what the method is called
>            }
>        }
>    }
>
>    latch.await();
>    // read holder[0]
> }
>
>
> Is this handoff safe? Or am I better off using an AtomicReference? I
> imagine that it is not, as the holder array could still be null (there is no
> synchronized write to it) after the latch is opened.
>
> Dhanji.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091215/545dff20/attachment.html>

From dhanji at gmail.com  Tue Dec 15 19:23:36 2009
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Wed, 16 Dec 2009 11:23:36 +1100
Subject: [concurrency-interest] Latches and safe handoff
In-Reply-To: <63b4e4050912151620w3289c68apab37ad2dcd49c516@mail.gmail.com>
References: <aa067ea10912151600lf789acej372763483df4f4b4@mail.gmail.com>
	<63b4e4050912151620w3289c68apab37ad2dcd49c516@mail.gmail.com>
Message-ID: <aa067ea10912151623t189b1db5s7e0159d6e4de196a@mail.gmail.com>

On Wed, Dec 16, 2009 at 11:20 AM, Tim Peierls <tim at peierls.net> wrote:

> It is safe for the obvious otherThreadPerformer suspects, i.e., something
> involving Thread.start/join, Executor.execute, or things like that. (You'll
> need holder to be final.)


I think it's an executor underneath somewhere, yea.

>
> Seems like you're working very hard to avoid Future/Callable, though.
>
>
Think of it as a thought experiment. =)

Dhanji.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091216/ff31033c/attachment.html>

From willmcqueen at yahoo.com  Tue Dec 15 20:01:10 2009
From: willmcqueen at yahoo.com (Will McQueen)
Date: Tue, 15 Dec 2009 17:01:10 -0800 (PST)
Subject: [concurrency-interest] ConcurrentModificationException not
	applicable to Map.values()?
Message-ID: <29845.98379.qm@web50104.mail.re2.yahoo.com>

Hi,

I'm puzzled why ConcurrentModificationException is not thrown in the case where I use a ConcurrentHashMap instead of HashMap. The code below is single-threaded so that ConcurrrentModificationException should definitely be thrown (as opposed to "good-faith-effort"). [Ref: JCIP "Iterators and ConcurrentModificationException"]

Below is my code that shows 2 cases -- the first where ConcurrentModificationException is not thrown (that's the case I don't understand), and the second case shows where it is thrown.

This example below shows ConcurrenHashMap, which I use in the multi-threaded version of my code. So technically I could use just a HashMap for this single-threaded demo.... but then this would not reproduce this issue. That is, puzzlingly, case #1 *does* throw the expected ConcurrentModificationException when I use a HashMap instead of ConcurrentHashMap (!).

public final class QuickTest {
??? public static void main(String[] args) {
??????? /*** CASE 1 ****/
??????? final Map<String,String> map;
??????? map = new ConcurrentHashMap<String,String>();
??????? map.put("a","1");
??????? map.put("b", "2");
??????? map.put("c", "3");

??????? Collection<String> view = map.values();
??????? Iterator<String> itr = view.iterator();
??????? String e1 = itr.next();
??????? System.out.println(e1);
??????? view.remove(e1); //direct removal through the collection, not through the iterator
??????? String e2 = itr.next(); //??? Why doesn't this throw ConcurrentModificationException?
??????? System.out.println(e2);

??????? /*** CASE 2 ***/

??????? final List<String> list;
??????? list = new ArrayList<String>();
??????? list.add("1");
??????? list.add("2");
??????? list.add("3");

??????? itr = list.iterator();
??????? e1 = itr.next();
??????? System.out.println(e1);
??????? list.remove(e1); //direct removal through the collection, not through the iterator
??????? e2 = itr.next(); //(throws ConcurrentModificationException, as expected)
??????? System.out.println(e2);
??? }
}

Your insight and help is greatly appreciated. Thank you.

Cheers,
Will



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091215/7cad4f89/attachment.html>

From willmcqueen at yahoo.com  Tue Dec 15 20:05:05 2009
From: willmcqueen at yahoo.com (Will McQueen)
Date: Tue, 15 Dec 2009 17:05:05 -0800 (PST)
Subject: [concurrency-interest] ConcurrentModificationException not
	applicable to Map.values()?
Message-ID: <309527.51614.qm@web50101.mail.re2.yahoo.com>

I forgot to change the subject title... it should be:
"ConcurrentModificationException not applicable to ConcurrentHashMap?"

--- On Tue, 12/15/09, Will McQueen <willmcqueen at yahoo.com> wrote:

From: Will McQueen <willmcqueen at yahoo.com>
Subject: ConcurrentModificationException not applicable to Map.values()?
To: concurrency-interest at cs.oswego.edu
Date: Tuesday, December 15, 2009, 5:01 PM

Hi,

I'm puzzled why ConcurrentModificationException is not thrown in the case where I use a ConcurrentHashMap instead of HashMap. The code below is single-threaded so that ConcurrrentModificationException should definitely be thrown (as opposed to "good-faith-effort"). [Ref: JCIP "Iterators and ConcurrentModificationException"]

Below is my code that shows 2 cases -- the first where ConcurrentModificationException is not thrown (that's the case I don't understand), and the second case shows where it is thrown.

This example below shows ConcurrenHashMap, which I use in the multi-threaded version of my code. So technically I could use just a HashMap for this single-threaded demo.... but then this would not reproduce this issue. That is, puzzlingly, case #1 *does* throw the expected ConcurrentModificationException when I use a HashMap instead of
 ConcurrentHashMap (!).

public final class QuickTest {
??? public static void main(String[] args) {
??????? /*** CASE 1 ****/
??????? final Map<String,String> map;
??????? map = new ConcurrentHashMap<String,String>();
??????? map.put("a","1");
??????? map.put("b", "2");
??????? map.put("c", "3");

??????? Collection<String> view = map.values();
??????? Iterator<String> itr = view.iterator();
??????? String e1 = itr.next();
??????? System.out.println(e1);
??????? view.remove(e1); //direct
 removal through the collection, not through the iterator
??????? String e2 = itr.next(); //??? Why doesn't this throw ConcurrentModificationException?
??????? System.out.println(e2);

??????? /*** CASE 2 ***/

??????? final List<String> list;
??????? list = new ArrayList<String>();
??????? list.add("1");
??????? list.add("2");
??????? list.add("3");

??????? itr = list.iterator();
??????? e1 = itr.next();
??????? System.out.println(e1);
??????? list.remove(e1); //direct removal through
 the collection, not through the iterator
??????? e2 = itr.next(); //(throws ConcurrentModificationException, as expected)
??????? System.out.println(e2);
??? }
}

Your insight and help is greatly appreciated. Thank you.

Cheers,
Will





      


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091215/a68b1b92/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Dec 15 20:07:18 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 16 Dec 2009 11:07:18 +1000
Subject: [concurrency-interest] ConcurrentModificationException
	notapplicable to Map.values()?
In-Reply-To: <29845.98379.qm@web50104.mail.re2.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEJBIEAA.davidcholmes@aapt.net.au>

Because CHM iterators support concurrency - from the CHM class javadoc:

"Similarly, Iterators and Enumerations return elements reflecting the state
of the hash table at some point at or since the creation of the
iterator/enumeration. They do not throw ConcurrentModificationException. "

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Will McQueen
  Sent: Wednesday, 16 December 2009 11:01 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] ConcurrentModificationException
notapplicable to Map.values()?


        Hi,

        I'm puzzled why ConcurrentModificationException is not thrown in the
case where I use a ConcurrentHashMap instead of HashMap. The code below is
single-threaded so that ConcurrrentModificationException should definitely
be thrown (as opposed to "good-faith-effort"). [Ref: JCIP "Iterators and
ConcurrentModificationException"]

        Below is my code that shows 2 cases -- the first where
ConcurrentModificationException is not thrown (that's the case I don't
understand), and the second case shows where it is thrown.

        This example below shows ConcurrenHashMap, which I use in the
multi-threaded version of my code. So technically I could use just a HashMap
for this single-threaded demo.... but then this would not reproduce this
issue. That is, puzzlingly, case #1 *does* throw the expected
ConcurrentModificationException when I use a HashMap instead of
ConcurrentHashMap (!).

        public final class QuickTest {
            public static void main(String[] args) {
                /*** CASE 1 ****/
                final Map<String,String> map;
                map = new ConcurrentHashMap<String,String>();
                map.put("a","1");
                map.put("b", "2");
                map.put("c", "3");

                Collection<String> view = map.values();
                Iterator<String> itr = view.iterator();
                String e1 = itr.next();
                System.out.println(e1);
                view.remove(e1); //direct removal through the collection,
not through the iterator
                String e2 = itr.next(); //??? Why doesn't this throw
ConcurrentModificationException?
                System.out.println(e2);

                /*** CASE 2 ***/

                final List<String> list;
                list = new ArrayList<String>();
                list.add("1");
                list.add("2");
                list.add("3");

                itr = list.iterator();
                e1 = itr.next();
                System.out.println(e1);
                list.remove(e1); //direct removal through the collection,
not through the iterator
                e2 = itr.next(); //(throws ConcurrentModificationException,
as expected)
                System.out.println(e2);
            }
        }

        Your insight and help is greatly appreciated. Thank you.

        Cheers,
        Will


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091216/dfc2d4fc/attachment.html>

From bronee at gmail.com  Tue Dec 15 20:21:05 2009
From: bronee at gmail.com (Brian S O'Neill)
Date: Tue, 15 Dec 2009 17:21:05 -0800
Subject: [concurrency-interest] ConcurrentModificationException
 not	applicable to Map.values()?
In-Reply-To: <29845.98379.qm@web50104.mail.re2.yahoo.com>
References: <29845.98379.qm@web50104.mail.re2.yahoo.com>
Message-ID: <4B283601.4040808@gmail.com>

Did you bother to read the clear and well-written documentation that 
accompanies ConcurrentHashMap before posting to the list?

Will McQueen wrote:
> Hi,
>
> I'm puzzled why ConcurrentModificationException is not thrown in the 
> case where I use a ConcurrentHashMap instead of HashMap. The code 
> below is single-threaded so that ConcurrrentModificationException 
> should definitely be thrown (as opposed to "good-faith-effort"). [Ref: 
> JCIP "Iterators and ConcurrentModificationException"]
>
> Below is my code that shows 2 cases -- the first where 
> ConcurrentModificationException is not thrown (that's the case I don't 
> understand), and the second case shows where it is thrown.
>
> This example below shows ConcurrenHashMap, which I use in the 
> multi-threaded version of my code. So technically I could use just a 
> HashMap for this single-threaded demo.... but then this would not 
> reproduce this issue. That is, puzzlingly, case #1 *does* throw the 
> expected ConcurrentModificationException when I use a HashMap instead 
> of ConcurrentHashMap (!).
>
> public final class QuickTest {
>     public static void main(String[] args) {
>         /*** CASE 1 ****/
>         final Map<String,String> map;
>         map = new ConcurrentHashMap<String,String>();
>         map.put("a","1");
>         map.put("b", "2");
>         map.put("c", "3");
>
>         Collection<String> view = map.values();
>         Iterator<String> itr = view.iterator();
>         String e1 = itr.next();
>         System.out.println(e1);
>         view.remove(e1); //direct removal through the collection, not 
> through the iterator
>         String e2 = itr.next(); //??? Why doesn't this throw 
> ConcurrentModificationException?
>         System.out.println(e2);
>
>         /*** CASE 2 ***/
>
>         final List<String> list;
>         list = new ArrayList<String>();
>         list.add("1");
>         list.add("2");
>         list.add("3");
>
>         itr = list.iterator();
>         e1 = itr.next();
>         System.out.println(e1);
>         list.remove(e1); //direct removal through the collection, not 
> through the iterator
>         e2 = itr.next(); //(throws ConcurrentModificationException, as 
> expected)
>         System.out.println(e2);
>     }
> }
>
> Your insight and help is greatly appreciated. Thank you.
>
> Cheers,
> Will
>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   

From opinali at gmail.com  Thu Dec 17 09:08:04 2009
From: opinali at gmail.com (Osvaldo Doederlein)
Date: Thu, 17 Dec 2009 12:08:04 -0200
Subject: [concurrency-interest] Collection.synchronizedXxx() : deadlocks,
	javadocs
Message-ID: <fb5ec5090912170608rda5415aia60a0be10c475929@mail.gmail.com>

Hi,

I just suffered the following deadlock, stress-testing my app in a JavaEE
application server:

Thread 1:
    at java.util.Collections$SynchronizedSet.hashCode(Collections.java:1658)
    - waiting to lock <0xfffffd7fdd8ce548> (a
java.util.Collections$SynchronizedSet)
Thread 2:
    at
java.util.Collections$SynchronizedCollection.size(Collections.java:1557)
    - waiting to lock <0xfffffd7fde2bb1a0> (a
java.util.Collections$SynchronizedSet)
    at java.util.AbstractSet.equals(AbstractSet.java:75)
    at java.util.Collections$SynchronizedSet.equals(Collections.java:1655)
    - locked <0xfffffd7fdd8ce548> (a java.util.Collections$SynchronizedSet)
Thread 3:
    at
java.util.Collections$SynchronizedCollection.size(Collections.java:1557)
    - waiting to lock <0xfffffd7fdd8ce548> (a
java.util.Collections$SynchronizedSet)
    at java.util.AbstractSet.equals(AbstractSet.java:75)
    at java.util.Collections$SynchronizedSet.equals(Collections.java:1655)
    - locked <0xfffffd7fde2bb1a0> (a java.util.Collections$SynchronizedSet)

This appserver has some internal Sets containing credentials information,
protected by Collections.synchronizedSet(). It seems the credentials sets
are often compared to each other, by just calling set1.equals(set2). This
causes a deadlock because equals() will invoke methods on set2, that are
also synchronized, but the appserver does this concurrently in many threads
for each application transaction. (My JVM dump reveals no less than 20 app
threads involved in this dealock: 18 of these waiting on 0xfffffd7fdd8ce548
to call hashCode(), 1 waiting on the same lock to call size(), and 1 waiting
on 0xfffffd7fde2bb1a0 also to call size().)

Now this is obviously a bug in the appserver, it should be using a lock on
the entire group of such Sets, at least for operations that involve more
than one Set like equals(). The javadocs already make clear that the
synchronized wrappers are not safe in some cases: "It is imperative that the
user manually synchronize on the returned sorted set when iterating over it
or any of its subSet, headSet, or tailSet views." But this documentation
doesn't cover other methods that may take another synchronized collection as
a parameters and may iterate it ("implementation detail" although easy to
guess), like containsAll(), retainAll(), etc. The size() method is another
special case, equals() could deadlock even when comparing two empty Sets,
because it needs minimally to invoke size() in the other collection and this
is sufficient to deadlock.

I wonder if we should have even more warnings in the javadocs, covering all
'dangerous' methods. This is not a stupid n00b bug, it's code from one of
the leading commercial application servers and I suppose its developers are
very skilled, still this bug is there (I'm going to report it today).
Generally, the synchronized wrappers are a pretty bad options, remarkably
since J2SE 5.0. I'd even go as far as suggesting that we just deprecate all
the Collections.synchronizedXxx() methods; besides deadlocks, it's way too
easy to have races by sequentially invoking multiple methods in the same
collections. The synchronizedXxx() APIs send the wrong message that locking
individual methods of the collection objects is adequate, which in my
experience is a bad (or at least fragile) assumption in 99% of the cases -
collections are just too fine-grained. There are other pitfalls, like
hashCode/equals() not delegating to the backing collection only for
synchronizedCollection().

In a separate issue, the collections returned by synchronizedXxx() are
synchronized using the original collection's monitor, so that this is safe:

Set sync1 = Collections.synchronizedSet(set1);
Set sync2 = Collections.synchronizedSet(set2);

synchronized (set1) {
  synchronized (set2) {
    if (sync1.equals(sync2) {...}
  }
}

In the equals() call above, no deadlock is possible, even with other threads
that invoke methods in sync1 or sync2, as long as any threads that use both
will perform the same explicit locking in the same order set1->set2.

Encapsulating a mutex is a common practice, but in this case I wonder if
it's a good idea, at least as implemented (there is no public method to
provide the mutex, allowing the user to trade some scalability for code
that's simple and guaranteed deadlock-free). The user may decide to write
synchronized(sync1)... to prevent deadlocks like the one I reported, and
this is dangerous precisely because it would work most of the time - only
really convoluted code would still deadlock. The javadocs don't give any
clue of how the returned collections are synchronized, which is also bad
because I cannot rely that the code above will always work - I just know it
works for the particular Sun JDK release which sources I inspected. Some
code out there should certainly use this implementation knowledge to safely
synchronize groups of synchronizedXxx()'s collections, so this
implementation cannot be changed for good or bad. Once again, I suggest
additional documentation, to make the synchronization strategy explicit and
contractual.

P.S.: I also noticed some optimization opportunities along the collections
hierarchy. For example, AbstractSet.equals(other) could be redefined in
sorted sets, when other is also sorted, to not require lookups of each value
- just iterate both in the same direction comparing each pair of values, so
the cost is O(N) on size (the generic implementation is O(N log(N)) for
SortedSet.equals(SortedSet)). Even more optimization is often possible for
exact classes: for TreeSet.equals(TreeSet), we can visit both internal trees
recursively, without the allocation and indirection costs of a Iterator,
without initially finding the first entry, and without the complication of
successor(). It's a matter of finding the sweet spot between extra
performance and extra code size and maintenance - I suggest that at least,
those cases with demonstrably Big-O advantage are considered. The current
implementation still carries many size/speed tradeoff decisions made back in
J2SE 1.2, and I'm all against bloat but collections are something really
core, and improvements (especially of Big-O scale) can mean a lot. Notice
that the special case of mixed operations with several collections of the
same general kind, or even exact class - for equals(), containsAll() etc. -
is a very common case, one that's very worth of optimizing for.

A+
Osvaldo
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091217/0862f1d2/attachment.html>

From eshioji at gmail.com  Sat Dec 19 01:21:15 2009
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 19 Dec 2009 15:21:15 +0900
Subject: [concurrency-interest] Is it a good idea to try manipulating the
	way JVM reorders?
Message-ID: <b20868900912182221p3b7f539dr49b749749c20395b@mail.gmail.com>

Hi All,

I came up with 2 kinds of solution to solve a particular problem, of
which one seems to me simple enough. The other one however, attempts
to manipulate the way JVM reorders using AtomicStampedReference to
optimize, and I was wondering whether this is a good idea (in
general).

To give a little background, a bunch of servers will compute results
of sub-tasks that belong to a single task and report them back to the
central server. This piece of code is used to register the results,
and also check whether all the subtasks for the task has completed and
if so, report that fact only once. For simplicity, null checks, access
level modifiers etc. are omitted.

The important point is that, all task must be reported once and only
once as soon as it is completed (completed means all subTaskResults
are set).



So, here is the first, simple solution:

class Task {
    //Populate with bunch of (Long, new AtomicReference()) pairs
    //Actual app uses read only HashMap
    Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();

    Semaphore permission = new Semaphore(1);

    public Task set(id, subTaskResult){
           subtasks.get(id).set(result);
           return check() ? this : null;
    }

    private boolean check(){
          for(AtomicReference ref : subtasks){
              if(ref.get()==null){
                  return false;
              }
          }//for
          return permission.tryAquire();
    }

  }//class

Now, here is a naive optimization attempt that I think is not thread-safe:

class Task {
    //Populate with bunch of (Long, new AtomicReference()) pairs
    //Actual app uses read only HashMap
    Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
    AtomicInteger counter = new AtomicInteger(subtasks.size());

    public Task set(id, subTaskResult){
           //null check omitted
           subtasks.get(id).set(result);
           //In the actual app, if !compareAndSet(null, result) return null;
           return check() ? this : null;
    }

    private boolean check(){
           return counter.decrementAndGet() == 0;
    }

  }//class

I concluded a thread can observe a decremented counter (by another
thread) before the result is set in AtomicReference (by that other
thread) because of reordering.


So, I thought for a while and came up with this idea:
Solution III

class Task {
    //Populate with bunch of (Long, new AtomicStampedReference()) pairs
    //Actual app uses read only HashMap
    Map<Id, AtomicStampedReference<SubTaskResult>> subtasks = populatedMap();
    AtomicInteger counter = new AtomicInteger(subtasks.size());

    public Task set(id, subTaskResult){
           AtomicStampedReference ref = subtasks.get(id);
           ref.set(subTaskResult,-1);
           int count = counter.decrementAndGet();
           ref.attemptStamp(subTaskResult, count);
           return count == 0;
    }

  }//class

I reasoned that, because now there is a dependency (and reordering
will be detectable), the JVM must call set(subTaskResult) before
calling decrementAndGet(), and thus this implementation becomes thread
safe.

Am I reasoning right? Also, is it permissive to try manipulating
reordering like this?

Thank you.




Note: I had posted this question also on stackoverflow:
http://stackoverflow.com/questions/1931416/is-this-java-code-thread-safe

From joe.bowbeer at gmail.com  Sat Dec 19 01:47:06 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 18 Dec 2009 22:47:06 -0800
Subject: [concurrency-interest] Is it a good idea to try manipulating
	the way JVM reorders?
In-Reply-To: <b20868900912182221p3b7f539dr49b749749c20395b@mail.gmail.com>
References: <b20868900912182221p3b7f539dr49b749749c20395b@mail.gmail.com>
Message-ID: <31f2a7bd0912182247y4a673410hffd644bcba3940d6@mail.gmail.com>

By your description, this seems like a job for CyclicBarrier (or Phaser in
Java 7).

The barrier task, triggered when all subtasks have completed, would report
to the central server.

On Fri, Dec 18, 2009 at 10:21 PM, Enno Shioji wrote:

> Hi All,
>
> I came up with 2 kinds of solution to solve a particular problem, of
> which one seems to me simple enough. The other one however, attempts
> to manipulate the way JVM reorders using AtomicStampedReference to
> optimize, and I was wondering whether this is a good idea (in
> general).
>
> To give a little background, a bunch of servers will compute results
> of sub-tasks that belong to a single task and report them back to the
> central server. This piece of code is used to register the results,
> and also check whether all the subtasks for the task has completed and
> if so, report that fact only once. For simplicity, null checks, access
> level modifiers etc. are omitted.
>
> The important point is that, all task must be reported once and only
> once as soon as it is completed (completed means all subTaskResults
> are set).
>
>
>
> So, here is the first, simple solution:
>
> class Task {
>    //Populate with bunch of (Long, new AtomicReference()) pairs
>    //Actual app uses read only HashMap
>    Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
>
>    Semaphore permission = new Semaphore(1);
>
>    public Task set(id, subTaskResult){
>           subtasks.get(id).set(result);
>           return check() ? this : null;
>    }
>
>    private boolean check(){
>          for(AtomicReference ref : subtasks){
>              if(ref.get()==null){
>                  return false;
>              }
>          }//for
>          return permission.tryAquire();
>    }
>
>  }//class
>
> Now, here is a naive optimization attempt that I think is not thread-safe:
>
> class Task {
>    //Populate with bunch of (Long, new AtomicReference()) pairs
>    //Actual app uses read only HashMap
>    Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
>    AtomicInteger counter = new AtomicInteger(subtasks.size());
>
>    public Task set(id, subTaskResult){
>           //null check omitted
>           subtasks.get(id).set(result);
>           //In the actual app, if !compareAndSet(null, result) return null;
>           return check() ? this : null;
>    }
>
>    private boolean check(){
>           return counter.decrementAndGet() == 0;
>    }
>
>  }//class
>
> I concluded a thread can observe a decremented counter (by another
> thread) before the result is set in AtomicReference (by that other
> thread) because of reordering.
>
>
> So, I thought for a while and came up with this idea:
> Solution III
>
> class Task {
>    //Populate with bunch of (Long, new AtomicStampedReference()) pairs
>    //Actual app uses read only HashMap
>    Map<Id, AtomicStampedReference<SubTaskResult>> subtasks =
> populatedMap();
>    AtomicInteger counter = new AtomicInteger(subtasks.size());
>
>    public Task set(id, subTaskResult){
>           AtomicStampedReference ref = subtasks.get(id);
>           ref.set(subTaskResult,-1);
>           int count = counter.decrementAndGet();
>           ref.attemptStamp(subTaskResult, count);
>           return count == 0;
>    }
>
>  }//class
>
> I reasoned that, because now there is a dependency (and reordering
> will be detectable), the JVM must call set(subTaskResult) before
> calling decrementAndGet(), and thus this implementation becomes thread
> safe.
>
> Am I reasoning right? Also, is it permissive to try manipulating
> reordering like this?
>
> Thank you.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091218/d414e274/attachment.html>

From eshioji at gmail.com  Sat Dec 19 03:14:36 2009
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 19 Dec 2009 17:14:36 +0900
Subject: [concurrency-interest] Is it a good idea to try manipulating
	the way JVM reorders?
In-Reply-To: <31f2a7bd0912182247y4a673410hffd644bcba3940d6@mail.gmail.com>
References: <b20868900912182221p3b7f539dr49b749749c20395b@mail.gmail.com>
	<31f2a7bd0912182247y4a673410hffd644bcba3940d6@mail.gmail.com>
Message-ID: <b20868900912190014l2480038rd77784f2cbb97474@mail.gmail.com>

Hi Joe,


Thank you for your answer!

I fail to get though how a CyclicBarrier could be used here. I mean,
we'll have bunch of threads waiting on a barrier for no good reason..
In our app, we'll have thousands of Tasks with hundreds of subtasks
per Task, so it will become a context switch nightmare..

Or am I missing something?


Regards,
Enno



On Sat, Dec 19, 2009 at 3:47 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> By your description, this seems like a job for CyclicBarrier (or Phaser in
> Java 7).
>
> The barrier task, triggered when all subtasks have completed, would report
> to the central server.
>
> On Fri, Dec 18, 2009 at 10:21 PM, Enno Shioji wrote:
>>
>> Hi All,
>>
>> I came up with 2 kinds of solution to solve a particular problem, of
>> which one seems to me simple enough. The other one however, attempts
>> to manipulate the way JVM reorders using AtomicStampedReference to
>> optimize, and I was wondering whether this is a good idea (in
>> general).
>>
>> To give a little background, a bunch of servers will compute results
>> of sub-tasks that belong to a single task and report them back to the
>> central server. This piece of code is used to register the results,
>> and also check whether all the subtasks for the task has completed and
>> if so, report that fact only once. For simplicity, null checks, access
>> level modifiers etc. are omitted.
>>
>> The important point is that, all task must be reported once and only
>> once as soon as it is completed (completed means all subTaskResults
>> are set).
>>
>>
>>
>> So, here is the first, simple solution:
>>
>> class Task {
>> ? ?//Populate with bunch of (Long, new AtomicReference()) pairs
>> ? ?//Actual app uses read only HashMap
>> ? ?Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
>>
>> ? ?Semaphore permission = new Semaphore(1);
>>
>> ? ?public Task set(id, subTaskResult){
>> ? ? ? ? ? subtasks.get(id).set(result);
>> ? ? ? ? ? return check() ? this : null;
>> ? ?}
>>
>> ? ?private boolean check(){
>> ? ? ? ? ?for(AtomicReference ref : subtasks){
>> ? ? ? ? ? ? ?if(ref.get()==null){
>> ? ? ? ? ? ? ? ? ?return false;
>> ? ? ? ? ? ? ?}
>> ? ? ? ? ?}//for
>> ? ? ? ? ?return permission.tryAquire();
>> ? ?}
>>
>> ?}//class
>>
>> Now, here is a naive optimization attempt that I think is not thread-safe:
>>
>> class Task {
>> ? ?//Populate with bunch of (Long, new AtomicReference()) pairs
>> ? ?//Actual app uses read only HashMap
>> ? ?Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
>> ? ?AtomicInteger counter = new AtomicInteger(subtasks.size());
>>
>> ? ?public Task set(id, subTaskResult){
>> ? ? ? ? ? //null check omitted
>> ? ? ? ? ? subtasks.get(id).set(result);
>> ? ? ? ? ? //In the actual app, if !compareAndSet(null, result) return
>> null;
>> ? ? ? ? ? return check() ? this : null;
>> ? ?}
>>
>> ? ?private boolean check(){
>> ? ? ? ? ? return counter.decrementAndGet() == 0;
>> ? ?}
>>
>> ?}//class
>>
>> I concluded a thread can observe a decremented counter (by another
>> thread) before the result is set in AtomicReference (by that other
>> thread) because of reordering.
>>
>>
>> So, I thought for a while and came up with this idea:
>> Solution III
>>
>> class Task {
>> ? ?//Populate with bunch of (Long, new AtomicStampedReference()) pairs
>> ? ?//Actual app uses read only HashMap
>> ? ?Map<Id, AtomicStampedReference<SubTaskResult>> subtasks =
>> populatedMap();
>> ? ?AtomicInteger counter = new AtomicInteger(subtasks.size());
>>
>> ? ?public Task set(id, subTaskResult){
>> ? ? ? ? ? AtomicStampedReference ref = subtasks.get(id);
>> ? ? ? ? ? ref.set(subTaskResult,-1);
>> ? ? ? ? ? int count = counter.decrementAndGet();
>> ? ? ? ? ? ref.attemptStamp(subTaskResult, count);
>> ? ? ? ? ? return count == 0;
>> ? ?}
>>
>> ?}//class
>>
>> I reasoned that, because now there is a dependency (and reordering
>> will be detectable), the JVM must call set(subTaskResult) before
>> calling decrementAndGet(), and thus this implementation becomes thread
>> safe.
>>
>> Am I reasoning right? Also, is it permissive to try manipulating
>> reordering like this?
>>
>> Thank you.
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From joe.bowbeer at gmail.com  Sat Dec 19 04:29:05 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 19 Dec 2009 01:29:05 -0800
Subject: [concurrency-interest] Is it a good idea to try manipulating
	the way JVM reorders?
In-Reply-To: <b20868900912190014l2480038rd77784f2cbb97474@mail.gmail.com>
References: <b20868900912182221p3b7f539dr49b749749c20395b@mail.gmail.com>
	<31f2a7bd0912182247y4a673410hffd644bcba3940d6@mail.gmail.com>
	<b20868900912190014l2480038rd77784f2cbb97474@mail.gmail.com>
Message-ID: <31f2a7bd0912190129n68f457ej1a78e233cb14ba94@mail.gmail.com>

Right.  CyclicBarrier will not do but Phaser would.

http://download.java.net/jdk7/docs/api/java/util/concurrent/Phaser.html

.. with Java 6 compatible version available from
http://gee.cs.oswego.edu/dl/concurrency-interest/

Each subtask would arrive (without waiting) until all have arrived and then
the Phaser's onAdvance action would notify the central server.

--Joe

On Sat, Dec 19, 2009 at 12:14 AM, Enno Shioji wrote:

> Hi Joe,
>
>
> Thank you for your answer!
>
> I fail to get though how a CyclicBarrier could be used here. I mean,
> we'll have bunch of threads waiting on a barrier for no good reason..
> In our app, we'll have thousands of Tasks with hundreds of subtasks
> per Task, so it will become a context switch nightmare..
>
> Or am I missing something?
>
>
> Regards,
> Enno
>
>
>
> On Sat, Dec 19, 2009 at 3:47 PM, Joe Bowbeer wrote:
> > By your description, this seems like a job for CyclicBarrier (or Phaser
> in
> > Java 7).
> >
> > The barrier task, triggered when all subtasks have completed, would
> report
> > to the central server.
> >
> > On Fri, Dec 18, 2009 at 10:21 PM, Enno Shioji wrote:
> >>
> >> Hi All,
> >>
> >> I came up with 2 kinds of solution to solve a particular problem, of
> >> which one seems to me simple enough. The other one however, attempts
> >> to manipulate the way JVM reorders using AtomicStampedReference to
> >> optimize, and I was wondering whether this is a good idea (in
> >> general).
> >>
> >> To give a little background, a bunch of servers will compute results
> >> of sub-tasks that belong to a single task and report them back to the
> >> central server. This piece of code is used to register the results,
> >> and also check whether all the subtasks for the task has completed and
> >> if so, report that fact only once. For simplicity, null checks, access
> >> level modifiers etc. are omitted.
> >>
> >> The important point is that, all task must be reported once and only
> >> once as soon as it is completed (completed means all subTaskResults
> >> are set).
> >>
> >>
> >>
> >> So, here is the first, simple solution:
> >>
> >> class Task {
> >>    //Populate with bunch of (Long, new AtomicReference()) pairs
> >>    //Actual app uses read only HashMap
> >>    Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
> >>
> >>    Semaphore permission = new Semaphore(1);
> >>
> >>    public Task set(id, subTaskResult){
> >>           subtasks.get(id).set(result);
> >>           return check() ? this : null;
> >>    }
> >>
> >>    private boolean check(){
> >>          for(AtomicReference ref : subtasks){
> >>              if(ref.get()==null){
> >>                  return false;
> >>              }
> >>          }//for
> >>          return permission.tryAquire();
> >>    }
> >>
> >>  }//class
> >>
> >> Now, here is a naive optimization attempt that I think is not
> thread-safe:
> >>
> >> class Task {
> >>    //Populate with bunch of (Long, new AtomicReference()) pairs
> >>    //Actual app uses read only HashMap
> >>    Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
> >>    AtomicInteger counter = new AtomicInteger(subtasks.size());
> >>
> >>    public Task set(id, subTaskResult){
> >>           //null check omitted
> >>           subtasks.get(id).set(result);
> >>           //In the actual app, if !compareAndSet(null, result) return
> >> null;
> >>           return check() ? this : null;
> >>    }
> >>
> >>    private boolean check(){
> >>           return counter.decrementAndGet() == 0;
> >>    }
> >>
> >>  }//class
> >>
> >> I concluded a thread can observe a decremented counter (by another
> >> thread) before the result is set in AtomicReference (by that other
> >> thread) because of reordering.
> >>
> >>
> >> So, I thought for a while and came up with this idea:
> >> Solution III
> >>
> >> class Task {
> >>    //Populate with bunch of (Long, new AtomicStampedReference()) pairs
> >>    //Actual app uses read only HashMap
> >>    Map<Id, AtomicStampedReference<SubTaskResult>> subtasks =
> >> populatedMap();
> >>    AtomicInteger counter = new AtomicInteger(subtasks.size());
> >>
> >>    public Task set(id, subTaskResult){
> >>           AtomicStampedReference ref = subtasks.get(id);
> >>           ref.set(subTaskResult,-1);
> >>           int count = counter.decrementAndGet();
> >>           ref.attemptStamp(subTaskResult, count);
> >>           return count == 0;
> >>    }
> >>
> >>  }//class
> >>
> >> I reasoned that, because now there is a dependency (and reordering
> >> will be detectable), the JVM must call set(subTaskResult) before
> >> calling decrementAndGet(), and thus this implementation becomes thread
> >> safe.
> >>
> >> Am I reasoning right? Also, is it permissive to try manipulating
> >> reordering like this?
> >>
> >> Thank you.
> >>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091219/0a501ec0/attachment-0001.html>

From davidcholmes at aapt.net.au  Sat Dec 19 05:04:01 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 19 Dec 2009 20:04:01 +1000
Subject: [concurrency-interest] Is it a good idea to try manipulating
	theway JVM reorders?
In-Reply-To: <b20868900912182221p3b7f539dr49b749749c20395b@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEJJIEAA.davidcholmes@aapt.net.au>

Enno Shioji writes:
> Now, here is a naive optimization attempt that I think is not thread-safe:
>
> class Task {
>     //Populate with bunch of (Long, new AtomicReference()) pairs
>     //Actual app uses read only HashMap
>     Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
>     AtomicInteger counter = new AtomicInteger(subtasks.size());
>
>     public Task set(id, subTaskResult){
>            //null check omitted
>            subtasks.get(id).set(result);
>            //In the actual app, if !compareAndSet(null, result)
> return null;
>            return check() ? this : null;
>     }
>
>     private boolean check(){
>            return counter.decrementAndGet() == 0;
>     }
>
>   }//class
>
> I concluded a thread can observe a decremented counter (by another
> thread) before the result is set in AtomicReference (by that other
> thread) because of reordering.

Which "another thread" are you referring to? The AtomicInteger has volatile
semantics and will be read and written by all threads storing a result, so
the results can not appear in the Map after the corresponding decrement of
the counter. For each thread the write to the map happens-before the
decrement (program order) and each decrement to a non-zero value must
happen-before the decrement to zero (counter acts as volatile). Consequently
all the results stores must happen-before a zero counter value is seen. I'd
go further and say that any thread that reads the counter value N, must be
able to see the results stored by threads that set a counter value greater
than N.

David Holmes


From eshioji at gmail.com  Sat Dec 19 05:20:49 2009
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 19 Dec 2009 19:20:49 +0900
Subject: [concurrency-interest] Is it a good idea to try manipulating
	theway JVM reorders?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEJJIEAA.davidcholmes@aapt.net.au>
References: <b20868900912182221p3b7f539dr49b749749c20395b@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEJJIEAA.davidcholmes@aapt.net.au>
Message-ID: <b20868900912190220m29d6570oc93062183bfa5a75@mail.gmail.com>

@Joe Thank you! I didn't know such utilities exist. I'll definitely
have a look at it.

Maybe I'm not understanding the Java Memory Model correctly, but for
example, is there really a difference between methodA and methodB
below?


class Sample {
     AtomicReference<Boolean> ref = new AtomicReference<Boolean>();
     Phaser phaser = getPhaser();

     methodA(){
        ref.set(Boolean.TRUE);
        phaser.arrive();
     }

     methodB(){
         phaser.arrive();
         ref.set(Boolean.TRUE);
     }
}//class

I wonder because, according to Java Concurrency in Practice:
"There is no guarantee that operations in one thread will be performed
in the order given by the program, as long as the reordering is not
detectable from within that thread even if the reordering is apparent
to other threads.[1]"

Doesn't this mean that the compiler is free to reorder
"phaser.arrive();" and "ref.set(Boolean.TRUE);" because indeed, that
reordering is not detectable within that thread, while it is apparent
to other threads? Thus, I thought you need to do something to prevent
that.


On Sat, Dec 19, 2009 at 7:04 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Enno Shioji writes:
>> Now, here is a naive optimization attempt that I think is not thread-safe:
>>
>> class Task {
>> ? ? //Populate with bunch of (Long, new AtomicReference()) pairs
>> ? ? //Actual app uses read only HashMap
>> ? ? Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
>> ? ? AtomicInteger counter = new AtomicInteger(subtasks.size());
>>
>> ? ? public Task set(id, subTaskResult){
>> ? ? ? ? ? ?//null check omitted
>> ? ? ? ? ? ?subtasks.get(id).set(result);
>> ? ? ? ? ? ?//In the actual app, if !compareAndSet(null, result)
>> return null;
>> ? ? ? ? ? ?return check() ? this : null;
>> ? ? }
>>
>> ? ? private boolean check(){
>> ? ? ? ? ? ?return counter.decrementAndGet() == 0;
>> ? ? }
>>
>> ? }//class
>>
>> I concluded a thread can observe a decremented counter (by another
>> thread) before the result is set in AtomicReference (by that other
>> thread) because of reordering.
>
> Which "another thread" are you referring to? The AtomicInteger has volatile
> semantics and will be read and written by all threads storing a result, so
> the results can not appear in the Map after the corresponding decrement of
> the counter. For each thread the write to the map happens-before the
> decrement (program order) and each decrement to a non-zero value must
> happen-before the decrement to zero (counter acts as volatile). Consequently
> all the results stores must happen-before a zero counter value is seen. I'd
> go further and say that any thread that reads the counter value N, must be
> able to see the results stored by threads that set a counter value greater
> than N.
>
> David Holmes
>
>


From davidcholmes at aapt.net.au  Sat Dec 19 05:32:52 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 19 Dec 2009 20:32:52 +1000
Subject: [concurrency-interest] Is it a good idea to try manipulating
	theway JVM reorders?
In-Reply-To: <b20868900912190220m29d6570oc93062183bfa5a75@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEJKIEAA.davidcholmes@aapt.net.au>

> Maybe I'm not understanding the Java Memory Model correctly, but for
> example, is there really a difference between methodA and methodB
> below?
>
> class Sample {
>      AtomicReference<Boolean> ref = new AtomicReference<Boolean>();
>      Phaser phaser = getPhaser();
>
>      methodA(){
>         ref.set(Boolean.TRUE);
>         phaser.arrive();
>      }
>
>      methodB(){
>          phaser.arrive();
>          ref.set(Boolean.TRUE);
>      }
> }//class
>
> I wonder because, according to Java Concurrency in Practice:
> "There is no guarantee that operations in one thread will be performed
> in the order given by the program, as long as the reordering is not
> detectable from within that thread even if the reordering is apparent
> to other threads.[1]"
>
> Doesn't this mean that the compiler is free to reorder
> "phaser.arrive();" and "ref.set(Boolean.TRUE);" because indeed, that
> reordering is not detectable within that thread, while it is apparent
> to other threads? Thus, I thought you need to do something to prevent
> that.

You _have_ done something to prevent that: you've used a synchronization
facility the introduces happens-before relationships that ensure the
reordering can't take place (at least in the real example!)

As you say, a thread calling methodA() can't tell if anything inside
methodA() gets reordered, but another thread can tell. That's fine if that
other thread hasn't executed anything that established a happens-before
relationship with an action in the first thread, but if it has then the
allowables reorderings are restricted - that is after-all what the memory
model does.

So in your real code each thread stores a result before decrementing the
counter; and each decrement of the count to N+1 happens-before the decrement
to N; hence the storage of the result by the thread that sets N+1,
happens-before the decrement to N. (Note that you can't tell in what order
the results actually happened, but they both happen before the decrement to
N.)

Hope that clarifies things.

David Holmes

>
> On Sat, Dec 19, 2009 at 7:04 PM, David Holmes
> <davidcholmes at aapt.net.au> wrote:
> > Enno Shioji writes:
> >> Now, here is a naive optimization attempt that I think is not
> thread-safe:
> >>
> >> class Task {
> >> ? ? //Populate with bunch of (Long, new AtomicReference()) pairs
> >> ? ? //Actual app uses read only HashMap
> >> ? ? Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
> >> ? ? AtomicInteger counter = new AtomicInteger(subtasks.size());
> >>
> >> ? ? public Task set(id, subTaskResult){
> >> ? ? ? ? ? ?//null check omitted
> >> ? ? ? ? ? ?subtasks.get(id).set(result);
> >> ? ? ? ? ? ?//In the actual app, if !compareAndSet(null, result)
> >> return null;
> >> ? ? ? ? ? ?return check() ? this : null;
> >> ? ? }
> >>
> >> ? ? private boolean check(){
> >> ? ? ? ? ? ?return counter.decrementAndGet() == 0;
> >> ? ? }
> >>
> >> ? }//class
> >>
> >> I concluded a thread can observe a decremented counter (by another
> >> thread) before the result is set in AtomicReference (by that other
> >> thread) because of reordering.
> >
> > Which "another thread" are you referring to? The AtomicInteger
> has volatile
> > semantics and will be read and written by all threads storing a
> result, so
> > the results can not appear in the Map after the corresponding
> decrement of
> > the counter. For each thread the write to the map happens-before the
> > decrement (program order) and each decrement to a non-zero value must
> > happen-before the decrement to zero (counter acts as volatile).
> Consequently
> > all the results stores must happen-before a zero counter value
> is seen. I'd
> > go further and say that any thread that reads the counter value
> N, must be
> > able to see the results stored by threads that set a counter
> value greater
> > than N.
> >
> > David Holmes
> >
> >



From joe.bowbeer at gmail.com  Sat Dec 19 05:51:46 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 19 Dec 2009 02:51:46 -0800
Subject: [concurrency-interest] Is it a good idea to try manipulating
	theway JVM reorders?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEJKIEAA.davidcholmes@aapt.net.au>
References: <b20868900912190220m29d6570oc93062183bfa5a75@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEJKIEAA.davidcholmes@aapt.net.au>
Message-ID: <31f2a7bd0912190251jfc2af4ckaabf3b20085c8809@mail.gmail.com>

To clarify further, I hope:

1. The Atomic* methods such as decrementAndGet perform synchronization
actions, which do impose an inter-thread ordering.  The same goes for
Phaser's methods, such as advance().

2. Phaser is suitable for this class of problem, and I think would result in
more readable code.

3. One possible weakness in all these solutions is the HashMap.  Consider
using a ConcurrentHashMap.

--Joe

On Sat, Dec 19, 2009 at 2:32 AM, David Holmes wrote:

> > Maybe I'm not understanding the Java Memory Model correctly, but for
> > example, is there really a difference between methodA and methodB
> > below?
> >
> > class Sample {
> >      AtomicReference<Boolean> ref = new AtomicReference<Boolean>();
> >      Phaser phaser = getPhaser();
> >
> >      methodA(){
> >         ref.set(Boolean.TRUE);
> >         phaser.arrive();
> >      }
> >
> >      methodB(){
> >          phaser.arrive();
> >          ref.set(Boolean.TRUE);
> >      }
> > }//class
> >
> > I wonder because, according to Java Concurrency in Practice:
> > "There is no guarantee that operations in one thread will be performed
> > in the order given by the program, as long as the reordering is not
> > detectable from within that thread even if the reordering is apparent
> > to other threads.[1]"
> >
> > Doesn't this mean that the compiler is free to reorder
> > "phaser.arrive();" and "ref.set(Boolean.TRUE);" because indeed, that
> > reordering is not detectable within that thread, while it is apparent
> > to other threads? Thus, I thought you need to do something to prevent
> > that.
>
> You _have_ done something to prevent that: you've used a synchronization
> facility the introduces happens-before relationships that ensure the
> reordering can't take place (at least in the real example!)
>
> As you say, a thread calling methodA() can't tell if anything inside
> methodA() gets reordered, but another thread can tell. That's fine if that
> other thread hasn't executed anything that established a happens-before
> relationship with an action in the first thread, but if it has then the
> allowables reorderings are restricted - that is after-all what the memory
> model does.
>
> So in your real code each thread stores a result before decrementing the
> counter; and each decrement of the count to N+1 happens-before the
> decrement
> to N; hence the storage of the result by the thread that sets N+1,
> happens-before the decrement to N. (Note that you can't tell in what order
> the results actually happened, but they both happen before the decrement to
> N.)
>
> Hope that clarifies things.
>
> David Holmes
>
> >
> > On Sat, Dec 19, 2009 at 7:04 PM, David Holmes wrote:
> > > Enno Shioji writes:
> > >> Now, here is a naive optimization attempt that I think is not
> > thread-safe:
> > >>
> > >> class Task {
> > >>     //Populate with bunch of (Long, new AtomicReference()) pairs
> > >>     //Actual app uses read only HashMap
> > >>     Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
> > >>     AtomicInteger counter = new AtomicInteger(subtasks.size());
> > >>
> > >>     public Task set(id, subTaskResult){
> > >>            //null check omitted
> > >>            subtasks.get(id).set(result);
> > >>            //In the actual app, if !compareAndSet(null, result)
> > >> return null;
> > >>            return check() ? this : null;
> > >>     }
> > >>
> > >>     private boolean check(){
> > >>            return counter.decrementAndGet() == 0;
> > >>     }
> > >>
> > >>   }//class
> > >>
> > >> I concluded a thread can observe a decremented counter (by another
> > >> thread) before the result is set in AtomicReference (by that other
> > >> thread) because of reordering.
> > >
> > > Which "another thread" are you referring to? The AtomicInteger
> > has volatile
> > > semantics and will be read and written by all threads storing a
> > result, so
> > > the results can not appear in the Map after the corresponding
> > decrement of
> > > the counter. For each thread the write to the map happens-before the
> > > decrement (program order) and each decrement to a non-zero value must
> > > happen-before the decrement to zero (counter acts as volatile).
> > Consequently
> > > all the results stores must happen-before a zero counter value
> > is seen. I'd
> > > go further and say that any thread that reads the counter value
> > N, must be
> > > able to see the results stored by threads that set a counter
> > value greater
> > > than N.
> > >
> > > David Holmes
> > >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091219/2b101d9a/attachment.html>

From eshioji at gmail.com  Sat Dec 19 05:56:13 2009
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 19 Dec 2009 19:56:13 +0900
Subject: [concurrency-interest] Is it a good idea to try manipulating
	theway JVM reorders?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEJKIEAA.davidcholmes@aapt.net.au>
References: <b20868900912190220m29d6570oc93062183bfa5a75@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEJKIEAA.davidcholmes@aapt.net.au>
Message-ID: <b20868900912190256i4fdb14d3q1fd0f632a87bc022@mail.gmail.com>

Oh, OK, I understood. Now I can sleep in peace.

By the way, you are one of the authors of Java Concurrency in Practice, right?
I can't thank you enough! It helped me A LOT with my projects.

Thanks for your time, and good night!

Regards,
Enno




On Sat, Dec 19, 2009 at 7:32 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>> Maybe I'm not understanding the Java Memory Model correctly, but for
>> example, is there really a difference between methodA and methodB
>> below?
>>
>> class Sample {
>> ? ? ?AtomicReference<Boolean> ref = new AtomicReference<Boolean>();
>> ? ? ?Phaser phaser = getPhaser();
>>
>> ? ? ?methodA(){
>> ? ? ? ? ref.set(Boolean.TRUE);
>> ? ? ? ? phaser.arrive();
>> ? ? ?}
>>
>> ? ? ?methodB(){
>> ? ? ? ? ?phaser.arrive();
>> ? ? ? ? ?ref.set(Boolean.TRUE);
>> ? ? ?}
>> }//class
>>
>> I wonder because, according to Java Concurrency in Practice:
>> "There is no guarantee that operations in one thread will be performed
>> in the order given by the program, as long as the reordering is not
>> detectable from within that thread even if the reordering is apparent
>> to other threads.[1]"
>>
>> Doesn't this mean that the compiler is free to reorder
>> "phaser.arrive();" and "ref.set(Boolean.TRUE);" because indeed, that
>> reordering is not detectable within that thread, while it is apparent
>> to other threads? Thus, I thought you need to do something to prevent
>> that.
>
> You _have_ done something to prevent that: you've used a synchronization
> facility the introduces happens-before relationships that ensure the
> reordering can't take place (at least in the real example!)
>
> As you say, a thread calling methodA() can't tell if anything inside
> methodA() gets reordered, but another thread can tell. That's fine if that
> other thread hasn't executed anything that established a happens-before
> relationship with an action in the first thread, but if it has then the
> allowables reorderings are restricted - that is after-all what the memory
> model does.
>
> So in your real code each thread stores a result before decrementing the
> counter; and each decrement of the count to N+1 happens-before the decrement
> to N; hence the storage of the result by the thread that sets N+1,
> happens-before the decrement to N. (Note that you can't tell in what order
> the results actually happened, but they both happen before the decrement to
> N.)
>
> Hope that clarifies things.
>
> David Holmes
>
>>
>> On Sat, Dec 19, 2009 at 7:04 PM, David Holmes
>> <davidcholmes at aapt.net.au> wrote:
>> > Enno Shioji writes:
>> >> Now, here is a naive optimization attempt that I think is not
>> thread-safe:
>> >>
>> >> class Task {
>> >> ? ? //Populate with bunch of (Long, new AtomicReference()) pairs
>> >> ? ? //Actual app uses read only HashMap
>> >> ? ? Map<Id, AtomicReference<SubTaskResult>> subtasks = populatedMap();
>> >> ? ? AtomicInteger counter = new AtomicInteger(subtasks.size());
>> >>
>> >> ? ? public Task set(id, subTaskResult){
>> >> ? ? ? ? ? ?//null check omitted
>> >> ? ? ? ? ? ?subtasks.get(id).set(result);
>> >> ? ? ? ? ? ?//In the actual app, if !compareAndSet(null, result)
>> >> return null;
>> >> ? ? ? ? ? ?return check() ? this : null;
>> >> ? ? }
>> >>
>> >> ? ? private boolean check(){
>> >> ? ? ? ? ? ?return counter.decrementAndGet() == 0;
>> >> ? ? }
>> >>
>> >> ? }//class
>> >>
>> >> I concluded a thread can observe a decremented counter (by another
>> >> thread) before the result is set in AtomicReference (by that other
>> >> thread) because of reordering.
>> >
>> > Which "another thread" are you referring to? The AtomicInteger
>> has volatile
>> > semantics and will be read and written by all threads storing a
>> result, so
>> > the results can not appear in the Map after the corresponding
>> decrement of
>> > the counter. For each thread the write to the map happens-before the
>> > decrement (program order) and each decrement to a non-zero value must
>> > happen-before the decrement to zero (counter acts as volatile).
>> Consequently
>> > all the results stores must happen-before a zero counter value
>> is seen. I'd
>> > go further and say that any thread that reads the counter value
>> N, must be
>> > able to see the results stored by threads that set a counter
>> value greater
>> > than N.
>> >
>> > David Holmes
>> >
>> >
>
>


From eshioji at gmail.com  Sat Dec 19 06:24:41 2009
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 19 Dec 2009 20:24:41 +0900
Subject: [concurrency-interest] Is it a good idea to try manipulating
	theway JVM reorders?
In-Reply-To: <31f2a7bd0912190251jfc2af4ckaabf3b20085c8809@mail.gmail.com>
References: <b20868900912190220m29d6570oc93062183bfa5a75@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEJKIEAA.davidcholmes@aapt.net.au>
	<31f2a7bd0912190251jfc2af4ckaabf3b20085c8809@mail.gmail.com>
Message-ID: <b20868900912190324p5b4992dh5cfdca0ce55be89@mail.gmail.com>

Thanks again for the clarification! I think now I see the picture.

> 3. One possible weakness in all these solutions is the HashMap.  Consider
> using a ConcurrentHashMap.
I was intending to use an unmodifiable HashMap that will be populated
once at object creation, and will not be modified afterwards, like
this:

class Sample {
   private final Map<Id, AtomicReference> map;

   public Task(){
      this.map = Collections.unmodifiableMap(getPopulatedMap());
   }

   private static Map<Id, AtomicReference> getPopulatedMap(){
       //instantiate HashMap and populate it with (Id, AtomicReference) pairs
   }
}//class

This is okay, right?


Regards,
Enno




On Sat, Dec 19, 2009 at 7:51 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> To clarify further, I hope:
>
> 1. The Atomic* methods such as decrementAndGet perform synchronization
> actions, which do impose an inter-thread ordering.? The same goes for
> Phaser's methods, such as advance().
>
> 2. Phaser is suitable for this class of problem, and I think would result in
> more readable code.
>
> 3. One possible weakness in all these solutions is the HashMap.? Consider
> using a ConcurrentHashMap.
>
> --Joe
>
> On Sat, Dec 19, 2009 at 2:32 AM, David Holmes wrote:
>>
>> > Maybe I'm not understanding the Java Memory Model correctly, but for
>> > example, is there really a difference between methodA and methodB
>> > below?
>> >
>> > class Sample {
>> > ? ? ?AtomicReference<Boolean> ref = new AtomicReference<Boolean>();
>> > ? ? ?Phaser phaser = getPhaser();
>> >
>> > ? ? ?methodA(){
>> > ? ? ? ? ref.set(Boolean.TRUE);
>> > ? ? ? ? phaser.arrive();
>> > ? ? ?}
>> >
>> > ? ? ?methodB(){
>> > ? ? ? ? ?phaser.arrive();
>> > ? ? ? ? ?ref.set(Boolean.TRUE);
>> > ? ? ?}
>> > }//class
>> >
>> > I wonder because, according to Java Concurrency in Practice:
>> > "There is no guarantee that operations in one thread will be performed
>> > in the order given by the program, as long as the reordering is not
>> > detectable from within that thread even if the reordering is apparent
>> > to other threads.[1]"
>> >
>> > Doesn't this mean that the compiler is free to reorder
>> > "phaser.arrive();" and "ref.set(Boolean.TRUE);" because indeed, that
>> > reordering is not detectable within that thread, while it is apparent
>> > to other threads? Thus, I thought you need to do something to prevent
>> > that.
>>
>> You _have_ done something to prevent that: you've used a synchronization
>> facility the introduces happens-before relationships that ensure the
>> reordering can't take place (at least in the real example!)
>>
>> As you say, a thread calling methodA() can't tell if anything inside
>> methodA() gets reordered, but another thread can tell. That's fine if that
>> other thread hasn't executed anything that established a happens-before
>> relationship with an action in the first thread, but if it has then the
>> allowables reorderings are restricted - that is after-all what the memory
>> model does.
>>
>> So in your real code each thread stores a result before decrementing the
>> counter; and each decrement of the count to N+1 happens-before the
>> decrement
>> to N; hence the storage of the result by the thread that sets N+1,
>> happens-before the decrement to N. (Note that you can't tell in what order
>> the results actually happened, but they both happen before the decrement
>> to
>> N.)
>>
>> Hope that clarifies things.
>>
>> David Holmes
>>
>> >
>> > On Sat, Dec 19, 2009 at 7:04 PM, David Holmes wrote:
>> > > Enno Shioji writes:
>> > >> Now, here is a naive optimization attempt that I think is not
>> > thread-safe:
>> > >>
>> > >> class Task {
>> > >> ? ? //Populate with bunch of (Long, new AtomicReference()) pairs
>> > >> ? ? //Actual app uses read only HashMap
>> > >> ? ? Map<Id, AtomicReference<SubTaskResult>> subtasks =
>> > >> populatedMap();
>> > >> ? ? AtomicInteger counter = new AtomicInteger(subtasks.size());
>> > >>
>> > >> ? ? public Task set(id, subTaskResult){
>> > >> ? ? ? ? ? ?//null check omitted
>> > >> ? ? ? ? ? ?subtasks.get(id).set(result);
>> > >> ? ? ? ? ? ?//In the actual app, if !compareAndSet(null, result)
>> > >> return null;
>> > >> ? ? ? ? ? ?return check() ? this : null;
>> > >> ? ? }
>> > >>
>> > >> ? ? private boolean check(){
>> > >> ? ? ? ? ? ?return counter.decrementAndGet() == 0;
>> > >> ? ? }
>> > >>
>> > >> ? }//class
>> > >>
>> > >> I concluded a thread can observe a decremented counter (by another
>> > >> thread) before the result is set in AtomicReference (by that other
>> > >> thread) because of reordering.
>> > >
>> > > Which "another thread" are you referring to? The AtomicInteger
>> > has volatile
>> > > semantics and will be read and written by all threads storing a
>> > result, so
>> > > the results can not appear in the Map after the corresponding
>> > decrement of
>> > > the counter. For each thread the write to the map happens-before the
>> > > decrement (program order) and each decrement to a non-zero value must
>> > > happen-before the decrement to zero (counter acts as volatile).
>> > Consequently
>> > > all the results stores must happen-before a zero counter value
>> > is seen. I'd
>> > > go further and say that any thread that reads the counter value
>> > N, must be
>> > > able to see the results stored by threads that set a counter
>> > value greater
>> > > than N.
>> > >
>> > > David Holmes
>> > >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From joe.bowbeer at gmail.com  Sat Dec 19 06:44:50 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 19 Dec 2009 03:44:50 -0800
Subject: [concurrency-interest] Is it a good idea to try manipulating
	theway JVM reorders?
In-Reply-To: <b20868900912190324p5b4992dh5cfdca0ce55be89@mail.gmail.com>
References: <b20868900912190220m29d6570oc93062183bfa5a75@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEJKIEAA.davidcholmes@aapt.net.au>
	<31f2a7bd0912190251jfc2af4ckaabf3b20085c8809@mail.gmail.com>
	<b20868900912190324p5b4992dh5cfdca0ce55be89@mail.gmail.com>
Message-ID: <31f2a7bd0912190344pf9b5137r4b07f40da3adfb39@mail.gmail.com>

A read-only HashMap is OK if, for example, it is created before any of the
tasks are scheduled for execution.

The necessary conditions for Safe Publication are covered in Section 3.5:

  http://javaconcurrencyinpractice.com/

Some flavors of read-only HashMap, such as an access-ordered LinkedHashMap,
would not be OK...

--Joe

On Sat, Dec 19, 2009 at 3:24 AM, Enno Shioji wrote:

> Thanks again for the clarification! I think now I see the picture.
>
> > 3. One possible weakness in all these solutions is the HashMap.  Consider
> > using a ConcurrentHashMap.
> I was intending to use an unmodifiable HashMap that will be populated
> once at object creation, and will not be modified afterwards, like
> this:
>
> class Sample {
>   private final Map<Id, AtomicReference> map;
>
>   public Task(){
>      this.map = Collections.unmodifiableMap(getPopulatedMap());
>   }
>
>   private static Map<Id, AtomicReference> getPopulatedMap(){
>       //instantiate HashMap and populate it with (Id, AtomicReference)
> pairs
>   }
> }//class
>
> This is okay, right?
>
>
> Regards,
> Enno
>
>
>
>
> On Sat, Dec 19, 2009 at 7:51 PM, Joe Bowbeer wrote:
> > To clarify further, I hope:
> >
> > 1. The Atomic* methods such as decrementAndGet perform synchronization
> > actions, which do impose an inter-thread ordering.  The same goes for
> > Phaser's methods, such as advance().
> >
> > 2. Phaser is suitable for this class of problem, and I think would result
> in
> > more readable code.
> >
> > 3. One possible weakness in all these solutions is the HashMap.  Consider
> > using a ConcurrentHashMap.
> >
> > --Joe
> >
> > On Sat, Dec 19, 2009 at 2:32 AM, David Holmes wrote:
> >>
> >> > Maybe I'm not understanding the Java Memory Model correctly, but for
> >> > example, is there really a difference between methodA and methodB
> >> > below?
> >> >
> >> > class Sample {
> >> >      AtomicReference<Boolean> ref = new AtomicReference<Boolean>();
> >> >      Phaser phaser = getPhaser();
> >> >
> >> >      methodA(){
> >> >         ref.set(Boolean.TRUE);
> >> >         phaser.arrive();
> >> >      }
> >> >
> >> >      methodB(){
> >> >          phaser.arrive();
> >> >          ref.set(Boolean.TRUE);
> >> >      }
> >> > }//class
> >> >
> >> > I wonder because, according to Java Concurrency in Practice:
> >> > "There is no guarantee that operations in one thread will be performed
> >> > in the order given by the program, as long as the reordering is not
> >> > detectable from within that thread even if the reordering is apparent
> >> > to other threads.[1]"
> >> >
> >> > Doesn't this mean that the compiler is free to reorder
> >> > "phaser.arrive();" and "ref.set(Boolean.TRUE);" because indeed, that
> >> > reordering is not detectable within that thread, while it is apparent
> >> > to other threads? Thus, I thought you need to do something to prevent
> >> > that.
> >>
> >> You _have_ done something to prevent that: you've used a synchronization
> >> facility the introduces happens-before relationships that ensure the
> >> reordering can't take place (at least in the real example!)
> >>
> >> As you say, a thread calling methodA() can't tell if anything inside
> >> methodA() gets reordered, but another thread can tell. That's fine if
> that
> >> other thread hasn't executed anything that established a happens-before
> >> relationship with an action in the first thread, but if it has then the
> >> allowables reorderings are restricted - that is after-all what the
> memory
> >> model does.
> >>
> >> So in your real code each thread stores a result before decrementing the
> >> counter; and each decrement of the count to N+1 happens-before the
> >> decrement
> >> to N; hence the storage of the result by the thread that sets N+1,
> >> happens-before the decrement to N. (Note that you can't tell in what
> order
> >> the results actually happened, but they both happen before the decrement
> >> to
> >> N.)
> >>
> >> Hope that clarifies things.
> >>
> >> David Holmes
> >>
> >> >
> >> > On Sat, Dec 19, 2009 at 7:04 PM, David Holmes wrote:
> >> > > Enno Shioji writes:
> >> > >> Now, here is a naive optimization attempt that I think is not
> >> > thread-safe:
> >> > >>
> >> > >> class Task {
> >> > >>     //Populate with bunch of (Long, new AtomicReference()) pairs
> >> > >>     //Actual app uses read only HashMap
> >> > >>     Map<Id, AtomicReference<SubTaskResult>> subtasks =
> >> > >> populatedMap();
> >> > >>     AtomicInteger counter = new AtomicInteger(subtasks.size());
> >> > >>
> >> > >>     public Task set(id, subTaskResult){
> >> > >>            //null check omitted
> >> > >>            subtasks.get(id).set(result);
> >> > >>            //In the actual app, if !compareAndSet(null, result)
> >> > >> return null;
> >> > >>            return check() ? this : null;
> >> > >>     }
> >> > >>
> >> > >>     private boolean check(){
> >> > >>            return counter.decrementAndGet() == 0;
> >> > >>     }
> >> > >>
> >> > >>   }//class
> >> > >>
> >> > >> I concluded a thread can observe a decremented counter (by another
> >> > >> thread) before the result is set in AtomicReference (by that other
> >> > >> thread) because of reordering.
> >> > >
> >> > > Which "another thread" are you referring to? The AtomicInteger
> >> > has volatile
> >> > > semantics and will be read and written by all threads storing a
> >> > result, so
> >> > > the results can not appear in the Map after the corresponding
> >> > decrement of
> >> > > the counter. For each thread the write to the map happens-before the
> >> > > decrement (program order) and each decrement to a non-zero value
> must
> >> > > happen-before the decrement to zero (counter acts as volatile).
> >> > Consequently
> >> > > all the results stores must happen-before a zero counter value
> >> > is seen. I'd
> >> > > go further and say that any thread that reads the counter value
> >> > N, must be
> >> > > able to see the results stored by threads that set a counter
> >> > value greater
> >> > > than N.
> >> > >
> >> > > David Holmes
> >> > >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091219/804dc22a/attachment.html>

From eshioji at gmail.com  Sat Dec 19 07:10:52 2009
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 19 Dec 2009 21:10:52 +0900
Subject: [concurrency-interest] Is it a good idea to try manipulating
	theway JVM reorders?
In-Reply-To: <31f2a7bd0912190344pf9b5137r4b07f40da3adfb39@mail.gmail.com>
References: <b20868900912190220m29d6570oc93062183bfa5a75@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEJKIEAA.davidcholmes@aapt.net.au>
	<31f2a7bd0912190251jfc2af4ckaabf3b20085c8809@mail.gmail.com>
	<b20868900912190324p5b4992dh5cfdca0ce55be89@mail.gmail.com>
	<31f2a7bd0912190344pf9b5137r4b07f40da3adfb39@mail.gmail.com>
Message-ID: <b20868900912190410l653c1c46ycfd2f4a497d3c753@mail.gmail.com>

I understand. And I see you are also one of the authors of JCP!
Man, you guys keep helping me.

Thanks again for your time and for the book.
Have a good night!


Regards,
Enno



On Sat, Dec 19, 2009 at 8:44 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> A read-only HashMap is OK if, for example, it is created before any of the
> tasks are scheduled for execution.
>
> The necessary conditions for Safe Publication are covered in Section 3.5:
>
> ? http://javaconcurrencyinpractice.com/
>
> Some flavors of read-only HashMap, such as an access-ordered LinkedHashMap,
> would not be OK...
>
> --Joe
>
> On Sat, Dec 19, 2009 at 3:24 AM, Enno Shioji wrote:
>>
>> Thanks again for the clarification! I think now I see the picture.
>>
>> > 3. One possible weakness in all these solutions is the HashMap.
>> > ?Consider
>> > using a ConcurrentHashMap.
>> I was intending to use an unmodifiable HashMap that will be populated
>> once at object creation, and will not be modified afterwards, like
>> this:
>>
>> class Sample {
>> ? private final Map<Id, AtomicReference> map;
>>
>> ? public Task(){
>> ? ? ?this.map = Collections.unmodifiableMap(getPopulatedMap());
>> ? }
>>
>> ? private static Map<Id, AtomicReference> getPopulatedMap(){
>> ? ? ? //instantiate HashMap and populate it with (Id, AtomicReference)
>> pairs
>> ? }
>> }//class
>>
>> This is okay, right?
>>
>>
>> Regards,
>> Enno
>>
>>
>>
>>
>> On Sat, Dec 19, 2009 at 7:51 PM, Joe Bowbeer wrote:
>> > To clarify further, I hope:
>> >
>> > 1. The Atomic* methods such as decrementAndGet perform synchronization
>> > actions, which do impose an inter-thread ordering.? The same goes for
>> > Phaser's methods, such as advance().
>> >
>> > 2. Phaser is suitable for this class of problem, and I think would
>> > result in
>> > more readable code.
>> >
>> > 3. One possible weakness in all these solutions is the HashMap.
>> > Consider
>> > using a ConcurrentHashMap.
>> >
>> > --Joe
>> >
>> > On Sat, Dec 19, 2009 at 2:32 AM, David Holmes wrote:
>> >>
>> >> > Maybe I'm not understanding the Java Memory Model correctly, but for
>> >> > example, is there really a difference between methodA and methodB
>> >> > below?
>> >> >
>> >> > class Sample {
>> >> > ? ? ?AtomicReference<Boolean> ref = new AtomicReference<Boolean>();
>> >> > ? ? ?Phaser phaser = getPhaser();
>> >> >
>> >> > ? ? ?methodA(){
>> >> > ? ? ? ? ref.set(Boolean.TRUE);
>> >> > ? ? ? ? phaser.arrive();
>> >> > ? ? ?}
>> >> >
>> >> > ? ? ?methodB(){
>> >> > ? ? ? ? ?phaser.arrive();
>> >> > ? ? ? ? ?ref.set(Boolean.TRUE);
>> >> > ? ? ?}
>> >> > }//class
>> >> >
>> >> > I wonder because, according to Java Concurrency in Practice:
>> >> > "There is no guarantee that operations in one thread will be
>> >> > performed
>> >> > in the order given by the program, as long as the reordering is not
>> >> > detectable from within that thread even if the reordering is apparent
>> >> > to other threads.[1]"
>> >> >
>> >> > Doesn't this mean that the compiler is free to reorder
>> >> > "phaser.arrive();" and "ref.set(Boolean.TRUE);" because indeed, that
>> >> > reordering is not detectable within that thread, while it is apparent
>> >> > to other threads? Thus, I thought you need to do something to prevent
>> >> > that.
>> >>
>> >> You _have_ done something to prevent that: you've used a
>> >> synchronization
>> >> facility the introduces happens-before relationships that ensure the
>> >> reordering can't take place (at least in the real example!)
>> >>
>> >> As you say, a thread calling methodA() can't tell if anything inside
>> >> methodA() gets reordered, but another thread can tell. That's fine if
>> >> that
>> >> other thread hasn't executed anything that established a happens-before
>> >> relationship with an action in the first thread, but if it has then the
>> >> allowables reorderings are restricted - that is after-all what the
>> >> memory
>> >> model does.
>> >>
>> >> So in your real code each thread stores a result before decrementing
>> >> the
>> >> counter; and each decrement of the count to N+1 happens-before the
>> >> decrement
>> >> to N; hence the storage of the result by the thread that sets N+1,
>> >> happens-before the decrement to N. (Note that you can't tell in what
>> >> order
>> >> the results actually happened, but they both happen before the
>> >> decrement
>> >> to
>> >> N.)
>> >>
>> >> Hope that clarifies things.
>> >>
>> >> David Holmes
>> >>
>> >> >
>> >> > On Sat, Dec 19, 2009 at 7:04 PM, David Holmes wrote:
>> >> > > Enno Shioji writes:
>> >> > >> Now, here is a naive optimization attempt that I think is not
>> >> > thread-safe:
>> >> > >>
>> >> > >> class Task {
>> >> > >> ? ? //Populate with bunch of (Long, new AtomicReference()) pairs
>> >> > >> ? ? //Actual app uses read only HashMap
>> >> > >> ? ? Map<Id, AtomicReference<SubTaskResult>> subtasks =
>> >> > >> populatedMap();
>> >> > >> ? ? AtomicInteger counter = new AtomicInteger(subtasks.size());
>> >> > >>
>> >> > >> ? ? public Task set(id, subTaskResult){
>> >> > >> ? ? ? ? ? ?//null check omitted
>> >> > >> ? ? ? ? ? ?subtasks.get(id).set(result);
>> >> > >> ? ? ? ? ? ?//In the actual app, if !compareAndSet(null, result)
>> >> > >> return null;
>> >> > >> ? ? ? ? ? ?return check() ? this : null;
>> >> > >> ? ? }
>> >> > >>
>> >> > >> ? ? private boolean check(){
>> >> > >> ? ? ? ? ? ?return counter.decrementAndGet() == 0;
>> >> > >> ? ? }
>> >> > >>
>> >> > >> ? }//class
>> >> > >>
>> >> > >> I concluded a thread can observe a decremented counter (by another
>> >> > >> thread) before the result is set in AtomicReference (by that other
>> >> > >> thread) because of reordering.
>> >> > >
>> >> > > Which "another thread" are you referring to? The AtomicInteger
>> >> > has volatile
>> >> > > semantics and will be read and written by all threads storing a
>> >> > result, so
>> >> > > the results can not appear in the Map after the corresponding
>> >> > decrement of
>> >> > > the counter. For each thread the write to the map happens-before
>> >> > > the
>> >> > > decrement (program order) and each decrement to a non-zero value
>> >> > > must
>> >> > > happen-before the decrement to zero (counter acts as volatile).
>> >> > Consequently
>> >> > > all the results stores must happen-before a zero counter value
>> >> > is seen. I'd
>> >> > > go further and say that any thread that reads the counter value
>> >> > N, must be
>> >> > > able to see the results stored by threads that set a counter
>> >> > value greater
>> >> > > than N.
>> >> > >
>> >> > > David Holmes
>> >> > >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From G.Arora at iontrading.com  Fri Dec 25 04:09:42 2009
From: G.Arora at iontrading.com (Gaurav Arora)
Date: Fri, 25 Dec 2009 09:09:42 -0000
Subject: [concurrency-interest] Putting tasks directly on TPE queue
Message-ID: <7E7A48FF6AFD9E42A812907387ADA44C05DA6B45@w2k3msx01.ranplc.co.uk>

I have read a few threads dating back 2007 about putting tasks directly
on a TPE queue. The general reply (as is mentioned in the API as well)
is to not access the queue directly. In all those threads, the
corePoolSize and the maximumPoolSize were different. What if the core
pool size is the same as the maximum pool size and all core threads are
started before any tasks are submitted on the queue? Is it still a bad
idea to put tasks directly on the queue? Can it can cause hard to track
down failures?

 

Gaurav




Gaurav Arora
ION Trading
4th Floor, Tower B, Logix Cyber Park, Plot C - 28 & 29, Sector - 62, Noida - 201 301, Uttar Pradesh, INDIA
T: +91 120 4628 400

G.Arora at iontrading.com
http://www.iontrading.com/

***********************************************
This email and any attachments may contain information which is confidential and/or privileged. The information is intended exclusively for the addressee and the views expressed may not be official policy, but the personal views of the originator. If you are not the intended recipient, be aware that any disclosure, copying, distribution or use of the contents is prohibited. If you have received this email and any file transmitted with it in error, please notify the sender by telephone or return email immediately and delete the material from your computer. Internet communications are not secure and ION Trading is not responsible for their abuse by third parties, nor for any alteration or corruption in transmission, nor for any damage or loss caused by any virus or other defect. ION Trading accepts no liability or responsibility arising out of or in any way connected to this email.
***********************************************


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091225/2da0667e/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/gif
Size: 619 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091225/2da0667e/attachment.gif>

From gustav.trede at gmail.com  Fri Dec 25 06:14:17 2009
From: gustav.trede at gmail.com (gustav trede)
Date: Fri, 25 Dec 2009 12:14:17 +0100
Subject: [concurrency-interest] Putting tasks directly on TPE queue
In-Reply-To: <7E7A48FF6AFD9E42A812907387ADA44C05DA6B45@w2k3msx01.ranplc.co.uk>
References: <7E7A48FF6AFD9E42A812907387ADA44C05DA6B45@w2k3msx01.ranplc.co.uk>
Message-ID: <311e0eaf0912250314t20311823nd75cfb9cdf881f39@mail.gmail.com>

2009/12/25 Gaurav Arora <G.Arora at iontrading.com>

>   I have read a few threads dating back 2007 about putting tasks directly
> on a TPE queue. The general reply (as is mentioned in the API as well) is to
> not access the queue directly. In all those threads, the corePoolSize and
> the maximumPoolSize were different. What if the core pool size is the same
> as the maximum pool size and all core threads are started before any tasks
> are submitted on the queue? Is it still a bad idea to put tasks directly on
> the queue? Can it can cause hard to track down failures?
>
> We use that concept in Grizzly, depending on reconfigure(threadpoolconfig
config) method params we use LTQ if possible,
switching pool impl transparent in runtime.

https://grizzly.dev.java.net/source/browse/grizzly/trunk/code/modules/utils/src/main/java/com/sun/grizzly/util/FixedThreadPool.java?rev=4055<https://grizzly.dev.java.net/source/browse/grizzly/trunk/code/modules/utils/src/main/java/com/sun/grizzly/util/FixedThreadPool.java?rev=4055&view=markup>

https://grizzly.dev.java.net/source/browse/grizzly/trunk/code/modules/utils/src/main/java/com/sun/grizzly/util/GrizzlyExecutorService.java?rev=4054

-- 
regards
 gustav trede
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091225/411d503d/attachment.html>

From joe.bowbeer at gmail.com  Fri Dec 25 14:27:56 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 25 Dec 2009 11:27:56 -0800
Subject: [concurrency-interest] Putting tasks directly on TPE queue
In-Reply-To: <7E7A48FF6AFD9E42A812907387ADA44C05DA6B45@w2k3msx01.ranplc.co.uk>
References: <7E7A48FF6AFD9E42A812907387ADA44C05DA6B45@w2k3msx01.ranplc.co.uk>
Message-ID: <31f2a7bd0912251127n6454dc6bv35f5cd695a4410e6@mail.gmail.com>

Why wouldn't you want to add tasks to the queue using TPE.execute ?

As you write: accessing the queue midstream isn't supported.  The getQueue
method is only provided for debugging and monitoring.

Whether your app can get away with it may depend on implementation details
of a specific revision of ThreadPoolExecutor.  Search for occurrences of
workQueue, and workQueue.isEmpty() in particular, in the source code for
your chosen revision of TPE :)

--Joe

On Fri, Dec 25, 2009 at 1:09 AM, Gaurav Arora <G.Arora at iontrading.com>wrote:

>   I have read a few threads dating back 2007 about putting tasks directly
> on a TPE queue. The general reply (as is mentioned in the API as well) is to
> not access the queue directly. In all those threads, the corePoolSize and
> the maximumPoolSize were different. What if the core pool size is the same
> as the maximum pool size and all core threads are started before any tasks
> are submitted on the queue? Is it still a bad idea to put tasks directly on
> the queue? Can it can cause hard to track down failures?
>
> Gaurav
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091225/a04c36bd/attachment.html>

From gauravty at in.ibm.com  Fri Dec 25 17:32:16 2009
From: gauravty at in.ibm.com (Gaurav Kant Tyagi)
Date: Sat, 26 Dec 2009 04:02:16 +0530
Subject: [concurrency-interest] Gaurav Kant Tyagi is out of the office.
Message-ID: <OF1C859D7B.B86A1252-ON65257697.007BCE19-65257697.007BCE19@in.ibm.com>

I will be out of the office starting  12/24/2009 and will not return until
01/04/2010.

I will respond to your message when I return.

CST coverage: Yogesh Kulkarni is my back-up for any project related
request.
IST coverage: Ashish Birla will be my back up for project related
activities.


From alarmnummer at gmail.com  Sat Dec 26 19:00:10 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 27 Dec 2009 01:00:10 +0100
Subject: [concurrency-interest] good design vs performance (object creation)
Message-ID: <1466c1d60912261600x2cc48a4cj382f81249e405e2@mail.gmail.com>

Hi Guys,

although the question is not directly concurrency related, I want to
place it on this mailinglist because here are a lot of guys that
encounter the issue from time to time.

I'm currently working on a STM implementation and one of the things
that I have found out is that object creation can cause quite a big
slowdown, especially if done
millions of times per second (e.g. 1M to 20M times a second)  If I
look at the code of java.util.concurrent I see a lot of
nodes/iterators being created for example and this can
be a great thing for good oo design but my experience is that it can
cause a considerable slow down.

My question is: how much should the design of a class be influenced by
preventing object creation? I'm at the point where I need to rewrite
the same algorithm
multiple times for different datastructures (e.g. array based versus
map based.. where an array is faster for very small collections). I
could create an ArrayIterator
so that the algorithm is able to work with iterators and it doesn't
matter which data structure is used, but it requires an additional
iterator object. I also know that
garbage collectors are quite smart and that very short lived objects
are very cheap to be garbage collected. I also know that some modern
jvm's are able
to allocate objects in the stack instead of the heap, so even less
worries about gc overhead.

I need some feedback/guidelines from developers that have experienced
this in the trenches and really know what they are talking about. How
for should one go?
I would be very happy to stop ruining my code... but on the other side
I also want a good performance

Peter

From dhanji at gmail.com  Sat Dec 26 21:42:00 2009
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Sun, 27 Dec 2009 13:42:00 +1100
Subject: [concurrency-interest] good design vs performance (object
	creation)
In-Reply-To: <1466c1d60912261600x2cc48a4cj382f81249e405e2@mail.gmail.com>
References: <1466c1d60912261600x2cc48a4cj382f81249e405e2@mail.gmail.com>
Message-ID: <aa067ea10912261842x87b6d58lb87101bd14516598@mail.gmail.com>

In general you want to write your code and prove its correctness first, and
then optimize for performance. If your code is readable and maintainable
this becomes easier to do after the fact. You should also never optimize
code without being able to establish a clear, reproduceable performance
bottleneck that can be *described* (not merely observed).

As you note creation of thousands or even millions of objects is not often a
serious concern if they are short lived and the GC is tuned accordingly.
Particularly for things like iterators and so on.

In my experience the real problems arise in two very different scenarios:
- Premature optimations: growable caches, pools, SoftReferences etc. This is
when you accidentally make objects long lived (for instance, to prevent
their number from getting out of hand). This can really hurt GC performance.
- Code immuaturity: This is the opposite end of the spectrum where you just
assume Java takes care of memory management for you altogether. A classic
example of this is calling clear() on an expanding array list and forgetting
to call trimToSize(). Or the symmetrical action with j.u.HashMap. Creating
large numbers of objects in an inner loop is similarly a problem.

A good library should go through a thorough memory complexity analysis for
each algorithm or utility introduced. This, almost always, will point you
directly to a leak or potential bottleneck. When writing a concurrent data
structure, I typically set up an expectation of computational complexity and
walk through the various ways in which I can trade correctness for
performance (a concurrent LRU cache is a good example).

Like everything else in engineering, you need to set up a bunch of sliders
that represent tradeoffs and move them around till you get a good balance
between API ease-of-use, performance, development time and long term
maintainability of your code internals.

Dhanji.

On Sun, Dec 27, 2009 at 11:00 AM, Peter Veentjer <alarmnummer at gmail.com>wrote:

> Hi Guys,
>
> although the question is not directly concurrency related, I want to
> place it on this mailinglist because here are a lot of guys that
> encounter the issue from time to time.
>
> I'm currently working on a STM implementation and one of the things
> that I have found out is that object creation can cause quite a big
> slowdown, especially if done
> millions of times per second (e.g. 1M to 20M times a second)  If I
> look at the code of java.util.concurrent I see a lot of
> nodes/iterators being created for example and this can
> be a great thing for good oo design but my experience is that it can
> cause a considerable slow down.
>
> My question is: how much should the design of a class be influenced by
> preventing object creation? I'm at the point where I need to rewrite
> the same algorithm
> multiple times for different datastructures (e.g. array based versus
> map based.. where an array is faster for very small collections). I
> could create an ArrayIterator
> so that the algorithm is able to work with iterators and it doesn't
> matter which data structure is used, but it requires an additional
> iterator object. I also know that
> garbage collectors are quite smart and that very short lived objects
> are very cheap to be garbage collected. I also know that some modern
> jvm's are able
> to allocate objects in the stack instead of the heap, so even less
> worries about gc overhead.
>
> I need some feedback/guidelines from developers that have experienced
> this in the trenches and really know what they are talking about. How
> for should one go?
> I would be very happy to stop ruining my code... but on the other side
> I also want a good performance
>
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091227/4cc5069e/attachment.html>

From alarmnummer at gmail.com  Sun Dec 27 07:28:47 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 27 Dec 2009 13:28:47 +0100
Subject: [concurrency-interest] good design vs performance (object
	creation)
In-Reply-To: <aa067ea10912261842x87b6d58lb87101bd14516598@mail.gmail.com>
References: <1466c1d60912261600x2cc48a4cj382f81249e405e2@mail.gmail.com>
	<aa067ea10912261842x87b6d58lb87101bd14516598@mail.gmail.com>
Message-ID: <41A61A94-DBC5-4151-9874-A911E4475D71@gmail.com>


Hi dhinji,

This is the standard answer i give to developers as well and in most  
cases it is very practical and good.

But i already spend a lot of time pinpointing performance problems in  
system and i found out that removing object creation often leads to a  
considerable performance improvement if its done millions of times a  
second. I found out that eventually object creation and/or cas  
communication caused the bottleneck and not the rest of the code.


On 27 dec 2009, at 03:42, "Dhanji R. Prasanna" <dhanji at gmail.com> wrote:

> In general you want to write your code and prove its correctness  
> first, and then optimize for performance. If your code is readable  
> and maintainable this becomes easier to do after the fact. You  
> should also never optimize code without being able to establish a  
> clear, reproduceable performance bottleneck that can be *described*  
> (not merely observed).
>
> As you note creation of thousands or even millions of objects is not  
> often a serious concern if they are short lived and the GC is tuned  
> accordingly. Particularly for things like iterators and so on.
>
> In my experience the real problems arise in two very different  
> scenarios:
> - Premature optimations: growable caches, pools, SoftReferences etc.  
> This is when you accidentally make objects long lived (for instance,  
> to prevent their number from getting out of hand). This can really  
> hurt GC performance.
> - Code immuaturity: This is the opposite end of the spectrum where  
> you just assume Java takes care of memory management for you  
> altogether. A classic example of this is calling clear() on an  
> expanding array list and forgetting to call trimToSize(). Or the  
> symmetrical action with j.u.HashMap. Creating large numbers of  
> objects in an inner loop is similarly a problem.
>
> A good library should go through a thorough memory complexity  
> analysis for each algorithm or utility introduced. This, almost  
> always, will point you directly to a leak or potential bottleneck.  
> When writing a concurrent data structure, I typically set up an  
> expectation of computational complexity and walk through the various  
> ways in which I can trade correctness for performance (a concurrent  
> LRU cache is a good example).
>
> Like everything else in engineering, you need to set up a bunch of  
> sliders that represent tradeoffs and move them around till you get a  
> good balance between API ease-of-use, performance, development time  
> and long term maintainability of your code internals.
>
> Dhanji.
>
> On Sun, Dec 27, 2009 at 11:00 AM, Peter Veentjer <alarmnummer at gmail.com 
> > wrote:
> Hi Guys,
>
> although the question is not directly concurrency related, I want to
> place it on this mailinglist because here are a lot of guys that
> encounter the issue from time to time.
>
> I'm currently working on a STM implementation and one of the things
> that I have found out is that object creation can cause quite a big
> slowdown, especially if done
> millions of times per second (e.g. 1M to 20M times a second)  If I
> look at the code of java.util.concurrent I see a lot of
> nodes/iterators being created for example and this can
> be a great thing for good oo design but my experience is that it can
> cause a considerable slow down.
>
> My question is: how much should the design of a class be influenced by
> preventing object creation? I'm at the point where I need to rewrite
> the same algorithm
> multiple times for different datastructures (e.g. array based versus
> map based.. where an array is faster for very small collections). I
> could create an ArrayIterator
> so that the algorithm is able to work with iterators and it doesn't
> matter which data structure is used, but it requires an additional
> iterator object. I also know that
> garbage collectors are quite smart and that very short lived objects
> are very cheap to be garbage collected. I also know that some modern
> jvm's are able
> to allocate objects in the stack instead of the heap, so even less
> worries about gc overhead.
>
> I need some feedback/guidelines from developers that have experienced
> this in the trenches and really know what they are talking about. How
> for should one go?
> I would be very happy to stop ruining my code... but on the other side
> I also want a good performance
>
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091227/a930e736/attachment.html>

From dl at cs.oswego.edu  Sun Dec 27 09:58:29 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 27 Dec 2009 09:58:29 -0500
Subject: [concurrency-interest] good design vs performance
	(object	creation)
In-Reply-To: <41A61A94-DBC5-4151-9874-A911E4475D71@gmail.com>
References: <1466c1d60912261600x2cc48a4cj382f81249e405e2@mail.gmail.com>	<aa067ea10912261842x87b6d58lb87101bd14516598@mail.gmail.com>
	<41A61A94-DBC5-4151-9874-A911E4475D71@gmail.com>
Message-ID: <4B377615.6090305@cs.oswego.edu>

Peter Veentjer wrote:
>  I found out that eventually object creation and/or cas 
> communication caused the bottleneck and not the rest of the code.

Since creation vs sync usually trade off against each
other, there's no good general rule of thumb about it.
You need to consider issues such as whether
the sync is usually localized and uncontended, whether
garbage nodes etc are cheap to collect (usually die fast
and/or no circular or linear pointer chains), and so on.

-Doug


From ramesh.mandaleeka at gmail.com  Sun Dec 27 19:59:53 2009
From: ramesh.mandaleeka at gmail.com (Ramesh Mandaleeka)
Date: Sun, 27 Dec 2009 19:59:53 -0500
Subject: [concurrency-interest] good design vs performance (object
	creation)
In-Reply-To: <aa067ea10912261842x87b6d58lb87101bd14516598@mail.gmail.com>
References: <1466c1d60912261600x2cc48a4cj382f81249e405e2@mail.gmail.com> 
	<aa067ea10912261842x87b6d58lb87101bd14516598@mail.gmail.com>
Message-ID: <a55f9dae0912271659w5d29c8a0m83510dec2758269@mail.gmail.com>

Hi Dhanji,

>> A classic example of this is calling clear() on an expanding array list
and forgetting to call trimToSize()

Can you elaborate what do you mean? Is there any side effect by calling
clear() method? Do we need to call trimToSize() method after clear() method?

Thanks,
Ramesh

On Sat, Dec 26, 2009 at 9:42 PM, Dhanji R. Prasanna <dhanji at gmail.com>wrote:

> In general you want to write your code and prove its correctness first, and
> then optimize for performance. If your code is readable and maintainable
> this becomes easier to do after the fact. You should also never optimize
> code without being able to establish a clear, reproduceable performance
> bottleneck that can be *described* (not merely observed).
>
> As you note creation of thousands or even millions of objects is not often
> a serious concern if they are short lived and the GC is tuned accordingly.
> Particularly for things like iterators and so on.
>
> In my experience the real problems arise in two very different scenarios:
> - Premature optimations: growable caches, pools, SoftReferences etc. This
> is when you accidentally make objects long lived (for instance, to prevent
> their number from getting out of hand). This can really hurt GC performance.
> - Code immuaturity: This is the opposite end of the spectrum where you just
> assume Java takes care of memory management for you altogether. A classic
> example of this is calling clear() on an expanding array list and forgetting
> to call trimToSize(). Or the symmetrical action with j.u.HashMap. Creating
> large numbers of objects in an inner loop is similarly a problem.
>
> A good library should go through a thorough memory complexity analysis for
> each algorithm or utility introduced. This, almost always, will point you
> directly to a leak or potential bottleneck. When writing a concurrent data
> structure, I typically set up an expectation of computational complexity and
> walk through the various ways in which I can trade correctness for
> performance (a concurrent LRU cache is a good example).
>
> Like everything else in engineering, you need to set up a bunch of sliders
> that represent tradeoffs and move them around till you get a good balance
> between API ease-of-use, performance, development time and long term
> maintainability of your code internals.
>
> Dhanji.
>
>
> On Sun, Dec 27, 2009 at 11:00 AM, Peter Veentjer <alarmnummer at gmail.com>wrote:
>
>> Hi Guys,
>>
>> although the question is not directly concurrency related, I want to
>> place it on this mailinglist because here are a lot of guys that
>> encounter the issue from time to time.
>>
>> I'm currently working on a STM implementation and one of the things
>> that I have found out is that object creation can cause quite a big
>> slowdown, especially if done
>> millions of times per second (e.g. 1M to 20M times a second)  If I
>> look at the code of java.util.concurrent I see a lot of
>> nodes/iterators being created for example and this can
>> be a great thing for good oo design but my experience is that it can
>> cause a considerable slow down.
>>
>> My question is: how much should the design of a class be influenced by
>> preventing object creation? I'm at the point where I need to rewrite
>> the same algorithm
>> multiple times for different datastructures (e.g. array based versus
>> map based.. where an array is faster for very small collections). I
>> could create an ArrayIterator
>> so that the algorithm is able to work with iterators and it doesn't
>> matter which data structure is used, but it requires an additional
>> iterator object. I also know that
>> garbage collectors are quite smart and that very short lived objects
>> are very cheap to be garbage collected. I also know that some modern
>> jvm's are able
>> to allocate objects in the stack instead of the heap, so even less
>> worries about gc overhead.
>>
>> I need some feedback/guidelines from developers that have experienced
>> this in the trenches and really know what they are talking about. How
>> for should one go?
>> I would be very happy to stop ruining my code... but on the other side
>> I also want a good performance
>>
>> Peter
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091227/8f0d6256/attachment.html>

From ian.rogers at manchester.ac.uk  Mon Dec 28 12:00:55 2009
From: ian.rogers at manchester.ac.uk (Ian Rogers)
Date: Mon, 28 Dec 2009 12:00:55 -0500
Subject: [concurrency-interest] good design vs performance (object
	creation)
In-Reply-To: <41A61A94-DBC5-4151-9874-A911E4475D71@gmail.com>
References: <1466c1d60912261600x2cc48a4cj382f81249e405e2@mail.gmail.com>
	<aa067ea10912261842x87b6d58lb87101bd14516598@mail.gmail.com>
	<41A61A94-DBC5-4151-9874-A911E4475D71@gmail.com>
Message-ID: <b1e4cffb0912280900l44a93197kc75fe5b32371ddb7@mail.gmail.com>

Hi Peter,

2009/12/27 Peter Veentjer <alarmnummer at gmail.com>:
>
> Hi dhinji,
> This is the standard answer i give to developers as well and in most cases
> it is very practical and good.
> But i already spend a lot of time pinpointing performance problems in system
> and i found out that removing object creation often leads to a considerable
> performance improvement if its done millions of times a second. I found out
> that eventually object creation and/or cas communication caused the
> bottleneck and not the rest of the code.

When you are doing your performance work are you using a "warmed up"
JVM? A JVM will try to prove that an object doesn't escape a method
and/or a thread. Such knowledge can allow the JVM to avoid locking
thread-local objects and to perform what's known as scalar replacement
of objects - iterators being the prime example of what gets replaced.
With scalar replacement of objects a simple iterator with a next field
will have that field become a local variables and the get/put field
operations performed on it replaced with local variable accesses. If
you write the iterator to a field or pass it around to large methods,
then the JVM's compilers need more time to perform analysis as scalar
replacement optimizations can only be performed conservatively. In
general GC is very cheap, you only pay a cost for live objects. If you
do have a genuine performance issue with a JVM then you should
probably file a bug, this gives best performance to everyone and you
cleaner APIs.

For more information on performance why not check out Cliff Click's blog [1].

Regards,
Ian Rogers

[1] http://blogs.azulsystems.com/cliff/

From kimo at webnetic.net  Mon Dec 28 15:39:59 2009
From: kimo at webnetic.net (Kimo Crossman)
Date: Mon, 28 Dec 2009 12:39:59 -0800
Subject: [concurrency-interest] Idempotent work stealing (2009)
Message-ID: <ef80160b0912281239n761e79fdj5eb5a5db62ae8ddc@mail.gmail.com>

This paper was interesting to me when it came out, both for the different
approach taken for work stealing and for the emphasis on not using atomic
instructions like CAS to improve speed..

Has there been any discussion on this list about implementing these
algorithms for the j.u.c.

(I'm surprised to not find a Search function for the archives of this list)

best

kimo crossman


*Idempotent work stealing (2009)*
*
http://domino.research.ibm.com/comm/research_people.nsf/pages/mtvechev.pubs.html/$FILE/idempotentWSQ09.pdf<http://docs.google.com/viewer?url=http://domino.research.ibm.com/comm/research_people.nsf/pages/mtvechev.pubs.html/$FILE/idempotentWSQ09.pdf>
*

Maged M. Michael<http://portal.acm.org/author_page.cfm?id=81332515587&coll=GUIDE&dl=GUIDE&trk=0&CFID=70118550&CFTOKEN=52758888>
 IBM
Thomas J. Watson Research Center, Yorktown Height, NY, USA Martin T.
Vechev<http://portal.acm.org/author_page.cfm?id=81100269652&coll=GUIDE&dl=GUIDE&trk=0&CFID=70118550&CFTOKEN=52758888>
 IBM
Thomas J. Watson Research Center, Hawthorne, NY, USA Vijay A.
Saraswat<http://portal.acm.org/author_page.cfm?id=81100152268&coll=GUIDE&dl=GUIDE&trk=0&CFID=70118550&CFTOKEN=52758888>
 IBM
Thomas J. Watson Research Center, Hawthorne, NY, USA

Load balancing is a technique which allows efficient parallelization of
irregular workloads, and a key component of many applications and
parallelizing runtimes. Work-stealing is a popular technique for
implementing load balancing, where each parallel thread maintains its own
work set of items and occasionally steals items from the sets of other
threads.

The conventional semantics of work stealing guarantee that each inserted
task is eventually extracted exactly once. However, correctness of a wide
class of applications allows for relaxed semantics, because either: i) the
application already explicitly checks that no work is repeated or ii) the
application can tolerate repeated work.

In this paper, we introduce *idempotent work tealing*, and present several
new algorithms that exploit the relaxed semantics to deliver better
performance. The semantics of the new algorithms guarantee that each
inserted task is eventually extracted *at least* once-instead of *exactly*
 once.

*For a wide range of applications dealing with irregular computation
patterns. Sample domains include: parallel garbage collection, fixed point
computations in program analysis, constraint solvers (e.g. SAT solvers),
state space search exploration in model checking as well as integer and
mixed programming solvers.*

On mainstream processors, algorithms for conventional work stealing require
special atomic instructions (CAS) or store-load memory ordering fence
instructions in the owner's critical path operations. In general, these
instructions are substantially slower than regular memory access
instructions. By exploiting the relaxed semantics, our algorithms avoid
these instructions in the owner's operations.

We evaluated our algorithms using common graph problems and micro-benchmarks
and compared them to well-known conventional work stealing algorithms, the
THE Cilk and Chase-Lev algorithms. We found that our best algorithm (with
LIFO extraction) outperforms existing algorithms in nearly all cases, and
often by significant margin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091228/d88c0a2b/attachment.html>

From dhanji at gmail.com  Mon Dec 28 18:21:53 2009
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Tue, 29 Dec 2009 10:21:53 +1100
Subject: [concurrency-interest] good design vs performance (object
	creation)
In-Reply-To: <a55f9dae0912271659w5d29c8a0m83510dec2758269@mail.gmail.com>
References: <1466c1d60912261600x2cc48a4cj382f81249e405e2@mail.gmail.com>
	<aa067ea10912261842x87b6d58lb87101bd14516598@mail.gmail.com>
	<a55f9dae0912271659w5d29c8a0m83510dec2758269@mail.gmail.com>
Message-ID: <aa067ea10912281521h44478511qe33c523f1a3a5b12@mail.gmail.com>

On Mon, Dec 28, 2009 at 11:59 AM, Ramesh Mandaleeka
<ramesh.mandaleeka at gmail.com> wrote:
> Hi Dhanji,
>
>>> A classic example of this is calling clear() on an expanding array list
>>> and forgetting to call trimToSize()
>
> Can you elaborate what do you mean? Is there any side effect by calling
> clear() method? Do we need to call trimToSize() method after clear() method?
>

The side effect is in the growth of the array *before* you call
clear(). If it grows very large then you call clear--it remains
largely empty, wasting memory and creating memory pressure. You would
be surprised how much of a problem this is in real production
environments. It almost always better for the GC to throw away the
entire array and grow it back to the desired size.

Dhanji.

From dl at cs.oswego.edu  Mon Dec 28 19:13:13 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 28 Dec 2009 19:13:13 -0500
Subject: [concurrency-interest] Idempotent work stealing (2009)
In-Reply-To: <ef80160b0912281239n761e79fdj5eb5a5db62ae8ddc@mail.gmail.com>
References: <ef80160b0912281239n761e79fdj5eb5a5db62ae8ddc@mail.gmail.com>
Message-ID: <4B394999.3030007@cs.oswego.edu>

Kimo Crossman wrote:

> Idempotent work stealing (2009) ...

> This paper was interesting to me when it came out, both for the different
> approach taken for work stealing and for the emphasis on not using atomic
> instructions like CAS to improve speed..

As is briefly mentioned in the internal documentation
(http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166y/ForkJoinWorkerThread.java?view=log)
The current FJ work-stealing algorithm is similar to
one of those one by Maged Michael et al but uses one
bookkeeping CAS per task execution. This is needed to
aggressively null out queue slots to in turn ensure
executed tasks can be quickly GCed, but additionally
serves to avoid need for task state CAS upon running.
You would otherwise need that secondary CAS in the normal
case where tasks should not be run more than once.
If you don't need either of these features
(prompt GCability and at-most-once execution) then
their algorithm is likely faster, but this is too
uncommon a case to support in j.u.c.

> 
> (I'm surprised to not find a Search function for the archives of this list)
> 


(You can search the archives on http://concurrency.markmail.org/ .
But including "concurrency-interest" in google searches
is often at least as good because it also finds other things
that reference list discussions.)

-Doug



From blair at orcaware.com  Tue Dec 29 23:22:57 2009
From: blair at orcaware.com (Blair Zajac)
Date: Tue, 29 Dec 2009 20:22:57 -0800
Subject: [concurrency-interest] JDK7 schedule and extra166y
Message-ID: <4B3AD5A1.4090909@orcaware.com>

With the JDK7 pushing out to late next year and getting closures, what 
are the plans with extra166y?  Will it make it into JDK7?  Will JDK7 see 
some version of CustomConcurrentHashMap?

How much will closures effect the collections already in JDK7?

Thanks,
Blair

From blair at orcaware.com  Wed Dec 30 01:33:39 2009
From: blair at orcaware.com (Blair Zajac)
Date: Tue, 29 Dec 2009 22:33:39 -0800
Subject: [concurrency-interest] CustomConcurrentHashMap feedback
Message-ID: <4B3AF443.1050502@orcaware.com>

I was looking at CustomConcurrentHashMap.KeyGen today for it's intern() 
method.  Right now I'm using Google Collection's for an intern but I 
have to write my own FinalizableSoftReference subclass that allows me to 
use equals() instead of == for determining equality.  I'm hoping to find 
a single map class that has all these features and was trying 
CustomConcurrentHashMap.

Two issues I've noticed so far with CustomConcurrentHashMap.

1) Putting a value into CustomConcurrentHashMap.KeyGen doesn't start the 
reference queue reclaiming thread.  I just tested this by putting a 
println() in ReclamationThread's constructor and it's never called.

2) In a web application will ReclamationThread prevent the web 
application from being completely unloaded?  It looks like there's 
nothing there to stop the thread when the application's class loader is 
unloaded.

Google Collections handles 2) so could be used to have ReclamationThread 
shutdown.

Happy New Year!
Blair

From i30817 at gmail.com  Wed Dec 30 20:38:00 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Thu, 31 Dec 2009 01:38:00 +0000
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
Message-ID: <212322090912301738i598a3c74t1d343e6dec026a2b@mail.gmail.com>

I have a TPE subclass and i'm seeing (disturbingly only on some dual core
processors) a hang when the application is closed and there are lots of
tasks to be processed. Specifically no-more tasks appear to be processed,
but the (daemon) threads are still alive at the time shutdown hooks are
called.
(daemon, alive, not interrupted).

Threadfactory used by the subclass creates daemon threads, and the class is
configurated with a keepAliveTime > 0 in this case (to reuse the threads)
and a LIFO queue instead of the normal one.

I appear to be able to avoid it with a shutdown hook, but i would like to
know if all ThreadPoolExecutors are vulnerable to this.

(disturbingly too, after this, the debugger says the java process is closed,
however windows task manager disagrees).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091231/d6b11dab/attachment.html>

From sberlin at gmail.com  Thu Dec 31 14:00:45 2009
From: sberlin at gmail.com (Sam Berlin)
Date: Thu, 31 Dec 2009 14:00:45 -0500
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
In-Reply-To: <212322090912301738i598a3c74t1d343e6dec026a2b@mail.gmail.com>
References: <212322090912301738i598a3c74t1d343e6dec026a2b@mail.gmail.com>
Message-ID: <19196d860912311100v7623d1d2x50cae9ffbccbdd86@mail.gmail.com>

I have also seen behavior like this, but have not been able to track it down
to any particular classes or methods.  On Windows, Task Manager shows the
process continues to run.  Any file locks that have been acquired continue
to remain acquired.  However, the JRE appears to be shut down (and any tools
that attempt to attach to live JREs do not show any JRE as being alive).
It's rare, but frequent enough to be frustratingly annoying.

Sam

On Wed, Dec 30, 2009 at 8:38 PM, Paulo Levi <i30817 at gmail.com> wrote:

> I have a TPE subclass and i'm seeing (disturbingly only on some dual core
> processors) a hang when the application is closed and there are lots of
> tasks to be processed. Specifically no-more tasks appear to be processed,
> but the (daemon) threads are still alive at the time shutdown hooks are
> called.
> (daemon, alive, not interrupted).
>
> Threadfactory used by the subclass creates daemon threads, and the class is
> configurated with a keepAliveTime > 0 in this case (to reuse the threads)
> and a LIFO queue instead of the normal one.
>
> I appear to be able to avoid it with a shutdown hook, but i would like to
> know if all ThreadPoolExecutors are vulnerable to this.
>
> (disturbingly too, after this, the debugger says the java process is
> closed, however windows task manager disagrees).
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20091231/f5bdfd79/attachment.html>

From davidcholmes at aapt.net.au  Thu Dec 31 20:02:15 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 1 Jan 2010 11:02:15 +1000
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
In-Reply-To: <212322090912301738i598a3c74t1d343e6dec026a2b@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEKHIEAA.davidcholmes@aapt.net.au>

Does ctrl-/ produce a thread dump showing where the threads are? During a
shutdown there are sufficient system threads active that your app threads
may not get a chance to progress further. Once the shutdown reaches a
certain stage then all application threads (daemon or non-daemon) will stop
executing as the VM comes to a safepoint for termination.

Why do you expect processing to continue, do you use a shutdown hook that
takes a long time to run?

David Holmes

 -----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Paulo Levi
Sent: Thursday, 31 December 2009 11:38 AM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit


  I have a TPE subclass and i'm seeing (disturbingly only on some dual core
processors) a hang when the application is closed and there are lots of
tasks to be processed. Specifically no-more tasks appear to be processed,
but the (daemon) threads are still alive at the time shutdown hooks are
called.
  (daemon, alive, not interrupted).

  Threadfactory used by the subclass creates daemon threads, and the class
is configurated with a keepAliveTime > 0 in this case (to reuse the threads)
and a LIFO queue instead of the normal one.

  I appear to be able to avoid it with a shutdown hook, but i would like to
know if all ThreadPoolExecutors are vulnerable to this.

  (disturbingly too, after this, the debugger says the java process is
closed, however windows task manager disagrees).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100101/cac7e73e/attachment.html>

