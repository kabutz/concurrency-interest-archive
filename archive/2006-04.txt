From studdugie at gmail.com  Sat Apr  1 02:33:00 2006
From: studdugie at gmail.com (studdugie)
Date: Sat Apr  1 02:33:04 2006
Subject: [concurrency-interest] A little lock free algorithm [the code]
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEFLGNAA.dcholmes@optusnet.com.au>
References: <5a59ce530603311024o299ca39ak6d5f7e4a3c591f9e@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEFLGNAA.dcholmes@optusnet.com.au>
Message-ID: <5a59ce530603312333w76cee371sa7b685f43339954a@mail.gmail.com>

On 3/31/06, David Holmes <dcholmes@optusnet.com.au> wrote:
> Sorry about that. I was focussing on the visibility issues and overlooked
> the details of the semantic requirements.
>
> If you want the execution of the if block to only be performed by one thread
> per "time window" then the test of the time has to be atomic with respect to
> updating expiration. Simplest way is to re-do the test when you have the
> "lock":
>
>     void increment()
>     {
>         long millis;
>         // quick check if nothing to do
>         if( dead || (millis = System.currentTimeMillis()) < expiration )
>             return;
>
>         if( locked.compareAndSet( false, true ) )
>         {
>             // check again as things might have changed since the first
> check
>             if( dead || millis < expiration )
>               return;
>
>             backoff += BACKOFF_INCREMENT;
>             if( backoff > BACKOFF_MAX )
>                 dead = true;
>             else
>                 expiration = millis + backoff;
>             locked.set( false );
>         }
>     }
>
> This is still approximate as you really need to snapshot the current time as
> close as possible to the detection of the failure - otherwise you might get
> preempted and get a distorted view of when the failure actually happened. Of
> course there is never a guarantee as you can be preempted at any time.

Truth be told, I don't need the timestamp to be absolutely right I
just need it to be close enough and given that we are talking about
minutes, hours, and even days backoffs a thread would have to be
preempted for a very long time for the timestamp to be so skewed as to
be useless.

> Looking closer I'm not clear on exactly how you want this to work. When and
> how are TFailure objects created? I'm wondering how long might elapse
> between the initialization of "expiration" and the first call to increment.
> I suspect your logic is something like:
>
> if connect_to_host_failed then
>    if host.tfailure == null
>        host.tfailure = new Tfailure()
>    else
>       host.tfailure.increment()
>
> in which case how do you make the construction/publication thread-safe?
I use ConcurrentMap's putIfAbsent(K,V) method to make publication
thread-safe. So we end up w/:
TFailure host = failures.get( ip );
if( null == host )
    failures.putIfAbsent( ip, new TFailure() );
else
    host.increment();

> Also your expiration logic would seem to lead to a host being declared dead
> after a certain number of failures, regardless of successes in between the
> failures. Eventually backoff will be greater than BACKOFF_MAX because it is
> never reduced. I also don't understand why you want the failure window to
> get larger and larger.
Not quite. For every successful connection made there is a call to
"failures" to remove any entry for the successful "ip". So basically
whenever a connection gets made the host gets a new lease on life.
What can I say? I'm not a vengeful god. I'm all about redemption.

Cheers,

Dane

From tom at epcc.ed.ac.uk  Mon Apr  3 07:33:57 2006
From: tom at epcc.ed.ac.uk (Tom Sugden)
Date: Mon Apr  3 07:34:02 2006
Subject: [concurrency-interest] backport: problem with shutdownNow() during
	invokeAll()
In-Reply-To: <4422E64E.6070307@quiotix.com>
Message-ID: <200604031133.k33BXpqZ015724@e450.epcc.ed.ac.uk>

Hi,

I'm having some difficulty with the shutdownNow functionality of the cached
thread pool executor service. The scenario I am trying to test is this:

   - submit a number of tasks for execution using the invokeAll() method
   - wait until some of the tasks have definitely begun execution
   - invoke shutdownNow() on the executor service

I would expect this to:

   - prevent any tasks that have yet to be called from ever being called
   - interrupt any tasks that have already been called

In a loop of 100 iterations, my program regularly stops in a deadlock. I'm
quite new to these concurrency utilities, so there's a fair chance I'm doing
something stupid! It would be a great help if somebody could cast an eye
over my code below and offer advice. 

The strangest thing is this: if the lines marked "*** A ***" and *** B ***"
are commented out, the program runs fine over thousands of iterations. This
suggests (but doesn't prove) that the deadlock arises if one of the tasks
has already completed before the shutdownNow() invocation.

Please note that I am using the Java 1.4 backport of the concurrency
utilities.

Cheers,
Tom Sugden

---

package concurrency.test;

import java.util.ArrayList;
import java.util.List;

import edu.emory.mathcs.backport.java.util.concurrent.Callable;
import edu.emory.mathcs.backport.java.util.concurrent.ExecutorService;
import edu.emory.mathcs.backport.java.util.concurrent.Executors;
import edu.emory.mathcs.backport.java.util.concurrent.TimeUnit;

/**
 * Submits a list of callable tasks to an executor service, then terminates
the
 * executor service at an intermediate stage of processing.
 */
public class BlockingTaskExecutorTest
{
    public static void main(String[] args) throws InterruptedException
    {
        for (int i = 1; i <= 100; i++)
        {
            System.out.println("Iteration: " + i);
            
            final ExecutorService executor =
Executors.newCachedThreadPool();

            final NotificationReceiver notifier1 = new
NotificationReceiver();
            final NotificationReceiver notifier2 = new
NotificationReceiver();

            final Callable task1 = new BlockingTask(notifier1);
            final Callable task2 = new BlockingTask(notifier2);
            final Callable task3 = new NonBlockingTask();        // *** A
***

            final List tasks = new ArrayList();
            tasks.add(task1);
            tasks.add(task2);
            tasks.add(task3);                                    // *** B
***

            // start a thread to invoke the tasks
            Thread thread = new Thread()
            {
                public void run()
                {
                    try
                    {
                        executor.invokeAll(tasks);
                    }
                    catch (Throwable e)
                    {
                        e.printStackTrace();
                    }
                }
            };
            thread.start();
            
            // wait until tasks begin execution
            notifier1.waitForNotification();
            notifier2.waitForNotification();

            // now try to shutdown the executor service while tasks are
blocked. 
            // This should cause the tasks to be interupted.
            executor.shutdownNow();
            boolean stopped = executor.awaitTermination(5,
TimeUnit.SECONDS);
            System.out.println("Terminated? " + stopped);

            // wait for the invocation thread to complete
            thread.join();
        }
    }
}

/**
 * A helper class with a method to wait for a notification. The notification
is
 * received via the <code>sendNotification</code> method.
 */
class NotificationReceiver
{
    /** Has the notifier been notified? */
    boolean mNotified = false;

    /**
     * Notify the notification receiver.
     */
    public synchronized void sendNotification()
    {
        mNotified = true;
        notifyAll();
    }

    /**
     * Waits until a notification has been received.
     * 
     * @throws InterruptedException
     *             if the wait is interrupted
     */
    public synchronized void waitForNotification() throws
InterruptedException
    {
        while (!mNotified)
        {
            wait();
        }
    }
}

/**
 * A callable task that blocks until it is interupted. This task sends a 
 * notification to a notification receiver when it is first called.
 */
class BlockingTask implements Callable
{
    private final NotificationReceiver mReceiver;
    
    BlockingTask(NotificationReceiver notifier)
    {
        mReceiver = notifier;
    }

    /*
     * (non-Javadoc)
     * @see edu.emory.mathcs.backport.java.util.concurrent.Callable#call()
     */
    public Object call() throws Exception
    {
        mReceiver.sendNotification();
        
        // wait indefinitely until task is interupted
        while (true)
        {
            synchronized (this)
            {
                wait();
            }
        }
    }
}

/**
 * A callable task that simply returns a string result.
 */
class NonBlockingTask implements Callable
{
    /*
     * (non-Javadoc)
     * @see edu.emory.mathcs.backport.java.util.concurrent.Callable#call()
     */
    public Object call() throws Exception
    {
        return "NonBlockingTaskResult";
    }
}

From tim at peierls.net  Mon Apr  3 14:47:08 2006
From: tim at peierls.net (Tim Peierls)
Date: Mon Apr  3 14:47:20 2006
Subject: [concurrency-interest] backport: problem with shutdownNow()
	during invokeAll()
In-Reply-To: <200604031133.k33BXpqZ015724@e450.epcc.ed.ac.uk>
References: <4422E64E.6070307@quiotix.com>
	<200604031133.k33BXpqZ015724@e450.epcc.ed.ac.uk>
Message-ID: <63b4e4050604031147i323e3891u654a88d3681ac693@mail.gmail.com>

A task that has been executed with executor.execute(task) but has not yet
run internally might not see the interruption sent by shutdownNow, in which
case Future.get() for this task can block indefinitely. I think that is what
is happening with your NonBlockingTask; it hasn't had a chance to run by the
time shutdownNow interrupts the thread it is scheduled to run on, so that
when invokeAll tries to get its value, it blocks forever.

The upshot is that you shouldn't mix shutdownNow with invokeAll. Consider
using timed InvokeAll, or if you only need one result, invokeAny. That way
you can reuse the executor over multiple iterations.

This is true for all j.u.c flavors, not just the backport. I reproduced the
behavior using a recent Mustang build.

--tim

On 4/3/06, Tom Sugden < tom@epcc.ed.ac.uk > wrote:
>
> Hi,
>
> I'm having some difficulty with the shutdownNow functionality of the
> cached
> thread pool executor service. The scenario I am trying to test is this:
>
>    - submit a number of tasks for execution using the invokeAll() method
>    - wait until some of the tasks have definitely begun execution
>    - invoke shutdownNow() on the executor service
>
> I would expect this to:
>
>    - prevent any tasks that have yet to be called from ever being called
>    - interrupt any tasks that have already been called
>
> In a loop of 100 iterations, my program regularly stops in a deadlock. I'm
> quite new to these concurrency utilities, so there's a fair chance I'm
> doing
> something stupid! It would be a great help if somebody could cast an eye
> over my code below and offer advice.
>
> The strangest thing is this: if the lines marked "*** A ***" and *** B
> ***"
> are commented out, the program runs fine over thousands of iterations.
> This
> suggests (but doesn't prove) that the deadlock arises if one of the tasks
> has already completed before the shutdownNow() invocation.
>
> Please note that I am using the Java 1.4 backport of the concurrency
> utilities.
>
> Cheers,
> Tom Sugden
>
> ---
>
> package concurrency.test;
>
> import java.util.ArrayList;
> import java.util.List;
>
> import edu.emory.mathcs.backport.java.util.concurrent.Callable;
> import edu.emory.mathcs.backport.java.util.concurrent.ExecutorService ;
> import edu.emory.mathcs.backport.java.util.concurrent.Executors;
> import edu.emory.mathcs.backport.java.util.concurrent.TimeUnit;
>
> /**
> * Submits a list of callable tasks to an executor service, then terminates
>
> the
> * executor service at an intermediate stage of processing.
> */
> public class BlockingTaskExecutorTest
> {
>     public static void main(String[] args) throws InterruptedException
>     {
>         for (int i = 1; i <= 100; i++)
>         {
>             System.out.println("Iteration: " + i);
>
>             final ExecutorService executor =
> Executors.newCachedThreadPool();
>
>             final NotificationReceiver notifier1 = new
> NotificationReceiver();
>             final NotificationReceiver notifier2 = new
> NotificationReceiver();
>
>             final Callable task1 = new BlockingTask(notifier1);
>             final Callable task2 = new BlockingTask(notifier2);
>             final Callable task3 = new NonBlockingTask();        // *** A
> ***
>
>             final List tasks = new ArrayList();
>             tasks.add(task1);
>             tasks.add(task2);
>              tasks.add(task3);                                    // *** B
> ***
>
>             // start a thread to invoke the tasks
>             Thread thread = new Thread()
>             {
>                 public void run()
>                 {
>                     try
>                     {
>                         executor.invokeAll(tasks);
>                     }
>                     catch (Throwable e)
>                     {
>                         e.printStackTrace();
>                     }
>                 }
>             };
>             thread.start();
>
>             // wait until tasks begin execution
>             notifier1.waitForNotification ();
>             notifier2.waitForNotification();
>
>             // now try to shutdown the executor service while tasks are
> blocked.
>             // This should cause the tasks to be interupted.
>              executor.shutdownNow();
>             boolean stopped = executor.awaitTermination(5,
> TimeUnit.SECONDS);
>             System.out.println("Terminated? " + stopped);
>
>             // wait for the invocation thread to complete
>             thread.join();
>         }
>     }
> }
>
> /**
> * A helper class with a method to wait for a notification. The
> notification
> is
> * received via the <code>sendNotification</code> method.
> */
> class NotificationReceiver
> {
>     /** Has the notifier been notified? */
>     boolean mNotified = false;
>
>     /**
>      * Notify the notification receiver.
>      */
>     public synchronized void sendNotification()
>     {
>         mNotified = true;
>         notifyAll();
>     }
>
>     /**
>      * Waits until a notification has been received.
>      *
>      * @throws InterruptedException
>      *             if the wait is interrupted
>      */
>     public synchronized void waitForNotification() throws
> InterruptedException
>     {
>         while (!mNotified)
>         {
>             wait();
>         }
>     }
> }
>
> /**
> * A callable task that blocks until it is interupted. This task sends a
> * notification to a notification receiver when it is first called.
> */
> class BlockingTask implements Callable
> {
>     private final NotificationReceiver mReceiver;
>
>     BlockingTask(NotificationReceiver notifier)
>     {
>         mReceiver = notifier;
>     }
>
>     /*
>      * (non-Javadoc)
>      * @see edu.emory.mathcs.backport.java.util.concurrent.Callable#call()
>      */
>     public Object call() throws Exception
>     {
>         mReceiver.sendNotification();
>
>         // wait indefinitely until task is interupted
>         while (true)
>         {
>             synchronized (this)
>             {
>                 wait();
>             }
>         }
>     }
> }
>
> /**
> * A callable task that simply returns a string result.
> */
> class NonBlockingTask implements Callable
> {
>     /*
>      * (non-Javadoc)
>      * @see edu.emory.mathcs.backport.java.util.concurrent.Callable#call()
>      */
>     public Object call() throws Exception
>     {
>         return "NonBlockingTaskResult";
>     }
> }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
>  http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060403/c3a6d93d/attachment.html
From joe.bowbeer at gmail.com  Mon Apr  3 16:24:54 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon Apr  3 16:25:00 2006
Subject: [concurrency-interest] backport: problem with shutdownNow()
	during invokeAll()
In-Reply-To: <200604031133.k33BXpqZ015724@e450.epcc.ed.ac.uk>
References: <4422E64E.6070307@quiotix.com>
	<200604031133.k33BXpqZ015724@e450.epcc.ed.ac.uk>
Message-ID: <31f2a7bd0604031324v280cddc3k7585e4cb20903743@mail.gmail.com>

Adding to Tim's comment:

Note that shutdownNow returns a list of tasks that were still on the
queue, uncompleted and uninterrupted.


On 4/3/06, Tom Sugden <tom@epcc.ed.ac.uk> wrote:
> Hi,
>
> I'm having some difficulty with the shutdownNow functionality of the cached
> thread pool executor service. The scenario I am trying to test is this:
>
>    - submit a number of tasks for execution using the invokeAll() method
>    - wait until some of the tasks have definitely begun execution
>    - invoke shutdownNow() on the executor service
>
> I would expect this to:
>
>    - prevent any tasks that have yet to be called from ever being called
>    - interrupt any tasks that have already been called
>

From bart.jacobs at cs.kuleuven.be  Mon Apr  3 16:41:44 2006
From: bart.jacobs at cs.kuleuven.be (Bart Jacobs)
Date: Mon Apr  3 16:42:21 2006
Subject: [concurrency-interest] backport: problem with shutdownNow()	during
	invokeAll()
In-Reply-To: <63b4e4050604031147i323e3891u654a88d3681ac693@mail.gmail.com>
References: <4422E64E.6070307@quiotix.com>	<200604031133.k33BXpqZ015724@e450.epcc.ed.ac.uk>
	<63b4e4050604031147i323e3891u654a88d3681ac693@mail.gmail.com>
Message-ID: <44318888.7030505@cs.kuleuven.be>

I don't know if there are any use cases for this, but perhaps there 
should be an ExecutorTask interface, defined as follows:

public interface ExecutorTask extends Runnable {
    public void run();
    public void cancel();
}

This would be used by someone submitting a task instead of Runnable if 
they want to be notified if it turns out that the task they submitted 
will not be run after all. It would be the responsibility of whoever 
calls shutdownNow() to walk the returned collection and call cancel() if 
applicable:

for (Runnable task : executor.shutdownNow()) {
    if (task instanceof ExecutorTask)
        ((ExecutorTask)task).cancel();
}

FutureTask would implement ExecutorTask and cancel itself when getting a 
cancel() call. This would enable the scenario below.

Best,-
Bart Jacobs

Tim Peierls schreef:
> A task that has been executed with executor.execute(task) but has not 
> yet run internally might not see the interruption sent by shutdownNow, 
> in which case Future.get() for this task can block indefinitely. I 
> think that is what is happening with your NonBlockingTask; it hasn't 
> had a chance to run by the time shutdownNow interrupts the thread it 
> is scheduled to run on, so that when invokeAll tries to get its value, 
> it blocks forever.
>
> The upshot is that you shouldn't mix shutdownNow with invokeAll. 
> Consider using timed InvokeAll, or if you only need one result, 
> invokeAny. That way you can reuse the executor over multiple iterations.
>
> This is true for all j.u.c flavors, not just the backport. I 
> reproduced the behavior using a recent Mustang build.
>
> --tim
>
> On 4/3/06, *Tom Sugden* < tom@epcc.ed.ac.uk 
> <mailto:tom@epcc.ed.ac.uk>> wrote:
>
>     Hi,
>
>     I'm having some difficulty with the shutdownNow functionality of
>     the cached
>     thread pool executor service. The scenario I am trying to test is
>     this:
>
>        - submit a number of tasks for execution using the invokeAll()
>     method
>        - wait until some of the tasks have definitely begun execution
>        - invoke shutdownNow() on the executor service
>
>     I would expect this to:
>
>        - prevent any tasks that have yet to be called from ever being
>     called
>        - interrupt any tasks that have already been called
>
>     In a loop of 100 iterations, my program regularly stops in a
>     deadlock. I'm
>     quite new to these concurrency utilities, so there's a fair chance
>     I'm doing
>     something stupid! It would be a great help if somebody could cast
>     an eye
>     over my code below and offer advice.
>
>     The strangest thing is this: if the lines marked "*** A ***" and
>     *** B ***"
>     are commented out, the program runs fine over thousands of
>     iterations. This
>     suggests (but doesn't prove) that the deadlock arises if one of
>     the tasks
>     has already completed before the shutdownNow() invocation.
>
>     Please note that I am using the Java 1.4 backport of the concurrency
>     utilities.
>
>     Cheers,
>     Tom Sugden
>
>     ---
>
>     package concurrency.test;
>
>     import java.util.ArrayList;
>     import java.util.List;
>
>     import edu.emory.mathcs.backport.java.util.concurrent.Callable;
>     import
>     edu.emory.mathcs.backport.java.util.concurrent.ExecutorService ;
>     import edu.emory.mathcs.backport.java.util.concurrent.Executors;
>     import edu.emory.mathcs.backport.java.util.concurrent.TimeUnit;
>
>     /**
>     * Submits a list of callable tasks to an executor service, then
>     terminates
>     the
>     * executor service at an intermediate stage of processing.
>     */
>     public class BlockingTaskExecutorTest
>     {
>         public static void main(String[] args) throws InterruptedException
>         {
>             for (int i = 1; i <= 100; i++)
>             {
>                 System.out.println("Iteration: " + i);
>
>                 final ExecutorService executor =
>     Executors.newCachedThreadPool();
>
>                 final NotificationReceiver notifier1 = new
>     NotificationReceiver();
>                 final NotificationReceiver notifier2 = new
>     NotificationReceiver();
>
>                 final Callable task1 = new BlockingTask(notifier1);
>                 final Callable task2 = new BlockingTask(notifier2);
>                 final Callable task3 = new
>     NonBlockingTask();        // *** A
>     ***
>
>                 final List tasks = new ArrayList();
>                 tasks.add(task1);
>                 tasks.add(task2);
>                 
>     tasks.add(task3);                                    // *** B
>     ***
>
>                 // start a thread to invoke the tasks
>                 Thread thread = new Thread()
>                 {
>                     public void run()
>                     {
>                         try
>                         {
>                             executor.invokeAll(tasks);
>                         }
>                         catch (Throwable e)
>                         {
>                             e.printStackTrace();
>                         }
>                     }
>                 };
>                 thread.start();
>
>                 // wait until tasks begin execution
>                 notifier1.waitForNotification ();
>                 notifier2.waitForNotification();
>
>                 // now try to shutdown the executor service while
>     tasks are
>     blocked.
>                 // This should cause the tasks to be interupted.
>                  executor.shutdownNow();
>                 boolean stopped = executor.awaitTermination(5,
>     TimeUnit.SECONDS);
>                 System.out.println("Terminated? " + stopped);
>
>                 // wait for the invocation thread to complete
>                 thread.join();
>             }
>         }
>     }
>
>     /**
>     * A helper class with a method to wait for a notification. The
>     notification
>     is
>     * received via the <code>sendNotification</code> method.
>     */
>     class NotificationReceiver
>     {
>         /** Has the notifier been notified? */
>         boolean mNotified = false;
>
>         /**
>          * Notify the notification receiver.
>          */
>         public synchronized void sendNotification()
>         {
>             mNotified = true;
>             notifyAll();
>         }
>
>         /**
>          * Waits until a notification has been received.
>          *
>          * @throws InterruptedException
>          *             if the wait is interrupted
>          */
>         public synchronized void waitForNotification() throws
>     InterruptedException
>         {
>             while (!mNotified)
>             {
>                 wait();
>             }
>         }
>     }
>
>     /**
>     * A callable task that blocks until it is interupted. This task
>     sends a
>     * notification to a notification receiver when it is first called.
>     */
>     class BlockingTask implements Callable
>     {
>         private final NotificationReceiver mReceiver;
>
>         BlockingTask(NotificationReceiver notifier)
>         {
>             mReceiver = notifier;
>         }
>
>         /*
>          * (non-Javadoc)
>          * @see
>     edu.emory.mathcs.backport.java.util.concurrent.Callable#call()
>          */
>         public Object call() throws Exception
>         {
>             mReceiver.sendNotification();
>
>             // wait indefinitely until task is interupted
>             while (true)
>             {
>                 synchronized (this)
>                 {
>                     wait();
>                 }
>             }
>         }
>     }
>
>     /**
>     * A callable task that simply returns a string result.
>     */
>     class NonBlockingTask implements Callable
>     {
>         /*
>          * (non-Javadoc)
>          * @see
>     edu.emory.mathcs.backport.java.util.concurrent.Callable#call()
>          */
>         public Object call() throws Exception
>         {
>             return "NonBlockingTaskResult";
>         }
>     }
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest@altair.cs.oswego.edu
>     <mailto:Concurrency-interest@altair.cs.oswego.edu>
>     http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>   
From joe.bowbeer at gmail.com  Mon Apr  3 17:37:22 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon Apr  3 17:37:25 2006
Subject: [concurrency-interest] backport: problem with shutdownNow()
	during invokeAll()
In-Reply-To: <44318888.7030505@cs.kuleuven.be>
References: <4422E64E.6070307@quiotix.com>
	<200604031133.k33BXpqZ015724@e450.epcc.ed.ac.uk>
	<63b4e4050604031147i323e3891u654a88d3681ac693@mail.gmail.com>
	<44318888.7030505@cs.kuleuven.be>
Message-ID: <31f2a7bd0604031437s6783fc04rd059e0b8b1e6d0a0@mail.gmail.com>

Interesting idea.  Given that ExecutorTask interface would be so close
to FutureTask and FutureTask is so flexible already, would
ExecutorTask really carry its weight?

FWIW, I always create my own tasks directly, using FutureTask
constructor (or my own subclass), unless I'm doing something very
simple without any corner cases.  (Well, as I said, I always create my
own tasks using FutureTask before executing...)

This also avoids having to map the tasks returned from shutdownNow
back to the tasks submitted or invoked.  It also allows me to track
the tracks through the process starting from before they were
submitted for execution.


Note however that my original comment may have been a red herring.

As Tim pointed out to me off-list:

In Tom's case, the queue is always empty because he uses
newCachedThreadPool with its internal SynchronousQueue.


On 4/3/06, Bart Jacobs <bart.jacobs@cs.kuleuven.be> wrote:
> I don't know if there are any use cases for this, but perhaps there
> should be an ExecutorTask interface, defined as follows:
>
> public interface ExecutorTask extends Runnable {
>     public void run();
>     public void cancel();
> }
>
> This would be used by someone submitting a task instead of Runnable if
> they want to be notified if it turns out that the task they submitted
> will not be run after all. It would be the responsibility of whoever
> calls shutdownNow() to walk the returned collection and call cancel() if
> applicable:
>
> for (Runnable task : executor.shutdownNow()) {
>     if (task instanceof ExecutorTask)
>         ((ExecutorTask)task).cancel();
> }
>
> FutureTask would implement ExecutorTask and cancel itself when getting a
> cancel() call. This would enable the scenario below.
>
> Best,-
> Bart Jacobs
>
> Tim Peierls schreef:
> > A task that has been executed with executor.execute(task) but has not
> > yet run internally might not see the interruption sent by shutdownNow,
> > in which case Future.get() for this task can block indefinitely. I
> > think that is what is happening with your NonBlockingTask; it hasn't
> > had a chance to run by the time shutdownNow interrupts the thread it
> > is scheduled to run on, so that when invokeAll tries to get its value,
> > it blocks forever.
> >
> > The upshot is that you shouldn't mix shutdownNow with invokeAll.
> > Consider using timed InvokeAll, or if you only need one result,
> > invokeAny. That way you can reuse the executor over multiple iterations.
> >
> > This is true for all j.u.c flavors, not just the backport. I
> > reproduced the behavior using a recent Mustang build.
> >
> > --tim
> >

From tim at peierls.net  Mon Apr  3 17:57:57 2006
From: tim at peierls.net (Tim Peierls)
Date: Mon Apr  3 17:58:04 2006
Subject: [concurrency-interest] backport: problem with shutdownNow()
	during invokeAll()
In-Reply-To: <31f2a7bd0604031324v280cddc3k7585e4cb20903743@mail.gmail.com>
References: <4422E64E.6070307@quiotix.com>
	<200604031133.k33BXpqZ015724@e450.epcc.ed.ac.uk>
	<31f2a7bd0604031324v280cddc3k7585e4cb20903743@mail.gmail.com>
Message-ID: <63b4e4050604031457m6b2459ar6f6ff461b272604@mail.gmail.com>

For newCachedThreadPool(), which uses a SynchronousQueue, the list returned
by shutdownNow is always empty, so you might think that all submitted tasks
either complete or are cancelled by shutdownNow.

But a Runnable can be accepted by a worker thread and never run. shutdownNow
tries to cancel all running tasks by interrupting all worker threads, and a
newly-accepted runnable might be abandoned without ever running. I don't
know of an easy way to detect this situation, so try to design things in
such a way that it doesn't matter if it happens.

(Shameless plug: There is a nice way to collect the list of tasks that were
started and cancelled around the time of shutdownNow; see Section 7.2.5 of
the upcoming book Java Concurrency in Practice.)

I still think the best advice is to avoid using shutdownNow as a way to
cancel a group of tasks. Consider using the invokeAll/invokeAny methods,
thereby allowing the pool to be reused.

--tim

On 4/3/06, Joe Bowbeer <joe.bowbeer@gmail.com> wrote:
>
> Adding to Tim's comment:
>
> Note that shutdownNow returns a list of tasks that were still on the
> queue, uncompleted and uninterrupted.
>
>
> On 4/3/06, Tom Sugden <tom@epcc.ed.ac.uk> wrote:
> > Hi,
> >
> > I'm having some difficulty with the shutdownNow functionality of the
> cached
> > thread pool executor service. The scenario I am trying to test is this:
> >
> >    - submit a number of tasks for execution using the invokeAll() method
> >    - wait until some of the tasks have definitely begun execution
> >    - invoke shutdownNow() on the executor service
> >
> > I would expect this to:
> >
> >    - prevent any tasks that have yet to be called from ever being called
> >    - interrupt any tasks that have already been called
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060403/228a979a/attachment.html
From wangchao_csg at hotmail.com  Wed Apr  5 06:24:35 2006
From: wangchao_csg at hotmail.com (wangchao_csg@hotmail.com)
Date: Wed Apr  5 06:24:50 2006
Subject: [concurrency-interest] How to use atomic package with memory mapped
	file
Message-ID: <BAY113-F13586AA59AD0F9888B634AFDCB0@phx.gbl>

An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060405/823a9d58/attachment.html
From matthias.ernst at coremedia.com  Wed Apr  5 09:24:36 2006
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Wed Apr  5 09:26:04 2006
Subject: [concurrency-interest] How to use atomic package with memory
	mappedfile
References: <BAY113-F13586AA59AD0F9888B634AFDCB0@phx.gbl>
Message-ID: <F34C8A704C489B46B9E9FBDBD1B91D5F636166@MARS.coremedia.com>

Chao,
 
indeed there is no way to use j.u.c.atomic.* with mapped byte buffers. 
 
An undocumented and non-portable (between VM implementations) way I figured out
is be the class sun.misc.Unsafe#compareAndSwapInt, passing null as the object and the
buffer address as field offset. You would need to get the buffer address through JNI or private
field access in the buffer object. I do not know about the memory effects, i.e. whether #compareAndSwap inserts the necessary memory barrier in order to not reorder previous #puts. In order to access the Unsafe singleton you need to have your code in the boot class path.
 
I think a few compareAndSwap methods on buffers might make a neat addition.
 
Matthias

________________________________

Hi, 

It seems that there is no way to use java.nio.MappedByteBuffer to map a disk file into memory, and at the same time, use java.util.concurrent.atomic.* objects' atomic methods to update the direct bufer sections' content. Is that true for JDK1.5?

If it is true, how to implement a thread safe shared memory based common facility in a performant way. Does it mean we need to write native code to do necessary synchronization among threads? Anyone can give me an hint?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060405/29bc21a9/attachment.html
From tom at epcc.ed.ac.uk  Wed Apr  5 11:36:23 2006
From: tom at epcc.ed.ac.uk (Tom Sugden)
Date: Wed Apr  5 11:36:37 2006
Subject: [concurrency-interest] backport: problem with shutdownNow()
	during invokeAll()
In-Reply-To: <63b4e4050604031457m6b2459ar6f6ff461b272604@mail.gmail.com>
Message-ID: <200604051536.k35FaNXp018070@e450.epcc.ed.ac.uk>

Hi,

Thanks for all the responses to my question. They were very useful and
taught me some more about the concurrency utilities. I've redeveloped my
code so that:

   - the ExecutorService is reused and shutdownNow() is not used for
cancellation purposes
   - tasks are submitted as anonymous FutureTasks, allowing me to 
     override done() to invoke a callback
   - tasks are cancelled via the FutureTask#cancel(boolean) method

Everything seems to be working nicely. I did notice one other piece of
surprising behaviour during my experiments, but will post another email
about it, just in case it is unintended.

Thanks again,
Tom

-----Original Message-----
From: tpeierls@gmail.com [mailto:tpeierls@gmail.com] On Behalf Of Tim
Peierls
Sent: 03 April 2006 22:58
To: Tom Sugden; Joe Bowbeer
Cc: concurrency-interest@cs.oswego.edu
Subject: Re: [concurrency-interest] backport: problem with shutdownNow()
during invokeAll()

For newCachedThreadPool(), which uses a SynchronousQueue, the list returned
by shutdownNow is always empty, so you might think that all submitted tasks
either complete or are cancelled by shutdownNow.

But a Runnable can be accepted by a worker thread and never run. shutdownNow
tries to cancel all running tasks by interrupting all worker threads, and a
newly-accepted runnable might be abandoned without ever running. I don't
know of an easy way to detect this situation, so try to design things in
such a way that it doesn't matter if it happens.

(Shameless plug: There is a nice way to collect the list of tasks that were
started and cancelled around the time of shutdownNow; see Section 7.2.5 of
the upcoming book Java Concurrency in Practice.)

I still think the best advice is to avoid using shutdownNow as a way to
cancel a group of tasks. Consider using the invokeAll/invokeAny methods,
thereby allowing the pool to be reused.

--tim

On 4/3/06, Joe Bowbeer <joe.bowbeer@gmail.com> wrote:
>
> Adding to Tim's comment:
>
> Note that shutdownNow returns a list of tasks that were still on the 
> queue, uncompleted and uninterrupted.
>
>
> On 4/3/06, Tom Sugden <tom@epcc.ed.ac.uk> wrote:
> > Hi,
> >
> > I'm having some difficulty with the shutdownNow functionality of the
> cached
> > thread pool executor service. The scenario I am trying to test is this:
> >
> >    - submit a number of tasks for execution using the invokeAll() method
> >    - wait until some of the tasks have definitely begun execution
> >    - invoke shutdownNow() on the executor service
> >
> > I would expect this to:
> >
> >    - prevent any tasks that have yet to be called from ever being called
> >    - interrupt any tasks that have already been called
>

From tom at epcc.ed.ac.uk  Wed Apr  5 12:20:20 2006
From: tom at epcc.ed.ac.uk (Tom Sugden)
Date: Wed Apr  5 12:20:30 2006
Subject: [concurrency-interest] backport: unchecked exceptions swallowed by
	executor service
Message-ID: <200604051620.k35GKKvW020224@e450.epcc.ed.ac.uk>

Hi,

I was surprised by the behaviour that occurs when an unchecked exception is
raised by an overridden implementation of FutureTask#done() method. The
exception seems to be swallowed and the executor maintains a user thread for
some time afterwards. This isn't causing any problems for me at the moment,
but I was just wondering whether it was the intended behaviour? It could be
difficult to debug if your done() implementation had a programming error
that raised an unchecked exception.

A sample program is pasted below.

Cheers,
Tom

---

import edu.emory.mathcs.backport.java.util.concurrent.ExecutorService;
import edu.emory.mathcs.backport.java.util.concurrent.Executors;
import edu.emory.mathcs.backport.java.util.concurrent.FutureTask;

public class ExecutorTest
{
    public static void main(String[] args) throws Exception
    {
        ExecutorService executor = Executors.newCachedThreadPool();

        Runnable r = new Runnable()
        {
            public void run()
            {
                System.out.println("Running");
            }
        };
        
        FutureTask task = new FutureTask(r, null)
        {
            public void done()
            {
                System.out.println("Done!");
                throw new NullPointerException();
            }
        };

        executor.submit(task);
        
        task.get();
        System.out.println("End");
    }
}

From tim at peierls.net  Wed Apr  5 16:54:19 2006
From: tim at peierls.net (Tim Peierls)
Date: Wed Apr  5 16:54:26 2006
Subject: [concurrency-interest] backport: unchecked exceptions swallowed
	by executor service
In-Reply-To: <200604051620.k35GKKvW020224@e450.epcc.ed.ac.uk>
References: <200604051620.k35GKKvW020224@e450.epcc.ed.ac.uk>
Message-ID: <63b4e4050604051354v193de65cy3499aaf38f23352f@mail.gmail.com>

The executor's worker thread hangs on for 60 seconds -- the default
keepalive value for newCachedThreadPool() -- whether or not your task throws
an exception in done(). Try commenting out the throw NPE line to see this.

You want to shutdown the executor if you are done with it. The standard way
to do this in a main method is:

ExecutorService exec = ...;
try {
    ... do stuff with exec ...
} finally {
    exec.shutdown();
    if (!exec.awaitTermination(TIMEOUT, TIMEOUT_UNIT)) {
        System.err.printf("exec did not terminate after %d %s%n", TIMEOUT,
TIMEOUT_UNIT);
    }
}

--tim


On 4/5/06, Tom Sugden <tom@epcc.ed.ac.uk> wrote:
>
> Hi,
>
> I was surprised by the behaviour that occurs when an unchecked exception
> is
> raised by an overridden implementation of FutureTask#done() method. The
> exception seems to be swallowed and the executor maintains a user thread
> for
> some time afterwards. This isn't causing any problems for me at the
> moment,
> but I was just wondering whether it was the intended behaviour? It could
> be
> difficult to debug if your done() implementation had a programming error
> that raised an unchecked exception.
>
> A sample program is pasted below.
>
> Cheers,
> Tom
>
> ---
>
> import edu.emory.mathcs.backport.java.util.concurrent.ExecutorService;
> import edu.emory.mathcs.backport.java.util.concurrent.Executors;
> import edu.emory.mathcs.backport.java.util.concurrent.FutureTask;
>
> public class ExecutorTest
> {
>     public static void main(String[] args) throws Exception
>     {
>         ExecutorService executor = Executors.newCachedThreadPool();
>
>         Runnable r = new Runnable()
>         {
>             public void run()
>             {
>                 System.out.println("Running");
>             }
>         };
>
>         FutureTask task = new FutureTask(r, null)
>         {
>             public void done()
>             {
>                 System.out.println("Done!");
>                 throw new NullPointerException();
>             }
>         };
>
>         executor.submit(task);
>
>         task.get();
>         System.out.println("End");
>     }
> }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060405/c2180629/attachment.html
From narayanaswamy at huawei.com  Fri Apr  7 12:38:51 2006
From: narayanaswamy at huawei.com (NarayanaSwamy)
Date: Fri Apr  7 12:50:54 2006
Subject: [concurrency-interest] Regarding run when blocked policy
Message-ID: <000501c65a61$c132b050$9612120a@china.huawei.com>

 

Hi,

Created the PooledExecuter with the following 

1.       No Queue

2.       min pool size as 11

3.       max pool size as 13

4.       keep alive time as -1

5.       created 11 threads initially

6.       blocked execution policy is RunWhenBlocked

 

Now I am executing 26 runnable tasks. (All are independent and each task
will sleep for 6 minutes)  [Issuing using a for loop]

 

I found that less than 14 tasks are executed initially (This is very
random). 

 

I think, the SynchronousChnnel.offer returns false and so this happens.. But
I am not able to get the correct reason.

Can you please let me know the details regarding this?

 

Regards,

NarayanaSwamy A.

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060407/86e17657/attachment.html
From narayanaswamy at huawei.com  Fri Apr  7 13:39:03 2006
From: narayanaswamy at huawei.com (NarayanaSwamy)
Date: Fri Apr  7 13:39:57 2006
Subject: [concurrency-interest] Regarding DiscardOldestWhen Blocked
Message-ID: <000a01c65a6a$29d5f9c0$9612120a@china.huawei.com>

Hi,

I tried the following combination for pooled executer

 

1.       Create a pooled executer with BoundedLinkedQueue of capacity 7.

2.       Set the minimum, maximum threads as 2.

3.       Set the keep alive time as -1

4.       Create the two threads

5.       set the policy as Discard oldest when blocked

 

Now when executing 11 tasks, 3 are getting dropped randomly (Sometimes may
be 4)

Most of the times (2nd and 3rd or 4th task is getting dropped)

Randomly 2nd ,3rd and 4th are getting dropped ..

 

Can you please let me know the details?

 

Regards,

NarayanaSwamy A.

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060407/3deadc52/attachment.html
From joe.bowbeer at gmail.com  Fri Apr  7 16:12:31 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri Apr  7 16:12:34 2006
Subject: [concurrency-interest] Regarding DiscardOldestWhen Blocked
In-Reply-To: <000a01c65a6a$29d5f9c0$9612120a@china.huawei.com>
References: <000a01c65a6a$29d5f9c0$9612120a@china.huawei.com>
Message-ID: <31f2a7bd0604071312m28561012q8d8f3dcb80ecebb2@mail.gmail.com>

With 11 tasks, queue limit of 7, and drop-oldest discard policy, the
first 4 tasks submitted could be dropped in the worst case.

If the pool threads start pulling tasks from the queue faster than
they can all be submitted, then some of the first tasks submitted (the
"oldest" tasks) will not be dropped.  That explains why you never see
task 1 dropped.

Drop-oldest is a heuristic designed for non-critical time-dependent
tasks where the result is likely to be stale the  longer the task sits
in the queue.  (Banner ads, for example.)


On 4/7/06, NarayanaSwamy <narayanaswamy@huawei.com> wrote:
>
> Hi,
>
> I tried the following combination for pooled executer
>
> 1.       Create a pooled executer with BoundedLinkedQueue of capacity 7.
> 2.       Set the minimum, maximum threads as 2.
> 3.       Set the keep alive time as -1
> 4.       Create the two threads
> 5.       set the policy as Discard oldest when blocked
>
> Now when executing 11 tasks, 3 are getting dropped randomly (Sometimes may
> be 4)
>
> Most of the times (2nd and 3rd or 4th task is getting dropped)
>
> Randomly 2nd ,3rd and 4th are getting dropped ..
>
> Can you please let me know the details?
>
> Regards,
>
> NarayanaSwamy A.
>

From joe.bowbeer at gmail.com  Fri Apr  7 16:26:19 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri Apr  7 16:26:23 2006
Subject: [concurrency-interest] Regarding run when blocked policy
In-Reply-To: <000501c65a61$c132b050$9612120a@china.huawei.com>
References: <000501c65a61$c132b050$9612120a@china.huawei.com>
Message-ID: <31f2a7bd0604071326m40eced25k81e0d8ac515b342b@mail.gmail.com>

With max pool size of 13 and run-when-blocked policy (aka
"caller-runs"), the maximum number of concurrent tasks is

13 + (# of submitting threads)

In your case, this would be 13 long-running tasks executed by pool
threads, and then every subsequent task will be executed by the thread
that submits it.  (The meaning of run-when-blocked is that the thread
that submits the blocked task will run the task itself.)

If you have one thread submitting tasks, then a max of 14 is expected.


On 4/7/06, NarayanaSwamy <narayanaswamy@huawei.com> wrote:
>
> Hi,
>
> Created the PooledExecuter with the following
>
> 1.       No Queue
> 2.       min pool size as 11
> 3.       max pool size as 13
> 4.       keep alive time as -1
> 5.       created 11 threads initially
> 6.       blocked execution policy is RunWhenBlocked
>
> Now I am executing 26 runnable tasks. (All are independent and each task
> will sleep for 6 minutes)  [Issuing using a for loop]
>
> I found that less than 14 tasks are executed initially (This is very
> random).
>
> I think, the SynchronousChnnel.offer returns false and so this happens?. But
> I am not able to get the correct reason.
>
> Can you please let me know the details regarding this?
>
> Regards,
>
> NarayanaSwamy A.
>

From dcholmes at optusnet.com.au  Fri Apr  7 21:51:10 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri Apr  7 21:51:25 2006
Subject: [concurrency-interest] Regarding DiscardOldestWhen Blocked
In-Reply-To: <000a01c65a6a$29d5f9c0$9612120a@china.huawei.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEJPGNAA.dcholmes@optusnet.com.au>

Again it is a timing issue. If you submit tasks faster then they get pulled
off the queue then you will be able to cause task 2 (or even task 1) to get
dropped as it is the oldest, and you might cause up to 4 tasks to get
dropped.

Again the issue is pre-starting the pool threads rather than creating them
on demand. If you are going to pre-start then either don't immediately
bombard the pool with submissions, or else ensure you have sufficient
capacity in the queue to handle the bombard.

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces@cs.oswego.edu
[mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of
NarayanaSwamy
  Sent: Saturday, 8 April 2006 3:39 AM
  To: concurrency-interest@cs.oswego.edu
  Subject: [concurrency-interest] Regarding DiscardOldestWhen Blocked


  Hi,

  I tried the following combination for pooled executer



  1.       Create a pooled executer with BoundedLinkedQueue of capacity 7.

  2.       Set the minimum, maximum threads as 2.

  3.       Set the keep alive time as -1

  4.       Create the two threads

  5.       set the policy as Discard oldest when blocked



  Now when executing 11 tasks, 3 are getting dropped randomly (Sometimes may
be 4)

  Most of the times (2nd and 3rd or 4th task is getting dropped)

  Randomly 2nd ,3rd and 4th are getting dropped ..



  Can you please let me know the details?



  Regards,

  NarayanaSwamy A.




-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060407/7ec28fdb/attachment-0001.html
From dcholmes at optusnet.com.au  Fri Apr  7 21:51:08 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri Apr  7 21:51:27 2006
Subject: [concurrency-interest] Regarding run when blocked policy
In-Reply-To: <000501c65a61$c132b050$9612120a@china.huawei.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEJPGNAA.dcholmes@optusnet.com.au>

There are a lot of potential timing issues here. How many CPU's do you have?
Do you wait for a while after creating the pool so that you can assume that
all the core pool threads are blocked waiting to take a task from the queue?

Otherwise consider this scenario:

1. You create the TPE and prestart the 11 core threads
2. Those core threads are in varying stages of getting to the queue to pull
a task from it
3. You submit task A:
   i. poolsize == coresize so you offer the task to the queue
   ii.assume no worker got to the queue yet, the offers fails
   iii. as poolSize < maxSize you create another pool thread *and* hand it
        task A
4. You submit task B:
   i. poolsize == coresize so you offer the task to the queue
   ii.assume no worker got to the queue yet, the offers fails
   iii. as poolSize < maxSize you create another pool thread *and* hand it
        task B
5. You submit task C:
   i. poolsize == coresize so you offer the task to the queue
   ii.assume no worker got to the queue yet, the offers fails
   iii. as poolSize == maxSize you reject the task
   iv. Task C runs in the submitter thread

So in this worst-case scenario you only got to execute two tasks before one
gets rejected. How many tasks actually run before the caller runs one
depends on how many pre-started threads manage to get to do a take() on the
queue.

If you don't pre-start the core threads then creating a thread hands a task
directly to it so you are guaranteed to get at least maxPoolSize tasks
accepted before any rejections.

What you are seeing is an intentional but somewhat quirky consequence of
both using a synchronous queue and pre-starting core thread.

Cheers,
David Holmes

  -----Original Message-----
  From: concurrency-interest-bounces@cs.oswego.edu
[mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of
NarayanaSwamy
  Sent: Saturday, 8 April 2006 2:39 AM
  To: concurrency-interest@cs.oswego.edu
  Subject: [concurrency-interest] Regarding run when blocked policy




  Hi,

  Created the PooledExecuter with the following

  1.       No Queue

  2.       min pool size as 11

  3.       max pool size as 13

  4.       keep alive time as -1

  5.       created 11 threads initially

  6.       blocked execution policy is RunWhenBlocked



  Now I am executing 26 runnable tasks. (All are independent and each task
will sleep for 6 minutes)  [Issuing using a for loop]



  I found that less than 14 tasks are executed initially (This is very
random).



  I think, the SynchronousChnnel.offer returns false and so this happens..
But I am not able to get the correct reason.

  Can you please let me know the details regarding this?



  Regards,

  NarayanaSwamy A.




-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060407/0bed8cae/attachment.html
From dcholmes at optusnet.com.au  Sat Apr  8 23:19:09 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat Apr  8 23:19:24 2006
Subject: [concurrency-interest] Regarding DiscardOldestWhen Blocked
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEJPGNAA.dcholmes@optusnet.com.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCGELBGNAA.dcholmes@optusnet.com.au>

Just a clarification. Pre-starting of the core threads in this case only
affects whether or not tasks 1 and 2 can be discarded as oldest. With
pre-start they can, because the core threads may not have had a chance to do
the take from the queue; but without pre-start tasks 1 and 2 are handed to
the core threads directly. Thanks Joe for pointing this out.

Cheers,
David
  -----Original Message-----
  From: concurrency-interest-bounces@cs.oswego.edu
[mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of David Holmes
  Sent: Saturday, 8 April 2006 11:51 AM
  To: NarayanaSwamy; concurrency-interest@cs.oswego.edu
  Subject: RE: [concurrency-interest] Regarding DiscardOldestWhen Blocked


  Again it is a timing issue. If you submit tasks faster then they get
pulled off the queue then you will be able to cause task 2 (or even task 1)
to get dropped as it is the oldest, and you might cause up to 4 tasks to get
dropped.

  Again the issue is pre-starting the pool threads rather than creating them
on demand. If you are going to pre-start then either don't immediately
bombard the pool with submissions, or else ensure you have sufficient
capacity in the queue to handle the bombard.

  Cheers,
  David Holmes
    -----Original Message-----
    From: concurrency-interest-bounces@cs.oswego.edu
[mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of
NarayanaSwamy
    Sent: Saturday, 8 April 2006 3:39 AM
    To: concurrency-interest@cs.oswego.edu
    Subject: [concurrency-interest] Regarding DiscardOldestWhen Blocked


    Hi,

    I tried the following combination for pooled executer



    1.       Create a pooled executer with BoundedLinkedQueue of capacity 7.

    2.       Set the minimum, maximum threads as 2.

    3.       Set the keep alive time as -1

    4.       Create the two threads

    5.       set the policy as Discard oldest when blocked



    Now when executing 11 tasks, 3 are getting dropped randomly (Sometimes
may be 4)

    Most of the times (2nd and 3rd or 4th task is getting dropped)

    Randomly 2nd ,3rd and 4th are getting dropped ..



    Can you please let me know the details?



    Regards,

    NarayanaSwamy A.




-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060408/c03e4295/attachment.html
From mike.quilleash at azuresolutions.com  Mon Apr 10 08:37:43 2006
From: mike.quilleash at azuresolutions.com (Mike Quilleash)
Date: Mon Apr 10 08:38:11 2006
Subject: [concurrency-interest] Double checked locking
Message-ID: <F1689FB09456E347A6E38343B99E680D017B20D7@THHS2EXBE2X.hostedservice2.net>

Hi there,
 
I have a small piece of code that generates sequential incrementing
id's.  The only requirement is that every call to this function return a
unique id to every other call in the past.  The code is used on multiple
machines so the central "next id" is help in a database table and each
JVM occasionally makes a call to fetch a block of ids from the database
table which are then consumed by the JVM at which point another block
will be fetched and so on.
 
Here's the current code....
 
    private int currentId = -1;
    private int allocatedId = -1;

    public synchronized Integer getNextId()
    {
        if ( allocatedId == -1 || allocatedId == currentId )
        {
            // get the low id allocated
            int lowId = DatabaseIdGenerator.generateId(objectKey,
blockSize).intValue();

            allocatedId = lowId + blockSize;

            // set the current id to the low id
            currentId = lowId;
        }

        // create the return value
        Integer ret = new Integer( currentId );

        // increment the current id to the next available id
        currentId++;

        return ret;
    }

 
I've been doing some performance profiling and found a bottleneck in
this function, on a multiple CPU there is some scalability loss which is
partly caused by blocking on this method's monitor.  So I've changed it
to be like this....
 
 
 
    private AtomicInteger nextId = new AtomicInteger( -1 );
 
    public int getNextId() throws HibernateException
    {
        if ( nextId.get() == -1 || allocatedId == nextId.get() )
        {
            synchronized( nextId )
            {
                if ( nextId.get() == -1 || allocatedId == nextId.get() )
                {
                    // get the low id allocated
                    int lowId = HibernateSession.generateId(objectKey,
blockSize).intValue();
 
                    allocatedId = lowId + blockSize;
 
                    // set the current id to the low id
                    nextId.set( lowId );
                }
            }
        }
 
        // create the return value
        int ret = nextId.getAndIncrement();
 
        return ret;
    }

 
 
So in theory the synchronization is only required when the local Atomic
runs out of ids when it goes to the db to fetch more.  I realize this is
using double checked locking which is a bad thing but I wondered if it
would work in this case, being slightly different from the lazy
intialization that this is usually used for.
 
Or alternatively any better way of doing this.
 
Appreciate any help or comments.
 
Cheers.
 
Mike.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060410/c04a678b/attachment.html
From bart.jacobs at cs.kuleuven.ac.be  Mon Apr 10 10:25:38 2006
From: bart.jacobs at cs.kuleuven.ac.be (Bart Jacobs)
Date: Mon Apr 10 10:30:24 2006
Subject: [concurrency-interest] Double checked locking
In-Reply-To: <F1689FB09456E347A6E38343B99E680D017B20D7@THHS2EXBE2X.hostedservice2.net>
References: <F1689FB09456E347A6E38343B99E680D017B20D7@THHS2EXBE2X.hostedservice2.net>
Message-ID: <443A6AE2.7020709@cs.kuleuven.ac.be>

Hi Mike,

This code is obviously wrong. You have a TOCTOU (time of check to time 
of use) problem. In particular, you're assuming at the getAndIncrement 
call that nextId is less than allocatedId, and you justify that 
assumption by the test you do at the top of the method. However, other 
threads may intervene in the meantime.

Another source of potential trouble, I think, is that allocatedId might 
change under your feet. Therefore, it's probably better to introduce an 
IdBlock class.

Here's my proposed tentative implementation. Note: I advise you to 
solicit review by others as well before settling on any implementation. 
You might even want to go so far as to construct a correctness argument. 
(But then you need to learn the rules of the game, which are hard at 
this level.)

class IdBlock {
    final int limit;
    final AtomicInteger nextId;

    IdBlock(int start, int limit) {
        this.limit = limit;
        this.nextId = new AtomicInteger(start);
    }
}

final Object mutex = new Object();
IdBlock currentBlock;  // Need not be volatile because has only final 
fields?

public int getNextId() throws HibernateException
{
    IdBlock block = currentBlock;
    if (block != null)
    {
        int id = block.nextId.getAndIncrement();
        if (id < block.limit)
            return id;
    }
    synchronized (mutex) {
        // We may have a new block by now, so try again.
        if (currentBlock != null && currentBlock != block)
        {
            int id = currentBlock.nextId.getAndIncrement();
            if (id < currentBlock.limit)
                return id;
        }
        // Allocate a new block
        int lowId = HibernateSession.generateId(objectKey, 
blockSize).intValue();
        currentBlock = new IdBlock(lowId + 1, lowId + blockSize);
        return lowId;
    }
}

Performance testing will have to show whether this implementation is 
faster than your original synchronized implementation, which has the 
great benefit of being simple. So unless a "clever" implementation turns 
out to have significant performance benefits, I advise that you stick 
with the simple synchronized solution.

Good luck,-
Bart

Mike Quilleash schreef:
>     private AtomicInteger nextId = new AtomicInteger( -1 );
>  
>     public int getNextId() throws HibernateException
>     {
>         if ( nextId.get() == -1 || allocatedId == nextId.get() )
>         {
>             synchronized( nextId )
>             {
>                 if ( nextId.get() == -1 || allocatedId == nextId.get() )
>                 {
>                     // get the low id allocated
>                     int lowId = HibernateSession.generateId(objectKey,
> blockSize).intValue();
>  
>                     allocatedId = lowId + blockSize;
>  
>                     // set the current id to the low id
>                     nextId.set( lowId );
>                 }
>             }
>         }
>  
>         // create the return value
>         int ret = nextId.getAndIncrement();
>  
>         return ret;
>     }
>
>  
>  
> So in theory the synchronization is only required when the local Atomic
> runs out of ids when it goes to the db to fetch more.  I realize this is
> using double checked locking which is a bad thing but I wondered if it
> would work in this case, being slightly different from the lazy
> intialization that this is usually used for.
>  
> Or alternatively any better way of doing this.
>  
>   
From jmanson at cs.purdue.edu  Mon Apr 10 11:15:37 2006
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Mon Apr 10 11:15:35 2006
Subject: [concurrency-interest] Double checked locking
In-Reply-To: <F1689FB09456E347A6E38343B99E680D017B20D7@THHS2EXBE2X.hostedservice2.net>
References: <F1689FB09456E347A6E38343B99E680D017B20D7@THHS2EXBE2X.hostedservice2.net>
Message-ID: <443A7699.8070407@cs.purdue.edu>

Mike,

 From a double-checked locking point of view, it is generally okay to 
use an AtomicInteger as the guard variable, because it provides the same 
guarantees as a volatile variable.

 From a concurrency point of view, this code allows one thread to call 
nextId.getAndIncrement() between the if statement and the 
getAndIncrement() of another.  You can end up with:

Initially, nextId = 1 and allocatedId = 2

Thread 1:

(1) if (nextId.get() == -1 || // returns false
(2)    allocatedId == nextId.get())  // returns false
       ...
(6) int ret = nextId.getAndIncrement(); // returns "3"

Thread 2:

(3) if (nextId.get() == -1 || // returns false
(4)    allocatedId == nextId.get())  // returns false
       ...
(5) int ret = nextId.getAndIncrement(); // returns "2"

If these are really ints, and performance is really, truly an issue, you 
can do something pretty clever with compareAndSet and AtomicLongs 
instead (subject to someone other than myself saying this is correct):

AtomicLong currAndMax = new AtomicLong();

public int getNextId() throws HibernateException {
   long current;
   long newCurrent, newMax;
   do {
     current = currAndMax.get();
     if ((current & 0xFFFFFFFFL) == (current >> 32)) {
       newCurrent = (long) HibernateSession.generateId(objectKey, 
blockSize).intValue();
       newMax = newCurrent + blockSize;
     } else {
       newCurrent = (current &  0xFFFFFFFFL) + 1L;
       newMax = current >> 32;
     }
   } while (!currAndMax.compareAndSet(current,
				     (newMax << 32) |  newCurrent));
   return (int) newCurrent;
}

This stores both the maximum id and the current id in a single variable, 
where the maximum id is the high-order 32 bits, and the current id is 
the low order 32 bits.  Both variables are simultaneously, atomically 
updated by the compareAndSet.  If someone performed an update to the 
variable between when the get() occurs and when the compareAndSet() 
occurs, then your updates get discarded and you try the method again.

This does open you up to potentially throwing away calls to generateId, 
so if you can't do that, then this won't work.  Also, it is very 
confusing, so maintainability might be an issue.

					Jeremy

Mike Quilleash wrote:
> Hi there,
>  
> I have a small piece of code that generates sequential incrementing 
> id's.  The only requirement is that every call to this function return a 
> unique id to every other call in the past.  The code is used on multiple 
> machines so the central "next id" is help in a database table and each 
> JVM occasionally makes a call to fetch a block of ids from the database 
> table which are then consumed by the JVM at which point another block 
> will be fetched and so on.
>  
> Here's the current code....
>  
>     private int currentId = -1;
>     private int allocatedId = -1;
>     public synchronized Integer getNextId()
>     {
>         if ( allocatedId == -1 || allocatedId == currentId )
>         {
>             // get the low id allocated
>             int lowId = DatabaseIdGenerator.generateId(objectKey, 
> blockSize).intValue();
> 
>             allocatedId = lowId + blockSize;
> 
>             // set the current id to the low id
>             currentId = lowId;
>         }
> 
>         // create the return value
>         Integer ret = new Integer( currentId );
> 
>         // increment the current id to the next available id
>         currentId++;
> 
>         return ret;
>     }
>  
> I've been doing some performance profiling and found a bottleneck in 
> this function, on a multiple CPU there is some scalability loss which is 
> partly caused by blocking on this method's monitor.  So I've changed it 
> to be like this....
>  
>  
>  
>     private AtomicInteger nextId = new AtomicInteger( -1 );
>  
>     public int getNextId() throws HibernateException
>     {
>         if ( nextId.get() == -1 || allocatedId == nextId.get() )
>         {
>             synchronized( nextId )
>             {
>                 if ( nextId.get() == -1 || allocatedId == nextId.get() )
>                 {
>                     // get the low id allocated
>                     int lowId = HibernateSession.generateId(objectKey, 
> blockSize).intValue();
>  
>                     allocatedId = lowId + blockSize;
>  
>                     // set the current id to the low id
>                     nextId.set( lowId );
>                 }
>             }
>         }
>  
>         // create the return value
>         int ret = nextId.getAndIncrement();
>  
>         return ret;
>     }
>  
>  
> So in theory the synchronization is only required when the local Atomic 
> runs out of ids when it goes to the db to fetch more.  I realize this is 
> using double checked locking which is a bad thing but I wondered if it 
> would work in this case, being slightly different from the lazy 
> intialization that this is usually used for.
>  
> Or alternatively any better way of doing this.
>  
> Appreciate any help or comments.
>  
> Cheers.
>  
> Mike.
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From mike.quilleash at azuresolutions.com  Mon Apr 10 11:57:14 2006
From: mike.quilleash at azuresolutions.com (Mike Quilleash)
Date: Mon Apr 10 11:57:23 2006
Subject: [concurrency-interest] Double checked locking
Message-ID: <F1689FB09456E347A6E38343B99E680D017B232A@THHS2EXBE2X.hostedservice2.net>

Thanks for your replies.

Unfortunately I need to use the same system for longs as well as ints so
I couldn't use this solution for both (although it is very cunning!).  

I'm going to try out Bart's suggestion first as it looks good and
eliminates the synchronization except when loading a new id block (the
id blocks tend to be sized 10k - 1m) so this should be fine.  The
function is heavily travelled esp on a multi cpu box (maybe tens of
thousands of hits per second) so eliminating this may help.

If anyone has any comments on Bart's solution I'd appreciate it.  I'll
let you know if it helps my performance.

Thanks again.

-----Original Message-----
From: Jeremy Manson [mailto:jmanson@cs.purdue.edu] 
Sent: 10 April 2006 16:16
To: Mike Quilleash
Cc: concurrency-interest@cs.oswego.edu
Subject: Re: [concurrency-interest] Double checked locking

Mike,

 From a double-checked locking point of view, it is generally okay to
use an AtomicInteger as the guard variable, because it provides the same
guarantees as a volatile variable.

 From a concurrency point of view, this code allows one thread to call
nextId.getAndIncrement() between the if statement and the
getAndIncrement() of another.  You can end up with:

Initially, nextId = 1 and allocatedId = 2

Thread 1:

(1) if (nextId.get() == -1 || // returns false
(2)    allocatedId == nextId.get())  // returns false
       ...
(6) int ret = nextId.getAndIncrement(); // returns "3"

Thread 2:

(3) if (nextId.get() == -1 || // returns false
(4)    allocatedId == nextId.get())  // returns false
       ...
(5) int ret = nextId.getAndIncrement(); // returns "2"

If these are really ints, and performance is really, truly an issue, you
can do something pretty clever with compareAndSet and AtomicLongs
instead (subject to someone other than myself saying this is correct):

AtomicLong currAndMax = new AtomicLong();

public int getNextId() throws HibernateException {
   long current;
   long newCurrent, newMax;
   do {
     current = currAndMax.get();
     if ((current & 0xFFFFFFFFL) == (current >> 32)) {
       newCurrent = (long) HibernateSession.generateId(objectKey,
blockSize).intValue();
       newMax = newCurrent + blockSize;
     } else {
       newCurrent = (current &  0xFFFFFFFFL) + 1L;
       newMax = current >> 32;
     }
   } while (!currAndMax.compareAndSet(current,
				     (newMax << 32) |  newCurrent));
   return (int) newCurrent;
}

This stores both the maximum id and the current id in a single variable,
where the maximum id is the high-order 32 bits, and the current id is
the low order 32 bits.  Both variables are simultaneously, atomically
updated by the compareAndSet.  If someone performed an update to the
variable between when the get() occurs and when the compareAndSet()
occurs, then your updates get discarded and you try the method again.

This does open you up to potentially throwing away calls to generateId,
so if you can't do that, then this won't work.  Also, it is very
confusing, so maintainability might be an issue.

					Jeremy

Mike Quilleash wrote:
> Hi there,
>  
> I have a small piece of code that generates sequential incrementing 
> id's.  The only requirement is that every call to this function return

> a unique id to every other call in the past.  The code is used on 
> multiple machines so the central "next id" is help in a database table

> and each JVM occasionally makes a call to fetch a block of ids from 
> the database table which are then consumed by the JVM at which point 
> another block will be fetched and so on.
>  
> Here's the current code....
>  
>     private int currentId = -1;
>     private int allocatedId = -1;
>     public synchronized Integer getNextId()
>     {
>         if ( allocatedId == -1 || allocatedId == currentId )
>         {
>             // get the low id allocated
>             int lowId = DatabaseIdGenerator.generateId(objectKey,
> blockSize).intValue();
> 
>             allocatedId = lowId + blockSize;
> 
>             // set the current id to the low id
>             currentId = lowId;
>         }
> 
>         // create the return value
>         Integer ret = new Integer( currentId );
> 
>         // increment the current id to the next available id
>         currentId++;
> 
>         return ret;
>     }
>  
> I've been doing some performance profiling and found a bottleneck in 
> this function, on a multiple CPU there is some scalability loss which 
> is partly caused by blocking on this method's monitor.  So I've 
> changed it to be like this....
>  
>  
>  
>     private AtomicInteger nextId = new AtomicInteger( -1 );
>  
>     public int getNextId() throws HibernateException
>     {
>         if ( nextId.get() == -1 || allocatedId == nextId.get() )
>         {
>             synchronized( nextId )
>             {
>                 if ( nextId.get() == -1 || allocatedId == nextId.get()
)
>                 {
>                     // get the low id allocated
>                     int lowId = HibernateSession.generateId(objectKey,
> blockSize).intValue();
>  
>                     allocatedId = lowId + blockSize;
>  
>                     // set the current id to the low id
>                     nextId.set( lowId );
>                 }
>             }
>         }
>  
>         // create the return value
>         int ret = nextId.getAndIncrement();
>  
>         return ret;
>     }
>  
>  
> So in theory the synchronization is only required when the local 
> Atomic runs out of ids when it goes to the db to fetch more.  I 
> realize this is using double checked locking which is a bad thing but 
> I wondered if it would work in this case, being slightly different 
> from the lazy intialization that this is usually used for.
>  
> Or alternatively any better way of doing this.
>  
> Appreciate any help or comments.
>  
> Cheers.
>  
> Mike.
> 
> 
> ----------------------------------------------------------------------
> --
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From alarmnummer at gmail.com  Wed Apr 12 09:47:39 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed Apr 12 09:47:43 2006
Subject: [concurrency-interest] multi core/cpu systems and Java
Message-ID: <1466c1d60604120647s507e424qcb2f5748a0f928e3@mail.gmail.com>

I have a question about multicore/multicpu processors (and especially
the Intel and AMD processors) and Java and threads.

The question is:
are Java threads (of a single vm) executed on all core`s or are they
bound to a single cpu?
With other words: if you are executing a single cpu-bound application,
do you get any benefits of a multicore system? The most logical answer
would be yes.. but a colleague claims that java threads are bound to a
single cpu so you won't get any performance gain.

The operating systems are: Windows and Linux 2.6 and the vm is the
standard 5.0 version from Sun.

From jmanson at cs.purdue.edu  Wed Apr 12 10:07:00 2006
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Apr 12 10:07:17 2006
Subject: [concurrency-interest] multi core/cpu systems and Java
In-Reply-To: <1466c1d60604120647s507e424qcb2f5748a0f928e3@mail.gmail.com>
References: <1466c1d60604120647s507e424qcb2f5748a0f928e3@mail.gmail.com>
Message-ID: <443D0984.4080204@cs.purdue.edu>

Your colleague is incorrect.  Sun's JVM will distribute its threads 
among however many processors there are on a machine and however many 
cores there are on a given processor.  Java would be unsuitable for use 
on modern server architectures if this were not the case.

I believe that it was true that Java was bound to a single CPU once upon 
a time, but that was only in versions 1.0 and 1.1.  ISTR that there was 
a patch for it in 1.1, and 1.2 had native support.  My memory on this 
might be faulty.

					Jeremy

Peter Veentjer wrote:
> I have a question about multicore/multicpu processors (and especially
> the Intel and AMD processors) and Java and threads.
> 
> The question is:
> are Java threads (of a single vm) executed on all core`s or are they
> bound to a single cpu?
> With other words: if you are executing a single cpu-bound application,
> do you get any benefits of a multicore system? The most logical answer
> would be yes.. but a colleague claims that java threads are bound to a
> single cpu so you won't get any performance gain.
> 
> The operating systems are: Windows and Linux 2.6 and the vm is the
> standard 5.0 version from Sun.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From brian at quiotix.com  Wed Apr 12 11:49:45 2006
From: brian at quiotix.com (Brian Goetz)
Date: Wed Apr 12 11:49:57 2006
Subject: [concurrency-interest] multi core/cpu systems and Java
In-Reply-To: <1466c1d60604120647s507e424qcb2f5748a0f928e3@mail.gmail.com>
References: <1466c1d60604120647s507e424qcb2f5748a0f928e3@mail.gmail.com>
Message-ID: <443D2199.2060104@quiotix.com>

The short answer to nearly all questions having to do with 
logical/physical processors, and processor affinity: that's an OS issue, 
not a JVM issue.

The only interpretation under which your colleague would be right is an 
application with only a single thread; the basic unit of scheduling for 
most OSes is the thread.  A program with only one thread can only run on 
one CPU, but a program with more threads can use multiple CPUs.

Peter Veentjer wrote:
> I have a question about multicore/multicpu processors (and especially
> the Intel and AMD processors) and Java and threads.
> 
> The question is:
> are Java threads (of a single vm) executed on all core`s or are they
> bound to a single cpu?
> With other words: if you are executing a single cpu-bound application,
> do you get any benefits of a multicore system? The most logical answer
> would be yes.. but a colleague claims that java threads are bound to a
> single cpu so you won't get any performance gain.
> 
> The operating systems are: Windows and Linux 2.6 and the vm is the
> standard 5.0 version from Sun.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
From mike_edwards at uk.ibm.com  Wed Apr 12 12:54:05 2006
From: mike_edwards at uk.ibm.com (Mike Edwards)
Date: Wed Apr 12 12:53:46 2006
Subject: [concurrency-interest] Re: Concurrency-interest Digest, Vol 15,
	Issue 11
In-Reply-To: <200604121600.k3CG0HZr004246@cs.oswego.edu>
Message-ID: <OF8B4E4697.0607EEA1-ON8025714E.005B7520-8025714E.005CC829@uk.ibm.com>

Peter,

In general, modern JVM implementations use the threading system provided 
by the
operating system.  As a result when you're using multiple threads in Java, 
the threads
are distributed to the processor cores by the OS, which typically mean 
that they are
run on whichever core is available.

So your colleague is incorrect on this point.  Tell him to go look at the 
SpecJBB
scores on the spec website, which show virtually linear scaling of Java 
running on large
multiprocessor systems with eg 128 cores. (www.spec.org).

The funny thing is that there is some demand from modern system designs to 
do things
differently - there is increasing use of NUMA style architectures, where 
performance of 
access to memory varies depending on which processor and which section of 
memory 
is involved.  This implies that it may benefit performance by running a 
thread on a 
processor that is "close" to the memory it references.  Java is not so 
good at doing 
this because of the nature of the Java Heap - the heap typically spans 
some large
block of memory and a thread can access objects anywhere in the heap.....


Yours,  Mike.

Strategist - Emerging Technologies.
IBM Hursley Park, Mail Point 146, Winchester, SO21 2JN, Great Britain.

> Date: Wed, 12 Apr 2006 14:47:39 +0100
> From: "Peter Veentjer" <alarmnummer@gmail.com>
> Subject: [concurrency-interest] multi core/cpu systems and Java
> To: Concurrency-interest@cs.oswego.edu
> Message-ID:
>    <1466c1d60604120647s507e424qcb2f5748a0f928e3@mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> I have a question about multicore/multicpu processors (and especially
> the Intel and AMD processors) and Java and threads.
> 
> The question is:
> are Java threads (of a single vm) executed on all core`s or are they
> bound to a single cpu?
> With other words: if you are executing a single cpu-bound application,
> do you get any benefits of a multicore system? The most logical answer
> would be yes.. but a colleague claims that java threads are bound to a
> single cpu so you won't get any performance gain.
> 
> The operating systems are: Windows and Linux 2.6 and the vm is the
> standard 5.0 version from Sun.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060412/f8e56452/attachment.html
From eross at m-Qube.com  Fri Apr 14 11:55:47 2006
From: eross at m-Qube.com (Elias Ross)
Date: Fri Apr 14 11:56:00 2006
Subject: [concurrency-interest] Gotcha in ConcurrentHashMap;
	need better docs?
Message-ID: <1145030148.24992.92.camel@scrub>


Take a look at this:

Map m = new ConcurrentHashMap(); 
m.put("foo", "bar"); 
Iterator i = m.entrySet().iterator(); 
Map.Entry me = (Map.Entry) i.next(); 
m.remove("foo"); // in another thread
me.getValue(); // returns null

The JavaDoc:

> Like Hashtable but unlike HashMap, this class does NOT allow null to
> be used as a key or value. 

I'm wondering if there was some way to enhance the documentation so that
people wouldn't assume that Map.Entry.getValue() never returns null.

JBoss bug:

http://jira.jboss.com/jira/browse/JBAS-2973


From dl at cs.oswego.edu  Fri Apr 14 13:44:57 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri Apr 14 13:45:00 2006
Subject: [concurrency-interest] Gotcha in ConcurrentHashMap;	need better
	docs?
In-Reply-To: <1145030148.24992.92.camel@scrub>
References: <1145030148.24992.92.camel@scrub>
Message-ID: <443FDF99.1030501@cs.oswego.edu>

Elias Ross wrote:
> Take a look at this:
> 
> Map m = new ConcurrentHashMap(); 
> m.put("foo", "bar"); 
> Iterator i = m.entrySet().iterator(); 
> Map.Entry me = (Map.Entry) i.next(); 
> m.remove("foo"); // in another thread
> me.getValue(); // returns null
> 
> The JavaDoc:
> 
>> Like Hashtable but unlike HashMap, this class does NOT allow null to
>> be used as a key or value. 
> 
> I'm wondering if there was some way to enhance the documentation so that
> people wouldn't assume that Map.Entry.getValue() never returns null.
> 

Better yet, the Mustang version does not return null; instead it
returns a snapshot of accessed value. See
   http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6312056

Doing otherwise was arguably a bug (which is why we filed a bug
against ourselves.)  While almost no one
has a good intuition about what "should" happen, no one
expects null to be returned.

(Sorry that this like lots of other small things wait until
Mustang for full release, even though the codebase was changed
last summer.)

-Doug

From eross at m-Qube.com  Fri Apr 14 15:02:26 2006
From: eross at m-Qube.com (Elias Ross)
Date: Fri Apr 14 15:02:40 2006
Subject: [concurrency-interest] Gotcha in ConcurrentHashMap;	need
	better docs?
In-Reply-To: <443FDF99.1030501@cs.oswego.edu>
References: <1145030148.24992.92.camel@scrub> <443FDF99.1030501@cs.oswego.edu>
Message-ID: <1145041347.7004.47.camel@scrub>

On Fri, 2006-04-14 at 13:44 -0400, Doug Lea wrote:

> Better yet, the Mustang version does not return null; instead it
> returns a snapshot of accessed value. See
>    http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6312056
> 
> Doing otherwise was arguably a bug (which is why we filed a bug
> against ourselves.)  While almost no one
> has a good intuition about what "should" happen, no one
> expects null to be returned.
> 

Returning the snapshot value is probably the best solution.

I didn't see the snapshot approach listed in the bug or bug comments.  I
also haven't seen anything mentioned in the JavaDoc here:

http://download.java.net/jdk6/docs/api/java/util/concurrent/ConcurrentHashMap.html#entrySet()


From dl at cs.oswego.edu  Fri Apr 14 16:20:35 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri Apr 14 16:20:38 2006
Subject: [concurrency-interest] Gotcha in ConcurrentHashMap;	need	better
	docs?
In-Reply-To: <1145041347.7004.47.camel@scrub>
References: <1145030148.24992.92.camel@scrub> <443FDF99.1030501@cs.oswego.edu>
	<1145041347.7004.47.camel@scrub>
Message-ID: <44400413.60801@cs.oswego.edu>

Elias Ross wrote:
> 
> 
> Returning the snapshot value is probably the best solution.
> 
>  I
> also haven't seen anything mentioned in the JavaDoc here:
> 

This is because the docs said and still way that the iterator
is "weakly consistent" which it now is, even in this corner-case
sense.

-Doug



From dl at cs.oswego.edu  Wed Apr 19 12:17:19 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed Apr 19 12:17:22 2006
Subject: [concurrency-interest] Navigable Set and Map improvements
Message-ID: <4446628F.4040108@cs.oswego.edu>

One of the good things about our putting out APIs and code
ourselves is that people get a chance to evaluate usages before they
are frozen into J2SE releases. While classes can usually evolve,
interfaces inside the java.* packages never can, so it's important
to get them right for the first time for major releases.

Various engineers at Google have been using over the past year our
jsr166x package versions of NavigableSet, NavigableMap,
ConcurrentNavigableMap, and the Tree and ConcurrentSkipList
implementations. And they started noticing things that they
wanted to do but weren't supported. So we are trying to get
some enhancements into Mustang while there is still a chance.
In particular:

1. Exposing descending iterators and keySets didn't allow
people to treat the descending version of a set or map
as itself a set or map. So, we add:
   NavigableMap.descendingMap();
   NavigableSet.descendingSet();
(Taking away the now-useless NavigableMap.descendingEntrySet).

2. The old Sorted{Set,Map} conventions about allowing only inclusive
lower bounds and only exclusive upper bounds were too inflexible,
so we allow you to specify all combinations of inclusion/exclusion,
as in:
   NavigableMap.subMap(K lo, boolean loInclusive, K hi, boolean hiIinclusive)

The main impact on those who have already been using these APIs
in Mustang betas is that anyone using descendingEntrySet will
need to replace with descendingMap().entrySet(). We don't feel that
it is a bad idea to kill this because anyone using it surely
wished they had a descendingMap anyway, and were working around its
absence.

You can find APIs and code in the usual places. Any comments
would be appreciated.

API specs:
   http://gee.cs.oswego.edu/dl/jsr166/dist/docs/
CVS sources:
   http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/

(Disclaimer: We aren't sure that these will actually make it into Mustang.)

-Doug

From brian at quiotix.com  Wed Apr 19 14:11:25 2006
From: brian at quiotix.com (Brian Goetz)
Date: Wed Apr 19 14:11:33 2006
Subject: [concurrency-interest] Java Concurrency in Practice goes to press
Message-ID: <44467D4D.9000003@quiotix.com>

I am pleased to announce that Java Concurrency in Practice has gone to 
press, and printed books should be available by the JavaOne conference 
in less than a month.  (Amazon is still saying July, but I suspect they 
will be shipping copies before then.)

   http://www.amazon.com/exec/obidos/ASIN/0321349601/ref=nosim/none0b69

From tim at peierls.net  Wed Apr 19 14:51:40 2006
From: tim at peierls.net (Tim Peierls)
Date: Wed Apr 19 14:51:43 2006
Subject: [concurrency-interest] Java Concurrency in Practice goes to press
In-Reply-To: <44467D4D.9000003@quiotix.com>
References: <44467D4D.9000003@quiotix.com>
Message-ID: <63b4e4050604191151o6d1c4a58oc653530b4b59690a@mail.gmail.com>

And note that the jacket cover with the bees currently displayed by Amazon
is history. The real cover is niftier.

--tim

On 4/19/06, Brian Goetz <brian@quiotix.com> wrote:
>
> I am pleased to announce that Java Concurrency in Practice has gone to
> press, and printed books should be available by the JavaOne conference
> in less than a month.  (Amazon is still saying July, but I suspect they
> will be shipping copies before then.)
>
>    http://www.amazon.com/exec/obidos/ASIN/0321349601/ref=nosim/none0b69
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060419/27c4fddc/attachment.html
From tackline at tackline.plus.com  Sat Apr 22 15:47:27 2006
From: tackline at tackline.plus.com (Thomas Hawtin)
Date: Sat Apr 22 16:42:09 2006
Subject: [concurrency-interest] Navigable Set and Map improvements
In-Reply-To: <4446628F.4040108@cs.oswego.edu>
References: <4446628F.4040108@cs.oswego.edu>
Message-ID: <444A884F.9060000@tackline.plus.com>

Doug Lea wrote:
> 
> 1. Exposing descending iterators and keySets didn't allow
> people to treat the descending version of a set or map
> as itself a set or map. So, we add:
>   NavigableMap.descendingMap();
>   NavigableSet.descendingSet();
> (Taking away the now-useless NavigableMap.descendingEntrySet).

Those method names indicate idempotency. It may return a map that, 
considered on its own, is ascending. map.descendingMap() looks as if it 
is just the same as map.descendingMap().descendingMap(). reversed, 
asReverse or reverseOrder seem more obvious to me, and in keeping with 
Collections and Arrays.

> 2. The old Sorted{Set,Map} conventions about allowing only inclusive
> lower bounds and only exclusive upper bounds were too inflexible,
> so we allow you to specify all combinations of inclusion/exclusion,
> as in:
>   NavigableMap.subMap(K lo, boolean loInclusive, K hi, boolean 
> hiIinclusive)

I find boolean literals random sprinkled in method calls a tad opaque. 
At least use enums. Preferably four methods (the best argument against 
that seems to be that JavaDoc handling is very poor).


Having a quick look at the CVS, it looks as if sub-maps break 
serialisation compatibility.


Also the API documentation (currently in mustang) for Map.Entry seems to 
be out of date. It does not indicate the valid life time of entries 
acquired by means other than entrySet.

Tom Hawtin
From dl at cs.oswego.edu  Sat Apr 22 18:50:26 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat Apr 22 18:50:29 2006
Subject: [concurrency-interest] Navigable Set and Map improvements
In-Reply-To: <444A884F.9060000@tackline.plus.com>
References: <4446628F.4040108@cs.oswego.edu>
	<444A884F.9060000@tackline.plus.com>
Message-ID: <444AB332.4020208@cs.oswego.edu>

Thomas Hawtin wrote:
> Doug Lea wrote:
>>
>> 1. Exposing descending iterators and keySets didn't allow
>> people to treat the descending version of a set or map
>> as itself a set or map. So, we add:
>>   NavigableMap.descendingMap();
>>   NavigableSet.descendingSet();
>> (Taking away the now-useless NavigableMap.descendingEntrySet).
> 
> Those method names indicate idempotency. It may return a map that, 
> considered on its own, is ascending. map.descendingMap() looks as if it 
> is just the same as map.descendingMap().descendingMap(). reversed, 
> asReverse or reverseOrder seem more obvious to me, and in keeping with 
> Collections and Arrays.

Thanks! This a good thought; we hadn't considered that some
people might interpret "descending" as an absolute term.
We'll mull it over.

> 
>> 2. The old Sorted{Set,Map} conventions about allowing only inclusive
>> lower bounds and only exclusive upper bounds were too inflexible,
>> so we allow you to specify all combinations of inclusion/exclusion,
>> as in:
>>   NavigableMap.subMap(K lo, boolean loInclusive, K hi, boolean 
>> hiIinclusive)
> 
> I find boolean literals random sprinkled in method calls a tad opaque. 

We did discuss this. We think the ability to use boolean
expressions here is useful, rather than having to write, for example,
complement as:
  (d == NavigableMap.INCLUSIVE)? NavigableMap.EXCLUSIVE : NavigableMap.INCLUSIVE
instead of just
   !d
And inclusive==true seems pretty intuitive, at least considering
the audience for this kind of functionality.

> Having a quick look at the CVS, it looks as if sub-maps break 
> serialisation compatibility.

Not as of a few checkins ago.

> 
> Also the API documentation (currently in mustang) for Map.Entry seems to 
> be out of date. It does not indicate the valid life time of entries 
> acquired by means other than entrySet.
>

Thanks. I'll look into it.

-Doug


From gergg at cox.net  Sun Apr 23 01:02:04 2006
From: gergg at cox.net (Gregg Wonderly)
Date: Sun Apr 23 01:02:14 2006
Subject: [concurrency-interest] ScheduledThreadPoolExecutor.remove()
Message-ID: <444B0A4C.4060103@cox.net>

I'm confused on how to use remove(Runnable).  The code says that the argument 
must be a ScheduledFutureTask before it will be removed.  However, Runnable is 
not returned by any of the schedule() methods.  It would be nice to not have to 
cast to do the remove((Runnable)schedFuture), nor cast the return of schedule to 
Runnable.

Also, can ScheduledFutureTask have a toString() that returns toString() on the 
embedded Runnable so that it is easier to track the contents and map that to 
expected contents?

Gregg Wonderly
From joe.bowbeer at gmail.com  Sun Apr 23 03:39:10 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun Apr 23 03:39:14 2006
Subject: [concurrency-interest] ScheduledThreadPoolExecutor.remove()
In-Reply-To: <444B0A4C.4060103@cox.net>
References: <444B0A4C.4060103@cox.net>
Message-ID: <31f2a7bd0604230039o3326ae59o43e147658bbf5d2f@mail.gmail.com>

On 4/22/06, Gregg Wonderly <gergg@cox.net> wrote:
> I'm confused on how to use remove(Runnable). [...]

I understand your predicament.

A few details, in case they might lead to a workaround:

remove(Runnable) is a low level operation on the Runnable queue
inherited from ThreadPoolExecutor.

I'm curious to know why cancelling the scheduledFuture won't suffice
in your case.  To prevent cancelled tasks from building up in the
Runnable queue, you can call purge() periodically.


On 4/22/06, Gregg Wonderly <gergg@cox.net> wrote:
> I'm confused on how to use remove(Runnable).  The code says that the argument
> must be a ScheduledFutureTask before it will be removed.  However, Runnable is
> not returned by any of the schedule() methods.  It would be nice to not have to
> cast to do the remove((Runnable)schedFuture), nor cast the return of schedule to
> Runnable.
>
> Also, can ScheduledFutureTask have a toString() that returns toString() on the
> embedded Runnable so that it is easier to track the contents and map that to
> expected contents?
>
> Gregg Wonderly
>

From holger at wizards.de  Sun Apr 23 08:01:33 2006
From: holger at wizards.de (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Sun Apr 23 08:01:51 2006
Subject: [concurrency-interest] ScheduledThreadPoolExecutor.remove()
In-Reply-To: <444B0A4C.4060103@cox.net>
References: <444B0A4C.4060103@cox.net>
Message-ID: <444B6C9D.3060808@wizards.de>

Gregg Wonderly wrote:
> I'm confused on how to use remove(Runnable).  The code says that the
> argument must be a ScheduledFutureTask before it will be removed. 

Things are worse than you might suspect because even if you succeed (for
all the wrong reasons) you'll reap a ClassCastException. You can take a
loot at the archives for a thread in last December called "Confusing API:
ScheduledThreadPoolExecutor vs. remove()" which will explain all the
issues; in short: the docs are a bit misleading and there's a hidden JDK
bug that was fixed in Mustang b71.
If you really need a custom ScheduledFutureExecutor I can send you a test
class that exposes the bug and demonstrates how to get this right.

Holger

From gregg at cytetech.com  Sun Apr 23 08:18:47 2006
From: gregg at cytetech.com (Gregg Wonderly)
Date: Sun Apr 23 08:18:51 2006
Subject: [concurrency-interest] ScheduledThreadPoolExecutor.remove()
In-Reply-To: <31f2a7bd0604230039o3326ae59o43e147658bbf5d2f@mail.gmail.com>
References: <444B0A4C.4060103@cox.net>
	<31f2a7bd0604230039o3326ae59o43e147658bbf5d2f@mail.gmail.com>
Message-ID: <444B70A7.2070601@cytetech.com>



Joe Bowbeer wrote:
> I'm curious to know why cancelling the scheduledFuture won't suffice
> in your case.  To prevent cancelled tasks from building up in the
> Runnable queue, you can call purge() periodically.

In my application, I am cancelling 100% of these timers in the normal case.  I 
just want the task out of the queue with an easy and guaranteed removal.  There 
are perhaps 70 of these created per second in this application.

Gregg Wonderly
From dl at cs.oswego.edu  Sun Apr 23 08:27:41 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun Apr 23 08:27:45 2006
Subject: [concurrency-interest] ScheduledThreadPoolExecutor.remove()
In-Reply-To: <444B70A7.2070601@cytetech.com>
References: <444B0A4C.4060103@cox.net>	<31f2a7bd0604230039o3326ae59o43e147658bbf5d2f@mail.gmail.com>
	<444B70A7.2070601@cytetech.com>
Message-ID: <444B72BD.60902@cs.oswego.edu>

Gregg Wonderly wrote:
> 
> 
> Joe Bowbeer wrote:
>> I'm curious to know why cancelling the scheduledFuture won't suffice
>> in your case.  To prevent cancelled tasks from building up in the
>> Runnable queue, you can call purge() periodically.
> 
> In my application, I am cancelling 100% of these timers in the normal 
> case.  I just want the task out of the queue with an easy and guaranteed 
> removal.  There are perhaps 70 of these created per second in this 
> application.
> 

Joe's advice is still best. The worker threads will often be
contending with and racing with purge() to extract tasks (in which case the
worker threads generally win, in which case, the cancelled tasks won't run
anyway.) You want to balance contention with resource management, so
in your case, calling purge every minute or so sounds like a reasonable
way to go about it.

Someday, we'll have to consider classes that are designed for use
in network protocols and the like in which many timeout tasks are
scheduled, but very few ever run. ScheduledThreadPoolExecutor isn't
really optimized for this case.

-Doug

From joe.bowbeer at gmail.com  Sun Apr 23 08:44:39 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun Apr 23 08:44:43 2006
Subject: [concurrency-interest] ScheduledThreadPoolExecutor.remove()
In-Reply-To: <444B70A7.2070601@cytetech.com>
References: <444B0A4C.4060103@cox.net>
	<31f2a7bd0604230039o3326ae59o43e147658bbf5d2f@mail.gmail.com>
	<444B70A7.2070601@cytetech.com>
Message-ID: <31f2a7bd0604230544j9dcf3e8y2d7276895e43fcf6@mail.gmail.com>

70 per second doesn't sound excessive.  I expect simple cancellation
will suffice without causing a jam.

If garbage pile-up is a problem, I'm thinking you might be able to
handle this effectively in the rejected execution handler: something
similar to discard-oldest, but instead of discarding a live task,
purge the queue of any lingering cancelled tasks at this time.

Purging a bunch of tasks from the queue once in a while should be more
efficient than removing each one individually.


On 4/23/06, Gregg Wonderly <gregg@cytetech.com> wrote:
>
>
> Joe Bowbeer wrote:
> > I'm curious to know why cancelling the scheduledFuture won't suffice
> > in your case.  To prevent cancelled tasks from building up in the
> > Runnable queue, you can call purge() periodically.
>
> In my application, I am cancelling 100% of these timers in the normal case.  I
> just want the task out of the queue with an easy and guaranteed removal.  There
> are perhaps 70 of these created per second in this application.
>
> Gregg Wonderly
>

From tackline at tackline.plus.com  Sun Apr 23 10:21:59 2006
From: tackline at tackline.plus.com (Thomas Hawtin)
Date: Sun Apr 23 11:16:28 2006
Subject: [concurrency-interest] Navigable Set and Map improvements
In-Reply-To: <444AB332.4020208@cs.oswego.edu>
References: <4446628F.4040108@cs.oswego.edu>
	<444A884F.9060000@tackline.plus.com>
	<444AB332.4020208@cs.oswego.edu>
Message-ID: <444B8D87.8040007@tackline.plus.com>

Doug Lea wrote:
> 
> We did discuss this. We think the ability to use boolean
> expressions here is useful, rather than having to write, for example,
> complement as:
>  (d == NavigableMap.INCLUSIVE)? NavigableMap.EXCLUSIVE : 
> NavigableMap.INCLUSIVE
> instead of just
>   !d

     d.invert()

I guess naming of the binary operations is tricky.

Tom Hawtin
From dawidk at mathcs.emory.edu  Sun Apr 23 12:53:30 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Sun Apr 23 12:53:51 2006
Subject: [concurrency-interest] ScheduledThreadPoolExecutor.remove()
In-Reply-To: <444B72BD.60902@cs.oswego.edu>
References: <444B0A4C.4060103@cox.net>	<31f2a7bd0604230039o3326ae59o43e14765
	8bbf5d2f@mail.gmail.com><444B70A7.2070601@cytetech.com> 
	<444B72BD.60902@cs.oswego.edu>
Message-ID: <444BB10A.1090205@mathcs.emory.edu>

Doug Lea wrote:
> Gregg Wonderly wrote:
>>
>>
>> Joe Bowbeer wrote:
>>> I'm curious to know why cancelling the scheduledFuture won't suffice
>>> in your case.  To prevent cancelled tasks from building up in the
>>> Runnable queue, you can call purge() periodically.
>>
>> In my application, I am cancelling 100% of these timers in the normal 
>> case.  I just want the task out of the queue with an easy and 
>> guaranteed removal.  There are perhaps 70 of these created per second 
>> in this application.
>>

> You want to balance contention with resource management, so
> in your case, calling purge every minute or so sounds like a reasonable
> way to go about it.
>
> Someday, we'll have to consider classes that are designed for use
> in network protocols and the like in which many timeout tasks are
> scheduled, but very few ever run. ScheduledThreadPoolExecutor isn't
> really optimized for this case.

I had a very similar problem to Gregg. The solution that worked out for 
me was - as you say - to cancel and then to purge periodically, where 
"periodically" is not at a fixed time interval though, but based on a 
simple heuristics. After a task is canceled, the "cancellation count" is 
incremented. Then, purge() is called if (1) the cancellation count 
exceeds a threshold (e.g. 10000) or if the ratio of the cancellation 
count to the queue length exceeds a threshold (e.g. 0.75). This seems to 
work reasonably, since it assures that the linear purge is called 
sparingly if queues are long and cancellations rare, but at the same 
time, if 100% or close of all tasks are being canceled, they are purged 
very quickly. I wonder if anybody has any comments on this approach?

Regards,
Dawid

From gregg at cytetech.com  Sun Apr 23 16:19:53 2006
From: gregg at cytetech.com (Gregg Wonderly)
Date: Sun Apr 23 16:20:20 2006
Subject: [concurrency-interest] ScheduledThreadPoolExecutor.remove()
In-Reply-To: <31f2a7bd0604230544j9dcf3e8y2d7276895e43fcf6@mail.gmail.com>
References: <444B0A4C.4060103@cox.net>	<31f2a7bd0604230039o3326ae59o43e147658bbf5d2f@mail.gmail.com>	<444B70A7.2070601@cytetech.com>
	<31f2a7bd0604230544j9dcf3e8y2d7276895e43fcf6@mail.gmail.com>
Message-ID: <444BE169.8080900@cytetech.com>



Joe Bowbeer wrote:
> 70 per second doesn't sound excessive.  I expect simple cancellation
> will suffice without causing a jam.

What it comes down to is proof of correctness for my application.  It has to 
work, no matter what code paths are involved.  Adding a scheduled thread to 
periodically do a purge might seems sensible at first glance.  However, for me, 
guessing a time and having the system be dependent on the load and that timing 
is perilous.  Involving a third party for purging just doesn't excite me.  I was 
doing this before with java.util.Timer (once we had purge).

The queue and ScheduledThreadPoolExecutor is already a multi-threaded, contended 
resource, so I'm a little surprised at the adversion to remove().  I suppose 
that a lock to remove 20 entries is cheaper (cache issues and locking) than 20 
locks.

> If garbage pile-up is a problem, I'm thinking you might be able to
> handle this effectively in the rejected execution handler: something
> similar to discard-oldest, but instead of discarding a live task,
> purge the queue of any lingering cancelled tasks at this time.
> 
> Purging a bunch of tasks from the queue once in a while should be more
> efficient than removing each one individually.

Logically, this all make sense, practically it complicates the application 
tremendously and makes it necessary for different parts of the code to know 
about what the others are doing.  I don't need a limit on the queue.  I just 
need to be able to schedule these timeout events, and cancel them when the work 
completes normally.

For now, I have

ScheduledThreadPoolExecutor schedExec;
ScheduledFuture toCancel;

public void setTimeout( long timeout ) {
	synchronized( schedExec ) {
		toCancel = schedExec.schedule(
			this, timeout, TimeUnits.SECONDS );
	}
}

and

volatile boolean didRun;
public void cancel() {
	boolean how = false;

	synchronized( schedExec ) {
		how = toCancel.cancel();
		schedExec.remove( (Runnable)toCancel );
	}

	if( !how && !didRun ) {
		throw new IllegalStateException(
			"Did not run nor cancel: "+this );
	}
}

public void run() {
	didRun = true;
	... code to handle timeout ...
}

Gregg Wonderly
From marko.asplund at kronodoc.fi  Mon Apr 24 06:48:11 2006
From: marko.asplund at kronodoc.fi (Marko Asplund)
Date: Mon Apr 24 06:48:20 2006
Subject: [concurrency-interest] using the jsr166 package with Java SE 5?
Message-ID: <BF546A30-A8AC-4BD8-8B67-7C2084CF6EEF@kronodoc.fi>

hi

i'd like to use some of the new collection classes such as  
ConcurrentSkipListMap in the jsr166 package with Java SE 5. is it  
possible to use the package with Java SE 5?

the jsr166.jar doesn't work with Java SE 5 because apparently it's  
been compiled with Mustang.

the backport-util-concurrent package doesn't seem to work with the  
new language features in Java SE 5.


br.
--
	marko
From dl at cs.oswego.edu  Mon Apr 24 07:25:45 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon Apr 24 07:25:50 2006
Subject: [concurrency-interest] using the jsr166 package with Java SE
 5?
In-Reply-To: <BF546A30-A8AC-4BD8-8B67-7C2084CF6EEF@kronodoc.fi>
References: <BF546A30-A8AC-4BD8-8B67-7C2084CF6EEF@kronodoc.fi>
Message-ID: <444CB5B9.4020404@cs.oswego.edu>

Marko Asplund wrote:
> hi
> 
> i'd like to use some of the new collection classes such as 
> ConcurrentSkipListMap in the jsr166 package with Java SE 5. is it 
> possible to use the package with Java SE 5?
> 
> the jsr166.jar doesn't work with Java SE 5 because apparently it's been 
> compiled with Mustang.
> 

Ordinarily, I'd point you to the jsr166x.jar (not jsr166.jar) at
http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
But at the moment, the jsr166x versions of Navigable{Map,Set}
aren't in sync with recent changes in Mustang versions. If
anyone would like to volunteer to do so, please feel free!
Otherwise it might be a while.

-Doug
From jason_mehrens at hotmail.com  Tue Apr 25 12:06:27 2006
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Tue Apr 25 12:06:39 2006
Subject: [concurrency-interest] ConcurrentNavigableMap additional methods:
	6415641
Message-ID: <BAY105-F15CF5CF695BB4755B2B3AD83BF0@phx.gbl>

Since the ConcurrentNavigableMap has methods to poll entries from the map, 
it might be worth considering adding the related methods (modified from the 
BlockingQueue):
int drainTo(Map<? super K, ? super V> m)
int drainTo(Map<? super K, ? super V> m, int maxEntries)

The main reason I can think of for adding these methods is from the 
BlockingQueue documentation which states: "This operation may be more 
efficient than repeatedly polling...".  It would also allow some maps to 
make this operation of polling all entries atomic.

Regards,

Jason Mehrens


From tim at peierls.net  Tue Apr 25 15:51:21 2006
From: tim at peierls.net (Tim Peierls)
Date: Tue Apr 25 15:51:24 2006
Subject: [concurrency-interest] ConcurrentNavigableMap additional methods:
	6415641
In-Reply-To: <BAY105-F15CF5CF695BB4755B2B3AD83BF0@phx.gbl>
References: <BAY105-F15CF5CF695BB4755B2B3AD83BF0@phx.gbl>
Message-ID: <63b4e4050604251251n50a5aafan68647d6fd1bc2202@mail.gmail.com>

On 4/25/06, Jason Mehrens <jason_mehrens@hotmail.com> wrote:
>
> Since the ConcurrentNavigableMap has methods to poll entries from the map,
> it might be worth considering adding the related methods (modified from the
> BlockingQueue):
> int drainTo(Map<? super K, ? super V> m)
> int drainTo(Map<? super K, ? super V> m, int maxEntries)
>
> The main reason I can think of for adding these methods is from the
> BlockingQueue documentation which states: "This operation may be more
> efficient than repeatedly polling...".


The BlockingQueue spec says that Collection methods need not be implemented
efficiently, leaving drainTo as the only available efficient bulk removal
operation for blocking queues. But the same is not true of other
subinterfaces of Collection; in particular, it is reasonable to expect
efficient iteration over the entries of a ConcurrentNavigableMap.


It would also allow some maps to make this operation of polling all entries
> atomic.


Not ConcurrentHashMap or ConcurrentSkipListMap, for which only weak
consistency of iterators is guaranteed. If you want strongly consistent
snapshots, you need something like Collections.synchronizedMap.

I can imagine an implementation of ConcurrentNavigableMap that does provide
strong consistency for its iterators, but in that case you don't need an
additional method in the interface -- i.e., the following would do the right
thing:

ConcurrentNavigableMap<X, Y> scm = new StronglyConsistentConcurrentMap<X,
Y>();
...
// relies on strongly consistent iteration over scm:
Map<X, Y> snapshot = new HashMap<X, Y>(scm);

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060425/14f17006/attachment.html
From jason_mehrens at hotmail.com  Wed Apr 26 10:33:27 2006
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Wed Apr 26 10:33:39 2006
Subject: [concurrency-interest] ConcurrentNavigableMap additional methods:
	6415641
Message-ID: <BAY105-F1080DDCFE91FCC2987887083BC0@phx.gbl>

Tim,

Thanks for the comments.  I wanted to clarify that strong/weak Iteration is 
not what I was focusing on (maybe I should have been).

The code bellow is an example of how to drain a NavigableMap to another Map 
(clearly not atomic):
  public static <K,V> int drainTo(ConcurrentNavigableMap<K,V> m, Map<? super 
K, ? super V> fill) {
    int count = 0;
    Map.Entry<K,V> e = m.pollFirstEntry();
    while(e != null) {
      fill.put(e.getKey(), e.getValue());
      count++;
      e = m.pollFirstEntry();
    }
    return count;
  }

Take the ConcurrentSkipListMap (the ConcurrentHashMap does not apply because 
it is not a NavigableMap), there is one volatile reference to the head node. 
  The clear operation simply creates and assigns a new head node to clear 
the map.  So if there was a drainTo method on the ConcurrentSkipListMap, it 
could simply CAS the head reference (an atomic operation) and traverse the 
snapshot without the cost of unlinking nodes.  I haven't tested the idea so 
I could be totally wrong.
The presence of a drainTo method could be unnecessary, I just wanted to be 
sure that the idea is out there for review.

Regards,

Jason Mehrens

>From: "Tim Peierls" <tim@peierls.net>
>To: "Jason Mehrens" <jason_mehrens@hotmail.com>
>CC: concurrency-interest@cs.oswego.edu
>Subject: Re: [concurrency-interest] ConcurrentNavigableMap additional 
>methods: 6415641
>Date: Tue, 25 Apr 2006 15:51:21 -0400
>
>On 4/25/06, Jason Mehrens <jason_mehrens@hotmail.com> wrote:
> >
> > Since the ConcurrentNavigableMap has methods to poll entries from the 
>map,
> > it might be worth considering adding the related methods (modified from 
>the
> > BlockingQueue):
> > int drainTo(Map<? super K, ? super V> m)
> > int drainTo(Map<? super K, ? super V> m, int maxEntries)
> >
> > The main reason I can think of for adding these methods is from the
> > BlockingQueue documentation which states: "This operation may be more
> > efficient than repeatedly polling...".
>
>
>The BlockingQueue spec says that Collection methods need not be implemented
>efficiently, leaving drainTo as the only available efficient bulk removal
>operation for blocking queues. But the same is not true of other
>subinterfaces of Collection; in particular, it is reasonable to expect
>efficient iteration over the entries of a ConcurrentNavigableMap.
>
>
>It would also allow some maps to make this operation of polling all entries
> > atomic.
>
>
>Not ConcurrentHashMap or ConcurrentSkipListMap, for which only weak
>consistency of iterators is guaranteed. If you want strongly consistent
>snapshots, you need something like Collections.synchronizedMap.
>
>I can imagine an implementation of ConcurrentNavigableMap that does provide
>strong consistency for its iterators, but in that case you don't need an
>additional method in the interface -- i.e., the following would do the 
>right
>thing:
>
>ConcurrentNavigableMap<X, Y> scm = new StronglyConsistentConcurrentMap<X,
>Y>();
>...
>// relies on strongly consistent iteration over scm:
>Map<X, Y> snapshot = new HashMap<X, Y>(scm);
>
>--tim


From hlship at gmail.com  Wed Apr 26 14:04:58 2006
From: hlship at gmail.com (Howard Lewis Ship)
Date: Wed Apr 26 14:05:06 2006
Subject: [concurrency-interest] Lock.isLockedByCurrentThread()
Message-ID: <ecd0e3310604261104xfd72e3fq964aeac7ccd2ce47@mail.gmail.com>

I've been doing some work using JDK 1.5 concurrency, as well as AspectJ.

My goal is to handle synchronization via annotations with AspectJ, i.e.

private int _counter;

@Synchronized.Read
public int getCounter() { return _counter; }

@Synchronized.Write
public void incrementCounter() { _counter++; }


This appears to be working quite well, but I had some issues
implementing it.  At the core of this is a ReentrantReadWriteLock.

First, if an @Read method invokes an @Write method (even indirectly),
the aspect takes care of temporarily releasing the read lock before
acquiring the write lock.

Secondly, if a @Read method invokes another @Read method (even
indirectly) I don't want
to re-lock the ReadWriteLock's read lock. Without this code, a thread
that goes through multiple @Read methods before hitting a @Write
method will hang, because only a single read lock will be released,
rather than the set of them (based on the number of chained method
calls).

To handle this case I had to resort to a ThreadLocal companion to the
ReentranReadWriteLock, to track whether the read lock was, in fact,
locked.  @Read methods only lock the read lock if the flag is clear.
@Write methods know to release the read lock before lock the write
lock.

This would be greatly simplified if Lock included a
isLockedByCurrentThread() method.

Is this something that could be added to API?

For your interest, I've included the aspect source below (ASL 2.0 license):

package org.apache.tapestry.internal.aspects;

import java.util.concurrent.locks.ReadWriteLock;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import org.apache.tapestry.internal.annotations.Synchronized;
import org.aspectj.lang.JoinPoint;

/**
 * Manages multi-threaded access to methods of an object instance using a
 * {@link java.util.concurrent.locks.ReentrantReadWriteLock}, driven by the
 * {@link Synchronized.Read} and {@link Synchronized.Write}
annotations. Methods that have the
 * Synchronized.Read annotation witll be advised to obtain and release
the read lock around their
 * execution. Methods with the Synchronized.Write annotation will
obtain and release the write lock
 * around their execution. Methods annotated with Synchronized.Read
that call a method annotated
 * with Synchronized.Write (within the same instance) will release the
read lock before invoking the
 * method.
 * <p>
 * This implementation makes use of a ThreadLocal to determine if the
current thread has acquired
 * the read lock (or not). This prevents the read lock from being
re-locked, and allows the aspect
 * to release the read lock temporarily when acquiring the write lock.
 * <p>
 * This aspect also enforces that the annotations are only applied to
instance (not static) methods,
 * and that a method may be either read or write, but not both.
 *
 * @author Howard M. Lewis Ship
 * @see org.apache.tapestry.internal.annotations.Synchronized
 */
public abstract aspect Synchronization extends AbstractClassTargetting
perthis(annotatedClasses())
{
    private final ReadWriteLock _lock = new ReentrantReadWriteLock();

    private final ThreadLocal<Boolean> _threadHasReadLock = new
ThreadLocal<Boolean>()
    {
        @Override
        protected Boolean initialValue()
        {
            return false;
        }
    };

    private void log(JoinPoint jp, String message)
    {
        if (false)
            System.out.println(String.format("%s\n    %s:\n    %s\n",
jp, _lock, message));
    }

    pointcut annotatedClasses() :
         targetClasses() && within(@Synchronized *);

    declare error :
        targetClasses() &&
        execution(@(Synchronized.Read || Synchronized.Write) static * *(..)) :
            "Synchronized.Read and Synchronized.Write annotations may
only be applied to instance methods.";

    declare error :
        targetClasses() &&
        execution(@(Synchronized.Read || Synchronized.Write) * *(..)) &&
        within(!@(Synchronized) Object+) :
            "The class must be annotated with Synchronized in order to
use the Synchronized.Read or Synchronized.Write annotations.";

    declare error :
        targetClasses() &&
        execution(@Synchronized.Read @Synchronized.Write * *(..)) :
            "A method may be annotated with Synchronized.Read or with
Synchronized.Write but not both.";

    pointcut writeMethods() :
            execution(@Synchronized.Write * *(..));

    pointcut readMethods() :
        execution(@Synchronized.Read * *(..));

    Object around() : readMethods()
    {
        boolean locked = _threadHasReadLock.get();

        if (!locked)
        {
            log(thisJoinPoint, "acquiring read lock");

            _lock.readLock().lock();

            _threadHasReadLock.set(true);
        }

        try
        {
            return proceed();
        }
        finally
        {
            if (!locked)
            {
                log(thisJoinPoint, "releasing read lock");

                _lock.readLock().unlock();

                _threadHasReadLock.set(false);
            }
        }
    }

    Object around() : writeMethods()
    {
        boolean locked = _threadHasReadLock.get();

        if (locked)
        {
            log(thisJoinPoint, "releasing read lock (for upgrade)");

            _lock.readLock().unlock();

            _threadHasReadLock.set(false);
        }

        log(thisJoinPoint, "acquiring write lock");

        _lock.writeLock().lock();

        try
        {
            return proceed();
        }
        finally
        {
            log(thisJoinPoint, "releasing write lock");
            _lock.writeLock().unlock();

            if (locked)
            {
                log(thisJoinPoint, "acquiring read lock (for downgrade)");

                _lock.readLock().lock();

                _threadHasReadLock.set(true);
            }
        }
    }
}


--
Howard M. Lewis Ship
Independent J2EE / Open-Source Java Consultant
Creator, Apache Tapestry
Creator, Jakarta HiveMind

Professional Tapestry training, mentoring, support
and project work.  http://howardlewisship.com

From dl at cs.oswego.edu  Wed Apr 26 14:25:58 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed Apr 26 14:26:02 2006
Subject: [concurrency-interest] Lock.isLockedByCurrentThread()
In-Reply-To: <ecd0e3310604261104xfd72e3fq964aeac7ccd2ce47@mail.gmail.com>
References: <ecd0e3310604261104xfd72e3fq964aeac7ccd2ce47@mail.gmail.com>
Message-ID: <444FBB36.8090405@cs.oswego.edu>

Howard Lewis Ship wrote:

> To handle this case I had to resort to a ThreadLocal companion to the
> ReentranReadWriteLock, to track whether the read lock was, in fact,
> locked.  @Read methods only lock the read lock if the flag is clear.
> @Write methods know to release the read lock before lock the write
> lock.

For Mustang, we added new query methods that should be of use here.
See for example ReentrantReadWriteLock.getReadHoldCount. These
have been integrated into Mustang builds for a while now; see our
base source javadoc at: 
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/locks/ReentrantReadWriteLock.html


-Doug
From tim at peierls.net  Wed Apr 26 16:12:09 2006
From: tim at peierls.net (Tim Peierls)
Date: Wed Apr 26 16:12:17 2006
Subject: [concurrency-interest] ConcurrentNavigableMap additional methods:
	6415641
In-Reply-To: <BAY105-F1080DDCFE91FCC2987887083BC0@phx.gbl>
References: <BAY105-F1080DDCFE91FCC2987887083BC0@phx.gbl>
Message-ID: <63b4e4050604261312y3380e056je9fff49058802a7@mail.gmail.com>

Do you have any use cases for an atomic "drainTo" operation on
ConcurrentNavigableMap? And why do you restrict it to CNM?

If you aren't too worried about consistency, you could create a wrapper
class that delegates to an underlying CNM via an AtomicReference, and
implements drainTo by cas'ing a new empty delegate and filling the result
map from the original delegate.
--tim

On 4/26/06, Jason Mehrens <jason_mehrens@hotmail.com> wrote:
>
> Tim,
>
> Thanks for the comments.  I wanted to clarify that strong/weak Iteration
> is
> not what I was focusing on (maybe I should have been).
>
> The code bellow is an example of how to drain a NavigableMap to another
> Map
> (clearly not atomic):
>   public static <K,V> int drainTo(ConcurrentNavigableMap<K,V> m, Map<?
> super
> K, ? super V> fill) {
>     int count = 0;
>     Map.Entry<K,V> e = m.pollFirstEntry();
>     while(e != null) {
>       fill.put(e.getKey(), e.getValue());
>       count++;
>       e = m.pollFirstEntry();
>     }
>     return count;
>   }
>
> Take the ConcurrentSkipListMap (the ConcurrentHashMap does not apply
> because
> it is not a NavigableMap), there is one volatile reference to the head
> node.
>   The clear operation simply creates and assigns a new head node to clear
> the map.  So if there was a drainTo method on the ConcurrentSkipListMap,
> it
> could simply CAS the head reference (an atomic operation) and traverse the
> snapshot without the cost of unlinking nodes.  I haven't tested the idea
> so
> I could be totally wrong.
> The presence of a drainTo method could be unnecessary, I just wanted to be
>
> sure that the idea is out there for review.
>
> Regards,
>
> Jason Mehrens
>
> >From: "Tim Peierls" <tim@peierls.net>
> >To: "Jason Mehrens" < jason_mehrens@hotmail.com>
> >CC: concurrency-interest@cs.oswego.edu
> >Subject: Re: [concurrency-interest] ConcurrentNavigableMap additional
> >methods: 6415641
> >Date: Tue, 25 Apr 2006 15:51:21 -0400
> >
> >On 4/25/06, Jason Mehrens <jason_mehrens@hotmail.com> wrote:
> > >
> > > Since the ConcurrentNavigableMap has methods to poll entries from the
> >map,
> > > it might be worth considering adding the related methods (modified
> from
> >the
> > > BlockingQueue):
> > > int drainTo(Map<? super K, ? super V> m)
> > > int drainTo(Map<? super K, ? super V> m, int maxEntries)
> > >
> > > The main reason I can think of for adding these methods is from the
> > > BlockingQueue documentation which states: "This operation may be more
> > > efficient than repeatedly polling...".
> >
> >
> >The BlockingQueue spec says that Collection methods need not be
> implemented
> >efficiently, leaving drainTo as the only available efficient bulk removal
> >operation for blocking queues. But the same is not true of other
> >subinterfaces of Collection; in particular, it is reasonable to expect
> >efficient iteration over the entries of a ConcurrentNavigableMap.
> >
> >
> >It would also allow some maps to make this operation of polling all
> entries
> > > atomic.
> >
> >
> >Not ConcurrentHashMap or ConcurrentSkipListMap, for which only weak
> >consistency of iterators is guaranteed. If you want strongly consistent
> >snapshots, you need something like Collections.synchronizedMap.
> >
> >I can imagine an implementation of ConcurrentNavigableMap that does
> provide
> >strong consistency for its iterators, but in that case you don't need an
> >additional method in the interface -- i.e., the following would do the
> >right
> >thing:
> >
> >ConcurrentNavigableMap<X, Y> scm = new StronglyConsistentConcurrentMap<X,
> >Y>();
> >...
> >// relies on strongly consistent iteration over scm:
> >Map<X, Y> snapshot = new HashMap<X, Y>(scm);
> >
> >--tim
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060426/196a3d92/attachment.html
From jason_mehrens at hotmail.com  Thu Apr 27 13:53:33 2006
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Thu Apr 27 13:53:46 2006
Subject: [concurrency-interest] ConcurrentNavigableMap additional methods:
	6415641
Message-ID: <BAY105-F13A4F1376C34D29C74DD9F83BD0@phx.gbl>

The only use case I can think of would be clearing a global cache and using 
the resulting Map to log the entries ejected from the cache at that point in 
time.  Given the nature of the ConcurrentMaps, there is probably no real 
difference between repeated polling and drainTo.  However, I would think the 
same would hold true of BlockingQueues.  As you stated before, "..drainTo as 
the only available efficient bulk removal operation for blocking queues."  
But, I would think clear() is the other efficient bulk removal operation 
(even though it is specified by Collection).  You just can't get any 
additional information from clear().  Which is why it "might" be useful to 
have a drainTo specified in the interface.
Third party classes like a "copy on write binary tree" or a "NavigableMap to 
ConcurrentNavigableMap wrapper" could have fast clear and drainTo methods 
but slower polling methods.  Overall those types are poor performers but you 
get the idea.

I would restrict it to the CNM because it is similar to the BlockingDeque.  
The NavigableMap and Deque specify methods to poll from each end but no 
drainTo method.  The thread-safe BlockingDeque adds the drainTo (from 
BlockingQueue) but the thread-safe ConcurrentNavigableMap doesn't.  While 
Collections and Maps are different, the lack of a drainTo for a CNM seems 
unbalanced to me.  But a small clear API is important too.

cheers,

Jason Mehrens

>From: "Tim Peierls" <tim@peierls.net>
>To: "Jason Mehrens" <jason_mehrens@hotmail.com>
>CC: concurrency-interest@cs.oswego.edu
>Subject: Re: [concurrency-interest] ConcurrentNavigableMap additional 
>methods: 6415641
>Date: Wed, 26 Apr 2006 16:12:09 -0400
>
>Do you have any use cases for an atomic "drainTo" operation on
>ConcurrentNavigableMap? And why do you restrict it to CNM?
>
>If you aren't too worried about consistency, you could create a wrapper
>class that delegates to an underlying CNM via an AtomicReference, and
>implements drainTo by cas'ing a new empty delegate and filling the result
>map from the original delegate.
>--tim


From satnams at microsoft.com  Fri Apr 28 12:54:34 2006
From: satnams at microsoft.com (Satnam Singh)
Date: Fri Apr 28 12:55:15 2006
Subject: [concurrency-interest] ArrayBlockingQueue: concurrent put and take
Message-ID: <4371C55AE7D7624EBB6D0C5A4DB585A40A392576@RED-MSG-50.redmond.corp.microsoft.com>

Is it possible to implement ArrayBlockingQueue to support concurrent
calls to put and take?

One could use a slightly different representation to help partition the
data structure to facilitate concurrent access but then perhaps this is
not in the spirit of the specification for ArrayBlockingQueue?

 

I've been looking at an STM-based implementation but a straight-forward
approach where transacted variables are used for the head, tail and
empty-status does not seem to allow concurrent calls to put and take.

 

Thank you.

 

Kind regards,


Satnam

 

________________________________

Satnam Singh
Microsoft
One Microsoft Way
Redmond
Washington 98052-6399
USA

http://research.microsoft.com/~satnams 
Email: <mailto:satnams@microsoft.com> satnams@microsoft.com
<mailto:satnams@microsoft.comTelephone> 
Telephone: +1 425 705 8208
Cell: +1 408 718 2588
UK cell: +44 7979 648412
Pager:  4087182588@cingularME.com <mailto:4087182588@cingularME.com>  

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060428/cff5a0df/attachment.html
From brian at quiotix.com  Fri Apr 28 15:08:46 2006
From: brian at quiotix.com (Brian Goetz)
Date: Fri Apr 28 15:08:59 2006
Subject: [concurrency-interest] ArrayBlockingQueue: concurrent put and take
In-Reply-To: <4371C55AE7D7624EBB6D0C5A4DB585A40A392576@RED-MSG-50.redmond.corp.microsoft.com>
References: <4371C55AE7D7624EBB6D0C5A4DB585A40A392576@RED-MSG-50.redmond.corp.microsoft.com>
Message-ID: <4452683E.406@quiotix.com>

> Is it possible to implement ArrayBlockingQueue to support concurrent 
> calls to put and take?

No, but LinkedBlockingQueue does.  It uses a two-lock algorithm to 
support concurrent puts and takes.  LBQ offers greater concurrency, but 
more memory churn.  That's why there are two versions.

> One could use a slightly different representation to help partition the 
> data structure to facilitate concurrent access but then perhaps this is 
> not in the spirit of the specification for ArrayBlockingQueue?

Right.  You're taking an incremental step towards LinkedBlockingQueue. 
In most cases, allocation is dirt cheap -- certainly cheaper than 
contention -- so in most cases LBQ is preferable.  In RT environments, 
where memory is more constrained and GC pauses are less acceptable, ABQ 
may be more appropriate.

From dl at cs.oswego.edu  Fri Apr 28 19:57:02 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri Apr 28 19:57:05 2006
Subject: [concurrency-interest] Concurrency Utilities for Java EE preview
Message-ID: <4452ABCE.5030705@cs.oswego.edu>


Some members of this list will recall that over two years ago,
there were JSRs submitted for standardized versions of IBM
and BEA's commonj work managers, followed by discussions that
these should be integrated with the java.util.concurrent
Executor framework. This effort hit lots of snags (too weird
to get into) along the way. But we (mainly due to the hard
work of spec leads Chris Johnson of IBM and Revanuru Naresh
of BEA) finally have a preliminary proposal together.
You can find these at the new, and not yet very populated, site:
http://gee.cs.oswego.edu/dl/concurrencyee-interest/
Along with a new mailing list that you can use to send comments
(please use concurrencyee-interest, not concurrency-interest
for feedback, so as not to confuse us too much :-) While this
is not yet an official JSR draft, we wanted to get it out so people
can read it before JavaOne, where there will be a BoF (0980
thursday 9:30) presenting and discussing it.

A few quick notes about it to bridge the SE and EE crowds.

1. Even in EE programs, you can use anything in java.util.concurrent.
However, you cannot usually create your own threads, even implicitly
(as in ThreadPoolExecutors). So the proposed javax.util.concurrent
facilities mainly provide forms of Executors that you CAN create,
and that maintain EE context, etc. And that are configurable in ways
that make sense in EE applications. Plus some associated utilties
to manage context and the like. All of which you can then use with the
regular java.util.concurrent utilities.

2. There is not yet an available implementation. Those of you who
work on EE systems, especially open source ones (JBoss, Geronimo, Jonas,
Glassfish, ..) , are kindly invited to build them! We'd be happy to help
provide guidance in any way we can

-Doug






From the.mindstorm.mailinglist at gmail.com  Sun Apr 30 09:37:38 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Sun Apr 30 09:37:41 2006
Subject: [concurrency-interest] implementing a DB write-behind algorithm
Message-ID: <c6f400460604300637s11032d5ah8583355f8cbc7990@mail.gmail.com>

Hi!

I firstly have to confess that when getting to concurrency related
problems, I am getting confused quite quickly :-).

Now, the current problem I am trying to solve is: I am trying to
figure out how to implement a DB write-behind strategy. Multiple
processes will post records to be written to the DB, but the actual
writes should happen on a separate process. So, far I was thinking
about 2 possible approaches:
a) continous write-behind: multiple processes write to a queue which
is continously polled by a separate process. When an element is found
on the queue, than the write process removes it from queue and
attempts to write it to the DB.

To have this done, I was looking in the direction of ConcurrentLinkedQueue.

b) batched write-behind: multiple processes post to a size-bounded
queue. When the max size is reached, the original queue is passed to
the parallel write process and replaced with a new queue.

To have this done, I was looking in the direction of
LinkedBlockingQueue with an additional atomic operation of swapping
the old queue with the new empty one.

My question is: am I looking in the right direction or I am completely
wrong. Any ideas and help are highly appreciated.

./alex
--
.w( the_mindstorm )p.

From tim at peierls.net  Sun Apr 30 14:36:16 2006
From: tim at peierls.net (Tim Peierls)
Date: Sun Apr 30 14:36:29 2006
Subject: [concurrency-interest] implementing a DB write-behind algorithm
In-Reply-To: <c6f400460604300637s11032d5ah8583355f8cbc7990@mail.gmail.com>
References: <c6f400460604300637s11032d5ah8583355f8cbc7990@mail.gmail.com>
Message-ID: <63b4e4050604301136hdbe14b5je76bf4ea10f088e9@mail.gmail.com>

On 4/30/06, Alexandru Popescu <the.mindstorm.mailinglist@gmail.com > wrote:
>
> I firstly have to confess that when getting to concurrency related
> problems, I am getting confused quite quickly :-).


You're not alone! :-)


Now, the current problem I am trying to solve is: I am trying to
> figure out how to implement a DB write-behind strategy. Multiple
> processes will post records to be written to the DB, but the actual
> writes should happen on a separate process. So, far I was thinking
> about 2 possible approaches:
> a) continous write-behind: multiple processes write to a queue which
> is continously polled by a separate process. When an element is found
> on the queue, than the write process removes it from queue and
> attempts to write it to the DB.
>
> To have this done, I was looking in the direction of
> ConcurrentLinkedQueue.
>
> b) batched write-behind: multiple processes post to a size-bounded
> queue. When the max size is reached, the original queue is passed to
> the parallel write process and replaced with a new queue.
>
> To have this done, I was looking in the direction of
> LinkedBlockingQueue with an additional atomic operation of swapping
> the old queue with the new empty one.
>
> My question is: am I looking in the right direction or I am completely
> wrong. Any ideas and help are highly appreciated.
>

The use of BlockingQueue.put makes it possible to implement strategies that
make the caller block, while still permitting strategies that don't block.
So I would avoid ConcurrentLinkedQueue here, because it does not implement
the BlockingQueue interface.

You can use an unbounded LinkedBlockingQueue for continuous write-behind,
and ArrayBlockingQueue (always bounded) for batched write-behind. Instead of
swapping in a new queue, the consumer thread could just poll until the batch
size was reached (using a timeout to avoid the risk of batches never
completing), and then send the batch. The batch size need not be the same as
the queue capacity.

Here's an uncompiled, untested fragment that illustrates the idea:

public interface WriteBehind<T> {
    void put(T record) throws InterruptedException;
}

public interface RecordWriter<T> {
    void write(List<T> records) throws InterruptedException;
}

class AbstractWriteBehind<T> implements WriteBehind<T> {
    private final BlockingQueue<T> queue;
    private final RecordWriter<T> writer;
    @GuardedBy("this") private Future<Void> consumer = null;

    protected AbstractWriteBehind(BlockingQueue<T> queue, RecordWriter<T>
writer) {
        this.queue = queue;
        this.writer = writer;
    }

    class Consumer implements Callable<Void> {
        public Void call() throws InterruptedException {
            consume(queue, writer);
            return null;
        }
    }

    public synchronized void start() {
        if (consumer == null) {
            ExecutorService exec = Executors.newSingleThreadExecutor();
            try {
                consumer = exec.submit(new Consumer());
            } finally {
                exec.shutdown();
            }
        }
    }

    public synchronized boolean isRunning() {
        return consumer != null;
    }

    public synchronized void stop() {
        if (consumer != null) {
            consumer.cancel(true);
            consumer = null;
        }
    }

    public final void put(T record) throws InterruptedException {
        queue.put(record);
    }

    protected abstract void consume(BlockingQueue<T> queue, RecordWriter<T>
writer)
        throws InterruptedException;
}

class ContinuousWriteBehind<T> extends AbstractWriteBehind<T> {
    ContinousWriteBehind(RecordWriter<T> writer) {
        super(new LinkedBlockingQueue<T>(), writer);
    }

    protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
            throws InterruptedException {
        for (T rec; (rec = q.take()) != null; )
            writer.write(Collections.singletonList(rec));
    }
}

class BatchedWriteBehind<T> extends AbstractWriteBehind<T> {
    private final int maxBuf;
    private final List<T> buf;
    private final long time;
    private final TimeUnit unit;

    BatchedWriteBehind(RecordWriter<T> writer, int capacity, int maxBuf,
                       long time, TimeUnit unit) {
        super(new ArrayBlockingQueue<T>(capacity), writer);
        this.maxBuf = maxBuf;
        this.buf = new ArrayList<T>(maxBuf);
        this.time = time;
        this.unit = unit;
    }

    protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
            throws InterruptedException {
        for (T rec; (rec = q.take()) != null; ) {
            buf.add(rec);
            while (buf.size() < maxBuf && (rec = q.poll(time, unit)) !=
null)
                buf.add(rec);
            writer.write(buf);
            buf.clear();
        }
    }
}

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060430/909ce171/attachment.html
From the.mindstorm.mailinglist at gmail.com  Sun Apr 30 15:04:56 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Sun Apr 30 15:04:59 2006
Subject: [concurrency-interest] implementing a DB write-behind algorithm
In-Reply-To: <63b4e4050604301136hdbe14b5je76bf4ea10f088e9@mail.gmail.com>
References: <c6f400460604300637s11032d5ah8583355f8cbc7990@mail.gmail.com>
	<63b4e4050604301136hdbe14b5je76bf4ea10f088e9@mail.gmail.com>
Message-ID: <c6f400460604301204g65fa38d9v9358e42baed363a3@mail.gmail.com>

Hi Tim!

And thanks for the first comments :-).

My intention is mainly the minimize any/most of the locks of the
writters, so responsiveness is maximum on this side. (the reason for
looking at ConcurrentLinkedQueue).

Thanks also for the code sample. It makes lot of sense to me. However,
I have a few comments (in case I got it write):
- considering that LinkedBlockingQueue is using different locks for
put/take it looks like there is not penalty introduced by the
consumer. Am I getting this right?
- it looks like the batched writter is doing a continuous job on
polling the queue. I was thinking that maybe I can find a way that
this batched writter to do its job only when the limit was reached.

Considering that while adding elements to the queue, I can determine
the remaining capacity of the queue, than I might trigger manually the
writter process and pass it the content of the current queue (probably
use for this the drainTo()).

./alex
--
.w( the_mindstorm )p.



On 4/30/06, Tim Peierls <tim@peierls.net> wrote:
> On 4/30/06, Alexandru Popescu
> <the.mindstorm.mailinglist@gmail.com > wrote:
>
> > I firstly have to confess that when getting to concurrency related
> > problems, I am getting confused quite quickly :-).
>
>
> You're not alone! :-)
>
>
> > Now, the current problem I am trying to solve is: I am trying to
> > figure out how to implement a DB write-behind strategy. Multiple
> > processes will post records to be written to the DB, but the actual
> > writes should happen on a separate process. So, far I was thinking
> > about 2 possible approaches:
> > a) continous write-behind: multiple processes write to a queue which
> > is continously polled by a separate process. When an element is found
> > on the queue, than the write process removes it from queue and
> > attempts to write it to the DB.
> >
> > To have this done, I was looking in the direction of
> ConcurrentLinkedQueue.
> >
> > b) batched write-behind: multiple processes post to a size-bounded
> > queue. When the max size is reached, the original queue is passed to
> > the parallel write process and replaced with a new queue.
> >
> > To have this done, I was looking in the direction of
> > LinkedBlockingQueue with an additional atomic operation of swapping
> > the old queue with the new empty one.
> >
> > My question is: am I looking in the right direction or I am completely
> > wrong. Any ideas and help are highly appreciated.
> >
>
> The use of BlockingQueue.put makes it possible to implement strategies that
> make the caller block, while still permitting strategies that don't block.
> So I would avoid ConcurrentLinkedQueue here, because it does not implement
> the BlockingQueue interface.
>
> You can use an unbounded LinkedBlockingQueue for continuous write-behind,
> and ArrayBlockingQueue (always bounded) for batched write-behind. Instead of
> swapping in a new queue, the consumer thread could just poll until the batch
> size was reached (using a timeout to avoid the risk of batches never
> completing), and then send the batch. The batch size need not be the same as
> the queue capacity.
>
> Here's an uncompiled, untested fragment that illustrates the idea:
>
>  public interface WriteBehind<T> {
>     void put(T record) throws InterruptedException;
> }
>
> public interface RecordWriter<T> {
>     void write(List<T> records) throws InterruptedException;
>  }
>
> class AbstractWriteBehind<T> implements WriteBehind<T> {
>     private final BlockingQueue<T> queue;
>     private final RecordWriter<T> writer;
>     @GuardedBy("this") private Future<Void> consumer = null;
>
>     protected AbstractWriteBehind(BlockingQueue<T> queue,
> RecordWriter<T> writer) {
>         this.queue = queue;
>         this.writer = writer;
>     }
>
>     class Consumer implements Callable<Void> {
>         public Void call() throws InterruptedException {
>             consume(queue, writer);
>             return null;
>         }
>     }
>
>     public synchronized void start() {
>         if (consumer == null) {
>             ExecutorService exec =
> Executors.newSingleThreadExecutor();
>             try {
>                 consumer = exec.submit(new Consumer());
>             } finally {
>                 exec.shutdown();
>              }
>         }
>     }
>
>     public synchronized boolean isRunning() {
>         return consumer != null;
>     }
>
>     public synchronized void stop() {
>         if (consumer != null) {
>             consumer.cancel(true);
>             consumer = null;
>         }
>     }
>
>     public final void put(T record) throws InterruptedException {
>         queue.put(record);
>     }
>
>     protected abstract void consume(BlockingQueue<T> queue, RecordWriter<T>
> writer)
>         throws InterruptedException;
> }
>
> class ContinuousWriteBehind<T> extends AbstractWriteBehind<T> {
>     ContinousWriteBehind(RecordWriter<T> writer) {
>         super(new LinkedBlockingQueue<T>(), writer);
>     }
>
>     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
>             throws InterruptedException {
>         for (T rec; (rec = q.take()) != null; )
>             writer.write (Collections.singletonList(rec));
>     }
> }
>
> class BatchedWriteBehind<T> extends AbstractWriteBehind<T> {
>     private final int maxBuf;
>     private final List<T> buf;
>     private final long time;
>     private final TimeUnit unit;
>
>     BatchedWriteBehind(RecordWriter<T> writer, int
> capacity, int maxBuf,
>                        long time, TimeUnit unit) {
>         super(new ArrayBlockingQueue<T>(capacity), writer);
>         this.maxBuf = maxBuf;
>         this.buf = new ArrayList<T>(maxBuf);
>         this.time = time;
>         this.unit = unit;
>     }
>
>     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
>             throws InterruptedException {
>         for (T rec; (rec = q.take()) != null; ) {
>             buf.add(rec);
>             while (buf.size() < maxBuf && (rec = q.poll(time, unit)) !=
> null)
>                  buf.add(rec);
>             writer.write(buf);
>             buf.clear();
>         }
>     }
> }
>
>  --tim
>

From the.mindstorm.mailinglist at gmail.com  Sun Apr 30 15:14:07 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Sun Apr 30 15:14:14 2006
Subject: [concurrency-interest] implementing a DB write-behind algorithm
In-Reply-To: <c6f400460604301204g65fa38d9v9358e42baed363a3@mail.gmail.com>
References: <c6f400460604300637s11032d5ah8583355f8cbc7990@mail.gmail.com>
	<63b4e4050604301136hdbe14b5je76bf4ea10f088e9@mail.gmail.com>
	<c6f400460604301204g65fa38d9v9358e42baed363a3@mail.gmail.com>
Message-ID: <c6f400460604301214l7abd4d95i5c1b7341b94d8b6b@mail.gmail.com>

... and another thing (for which I am not sure, but my gut feeling is
saying so)... the writter process is taking longer time to process
than the queue posters, and the solution would lead to block all
posters till the writter finishes - because of the size-bound. (hope I
explained it good enough to be understandable :-) ). Swapping a new
queue may solve this issue.

./alex
--
.w( the_mindstorm )p.



On 4/30/06, Alexandru Popescu <the.mindstorm.mailinglist@gmail.com> wrote:
> Hi Tim!
>
> And thanks for the first comments :-).
>
> My intention is mainly the minimize any/most of the locks of the
> writters, so responsiveness is maximum on this side. (the reason for
> looking at ConcurrentLinkedQueue).
>
> Thanks also for the code sample. It makes lot of sense to me. However,
> I have a few comments (in case I got it write):
> - considering that LinkedBlockingQueue is using different locks for
> put/take it looks like there is not penalty introduced by the
> consumer. Am I getting this right?
> - it looks like the batched writter is doing a continuous job on
> polling the queue. I was thinking that maybe I can find a way that
> this batched writter to do its job only when the limit was reached.
>
> Considering that while adding elements to the queue, I can determine
> the remaining capacity of the queue, than I might trigger manually the
> writter process and pass it the content of the current queue (probably
> use for this the drainTo()).
>
> ./alex
> --
> .w( the_mindstorm )p.
>
>
>
> On 4/30/06, Tim Peierls <tim@peierls.net> wrote:
> > On 4/30/06, Alexandru Popescu
> > <the.mindstorm.mailinglist@gmail.com > wrote:
> >
> > > I firstly have to confess that when getting to concurrency related
> > > problems, I am getting confused quite quickly :-).
> >
> >
> > You're not alone! :-)
> >
> >
> > > Now, the current problem I am trying to solve is: I am trying to
> > > figure out how to implement a DB write-behind strategy. Multiple
> > > processes will post records to be written to the DB, but the actual
> > > writes should happen on a separate process. So, far I was thinking
> > > about 2 possible approaches:
> > > a) continous write-behind: multiple processes write to a queue which
> > > is continously polled by a separate process. When an element is found
> > > on the queue, than the write process removes it from queue and
> > > attempts to write it to the DB.
> > >
> > > To have this done, I was looking in the direction of
> > ConcurrentLinkedQueue.
> > >
> > > b) batched write-behind: multiple processes post to a size-bounded
> > > queue. When the max size is reached, the original queue is passed to
> > > the parallel write process and replaced with a new queue.
> > >
> > > To have this done, I was looking in the direction of
> > > LinkedBlockingQueue with an additional atomic operation of swapping
> > > the old queue with the new empty one.
> > >
> > > My question is: am I looking in the right direction or I am completely
> > > wrong. Any ideas and help are highly appreciated.
> > >
> >
> > The use of BlockingQueue.put makes it possible to implement strategies that
> > make the caller block, while still permitting strategies that don't block.
> > So I would avoid ConcurrentLinkedQueue here, because it does not implement
> > the BlockingQueue interface.
> >
> > You can use an unbounded LinkedBlockingQueue for continuous write-behind,
> > and ArrayBlockingQueue (always bounded) for batched write-behind. Instead of
> > swapping in a new queue, the consumer thread could just poll until the batch
> > size was reached (using a timeout to avoid the risk of batches never
> > completing), and then send the batch. The batch size need not be the same as
> > the queue capacity.
> >
> > Here's an uncompiled, untested fragment that illustrates the idea:
> >
> >  public interface WriteBehind<T> {
> >     void put(T record) throws InterruptedException;
> > }
> >
> > public interface RecordWriter<T> {
> >     void write(List<T> records) throws InterruptedException;
> >  }
> >
> > class AbstractWriteBehind<T> implements WriteBehind<T> {
> >     private final BlockingQueue<T> queue;
> >     private final RecordWriter<T> writer;
> >     @GuardedBy("this") private Future<Void> consumer = null;
> >
> >     protected AbstractWriteBehind(BlockingQueue<T> queue,
> > RecordWriter<T> writer) {
> >         this.queue = queue;
> >         this.writer = writer;
> >     }
> >
> >     class Consumer implements Callable<Void> {
> >         public Void call() throws InterruptedException {
> >             consume(queue, writer);
> >             return null;
> >         }
> >     }
> >
> >     public synchronized void start() {
> >         if (consumer == null) {
> >             ExecutorService exec =
> > Executors.newSingleThreadExecutor();
> >             try {
> >                 consumer = exec.submit(new Consumer());
> >             } finally {
> >                 exec.shutdown();
> >              }
> >         }
> >     }
> >
> >     public synchronized boolean isRunning() {
> >         return consumer != null;
> >     }
> >
> >     public synchronized void stop() {
> >         if (consumer != null) {
> >             consumer.cancel(true);
> >             consumer = null;
> >         }
> >     }
> >
> >     public final void put(T record) throws InterruptedException {
> >         queue.put(record);
> >     }
> >
> >     protected abstract void consume(BlockingQueue<T> queue, RecordWriter<T>
> > writer)
> >         throws InterruptedException;
> > }
> >
> > class ContinuousWriteBehind<T> extends AbstractWriteBehind<T> {
> >     ContinousWriteBehind(RecordWriter<T> writer) {
> >         super(new LinkedBlockingQueue<T>(), writer);
> >     }
> >
> >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> >             throws InterruptedException {
> >         for (T rec; (rec = q.take()) != null; )
> >             writer.write (Collections.singletonList(rec));
> >     }
> > }
> >
> > class BatchedWriteBehind<T> extends AbstractWriteBehind<T> {
> >     private final int maxBuf;
> >     private final List<T> buf;
> >     private final long time;
> >     private final TimeUnit unit;
> >
> >     BatchedWriteBehind(RecordWriter<T> writer, int
> > capacity, int maxBuf,
> >                        long time, TimeUnit unit) {
> >         super(new ArrayBlockingQueue<T>(capacity), writer);
> >         this.maxBuf = maxBuf;
> >         this.buf = new ArrayList<T>(maxBuf);
> >         this.time = time;
> >         this.unit = unit;
> >     }
> >
> >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> >             throws InterruptedException {
> >         for (T rec; (rec = q.take()) != null; ) {
> >             buf.add(rec);
> >             while (buf.size() < maxBuf && (rec = q.poll(time, unit)) !=
> > null)
> >                  buf.add(rec);
> >             writer.write(buf);
> >             buf.clear();
> >         }
> >     }
> > }
> >
> >  --tim
> >
>

From Richie.Jefts at APCC.com  Sun Apr 30 15:41:23 2006
From: Richie.Jefts at APCC.com (Richie.Jefts@APCC.com)
Date: Sun Apr 30 15:42:03 2006
Subject: [concurrency-interest] implementing a DB write-behind algorithm
In-Reply-To: <c6f400460604301214l7abd4d95i5c1b7341b94d8b6b@mail.gmail.com>
Message-ID: <OFC2B9EAC5.28255A49-ON85257160.006AC52F-86257160.006BC5F7@apcc.com>

If new jobs are queued faster than the writers can write, there is not a 
whole lot you can do. Something has to give. You can have the producers 
wait until writer is ready (which currently happens), have the producers 
do a CallersRunPolicy or throw away the produced item. Swapping the queues 
won't solve the fundamental issue that producers are faster than 
consumers.

You could also make the writers use a threadpool if writes can happen 
concurrently. In this case, each thread of the continuous write behind can 
pretty much do what Tim suggested, read from queue and write the data. The 
BatchedWriteBehind you could modify to something like:

protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
    throws InterruptedException {

    BlockingQueue<List<T>> itemQueue = new 
ArrayBlockingQueue<List<T>>(capacity);
 
    for (T rec; (rec = q.take()) != null; ) {
          List<T> buf = new ArrayList<T>();
        buf.add(rec);
        while (buf.size() < maxBuf && (rec = q.poll(time, unit)) != null)
            buf.add(rec);

        itemQueue.add(buf);
    }
}

then have threads on the writer side do:

public void run() {
    for (List<T> list; (list = itemQueue.take()) != null; )
        writer.write(list);
}

That would ensure each writer gets a full batch of data.

richie




"Alexandru Popescu" <the.mindstorm.mailinglist@gmail.com> 
Sent by: concurrency-interest-bounces@cs.oswego.edu
04/30/2006 02:14 PM

To
concurrency-interest@cs.oswego.edu
cc
evo1 <the_mindstorm@evolva.ro>, Tim Peierls <tim@peierls.net>
Subject
Re: [concurrency-interest] implementing a DB write-behind algorithm






... and another thing (for which I am not sure, but my gut feeling is
saying so)... the writter process is taking longer time to process
than the queue posters, and the solution would lead to block all
posters till the writter finishes - because of the size-bound. (hope I
explained it good enough to be understandable :-) ). Swapping a new
queue may solve this issue.

./alex
--
.w( the_mindstorm )p.



On 4/30/06, Alexandru Popescu <the.mindstorm.mailinglist@gmail.com> wrote:
> Hi Tim!
>
> And thanks for the first comments :-).
>
> My intention is mainly the minimize any/most of the locks of the
> writters, so responsiveness is maximum on this side. (the reason for
> looking at ConcurrentLinkedQueue).
>
> Thanks also for the code sample. It makes lot of sense to me. However,
> I have a few comments (in case I got it write):
> - considering that LinkedBlockingQueue is using different locks for
> put/take it looks like there is not penalty introduced by the
> consumer. Am I getting this right?
> - it looks like the batched writter is doing a continuous job on
> polling the queue. I was thinking that maybe I can find a way that
> this batched writter to do its job only when the limit was reached.
>
> Considering that while adding elements to the queue, I can determine
> the remaining capacity of the queue, than I might trigger manually the
> writter process and pass it the content of the current queue (probably
> use for this the drainTo()).
>
> ./alex
> --
> .w( the_mindstorm )p.
>
>
>
> On 4/30/06, Tim Peierls <tim@peierls.net> wrote:
> > On 4/30/06, Alexandru Popescu
> > <the.mindstorm.mailinglist@gmail.com > wrote:
> >
> > > I firstly have to confess that when getting to concurrency related
> > > problems, I am getting confused quite quickly :-).
> >
> >
> > You're not alone! :-)
> >
> >
> > > Now, the current problem I am trying to solve is: I am trying to
> > > figure out how to implement a DB write-behind strategy. Multiple
> > > processes will post records to be written to the DB, but the actual
> > > writes should happen on a separate process. So, far I was thinking
> > > about 2 possible approaches:
> > > a) continous write-behind: multiple processes write to a queue which
> > > is continously polled by a separate process. When an element is 
found
> > > on the queue, than the write process removes it from queue and
> > > attempts to write it to the DB.
> > >
> > > To have this done, I was looking in the direction of
> > ConcurrentLinkedQueue.
> > >
> > > b) batched write-behind: multiple processes post to a size-bounded
> > > queue. When the max size is reached, the original queue is passed to
> > > the parallel write process and replaced with a new queue.
> > >
> > > To have this done, I was looking in the direction of
> > > LinkedBlockingQueue with an additional atomic operation of swapping
> > > the old queue with the new empty one.
> > >
> > > My question is: am I looking in the right direction or I am 
completely
> > > wrong. Any ideas and help are highly appreciated.
> > >
> >
> > The use of BlockingQueue.put makes it possible to implement strategies 
that
> > make the caller block, while still permitting strategies that don't 
block.
> > So I would avoid ConcurrentLinkedQueue here, because it does not 
implement
> > the BlockingQueue interface.
> >
> > You can use an unbounded LinkedBlockingQueue for continuous 
write-behind,
> > and ArrayBlockingQueue (always bounded) for batched write-behind. 
Instead of
> > swapping in a new queue, the consumer thread could just poll until the 
batch
> > size was reached (using a timeout to avoid the risk of batches never
> > completing), and then send the batch. The batch size need not be the 
same as
> > the queue capacity.
> >
> > Here's an uncompiled, untested fragment that illustrates the idea:
> >
> >  public interface WriteBehind<T> {
> >     void put(T record) throws InterruptedException;
> > }
> >
> > public interface RecordWriter<T> {
> >     void write(List<T> records) throws InterruptedException;
> >  }
> >
> > class AbstractWriteBehind<T> implements WriteBehind<T> {
> >     private final BlockingQueue<T> queue;
> >     private final RecordWriter<T> writer;
> >     @GuardedBy("this") private Future<Void> consumer = null;
> >
> >     protected AbstractWriteBehind(BlockingQueue<T> queue,
> > RecordWriter<T> writer) {
> >         this.queue = queue;
> >         this.writer = writer;
> >     }
> >
> >     class Consumer implements Callable<Void> {
> >         public Void call() throws InterruptedException {
> >             consume(queue, writer);
> >             return null;
> >         }
> >     }
> >
> >     public synchronized void start() {
> >         if (consumer == null) {
> >             ExecutorService exec =
> > Executors.newSingleThreadExecutor();
> >             try {
> >                 consumer = exec.submit(new Consumer());
> >             } finally {
> >                 exec.shutdown();
> >              }
> >         }
> >     }
> >
> >     public synchronized boolean isRunning() {
> >         return consumer != null;
> >     }
> >
> >     public synchronized void stop() {
> >         if (consumer != null) {
> >             consumer.cancel(true);
> >             consumer = null;
> >         }
> >     }
> >
> >     public final void put(T record) throws InterruptedException {
> >         queue.put(record);
> >     }
> >
> >     protected abstract void consume(BlockingQueue<T> queue, 
RecordWriter<T>
> > writer)
> >         throws InterruptedException;
> > }
> >
> > class ContinuousWriteBehind<T> extends AbstractWriteBehind<T> {
> >     ContinousWriteBehind(RecordWriter<T> writer) {
> >         super(new LinkedBlockingQueue<T>(), writer);
> >     }
> >
> >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> >             throws InterruptedException {
> >         for (T rec; (rec = q.take()) != null; )
> >             writer.write (Collections.singletonList(rec));
> >     }
> > }
> >
> > class BatchedWriteBehind<T> extends AbstractWriteBehind<T> {
> >     private final int maxBuf;
> >     private final List<T> buf;
> >     private final long time;
> >     private final TimeUnit unit;
> >
> >     BatchedWriteBehind(RecordWriter<T> writer, int
> > capacity, int maxBuf,
> >                        long time, TimeUnit unit) {
> >         super(new ArrayBlockingQueue<T>(capacity), writer);
> >         this.maxBuf = maxBuf;
> >         this.buf = new ArrayList<T>(maxBuf);
> >         this.time = time;
> >         this.unit = unit;
> >     }
> >
> >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> >             throws InterruptedException {
> >         for (T rec; (rec = q.take()) != null; ) {
> >             buf.add(rec);
> >             while (buf.size() < maxBuf && (rec = q.poll(time, unit)) 
!=
> > null)
> >                  buf.add(rec);
> >             writer.write(buf);
> >             buf.clear();
> >         }
> >     }
> > }
> >
> >  --tim
> >
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060430/b0432813/attachment.html
From the.mindstorm.mailinglist at gmail.com  Sun Apr 30 15:53:15 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Sun Apr 30 15:53:26 2006
Subject: [concurrency-interest] implementing a DB write-behind algorithm
In-Reply-To: <OFC2B9EAC5.28255A49-ON85257160.006AC52F-86257160.006BC5F7@apcc.com>
References: <c6f400460604301214l7abd4d95i5c1b7341b94d8b6b@mail.gmail.com>
	<OFC2B9EAC5.28255A49-ON85257160.006AC52F-86257160.006BC5F7@apcc.com>
Message-ID: <c6f400460604301253l454a5d58j52992c0cf952be5a@mail.gmail.com>

On 4/30/06, Richie.Jefts@apcc.com <Richie.Jefts@apcc.com> wrote:
>
>
> If new jobs are queued faster than the writers can write, there is not a
> whole lot you can do. Something has to give. You can have the producers wait
> until writer is ready (which currently happens), have the producers do a
> CallersRunPolicy or throw away the produced item.
>



Swapping the queues won't solve the fundamental issue that producers are
> faster than consumers.
>

Sorry, but I am missing the reason why... the producers will continue to
post on empty queue, while  writter(s)  will continue to write. Balancing
the two will keep the producers not block.


You could also make the writers use a threadpool if writes can happen
> concurrently.
>

I missed mentioning this part: yes writes can happen concurrently.

./alex
--
.w( the_mindstorm )p.


In this case, each thread of the continuous write behind can pretty much do
> what Tim suggested, read from queue and write the data. The
> BatchedWriteBehind you could modify to something like:
>
>
> protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
>    throws InterruptedException {
>
>     BlockingQueue<List<T>> itemQueue = new
> ArrayBlockingQueue<List<T>>(capacity);
>
>    for (T rec; (rec = q.take()) != null; ) {
>           List<T> buf = new ArrayList<T>();
>
>        buf.add(rec);
>        while (buf.size() < maxBuf && (rec = q.poll(time, unit)) != null)
>            buf.add(rec);
>
>
>         itemQueue.add(buf);
>    }
> }
>
> then have threads on the writer side do:
>
> public void run() {
>     for (List<T> list; (list = itemQueue.take()) != null; )
>        writer.write(list);
> }
>
> That would ensure each writer gets a full batch of data.
>
> richie
>
>
>
>  *"Alexandru Popescu" <the.mindstorm.mailinglist@gmail.com>*
> Sent by: concurrency-interest-bounces@cs.oswego.edu
>
> 04/30/2006 02:14 PM
>   To
> concurrency-interest@cs.oswego.edu  cc
> evo1 <the_mindstorm@evolva.ro>, Tim Peierls <tim@peierls.net>  Subject
> Re: [concurrency-interest] implementing a DB write-behind algorithm
>
>
>
>
>
>
> ... and another thing (for which I am not sure, but my gut feeling is
> saying so)... the writter process is taking longer time to process
> than the queue posters, and the solution would lead to block all
> posters till the writter finishes - because of the size-bound. (hope I
> explained it good enough to be understandable :-) ). Swapping a new
> queue may solve this issue.
>
> ./alex
> --
> .w( the_mindstorm )p.
>
>
>
> On 4/30/06, Alexandru Popescu <the.mindstorm.mailinglist@gmail.com> wrote:
> > Hi Tim!
> >
> > And thanks for the first comments :-).
> >
> > My intention is mainly the minimize any/most of the locks of the
> > writters, so responsiveness is maximum on this side. (the reason for
> > looking at ConcurrentLinkedQueue).
> >
> > Thanks also for the code sample. It makes lot of sense to me. However,
> > I have a few comments (in case I got it write):
> > - considering that LinkedBlockingQueue is using different locks for
> > put/take it looks like there is not penalty introduced by the
> > consumer. Am I getting this right?
> > - it looks like the batched writter is doing a continuous job on
> > polling the queue. I was thinking that maybe I can find a way that
> > this batched writter to do its job only when the limit was reached.
> >
> > Considering that while adding elements to the queue, I can determine
> > the remaining capacity of the queue, than I might trigger manually the
> > writter process and pass it the content of the current queue (probably
> > use for this the drainTo()).
> >
> > ./alex
> > --
> > .w( the_mindstorm )p.
> >
> >
> >
> > On 4/30/06, Tim Peierls <tim@peierls.net> wrote:
> > > On 4/30/06, Alexandru Popescu
> > > <the.mindstorm.mailinglist@gmail.com > wrote:
> > >
> > > > I firstly have to confess that when getting to concurrency related
> > > > problems, I am getting confused quite quickly :-).
> > >
> > >
> > > You're not alone! :-)
> > >
> > >
> > > > Now, the current problem I am trying to solve is: I am trying to
> > > > figure out how to implement a DB write-behind strategy. Multiple
> > > > processes will post records to be written to the DB, but the actual
> > > > writes should happen on a separate process. So, far I was thinking
> > > > about 2 possible approaches:
> > > > a) continous write-behind: multiple processes write to a queue which
> > > > is continously polled by a separate process. When an element is
> found
> > > > on the queue, than the write process removes it from queue and
> > > > attempts to write it to the DB.
> > > >
> > > > To have this done, I was looking in the direction of
> > > ConcurrentLinkedQueue.
> > > >
> > > > b) batched write-behind: multiple processes post to a size-bounded
> > > > queue. When the max size is reached, the original queue is passed to
> > > > the parallel write process and replaced with a new queue.
> > > >
> > > > To have this done, I was looking in the direction of
> > > > LinkedBlockingQueue with an additional atomic operation of swapping
> > > > the old queue with the new empty one.
> > > >
> > > > My question is: am I looking in the right direction or I am
> completely
> > > > wrong. Any ideas and help are highly appreciated.
> > > >
> > >
> > > The use of BlockingQueue.put makes it possible to implement strategies
> that
> > > make the caller block, while still permitting strategies that don't
> block.
> > > So I would avoid ConcurrentLinkedQueue here, because it does not
> implement
> > > the BlockingQueue interface.
> > >
> > > You can use an unbounded LinkedBlockingQueue for continuous
> write-behind,
> > > and ArrayBlockingQueue (always bounded) for batched write-behind.
> Instead of
> > > swapping in a new queue, the consumer thread could just poll until the
> batch
> > > size was reached (using a timeout to avoid the risk of batches never
> > > completing), and then send the batch. The batch size need not be the
> same as
> > > the queue capacity.
> > >
> > > Here's an uncompiled, untested fragment that illustrates the idea:
> > >
> > >  public interface WriteBehind<T> {
> > >     void put(T record) throws InterruptedException;
> > > }
> > >
> > > public interface RecordWriter<T> {
> > >     void write(List<T> records) throws InterruptedException;
> > >  }
> > >
> > > class AbstractWriteBehind<T> implements WriteBehind<T> {
> > >     private final BlockingQueue<T> queue;
> > >     private final RecordWriter<T> writer;
> > >     @GuardedBy("this") private Future<Void> consumer = null;
> > >
> > >     protected AbstractWriteBehind(BlockingQueue<T> queue,
> > > RecordWriter<T> writer) {
> > >         this.queue = queue;
> > >         this.writer = writer;
> > >     }
> > >
> > >     class Consumer implements Callable<Void> {
> > >         public Void call() throws InterruptedException {
> > >             consume(queue, writer);
> > >             return null;
> > >         }
> > >     }
> > >
> > >     public synchronized void start() {
> > >         if (consumer == null) {
> > >             ExecutorService exec =
> > > Executors.newSingleThreadExecutor();
> > >             try {
> > >                 consumer = exec.submit(new Consumer());
> > >             } finally {
> > >                 exec.shutdown();
> > >              }
> > >         }
> > >     }
> > >
> > >     public synchronized boolean isRunning() {
> > >         return consumer != null;
> > >     }
> > >
> > >     public synchronized void stop() {
> > >         if (consumer != null) {
> > >             consumer.cancel(true);
> > >             consumer = null;
> > >         }
> > >     }
> > >
> > >     public final void put(T record) throws InterruptedException {
> > >         queue.put(record);
> > >     }
> > >
> > >     protected abstract void consume(BlockingQueue<T> queue,
> RecordWriter<T>
> > > writer)
> > >         throws InterruptedException;
> > > }
> > >
> > > class ContinuousWriteBehind<T> extends AbstractWriteBehind<T> {
> > >     ContinousWriteBehind(RecordWriter<T> writer) {
> > >         super(new LinkedBlockingQueue<T>(), writer);
> > >     }
> > >
> > >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> > >             throws InterruptedException {
> > >         for (T rec; (rec = q.take()) != null; )
> > >             writer.write (Collections.singletonList(rec));
> > >     }
> > > }
> > >
> > > class BatchedWriteBehind<T> extends AbstractWriteBehind<T> {
> > >     private final int maxBuf;
> > >     private final List<T> buf;
> > >     private final long time;
> > >     private final TimeUnit unit;
> > >
> > >     BatchedWriteBehind(RecordWriter<T> writer, int
> > > capacity, int maxBuf,
> > >                        long time, TimeUnit unit) {
> > >         super(new ArrayBlockingQueue<T>(capacity), writer);
> > >         this.maxBuf = maxBuf;
> > >         this.buf = new ArrayList<T>(maxBuf);
> > >         this.time = time;
> > >         this.unit = unit;
> > >     }
> > >
> > >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> > >             throws InterruptedException {
> > >         for (T rec; (rec = q.take()) != null; ) {
> > >             buf.add(rec);
> > >             while (buf.size() < maxBuf && (rec = q.poll(time, unit))
> !=
> > > null)
> > >                  buf.add(rec);
> > >             writer.write(buf);
> > >             buf.clear();
> > >         }
> > >     }
> > > }
> > >
> > >  --tim
> > >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060430/f0bf014f/attachment-0001.html
From Richie.Jefts at APCC.com  Sun Apr 30 16:05:21 2006
From: Richie.Jefts at APCC.com (Richie.Jefts@APCC.com)
Date: Sun Apr 30 16:06:05 2006
Subject: [concurrency-interest] implementing a DB write-behind algorithm
In-Reply-To: <c6f400460604301253l454a5d58j52992c0cf952be5a@mail.gmail.com>
Message-ID: <OFE32CECE6.F47A493E-ON85257160.006D9308-86257160.006DF7B6@apcc.com>

The ContinousWriteBehind does not block producers since it uses an 
unbounded queue. You could change the BatchWriteBehind to also use an 
unbounded queue so producers do not wait until writes are complete (switch 
the ArrayBlockingQueue to LinkedBlockingQueue). In this case the queues 
will continue to grow while writes are happening. But do you really want 
this behavior? If writes are always slower than producers you'll 
eventually hit out of memory problems.

richie




"Alexandru Popescu" <the.mindstorm.mailinglist@gmail.com> 
04/30/2006 02:53 PM

To
"Richie.Jefts@apcc.com" <Richie.Jefts@apcc.com>
cc
concurrency-interest@cs.oswego.edu, 
concurrency-interest-bounces@cs.oswego.edu, evo1 
<the_mindstorm@evolva.ro>, "Tim Peierls" <tim@peierls.net>
Subject
Re: [concurrency-interest] implementing a DB write-behind algorithm








On 4/30/06, Richie.Jefts@apcc.com <Richie.Jefts@apcc.com> wrote: 

If new jobs are queued faster than the writers can write, there is not a 
whole lot you can do. Something has to give. You can have the producers 
wait until writer is ready (which currently happens), have the producers 
do a CallersRunPolicy or throw away the produced item. 

 

Swapping the queues won't solve the fundamental issue that producers are 
faster than consumers.

Sorry, but I am missing the reason why... the producers will continue to 
post on empty queue, while  writter(s)  will continue to write. Balancing 
the two will keep the producers not block. 
 

You could also make the writers use a threadpool if writes can happen 
concurrently. 

I missed mentioning this part: yes writes can happen concurrently.

./alex
--
.w( the_mindstorm )p.
 

In this case, each thread of the continuous write behind can pretty much 
do what Tim suggested, read from queue and write the data. The 
BatchedWriteBehind you could modify to something like: 


protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
   throws InterruptedException { 

    BlockingQueue<List<T>> itemQueue = new 
ArrayBlockingQueue<List<T>>(capacity); 
 
   for (T rec; (rec = q.take()) != null; ) { 
          List<T> buf = new ArrayList<T>();

       buf.add(rec);
       while (buf.size() < maxBuf && (rec = q.poll(time, unit)) != null)
           buf.add(rec);


        itemQueue.add(buf);
   }
} 

then have threads on the writer side do: 

public void run() { 
    for (List<T> list; (list = itemQueue.take()) != null; )
       writer.write(list); 
} 

That would ensure each writer gets a full batch of data.

richie 



"Alexandru Popescu" <the.mindstorm.mailinglist@gmail.com> 
Sent by: concurrency-interest-bounces@cs.oswego.edu 
04/30/2006 02:14 PM 


To
concurrency-interest@cs.oswego.edu 
cc
evo1 <the_mindstorm@evolva.ro>, Tim Peierls <tim@peierls.net> 
Subject
Re: [concurrency-interest] implementing a DB write-behind algorithm








... and another thing (for which I am not sure, but my gut feeling is
saying so)... the writter process is taking longer time to process
than the queue posters, and the solution would lead to block all
posters till the writter finishes - because of the size-bound. (hope I
explained it good enough to be understandable :-) ). Swapping a new
queue may solve this issue.

./alex
--
.w( the_mindstorm )p.



On 4/30/06, Alexandru Popescu <the.mindstorm.mailinglist@gmail.com> wrote:
> Hi Tim!
>
> And thanks for the first comments :-).
>
> My intention is mainly the minimize any/most of the locks of the
> writters, so responsiveness is maximum on this side. (the reason for
> looking at ConcurrentLinkedQueue).
>
> Thanks also for the code sample. It makes lot of sense to me. However,
> I have a few comments (in case I got it write):
> - considering that LinkedBlockingQueue is using different locks for
> put/take it looks like there is not penalty introduced by the
> consumer. Am I getting this right?
> - it looks like the batched writter is doing a continuous job on
> polling the queue. I was thinking that maybe I can find a way that
> this batched writter to do its job only when the limit was reached.
>
> Considering that while adding elements to the queue, I can determine
> the remaining capacity of the queue, than I might trigger manually the
> writter process and pass it the content of the current queue (probably
> use for this the drainTo()).
>
> ./alex
> --
> .w( the_mindstorm )p.
>
>
>
> On 4/30/06, Tim Peierls <tim@peierls.net> wrote:
> > On 4/30/06, Alexandru Popescu
> > <the.mindstorm.mailinglist@gmail.com > wrote:
> >
> > > I firstly have to confess that when getting to concurrency related
> > > problems, I am getting confused quite quickly :-).
> >
> >
> > You're not alone! :-)
> >
> >
> > > Now, the current problem I am trying to solve is: I am trying to
> > > figure out how to implement a DB write-behind strategy. Multiple
> > > processes will post records to be written to the DB, but the actual
> > > writes should happen on a separate process. So, far I was thinking
> > > about 2 possible approaches:
> > > a) continous write-behind: multiple processes write to a queue which
> > > is continously polled by a separate process. When an element is 
found
> > > on the queue, than the write process removes it from queue and
> > > attempts to write it to the DB.
> > >
> > > To have this done, I was looking in the direction of
> > ConcurrentLinkedQueue.
> > >
> > > b) batched write-behind: multiple processes post to a size-bounded
> > > queue. When the max size is reached, the original queue is passed to
> > > the parallel write process and replaced with a new queue.
> > >
> > > To have this done, I was looking in the direction of
> > > LinkedBlockingQueue with an additional atomic operation of swapping
> > > the old queue with the new empty one.
> > >
> > > My question is: am I looking in the right direction or I am 
completely
> > > wrong. Any ideas and help are highly appreciated.
> > >
> >
> > The use of BlockingQueue.put makes it possible to implement strategies 
that
> > make the caller block, while still permitting strategies that don't 
block.
> > So I would avoid ConcurrentLinkedQueue here, because it does not 
implement
> > the BlockingQueue interface.
> >
> > You can use an unbounded LinkedBlockingQueue for continuous 
write-behind,
> > and ArrayBlockingQueue (always bounded) for batched write-behind. 
Instead of
> > swapping in a new queue, the consumer thread could just poll until the 
batch
> > size was reached (using a timeout to avoid the risk of batches never
> > completing), and then send the batch. The batch size need not be the 
same as
> > the queue capacity.
> >
> > Here's an uncompiled, untested fragment that illustrates the idea:
> >
> >  public interface WriteBehind<T> {
> >     void put(T record) throws InterruptedException;
> > }
> >
> > public interface RecordWriter<T> {
> >     void write(List<T> records) throws InterruptedException;
> >  }
> >
> > class AbstractWriteBehind<T> implements WriteBehind<T> {
> >     private final BlockingQueue<T> queue;
> >     private final RecordWriter<T> writer;
> >     @GuardedBy("this") private Future<Void> consumer = null;
> >
> >     protected AbstractWriteBehind(BlockingQueue<T> queue,
> > RecordWriter<T> writer) {
> >         this.queue = queue;
> >         this.writer = writer;
> >     }
> >
> >     class Consumer implements Callable<Void> {
> >         public Void call() throws InterruptedException {
> >             consume(queue, writer);
> >             return null;
> >         }
> >     }
> >
> >     public synchronized void start() {
> >         if (consumer == null) {
> >             ExecutorService exec =
> > Executors.newSingleThreadExecutor();
> >             try {
> >                 consumer = exec.submit(new Consumer());
> >             } finally {
> >                 exec.shutdown();
> >              }
> >         }
> >     }
> >
> >     public synchronized boolean isRunning() {
> >         return consumer != null;
> >     }
> >
> >     public synchronized void stop() {
> >         if (consumer != null) {
> >             consumer.cancel(true);
> >             consumer = null;
> >         }
> >     }
> >
> >     public final void put(T record) throws InterruptedException {
> >         queue.put(record);
> >     }
> >
> >     protected abstract void consume(BlockingQueue<T> queue, 
RecordWriter<T>
> > writer)
> >         throws InterruptedException;
> > }
> >
> > class ContinuousWriteBehind<T> extends AbstractWriteBehind<T> {
> >     ContinousWriteBehind(RecordWriter<T> writer) {
> >         super(new LinkedBlockingQueue<T>(), writer);
> >     }
> >
> >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> >             throws InterruptedException {
> >         for (T rec; (rec = q.take()) != null; )
> >             writer.write (Collections.singletonList(rec));
> >     }
> > }
> >
> > class BatchedWriteBehind<T> extends AbstractWriteBehind<T> {
> >     private final int maxBuf;
> >     private final List<T> buf;
> >     private final long time;
> >     private final TimeUnit unit;
> >
> >     BatchedWriteBehind(RecordWriter<T> writer, int
> > capacity, int maxBuf,
> >                        long time, TimeUnit unit) {
> >         super(new ArrayBlockingQueue<T>(capacity), writer);
> >         this.maxBuf = maxBuf;
> >         this.buf = new ArrayList<T>(maxBuf);
> >         this.time = time;
> >         this.unit = unit;
> >     }
> >
> >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> >             throws InterruptedException {
> >         for (T rec; (rec = q.take()) != null; ) {
> >             buf.add(rec);
> >             while (buf.size() < maxBuf && (rec = q.poll(time, unit)) 
!=
> > null)
> >                  buf.add(rec);
> >             writer.write(buf);
> >             buf.clear();
> >         }
> >     }
> > }
> >
> >  --tim
> >
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060430/16ca717e/attachment.html
From the.mindstorm.mailinglist at gmail.com  Sun Apr 30 16:16:15 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Sun Apr 30 16:16:36 2006
Subject: [concurrency-interest] implementing a DB write-behind algorithm
In-Reply-To: <OFE32CECE6.F47A493E-ON85257160.006D9308-86257160.006DF7B6@apcc.com>
References: <c6f400460604301253l454a5d58j52992c0cf952be5a@mail.gmail.com>
	<OFE32CECE6.F47A493E-ON85257160.006D9308-86257160.006DF7B6@apcc.com>
Message-ID: <c6f400460604301316k64efa066kff91f3853f98691f@mail.gmail.com>

On 4/30/06, Richie.Jefts@apcc.com <Richie.Jefts@apcc.com> wrote:
>
>
> The ContinousWriteBehind does not block producers since it uses an
> unbounded queue. You could change the BatchWriteBehind to also use an
> unbounded queue so producers do not wait until writes are complete (switch
> the ArrayBlockingQueue to LinkedBlockingQueue). In this case the queues will
> continue to grow while writes are happening. But do you really want this
> behavior? If writes are always slower than producers you'll eventually hit
> out of memory problems.
>
> richie
>

Thanks Rickie. Probably, I need to clarify one thing: a "write" operation is
slower than a "put" operation (a put op means adding some data to a queue,
while a write op means persisting it in a DB). But this doesn't imply that
the system will hit the system limits, because you can have multiple writers
(still I agree with you that theoretically speaking it may happen if no good
balance can be computed).
Now, in time the "put" ops frequency are fluctuant, while the writes can be
continuous.

hope this clarifies some of the points I've missed,

./alex
--
.w( the_mindstorm )p.


*"Alexandru Popescu" <the.mindstorm.mailinglist@gmail.com>*
>
> 04/30/2006 02:53 PM
>   To
> "Richie.Jefts@apcc.com" <Richie.Jefts@apcc.com>
>  cc
> concurrency-interest@cs.oswego.edu,
> concurrency-interest-bounces@cs.oswego.edu, evo1 <the_mindstorm@evolva.ro>,
> "Tim Peierls" <tim@peierls.net>
>  Subject
> Re: [concurrency-interest] implementing a DB write-behind algorithm
>
>
>
>
>
>
>
>
> On 4/30/06, *Richie.Jefts@apcc.com* <Richie.Jefts@apcc.com> <*
> Richie.Jefts@apcc.com* <Richie.Jefts@apcc.com>> wrote:
>
> If new jobs are queued faster than the writers can write, there is not a
> whole lot you can do. Something has to give. You can have the producers wait
> until writer is ready (which currently happens), have the producers do a
> CallersRunPolicy or throw away the produced item.
>
>
>
> Swapping the queues won't solve the fundamental issue that producers are
> faster than consumers.
>
> Sorry, but I am missing the reason why... the producers will continue to
> post on empty queue, while  writter(s)  will continue to write. Balancing
> the two will keep the producers not block.
>
>
> You could also make the writers use a threadpool if writes can happen
> concurrently.
>
> I missed mentioning this part: yes writes can happen concurrently.
>
> ./alex
> --
> .w( the_mindstorm )p.
>
>
> In this case, each thread of the continuous write behind can pretty much
> do what Tim suggested, read from queue and write the data. The
> BatchedWriteBehind you could modify to something like:
>
>
> protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
>   throws InterruptedException {
>
>     BlockingQueue<List<T>> itemQueue = new
> ArrayBlockingQueue<List<T>>(capacity);
>
>   for (T rec; (rec = q.take()) != null; ) {
>          List<T> buf = new ArrayList<T>();
>
>       buf.add(rec);
>       while (buf.size() < maxBuf && (rec = q.poll(time, unit)) != null)
>           buf.add(rec);
>
>
>        itemQueue.add(buf);
>   }
> }
>
> then have threads on the writer side do:
>
> public void run() {
>    for (List<T> list; (list = itemQueue.take()) != null; )
>       writer.write(list);
> }
>
> That would ensure each writer gets a full batch of data.
>
> richie
>
>
>   *"Alexandru Popescu" <**the.mindstorm.mailinglist@gmail.com*<the.mindstorm.mailinglist@gmail.com>
> *>*
> Sent by: *concurrency-interest-bounces@cs.oswego.edu *<concurrency-interest-bounces@cs.oswego.edu>
>
> 04/30/2006 02:14 PM
>
>   To
> *concurrency-interest@cs.oswego.edu* <concurrency-interest@cs.oswego.edu>
> cc
> evo1 <*the_mindstorm@evolva.ro* <the_mindstorm@evolva.ro>>, Tim Peierls <*
> tim@peierls.net* <tim@peierls.net>>  Subject
> Re: [concurrency-interest] implementing a DB write-behind algorithm
>
>
>
>
>
>
>
>
> ... and another thing (for which I am not sure, but my gut feeling is
> saying so)... the writter process is taking longer time to process
> than the queue posters, and the solution would lead to block all
> posters till the writter finishes - because of the size-bound. (hope I
> explained it good enough to be understandable :-) ). Swapping a new
> queue may solve this issue.
>
> ./alex
> --
> .w( the_mindstorm )p.
>
>
>
> On 4/30/06, Alexandru Popescu <*the.mindstorm.mailinglist@gmail.com*<the.mindstorm.mailinglist@gmail.com>>
> wrote:
> > Hi Tim!
> >
> > And thanks for the first comments :-).
> >
> > My intention is mainly the minimize any/most of the locks of the
> > writters, so responsiveness is maximum on this side. (the reason for
> > looking at ConcurrentLinkedQueue).
> >
> > Thanks also for the code sample. It makes lot of sense to me. However,
> > I have a few comments (in case I got it write):
> > - considering that LinkedBlockingQueue is using different locks for
> > put/take it looks like there is not penalty introduced by the
> > consumer. Am I getting this right?
> > - it looks like the batched writter is doing a continuous job on
> > polling the queue. I was thinking that maybe I can find a way that
> > this batched writter to do its job only when the limit was reached.
> >
> > Considering that while adding elements to the queue, I can determine
> > the remaining capacity of the queue, than I might trigger manually the
> > writter process and pass it the content of the current queue (probably
> > use for this the drainTo()).
> >
> > ./alex
> > --
> > .w( the_mindstorm )p.
> >
> >
> >
> > On 4/30/06, Tim Peierls <*tim@peierls.net* <tim@peierls.net>> wrote:
> > > On 4/30/06, Alexandru Popescu
> > > <*the.mindstorm.mailinglist@gmail.com*<the.mindstorm.mailinglist@gmail.com>> wrote:
> > >
> > > > I firstly have to confess that when getting to concurrency related
> > > > problems, I am getting confused quite quickly :-).
> > >
> > >
> > > You're not alone! :-)
> > >
> > >
> > > > Now, the current problem I am trying to solve is: I am trying to
> > > > figure out how to implement a DB write-behind strategy. Multiple
> > > > processes will post records to be written to the DB, but the actual
> > > > writes should happen on a separate process. So, far I was thinking
> > > > about 2 possible approaches:
> > > > a) continous write-behind: multiple processes write to a queue which
> > > > is continously polled by a separate process. When an element is
> found
> > > > on the queue, than the write process removes it from queue and
> > > > attempts to write it to the DB.
> > > >
> > > > To have this done, I was looking in the direction of
> > > ConcurrentLinkedQueue.
> > > >
> > > > b) batched write-behind: multiple processes post to a size-bounded
> > > > queue. When the max size is reached, the original queue is passed to
> > > > the parallel write process and replaced with a new queue.
> > > >
> > > > To have this done, I was looking in the direction of
> > > > LinkedBlockingQueue with an additional atomic operation of swapping
> > > > the old queue with the new empty one.
> > > >
> > > > My question is: am I looking in the right direction or I am
> completely
> > > > wrong. Any ideas and help are highly appreciated.
> > > >
> > >
> > > The use of BlockingQueue.put makes it possible to implement strategies
> that
> > > make the caller block, while still permitting strategies that don't
> block.
> > > So I would avoid ConcurrentLinkedQueue here, because it does not
> implement
> > > the BlockingQueue interface.
> > >
> > > You can use an unbounded LinkedBlockingQueue for continuous
> write-behind,
> > > and ArrayBlockingQueue (always bounded) for batched write-behind.
> Instead of
> > > swapping in a new queue, the consumer thread could just poll until the
> batch
> > > size was reached (using a timeout to avoid the risk of batches never
> > > completing), and then send the batch. The batch size need not be the
> same as
> > > the queue capacity.
> > >
> > > Here's an uncompiled, untested fragment that illustrates the idea:
> > >
> > >  public interface WriteBehind<T> {
> > >     void put(T record) throws InterruptedException;
> > > }
> > >
> > > public interface RecordWriter<T> {
> > >     void write(List<T> records) throws InterruptedException;
> > >  }
> > >
> > > class AbstractWriteBehind<T> implements WriteBehind<T> {
> > >     private final BlockingQueue<T> queue;
> > >     private final RecordWriter<T> writer;
> > >     @GuardedBy("this") private Future<Void> consumer = null;
> > >
> > >     protected AbstractWriteBehind(BlockingQueue<T> queue,
> > > RecordWriter<T> writer) {
> > >         this.queue = queue;
> > >         this.writer = writer;
> > >     }
> > >
> > >     class Consumer implements Callable<Void> {
> > >         public Void call() throws InterruptedException {
> > >             consume(queue, writer);
> > >             return null;
> > >         }
> > >     }
> > >
> > >     public synchronized void start() {
> > >         if (consumer == null) {
> > >             ExecutorService exec =
> > > Executors.newSingleThreadExecutor();
> > >             try {
> > >                 consumer = exec.submit(new Consumer());
> > >             } finally {
> > >                 exec.shutdown();
> > >              }
> > >         }
> > >     }
> > >
> > >     public synchronized boolean isRunning() {
> > >         return consumer != null;
> > >     }
> > >
> > >     public synchronized void stop() {
> > >         if (consumer != null) {
> > >             consumer.cancel(true);
> > >             consumer = null;
> > >         }
> > >     }
> > >
> > >     public final void put(T record) throws InterruptedException {
> > >         queue.put(record);
> > >     }
> > >
> > >     protected abstract void consume(BlockingQueue<T> queue,
> RecordWriter<T>
> > > writer)
> > >         throws InterruptedException;
> > > }
> > >
> > > class ContinuousWriteBehind<T> extends AbstractWriteBehind<T> {
> > >     ContinousWriteBehind(RecordWriter<T> writer) {
> > >         super(new LinkedBlockingQueue<T>(), writer);
> > >     }
> > >
> > >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> > >             throws InterruptedException {
> > >         for (T rec; (rec = q.take()) != null; )
> > >             writer.write (Collections.singletonList(rec));
> > >     }
> > > }
> > >
> > > class BatchedWriteBehind<T> extends AbstractWriteBehind<T> {
> > >     private final int maxBuf;
> > >     private final List<T> buf;
> > >     private final long time;
> > >     private final TimeUnit unit;
> > >
> > >     BatchedWriteBehind(RecordWriter<T> writer, int
> > > capacity, int maxBuf,
> > >                        long time, TimeUnit unit) {
> > >         super(new ArrayBlockingQueue<T>(capacity), writer);
> > >         this.maxBuf = maxBuf;
> > >         this.buf = new ArrayList<T>(maxBuf);
> > >         this.time = time;
> > >         this.unit = unit;
> > >     }
> > >
> > >     protected void consume(BlockingQueue<T> q, RecordWriter<T> writer)
> > >             throws InterruptedException {
> > >         for (T rec; (rec = q.take()) != null; ) {
> > >             buf.add(rec);
> > >             while (buf.size() < maxBuf && (rec = q.poll(time, unit))
> !=
> > > null)
> > >                  buf.add(rec);
> > >             writer.write(buf);
> > >             buf.clear();
> > >         }
> > >     }
> > > }
> > >
> > >  --tim
> > >
> >
>
> _______________________________________________
> Concurrency-interest mailing list*
> **Concurrency-interest@altair.cs.oswego.edu*<Concurrency-interest@altair.cs.oswego.edu>
> *
> **http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest*<http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060430/7756c44c/attachment-0001.html
From dl at cs.oswego.edu  Sun Apr 30 18:29:30 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun Apr 30 18:29:34 2006
Subject: [concurrency-interest] ConcurrentNavigableMap additional methods:
	6415641
In-Reply-To: <BAY105-F13A4F1376C34D29C74DD9F83BD0@phx.gbl>
References: <BAY105-F13A4F1376C34D29C74DD9F83BD0@phx.gbl>
Message-ID: <44553A4A.8000806@cs.oswego.edu>

Jason Mehrens wrote:
> The only use case I can think of would be clearing a global cache and using
> the resulting Map to log the entries ejected from the cache at that point in
> time.  ...  Which is why it "might" be useful to have a drainTo specified in
> the interface.

Well, I'm afraid I'm with Tim that it's probably not worth adding to API.
If there were such a thing as a BlockingNavigableMap
(i.e., that extended ConcurrentNavigableMap to support
methods like takeFirst), then drainTo should
surely be in such an API. But no one has asked us for such
a thing yet.

(BTW, everyone should congratulate Jason for winning the Sun Mustang
Regression contest by pointing out failure to maintain interrupt
conventions in our SynchronousQueue update!)

-Doug
