From dl at cs.oswego.edu  Mon Apr  5 12:15:02 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 05 Apr 2010 12:15:02 -0400
Subject: [concurrency-interest] ForkJoin updates
Message-ID: <4BBA0C86.3090309@cs.oswego.edu>


New versions of internals for ForkJoin{Task, Pool, WorkerThread} are
now available in jsr166y as well and proto-java.util.concurrent.
You can run these (on Java 6 or jdk7 builds) by using new jars.
You'll probably see some performance improvement. Please let
me know if otherwise. After giving them some time to settle,
we will update the openjdk7 versions.
Here are the usual links:

jsr166y:
* API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
* jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166y.jar (compiled using 
Java6 javac).
* Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166y/

openjdk7-ready java.util.concurrent:
* API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/docs/
* jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
* Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/

-Doug


From ashwin.jayaprakash at gmail.com  Mon Apr  5 18:47:18 2010
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Mon, 5 Apr 2010 15:47:18 -0700
Subject: [concurrency-interest] Immutable Entry objects in j.u.TreeMap
Message-ID: <r2u837c23d41004051547w1907a294la21352e26a8b0370@mail.gmail.com>

I was looking at the JDK 6 java.util.TreeMap source code to see if any of
the xxEntry methods were returning actual Entry objects or immutable
snapshots like in j.u.c.ConcurrentSkipMap. I see that the return values in
TreeMap are confusing. I hope someone here can throw some light on this:

java.util.TreeMap:

1) entrySet, headMap, tailMap - seem to return Map.Entry references that are
still referred to by the map. There's no confusion here.

2) firstEntry, lastEntry, lowerEntry, floorEntry...ceilingEntry - these wrap
the actual Map.Entry objects with exportEntry(). So, these are actually
returning AbstractMap.SimpleImmutableEntry instances. Why the difference?
Why isn't setValue(xx) not allowed on these entries?

Ashwin.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100405/1487e2b8/attachment.html>

From neal at gafter.com  Mon Apr  5 19:10:10 2010
From: neal at gafter.com (Neal Gafter)
Date: Mon, 5 Apr 2010 16:10:10 -0700
Subject: [concurrency-interest] Immutable Entry objects in j.u.TreeMap
In-Reply-To: <r2u837c23d41004051547w1907a294la21352e26a8b0370@mail.gmail.com>
References: <r2u837c23d41004051547w1907a294la21352e26a8b0370@mail.gmail.com>
Message-ID: <o2k15e8b9d21004051610gbc073f0ekfdb651164e1ef0a3@mail.gmail.com>

On Mon, Apr 5, 2010 at 3:47 PM, Ashwin Jayaprakash <
ashwin.jayaprakash at gmail.com> wrote:

> I was looking at the JDK 6 java.util.TreeMap source code to see if any of
> the xxEntry methods were returning actual Entry objects or immutable
> snapshots like in j.u.c.ConcurrentSkipMap. I see that the return values in
> TreeMap are confusing. I hope someone here can throw some light on this:
>
> java.util.TreeMap:
>
> 1) entrySet, headMap, tailMap - seem to return Map.Entry references that
> are still referred to by the map. There's no confusion here.
>
> 2) firstEntry, lastEntry, lowerEntry, floorEntry...ceilingEntry - these
> wrap the actual Map.Entry objects with exportEntry(). So, these are actually
> returning AbstractMap.SimpleImmutableEntry instances. Why the difference?
> Why isn't setValue(xx) not allowed on these entries?


The documentation for Map.Entry sheds some light on this, even though
Entry's documentation isn't correct.  Apparently Map.Entry objects are only
supposed to be valid for the duration of that one iteration, but firstEntry,
lastEntry, lowerEntry, floorEntry, and ceilingEntry aren't associated with
an iteration.

I mentioned that the documentation for Map.Entry isn't correct.  I quote:

A map entry (key-value pair). The Map.entrySet method returns a
collection-view of the map, whose elements are of this class. The *only* way
to obtain a reference to a map entry is from the iterator of this
collection-view.

Clearly, methods like firstEntry et al violate this.

Cheers,
Neal
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100405/22f1a4c4/attachment.html>

From ashwin.jayaprakash at gmail.com  Mon Apr  5 19:33:19 2010
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Mon, 5 Apr 2010 16:33:19 -0700
Subject: [concurrency-interest] Immutable Entry objects in j.u.TreeMap
In-Reply-To: <o2k15e8b9d21004051610gbc073f0ekfdb651164e1ef0a3@mail.gmail.com>
References: <r2u837c23d41004051547w1907a294la21352e26a8b0370@mail.gmail.com> 
	<o2k15e8b9d21004051610gbc073f0ekfdb651164e1ef0a3@mail.gmail.com>
Message-ID: <l2g837c23d41004051633t26ed439akc2fece22135203c2@mail.gmail.com>

Hmmm..there are 3 problems here:

1) Map.Entry is implemented by SimpleImmutableEntry and it violates the
Liskov substitution principle. I guess that can be forgiven

2) firstEntry, lastEntry etc keep creating tiny objects for every call. This
is a problem, don't you think?

3) Just because it was documented as "*The only way to obtain a reference to
a map entry is from the iterator of this collection-view. *" should we fit
the code around the docs? Instead of the other way around?

Thanks for replying.

Ashwin.


On Mon, Apr 5, 2010 at 4:10 PM, Neal Gafter <neal at gafter.com> wrote:

> On Mon, Apr 5, 2010 at 3:47 PM, Ashwin Jayaprakash <
> ashwin.jayaprakash at gmail.com> wrote:
>
>> I was looking at the JDK 6 java.util.TreeMap source code to see if any of
>> the xxEntry methods were returning actual Entry objects or immutable
>> snapshots like in j.u.c.ConcurrentSkipMap. I see that the return values in
>> TreeMap are confusing. I hope someone here can throw some light on this:
>>
>> java.util.TreeMap:
>>
>> 1) entrySet, headMap, tailMap - seem to return Map.Entry references that
>> are still referred to by the map. There's no confusion here.
>>
>> 2) firstEntry, lastEntry, lowerEntry, floorEntry...ceilingEntry - these
>> wrap the actual Map.Entry objects with exportEntry(). So, these are actually
>> returning AbstractMap.SimpleImmutableEntry instances. Why the difference?
>> Why isn't setValue(xx) not allowed on these entries?
>
>
> The documentation for Map.Entry sheds some light on this, even though
> Entry's documentation isn't correct.  Apparently Map.Entry objects are only
> supposed to be valid for the duration of that one iteration, but firstEntry,
> lastEntry, lowerEntry, floorEntry, and ceilingEntry aren't associated with
> an iteration.
>
> I mentioned that the documentation for Map.Entry isn't correct.  I quote:
>
> A map entry (key-value pair). The Map.entrySet method returns a
> collection-view of the map, whose elements are of this class. The *only*way to obtain a reference to a map entry is from the iterator of this
> collection-view.
>
> Clearly, methods like firstEntry et al violate this.
>
> Cheers,
> Neal
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100405/56b0a50c/attachment.html>

From cdracm at mail.ru  Tue Apr  6 03:51:49 2010
From: cdracm at mail.ru (Alexey Kudravtsev)
Date: Tue, 6 Apr 2010 11:51:49 +0400
Subject: [concurrency-interest] Thread exhaustion
Message-ID: <FAFD41728A74443D9976C518A77EC153@Labs.IntelliJ.Net>

Hello,
I am now confronting the simple task of mass parallelism that should have been a perfect fit for FJ.
I have many (millions) elements of some sort  and relatively many (thousands) processors.
Each processor should process each element, independently.
So I wrote the program to simulate this:
-----------------------------------
public class Test {
    public static void main(String[] args) {
        final ForkJoinPool pool = new ForkJoinPool();
        pool.setMaximumPoolSize(Runtime.getRuntime().availableProcessors());

        ParallelArray<ParallelArray<Object>> matrix = ParallelArray.create(1000, ParallelArray.class, pool);
        // initialize
        matrix.replaceWithGeneratedValue(new Ops.Generator<ParallelArray<Object>>() {
            public ParallelArray<Object> op() {
                return ParallelArray.create(10000, Object.class, pool);
            }
        });
        // simulate processing: for each of 1000 processors concurrently process each of 10000 elements 
        matrix.apply(new Ops.Procedure<ParallelArray<Object>>() {
            public void op(ParallelArray<Object> elems) {
                // my workflow requires me to say processor.started() here
                elems.apply(new Ops.Procedure<Object>() {
                    public void op(Object elem) {
                        //apply processor to elem
                    }
                });
                // my workflow requires me to say processor.finished() here
            }
        });
    }
}
-----------------------------------------

It deadblocks immediately upon execution because of thread exhaustion (see http://gafter.blogspot.com/2006/11/thread-pool-puzzler.html
for details. Basically, each ParallelArray.apply() task calls join() which blocks the current thread until there are no threads left)
If I comment out pool.setMaximumPoolSize() line, it kinda works, but generate excessive number of 'spare' threads.
The snippet above allocated 64 threads on my machine, and my real data sets caused allocation of hundreds(!) spare threads, most of which do nothing just waiting. 

So I figured that replacing join() in ParallelArray.apply() with helpJoin() could help.
Unfortunately, the program still deadlocks even then. As far as I understand, the problem is that helpJoin() tries to steal tasks from other worker thread queue, and,
failing that, blocks hard, waiting for completion of its own task. There is a possibility that all worker thread queues went empty for a moment, and in this instant the helpJoin() will block its current thread.

What I need is to modify helpJoin() behaviour, or maybe introduce another method (named cinderellaHelpJoin() because she was not afraid of hard work) which does following:
- tries to execute tasks from local worker thread queue (helpJoin() does that)
- if can't, tries to steal tasks from other worker thread queues (helpJoin() does that)
- if can't, pop tasks off the ForkJoinPool submission queue (helpJoin() does not do that currently)
- if can't, wait for one of the queues above become nonempty or its own task' isDone(). (helpJoin() does not do that currently)

What do you think?

Alexey Kudravtsev

P.S. I think the fact that ParallelArray tasks use join() instead of helpJoin() or equivalent makes them not very useful.
All of the ParallelArray tasks are independent  and therefore are perfectly eligible for helpJoining each other.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100406/185ec7d5/attachment-0001.html>

From dl at cs.oswego.edu  Tue Apr  6 06:49:27 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 06 Apr 2010 06:49:27 -0400
Subject: [concurrency-interest] Thread exhaustion
In-Reply-To: <FAFD41728A74443D9976C518A77EC153@Labs.IntelliJ.Net>
References: <FAFD41728A74443D9976C518A77EC153@Labs.IntelliJ.Net>
Message-ID: <4BBB11B7.3000100@cs.oswego.edu>

On 04/06/10 03:51, Alexey Kudravtsev wrote:
> I am now confronting the simple task of mass parallelism that should
> have been a perfect fit for FJ.
> So I wrote the program to simulate this:

Could you send me (off-list) the full version of this so
I can diagnose?

>  Basically, each ParallelArray.apply() task calls join()
> which blocks the current thread until there are no threads left)
> If I comment out pool.setMaximumPoolSize() line, it kinda works, but
> generate excessive number of 'spare' threads.

(It is rarely a good idea to setMaximumSize in FJ.)

> The snippet above allocated 64 threads on my machine, and my real data
> sets caused allocation of hundreds(!) spare threads, most of which do
> nothing just waiting.
> So I figured that replacing join() in ParallelArray.apply() with
> helpJoin() could help.
> Unfortunately, the program still deadlocks even then. As far as I
> understand, the problem is that helpJoin() tries to steal tasks from
> other worker thread queue, and,
> failing that, blocks hard, waiting for completion of its own task. There
> is a possibility that all worker thread queues went empty for a moment,
> and in this instant the helpJoin() will block its current thread.

Could you please try both join- and helpJoin- based versions
using the jsr166y update (from Apr 5+)? It includes improved spare
thread throttling for join and also an improved form of helpJoin
that falls back to thread creation if needed.


> P.S. I think the fact that ParallelArray tasks use join() instead of
> helpJoin() or equivalent makes them not very useful.
> All of the ParallelArray tasks are independent and therefore are
> perfectly eligible for helpJoining each other.
>

ParallelArray cannot be sure that a computation does
not itself start another independent FJ computation,
that would require join, not helpJoin to work well.
It may be possible to adapt ParallelArray methods to be
a little smarter about this though.

-Doug


From dl at cs.oswego.edu  Tue Apr  6 06:58:42 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 06 Apr 2010 06:58:42 -0400
Subject: [concurrency-interest] Immutable Entry objects in j.u.TreeMap
In-Reply-To: <l2g837c23d41004051633t26ed439akc2fece22135203c2@mail.gmail.com>
References: <r2u837c23d41004051547w1907a294la21352e26a8b0370@mail.gmail.com>
	<o2k15e8b9d21004051610gbc073f0ekfdb651164e1ef0a3@mail.gmail.com>
	<l2g837c23d41004051633t26ed439akc2fece22135203c2@mail.gmail.com>
Message-ID: <4BBB13E2.5070700@cs.oswego.edu>

On 04/05/10 19:33, Ashwin Jayaprakash wrote:

> 2) firstEntry, lastEntry etc keep creating tiny objects for every call.
> This is a problem, don't you think?

It would be a bigger problem to return internal nodes that could
be modified from outside the map.
Note that if want to trade time for space, you can
call first, last etc instead of firstEntry lastEntry, etc, and
then call map.get of the returned key when you need value.

-Doug



From dl at cs.oswego.edu  Tue Apr  6 10:51:47 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 06 Apr 2010 10:51:47 -0400
Subject: [concurrency-interest] Immutable Entry objects in j.u.TreeMap
In-Reply-To: <v2g51a29d951004060431me1f9aef4ke1de54371109fa30@mail.gmail.com>
References: <r2u837c23d41004051547w1907a294la21352e26a8b0370@mail.gmail.com>	
	<o2k15e8b9d21004051610gbc073f0ekfdb651164e1ef0a3@mail.gmail.com>	
	<l2g837c23d41004051633t26ed439akc2fece22135203c2@mail.gmail.com>	
	<4BBB13E2.5070700@cs.oswego.edu>
	<v2g51a29d951004060431me1f9aef4ke1de54371109fa30@mail.gmail.com>
Message-ID: <4BBB4A83.6070602@cs.oswego.edu>

On 04/06/10 07:31, Benedict Elliott Smith wrote:
> Hi - don't think I've posted to this list before now...
>
> Why would it be a bigger problem to return an "internal" node? All
> internal state is private to the class, so the only accessor a client
> has is the setValue() method which cannot break the integrity of the
> map. Besides which, as mentioned, the "internal" node can be grabbed
> from the entrySet() so if this is a good argument for avoiding returning
> it (which I don't think it is), it is being broken elsewhere anyway.
>
>

I agree that the inconsistency here is hard to defend except on
historical grounds (Collection APIs had to accommodate Java 1.1
classes), but the specs for NavigableMap say...

Implementations of entry-returning methods are expected to return Map.Entry 
pairs representing snapshots of mappings at the time they were produced, and 
thus generally do not  support the optional Entry.setValue method. Note however 
that it is possible to change mappings in the associated map using method put.

(http://java.sun.com/javase/6/docs/api/java/util/NavigableMap.html)

-Doug

From dl at cs.oswego.edu  Wed Apr  7 07:09:05 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 07 Apr 2010 07:09:05 -0400
Subject: [concurrency-interest] Thread exhaustion
In-Reply-To: <4BBB11B7.3000100@cs.oswego.edu>
References: <FAFD41728A74443D9976C518A77EC153@Labs.IntelliJ.Net>
	<4BBB11B7.3000100@cs.oswego.edu>
Message-ID: <4BBC67D1.3000308@cs.oswego.edu>

Bringing this exchange back to list...

On 04/06/10 06:49, Doug Lea wrote:
> On 04/06/10 03:51, Alexey Kudravtsev wrote:
>> I am now confronting the simple task of mass parallelism that should
>> have been a perfect fit for FJ.
>> So I wrote the program to simulate this:
>
> Could you send me (off-list) the full version of this so
> I can diagnose?

[Which now works after fixing a usage error. but a few
general issues remain:]

>
>> Basically, each ParallelArray.apply() task calls join()
>> which blocks the current thread until there are no threads left)
>> If I comment out pool.setMaximumPoolSize() line, it kinda works, but
>> generate excessive number of 'spare' threads.
>
> (It is rarely a good idea to setMaximumSize in FJ.)

But FJ could cope with this a little better than it does now,
by automatically changing join strategies when max threads are
reached. I'll look into this.

>
>> P.S. I think the fact that ParallelArray tasks use join() instead of
>> helpJoin() or equivalent makes them not very useful.
>> All of the ParallelArray tasks are independent and therefore are
>> perfectly eligible for helpJoining each other.
>>
>
> ParallelArray cannot be sure that a computation does
> not itself start another independent FJ computation,
> that would require join, not helpJoin to work well.
> It may be possible to adapt ParallelArray methods to be
> a little smarter about this though.
>

The main issue here (generally known as "nested parallelism")
is that when each operation of one parallel computation
itself spawns another distinct independent parallel computation,
then it is possible to require a peak usage of O(p-squared)
total threads (where p is parallelism level) in join() version,
or to "bury" O(p-squared) total tasks in the runtime stacks of
the p threads in helpJoin() version. (Or some in-between variant
of these.) In practice, the helpJoin() versions normally need
significantly fewer total resources but in the worst case
will need more and be slower. When arranging FJ computations
manually, you can often avoid worst cases by using helpJoin()
for outer parallelism and join() for inner. If/when lambdas/closures
become available and we reconsider frameworks like ParallelArray,
it be a good idea to explore automating such mechanics.

-Doug

From mboard182 at gmail.com  Wed Apr  7 13:11:19 2010
From: mboard182 at gmail.com (Mike)
Date: Wed, 7 Apr 2010 10:11:19 -0700
Subject: [concurrency-interest] question/bug in JCiP: Listing 4.12 ?
Message-ID: <h2va57a99231004071011yb60296f8j91785d8a1e927a65@mail.gmail.com>

Hi,

In working through Java Concurrency in Practice, I have struggled to
understand a detail of Listing 4.12.  After researching it, I am wondering
if I've found a problem.

Specifically, it seams that the 'locations' map should not require
internal synchronization, since it is accessed only by readers.

This is an 'overkill' bug, not a true flaw, but I feel it is important as I
am trying to understand exactly what the minimal synchronization approach is
to provide the class' @ThreadSafe guarantee.

Note: everyone has slightly different terminology, I've attempted to quote
"" phrases I'm defining per JCiP.  Also, I've included relevant code at the
end.

-PublishingVehicleTracker ('PVT' in my text below) is properly constructed
(no escaped this, etc)
-after construction its state cannot be modified, so PVT is logically
immutable.  ("effectively immutable")
--PVT's state is defined as the keys and *handles* to SafePoints only
--while the map itself is obviously mutable, it is only published through an
immutable wrapper
-access to the state of the SafePoint instances in the Map is handled via
"delegation"
-PVT's design (as writen, at least) imposes no invariants or restrictions on
valid state transitions for SafePoint instances

Therefore, after client code "safely publishes" an instance of PVT, it seams
to me that access to its underlying Map is threadsafe even if 'locations'
were just a HashMap.

It seems obvious that since access to 'locations' is read-only, there are no
race conditions  --  which leaves memory visibility as the issue.   think
the root of my confusion here is precisely how is visibility guaranteed.  Is
it from the client of PVT safely publishing the PVT instance?

It occurs to me that I am also relying on the assumption that the underlying
hashtable is deterministic and 'un'-lazy in its internals - technically I
guess there could exist an implementation that tunes itself,
possibly resizing, as it observes usage over time.  Without a syncronized
map in that case, races would be a possibility.

I tend first assume a problem with my understanding before a bug, so please
enlighten me here :)  Also, I have a pretty strong C++ background, with some
understanding of compilers, barriers, etc, if that helps your answers.

Thanks!  -mboard182


/////////////begin code

@ThreadSafe
public class PublishingVehicleTracker {

private final Map<String, SafePoint> locations;
private final Map<String, SafePoint> unmodifiableMap;

public PublishingVehicleTracker(
                        Map<String, SafePoint> locations) {

    this.locations
        = new ConcurrentHashMap<String, SafePoint>(locations);

    this.unmodifiableMap
        = Collections.unmodifiableMap(this.locations);
}

public Map<String, SafePoint> getLocations() {

    return unmodifiableMap;
}

public SafePoint getLocation(String id) {

    return locations.get(id);
}

public void setLocation(String id, int x, int y) {

    if (!locations.containsKey(id))

        throw new IllegalArgumentException(

            "invalid vehicle name: " + id);
    locations.get(id).set(x, y);

  }
}

// monitor protected helper-class

@ThreadSafe
public class SafePoint {

@GuardedBy("this") private int x, y;

private SafePoint(int[] a) { this(a[0], a[1]); }

public SafePoint(SafePoint p) { this(p.get()); }

public SafePoint(int x, int y) {

    this.x = x;
    this.y = y;
}

public synchronized int[] get() {

    return new int[] { x, y };
}

public synchronized void set(int x, int y) {

    this.x = x;
    this.y = y;
}

}

///////////end code
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100407/43990e98/attachment.html>

From tim at peierls.net  Wed Apr  7 14:14:39 2010
From: tim at peierls.net (Tim Peierls)
Date: Wed, 7 Apr 2010 14:14:39 -0400
Subject: [concurrency-interest] question/bug in JCiP: Listing 4.12 ?
In-Reply-To: <h2va57a99231004071011yb60296f8j91785d8a1e927a65@mail.gmail.com>
References: <h2va57a99231004071011yb60296f8j91785d8a1e927a65@mail.gmail.com>
Message-ID: <w2t63b4e4051004071114s49c4a06dr64c89c0518102be7@mail.gmail.com>

It would be OK to initialize the locations field with a HashMap rather than
a ConcurrentHashMap.

I can't remember why we did it the way we did, but it was probably to avoid
having to explain that HashMap is sufficient but fragile -- you can't
provide methods to add or remove vehicles, for example -- since that's not
the main point of the listing. But now that it has confused at least one
person, it should probably be a footnote.

--tim


On Wed, Apr 7, 2010 at 1:11 PM, Mike <mboard182 at gmail.com> wrote:

> Hi,
>
> In working through Java Concurrency in Practice, I have struggled to
> understand a detail of Listing 4.12.  After researching it, I am wondering
> if I've found a problem.
>
> Specifically, it seams that the 'locations' map should not require
> internal synchronization, since it is accessed only by readers.
>
> This is an 'overkill' bug, not a true flaw, but I feel it is important as I
> am trying to understand exactly what the minimal synchronization approach is
> to provide the class' @ThreadSafe guarantee.
>
> Note: everyone has slightly different terminology, I've attempted to quote
> "" phrases I'm defining per JCiP.  Also, I've included relevant code at the
> end.
>
>  -PublishingVehicleTracker ('PVT' in my text below) is properly constructed
> (no escaped this, etc)
> -after construction its state cannot be modified, so PVT is logically
> immutable.  ("effectively immutable")
> --PVT's state is defined as the keys and *handles* to SafePoints only
> --while the map itself is obviously mutable, it is only published through
> an immutable wrapper
> -access to the state of the SafePoint instances in the Map is handled via
> "delegation"
> -PVT's design (as writen, at least) imposes no invariants or restrictions
> on valid state transitions for SafePoint instances
>
> Therefore, after client code "safely publishes" an instance of PVT, it
> seams to me that access to its underlying Map is threadsafe even if
> 'locations' were just a HashMap.
>
> It seems obvious that since access to 'locations' is read-only, there are
> no race conditions  --  which leaves memory visibility as the issue.   think
> the root of my confusion here is precisely how is visibility guaranteed.  Is
> it from the client of PVT safely publishing the PVT instance?
>
> It occurs to me that I am also relying on the assumption that the
> underlying hashtable is deterministic and 'un'-lazy in its internals -
> technically I guess there could exist an implementation that tunes itself,
> possibly resizing, as it observes usage over time.  Without a syncronized
> map in that case, races would be a possibility.
>
> I tend first assume a problem with my understanding before a bug, so please
> enlighten me here :)  Also, I have a pretty strong C++ background, with some
> understanding of compilers, barriers, etc, if that helps your answers.
>
> Thanks!  -mboard182
>
>
> /////////////begin code
>
> @ThreadSafe
> public class PublishingVehicleTracker {
>
>
> private final Map<String, SafePoint> locations;
>
> private final Map<String, SafePoint> unmodifiableMap;
>
>
> public PublishingVehicleTracker(
>                         Map<String, SafePoint> locations) {
>
>
>     this.locations
>         = new ConcurrentHashMap<String, SafePoint>(locations);
>
>
>     this.unmodifiableMap
>         = Collections.unmodifiableMap(this.locations);
>
> }
>
> public Map<String, SafePoint> getLocations() {
>
>
>     return unmodifiableMap;
> }
>
> public SafePoint getLocation(String id) {
>
>
>     return locations.get(id);
>
> }
>
> public void setLocation(String id, int x, int y) {
>
>
>     if (!locations.containsKey(id))
>
>
>         throw new IllegalArgumentException(
>
>
>             "invalid vehicle name: " + id);
>     locations.get(id).set(x, y);
>
>
>   }
> }
>
> // monitor protected helper-class
>
> @ThreadSafe
> public class SafePoint {
>
>
> @GuardedBy("this") private int x, y;
>
>
> private SafePoint(int[] a) { this(a[0], a[1]); }
>
>
> public SafePoint(SafePoint p) { this(p.get()); }
>
>
> public SafePoint(int x, int y) {
>
>
>     this.x = x;
>     this.y = y;
>
> }
>
> public synchronized int[] get() {
>
>
>     return new int[] { x, y };
>
> }
>
> public synchronized void set(int x, int y) {
>
>
>     this.x = x;
>     this.y = y;
>
> }
>
> }
>
> ///////////end code
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100407/f84510d0/attachment-0001.html>

From joe.bowbeer at gmail.com  Wed Apr  7 14:26:38 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 7 Apr 2010 11:26:38 -0700
Subject: [concurrency-interest] question/bug in JCiP: Listing 4.12 ?
In-Reply-To: <h2va57a99231004071011yb60296f8j91785d8a1e927a65@mail.gmail.com>
References: <h2va57a99231004071011yb60296f8j91785d8a1e927a65@mail.gmail.com>
Message-ID: <z2z31f2a7bd1004071126ndfa073aaoab4da4a19600af03@mail.gmail.com>

To rephrase your question: Can the locations map in PVT be implemented as a
HashMap instead of a CHM, since it is read-only? I think the answer is yes.
But as we in the Java.utility.cocurrent business say: Why use a HashMap when
a ConcurrentHashMap will suffice?

On Apr 7, 2010 10:26 AM, "Mike" <mboard182 at gmail.com> wrote:

Hi,

In working through Java Concurrency in Practice, I have struggled to
understand a detail of Listing 4.12.  After researching it, I am wondering
if I've found a problem.

Specifically, it seams that the 'locations' map should not require
internal synchronization, since it is accessed only by readers.

This is an 'overkill' bug, not a true flaw, but I feel it is important as I
am trying to understand exactly what the minimal synchronization approach is
to provide the class' @ThreadSafe guarantee.

Note: everyone has slightly different terminology, I've attempted to quote
"" phrases I'm defining per JCiP.  Also, I've included relevant code at the
end.

 -PublishingVehicleTracker ('PVT' in my text below) is properly constructed
(no escaped this, etc)
-after construction its state cannot be modified, so PVT is logically
immutable.  ("effectively immutable")
--PVT's state is defined as the keys and *handles* to SafePoints only
--while the map itself is obviously mutable, it is only published through an
immutable wrapper
-access to the state of the SafePoint instances in the Map is handled via
"delegation"
-PVT's design (as writen, at least) imposes no invariants or restrictions on
valid state transitions for SafePoint instances

Therefore, after client code "safely publishes" an instance of PVT, it seams
to me that access to its underlying Map is threadsafe even if 'locations'
were just a HashMap.

It seems obvious that since access to 'locations' is read-only, there are no
race conditions  --  which leaves memory visibility as the issue.   think
the root of my confusion here is precisely how is visibility guaranteed.  Is
it from the client of PVT safely publishing the PVT instance?

It occurs to me that I am also relying on the assumption that the underlying
hashtable is deterministic and 'un'-lazy in its internals - technically I
guess there could exist an implementation that tunes itself,
possibly resizing, as it observes usage over time.  Without a syncronized
map in that case, races would be a possibility.

I tend first assume a problem with my understanding before a bug, so please
enlighten me here :)  Also, I have a pretty strong C++ background, with some
understanding of compilers, barriers, etc, if that helps your answers.

Thanks!  -mboard182


/////////////begin code

@ThreadSafe
public class PublishingVehicleTracker {


private final Map<String, SafePoint> locations;

private final Map<String, SafePoint> unmodifiableMap;


public PublishingVehicleTracker(
                        Map<String, SafePoint> locations) {


    this.locations
        = new ConcurrentHashMap<String, SafePoint>(locations);


    this.unmodifiableMap
        = Collections.unmodifiableMap(this.locations);

}

public Map<String, SafePoint> getLocations() {


    return unmodifiableMap;
}

public SafePoint getLocation(String id) {


    return locations.get(id);

}

public void setLocation(String id, int x, int y) {


    if (!locations.containsKey(id))


        throw new IllegalArgumentException(


            "invalid vehicle name: " + id);
    locations.get(id).set(x, y);


  }
}

// monitor protected helper-class

@ThreadSafe
public class SafePoint {


@GuardedBy("this") private int x, y;


private SafePoint(int[] a) { this(a[0], a[1]); }


public SafePoint(SafePoint p) { this(p.get()); }


public SafePoint(int x, int y) {


    this.x = x;
    this.y = y;

}

public synchronized int[] get() {


    return new int[] { x, y };

}

public synchronized void set(int x, int y) {


    this.x = x;
    this.y = y;

}

}

///////////end code

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100407/512684e0/attachment.html>

From joe.bowbeer at gmail.com  Wed Apr  7 14:28:49 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 7 Apr 2010 11:28:49 -0700
Subject: [concurrency-interest] question/bug in JCiP: Listing 4.12 ?
In-Reply-To: <z2z31f2a7bd1004071126ndfa073aaoab4da4a19600af03@mail.gmail.com>
References: <h2va57a99231004071011yb60296f8j91785d8a1e927a65@mail.gmail.com>
	<z2z31f2a7bd1004071126ndfa073aaoab4da4a19600af03@mail.gmail.com>
Message-ID: <i2t31f2a7bd1004071128qbd3296bel555f1cf18e3d9e1b@mail.gmail.com>

PS - the java.util folks have a different phrase...

On Apr 7, 2010 11:26 AM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:

To rephrase your question: Can the locations map in PVT be implemented as a
HashMap instead of a CHM, since it is read-only? I think the answer is yes.
But as we in the Java.utility.cocurrent business say: Why use a HashMap when
a ConcurrentHashMap will suffice?


>
> On Apr 7, 2010 10:26 AM, "Mike" <mboard182 at gmail.com> wrote:
>
> Hi,
>
> In working through Ja...
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100407/7dc7d1c3/attachment.html>

From mboard182 at gmail.com  Wed Apr  7 17:34:40 2010
From: mboard182 at gmail.com (Mike)
Date: Wed, 7 Apr 2010 14:34:40 -0700
Subject: [concurrency-interest] question/bug in JCiP: Listing 4.12 ?
In-Reply-To: <i2t31f2a7bd1004071128qbd3296bel555f1cf18e3d9e1b@mail.gmail.com>
References: <h2va57a99231004071011yb60296f8j91785d8a1e927a65@mail.gmail.com> 
	<z2z31f2a7bd1004071126ndfa073aaoab4da4a19600af03@mail.gmail.com> 
	<i2t31f2a7bd1004071128qbd3296bel555f1cf18e3d9e1b@mail.gmail.com>
Message-ID: <n2qa57a99231004071434h46e7b47cm14bb7fc1ec29ed4a@mail.gmail.com>

@Tim / @Joe - thanks for the (quite speedy...) reply.  And quickly,
fantastic book, thank you; I own it & have gained significantly in both my
java and design skills.

Good to hear I wasn't crazy.  @Joe, I would tend to agree with the "why that
when a safer one will do" sentiment - but many changes design improvements
would be required to support 'real-world' functionality, so IMHO here I feel
the example is muddled by the extraneous info.  And unless you fully
understand the ramifications of your code, you can easily write
complex sequential code accidentally...     Further, the sentence that
follows the listing ("PublishingVehicleTracker derives its thread safety
from delegation to an underlying ConcurrentHashMap") then seems to be
incorrect - doesn't the PVT "delegate" to the SafePoints, not the CHM?

So, as in my original post, it makes sense to me that a normal HashMap would
technically be OK here.

However, I am still uncertain with regards to visibility of the PVT's
internals.  Somewhere in the heap the jvm has put a bunch of bits for the
internals of the 'locations' map - String (key) and handle (value) pairs,
and others.

Is visibility not an issue here because the PVT meets all the criteria for
an immutable object?  (not just an "effectively immutable one)
-state cannot be modified (again, state is just key/value pairs here, which
aren't ever modified)
-all fields final
-properly constructed

Or if it is only "effectively immutable" (why?) then is visibility
guaranteed if client code "safely publishes" it (ala JCiP page 52)?

Thank you!
-mboard182


On Wed, Apr 7, 2010 at 11:28 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> PS - the java.util folks have a different phrase...
>
> On Apr 7, 2010 11:26 AM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:
>
> To rephrase your question: Can the locations map in PVT be implemented as a
> HashMap instead of a CHM, since it is read-only? I think the answer is yes.
> But as we in the Java.utility.cocurrent business say: Why use a HashMap when
> a ConcurrentHashMap will suffice?
>
>
> >
> > On Apr 7, 2010 10:26 AM, "Mike" <mboard182 at gmail.com> wrote:
> >
> > Hi,
> >
> > In working through Ja...
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100407/a75f61c9/attachment-0001.html>

From joe.bowbeer at gmail.com  Wed Apr  7 18:27:35 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 7 Apr 2010 15:27:35 -0700
Subject: [concurrency-interest] question/bug in JCiP: Listing 4.12 ?
In-Reply-To: <n2qa57a99231004071434h46e7b47cm14bb7fc1ec29ed4a@mail.gmail.com>
References: <h2va57a99231004071011yb60296f8j91785d8a1e927a65@mail.gmail.com>
	<z2z31f2a7bd1004071126ndfa073aaoab4da4a19600af03@mail.gmail.com>
	<i2t31f2a7bd1004071128qbd3296bel555f1cf18e3d9e1b@mail.gmail.com>
	<n2qa57a99231004071434h46e7b47cm14bb7fc1ec29ed4a@mail.gmail.com>
Message-ID: <g2u31f2a7bd1004071527n1023e7ccx1244dfe36cf62d5d@mail.gmail.com>

Safe publication of PVT is needed to guarantee visibility of the PVT
reference itself.

Regardless of safe publication, the immutable nature of the PVT data (e.g.
locations) guarantees that it is thread-safe.

See

http://jeremymanson.blogspot.com/2008/07/immutability-in-java-part-2.html
http://jeremymanson.blogspot.com/2008/04/immutability-in-java.html

Immutable in Java means "is transitively reachable from a final field, has
> not changed since the final field was set, and a reference to the object
> containing the final field did not escape the constructor".
>

Joe

On Wed, Apr 7, 2010 at 2:34 PM, Mike wrote:

> @Tim / @Joe - thanks for the (quite speedy...) reply.  And quickly,
> fantastic book, thank you; I own it & have gained significantly in both my
> java and design skills.
>
> Good to hear I wasn't crazy.  @Joe, I would tend to agree with the "why
> that when a safer one will do" sentiment - but many changes design
> improvements would be required to support 'real-world' functionality, so
> IMHO here I feel the example is muddled by the extraneous info.  And unless
> you fully understand the ramifications of your code, you can easily write
> complex sequential code accidentally...     Further, the sentence that
> follows the listing ("PublishingVehicleTracker derives its thread safety
> from delegation to an underlying ConcurrentHashMap") then seems to be
> incorrect - doesn't the PVT "delegate" to the SafePoints, not the CHM?
>
> So, as in my original post, it makes sense to me that a normal HashMap
> would technically be OK here.
>
> However, I am still uncertain with regards to visibility of the PVT's
> internals.  Somewhere in the heap the jvm has put a bunch of bits for the
> internals of the 'locations' map - String (key) and handle (value) pairs,
> and others.
>
> Is visibility not an issue here because the PVT meets all the criteria for
> an immutable object?  (not just an "effectively immutable one)
> -state cannot be modified (again, state is just key/value pairs here, which
> aren't ever modified)
> -all fields final
> -properly constructed
>
> Or if it is only "effectively immutable" (why?) then is visibility
> guaranteed if client code "safely publishes" it (ala JCiP page 52)?
>
> Thank you!
> -mboard182
>
>
> On Wed, Apr 7, 2010 at 11:28 AM, Joe Bowbeer wrote:
>
>> PS - the java.util folks have a different phrase...
>>
>> On Apr 7, 2010 11:26 AM, "Joe Bowbeer" wrote:
>>
>> To rephrase your question: Can the locations map in PVT be implemented as
>> a HashMap instead of a CHM, since it is read-only? I think the answer is
>> yes. But as we in the java.util.concurrent business say: Why use a HashMap
>> when a ConcurrentHashMap will suffice?
>>
>>
>> >
>> > On Apr 7, 2010 10:26 AM, "Mike" wrote:
>> >
>> > Hi,
>> >
>> > In working through Ja...
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100407/2295e9db/attachment.html>

From cpovirk at google.com  Thu Apr  8 12:46:01 2010
From: cpovirk at google.com (Chris Povirk)
Date: Thu, 8 Apr 2010 12:46:01 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for calls
	that "will never be interrupted"
In-Reply-To: <i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>
	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
Message-ID: <k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>

My goal is a family of operations like these:

// Neither of these throws InterruptedException.
V result = Futures.getUninterruptibly(future);
CountDownLatches.awaitUninterruptibly(latch);

These methods would retry the operation until it completes without
interruption and then restore the interrupt. ?(Another option is to
wrap the InterruptedException in a custom InterruptedRuntimeException
class; I'd be happy to hear people's opinions on this, as well.)

Google has implementations of some of these operations, and we're
planning to expand our coverage and release them in our open-source
Guava project. ?However, we're uncertain about a few issues.

Below I've listed the two main decisions we need to make along with a
number of pros and cons for each of their options.


Decision A: The interface to these uninterruptible methods could take
one of three forms:
1. Static Futures.getUninterruptibly methods.
2. A static uninterruptibleFuture method that returns an
UninterruptibleFuture, which adds getUninterruptibly methods to
Future.
3. A static uninterruptibleFuture method that returns an
UninterruptibleFuture, which overrides Future's get methods to no
longer throw InterruptedException.

(1) requires the fewest method declarations.

The method declarations from (1) are likely to be jumbled together
with unrelated declarations. ?A person who sees
BlockingQueues.putUninterruptibly, takeUninterruptibly,
offerUninterruptibly, and pollUninterruptibly may get the impression
that the BlockingQueues contains only InterruptedException-removing
methods, when perhaps it contains methods like takeMultiple(Queue<E>,
int). ?Plus, subclassing is awkward: does BlockingDeques provide a
static putUninterruptibly method, too? ?(An alternative is to put
methods for all interfaces in a single Uninterruptibles class.)

(1) is the best solution for classes with final methods (notably
Thread.join). ?Even (2) suffers from this problem:
joinUninterruptibly() would join the delegate thread, as desired, but
join() (which we can't override) would join the unstarted wrapper
thread. ?This is confusing. ?Luckily, Thread.join and other primitives
are used less frequently than they used to be, especially among the
kind of people that I hope will use the uninterruptible wrappers.
(Related: Developers who want to make their methods final can define
interfaces so that we don't need to override the class's methods.)

Declaring a variable or field to be an UninterruptibleFuture (possible
in (2) or (3)) can express our intention to use it that way to readers
and make the uninterruptible methods available in IDE autocompletion.

Methods can declare a return type of UninterruptibleFuture, making the
uninterruptible operations more convenient and bringing them to the
attention of more users.

(3) hides the original, interruptible methods. ?This is especially a
problem if a method has returned an UninterruptibleFuture to the user
for "convenience," as proposed above.

(2) is most consistent with the JDK's existing uninterruptible methods.

In principle, a Future implementation could implement
UninterruptibleFuture and some other Future subinterface (e.g., one
whose get methods do not throw ExecutionException). ?In practice, this
seems unlikely.

"UninterruptibleFuture" is a poor name. ?The name may suggest to some
that a *task* whose value the Future awaits may not be interrupted
through Future.cancel. ?But "Uninterruptible" refers only to the get
methods; it's more like "UninterruptiblyGettableFuture," which is an
ugly name.

"UninterruptibleFuture" is a particularly poor name in the case of
(2), since its get methods would still be interruptible. ?(This is
another problem that "UninterruptiblyGettableFuture" could solve.)


Decision B: There are two ways to handle the retry boilerplate:
1. Repeat it everywhere it's needed.
2. Delegate to a performUninterruptibly method that takes an
InterruptibleOperation parameter.
(<http://cs.oswego.edu/pipermail/concurrency-interest/2007-March/003775.html>)

(A colleague proposed a reflection-based alternative, but we found it
to have two problems: ?First, CountDownLatch isn't an interface, so we
can't extend it to create an UninterruptibleCountDownLatch interface
to pass to Proxy. ?Second, we can't handle timeouts without either
annotating the long+TimeUnit parameters or assuming that there's a
convention for them.)

(1) has the usual advantages and disadvantages of copy-and-paste programming.

If there's a significant performance difference, (1) is surely the winner.

(2) requires three variants of InterruptibleOperation: one for
operations without timeout, one for operations that time out by
returning a sentinel value, and one for operations that time out by
throwing TimeoutException. ?(We might be able to combine the last two
with suitable generics, but my claim of "three variants" already
presumes some awkward parameterization to handle exceptions.)


I'm currently leaning toward 2-1
(UninterruptibleFuture.getUninterruptibly, repeat the boilerplate),
but some smart people disagree. ?Opinions?


From joe.bowbeer at gmail.com  Thu Apr  8 13:57:18 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 8 Apr 2010 10:57:18 -0700
Subject: [concurrency-interest] InterruptedException-free wrappers for
	calls that "will never be interrupted"
In-Reply-To: <k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>
	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
Message-ID: <u2v31f2a7bd1004081057g5dbcb260gff2e9a0ea328d000@mail.gmail.com>

I think the proper choice depends on the intent:

Are you expecting the thread waiting on the future to be interrupted (but
want to keep waiting regardless), or are you trying to hide the interrupted
exception in your API (because the waiting thread will never be
interrupted)?

Based on the subject of this message, I assume the waiting thread will never
be interrupted.

If so, then rethrowing IE as a RuntimeException (e.g. IllegalStateException)
seems like the right thing to do.  Ideally, this policy would be designed
into your higher-level API.  Of the options you list at the system level, I
prefer a static tool class method, such as Futures.getUninterruptibly.

The former (waiting regardless and then reasserting) is not responsive to
interrupts.  This seems more specialized, and would not be applicable to
most Futures in practice.

Note that if you convert the future to send a result-bearing event when its
done, then get isn't needed.  Moving the problem somewhere else...

Joe

On Thu, Apr 8, 2010 at 9:46 AM, Chris Povirk <cpovirk at google.com> wrote:

> My goal is a family of operations like these:
>
> // Neither of these throws InterruptedException.
> V result = Futures.getUninterruptibly(future);
> CountDownLatches.awaitUninterruptibly(latch);
>
> These methods would retry the operation until it completes without
> interruption and then restore the interrupt.  (Another option is to
> wrap the InterruptedException in a custom InterruptedRuntimeException
> class; I'd be happy to hear people's opinions on this, as well.)
>
> Google has implementations of some of these operations, and we're
> planning to expand our coverage and release them in our open-source
> Guava project.  However, we're uncertain about a few issues.
>
> Below I've listed the two main decisions we need to make along with a
> number of pros and cons for each of their options.
>
>
> Decision A: The interface to these uninterruptible methods could take
> one of three forms:
> 1. Static Futures.getUninterruptibly methods.
> 2. A static uninterruptibleFuture method that returns an
> UninterruptibleFuture, which adds getUninterruptibly methods to
> Future.
> 3. A static uninterruptibleFuture method that returns an
> UninterruptibleFuture, which overrides Future's get methods to no
> longer throw InterruptedException.
>
> (1) requires the fewest method declarations.
>
> The method declarations from (1) are likely to be jumbled together
> with unrelated declarations.  A person who sees
> BlockingQueues.putUninterruptibly, takeUninterruptibly,
> offerUninterruptibly, and pollUninterruptibly may get the impression
> that the BlockingQueues contains only InterruptedException-removing
> methods, when perhaps it contains methods like takeMultiple(Queue<E>,
> int).  Plus, subclassing is awkward: does BlockingDeques provide a
> static putUninterruptibly method, too?  (An alternative is to put
> methods for all interfaces in a single Uninterruptibles class.)
>
> (1) is the best solution for classes with final methods (notably
> Thread.join).  Even (2) suffers from this problem:
> joinUninterruptibly() would join the delegate thread, as desired, but
> join() (which we can't override) would join the unstarted wrapper
> thread.  This is confusing.  Luckily, Thread.join and other primitives
> are used less frequently than they used to be, especially among the
> kind of people that I hope will use the uninterruptible wrappers.
> (Related: Developers who want to make their methods final can define
> interfaces so that we don't need to override the class's methods.)
>
> Declaring a variable or field to be an UninterruptibleFuture (possible
> in (2) or (3)) can express our intention to use it that way to readers
> and make the uninterruptible methods available in IDE autocompletion.
>
> Methods can declare a return type of UninterruptibleFuture, making the
> uninterruptible operations more convenient and bringing them to the
> attention of more users.
>
> (3) hides the original, interruptible methods.  This is especially a
> problem if a method has returned an UninterruptibleFuture to the user
> for "convenience," as proposed above.
>
> (2) is most consistent with the JDK's existing uninterruptible methods.
>
> In principle, a Future implementation could implement
> UninterruptibleFuture and some other Future subinterface (e.g., one
> whose get methods do not throw ExecutionException).  In practice, this
> seems unlikely.
>
> "UninterruptibleFuture" is a poor name.  The name may suggest to some
> that a *task* whose value the Future awaits may not be interrupted
> through Future.cancel.  But "Uninterruptible" refers only to the get
> methods; it's more like "UninterruptiblyGettableFuture," which is an
> ugly name.
>
> "UninterruptibleFuture" is a particularly poor name in the case of
> (2), since its get methods would still be interruptible.  (This is
> another problem that "UninterruptiblyGettableFuture" could solve.)
>
>
> Decision B: There are two ways to handle the retry boilerplate:
> 1. Repeat it everywhere it's needed.
> 2. Delegate to a performUninterruptibly method that takes an
> InterruptibleOperation parameter.
> (<
> http://cs.oswego.edu/pipermail/concurrency-interest/2007-March/003775.html
> >)
>
> (A colleague proposed a reflection-based alternative, but we found it
> to have two problems:  First, CountDownLatch isn't an interface, so we
> can't extend it to create an UninterruptibleCountDownLatch interface
> to pass to Proxy.  Second, we can't handle timeouts without either
> annotating the long+TimeUnit parameters or assuming that there's a
> convention for them.)
>
> (1) has the usual advantages and disadvantages of copy-and-paste
> programming.
>
> If there's a significant performance difference, (1) is surely the winner.
>
> (2) requires three variants of InterruptibleOperation: one for
> operations without timeout, one for operations that time out by
> returning a sentinel value, and one for operations that time out by
> throwing TimeoutException.  (We might be able to combine the last two
> with suitable generics, but my claim of "three variants" already
> presumes some awkward parameterization to handle exceptions.)
>
>
> I'm currently leaning toward 2-1
> (UninterruptibleFuture.getUninterruptibly, repeat the boilerplate),
> but some smart people disagree.  Opinions?
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100408/e6b740a8/attachment.html>

From jnewsham at referentia-test.com  Thu Apr  8 20:32:11 2010
From: jnewsham at referentia-test.com (Jim Newsham)
Date: Thu, 08 Apr 2010 14:32:11 -1000
Subject: [concurrency-interest] InterruptedException-free wrappers for
 calls that "will never be interrupted"
In-Reply-To: <k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
Message-ID: <4BBE758B.7090502@referentia-test.com>


Regarding decision A.  We have a collection of static 
XXXUninterruptibly() methods in a single utility class.  This is like 
your approach #1, except they are all in the same class (ours is called 
ThreadUtils and has some other thread-related methods, but I've 
sometimes considered breaking them out into a separate class which 
contained only XXXUninterruptibly()...  maybe 
UninterruptibleUtils.get(Future) or Uninterruptibly.get(Future)?).  We 
didn't really consider (or have a need for) the wrapper approach -- it's 
really the invoker who decides whether to mask interruption.

Regarding decision B.  In our case, the code for each method is 
repeated, but it's really not much code at all so I have no issue with it.

Jim

On 4/8/2010 6:46 AM, Chris Povirk wrote:
> My goal is a family of operations like these:
>
> // Neither of these throws InterruptedException.
> V result = Futures.getUninterruptibly(future);
> CountDownLatches.awaitUninterruptibly(latch);
>
> These methods would retry the operation until it completes without
> interruption and then restore the interrupt.  (Another option is to
> wrap the InterruptedException in a custom InterruptedRuntimeException
> class; I'd be happy to hear people's opinions on this, as well.)
>
> Google has implementations of some of these operations, and we're
> planning to expand our coverage and release them in our open-source
> Guava project.  However, we're uncertain about a few issues.
>
> Below I've listed the two main decisions we need to make along with a
> number of pros and cons for each of their options.
>
>
> Decision A: The interface to these uninterruptible methods could take
> one of three forms:
> 1. Static Futures.getUninterruptibly methods.
> 2. A static uninterruptibleFuture method that returns an
> UninterruptibleFuture, which adds getUninterruptibly methods to
> Future.
> 3. A static uninterruptibleFuture method that returns an
> UninterruptibleFuture, which overrides Future's get methods to no
> longer throw InterruptedException.
>
> (1) requires the fewest method declarations.
>
> The method declarations from (1) are likely to be jumbled together
> with unrelated declarations.  A person who sees
> BlockingQueues.putUninterruptibly, takeUninterruptibly,
> offerUninterruptibly, and pollUninterruptibly may get the impression
> that the BlockingQueues contains only InterruptedException-removing
> methods, when perhaps it contains methods like takeMultiple(Queue<E>,
> int).  Plus, subclassing is awkward: does BlockingDeques provide a
> static putUninterruptibly method, too?  (An alternative is to put
> methods for all interfaces in a single Uninterruptibles class.)
>
> (1) is the best solution for classes with final methods (notably
> Thread.join).  Even (2) suffers from this problem:
> joinUninterruptibly() would join the delegate thread, as desired, but
> join() (which we can't override) would join the unstarted wrapper
> thread.  This is confusing.  Luckily, Thread.join and other primitives
> are used less frequently than they used to be, especially among the
> kind of people that I hope will use the uninterruptible wrappers.
> (Related: Developers who want to make their methods final can define
> interfaces so that we don't need to override the class's methods.)
>
> Declaring a variable or field to be an UninterruptibleFuture (possible
> in (2) or (3)) can express our intention to use it that way to readers
> and make the uninterruptible methods available in IDE autocompletion.
>
> Methods can declare a return type of UninterruptibleFuture, making the
> uninterruptible operations more convenient and bringing them to the
> attention of more users.
>
> (3) hides the original, interruptible methods.  This is especially a
> problem if a method has returned an UninterruptibleFuture to the user
> for "convenience," as proposed above.
>
> (2) is most consistent with the JDK's existing uninterruptible methods.
>
> In principle, a Future implementation could implement
> UninterruptibleFuture and some other Future subinterface (e.g., one
> whose get methods do not throw ExecutionException).  In practice, this
> seems unlikely.
>
> "UninterruptibleFuture" is a poor name.  The name may suggest to some
> that a *task* whose value the Future awaits may not be interrupted
> through Future.cancel.  But "Uninterruptible" refers only to the get
> methods; it's more like "UninterruptiblyGettableFuture," which is an
> ugly name.
>
> "UninterruptibleFuture" is a particularly poor name in the case of
> (2), since its get methods would still be interruptible.  (This is
> another problem that "UninterruptiblyGettableFuture" could solve.)
>
>
> Decision B: There are two ways to handle the retry boilerplate:
> 1. Repeat it everywhere it's needed.
> 2. Delegate to a performUninterruptibly method that takes an
> InterruptibleOperation parameter.
> (<http://cs.oswego.edu/pipermail/concurrency-interest/2007-March/003775.html>)
>
> (A colleague proposed a reflection-based alternative, but we found it
> to have two problems:  First, CountDownLatch isn't an interface, so we
> can't extend it to create an UninterruptibleCountDownLatch interface
> to pass to Proxy.  Second, we can't handle timeouts without either
> annotating the long+TimeUnit parameters or assuming that there's a
> convention for them.)
>
> (1) has the usual advantages and disadvantages of copy-and-paste programming.
>
> If there's a significant performance difference, (1) is surely the winner.
>
> (2) requires three variants of InterruptibleOperation: one for
> operations without timeout, one for operations that time out by
> returning a sentinel value, and one for operations that time out by
> throwing TimeoutException.  (We might be able to combine the last two
> with suitable generics, but my claim of "three variants" already
> presumes some awkward parameterization to handle exceptions.)
>
>
> I'm currently leaning toward 2-1
> (UninterruptibleFuture.getUninterruptibly, repeat the boilerplate),
> but some smart people disagree.  Opinions?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>    


From jnewsham at referentia-test.com  Thu Apr  8 20:37:42 2010
From: jnewsham at referentia-test.com (Jim Newsham)
Date: Thu, 08 Apr 2010 14:37:42 -1000
Subject: [concurrency-interest] InterruptedException-free wrappers for
 calls that "will never be interrupted"
In-Reply-To: <u2v31f2a7bd1004081057g5dbcb260gff2e9a0ea328d000@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
	<u2v31f2a7bd1004081057g5dbcb260gff2e9a0ea328d000@mail.gmail.com>
Message-ID: <4BBE76D6.2060803@referentia-test.com>

On 4/8/2010 7:57 AM, Joe Bowbeer wrote:
> I think the proper choice depends on the intent:
>
> Are you expecting the thread waiting on the future to be interrupted 
> (but want to keep waiting regardless), or are you trying to hide the 
> interrupted exception in your API (because the waiting thread will 
> never be interrupted)?
>

I can't respond for the OP, but for our case it's "both".  We don't 
expect/want the thread to be interrupted (thus no need for 
InterruptedException), but in case it is, we don't want the get() to 
terminate.  Essentially, we don't support interruption and so we want to 
ignore it and not handle the exception (for the purposes of the 
Future.get(); of course we're reasserting so that if some later call 
supports interruption it will respond).

Jim

> Based on the subject of this message, I assume the waiting thread will 
> never be interrupted.
>
> If so, then rethrowing IE as a RuntimeException (e.g. 
> IllegalStateException) seems like the right thing to do.  Ideally, 
> this policy would be designed into your higher-level API.  Of the 
> options you list at the system level, I prefer a static tool class 
> method, such as Futures.getUninterruptibly.
>
> The former (waiting regardless and then reasserting) is not responsive 
> to interrupts.  This seems more specialized, and would not be 
> applicable to most Futures in practice.
>
> Note that if you convert the future to send a result-bearing event 
> when its done, then get isn't needed.  Moving the problem somewhere 
> else...
>
> Joe
>
> On Thu, Apr 8, 2010 at 9:46 AM, Chris Povirk <cpovirk at google.com 
> <mailto:cpovirk at google.com>> wrote:
>
>     My goal is a family of operations like these:
>
>     // Neither of these throws InterruptedException.
>     V result = Futures.getUninterruptibly(future);
>     CountDownLatches.awaitUninterruptibly(latch);
>
>     These methods would retry the operation until it completes without
>     interruption and then restore the interrupt.  (Another option is to
>     wrap the InterruptedException in a custom InterruptedRuntimeException
>     class; I'd be happy to hear people's opinions on this, as well.)
>
>     Google has implementations of some of these operations, and we're
>     planning to expand our coverage and release them in our open-source
>     Guava project.  However, we're uncertain about a few issues.
>
>     Below I've listed the two main decisions we need to make along with a
>     number of pros and cons for each of their options.
>
>
>     Decision A: The interface to these uninterruptible methods could take
>     one of three forms:
>     1. Static Futures.getUninterruptibly methods.
>     2. A static uninterruptibleFuture method that returns an
>     UninterruptibleFuture, which adds getUninterruptibly methods to
>     Future.
>     3. A static uninterruptibleFuture method that returns an
>     UninterruptibleFuture, which overrides Future's get methods to no
>     longer throw InterruptedException.
>
>     (1) requires the fewest method declarations.
>
>     The method declarations from (1) are likely to be jumbled together
>     with unrelated declarations.  A person who sees
>     BlockingQueues.putUninterruptibly, takeUninterruptibly,
>     offerUninterruptibly, and pollUninterruptibly may get the impression
>     that the BlockingQueues contains only InterruptedException-removing
>     methods, when perhaps it contains methods like takeMultiple(Queue<E>,
>     int).  Plus, subclassing is awkward: does BlockingDeques provide a
>     static putUninterruptibly method, too?  (An alternative is to put
>     methods for all interfaces in a single Uninterruptibles class.)
>
>     (1) is the best solution for classes with final methods (notably
>     Thread.join).  Even (2) suffers from this problem:
>     joinUninterruptibly() would join the delegate thread, as desired, but
>     join() (which we can't override) would join the unstarted wrapper
>     thread.  This is confusing.  Luckily, Thread.join and other primitives
>     are used less frequently than they used to be, especially among the
>     kind of people that I hope will use the uninterruptible wrappers.
>     (Related: Developers who want to make their methods final can define
>     interfaces so that we don't need to override the class's methods.)
>
>     Declaring a variable or field to be an UninterruptibleFuture (possible
>     in (2) or (3)) can express our intention to use it that way to readers
>     and make the uninterruptible methods available in IDE autocompletion.
>
>     Methods can declare a return type of UninterruptibleFuture, making the
>     uninterruptible operations more convenient and bringing them to the
>     attention of more users.
>
>     (3) hides the original, interruptible methods.  This is especially a
>     problem if a method has returned an UninterruptibleFuture to the user
>     for "convenience," as proposed above.
>
>     (2) is most consistent with the JDK's existing uninterruptible
>     methods.
>
>     In principle, a Future implementation could implement
>     UninterruptibleFuture and some other Future subinterface (e.g., one
>     whose get methods do not throw ExecutionException).  In practice, this
>     seems unlikely.
>
>     "UninterruptibleFuture" is a poor name.  The name may suggest to some
>     that a *task* whose value the Future awaits may not be interrupted
>     through Future.cancel.  But "Uninterruptible" refers only to the get
>     methods; it's more like "UninterruptiblyGettableFuture," which is an
>     ugly name.
>
>     "UninterruptibleFuture" is a particularly poor name in the case of
>     (2), since its get methods would still be interruptible.  (This is
>     another problem that "UninterruptiblyGettableFuture" could solve.)
>
>
>     Decision B: There are two ways to handle the retry boilerplate:
>     1. Repeat it everywhere it's needed.
>     2. Delegate to a performUninterruptibly method that takes an
>     InterruptibleOperation parameter.
>     (<http://cs.oswego.edu/pipermail/concurrency-interest/2007-March/003775.html>)
>
>     (A colleague proposed a reflection-based alternative, but we found it
>     to have two problems:  First, CountDownLatch isn't an interface, so we
>     can't extend it to create an UninterruptibleCountDownLatch interface
>     to pass to Proxy.  Second, we can't handle timeouts without either
>     annotating the long+TimeUnit parameters or assuming that there's a
>     convention for them.)
>
>     (1) has the usual advantages and disadvantages of copy-and-paste
>     programming.
>
>     If there's a significant performance difference, (1) is surely the
>     winner.
>
>     (2) requires three variants of InterruptibleOperation: one for
>     operations without timeout, one for operations that time out by
>     returning a sentinel value, and one for operations that time out by
>     throwing TimeoutException.  (We might be able to combine the last two
>     with suitable generics, but my claim of "three variants" already
>     presumes some awkward parameterization to handle exceptions.)
>
>
>     I'm currently leaning toward 2-1
>     (UninterruptibleFuture.getUninterruptibly, repeat the boilerplate),
>     but some smart people disagree.  Opinions?
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>    

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100408/2e2c4ca1/attachment.html>

From david.lloyd at redhat.com  Thu Apr  8 20:52:06 2010
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Thu, 08 Apr 2010 19:52:06 -0500
Subject: [concurrency-interest] InterruptedException-free wrappers for
 calls that "will never be interrupted"
In-Reply-To: <4BBE758B.7090502@referentia-test.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
	<4BBE758B.7090502@referentia-test.com>
Message-ID: <4BBE7A36.1030106@redhat.com>

Since you only have uninterruptible variants, maybe you could drop the 
"Uninterruptibly"?  That's a horrible word to type ;)

On 04/08/2010 07:32 PM, Jim Newsham wrote:
>
> Regarding decision A. We have a collection of static
> XXXUninterruptibly() methods in a single utility class. This is like
> your approach #1, except they are all in the same class (ours is called
> ThreadUtils and has some other thread-related methods, but I've
> sometimes considered breaking them out into a separate class which
> contained only XXXUninterruptibly()... maybe
> UninterruptibleUtils.get(Future) or Uninterruptibly.get(Future)?). We
> didn't really consider (or have a need for) the wrapper approach -- it's
> really the invoker who decides whether to mask interruption.
>
> Regarding decision B. In our case, the code for each method is repeated,
> but it's really not much code at all so I have no issue with it.
>
> Jim
>
> On 4/8/2010 6:46 AM, Chris Povirk wrote:
>> My goal is a family of operations like these:
>>
>> // Neither of these throws InterruptedException.
>> V result = Futures.getUninterruptibly(future);
>> CountDownLatches.awaitUninterruptibly(latch);
>>
>> These methods would retry the operation until it completes without
>> interruption and then restore the interrupt. (Another option is to
>> wrap the InterruptedException in a custom InterruptedRuntimeException
>> class; I'd be happy to hear people's opinions on this, as well.)
>>
>> Google has implementations of some of these operations, and we're
>> planning to expand our coverage and release them in our open-source
>> Guava project. However, we're uncertain about a few issues.
>>
>> Below I've listed the two main decisions we need to make along with a
>> number of pros and cons for each of their options.
>>
>>
>> Decision A: The interface to these uninterruptible methods could take
>> one of three forms:
>> 1. Static Futures.getUninterruptibly methods.
>> 2. A static uninterruptibleFuture method that returns an
>> UninterruptibleFuture, which adds getUninterruptibly methods to
>> Future.
>> 3. A static uninterruptibleFuture method that returns an
>> UninterruptibleFuture, which overrides Future's get methods to no
>> longer throw InterruptedException.
>>
>> (1) requires the fewest method declarations.
>>
>> The method declarations from (1) are likely to be jumbled together
>> with unrelated declarations. A person who sees
>> BlockingQueues.putUninterruptibly, takeUninterruptibly,
>> offerUninterruptibly, and pollUninterruptibly may get the impression
>> that the BlockingQueues contains only InterruptedException-removing
>> methods, when perhaps it contains methods like takeMultiple(Queue<E>,
>> int). Plus, subclassing is awkward: does BlockingDeques provide a
>> static putUninterruptibly method, too? (An alternative is to put
>> methods for all interfaces in a single Uninterruptibles class.)
>>
>> (1) is the best solution for classes with final methods (notably
>> Thread.join). Even (2) suffers from this problem:
>> joinUninterruptibly() would join the delegate thread, as desired, but
>> join() (which we can't override) would join the unstarted wrapper
>> thread. This is confusing. Luckily, Thread.join and other primitives
>> are used less frequently than they used to be, especially among the
>> kind of people that I hope will use the uninterruptible wrappers.
>> (Related: Developers who want to make their methods final can define
>> interfaces so that we don't need to override the class's methods.)
>>
>> Declaring a variable or field to be an UninterruptibleFuture (possible
>> in (2) or (3)) can express our intention to use it that way to readers
>> and make the uninterruptible methods available in IDE autocompletion.
>>
>> Methods can declare a return type of UninterruptibleFuture, making the
>> uninterruptible operations more convenient and bringing them to the
>> attention of more users.
>>
>> (3) hides the original, interruptible methods. This is especially a
>> problem if a method has returned an UninterruptibleFuture to the user
>> for "convenience," as proposed above.
>>
>> (2) is most consistent with the JDK's existing uninterruptible methods.
>>
>> In principle, a Future implementation could implement
>> UninterruptibleFuture and some other Future subinterface (e.g., one
>> whose get methods do not throw ExecutionException). In practice, this
>> seems unlikely.
>>
>> "UninterruptibleFuture" is a poor name. The name may suggest to some
>> that a *task* whose value the Future awaits may not be interrupted
>> through Future.cancel. But "Uninterruptible" refers only to the get
>> methods; it's more like "UninterruptiblyGettableFuture," which is an
>> ugly name.
>>
>> "UninterruptibleFuture" is a particularly poor name in the case of
>> (2), since its get methods would still be interruptible. (This is
>> another problem that "UninterruptiblyGettableFuture" could solve.)
>>
>>
>> Decision B: There are two ways to handle the retry boilerplate:
>> 1. Repeat it everywhere it's needed.
>> 2. Delegate to a performUninterruptibly method that takes an
>> InterruptibleOperation parameter.
>> (<http://cs.oswego.edu/pipermail/concurrency-interest/2007-March/003775.html>)
>>
>>
>> (A colleague proposed a reflection-based alternative, but we found it
>> to have two problems: First, CountDownLatch isn't an interface, so we
>> can't extend it to create an UninterruptibleCountDownLatch interface
>> to pass to Proxy. Second, we can't handle timeouts without either
>> annotating the long+TimeUnit parameters or assuming that there's a
>> convention for them.)
>>
>> (1) has the usual advantages and disadvantages of copy-and-paste
>> programming.
>>
>> If there's a significant performance difference, (1) is surely the
>> winner.
>>
>> (2) requires three variants of InterruptibleOperation: one for
>> operations without timeout, one for operations that time out by
>> returning a sentinel value, and one for operations that time out by
>> throwing TimeoutException. (We might be able to combine the last two
>> with suitable generics, but my claim of "three variants" already
>> presumes some awkward parameterization to handle exceptions.)
>>
>>
>> I'm currently leaning toward 2-1
>> (UninterruptibleFuture.getUninterruptibly, repeat the boilerplate),
>> but some smart people disagree. Opinions?
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
- DML ?

From cpovirk at google.com  Thu Apr  8 23:36:21 2010
From: cpovirk at google.com (Chris Povirk)
Date: Thu, 8 Apr 2010 23:36:21 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for
	calls that "will never be interrupted"
In-Reply-To: <u2v31f2a7bd1004081057g5dbcb260gff2e9a0ea328d000@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>
	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
	<u2v31f2a7bd1004081057g5dbcb260gff2e9a0ea328d000@mail.gmail.com>
Message-ID: <s2h77492afe1004082036gcfdbc5ek26aaed90b692b8f@mail.gmail.com>

Joe,

Thanks for the response.

You're right that the subject line I chose has artificially narrowed
the discussion to the "never interrupted" case.  In the end, the
question I'm interested in is "What should a Future.get that doesn't
throw InterruptedException look like?"  It's a truism that a
loop-and-retry implementation is the right implementation when you
want loop-and-retry, as you point out, so I didn't want to dwell on
this point.  But the result was an e-mail that focused too much on the
"never interrupted" case.  I believe that there is value in supporting
both use cases, and, as I'll now argue, I believe that loop-and-retry
can do that.

We agree about the unique advantage of loop-and-retry.  I disagree,
however, that that strategy is more specialized than the
RuntimeException strategy.  I see loop-and-retry as the better
approach for calls that "will never be interrupted."

The reason is the same as the reason that I put scare quotes around
"will never be interrupted."  I think we can agree that, if an
operation truly is never interrupted, any reaction to
InterruptedException is equally effective -- retry, wrap, ignore,
System.exit, Runtime.exec("rm -rf /")....  The question is what to do
when the "impossible" interrupt occurs.  I believe (based on evidence
from Google and non-Google code) that we as programmers are much too
quick to assume that an interrupt is impossible.
(executor.submit(task).cancel(true) would catch a lot of our libraries
by surprise.)

You've pointed out the responsiveness advantage of handling interrupts
by throwing a RuntimeException.  This is an advantage, but I don't see
it as a large one.  Uninterruptible tasks already exist --
uninterruptible IO, heavy math computations, or callers of
Semaphore.acquireUninterruptibly, for example.  A task that does not
exit when interrupted can be a problem, but it's a problem that many
systems must already cope with.

More important is the disadvantage of throwing a RuntimeException: it
encourages the programmer to believe that the operation will always
complete even though the implementation makes no such guarantee.  If
the programmer believed that the exception were possible, he would
have caught or propagated the original InterruptedException; there
would be no need for an uninterruptible wrapper.  We've made it easier
for him to believe that queue.putUninterruptibly(task) "can't fail,"
but if it does, his program drops a task on the floor.

Additionally, I believe there's less value in a library implementation
of the RuntimeException-throwing variant because it's comparatively
easy for the programmer to write himself (especially if the
InterruptedRuntimeException constructor restores the interrupt for
him):

try {
  ... perform any number of interruptible operations ...
} catch (InterruptedException e) {
  throw new InterruptedRuntimeException(e);
}

As opposed to:

... maybe declare a field outside the finally block ...
boolean interrupted = false;
try {
  long timeoutNanos = timeoutUnit.toNanos(timeoutDuration);
  long end = System.nanoTime() + timeoutNanos;
  while (true) {
    try {
      ... perform one number of interruptible operations ...
    } catch (InterruptedException e) {
      timeoutNanos = end - System.nanoTime();
      interrupted = true;
    }
  }
} finally {
  if (interrupted) {
    Thread.currentThread().interrupt();
  }
}
... repeat for each other interruptible operation...


Jim: Thanks, we've considered throwing everything into an
Uninterruptibles class.  This solves a couple of the problems that
arise from splitting the methods across classes named Futures,
BlockingQueues, etc.

From joe.bowbeer at gmail.com  Fri Apr  9 02:33:38 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 8 Apr 2010 23:33:38 -0700
Subject: [concurrency-interest] InterruptedException-free wrappers for
	calls that "will never be interrupted"
In-Reply-To: <s2h77492afe1004082036gcfdbc5ek26aaed90b692b8f@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>
	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
	<u2v31f2a7bd1004081057g5dbcb260gff2e9a0ea328d000@mail.gmail.com>
	<s2h77492afe1004082036gcfdbc5ek26aaed90b692b8f@mail.gmail.com>
Message-ID: <i2g31f2a7bd1004082333r56af989cg3d99b494d9e86703@mail.gmail.com>

Chris,

My suggestion to thrown an IllegalStateException was a vote for
"failing-fast" if the no-interrupt precondition was violated. It was not
meant to fool the programmers into thinking that their operation would
always complete.

Programming "responsively" with respect to interrupts is hard to do and
tedious to maintain, and I am concerned that programmers will be fooled into
thinking that some API is responsive to interrupts -- because it preserves
the interrupt flag and may eventually throw an IE -- when in practice the
code can wait forever before it finally responds to the interrupt.

I also question whether reasserting the interrupt is adequate.  If every
method reasserts the interrupt, when is the interrupt actually handled?

If the intent is to wait until Future.get completes, even if interrupted,
then why not throw IE upon completion?

Joe


On Thu, Apr 8, 2010 at 8:36 PM, Chris Povirk wrote:

> Joe,
>
> Thanks for the response.
>
> You're right that the subject line I chose has artificially narrowed
> the discussion to the "never interrupted" case.  In the end, the
> question I'm interested in is "What should a Future.get that doesn't
> throw InterruptedException look like?"  It's a truism that a
> loop-and-retry implementation is the right implementation when you
> want loop-and-retry, as you point out, so I didn't want to dwell on
> this point.  But the result was an e-mail that focused too much on the
> "never interrupted" case.  I believe that there is value in supporting
> both use cases, and, as I'll now argue, I believe that loop-and-retry
> can do that.
>
> We agree about the unique advantage of loop-and-retry.  I disagree,
> however, that that strategy is more specialized than the
> RuntimeException strategy.  I see loop-and-retry as the better
> approach for calls that "will never be interrupted."
>
> The reason is the same as the reason that I put scare quotes around
> "will never be interrupted."  I think we can agree that, if an
> operation truly is never interrupted, any reaction to
> InterruptedException is equally effective -- retry, wrap, ignore,
> System.exit, Runtime.exec("rm -rf /")....  The question is what to do
> when the "impossible" interrupt occurs.  I believe (based on evidence
> from Google and non-Google code) that we as programmers are much too
> quick to assume that an interrupt is impossible.
> (executor.submit(task).cancel(true) would catch a lot of our libraries
> by surprise.)
>
> You've pointed out the responsiveness advantage of handling interrupts
> by throwing a RuntimeException.  This is an advantage, but I don't see
> it as a large one.  Uninterruptible tasks already exist --
> uninterruptible IO, heavy math computations, or callers of
> Semaphore.acquireUninterruptibly, for example.  A task that does not
> exit when interrupted can be a problem, but it's a problem that many
> systems must already cope with.
>
> More important is the disadvantage of throwing a RuntimeException: it
> encourages the programmer to believe that the operation will always
> complete even though the implementation makes no such guarantee.  If
> the programmer believed that the exception were possible, he would
> have caught or propagated the original InterruptedException; there
> would be no need for an uninterruptible wrapper.  We've made it easier
> for him to believe that queue.putUninterruptibly(task) "can't fail,"
> but if it does, his program drops a task on the floor.
>
> Additionally, I believe there's less value in a library implementation
> of the RuntimeException-throwing variant because it's comparatively
> easy for the programmer to write himself (especially if the
> InterruptedRuntimeException constructor restores the interrupt for
> him):
>
> try {
>  ... perform any number of interruptible operations ...
> } catch (InterruptedException e) {
>  throw new InterruptedRuntimeException(e);
> }
>
> As opposed to:
>
> ... maybe declare a field outside the finally block ...
> boolean interrupted = false;
> try {
>  long timeoutNanos = timeoutUnit.toNanos(timeoutDuration);
>  long end = System.nanoTime() + timeoutNanos;
>  while (true) {
>    try {
>      ... perform one number of interruptible operations ...
>    } catch (InterruptedException e) {
>      timeoutNanos = end - System.nanoTime();
>      interrupted = true;
>    }
>  }
> } finally {
>  if (interrupted) {
>    Thread.currentThread().interrupt();
>  }
> }
> ... repeat for each other interruptible operation...
>
>
> Jim: Thanks, we've considered throwing everything into an
> Uninterruptibles class.  This solves a couple of the problems that
> arise from splitting the methods across classes named Futures,
> BlockingQueues, etc.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100408/37aac841/attachment.html>

From dl at cs.oswego.edu  Fri Apr  9 09:43:32 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 09 Apr 2010 09:43:32 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for
 calls	that "will never be interrupted"
In-Reply-To: <k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
Message-ID: <4BBF2F04.6010608@cs.oswego.edu>

On 04/08/10 12:46, Chris Povirk wrote:
> My goal is a family of operations like these:
>
> // Neither of these throws InterruptedException.

I don't have any concrete suggestions at the moment, but
here are a few observations:

* The story about how interrupt and InterruptedException
work is another one of those things that only make sense in
historical context. InterruptedException was first introduced
(around 1994) as Java moved from alpha to 1.0 (before that,
Object.wait etc did not throw IE). Then Checked exceptions
were introduced. While I wasn't a part of any of this,
my guess is that IE was classified as checked so as to
force people to fix their wait() constructions to accommodate
this change. It has always been odd that built-in locks
cannot throw IE yet built-in waits must throw IE. This is
one reason to use ReentrantLock etc, which support all the
possible forms. Perhaps this distinction should have been more
uniformly propagated to other blocking methods. Perhaps it still could.

* While there are a few others cases, the main situation in
which you want to continue even if interrupted is if you are
part of a computation that can only cancel/terminate cooperatively
or globally. This is why ForkJoinTask.join and other FJ methods
(and similarly, Phaser) do not throw IE -- interruption of any
subtask wait doesn't imply anything about cancelling the aggregate
computation; instead you need explicit cancel(). As more
and more code is written involving cooperative parallel
computations, this mode becomes more common. FJ includes
an extensibility hook ForkJoinPool.ManagedBlocker than allows
this mode to be used for any blocking operation. It might be
a good idea to build in support for a few common ones (like
basic FIFO queues). Note though that Phaser generalizes
CountDownLatch so there is no real need for modified form
of CountDownLatch. Similarly ForkJoinTask for Future.

* The implementations of most blocking constructions
in j.u,c. components include a fair amount of overhead to ensure
responsiveness to interruption. Adding looping catch-and-retry
overhead on top of this just to suppress IE is a sad way to cope,
It would be a lot faster if the underlying methods did this themselves.

-Doug


From cpovirk at google.com  Fri Apr  9 11:52:51 2010
From: cpovirk at google.com (Chris Povirk)
Date: Fri, 9 Apr 2010 11:52:51 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for
	calls that "will never be interrupted"
In-Reply-To: <i2g31f2a7bd1004082333r56af989cg3d99b494d9e86703@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>
	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
	<u2v31f2a7bd1004081057g5dbcb260gff2e9a0ea328d000@mail.gmail.com>
	<s2h77492afe1004082036gcfdbc5ek26aaed90b692b8f@mail.gmail.com>
	<i2g31f2a7bd1004082333r56af989cg3d99b494d9e86703@mail.gmail.com>
Message-ID: <l2m77492afe1004090852tce976152rf1c1f6c62db1b83a@mail.gmail.com>

Joe,

I understand that fail-fast behavior is a worthy goal.  My bigger
concern is the other effect of the change, which is to convert a
checked exception to an unchecked exception without changing the
circumstances under which the exception occurs.  My question, then, is
what you expect programmers to do with the IllegalStateException.
Either they're expected to catch it (in which case I think they're
better served by the original checked exception[*]), or they're
expected to ignore it (in which case they're vulnerable to the "this
will never happen"mistake).  We may just disagree over the merits of
code that fails quickly but at a spot the programmer doesn't expect
the code to fail.

Alternatively, we may disagree over the likelihood that the programmer
will understand that, despite the lack of a checked exception, the
interruptible operation may fail.  My experience is that, even in the
case of the checked InterruptedException, programmers make the
unjustified assumption that it will never occur:
http://code.google.com/query/#q=%22catch%20interruptedexception%22

Maybe some of these are justified, but much of the code seems to be in
library methods that can't possibly know they won't be interrupted.

This brings me to my secret goal: The next time we announce the latest
batch of new Java libraries, I want to be able to say: "Annoyed by
InterruptedException?  Reade Goetz's 'Dealing with
InterruptedException,' and consider our new family of
InterruptedException-free methods in the Uninterruptibles class (e.g.,
Uninterruptibles.get(Future)) to make your operations uncancellable."
In an ideal world, everyone will read the article and apply it.  In
the real world, we will (I hope) see many more Uninterruptibles.get
calls in place of the catch-and-swallow idiom I see too often now.
It's partially an education problem, but I can understand if people
who don't expect to use task cancellation don't want to read 2400
words about InterruptedExcetion just to implement it.  (And the
implementation can be, as you say, tedious.)  One role of
Uninterruptibles.get is to provide an easier solution for them.  As a
bonus, the Uninterruptibles class should provide a good place to link
to Goetz's article so that anyone interested enough to investigate
Uninterruptibles.get can learn more.

> I also question whether reasserting the interrupt is adequate.? If every
> method reasserts the interrupt, when is the interrupt actually handled?

I admit that it's possible that it will be handled nowhere.  If forced
to choose between "abort unexpectedly and continue processing with n
threads" or "resume and continue processing with n-1 threads for a
time," I'll choose the latter.  "Abort in a controlled manner and
continue processing with n threads" is the ideal, but by accepting any
InterruptedException-free method, we've in effect abandoned the ideal.

That aside, it's possible that even if one library I call loops and
restores the interrupt, the next library I call will use it as a
cancellation signal, so it will still take effect, just not as quickly
as it could have.

> If the intent is to wait until Future.get completes, even if interrupted,
> then why not throw IE upon completion?

Most callers of Future.get are interested in the return value, which
they won't receive if the method throws.  Maybe that's OK if the
caller was going to do a lot more work with that value before
returning (so the IE prevented the operation from continuing), but if
"return Uninterruptibles.get(future)" is the last significant thing
the caller does, then throwing IE serves only to cancel a job after
it's done but before it's returned its result.  My intent isn't so
much to wait for completion as it is to ensure that, if the operation
*looks* like it will always complete (i.e., no checked exception),
then it will.  The choices in my mind are "Throw IE immediately when
interrupted" (to get the full benefit of interruption) or "Don't throw
IE at all."  Future isn't the only example here; consider the expected
blockingQueue.put(task) failure I mentioned earlier.

Additionally, not throwing IE is the more flexible approach, as it's
simple to manually throw one if desired (especially in comparison to
the code to do the reverse):

V result = Uninterruptibles.get(future);
// Could write a utility method to reduce this to one line:
if (Thread.interrupted()) {
  throw new InterruptedException();
}

[*] The original programmer of a piece of code seems to be good enough
at ignoring a checked exception.  Even if we can convince him not to
ignore an unchecked exception (probably through a scary-sounding
method name), later programmers may not go back to read the
documentation.  This is especially true if they don't touch the method
that calls getNeverInterrupted itself but instead touch a method that
calls it.  Who would guess that issueQuery() would throw
IllegalStateException if interrupted?  (Or do we expect people to call
it "issueQueryNeverInterrupted?")


Doug: Thanks.  Still digesting all of that.


From hans.boehm at hp.com  Fri Apr  9 15:42:16 2010
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 9 Apr 2010 19:42:16 +0000
Subject: [concurrency-interest] InterruptedException-free wrappers for
 calls	that "will never be interrupted"
In-Reply-To: <4BBF2F04.6010608@cs.oswego.edu>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>
	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
	<4BBF2F04.6010608@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D04269FF81298B@GVW0436EXB.americas.hpqcorp.net>

> From:  Doug Lea
> It has always been odd that built-in locks cannot 
> throw IE yet built-in waits must throw IE.
I'm not sure I quite understand this comment.

This still strikes me as reasonable behavior in the large majority of cases, though you can clearly find occasional cases for which it's inappropriate.  Most code avoids waiting for a lock for extended periods, while wait() is clearly used routinely to block for arbitrary time periods.  This also seems closely related to whether or not you want to accommodate timeouts, where we make a similar distinction.

Hans
 
> This is one reason 
> to use ReentrantLock etc, which support all the possible 
> forms. Perhaps this distinction should have been more 
> uniformly propagated to other blocking methods. Perhaps it 
> still could.
> 


From dl at cs.oswego.edu  Sat Apr 10 07:35:25 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 10 Apr 2010 07:35:25 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for
 calls	that "will never be interrupted"
In-Reply-To: <238A96A773B3934685A7269CC8A8D04269FF81298B@GVW0436EXB.americas.hpqcorp.net>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
	<4BBF2F04.6010608@cs.oswego.edu>
	<238A96A773B3934685A7269CC8A8D04269FF81298B@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4BC0627D.30504@cs.oswego.edu>

On 04/09/10 15:42, Boehm, Hans wrote:
>> From:  Doug Lea It has always been odd that built-in locks cannot throw IE
>> yet built-in waits must throw IE.
> I'm not sure I quite understand this comment.
>
> This still strikes me as reasonable behavior in the large majority of cases,
> though you can clearly find occasional cases for which it's inappropriate.

I agree. The problem (prior to ReentrantLock) was that these occasional
cases could not be expressed at all.

-Doug


From dl at cs.oswego.edu  Sat Apr 10 09:47:42 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 10 Apr 2010 09:47:42 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for
 calls	that "will never be interrupted"
In-Reply-To: <k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
Message-ID: <4BC0817E.50205@cs.oswego.edu>

On 04/08/10 12:46, Chris Povirk wrote:
> My goal is a family of operations like these:
>
> // Neither of these throws InterruptedException.
> V result = Futures.getUninterruptibly(future);
> CountDownLatches.awaitUninterruptibly(latch);
>

Here are some more notes on this, with some embedded
concrete proposals. My sense is that it might be a bad idea
to uniformly paper these over via layered libraries, but
instead to better deal with at least some of them in j.u.c proper.

Backing up: You can (very!) roughly divide concurrency
into two categories:
  - latency-driven: "reactive", typically "asynchronous"  techniques
      dealing with intrinsic concurrency, as in servers with multiple
      clients etc
  - throughput-driven: "parallel" techniques that speed up programs
      via multiple CPUS/cores/threads.

There are a lot of in-betweens that trade off some aspects of
responsiveness/latency vs throughput. But it is still a useful
way to think about the  issues at hand.

Most (but not all) initial j.u.c components were oriented
toward reactive concurrency. Over time we've added more support
for throughput-driven parallel programming, in response to the
increasing availability of multicores and MPs.
The main addition is of course the ForkJoin framework,
that is so much geared toward aggregate throughput
via structured parallel computations that it does not even
support use of some asynchronous techniques like blocking on
queues.

As mentioned in my last post, most aspects of throughput-driven
frameworks and components don't hugely differ from others,
but one place they do is dealing with interrupts. Here, interrupts
per se are not typically useful to deal with by users, but they
still may play some internal role in supporting cancellation.
Cancellation is, often enough, an ingredient of parallel
computation control. You'd like responsiveness to events that
allow you not to compute things -- for example cancelling some
subtasks of a parallel search if any one of them succeeds --
thus improving throughput for other parts of a program.
Also, interrupts may play a role in shutting down an entire
parallel component/framework, as a more extreme form of
cancellation.

These and related thoughts led to defining ForkJoinTask
as an abstract base class extension of Future.
Among other things, ForkJoinTask introduces method "join"
which is similar to the proposed "Futures.getUninterruptibly(future)".
It would be possible to do this more generally by introducing
interface JoinableFuture between Future and ForkJoinTask, so
people could use this outside of FJ. I'm not positive that this is a
great idea in the long run, but it may be helpful for
those who use a more throughput-oriented approach while still
using some of the other more reactive-oriented j.u.c components,
and would also make it easier to mix and match Futures and FJ.
Comments and suggestions about this would be welcome.
As Chris mentioned, doing this would also lead to introducing
a stand-alone class like JoinableFutureTask or maybe a factory
that could create instances of a default implementation.

A similar but easier story applies to Phaser, that
generalizes both CountDownLatch and CyclicBarrier.
Phaser.arriveAndAwaitAdvance can and should be used instead of
layering CountDownLatches.awaitUninterruptibly(latch).

Also, several of the other AQS-based synchronizers already
support uninterruptible waits. And all of the various non-blocking
data structures are of course unaffected.

The main classes that don't fit into this well are the
various BlockingQueues (and extensions thereof). It is not
completely clear to me that they should. The main role
that they play in throughput-driven programming is under the
hood, in mechanics that coordinate handling tasks or data. But
these mechanics often need to deal with interrupts anyway
in support of various forms of cancellation/shutdown. However,
if a good case can be made for it, we do have the new
TransferQueue interface available to exploit to add some
associated methods, that could at least be used in a
correspondingly updated LinkedTransferQueue class. Does
anyone have a good story about why this might be worth doing?

-Doug





From gregg at wonderly.org  Tue Apr 13 17:28:45 2010
From: gregg at wonderly.org (Gregg Wonderly)
Date: Tue, 13 Apr 2010 16:28:45 -0500
Subject: [concurrency-interest] The real impact of RFE 4630118?
Message-ID: <4BC4E20D.60500@wonderly.org>

I've been aware of http://bugs.sun.com/view_bug.do?bug_id=4630118 for some time, 
and have two votes on it currently.  What I wonder about, is whether there is 
anything about solving it, which is actually harder than the proposal actually 
sounds.

Having that issue resolved would make so many caching and memory pressure relief 
things possible to deal with, which currently are really hard to manage with 
public APIs, because it is impossible to guarantee that values don't reference keys.

Thoughts?

Gregg Wonderly

From neal at gafter.com  Tue Apr 13 17:41:21 2010
From: neal at gafter.com (Neal Gafter)
Date: Tue, 13 Apr 2010 14:41:21 -0700
Subject: [concurrency-interest] The real impact of RFE 4630118?
In-Reply-To: <4BC4E20D.60500@wonderly.org>
References: <4BC4E20D.60500@wonderly.org>
Message-ID: <p2t15e8b9d21004131441q12d2cb94l8c6697372c260ec4@mail.gmail.com>

I think this is relevant:

http://swiki-lifia.info.unlp.edu.ar/ContextAware/uploads/29/Ephemerons_-_A_New_Finalization_Mechanism.pdf

On Tue, Apr 13, 2010 at 2:28 PM, Gregg Wonderly <gregg at wonderly.org> wrote:

> I've been aware of http://bugs.sun.com/view_bug.do?bug_id=4630118 for some
> time, and have two votes on it currently.  What I wonder about, is whether
> there is anything about solving it, which is actually harder than the
> proposal actually sounds.
>
> Having that issue resolved would make so many caching and memory pressure
> relief things possible to deal with, which currently are really hard to
> manage with public APIs, because it is impossible to guarantee that values
> don't reference keys.
>
> Thoughts?
>
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100413/79196af4/attachment.html>

From cpovirk at google.com  Tue Apr 13 21:44:00 2010
From: cpovirk at google.com (Chris Povirk)
Date: Tue, 13 Apr 2010 21:44:00 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for
	calls that "will never be interrupted"
In-Reply-To: <4BC0817E.50205@cs.oswego.edu>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>
	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
	<4BC0817E.50205@cs.oswego.edu>
Message-ID: <n2t77492afe1004131844xbf49a2e2s6b5ddebd118e451e@mail.gmail.com>

The main case I have in mind for uninterruptible methods is one that
I'm not sure how to put into your very (very! :)) rough categories.
I'm thinking of code that makes an RPC and waits for the result.  In a
sense, there's no concurrency at all:  The caller hands the request to
a network thread, which passes it to another process, which returns
the result, which is received by the network thread, which passes it
back to the caller.  At each step, only one thread (sometimes a thread
in a different process entirely) is processing the request.  The fact
that the original caller is waiting for another thread is almost an
artificial implementation detail of the RPC system, which happens to
need to demux the various responses it receives.

The caller of the RPC method, of course, can itself be placed into one
of your two rough categories.  Maybe there are several running in
parallel, in which case cancellation may be valuable.  But it's also
possible that only we're a boring web server making a single boring
request to the backend.  Here cancellation via interruption seems like
a less important feature (if I'm understanding your comments).

Now, if cancellation via interruption were free, it might still be
worth supporting.  But an extra checked exception has some cost to
callers, even those who don't expect to use it.  I see an
uninterruptible wrapper as a way to reduce these costs for those
callers who don't see a corresponding benefit.

But since we're talking about RPCs, which already have other failure
modes, maybe interruption can be folded into the general
error-signaling framework.  If the RPC framework declares "throws
ServiceException," maybe it should be catching InterruptedException
and, after restoring the interrupt, wrapping it in a ServiceException
(or subclass) and throwing that.  I've seen this done but haven't
heard anything approach authoritative guidance.

(My RPC scenario may not generalize well beyond Future.  I've used a
CountDownLatch to implement Future, but beyond that I'm not sure.
BlockingQueue was a request from a coworker; I suppose it could be
used similarly if the other process's response were streamed back
instead of returned as a single chunk.)

Thanks, Doug.

From dl at cs.oswego.edu  Wed Apr 14 06:57:05 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 14 Apr 2010 06:57:05 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for
 calls that "will never be interrupted"
In-Reply-To: <n2t77492afe1004131844xbf49a2e2s6b5ddebd118e451e@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>	<4BC0817E.50205@cs.oswego.edu>
	<n2t77492afe1004131844xbf49a2e2s6b5ddebd118e451e@mail.gmail.com>
Message-ID: <4BC59F81.5000807@cs.oswego.edu>

On 04/13/10 21:44, Chris Povirk wrote:

> My RPC scenario may not generalize well beyond Future.

So, what do you think of at least addressing this case,
by adding j.u.c.JoinableFuture, that basically pushes up
ForkJoinTask.join to interface level:

interface JoinableFuture<V> extends Future<V> {
     /**
      * Returns the result of the computation when it {@link #isDone is done}.
      * This method differs from {@link #get()} in that
      * abnormal completion results in {@code RuntimeException} or
      * {@code Error}, not {@code ExecutionException}. Further,
      * this method continues to wait for completion even if the
      * calling thread is interrupted.
      * @return the computed result
      */
     V join();
}

Note that this version both escalates exceptions
and ignores interrupts. ForkJoinTask requires this form.
If we want versions of methods that do only one or the other,
things get messier.

We could also retrofit FutureTask to implement JoinableFuture.
We'd need to weigh the risk that someone
out there already added method join to a FutureTask subclass,
and back up to adding a JoinableFutureTask class if we decide
to be cowardly about it. A quick check of Google code search
looking for "extends FutureTask join" doesn't find any such cases.

-Doug



From dl at cs.oswego.edu  Wed Apr 14 08:52:46 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 14 Apr 2010 08:52:46 -0400
Subject: [concurrency-interest] Extended access methods for Atomics (and AQS)
Message-ID: <4BC5BA9E.9010906@cs.oswego.edu>

Last fall, we tabled discussion of the proposed Fences API.
But regardless of the outcome of and future discussions when
we re-raise it, there seemed to be consensus that at the very
least, we should extend the methods of Atomic classes to support
the various memory effect modes that you otherwise would need
Fences methods to obtain.

I'm finally trying this out. A draft update of AtomicInteger
with these changes is at:
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/AtomicInteger.html
(For now, only class AtomicInteger). Ignoring the arithmetic methods
(getAndAdd etc), the API looks like (with "*" in front of
the added methods)

"volatile-write" mode
    void set(int v)
    boolean compareAndSet(int e, int v);
    int setAndGet(int v);

store-ordered (aka release, pseudo-final-field) mode
*  void setInStoreOrder(int v);
*  boolean compareAndSetInStoreOrder(int e, int v);
    void lazySet(int v) // synonym for setInStoreOrder

load-ordered (aka volatile-read, acquire) mode
    int get();
*  compareAndSetAndGet(int e, int v);

relaxed-order (aka non-volatile) mode
*  int getInRelaxedOrder();
*  void setInRelaxedOrder(int v);
    boolean weakCompareAndSet(int e, int v);

Comments and suggestions about method names and
semantics would be very welcome. (Notice that
the naming scheme denigrates "lazySet", a name
that no one likes!)

The form of these methods is roughly similar to
C++0x modes (at least the ones supportable in Java),
but avoids the sometimes-controversial terms
"acquire" and "release" in mode/method names.

For now, the specs are just done informally. Assuming we
go ahead with this, we'd adapt the more formal versions done
in Fences drafts and place them in j.u.c.atomic package docs
to spell out better.

Versions for other stand-alone Atomic objects (AtomicLong,
AtomicReference) would be equally straightforward. As always, the
main problems lie in the FieldUpdater forms, which are sadly
also the most useful. The dynamic type checking overhead
required for these forms is often more expensive than
any savings you get from weaker access modes. (This is
why Fences versions continue to be attractive.) However,
for uniformity, extended methods in these would also be
supported. Similarly for the Atomic*Array classes.
And it is still conceivable that new JVM mechanics being put
into place for JSR292 might be used to reduce overhead.

One further accommodation is that the encapsulated int
state value inside AbstractQueuedSynchronizer should also
support at least the setInStoreOrder method (none of
the other new methods seem to ever apply). Similarly
for AbstractQueuedLongSynchronizer. Using this as
appropriate speeds up ReentrantLock by 5% - 20%, so
is a good idea regardless of all other changes.

-Doug


From bryan at systap.com  Wed Apr 14 09:53:09 2010
From: bryan at systap.com (Bryan Thompson)
Date: Wed, 14 Apr 2010 08:53:09 -0500
Subject: [concurrency-interest] Executors: Wrapper to cancel tasks in
	shutdownNow()?
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED44D612AE500@AUSP01VMBX06.collaborationhost.net>

Hello,

Is there some handy delegation pattern available, such as found on Executors, to wrap an ExecutorService implementation such that ExecutorService#shutdownNow() cancels each task in the returned list?  While this might not always be want people want to do, it does seem to be a very common pattern.

Thanks,
Bryan

From tim at peierls.net  Wed Apr 14 10:41:45 2010
From: tim at peierls.net (Tim Peierls)
Date: Wed, 14 Apr 2010 10:41:45 -0400
Subject: [concurrency-interest] Executors: Wrapper to cancel tasks in
	shutdownNow()?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED44D612AE500@AUSP01VMBX06.collaborationhost.net>
References: <DE10B00CCE0DC54883734F3060AC9ED44D612AE500@AUSP01VMBX06.collaborationhost.net>
Message-ID: <y2n63b4e4051004140741w31f3fe22of726599551a71749@mail.gmail.com>

The tasks returned by shutdownNow haven't begun execution, so there's no
need to cancel them.

--tim

On Wed, Apr 14, 2010 at 9:53 AM, Bryan Thompson <bryan at systap.com> wrote:

> Hello,
>
> Is there some handy delegation pattern available, such as found on
> Executors, to wrap an ExecutorService implementation such that
> ExecutorService#shutdownNow() cancels each task in the returned list?  While
> this might not always be want people want to do, it does seem to be a very
> common pattern.
>
> Thanks,
> Bryan
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100414/2d4ed8a1/attachment.html>

From bryan at systap.com  Wed Apr 14 10:42:51 2010
From: bryan at systap.com (Bryan Thompson)
Date: Wed, 14 Apr 2010 09:42:51 -0500
Subject: [concurrency-interest] Executors: Wrapper to cancel tasks in
 shutdownNow()?
In-Reply-To: <y2n63b4e4051004140741w31f3fe22of726599551a71749@mail.gmail.com>
References: <DE10B00CCE0DC54883734F3060AC9ED44D612AE500@AUSP01VMBX06.collaborationhost.net>
	<y2n63b4e4051004140741w31f3fe22of726599551a71749@mail.gmail.com>
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED44D612AE543@AUSP01VMBX06.collaborationhost.net>

Yes, but threads can already be waiting on their Future's, right? Bryan

________________________________
From: tpeierls at gmail.com [mailto:tpeierls at gmail.com] On Behalf Of Tim Peierls
Sent: Wednesday, April 14, 2010 10:42 AM
To: Bryan Thompson
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Executors: Wrapper to cancel tasks in shutdownNow()?

The tasks returned by shutdownNow haven't begun execution, so there's no need to cancel them.

--tim

On Wed, Apr 14, 2010 at 9:53 AM, Bryan Thompson <bryan at systap.com<mailto:bryan at systap.com>> wrote:
Hello,

Is there some handy delegation pattern available, such as found on Executors, to wrap an ExecutorService implementation such that ExecutorService#shutdownNow() cancels each task in the returned list?  While this might not always be want people want to do, it does seem to be a very common pattern.

Thanks,
Bryan
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100414/c33dc818/attachment-0001.html>

From bryan at systap.com  Wed Apr 14 10:55:00 2010
From: bryan at systap.com (Bryan Thompson)
Date: Wed, 14 Apr 2010 09:55:00 -0500
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <4BC5BA9E.9010906@cs.oswego.edu>
References: <4BC5BA9E.9010906@cs.oswego.edu>
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED44D612AE565@AUSP01VMBX06.collaborationhost.net>

Doug,

What about eventually consistent increment() and add() methods which do not rely on CAS operations and are thus not subject to spins under concurrent requests?  There are plenty of use cases where all I want is to increment a counter and the atomicity of the operation is not required, just the guarantee that the counter will (eventually) reflect all increment() and decrement() requests.

Bryan

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Doug Lea
> Sent: Wednesday, April 14, 2010 8:53 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Extended access methods for 
> Atomics (and AQS)
> 
> Last fall, we tabled discussion of the proposed Fences API.
> But regardless of the outcome of and future discussions when 
> we re-raise it, there seemed to be consensus that at the very 
> least, we should extend the methods of Atomic classes to 
> support the various memory effect modes that you otherwise 
> would need Fences methods to obtain.
> 
> I'm finally trying this out. A draft update of AtomicInteger 
> with these changes is at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurr
> ent/atomic/AtomicInteger.html
> (For now, only class AtomicInteger). Ignoring the arithmetic 
> methods (getAndAdd etc), the API looks like (with "*" in 
> front of the added methods)
> 
> "volatile-write" mode
>     void set(int v)
>     boolean compareAndSet(int e, int v);
>     int setAndGet(int v);
> 
> store-ordered (aka release, pseudo-final-field) mode
> *  void setInStoreOrder(int v);
> *  boolean compareAndSetInStoreOrder(int e, int v);
>     void lazySet(int v) // synonym for setInStoreOrder
> 
> load-ordered (aka volatile-read, acquire) mode
>     int get();
> *  compareAndSetAndGet(int e, int v);
> 
> relaxed-order (aka non-volatile) mode
> *  int getInRelaxedOrder();
> *  void setInRelaxedOrder(int v);
>     boolean weakCompareAndSet(int e, int v);
> 
> Comments and suggestions about method names and semantics 
> would be very welcome. (Notice that the naming scheme 
> denigrates "lazySet", a name that no one likes!)
> 
> The form of these methods is roughly similar to
> C++0x modes (at least the ones supportable in Java),
> but avoids the sometimes-controversial terms "acquire" and 
> "release" in mode/method names.
> 
> For now, the specs are just done informally. Assuming we go 
> ahead with this, we'd adapt the more formal versions done in 
> Fences drafts and place them in j.u.c.atomic package docs to 
> spell out better.
> 
> Versions for other stand-alone Atomic objects (AtomicLong,
> AtomicReference) would be equally straightforward. As always, 
> the main problems lie in the FieldUpdater forms, which are 
> sadly also the most useful. The dynamic type checking 
> overhead required for these forms is often more expensive 
> than any savings you get from weaker access modes. (This is 
> why Fences versions continue to be attractive.) However, for 
> uniformity, extended methods in these would also be 
> supported. Similarly for the Atomic*Array classes.
> And it is still conceivable that new JVM mechanics being put 
> into place for JSR292 might be used to reduce overhead.
> 
> One further accommodation is that the encapsulated int state 
> value inside AbstractQueuedSynchronizer should also support 
> at least the setInStoreOrder method (none of the other new 
> methods seem to ever apply). Similarly for 
> AbstractQueuedLongSynchronizer. Using this as appropriate 
> speeds up ReentrantLock by 5% - 20%, so is a good idea 
> regardless of all other changes.
> 
> -Doug
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From tim at peierls.net  Wed Apr 14 11:00:30 2010
From: tim at peierls.net (Tim Peierls)
Date: Wed, 14 Apr 2010 11:00:30 -0400
Subject: [concurrency-interest] Executors: Wrapper to cancel tasks in
	shutdownNow()?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED44D612AE543@AUSP01VMBX06.collaborationhost.net>
References: <DE10B00CCE0DC54883734F3060AC9ED44D612AE500@AUSP01VMBX06.collaborationhost.net>
	<y2n63b4e4051004140741w31f3fe22of726599551a71749@mail.gmail.com>
	<DE10B00CCE0DC54883734F3060AC9ED44D612AE543@AUSP01VMBX06.collaborationhost.net>
Message-ID: <j2z63b4e4051004140800y1bf5f884lbccac7abaf873107@mail.gmail.com>

Right, sorry. Would something like this do the trick for you?

public class Util {
    private Util() { /* uninstantiable */ }

    public static void shutdownNowCancelingUnstarted(ExecutorService exec) {
        for (Runnable unstarted : exec.shutdownNow()) {
            if (unstarted instanceof RunnableFuture) {
                RunnableFuture<?> task = (RunnableFuture<?>) unstarted;
                task.cancel(false);
            }
        }
    }
}

--tim

On Wed, Apr 14, 2010 at 10:42 AM, Bryan Thompson <bryan at systap.com> wrote:

>  Yes, but threads can already be waiting on their Future's, right? Bryan
>
>  ------------------------------
> *From:* tpeierls at gmail.com [mailto:tpeierls at gmail.com] *On Behalf Of *Tim
> Peierls
> *Sent:* Wednesday, April 14, 2010 10:42 AM
> *To:* Bryan Thompson
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Executors: Wrapper to cancel tasks
> in shutdownNow()?
>
> The tasks returned by shutdownNow haven't begun execution, so there's no
> need to cancel them.
>
> --tim
>
> On Wed, Apr 14, 2010 at 9:53 AM, Bryan Thompson <bryan at systap.com> wrote:
>
>> Hello,
>>
>> Is there some handy delegation pattern available, such as found on
>> Executors, to wrap an ExecutorService implementation such that
>> ExecutorService#shutdownNow() cancels each task in the returned list?  While
>> this might not always be want people want to do, it does seem to be a very
>> common pattern.
>>
>> Thanks,
>> Bryan
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100414/3877c42d/attachment.html>

From bryan at systap.com  Wed Apr 14 11:05:55 2010
From: bryan at systap.com (Bryan Thompson)
Date: Wed, 14 Apr 2010 10:05:55 -0500
Subject: [concurrency-interest] Executors: Wrapper to cancel tasks in
 shutdownNow()?
In-Reply-To: <j2z63b4e4051004140800y1bf5f884lbccac7abaf873107@mail.gmail.com>
References: <DE10B00CCE0DC54883734F3060AC9ED44D612AE500@AUSP01VMBX06.collaborationhost.net>
	<y2n63b4e4051004140741w31f3fe22of726599551a71749@mail.gmail.com>
	<DE10B00CCE0DC54883734F3060AC9ED44D612AE543@AUSP01VMBX06.collaborationhost.net>
	<j2z63b4e4051004140800y1bf5f884lbccac7abaf873107@mail.gmail.com>
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED44D612AE580@AUSP01VMBX06.collaborationhost.net>

Yes it would.

This sort of thing is not really called out in the javdoc for ExecutorService#shutdownNow().  I was thinking that it would be nice if people could use a standard wrapper ala the Executors delegation patterns for this.  That might help to reduce problems whose cause is a task which was never cancelled due to shutdownNow().

Bryan

________________________________
From: tpeierls at gmail.com [mailto:tpeierls at gmail.com] On Behalf Of Tim Peierls
Sent: Wednesday, April 14, 2010 11:01 AM
To: Bryan Thompson
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Executors: Wrapper to cancel tasks in shutdownNow()?

Right, sorry. Would something like this do the trick for you?

public class Util {
    private Util() { /* uninstantiable */ }

    public static void shutdownNowCancelingUnstarted(ExecutorService exec) {
        for (Runnable unstarted : exec.shutdownNow()) {
            if (unstarted instanceof RunnableFuture) {
                RunnableFuture<?> task = (RunnableFuture<?>) unstarted;
                task.cancel(false);
            }
        }
    }
}

--tim

On Wed, Apr 14, 2010 at 10:42 AM, Bryan Thompson <bryan at systap.com<mailto:bryan at systap.com>> wrote:
Yes, but threads can already be waiting on their Future's, right? Bryan

________________________________
From: tpeierls at gmail.com<mailto:tpeierls at gmail.com> [mailto:tpeierls at gmail.com<mailto:tpeierls at gmail.com>] On Behalf Of Tim Peierls
Sent: Wednesday, April 14, 2010 10:42 AM
To: Bryan Thompson
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Executors: Wrapper to cancel tasks in shutdownNow()?

The tasks returned by shutdownNow haven't begun execution, so there's no need to cancel them.

--tim

On Wed, Apr 14, 2010 at 9:53 AM, Bryan Thompson <bryan at systap.com<mailto:bryan at systap.com>> wrote:
Hello,

Is there some handy delegation pattern available, such as found on Executors, to wrap an ExecutorService implementation such that ExecutorService#shutdownNow() cancels each task in the returned list?  While this might not always be want people want to do, it does seem to be a very common pattern.

Thanks,
Bryan
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100414/7376812d/attachment.html>

From tim at peierls.net  Wed Apr 14 11:41:11 2010
From: tim at peierls.net (Tim Peierls)
Date: Wed, 14 Apr 2010 11:41:11 -0400
Subject: [concurrency-interest] Executors: Wrapper to cancel tasks in
	shutdownNow()?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED44D612AE580@AUSP01VMBX06.collaborationhost.net>
References: <DE10B00CCE0DC54883734F3060AC9ED44D612AE500@AUSP01VMBX06.collaborationhost.net>
	<y2n63b4e4051004140741w31f3fe22of726599551a71749@mail.gmail.com>
	<DE10B00CCE0DC54883734F3060AC9ED44D612AE543@AUSP01VMBX06.collaborationhost.net>
	<j2z63b4e4051004140800y1bf5f884lbccac7abaf873107@mail.gmail.com>
	<DE10B00CCE0DC54883734F3060AC9ED44D612AE580@AUSP01VMBX06.collaborationhost.net>
Message-ID: <w2i63b4e4051004140841j70f7ea28k114cf83223f3de83@mail.gmail.com>

It's not hard to create a forwarding wrapper with AbstractExecutorService
(although it would help to have a standard ForwardingExecutorService), but
why go to the trouble of burning the decision of whether to cancel the tasks
returned by shutdownNow into the executor service at construction, when you
can have a simple utility method that lets you decide at the moment of
shutdown?

    Util.shutdownNow(exec, cancelUnstartedTasks);

It's a decision that you'd want to make as late as possible, I would think.
And it hardly seems worth standardizing a simple for-loop. I could see
sticking it in as example code in ExecutorService.

--tim

On Wed, Apr 14, 2010 at 11:05 AM, Bryan Thompson <bryan at systap.com> wrote:

>  Yes it would.
>
> This sort of thing is not really called out in the javdoc for
> ExecutorService#shutdownNow().  I was thinking that it would be nice if
> people could use a standard wrapper ala the Executors delegation patterns
> for this.  That might help to reduce problems whose cause is a task which
> was never cancelled due to shutdownNow().
>
> Bryan
>
>  ------------------------------
> *From:* tpeierls at gmail.com [mailto:tpeierls at gmail.com] *On Behalf Of *Tim
> Peierls
> *Sent:* Wednesday, April 14, 2010 11:01 AM
>
> *To:* Bryan Thompson
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Executors: Wrapper to cancel tasks
> in shutdownNow()?
>
> Right, sorry. Would something like this do the trick for you?
>
> public class Util {
>     private Util() { /* uninstantiable */ }
>
>     public static void shutdownNowCancelingUnstarted(ExecutorService exec)
> {
>         for (Runnable unstarted : exec.shutdownNow()) {
>             if (unstarted instanceof RunnableFuture) {
>                 RunnableFuture<?> task = (RunnableFuture<?>) unstarted;
>                 task.cancel(false);
>             }
>         }
>     }
> }
>
> --tim
>
> On Wed, Apr 14, 2010 at 10:42 AM, Bryan Thompson <bryan at systap.com> wrote:
>
>>  Yes, but threads can already be waiting on their Future's, right? Bryan
>>
>>  ------------------------------
>> *From:* tpeierls at gmail.com [mailto:tpeierls at gmail.com] *On Behalf Of *Tim
>> Peierls
>> *Sent:* Wednesday, April 14, 2010 10:42 AM
>> *To:* Bryan Thompson
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Executors: Wrapper to cancel tasks
>> in shutdownNow()?
>>
>>   The tasks returned by shutdownNow haven't begun execution, so there's
>> no need to cancel them.
>>
>> --tim
>>
>> On Wed, Apr 14, 2010 at 9:53 AM, Bryan Thompson <bryan at systap.com> wrote:
>>
>>> Hello,
>>>
>>> Is there some handy delegation pattern available, such as found on
>>> Executors, to wrap an ExecutorService implementation such that
>>> ExecutorService#shutdownNow() cancels each task in the returned list?  While
>>> this might not always be want people want to do, it does seem to be a very
>>> common pattern.
>>>
>>> Thanks,
>>> Bryan
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100414/a76df23d/attachment-0001.html>

From cpovirk at google.com  Wed Apr 14 13:11:28 2010
From: cpovirk at google.com (Chris Povirk)
Date: Wed, 14 Apr 2010 13:11:28 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for
	calls that "will never be interrupted"
In-Reply-To: <4BC59F81.5000807@cs.oswego.edu>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>
	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>
	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>
	<4BC0817E.50205@cs.oswego.edu>
	<n2t77492afe1004131844xbf49a2e2s6b5ddebd118e451e@mail.gmail.com>
	<4BC59F81.5000807@cs.oswego.edu>
Message-ID: <v2i77492afe1004141011s8f143c93m807dbc75b15dce28@mail.gmail.com>

Given that failures are inevitable in an RPC system, a join() method
without a checked exception is a non-starter for what I have in mind.
Maybe a class like SwingWorker, which hands off a task to another
thread but isn't necessarily expected to fail (though it allows for it
by declaring "throws Exception"), would benefit from such an
interface.  That's a pretty small niche, though, to justify an
interface on top of the ForkJoinTask class.

I can certainly see the mess we end up in by trying to support all the
interruptible / checked exception combinations.  And that's all
ignoring timeouts.  It's almost enough to make me want to write:

public <T extends Exception, F extends Exception, I extends Exception>
    V megaGet(
        TimeoutPolicy<T> timeoutPolicy,
        FailurePolicy<F> failurePolicy,
        InterruptionPolicy<I> interruptionPolicy) throws T, F, I;

But again, I think maybe I can console myself with "restore interrupt,
wrap with RpcException, and throw" to handle interrupts when I'm
already declaring a checked RpcException.  Is this totally off base?

From edharned2002 at yahoo.com  Wed Apr 14 15:55:36 2010
From: edharned2002 at yahoo.com (Edward Harned)
Date: Wed, 14 Apr 2010 12:55:36 -0700 (PDT)
Subject: [concurrency-interest] InterruptedException-free wrappers for
	calls that "will never be interrupted"
In-Reply-To: <v2i77492afe1004141011s8f143c93m807dbc75b15dce28@mail.gmail.com>
Message-ID: <432445.31305.qm@web35503.mail.mud.yahoo.com>

Chris:

What an interesting session. 
But the real problem is interrupt().

You can sing and dance all day trying to handle InterruptedException
and re-throwing this and that, 
but you will never achieve a sustainable goal until you solve the basic problem.

Different programmers want to interrupt different code at different times 
and they think the interrupted code should behave the way the 
interrupters want it to behave. That can never be.

Doug Lea wrote, 
"interruption of any subtask wait doesn't imply anything ... 
instead you need explicit cancel()." 
Exactly. Dead on.

Do you want to interrupt some type of "wait" or 
do you want to tell the executing thread to stop what it is doing 
after the current iteration or drop dead now? 
And try to roll-back anything that it can or just leave it alone? 
etc.

interrupt() is currently overused/misused/course-grained in wishful thinking 
and can lead to erroneous results. 
It seems to me your options (as mentioned in your first post) 
can only make it worse. 
In both the concurrency environments mentioned by Doug Lea, we need a cancel().

Implement various, specific cancel() methods 
instead of trying to handle the indefinite interrupt().

Implementing a cancel() in an RPC environment requires a second client thread to 
"inform" the server code of the desired action since the original caller is
suspended waiting for a reply. 
(RPC is a a wee bit different scenario than your original "goal.") 

Ed





      

From davidcholmes at aapt.net.au  Wed Apr 14 19:49:35 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 15 Apr 2010 09:49:35 +1000
Subject: [concurrency-interest] Extended access methods for Atomics (and
	AQS)
In-Reply-To: <4BC5BA9E.9010906@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>

Hi Doug,

I think "InStoreorder" and "InRelaxedOrder" could easily be misunderstood as
relating to "total store ordering", or "relaxed memory models" or any
particular architecural memory model that uses that kind of terminology.

If "InStoreOrder" means "with memory effects equivalent to setting a final
variable" then lets just say that:

   setAsIfFinal(int newVal)

and similarly:

   setAsNonVolatile(int newVal)


I guess this is somewhat better than Fences in that the semantics of the
methods are easier to understand. But these are still methods that the vast
majority of programmers won't know when it is valid to use them. They will
only discover that they seem faster and so use them regardless :( Can we not
"hide" them a bit better eg:

  class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicInteger extends
AtomicInteger {
      public void setAsNonVolatile(int newVal) { ... }
      ...
  }

or a utility class

  class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicHelper {
     static void setAsNonVolatile(AtomicInteger x, int newVal) { ... }
     ...
  }

> One further accommodation is that the encapsulated int
> state value inside AbstractQueuedSynchronizer should also
> support at least the setInStoreOrder method

I fear the chance of misuse greatly outweighs any performance benefit.


Aside: if I ever see a bug report involving these new methods my response
will be to request the submitter to rewrite their program using the "proper"
methods. Without tools or formalisms to establish correctness of use these
methods will in many cases just be bugs waiting to happen.

Cheers,
David


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug Lea
> Sent: Wednesday, 14 April 2010 10:53 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Extended access methods for Atomics (and
> AQS)
>
>
> Last fall, we tabled discussion of the proposed Fences API.
> But regardless of the outcome of and future discussions when
> we re-raise it, there seemed to be consensus that at the very
> least, we should extend the methods of Atomic classes to support
> the various memory effect modes that you otherwise would need
> Fences methods to obtain.
>
> I'm finally trying this out. A draft update of AtomicInteger
> with these changes is at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
> atomic/AtomicInteger.html
> (For now, only class AtomicInteger). Ignoring the arithmetic methods
> (getAndAdd etc), the API looks like (with "*" in front of
> the added methods)
>
> "volatile-write" mode
>     void set(int v)
>     boolean compareAndSet(int e, int v);
>     int setAndGet(int v);
>
> store-ordered (aka release, pseudo-final-field) mode
> *  void setInStoreOrder(int v);
> *  boolean compareAndSetInStoreOrder(int e, int v);
>     void lazySet(int v) // synonym for setInStoreOrder
>
> load-ordered (aka volatile-read, acquire) mode
>     int get();
> *  compareAndSetAndGet(int e, int v);
>
> relaxed-order (aka non-volatile) mode
> *  int getInRelaxedOrder();
> *  void setInRelaxedOrder(int v);
>     boolean weakCompareAndSet(int e, int v);
>
> Comments and suggestions about method names and
> semantics would be very welcome. (Notice that
> the naming scheme denigrates "lazySet", a name
> that no one likes!)
>
> The form of these methods is roughly similar to
> C++0x modes (at least the ones supportable in Java),
> but avoids the sometimes-controversial terms
> "acquire" and "release" in mode/method names.
>
> For now, the specs are just done informally. Assuming we
> go ahead with this, we'd adapt the more formal versions done
> in Fences drafts and place them in j.u.c.atomic package docs
> to spell out better.
>
> Versions for other stand-alone Atomic objects (AtomicLong,
> AtomicReference) would be equally straightforward. As always, the
> main problems lie in the FieldUpdater forms, which are sadly
> also the most useful. The dynamic type checking overhead
> required for these forms is often more expensive than
> any savings you get from weaker access modes. (This is
> why Fences versions continue to be attractive.) However,
> for uniformity, extended methods in these would also be
> supported. Similarly for the Atomic*Array classes.
> And it is still conceivable that new JVM mechanics being put
> into place for JSR292 might be used to reduce overhead.
>
> One further accommodation is that the encapsulated int
> state value inside AbstractQueuedSynchronizer should also
> support at least the setInStoreOrder method (none of
> the other new methods seem to ever apply). Similarly
> for AbstractQueuedLongSynchronizer. Using this as
> appropriate speeds up ReentrantLock by 5% - 20%, so
> is a good idea regardless of all other changes.
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Wed Apr 14 19:56:44 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 15 Apr 2010 09:56:44 +1000
Subject: [concurrency-interest] InterruptedException-free wrappers
	forcalls that "will never be interrupted"
In-Reply-To: <v2i77492afe1004141011s8f143c93m807dbc75b15dce28@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEOHIFAA.davidcholmes@aapt.net.au>

Chris,

I've read this thread but still don't understand what problem you think
needs solving. Anything that aids developers in not thinking about
interrupts and cancellation when they are utilizing blocking methods is just
a bad idea.

> But again, I think maybe I can console myself with "restore interrupt,
> wrap with RpcException, and throw" to handle interrupts when I'm
> already declaring a checked RpcException.  Is this totally off base?

If interruption is just one of many potential failures modes then converting
it to a general exception and re-asserting the interrupt is a perfectly
legitimate response. REF CPJ 2e.

Cheers,
David Holmes


From dl at cs.oswego.edu  Thu Apr 15 07:06:36 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 15 Apr 2010 07:06:36 -0400
Subject: [concurrency-interest] InterruptedException-free wrappers for
 calls that "will never be interrupted"
In-Reply-To: <v2i77492afe1004141011s8f143c93m807dbc75b15dce28@mail.gmail.com>
References: <t2s77492afe1004080909n3efb2eccmb722e0e20be5abed@mail.gmail.com>	<i2g77492afe1004080930v235baaf6jcd15da1ebce5433f@mail.gmail.com>	<k2n77492afe1004080946i6d8850a9t8b922f25cfc60913@mail.gmail.com>	<4BC0817E.50205@cs.oswego.edu>	<n2t77492afe1004131844xbf49a2e2s6b5ddebd118e451e@mail.gmail.com>	<4BC59F81.5000807@cs.oswego.edu>
	<v2i77492afe1004141011s8f143c93m807dbc75b15dce28@mail.gmail.com>
Message-ID: <4BC6F33C.3020907@cs.oswego.edu>

On 04/14/10 13:11, Chris Povirk wrote:
> Given that failures are inevitable in an RPC system, a join() method
> without a checked exception is a non-starter for what I have in mind.

OK. It seems that you should proceed with a set of
wrappers that are specific to your RPC framework.

And given this, unless others think it is worthwhile, we don't
yet have enough motivation to introduce JoinableFuture. Some
of us like JoinableFuture as a nice conceptual bridge between
Future and ForkJoinTask, but that might not be a good enough
reason to add it. If anyone has a better rationale,
along with use cases, please let us know.


-Doug



From dl at cs.oswego.edu  Thu Apr 15 07:57:11 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 15 Apr 2010 07:57:11 -0400
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
Message-ID: <4BC6FF17.7060308@cs.oswego.edu>

On 04/14/10 19:49, David Holmes wrote:

> I think "InStoreorder" and "InRelaxedOrder" could easily be misunderstood as
> relating to "total store ordering", or "relaxed memory models" or any
> particular architecural memory model that uses that kind of terminology.
>

How about "setInStoreFencedOrder" (borrowing from one
version of our Fences API terminology)?  Note that this method
is the same as existing "lazySet", which we adopted in part because
people wanted an obscure name, but it is so obscure that people
don't even try to understand it. (Mainly because the "lazy" in lazySet
is deceptive. As a quality of implementation matter, store-fenced
writes should be issued as soon as eligible, and are in hotspot
and other JVMs. The sense of laziness is only wrt Sequential
Consistency, which is a connection most people don't make.
Also, as Hans has pointed out a few times, the specs for this
method really ought to be spelled out along the lines of what we
did for Fences.storeFence (aka orderWrites).)

I don't see the problem with using the term "relaxed"
to mean "non-volatile, non-final", as introduced by the C++0x folks.
You need some term to denote this (just "non-volatile" is not quite
accurate), so might as well observe precedent?

> I guess this is somewhat better than Fences in that the semantics of the
> methods are easier to understand. But these are still methods that the vast
> majority of programmers won't know when it is valid to use them. They will
> only discover that they seem faster and so use them regardless :( Can we not
> "hide" them a bit better

Well, the current options are maximally hidden and maximally ugly.
As people have already pointed out, those who do know when to
use them currently need to access them via Unsafe. Which, as always,
bothers me. People increasingly take this path, making it even
harder to get constructions right, and making it impossible to run their
code on platforms with security managers that prevent Unsafe access.
Would you rather see this practice continue?

While I am at it ...

The AtomicInteger draft contains store- and load- fenced
forms of CAS that represent the only "new" functionality
introduced here:
   boolean compareAndSetInStoreOrder(int e, int v);
   int compareAndSetAndGet(int e, int v);
These provide orderings for CAS matching
the ones for get and set. Some people have been
asking for them for a long time (especially for Azul and
Itanium, also probably worthwhile on ARM and POWER).
They are not at all commonly used, and are always
implementable using plain CAS. But defining them makes
available for special intrinsification on some platforms.
This is a similar story as we have already for
weakCompareAndSet, which I think these days is only
specially intrinsified on Azul's JVM.

-Doug

From davidcholmes at aapt.net.au  Thu Apr 15 08:49:47 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 15 Apr 2010 22:49:47 +1000
Subject: [concurrency-interest] Extended access methods for Atomics (and
	AQS)
In-Reply-To: <4BC6FF17.7060308@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>

Hi Doug,

Doug Lea writes:
> On 04/14/10 19:49, David Holmes wrote:
>
> > I think "InStoreorder" and "InRelaxedOrder" could easily be
> misunderstood as
> > relating to "total store ordering", or "relaxed memory models" or any
> > particular architecural memory model that uses that kind of terminology.
> >
>
> How about "setInStoreFencedOrder" (borrowing from one
> version of our Fences API terminology)?  Note that this method
> is the same as existing "lazySet", which we adopted in part because
> people wanted an obscure name, but it is so obscure that people
> don't even try to understand it. (Mainly because the "lazy" in lazySet
> is deceptive. As a quality of implementation matter, store-fenced
> writes should be issued as soon as eligible, and are in hotspot
> and other JVMs. The sense of laziness is only wrt Sequential
> Consistency, which is a connection most people don't make.
> Also, as Hans has pointed out a few times, the specs for this
> method really ought to be spelled out along the lines of what we
> did for Fences.storeFence (aka orderWrites).)
>
> I don't see the problem with using the term "relaxed"
> to mean "non-volatile, non-final", as introduced by the C++0x folks.
> You need some term to denote this (just "non-volatile" is not quite
> accurate), so might as well observe precedent?

If non-volatile doesn't accurately describe it then using the term in the
name would be misleading. In that case no simple naming scheme seems to
exist - whatever you call it, it will be obscure and certainly
non-intuitive. "relaxed" might be as good a term as any other, though it
leaves me wondering what the difference is between a "relaxed" store and a
plain store ?

> > I guess this is somewhat better than Fences in that the semantics of the
> > methods are easier to understand. But these are still methods
> > that the vast majority of programmers won't know when it is valid to use
> > them. They will only discover that they seem faster and so use them
regardless
> :( Can we not "hide" them a bit better
>
> Well, the current options are maximally hidden and maximally ugly.
> As people have already pointed out, those who do know when to
> use them currently need to access them via Unsafe. Which, as always,
> bothers me. People increasingly take this path, making it even
> harder to get constructions right, and making it impossible to run their
> code on platforms with security managers that prevent Unsafe access.
> Would you rather see this practice continue?

If these were the only choices - yes. But we can less maximally hide by
moving into a public API. I just don't think it is a good idea to have these
obscure, rarely usable methods sitting along side the methods that get used
all the time.

> While I am at it ...
>
> The AtomicInteger draft contains store- and load- fenced
> forms of CAS that represent the only "new" functionality
> introduced here:
>    boolean compareAndSetInStoreOrder(int e, int v);
>    int compareAndSetAndGet(int e, int v);
> These provide orderings for CAS matching
> the ones for get and set. Some people have been
> asking for them for a long time (especially for Azul and
> Itanium, also probably worthwhile on ARM and POWER).
> They are not at all commonly used, and are always
> implementable using plain CAS. But defining them makes
> available for special intrinsification on some platforms.
> This is a similar story as we have already for
> weakCompareAndSet, which I think these days is only
> specially intrinsified on Azul's JVM.

weakCAS has a much cleaner story than these relaxed orderings - to allow for
the "spurious" failure of ll/sc based implementations. That's a lot simpler
to explain than relazed ordering stuff.

Perhaps I'm missing a piece of the picture here. Is there some formalism or
methodology that can be readily used to determine when an algorithm supports
these weakly ordered variants? If that's not the case then I confidently
claim that most uses of these methods will in fact be incorrect. People
(even those that should know better) will opt for the seemingly more
performant code over the correct code.

David

> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Thu Apr 15 10:45:14 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 15 Apr 2010 10:45:14 -0400
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED44D612AE565@AUSP01VMBX06.collaborationhost.net>
References: <4BC5BA9E.9010906@cs.oswego.edu>
	<DE10B00CCE0DC54883734F3060AC9ED44D612AE565@AUSP01VMBX06.collaborationhost.net>
Message-ID: <4BC7267A.5080306@cs.oswego.edu>

On 04/14/10 10:55, Bryan Thompson wrote:

> What about eventually consistent increment() and add() methods which do not
> rely on CAS operations and are thus not subject to spins under concurrent
> requests?

We do define the AtomicInteger increment and add operations so
they can be intrinsified on platforms supporting atomic arithmetic
read-modify-write. I don't think any JVMs do so; probably
because these aren't usually faster, or enough faster, on most
platforms than CAS versions to bother.

-Doug


From davidcholmes at aapt.net.au  Thu Apr 15 20:01:19 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 16 Apr 2010 10:01:19 +1000
Subject: [concurrency-interest] Extended access methods for Atomics (and
	AQS)
In-Reply-To: <4BC7267A.5080306@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEOPIFAA.davidcholmes@aapt.net.au>

Doug,

Doug writes:
> On 04/14/10 10:55, Bryan Thompson wrote:
>
> > What about eventually consistent increment() and add() methods
> > which do not rely on CAS operations and are thus not subject to
> > spins under concurrent requests?
>
> We do define the AtomicInteger increment and add operations so
> they can be intrinsified on platforms supporting atomic arithmetic
> read-modify-write.

No we define them using CAS at the Java level. I think this is necessary to
ensure we can return previous values "correctly". For example we might use a
lock:add on x86 to do an atomic addition, but we don't have a way to
atomically capture the previous value.

David

I don't think any JVMs do so; probably
> because these aren't usually faster, or enough faster, on most
> platforms than CAS versions to bother.
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From hans.boehm at hp.com  Thu Apr 15 20:46:56 2010
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 16 Apr 2010 00:46:56 +0000
Subject: [concurrency-interest] Extended access methods for Atomics
	(and	AQS)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>
References: <4BC6FF17.7060308@cs.oswego.edu>
	<NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>
Message-ID: <238A96A773B3934685A7269CC8A8D0426DF5A1C9C4@GVW0436EXB.americas.hpqcorp.net>

Even if we don't immediately give these any sort of formal semantics, it would be good to be clearer on what these are intended to mean.  That also affects the naming.  I think the semantics are currently unclear along a few dimensions:

1) Do they implicitly include fences?  I think we agree that the answer is no, i.e.

x1 = 1;
a.setInStoreOrder(1);
x2 = 2;

does not guarantee that the assignments to x1 and x2 become visible in order.  If this is correct, and I strongly believe it should be, I think we should avoid mentioning "fence" in the name.

2) Do they obey "cache coherency" rules, i.e. do the operations on a single variable appear totally ordered?  In particular, if we have x initially zero and:

Thread 1:
x = 1;
x = 2;

Thread 2:
r1 = x;
r2 = x;

Can we have r2 < r1?  Ordinary Java variables intentionally allow this.  When this was discussed in the context of C++0x atomic<T>, the feeling was that this was too weakly ordered to be useful.  I tent to agree.  (This does affect implementation cost, due both to the "reads kill" issue, and because a few architectures, notably Itanium, allow the unexpected behavior for ordinary loads.)  If this is disallowed, we probably shouldn't name this to suggest that it's equivalent to ordinary memory operations.  And I suspect it should be disallowed.

3) Can I really use "StoreOrder" variants to emulate "final"?  To get "final" semantics, I may need a fence or other special instruction on the store side, but I also need to make sure that when I read a pseudo-final field x.pf, the load of x and x.pf are dependent in the object code, and the hardware enforces ordering based on that dependency.  Current hardware usually does the right thing, and the compiler probably already does, too.  (Unless it does some sort of value speculation.) But this would be a very subtle change to the semantics of ordinary field accesses.  And getting the spec right is tricky.  In particular, I suspect we don't want to guarantee that the load of i in a[i] is implicitly ordered before that of a[i]. (Consider the case in which the compiler knows that a has exactly one element.)  I think this affects whether we want to mention "final" in the name.

Hans


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of David Holmes
> Sent: Thursday, April 15, 2010 5:50 AM
> To: Doug Lea; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Extended access methods 
> for Atomics (and AQS)
> 
> Hi Doug,
> 
> Doug Lea writes:
> > On 04/14/10 19:49, David Holmes wrote:
> >
> > > I think "InStoreorder" and "InRelaxedOrder" could easily be
> > misunderstood as
> > > relating to "total store ordering", or "relaxed memory models" or 
> > > any particular architecural memory model that uses that 
> kind of terminology.
> > >
> >
> > How about "setInStoreFencedOrder" (borrowing from one 
> version of our 
> > Fences API terminology)?  Note that this method is the same as 
> > existing "lazySet", which we adopted in part because people 
> wanted an 
> > obscure name, but it is so obscure that people don't even try to 
> > understand it. (Mainly because the "lazy" in lazySet is 
> deceptive. As 
> > a quality of implementation matter, store-fenced writes should be 
> > issued as soon as eligible, and are in hotspot and other JVMs. The 
> > sense of laziness is only wrt Sequential Consistency, which is a 
> > connection most people don't make.
> > Also, as Hans has pointed out a few times, the specs for 
> this method 
> > really ought to be spelled out along the lines of what we did for 
> > Fences.storeFence (aka orderWrites).)
> >
> > I don't see the problem with using the term "relaxed"
> > to mean "non-volatile, non-final", as introduced by the C++0x folks.
> > You need some term to denote this (just "non-volatile" is not quite 
> > accurate), so might as well observe precedent?
> 
> If non-volatile doesn't accurately describe it then using the 
> term in the name would be misleading. In that case no simple 
> naming scheme seems to exist - whatever you call it, it will 
> be obscure and certainly non-intuitive. "relaxed" might be as 
> good a term as any other, though it leaves me wondering what 
> the difference is between a "relaxed" store and a plain store ?
> 
> > > I guess this is somewhat better than Fences in that the 
> semantics of 
> > > the methods are easier to understand. But these are still methods 
> > > that the vast majority of programmers won't know when it 
> is valid to 
> > > use them. They will only discover that they seem faster 
> and so use 
> > > them
> regardless
> > :( Can we not "hide" them a bit better
> >
> > Well, the current options are maximally hidden and maximally ugly.
> > As people have already pointed out, those who do know when 
> to use them 
> > currently need to access them via Unsafe. Which, as always, bothers 
> > me. People increasingly take this path, making it even 
> harder to get 
> > constructions right, and making it impossible to run their code on 
> > platforms with security managers that prevent Unsafe access.
> > Would you rather see this practice continue?
> 
> If these were the only choices - yes. But we can less 
> maximally hide by moving into a public API. I just don't 
> think it is a good idea to have these obscure, rarely usable 
> methods sitting along side the methods that get used all the time.
> 
> > While I am at it ...
> >
> > The AtomicInteger draft contains store- and load- fenced 
> forms of CAS 
> > that represent the only "new" functionality introduced here:
> >    boolean compareAndSetInStoreOrder(int e, int v);
> >    int compareAndSetAndGet(int e, int v); These provide 
> orderings for 
> > CAS matching the ones for get and set. Some people have been asking 
> > for them for a long time (especially for Azul and Itanium, also 
> > probably worthwhile on ARM and POWER).
> > They are not at all commonly used, and are always 
> implementable using 
> > plain CAS. But defining them makes available for special 
> > intrinsification on some platforms.
> > This is a similar story as we have already for weakCompareAndSet, 
> > which I think these days is only specially intrinsified on 
> Azul's JVM.
> 
> weakCAS has a much cleaner story than these relaxed orderings 
> - to allow for the "spurious" failure of ll/sc based 
> implementations. That's a lot simpler to explain than relazed 
> ordering stuff.
> 
> Perhaps I'm missing a piece of the picture here. Is there 
> some formalism or methodology that can be readily used to 
> determine when an algorithm supports these weakly ordered 
> variants? If that's not the case then I confidently claim 
> that most uses of these methods will in fact be incorrect. 
> People (even those that should know better) will opt for the 
> seemingly more performant code over the correct code.
> 
> David
> 
> > -Doug
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From dl at cs.oswego.edu  Fri Apr 16 08:17:49 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 16 Apr 2010 08:17:49 -0400
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <238A96A773B3934685A7269CC8A8D0426DF5A1C9C4@GVW0436EXB.americas.hpqcorp.net>
References: <4BC6FF17.7060308@cs.oswego.edu>
	<NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>
	<238A96A773B3934685A7269CC8A8D0426DF5A1C9C4@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4BC8556D.5010002@cs.oswego.edu>

On 04/15/10 20:46, Boehm, Hans wrote:
> Even if we don't immediately give these any sort of formal semantics, it would be good to be clearer on what these are intended to mean.  That also affects the naming.  I think the semantics are currently unclear along a few dimensions:
>
> 1) Do they implicitly include fences?  I think we agree that the answer is no, i.e.
>
> x1 = 1;
> a.setInStoreOrder(1);
> x2 = 2;
>
> does not guarantee that the assignments to x1 and x2 become visible in order.  If this is correct, and I strongly believe it should be, I think we should avoid mentioning "fence" in the name.
>

Good point. Even "setInStoreOrder" is not crystal clear -- we do
not want to imply orderings wrt subsequent stores (although as
a quality of implementation matter, they will still be issued
"promptly"). Perhaps "setInReleaseOrder" is clearer even though
it has the different problem of using the overloaded term "release"
(and thus leads to unrelated confusion). Further suggestions would be welcome.


> 2) Do they obey "cache coherency" rules, i.e. do the operations on a single variable appear totally ordered?  In particular, if we have x initially zero and:
>
> Thread 1:
> x = 1;
> x = 2;
>
> Thread 2:
> r1 = x;
> r2 = x;
>
> Can we have r2<  r1?  Ordinary Java variables intentionally allow this.  When this was discussed in the context of C++0x atomic<T>, the feeling was that this was too weakly ordered to be useful.  I tent to agree.  (This does affect implementation cost, due both to the "reads kill" issue, and because a few architectures, notably Itanium, allow the unexpected behavior for ordinary loads.)  If this is disallowed, we probably shouldn't name this to suggest that it's equivalent to ordinary memory operations.  And I suspect it should be disallowed.
>

I'm not sure this requires any special attention. Recasting the example
with the actual method calls would by default use volatile/acquiring
reads:

T1:
x.setInStoreOrder(1);
x.setInStoreOrder(2);

T2:
r1 = x.get(); // == volatile read
r2 = x.get();

You'd need to explicitly use getInRelaxedOrder() instead of get()
to open yourself up to surprising results.


> 3) Can I really use "StoreOrder" variants to emulate "final"?

Well, the disclaimer that it is like final except that you can reset
it makes the ties to the JMM/JLS final-field specs metaphorical at best.
We still do want to convey that the method can be used to these effects
in typical cases. But the specs themselves will be messy. The state
we left Fences specs  seem close to being usable here if you bind the
accesses with the fences in the rules:
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html

-Doug




From gdenys at yahoo.com  Fri Apr 16 11:26:53 2010
From: gdenys at yahoo.com (Denys Geert)
Date: Fri, 16 Apr 2010 08:26:53 -0700 (PDT)
Subject: [concurrency-interest] Issues with ForkJoin update
In-Reply-To: <4BC8556D.5010002@cs.oswego.edu>
References: <4BC6FF17.7060308@cs.oswego.edu>
	<NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>
	<238A96A773B3934685A7269CC8A8D0426DF5A1C9C4@GVW0436EXB.americas.hpqcorp.net>
	<4BC8556D.5010002@cs.oswego.edu>
Message-ID: <297736.91342.qm@web51408.mail.re2.yahoo.com>

Hi concurreny folks,

We recently tried the new update for FJ and have some problems with it.

We have coded a rather canonical use case of FJ (see below), a top-level task which recursively spawns child tasks to calculate the number of "tiles" (as in image tiles). With
the previous FJ version it finishes within a second, with the new version
it hangs "forever".

What's probably atypical about our use case is that we limit the FJ
pool (setMaximumPoolSize). The reason is that we have some tasks that are quite heavy
resource users (memory wise) so we need to limit to number of concurrent
threads. This all worked brilliantly using "helpJoin" on our current
version of FJ. Switching to "join" does not seem like an option for us.

When I plug in the new version of FJ, it often fails with a StackOverflow and occasionally hangs (which also happens in combination with StackOverflowErrors which seem to be swallowed within FJ). The code below illustrates the hanging problem, at least in my environment (Linux 32bit, JDK 1.6.0_15) .

When connecting a profiler, most FJ pool threads are "scanning" (one thread was in awaitDone):

"ForkJoinPool-1-worker-1" daemon prio=10 tid=0x7f503c00 nid=0x6cac runnable [0x7f8a3000]
   java.lang.Thread.State: RUNNABLE
    at jsr166y.ForkJoinWorkerThread.scanWhileJoining(ForkJoinWorkerThread.java:909)
    at jsr166y.ForkJoinTask.quietlyHelpJoin(ForkJoinTask.java:836)
    at jsr166y.ForkJoinTask.helpJoin(ForkJoinTask.java:816)
    at CountingTaskTest$CountingTask.computeChildTiles(CountingTaskTest.java:57)
    at CountingTaskTest$CountingTask.compute(CountingTaskTest.java:41)
    at CountingTaskTest$CountingTask.compute(CountingTaskTest.java:1)
    at jsr166y.RecursiveTask.exec(RecursiveTask.java:64)
    at jsr166y.ForkJoinTask.tryExec(ForkJoinTask.java:242)
    at jsr166y.ForkJoinTask.quietlyHelpJoin(ForkJoinTask.java:842)
    at jsr166y.ForkJoinTask.helpJoin(ForkJoinTask.java:816)
    at CountingTaskTest$CountingTask.computeChildTiles(CountingTaskTest.java:57)
    at CountingTaskTest$CountingTask.compute(CountingTaskTest.java:41)
    at CountingTaskTest$CountingTask.compute(CountingTaskTest.java:1)
    at jsr166y.RecursiveTask.exec(RecursiveTask.java:64)
    ....

The stack trace is extremely long. In the previous version, I had the impression the stack trace was proportional to the depth of child tasks spawned. Now this does not seem to be the case anymore.

Any insights would be appreciated.

The code and test code follow:
--------------------
import java.util.Random;

import jsr166y.ForkJoinPool;
import jsr166y.RecursiveTask;

public class CountingTaskTest {
  
  public interface Tiles {
    public long tryComputeTileCount(int aLevel, long aRow, long aCol);
  }
  
  public static class CountingTask extends RecursiveTask<Long> {
    private final int fLevel;
    private final long fCol, fRow;

    private final Tiles fTiles;

    public CountingTask(int aLevel, long aRow, long aCol, Tiles aTiles) {
      fLevel = aLevel;
      fCol = aCol;
      fRow = aRow;
      fTiles = aTiles;
    }

    public Long compute() {
      long numTiles = fTiles.tryComputeTileCount(fLevel, fRow, fCol);
      if (numTiles != -1) {
        return numTiles;
      }
      else {
        return computeChildTiles() + 1;
      }
    }

    private long computeChildTiles() {
      CountingTask task00 = createChildTask(0, 0);
      CountingTask task10 = createChildTask(0, 1);
      CountingTask task01 = createChildTask(1, 0);
      CountingTask task11 = createChildTask(1, 1);

      task00.fork();
      task10.fork();
      task01.fork();
      task11.fork();

      long ret = 0L;
      ret += task00.helpJoin();
      ret += task10.helpJoin();
      ret += task01.helpJoin();
      ret += task11.helpJoin();
      return ret;
    }

    private CountingTask createChildTask(int fRowOffset, int fColumnOffset) {
      return new CountingTask(fLevel + 1, fRowOffset, fColumnOffset, fTiles);
    }

  }

  public static class SimulatedTiles implements Tiles {
    public long tryComputeTileCount(int aLevel, long aRow, long aCol) {
      long seed = 369019586650768L;
      final Random r = new Random(seed);
      if (r.nextInt(4) == 0 || aLevel > 7) {
        return 1;
      } else {
        return -1;
      }
    }
  }
  
  public static void main(String[] args) {
    Tiles tiles = new SimulatedTiles();
    int availableProcessors = 4;
    ForkJoinPool pool = new ForkJoinPool(availableProcessors);
    pool.setMaintainsParallelism(true);
    pool.setMaximumPoolSize(availableProcessors);
    pool.invoke(new CountingTask(0, 0, 0, tiles));
  }

}

----------



      

From dl at cs.oswego.edu  Fri Apr 16 12:36:52 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 16 Apr 2010 12:36:52 -0400
Subject: [concurrency-interest] Issues with ForkJoin update
In-Reply-To: <297736.91342.qm@web51408.mail.re2.yahoo.com>
References: <4BC6FF17.7060308@cs.oswego.edu>	<NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>	<238A96A773B3934685A7269CC8A8D0426DF5A1C9C4@GVW0436EXB.americas.hpqcorp.net>	<4BC8556D.5010002@cs.oswego.edu>
	<297736.91342.qm@web51408.mail.re2.yahoo.com>
Message-ID: <4BC89224.9090506@cs.oswego.edu>

On 04/16/10 11:26, Denys Geert wrote:

> What's probably atypical about our use case is that we limit the FJ
> pool (setMaximumPoolSize).

Thanks for reporting this. I'm beginning to regret defining
a method that allows people to do this :-) The update last week
supporting better thread throttling didn't cope with this well.
I will check in another update that does in the next few days.
For now though...

>      private long computeChildTiles() {
>        CountingTask task00 = createChildTask(0, 0);
>        CountingTask task10 = createChildTask(0, 1);
>        CountingTask task01 = createChildTask(1, 0);
>        CountingTask task11 = createChildTask(1, 1);
>
>        task00.fork();
>        task10.fork();
>        task01.fork();
>        task11.fork();
>
>        long ret = 0L;
>        ret += task00.helpJoin();
>        ret += task10.helpJoin();
>        ret += task01.helpJoin();
>        ret += task11.helpJoin();
>        return ret;
>      }
>

Whenever possible, you should join tasks in LIFO order of
forking them. If you change this accordingly...
         ret += task11.helpJoin();
         ret += task01.helpJoin();
         ret += task10.helpJoin();
         ret += task00.helpJoin();
... you will avoid the issues.

-Doug





From gregg at cytetech.com  Fri Apr 16 13:30:17 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 16 Apr 2010 12:30:17 -0500
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
Message-ID: <4BC89EA9.1020204@cytetech.com>

I sent this earlier and apparently only replied to David...  I changed some 
wording to try and be clearer.

I agree with David, that the use of names which include the keywords that 
developers are already learning the meaning/use of would be helpful.  I also 
concur that in many cases, there may be bugs created by misuse just for the sake 
of speed, or because they are copying code from somewhere else and using it 
incorrectly.

I'd think that a subclass, which completely encompasses the "asIfFinal" or 
"nonVolatile" etc aspects of use would be an improvement because it would 
provide a place for "reading javadoc" about why you might choose to use the 
class.  The subclass could also guarantee that the "visible" behavior was no 
"better" than the class was documented to provide.  I say "better" in the sense 
that putting these methods in AtomicInteger, for example, means that all the 
"set" and "get" functions are available.  I'd guess that you would not want 
results from the other methods that would produce differing or dissimilar 
behavior.

It seems that's where the bugs would start to appear because one branch of 
existing code is using appropriate methods correctly and some new code is not, 
and occasionally visibility if fixed by one branch, while the other occasionally 
misses a value because of a data race for example.

I'm wondering if subclass names which have to do with specific use cases such as 
"AtomicIntWithReadAsIfFinalAndWriteAsIfVolatile" etc. are useful?

Gregg Wonderly

David Holmes wrote:
 > Hi Doug,
 >
 > I think "InStoreorder" and "InRelaxedOrder" could easily be misunderstood as
 > relating to "total store ordering", or "relaxed memory models" or any
 > particular architecural memory model that uses that kind of terminology.
 >
 > If "InStoreOrder" means "with memory effects equivalent to setting a final
 > variable" then lets just say that:
 >
 >    setAsIfFinal(int newVal)
 >
 > and similarly:
 >
 >    setAsNonVolatile(int newVal)
 >
 >
 > I guess this is somewhat better than Fences in that the semantics of the
 > methods are easier to understand. But these are still methods that the vast
 > majority of programmers won't know when it is valid to use them. They will
 > only discover that they seem faster and so use them regardless :( Can we not
 > "hide" them a bit better eg:
 >
 >   class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicInteger extends
 > AtomicInteger {
 >       public void setAsNonVolatile(int newVal) { ... }
 >       ...
 >   }
 >
 > or a utility class
 >
 >   class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicHelper {
 >      static void setAsNonVolatile(AtomicInteger x, int newVal) { ... }
 >      ...
 >   }
 >
 >> One further accommodation is that the encapsulated int
 >> state value inside AbstractQueuedSynchronizer should also
 >> support at least the setInStoreOrder method
 >
 > I fear the chance of misuse greatly outweighs any performance benefit.
 >
 >
 > Aside: if I ever see a bug report involving these new methods my response
 > will be to request the submitter to rewrite their program using the "proper"
 > methods. Without tools or formalisms to establish correctness of use these
 > methods will in many cases just be bugs waiting to happen.
 >
 > Cheers,
 > David
 >
 >
 >> -----Original Message-----
 >> From: concurrency-interest-bounces at cs.oswego.edu
 >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug Lea
 >> Sent: Wednesday, 14 April 2010 10:53 PM
 >> To: concurrency-interest at cs.oswego.edu
 >> Subject: [concurrency-interest] Extended access methods for Atomics (and
 >> AQS)
 >>
 >>
 >> Last fall, we tabled discussion of the proposed Fences API.
 >> But regardless of the outcome of and future discussions when
 >> we re-raise it, there seemed to be consensus that at the very
 >> least, we should extend the methods of Atomic classes to support
 >> the various memory effect modes that you otherwise would need
 >> Fences methods to obtain.
 >>
 >> I'm finally trying this out. A draft update of AtomicInteger
 >> with these changes is at:
 >> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
 >> atomic/AtomicInteger.html
 >> (For now, only class AtomicInteger). Ignoring the arithmetic methods
 >> (getAndAdd etc), the API looks like (with "*" in front of
 >> the added methods)
 >>
 >> "volatile-write" mode
 >>     void set(int v)
 >>     boolean compareAndSet(int e, int v);
 >>     int setAndGet(int v);
 >>
 >> store-ordered (aka release, pseudo-final-field) mode
 >> *  void setInStoreOrder(int v);
 >> *  boolean compareAndSetInStoreOrder(int e, int v);
 >>     void lazySet(int v) // synonym for setInStoreOrder
 >>
 >> load-ordered (aka volatile-read, acquire) mode
 >>     int get();
 >> *  compareAndSetAndGet(int e, int v);
 >>
 >> relaxed-order (aka non-volatile) mode
 >> *  int getInRelaxedOrder();
 >> *  void setInRelaxedOrder(int v);
 >>     boolean weakCompareAndSet(int e, int v);
 >>
 >> Comments and suggestions about method names and
 >> semantics would be very welcome. (Notice that
 >> the naming scheme denigrates "lazySet", a name
 >> that no one likes!)
 >>
 >> The form of these methods is roughly similar to
 >> C++0x modes (at least the ones supportable in Java),
 >> but avoids the sometimes-controversial terms
 >> "acquire" and "release" in mode/method names.
 >>
 >> For now, the specs are just done informally. Assuming we
 >> go ahead with this, we'd adapt the more formal versions done
 >> in Fences drafts and place them in j.u.c.atomic package docs
 >> to spell out better.
 >>
 >> Versions for other stand-alone Atomic objects (AtomicLong,
 >> AtomicReference) would be equally straightforward. As always, the
 >> main problems lie in the FieldUpdater forms, which are sadly
 >> also the most useful. The dynamic type checking overhead
 >> required for these forms is often more expensive than
 >> any savings you get from weaker access modes. (This is
 >> why Fences versions continue to be attractive.) However,
 >> for uniformity, extended methods in these would also be
 >> supported. Similarly for the Atomic*Array classes.
 >> And it is still conceivable that new JVM mechanics being put
 >> into place for JSR292 might be used to reduce overhead.
 >>
 >> One further accommodation is that the encapsulated int
 >> state value inside AbstractQueuedSynchronizer should also
 >> support at least the setInStoreOrder method (none of
 >> the other new methods seem to ever apply). Similarly
 >> for AbstractQueuedLongSynchronizer. Using this as
 >> appropriate speeds up ReentrantLock by 5% - 20%, so
 >> is a good idea regardless of all other changes.
 >>
 >> -Doug
 >>
 >> _______________________________________________
 >> Concurrency-interest mailing list
 >> Concurrency-interest at cs.oswego.edu
 >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 >
 > _______________________________________________
 > Concurrency-interest mailing list
 > Concurrency-interest at cs.oswego.edu
 > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 >
 >



From hans.boehm at hp.com  Fri Apr 16 18:56:16 2010
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 16 Apr 2010 22:56:16 +0000
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <4BC8556D.5010002@cs.oswego.edu>
References: <4BC6FF17.7060308@cs.oswego.edu>
	<NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>
	<238A96A773B3934685A7269CC8A8D0426DF5A1C9C4@GVW0436EXB.americas.hpqcorp.net>
	<4BC8556D.5010002@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D0426DF5A1D013@GVW0436EXB.americas.hpqcorp.net>

> From:  Doug Lea
> 
> On 04/15/10 20:46, Boehm, Hans wrote:
> > Even if we don't immediately give these any sort of formal 
> semantics, it would be good to be clearer on what these are 
> intended to mean.  That also affects the naming.  I think the 
> semantics are currently unclear along a few dimensions:
> >
> > 1) Do they implicitly include fences?  I think we agree 
> that the answer is no, i.e.
> >
> > x1 = 1;
> > a.setInStoreOrder(1);
> > x2 = 2;
> >
> > does not guarantee that the assignments to x1 and x2 become 
> visible in order.  If this is correct, and I strongly believe 
> it should be, I think we should avoid mentioning "fence" in the name.
> >
> 
> Good point. Even "setInStoreOrder" is not crystal clear -- we 
> do not want to imply orderings wrt subsequent stores 
> (although as a quality of implementation matter, they will 
> still be issued "promptly"). Perhaps "setInReleaseOrder" is 
> clearer even though it has the different problem of using the 
> overloaded term "release"
> (and thus leads to unrelated confusion). Further suggestions 
> would be welcome.
I'd certainly also be open to other ideas, but I'm not sure that "release"
is so bad here.  It does seem to have slightly different meanings in
different contexts.  But my impression is that they are all close enough
to convey the general idea.

> 
> 
> > 2) Do they obey "cache coherency" rules, i.e. do the 
> operations on a single variable appear totally ordered?  In 
> particular, if we have x initially zero and:
> >
> > Thread 1:
> > x = 1;
> > x = 2;
> >
> > Thread 2:
> > r1 = x;
> > r2 = x;
> >
> > Can we have r2<  r1?  Ordinary Java variables intentionally 
> allow this.  When this was discussed in the context of C++0x 
> atomic<T>, the feeling was that this was too weakly ordered 
> to be useful.  I tent to agree.  (This does affect 
> implementation cost, due both to the "reads kill" issue, and 
> because a few architectures, notably Itanium, allow the 
> unexpected behavior for ordinary loads.)  If this is 
> disallowed, we probably shouldn't name this to suggest that 
> it's equivalent to ordinary memory operations.  And I suspect 
> it should be disallowed.
> >
> 
> I'm not sure this requires any special attention. Recasting 
> the example with the actual method calls would by default use 
> volatile/acquiring
> reads:
> 
> T1:
> x.setInStoreOrder(1);
> x.setInStoreOrder(2);
> 
> T2:
> r1 = x.get(); // == volatile read
> r2 = x.get();
> 
> You'd need to explicitly use getInRelaxedOrder() instead of 
> get() to open yourself up to surprising results.
I'm sorry.  I wasn't precise enough here.  This indeed matters
only for "relaxed" order, and I should have stated that explicitly.
This is really a question about relaxed operations, and whether
we want the naming or description to refer to ordinary variable
accesses.  But I think it remains an important question if we want
to support "relaxed" operations.

> 
> 
> > 3) Can I really use "StoreOrder" variants to emulate "final"?
> 
> Well, the disclaimer that it is like final except that you 
> can reset it makes the ties to the JMM/JLS final-field specs 
> metaphorical at best.
> We still do want to convey that the method can be used to 
> these effects in typical cases. But the specs themselves will 
> be messy. The state we left Fences specs  seem close to being 
> usable here if you bind the accesses with the fences in the rules:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurr
> ent/atomic/Fences.html
Looks promising to me.

I guess the intent would be that I could use a getInRelaxedOrder() to read
the pseudo-final field, and the compiler would have to preserve "deep accesses"
through that?  That may well work, based on semantics like those in the
fences draft.  Partially contradicting what I said in my last message,
this may have the large advantage over the fence-based versions that it
seems to have minimal impact on anything that doesn't explicitly
use getInRelaxedOrder().

Hans

> 
> -Doug
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From hans.boehm at hp.com  Sat Apr 17 15:09:10 2010
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sat, 17 Apr 2010 19:09:10 +0000
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <4BC89EA9.1020204@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
	<4BC89EA9.1020204@cytetech.com>
Message-ID: <238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>

I'm not sure I fully understand this proposal.  However, I think it's important to structure the interface so that, if you really need to use accessors with weaker ordering, you can mix them with more strongly ordered ones, including volatile style accesses.  For example, the second load in double-checked locking doesn't actually race with anything.  Thus it's unusually safe to use a "relaxed" (or even "nonVolatile") access here.  But that doesn't mean you want "relaxed" accesses everywhere.

I think it's also very reasonable to write code initially to use only volatile-style (sequentially consistent)accesses, and then relax that only in performance critical cases that you've hopefully spent a lot of time thinking about.

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Gregg Wonderly
> Sent: Friday, April 16, 2010 10:30 AM
> To: dholmes at ieee.org
> Cc: Doug Lea; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Extended access methods 
> for Atomics (and AQS)
> 
> I sent this earlier and apparently only replied to David...  
> I changed some wording to try and be clearer.
> 
> I agree with David, that the use of names which include the 
> keywords that developers are already learning the meaning/use 
> of would be helpful.  I also concur that in many cases, there 
> may be bugs created by misuse just for the sake of speed, or 
> because they are copying code from somewhere else and using 
> it incorrectly.
> 
> I'd think that a subclass, which completely encompasses the 
> "asIfFinal" or "nonVolatile" etc aspects of use would be an 
> improvement because it would provide a place for "reading 
> javadoc" about why you might choose to use the class.  The 
> subclass could also guarantee that the "visible" behavior was 
> no "better" than the class was documented to provide.  I say 
> "better" in the sense that putting these methods in 
> AtomicInteger, for example, means that all the "set" and 
> "get" functions are available.  I'd guess that you would not 
> want results from the other methods that would produce 
> differing or dissimilar behavior.
> 
> It seems that's where the bugs would start to appear because 
> one branch of existing code is using appropriate methods 
> correctly and some new code is not, and occasionally 
> visibility if fixed by one branch, while the other 
> occasionally misses a value because of a data race for example.
> 
> I'm wondering if subclass names which have to do with 
> specific use cases such as 
> "AtomicIntWithReadAsIfFinalAndWriteAsIfVolatile" etc. are useful?
> 
> Gregg Wonderly
> 
> David Holmes wrote:
>  > Hi Doug,
>  >
>  > I think "InStoreorder" and "InRelaxedOrder" could easily 
> be misunderstood as  > relating to "total store ordering", or 
> "relaxed memory models" or any  > particular architecural 
> memory model that uses that kind of terminology.
>  >
>  > If "InStoreOrder" means "with memory effects equivalent to 
> setting a final  > variable" then lets just say that:
>  >
>  >    setAsIfFinal(int newVal)
>  >
>  > and similarly:
>  >
>  >    setAsNonVolatile(int newVal)
>  >
>  >
>  > I guess this is somewhat better than Fences in that the 
> semantics of the  > methods are easier to understand. But 
> these are still methods that the vast  > majority of 
> programmers won't know when it is valid to use them. They 
> will  > only discover that they seem faster and so use them 
> regardless :( Can we not  > "hide" them a bit better eg:
>  >
>  >   class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicInteger extends
>  > AtomicInteger {
>  >       public void setAsNonVolatile(int newVal) { ... }
>  >       ...
>  >   }
>  >
>  > or a utility class
>  >
>  >   class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicHelper {
>  >      static void setAsNonVolatile(AtomicInteger x, int 
> newVal) { ... }
>  >      ...
>  >   }
>  >
>  >> One further accommodation is that the encapsulated int  
> >> state value inside AbstractQueuedSynchronizer should also  
> >> support at least the setInStoreOrder method  >  > I fear 
> the chance of misuse greatly outweighs any performance benefit.
>  >
>  >
>  > Aside: if I ever see a bug report involving these new 
> methods my response  > will be to request the submitter to 
> rewrite their program using the "proper"
>  > methods. Without tools or formalisms to establish 
> correctness of use these  > methods will in many cases just 
> be bugs waiting to happen.
>  >
>  > Cheers,
>  > David
>  >
>  >
>  >> -----Original Message-----
>  >> From: concurrency-interest-bounces at cs.oswego.edu
>  >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On 
> Behalf Of Doug Lea  >> Sent: Wednesday, 14 April 2010 10:53 
> PM  >> To: concurrency-interest at cs.oswego.edu
>  >> Subject: [concurrency-interest] Extended access methods 
> for Atomics (and  >> AQS)  >>  >>  >> Last fall, we tabled 
> discussion of the proposed Fences API.
>  >> But regardless of the outcome of and future discussions 
> when  >> we re-raise it, there seemed to be consensus that at 
> the very  >> least, we should extend the methods of Atomic 
> classes to support  >> the various memory effect modes that 
> you otherwise would need  >> Fences methods to obtain.
>  >>
>  >> I'm finally trying this out. A draft update of 
> AtomicInteger  >> with these changes is at:
>  >> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
>  >> atomic/AtomicInteger.html
>  >> (For now, only class AtomicInteger). Ignoring the 
> arithmetic methods  >> (getAndAdd etc), the API looks like 
> (with "*" in front of  >> the added methods)  >>  >> 
> "volatile-write" mode
>  >>     void set(int v)
>  >>     boolean compareAndSet(int e, int v);
>  >>     int setAndGet(int v);
>  >>
>  >> store-ordered (aka release, pseudo-final-field) mode  >> 
> *  void setInStoreOrder(int v);  >> *  boolean 
> compareAndSetInStoreOrder(int e, int v);
>  >>     void lazySet(int v) // synonym for setInStoreOrder
>  >>
>  >> load-ordered (aka volatile-read, acquire) mode
>  >>     int get();
>  >> *  compareAndSetAndGet(int e, int v);  >>  >> 
> relaxed-order (aka non-volatile) mode  >> *  int 
> getInRelaxedOrder();  >> *  void setInRelaxedOrder(int v);
>  >>     boolean weakCompareAndSet(int e, int v);
>  >>
>  >> Comments and suggestions about method names and  >> 
> semantics would be very welcome. (Notice that  >> the naming 
> scheme denigrates "lazySet", a name  >> that no one likes!)  
> >>  >> The form of these methods is roughly similar to  >> 
> C++0x modes (at least the ones supportable in Java),  >> but 
> avoids the sometimes-controversial terms  >> "acquire" and 
> "release" in mode/method names.
>  >>
>  >> For now, the specs are just done informally. Assuming we  
> >> go ahead with this, we'd adapt the more formal versions 
> done  >> in Fences drafts and place them in j.u.c.atomic 
> package docs  >> to spell out better.
>  >>
>  >> Versions for other stand-alone Atomic objects 
> (AtomicLong,  >> AtomicReference) would be equally 
> straightforward. As always, the  >> main problems lie in the 
> FieldUpdater forms, which are sadly  >> also the most useful. 
> The dynamic type checking overhead  >> required for these 
> forms is often more expensive than  >> any savings you get 
> from weaker access modes. (This is  >> why Fences versions 
> continue to be attractive.) However,  >> for uniformity, 
> extended methods in these would also be  >> supported. 
> Similarly for the Atomic*Array classes.
>  >> And it is still conceivable that new JVM mechanics being 
> put  >> into place for JSR292 might be used to reduce overhead.
>  >>
>  >> One further accommodation is that the encapsulated int  
> >> state value inside AbstractQueuedSynchronizer should also  
> >> support at least the setInStoreOrder method (none of  >> 
> the other new methods seem to ever apply). Similarly  >> for 
> AbstractQueuedLongSynchronizer. Using this as  >> appropriate 
> speeds up ReentrantLock by 5% - 20%, so  >> is a good idea 
> regardless of all other changes.
>  >>
>  >> -Doug
>  >>
>  >> _______________________________________________
>  >> Concurrency-interest mailing list
>  >> Concurrency-interest at cs.oswego.edu
>  >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>  >
>  > _______________________________________________
>  > Concurrency-interest mailing list
>  > Concurrency-interest at cs.oswego.edu
>  > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>  >
>  >
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From joe.bowbeer at gmail.com  Sat Apr 17 16:05:01 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 17 Apr 2010 13:05:01 -0700
Subject: [concurrency-interest] Extended access methods for Atomics (and
	AQS)
In-Reply-To: <238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <w2u31f2a7bd1004171305gfa6d0379j45dc208de5bf3dc0@mail.gmail.com>

I think the proposal is to add AtomicIntegerEx extends AtomicInteger, and so
on, and to add the new methods to these for-experts-only subclasses.


On Sat, Apr 17, 2010 at 12:09 PM, Boehm, Hans wrote:

> I'm not sure I fully understand this proposal.  However, I think it's
> important to structure the interface so that, if you really need to use
> accessors with weaker ordering, you can mix them with more strongly ordered
> ones, including volatile style accesses.  For example, the second load in
> double-checked locking doesn't actually race with anything.  Thus it's
> unusually safe to use a "relaxed" (or even "nonVolatile") access here.  But
> that doesn't mean you want "relaxed" accesses everywhere.
>
> I think it's also very reasonable to write code initially to use only
> volatile-style (sequentially consistent)accesses, and then relax that only
> in performance critical cases that you've hopefully spent a lot of time
> thinking about.
>
> Hans
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100417/0cb3d004/attachment.html>

From dl at cs.oswego.edu  Sun Apr 18 11:35:57 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 18 Apr 2010 11:35:57 -0400
Subject: [concurrency-interest] Issues with ForkJoin update
In-Reply-To: <4BC89224.9090506@cs.oswego.edu>
References: <4BC6FF17.7060308@cs.oswego.edu>	<NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>	<238A96A773B3934685A7269CC8A8D0426DF5A1C9C4@GVW0436EXB.americas.hpqcorp.net>	<4BC8556D.5010002@cs.oswego.edu>	<297736.91342.qm@web51408.mail.re2.yahoo.com>
	<4BC89224.9090506@cs.oswego.edu>
Message-ID: <4BCB26DD.8060309@cs.oswego.edu>

On 04/16/10 12:36, Doug Lea wrote:
> On 04/16/10 11:26, Denys Geert wrote:
>
>> What's probably atypical about our use case is that we limit the FJ
>> pool (setMaximumPoolSize).
>
> Thanks for reporting this. I'm beginning to regret defining
> a method that allows people to do this ...

I checked in an update that handles max pool size settings for
helpJoin, but also strengthened the wording on setMaximumPoolSize
javadoc to make it clearer that this method is not usually what people
want. It imposes a hard limit rather than providing an overall
thread minimization policy supported by method setMaintainsParallelism.
There are only a few (uncommon) reasonable use cases for
setMaximumPoolSize. But still a few, so I don't think we should
remove it, just discourage it.

Pasting from updated javadoc:

setMaximumPoolSize

public void setMaximumPoolSize(int newMax)

     Sets the maximum number of threads allowed to exist in the pool. The given 
value should normally be greater than or equal to the parallelism level. Setting 
this value has no effect on current pool size. It controls construction of new 
threads. The use of this method may cause tasks that intrinsically require extra 
threads for dependent computations to indefinitely stall. If you are instead 
trying to minimize internal thread creation, consider setting 
setMaintainsParallelism(boolean) as false.

Here are the usual links:

jsr166y:
* API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
* jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166y.jar (compiled using 
Java6 javac).
* Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166y/

openjdk7-ready java.util.concurrent:
* API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/docs/
* jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
* Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/

-Doug

From dl at cs.oswego.edu  Mon Apr 19 09:38:05 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 19 Apr 2010 09:38:05 -0400
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEOMIFAA.davidcholmes@aapt.net.au>
Message-ID: <4BCC5CBD.9080000@cs.oswego.edu>

On 04/15/10 08:49, David Holmes wrote:
>  I just don't think it is a good idea to have these
> obscure, rarely usable methods sitting along side the methods that get used
> all the time.
>

I never know what to do about the immorality accusations
whenever trying to improve the current state of affairs
about ordering control for atomics. But here's another pass
at laying out the motivation and current status:

AtomicX already contains lazySet, which has a
naming bug (=> setInReleaseOrder) and specs in need
of improvement.  But AtomicX doesn't contain the
complementary weaker-than-volatile form of get
(=> getInRelaxedOrder) that is sometimes used with
it (as mentioned by Hans). My draft proposal fixes these,
and further regularizes API to accommodate a long-standing
RFE to add corresponding forms of CAS. The net result
has pretty much the same structure as C++0x atomics, which
is not only a good sanity check, but also in the future
(as C++0x atomics become supported by C++ compilers) will
improve confidence that JVMs written in C++, runtime-systems
supporting both C++ and Java, etc, work consistently.

If we had not already previously included some methods for
non-volatile ordering control (i.e., weakCompareAndSet, lazySet)
in AtomicX, I might be more sympathetic to creating classes
AtomicXWithExtendOrderingControl. But as it stands, I don't think
the mere act of further fleshing them out and regularizing them
is a good reason to do so.

I illustrated API with AtomicInteger, but as always,
the main practical need is for ordering methods applying
to the fields of other objects (including array elements).
These are more typical/desirable because without them:
(1) if you need ordering control and must create
another Atomic object to obtain it, then you have traded one
problem for two problems, because you must now figure out
how to correctly maintain reference to that Atomic;
(2) Creating AtomicXs (especially cases like arrays of
AtomicIntegers as opposed to arrays of ints supporting ordering
control) can (and usually does) lead to so much space and
indirection bloat that you may be better off just running slow
lock-based code (assuming you can figure out how to use locks in
such cases, which is not always possible).

If Java/JVMs supported some way to force object-inlining
so that a (suitably declared) AtomicInteger field of
another object could be embedded as an int field but
accessed via atomics, there would be no need for alternatives.
But I don't think this will happen anytime soon.

Short of such major language/JVM changes, one way of handling
these cases is with Fences, that separate out the ordering
control from the reads and writes (although unfortunately not CAS).
This way, programmers just do the field accesses but surround
them with acquire-, release-, or volatile- style fence method
calls (which would normally be instrinsified and inlined).

But many people objected to this approach: Some on
morality grounds, others on usability grounds, and others
because the specs under this form of separation are very
hard to nail down and incorporate into JMM/JLS.
It seems that most people agree that it would be better
all around to support and spec ordered accesses
than separating accesses plus orderings.

One way to do this is to extend AtomicXFieldUpdaters.
This is not a very nice choice. Java, Java bytes codes, and
JVMs were not originally designed to support "l-value"
operations on fields of objects beyond simple read/write.
The only path to do so is via the defacto-standard "Unsafe"
APIs that bypass all of the normal apparatus (including
verification and safety checks) to perform ordered/atomic
accesses at particular offsets of objects/arrays. So if
you want to export these methods in public APIs, you must surround
them with Java-level dynamic type- and accessibility-
checking (and further, restrict use to fields marked as
volatile, which doesn't work for arrays).

Which brings us to the same kind of cure-worse-than-disease
problem seen with the alternative of using standalone Atomics:
On most platforms, the overhead of performing, say
updater.lazySet(x) (aka setInReleaseOrder(x)) is greater than
any savings you get from avoiding underlying hardware fences
if you simply did a direct volatile write. (Worse, in some
cases the checks may themselves entail stronger fences.)
Even in their current forms (without the above minor extensions)
no one is tempted to use these methods. (Google code search
does not find even one  application-level usage of
AtomicIntegerFieldupdater.lazySet.) Instead, as
is occasionally posted on this list, people developing carefully
weakly ordered code have learned to use Unsafe directly,
avoiding the overhead. Although at the price of having no
language-level static checking and containing code that does
not work on systems with security managers.

My hope has been that there might be some way to reduce
this dynamic checking overhead in updaters, so that people
can stop using Unsafe directly. But after some further exploration,
I'm less optimistic about the prospects -- even with JSR292-based
support, the cases in which you can trade improved updater
creation-time checks for fewer or faster dynamic per-update
checks don't seem to cover many usages.

Which leaves me not having a really good plan at the moment.
This is a problem that is not going to go away all by itself,
so concrete suggestions would be welcome.

-Doug


From dmohindr at andrew.cmu.edu  Mon Apr 19 10:22:14 2010
From: dmohindr at andrew.cmu.edu (Dhruv Mohindra)
Date: Mon, 19 Apr 2010 10:22:14 -0400
Subject: [concurrency-interest] Java Concurrency Guidelines - Call For
	Reviewers
Message-ID: <4BCC6716.7060905@andrew.cmu.edu>

Hello everyone,

The CERT Program at Carnegie Mellon University's Software Engineering 
Institute and Sun Microsystems have been collaborating on the 
development of the CERT Sun Microsystems Secure Coding Standard for 
Java.  This coding standard is being developed as a community effort on 
the CERT secure coding wiki at www.securecoding.cert.org 
<http://www.securecoding.cert.org>.

We are planning to release a Technical Report on Java Concurrency 
Guidelines ahead of publishing the completed secure coding standard. We 
would greatly appreciate your help in ensuring the quality of this 
report by reviewing and commenting on the material in the concurrency 
section, which can be accessed at:

https://www.securecoding.cert.org/confluence/x/NIYbAQ

To add comments to existing guidelines, we encourage you to sign up for 
an account on the website. Alternatively, you can email your suggestions 
directly to me. Your contribution is necessary for making this effort a 
success.

Thanks,

Dhruv Mohindra
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100419/bb77ca8d/attachment.html>

From gergg at cox.net  Mon Apr 19 16:39:13 2010
From: gergg at cox.net (Gregg Wonderly)
Date: Mon, 19 Apr 2010 15:39:13 -0500
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4BCCBF71.8060508@cox.net>

Hans, from my perspective, as just a developer that tires of having to deal with 
hardware issues, is that we shouldn't keep trying to make software easier to get 
wrong.

Multi-threading in Java, due to weak visibility guarentees, is painful. 
Further, I find it really hard to teach people this stuff, and I am not sure why 
we find it so easy to keep adding to what needs to be learned.

What I thinking about, is the fact that I see the read vs write accesses as a 
range of possibilities.  If you order them from weaker on the outside to 
stronger on the inside as something like

	non-volatile -> relaxed -> volatile:read

and

	write:volatile <- relaxed <- non-volatile

then you can imagine a "window" where particular guarantees can hold.  Outside 
of that window, things are weaker.  For any particular application, I am 
thinking that this window is never completely open.

Writes that are relaxed might be the weakest you want, while reads might even be 
non-volatile, but I'm not sure why you would make wide use of AtomicX in that case.

I am thinking then, that a set of classes with names like:

NonVolatileReadRelaxedWriteAtomicInt
RelaxedReadVolatileWriteAtomicInt

etc would be helpful in that they could have all of the methods that AtomicInt 
has, and as subclasses, they could override methods that are weaker than the 
class name stipulates and make them live up to the "name".

This would allow some interesting "plugability" for quick testing of 
visibility/synchronization as a "fix" for something.  You could plug in
VolatileReadVolatileWriteAtomicInt and see if that fixes the problem or at least 
changes the behavior in a way that suggests what the issue might be.

I still worry the most about people using weaker access by habit and then 
threading changing so that a different thread carries out a task then was used 
before and now the weaker access is not appropriate.  Those kinds of problems 
are very difficult to find, and tooling or API support to find and debug these 
types of things seems like an upfront need.

Gregg Wonderly

Boehm, Hans wrote:
> I'm not sure I fully understand this proposal.  However, I think it's important to 
> structure the interface so that, if you really need to use accessors with weaker
> ordering, you can mix them with more strongly ordered ones, including volatile
> style accesses.  For example, the second load in double-checked locking doesn't
 > actually race with anything.  Thus it's unusually safe to use a "relaxed" (or
> even "nonVolatile") access here.  But that doesn't mean you want "relaxed" 
> accesses everywhere.
> 
> I think it's also very reasonable to write code initially to use only volatile-style (sequentially consistent)accesses, and then relax that only in performance critical cases that you've hopefully spent a lot of time thinking about.
> 
> Hans
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu 
>> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
>> Of Gregg Wonderly
>> Sent: Friday, April 16, 2010 10:30 AM
>> To: dholmes at ieee.org
>> Cc: Doug Lea; concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Extended access methods 
>> for Atomics (and AQS)
>>
>> I sent this earlier and apparently only replied to David...  
>> I changed some wording to try and be clearer.
>>
>> I agree with David, that the use of names which include the 
>> keywords that developers are already learning the meaning/use 
>> of would be helpful.  I also concur that in many cases, there 
>> may be bugs created by misuse just for the sake of speed, or 
>> because they are copying code from somewhere else and using 
>> it incorrectly.
>>
>> I'd think that a subclass, which completely encompasses the 
>> "asIfFinal" or "nonVolatile" etc aspects of use would be an 
>> improvement because it would provide a place for "reading 
>> javadoc" about why you might choose to use the class.  The 
>> subclass could also guarantee that the "visible" behavior was 
>> no "better" than the class was documented to provide.  I say 
>> "better" in the sense that putting these methods in 
>> AtomicInteger, for example, means that all the "set" and 
>> "get" functions are available.  I'd guess that you would not 
>> want results from the other methods that would produce 
>> differing or dissimilar behavior.
>>
>> It seems that's where the bugs would start to appear because 
>> one branch of existing code is using appropriate methods 
>> correctly and some new code is not, and occasionally 
>> visibility if fixed by one branch, while the other 
>> occasionally misses a value because of a data race for example.
>>
>> I'm wondering if subclass names which have to do with 
>> specific use cases such as 
>> "AtomicIntWithReadAsIfFinalAndWriteAsIfVolatile" etc. are useful?
>>
>> Gregg Wonderly
>>
>> David Holmes wrote:
>>  > Hi Doug,
>>  >
>>  > I think "InStoreorder" and "InRelaxedOrder" could easily 
>> be misunderstood as  > relating to "total store ordering", or 
>> "relaxed memory models" or any  > particular architecural 
>> memory model that uses that kind of terminology.
>>  >
>>  > If "InStoreOrder" means "with memory effects equivalent to 
>> setting a final  > variable" then lets just say that:
>>  >
>>  >    setAsIfFinal(int newVal)
>>  >
>>  > and similarly:
>>  >
>>  >    setAsNonVolatile(int newVal)
>>  >
>>  >
>>  > I guess this is somewhat better than Fences in that the 
>> semantics of the  > methods are easier to understand. But 
>> these are still methods that the vast  > majority of 
>> programmers won't know when it is valid to use them. They 
>> will  > only discover that they seem faster and so use them 
>> regardless :( Can we not  > "hide" them a bit better eg:
>>  >
>>  >   class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicInteger extends
>>  > AtomicInteger {
>>  >       public void setAsNonVolatile(int newVal) { ... }
>>  >       ...
>>  >   }
>>  >
>>  > or a utility class
>>  >
>>  >   class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicHelper {
>>  >      static void setAsNonVolatile(AtomicInteger x, int 
>> newVal) { ... }
>>  >      ...
>>  >   }
>>  >
>>  >> One further accommodation is that the encapsulated int  
>>>> state value inside AbstractQueuedSynchronizer should also  
>>>> support at least the setInStoreOrder method  >  > I fear 
>> the chance of misuse greatly outweighs any performance benefit.
>>  >
>>  >
>>  > Aside: if I ever see a bug report involving these new 
>> methods my response  > will be to request the submitter to 
>> rewrite their program using the "proper"
>>  > methods. Without tools or formalisms to establish 
>> correctness of use these  > methods will in many cases just 
>> be bugs waiting to happen.
>>  >
>>  > Cheers,
>>  > David
>>  >
>>  >
>>  >> -----Original Message-----
>>  >> From: concurrency-interest-bounces at cs.oswego.edu
>>  >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On 
>> Behalf Of Doug Lea  >> Sent: Wednesday, 14 April 2010 10:53 
>> PM  >> To: concurrency-interest at cs.oswego.edu
>>  >> Subject: [concurrency-interest] Extended access methods 
>> for Atomics (and  >> AQS)  >>  >>  >> Last fall, we tabled 
>> discussion of the proposed Fences API.
>>  >> But regardless of the outcome of and future discussions 
>> when  >> we re-raise it, there seemed to be consensus that at 
>> the very  >> least, we should extend the methods of Atomic 
>> classes to support  >> the various memory effect modes that 
>> you otherwise would need  >> Fences methods to obtain.
>>  >>
>>  >> I'm finally trying this out. A draft update of 
>> AtomicInteger  >> with these changes is at:
>>  >> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
>>  >> atomic/AtomicInteger.html
>>  >> (For now, only class AtomicInteger). Ignoring the 
>> arithmetic methods  >> (getAndAdd etc), the API looks like 
>> (with "*" in front of  >> the added methods)  >>  >> 
>> "volatile-write" mode
>>  >>     void set(int v)
>>  >>     boolean compareAndSet(int e, int v);
>>  >>     int setAndGet(int v);
>>  >>
>>  >> store-ordered (aka release, pseudo-final-field) mode  >> 
>> *  void setInStoreOrder(int v);  >> *  boolean 
>> compareAndSetInStoreOrder(int e, int v);
>>  >>     void lazySet(int v) // synonym for setInStoreOrder
>>  >>
>>  >> load-ordered (aka volatile-read, acquire) mode
>>  >>     int get();
>>  >> *  compareAndSetAndGet(int e, int v);  >>  >> 
>> relaxed-order (aka non-volatile) mode  >> *  int 
>> getInRelaxedOrder();  >> *  void setInRelaxedOrder(int v);
>>  >>     boolean weakCompareAndSet(int e, int v);
>>  >>
>>  >> Comments and suggestions about method names and  >> 
>> semantics would be very welcome. (Notice that  >> the naming 
>> scheme denigrates "lazySet", a name  >> that no one likes!)  
>>>>  >> The form of these methods is roughly similar to  >> 
>> C++0x modes (at least the ones supportable in Java),  >> but 
>> avoids the sometimes-controversial terms  >> "acquire" and 
>> "release" in mode/method names.
>>  >>
>>  >> For now, the specs are just done informally. Assuming we  
>>>> go ahead with this, we'd adapt the more formal versions 
>> done  >> in Fences drafts and place them in j.u.c.atomic 
>> package docs  >> to spell out better.
>>  >>
>>  >> Versions for other stand-alone Atomic objects 
>> (AtomicLong,  >> AtomicReference) would be equally 
>> straightforward. As always, the  >> main problems lie in the 
>> FieldUpdater forms, which are sadly  >> also the most useful. 
>> The dynamic type checking overhead  >> required for these 
>> forms is often more expensive than  >> any savings you get 
>> from weaker access modes. (This is  >> why Fences versions 
>> continue to be attractive.) However,  >> for uniformity, 
>> extended methods in these would also be  >> supported. 
>> Similarly for the Atomic*Array classes.
>>  >> And it is still conceivable that new JVM mechanics being 
>> put  >> into place for JSR292 might be used to reduce overhead.
>>  >>
>>  >> One further accommodation is that the encapsulated int  
>>>> state value inside AbstractQueuedSynchronizer should also  
>>>> support at least the setInStoreOrder method (none of  >> 
>> the other new methods seem to ever apply). Similarly  >> for 
>> AbstractQueuedLongSynchronizer. Using this as  >> appropriate 
>> speeds up ReentrantLock by 5% - 20%, so  >> is a good idea 
>> regardless of all other changes.
>>  >>
>>  >> -Doug
>>  >>
>>  >> _______________________________________________
>>  >> Concurrency-interest mailing list
>>  >> Concurrency-interest at cs.oswego.edu
>>  >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>  >
>>  > _______________________________________________
>>  > Concurrency-interest mailing list
>>  > Concurrency-interest at cs.oswego.edu
>>  > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>  >
>>  >
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From davidcholmes at aapt.net.au  Mon Apr 19 18:23:11 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 20 Apr 2010 08:23:11 +1000
Subject: [concurrency-interest] Extended access methods for Atomics (and
	AQS)
In-Reply-To: <4BCC5CBD.9080000@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEPJIFAA.davidcholmes@aapt.net.au>

> If we had not already previously included some methods for
> non-volatile ordering control (i.e., weakCompareAndSet, lazySet)
> in AtomicX, I might be more sympathetic to creating classes

weakCompareAndSet has nothing to do with non-volatile-ordering.

As for lazySet ... that had dropped off my radar. So I guess the genie is
already out of the bottle.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug Lea
> Sent: Monday, 19 April 2010 11:38 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Extended access methods for Atomics
> (and AQS)
>
>
> On 04/15/10 08:49, David Holmes wrote:
> >  I just don't think it is a good idea to have these
> > obscure, rarely usable methods sitting along side the methods
> that get used
> > all the time.
> >
>
> I never know what to do about the immorality accusations
> whenever trying to improve the current state of affairs
> about ordering control for atomics. But here's another pass
> at laying out the motivation and current status:
>
> AtomicX already contains lazySet, which has a
> naming bug (=> setInReleaseOrder) and specs in need
> of improvement.  But AtomicX doesn't contain the
> complementary weaker-than-volatile form of get
> (=> getInRelaxedOrder) that is sometimes used with
> it (as mentioned by Hans). My draft proposal fixes these,
> and further regularizes API to accommodate a long-standing
> RFE to add corresponding forms of CAS. The net result
> has pretty much the same structure as C++0x atomics, which
> is not only a good sanity check, but also in the future
> (as C++0x atomics become supported by C++ compilers) will
> improve confidence that JVMs written in C++, runtime-systems
> supporting both C++ and Java, etc, work consistently.
>
> If we had not already previously included some methods for
> non-volatile ordering control (i.e., weakCompareAndSet, lazySet)
> in AtomicX, I might be more sympathetic to creating classes
> AtomicXWithExtendOrderingControl. But as it stands, I don't think
> the mere act of further fleshing them out and regularizing them
> is a good reason to do so.
>
> I illustrated API with AtomicInteger, but as always,
> the main practical need is for ordering methods applying
> to the fields of other objects (including array elements).
> These are more typical/desirable because without them:
> (1) if you need ordering control and must create
> another Atomic object to obtain it, then you have traded one
> problem for two problems, because you must now figure out
> how to correctly maintain reference to that Atomic;
> (2) Creating AtomicXs (especially cases like arrays of
> AtomicIntegers as opposed to arrays of ints supporting ordering
> control) can (and usually does) lead to so much space and
> indirection bloat that you may be better off just running slow
> lock-based code (assuming you can figure out how to use locks in
> such cases, which is not always possible).
>
> If Java/JVMs supported some way to force object-inlining
> so that a (suitably declared) AtomicInteger field of
> another object could be embedded as an int field but
> accessed via atomics, there would be no need for alternatives.
> But I don't think this will happen anytime soon.
>
> Short of such major language/JVM changes, one way of handling
> these cases is with Fences, that separate out the ordering
> control from the reads and writes (although unfortunately not CAS).
> This way, programmers just do the field accesses but surround
> them with acquire-, release-, or volatile- style fence method
> calls (which would normally be instrinsified and inlined).
>
> But many people objected to this approach: Some on
> morality grounds, others on usability grounds, and others
> because the specs under this form of separation are very
> hard to nail down and incorporate into JMM/JLS.
> It seems that most people agree that it would be better
> all around to support and spec ordered accesses
> than separating accesses plus orderings.
>
> One way to do this is to extend AtomicXFieldUpdaters.
> This is not a very nice choice. Java, Java bytes codes, and
> JVMs were not originally designed to support "l-value"
> operations on fields of objects beyond simple read/write.
> The only path to do so is via the defacto-standard "Unsafe"
> APIs that bypass all of the normal apparatus (including
> verification and safety checks) to perform ordered/atomic
> accesses at particular offsets of objects/arrays. So if
> you want to export these methods in public APIs, you must surround
> them with Java-level dynamic type- and accessibility-
> checking (and further, restrict use to fields marked as
> volatile, which doesn't work for arrays).
>
> Which brings us to the same kind of cure-worse-than-disease
> problem seen with the alternative of using standalone Atomics:
> On most platforms, the overhead of performing, say
> updater.lazySet(x) (aka setInReleaseOrder(x)) is greater than
> any savings you get from avoiding underlying hardware fences
> if you simply did a direct volatile write. (Worse, in some
> cases the checks may themselves entail stronger fences.)
> Even in their current forms (without the above minor extensions)
> no one is tempted to use these methods. (Google code search
> does not find even one  application-level usage of
> AtomicIntegerFieldupdater.lazySet.) Instead, as
> is occasionally posted on this list, people developing carefully
> weakly ordered code have learned to use Unsafe directly,
> avoiding the overhead. Although at the price of having no
> language-level static checking and containing code that does
> not work on systems with security managers.
>
> My hope has been that there might be some way to reduce
> this dynamic checking overhead in updaters, so that people
> can stop using Unsafe directly. But after some further exploration,
> I'm less optimistic about the prospects -- even with JSR292-based
> support, the cases in which you can trade improved updater
> creation-time checks for fewer or faster dynamic per-update
> checks don't seem to cover many usages.
>
> Which leaves me not having a really good plan at the moment.
> This is a problem that is not going to go away all by itself,
> so concrete suggestions would be welcome.
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Mon Apr 19 18:52:07 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 20 Apr 2010 08:52:07 +1000
Subject: [concurrency-interest] Extended access methods for Atomics
	(andAQS)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEPJIFAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEPKIFAA.davidcholmes@aapt.net.au>

I wrote:
> > If we had not already previously included some methods for
> > non-volatile ordering control (i.e., weakCompareAndSet, lazySet)
> > in AtomicX, I might be more sympathetic to creating classes
>
> weakCompareAndSet has nothing to do with non-volatile-ordering.

Which turns out to be wrong - sorry Doug. Although the primary function of
weakCAS is to allow spurious failures that are more amenable to a ll/sc
implementation, the package docs also define that it has weaker ordering
properties than strong-CAS.

David


> As for lazySet ... that had dropped off my radar. So I guess the genie is
> already out of the bottle.
>
> David
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug Lea
> > Sent: Monday, 19 April 2010 11:38 PM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] Extended access methods for Atomics
> > (and AQS)
> >
> >
> > On 04/15/10 08:49, David Holmes wrote:
> > >  I just don't think it is a good idea to have these
> > > obscure, rarely usable methods sitting along side the methods
> > that get used
> > > all the time.
> > >
> >
> > I never know what to do about the immorality accusations
> > whenever trying to improve the current state of affairs
> > about ordering control for atomics. But here's another pass
> > at laying out the motivation and current status:
> >
> > AtomicX already contains lazySet, which has a
> > naming bug (=> setInReleaseOrder) and specs in need
> > of improvement.  But AtomicX doesn't contain the
> > complementary weaker-than-volatile form of get
> > (=> getInRelaxedOrder) that is sometimes used with
> > it (as mentioned by Hans). My draft proposal fixes these,
> > and further regularizes API to accommodate a long-standing
> > RFE to add corresponding forms of CAS. The net result
> > has pretty much the same structure as C++0x atomics, which
> > is not only a good sanity check, but also in the future
> > (as C++0x atomics become supported by C++ compilers) will
> > improve confidence that JVMs written in C++, runtime-systems
> > supporting both C++ and Java, etc, work consistently.
> >
> > If we had not already previously included some methods for
> > non-volatile ordering control (i.e., weakCompareAndSet, lazySet)
> > in AtomicX, I might be more sympathetic to creating classes
> > AtomicXWithExtendOrderingControl. But as it stands, I don't think
> > the mere act of further fleshing them out and regularizing them
> > is a good reason to do so.
> >
> > I illustrated API with AtomicInteger, but as always,
> > the main practical need is for ordering methods applying
> > to the fields of other objects (including array elements).
> > These are more typical/desirable because without them:
> > (1) if you need ordering control and must create
> > another Atomic object to obtain it, then you have traded one
> > problem for two problems, because you must now figure out
> > how to correctly maintain reference to that Atomic;
> > (2) Creating AtomicXs (especially cases like arrays of
> > AtomicIntegers as opposed to arrays of ints supporting ordering
> > control) can (and usually does) lead to so much space and
> > indirection bloat that you may be better off just running slow
> > lock-based code (assuming you can figure out how to use locks in
> > such cases, which is not always possible).
> >
> > If Java/JVMs supported some way to force object-inlining
> > so that a (suitably declared) AtomicInteger field of
> > another object could be embedded as an int field but
> > accessed via atomics, there would be no need for alternatives.
> > But I don't think this will happen anytime soon.
> >
> > Short of such major language/JVM changes, one way of handling
> > these cases is with Fences, that separate out the ordering
> > control from the reads and writes (although unfortunately not CAS).
> > This way, programmers just do the field accesses but surround
> > them with acquire-, release-, or volatile- style fence method
> > calls (which would normally be instrinsified and inlined).
> >
> > But many people objected to this approach: Some on
> > morality grounds, others on usability grounds, and others
> > because the specs under this form of separation are very
> > hard to nail down and incorporate into JMM/JLS.
> > It seems that most people agree that it would be better
> > all around to support and spec ordered accesses
> > than separating accesses plus orderings.
> >
> > One way to do this is to extend AtomicXFieldUpdaters.
> > This is not a very nice choice. Java, Java bytes codes, and
> > JVMs were not originally designed to support "l-value"
> > operations on fields of objects beyond simple read/write.
> > The only path to do so is via the defacto-standard "Unsafe"
> > APIs that bypass all of the normal apparatus (including
> > verification and safety checks) to perform ordered/atomic
> > accesses at particular offsets of objects/arrays. So if
> > you want to export these methods in public APIs, you must surround
> > them with Java-level dynamic type- and accessibility-
> > checking (and further, restrict use to fields marked as
> > volatile, which doesn't work for arrays).
> >
> > Which brings us to the same kind of cure-worse-than-disease
> > problem seen with the alternative of using standalone Atomics:
> > On most platforms, the overhead of performing, say
> > updater.lazySet(x) (aka setInReleaseOrder(x)) is greater than
> > any savings you get from avoiding underlying hardware fences
> > if you simply did a direct volatile write. (Worse, in some
> > cases the checks may themselves entail stronger fences.)
> > Even in their current forms (without the above minor extensions)
> > no one is tempted to use these methods. (Google code search
> > does not find even one  application-level usage of
> > AtomicIntegerFieldupdater.lazySet.) Instead, as
> > is occasionally posted on this list, people developing carefully
> > weakly ordered code have learned to use Unsafe directly,
> > avoiding the overhead. Although at the price of having no
> > language-level static checking and containing code that does
> > not work on systems with security managers.
> >
> > My hope has been that there might be some way to reduce
> > this dynamic checking overhead in updaters, so that people
> > can stop using Unsafe directly. But after some further exploration,
> > I'm less optimistic about the prospects -- even with JSR292-based
> > support, the cases in which you can trade improved updater
> > creation-time checks for fewer or faster dynamic per-update
> > checks don't seem to cover many usages.
> >
> > Which leaves me not having a really good plan at the moment.
> > This is a problem that is not going to go away all by itself,
> > so concrete suggestions would be welcome.
> >
> > -Doug
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From hans.boehm at hp.com  Mon Apr 19 19:14:03 2010
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 19 Apr 2010 23:14:03 +0000
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <4BCCBF71.8060508@cox.net>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
	<4BCCBF71.8060508@cox.net>
Message-ID: <238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>

>From my perspective, it's important that

a) There is a usable subset of the language for which things are simple.  I think that currently consists of programs that (1) contain no data races, and (2) avoid a certain set of library calls that include lazySet and a handful of others.  Java guarantees sequential consistency for this subset, and the visibility issues don't arise.

b) There is an easy way to tell when you are leaving this subset.  This is currently getting a bit ugly.

For C++0x, the distinction (b) was made by requiring an explicit memory_order_X argument when you want to leave this subset.  I'm not sure what the best way is to make this distinction in Java.  As Doug points out, the pre-existence of lazySet and weakCompareAndSet complicate matters.  Possibly so does the desire to deal with array elements and the like; I'm not sure.

It seems to me that either new classes or a C++0x-like approach would require at least deprecation of lazySet and weakCompareAndSet.  That's likely to be to be a tough sell.

> From: Gregg Wonderly [mailto:gergg at cox.net] 

> What I thinking about, is the fact that I see the read vs 
> write accesses as a range of possibilities.  If you order 
> them from weaker on the outside to stronger on the inside as 
> something like
> 
> 	non-volatile -> relaxed -> volatile:read
> 
> and
> 
> 	write:volatile <- relaxed <- non-volatile
> 
> then you can imagine a "window" where particular guarantees 
> can hold.  Outside of that window, things are weaker.  For 
> any particular application, I am thinking that this window is 
> never completely open.
> 
I'm not quite sure what you mean by "window" here.  One of the problems with the weaker ordering guarantees is that we don't seem to have a good handle on using weakly ordered operations (or data races on ordinary variables) in a way that is only locally visible.  Using weakly ordered operations anywhere seems to involve some danger of "contaminating" the whole program.  There do seem to be a few important idioms, like double-checked locking, that do localize the damage caused by weakly-ordered operations, but I don't know how to reason about that in general.

If I had to pick three possible ordering guarantees for atomic variables, I would add some flavor of acquire-release ordering, and keep only one of "non-volatile" and "relaxed".

Hans

From gustav.trede at gmail.com  Tue Apr 20 02:43:26 2010
From: gustav.trede at gmail.com (gustav trede)
Date: Tue, 20 Apr 2010 08:43:26 +0200
Subject: [concurrency-interest] Extended access methods for Atomics (and
	AQS)
In-Reply-To: <4BCCBF71.8060508@cox.net>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
	<4BCCBF71.8060508@cox.net>
Message-ID: <r2w311e0eaf1004192343g97c1b17er74f8c606c58cfde6@mail.gmail.com>

Hello ,

So because of the fact that most people being "software engineers" are
evidently unsuitable for it and belongs in userland we should keep ignoring
the technical real world and make it hard or impossible for the coders to do
some actual good solutions ?.

Why is it that these people who cant comprehend or don't want to due to lack
of real passion and motivation insists that there must not exist stuff they
cant use without risk that should set the agenda ?
Nobody forces them to use it.

Its an attitude and fundamentally a competence problem if people chooses to
use stuff they don't understand that leads to being part of IT problems
instead of solutions regardless of cookie cutter tool given to work with.
If they neither can or want to deal with the technical real world, then why
are they tormenting them self and their colleagues with their presence as
developers instead of choosing a profession that fits them ?.

The hardware is keeping to evolve and becoming more complex just as the
number of software abstractions layer and API standards keep increasing, it
requires people will real interest and passion that don't react with fear
but becomes inspired by problems and hence analyze and read docs and specs
involved _before_ starting to implement.


On 19 April 2010 22:39, Gregg Wonderly <gergg at cox.net> wrote:

> Hans, from my perspective, as just a developer that tires of having to deal
> with hardware issues, is that we shouldn't keep trying to make software
> easier to get wrong.
>
> Multi-threading in Java, due to weak visibility guarentees, is painful.
> Further, I find it really hard to teach people this stuff, and I am not sure
> why we find it so easy to keep adding to what needs to be learned.
>
> What I thinking about, is the fact that I see the read vs write accesses as
> a range of possibilities.  If you order them from weaker on the outside to
> stronger on the inside as something like
>
>        non-volatile -> relaxed -> volatile:read
>
> and
>
>        write:volatile <- relaxed <- non-volatile
>
> then you can imagine a "window" where particular guarantees can hold.
>  Outside of that window, things are weaker.  For any particular application,
> I am thinking that this window is never completely open.
>
> Writes that are relaxed might be the weakest you want, while reads might
> even be non-volatile, but I'm not sure why you would make wide use of
> AtomicX in that case.
>
> I am thinking then, that a set of classes with names like:
>
> NonVolatileReadRelaxedWriteAtomicInt
> RelaxedReadVolatileWriteAtomicInt
>
> etc would be helpful in that they could have all of the methods that
> AtomicInt has, and as subclasses, they could override methods that are
> weaker than the class name stipulates and make them live up to the "name".
>
> This would allow some interesting "plugability" for quick testing of
> visibility/synchronization as a "fix" for something.  You could plug in
> VolatileReadVolatileWriteAtomicInt and see if that fixes the problem or at
> least changes the behavior in a way that suggests what the issue might be.
>
> I still worry the most about people using weaker access by habit and then
> threading changing so that a different thread carries out a task then was
> used before and now the weaker access is not appropriate.  Those kinds of
> problems are very difficult to find, and tooling or API support to find and
> debug these types of things seems like an upfront need.
>
> Gregg Wonderly
>
>
> Boehm, Hans wrote:
>
>> I'm not sure I fully understand this proposal.  However, I think it's
>> important to structure the interface so that, if you really need to use
>> accessors with weaker
>> ordering, you can mix them with more strongly ordered ones, including
>> volatile
>> style accesses.  For example, the second load in double-checked locking
>> doesn't
>>
> > actually race with anything.  Thus it's unusually safe to use a "relaxed"
> (or
>
>> even "nonVolatile") access here.  But that doesn't mean you want "relaxed"
>> accesses everywhere.
>>
>> I think it's also very reasonable to write code initially to use only
>> volatile-style (sequentially consistent)accesses, and then relax that only
>> in performance critical cases that you've hopefully spent a lot of time
>> thinking about.
>>
>> Hans
>>
>>  -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:
>>> concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Gregg Wonderly
>>> Sent: Friday, April 16, 2010 10:30 AM
>>> To: dholmes at ieee.org
>>> Cc: Doug Lea; concurrency-interest at cs.oswego.edu
>>> Subject: Re: [concurrency-interest] Extended access methods for Atomics
>>> (and AQS)
>>>
>>> I sent this earlier and apparently only replied to David...  I changed
>>> some wording to try and be clearer.
>>>
>>> I agree with David, that the use of names which include the keywords that
>>> developers are already learning the meaning/use of would be helpful.  I also
>>> concur that in many cases, there may be bugs created by misuse just for the
>>> sake of speed, or because they are copying code from somewhere else and
>>> using it incorrectly.
>>>
>>> I'd think that a subclass, which completely encompasses the "asIfFinal"
>>> or "nonVolatile" etc aspects of use would be an improvement because it would
>>> provide a place for "reading javadoc" about why you might choose to use the
>>> class.  The subclass could also guarantee that the "visible" behavior was no
>>> "better" than the class was documented to provide.  I say "better" in the
>>> sense that putting these methods in AtomicInteger, for example, means that
>>> all the "set" and "get" functions are available.  I'd guess that you would
>>> not want results from the other methods that would produce differing or
>>> dissimilar behavior.
>>>
>>> It seems that's where the bugs would start to appear because one branch
>>> of existing code is using appropriate methods correctly and some new code is
>>> not, and occasionally visibility if fixed by one branch, while the other
>>> occasionally misses a value because of a data race for example.
>>>
>>> I'm wondering if subclass names which have to do with specific use cases
>>> such as "AtomicIntWithReadAsIfFinalAndWriteAsIfVolatile" etc. are useful?
>>>
>>> Gregg Wonderly
>>>
>>> David Holmes wrote:
>>>  > Hi Doug,
>>>  >
>>>  > I think "InStoreorder" and "InRelaxedOrder" could easily be
>>> misunderstood as  > relating to "total store ordering", or "relaxed memory
>>> models" or any  > particular architecural memory model that uses that kind
>>> of terminology.
>>>  >
>>>  > If "InStoreOrder" means "with memory effects equivalent to setting a
>>> final  > variable" then lets just say that:
>>>  >
>>>  >    setAsIfFinal(int newVal)
>>>  >
>>>  > and similarly:
>>>  >
>>>  >    setAsNonVolatile(int newVal)
>>>  >
>>>  >
>>>  > I guess this is somewhat better than Fences in that the semantics of
>>> the  > methods are easier to understand. But these are still methods that
>>> the vast  > majority of programmers won't know when it is valid to use them.
>>> They will  > only discover that they seem faster and so use them regardless
>>> :( Can we not  > "hide" them a bit better eg:
>>>  >
>>>  >   class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicInteger extends
>>>  > AtomicInteger {
>>>  >       public void setAsNonVolatile(int newVal) { ... }
>>>  >       ...
>>>  >   }
>>>  >
>>>  > or a utility class
>>>  >
>>>  >   class DontUseMeUnlessYouKnowWhatYouAreDoingAtomicHelper {
>>>  >      static void setAsNonVolatile(AtomicInteger x, int newVal) { ... }
>>>  >      ...
>>>  >   }
>>>  >
>>>  >> One further accommodation is that the encapsulated int
>>>
>>>> state value inside AbstractQueuedSynchronizer should also  support at
>>>>> least the setInStoreOrder method  >  > I fear
>>>>>
>>>> the chance of misuse greatly outweighs any performance benefit.
>>>  >
>>>  >
>>>  > Aside: if I ever see a bug report involving these new methods my
>>> response  > will be to request the submitter to rewrite their program using
>>> the "proper"
>>>  > methods. Without tools or formalisms to establish correctness of use
>>> these  > methods will in many cases just be bugs waiting to happen.
>>>  >
>>>  > Cheers,
>>>  > David
>>>  >
>>>  >
>>>  >> -----Original Message-----
>>>  >> From: concurrency-interest-bounces at cs.oswego.edu
>>>  >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug
>>> Lea  >> Sent: Wednesday, 14 April 2010 10:53 PM  >> To:
>>> concurrency-interest at cs.oswego.edu
>>>  >> Subject: [concurrency-interest] Extended access methods for Atomics
>>> (and  >> AQS)  >>  >>  >> Last fall, we tabled discussion of the proposed
>>> Fences API.
>>>  >> But regardless of the outcome of and future discussions when  >> we
>>> re-raise it, there seemed to be consensus that at the very  >> least, we
>>> should extend the methods of Atomic classes to support  >> the various
>>> memory effect modes that you otherwise would need  >> Fences methods to
>>> obtain.
>>>  >>
>>>  >> I'm finally trying this out. A draft update of AtomicInteger  >> with
>>> these changes is at:
>>>  >> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
>>>  >> atomic/AtomicInteger.html
>>>  >> (For now, only class AtomicInteger). Ignoring the arithmetic methods
>>>  >> (getAndAdd etc), the API looks like (with "*" in front of  >> the added
>>> methods)  >>  >> "volatile-write" mode
>>>  >>     void set(int v)
>>>  >>     boolean compareAndSet(int e, int v);
>>>  >>     int setAndGet(int v);
>>>  >>
>>>  >> store-ordered (aka release, pseudo-final-field) mode  >> *  void
>>> setInStoreOrder(int v);  >> *  boolean compareAndSetInStoreOrder(int e, int
>>> v);
>>>  >>     void lazySet(int v) // synonym for setInStoreOrder
>>>  >>
>>>  >> load-ordered (aka volatile-read, acquire) mode
>>>  >>     int get();
>>>  >> *  compareAndSetAndGet(int e, int v);  >>  >> relaxed-order (aka
>>> non-volatile) mode  >> *  int getInRelaxedOrder();  >> *  void
>>> setInRelaxedOrder(int v);
>>>  >>     boolean weakCompareAndSet(int e, int v);
>>>  >>
>>>  >> Comments and suggestions about method names and  >> semantics would
>>> be very welcome. (Notice that  >> the naming scheme denigrates "lazySet", a
>>> name  >> that no one likes!)
>>>
>>>>  >> The form of these methods is roughly similar to  >>
>>>>>
>>>> C++0x modes (at least the ones supportable in Java),  >> but avoids the
>>> sometimes-controversial terms  >> "acquire" and "release" in mode/method
>>> names.
>>>  >>
>>>  >> For now, the specs are just done informally. Assuming we
>>>
>>>> go ahead with this, we'd adapt the more formal versions
>>>>>
>>>> done  >> in Fences drafts and place them in j.u.c.atomic package docs
>>>  >> to spell out better.
>>>  >>
>>>  >> Versions for other stand-alone Atomic objects (AtomicLong,  >>
>>> AtomicReference) would be equally straightforward. As always, the  >> main
>>> problems lie in the FieldUpdater forms, which are sadly  >> also the most
>>> useful. The dynamic type checking overhead  >> required for these forms is
>>> often more expensive than  >> any savings you get from weaker access modes.
>>> (This is  >> why Fences versions continue to be attractive.) However,  >>
>>> for uniformity, extended methods in these would also be  >> supported.
>>> Similarly for the Atomic*Array classes.
>>>  >> And it is still conceivable that new JVM mechanics being put  >> into
>>> place for JSR292 might be used to reduce overhead.
>>>  >>
>>>  >> One further accommodation is that the encapsulated int
>>>
>>>> state value inside AbstractQueuedSynchronizer should also  support at
>>>>> least the setInStoreOrder method (none of  >>
>>>>>
>>>> the other new methods seem to ever apply). Similarly  >> for
>>> AbstractQueuedLongSynchronizer. Using this as  >> appropriate speeds up
>>> ReentrantLock by 5% - 20%, so  >> is a good idea regardless of all other
>>> changes.
>>>  >>
>>>  >> -Doug
>>>  >>
>>>  >> _______________________________________________
>>>  >> Concurrency-interest mailing list
>>>  >> Concurrency-interest at cs.oswego.edu
>>>  >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>  >
>>>  > _______________________________________________
>>>  > Concurrency-interest mailing list
>>>  > Concurrency-interest at cs.oswego.edu
>>>  > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>  >
>>>  >
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>  _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
regards
 gustav trede
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100420/a01509cc/attachment-0001.html>

From dl at cs.oswego.edu  Tue Apr 20 07:17:41 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 20 Apr 2010 07:17:41 -0400
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
	<4BCCBF71.8060508@cox.net>
	<238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4BCD8D55.3080600@cs.oswego.edu>

On 04/19/10 19:14, Boehm, Hans wrote:
>> From my perspective, it's important that
>
> a) There is a usable subset of the language for which things are simple.

> b) There is an easy way to tell when you are leaving this subset.

The initial idea was that this subset need not include AtomicX.
But I guess as AtomicX becomes more "ordinary", some people
are less comfortable with this? Perhaps AtomicXFieldUpdater is still
exotic and ugly enough to qualify? If not, calls to Unsafe
definitely do, since you are no longer even coding in "Java",
but instead coding in JVMese. (Unsafe has never been standardized
as part of the language or JVM).

Most of the small community of users of weak ordering methods
would not object to making it arbitrarily ugly to use them.
I suspect that they would be OK with continuing to use
Unsafe calls if we could solve the problems
of standardizing them (as part of JVM, not Java spec?) and
finding some path to allow such code to run on systems with
security managers. (Ideas welcome.)

The main exceptions to this are some of the less esoteric
usages we discussed with Fences, such as safe publication
of fields that act as final but cannot be declared as final
because their values are not set inside constructors
(for example in dependency injection frameworks).

-Doug


From dl at cs.oswego.edu  Tue Apr 20 07:49:58 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 20 Apr 2010 07:49:58 -0400
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <4BCCBF71.8060508@cox.net>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
	<4BCCBF71.8060508@cox.net>
Message-ID: <4BCD94E6.6000601@cs.oswego.edu>

On 04/19/10 16:39, Gregg Wonderly wrote:
>
> If you order them from weaker on the
> outside to stronger on the inside as something like
>
> non-volatile -> relaxed -> volatile:read
>

Just for the sake of clarifying terminology:
"relaxed" refers to accessing an atomic variable, that by default
acts as volatile, as if it were neither volatile nor final.
So, "non-volatile" and "relaxed" are basically the same concept.

-Doug



From gergg at cox.net  Tue Apr 20 12:07:01 2010
From: gergg at cox.net (Gregg Wonderly)
Date: Tue, 20 Apr 2010 11:07:01 -0500
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <r2w311e0eaf1004192343g97c1b17er74f8c606c58cfde6@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>	<4BC89EA9.1020204@cytetech.com>	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>	<4BCCBF71.8060508@cox.net>
	<r2w311e0eaf1004192343g97c1b17er74f8c606c58cfde6@mail.gmail.com>
Message-ID: <4BCDD125.6030005@cox.net>

gustav trede wrote:
> Hello ,
> 
> So because of the fact that most people being "software engineers" are 
> evidently unsuitable for it and belongs in userland we should keep 
> ignoring the technical real world and make it hard or impossible for the 
> coders to do some actual good solutions ?.

Gustav, I appreciate this perspective, and I'm all for allow developers to 
maximize use of hardware.  The issue for me, is that I started using Java in 
1996 as my sole language of development, because I didn't want to have to mess 
around with verbose and problematic memory management.

The availability of simple "synchronized" made it very easy to manage 
synchronization between a handful of threads in a uni-processor environment.

As hardware has reached a standstill on clock speeds, we of course now have 
multi-core, hyper-threaded processors and multiples of those to boot.

So, now, we have contention for resources as a much more important performance 
issue, because you can't just crank up the clock speed.  Most things that you 
can do end up being less time consuming (sometimes by a big margin) than the 
time through a "lock", so threads eventually pile up in a line behind locks and 
we get much more interested in lock-free APIs.

All of this work to use fences to manage visibility and sometimes even 
synchronization, along with CAS, just feels like the wrong solution path to me.

It exposes hardware issues and encourages locks as a solution, both of which 
help make software much less portable and much more fragile over the longer life 
of the software.

Being a great software engineer and doing awesome things with software is a 
great achievement.  But, we are all mortal.  In the end, I think its much 
smarter to think about how we can keep software simple (and I don't just mean 
easy for ignorant people to write), so that success is easier.

In the end, we might have to just deal with the fencing directly.  But, I am not 
sure that there is really a great deal to be gained from that.  Instead, we 
might be better served to create classes with focused support for a particular 
problem which might use the "Unsafe" operations, but provide a "monitor" like 
function around that use which keeps software from decaying into "wrong".

Gregg Wonderly

From gergg at cox.net  Tue Apr 20 12:09:03 2010
From: gergg at cox.net (Gregg Wonderly)
Date: Tue, 20 Apr 2010 11:09:03 -0500
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <4BCD94E6.6000601@cs.oswego.edu>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>	<4BC89EA9.1020204@cytetech.com>	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>	<4BCCBF71.8060508@cox.net>
	<4BCD94E6.6000601@cs.oswego.edu>
Message-ID: <4BCDD19F.9050105@cox.net>

I guess I am not remembering the details correctly, sorry for confusing terminology.

Gregg Wonderly

Doug Lea wrote:
> On 04/19/10 16:39, Gregg Wonderly wrote:
>>
>> If you order them from weaker on the
>> outside to stronger on the inside as something like
>>
>> non-volatile -> relaxed -> volatile:read
>>
> 
> Just for the sake of clarifying terminology:
> "relaxed" refers to accessing an atomic variable, that by default
> acts as volatile, as if it were neither volatile nor final.
> So, "non-volatile" and "relaxed" are basically the same concept.
> 
> -Doug
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From gregg at cytetech.com  Tue Apr 20 12:19:37 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 20 Apr 2010 11:19:37 -0500
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
	<4BCCBF71.8060508@cox.net>
	<238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4BCDD419.6050104@cytetech.com>

Boehm, Hans wrote:
>>From my perspective, it's important that
> 
> a) There is a usable subset of the language for which things are simple. 
> I think that currently consists of programs that (1) contain no data races,
> and (2) avoid a certain set of library calls that include lazySet and a 
> handful of others.  Java guarantees sequential consistency for this subset,
> and the visibility issues don't arise.

Data races seem to often appear as people try to make use of more "cores" by 
creating threading and associated localized processing which can result in 
publication of values between threads which then has to be mediated for 
visibility guarantees.  This is the specific problem that I seem to read about 
and see people having problems with over and over.

> b) There is an easy way to tell when you are leaving this subset. 
 > This is currently getting a bit ugly.

When there is more than one thread running, you have this problem.  For Java 
client programs, you have the EDT and the applications main thread that are 
separate.  If you then drive everything from EDT event dispatches, that can work 
for applications without networking or other background activity.  As soon as 
you add another thread, the possibilities of data races soar.

> For C++0x, the distinction (b) was made by requiring an explicit 
 > memory_order_X argument when you want to leave this subset.  I'm not sure
> what the best way is to make this distinction in Java.  As Doug points out,
> the pre-existence of lazySet and weakCompareAndSet complicate matters. 
> Possibly so does the desire to deal with array elements and the like; I'm not sure.

Only publication of arrays creates the problem in a multi-threaded environment. 
  A single thread has no problems with an array.  Multiple threads are going to 
be a given in any Java application it seems to me, so I'm not sure how any Java 
application can have sequential consistency without some degree of 
"synchronized" or "volatile" for any mutable values.

It is all of the overhead and problems associated with the "delay" through such 
"operations" that we are talking about addressing I thought.

> It seems to me that either new classes or a C++0x-like approach would require
 > at least deprecation of lazySet and weakCompareAndSet.  That's likely to be
 > to be a tough sell.

Correctness of software is an important detail.  Being able to somehow 
understand and prove this is vital as we decide more and more on making software 
systems a part of things we depend on for continuous duty.

Gregg Wonderly

>> From: Gregg Wonderly [mailto:gergg at cox.net] 
> 
>> What I thinking about, is the fact that I see the read vs 
>> write accesses as a range of possibilities.  If you order 
>> them from weaker on the outside to stronger on the inside as 
>> something like
>>
>> 	non-volatile -> relaxed -> volatile:read
>>
>> and
>>
>> 	write:volatile <- relaxed <- non-volatile
>>
>> then you can imagine a "window" where particular guarantees 
>> can hold.  Outside of that window, things are weaker.  For 
>> any particular application, I am thinking that this window is 
>> never completely open.
>>
> I'm not quite sure what you mean by "window" here.  One of the problems with the weaker ordering guarantees is that we don't seem to have a good handle on using weakly ordered operations (or data races on ordinary variables) in a way that is only locally visible.  Using weakly ordered operations anywhere seems to involve some danger of "contaminating" the whole program.  There do seem to be a few important idioms, like double-checked locking, that do localize the damage caused by weakly-ordered operations, but I don't know how to reason about that in general.
> 
> If I had to pick three possible ordering guarantees for atomic variables, I would add some flavor of acquire-release ordering, and keep only one of "non-volatile" and "relaxed".
> 
> Hans
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From joe.bowbeer at gmail.com  Tue Apr 20 13:06:18 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 20 Apr 2010 10:06:18 -0700
Subject: [concurrency-interest] Extended access methods for Atomics (and
	AQS)
In-Reply-To: <4BCDD419.6050104@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
	<4BCCBF71.8060508@cox.net>
	<238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>
	<4BCDD419.6050104@cytetech.com>
Message-ID: <k2v31f2a7bd1004201006qa705eb70q19ad0aec0271b87f@mail.gmail.com>

I think it would be helpful to hear directly from more of the folks who use
the Unsafe stuff currently.

Is there anyone (besides Doug:) who can pipe up?

On Tue, Apr 20, 2010 at 9:19 AM, Gregg Wonderly wrote:

> Boehm, Hans wrote:
>
>> From my perspective, it's important that
>>>
>>
>> a) There is a usable subset of the language for which things are simple. I
>> think that currently consists of programs that (1) contain no data races,
>> and (2) avoid a certain set of library calls that include lazySet and a
>> handful of others.  Java guarantees sequential consistency for this subset,
>> and the visibility issues don't arise.
>>
>
> Data races seem to often appear as people try to make use of more "cores"
> by creating threading and associated localized processing which can result
> in publication of values between threads which then has to be mediated for
> visibility guarantees.  This is the specific problem that I seem to read
> about and see people having problems with over and over.
>
>
>  b) There is an easy way to tell when you are leaving this subset.
>>
> > This is currently getting a bit ugly.
>
> When there is more than one thread running, you have this problem.  For
> Java client programs, you have the EDT and the applications main thread that
> are separate.  If you then drive everything from EDT event dispatches, that
> can work for applications without networking or other background activity.
>  As soon as you add another thread, the possibilities of data races soar.
>
>
>  For C++0x, the distinction (b) was made by requiring an explicit
>>
> > memory_order_X argument when you want to leave this subset.  I'm not sure
>
>> what the best way is to make this distinction in Java.  As Doug points
>> out,
>> the pre-existence of lazySet and weakCompareAndSet complicate matters.
>> Possibly so does the desire to deal with array elements and the like; I'm
>> not sure.
>>
>
> Only publication of arrays creates the problem in a multi-threaded
> environment.  A single thread has no problems with an array.  Multiple
> threads are going to be a given in any Java application it seems to me, so
> I'm not sure how any Java application can have sequential consistency
> without some degree of "synchronized" or "volatile" for any mutable values.
>
> It is all of the overhead and problems associated with the "delay" through
> such "operations" that we are talking about addressing I thought.
>
>
>  It seems to me that either new classes or a C++0x-like approach would
>> require
>>
> > at least deprecation of lazySet and weakCompareAndSet.  That's likely to
> be
> > to be a tough sell.
>
> Correctness of software is an important detail.  Being able to somehow
> understand and prove this is vital as we decide more and more on making
> software systems a part of things we depend on for continuous duty.
>
> Gregg Wonderly
>
>
>  From: Gregg Wonderly
>>>
>>
>>  What I thinking about, is the fact that I see the read vs write accesses
>>> as a range of possibilities.  If you order them from weaker on the outside
>>> to stronger on the inside as something like
>>>
>>>        non-volatile -> relaxed -> volatile:read
>>>
>>> and
>>>
>>>        write:volatile <- relaxed <- non-volatile
>>>
>>> then you can imagine a "window" where particular guarantees can hold.
>>>  Outside of that window, things are weaker.  For any particular application,
>>> I am thinking that this window is never completely open.
>>>
>>>  I'm not quite sure what you mean by "window" here.  One of the problems
>> with the weaker ordering guarantees is that we don't seem to have a good
>> handle on using weakly ordered operations (or data races on ordinary
>> variables) in a way that is only locally visible.  Using weakly ordered
>> operations anywhere seems to involve some danger of "contaminating" the
>> whole program.  There do seem to be a few important idioms, like
>> double-checked locking, that do localize the damage caused by weakly-ordered
>> operations, but I don't know how to reason about that in general.
>>
>> If I had to pick three possible ordering guarantees for atomic variables,
>> I would add some flavor of acquire-release ordering, and keep only one of
>> "non-volatile" and "relaxed".
>>
>> Hans
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100420/77525996/attachment.html>

From michael.m.spiegel at gmail.com  Tue Apr 20 13:45:26 2010
From: michael.m.spiegel at gmail.com (Michael Spiegel)
Date: Tue, 20 Apr 2010 13:45:26 -0400
Subject: [concurrency-interest] Extended access methods for Atomics (and
	AQS)
In-Reply-To: <k2v31f2a7bd1004201006qa705eb70q19ad0aec0271b87f@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
	<4BCCBF71.8060508@cox.net>
	<238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>
	<4BCDD419.6050104@cytetech.com>
	<k2v31f2a7bd1004201006qa705eb70q19ad0aec0271b87f@mail.gmail.com>
Message-ID: <n2j1901f69e1004201045m1f50ee82maf89a975fef9b558@mail.gmail.com>

I use unsafe.putObjectVolatile() in my implementation of
ConcurrentSkipTreeSet that is backed by an underlying
ConcurrentSkipTreeMap.  It is identical to the trick that Doug uses in
j.u.c.ConcurrentSkipListSet.

On Tue, Apr 20, 2010 at 1:06 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> I think it would be helpful to hear directly from more of the folks who use
> the Unsafe stuff currently.
>
> Is there anyone (besides Doug:) who can pipe up?
>
> On Tue, Apr 20, 2010 at 9:19 AM, Gregg Wonderly wrote:
>>
>> Boehm, Hans wrote:
>>>>
>>>> >From my perspective, it's important that
>>>
>>> a) There is a usable subset of the language for which things are simple.
>>> I think that currently consists of programs that (1) contain no data races,
>>> and (2) avoid a certain set of library calls that include lazySet and a
>>> handful of others. ?Java guarantees sequential consistency for this subset,
>>> and the visibility issues don't arise.
>>
>> Data races seem to often appear as people try to make use of more "cores"
>> by creating threading and associated localized processing which can result
>> in publication of values between threads which then has to be mediated for
>> visibility guarantees. ?This is the specific problem that I seem to read
>> about and see people having problems with over and over.
>>
>>> b) There is an easy way to tell when you are leaving this subset.
>>
>> > This is currently getting a bit ugly.
>>
>> When there is more than one thread running, you have this problem. ?For
>> Java client programs, you have the EDT and the applications main thread that
>> are separate. ?If you then drive everything from EDT event dispatches, that
>> can work for applications without networking or other background activity.
>> ?As soon as you add another thread, the possibilities of data races soar.
>>
>>> For C++0x, the distinction (b) was made by requiring an explicit
>>
>> > memory_order_X argument when you want to leave this subset. ?I'm not
>> > sure
>>>
>>> what the best way is to make this distinction in Java. ?As Doug points
>>> out,
>>> the pre-existence of lazySet and weakCompareAndSet complicate matters.
>>> Possibly so does the desire to deal with array elements and the like; I'm
>>> not sure.
>>
>> Only publication of arrays creates the problem in a multi-threaded
>> environment. ?A single thread has no problems with an array. ?Multiple
>> threads are going to be a given in any Java application it seems to me, so
>> I'm not sure how any Java application can have sequential consistency
>> without some degree of "synchronized" or "volatile" for any mutable values.
>>
>> It is all of the overhead and problems associated with the "delay" through
>> such "operations" that we are talking about addressing I thought.
>>
>>> It seems to me that either new classes or a C++0x-like approach would
>>> require
>>
>> > at least deprecation of lazySet and weakCompareAndSet. ?That's likely to
>> > be
>> > to be a tough sell.
>>
>> Correctness of software is an important detail. ?Being able to somehow
>> understand and prove this is vital as we decide more and more on making
>> software systems a part of things we depend on for continuous duty.
>>
>> Gregg Wonderly
>>
>>>> From: Gregg Wonderly
>>>
>>>> What I thinking about, is the fact that I see the read vs write accesses
>>>> as a range of possibilities. ?If you order them from weaker on the outside
>>>> to stronger on the inside as something like
>>>>
>>>> ? ? ? ?non-volatile -> relaxed -> volatile:read
>>>>
>>>> and
>>>>
>>>> ? ? ? ?write:volatile <- relaxed <- non-volatile
>>>>
>>>> then you can imagine a "window" where particular guarantees can hold.
>>>> ?Outside of that window, things are weaker. ?For any particular application,
>>>> I am thinking that this window is never completely open.
>>>>
>>> I'm not quite sure what you mean by "window" here. ?One of the problems
>>> with the weaker ordering guarantees is that we don't seem to have a good
>>> handle on using weakly ordered operations (or data races on ordinary
>>> variables) in a way that is only locally visible. ?Using weakly ordered
>>> operations anywhere seems to involve some danger of "contaminating" the
>>> whole program. ?There do seem to be a few important idioms, like
>>> double-checked locking, that do localize the damage caused by weakly-ordered
>>> operations, but I don't know how to reason about that in general.
>>>
>>> If I had to pick three possible ordering guarantees for atomic variables,
>>> I would add some flavor of acquire-release ordering, and keep only one of
>>> "non-volatile" and "relaxed".
>>>
>>> Hans
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From concurrency-interest at cs.oswego.edu  Tue Apr 20 14:41:41 2010
From: concurrency-interest at cs.oswego.edu (VIAGRA  Official Seller)
Date: Tue, 20 Apr 2010 14:41:41 -0400 (EDT)
Subject: [concurrency-interest] "BEST PRICE "  49% OFF!
Message-ID: <20100421014142.3343.qmail@sheren>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100420/358eb6b1/attachment.html>

From david.lloyd at redhat.com  Tue Apr 20 15:24:35 2010
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Tue, 20 Apr 2010 14:24:35 -0500
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <n2j1901f69e1004201045m1f50ee82maf89a975fef9b558@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>	<4BC89EA9.1020204@cytetech.com>	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>	<4BCCBF71.8060508@cox.net>	<238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>	<4BCDD419.6050104@cytetech.com>	<k2v31f2a7bd1004201006qa705eb70q19ad0aec0271b87f@mail.gmail.com>
	<n2j1901f69e1004201045m1f50ee82maf89a975fef9b558@mail.gmail.com>
Message-ID: <4BCDFF73.8060406@redhat.com>

Was it not established that using a simple Field.set() is as effective as 
unsafe.putObjectVolatile() in the case of final fields?

On 04/20/2010 12:45 PM, Michael Spiegel wrote:
> I use unsafe.putObjectVolatile() in my implementation of
> ConcurrentSkipTreeSet that is backed by an underlying
> ConcurrentSkipTreeMap.  It is identical to the trick that Doug uses in
> j.u.c.ConcurrentSkipListSet.
>
> On Tue, Apr 20, 2010 at 1:06 PM, Joe Bowbeer<joe.bowbeer at gmail.com>  wrote:
>> I think it would be helpful to hear directly from more of the folks who use
>> the Unsafe stuff currently.
>>
>> Is there anyone (besides Doug:) who can pipe up?
>>
>> On Tue, Apr 20, 2010 at 9:19 AM, Gregg Wonderly wrote:
>>>
>>> Boehm, Hans wrote:
>>>>>
>>>>> > From my perspective, it's important that
>>>>
>>>> a) There is a usable subset of the language for which things are simple.
>>>> I think that currently consists of programs that (1) contain no data races,
>>>> and (2) avoid a certain set of library calls that include lazySet and a
>>>> handful of others.  Java guarantees sequential consistency for this subset,
>>>> and the visibility issues don't arise.
>>>
>>> Data races seem to often appear as people try to make use of more "cores"
>>> by creating threading and associated localized processing which can result
>>> in publication of values between threads which then has to be mediated for
>>> visibility guarantees.  This is the specific problem that I seem to read
>>> about and see people having problems with over and over.
>>>
>>>> b) There is an easy way to tell when you are leaving this subset.
>>>
>>>> This is currently getting a bit ugly.
>>>
>>> When there is more than one thread running, you have this problem.  For
>>> Java client programs, you have the EDT and the applications main thread that
>>> are separate.  If you then drive everything from EDT event dispatches, that
>>> can work for applications without networking or other background activity.
>>>   As soon as you add another thread, the possibilities of data races soar.
>>>
>>>> For C++0x, the distinction (b) was made by requiring an explicit
>>>
>>>> memory_order_X argument when you want to leave this subset.  I'm not
>>>> sure
>>>>
>>>> what the best way is to make this distinction in Java.  As Doug points
>>>> out,
>>>> the pre-existence of lazySet and weakCompareAndSet complicate matters.
>>>> Possibly so does the desire to deal with array elements and the like; I'm
>>>> not sure.
>>>
>>> Only publication of arrays creates the problem in a multi-threaded
>>> environment.  A single thread has no problems with an array.  Multiple
>>> threads are going to be a given in any Java application it seems to me, so
>>> I'm not sure how any Java application can have sequential consistency
>>> without some degree of "synchronized" or "volatile" for any mutable values.
>>>
>>> It is all of the overhead and problems associated with the "delay" through
>>> such "operations" that we are talking about addressing I thought.
>>>
>>>> It seems to me that either new classes or a C++0x-like approach would
>>>> require
>>>
>>>> at least deprecation of lazySet and weakCompareAndSet.  That's likely to
>>>> be
>>>> to be a tough sell.
>>>
>>> Correctness of software is an important detail.  Being able to somehow
>>> understand and prove this is vital as we decide more and more on making
>>> software systems a part of things we depend on for continuous duty.
>>>
>>> Gregg Wonderly
>>>
>>>>> From: Gregg Wonderly
>>>>
>>>>> What I thinking about, is the fact that I see the read vs write accesses
>>>>> as a range of possibilities.  If you order them from weaker on the outside
>>>>> to stronger on the inside as something like
>>>>>
>>>>>         non-volatile ->  relaxed ->  volatile:read
>>>>>
>>>>> and
>>>>>
>>>>>         write:volatile<- relaxed<- non-volatile
>>>>>
>>>>> then you can imagine a "window" where particular guarantees can hold.
>>>>>   Outside of that window, things are weaker.  For any particular application,
>>>>> I am thinking that this window is never completely open.
>>>>>
>>>> I'm not quite sure what you mean by "window" here.  One of the problems
>>>> with the weaker ordering guarantees is that we don't seem to have a good
>>>> handle on using weakly ordered operations (or data races on ordinary
>>>> variables) in a way that is only locally visible.  Using weakly ordered
>>>> operations anywhere seems to involve some danger of "contaminating" the
>>>> whole program.  There do seem to be a few important idioms, like
>>>> double-checked locking, that do localize the damage caused by weakly-ordered
>>>> operations, but I don't know how to reason about that in general.
>>>>
>>>> If I had to pick three possible ordering guarantees for atomic variables,
>>>> I would add some flavor of acquire-release ordering, and keep only one of
>>>> "non-volatile" and "relaxed".
>>>>
>>>> Hans
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
- DML ?

From dl at cs.oswego.edu  Wed Apr 21 07:34:44 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 21 Apr 2010 07:34:44 -0400
Subject: [concurrency-interest] Extended access methods for Atomics (and
 AQS)
In-Reply-To: <k2v31f2a7bd1004201006qa705eb70q19ad0aec0271b87f@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>	<4BC89EA9.1020204@cytetech.com>	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>	<4BCCBF71.8060508@cox.net>	<238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>	<4BCDD419.6050104@cytetech.com>
	<k2v31f2a7bd1004201006qa705eb70q19ad0aec0271b87f@mail.gmail.com>
Message-ID: <4BCEE2D4.7020907@cs.oswego.edu>

On 04/20/10 13:06, Joe Bowbeer wrote:
> I think it would be helpful to hear directly from more of the folks who
> use the Unsafe stuff currently.
>

You might not hear from too many of them on this list.
If you peruse google code search looking for usages,
you find (only) a dozen or so of them, including for example
Cliff Click's alternative forms of concurrent hash tables.
(Cliff's not on this list.)
Plus, there are those (as represented by the posting from
Benedict last week) who are frustrated not being able to use
efficient atomics or ordering control as they now stand.

Extrapolating, this might mean that there are
maybe a hundred or so total targeted usages, which is what
I meant by "small community". But this small community
includes many of those trying to create new efficient
concurrent components.

-Doug




From martinrb at google.com  Thu Apr 22 03:09:29 2010
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 22 Apr 2010 00:09:29 -0700
Subject: [concurrency-interest] Extended access methods for Atomics (and
	AQS)
In-Reply-To: <4BCEE2D4.7020907@cs.oswego.edu>
References: <NFBBKALFDCPFIDBNKAPCCEOHIFAA.davidcholmes@aapt.net.au>
	<4BC89EA9.1020204@cytetech.com>
	<238A96A773B3934685A7269CC8A8D0426DF5A1D0F3@GVW0436EXB.americas.hpqcorp.net>
	<4BCCBF71.8060508@cox.net>
	<238A96A773B3934685A7269CC8A8D0426DF5AA45D8@GVW0436EXB.americas.hpqcorp.net>
	<4BCDD419.6050104@cytetech.com>
	<k2v31f2a7bd1004201006qa705eb70q19ad0aec0271b87f@mail.gmail.com>
	<4BCEE2D4.7020907@cs.oswego.edu>
Message-ID: <r2y1ccfd1c11004220009k4dd8484dx24121925e23e37c1@mail.gmail.com>

As we've seen in threads elsewhere,
the documentation of the existing methods
in j.u.c.atomic can be improved with
regards to security and memory model effects,
and to provide better usage guidance.

Given the history, I would think it best to
add any new methods to the existing classes,
but to provide the better guidance and more
off-putting names.  We really can say in the
javadoc:

<b>This method is for use by experts only.</b>

People scratch their heads about lazySet
http://stackoverflow.com/questions/1468007/atomicinteger-lazyset-and-set
Doug has already written some excellent doc
that just happens to be in a place few people will look.

I also think that we could remove most of the
performance overhead of Updater classes
relative to Unsafe, given some heroic
engineering effort.  Hotspot could intrinsify
a call to newUpdater and return an instance
of a synthetic specialized class that did the
type checks in the signature of the methods.
Not that I'm volunteering.

From gkorland at gmail.com  Thu Apr 22 13:56:56 2010
From: gkorland at gmail.com (Guy Korland)
Date: Thu, 22 Apr 2010 20:56:56 +0300
Subject: [concurrency-interest] LinkedTransferQueue port to JDK5
Message-ID: <v2k79be5fa31004221056l29549e23h7e46ce1c79c268c6@mail.gmail.com>

I'm trying to port LinkedTransferQueue to JDK5 but there are two lines
that are using methods that are only available since JDK6 (lines 661 & 665).

Are those lines necessary? Can't I replace them with the JDK5 version?
Meaning without the "this"


      658             else if (timed) {

      659                 long now = System.nanoTime();

      660                 if ((nanos -= now - lastTime) > 0)

*      661                     LockSupport.parkNanos(this, nanos);*

      662                 lastTime = now;

      663             }

      664             else {

*      665                 LockSupport.park(this);*

      666                 s.waiter = null;

      667                 spins = -1;                   // spin if
front upon wakeup

      668             }

Thanks,
Guy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100422/ed84e3be/attachment.html>

From davidcholmes at aapt.net.au  Thu Apr 22 15:59:49 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 23 Apr 2010 05:59:49 +1000
Subject: [concurrency-interest] LinkedTransferQueue port to JDK5
In-Reply-To: <v2k79be5fa31004221056l29549e23h7e46ce1c79c268c6@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEALIGAA.davidcholmes@aapt.net.au>

Hi Guy,

I believe that is correct. The "this" is only used to keep track of
AbstractOwnableSynchronizers so that they can be reported in thread dumps.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Guy Korland
  Sent: Friday, 23 April 2010 3:57 AM
  To: concurrency-interest
  Subject: [concurrency-interest] LinkedTransferQueue port to JDK5


I'm trying to port LinkedTransferQueue to JDK5 but there are two linesthat
are using methods that are only available since JDK6 (lines 661 & 665).

Are those lines necessary? Can't I replace them with the JDK5
version?Meaning without the "this"      658             else if (timed)
{      659                 long now = System.nanoTime();
      660                 if ((nanos -= now - lastTime) > 0)      661
LockSupport.parkNanos(this, nanos);      662                 lastTime = now;
663             }      664             else {
      665                 LockSupport.park(this);      666
s.waiter = null;

      667                 spins = -1;                   // spin if front
upon wakeup
      668             }Thanks,Guy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100423/89c914f0/attachment.html>

From kramasundar at yahoo.com  Fri Apr 23 06:48:48 2010
From: kramasundar at yahoo.com (Ramsundar Kandasamy)
Date: Fri, 23 Apr 2010 03:48:48 -0700 (PDT)
Subject: [concurrency-interest] Graphical representation of concurrent
	activities
Message-ID: <426866.12250.qm@web113201.mail.gq1.yahoo.com>


Hi Guys,

somewhere in the past I saw a link (posted in this mailing-list) from where i downloaded a jar file(?) that showed me some thread concepts visually/graphically.

Unfortunately i lost it somewhere. If anyone still has the link bookmarked, could you please forward that to me?

Thanks,
Ramsundar Kandasamy

kramasundar.blogspot.com?? ?? 



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100423/b9bdc893/attachment.html>

From joe.bowbeer at gmail.com  Fri Apr 23 07:05:18 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 23 Apr 2010 04:05:18 -0700
Subject: [concurrency-interest] Graphical representation of concurrent
	activities
In-Reply-To: <426866.12250.qm@web113201.mail.gq1.yahoo.com>
References: <426866.12250.qm@web113201.mail.gq1.yahoo.com>
Message-ID: <n2p31f2a7bd1004230405l3272f0dajb0ab4a2f2846bf1e@mail.gmail.com>

Victor Grazi sent this link last June:

http://sourceforge.net/projects/javaconcurrenta/

On Fri, Apr 23, 2010 at 3:48 AM, Ramsundar Kandasamy wrote:

>
> Hi Guys,
>
> somewhere in the past I saw a link (posted in this mailing-list) from where
> i downloaded a jar file(?) that showed me some thread concepts
> visually/graphically.
>
> Unfortunately i lost it somewhere. If anyone still has the link bookmarked,
> could you please forward that to me?
>
> Thanks,
> Ramsundar Kandasamy
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100423/00f328dc/attachment.html>

From blackdrag at gmx.org  Fri Apr 23 08:36:11 2010
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Fri, 23 Apr 2010 14:36:11 +0200
Subject: [concurrency-interest] when is safe publication safe?
Message-ID: <4BD1943B.50407@gmx.org>

hi all,

assuming I have Thread t1, that creates an object x and assigns it for 
example to a field and Thread t2 is reading that field. Am I now right 
in assuming that unless I have memory barriers in t1 and t2 I have no 
way to ensure that t2 will see a fully initialized object (x represents 
a mutable)? Am I right that for example AtomicReference#lazySet will not 
help me here? I am aware that this question goes in the direction of 
double checked locking (only no instantaneous visibility and no 
singleton), that is why I assume that it cannot be guaranteed. 
Especially since there will be no "happens-before" relation in t2.

Assuming that I am right I have further questions... For example why was 
it decided that an object can be seen uninitialized by another thread? I 
mean, sure I am aware of reordering of instructions, but does it give 
really that much to make an object visible to another thread without 
having it initialized?

And finally I did read here about Fences. Would they help making my safe 
publication possible?

bye blackdrag

-- 
Jochen "blackdrag" Theodorou
The Groovy Project Tech Lead (http://groovy.codehaus.org)
http://blackdragsview.blogspot.com/


From gdenys at yahoo.com  Fri Apr 23 09:26:03 2010
From: gdenys at yahoo.com (Denys Geert)
Date: Fri, 23 Apr 2010 06:26:03 -0700 (PDT)
Subject: [concurrency-interest] ForkJoin for Java5
Message-ID: <37318.33669.qm@web51404.mail.re2.yahoo.com>

Hi,

As far as I remember, there is no Java5 backport of FJ, because the performance degradation would be significant. I was wondering if anybody could quantify this. 

We may be looking to backport FJ to Java5 to support a few customers that insist on Java5 for the time being. Having some reduced performance is acceptable, but it would be good to know what we can expect before taking on the backport.

If anybody has experience with this or can venture an educated guess, that would be very helpful. Also, what are the major dependencies on Java 6?

Thanks,
Geert.



      

From dl at cs.oswego.edu  Fri Apr 23 10:32:07 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 23 Apr 2010 10:32:07 -0400
Subject: [concurrency-interest] ForkJoin for Java5
In-Reply-To: <37318.33669.qm@web51404.mail.re2.yahoo.com>
References: <37318.33669.qm@web51404.mail.re2.yahoo.com>
Message-ID: <4BD1AF67.8090805@cs.oswego.edu>

On 04/23/10 09:26, Denys Geert wrote:
> Hi,
>
> As far as I remember, there is no Java5 backport of FJ, because the
> performance degradation would be significant. I was wondering if anybody
> could quantify this.
>
> We may be looking to backport FJ to Java5 to support a few customers that
> insist on Java5 for the time being. Having some reduced performance is
> acceptable, but it would be good to know what we can expect before taking on
> the backport.
>
> If anybody has experience with this or can venture an educated guess, that
> would be very helpful. Also, what are the major dependencies on Java 6?
>

It depends on the machine, but a quick check across ones
I have around here shows slowdown averaging around 30%,
generally worse for machines with more processors.

The main impact is the loss of lazySet (actually, its Unsafe mapping)
which you need to replace with plain volatile writes.

-Doug

From alarmnummer at gmail.com  Fri Apr 23 18:10:56 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sat, 24 Apr 2010 00:10:56 +0200
Subject: [concurrency-interest] High performance clock/counter
Message-ID: <q2h1466c1d61004231510z31978fcchdfa0e247b665f3e@mail.gmail.com>

High All,

I'm looking for a high performance counter to be used inside an STM.

Atm I use an AtomicLong, but if the number threads increase, the
performance drops, example:

Thread count: 1
Update transactions/second: 74,149,986.633
Update transactions/second: 74,149,986.633 per core

Thread count: 2
Update transactions/second: 22,864,414.996
Update transactions/second: 11,432,207.498 per core

Thread count: 4
Update transactions/second: 21,958,871.971
Update transactions/second: 5,489,717.993 per core

Thread count: 8
Update transactions/second: 25,207,393.631
Update transactions/second: 3,150,924.204 per core

I have a machine with 8 cores.

I already use a 'relaxed' increment, it is allowed that concurrent
increments on the clock,
return the same value. So I'm using the following:

long writeVersion;
if (strict) {
    writeVersion = clock.incrementAndGet();
} else {
    long value = clock.get();
    writeVersion = value+1;
    clock.compareAndSet(value, writeVersion);
}

The performance numbers above are creating using the strict = false.

I was also playing with the Counter of the high scale lib from Cliff Click,
and the increment is amazingly fast (it gets faster the more cores I
throw at it). But
the big problem is that the increment doesn't return a value, and I need this
to determine the writeversion of the transaction. If I do a
counter.get after the
increment, the performance drops far under the AtomicLong with the
relaxed increment.

So does someone know about a library/approach to solve this situation?

From davidcholmes at aapt.net.au  Fri Apr 23 18:32:31 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 24 Apr 2010 08:32:31 +1000
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <4BD1943B.50407@gmx.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>

> Jochen Theodorou writes:
> assuming I have Thread t1, that creates an object x and assigns it for
> example to a field and Thread t2 is reading that field. Am I now right
> in assuming that unless I have memory barriers in t1 and t2 I have no
> way to ensure that t2 will see a fully initialized object (x represents
> a mutable)? Am I right that for example AtomicReference#lazySet will not
> help me here? I am aware that this question goes in the direction of
> double checked locking (only no instantaneous visibility and no
> singleton), that is why I assume that it cannot be guaranteed.
> Especially since there will be no "happens-before" relation in t2.

The key point is that for safe-publication there must be a happens-before
ordering between the read of x and the construction of the object that x
refers to. As with DCL there's no solution that only does something special
on the writer-side - so lazySet, or non-lazy-set won't help if there's no
AtomicReference.get (and I think in that case lazySet won't help anyway).

> Assuming that I am right I have further questions... For example why was
> it decided that an object can be seen uninitialized by another thread? I
> mean, sure I am aware of reordering of instructions, but does it give
> really that much to make an object visible to another thread without
> having it initialized?

It can't be seen uninitialized, but it can be seen in a state anywhere
between default-initialized and fully-constructed.

To make every object safe for publication would require additional
"barriers" that would severely impact performance - given that the vast
majority of objects created are not intended to be shared.

> And finally I did read here about Fences. Would they help making my safe
> publication possible?

Probably/possibly ... but that's the thing about Fences, trying to figure
out exactly what they do for you. ;-) Someone else may have a more helpful
answer on Fences.

Cheers,
David Holmes

> bye blackdrag
>
> --
> Jochen "blackdrag" Theodorou
> The Groovy Project Tech Lead (http://groovy.codehaus.org)
> http://blackdragsview.blogspot.com/
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From alarmnummer at gmail.com  Fri Apr 23 18:54:33 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sat, 24 Apr 2010 00:54:33 +0200
Subject: [concurrency-interest] High performance clock/counter
In-Reply-To: <q2h1466c1d61004231510z31978fcchdfa0e247b665f3e@mail.gmail.com>
References: <q2h1466c1d61004231510z31978fcchdfa0e247b665f3e@mail.gmail.com>
Message-ID: <t2r1466c1d61004231554v26e333a3lc1fd463560c9376b@mail.gmail.com>

I have been experimenting with the Counter of Cliff (the
ConcurrentAutoTable) and changed
the following:

 /** {@link #add} with +1 */
  public void increment()   { add_if_mask( 1L,0); }

to:

 /** {@link #add} with +1 */
  public long increment()   { return add_if_mask( 1L,0)+1; }

I do get good increments but once and a while this function returns a
value smaller than the previous version.  So apparently this is not
the way to go.


On Sat, Apr 24, 2010 at 12:10 AM, Peter Veentjer <alarmnummer at gmail.com> wrote:
> High All,
>
> I'm looking for a high performance counter to be used inside an STM.
>
> Atm I use an AtomicLong, but if the number threads increase, the
> performance drops, example:
>
> Thread count: 1
> Update transactions/second: 74,149,986.633
> Update transactions/second: 74,149,986.633 per core
>
> Thread count: 2
> Update transactions/second: 22,864,414.996
> Update transactions/second: 11,432,207.498 per core
>
> Thread count: 4
> Update transactions/second: 21,958,871.971
> Update transactions/second: 5,489,717.993 per core
>
> Thread count: 8
> Update transactions/second: 25,207,393.631
> Update transactions/second: 3,150,924.204 per core
>
> I have a machine with 8 cores.
>
> I already use a 'relaxed' increment, it is allowed that concurrent
> increments on the clock,
> return the same value. So I'm using the following:
>
> long writeVersion;
> if (strict) {
> ? ?writeVersion = clock.incrementAndGet();
> } else {
> ? ?long value = clock.get();
> ? ?writeVersion = value+1;
> ? ?clock.compareAndSet(value, writeVersion);
> }
>
> The performance numbers above are creating using the strict = false.
>
> I was also playing with the Counter of the high scale lib from Cliff Click,
> and the increment is amazingly fast (it gets faster the more cores I
> throw at it). But
> the big problem is that the increment doesn't return a value, and I need this
> to determine the writeversion of the transaction. If I do a
> counter.get after the
> increment, the performance drops far under the AtomicLong with the
> relaxed increment.
>
> So does someone know about a library/approach to solve this situation?
>


From jdmarshall at gmail.com  Fri Apr 23 20:51:28 2010
From: jdmarshall at gmail.com (jason marshall)
Date: Fri, 23 Apr 2010 17:51:28 -0700
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
References: <4BD1943B.50407@gmx.org>
	<NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
Message-ID: <p2n3cf41bb91004231751vdab82859r74d40c5ed29c1c2c@mail.gmail.com>

Some people insist on an intermediate state between published and
uninitialized where collaboration between or within modules is possible.
It's not so clear to me how exactly within Java semantics one would
articulate this twilight state adequately.

Unfortunately the only examples I can think of off the top of my head
involve pooling of resources.  I'm not too keen on using caching as a
defense for any language feature, and any language features that encourage
other solutions to amortizing the cost of a calculation over multiple uses,
I'm energetically in favor of (such as linear programming techniques).

I'm sure there's something in the Publish-Subscribe world that illustrates
this need, especially where composition happens (your PubSub object itself
relies on PubSub behavior internally).  Composition seems to be the achilles
heel of a lot of very interesting programming idioms.  (STM, I'm looking at
you, with no small sense of irony)

-Jason




On Fri, Apr 23, 2010 at 3:32 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> > Jochen Theodorou writes:
> > assuming I have Thread t1, that creates an object x and assigns it for
> > example to a field and Thread t2 is reading that field. Am I now right
> > in assuming that unless I have memory barriers in t1 and t2 I have no
> > way to ensure that t2 will see a fully initialized object (x represents
> > a mutable)? Am I right that for example AtomicReference#lazySet will not
> > help me here? I am aware that this question goes in the direction of
> > double checked locking (only no instantaneous visibility and no
> > singleton), that is why I assume that it cannot be guaranteed.
> > Especially since there will be no "happens-before" relation in t2.
>
> The key point is that for safe-publication there must be a happens-before
> ordering between the read of x and the construction of the object that x
> refers to. As with DCL there's no solution that only does something special
> on the writer-side - so lazySet, or non-lazy-set won't help if there's no
> AtomicReference.get (and I think in that case lazySet won't help anyway).
>
> > Assuming that I am right I have further questions... For example why was
> > it decided that an object can be seen uninitialized by another thread? I
> > mean, sure I am aware of reordering of instructions, but does it give
> > really that much to make an object visible to another thread without
> > having it initialized?
>
> It can't be seen uninitialized, but it can be seen in a state anywhere
> between default-initialized and fully-constructed.
>
> To make every object safe for publication would require additional
> "barriers" that would severely impact performance - given that the vast
> majority of objects created are not intended to be shared.
>
> > And finally I did read here about Fences. Would they help making my safe
> > publication possible?
>
> Probably/possibly ... but that's the thing about Fences, trying to figure
> out exactly what they do for you. ;-) Someone else may have a more helpful
> answer on Fences.
>
> Cheers,
> David Holmes
>
> > bye blackdrag
> >
> > --
> > Jochen "blackdrag" Theodorou
> > The Groovy Project Tech Lead (http://groovy.codehaus.org)
> > http://blackdragsview.blogspot.com/
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
- Jason
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100423/d232856f/attachment.html>

From dl at cs.oswego.edu  Sat Apr 24 07:21:05 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 24 Apr 2010 07:21:05 -0400
Subject: [concurrency-interest] High performance clock/counter
In-Reply-To: <q2h1466c1d61004231510z31978fcchdfa0e247b665f3e@mail.gmail.com>
References: <q2h1466c1d61004231510z31978fcchdfa0e247b665f3e@mail.gmail.com>
Message-ID: <4BD2D421.6000205@cs.oswego.edu>

On 04/23/10 18:10, Peter Veentjer wrote:

> I'm looking for a high performance counter to be used inside an STM.
>

We've considered putting some counter-based classes
in j.u.c., but supporting the range of needs and tradeoffs
doesn't seem amenable to a small set of solutions.
Among the possibilities:

* If you are willing to live with approximate results
on reads, then striping can work well (as in Cliff Click's
high-scale lib -- http://sourceforge.net/projects/high-scale-lib/)

* For accurate reads, See Herlihy & Shavit's book for discussion
of tree-based  counters and counting networks, that entail a fair
amount of overhead.

* If you only need to detect special properties/values
(non-zero in particular) see SNZI (Scalable non-zero indicators)
by Mark Moir's group -- http://research.sun.com/scalable/pubs/index.html,
that was developed mainly for STM support.

* For some problems involving contended counters, the
best solutions often entail reducing contention pressure
by engaging in some domain-specific alternative action
when an attempted CAS on a simple variable fails.

-Doug

From blackdrag at gmx.org  Sat Apr 24 08:20:48 2010
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Sat, 24 Apr 2010 14:20:48 +0200
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
Message-ID: <4BD2E220.8080905@gmx.org>

David Holmes wrote:
>> Jochen Theodorou writes:
>> assuming I have Thread t1, that creates an object x and assigns it for
>> example to a field and Thread t2 is reading that field. Am I now right
>> in assuming that unless I have memory barriers in t1 and t2 I have no
>> way to ensure that t2 will see a fully initialized object (x represents
>> a mutable)? Am I right that for example AtomicReference#lazySet will not
>> help me here? I am aware that this question goes in the direction of
>> double checked locking (only no instantaneous visibility and no
>> singleton), that is why I assume that it cannot be guaranteed.
>> Especially since there will be no "happens-before" relation in t2.
> 
> The key point is that for safe-publication there must be a happens-before
> ordering between the read of x and the construction of the object that x
> refers to. As with DCL there's no solution that only does something special
> on the writer-side - so lazySet, or non-lazy-set won't help if there's no
> AtomicReference.get (and I think in that case lazySet won't help anyway).

ok, so I got more or less right. Good to know.


>> Assuming that I am right I have further questions... For example why was
>> it decided that an object can be seen uninitialized by another thread? I
>> mean, sure I am aware of reordering of instructions, but does it give
>> really that much to make an object visible to another thread without
>> having it initialized?
> 
> It can't be seen uninitialized, but it can be seen in a state anywhere
> between default-initialized and fully-constructed.

yeah, sorry, I did mean that of course..

> To make every object safe for publication would require additional
> "barriers" that would severely impact performance - given that the vast
> majority of objects created are not intended to be shared.

I guess I have here the problem in understanding... is such a barrier 
the only way to avoid reordering? I don't mean in Java, I mean on the 
CPUs that Java targets. I was assuming, without knowing, that there are 
instruction I can give the CPU that avoid reordering of for example the 
next y instructions. Or somehow to mark an area that cannot be reordered.

thnaks for the answer David

bye blackdrag


-- 
Jochen "blackdrag" Theodorou
The Groovy Project Tech Lead (http://groovy.codehaus.org)
http://blackdragsview.blogspot.com/


From blackdrag at gmx.org  Sat Apr 24 08:39:51 2010
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Sat, 24 Apr 2010 14:39:51 +0200
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
Message-ID: <4BD2E697.3020305@gmx.org>

David Holmes wrote:
[...]
>> And finally I did read here about Fences. Would they help making my safe
>> publication possible?
> 
> Probably/possibly ... but that's the thing about Fences, trying to figure
> out exactly what they do for you. ;-) Someone else may have a more helpful
> answer on Fences.

I should maybe mention what I want this for. In the Groovy programming 
language, which I have the pleasure to help develop we have a lot of 
runtime data, that is of course shared between threads. But the current 
ways of synchronization do cost a lot of performance and since 
performance of the language is an important point, we have to draw from 
all registers to make it somehow more fast. I made several experiments 
and always come to the conclusion this and that won't work if my 
understanding of the Java Memory Model is right. I come to the 
conclusion that if it were possible to create an object in Thread t1 and 
"write that main memory", so that Thread t2 will eventually see the 
fully initialized object and some point in the future without having to 
go through memory barriers, then I will get more speed out of the 
language. The problem is that I have mutable objects I need to publish, 
so the point of being fully initialized is important to me. That t2 may 
not see the object unless some memory barrier is used is ok for me too. 
If the Groovy programmer wants it being visible, he can ensure with 
memory barriers it is going to be so.

Currently Groovy is unable to make a simple method invocation without 
passing several memory barriers. And since it is for each method 
invocation you can maybe imagine how bad this can turn out.

My current take is that with what Java (API and language) offers, it is 
impossible. So I was kind of hoping that either someone could tell me 
how to make it right or if some people are working on something to make 
  it possible.

If it is not on the scope of anyone I will have to think if I can use 
native code to achieve that.

bye blackdrag

-- 
Jochen "blackdrag" Theodorou
The Groovy Project Tech Lead (http://groovy.codehaus.org)
http://blackdragsview.blogspot.com/


From dl at cs.oswego.edu  Sat Apr 24 09:18:03 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 24 Apr 2010 09:18:03 -0400
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <4BD2E697.3020305@gmx.org>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
	<4BD2E697.3020305@gmx.org>
Message-ID: <4BD2EF8B.603@cs.oswego.edu>

On 04/24/10 08:39, Jochen Theodorou wrote:

>
> I should maybe mention what I want this for. In the Groovy programming
> language, which I have the pleasure to help develop we have a lot of
> runtime data, that is of course shared between threads. But the current
> ways of synchronization do cost a lot of performance and since
> performance of the language is an important point, we have to draw from
> all registers to make it somehow more fast. I made several experiments
> and always come to the conclusion this and that won't work if my
> understanding of the Java Memory Model is right. I come to the
> conclusion that if it were possible to create an object in Thread t1 and
> "write that main memory", so that Thread t2 will eventually see the
> fully initialized object and some point in the future without having to
> go through memory barriers, then I will get more speed out of the
> language.

Yes, this was the main motivation for creating the (messy!)
rules for final fields in the JMM, that only conditionally
preserve consistency (i.e., you get what you expect only if
no leaks of "this" in constructors etc).
Unfortunately, your fields aren't always final. This is not an
uncommon problem. It is the first example in the draft Fences API
javadoc
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html
It would be nice to put into place some means to cope.
However, the Fences API itself met a fair amount
of resistance so we are still not sure where this will go.

> The problem is that I have mutable objects I need to publish,
> so the point of being fully initialized is important to me. That t2 may
> not see the object unless some memory barrier is used is ok for me too.
> If the Groovy programmer wants it being visible, he can ensure with
> memory barriers it is going to be so.

As a first step, consider exactly what effects/semantics you want
here, and the ways you intend people to be able to write conditionally
correct Groovy code.  Is there a well defined publication-ready state
such that any thread seeing it is either helping to initialize it
(with no guarantees about what values are read) or already incorrect
on some other grounds? If so, forcing release-style fences on
each field write may suffice. Except that we don't support a
reasonable way to do this from Java level yet.

-Doug






From blackdrag at gmx.org  Sun Apr 25 05:31:28 2010
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Sun, 25 Apr 2010 11:31:28 +0200
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <4BD2EF8B.603@cs.oswego.edu>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>	<4BD2E697.3020305@gmx.org>
	<4BD2EF8B.603@cs.oswego.edu>
Message-ID: <4BD40BF0.9070509@gmx.org>

Doug Lea wrote:
[...]
> Yes, this was the main motivation for creating the (messy!)
> rules for final fields in the JMM, that only conditionally
> preserve consistency (i.e., you get what you expect only if
> no leaks of "this" in constructors etc).

better some messy rules than not at all in this case.

> Unfortunately, your fields aren't always final. This is not an
> uncommon problem. It is the first example in the draft Fences API
> javadoc
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html 
> 
> It would be nice to put into place some means to cope.
> However, the Fences API itself met a fair amount
> of resistance so we are still not sure where this will go.

your example was more or less the reason I started writing here. And we 
people from JSR 292 are absolutely interested in such things I would 
say. But our focus is not so much if the user can do bad things with 
that, our focus is to somehow fight the limitations of the JVM. Because 
many of these limitations do make dynamic language on the JVM 
unnecessarily slow and complex. A normal user is maybe ok if 1 in a 
10000 method calls is a bit slower, but in my case it is each of the 
10000 method calls, that is much slower. Every little bit extra I have 
to pay makes it slower.

>> The problem is that I have mutable objects I need to publish,
>> so the point of being fully initialized is important to me. That t2 may
>> not see the object unless some memory barrier is used is ok for me too.
>> If the Groovy programmer wants it being visible, he can ensure with
>> memory barriers it is going to be so.
> 
> As a first step, consider exactly what effects/semantics you want
> here, and the ways you intend people to be able to write conditionally
> correct Groovy code. 

People wouldn't have to write conditionally correct Groovy code. they 
would write normal code as they would in Java (Groovy and Java are very 
near). Imagine Groovy as a kind of library that is involved in each 
method call, so the effects/semantics are solely reduced to the library 
itself and that user code may cause a more early publication due to a 
piggyback ride on memory barriers the user used.

> Is there a well defined publication-ready state
> such that any thread seeing it is either helping to initialize it
> (with no guarantees about what values are read) or already incorrect
> on some other grounds?

the initialization is done by the thread first needing that object. 
There is further normal synchronization if that object is not there yet. 
Which means If threads t1 and t2 request the object then for example t1 
does the init, and t2 will have to wait. I was considering changing that 
into t1 and t2 doing their own versions and who ever writes last, gets 
the real version. But that would be for me a next step and I am not yet 
sure how to do that yet. I would have to handle different versions of 
the object for a write, which makes it much more difficult, because the 
current semantics would maybe not fully allow this.

> If so, forcing release-style fences on
> each field write may suffice. Except that we don't support a
> reasonable way to do this from Java level yet.

from the Java level sounds like there would be some way...

bye blackdrag

-- 
Jochen "blackdrag" Theodorou
The Groovy Project Tech Lead (http://groovy.codehaus.org)
http://blackdragsview.blogspot.com/


From dl at cs.oswego.edu  Sun Apr 25 08:44:32 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 25 Apr 2010 08:44:32 -0400
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <4BD40BF0.9070509@gmx.org>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>	<4BD2E697.3020305@gmx.org>	<4BD2EF8B.603@cs.oswego.edu>
	<4BD40BF0.9070509@gmx.org>
Message-ID: <4BD43930.5090003@cs.oswego.edu>

On 04/25/10 05:31, Jochen Theodorou wrote:
>> As a first step, consider exactly what effects/semantics you want
>> here, and the ways you intend people to be able to write conditionally
>> correct Groovy code.
>
> People wouldn't have to write conditionally correct Groovy code. they
> would write normal code as they would in Java (Groovy and Java are very
> near).

It seems implausible that you could do enough
analysis at load/run time to determine whether you need
full locking in the presence of multithreaded racy initialization
vs much cheaper release fences. This would require at least some
high-quality escape analysis. And the code generated
would differ both for the writing and reading callers.

As I mentioned, an alternative is to lay down some rules.
If people stick to the rules they get consistent (in the sense
of data-race-free) executions, else they might not. And of
such rules, I think the ones that can apply here amount
to saying that other threads performing initializations cannot
trust any of their reads of the partially initialized object.
And further, they cannot leak refs to that object outside of the
group of initializer threads.

This is not hugely different than the Swing threading rules
(http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
but applies only during initialization.

-Doug




From blackdrag at gmx.org  Sun Apr 25 11:48:57 2010
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Sun, 25 Apr 2010 17:48:57 +0200
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <4BD43930.5090003@cs.oswego.edu>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>	<4BD2E697.3020305@gmx.org>	<4BD2EF8B.603@cs.oswego.edu>	<4BD40BF0.9070509@gmx.org>
	<4BD43930.5090003@cs.oswego.edu>
Message-ID: <4BD46469.7030502@gmx.org>

Doug Lea wrote:
> On 04/25/10 05:31, Jochen Theodorou wrote:
>>> As a first step, consider exactly what effects/semantics you want
>>> here, and the ways you intend people to be able to write conditionally
>>> correct Groovy code.
>>
>> People wouldn't have to write conditionally correct Groovy code. they
>> would write normal code as they would in Java (Groovy and Java are very
>> near).
> 
> It seems implausible that you could do enough
> analysis at load/run time to determine whether you need
> full locking in the presence of multithreaded racy initialization
> vs much cheaper release fences. This would require at least some
> high-quality escape analysis. And the code generated
> would differ both for the writing and reading callers.

maybe I did explain it not good. Let us assume I have the Groovy code:

1+1

Then this is really something along the lines of:

SBA.getMetaClassOf(1).invoke("plus",1)

and SBA.getMetaClassOf(1) would return the meta class of Integer. Since 
this is purely a runtime construct, it does not exist until the first 
time this meta class is requested. So getMetaClassOf would be the place 
to initialize the meta class, that would register it in a global 
structure and on subsequent invocation use that cached meta class. If 
two threads execute the code above, then one would do the 
initialization, while the other has to wait. The waiting thread would 
then read the initialized global meta class. On subsequent invocations 
both threads would just read. Since changes of the meta class are rare, 
we would in 99% of all cases simply read the existing value. Since we 
have to be memory aware, these meta class can be unloaded at runtime 
too. They are SoftReferenced so it is done only if really needed. But 
rather than the normal change a reinitialization might be needed much 
more often.

As you see the user code "1+1" does contain zero synchronization code. 
The memory barriers are all in the runtime. It is not that this cannot 
be solved by using what Java already has, it is that this is too expensive.

> As I mentioned, an alternative is to lay down some rules.
> If people stick to the rules they get consistent (in the sense
> of data-race-free) executions, else they might not. And of
> such rules, I think the ones that can apply here amount
> to saying that other threads performing initializations cannot
> trust any of their reads of the partially initialized object.
> And further, they cannot leak refs to that object outside of the
> group of initializer threads.
> 
> This is not hugely different than the Swing threading rules
> (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
> but applies only during initialization.

but unlike what the above may suggest there is no single initialization 
phase. The meta classes are created on demand. We cannot know beforehand 
which meta classes are needed and doing them all before starting would 
increase the startup time big times.

If there were of course a way to recognize a partially initialized 
object I could maybe think of something... but is there a reliable one?

bye blackdrag

-- 
Jochen "blackdrag" Theodorou
The Groovy Project Tech Lead (http://groovy.codehaus.org)
http://blackdragsview.blogspot.com/


From aph at redhat.com  Sun Apr 25 12:54:43 2010
From: aph at redhat.com (Andrew Haley)
Date: Sun, 25 Apr 2010 17:54:43 +0100
Subject: [concurrency-interest] High performance clock/counter
In-Reply-To: <4BD2D421.6000205@cs.oswego.edu>
References: <q2h1466c1d61004231510z31978fcchdfa0e247b665f3e@mail.gmail.com>
	<4BD2D421.6000205@cs.oswego.edu>
Message-ID: <4BD473D3.3010408@redhat.com>

On 04/24/2010 12:21 PM, Doug Lea wrote:
> On 04/23/10 18:10, Peter Veentjer wrote:
> 
>> I'm looking for a high performance counter to be used inside an STM.
>>
> 
> We've considered putting some counter-based classes
> in j.u.c., but supporting the range of needs and tradeoffs
> doesn't seem amenable to a small set of solutions.
> Among the possibilities:
> 
> * If you are willing to live with approximate results
> on reads, then striping can work well (as in Cliff Click's
> high-scale lib -- http://sourceforge.net/projects/high-scale-lib/)
> 
> * For accurate reads, See Herlihy & Shavit's book for discussion
> of tree-based  counters and counting networks, that entail a fair
> amount of overhead.
> 
> * If you only need to detect special properties/values
> (non-zero in particular) see SNZI (Scalable non-zero indicators)
> by Mark Moir's group -- http://research.sun.com/scalable/pubs/index.html,
> that was developed mainly for STM support.
> 
> * For some problems involving contended counters, the
> best solutions often entail reducing contention pressure
> by engaging in some domain-specific alternative action
> when an attempted CAS on a simple variable fails.

All of this reminds me of something I was meaning to ask.

None of the counters suitable for use in an implementation of an
algorithm like TL2 are very nice: you either end up with high
contention on a single counter variable or something rather
heavyweight.  Would it make sense for the hardware to provide global
high-performance counters?  I'm thinking of a counter that could be
handled very efficiently by some sort of low-latency broadcast
protocol.

Andrew.

From joe.bowbeer at gmail.com  Sun Apr 25 19:26:39 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 25 Apr 2010 16:26:39 -0700
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <4BD46469.7030502@gmx.org>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
	<4BD2E697.3020305@gmx.org> <4BD2EF8B.603@cs.oswego.edu>
	<4BD40BF0.9070509@gmx.org> <4BD43930.5090003@cs.oswego.edu>
	<4BD46469.7030502@gmx.org>
Message-ID: <q2h31f2a7bd1004251626t674ad6e9p4db719d8ece74160@mail.gmail.com>

Jochen,

What you are describing seems like a caching problem as much as it is about
safe publication and/or dynamic languages.  The language runtime creates the
(immutable) instances and publishes them to the cache, right?  The
performance of the cache is the hot spot?

So are you using something like MapMaker to implement the cache?

http://code.google.com/p/google-collections/

What are you using to hold off a t2 when t1 is in the process of publishing
to the cache?  Some scheme involving a Future?

Joe

On Sun, Apr 25, 2010 at 8:48 AM, Jochen Theodorou wrote:

> Doug Lea wrote:
>
>> On 04/25/10 05:31, Jochen Theodorou wrote:
>>
>>> As a first step, consider exactly what effects/semantics you want
>>>> here, and the ways you intend people to be able to write conditionally
>>>> correct Groovy code.
>>>>
>>>
>>> People wouldn't have to write conditionally correct Groovy code. they
>>> would write normal code as they would in Java (Groovy and Java are very
>>> near).
>>>
>>
>> It seems implausible that you could do enough
>> analysis at load/run time to determine whether you need
>> full locking in the presence of multithreaded racy initialization
>> vs much cheaper release fences. This would require at least some
>> high-quality escape analysis. And the code generated
>> would differ both for the writing and reading callers.
>>
>
> maybe I did explain it not good. Let us assume I have the Groovy code:
>
> 1+1
>
> Then this is really something along the lines of:
>
> SBA.getMetaClassOf(1).invoke("plus",1)
>
> and SBA.getMetaClassOf(1) would return the meta class of Integer. Since
> this is purely a runtime construct, it does not exist until the first time
> this meta class is requested. So getMetaClassOf would be the place to
> initialize the meta class, that would register it in a global structure and
> on subsequent invocation use that cached meta class. If two threads execute
> the code above, then one would do the initialization, while the other has to
> wait. The waiting thread would then read the initialized global meta class.
> On subsequent invocations both threads would just read. Since changes of the
> meta class are rare, we would in 99% of all cases simply read the existing
> value. Since we have to be memory aware, these meta class can be unloaded at
> runtime too. They are SoftReferenced so it is done only if really needed.
> But rather than the normal change a reinitialization might be needed much
> more often.
>
> As you see the user code "1+1" does contain zero synchronization code. The
> memory barriers are all in the runtime. It is not that this cannot be solved
> by using what Java already has, it is that this is too expensive.
>
>
>  As I mentioned, an alternative is to lay down some rules.
>> If people stick to the rules they get consistent (in the sense
>> of data-race-free) executions, else they might not. And of
>> such rules, I think the ones that can apply here amount
>> to saying that other threads performing initializations cannot
>> trust any of their reads of the partially initialized object.
>> And further, they cannot leak refs to that object outside of the
>> group of initializer threads.
>>
>> This is not hugely different than the Swing threading rules
>> (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
>> but applies only during initialization.
>>
>
> but unlike what the above may suggest there is no single initialization
> phase. The meta classes are created on demand. We cannot know beforehand
> which meta classes are needed and doing them all before starting would
> increase the startup time big times.
>
> If there were of course a way to recognize a partially initialized object I
> could maybe think of something... but is there a reliable one?
>
>
> bye blackdrag
>
> --
> Jochen "blackdrag" Theodorou
> The Groovy Project Tech Lead (http://groovy.codehaus.org)
> http://blackdragsview.blogspot.com/
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100425/3d7a1e04/attachment.html>

From joe.bowbeer at gmail.com  Mon Apr 26 01:17:44 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 25 Apr 2010 22:17:44 -0700
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <q2h31f2a7bd1004251626t674ad6e9p4db719d8ece74160@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
	<4BD2E697.3020305@gmx.org> <4BD2EF8B.603@cs.oswego.edu>
	<4BD40BF0.9070509@gmx.org> <4BD43930.5090003@cs.oswego.edu>
	<4BD46469.7030502@gmx.org>
	<q2h31f2a7bd1004251626t674ad6e9p4db719d8ece74160@mail.gmail.com>
Message-ID: <o2x31f2a7bd1004252217i2a98cb22x7164f21bb4ce72ba@mail.gmail.com>

While I'm accumulating questions...

Why is ThreadLocal not the preferred cache in this case?

Joe

On Sun, Apr 25, 2010 at 4:26 PM, Joe Bowbeer wrote:

> Jochen,
>
> What you are describing seems like a caching problem as much as it is about
> safe publication and/or dynamic languages.  The language runtime creates the
> (immutable) instances and publishes them to the cache, right?  The
> performance of the cache is the hot spot?
>
> So are you using something like MapMaker to implement the cache?
>
> http://code.google.com/p/google-collections/
>
> What are you using to hold off a t2 when t1 is in the process of publishing
> to the cache?  Some scheme involving a Future?
>
> Joe
>
>
> On Sun, Apr 25, 2010 at 8:48 AM, Jochen Theodorou wrote:
>
>> Doug Lea wrote:
>>
>>> On 04/25/10 05:31, Jochen Theodorou wrote:
>>>
>>>> As a first step, consider exactly what effects/semantics you want
>>>>> here, and the ways you intend people to be able to write conditionally
>>>>> correct Groovy code.
>>>>>
>>>>
>>>> People wouldn't have to write conditionally correct Groovy code. they
>>>> would write normal code as they would in Java (Groovy and Java are very
>>>> near).
>>>>
>>>
>>> It seems implausible that you could do enough
>>> analysis at load/run time to determine whether you need
>>> full locking in the presence of multithreaded racy initialization
>>> vs much cheaper release fences. This would require at least some
>>> high-quality escape analysis. And the code generated
>>> would differ both for the writing and reading callers.
>>>
>>
>> maybe I did explain it not good. Let us assume I have the Groovy code:
>>
>> 1+1
>>
>> Then this is really something along the lines of:
>>
>> SBA.getMetaClassOf(1).invoke("plus",1)
>>
>> and SBA.getMetaClassOf(1) would return the meta class of Integer. Since
>> this is purely a runtime construct, it does not exist until the first time
>> this meta class is requested. So getMetaClassOf would be the place to
>> initialize the meta class, that would register it in a global structure and
>> on subsequent invocation use that cached meta class. If two threads execute
>> the code above, then one would do the initialization, while the other has to
>> wait. The waiting thread would then read the initialized global meta class.
>> On subsequent invocations both threads would just read. Since changes of the
>> meta class are rare, we would in 99% of all cases simply read the existing
>> value. Since we have to be memory aware, these meta class can be unloaded at
>> runtime too. They are SoftReferenced so it is done only if really needed.
>> But rather than the normal change a reinitialization might be needed much
>> more often.
>>
>> As you see the user code "1+1" does contain zero synchronization code. The
>> memory barriers are all in the runtime. It is not that this cannot be solved
>> by using what Java already has, it is that this is too expensive.
>>
>>
>>  As I mentioned, an alternative is to lay down some rules.
>>> If people stick to the rules they get consistent (in the sense
>>> of data-race-free) executions, else they might not. And of
>>> such rules, I think the ones that can apply here amount
>>> to saying that other threads performing initializations cannot
>>> trust any of their reads of the partially initialized object.
>>> And further, they cannot leak refs to that object outside of the
>>> group of initializer threads.
>>>
>>> This is not hugely different than the Swing threading rules
>>> (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
>>> but applies only during initialization.
>>>
>>
>> but unlike what the above may suggest there is no single initialization
>> phase. The meta classes are created on demand. We cannot know beforehand
>> which meta classes are needed and doing them all before starting would
>> increase the startup time big times.
>>
>> If there were of course a way to recognize a partially initialized object
>> I could maybe think of something... but is there a reliable one?
>>
>>
>> bye blackdrag
>>
>> --
>> Jochen "blackdrag" Theodorou
>> The Groovy Project Tech Lead (http://groovy.codehaus.org)
>> http://blackdragsview.blogspot.com/
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100425/6f671267/attachment.html>

From forax at univ-mlv.fr  Mon Apr 26 05:35:50 2010
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Mon, 26 Apr 2010 11:35:50 +0200
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <o2x31f2a7bd1004252217i2a98cb22x7164f21bb4ce72ba@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>	<4BD2E697.3020305@gmx.org>
	<4BD2EF8B.603@cs.oswego.edu>	<4BD40BF0.9070509@gmx.org>
	<4BD43930.5090003@cs.oswego.edu>	<4BD46469.7030502@gmx.org>	<q2h31f2a7bd1004251626t674ad6e9p4db719d8ece74160@mail.gmail.com>
	<o2x31f2a7bd1004252217i2a98cb22x7164f21bb4ce72ba@mail.gmail.com>
Message-ID: <4BD55E76.2060600@univ-mlv.fr>

Le 26/04/2010 07:17, Joe Bowbeer a ?crit :
> While I'm accumulating questions...
>
> Why is ThreadLocal not the preferred cache in this case?
>
> Joe

You can also mutate the metaclass by example by adding a new method 
dynamically,
in that case, all threads must see the modification.
So ThreadLocal doesn't solve the problem here.

R?mi

>
> On Sun, Apr 25, 2010 at 4:26 PM, Joe Bowbeer wrote:
>
>     Jochen,
>
>     What you are describing seems like a caching problem as much as it
>     is about safe publication and/or dynamic languages.  The language
>     runtime creates the (immutable) instances and publishes them to
>     the cache, right?  The performance of the cache is the hot spot?
>
>     So are you using something like MapMaker to implement the cache?
>
>     http://code.google.com/p/google-collections/
>
>     What are you using to hold off a t2 when t1 is in the process of
>     publishing to the cache?  Some scheme involving a Future?
>
>     Joe
>
>
>     On Sun, Apr 25, 2010 at 8:48 AM, Jochen Theodorou wrote:
>
>         Doug Lea wrote:
>
>             On 04/25/10 05:31, Jochen Theodorou wrote:
>
>                     As a first step, consider exactly what
>                     effects/semantics you want
>                     here, and the ways you intend people to be able to
>                     write conditionally
>                     correct Groovy code.
>
>
>                 People wouldn't have to write conditionally correct
>                 Groovy code. they
>                 would write normal code as they would in Java (Groovy
>                 and Java are very
>                 near).
>
>
>             It seems implausible that you could do enough
>             analysis at load/run time to determine whether you need
>             full locking in the presence of multithreaded racy
>             initialization
>             vs much cheaper release fences. This would require at
>             least some
>             high-quality escape analysis. And the code generated
>             would differ both for the writing and reading callers.
>
>
>         maybe I did explain it not good. Let us assume I have the
>         Groovy code:
>
>         1+1
>
>         Then this is really something along the lines of:
>
>         SBA.getMetaClassOf(1).invoke("plus",1)
>
>         and SBA.getMetaClassOf(1) would return the meta class of
>         Integer. Since this is purely a runtime construct, it does not
>         exist until the first time this meta class is requested. So
>         getMetaClassOf would be the place to initialize the meta
>         class, that would register it in a global structure and on
>         subsequent invocation use that cached meta class. If two
>         threads execute the code above, then one would do the
>         initialization, while the other has to wait. The waiting
>         thread would then read the initialized global meta class. On
>         subsequent invocations both threads would just read. Since
>         changes of the meta class are rare, we would in 99% of all
>         cases simply read the existing value. Since we have to be
>         memory aware, these meta class can be unloaded at runtime too.
>         They are SoftReferenced so it is done only if really needed.
>         But rather than the normal change a reinitialization might be
>         needed much more often.
>
>         As you see the user code "1+1" does contain zero
>         synchronization code. The memory barriers are all in the
>         runtime. It is not that this cannot be solved by using what
>         Java already has, it is that this is too expensive.
>
>
>             As I mentioned, an alternative is to lay down some rules.
>             If people stick to the rules they get consistent (in the sense
>             of data-race-free) executions, else they might not. And of
>             such rules, I think the ones that can apply here amount
>             to saying that other threads performing initializations cannot
>             trust any of their reads of the partially initialized object.
>             And further, they cannot leak refs to that object outside
>             of the
>             group of initializer threads.
>
>             This is not hugely different than the Swing threading rules
>             (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
>             but applies only during initialization.
>
>
>         but unlike what the above may suggest there is no single
>         initialization phase. The meta classes are created on demand.
>         We cannot know beforehand which meta classes are needed and
>         doing them all before starting would increase the startup time
>         big times.
>
>         If there were of course a way to recognize a partially
>         initialized object I could maybe think of something... but is
>         there a reliable one?
>
>
>         bye blackdrag
>
>         -- 
>         Jochen "blackdrag" Theodorou
>         The Groovy Project Tech Lead (http://groovy.codehaus.org)
>         http://blackdragsview.blogspot.com/
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>    

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100426/8ef5da1b/attachment-0001.html>

From tgautier at terracottatech.com  Mon Apr 26 11:34:53 2010
From: tgautier at terracottatech.com (Taylor Gautier)
Date: Mon, 26 Apr 2010 08:34:53 -0700
Subject: [concurrency-interest] when is safe publication safe?
Message-ID: <83AA4DC3-0483-4BC8-AE2E-FC2C1E062A74@terracottatech.com>


This is very interesting. What you have here are read many, write  
infrequently, but writes must be coherent for readers but once  
'refreshed' readers shouldn't incur barrier penalties.

Terracotta solves this problem at the network level for many jvms by  
allowing local readers and writers to acquire a lock with a lock lease  
which can be revoked asynchronously.  This allows local jvms to  
proceed with a read lock without incurring a lock read penalty.   I  
mention Terracotta because  for this problem JVMs in a cluster are  
analogous to threads in the JVM.

Is it possible to construct the same thing in threads using java  
primitives?

Does a ReentrantReadWriteLock help? I don't think so. What you would  
need is for all threads to acquire and hold a read lock but have an  
ability to acquiesce it on demand for a writer without incurring  
memory barrier hits.

I can't think of any way to 'cache' a read lock in ThreadLocal such  
that it doesn't incur barriers on every read and yet also can yield to  
a writer asynchronously.

On Apr 26, 2010, at 2:35 AM, R?mi Forax <forax at univ-mlv.fr> wrote:

> Le 26/04/2010 07:17, Joe Bowbeer a ?crit :
>>
>> While I'm accumulating questions...
>>
>> Why is ThreadLocal not the preferred cache in this case?
>>
>> Joe
>
> You can also mutate the metaclass by example by adding a new method  
> dynamically,
> in that case, all threads must see the modification.
> So ThreadLocal doesn't solve the problem here.
>
> R?mi
>
>>
>> On Sun, Apr 25, 2010 at 4:26 PM, Joe Bowbeer wrote:
>> Jochen,
>>
>> What you are describing seems like a caching problem as much as it  
>> is about safe publication and/or dynamic languages.  The language  
>> runtime creates the (immutable) instances and publishes them to the  
>> cache, right?  The performance of the cache is the hot spot?
>>
>> So are you using something like MapMaker to implement the cache?
>>
>> http://code.google.com/p/google-collections/
>>
>> What are you using to hold off a t2 when t1 is in the process of  
>> publishing to the cache?  Some scheme involving a Future?
>>
>> Joe
>>
>>
>> On Sun, Apr 25, 2010 at 8:48 AM, Jochen Theodorou wrote:
>> Doug Lea wrote:
>> On 04/25/10 05:31, Jochen Theodorou wrote:
>> As a first step, consider exactly what effects/semantics you want
>> here, and the ways you intend people to be able to write  
>> conditionally
>> correct Groovy code.
>>
>> People wouldn't have to write conditionally correct Groovy code. they
>> would write normal code as they would in Java (Groovy and Java are  
>> very
>> near).
>>
>> It seems implausible that you could do enough
>> analysis at load/run time to determine whether you need
>> full locking in the presence of multithreaded racy initialization
>> vs much cheaper release fences. This would require at least some
>> high-quality escape analysis. And the code generated
>> would differ both for the writing and reading callers.
>>
>> maybe I did explain it not good. Let us assume I have the Groovy  
>> code:
>>
>> 1+1
>>
>> Then this is really something along the lines of:
>>
>> SBA.getMetaClassOf(1).invoke("plus",1)
>>
>> and SBA.getMetaClassOf(1) would return the meta class of Integer.  
>> Since this is purely a runtime construct, it does not exist until  
>> the first time this meta class is requested. So getMetaClassOf  
>> would be the place to initialize the meta class, that would  
>> register it in a global structure and on subsequent invocation use  
>> that cached meta class. If two threads execute the code above, then  
>> one would do the initialization, while the other has to wait. The  
>> waiting thread would then read the initialized global meta class.  
>> On subsequent invocations both threads would just read. Since  
>> changes of the meta class are rare, we would in 99% of all cases  
>> simply read the existing value. Since we have to be memory aware,  
>> these meta class can be unloaded at runtime too. They are  
>> SoftReferenced so it is done only if really needed. But rather than  
>> the normal change a reinitialization might be needed much more often.
>>
>> As you see the user code "1+1" does contain zero synchronization  
>> code. The memory barriers are all in the runtime. It is not that  
>> this cannot be solved by using what Java already has, it is that  
>> this is too expensive.
>>
>>
>> As I mentioned, an alternative is to lay down some rules.
>> If people stick to the rules they get consistent (in the sense
>> of data-race-free) executions, else they might not. And of
>> such rules, I think the ones that can apply here amount
>> to saying that other threads performing initializations cannot
>> trust any of their reads of the partially initialized object.
>> And further, they cannot leak refs to that object outside of the
>> group of initializer threads.
>>
>> This is not hugely different than the Swing threading rules
>> (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
>> but applies only during initialization.
>>
>> but unlike what the above may suggest there is no single  
>> initialization phase. The meta classes are created on demand. We  
>> cannot know beforehand which meta classes are needed and doing them  
>> all before starting would increase the startup time big times.
>>
>> If there were of course a way to recognize a partially initialized  
>> object I could maybe think of something... but is there a reliable  
>> one?
>>
>>
>> bye blackdrag
>>
>> -- 
>> Jochen "blackdrag" Theodorou
>> The Groovy Project Tech Lead (http://groovy.codehaus.org)
>> http://blackdragsview.blogspot.com/
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100426/02e7f93a/attachment.html>

From gregg at cytetech.com  Mon Apr 26 12:39:59 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 26 Apr 2010 11:39:59 -0500
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <83AA4DC3-0483-4BC8-AE2E-FC2C1E062A74@terracottatech.com>
References: <83AA4DC3-0483-4BC8-AE2E-FC2C1E062A74@terracottatech.com>
Message-ID: <4BD5C1DF.50106@cytetech.com>

What you describe, can be done with lighter weight locks, because it is a 
"cache" where "eventually" you want the right one.  In this discussion, we are 
talking about programming language features that change data, which must "appear 
  changed" at all times "after it changes".  This implies that there can't be a 
window of "exposure of the change" to me.

A network cache, even if it is locking on every read, will appear much faster 
than going across the network.  In memory, locked vs not-locked is similar in 
relative performance increases for current hardware.  What we need, from my 
perspective, is more hardware control of this issue where the hardware does the 
right thing without software (at least at the application layer) having to worry 
about how to make things "work".

The current state of the JVM, to me, is kind of like using core memory in an RFI 
environment.  Bits can change all around you, continuously and you may never see 
exactly what you expect, because the "runtime environment" can not provide any 
guarantees about how external forces affect it.

We can all use synchronized on everything, or even declare every variable either 
"volatile" or "final".  All of this is boiling up around us.  There are complete 
environments that exists as libraries which have behaviors that work, because of 
certain constraints on how they are used, or on how IDEs construct code templates.

I, still think it is best to just work on creating custom "monitor like" API 
based classes which wrap up the details of providing "acceleration" of "reads" 
and the like, instead of having all this fragile visibility, locking and fences 
strewn about in the code.  Sure it feels like a waste to create a class for a 
single use, but in the end, it really helps compartmentalize the functionality 
in a way that you can "fix" and "test" the logic very easily without having to 
understand how all of these synchronization and concurrency features are woven 
into a particular block of code.

The visibility thing is really the key for me, because the happens-before 
injections that can occur when synchronized and other effecting constructs are 
used in user code, can really change the "correctness" of software when certain 
things that used to happen in a "synchronized" block get pulled out.

For example, lets say that you put logging statements into a synchronized block, 
such as:

	log.fine("State now looks like: "+value );

You put it into the synchronized block, because "value" is atomically related to 
the lock, and has no locking, and you need synchronized to "read" its value.

You have now created a happens before "read" on value.  Somewhere later in the 
code, you have an unlocked read (by mistake, but the code is working) on a 
"simple" var that is a member of "value", but not a complex structure.  Let's 
say, that you run your code this way, and the application has no problems, so 
you don't realize that the unlocked read is a problem.

Later, in a code review, someone says "you should conditionalize that logging so 
that "value" is not evaluated if logging is off.  So, you add

	if( log.isLoggable(Level.FINE) )

and suddenly testing reveals that randomly something is not working related to 
that "simple" var value.

Through some analysis and debugging, you will eventually discover that the 
logging was reading the value of the "simple" var, and that "happens-before" was 
creating the "visibility" of the "simple" var and making things work.

It is exactly this kind of very common programming structures that I think have 
greatly degraded the chance for Java programmers to be realistically successful 
at writing safe, concurrent software structures.

There are a wealth of subtle things that the current visibility issues affect in 
often intangible ways.  Truly, it takes expert analysis and a lot of effort to 
remember all the details of how concurrency really works in Java.

Gregg Wonderly

Taylor Gautier wrote:
> 
> This is very interesting. What you have here are read many, write 
> infrequently, but writes must be coherent for readers but once 
> 'refreshed' readers shouldn't incur barrier penalties.  
> 
> Terracotta solves this problem at the network level for many jvms by 
> allowing local readers and writers to acquire a lock with a lock lease 
> which can be revoked asynchronously.  This allows local jvms to proceed 
> with a read lock without incurring a lock read penalty.   I mention 
> Terracotta because  for this problem JVMs in a cluster are analogous to 
> threads in the JVM. 
>  
> Is it possible to construct the same thing in threads using java 
> primitives? 
> 
> Does a ReentrantReadWriteLock help? I don't think so. What you would 
> need is for all threads to acquire and hold a read lock but have an 
> ability to acquiesce it on demand for a writer without incurring memory 
> barrier hits.   
> 
> I can't think of any way to 'cache' a read lock in ThreadLocal such that 
> it doesn't incur barriers on every read and yet also can yield to a 
> writer asynchronously. 
> 
> On Apr 26, 2010, at 2:35 AM, R?mi Forax < 
> <mailto:forax at univ-mlv.fr>forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>> 
> wrote:
> 
>> Le 26/04/2010 07:17, Joe Bowbeer a ?crit :
>>> While I'm accumulating questions...
>>>
>>> Why is ThreadLocal not the preferred cache in this case?
>>>
>>> Joe
>>
>> You can also mutate the metaclass by example by adding a new method 
>> dynamically,
>> in that case, all threads must see the modification.
>> So ThreadLocal doesn't solve the problem here.
>>
>> R?mi
>>
>>>
>>> On Sun, Apr 25, 2010 at 4:26 PM, Joe Bowbeer wrote:
>>>
>>>     Jochen,
>>>
>>>     What you are describing seems like a caching problem as much as
>>>     it is about safe publication and/or dynamic languages.  The
>>>     language runtime creates the (immutable) instances and publishes
>>>     them to the cache, right?  The performance of the cache is the
>>>     hot spot?
>>>
>>>     So are you using something like MapMaker to implement the cache?
>>>
>>>     <http://code.google.com/p/google-collections/>
>>>     <http://code.google.com/p/google-collections/>http://code.google.com/p/google-collections/
>>>
>>>     What are you using to hold off a t2 when t1 is in the process of
>>>     publishing to the cache?  Some scheme involving a Future?
>>>
>>>     Joe
>>>
>>>
>>>     On Sun, Apr 25, 2010 at 8:48 AM, Jochen Theodorou wrote:
>>>
>>>         Doug Lea wrote:
>>>
>>>             On 04/25/10 05:31, Jochen Theodorou wrote:
>>>
>>>                     As a first step, consider exactly what
>>>                     effects/semantics you want
>>>                     here, and the ways you intend people to be able
>>>                     to write conditionally
>>>                     correct Groovy code.
>>>
>>>
>>>                 People wouldn't have to write conditionally correct
>>>                 Groovy code. they
>>>                 would write normal code as they would in Java (Groovy
>>>                 and Java are very
>>>                 near).
>>>
>>>
>>>             It seems implausible that you could do enough
>>>             analysis at load/run time to determine whether you need
>>>             full locking in the presence of multithreaded racy
>>>             initialization
>>>             vs much cheaper release fences. This would require at
>>>             least some
>>>             high-quality escape analysis. And the code generated
>>>             would differ both for the writing and reading callers.
>>>
>>>
>>>         maybe I did explain it not good. Let us assume I have the
>>>         Groovy code:
>>>
>>>         1+1
>>>
>>>         Then this is really something along the lines of:
>>>
>>>         SBA.getMetaClassOf(1).invoke("plus",1)
>>>
>>>         and SBA.getMetaClassOf(1) would return the meta class of
>>>         Integer. Since this is purely a runtime construct, it does
>>>         not exist until the first time this meta class is requested.
>>>         So getMetaClassOf would be the place to initialize the meta
>>>         class, that would register it in a global structure and on
>>>         subsequent invocation use that cached meta class. If two
>>>         threads execute the code above, then one would do the
>>>         initialization, while the other has to wait. The waiting
>>>         thread would then read the initialized global meta class. On
>>>         subsequent invocations both threads would just read. Since
>>>         changes of the meta class are rare, we would in 99% of all
>>>         cases simply read the existing value. Since we have to be
>>>         memory aware, these meta class can be unloaded at runtime
>>>         too. They are SoftReferenced so it is done only if really
>>>         needed. But rather than the normal change a reinitialization
>>>         might be needed much more often.
>>>
>>>         As you see the user code "1+1" does contain zero
>>>         synchronization code. The memory barriers are all in the
>>>         runtime. It is not that this cannot be solved by using what
>>>         Java already has, it is that this is too expensive.
>>>
>>>
>>>             As I mentioned, an alternative is to lay down some rules.
>>>             If people stick to the rules they get consistent (in the
>>>             sense
>>>             of data-race-free) executions, else they might not. And of
>>>             such rules, I think the ones that can apply here amount
>>>             to saying that other threads performing initializations
>>>             cannot
>>>             trust any of their reads of the partially initialized object.
>>>             And further, they cannot leak refs to that object outside
>>>             of the
>>>             group of initializer threads.
>>>
>>>             This is not hugely different than the Swing threading rules
>>>             (
>>>             <http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html>
>>>             <http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html>http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
>>>             but applies only during initialization.
>>>
>>>
>>>         but unlike what the above may suggest there is no single
>>>         initialization phase. The meta classes are created on demand.
>>>         We cannot know beforehand which meta classes are needed and
>>>         doing them all before starting would increase the startup
>>>         time big times.
>>>
>>>         If there were of course a way to recognize a partially
>>>         initialized object I could maybe think of something... but is
>>>         there a reliable one?
>>>
>>>
>>>         bye blackdrag
>>>
>>>         -- 
>>>         Jochen "blackdrag" Theodorou
>>>         The Groovy Project Tech Lead ( <http://groovy.codehaus.org>
>>>         <http://groovy.codehaus.org>http://groovy.codehaus.org)
>>>         <http://blackdragsview.blogspot.com/>
>>>         <http://blackdragsview.blogspot.com/>http://blackdragsview.blogspot.com/
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>>  <mailto:Concurrency-interest at cs.oswego.edu>Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>  <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>   
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> <mailto:Concurrency-interest at cs.oswego.edu>Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From blackdrag at gmx.org  Mon Apr 26 17:05:29 2010
From: blackdrag at gmx.org (Jochen Theodorou)
Date: Mon, 26 Apr 2010 23:05:29 +0200
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <83AA4DC3-0483-4BC8-AE2E-FC2C1E062A74@terracottatech.com>
References: <83AA4DC3-0483-4BC8-AE2E-FC2C1E062A74@terracottatech.com>
Message-ID: <4BD60019.5010702@gmx.org>

Taylor Gautier wrote:
> 
> This is very interesting. What you have here are read many, write 
> infrequently, but writes must be coherent for readers but once 
> 'refreshed' readers shouldn't incur barrier penalties.  

Mostly, yes. I think I can live with some incoherences, but it would be 
more easy without them. But of course, it should not happen that one 
modification of a metaclass is visible, while another one on the same 
metaclass done before is not. Also two meta classes modified by one 
thread and only one of them becoming visible in the other might not be 
so good. So maybe it really does have to be coherent.

> Terracotta solves this problem at the network level for many jvms by 
> allowing local readers and writers to acquire a lock with a lock lease 
> which can be revoked asynchronously.  This allows local jvms to proceed 
> with a read lock without incurring a lock read penalty.   I mention 
> Terracotta because  for this problem JVMs in a cluster are analogous to 
> threads in the JVM. 

How does Terracotta perform if the code uses a lot of locks? I did hear 
it is not so well.

> Is it possible to construct the same thing in threads using java 
> primitives? 
> 
> Does a ReentrantReadWriteLock help? I don't think so. What you would 
> need is for all threads to acquire and hold a read lock but have an 
> ability to acquiesce it on demand for a writer without incurring memory 
> barrier hits.   
> 
> I can't think of any way to 'cache' a read lock in ThreadLocal such that 
> it doesn't incur barriers on every read and yet also can yield to a 
> writer asynchronously. 

additionally has the data leak problem. Since there is no defined entry 
and exit point I can use, I cannot remove the stored ThreadLocal data 
the correct way. Well I can, but then the caching would cost more than 
what I wanted to cache.

bye Jochen

-- 
Jochen "blackdrag" Theodorou
The Groovy Project Tech Lead (http://groovy.codehaus.org)
http://blackdragsview.blogspot.com/


From hans.boehm at hp.com  Mon Apr 26 17:02:26 2010
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 26 Apr 2010 21:02:26 +0000
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <4BD46469.7030502@gmx.org>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
	<4BD2E697.3020305@gmx.org>	<4BD2EF8B.603@cs.oswego.edu>
	<4BD40BF0.9070509@gmx.org>	<4BD43930.5090003@cs.oswego.edu>
	<4BD46469.7030502@gmx.org>
Message-ID: <238A96A773B3934685A7269CC8A8D0426DF6308CF4@GVW0436EXB.americas.hpqcorp.net>

To clarify, hopefully:

The core problem here is presumably that getMetaClassOf() reads a field, call it mc, containing a reference to the meta class information, and initialization of that field may race with the access.  The classic solution would be to declare mc "volatile", which avoids the data race and (in the absence of other data races) restores sequential consistency, so you don't have to worry about visibility issues.  (If you do want to worry about happens before ordering, it establishes the right happens before ordering.)

The reason you may not be happy with this is that it's slow on some architectures.  However the performance hit on X86 should be negligible, since an ordinary load is sufficient to implement the volatile load.  The store will be more expensive, but that's rare.  Thus you are presumably concerned about non-X86 architectures?

It's a bit confusing to talk about memory barriers since, as far as I know, the only Java class with "barrier" in the name is CyclicBarrier, which is something very different.

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Jochen Theodorou
> Sent: Sunday, April 25, 2010 8:49 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] when is safe publication safe?
> 
> Doug Lea wrote:
> > On 04/25/10 05:31, Jochen Theodorou wrote:
> >>> As a first step, consider exactly what effects/semantics you want 
> >>> here, and the ways you intend people to be able to write 
> >>> conditionally correct Groovy code.
> >>
> >> People wouldn't have to write conditionally correct Groovy 
> code. they 
> >> would write normal code as they would in Java (Groovy and Java are 
> >> very near).
> > 
> > It seems implausible that you could do enough analysis at load/run 
> > time to determine whether you need full locking in the presence of 
> > multithreaded racy initialization vs much cheaper release 
> fences. This 
> > would require at least some high-quality escape analysis. 
> And the code 
> > generated would differ both for the writing and reading callers.
> 
> maybe I did explain it not good. Let us assume I have the Groovy code:
> 
> 1+1
> 
> Then this is really something along the lines of:
> 
> SBA.getMetaClassOf(1).invoke("plus",1)
> 
> and SBA.getMetaClassOf(1) would return the meta class of 
> Integer. Since this is purely a runtime construct, it does 
> not exist until the first time this meta class is requested. 
> So getMetaClassOf would be the place to initialize the meta 
> class, that would register it in a global structure and on 
> subsequent invocation use that cached meta class. If two 
> threads execute the code above, then one would do the 
> initialization, while the other has to wait. The waiting 
> thread would then read the initialized global meta class. On 
> subsequent invocations both threads would just read. Since 
> changes of the meta class are rare, we would in 99% of all 
> cases simply read the existing value. Since we have to be 
> memory aware, these meta class can be unloaded at runtime 
> too. They are SoftReferenced so it is done only if really 
> needed. But rather than the normal change a reinitialization 
> might be needed much more often.
> 
> As you see the user code "1+1" does contain zero 
> synchronization code. 
> The memory barriers are all in the runtime. It is not that 
> this cannot be solved by using what Java already has, it is 
> that this is too expensive.
> 
> > As I mentioned, an alternative is to lay down some rules.
> > If people stick to the rules they get consistent (in the sense of 
> > data-race-free) executions, else they might not. And of 
> such rules, I 
> > think the ones that can apply here amount to saying that 
> other threads 
> > performing initializations cannot trust any of their reads of the 
> > partially initialized object.
> > And further, they cannot leak refs to that object outside 
> of the group 
> > of initializer threads.
> > 
> > This is not hugely different than the Swing threading rules
> > 
> (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
> > but applies only during initialization.
> 
> but unlike what the above may suggest there is no single 
> initialization phase. The meta classes are created on demand. 
> We cannot know beforehand which meta classes are needed and 
> doing them all before starting would increase the startup 
> time big times.
> 
> If there were of course a way to recognize a partially 
> initialized object I could maybe think of something... but is 
> there a reliable one?
> 
> bye blackdrag
> 
> --
> Jochen "blackdrag" Theodorou
> The Groovy Project Tech Lead (http://groovy.codehaus.org) 
> http://blackdragsview.blogspot.com/
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From joe.bowbeer at gmail.com  Mon Apr 26 21:20:11 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 26 Apr 2010 18:20:11 -0700
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <238A96A773B3934685A7269CC8A8D0426DF6308CF4@GVW0436EXB.americas.hpqcorp.net>
References: <NFBBKALFDCPFIDBNKAPCOEBAIGAA.davidcholmes@aapt.net.au>
	<4BD2E697.3020305@gmx.org> <4BD2EF8B.603@cs.oswego.edu>
	<4BD40BF0.9070509@gmx.org> <4BD43930.5090003@cs.oswego.edu>
	<4BD46469.7030502@gmx.org>
	<238A96A773B3934685A7269CC8A8D0426DF6308CF4@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <p2j31f2a7bd1004261820yb979e67ez1b07ceeb5f41e253@mail.gmail.com>

I'm thinking of a versioned reference.  Adding a new method to an object
would rev the version.  A thread could optimistically use a cached instance
as long as the revision matched.  AtomicStampedReference can be used to
associate a version with a reference?

I'm just throwing out ideas in an effort to understand the problem better...
 Hope you don't mind.


On Mon, Apr 26, 2010 at 2:02 PM, Boehm, Hans wrote:

> To clarify, hopefully:
>
> The core problem here is presumably that getMetaClassOf() reads a field,
> call it mc, containing a reference to the meta class information, and
> initialization of that field may race with the access.  The classic solution
> would be to declare mc "volatile", which avoids the data race and (in the
> absence of other data races) restores sequential consistency, so you don't
> have to worry about visibility issues.  (If you do want to worry about
> happens before ordering, it establishes the right happens before ordering.)
>
> The reason you may not be happy with this is that it's slow on some
> architectures.  However the performance hit on X86 should be negligible,
> since an ordinary load is sufficient to implement the volatile load.  The
> store will be more expensive, but that's rare.  Thus you are presumably
> concerned about non-X86 architectures?
>
> It's a bit confusing to talk about memory barriers since, as far as I know,
> the only Java class with "barrier" in the name is CyclicBarrier, which is
> something very different.
>
> Hans
>
> > -----Original Message-----
> > From: Jochen Theodorou
> > Sent: Sunday, April 25, 2010 8:49 AM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] when is safe publication safe?
> >
> > Doug Lea wrote:
> > > On 04/25/10 05:31, Jochen Theodorou wrote:
> > >>> As a first step, consider exactly what effects/semantics you want
> > >>> here, and the ways you intend people to be able to write
> > >>> conditionally correct Groovy code.
> > >>
> > >> People wouldn't have to write conditionally correct Groovy
> > code. they
> > >> would write normal code as they would in Java (Groovy and Java are
> > >> very near).
> > >
> > > It seems implausible that you could do enough analysis at load/run
> > > time to determine whether you need full locking in the presence of
> > > multithreaded racy initialization vs much cheaper release
> > fences. This
> > > would require at least some high-quality escape analysis.
> > And the code
> > > generated would differ both for the writing and reading callers.
> >
> > maybe I did explain it not good. Let us assume I have the Groovy code:
> >
> > 1+1
> >
> > Then this is really something along the lines of:
> >
> > SBA.getMetaClassOf(1).invoke("plus",1)
> >
> > and SBA.getMetaClassOf(1) would return the meta class of
> > Integer. Since this is purely a runtime construct, it does
> > not exist until the first time this meta class is requested.
> > So getMetaClassOf would be the place to initialize the meta
> > class, that would register it in a global structure and on
> > subsequent invocation use that cached meta class. If two
> > threads execute the code above, then one would do the
> > initialization, while the other has to wait. The waiting
> > thread would then read the initialized global meta class. On
> > subsequent invocations both threads would just read. Since
> > changes of the meta class are rare, we would in 99% of all
> > cases simply read the existing value. Since we have to be
> > memory aware, these meta class can be unloaded at runtime
> > too. They are SoftReferenced so it is done only if really
> > needed. But rather than the normal change a reinitialization
> > might be needed much more often.
> >
> > As you see the user code "1+1" does contain zero
> > synchronization code.
> > The memory barriers are all in the runtime. It is not that
> > this cannot be solved by using what Java already has, it is
> > that this is too expensive.
> >
> > > As I mentioned, an alternative is to lay down some rules.
> > > If people stick to the rules they get consistent (in the sense of
> > > data-race-free) executions, else they might not. And of
> > such rules, I
> > > think the ones that can apply here amount to saying that
> > other threads
> > > performing initializations cannot trust any of their reads of the
> > > partially initialized object.
> > > And further, they cannot leak refs to that object outside
> > of the group
> > > of initializer threads.
> > >
> > > This is not hugely different than the Swing threading rules
> > >
> > (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
> > > but applies only during initialization.
> >
> > but unlike what the above may suggest there is no single
> > initialization phase. The meta classes are created on demand.
> > We cannot know beforehand which meta classes are needed and
> > doing them all before starting would increase the startup
> > time big times.
> >
> > If there were of course a way to recognize a partially
> > initialized object I could maybe think of something... but is
> > there a reliable one?
> >
> > bye blackdrag
> >
> > --
> > Jochen "blackdrag" Theodorou
> > The Groovy Project Tech Lead (http://groovy.codehaus.org)
> > http://blackdragsview.blogspot.com/
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100426/6f1332ff/attachment-0001.html>

From davidcholmes at aapt.net.au  Mon Apr 26 21:47:48 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 27 Apr 2010 11:47:48 +1000
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <p2j31f2a7bd1004261820yb979e67ez1b07ceeb5f41e253@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEBOIGAA.davidcholmes@aapt.net.au>

The problem with using any existing j.u.c class, or even thread-safe j.u
class, is that the key issue here seems to be the desire to avoid the
memory-barriers on the read side when they are not needed. This is to me a
somewhat utopian goal - if we could tell when barriers were and were not
needed we would only inject them when necessary (and of course the cost of
determining this mustn't outweigh the cost of unconditionally using the
memory-barrier in the first place). But all our existing Java library
classes are correctly synchronized and have at a minimum volatile semantics
on reads, which implies the need for memory-barriers in the general case.

If a thread must see an update to the meta-class object then any read of a
version would have to be exact - which implies it has to be correctly
synchronized with respect to the updating thread, which implies a minimum of
volatile semantics on the read.

This seems to be a similar problem to the scalable-counter discussion. If
the count is exact you need correct synchronization; without correct
synchronization the count can't be exact. You have to pick which is more
important.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Joe Bowbeer
  Sent: Tuesday, 27 April 2010 11:20 AM
  To: concurrency-interest
  Subject: Re: [concurrency-interest] when is safe publication safe?


  I'm thinking of a versioned reference.  Adding a new method to an object
would rev the version.  A thread could optimistically use a cached instance
as long as the revision matched.  AtomicStampedReference can be used to
associate a version with a reference?


  I'm just throwing out ideas in an effort to understand the problem
better...  Hope you don't mind.




  On Mon, Apr 26, 2010 at 2:02 PM, Boehm, Hans wrote:

    To clarify, hopefully:

    The core problem here is presumably that getMetaClassOf() reads a field,
call it mc, containing a reference to the meta class information, and
initialization of that field may race with the access.  The classic solution
would be to declare mc "volatile", which avoids the data race and (in the
absence of other data races) restores sequential consistency, so you don't
have to worry about visibility issues.  (If you do want to worry about
happens before ordering, it establishes the right happens before ordering.)

    The reason you may not be happy with this is that it's slow on some
architectures.  However the performance hit on X86 should be negligible,
since an ordinary load is sufficient to implement the volatile load.  The
store will be more expensive, but that's rare.  Thus you are presumably
concerned about non-X86 architectures?

    It's a bit confusing to talk about memory barriers since, as far as I
know, the only Java class with "barrier" in the name is CyclicBarrier, which
is something very different.

    Hans


    > -----Original Message-----
    > From: Jochen Theodorou
    > Sent: Sunday, April 25, 2010 8:49 AM
    > To: concurrency-interest at cs.oswego.edu
    > Subject: Re: [concurrency-interest] when is safe publication safe?
    >
    > Doug Lea wrote:
    > > On 04/25/10 05:31, Jochen Theodorou wrote:
    > >>> As a first step, consider exactly what effects/semantics you want
    > >>> here, and the ways you intend people to be able to write
    > >>> conditionally correct Groovy code.
    > >>
    > >> People wouldn't have to write conditionally correct Groovy
    > code. they
    > >> would write normal code as they would in Java (Groovy and Java are
    > >> very near).
    > >
    > > It seems implausible that you could do enough analysis at load/run
    > > time to determine whether you need full locking in the presence of
    > > multithreaded racy initialization vs much cheaper release
    > fences. This
    > > would require at least some high-quality escape analysis.
    > And the code
    > > generated would differ both for the writing and reading callers.
    >
    > maybe I did explain it not good. Let us assume I have the Groovy code:
    >
    > 1+1
    >
    > Then this is really something along the lines of:
    >
    > SBA.getMetaClassOf(1).invoke("plus",1)
    >
    > and SBA.getMetaClassOf(1) would return the meta class of
    > Integer. Since this is purely a runtime construct, it does
    > not exist until the first time this meta class is requested.
    > So getMetaClassOf would be the place to initialize the meta
    > class, that would register it in a global structure and on
    > subsequent invocation use that cached meta class. If two
    > threads execute the code above, then one would do the
    > initialization, while the other has to wait. The waiting
    > thread would then read the initialized global meta class. On
    > subsequent invocations both threads would just read. Since
    > changes of the meta class are rare, we would in 99% of all
    > cases simply read the existing value. Since we have to be
    > memory aware, these meta class can be unloaded at runtime
    > too. They are SoftReferenced so it is done only if really
    > needed. But rather than the normal change a reinitialization
    > might be needed much more often.
    >
    > As you see the user code "1+1" does contain zero
    > synchronization code.
    > The memory barriers are all in the runtime. It is not that
    > this cannot be solved by using what Java already has, it is
    > that this is too expensive.
    >
    > > As I mentioned, an alternative is to lay down some rules.
    > > If people stick to the rules they get consistent (in the sense of
    > > data-race-free) executions, else they might not. And of
    > such rules, I
    > > think the ones that can apply here amount to saying that
    > other threads
    > > performing initializations cannot trust any of their reads of the
    > > partially initialized object.
    > > And further, they cannot leak refs to that object outside
    > of the group
    > > of initializer threads.
    > >
    > > This is not hugely different than the Swing threading rules
    > >
    > (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
    > > but applies only during initialization.
    >
    > but unlike what the above may suggest there is no single
    > initialization phase. The meta classes are created on demand.
    > We cannot know beforehand which meta classes are needed and
    > doing them all before starting would increase the startup
    > time big times.
    >
    > If there were of course a way to recognize a partially
    > initialized object I could maybe think of something... but is
    > there a reliable one?
    >
    > bye blackdrag
    >
    > --
    > Jochen "blackdrag" Theodorou
    > The Groovy Project Tech Lead (http://groovy.codehaus.org)
    > http://blackdragsview.blogspot.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100427/8becb98b/attachment.html>

From Hans.Boehm at hp.com  Tue Apr 27 01:04:59 2010
From: Hans.Boehm at hp.com (Hans Boehm)
Date: Mon, 26 Apr 2010 22:04:59 -0700 (PDT)
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEBOIGAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEBOIGAA.davidcholmes@aapt.net.au>
Message-ID: <alpine.LNX.1.10.1004262148510.7807@192.168.2.2>

I think it's hard to talk about the issues here without being
somewhat architecture specific.

On X86 or SPARC TSO or similar, there no memory fence/barrier instruction 
is needed on the read side, even if volatiles are used.  Ever.  So I 
presume we're concerned about other architectures that do need them.

On those architectures, we might be able to reduce the cost of the
fence if we had something like acquire/release ordering.  We could
theoretically eliminate it if the accesses in question are dependent,
and the architecture enforces ordering based on dependencies.
I think that's usually how final field implementations work.
We currently don't really have either of those mechanisms.

I'm not sure why versioned references matter here.  It sounded to me like 
just switching an ordinary volatile reference to a newly allocated "meta 
class" would do the trick.  The issue is with the cost.

Hans

On Tue, 27 Apr 2010, David Holmes wrote:

> The problem with using any existing j.u.c class, or even thread-safe j.u
> class, is that the key issue here seems to be the desire to avoid the
> memory-barriers on the read side when they are not needed. This is to me
> a somewhat utopian goal - if we could tell when barriers were and were
> not needed we would only inject them when necessary (and of course the
> cost of determining this mustn't outweigh the cost of unconditionally
> using the memory-barrier in the first place). But all our existing Java
> library classes are correctly synchronized and have at a minimum volatile
> semantics on reads, which implies the need for memory-barriers in the
> general case.
> ?
> If a thread must see an update to the meta-class object then any read of
> a version would have to be exact - which implies it has to be correctly
> synchronized with respect to the updating thread, which implies a minimum
> of volatile semantics on the read.
> ?
> This seems to be a similar problem to the scalable-counter discussion. If
> the count is exact you need correct synchronization; without correct
> synchronization the count can't be exact. You have to pick which is more
> important.
> ?
> David Holmes
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Joe
> Bowbeer
> Sent: Tuesday, 27 April 2010 11:20 AM
> To: concurrency-interest
> Subject: Re: [concurrency-interest] when is safe publication safe?
>
>       I'm thinking of a versioned reference. ?Adding a new method
>       to an object would rev the version. ?A thread could
>       optimistically use a cached instance as long as the revision
>       matched. ?AtomicStampedReference can be used to associate a
>       version with a reference?
> 
> I'm just throwing out ideas in an effort to understand the problem
> better... ?Hope you don't mind.
> 
> 
> On Mon, Apr 26, 2010 at 2:02 PM, Boehm, Hans?wrote:
>       To clarify, hopefully:
>
>       The core problem here is presumably that
>       getMetaClassOf() reads a field, call it mc, containing
>       a reference to the meta class information, and
>       initialization of that field may race with the access.
>       ?The classic solution would be to declare mc
>       "volatile", which avoids the data race and (in the
>       absence of other data races) restores sequential
>       consistency, so you don't have to worry about
>       visibility issues. ?(If you do want to worry about
>       happens before ordering, it establishes the right
>       happens before ordering.)
>
>       The reason you may not be happy with this is that it's
>       slow on some architectures. ?However the performance
>       hit on X86 should be negligible, since an ordinary load
>       is sufficient to implement the volatile load. ?The
>       store will be more expensive, but that's rare. ?Thus
>       you are presumably concerned about non-X86
>       architectures?
>
>       It's a bit confusing to talk about memory barriers
>       since, as far as I know, the only Java class with
>       "barrier" in the name is CyclicBarrier, which is
>       something very different.
>
>       Hans
> 
> > -----Original Message-----
> > From: Jochen Theodorou
> > Sent: Sunday, April 25, 2010 8:49 AM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] when is safe
> publication safe?
> >
> > Doug Lea wrote:
> > > On 04/25/10 05:31, Jochen Theodorou wrote:
> > >>> As a first step, consider exactly what
> effects/semantics you want
> > >>> here, and the ways you intend people to be able to
> write
> > >>> conditionally correct Groovy code.
> > >>
> > >> People wouldn't have to write conditionally correct
> Groovy
> > code. they
> > >> would write normal code as they would in Java (Groovy
> and Java are
> > >> very near).
> > >
> > > It seems implausible that you could do enough analysis at
> load/run
> > > time to determine whether you need full locking in the
> presence of
> > > multithreaded racy initialization vs much cheaper release
> > fences. This
> > > would require at least some high-quality escape analysis.
> > And the code
> > > generated would differ both for the writing and reading
> callers.
> >
> > maybe I did explain it not good. Let us assume I have the
> Groovy code:
> >
> > 1+1
> >
> > Then this is really something along the lines of:
> >
> > SBA.getMetaClassOf(1).invoke("plus",1)
> >
> > and SBA.getMetaClassOf(1) would return the meta class of
> > Integer. Since this is purely a runtime construct, it does
> > not exist until the first time this meta class is
> requested.
> > So getMetaClassOf would be the place to initialize the meta
> > class, that would register it in a global structure and on
> > subsequent invocation use that cached meta class. If two
> > threads execute the code above, then one would do the
> > initialization, while the other has to wait. The waiting
> > thread would then read the initialized global meta class.
> On
> > subsequent invocations both threads would just read. Since
> > changes of the meta class are rare, we would in 99% of all
> > cases simply read the existing value. Since we have to be
> > memory aware, these meta class can be unloaded at runtime
> > too. They are SoftReferenced so it is done only if really
> > needed. But rather than the normal change a
> reinitialization
> > might be needed much more often.
> >
> > As you see the user code "1+1" does contain zero
> > synchronization code.
> > The memory barriers are all in the runtime. It is not that
> > this cannot be solved by using what Java already has, it is
> > that this is too expensive.
> >
> > > As I mentioned, an alternative is to lay down some rules.
> > > If people stick to the rules they get consistent (in the
> sense of
> > > data-race-free) executions, else they might not. And of
> > such rules, I
> > > think the ones that can apply here amount to saying that
> > other threads
> > > performing initializations cannot trust any of their
> reads of the
> > > partially initialized object.
> > > And further, they cannot leak refs to that object outside
> > of the group
> > > of initializer threads.
> > >
> > > This is not hugely different than the Swing threading
> rules
> > >
> >
> (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
> > > but applies only during initialization.
> >
> > but unlike what the above may suggest there is no single
> > initialization phase. The meta classes are created on
> demand.
> > We cannot know beforehand which meta classes are needed and
> > doing them all before starting would increase the startup
> > time big times.
> >
> > If there were of course a way to recognize a partially
> > initialized object I could maybe think of something... but
> is
> > there a reliable one?
> >
> > bye blackdrag
> >
> > --
> > Jochen "blackdrag" Theodorou
> > The Groovy Project Tech Lead (http://groovy.codehaus.org)
> > http://blackdragsview.blogspot.com/
> 
> 
>

From davidcholmes at aapt.net.au  Tue Apr 27 02:16:54 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 27 Apr 2010 16:16:54 +1000
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <alpine.LNX.1.10.1004262148510.7807@192.168.2.2>
Message-ID: <NFBBKALFDCPFIDBNKAPCGECCIGAA.davidcholmes@aapt.net.au>

Hans,

Not speaking for the OP but I assume he is looking for the most efficient
way to express this in Java regardless of platform. Right now all there is
is volatile. volatile comes at a cost that depends on the platform, but
there still seems to be a common impression that volatile is expensive -
period. The question is whether there is something weaker than volatile (aka
Fences and other related discussions) that will still give the desired
semantics while improving performance in at least some cases.

But it sounds to me that the semantics will actually require volatile's
guarantees.

Cheers,
David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans
> Boehm
> Sent: Tuesday, 27 April 2010 3:05 PM
> To: dholmes at ieee.org
> Cc: Joe Bowbeer; concurrency-interest
> Subject: Re: [concurrency-interest] when is safe publication safe?
>
>
> I think it's hard to talk about the issues here without being
> somewhat architecture specific.
>
> On X86 or SPARC TSO or similar, there no memory fence/barrier instruction
> is needed on the read side, even if volatiles are used.  Ever.  So I
> presume we're concerned about other architectures that do need them.
>
> On those architectures, we might be able to reduce the cost of the
> fence if we had something like acquire/release ordering.  We could
> theoretically eliminate it if the accesses in question are dependent,
> and the architecture enforces ordering based on dependencies.
> I think that's usually how final field implementations work.
> We currently don't really have either of those mechanisms.
>
> I'm not sure why versioned references matter here.  It sounded to me like
> just switching an ordinary volatile reference to a newly allocated "meta
> class" would do the trick.  The issue is with the cost.
>
> Hans
>
> On Tue, 27 Apr 2010, David Holmes wrote:
>
> > The problem with using any existing j.u.c class, or even thread-safe j.u
> > class, is that the key issue here seems to be the desire to avoid the
> > memory-barriers on the read side when they are not needed. This is to me
> > a somewhat utopian goal - if we could tell when barriers were and were
> > not needed we would only inject them when necessary (and of course the
> > cost of determining this mustn't outweigh the cost of unconditionally
> > using the memory-barrier in the first place). But all our existing Java
> > library classes are correctly synchronized and have at a
> minimum volatile
> > semantics on reads, which implies the need for memory-barriers in the
> > general case.
> > ?
> > If a thread must see an update to the meta-class object then any read of
> > a version would have to be exact - which implies it has to be correctly
> > synchronized with respect to the updating thread, which implies
> a minimum
> > of volatile semantics on the read.
> > ?
> > This seems to be a similar problem to the scalable-counter
> discussion. If
> > the count is exact you need correct synchronization; without correct
> > synchronization the count can't be exact. You have to pick which is more
> > important.
> > ?
> > David Holmes
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Joe
> > Bowbeer
> > Sent: Tuesday, 27 April 2010 11:20 AM
> > To: concurrency-interest
> > Subject: Re: [concurrency-interest] when is safe publication safe?
> >
> >       I'm thinking of a versioned reference. ?Adding a new method
> >       to an object would rev the version. ?A thread could
> >       optimistically use a cached instance as long as the revision
> >       matched. ?AtomicStampedReference can be used to associate a
> >       version with a reference?
> >
> > I'm just throwing out ideas in an effort to understand the problem
> > better... ?Hope you don't mind.
> >
> >
> > On Mon, Apr 26, 2010 at 2:02 PM, Boehm, Hans?wrote:
> >       To clarify, hopefully:
> >
> >       The core problem here is presumably that
> >       getMetaClassOf() reads a field, call it mc, containing
> >       a reference to the meta class information, and
> >       initialization of that field may race with the access.
> >       ?The classic solution would be to declare mc
> >       "volatile", which avoids the data race and (in the
> >       absence of other data races) restores sequential
> >       consistency, so you don't have to worry about
> >       visibility issues. ?(If you do want to worry about
> >       happens before ordering, it establishes the right
> >       happens before ordering.)
> >
> >       The reason you may not be happy with this is that it's
> >       slow on some architectures. ?However the performance
> >       hit on X86 should be negligible, since an ordinary load
> >       is sufficient to implement the volatile load. ?The
> >       store will be more expensive, but that's rare. ?Thus
> >       you are presumably concerned about non-X86
> >       architectures?
> >
> >       It's a bit confusing to talk about memory barriers
> >       since, as far as I know, the only Java class with
> >       "barrier" in the name is CyclicBarrier, which is
> >       something very different.
> >
> >       Hans
> >
> > > -----Original Message-----
> > > From: Jochen Theodorou
> > > Sent: Sunday, April 25, 2010 8:49 AM
> > > To: concurrency-interest at cs.oswego.edu
> > > Subject: Re: [concurrency-interest] when is safe
> > publication safe?
> > >
> > > Doug Lea wrote:
> > > > On 04/25/10 05:31, Jochen Theodorou wrote:
> > > >>> As a first step, consider exactly what
> > effects/semantics you want
> > > >>> here, and the ways you intend people to be able to
> > write
> > > >>> conditionally correct Groovy code.
> > > >>
> > > >> People wouldn't have to write conditionally correct
> > Groovy
> > > code. they
> > > >> would write normal code as they would in Java (Groovy
> > and Java are
> > > >> very near).
> > > >
> > > > It seems implausible that you could do enough analysis at
> > load/run
> > > > time to determine whether you need full locking in the
> > presence of
> > > > multithreaded racy initialization vs much cheaper release
> > > fences. This
> > > > would require at least some high-quality escape analysis.
> > > And the code
> > > > generated would differ both for the writing and reading
> > callers.
> > >
> > > maybe I did explain it not good. Let us assume I have the
> > Groovy code:
> > >
> > > 1+1
> > >
> > > Then this is really something along the lines of:
> > >
> > > SBA.getMetaClassOf(1).invoke("plus",1)
> > >
> > > and SBA.getMetaClassOf(1) would return the meta class of
> > > Integer. Since this is purely a runtime construct, it does
> > > not exist until the first time this meta class is
> > requested.
> > > So getMetaClassOf would be the place to initialize the meta
> > > class, that would register it in a global structure and on
> > > subsequent invocation use that cached meta class. If two
> > > threads execute the code above, then one would do the
> > > initialization, while the other has to wait. The waiting
> > > thread would then read the initialized global meta class.
> > On
> > > subsequent invocations both threads would just read. Since
> > > changes of the meta class are rare, we would in 99% of all
> > > cases simply read the existing value. Since we have to be
> > > memory aware, these meta class can be unloaded at runtime
> > > too. They are SoftReferenced so it is done only if really
> > > needed. But rather than the normal change a
> > reinitialization
> > > might be needed much more often.
> > >
> > > As you see the user code "1+1" does contain zero
> > > synchronization code.
> > > The memory barriers are all in the runtime. It is not that
> > > this cannot be solved by using what Java already has, it is
> > > that this is too expensive.
> > >
> > > > As I mentioned, an alternative is to lay down some rules.
> > > > If people stick to the rules they get consistent (in the
> > sense of
> > > > data-race-free) executions, else they might not. And of
> > > such rules, I
> > > > think the ones that can apply here amount to saying that
> > > other threads
> > > > performing initializations cannot trust any of their
> > reads of the
> > > > partially initialized object.
> > > > And further, they cannot leak refs to that object outside
> > > of the group
> > > > of initializer threads.
> > > >
> > > > This is not hugely different than the Swing threading
> > rules
> > > >
> > >
> > (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
> > > > but applies only during initialization.
> > >
> > > but unlike what the above may suggest there is no single
> > > initialization phase. The meta classes are created on
> > demand.
> > > We cannot know beforehand which meta classes are needed and
> > > doing them all before starting would increase the startup
> > > time big times.
> > >
> > > If there were of course a way to recognize a partially
> > > initialized object I could maybe think of something... but
> > is
> > > there a reliable one?
> > >
> > > bye blackdrag
> > >
> > > --
> > > Jochen "blackdrag" Theodorou
> > > The Groovy Project Tech Lead (http://groovy.codehaus.org)
> > > http://blackdragsview.blogspot.com/
> >
> >
> >



From joe.bowbeer at gmail.com  Tue Apr 27 02:54:06 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 26 Apr 2010 23:54:06 -0700
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGECCIGAA.davidcholmes@aapt.net.au>
References: <alpine.LNX.1.10.1004262148510.7807@192.168.2.2>
	<NFBBKALFDCPFIDBNKAPCGECCIGAA.davidcholmes@aapt.net.au>
Message-ID: <s2x31f2a7bd1004262354ic09327afyc43bd83a90d5e904@mail.gmail.com>

As long as we're speculating, my understanding is that the 20-30x slowdown
is not due to the cost of volatile reads but rather to the execution of
unoptimized byte codes.  That is, the required synchronization (e.g.,
volatile) is preventing hotspot from optimizing the methods.

This is why I was suggesting a stamped reference: hoping this transformation
would encourage hotspot to proceed. (But I have no idea if it will.)

Joe

On Mon, Apr 26, 2010 at 11:16 PM, David Holmes wrote:

> Hans,
>
> Not speaking for the OP but I assume he is looking for the most efficient
> way to express this in Java regardless of platform. Right now all there is
> is volatile. volatile comes at a cost that depends on the platform, but
> there still seems to be a common impression that volatile is expensive -
> period. The question is whether there is something weaker than volatile
> (aka
> Fences and other related discussions) that will still give the desired
> semantics while improving performance in at least some cases.
>
> But it sounds to me that the semantics will actually require volatile's
> guarantees.
>
> Cheers,
> David
>
> > -----Original Message-----
> > From:Hans Boehm
> > Sent: Tuesday, 27 April 2010 3:05 PM
> > To: concurrency-interest
> > Subject: Re: [concurrency-interest] when is safe publication safe?
> >
> >
> > I think it's hard to talk about the issues here without being
> > somewhat architecture specific.
> >
> > On X86 or SPARC TSO or similar, there no memory fence/barrier instruction
> > is needed on the read side, even if volatiles are used.  Ever.  So I
> > presume we're concerned about other architectures that do need them.
> >
> > On those architectures, we might be able to reduce the cost of the
> > fence if we had something like acquire/release ordering.  We could
> > theoretically eliminate it if the accesses in question are dependent,
> > and the architecture enforces ordering based on dependencies.
> > I think that's usually how final field implementations work.
> > We currently don't really have either of those mechanisms.
> >
> > I'm not sure why versioned references matter here.  It sounded to me like
> > just switching an ordinary volatile reference to a newly allocated "meta
> > class" would do the trick.  The issue is with the cost.
> >
> > Hans
> >
> > On Tue, 27 Apr 2010, David Holmes wrote:
> >
> > > The problem with using any existing j.u.c class, or even thread-safe
> j.u
> > > class, is that the key issue here seems to be the desire to avoid the
> > > memory-barriers on the read side when they are not needed. This is to
> me
> > > a somewhat utopian goal - if we could tell when barriers were and were
> > > not needed we would only inject them when necessary (and of course the
> > > cost of determining this mustn't outweigh the cost of unconditionally
> > > using the memory-barrier in the first place). But all our existing Java
> > > library classes are correctly synchronized and have at a
> > minimum volatile
> > > semantics on reads, which implies the need for memory-barriers in the
> > > general case.
> > >
> > > If a thread must see an update to the meta-class object then any read
> of
> > > a version would have to be exact - which implies it has to be correctly
> > > synchronized with respect to the updating thread, which implies
> > a minimum
> > > of volatile semantics on the read.
> > >
> > > This seems to be a similar problem to the scalable-counter
> > discussion. If
> > > the count is exact you need correct synchronization; without correct
> > > synchronization the count can't be exact. You have to pick which is
> more
> > > important.
> > >
> > > David Holmes
> > > -----Original Message-----
> > > From: concurrency-interest-bounces at cs.oswego.edu
> > > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Joe
> > > Bowbeer
> > > Sent: Tuesday, 27 April 2010 11:20 AM
> > > To: concurrency-interest
> > > Subject: Re: [concurrency-interest] when is safe publication safe?
> > >
> > >       I'm thinking of a versioned reference.  Adding a new method
> > >       to an object would rev the version.  A thread could
> > >       optimistically use a cached instance as long as the revision
> > >       matched.  AtomicStampedReference can be used to associate a
> > >       version with a reference?
> > >
> > > I'm just throwing out ideas in an effort to understand the problem
> > > better...  Hope you don't mind.
> > >
> > >
> > > On Mon, Apr 26, 2010 at 2:02 PM, Boehm, Hans wrote:
> > >       To clarify, hopefully:
> > >
> > >       The core problem here is presumably that
> > >       getMetaClassOf() reads a field, call it mc, containing
> > >       a reference to the meta class information, and
> > >       initialization of that field may race with the access.
> > >        The classic solution would be to declare mc
> > >       "volatile", which avoids the data race and (in the
> > >       absence of other data races) restores sequential
> > >       consistency, so you don't have to worry about
> > >       visibility issues.  (If you do want to worry about
> > >       happens before ordering, it establishes the right
> > >       happens before ordering.)
> > >
> > >       The reason you may not be happy with this is that it's
> > >       slow on some architectures.  However the performance
> > >       hit on X86 should be negligible, since an ordinary load
> > >       is sufficient to implement the volatile load.  The
> > >       store will be more expensive, but that's rare.  Thus
> > >       you are presumably concerned about non-X86
> > >       architectures?
> > >
> > >       It's a bit confusing to talk about memory barriers
> > >       since, as far as I know, the only Java class with
> > >       "barrier" in the name is CyclicBarrier, which is
> > >       something very different.
> > >
> > >       Hans
> > >
> > > > -----Original Message-----
> > > > From: Jochen Theodorou
> > > > Sent: Sunday, April 25, 2010 8:49 AM
> > > > To: concurrency-interest at cs.oswego.edu
> > > > Subject: Re: [concurrency-interest] when is safe
> > > publication safe?
> > > >
> > > > Doug Lea wrote:
> > > > > On 04/25/10 05:31, Jochen Theodorou wrote:
> > > > >>> As a first step, consider exactly what
> > > effects/semantics you want
> > > > >>> here, and the ways you intend people to be able to
> > > write
> > > > >>> conditionally correct Groovy code.
> > > > >>
> > > > >> People wouldn't have to write conditionally correct
> > > Groovy
> > > > code. they
> > > > >> would write normal code as they would in Java (Groovy
> > > and Java are
> > > > >> very near).
> > > > >
> > > > > It seems implausible that you could do enough analysis at
> > > load/run
> > > > > time to determine whether you need full locking in the
> > > presence of
> > > > > multithreaded racy initialization vs much cheaper release
> > > > fences. This
> > > > > would require at least some high-quality escape analysis.
> > > > And the code
> > > > > generated would differ both for the writing and reading
> > > callers.
> > > >
> > > > maybe I did explain it not good. Let us assume I have the
> > > Groovy code:
> > > >
> > > > 1+1
> > > >
> > > > Then this is really something along the lines of:
> > > >
> > > > SBA.getMetaClassOf(1).invoke("plus",1)
> > > >
> > > > and SBA.getMetaClassOf(1) would return the meta class of
> > > > Integer. Since this is purely a runtime construct, it does
> > > > not exist until the first time this meta class is
> > > requested.
> > > > So getMetaClassOf would be the place to initialize the meta
> > > > class, that would register it in a global structure and on
> > > > subsequent invocation use that cached meta class. If two
> > > > threads execute the code above, then one would do the
> > > > initialization, while the other has to wait. The waiting
> > > > thread would then read the initialized global meta class.
> > > On
> > > > subsequent invocations both threads would just read. Since
> > > > changes of the meta class are rare, we would in 99% of all
> > > > cases simply read the existing value. Since we have to be
> > > > memory aware, these meta class can be unloaded at runtime
> > > > too. They are SoftReferenced so it is done only if really
> > > > needed. But rather than the normal change a
> > > reinitialization
> > > > might be needed much more often.
> > > >
> > > > As you see the user code "1+1" does contain zero
> > > > synchronization code.
> > > > The memory barriers are all in the runtime. It is not that
> > > > this cannot be solved by using what Java already has, it is
> > > > that this is too expensive.
> > > >
> > > > > As I mentioned, an alternative is to lay down some rules.
> > > > > If people stick to the rules they get consistent (in the
> > > sense of
> > > > > data-race-free) executions, else they might not. And of
> > > > such rules, I
> > > > > think the ones that can apply here amount to saying that
> > > > other threads
> > > > > performing initializations cannot trust any of their
> > > reads of the
> > > > > partially initialized object.
> > > > > And further, they cannot leak refs to that object outside
> > > > of the group
> > > > > of initializer threads.
> > > > >
> > > > > This is not hugely different than the Swing threading
> > > rules
> > > > >
> > > >
> > > (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html)
> > > > > but applies only during initialization.
> > > >
> > > > but unlike what the above may suggest there is no single
> > > > initialization phase. The meta classes are created on
> > > demand.
> > > > We cannot know beforehand which meta classes are needed and
> > > > doing them all before starting would increase the startup
> > > > time big times.
> > > >
> > > > If there were of course a way to recognize a partially
> > > > initialized object I could maybe think of something... but
> > > is
> > > > there a reliable one?
> > > >
> > > > bye blackdrag
> > > >
> > > > --
> > > > Jochen "blackdrag" Theodorou
> > > > The Groovy Project Tech Lead (http://groovy.codehaus.org)
> > > > http://blackdragsview.blogspot.com/
> > >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100426/150c90b6/attachment.html>

From teck at terracottatech.com  Tue Apr 27 09:33:04 2010
From: teck at terracottatech.com (Tim Eck)
Date: Tue, 27 Apr 2010 06:33:04 -0700 (PDT)
Subject: [concurrency-interest] High performance clock/counter
In-Reply-To: <mailman.8.1272111576.3191.concurrency-interest@cs.oswego.edu>
References: <mailman.8.1272111576.3191.concurrency-interest@cs.oswego.edu>
Message-ID: <015401cae60e$2e7cc560$8b765020$@com>

Hi Peter -- Do you need a global counter, or are you really seeking some
sort of ever increasing unique ID generator? Depending on the exact
properties required, you can make certain tradeoffs. In particular if you
just need ID generation and can live with gap and order comparison only
within a given thread you can use the shared global counter to allocate a
range to a thread local (this is much like TLAB in the VM). 

> -----Original Message-----
> Date: Sat, 24 Apr 2010 00:10:56 +0200
> From: Peter Veentjer <alarmnummer at gmail.com>
> Subject: [concurrency-interest] High performance clock/counter
> To: concurrency-interest at cs.oswego.edu
> Message-ID:
> 	<q2h1466c1d61004231510z31978fcchdfa0e247b665f3e at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> High All,
> 
> I'm looking for a high performance counter to be used inside an STM.
> 
> Atm I use an AtomicLong, but if the number threads increase, the
> performance drops, example:
> 
> Thread count: 1
> Update transactions/second: 74,149,986.633
> Update transactions/second: 74,149,986.633 per core
> 
> Thread count: 2
> Update transactions/second: 22,864,414.996
> Update transactions/second: 11,432,207.498 per core
> 
> Thread count: 4
> Update transactions/second: 21,958,871.971
> Update transactions/second: 5,489,717.993 per core
> 
> Thread count: 8
> Update transactions/second: 25,207,393.631
> Update transactions/second: 3,150,924.204 per core
> 
> I have a machine with 8 cores.
> 
> I already use a 'relaxed' increment, it is allowed that concurrent
> increments on the clock,
> return the same value. So I'm using the following:
> 
> long writeVersion;
> if (strict) {
>     writeVersion = clock.incrementAndGet();
> } else {
>     long value = clock.get();
>     writeVersion = value+1;
>     clock.compareAndSet(value, writeVersion);
> }
> 
> The performance numbers above are creating using the strict = false.
> 
> I was also playing with the Counter of the high scale lib from Cliff
> Click,
> and the increment is amazingly fast (it gets faster the more cores I
> throw at it). But
> the big problem is that the increment doesn't return a value, and I
> need this
> to determine the writeversion of the transaction. If I do a
> counter.get after the
> increment, the performance drops far under the AtomicLong with the
> relaxed increment.
> 
> So does someone know about a library/approach to solve this situation?
> 
> 


From hans.boehm at hp.com  Tue Apr 27 13:45:04 2010
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 27 Apr 2010 17:45:04 +0000
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGECCIGAA.davidcholmes@aapt.net.au>
References: <alpine.LNX.1.10.1004262148510.7807@192.168.2.2>
	<NFBBKALFDCPFIDBNKAPCGECCIGAA.davidcholmes@aapt.net.au>
Message-ID: <238A96A773B3934685A7269CC8A8D0426DF6309392@GVW0436EXB.americas.hpqcorp.net>

Has someone actually measured the cost of volatile across platforms?
Since most of us are presumably on X86 platforms, I wonder whether
there are also implementation problems underlying some of these
perceptions.  On X86, a volatile load should differ from a normal
load only in that the compiler can no longer do some reordering,
and a volatile load will normally prevent reuse of previously loaded
values.  I'd expect this to be in the barely measurable category
for most applications.

A volatile store on X86 does require the use of either xchg or mfence,
both of which are slow (dozens of cycles?  more on P4).  But if you're
frequently writing to a shared volatile, you will end up taking lots of
coherence cache misses no matter what.  Thus I suspect it's fairly rare
for the fence overhead to really dominate everything else.  Doug has pointed
out some examples for which it's a major issue, so I don't want to
downplay this problem.  But it's clearly not an issue for read-mostly
applications like the one under discussion here.

Again the above only applies to X86, and probably SPARC TSO, which I believe
is very similar.  Other architectures do have more overhead on the read side.
(For Itanium, it should still only be on the order of half a dozen cycles.)

Hans

> -----Original Message-----
> From: David Holmes [mailto:davidcholmes at aapt.net.au] 
> Sent: Monday, April 26, 2010 11:17 PM
> To: Boehm, Hans
> Cc: concurrency-interest
> Subject: RE: [concurrency-interest] when is safe publication safe?
> 
> Hans,
> 
> Not speaking for the OP but I assume he is looking for the 
> most efficient way to express this in Java regardless of 
> platform. Right now all there is is volatile. volatile comes 
> at a cost that depends on the platform, but there still seems 
> to be a common impression that volatile is expensive - 
> period. The question is whether there is something weaker 
> than volatile (aka Fences and other related discussions) that 
> will still give the desired semantics while improving 
> performance in at least some cases.
> 
> But it sounds to me that the semantics will actually require 
> volatile's guarantees.
> 
> Cheers,
> David
> 
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On 
> Behalf Of Hans 
> > Boehm
> > Sent: Tuesday, 27 April 2010 3:05 PM
> > To: dholmes at ieee.org
> > Cc: Joe Bowbeer; concurrency-interest
> > Subject: Re: [concurrency-interest] when is safe publication safe?
> >
> >
> > I think it's hard to talk about the issues here without 
> being somewhat 
> > architecture specific.
> >
> > On X86 or SPARC TSO or similar, there no memory fence/barrier 
> > instruction is needed on the read side, even if volatiles 
> are used.  
> > Ever.  So I presume we're concerned about other 
> architectures that do need them.
> >
> > On those architectures, we might be able to reduce the cost of the 
> > fence if we had something like acquire/release ordering.  We could 
> > theoretically eliminate it if the accesses in question are 
> dependent, 
> > and the architecture enforces ordering based on dependencies.
> > I think that's usually how final field implementations work.
> > We currently don't really have either of those mechanisms.
> >
> > I'm not sure why versioned references matter here.  It 
> sounded to me 
> > like just switching an ordinary volatile reference to a newly 
> > allocated "meta class" would do the trick.  The issue is 
> with the cost.
> >
> > Hans
> >
> > On Tue, 27 Apr 2010, David Holmes wrote:
> >
> > > The problem with using any existing j.u.c class, or even 
> thread-safe 
> > > j.u class, is that the key issue here seems to be the desire to 
> > > avoid the memory-barriers on the read side when they are 
> not needed. 
> > > This is to me a somewhat utopian goal - if we could tell when 
> > > barriers were and were not needed we would only inject them when 
> > > necessary (and of course the cost of determining this mustn't 
> > > outweigh the cost of unconditionally using the 
> memory-barrier in the 
> > > first place). But all our existing Java library classes are 
> > > correctly synchronized and have at a
> > minimum volatile
> > > semantics on reads, which implies the need for memory-barriers in 
> > > the general case.
> > > ?
> > > If a thread must see an update to the meta-class object then any 
> > > read of a version would have to be exact - which implies 
> it has to 
> > > be correctly synchronized with respect to the updating 
> thread, which 
> > > implies
> > a minimum
> > > of volatile semantics on the read.
> > > ?
> > > This seems to be a similar problem to the scalable-counter
> > discussion. If
> > > the count is exact you need correct synchronization; 
> without correct 
> > > synchronization the count can't be exact. You have to 
> pick which is 
> > > more important.
> > > ?
> > > David Holmes
> > > -----Original Message-----
> > > From: concurrency-interest-bounces at cs.oswego.edu
> > > [mailto:concurrency-interest-bounces at cs.oswego.edu]On 
> Behalf Of Joe 
> > > Bowbeer
> > > Sent: Tuesday, 27 April 2010 11:20 AM
> > > To: concurrency-interest
> > > Subject: Re: [concurrency-interest] when is safe publication safe?
> > >
> > >       I'm thinking of a versioned reference. ?Adding a new method
> > >       to an object would rev the version. ?A thread could
> > >       optimistically use a cached instance as long as the revision
> > >       matched. ?AtomicStampedReference can be used to associate a
> > >       version with a reference?
> > >
> > > I'm just throwing out ideas in an effort to understand 
> the problem 
> > > better... ?Hope you don't mind.
> > >
> > >
> > > On Mon, Apr 26, 2010 at 2:02 PM, Boehm, Hans?wrote:
> > >       To clarify, hopefully:
> > >
> > >       The core problem here is presumably that
> > >       getMetaClassOf() reads a field, call it mc, containing
> > >       a reference to the meta class information, and
> > >       initialization of that field may race with the access.
> > >       ?The classic solution would be to declare mc
> > >       "volatile", which avoids the data race and (in the
> > >       absence of other data races) restores sequential
> > >       consistency, so you don't have to worry about
> > >       visibility issues. ?(If you do want to worry about
> > >       happens before ordering, it establishes the right
> > >       happens before ordering.)
> > >
> > >       The reason you may not be happy with this is that it's
> > >       slow on some architectures. ?However the performance
> > >       hit on X86 should be negligible, since an ordinary load
> > >       is sufficient to implement the volatile load. ?The
> > >       store will be more expensive, but that's rare. ?Thus
> > >       you are presumably concerned about non-X86
> > >       architectures?
> > >
> > >       It's a bit confusing to talk about memory barriers
> > >       since, as far as I know, the only Java class with
> > >       "barrier" in the name is CyclicBarrier, which is
> > >       something very different.
> > >
> > >       Hans
> > >
> > > > -----Original Message-----
> > > > From: Jochen Theodorou
> > > > Sent: Sunday, April 25, 2010 8:49 AM
> > > > To: concurrency-interest at cs.oswego.edu
> > > > Subject: Re: [concurrency-interest] when is safe
> > > publication safe?
> > > >
> > > > Doug Lea wrote:
> > > > > On 04/25/10 05:31, Jochen Theodorou wrote:
> > > > >>> As a first step, consider exactly what
> > > effects/semantics you want
> > > > >>> here, and the ways you intend people to be able to
> > > write
> > > > >>> conditionally correct Groovy code.
> > > > >>
> > > > >> People wouldn't have to write conditionally correct
> > > Groovy
> > > > code. they
> > > > >> would write normal code as they would in Java (Groovy
> > > and Java are
> > > > >> very near).
> > > > >
> > > > > It seems implausible that you could do enough analysis at
> > > load/run
> > > > > time to determine whether you need full locking in the
> > > presence of
> > > > > multithreaded racy initialization vs much cheaper release
> > > > fences. This
> > > > > would require at least some high-quality escape analysis.
> > > > And the code
> > > > > generated would differ both for the writing and reading
> > > callers.
> > > >
> > > > maybe I did explain it not good. Let us assume I have the
> > > Groovy code:
> > > >
> > > > 1+1
> > > >
> > > > Then this is really something along the lines of:
> > > >
> > > > SBA.getMetaClassOf(1).invoke("plus",1)
> > > >
> > > > and SBA.getMetaClassOf(1) would return the meta class 
> of Integer. 
> > > > Since this is purely a runtime construct, it does not 
> exist until 
> > > > the first time this meta class is
> > > requested.
> > > > So getMetaClassOf would be the place to initialize the 
> meta class, 
> > > > that would register it in a global structure and on subsequent 
> > > > invocation use that cached meta class. If two threads 
> execute the 
> > > > code above, then one would do the initialization, while 
> the other 
> > > > has to wait. The waiting thread would then read the initialized 
> > > > global meta class.
> > > On
> > > > subsequent invocations both threads would just read. 
> Since changes 
> > > > of the meta class are rare, we would in 99% of all cases simply 
> > > > read the existing value. Since we have to be memory 
> aware, these 
> > > > meta class can be unloaded at runtime too. They are 
> SoftReferenced 
> > > > so it is done only if really needed. But rather than the normal 
> > > > change a
> > > reinitialization
> > > > might be needed much more often.
> > > >
> > > > As you see the user code "1+1" does contain zero 
> synchronization 
> > > > code.
> > > > The memory barriers are all in the runtime. It is not that this 
> > > > cannot be solved by using what Java already has, it is 
> that this 
> > > > is too expensive.
> > > >
> > > > > As I mentioned, an alternative is to lay down some rules.
> > > > > If people stick to the rules they get consistent (in the
> > > sense of
> > > > > data-race-free) executions, else they might not. And of
> > > > such rules, I
> > > > > think the ones that can apply here amount to saying that
> > > > other threads
> > > > > performing initializations cannot trust any of their
> > > reads of the
> > > > > partially initialized object.
> > > > > And further, they cannot leak refs to that object outside
> > > > of the group
> > > > > of initializer threads.
> > > > >
> > > > > This is not hugely different than the Swing threading
> > > rules
> > > > >
> > > >
> > > 
> (http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html
> > > )
> > > > > but applies only during initialization.
> > > >
> > > > but unlike what the above may suggest there is no single 
> > > > initialization phase. The meta classes are created on
> > > demand.
> > > > We cannot know beforehand which meta classes are needed 
> and doing 
> > > > them all before starting would increase the startup time big 
> > > > times.
> > > >
> > > > If there were of course a way to recognize a partially 
> initialized 
> > > > object I could maybe think of something... but
> > > is
> > > > there a reliable one?
> > > >
> > > > bye blackdrag
> > > >
> > > > --
> > > > Jochen "blackdrag" Theodorou
> > > > The Groovy Project Tech Lead (http://groovy.codehaus.org) 
> > > > http://blackdragsview.blogspot.com/
> > >
> > >
> > >
> 
> 

From dl at cs.oswego.edu  Tue Apr 27 18:46:54 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 27 Apr 2010 18:46:54 -0400
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <238A96A773B3934685A7269CC8A8D0426DF6309392@GVW0436EXB.americas.hpqcorp.net>
References: <alpine.LNX.1.10.1004262148510.7807@192.168.2.2>	<NFBBKALFDCPFIDBNKAPCGECCIGAA.davidcholmes@aapt.net.au>
	<238A96A773B3934685A7269CC8A8D0426DF6309392@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4BD7695E.3010101@cs.oswego.edu>

On 04/27/10 13:45, Boehm, Hans wrote:
> Has someone actually measured the cost of volatile across platforms?

I don't know of a systematic survey, but here's a rough guide
across six multi-coreable/multi-processor-able processors that
together probably account for nearly all JVMs on MPs out there.
Each kind of processor differs enough across models that this
is only a very rough guide, not suitable for quoting or
programming against, but maybe useful for perspective.

For volatile reads, three categories:
* Usually Cheap: x86/x64, sparc
   -- no hw fences or special instructions but lost optimizations.
* May be Noticeable: IA64, Azul
   -- may need acquire fences/insns that require < 10 cycles.
* May be Expensive: POWER, ARM
   -- may need general fences that require > 10 cycles

For volatile writes:
* All of the above need at least a trailing store-load or
   use of atomics that normally costs in the dozens of cycles, but
   less so on some recent x86 and Azul.
* On Azul, Power, and ARM, volatile writes may additionally need a
   preceding release fence, that is in the < 10 cycles range.

All those "may's" stem from availability, in principle, of
compiler techniques that can lead to cheaper mechanisms in
special cases.

In general, fences and atomics have been getting cheaper over
the past few years, especially on x86. But just about any
processor designed around 2002-2007 is likely to have costs closer
to (or over) 100 cycles than 10 cycles. In other words, on
most platforms, fence (and atomic CAS) costs used to be more
worth avoiding than they have been recently. As Hans points out,
even the more expensive kinds of fences are often cheaper than
cache misses these days. On the other hand, the base cost of loss
of many basic compiler optimizations (storing values in registers
etc) associated with volatiles is sometimes more costly in the
long run than any fence costs.

-Doug



From dl at cs.oswego.edu  Tue Apr 27 19:26:38 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 27 Apr 2010 19:26:38 -0400
Subject: [concurrency-interest] when is safe publication safe?
In-Reply-To: <4BD7695E.3010101@cs.oswego.edu>
References: <alpine.LNX.1.10.1004262148510.7807@192.168.2.2>	<NFBBKALFDCPFIDBNKAPCGECCIGAA.davidcholmes@aapt.net.au>	<238A96A773B3934685A7269CC8A8D0426DF6309392@GVW0436EXB.americas.hpqcorp.net>
	<4BD7695E.3010101@cs.oswego.edu>
Message-ID: <4BD772AE.9070105@cs.oswego.edu>

A little follow-up to tie some of these discussions together a bit:

On 04/27/10 18:46, Doug Lea wrote:
> For volatile writes:
> * All of the above need at least a trailing store-load or
> use of atomics that normally costs in the dozens of cycles, but
> less so on some recent x86 and Azul.

Note: This is the fence you don't get with lazySet or
the Fences draft on release-fences. It is the most expensive
of all of fences on all platforms, so is the one that some people
would like to avoid when they are sure they don't need it.
But as always, offering this in a cheap and solidily spec'ed
form that is attractive to use only when it is correct remains
an unsolved problem.

-Doug



From aph at redhat.com  Thu Apr 29 11:57:02 2010
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Apr 2010 16:57:02 +0100
Subject: [concurrency-interest] High performance clock/counter
In-Reply-To: <D99EF7E1-1A04-41B0-8B33-8F90FAB13D33@oracle.com>
References: <q2h1466c1d61004231510z31978fcchdfa0e247b665f3e@mail.gmail.com>
	<4BD2D421.6000205@cs.oswego.edu> <4BD473D3.3010408@redhat.com>
	<D99EF7E1-1A04-41B0-8B33-8F90FAB13D33@oracle.com>
Message-ID: <4BD9AC4E.9010005@redhat.com>

On 04/25/2010 06:51 PM, Dave Dice wrote:
> 
> On 2010-4-25, at 12:54 PM, Andrew Haley wrote:
> 
>>
>> None of the counters suitable for use in an implementation of an
>> algorithm like TL2 are very nice: you either end up with high
>> contention on a single counter variable or something rather
>> heavyweight.  Would it make sense for the hardware to provide global
>> high-performance counters?  I'm thinking of a counter that could be
>> handled very efficiently by some sort of low-latency broadcast
>> protocol.
> 
> In TL2 you can use a relaxed clock that only increments
> infrequently, albeit with a slight cost in increased false failure
> rates.  The critical safety invariant is that any computed WV must
> be > any previously read (observed) RV.  Incrementing the global
> clock is actually a performance concern, not a correctness concern.
> See "GV6" in section 2.2 of
> http://people.csail.mit.edu/mareko/transact09-lev.pdf.  

Aha!  Yes, I see.

> We didn't have room to describe the alternative clock management
> mechanisms in the original DISC paper, although the TL2 release
> notes deal with that topic in depth.  The release notes also discuss
> using hardware clocks.

OK, thanks.

By the way, I can't find the reference implementation of TL2.  I'm
pretty sure I saw it some time ago but I can't find it any more,
despite some considerable web searching.

Andrew.

From alarmnummer at gmail.com  Fri Apr 30 11:49:13 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Fri, 30 Apr 2010 17:49:13 +0200
Subject: [concurrency-interest] Atomic get/set vs compareAndSet
Message-ID: <h2p1466c1d61004300849jb4196044w887653df7f4abd2a@mail.gmail.com>

I have a question about the contention costs on the memory bus of a
cas vs a get/set pair.

e.g. an example of a cas.

 @Override
    public void ___releaseLock(Transaction expectedLockOwner) {
        ___LOCKOWNER_UPDATER.compareAndSet(this, null,
expectedLockOwner, set(this, null);
    }


example of a get/set

 @Override
    public void ___releaseLock(Transaction expectedLockOwner) {
        if (___LOCKOWNER_UPDATER.get(this) == expectedLockOwner) {
            ___LOCKOWNER_UPDATER.set(this, null);
        }
    }

>From a logical point of view both provide the same semantics. In case
of the get/set it can't
happen that another thread changes the lock owner after it has been
locked. So I don't
need to worry about another thread updating the lock owner after the
get has returned
the expectedLockOwner (get/set essentially have become atomic).

My question is if the get/set combination causes less contention on
the memory bus
compared to a cas. So it is a common practice to replace cas by
get/sets if possible?

From alarmnummer at gmail.com  Fri Apr 30 11:52:25 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Fri, 30 Apr 2010 17:52:25 +0200
Subject: [concurrency-interest] Atomic get/set vs compareAndSet
In-Reply-To: <h2p1466c1d61004300849jb4196044w887653df7f4abd2a@mail.gmail.com>
References: <h2p1466c1d61004300849jb4196044w887653df7f4abd2a@mail.gmail.com>
Message-ID: <t2w1466c1d61004300852j41322754sdccd7a74eaa753e8@mail.gmail.com>

I made a copy paste error in the example I see. This is the good code.

public void ___releaseLock(Transaction expectedLockOwner) {
 ___LOCKOWNER_UPDATER.compareAndSet(this, expectedLockOwner, null);
}

example of a get/set

public void ___releaseLock(Transaction expectedLockOwner) {
if (___LOCKOWNER_UPDATER.get(this) == expectedLockOwner) {
     ___LOCKOWNER_UPDATER.set(this, null);
}
}


On Fri, Apr 30, 2010 at 5:49 PM, Peter Veentjer <alarmnummer at gmail.com> wrote:
> I have a question about the contention costs on the memory bus of a
> cas vs a get/set pair.
>
> e.g. an example of a cas.
>
> ?@Override
> ? ?public void ___releaseLock(Transaction expectedLockOwner) {
> ? ? ? ?___LOCKOWNER_UPDATER.compareAndSet(this, null,
> expectedLockOwner, set(this, null);
> ? ?}
>
>
> example of a get/set
>
> ?@Override
> ? ?public void ___releaseLock(Transaction expectedLockOwner) {
> ? ? ? ?if (___LOCKOWNER_UPDATER.get(this) == expectedLockOwner) {
> ? ? ? ? ? ?___LOCKOWNER_UPDATER.set(this, null);
> ? ? ? ?}
> ? ?}
>
> From a logical point of view both provide the same semantics. In case
> of the get/set it can't
> happen that another thread changes the lock owner after it has been
> locked. So I don't
> need to worry about another thread updating the lock owner after the
> get has returned
> the expectedLockOwner (get/set essentially have become atomic).
>
> My question is if the get/set combination causes less contention on
> the memory bus
> compared to a cas. So it is a common practice to replace cas by
> get/sets if possible?
>


From davidcholmes at aapt.net.au  Fri Apr 30 18:01:38 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 1 May 2010 08:01:38 +1000
Subject: [concurrency-interest] Atomic get/set vs compareAndSet
In-Reply-To: <t2w1466c1d61004300852j41322754sdccd7a74eaa753e8@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEDAIGAA.davidcholmes@aapt.net.au>

Peter,

> > My question is if the get/set combination causes less contention on
> > the memory bus compared to a cas. So it is a common practice to
> > replace cas by get/sets if possible?

The answer to the second part is a qualified yes. In terms of locking
schemes if you need a CAS to acquire and a CAS to release then it's a 1/1
scheme. A common goal is to have a scheme that supports 1/0 locking ie no
CAS on unlock only a simple store (plus any needed fences). Your example is
slightly different in that you have to read the lockowner field, which means
you need a load+store.

But to answer in terms of contention on the bus ... well that depends on a
lot of detail. You are using atomic operations which imply volatile
semantics (I assume) so there may be extra fences involved with the get/set
combination compared to the CAS - and the fences might hurt you more than
the CAS would. But it really depends on the exact machine code that will
execute and on the exact system you are on. This is way too low a level of
detail for me to discuss. I'd recommend checking Dave Dice's blogs:

http://blogs.sun.com/dave/

as they cover a lot of things in this area.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Veentjer
> Sent: Saturday, 1 May 2010 1:52 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Atomic get/set vs compareAndSet
>
>
> I made a copy paste error in the example I see. This is the good code.
>
> public void ___releaseLock(Transaction expectedLockOwner) {
>  ___LOCKOWNER_UPDATER.compareAndSet(this, expectedLockOwner, null);
> }
>
> example of a get/set
>
> public void ___releaseLock(Transaction expectedLockOwner) {
> if (___LOCKOWNER_UPDATER.get(this) == expectedLockOwner) {
>      ___LOCKOWNER_UPDATER.set(this, null);
> }
> }
>
>
> On Fri, Apr 30, 2010 at 5:49 PM, Peter Veentjer
> <alarmnummer at gmail.com> wrote:
> > I have a question about the contention costs on the memory bus of a
> > cas vs a get/set pair.
> >
> > e.g. an example of a cas.
> >
> > ?@Override
> > ? ?public void ___releaseLock(Transaction expectedLockOwner) {
> > ? ? ? ?___LOCKOWNER_UPDATER.compareAndSet(this, null,
> > expectedLockOwner, set(this, null);
> > ? ?}
> >
> >
> > example of a get/set
> >
> > ?@Override
> > ? ?public void ___releaseLock(Transaction expectedLockOwner) {
> > ? ? ? ?if (___LOCKOWNER_UPDATER.get(this) == expectedLockOwner) {
> > ? ? ? ? ? ?___LOCKOWNER_UPDATER.set(this, null);
> > ? ? ? ?}
> > ? ?}
> >
> > From a logical point of view both provide the same semantics. In case
> > of the get/set it can't
> > happen that another thread changes the lock owner after it has been
> > locked. So I don't
> > need to worry about another thread updating the lock owner after the
> > get has returned
> > the expectedLockOwner (get/set essentially have become atomic).
> >
> > My question is if the get/set combination causes less contention on
> > the memory bus
> > compared to a cas. So it is a common practice to replace cas by
> > get/sets if possible?
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



