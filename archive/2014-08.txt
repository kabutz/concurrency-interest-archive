From zhong.j.yu at gmail.com  Sun Aug  3 15:06:28 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sun, 3 Aug 2014 14:06:28 -0500
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <539BEAE3.5030006@ivanov.biz>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
Message-ID: <CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>

> Also, apparently, in heavy I/O scenarios, you may have a much better system
> throughput waiting for things to happen in I/O (blocking I/O) vs being
> notified of I/O events (Selector-based I/O):
> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6 years old
> and kernel/Java realities might have changed, YMMV, but the difference
> is(was?) impressive. Also, Apache HTTP Client still swears by blocking I/O
> vs non-blocking one in terms of efficiency:
> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore

To add a small data point to this discussion, Tomcat with NIO is
apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
request-response, according to a benchmark I did recently [1]. But!
The difference is very small, and I would argue that it is negligible.

Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
NIO) is not very meaningful for real applications. I did once
replicate his claim with a test that does nothing with the bytes being
transferred; but as soon as you at least read each byte once, the
throughput difference becomes very unimpressive (and frankly I suspect
it's largely due to Java's implementation of NIO).

[1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html

Zhong Yu
bayou.io

From oleksandr.otenko at oracle.com  Tue Aug  5 10:12:32 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 05 Aug 2014 15:12:32 +0100
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com>
	<539B8FEE.8030107@cs.oswego.edu>	<539B9CAE.3010705@sosnoski.com>	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>	<539BAA7A.1020007@sosnoski.com>
	<539BB459.2040600@ivanov.biz>	<539BD019.8090703@sosnoski.com>
	<539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
Message-ID: <53E0E650.1040408@oracle.com>

That still leaves everyone wonder why one would prefer to code in 
continuation-passing style compared to straightforward blocking IO. Both 
IO being on par is not reason enough to switch.

Alex

On 03/08/2014 20:06, Zhong Yu wrote:
>> Also, apparently, in heavy I/O scenarios, you may have a much better system
>> throughput waiting for things to happen in I/O (blocking I/O) vs being
>> notified of I/O events (Selector-based I/O):
>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6 years old
>> and kernel/Java realities might have changed, YMMV, but the difference
>> is(was?) impressive. Also, Apache HTTP Client still swears by blocking I/O
>> vs non-blocking one in terms of efficiency:
>> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
> To add a small data point to this discussion, Tomcat with NIO is
> apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
> request-response, according to a benchmark I did recently [1]. But!
> The difference is very small, and I would argue that it is negligible.
>
> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
> NIO) is not very meaningful for real applications. I did once
> replicate his claim with a test that does nothing with the bytes being
> transferred; but as soon as you at least read each byte once, the
> throughput difference becomes very unimpressive (and frankly I suspect
> it's largely due to Java's implementation of NIO).
>
> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>
> Zhong Yu
> bayou.io
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From david.lloyd at redhat.com  Tue Aug  5 10:40:13 2014
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Tue, 05 Aug 2014 09:40:13 -0500
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <53E0E650.1040408@oracle.com>
References: <539B8AD4.8090702@sosnoski.com>	<539B8FEE.8030107@cs.oswego.edu>	<539B9CAE.3010705@sosnoski.com>	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>	<539BAA7A.1020007@sosnoski.com>	<539BB459.2040600@ivanov.biz>	<539BD019.8090703@sosnoski.com>	<539BEAE3.5030006@ivanov.biz>	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
Message-ID: <53E0ECCD.50001@redhat.com>

I don't have updated benchmarks, but we researched this extensively and 
found that for server applications, and with clean, optimized 
implementations, non-blocking I/O has a large advantave, especially with 
1,000s+ connections and even with blocking workers (i.e. servlets). 
Tomcat is (please forgive me for saying it) not really a good example of 
*anything* beyond "how people used to code back in 1999".

Take this with a BIG grain of salt, but this might give you an idea as 
to the relative performance status quo of various approaches and 
languages in terms of I/O:

http://www.techempower.com/benchmarks/

Relying on papers from a decade ago, and software from even longer ago, 
is probably even less useful than one might expect.  CPU architectures, 
kernels, Java itself, and the knowledge and experience of those writing 
Java software, have come a very, very long way since then.

On 08/05/2014 09:12 AM, Oleksandr Otenko wrote:
> That still leaves everyone wonder why one would prefer to code in
> continuation-passing style compared to straightforward blocking IO. Both
> IO being on par is not reason enough to switch.
>
> Alex
>
> On 03/08/2014 20:06, Zhong Yu wrote:
>>> Also, apparently, in heavy I/O scenarios, you may have a much better
>>> system
>>> throughput waiting for things to happen in I/O (blocking I/O) vs being
>>> notified of I/O events (Selector-based I/O):
>>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6 years
>>> old
>>> and kernel/Java realities might have changed, YMMV, but the difference
>>> is(was?) impressive. Also, Apache HTTP Client still swears by
>>> blocking I/O
>>> vs non-blocking one in terms of efficiency:
>>> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>> To add a small data point to this discussion, Tomcat with NIO is
>> apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
>> request-response, according to a benchmark I did recently [1]. But!
>> The difference is very small, and I would argue that it is negligible.
>>
>> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
>> NIO) is not very meaningful for real applications. I did once
>> replicate his claim with a test that does nothing with the bytes being
>> transferred; but as soon as you at least read each byte once, the
>> throughput difference becomes very unimpressive (and frankly I suspect
>> it's largely due to Java's implementation of NIO).
>>
>> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>>
>> Zhong Yu
>> bayou.io
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
- DML

From zhong.j.yu at gmail.com  Tue Aug  5 13:10:33 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 5 Aug 2014 12:10:33 -0500
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <53E0ECCD.50001@redhat.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com> <53E0ECCD.50001@redhat.com>
Message-ID: <CACuKZqGFTP7SyrQXUx=G3wQnxPm+29CQ2_JHM_iqq44A-vG-fg@mail.gmail.com>

On Tue, Aug 5, 2014 at 9:40 AM, David M. Lloyd <david.lloyd at redhat.com> wrote:
> I don't have updated benchmarks, but we researched this extensively and
> found that for server applications, and with clean, optimized
> implementations, non-blocking I/O has a large advantave, especially with
> 1,000s+ connections

I forgot to emphasize that my test was for a single, half-duplex
connection, designed to compare how much self-time each server impl
spends on a request-response cycle.

> and even with blocking workers (i.e. servlets). Tomcat
> is (please forgive me for saying it) not really a good example of *anything*
> beyond "how people used to code back in 1999".
>
> Take this with a BIG grain of salt, but this might give you an idea as to
> the relative performance status quo of various approaches and languages in
> terms of I/O:
>
> http://www.techempower.com/benchmarks/
>
> Relying on papers from a decade ago, and software from even longer ago, is
> probably even less useful than one might expect.  CPU architectures,
> kernels, Java itself, and the knowledge and experience of those writing Java
> software, have come a very, very long way since then.
>
>
> On 08/05/2014 09:12 AM, Oleksandr Otenko wrote:
>>
>> That still leaves everyone wonder why one would prefer to code in
>> continuation-passing style compared to straightforward blocking IO. Both
>> IO being on par is not reason enough to switch.
>>
>> Alex
>>
>> On 03/08/2014 20:06, Zhong Yu wrote:
>>>>
>>>> Also, apparently, in heavy I/O scenarios, you may have a much better
>>>> system
>>>> throughput waiting for things to happen in I/O (blocking I/O) vs being
>>>> notified of I/O events (Selector-based I/O):
>>>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6 years
>>>> old
>>>> and kernel/Java realities might have changed, YMMV, but the difference
>>>> is(was?) impressive. Also, Apache HTTP Client still swears by
>>>> blocking I/O
>>>> vs non-blocking one in terms of efficiency:
>>>> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>>>
>>> To add a small data point to this discussion, Tomcat with NIO is
>>> apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
>>> request-response, according to a benchmark I did recently [1]. But!
>>> The difference is very small, and I would argue that it is negligible.
>>>
>>> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
>>> NIO) is not very meaningful for real applications. I did once
>>> replicate his claim with a test that does nothing with the bytes being
>>> transferred; but as soon as you at least read each byte once, the
>>> throughput difference becomes very unimpressive (and frankly I suspect
>>> it's largely due to Java's implementation of NIO).
>>>
>>> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>>>
>>> Zhong Yu
>>> bayou.io
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> --
> - DML
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From zhong.j.yu at gmail.com  Tue Aug  5 13:59:49 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 5 Aug 2014 12:59:49 -0500
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <53E0E650.1040408@oracle.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
Message-ID: <CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>

On Tue, Aug 5, 2014 at 9:12 AM, Oleksandr Otenko
<oleksandr.otenko at oracle.com> wrote:
> That still leaves everyone wonder why one would prefer to code in
> continuation-passing style compared to straightforward blocking IO. Both IO
> being on par is not reason enough to switch.

The async-everywhere fad is brought by nodejs, which really has no
other choice, because javascript is single threaded.

On other platforms like Java, I believe that, in most use cases, the
*application code* that reads a request and generates the response
should be synchronous/blocking/threaded. The code usually involves
blocking IOs with internal components like DB server and file system
which are fast and predictable. The page views per second per machine
is often at single-digit, according to reports from companies like
stackoverflow and facebook. Even if the application needs to handle
thousands of requests/second/machine, we are only talking about a few
hundred application threads at any time.

However, the code of HTTP stack is preferred to be async, because
awaiting requests, reading requests, and writing responses can often
be affected by slow clients. Async-ness in HTTP stack reduces overhead
without imposing complexity to the application code.

There's a dilemma though - if the application code is writing bytes to
the response body with blocking write(), isn't it tying up a thread if
the client is slow? And if the write() is non-blocking, aren't we
buffering up too much data? I think this problem can often be solved
by a non-blocking write(obj) that buffers `obj` with lazy
serialization, see "optimization technique" in
http://bayou.io/draft/response.style.html#Response_body

Zhong Yu
bayou.io


>
> Alex
>
>
> On 03/08/2014 20:06, Zhong Yu wrote:
>>>
>>> Also, apparently, in heavy I/O scenarios, you may have a much better
>>> system
>>> throughput waiting for things to happen in I/O (blocking I/O) vs being
>>> notified of I/O events (Selector-based I/O):
>>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6 years old
>>> and kernel/Java realities might have changed, YMMV, but the difference
>>> is(was?) impressive. Also, Apache HTTP Client still swears by blocking
>>> I/O
>>> vs non-blocking one in terms of efficiency:
>>> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>>
>> To add a small data point to this discussion, Tomcat with NIO is
>> apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
>> request-response, according to a benchmark I did recently [1]. But!
>> The difference is very small, and I would argue that it is negligible.
>>
>> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
>> NIO) is not very meaningful for real applications. I did once
>> replicate his claim with a test that does nothing with the bytes being
>> transferred; but as soon as you at least read each byte once, the
>> throughput difference becomes very unimpressive (and frankly I suspect
>> it's largely due to Java's implementation of NIO).
>>
>> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>>
>> Zhong Yu
>> bayou.io
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From stanimir at riflexo.com  Tue Aug  5 19:31:35 2014
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 6 Aug 2014 02:31:35 +0300
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <53E0E650.1040408@oracle.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
Message-ID: <CAEJX8oqwCBCvTtDsJVXtFgzZQs0o_jRKtYcs6dPBTz-Lc-Qh_A@mail.gmail.com>

On Tue, Aug 5, 2014 at 5:12 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> That still leaves everyone wonder why one would prefer to code in
> continuation-passing style compared to straightforward blocking IO. Both IO
> being on par is not reason enough to switch.
>
> With thousands of persistent full duplex connections the non-blocking IO
offers better latency and control under load...or at least that's my
experience in server workloads.

Tomcat is not a good example of good NIO code and features some doubtful
choices like using CLQ for small object caches (the CLQ node being larger
than the cached object), so I'd leave it at that.

Stanimir



> Alex
>
>
> On 03/08/2014 20:06, Zhong Yu wrote:
>
>> Also, apparently, in heavy I/O scenarios, you may have a much better
>>> system
>>> throughput waiting for things to happen in I/O (blocking I/O) vs being
>>> notified of I/O events (Selector-based I/O):
>>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6 years
>>> old
>>> and kernel/Java realities might have changed, YMMV, but the difference
>>> is(was?) impressive. Also, Apache HTTP Client still swears by blocking
>>> I/O
>>> vs non-blocking one in terms of efficiency:
>>> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>>>
>> To add a small data point to this discussion, Tomcat with NIO is
>> apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
>> request-response, according to a benchmark I did recently [1]. But!
>> The difference is very small, and I would argue that it is negligible.
>>
>> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
>> NIO) is not very meaningful for real applications. I did once
>> replicate his claim with a test that does nothing with the bytes being
>> transferred; but as soon as you at least read each byte once, the
>> throughput difference becomes very unimpressive (and frankly I suspect
>> it's largely due to Java's implementation of NIO).
>>
>> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>>
>> Zhong Yu
>> bayou.io
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140806/edcf670d/attachment.html>

From stanimir at riflexo.com  Tue Aug  5 19:41:32 2014
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 6 Aug 2014 02:41:32 +0300
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
Message-ID: <CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>

>
> There's a dilemma though - if the application code is writing bytes to
> the response body with blocking write(), isn't it tying up a thread if
> the client is slow? And if the write() is non-blocking, aren't we
> buffering up too much data? I think this problem can often be solved
> by a non-blocking write(obj) that buffers `obj` with lazy
> serialization, see "optimization technique" in
> http://bayou.io/draft/response.style.html#Response_body
>
> Zhong Yu
>

The lazy serialization unfortunately requires the object to be fully
fetched (not depending on any context or an active database connection)
which is not that different than "buffering too much" - it's just not plain
ByteBuffer (or byte[]).
Personally I don't like lazy serialization as that leaves objects in the
queues and the latter may have implications of module (classes) redeploys
with slow clients. Also it makes a lot hard quantifying the expected queue
length per connection and shutting down slow connection.

Stanimir



>
> >
> > Alex
> >
> >
> > On 03/08/2014 20:06, Zhong Yu wrote:
> >>>
> >>> Also, apparently, in heavy I/O scenarios, you may have a much better
> >>> system
> >>> throughput waiting for things to happen in I/O (blocking I/O) vs being
> >>> notified of I/O events (Selector-based I/O):
> >>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6 years
> old
> >>> and kernel/Java realities might have changed, YMMV, but the difference
> >>> is(was?) impressive. Also, Apache HTTP Client still swears by blocking
> >>> I/O
> >>> vs non-blocking one in terms of efficiency:
> >>>
> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
> >>
> >> To add a small data point to this discussion, Tomcat with NIO is
> >> apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
> >> request-response, according to a benchmark I did recently [1]. But!
> >> The difference is very small, and I would argue that it is negligible.
> >>
> >> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
> >> NIO) is not very meaningful for real applications. I did once
> >> replicate his claim with a test that does nothing with the bytes being
> >> transferred; but as soon as you at least read each byte once, the
> >> throughput difference becomes very unimpressive (and frankly I suspect
> >> it's largely due to Java's implementation of NIO).
> >>
> >> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
> >>
> >> Zhong Yu
> >> bayou.io
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140806/3b798f97/attachment-0001.html>

From zhong.j.yu at gmail.com  Tue Aug  5 19:51:55 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 5 Aug 2014 18:51:55 -0500
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
Message-ID: <CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>

On Tue, Aug 5, 2014 at 6:41 PM, Stanimir Simeonoff <stanimir at riflexo.com> wrote:
>
>
>
>
>
>>
>> There's a dilemma though - if the application code is writing bytes to
>> the response body with blocking write(), isn't it tying up a thread if
>> the client is slow? And if the write() is non-blocking, aren't we
>> buffering up too much data? I think this problem can often be solved
>> by a non-blocking write(obj) that buffers `obj` with lazy
>> serialization, see "optimization technique" in
>> http://bayou.io/draft/response.style.html#Response_body
>>
>> Zhong Yu
>
>
> The lazy serialization unfortunately requires the object to be fully fetched
> (not depending on any context or an active database connection) which is not
> that different than "buffering too much" - it's just not plain ByteBuffer

There's a difference if the objects are shared among responses which
is a reasonable assumption for a lot of web applications.

> (or byte[]).
> Personally I don't like lazy serialization as that leaves objects in the
> queues and the latter may have implications of module (classes) redeploys
> with slow clients. Also it makes a lot hard quantifying the expected queue
> length per connection and shutting down slow connection.
>
> Stanimir
>
>
>>
>>
>> >
>> > Alex
>> >
>> >
>> > On 03/08/2014 20:06, Zhong Yu wrote:
>> >>>
>> >>> Also, apparently, in heavy I/O scenarios, you may have a much better
>> >>> system
>> >>> throughput waiting for things to happen in I/O (blocking I/O) vs being
>> >>> notified of I/O events (Selector-based I/O):
>> >>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6 years
>> >>> old
>> >>> and kernel/Java realities might have changed, YMMV, but the difference
>> >>> is(was?) impressive. Also, Apache HTTP Client still swears by blocking
>> >>> I/O
>> >>> vs non-blocking one in terms of efficiency:
>> >>>
>> >>> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>> >>
>> >> To add a small data point to this discussion, Tomcat with NIO is
>> >> apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
>> >> request-response, according to a benchmark I did recently [1]. But!
>> >> The difference is very small, and I would argue that it is negligible.
>> >>
>> >> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
>> >> NIO) is not very meaningful for real applications. I did once
>> >> replicate his claim with a test that does nothing with the bytes being
>> >> transferred; but as soon as you at least read each byte once, the
>> >> throughput difference becomes very unimpressive (and frankly I suspect
>> >> it's largely due to Java's implementation of NIO).
>> >>
>> >> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>> >>
>> >> Zhong Yu
>> >> bayou.io
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From dt at flyingtroika.com  Wed Aug  6 12:16:01 2014
From: dt at flyingtroika.com (DT)
Date: Wed, 06 Aug 2014 09:16:01 -0700
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com>
	<539B8FEE.8030107@cs.oswego.edu>	<539B9CAE.3010705@sosnoski.com>	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>	<539BAA7A.1020007@sosnoski.com>
	<539BB459.2040600@ivanov.biz>	<539BD019.8090703@sosnoski.com>
	<539BEAE3.5030006@ivanov.biz>	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>	<53E0E650.1040408@oracle.com>	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
Message-ID: <53E254C1.4060501@flyingtroika.com>

We have done multiple experiments in respect to nio and io java APIs and 
we have not seen that much improvement in throughput or latencies with NIO.
Got almost the same stats for udp , tcp and http based packets (running 
on windows and linux platforms). Though we noticed that the more traffic 
we handle the better results we got with NIO implementation in terms of 
latencies and overall throughput of the application (there is some sort 
of threshold when system starts reacting better). The idea was to move 
to java NIO APIs but due to the results we decided to make some more 
research. Its difficult to make a benchmark just because even a small 
change in linux kernel/nic can lead to different results. When we 
converted java logic into C++/C code and used linux non blocking/event 
based calls we have got much better optimization/performance. Good 
example is to compare nginx socket event module and Java NIO APIs.  
Probably we shoud not compare java non-blocking calls to c/c++ calls and 
implementation though I thought its a good idea to get a benchmark this way.

Thanks,
DT

On 8/5/2014 4:51 PM, Zhong Yu wrote:
> On Tue, Aug 5, 2014 at 6:41 PM, Stanimir Simeonoff <stanimir at riflexo.com> wrote:
>>
>>
>>
>>
>>> There's a dilemma though - if the application code is writing bytes to
>>> the response body with blocking write(), isn't it tying up a thread if
>>> the client is slow? And if the write() is non-blocking, aren't we
>>> buffering up too much data? I think this problem can often be solved
>>> by a non-blocking write(obj) that buffers `obj` with lazy
>>> serialization, see "optimization technique" in
>>> http://bayou.io/draft/response.style.html#Response_body
>>>
>>> Zhong Yu
>>
>> The lazy serialization unfortunately requires the object to be fully fetched
>> (not depending on any context or an active database connection) which is not
>> that different than "buffering too much" - it's just not plain ByteBuffer
> There's a difference if the objects are shared among responses which
> is a reasonable assumption for a lot of web applications.
>
>> (or byte[]).
>> Personally I don't like lazy serialization as that leaves objects in the
>> queues and the latter may have implications of module (classes) redeploys
>> with slow clients. Also it makes a lot hard quantifying the expected queue
>> length per connection and shutting down slow connection.
>>
>> Stanimir
>>
>>
>>>
>>>> Alex
>>>>
>>>>
>>>> On 03/08/2014 20:06, Zhong Yu wrote:
>>>>>> Also, apparently, in heavy I/O scenarios, you may have a much better
>>>>>> system
>>>>>> throughput waiting for things to happen in I/O (blocking I/O) vs being
>>>>>> notified of I/O events (Selector-based I/O):
>>>>>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6 years
>>>>>> old
>>>>>> and kernel/Java realities might have changed, YMMV, but the difference
>>>>>> is(was?) impressive. Also, Apache HTTP Client still swears by blocking
>>>>>> I/O
>>>>>> vs non-blocking one in terms of efficiency:
>>>>>>
>>>>>> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>>>>> To add a small data point to this discussion, Tomcat with NIO is
>>>>> apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
>>>>> request-response, according to a benchmark I did recently [1]. But!
>>>>> The difference is very small, and I would argue that it is negligible.
>>>>>
>>>>> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
>>>>> NIO) is not very meaningful for real applications. I did once
>>>>> replicate his claim with a test that does nothing with the bytes being
>>>>> transferred; but as soon as you at least read each byte once, the
>>>>> throughput difference becomes very unimpressive (and frankly I suspect
>>>>> it's largely due to Java's implementation of NIO).
>>>>>
>>>>> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>>>>>
>>>>> Zhong Yu
>>>>> bayou.io
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Wed Aug  6 13:14:35 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 6 Aug 2014 13:14:35 -0400
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <53E254C1.4060501@flyingtroika.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
	<53E254C1.4060501@flyingtroika.com>
Message-ID: <CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>

I don't think the point of NIO (or event-based i/o in general) is to have
better absolute latency/throughput in all cases than blocking i/o.
 Instead, it's really intended for being able to scale to tens (and
hundreds) of thousands of concurrent (and mostly idle at a point in time)
connections on a single server.  This makes intuitive sense since pretty
much just one core dedicated to doing i/o can saturate a NIC (given
sufficient read/write workload); the rest of the compute resources can be
dedicated to the CPU bound workload.  Creating a thread-per-connection in
those circumstances either doesn't make sense or simply won't work at all.


On Wed, Aug 6, 2014 at 12:16 PM, DT <dt at flyingtroika.com> wrote:

> We have done multiple experiments in respect to nio and io java APIs and
> we have not seen that much improvement in throughput or latencies with NIO.
> Got almost the same stats for udp , tcp and http based packets (running on
> windows and linux platforms). Though we noticed that the more traffic we
> handle the better results we got with NIO implementation in terms of
> latencies and overall throughput of the application (there is some sort of
> threshold when system starts reacting better). The idea was to move to java
> NIO APIs but due to the results we decided to make some more research. Its
> difficult to make a benchmark just because even a small change in linux
> kernel/nic can lead to different results. When we converted java logic into
> C++/C code and used linux non blocking/event based calls we have got much
> better optimization/performance. Good example is to compare nginx socket
> event module and Java NIO APIs.  Probably we shoud not compare java
> non-blocking calls to c/c++ calls and implementation though I thought its a
> good idea to get a benchmark this way.
>
> Thanks,
> DT
>
> On 8/5/2014 4:51 PM, Zhong Yu wrote:
>
>> On Tue, Aug 5, 2014 at 6:41 PM, Stanimir Simeonoff <stanimir at riflexo.com>
>> wrote:
>>
>>>
>>>
>>>
>>>
>>>  There's a dilemma though - if the application code is writing bytes to
>>>> the response body with blocking write(), isn't it tying up a thread if
>>>> the client is slow? And if the write() is non-blocking, aren't we
>>>> buffering up too much data? I think this problem can often be solved
>>>> by a non-blocking write(obj) that buffers `obj` with lazy
>>>> serialization, see "optimization technique" in
>>>> http://bayou.io/draft/response.style.html#Response_body
>>>>
>>>> Zhong Yu
>>>>
>>>
>>> The lazy serialization unfortunately requires the object to be fully
>>> fetched
>>> (not depending on any context or an active database connection) which is
>>> not
>>> that different than "buffering too much" - it's just not plain ByteBuffer
>>>
>> There's a difference if the objects are shared among responses which
>> is a reasonable assumption for a lot of web applications.
>>
>>  (or byte[]).
>>> Personally I don't like lazy serialization as that leaves objects in the
>>> queues and the latter may have implications of module (classes) redeploys
>>> with slow clients. Also it makes a lot hard quantifying the expected
>>> queue
>>> length per connection and shutting down slow connection.
>>>
>>> Stanimir
>>>
>>>
>>>
>>>>  Alex
>>>>>
>>>>>
>>>>> On 03/08/2014 20:06, Zhong Yu wrote:
>>>>>
>>>>>> Also, apparently, in heavy I/O scenarios, you may have a much better
>>>>>>> system
>>>>>>> throughput waiting for things to happen in I/O (blocking I/O) vs
>>>>>>> being
>>>>>>> notified of I/O events (Selector-based I/O):
>>>>>>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6
>>>>>>> years
>>>>>>> old
>>>>>>> and kernel/Java realities might have changed, YMMV, but the
>>>>>>> difference
>>>>>>> is(was?) impressive. Also, Apache HTTP Client still swears by
>>>>>>> blocking
>>>>>>> I/O
>>>>>>> vs non-blocking one in terms of efficiency:
>>>>>>>
>>>>>>> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttp
>>>>>>> Core
>>>>>>>
>>>>>> To add a small data point to this discussion, Tomcat with NIO is
>>>>>> apparently slower than Tomcat with Blocking-IO by 1,700ns for a simple
>>>>>> request-response, according to a benchmark I did recently [1]. But!
>>>>>> The difference is very small, and I would argue that it is negligible.
>>>>>>
>>>>>> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more than
>>>>>> NIO) is not very meaningful for real applications. I did once
>>>>>> replicate his claim with a test that does nothing with the bytes being
>>>>>> transferred; but as soon as you at least read each byte once, the
>>>>>> throughput difference becomes very unimpressive (and frankly I suspect
>>>>>> it's largely due to Java's implementation of NIO).
>>>>>>
>>>>>> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>>>>>>
>>>>>> Zhong Yu
>>>>>> bayou.io
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>>  _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>  _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140806/e983e419/attachment.html>

From oleksandr.otenko at oracle.com  Wed Aug  6 14:53:16 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 06 Aug 2014 19:53:16 +0100
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com>
	<539B8FEE.8030107@cs.oswego.edu>	<539B9CAE.3010705@sosnoski.com>	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>	<539BAA7A.1020007@sosnoski.com>
	<539BB459.2040600@ivanov.biz>	<539BD019.8090703@sosnoski.com>
	<539BEAE3.5030006@ivanov.biz>	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>	<53E0E650.1040408@oracle.com>	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>	<53E254C1.4060501@flyingtroika.com>
	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
Message-ID: <53E2799C.2070904@oracle.com>

But it's contagious. It is difficult to combine data push (event-based 
IO) and data pull (blocking IO) in the same program.

If you have tens of thousands connections, but computation cannot make 
progress without reading complete request, you still need to either 
switch to blocking IO, or switch to capturing the state of the 
computation (even if it's only the "bytes read so far") in some way. I 
don't see what we are meant to save by not using threads, which capture 
some of the state on the stack, and which offers language support 
(try/finally, synchronized, error propagation, etc).


Alex


On 06/08/2014 18:14, Vitaly Davidovich wrote:
> I don't think the point of NIO (or event-based i/o in general) is to 
> have better absolute latency/throughput in all cases than blocking 
> i/o.  Instead, it's really intended for being able to scale to tens 
> (and hundreds) of thousands of concurrent (and mostly idle at a point 
> in time) connections on a single server.  This makes intuitive sense 
> since pretty much just one core dedicated to doing i/o can saturate a 
> NIC (given sufficient read/write workload); the rest of the compute 
> resources can be dedicated to the CPU bound workload.  Creating a 
> thread-per-connection in those circumstances either doesn't make sense 
> or simply won't work at all.
>
>
> On Wed, Aug 6, 2014 at 12:16 PM, DT <dt at flyingtroika.com 
> <mailto:dt at flyingtroika.com>> wrote:
>
>     We have done multiple experiments in respect to nio and io java
>     APIs and we have not seen that much improvement in throughput or
>     latencies with NIO.
>     Got almost the same stats for udp , tcp and http based packets
>     (running on windows and linux platforms). Though we noticed that
>     the more traffic we handle the better results we got with NIO
>     implementation in terms of latencies and overall throughput of the
>     application (there is some sort of threshold when system starts
>     reacting better). The idea was to move to java NIO APIs but due to
>     the results we decided to make some more research. Its difficult
>     to make a benchmark just because even a small change in linux
>     kernel/nic can lead to different results. When we converted java
>     logic into C++/C code and used linux non blocking/event based
>     calls we have got much better optimization/performance. Good
>     example is to compare nginx socket event module and Java NIO APIs.
>      Probably we shoud not compare java non-blocking calls to c/c++
>     calls and implementation though I thought its a good idea to get a
>     benchmark this way.
>
>     Thanks,
>     DT
>
>     On 8/5/2014 4:51 PM, Zhong Yu wrote:
>
>         On Tue, Aug 5, 2014 at 6:41 PM, Stanimir Simeonoff
>         <stanimir at riflexo.com <mailto:stanimir at riflexo.com>> wrote:
>
>
>
>
>
>                 There's a dilemma though - if the application code is
>                 writing bytes to
>                 the response body with blocking write(), isn't it
>                 tying up a thread if
>                 the client is slow? And if the write() is
>                 non-blocking, aren't we
>                 buffering up too much data? I think this problem can
>                 often be solved
>                 by a non-blocking write(obj) that buffers `obj` with lazy
>                 serialization, see "optimization technique" in
>                 http://bayou.io/draft/response.style.html#Response_body
>
>                 Zhong Yu
>
>
>             The lazy serialization unfortunately requires the object
>             to be fully fetched
>             (not depending on any context or an active database
>             connection) which is not
>             that different than "buffering too much" - it's just not
>             plain ByteBuffer
>
>         There's a difference if the objects are shared among responses
>         which
>         is a reasonable assumption for a lot of web applications.
>
>             (or byte[]).
>             Personally I don't like lazy serialization as that leaves
>             objects in the
>             queues and the latter may have implications of module
>             (classes) redeploys
>             with slow clients. Also it makes a lot hard quantifying
>             the expected queue
>             length per connection and shutting down slow connection.
>
>             Stanimir
>
>
>
>                     Alex
>
>
>                     On 03/08/2014 20:06, Zhong Yu wrote:
>
>                             Also, apparently, in heavy I/O scenarios,
>                             you may have a much better
>                             system
>                             throughput waiting for things to happen in
>                             I/O (blocking I/O) vs being
>                             notified of I/O events (Selector-based I/O):
>                             http://www.mailinator.com/tymaPaulMultithreaded.pdf.
>                             Paper is 6 years
>                             old
>                             and kernel/Java realities might have
>                             changed, YMMV, but the difference
>                             is(was?) impressive. Also, Apache HTTP
>                             Client still swears by blocking
>                             I/O
>                             vs non-blocking one in terms of efficiency:
>
>                             http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>
>                         To add a small data point to this discussion,
>                         Tomcat with NIO is
>                         apparently slower than Tomcat with Blocking-IO
>                         by 1,700ns for a simple
>                         request-response, according to a benchmark I
>                         did recently [1]. But!
>                         The difference is very small, and I would
>                         argue that it is negligible.
>
>                         Paul Tyma's claim (that the throughput of
>                         Blocking-IO is 30% more than
>                         NIO) is not very meaningful for real
>                         applications. I did once
>                         replicate his claim with a test that does
>                         nothing with the bytes being
>                         transferred; but as soon as you at least read
>                         each byte once, the
>                         throughput difference becomes very
>                         unimpressive (and frankly I suspect
>                         it's largely due to Java's implementation of NIO).
>
>                         [1]
>                         http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>
>                         Zhong Yu
>                         bayou.io <http://bayou.io>
>                         _______________________________________________
>                         Concurrency-interest mailing list
>                         Concurrency-interest at cs.oswego.edu
>                         <mailto:Concurrency-interest at cs.oswego.edu>
>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140806/cfe832f0/attachment-0001.html>

From vitalyd at gmail.com  Wed Aug  6 15:12:37 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 6 Aug 2014 15:12:37 -0400
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <53E2799C.2070904@oracle.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
	<53E254C1.4060501@flyingtroika.com>
	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
	<53E2799C.2070904@oracle.com>
Message-ID: <CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>

"bytes read so far" isn't really computation -- that part would be handled
by the i/o thread.  It's signalled that a channel is readable, proceeds to
read whatever's in there, and if that doesn't form a full/processable
request, it puts it aside in a buffer.  Yes, you need to keep some state
around here, but I don't see this particular case as a big problem.  This
also gives the server a chance to do graceful slow client (or deliberately
malicious) handling by determining how much or how long data can be
buffered before a full request is received.

If you were going to do a thread-per-request model, what would happen if
you have 100k+ connections? I'm not aware of any OS that can handle that
many threads (even if only a small fraction of them is runnable, that's
going to be a fairly chunky absolute number).  Memory footprint will be an
issue as well, especially if the stack sizes get big.

Finally, as I mentioned earlier, for cases like HTTP servers it doesn't
make sense to have a thread-per-connection if you're expecting lots and
lots of concurrent connections.  There's a limit on the i/o bandwidth and
cpu consumption, so may as well manage that part explicitly (e.g. 1 i/o
thread servicing all file descriptors, and then ~N worker threads where N
is # of cpus).  But to reiterate, if you're handling say a few thousand
concurrent (but mostly idle at a time) connections, you can probably get
away with using a thread per request.


On Wed, Aug 6, 2014 at 2:53 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  But it's contagious. It is difficult to combine data push (event-based
> IO) and data pull (blocking IO) in the same program.
>
> If you have tens of thousands connections, but computation cannot make
> progress without reading complete request, you still need to either switch
> to blocking IO, or switch to capturing the state of the computation (even
> if it's only the "bytes read so far") in some way. I don't see what we are
> meant to save by not using threads, which capture some of the state on the
> stack, and which offers language support (try/finally, synchronized, error
> propagation, etc).
>
>
> Alex
>
>
>
> On 06/08/2014 18:14, Vitaly Davidovich wrote:
>
> I don't think the point of NIO (or event-based i/o in general) is to have
> better absolute latency/throughput in all cases than blocking i/o.
>  Instead, it's really intended for being able to scale to tens (and
> hundreds) of thousands of concurrent (and mostly idle at a point in time)
> connections on a single server.  This makes intuitive sense since pretty
> much just one core dedicated to doing i/o can saturate a NIC (given
> sufficient read/write workload); the rest of the compute resources can be
> dedicated to the CPU bound workload.  Creating a thread-per-connection in
> those circumstances either doesn't make sense or simply won't work at all.
>
>
> On Wed, Aug 6, 2014 at 12:16 PM, DT <dt at flyingtroika.com> wrote:
>
>> We have done multiple experiments in respect to nio and io java APIs and
>> we have not seen that much improvement in throughput or latencies with NIO.
>> Got almost the same stats for udp , tcp and http based packets (running
>> on windows and linux platforms). Though we noticed that the more traffic we
>> handle the better results we got with NIO implementation in terms of
>> latencies and overall throughput of the application (there is some sort of
>> threshold when system starts reacting better). The idea was to move to java
>> NIO APIs but due to the results we decided to make some more research. Its
>> difficult to make a benchmark just because even a small change in linux
>> kernel/nic can lead to different results. When we converted java logic into
>> C++/C code and used linux non blocking/event based calls we have got much
>> better optimization/performance. Good example is to compare nginx socket
>> event module and Java NIO APIs.  Probably we shoud not compare java
>> non-blocking calls to c/c++ calls and implementation though I thought its a
>> good idea to get a benchmark this way.
>>
>> Thanks,
>> DT
>>
>> On 8/5/2014 4:51 PM, Zhong Yu wrote:
>>
>>> On Tue, Aug 5, 2014 at 6:41 PM, Stanimir Simeonoff <stanimir at riflexo.com>
>>> wrote:
>>>
>>>>
>>>>
>>>>
>>>>
>>>>  There's a dilemma though - if the application code is writing bytes to
>>>>> the response body with blocking write(), isn't it tying up a thread if
>>>>> the client is slow? And if the write() is non-blocking, aren't we
>>>>> buffering up too much data? I think this problem can often be solved
>>>>> by a non-blocking write(obj) that buffers `obj` with lazy
>>>>> serialization, see "optimization technique" in
>>>>> http://bayou.io/draft/response.style.html#Response_body
>>>>>
>>>>> Zhong Yu
>>>>>
>>>>
>>>> The lazy serialization unfortunately requires the object to be fully
>>>> fetched
>>>> (not depending on any context or an active database connection) which
>>>> is not
>>>> that different than "buffering too much" - it's just not plain
>>>> ByteBuffer
>>>>
>>> There's a difference if the objects are shared among responses which
>>> is a reasonable assumption for a lot of web applications.
>>>
>>>  (or byte[]).
>>>> Personally I don't like lazy serialization as that leaves objects in the
>>>> queues and the latter may have implications of module (classes)
>>>> redeploys
>>>> with slow clients. Also it makes a lot hard quantifying the expected
>>>> queue
>>>> length per connection and shutting down slow connection.
>>>>
>>>> Stanimir
>>>>
>>>>
>>>>
>>>>>  Alex
>>>>>>
>>>>>>
>>>>>> On 03/08/2014 20:06, Zhong Yu wrote:
>>>>>>
>>>>>>>  Also, apparently, in heavy I/O scenarios, you may have a much better
>>>>>>>> system
>>>>>>>> throughput waiting for things to happen in I/O (blocking I/O) vs
>>>>>>>> being
>>>>>>>> notified of I/O events (Selector-based I/O):
>>>>>>>> http://www.mailinator.com/tymaPaulMultithreaded.pdf. Paper is 6
>>>>>>>> years
>>>>>>>> old
>>>>>>>> and kernel/Java realities might have changed, YMMV, but the
>>>>>>>> difference
>>>>>>>> is(was?) impressive. Also, Apache HTTP Client still swears by
>>>>>>>> blocking
>>>>>>>> I/O
>>>>>>>> vs non-blocking one in terms of efficiency:
>>>>>>>>
>>>>>>>>
>>>>>>>> http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>>>>>>>>
>>>>>>> To add a small data point to this discussion, Tomcat with NIO is
>>>>>>> apparently slower than Tomcat with Blocking-IO by 1,700ns for a
>>>>>>> simple
>>>>>>> request-response, according to a benchmark I did recently [1]. But!
>>>>>>> The difference is very small, and I would argue that it is
>>>>>>> negligible.
>>>>>>>
>>>>>>> Paul Tyma's claim (that the throughput of Blocking-IO is 30% more
>>>>>>> than
>>>>>>> NIO) is not very meaningful for real applications. I did once
>>>>>>> replicate his claim with a test that does nothing with the bytes
>>>>>>> being
>>>>>>> transferred; but as soon as you at least read each byte once, the
>>>>>>> throughput difference becomes very unimpressive (and frankly I
>>>>>>> suspect
>>>>>>> it's largely due to Java's implementation of NIO).
>>>>>>>
>>>>>>> [1] http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>>>>>>>
>>>>>>> Zhong Yu
>>>>>>> bayou.io
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>
>>>>>>  _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>>  _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140806/00489aeb/attachment.html>

From oleksandr.otenko at oracle.com  Wed Aug  6 15:39:45 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 06 Aug 2014 20:39:45 +0100
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com>	<539B8FEE.8030107@cs.oswego.edu>	<539B9CAE.3010705@sosnoski.com>	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>	<539BAA7A.1020007@sosnoski.com>	<539BB459.2040600@ivanov.biz>	<539BD019.8090703@sosnoski.com>	<539BEAE3.5030006@ivanov.biz>	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>	<53E0E650.1040408@oracle.com>	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>	<53E254C1.4060501@flyingtroika.com>	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>	<53E2799C.2070904@oracle.com>
	<CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
Message-ID: <53E28481.6030600@oracle.com>

I understand what you are saying. They don't look like performance 
decisions and they rather smell like a work around the inability to 
support 100+k stacks than a convenience.


Alex


On 06/08/2014 20:12, Vitaly Davidovich wrote:
> "bytes read so far" isn't really computation -- that part would be 
> handled by the i/o thread.  It's signalled that a channel is readable, 
> proceeds to read whatever's in there, and if that doesn't form a 
> full/processable request, it puts it aside in a buffer.  Yes, you need 
> to keep some state around here, but I don't see this particular case 
> as a big problem.  This also gives the server a chance to do graceful 
> slow client (or deliberately malicious) handling by determining how 
> much or how long data can be buffered before a full request is received.
>
> If you were going to do a thread-per-request model, what would happen 
> if you have 100k+ connections? I'm not aware of any OS that can handle 
> that many threads (even if only a small fraction of them is runnable, 
> that's going to be a fairly chunky absolute number).  Memory footprint 
> will be an issue as well, especially if the stack sizes get big.
>
> Finally, as I mentioned earlier, for cases like HTTP servers it 
> doesn't make sense to have a thread-per-connection if you're expecting 
> lots and lots of concurrent connections.  There's a limit on the i/o 
> bandwidth and cpu consumption, so may as well manage that part 
> explicitly (e.g. 1 i/o thread servicing all file descriptors, and then 
> ~N worker threads where N is # of cpus).  But to reiterate, if you're 
> handling say a few thousand concurrent (but mostly idle at a time) 
> connections, you can probably get away with using a thread per request.
>
>
> On Wed, Aug 6, 2014 at 2:53 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     But it's contagious. It is difficult to combine data push
>     (event-based IO) and data pull (blocking IO) in the same program.
>
>     If you have tens of thousands connections, but computation cannot
>     make progress without reading complete request, you still need to
>     either switch to blocking IO, or switch to capturing the state of
>     the computation (even if it's only the "bytes read so far") in
>     some way. I don't see what we are meant to save by not using
>     threads, which capture some of the state on the stack, and which
>     offers language support (try/finally, synchronized, error
>     propagation, etc).
>
>
>     Alex
>
>
>
>     On 06/08/2014 18:14, Vitaly Davidovich wrote:
>>     I don't think the point of NIO (or event-based i/o in general) is
>>     to have better absolute latency/throughput in all cases than
>>     blocking i/o.  Instead, it's really intended for being able to
>>     scale to tens (and hundreds) of thousands of concurrent (and
>>     mostly idle at a point in time) connections on a single server.
>>      This makes intuitive sense since pretty much just one core
>>     dedicated to doing i/o can saturate a NIC (given sufficient
>>     read/write workload); the rest of the compute resources can be
>>     dedicated to the CPU bound workload.  Creating a
>>     thread-per-connection in those circumstances either doesn't make
>>     sense or simply won't work at all.
>>
>>
>>     On Wed, Aug 6, 2014 at 12:16 PM, DT <dt at flyingtroika.com
>>     <mailto:dt at flyingtroika.com>> wrote:
>>
>>         We have done multiple experiments in respect to nio and io
>>         java APIs and we have not seen that much improvement in
>>         throughput or latencies with NIO.
>>         Got almost the same stats for udp , tcp and http based
>>         packets (running on windows and linux platforms). Though we
>>         noticed that the more traffic we handle the better results we
>>         got with NIO implementation in terms of latencies and overall
>>         throughput of the application (there is some sort of
>>         threshold when system starts reacting better). The idea was
>>         to move to java NIO APIs but due to the results we decided to
>>         make some more research. Its difficult to make a benchmark
>>         just because even a small change in linux kernel/nic can lead
>>         to different results. When we converted java logic into C++/C
>>         code and used linux non blocking/event based calls we have
>>         got much better optimization/performance. Good example is to
>>         compare nginx socket event module and Java NIO APIs.
>>          Probably we shoud not compare java non-blocking calls to
>>         c/c++ calls and implementation though I thought its a good
>>         idea to get a benchmark this way.
>>
>>         Thanks,
>>         DT
>>
>>         On 8/5/2014 4:51 PM, Zhong Yu wrote:
>>
>>             On Tue, Aug 5, 2014 at 6:41 PM, Stanimir Simeonoff
>>             <stanimir at riflexo.com <mailto:stanimir at riflexo.com>> wrote:
>>
>>
>>
>>
>>
>>                     There's a dilemma though - if the application
>>                     code is writing bytes to
>>                     the response body with blocking write(), isn't it
>>                     tying up a thread if
>>                     the client is slow? And if the write() is
>>                     non-blocking, aren't we
>>                     buffering up too much data? I think this problem
>>                     can often be solved
>>                     by a non-blocking write(obj) that buffers `obj`
>>                     with lazy
>>                     serialization, see "optimization technique" in
>>                     http://bayou.io/draft/response.style.html#Response_body
>>
>>                     Zhong Yu
>>
>>
>>                 The lazy serialization unfortunately requires the
>>                 object to be fully fetched
>>                 (not depending on any context or an active database
>>                 connection) which is not
>>                 that different than "buffering too much" - it's just
>>                 not plain ByteBuffer
>>
>>             There's a difference if the objects are shared among
>>             responses which
>>             is a reasonable assumption for a lot of web applications.
>>
>>                 (or byte[]).
>>                 Personally I don't like lazy serialization as that
>>                 leaves objects in the
>>                 queues and the latter may have implications of module
>>                 (classes) redeploys
>>                 with slow clients. Also it makes a lot hard
>>                 quantifying the expected queue
>>                 length per connection and shutting down slow connection.
>>
>>                 Stanimir
>>
>>
>>
>>                         Alex
>>
>>
>>                         On 03/08/2014 20:06, Zhong Yu wrote:
>>
>>                                 Also, apparently, in heavy I/O
>>                                 scenarios, you may have a much better
>>                                 system
>>                                 throughput waiting for things to
>>                                 happen in I/O (blocking I/O) vs being
>>                                 notified of I/O events
>>                                 (Selector-based I/O):
>>                                 http://www.mailinator.com/tymaPaulMultithreaded.pdf.
>>                                 Paper is 6 years
>>                                 old
>>                                 and kernel/Java realities might have
>>                                 changed, YMMV, but the difference
>>                                 is(was?) impressive. Also, Apache
>>                                 HTTP Client still swears by blocking
>>                                 I/O
>>                                 vs non-blocking one in terms of
>>                                 efficiency:
>>
>>                                 http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>>
>>                             To add a small data point to this
>>                             discussion, Tomcat with NIO is
>>                             apparently slower than Tomcat with
>>                             Blocking-IO by 1,700ns for a simple
>>                             request-response, according to a
>>                             benchmark I did recently [1]. But!
>>                             The difference is very small, and I would
>>                             argue that it is negligible.
>>
>>                             Paul Tyma's claim (that the throughput of
>>                             Blocking-IO is 30% more than
>>                             NIO) is not very meaningful for real
>>                             applications. I did once
>>                             replicate his claim with a test that does
>>                             nothing with the bytes being
>>                             transferred; but as soon as you at least
>>                             read each byte once, the
>>                             throughput difference becomes very
>>                             unimpressive (and frankly I suspect
>>                             it's largely due to Java's implementation
>>                             of NIO).
>>
>>                             [1]
>>                             http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>>
>>                             Zhong Yu
>>                             bayou.io <http://bayou.io>
>>                             _______________________________________________
>>                             Concurrency-interest mailing list
>>                             Concurrency-interest at cs.oswego.edu
>>                             <mailto:Concurrency-interest at cs.oswego.edu>
>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140806/47a1b247/attachment-0001.html>

From raould at gmail.com  Wed Aug  6 16:22:36 2014
From: raould at gmail.com (Raoul Duke)
Date: Wed, 6 Aug 2014 13:22:36 -0700
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
	<53E254C1.4060501@flyingtroika.com>
	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
	<53E2799C.2070904@oracle.com>
	<CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
Message-ID: <CAJ7XQb5EWttrBnoq170MRbfa7k=KJsp=dHzqWa5_j58wsriJ3Q@mail.gmail.com>

> If you were going to do a thread-per-request model, what would happen if you
> have 100k+ connections? I'm not aware of any OS that can handle that many
> threads (even if only a small fraction of them is runnable, that's going to
> be a fairly chunky absolute number).  Memory footprint will be an issue as
> well, especially if the stack sizes get big.

oh come on, you've never heard of a company that got bought for $19
billion in stock by facebook, or the stack they use? :-}

From vitalyd at gmail.com  Wed Aug  6 16:43:01 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 6 Aug 2014 16:43:01 -0400
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CAJ7XQb5EWttrBnoq170MRbfa7k=KJsp=dHzqWa5_j58wsriJ3Q@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
	<53E254C1.4060501@flyingtroika.com>
	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
	<53E2799C.2070904@oracle.com>
	<CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
	<CAJ7XQb5EWttrBnoq170MRbfa7k=KJsp=dHzqWa5_j58wsriJ3Q@mail.gmail.com>
Message-ID: <CAHjP37FH7JreveSRYvPT8=4pC5XSnrfB0BxrHk-iPtkDOZ3dCg@mail.gmail.com>

:) Erlang is a different matter altogether.  If you have a runtime with
fibers/coroutines, knock yourself out.

Sent from my phone
On Aug 6, 2014 4:39 PM, "Raoul Duke" <raould at gmail.com> wrote:

> > If you were going to do a thread-per-request model, what would happen if
> you
> > have 100k+ connections? I'm not aware of any OS that can handle that many
> > threads (even if only a small fraction of them is runnable, that's going
> to
> > be a fairly chunky absolute number).  Memory footprint will be an issue
> as
> > well, especially if the stack sizes get big.
>
> oh come on, you've never heard of a company that got bought for $19
> billion in stock by facebook, or the stack they use? :-}
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140806/02f63366/attachment.html>

From zhong.j.yu at gmail.com  Wed Aug  6 17:15:54 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 6 Aug 2014 16:15:54 -0500
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
	<53E254C1.4060501@flyingtroika.com>
	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
	<53E2799C.2070904@oracle.com>
	<CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
Message-ID: <CACuKZqEUXswKXPe=7ZkZYHiaFoL15c0cS5VLJ2sv4qKSwUrYpg@mail.gmail.com>

On Wed, Aug 6, 2014 at 2:12 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> what would happen if you have 100k+ connections?

You'd rule the world. To put the number in perspective, google
receives less than 100k searches per second.

Zhong Yu
bayou.io

From vitalyd at gmail.com  Wed Aug  6 17:23:00 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 6 Aug 2014 17:23:00 -0400
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CACuKZqEUXswKXPe=7ZkZYHiaFoL15c0cS5VLJ2sv4qKSwUrYpg@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
	<53E254C1.4060501@flyingtroika.com>
	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
	<53E2799C.2070904@oracle.com>
	<CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
	<CACuKZqEUXswKXPe=7ZkZYHiaFoL15c0cS5VLJ2sv4qKSwUrYpg@mail.gmail.com>
Message-ID: <CAHjP37F0ieQkJQZgc9CPnr===Z43vU1QxOukrrpD8+kVn9s6rg@mail.gmail.com>

I was talking about connections, not requests.


On Wed, Aug 6, 2014 at 5:15 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> On Wed, Aug 6, 2014 at 2:12 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
> > what would happen if you have 100k+ connections?
>
> You'd rule the world. To put the number in perspective, google
> receives less than 100k searches per second.
>
> Zhong Yu
> bayou.io
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140806/2e815d8d/attachment.html>

From zhong.j.yu at gmail.com  Wed Aug  6 18:25:33 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 6 Aug 2014 17:25:33 -0500
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CAHjP37F0ieQkJQZgc9CPnr===Z43vU1QxOukrrpD8+kVn9s6rg@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
	<53E254C1.4060501@flyingtroika.com>
	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
	<53E2799C.2070904@oracle.com>
	<CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
	<CACuKZqEUXswKXPe=7ZkZYHiaFoL15c0cS5VLJ2sv4qKSwUrYpg@mail.gmail.com>
	<CAHjP37F0ieQkJQZgc9CPnr===Z43vU1QxOukrrpD8+kVn9s6rg@mail.gmail.com>
Message-ID: <CACuKZqHPCxNdxc3fT-4kJqPRf2VJYG=8=qXt4EEODLk66kGfhQ@mail.gmail.com>

I mean there's a constant ratio between the two numbers, therefore we
can estimate the magnitude of connections from the number of requests
per second. We can put a generous upper bound on the ratio at 100
second, and bet that Google Search maintains less than 10M concurrent
connections globally.

When we are talking about this kind of numbers, don't forget the
anthropic bound -  there are only a few billion people in the world.

Zhong Yu
bayou.io


On Wed, Aug 6, 2014 at 4:23 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> I was talking about connections, not requests.
>
>
> On Wed, Aug 6, 2014 at 5:15 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>>
>> On Wed, Aug 6, 2014 at 2:12 PM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:
>> > what would happen if you have 100k+ connections?
>>
>> You'd rule the world. To put the number in perspective, google
>> receives less than 100k searches per second.
>>
>> Zhong Yu
>> bayou.io
>
>

From raould at gmail.com  Wed Aug  6 18:44:28 2014
From: raould at gmail.com (Raoul Duke)
Date: Wed, 6 Aug 2014 15:44:28 -0700
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CACuKZqHPCxNdxc3fT-4kJqPRf2VJYG=8=qXt4EEODLk66kGfhQ@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
	<53E254C1.4060501@flyingtroika.com>
	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
	<53E2799C.2070904@oracle.com>
	<CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
	<CACuKZqEUXswKXPe=7ZkZYHiaFoL15c0cS5VLJ2sv4qKSwUrYpg@mail.gmail.com>
	<CAHjP37F0ieQkJQZgc9CPnr===Z43vU1QxOukrrpD8+kVn9s6rg@mail.gmail.com>
	<CACuKZqHPCxNdxc3fT-4kJqPRf2VJYG=8=qXt4EEODLk66kGfhQ@mail.gmail.com>
Message-ID: <CAJ7XQb6VwTakxNfNObPFffX0WrtnpKoEDdrxzwyonK3nFG_3SQ@mail.gmail.com>

> When we are talking about this kind of numbers, don't forget the
> anthropic bound -  there are only a few billion people in the world.

hah! so what.

5 of them control all the machines in the world via their dark botnet,
and are sending out a zillion connections a second as dos attacks.

:-)

From stanimir at riflexo.com  Thu Aug  7 02:08:58 2014
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 7 Aug 2014 09:08:58 +0300
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <CACuKZqHPCxNdxc3fT-4kJqPRf2VJYG=8=qXt4EEODLk66kGfhQ@mail.gmail.com>
References: <539B8AD4.8090702@sosnoski.com> <539B8FEE.8030107@cs.oswego.edu>
	<539B9CAE.3010705@sosnoski.com>
	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>
	<539BAA7A.1020007@sosnoski.com> <539BB459.2040600@ivanov.biz>
	<539BD019.8090703@sosnoski.com> <539BEAE3.5030006@ivanov.biz>
	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>
	<53E0E650.1040408@oracle.com>
	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>
	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>
	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>
	<53E254C1.4060501@flyingtroika.com>
	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>
	<53E2799C.2070904@oracle.com>
	<CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
	<CACuKZqEUXswKXPe=7ZkZYHiaFoL15c0cS5VLJ2sv4qKSwUrYpg@mail.gmail.com>
	<CAHjP37F0ieQkJQZgc9CPnr===Z43vU1QxOukrrpD8+kVn9s6rg@mail.gmail.com>
	<CACuKZqHPCxNdxc3fT-4kJqPRf2VJYG=8=qXt4EEODLk66kGfhQ@mail.gmail.com>
Message-ID: <CAEJX8opsAh5pzhwkX=hV4vh7BC6fhP_Ndn9MusXhnRBYtrSkrw@mail.gmail.com>

On Thu, Aug 7, 2014 at 1:25 AM, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> I mean there's a constant ratio between the two numbers, therefore we
> can estimate the magnitude of connections from the number of requests
> per second. We can put a generous upper bound on the ratio at 100
> second, and bet that Google Search maintains less than 10M concurrent
> connections globally


This is true only for http-alike workloads. For instance pushing market
data (stocks/forex) results into hundreds (or thousands) of
packets/messages a second sent per connection and virtually no requests as
the model is largely subscribe and forget. And sometimes it's "forget for
weeks".
In such cases sending the messages would require context switch on each
message which is non-trivial.

Stanimir




> 4 at 5:15 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
> >>
> >> On Wed, Aug 6, 2014 at 2:12 PM, Vitaly Davidovich <vitalyd at gmail.com>
> >> wrote:
> >> > what would happen if you have 100k+ connections?
> >>
> >> You'd rule the world. To put the number in perspective, google
> >> receives less than 100k searches per second.
> >>
> >> Zhong Yu
> >> bayou.io
> >
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140807/af22064d/attachment-0001.html>

From bayinamine at gmail.com  Fri Aug  8 04:15:01 2014
From: bayinamine at gmail.com (=?GB2312?B?wt64tbTP?=)
Date: Fri, 8 Aug 2014 16:15:01 +0800
Subject: [concurrency-interest] ConcurrentHashMap in JDK7 code explanation
	(scanAndLockForPut)
Message-ID: <4F02E0C7-B7FD-4846-8CC6-60EB76874E80@gmail.com>


The source codes of the method scanAndLockForPut in ConcurrentHashMap in JDK7 says:

private HashEntry<K,V> scanAndLockForPut(K key, int hash, V value) {
    HashEntry<K,V> first = entryForHash(this, hash);
    HashEntry<K,V> e = first;
    HashEntry<K,V> node = null;
    int retries = -1; // negative while locating node
    while (!tryLock()) {
        HashEntry<K,V> f; // to recheck first below
        if (retries < 0) {
            if (e == null) {
                if (node == null) // speculatively create node
                    node = new HashEntry<K,V>(hash, key, value, null);
                retries = 0;
            }
            else if (key.equals(e.key))
                retries = 0;
            else
                e = e.next;
        }
        else if (++retries > MAX_SCAN_RETRIES) {
            lock();
            break;
        }
        else if ((retries & 1) == 0 &&
                 (f = entryForHash(this, hash)) != first) {
            e = first = f; // re-traverse if entry changed
            retries = -1;
        }
    }
    return node;
}
I understand what the codes mean, but what I don't is this else if entry:

else if ((retries & 1) == 0 && (f = entryForHash(this, hash)) != first)
My question is: Why do we have to do "(retries & 1) == 0"?

EDIT: I kind of figure it out. It's all because the constant MAX_SCAN_RETRIES:

static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() > 1 ? 64 : 1;
In single core processor, MAX_SCAN_RETRIES = 1. So the second time the thread steps into the loop "while(tryLock)", it doesn't have to check whether the first node was changed.

However, in multi cores processor, this will behave like checking whether the first node is changed every 2 times in the while loop.

Is the above explanation correct?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140808/2621f19f/attachment.html>

From dl at cs.oswego.edu  Mon Aug 11 06:34:42 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 11 Aug 2014 06:34:42 -0400 (EDT)
Subject: [concurrency-interest] ConcurrentHashMap in JDK7 code
 explanation (scanAndLockForPut)
In-Reply-To: <4F02E0C7-B7FD-4846-8CC6-60EB76874E80@gmail.com>
References: <4F02E0C7-B7FD-4846-8CC6-60EB76874E80@gmail.com>
Message-ID: <39122.208.97.121.124.1407753282.squirrel@altair.cs.oswego.edu>

>
> The source codes of the method scanAndLockForPut in ConcurrentHashMap in
> JDK7 says: ...
>
> I understand what the codes mean, but what I don't is this else if entry:
>
> else if ((retries & 1) == 0 && (f = entryForHash(this, hash)) != first)
> My question is: Why do we have to do "(retries & 1) == 0"?
>
> EDIT: I kind of figure it out. It's all because the constant
> MAX_SCAN_RETRIES:
>
> static final int MAX_SCAN_RETRIES =
> Runtime.getRuntime().availableProcessors() > 1 ? 64 : 1;
> In single core processor, MAX_SCAN_RETRIES = 1. So the second time the
> thread steps into the loop "while(tryLock)", it doesn't have to check
> whether the first node was changed.
>
> However, in multi cores processor, this will behave like checking whether
> the first node is changed every 2 times in the while loop.
>
> Is the above explanation
> correct?

Yes. We need only ensure that staleness is eventually detected.
Alternating the head-checks works fine, and simplifies use of
the same code for both uni- and multi- processors.

-Doug





From dt at flyingtroika.com  Tue Aug 12 02:22:21 2014
From: dt at flyingtroika.com (DT)
Date: Mon, 11 Aug 2014 23:22:21 -0700
Subject: [concurrency-interest] Blocking vs. non-blocking
In-Reply-To: <53E28481.6030600@oracle.com>
References: <539B8AD4.8090702@sosnoski.com>	<539B8FEE.8030107@cs.oswego.edu>	<539B9CAE.3010705@sosnoski.com>	<CAHjP37FxTqDGB+xxNJVY1mqJAXr3OrLwL9tQVbqPxGu0m3DY=w@mail.gmail.com>	<539BAA7A.1020007@sosnoski.com>	<539BB459.2040600@ivanov.biz>	<539BD019.8090703@sosnoski.com>	<539BEAE3.5030006@ivanov.biz>	<CACuKZqGSweF_Q67oui5j2s_qj54E9AGhQf0wTw4KVGHOqg5d6g@mail.gmail.com>	<53E0E650.1040408@oracle.com>	<CACuKZqHZMLe1QGKTv34PbyYQjEpEHgrPH4fdqSLEz0Cge2U0fg@mail.gmail.com>	<CAEJX8oq20bSK-UdbV6ic6e1TDpz9s4nqKF5wBOCT65v=wxnXRA@mail.gmail.com>	<CACuKZqGgHgpRUTjZRWzHOYTzfEhJCpRh1PAvJZvYPBHW=d9q8A@mail.gmail.com>	<53E254C1.4060501@flyingtroika.com>	<CAHjP37GVcqgYXkNCMWmY2A73efseNFL5nMMHBrTQ3jsa=q+irA@mail.gmail.com>	<53E2799C.2070904@oracle.com>
	<CAHjP37EqC5bp4uB591v2VmP=4B0JDWzWET4tn0MzhvptFrxxgw@mail.gmail.com>
	<53E28481.6030600@oracle.com>
Message-ID: <53E9B29D.6010105@flyingtroika.com>

Our perspective is to be able to distribute connections between multiple 
nodes which really boils down to the load balancer (more or less) - 
hardware based or software based load balancer is a separate discussion. 
I can tell that one single linux server can handle >~ 60 k concurrent 
connections (depends on the actual server, number of NICs, linux kernel 
has different scheduling algorithms that can be configured for 
processes/threads/io as well, experiments with scheduling is a very 
interesting work and can lead to different results).
I said concurrent but the real picture is that multiple 
ThreadPoolExecutors handle this load. 'concurrency' is bounded by the 
queue size (milliseconds or nano seconds bounded concurrency is also a 
good question...). Plus on top of that distributed caching handles a lot 
of load as well such as data processing, fast data lookups.  Non 
blocking data structure can help a lot in this case.
I will continue experimenting with non blocking NIO sockets though again 
so far it helps more to handle big data streams than small packets in 
terms of IOPs/CPU/latencies. The server is utilized better by handling 
big packets using NIO rather than using the threaded approach. So the 
size of the packets can make a difference in scaling. By the way having 
a good description how NIO is implemented on low level  would help for sure.

Practically it does not matter what number of connections a single 
server can handle at any given time because it is always going to be a 
bigger demand and number should increase. Of course we would like to 
utilize hardware as much as possible. The distribution 
mechanism/algorithm of socket connections by application caries a more 
important fundamental role in the way how application can be scaled. So 
thats why Non blocking approach is that much of our interest + 
synchronization between nodes + scheduling.

Thanks,
DT


On 8/6/2014 12:39 PM, Oleksandr Otenko wrote:
> I understand what you are saying. They don't look like performance 
> decisions and they rather smell like a work around the inability to 
> support 100+k stacks than a convenience.
>
>
> Alex
>
>
> On 06/08/2014 20:12, Vitaly Davidovich wrote:
>> "bytes read so far" isn't really computation -- that part would be 
>> handled by the i/o thread.  It's signalled that a channel is 
>> readable, proceeds to read whatever's in there, and if that doesn't 
>> form a full/processable request, it puts it aside in a buffer.  Yes, 
>> you need to keep some state around here, but I don't see this 
>> particular case as a big problem.  This also gives the server a 
>> chance to do graceful slow client (or deliberately malicious) 
>> handling by determining how much or how long data can be buffered 
>> before a full request is received.
>>
>> If you were going to do a thread-per-request model, what would happen 
>> if you have 100k+ connections? I'm not aware of any OS that can 
>> handle that many threads (even if only a small fraction of them is 
>> runnable, that's going to be a fairly chunky absolute number). 
>>  Memory footprint will be an issue as well, especially if the stack 
>> sizes get big.
>>
>> Finally, as I mentioned earlier, for cases like HTTP servers it 
>> doesn't make sense to have a thread-per-connection if you're 
>> expecting lots and lots of concurrent connections.  There's a limit 
>> on the i/o bandwidth and cpu consumption, so may as well manage that 
>> part explicitly (e.g. 1 i/o thread servicing all file descriptors, 
>> and then ~N worker threads where N is # of cpus).  But to reiterate, 
>> if you're handling say a few thousand concurrent (but mostly idle at 
>> a time) connections, you can probably get away with using a thread 
>> per request.
>>
>>
>> On Wed, Aug 6, 2014 at 2:53 PM, Oleksandr Otenko 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>     But it's contagious. It is difficult to combine data push
>>     (event-based IO) and data pull (blocking IO) in the same program.
>>
>>     If you have tens of thousands connections, but computation cannot
>>     make progress without reading complete request, you still need to
>>     either switch to blocking IO, or switch to capturing the state of
>>     the computation (even if it's only the "bytes read so far") in
>>     some way. I don't see what we are meant to save by not using
>>     threads, which capture some of the state on the stack, and which
>>     offers language support (try/finally, synchronized, error
>>     propagation, etc).
>>
>>
>>     Alex
>>
>>
>>
>>     On 06/08/2014 18:14, Vitaly Davidovich wrote:
>>>     I don't think the point of NIO (or event-based i/o in general)
>>>     is to have better absolute latency/throughput in all cases than
>>>     blocking i/o.  Instead, it's really intended for being able to
>>>     scale to tens (and hundreds) of thousands of concurrent (and
>>>     mostly idle at a point in time) connections on a single server.
>>>      This makes intuitive sense since pretty much just one core
>>>     dedicated to doing i/o can saturate a NIC (given sufficient
>>>     read/write workload); the rest of the compute resources can be
>>>     dedicated to the CPU bound workload.  Creating a
>>>     thread-per-connection in those circumstances either doesn't make
>>>     sense or simply won't work at all.
>>>
>>>
>>>     On Wed, Aug 6, 2014 at 12:16 PM, DT <dt at flyingtroika.com
>>>     <mailto:dt at flyingtroika.com>> wrote:
>>>
>>>         We have done multiple experiments in respect to nio and io
>>>         java APIs and we have not seen that much improvement in
>>>         throughput or latencies with NIO.
>>>         Got almost the same stats for udp , tcp and http based
>>>         packets (running on windows and linux platforms). Though we
>>>         noticed that the more traffic we handle the better results
>>>         we got with NIO implementation in terms of latencies and
>>>         overall throughput of the application (there is some sort of
>>>         threshold when system starts reacting better). The idea was
>>>         to move to java NIO APIs but due to the results we decided
>>>         to make some more research. Its difficult to make a
>>>         benchmark just because even a small change in linux
>>>         kernel/nic can lead to different results. When we converted
>>>         java logic into C++/C code and used linux non blocking/event
>>>         based calls we have got much better
>>>         optimization/performance. Good example is to compare nginx
>>>         socket event module and Java NIO APIs.  Probably we shoud
>>>         not compare java non-blocking calls to c/c++ calls and
>>>         implementation though I thought its a good idea to get a
>>>         benchmark this way.
>>>
>>>         Thanks,
>>>         DT
>>>
>>>         On 8/5/2014 4:51 PM, Zhong Yu wrote:
>>>
>>>             On Tue, Aug 5, 2014 at 6:41 PM, Stanimir Simeonoff
>>>             <stanimir at riflexo.com <mailto:stanimir at riflexo.com>> wrote:
>>>
>>>
>>>
>>>
>>>
>>>                     There's a dilemma though - if the application
>>>                     code is writing bytes to
>>>                     the response body with blocking write(), isn't
>>>                     it tying up a thread if
>>>                     the client is slow? And if the write() is
>>>                     non-blocking, aren't we
>>>                     buffering up too much data? I think this problem
>>>                     can often be solved
>>>                     by a non-blocking write(obj) that buffers `obj`
>>>                     with lazy
>>>                     serialization, see "optimization technique" in
>>>                     http://bayou.io/draft/response.style.html#Response_body
>>>
>>>                     Zhong Yu
>>>
>>>
>>>                 The lazy serialization unfortunately requires the
>>>                 object to be fully fetched
>>>                 (not depending on any context or an active database
>>>                 connection) which is not
>>>                 that different than "buffering too much" - it's just
>>>                 not plain ByteBuffer
>>>
>>>             There's a difference if the objects are shared among
>>>             responses which
>>>             is a reasonable assumption for a lot of web applications.
>>>
>>>                 (or byte[]).
>>>                 Personally I don't like lazy serialization as that
>>>                 leaves objects in the
>>>                 queues and the latter may have implications of
>>>                 module (classes) redeploys
>>>                 with slow clients. Also it makes a lot hard
>>>                 quantifying the expected queue
>>>                 length per connection and shutting down slow connection.
>>>
>>>                 Stanimir
>>>
>>>
>>>
>>>                         Alex
>>>
>>>
>>>                         On 03/08/2014 20:06, Zhong Yu wrote:
>>>
>>>                                 Also, apparently, in heavy I/O
>>>                                 scenarios, you may have a much better
>>>                                 system
>>>                                 throughput waiting for things to
>>>                                 happen in I/O (blocking I/O) vs being
>>>                                 notified of I/O events
>>>                                 (Selector-based I/O):
>>>                                 http://www.mailinator.com/tymaPaulMultithreaded.pdf.
>>>                                 Paper is 6 years
>>>                                 old
>>>                                 and kernel/Java realities might have
>>>                                 changed, YMMV, but the difference
>>>                                 is(was?) impressive. Also, Apache
>>>                                 HTTP Client still swears by blocking
>>>                                 I/O
>>>                                 vs non-blocking one in terms of
>>>                                 efficiency:
>>>
>>>                                 http://wiki.apache.org/HttpComponents/HttpClient3vsHttpClient4vsHttpCore
>>>
>>>                             To add a small data point to this
>>>                             discussion, Tomcat with NIO is
>>>                             apparently slower than Tomcat with
>>>                             Blocking-IO by 1,700ns for a simple
>>>                             request-response, according to a
>>>                             benchmark I did recently [1]. But!
>>>                             The difference is very small, and I
>>>                             would argue that it is negligible.
>>>
>>>                             Paul Tyma's claim (that the throughput
>>>                             of Blocking-IO is 30% more than
>>>                             NIO) is not very meaningful for real
>>>                             applications. I did once
>>>                             replicate his claim with a test that
>>>                             does nothing with the bytes being
>>>                             transferred; but as soon as you at least
>>>                             read each byte once, the
>>>                             throughput difference becomes very
>>>                             unimpressive (and frankly I suspect
>>>                             it's largely due to Java's
>>>                             implementation of NIO).
>>>
>>>                             [1]
>>>                             http://bayou.io/draft/Comparing_Java_HTTP_Servers_Latencies.html
>>>
>>>                             Zhong Yu
>>>                             bayou.io <http://bayou.io>
>>>                             _______________________________________________
>>>                             Concurrency-interest mailing list
>>>                             Concurrency-interest at cs.oswego.edu
>>>                             <mailto:Concurrency-interest at cs.oswego.edu>
>>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>                     _______________________________________________
>>>                     Concurrency-interest mailing list
>>>                     Concurrency-interest at cs.oswego.edu
>>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu
>>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140811/7485e2b6/attachment-0001.html>

From vgrazi at gmail.com  Thu Aug 14 08:12:41 2014
From: vgrazi at gmail.com (Victor Grazi)
Date: Thu, 14 Aug 2014 08:12:41 -0400
Subject: [concurrency-interest] Article on Hunting Concurrency Bugs by Heinz
	Kabutz on InfoQ
Message-ID: <CA+y1Pu_9u9F3eH-fEynszQRfWv-2nd_wJv6xJV5fyncT9Xo5Pw@mail.gmail.com>

There's a nice article by Dr. Heinz Kabutz on InfoQ
http://www.infoq.com/articles/Hunting-Concurrency-Bugs-1
Heinz always amazes me!

Twitter: @vgrazi
LinkedIn: www.linkedin.com/in/victorgrazi/
Java Concurrent Animated: http://sourceforge.net/projects/javaconcurrenta/
Skype: vgrazi2
Google Plus <https://plus.google.com/+VictorGrazi/>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140814/4bea9b5e/attachment.html>

From rreja2000 at yahoo.com  Mon Aug 25 01:25:50 2014
From: rreja2000 at yahoo.com (Rohit Reja)
Date: Sun, 24 Aug 2014 22:25:50 -0700
Subject: [concurrency-interest] Unbounded thread pool and memory overhead
In-Reply-To: <CA+y1Pu_9u9F3eH-fEynszQRfWv-2nd_wJv6xJV5fyncT9Xo5Pw@mail.gmail.com>
References: <CA+y1Pu_9u9F3eH-fEynszQRfWv-2nd_wJv6xJV5fyncT9Xo5Pw@mail.gmail.com>
Message-ID: <1408944350.35278.YahooMailNeo@web120401.mail.ne1.yahoo.com>

Hi,

We are using Executors#newCachedThreadPool() at various places in our product. We recently faced an issue that the threads consumed lots of virtual memory in a machine ( and eventually the system has no memory left) due to lots of threads being created. ( We haven't set the Xss param though).

I have 2 questions here:-

1. Why have this API choose not to have bounded threads ? What would be the best practices while using unbounded thread pools.
2. How can I build a thread pool with a minimum threads that grows to a max bound and then the task submission blocks?

Thanks,
Rohit


On Thursday, August 14, 2014 6:03 PM, Victor Grazi <vgrazi at gmail.com> wrote:
 


There's a nice article by Dr. Heinz Kabutz on InfoQ?http://www.infoq.com/articles/Hunting-Concurrency-Bugs-1
Heinz always amazes me!

Twitter: @vgrazi
LinkedIn:?www.linkedin.com/in/victorgrazi/
Java Concurrent Animated:?http://sourceforge.net/projects/javaconcurrenta/
Skype: vgrazi2

Google Plus
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140824/2125ff9c/attachment.html>

From Sebastian.Millies at softwareag.com  Mon Aug 25 03:55:23 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Mon, 25 Aug 2014 07:55:23 +0000
Subject: [concurrency-interest] CompletableFuture with delay
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>

Hello there,

I'd like to simulate asynchronous IO events in a test system (without actual IO).
For this purpose, I have defined a method that creates a future which returns a value after a delay.
In contrast to Future#get(Long,TimeUnit) this method does not wait, but uses a separate ScheduledFuture
to complete the future. Code is shown below.

So far, so good. But I also want to cancel the scheduled task (the ScheduledFuture returned from
ScheduledExecutorService#schedule()) when the future is cancelled before the timeout. (So that
I can keep my timeout tasks from piling up when timeouts are long in relation to the real
computations.)

Please look at the code below, where I create an additional future with whenComplete and use
that to cancel the task. Is that a correct solution? Or could this additional future be optimized
away or be garbage collected? Does the variable "future" that is returned from the method hold a
reference to the additional future? I couldn't tell from the source code of CompletableFuture.

  private static final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(TIMER_THREADS);

  public static <T> CompletableFuture<T> delayedSuccess(T value, int delay, TimeUnit unit) {
    CompletableFuture<T> future = new CompletableFuture<T>();
    ScheduledFuture<Boolean> task = scheduler.schedule(() -> future.complete(value), delay, unit);
    future.whenComplete((t, ex) -> {
      if (future.isCancelled())
        task.cancel(true);               // <== HERE
    });
    return future;
  }

Sebastian Millies
Expert Java Business Analytics
Phone: +49 681 210 3221 | Fax: +49 681 210 1801 | Sebastian.Millies at softwareag.com<mailto:Sebastian.Millies at ids-scheer.com>

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/96c8d1ef/attachment.html>

From peter.levart at gmail.com  Mon Aug 25 05:05:57 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 25 Aug 2014 11:05:57 +0200
Subject: [concurrency-interest] Unbounded thread pool and memory overhead
In-Reply-To: <1408944350.35278.YahooMailNeo@web120401.mail.ne1.yahoo.com>
References: <CA+y1Pu_9u9F3eH-fEynszQRfWv-2nd_wJv6xJV5fyncT9Xo5Pw@mail.gmail.com>
	<1408944350.35278.YahooMailNeo@web120401.mail.ne1.yahoo.com>
Message-ID: <53FAFC75.4060402@gmail.com>

On 08/25/2014 07:25 AM, Rohit Reja wrote:
> Hi,
>
> We are using Executors#newCachedThreadPool() at various places in our product. We recently faced an issue that the threads consumed lots of virtual memory in a machine ( and eventually the system has no memory left) due to lots of threads being created. ( We haven't set the Xss param though).
>
> I have 2 questions here:-
>
> 1. Why have this API choose not to have bounded threads ?

Hi Rohit,

Executors#newCachedThreadPool() is just a thin facade over 
ThreadPoolExecutor constructor:

     public static ExecutorService newCachedThreadPool() {
         return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                       60L, TimeUnit.SECONDS,
                                       new SynchronousQueue<Runnable>());
     }

ThreadPoolExecutor class and it's constructor are public API, so you can 
use it directly. Just specify an upper bound on 'maximumPoolSize' 
(instead of Integer.MAX_VALUE), use some other BlockingQueue instead of 
SynchronousQueue (like LinkedBlockingQueue) and you are done.


> What would be the best practices while using unbounded thread pools.

Don't use them in situations where task submission rate can outpace 
processing for extended/unpredictable periods of time or where you don't 
control task submission in some other way. Unbounded thread pool is just 
an optimized variant of approach where you start new Thread for each task.

> 2. How can I build a thread pool with a minimum threads that grows to a max bound and then the task submission blocks?

You might supply a bounded BlockingQueue and when it fills up to it's 
capacity and all 'maximumPoolSize' threads are executing tasks, the 
Executor will start throwing RejectedExecutionException on attempts to 
submit more tasks. No blocking unfortunately. But you could simulate it 
by using a single thread that "take()s" tasks from a front-end bounded 
BlockingQueue and submits them to an executor with a small bounded 
BlockingQueue. If this thread encounters a RejectedExecutionException, 
it sleeps a while and retries... This way you can still get fair 
queue-ing for threads submitting tasks to front-end queue and blocking 
while doing so, just by using for example an 
ArrayBlockingQueue(capacity, true) as a front-end queue.

Regards, Peter

>
> Thanks,
> Rohit
>
>
> On Thursday, August 14, 2014 6:03 PM, Victor Grazi <vgrazi at gmail.com> wrote:
>   
>
>
> There's a nice article by Dr. Heinz Kabutz on InfoQ http://www.infoq.com/articles/Hunting-Concurrency-Bugs-1
> Heinz always amazes me!
>
> Twitter: @vgrazi
> LinkedIn: www.linkedin.com/in/victorgrazi/
> Java Concurrent Animated: http://sourceforge.net/projects/javaconcurrenta/
> Skype: vgrazi2
>
> Google Plus
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/697a4a6f/attachment-0001.html>

From rreja2000 at yahoo.com  Mon Aug 25 05:11:46 2014
From: rreja2000 at yahoo.com (Rohit Reja)
Date: Mon, 25 Aug 2014 02:11:46 -0700
Subject: [concurrency-interest] Unbounded thread pool and memory overhead
In-Reply-To: <d6af2cbf-0908-41d1-9688-8c9c467e6ee5.maildroid@localhost>
References: <CA+y1Pu_9u9F3eH-fEynszQRfWv-2nd_wJv6xJV5fyncT9Xo5Pw@mail.gmail.com>
	<1408944350.35278.YahooMailNeo@web120401.mail.ne1.yahoo.com>
	<d6af2cbf-0908-41d1-9688-8c9c467e6ee5.maildroid@localhost>
Message-ID: <1408957906.82119.YahooMailNeo@web120401.mail.ne1.yahoo.com>

I tried using this constructor

?"new ThreadPoolExecutor(10, 100,
? ? ? ? ? ? ? ? 60L, TimeUnit.SECONDS,
>? ? ? ? ? ? ? ? new LinkedBlockingQueue<Runnable>());"
This results into creation of only 1 thread.
According to the documentation, "If there are more than corePoolSize but less than maximumPoolSize threads running, a new thread will be created only if the queue is full."

Essentially what I want is a cachedThreadPool with max threads < unbounded and queueing of tasks beyond a certain threshold.


- Rohit


On Monday, August 25, 2014 2:15 PM, "mail at sergey-mashkov.net" <mail at sergey-mashkov.net> wrote:
 


You can simply create instance of ThreadPoolExecutor using its constructor: 


http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html

So You can specify bounds 


Kind regards
Sergey Mashkov

-----Original Message-----
From: Rohit Reja <rreja2000 at yahoo.com>
To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
Sent: ??, 25 ???. 2014 9:37
Subject: [concurrency-interest] Unbounded thread pool and memory overhead


Hi,

We are using Executors#newCachedThreadPool() at various places in our product. We recently faced an issue that the threads consumed lots of virtual memory in a machine ( and eventually the system has no memory left) due to lots of threads being created. ( We haven't set the Xss param though).

I have 2 questions here:-

1. Why have this API choose not to have bounded threads ? What would be the best practices while using unbounded thread pools.
2. How can I build a thread pool with a minimum threads that grows to a max bound and then the task submission blocks?

Thanks,
Rohit


On Thursday, August 14, 2014 6:03 PM, Victor Grazi <vgrazi at gmail.com> wrote:
 


There's a nice article by Dr. Heinz Kabutz on InfoQ?http://www.infoq.com/articles/Hunting-Concurrency-Bugs-1
Heinz always amazes me!

Twitter: @vgrazi
LinkedIn:?www.linkedin.com/in/victorgrazi/
Java Concurrent Animated:?http://sourceforge.net/projects/javaconcurrenta/
Skype: vgrazi2

Google Plus
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/0ab87f2d/attachment.html>

From peter.levart at gmail.com  Mon Aug 25 07:31:21 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 25 Aug 2014 13:31:21 +0200
Subject: [concurrency-interest] Unbounded thread pool and memory overhead
In-Reply-To: <1408957906.82119.YahooMailNeo@web120401.mail.ne1.yahoo.com>
References: <CA+y1Pu_9u9F3eH-fEynszQRfWv-2nd_wJv6xJV5fyncT9Xo5Pw@mail.gmail.com>	<1408944350.35278.YahooMailNeo@web120401.mail.ne1.yahoo.com>	<d6af2cbf-0908-41d1-9688-8c9c467e6ee5.maildroid@localhost>
	<1408957906.82119.YahooMailNeo@web120401.mail.ne1.yahoo.com>
Message-ID: <53FB1E89.5020409@gmail.com>

On 08/25/2014 11:11 AM, Rohit Reja wrote:
> I tried using this constructor
>
>   "new ThreadPoolExecutor(10, 100,
>                  60L, TimeUnit.SECONDS,
>>                  new LinkedBlockingQueue<Runnable>());"
> This results into creation of only 1 thread.
> According to the documentation, "If there are more than corePoolSize but less than maximumPoolSize threads running, a new thread will be created only if the queue is full."
>
> Essentially what I want is a cachedThreadPool with max threads < unbounded and queueing of tasks beyond a certain threshold.

You're right. Then you can set corePoolSize == maximumPoolSize < 
MAX_INTEGER and then invoke 
ThreadPoolExecutor.allowCoreThreadTimeOut(true). Also set 
'keepAliveTime' to some suitable value > 0.

Regards, Peter

>
>
> - Rohit
>
>
> On Monday, August 25, 2014 2:15 PM, "mail at sergey-mashkov.net" <mail at sergey-mashkov.net> wrote:
>   
>
>
> You can simply create instance of ThreadPoolExecutor using its constructor:
>
>
> http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html
>
> So You can specify bounds
>
>
> Kind regards
> Sergey Mashkov
>
> -----Original Message-----
> From: Rohit Reja <rreja2000 at yahoo.com>
> To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
> Sent: ??, 25 ???. 2014 9:37
> Subject: [concurrency-interest] Unbounded thread pool and memory overhead
>
>
> Hi,
>
> We are using Executors#newCachedThreadPool() at various places in our product. We recently faced an issue that the threads consumed lots of virtual memory in a machine ( and eventually the system has no memory left) due to lots of threads being created. ( We haven't set the Xss param though).
>
> I have 2 questions here:-
>
> 1. Why have this API choose not to have bounded threads ? What would be the best practices while using unbounded thread pools.
> 2. How can I build a thread pool with a minimum threads that grows to a max bound and then the task submission blocks?
>
> Thanks,
> Rohit
>
>
> On Thursday, August 14, 2014 6:03 PM, Victor Grazi <vgrazi at gmail.com> wrote:
>   
>
>
> There's a nice article by Dr. Heinz Kabutz on InfoQ http://www.infoq.com/articles/Hunting-Concurrency-Bugs-1
> Heinz always amazes me!
>
> Twitter: @vgrazi
> LinkedIn: www.linkedin.com/in/victorgrazi/
> Java Concurrent Animated: http://sourceforge.net/projects/javaconcurrenta/
> Skype: vgrazi2
>
> Google Plus
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/452f7a19/attachment.html>

From peter.levart at gmail.com  Mon Aug 25 10:16:31 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 25 Aug 2014 16:16:31 +0200
Subject: [concurrency-interest] CompletableFuture with delay
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
Message-ID: <53FB453F.30106@gmail.com>

On 08/25/2014 09:55 AM, Millies, Sebastian wrote:
> Hello there,
>
> I'd like to simulate asynchronous IO events in a test system (without actual IO).
> For this purpose, I have defined a method that creates a future which returns a value after a delay.
> In contrast to Future#get(Long,TimeUnit) this method does not wait, but uses a separate ScheduledFuture
> to complete the future. Code is shown below.
>
> So far, so good. But I also want to cancel the scheduled task (the ScheduledFuture returned from
> ScheduledExecutorService#schedule()) when the future is cancelled before the timeout. (So that
> I can keep my timeout tasks from piling up when timeouts are long in relation to the real
> computations.)
>
> Please look at the code below, where I create an additional future with whenComplete and use
> that to cancel the task. Is that a correct solution? Or could this additional future be optimized
> away or be garbage collected? Does the variable "future" that is returned from the method hold a
> reference to the additional future? I couldn't tell from the source code of CompletableFuture.
>
>    private static final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(TIMER_THREADS);
>
>    public static <T> CompletableFuture<T> delayedSuccess(T value, int delay, TimeUnit unit) {
>      CompletableFuture<T> future = new CompletableFuture<T>();
>      ScheduledFuture<Boolean> task = scheduler.schedule(() -> future.complete(value), delay, unit);
>      future.whenComplete((t, ex) -> {
>        if (future.isCancelled())
>          task.cancel(true);               // <== HERE
>      });
>      return future;
>    }

Hi Sebastian,

The returned CompletableFuture 'future' references the BiConsumer 
function (in your case a lambda) until it completes. At that time it 
invokes it and releases the reference to it, so it can be GC-ed. The 
lambda captures the ScheduledFuture 'task' so it implicitly references 
it, when the lambda goes away, so would the captured 'task'.

But the returned 'future' (and BiConsumer lambda referenced by it) is 
captured by another lambda, the Runnable passed to scheduler.schedule(). 
This lambda and ScheduledFuture 'task' are referenced by the 'scheduler' 
until the time comes and the scheduler executes the delayed task and 
then they are released. This happens even when the ScheduledFuture 
'task' is canceled beforehand. Unless you configure 
ScheduledThreadPoolExecutor to remove the task on cancel:

((ScheduledThreadPoolExecutor) scheduler).setRemoveOnCancelPolicy(true);

By setting that, I think you get the timely clean-up of all the objects 
involved.

Regards, Peter

> Sebastian Millies
> Expert Java Business Analytics
> Phone: +49 681 210 3221 | Fax: +49 681 210 1801 | Sebastian.Millies at softwareag.com<mailto:Sebastian.Millies at ids-scheer.com>
>
> Software AG -- Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany -- Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/1989927b/attachment-0001.html>

From zhong.j.yu at gmail.com  Mon Aug 25 10:42:53 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Mon, 25 Aug 2014 09:42:53 -0500
Subject: [concurrency-interest] CompletableFuture with delay
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
Message-ID: <CACuKZqFPD9VsaxouGL2gGM99y1B-0v9dStQbNfE62v-pJZJk5Q@mail.gmail.com>

On Mon, Aug 25, 2014 at 2:55 AM, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:

     future.whenComplete((t, ex) -> {
>
>       if (future.isCancelled())
>
>         task.cancel(true);               // <== HERE
>
>     });
>
>
Shouldn't you always call `task.cancel()`, regardless whether the
completion of the future is due to cancellation?

Zhong Yu
bayou.io

P.S. You might want to look into other async APIs in java, like bayou.io
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/99a9fe22/attachment.html>

From Sebastian.Millies at softwareag.com  Mon Aug 25 11:00:12 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Mon, 25 Aug 2014 15:00:12 +0000
Subject: [concurrency-interest] CompletableFuture with delay
In-Reply-To: <CACuKZqFPD9VsaxouGL2gGM99y1B-0v9dStQbNfE62v-pJZJk5Q@mail.gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
	<CACuKZqFPD9VsaxouGL2gGM99y1B-0v9dStQbNfE62v-pJZJk5Q@mail.gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA638@HQMBX5.eur.ad.sag>

Well, I am supposing that if the future completes normally, that will be due to the task being done. That is, I am supposing that no one else would call complete() on the future. But of course you are correct if I were to go and use the construct in contexts were that assumption might be wrong.

FWIW: here?s how I actually use it to return a default value if an asynchronous supplier times out according to some specified delay. (The reference ?future? from the OP is assigned to the reference ?useDefault? in the following code, useDefault.cancel() is supposed to trigger the task cancellation and associated cleanup.)


/**

  * Returns a new CompletableFuture that is asynchronously completed by a task running in the given executor with the

  * value obtained by calling the given Supplier. If the supplier does not return the value before a specified

  * timeout, then the specified default value is used to complete the future. If either future completes, the other

  * will be cancelled.

  */

  public static <T> CompletableFuture<T> supplyAsyncWithTimeout(Supplier<T> supplier, Executor executor, T defaultValue,

    int timeout, TimeUnit unit) {

    CompletableFuture<T> compute = CompletableFuture.supplyAsync(supplier, executor);

    CompletableFuture<T> useDefault = delayedSuccess(defaultValue, timeout, unit);

      return compute.applyToEither(useDefault, Function.identity()).whenComplete((t, ex) -> {

              compute.cancel(true);

              useDefault.cancel(true);

            });

  }


n  Sebastian

From: Zhong Yu [mailto:zhong.j.yu at gmail.com]
Sent: Monday, August 25, 2014 4:43 PM
To: Millies, Sebastian
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] CompletableFuture with delay

On Mon, Aug 25, 2014 at 2:55 AM, Millies, Sebastian <Sebastian.Millies at softwareag.com<mailto:Sebastian.Millies at softwareag.com>> wrote:
    future.whenComplete((t, ex) -> {
      if (future.isCancelled())
        task.cancel(true);               // <== HERE
    });


Shouldn't you always call `task.cancel()`, regardless whether the completion of the future is due to cancellation?
Zhong Yu
bayou.io<http://bayou.io>
P.S. You might want to look into other async APIs in java, like bayou.io<http://bayou.io>

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/ac585153/attachment.html>

From Sebastian.Millies at softwareag.com  Mon Aug 25 11:11:00 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Mon, 25 Aug 2014 15:11:00 +0000
Subject: [concurrency-interest] CompletableFuture with delay
In-Reply-To: <53FB453F.30106@gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
	<53FB453F.30106@gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA65F@HQMBX5.eur.ad.sag>

From: Peter Levart [mailto:peter.levart at gmail.com]
Sent: Monday, August 25, 2014 4:17 PM
To: Millies, Sebastian; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] CompletableFuture with delay

On 08/25/2014 09:55 AM, Millies, Sebastian wrote:

Hello there,



I'd like to simulate asynchronous IO events in a test system (without actual IO).

For this purpose, I have defined a method that creates a future which returns a value after a delay.

In contrast to Future#get(Long,TimeUnit) this method does not wait, but uses a separate ScheduledFuture

to complete the future. Code is shown below.



So far, so good. But I also want to cancel the scheduled task (the ScheduledFuture returned from

ScheduledExecutorService#schedule()) when the future is cancelled before the timeout. (So that

I can keep my timeout tasks from piling up when timeouts are long in relation to the real

computations.)



Please look at the code below, where I create an additional future with whenComplete and use

that to cancel the task. Is that a correct solution? Or could this additional future be optimized

away or be garbage collected? Does the variable "future" that is returned from the method hold a

reference to the additional future? I couldn't tell from the source code of CompletableFuture.



  private static final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(TIMER_THREADS);



  public static <T> CompletableFuture<T> delayedSuccess(T value, int delay, TimeUnit unit) {

    CompletableFuture<T> future = new CompletableFuture<T>();

    ScheduledFuture<Boolean> task = scheduler.schedule(() -> future.complete(value), delay, unit);

    future.whenComplete((t, ex) -> {

      if (future.isCancelled())

        task.cancel(true);               // <== HERE

    });

    return future;

  }

>Hi Sebastian,
>
>The returned CompletableFuture 'future' references the BiConsumer function (in
>your case a lambda) until it completes. At that time it invokes it and
>releases the reference to it, so it can be GC-ed. The lambda captures the
>ScheduledFuture 'task' so it implicitly references it, when the lambda goes
>away, so would the captured 'task'.
>
>But the returned 'future' (and BiConsumer lambda referenced by it) is captured
>by another lambda, the Runnable passed to scheduler.schedule(). This lambda
>and ScheduledFuture 'task' are referenced by the 'scheduler' until the time
>comes and the scheduler executes the delayed task and then they are released.
>This happens even when the ScheduledFuture 'task' is canceled beforehand.
>Unless you configure ScheduledThreadPoolExecutor to remove the task on cancel:
>
>((ScheduledThreadPoolExecutor) scheduler).setRemoveOnCancelPolicy(true);
>
>By setting that, I think you get the timely clean-up of all the objects
>involved.
>
>Regards, Peter


Hello Peter,

thank you for the detailed explanation.
I suppose there is no way to avoid the downcast?

Regards,
Sebastian

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/a98733b2/attachment-0001.html>

From zhong.j.yu at gmail.com  Mon Aug 25 11:32:44 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Mon, 25 Aug 2014 10:32:44 -0500
Subject: [concurrency-interest] CompletableFuture with delay
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA638@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
	<CACuKZqFPD9VsaxouGL2gGM99y1B-0v9dStQbNfE62v-pJZJk5Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FA638@HQMBX5.eur.ad.sag>
Message-ID: <CACuKZqF2u55sBJtBJ0MgnJF-X27j5PzoXORS_o_VoO8k3L+ozA@mail.gmail.com>

If you expose it as CompletableFuture, the caller is able to complete() it.
If you expose it as CompletionStage, the caller has no way to "cancel()" it.

Also note that CompletableFuture composition does not propagate
cancellation. If the caller of your supplyAsyncWithTimeout() decides to
cancel the returned future for some other reason, it will not trigger
`compute.cancel()` or `task.cancel()`.

    future1 = ...
    future2 = future1.whenComplete(...)
    ...
    future2.cancel()  // does not affect future1

Zhong Yu
bayou.io
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/713b32b9/attachment.html>

From peter.levart at gmail.com  Mon Aug 25 12:21:46 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 25 Aug 2014 18:21:46 +0200
Subject: [concurrency-interest] CompletableFuture with delay
In-Reply-To: <CACuKZqF2u55sBJtBJ0MgnJF-X27j5PzoXORS_o_VoO8k3L+ozA@mail.gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>	<CACuKZqFPD9VsaxouGL2gGM99y1B-0v9dStQbNfE62v-pJZJk5Q@mail.gmail.com>	<32F15738E8E5524DA4F01A0FA4A8E490DF0FA638@HQMBX5.eur.ad.sag>
	<CACuKZqF2u55sBJtBJ0MgnJF-X27j5PzoXORS_o_VoO8k3L+ozA@mail.gmail.com>
Message-ID: <53FB629A.50302@gmail.com>

On 08/25/2014 05:32 PM, Zhong Yu wrote:
> If you expose it as CompletableFuture, the caller is able to complete() it.
> If you expose it as CompletionStage, the caller has no way to "cancel()" it.
>
> Also note that CompletableFuture composition does not propagate
> cancellation. If the caller of your supplyAsyncWithTimeout() decides to
> cancel the returned future for some other reason, it will not trigger
> `compute.cancel()` or `task.cancel()`.
>
>      future1 = ...
>      future2 = future1.whenComplete(...)
>      ...
>      future2.cancel()  // does not affect future1

I think that's because future2 is dependent on future1 and not vice 
versa. You could have:

future2 = future1.whenComplete(...);
future3 = future1.whenComplete(...);

Now of course, cancelling future2 should not have an effect on 
computation of future3.

If you call future1.cancel(), then both future2 and future3 will 
complete with CompletionException having the future1's 
CancelationException as their cause...

But that's not what Sebastian is doing. He is returning 'future1' and 
cancelling it does call the completion function.

Regards, Peter

>
> Zhong Yu
> bayou.io
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/e0eeac55/attachment.html>

From peter.levart at gmail.com  Mon Aug 25 12:27:06 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 25 Aug 2014 18:27:06 +0200
Subject: [concurrency-interest] CompletableFuture with delay
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA65F@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
	<53FB453F.30106@gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FA65F@HQMBX5.eur.ad.sag>
Message-ID: <53FB63DA.8060906@gmail.com>

On 08/25/2014 05:11 PM, Millies, Sebastian wrote:
>> >((ScheduledThreadPoolExecutor) scheduler).setRemoveOnCancelPolicy(true);
>> >
>> >By setting that, I think you get the timely clean-up of all the objects
>> >involved.
>> >
>> >Regards, Peter
> Hello Peter,
>
> thank you for the detailed explanation.
> I suppose there is no way to avoid the downcast?

Unless you do something like this:

private static final ScheduledThreadPoolExecutor scheduler = new ScheduledThreadPoolExecutor(TIMER_THREADS);



Peter

>
> Regards,
> Sebastian


From Sebastian.Millies at softwareag.com  Mon Aug 25 14:48:01 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Mon, 25 Aug 2014 18:48:01 +0000
Subject: [concurrency-interest] CompletableFuture with delay
In-Reply-To: <53FB629A.50302@gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
	<CACuKZqFPD9VsaxouGL2gGM99y1B-0v9dStQbNfE62v-pJZJk5Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FA638@HQMBX5.eur.ad.sag>
	<CACuKZqF2u55sBJtBJ0MgnJF-X27j5PzoXORS_o_VoO8k3L+ozA@mail.gmail.com>
	<53FB629A.50302@gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA766@HQMBX5.eur.ad.sag>

Although I really was not doing that in my OP, showing method delayedSuccess(), I did fall into exactly this trap with my second example, showing method supplyAsyncWithTimeout(), which I think is what Zhong was talking about. That method should be reformulated like this:


public static <T> CompletableFuture<T> supplyAsyncWithTimeout(Supplier<T> supplier, Executor executor, T defaultValue,

      int timeout, TimeUnit unit) {

    CompletableFuture<T> compute = CompletableFuture.supplyAsync(supplier, executor);

    CompletableFuture<T> useDefault = delayedSuccess(defaultValue, timeout, unit);

    CompletableFuture<T> either = compute.applyToEither(useDefault, Function.identity());

    either.whenComplete((t, ex) -> {

      compute.cancel(true);

      useDefault.cancel(true);

    });

    return either;

  }

Now the returned future can be cancelled, the cancellation will cause the completion method to be called, which in turn will propagate the cancellation to both embedded futures.


n  Sebastian

From: Peter Levart [mailto:peter.levart at gmail.com]
Sent: Monday, August 25, 2014 6:22 PM
To: Zhong Yu; Millies, Sebastian
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] CompletableFuture with delay

On 08/25/2014 05:32 PM, Zhong Yu wrote:

If you expose it as CompletableFuture, the caller is able to complete() it.

If you expose it as CompletionStage, the caller has no way to "cancel()" it.



Also note that CompletableFuture composition does not propagate

cancellation. If the caller of your supplyAsyncWithTimeout() decides to

cancel the returned future for some other reason, it will not trigger

`compute.cancel()` or `task.cancel()`.



    future1 = ...

    future2 = future1.whenComplete(...)

    ...

    future2.cancel()  // does not affect future1

I think that's because future2 is dependent on future1 and not vice versa. You could have:

future2 = future1.whenComplete(...);
future3 = future1.whenComplete(...);

Now of course, cancelling future2 should not have an effect on computation of future3.

If you call future1.cancel(), then both future2 and future3 will complete with CompletionException having the future1's CancelationException as their cause...

But that's not what Sebastian is doing. He is returning 'future1' and cancelling it does call the completion function.

Regards, Peter







Zhong Yu

bayou.io






_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/92b85834/attachment-0001.html>

From zhong.j.yu at gmail.com  Mon Aug 25 14:50:49 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Mon, 25 Aug 2014 13:50:49 -0500
Subject: [concurrency-interest] CompletableFuture with delay
In-Reply-To: <53FB629A.50302@gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FA3C9@HQMBX5.eur.ad.sag>
	<CACuKZqFPD9VsaxouGL2gGM99y1B-0v9dStQbNfE62v-pJZJk5Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FA638@HQMBX5.eur.ad.sag>
	<CACuKZqF2u55sBJtBJ0MgnJF-X27j5PzoXORS_o_VoO8k3L+ozA@mail.gmail.com>
	<53FB629A.50302@gmail.com>
Message-ID: <CACuKZqH6g_3KWQ-dDMmA_mFZNFWX=J2Xp2gMdYSeO+ZEhJOBEQ@mail.gmail.com>

On Mon, Aug 25, 2014 at 11:21 AM, Peter Levart <peter.levart at gmail.com> wrote:
> On 08/25/2014 05:32 PM, Zhong Yu wrote:
>
> If you expose it as CompletableFuture, the caller is able to complete() it.
> If you expose it as CompletionStage, the caller has no way to "cancel()" it.
>
> Also note that CompletableFuture composition does not propagate
> cancellation. If the caller of your supplyAsyncWithTimeout() decides to
> cancel the returned future for some other reason, it will not trigger
> `compute.cancel()` or `task.cancel()`.
>
>     future1 = ...
>     future2 = future1.whenComplete(...)
>     ...
>     future2.cancel()  // does not affect future1
>
>
> I think that's because future2 is dependent on future1 and not vice versa.
> You could have:
>
> future2 = future1.whenComplete(...);
> future3 = future1.whenComplete(...);
>
> Now of course, cancelling future2 should not have an effect on computation
> of future3.
>
> If you call future1.cancel(), then both future2 and future3 will complete
> with CompletionException having the future1's CancelationException as their
> cause...
>
> But that's not what Sebastian is doing. He is returning 'future1' and
> cancelling it does call the completion function.

I was referring to the new code he posted

    supplyAsyncWithTimeout(...) {
        ....
        return compute.applyToEither(...).whenComplete(...)

this is problematic because cancellation on returned future will not
reach inside to cancel `compute` or the scheduled task.

Whether cancellation should propagate by default depends on the target
use cases. In this particular case we see how tedious it is to have to
manually manage cancellation propagation.

Zhong Yu
bayou.io

From rreja2000 at yahoo.com  Mon Aug 25 23:06:41 2014
From: rreja2000 at yahoo.com (Rohit Reja)
Date: Mon, 25 Aug 2014 20:06:41 -0700
Subject: [concurrency-interest] Unbounded thread pool and memory overhead
In-Reply-To: <53FB1E89.5020409@gmail.com>
References: <CA+y1Pu_9u9F3eH-fEynszQRfWv-2nd_wJv6xJV5fyncT9Xo5Pw@mail.gmail.com>	<1408944350.35278.YahooMailNeo@web120401.mail.ne1.yahoo.com>	<d6af2cbf-0908-41d1-9688-8c9c467e6ee5.maildroid@localhost>
	<1408957906.82119.YahooMailNeo@web120401.mail.ne1.yahoo.com>
	<53FB1E89.5020409@gmail.com>
Message-ID: <1409022401.9436.YahooMailNeo@web120406.mail.ne1.yahoo.com>

Thanks Peter. The suggested configuration works.

Regards,
Rohit


On Monday, August 25, 2014 5:01 PM, Peter Levart <peter.levart at gmail.com> wrote:
 


On 08/25/2014 11:11 AM, Rohit Reja wrote:

I tried using this constructor ?"new ThreadPoolExecutor(10, 100,
? ? ? ? ? ? ? ? 60L, TimeUnit.SECONDS, 
>? ? ? ? ? ? ? ? new LinkedBlockingQueue<Runnable>());" 
>This results into creation of only 1 thread.
According to the documentation, "If there are more than corePoolSize but less than maximumPoolSize threads running, a new thread will be created only if the queue is full." Essentially what I want is a cachedThreadPool with max threads < unbounded and queueing of tasks beyond a certain threshold.
You're right. Then you can set corePoolSize == maximumPoolSize <
    MAX_INTEGER and then invoke
    ThreadPoolExecutor.allowCoreThreadTimeOut(true). Also set
    'keepAliveTime' to some suitable value > 0.

Regards, Peter


- Rohit On Monday, August 25, 2014 2:15 PM, "mail at sergey-mashkov.net" <mail at sergey-mashkov.net> wrote: You can simply create instance of ThreadPoolExecutor using its constructor: http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html So You can specify bounds  Kind regards
Sergey Mashkov -----Original Message-----
From: Rohit Reja <rreja2000 at yahoo.com> To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu> Sent: ??, 25 ???. 2014 9:37
Subject: [concurrency-interest] Unbounded thread pool and memory overhead Hi, We are using Executors#newCachedThreadPool() at various places in our product. We recently faced an issue that the threads consumed lots of virtual memory in a machine ( and eventually the system has no memory left) due to lots of threads being created. ( We haven't set the Xss param though). I have 2 questions here:- 1. Why have this API choose not to have bounded threads ? What would be the best practices while using unbounded thread pools.
2. How can I build a thread pool with a minimum threads that grows to a max bound and then the task submission blocks? Thanks,
Rohit On Thursday, August 14, 2014 6:03 PM, Victor Grazi <vgrazi at gmail.com> wrote: There's a nice article by Dr. Heinz Kabutz on InfoQ?http://www.infoq.com/articles/Hunting-Concurrency-Bugs-1 Heinz always amazes me! Twitter: @vgrazi
LinkedIn:?www.linkedin.com/in/victorgrazi/ Java Concurrent Animated:?http://sourceforge.net/projects/javaconcurrenta/ Skype: vgrazi2 Google Plus
_______________________________________________
Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
>
>
>
>_______________________________________________
Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140825/da0cbe85/attachment.html>

From Sebastian.Millies at softwareag.com  Tue Aug 26 18:15:07 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Tue, 26 Aug 2014 22:15:07 +0000
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>

Hello there,

over at comp.lang.java.programmer, Dario Crivelli of lightstreamer.com has made an interesting observation concerning error handling with CompletableFuture.

He said: ?[CompletableFuture] views exceptions as some sort of showstoppers, not as special conditions each of which may deserve a different treatment. In fact, it does not allow you to selectively handling some types of exceptions while keeping others for further steps: you can only handle all types of possible exceptions in the same step, perhaps to log an error at the end of the processing? and wonders why this is so.



If you pass futures around in the widespread kind of hierarchical structure, where each layer is responsible for handling certain exceptions, and exceptions may be converted and rethrown (think of data querying exceptions becoming converted to some kind of business logic exception), that may indeed become problematic. Looking at the signatures of

handle<http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#handle-java.util.function.BiFunction->(BiFunction<http://docs.oracle.com/javase/8/docs/api/java/util/function/BiFunction.html><? super T<http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html>,Throwable<http://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html>,? extends U> fn)
and
exceptionally<http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#exceptionally-java.util.function.Function->(Function<http://docs.oracle.com/javase/8/docs/api/java/util/function/Function.html><Throwable<http://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html>,? extends T<http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html>> fn)
one sees that one gets a catchall clause for Throwables, and that rethrowing is impossible because (Bi)Function#apply() does not throw.

I have not constructed a code example where this might be relevant. Nevertheless, I am passing Dario?s observation to this list, because I?d like to know the reason behind this design decision. I?d welcome any thoughts on this, including wether you believe that it really imposes a relevant restriction, and if so, how one might work around it.

Regards,
Sebastian



Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140826/737f7339/attachment.html>

From zhong.j.yu at gmail.com  Tue Aug 26 20:02:15 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 26 Aug 2014 19:02:15 -0500
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
Message-ID: <CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>

On Tue, Aug 26, 2014 at 5:15 PM, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:

>  Hello there,
>
>
>
> over at comp.lang.java.programmer, Dario Crivelli of lightstreamer.com
> has made an interesting observation concerning error handling with
> CompletableFuture.
>
> He said: ?[CompletableFuture] views exceptions as some sort of showstoppers, not as special conditions each of which may deserve a different treatment. In fact, it does not allow you to selectively handling some types of exceptions while keeping others for further steps: you can only handle all types of possible exceptions in the same step, perhaps to log an error at the end of the processing? and wonders why this is so.
>
>
>
> If you pass futures around in the widespread kind of hierarchical structure, where each layer is responsible for handling certain exceptions, and exceptions may be converted and rethrown (think of data querying exceptions becoming converted to some kind of business logic exception), that may indeed become problematic. Looking at the signatures of
>
> handle <http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#handle-java.util.function.BiFunction->(BiFunction <http://docs.oracle.com/javase/8/docs/api/java/util/function/BiFunction.html><? super T <http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html>,Throwable <http://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html>,? extends U> fn)
>
> and
>
> exceptionally
> <http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#exceptionally-java.util.function.Function->
> (Function
> <http://docs.oracle.com/javase/8/docs/api/java/util/function/Function.html>
> <Throwable
> <http://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html>,?
> extends T
> <http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html>
> > fn)
>
> one sees that one gets a catchall clause for Throwables, and that
> rethrowing is impossible because (Bi)Function#apply() does not throw.
>
>
>
>
>

Apparently, CompletableFuture does not  take too kindly to checked
exceptions [1].

A lot of people even consider checked exceptions a design mistake on the
language level. Nevertheless, most Java programmers and APIs do use checked
exceptions, for returning alternative results, and for flow controls.

A workaround is to create your own helper method like

    static <T,X> CF<T> catch_(CF<T> cf, Class<X> exceptionType, FuncE<X,T>
exceptionHandler) { ... }

where FuncE is similar to Function, except the apply() method throws
`Exception`

Then you can use it like

    catch_( future, SomeException.class, ex->someValue )

    catch_( future, SomeExcpetion.class, ex->throw new AnotherException() )

That is what we did in bayou Async API [2]. The problem you posted early
can be solved in bayou as

    Async.execute(...)
              .timeout(...)
              .catch_(TimeoutException.class, ex->defaultValue);

[1]
http://cs.oswego.edu/pipermail/concurrency-interest/2012-December/010486.html
[2]
http://bayou.io/release/0.9/docs/async/Async_Programming.html#Sequential_Actions



Zhong Yu
bayou.io
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140826/92af45a5/attachment.html>

From Sebastian.Millies at softwareag.com  Wed Aug 27 04:39:30 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Wed, 27 Aug 2014 08:39:30 +0000
Subject: [concurrency-interest] Layered exception handling with
 CompletableFuture
In-Reply-To: <CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>

Hi,

thank you for the helpful response.

It made me remember dimly some discussion of exception transparency on lambda-dev, and how it finally wasn?t possible to make it work. I do not believe this will ever be solved on the language level, and I regret that. In the same vein CompletableFuture.suppyAsync() does not accept a Callable, only a Supplier. In my situation there?s a bunch of pre-existing, exception throwing application methods (not to mention the Java library methods) that I now have a new requirement to combine asynchronously. So I find myself wrapping all the Callables, and tunneling the exceptions with RuntimeException, as Doug Lea recommends.

I have looked at your suggestion, and I?m sorry to say I do not see how it would help, unless I rewrite everything from scratch or use bayou (would need something like your Async interface).
And it also does not seem to address the one-stop exception-handling concern, because guess I would somehow need to call CompletableFuture#handle() and that can?t throw. Even in bayou, from a cursory look at Async.java, it seems that catch_() doesn?t rethrow the  exception if it is not of the appropriate class.

Best,
Sebastian

From: Zhong Yu [mailto:zhong.j.yu at gmail.com]
Sent: Wednesday, August 27, 2014 2:02 AM
To: Millies, Sebastian
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Layered exception handling with CompletableFuture

[OP snipped]
Apparently, CompletableFuture does not  take too kindly to checked exceptions [1].
A lot of people even consider checked exceptions a design mistake on the language level. Nevertheless, most Java programmers and APIs do use checked exceptions, for returning alternative results, and for flow controls.
A workaround is to create your own helper method like
    static <T,X> CF<T> catch_(CF<T> cf, Class<X> exceptionType, FuncE<X,T> exceptionHandler) { ... }
where FuncE is similar to Function, except the apply() method throws `Exception`

Then you can use it like
    catch_( future, SomeException.class, ex->someValue )
    catch_( future, SomeExcpetion.class, ex->throw new AnotherException() )
That is what we did in bayou Async API [2]. The problem you posted early can be solved in bayou as
    Async.execute(...)
              .timeout(...)
              .catch_(TimeoutException.class, ex->defaultValue);

[1] http://cs.oswego.edu/pipermail/concurrency-interest/2012-December/010486.html
[2] http://bayou.io/release/0.9/docs/async/Async_Programming.html#Sequential_Actions


Zhong Yu
bayou.io<http://bayou.io>

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140827/fe82a75f/attachment-0001.html>

From viktor.klang at gmail.com  Wed Aug 27 05:07:18 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 27 Aug 2014 11:07:18 +0200
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
Message-ID: <CANPzfU9kyfHVio=axcnMHP4aGyEN7PvcCSVjHBPknMLPYnJDjQ@mail.gmail.com>

I -always- use "sneakyThrows" to turn checked exceptions into unchecked
ones?there's 0 value in checked exceptions. (Checked exceptions is a Java
language feature, not a JVM feature)

http://www.jayway.com/2010/01/29/sneaky-throw/

The only rough edge I've found is with dynamic Proxies, which encode this
language feature into a runtime feature:
http://docs.oracle.com/javase/7/docs/api/java/lang/reflect/UndeclaredThrowableException.html


On Wed, Aug 27, 2014 at 10:39 AM, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:

>  Hi,
>
>
>
> thank you for the helpful response.
>
>
>
> It made me remember dimly some discussion of exception transparency on
> lambda-dev, and how it finally wasn?t possible to make it work. I do not
> believe this will ever be solved on the language level, and I regret that.
> In the same vein CompletableFuture.suppyAsync() does not accept a Callable,
> only a Supplier. In my situation there?s a bunch of pre-existing, exception
> throwing application methods (not to mention the Java library methods) that
> I now have a new requirement to combine asynchronously. So I find myself
> wrapping all the Callables, and tunneling the exceptions with
> RuntimeException, as Doug Lea recommends.
>
>
>
> I have looked at your suggestion, and I?m sorry to say I do not see how it
> would help, unless I rewrite everything from scratch or use bayou (would
> need something like your Async interface).
>
> And it also does not seem to address the one-stop exception-handling
> concern, because guess I would somehow need to call
> CompletableFuture#handle() and that can?t throw. Even in bayou, from a
> cursory look at Async.java, it seems that catch_() doesn?t rethrow the
> exception if it is not of the appropriate class.
>
>
>
> Best,
>
> Sebastian
>
>
>
> *From:* Zhong Yu [mailto:zhong.j.yu at gmail.com]
> *Sent:* Wednesday, August 27, 2014 2:02 AM
> *To:* Millies, Sebastian
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Layered exception handling with
> CompletableFuture
>
>
>
> [OP snipped]
>
> Apparently, CompletableFuture does not  take too kindly to checked
> exceptions [1].
>
> A lot of people even consider checked exceptions a design mistake on the
> language level. Nevertheless, most Java programmers and APIs do use checked
> exceptions, for returning alternative results, and for flow controls.
>
> A workaround is to create your own helper method like
>
>     static <T,X> CF<T> catch_(CF<T> cf, Class<X> exceptionType, FuncE<X,T>
> exceptionHandler) { ... }
>
> where FuncE is similar to Function, except the apply() method throws
> `Exception`
>
>
>
> Then you can use it like
>
>     catch_( future, SomeException.class, ex->someValue )
>
>     catch_( future, SomeExcpetion.class, ex->throw new AnotherException() )
>
> That is what we did in bayou Async API [2]. The problem you posted early
> can be solved in bayou as
>
>     Async.execute(...)
>               .timeout(...)
>               .catch_(TimeoutException.class, ex->defaultValue);
>
>
> [1]
> http://cs.oswego.edu/pipermail/concurrency-interest/2012-December/010486.html
> [2]
> http://bayou.io/release/0.9/docs/async/Async_Programming.html#Sequential_Actions
>
>
>   Zhong Yu
>
> bayou.io
>
>    Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297
> Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB
> 1562 - Vorstand/Management Board: Karl-Heinz Streibich
> (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140827/6dd042e6/attachment.html>

From mthornton at optrak.com  Wed Aug 27 06:14:57 2014
From: mthornton at optrak.com (Mark Thornton)
Date: Wed, 27 Aug 2014 11:14:57 +0100
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <CANPzfU9kyfHVio=axcnMHP4aGyEN7PvcCSVjHBPknMLPYnJDjQ@mail.gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
	<CANPzfU9kyfHVio=axcnMHP4aGyEN7PvcCSVjHBPknMLPYnJDjQ@mail.gmail.com>
Message-ID: <CAC_SY72NL9NxONvKYXG+rqS9GJKOqMrAooJUASEM3K9GQORHRw@mail.gmail.com>

Except that you then have trouble catching a checked exception that appears
not to be thrown. You have to wrap the code in a method which does declare
the checked exception(s) you want to catch.

Mark Thornton

On Wednesday, 27 August 2014, ?iktor ?lang <viktor.klang at gmail.com> wrote:

> I -always- use "sneakyThrows" to turn checked exceptions into unchecked
> ones?there's 0 value in checked exceptions. (Checked exceptions is a Java
> language feature, not a JVM feature)
>
> http://www.jayway.com/2010/01/29/sneaky-throw/
>
> The only rough edge I've found is with dynamic Proxies, which encode this
> language feature into a runtime feature:
> http://docs.oracle.com/javase/7/docs/api/java/lang/reflect/UndeclaredThrowableException.html
>
>
> On Wed, Aug 27, 2014 at 10:39 AM, Millies, Sebastian <
> Sebastian.Millies at softwareag.com
> <javascript:_e(%7B%7D,'cvml','Sebastian.Millies at softwareag.com');>> wrote:
>
>>  Hi,
>>
>>
>>
>> thank you for the helpful response.
>>
>>
>>
>> It made me remember dimly some discussion of exception transparency on
>> lambda-dev, and how it finally wasn?t possible to make it work. I do not
>> believe this will ever be solved on the language level, and I regret that.
>> In the same vein CompletableFuture.suppyAsync() does not accept a Callable,
>> only a Supplier. In my situation there?s a bunch of pre-existing, exception
>> throwing application methods (not to mention the Java library methods) that
>> I now have a new requirement to combine asynchronously. So I find myself
>> wrapping all the Callables, and tunneling the exceptions with
>> RuntimeException, as Doug Lea recommends.
>>
>>
>>
>> I have looked at your suggestion, and I?m sorry to say I do not see how
>> it would help, unless I rewrite everything from scratch or use bayou (would
>> need something like your Async interface).
>>
>> And it also does not seem to address the one-stop exception-handling
>> concern, because guess I would somehow need to call
>> CompletableFuture#handle() and that can?t throw. Even in bayou, from a
>> cursory look at Async.java, it seems that catch_() doesn?t rethrow the
>> exception if it is not of the appropriate class.
>>
>>
>>
>> Best,
>>
>> Sebastian
>>
>>
>>
>> *From:* Zhong Yu [mailto:zhong.j.yu at gmail.com
>> <javascript:_e(%7B%7D,'cvml','zhong.j.yu at gmail.com');>]
>> *Sent:* Wednesday, August 27, 2014 2:02 AM
>> *To:* Millies, Sebastian
>> *Cc:* concurrency-interest at cs.oswego.edu
>> <javascript:_e(%7B%7D,'cvml','concurrency-interest at cs.oswego.edu');>
>> *Subject:* Re: [concurrency-interest] Layered exception handling with
>> CompletableFuture
>>
>>
>>
>> [OP snipped]
>>
>> Apparently, CompletableFuture does not  take too kindly to checked
>> exceptions [1].
>>
>> A lot of people even consider checked exceptions a design mistake on the
>> language level. Nevertheless, most Java programmers and APIs do use checked
>> exceptions, for returning alternative results, and for flow controls.
>>
>> A workaround is to create your own helper method like
>>
>>     static <T,X> CF<T> catch_(CF<T> cf, Class<X> exceptionType,
>> FuncE<X,T> exceptionHandler) { ... }
>>
>> where FuncE is similar to Function, except the apply() method throws
>> `Exception`
>>
>>
>>
>> Then you can use it like
>>
>>     catch_( future, SomeException.class, ex->someValue )
>>
>>     catch_( future, SomeExcpetion.class, ex->throw new AnotherException()
>> )
>>
>> That is what we did in bayou Async API [2]. The problem you posted early
>> can be solved in bayou as
>>
>>     Async.execute(...)
>>               .timeout(...)
>>               .catch_(TimeoutException.class, ex->defaultValue);
>>
>>
>> [1]
>> http://cs.oswego.edu/pipermail/concurrency-interest/2012-December/010486.html
>> [2]
>> http://bayou.io/release/0.9/docs/async/Async_Programming.html#Sequential_Actions
>>
>>
>>   Zhong Yu
>>
>> bayou.io
>>
>>    Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297
>> Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB
>> 1562 - Vorstand/Management Board: Karl-Heinz Streibich
>> (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; -
>> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
>> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> <javascript:_e(%7B%7D,'cvml','Concurrency-interest at cs.oswego.edu');>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Cheers,
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140827/8f17ef8e/attachment-0001.html>

From viktor.klang at gmail.com  Wed Aug 27 06:57:59 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 27 Aug 2014 12:57:59 +0200
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <CAC_SY72NL9NxONvKYXG+rqS9GJKOqMrAooJUASEM3K9GQORHRw@mail.gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
	<CANPzfU9kyfHVio=axcnMHP4aGyEN7PvcCSVjHBPknMLPYnJDjQ@mail.gmail.com>
	<CAC_SY72NL9NxONvKYXG+rqS9GJKOqMrAooJUASEM3K9GQORHRw@mail.gmail.com>
Message-ID: <CANPzfU8sLX3mhQQ8cEB_k7TSpesdp2G+WFKCTe==6n+q5pGATA@mail.gmail.com>

On Wed, Aug 27, 2014 at 12:14 PM, Mark Thornton <mthornton at optrak.com>
wrote:

> Except that you then have trouble catching a checked exception that
> appears not to be thrown. You have to wrap the code in a method which does
> declare the checked exception(s) you want to catch.


I tend to catch Exception (one shouldn't catch Throwable/Error anyway) and
if you want to escalate you rethrow sneakily).

  public static RuntimeException sneaky(Throwable t) {
        if ( t == null ) throw new NullPointerException("t");
        AbstractActorCell.<RuntimeException>sneakyThrow0(t);
        return null;
    }

    @SuppressWarnings("unchecked")
    private static <T extends Throwable> void sneakyThrow0(Throwable t)
throws T {
        throw (T)t;
    }

    public static java.lang.String foo() {
        try {
            throw new SQLException("foo");
        } catch(SQLException e) { // Get rid of checked exceptions as you
need to deal with RuntimeExceptions anyway
            throw sneaky(e);
        }
    }

    public static int bar() {
        try {
            foo();
        } catch(Exception e) {
            if (e instanceof SQLException) {
              //deal with SQLExceptions
            } else
                throw sneaky(e);
        }
        return 2;
    }


>
> Mark Thornton
>
>
> On Wednesday, 27 August 2014, ?iktor ?lang <viktor.klang at gmail.com> wrote:
>
>> I -always- use "sneakyThrows" to turn checked exceptions into unchecked
>> ones?there's 0 value in checked exceptions. (Checked exceptions is a Java
>> language feature, not a JVM feature)
>>
>> http://www.jayway.com/2010/01/29/sneaky-throw/
>>
>> The only rough edge I've found is with dynamic Proxies, which encode this
>> language feature into a runtime feature:
>> http://docs.oracle.com/javase/7/docs/api/java/lang/reflect/UndeclaredThrowableException.html
>>
>>
>> On Wed, Aug 27, 2014 at 10:39 AM, Millies, Sebastian <
>> Sebastian.Millies at softwareag.com> wrote:
>>
>>>  Hi,
>>>
>>>
>>>
>>> thank you for the helpful response.
>>>
>>>
>>>
>>> It made me remember dimly some discussion of exception transparency on
>>> lambda-dev, and how it finally wasn?t possible to make it work. I do not
>>> believe this will ever be solved on the language level, and I regret that.
>>> In the same vein CompletableFuture.suppyAsync() does not accept a Callable,
>>> only a Supplier. In my situation there?s a bunch of pre-existing, exception
>>> throwing application methods (not to mention the Java library methods) that
>>> I now have a new requirement to combine asynchronously. So I find myself
>>> wrapping all the Callables, and tunneling the exceptions with
>>> RuntimeException, as Doug Lea recommends.
>>>
>>>
>>>
>>> I have looked at your suggestion, and I?m sorry to say I do not see how
>>> it would help, unless I rewrite everything from scratch or use bayou (would
>>> need something like your Async interface).
>>>
>>> And it also does not seem to address the one-stop exception-handling
>>> concern, because guess I would somehow need to call
>>> CompletableFuture#handle() and that can?t throw. Even in bayou, from a
>>> cursory look at Async.java, it seems that catch_() doesn?t rethrow the
>>> exception if it is not of the appropriate class.
>>>
>>>
>>>
>>> Best,
>>>
>>> Sebastian
>>>
>>>
>>>
>>> *From:* Zhong Yu [mailto:zhong.j.yu at gmail.com]
>>> *Sent:* Wednesday, August 27, 2014 2:02 AM
>>> *To:* Millies, Sebastian
>>> *Cc:* concurrency-interest at cs.oswego.edu
>>> *Subject:* Re: [concurrency-interest] Layered exception handling with
>>> CompletableFuture
>>>
>>>
>>>
>>> [OP snipped]
>>>
>>> Apparently, CompletableFuture does not  take too kindly to checked
>>> exceptions [1].
>>>
>>> A lot of people even consider checked exceptions a design mistake on the
>>> language level. Nevertheless, most Java programmers and APIs do use checked
>>> exceptions, for returning alternative results, and for flow controls.
>>>
>>> A workaround is to create your own helper method like
>>>
>>>     static <T,X> CF<T> catch_(CF<T> cf, Class<X> exceptionType,
>>> FuncE<X,T> exceptionHandler) { ... }
>>>
>>> where FuncE is similar to Function, except the apply() method throws
>>> `Exception`
>>>
>>>
>>>
>>> Then you can use it like
>>>
>>>     catch_( future, SomeException.class, ex->someValue )
>>>
>>>     catch_( future, SomeExcpetion.class, ex->throw new
>>> AnotherException() )
>>>
>>> That is what we did in bayou Async API [2]. The problem you posted early
>>> can be solved in bayou as
>>>
>>>     Async.execute(...)
>>>               .timeout(...)
>>>               .catch_(TimeoutException.class, ex->defaultValue);
>>>
>>>
>>> [1]
>>> http://cs.oswego.edu/pipermail/concurrency-interest/2012-December/010486.html
>>> [2]
>>> http://bayou.io/release/0.9/docs/async/Async_Programming.html#Sequential_Actions
>>>
>>>
>>>   Zhong Yu
>>>
>>> bayou.io
>>>
>>>    Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297
>>> Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB
>>> 1562 - Vorstand/Management Board: Karl-Heinz Streibich
>>> (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; -
>>> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
>>> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> --
>> Cheers,
>> ?
>>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140827/9c92f008/attachment.html>

From alessandro.alinone at lightstreamer.com  Wed Aug 27 07:44:20 2014
From: alessandro.alinone at lightstreamer.com (Alessandro Alinone)
Date: Wed, 27 Aug 2014 13:44:20 +0200
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <CACnv0spkKQQ-5SymoxUGJZokP=6U+V8Son7j=DyNWpGxnQZfBg@mail.gmail.com>
References: <CACnv0spkKQQ-5SymoxUGJZokP=6U+V8Son7j=DyNWpGxnQZfBg@mail.gmail.com>
Message-ID: <CACnv0srD+AAC8hdFVf4MGcYas0eOsNnkkAX5cA9svLXkDZs=1w@mail.gmail.com>

For the sake of discussion, Dario's original article, which triggered the
discussion on comp.lang.java.programmer, is this:

Exploiting Static Exception Checking in Asynchronous Java Code
http://blog.lightstreamer.com/2014/07/exception-handling-in-asynchronous-java.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140827/007eca74/attachment.html>

From viktor.klang at gmail.com  Wed Aug 27 07:54:19 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 27 Aug 2014 13:54:19 +0200
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <CACnv0srD+AAC8hdFVf4MGcYas0eOsNnkkAX5cA9svLXkDZs=1w@mail.gmail.com>
References: <CACnv0spkKQQ-5SymoxUGJZokP=6U+V8Son7j=DyNWpGxnQZfBg@mail.gmail.com>
	<CACnv0srD+AAC8hdFVf4MGcYas0eOsNnkkAX5cA9svLXkDZs=1w@mail.gmail.com>
Message-ID: <CANPzfU-nyGC0wZrHO+wBSv2mszynuuoSvF=88nE4XPUFbSPiQQ@mail.gmail.com>

On Wed, Aug 27, 2014 at 1:44 PM, Alessandro Alinone <
alessandro.alinone at lightstreamer.com> wrote:

> For the sake of discussion, Dario's original article, which triggered the
> discussion on comp.lang.java.programmer, is this:
>
> Exploiting Static Exception Checking in Asynchronous Java Code
>
> http://blog.lightstreamer.com/2014/07/exception-handling-in-asynchronous-java.html
>
>
>
I guess what I'm doing is challenging the following statement: "However, by
now focusing on the *Java* language, the above transformation would deprive
us of one of the most useful tools provided by this language: static
exception checking. "


>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140827/465eb69f/attachment-0001.html>

From peter.levart at gmail.com  Wed Aug 27 09:56:25 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 27 Aug 2014 15:56:25 +0200
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
Message-ID: <53FDE389.9040905@gmail.com>

On 08/27/2014 10:39 AM, Millies, Sebastian wrote:
> So I find myself wrapping all the Callables, and tunneling the exceptions with RuntimeException, as Doug Lea recommends.

The interesting part of javadoc for CompletionStage is the following:

- Two method forms support processing whether the triggering stage 
completed normally or exceptionally: Method /whenComplete/ allows 
injection of an action regardless of outcome, otherwise preserving the 
outcome in its completion. Method /handle/ additionally allows the stage 
to compute a replacement result that may enable further processing by 
other dependent stages. In all other cases, if a stage's computation 
terminates abruptly with an (unchecked) exception or error, then all 
dependent stages requiring its completion complete exceptionally as 
well, with a */CompletionException/* holding the exception as its cause.

So If you do the following:

         CompletableFuture.supplyAsync(() -> {
             throw new IllegalStateException();
         }).whenComplete((x, t) -> {
             System.out.println("Result: " + x + "\nException: " + t);
         });

...you will get:

Result: null
Exception: java.util.concurrent.CompletionException: 
java.lang.IllegalStateException

So if stage funcition/supplier throws an (unchecked) exception, it is 
always wrapped with CompletionException as stage outcome. The signature 
of exception handling functions passed to CompletionStage methods 
declares plain Throwable, but at least CompletableFuture seems to always 
arange so that CompletionException is passed to those functions unless 
you call methods like completeExceptionally on it. You can use this to 
your advantage (will get to that shortly)...

Another "implementation detail" of CompletableFuture is evident if you 
try this:

         CompletableFuture.supplyAsync(() -> {
             throw new CompletionException(new Exception("I'm checked 
exception"));
         }).whenComplete((x, t) -> {
             System.out.println("Result: " + x + "\nException: " + t);
         });

...you will get:

Result: null
Exception: java.util.concurrent.CompletionException: 
java.lang.Exception: I'm checked exception

So if stage funcition/supplier throws CompletionException, it is not 
wrapped with another CompletionException as the stage outcome but it 
becomes the stage outcome directly.

Finally the following "implementation detail" of CompletableFuture is 
also useful for our purpose. If you try:

         try {
             CompletableFuture.supplyAsync(() -> {
                 throw new CompletionException(new Exception("I'm 
checked exception"));
             }).get();
         } catch (ExecutionException | InterruptedException e) {
             System.out.println("Got: " + e);
         }

...you will get:

Got: java.util.concurrent.ExecutionException: java.lang.Exception: I'm 
checked exception

So if CompletableFuture outcome is CompletionException wrapping some 
Throwable, then calling get() on the future will throw 
ExecutionException wrapping this same Throwable. The cause of 
CompletionException is effectively unwrapped before being wrapped with 
ExecutionException in get().

This behaviour lends itself to the use of *CompletionException* as a 
general wrapper for any exception (checked or unchecked) thrown by the 
stage function/supplier.

You can create helper functional interfaces:

     interface SupplierX<T> {
         T get() throws Throwable;
     }

     interface FunctionX<T, R> {
         R apply(T t) throws Throwable;
     }

     // ... etc ... and utility static methods:

     static <T> Supplier<T> wrap(SupplierX<T> supplier) {
         return () -> {
             try {
                 return supplier.get();
             } catch (Throwable e) {
                 throw new CompletionException(e);
             }
         };
     }

     static <R> Function<Throwable, R> unwrapWrap(FunctionX<Throwable, 
R> fn) {
         return (throwable) -> {
             try {
                 return fn.apply(throwable instanceof CompletionException
                                 ? throwable.getCause()
                                 : throwable);
             } catch (Throwable e) {
                 throw new CompletionException(e);
             }
         };
     }

     // ... etc.


And then use them as following:


         CompletableFuture<String> cf = CompletableFuture.supplyAsync(wrap(
             () -> {
                 double r = Math.random();
                 if (r < 1d / 3d) return "OK";
                 else if (r < 2d / 3d) throw new FileNotFoundException();
                 else throw new EOFException();
             }
         ));

         cf = cf.exceptionally(unwrapWrap(
             (t) -> {
                 try {
                     throw t;
                 } catch (IOException e) {
                     throw new IllegalStateException("Converted from: " 
+ e);
                 }
             }
         ));

         cf = cf.exceptionally(unwrapWrap(
             (t) -> {
                 try {
                     throw t;
                 } catch (RuntimeException e) {
                     return "Handled: " + e;
                 }
             }
         ));

         System.out.println(cf.get());



Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140827/40b605f3/attachment.html>

From zhong.j.yu at gmail.com  Wed Aug 27 12:24:57 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 27 Aug 2014 11:24:57 -0500
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
Message-ID: <CACuKZqHLTxqcce0=N3yLPvLhQuEqXFASPWG8dkoddnXEK=GPTA@mail.gmail.com>

On Wed, Aug 27, 2014 at 3:39 AM, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:
>
> And it also does not seem to address the one-stop exception-handling
concern, because guess I would somehow need to call
CompletableFuture#handle() and that can?t throw. Even in bayou, from a
cursory look at Async.java, it seems that catch_() doesn?t rethrow the
 exception if it is not of the appropriate class.

If the exception does not match the type, it is not "caught", it will just
propagate through.

Zhong Yu
bayou.io
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140827/8375871f/attachment.html>

From zhong.j.yu at gmail.com  Wed Aug 27 12:45:43 2014
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 27 Aug 2014 11:45:43 -0500
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <53FDE389.9040905@gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
	<53FDE389.9040905@gmail.com>
Message-ID: <CACuKZqG6dxq5fpZeji+wG4dt=bm=QPNGHmBFiAje72_FgfpX6A@mail.gmail.com>

On Wed, Aug 27, 2014 at 8:56 AM, Peter Levart <peter.levart at gmail.com> wrote:
>
>         CompletableFuture<String> cf = CompletableFuture.supplyAsync(wrap(
>             () -> {
>                 double r = Math.random();
>                 if (r < 1d / 3d) return "OK";
>                 else if (r < 2d / 3d) throw new FileNotFoundException();
>                 else throw new EOFException();
>             }
>         ));
>
>         cf = cf.exceptionally(unwrapWrap(
>             (t) -> {
>                 try {
>                     throw t;
>                 } catch (IOException e) {
>                     throw new IllegalStateException("Converted from: " + e);
>                 }
>             }
>         ));
>
>         cf = cf.exceptionally(unwrapWrap(
>             (t) -> {
>                 try {
>                     throw t;
>                 } catch (RuntimeException e) {
>                     return "Handled: " + e;
>                 }
>             }
>         ));
>
>         System.out.println(cf.get());
>

Or... create a wrapper interface over CompletableFuture. The wrapper may
1. accept lambdas throwing checked exceptions
2. propagate cancellations by default
3. hide some methods (complete/obstrude) and expose some (cancel)
4. have shorter type/method names
5. reorg CF's methods, have fewer, composable methods
6. add more useful convenience methods

Zhong Yu
bayou.io

From Sebastian.Millies at softwareag.com  Thu Aug 28 15:13:55 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Thu, 28 Aug 2014 19:13:55 +0000
Subject: [concurrency-interest] Layered exception handling with
 CompletableFuture
In-Reply-To: <53FDE389.9040905@gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
	<53FDE389.9040905@gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490DF0FB532@HQMBX5.eur.ad.sag>

Hello Peter,

thank you for this excellent post! I now understand how layered exception handling works with CompletableFutures. The key point I wasn?t aware of is that exceptionally() can just pass on CompletionExceptions.

I do not need the SupplierX interface, I just use Callable, and I call the utility toSupplier(), which is a bit more specific. Given that CompletableFuture already does all this exception magic, I really don?t understand why there is no supplyAsync(Callable) method in the standard interface.

I like unwrapWrap (again except for its name, perhaps call it unwrapX, or unwrapCE ?). I particularly like the try-catch-idiom using unwrap inside exceptionally(), and the neat way it avoids peppering instanceof all over the place. Also nice is that the interface FunctionX can remain private in a utility class exposing these static helper methods.

Sebastian


From: Peter Levart [mailto:peter.levart at gmail.com]
Sent: Wednesday, August 27, 2014 3:56 PM
To: Millies, Sebastian; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Layered exception handling with CompletableFuture

On 08/27/2014 10:39 AM, Millies, Sebastian wrote:

So I find myself wrapping all the Callables, and tunneling the exceptions with RuntimeException, as Doug Lea recommends.

The interesting part of javadoc for CompletionStage is the following:

- Two method forms support processing whether the triggering stage completed normally or exceptionally: Method whenComplete allows injection of an action regardless of outcome, otherwise preserving the outcome in its completion. Method handle additionally allows the stage to compute a replacement result that may enable further processing by other dependent stages. In all other cases, if a stage's computation terminates abruptly with an (unchecked) exception or error, then all dependent stages requiring its completion complete exceptionally as well, with a CompletionException holding the exception as its cause.

So If you do the following:

        CompletableFuture.supplyAsync(() -> {
            throw new IllegalStateException();
        }).whenComplete((x, t) -> {
            System.out.println("Result: " + x + "\nException: " + t);
        });

...you will get:

Result: null
Exception: java.util.concurrent.CompletionException: java.lang.IllegalStateException

So if stage funcition/supplier throws an (unchecked) exception, it is always wrapped with CompletionException as stage outcome. The signature of exception handling functions passed to CompletionStage methods declares plain Throwable, but at least CompletableFuture seems to always arange so that CompletionException is passed to those functions unless you call methods like completeExceptionally on it. You can use this to your advantage (will get to that shortly)...

Another "implementation detail" of CompletableFuture is evident if you try this:

        CompletableFuture.supplyAsync(() -> {
            throw new CompletionException(new Exception("I'm checked exception"));
        }).whenComplete((x, t) -> {
            System.out.println("Result: " + x + "\nException: " + t);
        });

...you will get:

Result: null
Exception: java.util.concurrent.CompletionException: java.lang.Exception: I'm checked exception

So if stage funcition/supplier throws CompletionException, it is not wrapped with another CompletionException as the stage outcome but it becomes the stage outcome directly.

Finally the following "implementation detail" of CompletableFuture is also useful for our purpose. If you try:

        try {
            CompletableFuture.supplyAsync(() -> {
                throw new CompletionException(new Exception("I'm checked exception"));
            }).get();
        } catch (ExecutionException | InterruptedException e) {
            System.out.println("Got: " + e);
        }

...you will get:

Got: java.util.concurrent.ExecutionException: java.lang.Exception: I'm checked exception

So if CompletableFuture outcome is CompletionException wrapping some Throwable, then calling get() on the future will throw ExecutionException wrapping this same Throwable. The cause of CompletionException is effectively unwrapped before being wrapped with ExecutionException in get().

This behaviour lends itself to the use of CompletionException as a general wrapper for any exception (checked or unchecked) thrown by the stage function/supplier.

You can create helper functional interfaces:

    interface SupplierX<T> {
        T get() throws Throwable;
    }

    interface FunctionX<T, R> {
        R apply(T t) throws Throwable;
    }

    // ... etc ... and utility static methods:

    static <T> Supplier<T> wrap(SupplierX<T> supplier) {
        return () -> {
            try {
                return supplier.get();
            } catch (Throwable e) {
                throw new CompletionException(e);
            }
        };
    }

    static <R> Function<Throwable, R> unwrapWrap(FunctionX<Throwable, R> fn) {
        return (throwable) -> {
            try {
                return fn.apply(throwable instanceof CompletionException
                                ? throwable.getCause()
                                : throwable);
            } catch (Throwable e) {
                throw new CompletionException(e);
            }
        };
    }

    // ... etc.


And then use them as following:


        CompletableFuture<String> cf = CompletableFuture.supplyAsync(wrap(
            () -> {
                double r = Math.random();
                if (r < 1d / 3d) return "OK";
                else if (r < 2d / 3d) throw new FileNotFoundException();
                else throw new EOFException();
            }
        ));

        cf = cf.exceptionally(unwrapWrap(
            (t) -> {
                try {
                    throw t;
                } catch (IOException e) {
                    throw new IllegalStateException("Converted from: " + e);
                }
            }
        ));

        cf = cf.exceptionally(unwrapWrap(
            (t) -> {
                try {
                    throw t;
                } catch (RuntimeException e) {
                    return "Handled: " + e;
                }
            }
        ));

        System.out.println(cf.get());



Regards, Peter

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140828/dc6ef3a6/attachment-0001.html>

From dl at cs.oswego.edu  Sat Aug 30 10:51:30 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 30 Aug 2014 10:51:30 -0400
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <53FDE389.9040905@gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
	<53FDE389.9040905@gmail.com>
Message-ID: <5401E4F2.8050102@cs.oswego.edu>


Peter: Thanks for posting the nice explanations of options for
processing exceptions in CompletableFuture.  Here are a few notes
about some of the underlying design issues.

* The main rationale for introducing CompletableFuture was to finally
provide an efficient common basis for fluent completion / continuation
/ flow-based async programming.  Adding this was arguably a decade or
so too late (this style of programming was the only one discussed in
my 1999 CPJ book (sec 4.2) without good java.util.concurrent support).
But we couldn't realistically do this in j.u.c before the introduction
of lambdas and functional interfaces in JDK8.  (Non-JDK async
frameworks that did so had to invent workarounds for the lack of
these.)  For good reasons, JDK8 lambdas/types do not nicely accommodate
checked exceptions.  CompletableFuture turns out to be the first place
some people notice this. However, as others have already explained, no
actual functionality is lost; instead you may need to awkwardly move
in and out of checked vs unchecked mode, usually by inspecting
CompletionExceptions.  This awkwardness typically outweighs the
awkwardness users would otherwise have to suffer to express simple
stage processing.

* Flow-based async programming frameworks tend to differ in policies
about whether and how to support cancellation, blocking, async vs sync
completions, rescue-overrides, etc., and how users should be able to
express them. CompletableFuture tries to avoid these issues by
providing efficient mechanics that are as policy-free as we can make
them (at the expense of an API larger than some people like), while at
the same time staying light enough that others can wrap, restrict, or
extend them to support different usage styles with little to no
penalty versus building them from scratch. For example, most people
have found that it is easy to use CompletableFuture as a form of
"Promise", either by wrapping or just via usage conventions.  There
are surely framework developers who would still rather build from
scratch, but fewer than before the introduction of CompletableFuture.

* In part because of uncertainty whether the audience for
CompletableFuture would mainly be users vs framework authors, we did
not in JDK8 take the next step of introducing standard ways of
producing streams of CompletableFutures or their contents
(values/elements/events), each of which is the head of a completion
expression. As in "periodically { supply(...).thenApply(...)....; }"
It is easy to define utilities automating common use cases, including
simple periodic suppliers (via a common ScheduledExecutor), and
async-io completions (via adaptors from nio CompletionHandler).
However, beyond this, the policy/protocol space becomes hard to cope
with for a java.util.concurrent component: Do you want/need
rate-limiting, overrun, and drop policies? Synchronous or asynchronous
acks/flow-control/back-pressure? Batching/Buffering?  Logging?
Conversion into (and out of) java.util.Streams for (possibly parallel)
bulk operations on batches?  StreamIt-like split/merge parallelization
of in-flight processing? And so on. It might still be premature to
settle on a single form in the spirit/style of CompletableFuture that
can serve as a common basis for all (or most) of these. In the mean
time, it seems wrong to not provide some common utilities for delays
and IO, so we might at least offer a "CompletableFutures" utility
class including them, as well as exception-mode-conversion utilities,
plus probably some means of simplifying interop with frameworks like
reactive-streams and Rx.

* Bear in mind that CompletableFutures (and related constructs) are
far from the only ways to compose async processing. They are a lot
easier to use than most others though, because they make it easy to
express many staged processing flows that arise in practice.  Other
harder-to-used constructs (like CountedCompleters, or hand-crafted
combinations of threads, queues, and synchronizers) remain available
and worth considering if you routinely find yourself frustrated trying
to express unusual control constructs within the confines of
CompletableFuture's fluent API.

-Doug

From kasperni at gmail.com  Sun Aug 31 03:14:15 2014
From: kasperni at gmail.com (Kasper Nielsen)
Date: Sun, 31 Aug 2014 09:14:15 +0200
Subject: [concurrency-interest] Layered exception handling with
	CompletableFuture
In-Reply-To: <5401E4F2.8050102@cs.oswego.edu>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>
	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>
	<53FDE389.9040905@gmail.com> <5401E4F2.8050102@cs.oswego.edu>
Message-ID: <CAPs6150_r0D01-wh27hBD2AMcCjwQfHGqevxqOZ4D6b8WX4jhA@mail.gmail.com>

Sorry for hijacking this thread. But if additional functionality for
CompletableFuture is on the table. I have a couple of requests, some of
them just syntactically sugar.

1)
A simple way to create an immutable CompletionStage from a CompletionFuture.
Right now if I have something like like this
class ProvidedToUsers {
    CompletionFuture f;
    CompletionStage doAsyncStuff() {
        return f;
    }
}
Any client can just do:
xx.doAsyncStuff().toCompletionFuture().cancel();

Right now I'm using this to make an "immutable" future:

CompletableFuture<Void> cf = new CompletableFuture<>();
f.handle((a, b) -> {
    if (b == null) {
        cf.complete(null);
    } else {
        cf.completeExceptionally(b);
    }
    return null;
});
return cf;


2)
An easy way to create a CompletableFuture that is exceptionally completed:

return CompletableFuture.completedExceptionally(throwable);

is better than:

CompletableFuture<T> cf = new CompletableFuture<>();
cf.completeExceptionally(throwable);
return cf;

3)
An easy way to get the exceptional cause from a CompletableFuture, right
now I'm using this

if (cf.isCompletedExceptionally()) {
    try {
        cf.getNow(null);
    } catch (CancellationException e) {
        return e;
    } catch (CompletionException e) {
        return e.getCause();
    }
}
return null;

4)
With regards to async-io completions I'm using the following piece of code
to make completable futures that timeout at some point.

public static <T> CompletableFuture<T> timeout(CompletableFuture<T>
delegate, ScheduledExecutorService ses,
        long timeout, TimeUnit unit) {
    CompletableFuture<T> cf = new CompletableFuture<>();
    Future<?> f;
    try {
        f = ses.schedule(new Runnable() {
            public void run() {
                if (!delegate.isDone()) {
                    cf.completeExceptionally(new TimeoutException("Timed
out after " + timeout + " "
                            + unit.toString().toLowerCase()));
                }
            }
        }, timeout, unit);
    } catch (RejectedExecutionException e) {
        // Unfortunately TimeoutException does not allow exceptions in its
constructor
        cf.completeExceptionally(new RuntimeException("Could not schedule
timeout task, ", e));
        return cf;
    }
    delegate.handle(new BiFunction<T, Throwable, Void>() {
        public Void apply(T t, Throwable throwable) {
            // Users must manually purge if many outstanding tasks
            f.cancel(false);
            if (throwable != null) {
                cf.completeExceptionally(throwable);
            } else {
                cf.complete(t);
            }
            return null;
        }
    });
    return cf;
}

Something similar might be nice to add to CompletableFuture.

5)
Not related to CompletableFuture, but could CancellationException and
TimeoutException get constructors that also takes a
Throwable.

Cheers
  Kasper

On Sat, Aug 30, 2014 at 4:51 PM, Doug Lea <dl at cs.oswego.edu> wrote:

>
> It is easy to define utilities automating common use cases, including
> simple periodic suppliers (via a common ScheduledExecutor), and
> async-io completions (via adaptors from nio CompletionHandler).
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140831/cfc304c6/attachment.html>

From dl at cs.oswego.edu  Sun Aug 31 07:22:33 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 31 Aug 2014 07:22:33 -0400
Subject: [concurrency-interest] Soliciting suggestions for CompletableFuture
 utilities
In-Reply-To: <CAPs6150_r0D01-wh27hBD2AMcCjwQfHGqevxqOZ4D6b8WX4jhA@mail.gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490DF0FAD0B@HQMBX5.eur.ad.sag>	<CACuKZqERmsvXX01==Mp7wa0H9XrPMKGsuEMnQne-_2FiO+Le2Q@mail.gmail.com>	<32F15738E8E5524DA4F01A0FA4A8E490DF0FAE5E@HQMBX5.eur.ad.sag>	<53FDE389.9040905@gmail.com>	<5401E4F2.8050102@cs.oswego.edu>
	<CAPs6150_r0D01-wh27hBD2AMcCjwQfHGqevxqOZ4D6b8WX4jhA@mail.gmail.com>
Message-ID: <54030579.4060307@cs.oswego.edu>

[Changing subject header]

On 08/31/2014 03:14 AM, Kasper Nielsen wrote:
> Sorry for hijacking this thread. But if additional functionality for
> CompletableFuture is on the table. I have a couple of requests, some of them
> just syntactically sugar.

Yes, you were one step ahead of me! As mentioned, there are a few
clear candidates (delays, IO), but it is a good time to solicit
further suggestions and discussion. I'll wait for these to settle
before proposing concrete additions.

-Doug


>
> 1)
> A simple way to create an immutable CompletionStage from a CompletionFuture.
> Right now if I have something like like this
> class ProvidedToUsers {
>      CompletionFuture f;
>      CompletionStage doAsyncStuff() {
>          return f;
>      }
> }
> Any client can just do:
> xx.doAsyncStuff().toCompletionFuture().cancel();
>
> Right now I'm using this to make an "immutable" future:
>
> CompletableFuture<Void> cf = new CompletableFuture<>();
> f.handle((a, b) -> {
>      if (b == null) {
>          cf.complete(null);
>      } else {
>          cf.completeExceptionally(b);
>      }
>      return null;
> });
> return cf;
>
>
> 2)
> An easy way to create a CompletableFuture that is exceptionally completed:
>
> return CompletableFuture.completedExceptionally(throwable);
>
> is better than:
>
> CompletableFuture<T> cf = new CompletableFuture<>();
> cf.completeExceptionally(throwable);
> return cf;
>
> 3)
> An easy way to get the exceptional cause from a CompletableFuture, right now I'm
> using this
>
> if (cf.isCompletedExceptionally()) {
>      try {
>          cf.getNow(null);
>      } catch (CancellationException e) {
>          return e;
>      } catch (CompletionException e) {
>          return e.getCause();
>      }
> }
> return null;
>
> 4)
> With regards to async-io completions I'm using the following piece of code to
> make completable futures that timeout at some point.
>
> public static <T> CompletableFuture<T> timeout(CompletableFuture<T> delegate,
> ScheduledExecutorService ses,
>          long timeout, TimeUnit unit) {
>      CompletableFuture<T> cf = new CompletableFuture<>();
>      Future<?> f;
>      try {
>          f = ses.schedule(new Runnable() {
>              public void run() {
>                  if (!delegate.isDone()) {
>                      cf.completeExceptionally(new TimeoutException("Timed out
> after " + timeout + " "
>                              + unit.toString().toLowerCase()));
>                  }
>              }
>          }, timeout, unit);
>      } catch (RejectedExecutionException e) {
>          // Unfortunately TimeoutException does not allow exceptions in its
> constructor
>          cf.completeExceptionally(new RuntimeException("Could not schedule
> timeout task, ", e));
>          return cf;
>      }
>      delegate.handle(new BiFunction<T, Throwable, Void>() {
>          public Void apply(T t, Throwable throwable) {
>              // Users must manually purge if many outstanding tasks
>              f.cancel(false);
>              if (throwable != null) {
>                  cf.completeExceptionally(throwable);
>              } else {
>                  cf.complete(t);
>              }
>              return null;
>          }
>      });
>      return cf;
> }
>
> Something similar might be nice to add to CompletableFuture.
>
> 5)
> Not related to CompletableFuture, but could CancellationException and
> TimeoutException get constructors that also takes a
> Throwable.
>
> Cheers
>    Kasper
>
> On Sat, Aug 30, 2014 at 4:51 PM, Doug Lea <dl at cs.oswego.edu
> <mailto:dl at cs.oswego.edu>> wrote:
>
>
>     It is easy to define utilities automating common use cases, including
>     simple periodic suppliers (via a common ScheduledExecutor), and
>     async-io completions (via adaptors from nio CompletionHandler).
>
>     -Doug
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>


From kasperni at gmail.com  Sun Aug 31 09:35:48 2014
From: kasperni at gmail.com (Kasper Nielsen)
Date: Sun, 31 Aug 2014 15:35:48 +0200
Subject: [concurrency-interest] Fork-Join and getSurplusQueuedTaskCount
Message-ID: <CAPs6150Qs0A5MXaZePVwGBEBKEgyB213S=o9wCZzWDskTonKNg@mail.gmail.com>

Hi,

I'm trying to figure out if ForkJoinTask#getSurplusQueuedTaskCount is
worthwhile to use for basic array-like processing. RecursiveAction comes
with an example where it is used. But haven't seen it used in practice
anywhere. For example, the java.util.stream implementation does not make
use of it.

Any pointers?

Cheers
  Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20140831/3a26ef38/attachment.html>

From dl at cs.oswego.edu  Sun Aug 31 10:25:06 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 31 Aug 2014 10:25:06 -0400
Subject: [concurrency-interest] Fork-Join and getSurplusQueuedTaskCount
In-Reply-To: <CAPs6150Qs0A5MXaZePVwGBEBKEgyB213S=o9wCZzWDskTonKNg@mail.gmail.com>
References: <CAPs6150Qs0A5MXaZePVwGBEBKEgyB213S=o9wCZzWDskTonKNg@mail.gmail.com>
Message-ID: <54033042.7000201@cs.oswego.edu>

On 08/31/2014 09:35 AM, Kasper Nielsen wrote:
> Hi,
>
> I'm trying to figure out if ForkJoinTask#getSurplusQueuedTaskCount is worthwhile
> to use for basic array-like processing. RecursiveAction comes with an example
> where it is used. But haven't seen it used in practice anywhere. For example,
> the java.util.stream implementation does not make use of it.
>

If you have good size estimates, it is generally best to use them
for granularity control; normally aiming to create 4 to 8 times
more total tasks than threads/cores/parallelismLevel for the sake
of load balancing. Method  getSurplusQueuedTaskCount is available
as a fallback when you don't have any other basis of estimation.
This might occur for example in graph traversal algorithms
where you don't know anything about expected amount of work.
The method relies on statistical regularities, and in the
aggregate works better than all alternatives I know of.
(We do have a few test/demos of its use in the "loops" CVS.)
But it can encounter more run-to-run variance that you would
see with size estimates, if you had them. Since java.util.stream
normally relies on sizes anyway, it doesn't use it, although there
are a few cases where it might be worth exploring.

(I agree that it might be a little misleading that we
demonstrate getSurplusQueuedTaskCount in javadocs,
just for the sake of providing an example, in a context
where you probably wouldn't choose to use it. Maybe we should
at least add a comment to this effect.)

-Doug


