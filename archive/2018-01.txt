From nathanielwen at 163.com  Fri Jan  5 01:20:22 2018
From: nathanielwen at 163.com (=?UTF-8?B?5rip5piK56Wl?=)
Date: Fri, 5 Jan 2018 14:20:22 +0800 (GMT+08:00)
Subject: [concurrency-interest] Why "fast path" is faster than full enq in
	AQS
Message-ID: <214e83bd.2c95.160c4fa3e7a.Coremail.nathanielwen@163.com>

Hi, everyone:
     I'm reading the source code of "AbstractQueuedSynchronizer", getting a little confused of the reason Why "fast path" is faster than full enq in the comment "Try the fast path of enq; backup to full enq on failure" .
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180105/9338d539/attachment.html>

From davidcholmes at aapt.net.au  Fri Jan  5 02:30:18 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 5 Jan 2018 17:30:18 +1000
Subject: [concurrency-interest] Why "fast path" is faster than full enq
	in AQS
Message-ID: <001101d385f7$09ef2440$1dcd6cc0$@aapt.net.au>

What version of the code are you looking at? I can’t see anything like that in current OpenJDK sources.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 4:20 PM
To: concurrency-interest <concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

Hi, everyone:

     I'm reading the source code of "AbstractQueuedSynchronizer", getting a little confused of the reason Why "fast path" is faster than full enq in the comment "Try the fast path of enq; backup to full enq on failure" .

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180105/52ee6929/attachment.html>

From nathanielwen at 163.com  Fri Jan  5 02:34:17 2018
From: nathanielwen at 163.com (=?UTF-8?B?5rip5piK56Wl?=)
Date: Fri, 5 Jan 2018 15:34:17 +0800 (GMT+08:00)
Subject: [concurrency-interest] Why "fast path" is faster than full enq
 in AQS
In-Reply-To: <001101d385f7$09ef2440$1dcd6cc0$@aapt.net.au>
References: <001101d385f7$09ef2440$1dcd6cc0$@aapt.net.au>
Message-ID: <686770d9.2fc2.160c53dea62.Coremail.nathanielwen@163.com>

My JDK version is 1.8



private Node addWaiter(Node mode) {

    Node node = new Node(Thread.currentThread(), mode);

    // Try the fast path of enq; backup to full enq on failure

    Node pred = tail;

    if (pred != null) {

        node.prev = pred;

        if (compareAndSetTail(pred, node)) {

            pred.next = node;

            return node;

        }

    }

    enq(node);

    return node;

}

On 01/5/2018 15:30，David Holmes<davidcholmes at aapt.net.au> wrote：

What version of the code are you looking at? I can’t see anything like that in current OpenJDK sources.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 4:20 PM
To: concurrency-interest <concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

Hi, everyone:

     I'm reading the source code of "AbstractQueuedSynchronizer", getting a little confused of the reason Why "fast path" is faster than full enq in the comment "Try the fast path of enq; backup to full enq on failure" .
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180105/9949045b/attachment.html>

From davidcholmes at aapt.net.au  Fri Jan  5 07:00:02 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 5 Jan 2018 22:00:02 +1000
Subject: [concurrency-interest] Why "fast path" is faster than full enq
	in AQS
In-Reply-To: <686770d9.2fc2.160c53dea62.Coremail.nathanielwen@163.com>
References: <001101d385f7$09ef2440$1dcd6cc0$@aapt.net.au>
 <686770d9.2fc2.160c53dea62.Coremail.nathanielwen@163.com>
Message-ID: <002501d3861c$b86f3b20$294db160$@aapt.net.au>

The fast path just does a direct enqueue to the existing tail. The enq(), in the worst case, has to initialize the queue, and in general deals with contention due to concurrent enqueue attempts.

 

The code is different now.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 5:34 PM
To: dholmes at ieee.org
Cc: 'Concurrency-interest' <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

My JDK version is 1.8

 

private Node addWaiter(Node mode) {

    Node node = new Node(Thread.currentThread(), mode);

    // Try the fast path of enq; backup to full enq on failure

    Node pred = tail;

    if (pred != null) {

        node.prev = pred;

        if (compareAndSetTail(pred, node)) {

            pred.next = node;

            return node;

        }

    }

    enq(node);

    return node;

}

On 01/5/2018 15:30， <mailto:davidcholmes at aapt.net.au> David Holmes<davidcholmes at aapt.net.au> wrote： 

What version of the code are you looking at? I can’t see anything like that in current OpenJDK sources.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu <mailto:concurrency-interest-bounces at cs.oswego.edu> ] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 4:20 PM
To: concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> >
Subject: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

Hi, everyone:

     I'm reading the source code of "AbstractQueuedSynchronizer", getting a little confused of the reason Why "fast path" is faster than full enq in the comment "Try the fast path of enq; backup to full enq on failure" .

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180105/f7ed05fb/attachment-0001.html>

From nathanielwen at 163.com  Sat Jan  6 04:10:04 2018
From: nathanielwen at 163.com (wen)
Date: Sat, 6 Jan 2018 17:10:04 +0800 (GMT+08:00)
Subject: [concurrency-interest] Why "fast path" is faster than full enq
 in AQS
In-Reply-To: <002501d3861c$b86f3b20$294db160$@aapt.net.au>
References: <001101d385f7$09ef2440$1dcd6cc0$@aapt.net.au>
 <686770d9.2fc2.160c53dea62.Coremail.nathanielwen@163.com>
 <002501d3861c$b86f3b20$294db160$@aapt.net.au>
Message-ID: <f0cffd.4674.160cabbf6d1.Coremail.nathanielwen@163.com>

The full enq will loop only once when there is no contention and the tail isn't null. The reason why "fast-path" is faster is because in most cases the tail isn't null. So "if (tail != null)" is more efficient than "if (tail == null) ... else ..." in this very case, am I right?
On 01/5/2018 20:00，David Holmes<davidcholmes at aapt.net.au> wrote：

The fast path just does a direct enqueue to the existing tail. The enq(), in the worst case, has to initialize the queue, and in general deals with contention due to concurrent enqueue attempts.

 

The code is different now.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 5:34 PM
To:dholmes at ieee.org
Cc: 'Concurrency-interest' <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

My JDK version is 1.8

 

private Node addWaiter(Node mode) {

    Node node = new Node(Thread.currentThread(), mode);

    // Try the fast path of enq; backup to full enq on failure

    Node pred = tail;

    if (pred != null) {

        node.prev = pred;

        if (compareAndSetTail(pred, node)) {

            pred.next = node;

            return node;

        }

    }

    enq(node);

    return node;

}

On 01/5/2018 15:30，David Holmes<davidcholmes at aapt.net.au> wrote：

What version of the code are you looking at? I can’t see anything like that in current OpenJDK sources.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 4:20 PM
To: concurrency-interest <concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

Hi, everyone:

     I'm reading the source code of "AbstractQueuedSynchronizer", getting a little confused of the reason Why "fast path" is faster than full enq in the comment "Try the fast path of enq; backup to full enq on failure" .
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/a640f245/attachment.html>

From davidcholmes at aapt.net.au  Sat Jan  6 05:28:15 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 6 Jan 2018 20:28:15 +1000
Subject: [concurrency-interest] Why "fast path" is faster than full enq
	in AQS
In-Reply-To: <f0cffd.4674.160cabbf6d1.Coremail.nathanielwen@163.com>
References: <001101d385f7$09ef2440$1dcd6cc0$@aapt.net.au>
 <686770d9.2fc2.160c53dea62.Coremail.nathanielwen@163.com>
 <002501d3861c$b86f3b20$294db160$@aapt.net.au>
 <f0cffd.4674.160cabbf6d1.Coremail.nathanielwen@163.com>
Message-ID: <006e01d386d9$104e0490$30ea0db0$@aapt.net.au>

The old structure was intended to be more amenable to inlining the fast-path code IIRC. The new code doesn’t bother AFAICS.

 

David

 

From: wen [mailto:nathanielwen at 163.com] 
Sent: Saturday, January 6, 2018 7:10 PM
To: dholmes at ieee.org
Cc: 'Concurrency-interest' <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

The full enq will loop only once when there is no contention and the tail isn't null. The reason why "fast-path" is faster is because in most cases the tail isn't null. So "if (tail != null)" is more efficient than "if (tail == null) ... else ..." in this very case, am I right?

On 01/5/2018 20:00， <mailto:davidcholmes at aapt.net.au> David Holmes<davidcholmes at aapt.net.au> wrote： 

The fast path just does a direct enqueue to the existing tail. The enq(), in the worst case, has to initialize the queue, and in general deals with contention due to concurrent enqueue attempts.

 

The code is different now.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu <mailto:concurrency-interest-bounces at cs.oswego.edu> ] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 5:34 PM
To: dholmes at ieee.org <mailto:dholmes at ieee.org> 
Cc: 'Concurrency-interest' <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> >
Subject: Re: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

My JDK version is 1.8

 

private Node addWaiter(Node mode) {

    Node node = new Node(Thread.currentThread(), mode);

    // Try the fast path of enq; backup to full enq on failure

    Node pred = tail;

    if (pred != null) {

        node.prev = pred;

        if (compareAndSetTail(pred, node)) {

            pred.next = node;

            return node;

        }

    }

    enq(node);

    return node;

}

On 01/5/2018 15:30， <mailto:davidcholmes at aapt.net.au> David Holmes<davidcholmes at aapt.net.au> wrote： 

What version of the code are you looking at? I can’t see anything like that in current OpenJDK sources.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu <mailto:concurrency-interest-bounces at cs.oswego.edu> ] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 4:20 PM
To: concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> >
Subject: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

Hi, everyone:

     I'm reading the source code of "AbstractQueuedSynchronizer", getting a little confused of the reason Why "fast path" is faster than full enq in the comment "Try the fast path of enq; backup to full enq on failure" .

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/21549da1/attachment-0001.html>

From martinrb at google.com  Sat Jan  6 10:39:12 2018
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 6 Jan 2018 07:39:12 -0800
Subject: [concurrency-interest] Why "fast path" is faster than full enq
	in AQS
In-Reply-To: <006e01d386d9$104e0490$30ea0db0$@aapt.net.au>
References: <001101d385f7$09ef2440$1dcd6cc0$@aapt.net.au>
 <686770d9.2fc2.160c53dea62.Coremail.nathanielwen@163.com>
 <002501d3861c$b86f3b20$294db160$@aapt.net.au>
 <f0cffd.4674.160cabbf6d1.Coremail.nathanielwen@163.com>
 <006e01d386d9$104e0490$30ea0db0$@aapt.net.au>
Message-ID: <CA+kOe0-cX676MN0xLKcjXD3Op5-1dFsj8SGgvWZ8+H14gJ2Liw@mail.gmail.com>

Alternatively: initializeSyncQueue is now a separate cold-code method.

(All of this is unlikely to matter much)

On Sat, Jan 6, 2018 at 2:28 AM, David Holmes via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> The old structure was intended to be more amenable to inlining the
> fast-path code IIRC. The new code doesn’t bother AFAICS.
>
>
>
> David
>
>
>
> *From:* wen [mailto:nathanielwen at 163.com]
> *Sent:* Saturday, January 6, 2018 7:10 PM
>
> *To:* dholmes at ieee.org
> *Cc:* 'Concurrency-interest' <concurrency-interest at cs.oswego.edu>
> *Subject:* Re: [concurrency-interest] Why "fast path" is faster than full
> enq in AQS
>
>
>
> The full enq will loop only once when there is no contention and the tail
> isn't null. The reason why "fast-path" is faster is because in most cases
> the tail isn't null. So "if (tail != null)" is more efficient than "if
> (tail == null) ... else ..." in this very case, am I right?
>
> On 01/5/2018 20:00，David Holmes<davidcholmes at aapt.net.au>
> <davidcholmes at aapt.net.au> wrote：
>
> The fast path just does a direct enqueue to the existing tail. The enq(),
> in the worst case, has to initialize the queue, and in general deals with
> contention due to concurrent enqueue attempts.
>
>
>
> The code is different now.
>
>
>
> David
>
>
>
> *From:* Concurrency-interest [mailto:concurrency-interest-
> bounces at cs.oswego.edu] *On Behalf Of *??? via Concurrency-interest
> *Sent:* Friday, January 5, 2018 5:34 PM
> *To:* dholmes at ieee.org
> *Cc:* 'Concurrency-interest' <concurrency-interest at cs.oswego.edu>
> *Subject:* Re: [concurrency-interest] Why "fast path" is faster than full
> enq in AQS
>
>
>
> My JDK version is 1.8
>
>
>
> private Node addWaiter(Node mode) {
>
>     Node node = new Node(Thread.currentThread(), mode);
>
>     // Try the fast path of enq; backup to full enq on failure
>
>     Node pred = tail;
>
>     if (pred != null) {
>
>         node.prev = pred;
>
>         if (compareAndSetTail(pred, node)) {
>
>             pred.next = node;
>
>             return node;
>
>         }
>
>     }
>
>     enq(node);
>
>     return node;
>
> }
>
> On 01/5/2018 15:30，David Holmes<davidcholmes at aapt.net.au>
> <davidcholmes at aapt.net.au> wrote：
>
> What version of the code are you looking at? I can’t see anything like
> that in current OpenJDK sources.
>
>
>
> David
>
>
>
> *From:* Concurrency-interest [mailto:concurrency-interest-
> bounces at cs.oswego.edu] *On Behalf Of *??? via Concurrency-interest
> *Sent:* Friday, January 5, 2018 4:20 PM
> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
> *Subject:* [concurrency-interest] Why "fast path" is faster than full enq
> in AQS
>
>
>
> Hi, everyone:
>
>      I'm reading the source code of "AbstractQueuedSynchronizer", getting
> a little confused of the reason Why "fast path" is faster than full enq in
> the comment "Try the fast path of enq; backup to full enq on failure" .
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/81f10479/attachment.html>

From buka1974 at yahoo.com  Sat Jan  6 11:06:41 2018
From: buka1974 at yahoo.com (Andriy Bukatar)
Date: Sat, 6 Jan 2018 16:06:41 +0000 (UTC)
Subject: [concurrency-interest] non-concurrent collection visibility question
References: <894991254.1593625.1515254801366.ref@mail.yahoo.com>
Message-ID: <894991254.1593625.1515254801366@mail.yahoo.com>

  Hi all,
 First of, Happy New Year everyone and all the best in 2018! Secondly, have a question that should be easy to answer for most of you here . In some class (say AggregateBuffer) that has a method dispatching multiple buffers, we have a collection that holds references to those ring buffers: final List<Buffer> buffers = new ArrayList<>(); //simple array list, non-concurrent collection  Then we have a method to add a new publisher (effectively a new buffer). The buffer gets created and gets added to the array list. We know that this method always gets called by the same bootstrap-thread that wires all the components.  Publisher<M> addPublisher(){       Buffer buffer = new Buffer();       buffers.add(ringBuffer);} And finally we have a dispatching thread that calls receive() method on the AggregateBuffer class scrolling over the buffer collection and dispatching each buffer one by one using some dispatching strategy (say attempt to read up to N messages at a time from each buffer)      int receive()    {        for (Buffer buf:buffers)//the line in question        {            buffer.read()        }    } The question is -     if we make sure that the dispatcher-thread gets created and gets passed the reference to the AggregateBuffer class AFTER all the calls to addPublisher() are done with, is it guaranteed that dispatcher-thread will see all the changes to the non-concurrent "buffers" collection and if so then what guarantees that? In other words – is it possible that a dispatcher-thread may see an empty “buffers” collection?    thank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/e3f1404d/attachment-0001.html>

From valentin.male.kovalenko at gmail.com  Sat Jan  6 11:40:40 2018
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Sat, 6 Jan 2018 19:40:40 +0300
Subject: [concurrency-interest] non-concurrent collection visibility
	question
Message-ID: <CAO-wXwJLfsxRZc1O7RPGNQxh7JnmK-0q10BrtjWWFFOoNpfEXQ@mail.gmail.com>

Hi Andriy,

The answer to your question lies in the specification of "AFTER" order of
actions that you are using.

If by "AFTER" you mean an "order according to wall clock", then there is
nothing you can say about visibility based on such an order.

If by "AFTER" you mean happens-before order, that is B AFTER A if and only
if A happens-before B, then according to your description, all writes to
buffers list happens-before all reads done by receive() method. This means
that your receive() method is guaranteed to see all buffers that were added
to buffers list.

However, since you did not specify "AFTER" in the question, no one can say
what it means, except for you. So could you please specify what did you
mean by "AFTER"?

I would also discourage anyone from using such misleading words like
before/after (without specifying the order you are talking about),
immediately or visible immediately when reasoning about visibility in
multithread programming.

Regards,
Valentin

------------------------------
>
> Message: 2
> Date: Sat, 6 Jan 2018 16:06:41 +0000 (UTC)
> From: Andriy Bukatar <buka1974 at yahoo.com>
> To: "concurrency-interest at cs.oswego.edu"
>         <concurrency-interest at cs.oswego.edu>
> Subject: [concurrency-interest] non-concurrent collection visibility
>         question
> Message-ID: <894991254.1593625.1515254801366 at mail.yahoo.com>
> Content-Type: text/plain; charset="utf-8"
>
>   Hi all,
>  First of, Happy New Year everyone and all the best in 2018! Secondly,
> have a question that should be easy to answer for most of you here . In
> some class (say AggregateBuffer) that has a method dispatching multiple
> buffers, we have a collection that holds references to those ring
> buffers: final List<Buffer> buffers = new ArrayList<>(); //simple array
> list, non-concurrent collection  Then we have a method to add a new
> publisher (effectively a new buffer). The buffer gets created and gets
> added to the array list. We know that this method always gets called by the
> same bootstrap-thread that wires all the components.  Publisher<M>
> addPublisher(){       Buffer buffer = new Buffer();
> buffers.add(ringBuffer);} And finally we have a dispatching thread that
> calls receive() method on the AggregateBuffer class scrolling over the
> buffer collection and dispatching each buffer one by one using some
> dispatching strategy (say attempt to read up to N messages at a time from
> each buffer)      int receive()    {        for (Buffer buf:buffers)//the
> line in question        {            buffer.read()        }    } The
> question is -     if we make sure that the dispatcher-thread gets created
> and gets passed the reference to the AggregateBuffer class AFTER all the
> calls to addPublisher() are done with, is it guaranteed that
> dispatcher-thread will see all the changes to the non-concurrent "buffers"
> collection and if so then what guarantees that? In other words – is it
> possible that a dispatcher-thread may see an empty “buffers”
> collection?    thank you
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://cs.oswego.edu/pipermail/concurrency-
> interest/attachments/20180106/e3f1404d/attachment.html>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> ------------------------------
>
> End of Concurrency-interest Digest, Vol 156, Issue 3
> ****************************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/526e9782/attachment.html>

From buka1974 at yahoo.com  Sat Jan  6 12:07:40 2018
From: buka1974 at yahoo.com (Andriy Bukatar)
Date: Sat, 6 Jan 2018 12:07:40 -0500
Subject: [concurrency-interest] non-concurrent collection visibility
	question
In-Reply-To: <CAO-wXwJLfsxRZc1O7RPGNQxh7JnmK-0q10BrtjWWFFOoNpfEXQ@mail.gmail.com>
References: <CAO-wXwJLfsxRZc1O7RPGNQxh7JnmK-0q10BrtjWWFFOoNpfEXQ@mail.gmail.com>
Message-ID: <3DE457A2-66FA-48C4-95BA-108AF465F820@yahoo.com>

Thanks Valentin.
Yes, the devil is in the details.

I could rephrase the question as follows:

There’s a class C containing the non-concurrent list L.

Thread T1 adds a few items to the list L. 

Thread T2 starts (note it did not exist at the time when T1 was updating the list L). Thread T2 gets a reference to C and iterates over its list L.

So “AFTER” in this case is a “wall clock distance” between the time T1 performs the last update to L and the time when T2 starts. 

I know the answer to this question when T2 starts iterating over L before (or at the time when) T1 performs updates to non-concurrent L - no visibility guarantees whatsoever. 

The question I suppose is whether non-existence of T2 at the time of the last update to L has any impact on visibility of the contents of L.

Not sure if this makes it any clearer and my apologies if it does not.

Thank you


> On Jan 6, 2018, at 11:40 AM, Valentin Kovalenko <valentin.male.kovalenko at gmail.com> wrote:
> 
> Hi Andriy,
> 
> The answer to your question lies in the specification of "AFTER" order of actions that you are using.
> 
> If by "AFTER" you mean an "order according to wall clock", then there is nothing you can say about visibility based on such an order.
> 
> If by "AFTER" you mean happens-before order, that is B AFTER A if and only if A happens-before B, then according to your description, all writes to buffers list happens-before all reads done by receive() method. This means that your receive() method is guaranteed to see all buffers that were added to buffers list.
> 
> However, since you did not specify "AFTER" in the question, no one can say what it means, except for you. So could you please specify what did you mean by "AFTER"?
> 
> I would also discourage anyone from using such misleading words like before/after (without specifying the order you are talking about), immediately or visible immediately when reasoning about visibility in multithread programming.
> 
> Regards,
> Valentin
> 
>> ------------------------------
>> 
>> Message: 2
>> Date: Sat, 6 Jan 2018 16:06:41 +0000 (UTC)
>> From: Andriy Bukatar <buka1974 at yahoo.com>
>> To: "concurrency-interest at cs.oswego.edu"
>>         <concurrency-interest at cs.oswego.edu>
>> Subject: [concurrency-interest] non-concurrent collection visibility
>>         question
>> Message-ID: <894991254.1593625.1515254801366 at mail.yahoo.com>
>> Content-Type: text/plain; charset="utf-8"
>> 
>>   Hi all,
>>  First of, Happy New Year everyone and all the best in 2018! Secondly, have a question that should be easy to answer for most of you here . In some class (say AggregateBuffer) that has a method dispatching multiple buffers, we have a collection that holds references to those ring buffers: final List<Buffer> buffers = new ArrayList<>(); //simple array list, non-concurrent collection  Then we have a method to add a new publisher (effectively a new buffer). The buffer gets created and gets added to the array list. We know that this method always gets called by the same bootstrap-thread that wires all the components.  Publisher<M> addPublisher(){       Buffer buffer = new Buffer();       buffers.add(ringBuffer);} And finally we have a dispatching thread that calls receive() method on the AggregateBuffer class scrolling over the buffer collection and dispatching each buffer one by one using some dispatching strategy (say attempt to read up to N messages at a time from each buffer)      int receive()    {        for (Buffer buf:buffers)//the line in question        {            buffer.read()        }    } The question is -     if we make sure that the dispatcher-thread gets created and gets passed the reference to the AggregateBuffer class AFTER all the calls to addPublisher() are done with, is it guaranteed that dispatcher-thread will see all the changes to the non-concurrent "buffers" collection and if so then what guarantees that? In other words – is it possible that a dispatcher-thread may see an empty “buffers” collection?    thank you
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/e3f1404d/attachment.html>
>> 
>> ------------------------------
>> 
>> Subject: Digest Footer
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> ------------------------------
>> 
>> End of Concurrency-interest Digest, Vol 156, Issue 3
>> ****************************************************
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/abf59f3b/attachment.html>

From valentin.male.kovalenko at gmail.com  Sat Jan  6 12:50:37 2018
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Sat, 6 Jan 2018 20:50:37 +0300
Subject: [concurrency-interest] non-concurrent collection visibility
	question
Message-ID: <CAO-wXw+Q98bOz4c6yz+w9_tZ7C=s7VjvW-6ry5fuqfXUhguujQ@mail.gmail.com>

Thank you for the clarification. In this case, read actions in T2 are not
ordered by happens-before order (HB) with the write actions in T1. And all
these actions act on the same shared variables (fields of the list L). So
by definition, you have a data race. And there is no guarantee that you
will correctly read all the buffers that T1 added to the L.

I want to add, that generally there is no guarantee that you will see a
non-null when reading L field from T2. However is your case this field is
final, and the semantics of a final field guarantees HB* between a write to
the field L (T1 writes there a reference to a new ArrayList()) and a read
of the reference from L field in T2. Unfortunately, this HB* is not
transitive with the normal HB, hence you don't have guarantees you
initially asked about.

And one more thing: if you had started T2 from T1 after (in program order
(PO)) T1 added all buffers to L, then you would have had HB between all
actions in T1 (which are ordered in PO before the start of T2) and all
actions in T2. But as far as I understand, your case is different and T2 is
started not from T1.

Regards,
Valentin

On 6 January 2018 at 20:07, Andriy Bukatar <buka1974 at yahoo.com> wrote:

> Thanks Valentin.
> Yes, the devil is in the details.
>
> I could rephrase the question as follows:
>
> There’s a class C containing the non-concurrent list L.
>
> Thread T1 adds a few items to the list L.
>
> Thread T2 starts (note it did not exist at the time when T1 was updating
> the list L). Thread T2 gets a reference to C and iterates over its list L.
>
> So “AFTER” in this case is a “wall clock distance” between the time T1
> performs the last update to L and the time when T2 starts.
>
> I know the answer to this question when T2 starts iterating over L before
> (or at the time when) T1 performs updates to non-concurrent L - no
> visibility guarantees whatsoever.
>
> The question I suppose is whether non-existence of T2 at the time of the
> last update to L has any impact on visibility of the contents of L.
>
> Not sure if this makes it any clearer and my apologies if it does not.
>
> Thank you
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/ccffcb0e/attachment.html>

From valentin.male.kovalenko at gmail.com  Sat Jan  6 12:55:06 2018
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Sat, 6 Jan 2018 20:55:06 +0300
Subject: [concurrency-interest] non-concurrent collection visibility
	question
Message-ID: <CAO-wXwJqqWFaRfYbPyA7dXGOnaS4aibUTk_jeqfANStr9NBuFA@mail.gmail.com>

By the way, could you please elaborate further and tell how exactly T2
obtains a reference to the object X (unknown from your description) that
has this final field L (i.e. final List<Buffer> buffers = new
ArrayList<>())?

Regards,
Valentin

On 6 January 2018 at 20:50, Valentin Kovalenko <
valentin.male.kovalenko at gmail.com> wrote:

> Thank you for the clarification. In this case, read actions in T2 are not
> ordered by happens-before order (HB) with the write actions in T1. And all
> these actions act on the same shared variables (fields of the list L). So
> by definition, you have a data race. And there is no guarantee that you
> will correctly read all the buffers that T1 added to the L.
>
> I want to add, that generally there is no guarantee that you will see a
> non-null when reading L field from T2. However is your case this field is
> final, and the semantics of a final field guarantees HB* between a write to
> the field L (T1 writes there a reference to a new ArrayList()) and a read
> of the reference from L field in T2. Unfortunately, this HB* is not
> transitive with the normal HB, hence you don't have guarantees you
> initially asked about.
>
> And one more thing: if you had started T2 from T1 after (in program order
> (PO)) T1 added all buffers to L, then you would have had HB between all
> actions in T1 (which are ordered in PO before the start of T2) and all
> actions in T2. But as far as I understand, your case is different and T2 is
> started not from T1.
>
> Regards,
> Valentin
>
> On 6 January 2018 at 20:07, Andriy Bukatar <buka1974 at yahoo.com> wrote:
>
>> Thanks Valentin.
>> Yes, the devil is in the details.
>>
>> I could rephrase the question as follows:
>>
>> There’s a class C containing the non-concurrent list L.
>>
>> Thread T1 adds a few items to the list L.
>>
>> Thread T2 starts (note it did not exist at the time when T1 was updating
>> the list L). Thread T2 gets a reference to C and iterates over its list L.
>>
>> So “AFTER” in this case is a “wall clock distance” between the time T1
>> performs the last update to L and the time when T2 starts.
>>
>> I know the answer to this question when T2 starts iterating over L before
>> (or at the time when) T1 performs updates to non-concurrent L - no
>> visibility guarantees whatsoever.
>>
>> The question I suppose is whether non-existence of T2 at the time of the
>> last update to L has any impact on visibility of the contents of L.
>>
>> Not sure if this makes it any clearer and my apologies if it does not.
>>
>> Thank you
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/7db2f351/attachment-0001.html>

From buka1974 at yahoo.com  Sat Jan  6 13:04:36 2018
From: buka1974 at yahoo.com (Andriy Bukatar)
Date: Sat, 6 Jan 2018 13:04:36 -0500
Subject: [concurrency-interest] non-concurrent collection visibility
	question
In-Reply-To: <CAO-wXw+Q98bOz4c6yz+w9_tZ7C=s7VjvW-6ry5fuqfXUhguujQ@mail.gmail.com>
References: <CAO-wXw+Q98bOz4c6yz+w9_tZ7C=s7VjvW-6ry5fuqfXUhguujQ@mail.gmail.com>
Message-ID: <787B39A7-5AF2-4DED-B072-71BC3AA38207@yahoo.com>

Thanks very much Valentin. Actually your last statement about Program Order addresses my question in its entirety - T1 is a boot-strap thread that creates all the machinery and starts all the receivers (like T2) after it adds all the buffers to L. 

I believe this is the guarantee I was looking for.

Thank you


> On Jan 6, 2018, at 12:50 PM, Valentin Kovalenko <valentin.male.kovalenko at gmail.com> wrote:
> 
> Thank you for the clarification. In this case, read actions in T2 are not ordered by happens-before order (HB) with the write actions in T1. And all these actions act on the same shared variables (fields of the list L). So by definition, you have a data race. And there is no guarantee that you will correctly read all the buffers that T1 added to the L.
> 
> I want to add, that generally there is no guarantee that you will see a non-null when reading L field from T2. However is your case this field is final, and the semantics of a final field guarantees HB* between a write to the field L (T1 writes there a reference to a new ArrayList()) and a read of the reference from L field in T2. Unfortunately, this HB* is not transitive with the normal HB, hence you don't have guarantees you initially asked about.
> 
> And one more thing: if you had started T2 from T1 after (in program order (PO)) T1 added all buffers to L, then you would have had HB between all actions in T1 (which are ordered in PO before the start of T2) and all actions in T2. But as far as I understand, your case is different and T2 is started not from T1.
> 
> Regards,
> Valentin
> 
>> On 6 January 2018 at 20:07, Andriy Bukatar <buka1974 at yahoo.com> wrote:
>> Thanks Valentin.
>> Yes, the devil is in the details.
>> 
>> I could rephrase the question as follows:
>> 
>> There’s a class C containing the non-concurrent list L.
>> 
>> Thread T1 adds a few items to the list L. 
>> 
>> Thread T2 starts (note it did not exist at the time when T1 was updating the list L). Thread T2 gets a reference to C and iterates over its list L.
>> 
>> So “AFTER” in this case is a “wall clock distance” between the time T1 performs the last update to L and the time when T2 starts. 
>> 
>> I know the answer to this question when T2 starts iterating over L before (or at the time when) T1 performs updates to non-concurrent L - no visibility guarantees whatsoever. 
>> 
>> The question I suppose is whether non-existence of T2 at the time of the last update to L has any impact on visibility of the contents of L.
>> 
>> Not sure if this makes it any clearer and my apologies if it does not.
>> 
>> Thank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/f984ad64/attachment.html>

From valentin.male.kovalenko at gmail.com  Sat Jan  6 13:08:01 2018
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Sat, 6 Jan 2018 21:08:01 +0300
Subject: [concurrency-interest] non-concurrent collection visibility
	question
Message-ID: <CAO-wXwLjkpa9XXOLKh+2LQi4AyqsWKek3-+8+NzW0Mi6BmSYcg@mail.gmail.com>

Great, then you are OK.

Regards,
Valentin

On 6 January 2018 at 21:04, Andriy Bukatar <buka1974 at yahoo.com> wrote:

> Thanks very much Valentin. Actually your last statement about Program
> Order addresses my question in its entirety - T1 is a boot-strap thread
> that creates all the machinery and starts all the receivers (like T2) after
> it adds all the buffers to L.
>
> I believe this is the guarantee I was looking for.
>
> Thank you
>
>
>
> On Jan 6, 2018, at 12:50 PM, Valentin Kovalenko <valentin.male.kovalenko@
> gmail.com> wrote:
>
> Thank you for the clarification. In this case, read actions in T2 are not
> ordered by happens-before order (HB) with the write actions in T1. And all
> these actions act on the same shared variables (fields of the list L). So
> by definition, you have a data race. And there is no guarantee that you
> will correctly read all the buffers that T1 added to the L.
>
> I want to add, that generally there is no guarantee that you will see a
> non-null when reading L field from T2. However is your case this field is
> final, and the semantics of a final field guarantees HB* between a write to
> the field L (T1 writes there a reference to a new ArrayList()) and a read
> of the reference from L field in T2. Unfortunately, this HB* is not
> transitive with the normal HB, hence you don't have guarantees you
> initially asked about.
>
> And one more thing: if you had started T2 from T1 after (in program order
> (PO)) T1 added all buffers to L, then you would have had HB between all
> actions in T1 (which are ordered in PO before the start of T2) and all
> actions in T2. But as far as I understand, your case is different and T2 is
> started not from T1.
>
> Regards,
> Valentin
>
> On 6 January 2018 at 20:07, Andriy Bukatar <buka1974 at yahoo.com> wrote:
>
>> Thanks Valentin.
>> Yes, the devil is in the details.
>>
>> I could rephrase the question as follows:
>>
>> There’s a class C containing the non-concurrent list L.
>>
>> Thread T1 adds a few items to the list L.
>>
>> Thread T2 starts (note it did not exist at the time when T1 was updating
>> the list L). Thread T2 gets a reference to C and iterates over its list L.
>>
>> So “AFTER” in this case is a “wall clock distance” between the time T1
>> performs the last update to L and the time when T2 starts.
>>
>> I know the answer to this question when T2 starts iterating over L before
>> (or at the time when) T1 performs updates to non-concurrent L - no
>> visibility guarantees whatsoever.
>>
>> The question I suppose is whether non-existence of T2 at the time of the
>> last update to L has any impact on visibility of the contents of L.
>>
>> Not sure if this makes it any clearer and my apologies if it does not.
>>
>> Thank you
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/f1f8f4b6/attachment.html>

From buka1974 at yahoo.com  Sat Jan  6 13:09:49 2018
From: buka1974 at yahoo.com (Andriy Bukatar)
Date: Sat, 6 Jan 2018 13:09:49 -0500
Subject: [concurrency-interest] non-concurrent collection visibility
	question
In-Reply-To: <CAO-wXwJqqWFaRfYbPyA7dXGOnaS4aibUTk_jeqfANStr9NBuFA@mail.gmail.com>
References: <CAO-wXwJqqWFaRfYbPyA7dXGOnaS4aibUTk_jeqfANStr9NBuFA@mail.gmail.com>
Message-ID: <0643FD00-2E7D-49F7-A282-850A2F418A06@yahoo.com>

The reference to the class C containing list L is passed into the T2’s constructor or to the T2’s (non-synchronized) setter-method which I don’t believe constitutes a safe publication of C’s contents if that’s what you were getting at?

Thank you

> On Jan 6, 2018, at 12:55 PM, Valentin Kovalenko <valentin.male.kovalenko at gmail.com> wrote:
> 
> By the way, could you please elaborate further and tell how exactly T2 obtains a reference to the object X (unknown from your description) that has this final field L (i.e. final List<Buffer> buffers = new ArrayList<>())?
> 
> Regards,
> Valentin
> 
>> On 6 January 2018 at 20:50, Valentin Kovalenko <valentin.male.kovalenko at gmail.com> wrote:
>> Thank you for the clarification. In this case, read actions in T2 are not ordered by happens-before order (HB) with the write actions in T1. And all these actions act on the same shared variables (fields of the list L). So by definition, you have a data race. And there is no guarantee that you will correctly read all the buffers that T1 added to the L.
>> 
>> I want to add, that generally there is no guarantee that you will see a non-null when reading L field from T2. However is your case this field is final, and the semantics of a final field guarantees HB* between a write to the field L (T1 writes there a reference to a new ArrayList()) and a read of the reference from L field in T2. Unfortunately, this HB* is not transitive with the normal HB, hence you don't have guarantees you initially asked about.
>> 
>> And one more thing: if you had started T2 from T1 after (in program order (PO)) T1 added all buffers to L, then you would have had HB between all actions in T1 (which are ordered in PO before the start of T2) and all actions in T2. But as far as I understand, your case is different and T2 is started not from T1.
>> 
>> Regards,
>> Valentin
>> 
>>> On 6 January 2018 at 20:07, Andriy Bukatar <buka1974 at yahoo.com> wrote:
>>> Thanks Valentin.
>>> Yes, the devil is in the details.
>>> 
>>> I could rephrase the question as follows:
>>> 
>>> There’s a class C containing the non-concurrent list L.
>>> 
>>> Thread T1 adds a few items to the list L. 
>>> 
>>> Thread T2 starts (note it did not exist at the time when T1 was updating the list L). Thread T2 gets a reference to C and iterates over its list L.
>>> 
>>> So “AFTER” in this case is a “wall clock distance” between the time T1 performs the last update to L and the time when T2 starts. 
>>> 
>>> I know the answer to this question when T2 starts iterating over L before (or at the time when) T1 performs updates to non-concurrent L - no visibility guarantees whatsoever. 
>>> 
>>> The question I suppose is whether non-existence of T2 at the time of the last update to L has any impact on visibility of the contents of L.
>>> 
>>> Not sure if this makes it any clearer and my apologies if it does not.
>>> 
>>> Thank you
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180106/aca8ede7/attachment-0001.html>

From nathanielwen at 163.com  Sun Jan  7 21:35:15 2018
From: nathanielwen at 163.com (wen)
Date: Mon, 8 Jan 2018 10:35:15 +0800 (GMT+08:00)
Subject: [concurrency-interest] Why "fast path" is faster than full enq
 in AQS
In-Reply-To: <CA+kOe0-cX676MN0xLKcjXD3Op5-1dFsj8SGgvWZ8+H14gJ2Liw@mail.gmail.com>
References: <001101d385f7$09ef2440$1dcd6cc0$@aapt.net.au>
 <686770d9.2fc2.160c53dea62.Coremail.nathanielwen@163.com>
 <002501d3861c$b86f3b20$294db160$@aapt.net.au>
 <f0cffd.4674.160cabbf6d1.Coremail.nathanielwen@163.com>
 <006e01d386d9$104e0490$30ea0db0$@aapt.net.au>
 <CA+kOe0-cX676MN0xLKcjXD3Op5-1dFsj8SGgvWZ8+H14gJ2Liw@mail.gmail.com>
Message-ID: <2904acd9.1287.160d39f3960.Coremail.nathanielwen@163.com>

Thank you for your explanation, Martin and David. Thank you very much.

On 01/6/2018 23:39，Martin Buchholz<martinrb at google.com> wrote：
Alternatively: initializeSyncQueue is now a separate cold-code method.


(All of this is unlikely to matter much)


On Sat, Jan 6, 2018 at 2:28 AM, David Holmes via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:


The old structure was intended to be more amenable to inlining the fast-path code IIRC. The new code doesn’t bother AFAICS.

 

David

 

From: wen [mailto:nathanielwen at 163.com]
Sent: Saturday, January 6, 2018 7:10 PM


To:dholmes at ieee.org
Cc: 'Concurrency-interest' <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

The full enq will loop only once when there is no contention and the tail isn't null. The reason why "fast-path" is faster is because in most cases the tail isn't null. So "if (tail != null)" is more efficient than "if (tail == null) ... else ..." in this very case, am I right?

On 01/5/2018 20:00，David Holmes<davidcholmes at aapt.net.au> wrote：

The fast path just does a direct enqueue to the existing tail. The enq(), in the worst case, has to initialize the queue, and in general deals with contention due to concurrent enqueue attempts.

 

The code is different now.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 5:34 PM
To:dholmes at ieee.org
Cc: 'Concurrency-interest' <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

My JDK version is 1.8

 

private Node addWaiter(Node mode) {

    Node node = new Node(Thread.currentThread(), mode);

    // Try the fast path of enq; backup to full enq on failure

    Node pred = tail;

    if (pred != null) {

        node.prev = pred;

        if (compareAndSetTail(pred, node)) {

            pred.next = node;

            return node;

        }

    }

    enq(node);

    return node;

}

On 01/5/2018 15:30，David Holmes<davidcholmes at aapt.net.au> wrote：

What version of the code are you looking at? I can’t see anything like that in current OpenJDK sources.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of ??? via Concurrency-interest
Sent: Friday, January 5, 2018 4:20 PM
To: concurrency-interest <concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] Why "fast path" is faster than full enq in AQS

 

Hi, everyone:

     I'm reading the source code of "AbstractQueuedSynchronizer", getting a little confused of the reason Why "fast path" is faster than full enq in the comment "Try the fast path of enq; backup to full enq on failure" .


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180108/b7fab3c4/attachment.html>

From andy at stagirite.com  Tue Jan  9 11:48:37 2018
From: andy at stagirite.com (Andrew Nuss)
Date: Tue, 9 Jan 2018 08:48:37 -0800
Subject: [concurrency-interest] how to use Thread.yield to fix
	ConcurrentIntArrayList
Message-ID: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>

I have written a growable concurrent array list of ints.  For stats accumulation.  I think its ok, except for the issue
that it must currently be used by threads of the same priority or there could be deadlock due to a lower priority thread
changing the "ref" in a temporary way in the add() function, and not being able to complete the final set due to a higher
priority thread also for example starting an add() and looping waiting for the final set to occur.

Any ideas on how to fix, such as using Thread.yield() in some effective way?

Or is there something already out there that does this?

public class ConcurrentIntArrayList {

    

    private static final int        DEFAULT_CAPACITY = 16;

    

    private static class MyArray {

        

        private volatile int[]        ar;            // effectively final, but volatile to flush elem values between cores

        private final int            size;

        private final int            readsize;    // may be less than the above if a value being appended

        

        private MyArray (int[] ar, int size, int readsize)

        {

            this.ar = ar;

            this.size = size;

            this.readsize = readsize;

        }

    }

    

    private final AtomicReference<MyArray>        ref;

    private final Object                        mutex = new Object();

    

    /**

     * Construct an underlying array with initial capacity of 16

     */

    public ConcurrentIntArrayList ()

    {

        ref = new AtomicReference<>(new MyArray(new int[DEFAULT_CAPACITY], 0, 0));

    }

    

    public int size ()

    {

        do {

            MyArray ar = ref.get();

            if (ar.size != ar.readsize)

                continue;                                    // this COULD happen

            return ar.size;

        } while (true);

    }

    

    public int get (int index)

    {

        do {

            MyArray ar = ref.get();

            if (index >= ar.size)

                throw new IndexOutOfBoundsException();

            if (index < ar.readsize)

                return ar.ar[index];

        } while (true);

    }

    

    public void add (int value)

    {

        do {

            MyArray ar = ref.get();

            if (ar.size != ar.readsize)

                continue;                                    // this COULD happen

            if (ar.size == ar.ar.length) {

                synchronized (mutex) {

                    MyArray ar2 = ref.get();

                    if (ar == ar2) {

                        // we're the first, grow it

                        int[] tempar = new int[ar.size*2];

                        System.arraycopy(ar.ar, 0, tempar, 0, ar.size);

                        MyArray ar3 = new MyArray(tempar, ar.size, ar.size);

                        if (!ref.compareAndSet(ar, ar3)) {

                            // since we're in the mutex with synchronized lock, no other thread can

                            // be growing the array and since this MyArray "ar" as no other capacity

                            // no one can be adding values outside the synchronized block, and

                            // since this class has no removals, this should NOT happen!

                            continue;

                        }

                        ar = ar3;

                    } else {

                        ar = ar2;

                        if (ar.size == ar.ar.length || ar.size != ar.readsize)

                            continue;                        // this COULD happen

                    }

                }

            }

            

            // reserve room for the value, if this succeeds, then from after these two statements

            // to the ref.set() below, the array is effectively locked relative to other setters

            MyArray ar2 = new MyArray(ar.ar, ar.size+1, ar.size);

            if (!ref.compareAndSet(ar, ar2))

                continue;                                    // this COULD happen

            

            // NOTE: this critical section could fail if threads of different priority

            // use the add function.  A low priority thread could be at this point in the process

            // and a high priority thread could be stuck in add() because of this.  Deadlock.

            ar.ar[ar.size] = value;

            MyArray ar3 = new MyArray(ar.ar, ar.size+1, ar.size+1);

            ref.set(ar3);

            break;

        } while (true);

    }

    

    public int[] toArray ()

    {

        do {

            MyArray ar = ref.get();

            if (ar.size != ar.readsize)

                continue;                                    // this COULD happen

            int[] tempar = new int[ar.size];

            System.arraycopy(ar.ar, 0, tempar, 0, ar.size);

            return tempar;

        } while (true);

    }

}



From nathanila at gmail.com  Tue Jan  9 12:53:09 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Tue, 9 Jan 2018 10:53:09 -0700
Subject: [concurrency-interest] how to use Thread.yield to fix
 ConcurrentIntArrayList
In-Reply-To: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
References: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
Message-ID: <92a91998-47c1-f8e2-5569-ce866fcbb80b@gmail.com>

Thread.yield() will not solve the problem.  The higher priority threads 
will only yield to other threads of the same priority. Hence, the lower 
priority thread will never get a chance to run on the processor.  You 
will have to block enough threads so that the lower priority threads are 
allowed to run.  In other words, you are facing priority inversion.

Is there some way the higher priority thread can do the work that the 
lower priority thread is trying to do?

-Nathan

On 1/9/2018 9:48 AM, Andrew Nuss via Concurrency-interest wrote:
> I have written a growable concurrent array list of ints.  For stats accumulation.  I think its ok, except for the issue
> that it must currently be used by threads of the same priority or there could be deadlock due to a lower priority thread
> changing the "ref" in a temporary way in the add() function, and not being able to complete the final set due to a higher
> priority thread also for example starting an add() and looping waiting for the final set to occur.
>
> Any ideas on how to fix, such as using Thread.yield() in some effective way?
>
> Or is there something already out there that does this?
>
> public class ConcurrentIntArrayList {
>
>      
>
>      private static final int        DEFAULT_CAPACITY = 16;
>
>      
>
>      private static class MyArray {
>
>          
>
>          private volatile int[]        ar;            // effectively final, but volatile to flush elem values between cores
>
>          private final int            size;
>
>          private final int            readsize;    // may be less than the above if a value being appended
>
>          
>
>          private MyArray (int[] ar, int size, int readsize)
>
>          {
>
>              this.ar = ar;
>
>              this.size = size;
>
>              this.readsize = readsize;
>
>          }
>
>      }
>
>      
>
>      private final AtomicReference<MyArray>        ref;
>
>      private final Object                        mutex = new Object();
>
>      
>
>      /**
>
>       * Construct an underlying array with initial capacity of 16
>
>       */
>
>      public ConcurrentIntArrayList ()
>
>      {
>
>          ref = new AtomicReference<>(new MyArray(new int[DEFAULT_CAPACITY], 0, 0));
>
>      }
>
>      
>
>      public int size ()
>
>      {
>
>          do {
>
>              MyArray ar = ref.get();
>
>              if (ar.size != ar.readsize)
>
>                  continue;                                    // this COULD happen
>
>              return ar.size;
>
>          } while (true);
>
>      }
>
>      
>
>      public int get (int index)
>
>      {
>
>          do {
>
>              MyArray ar = ref.get();
>
>              if (index >= ar.size)
>
>                  throw new IndexOutOfBoundsException();
>
>              if (index < ar.readsize)
>
>                  return ar.ar[index];
>
>          } while (true);
>
>      }
>
>      
>
>      public void add (int value)
>
>      {
>
>          do {
>
>              MyArray ar = ref.get();
>
>              if (ar.size != ar.readsize)
>
>                  continue;                                    // this COULD happen
>
>              if (ar.size == ar.ar.length) {
>
>                  synchronized (mutex) {
>
>                      MyArray ar2 = ref.get();
>
>                      if (ar == ar2) {
>
>                          // we're the first, grow it
>
>                          int[] tempar = new int[ar.size*2];
>
>                          System.arraycopy(ar.ar, 0, tempar, 0, ar.size);
>
>                          MyArray ar3 = new MyArray(tempar, ar.size, ar.size);
>
>                          if (!ref.compareAndSet(ar, ar3)) {
>
>                              // since we're in the mutex with synchronized lock, no other thread can
>
>                              // be growing the array and since this MyArray "ar" as no other capacity
>
>                              // no one can be adding values outside the synchronized block, and
>
>                              // since this class has no removals, this should NOT happen!
>
>                              continue;
>
>                          }
>
>                          ar = ar3;
>
>                      } else {
>
>                          ar = ar2;
>
>                          if (ar.size == ar.ar.length || ar.size != ar.readsize)
>
>                              continue;                        // this COULD happen
>
>                      }
>
>                  }
>
>              }
>
>              
>
>              // reserve room for the value, if this succeeds, then from after these two statements
>
>              // to the ref.set() below, the array is effectively locked relative to other setters
>
>              MyArray ar2 = new MyArray(ar.ar, ar.size+1, ar.size);
>
>              if (!ref.compareAndSet(ar, ar2))
>
>                  continue;                                    // this COULD happen
>
>              
>
>              // NOTE: this critical section could fail if threads of different priority
>
>              // use the add function.  A low priority thread could be at this point in the process
>
>              // and a high priority thread could be stuck in add() because of this.  Deadlock.
>
>              ar.ar[ar.size] = value;
>
>              MyArray ar3 = new MyArray(ar.ar, ar.size+1, ar.size+1);
>
>              ref.set(ar3);
>
>              break;
>
>          } while (true);
>
>      }
>
>      
>
>      public int[] toArray ()
>
>      {
>
>          do {
>
>              MyArray ar = ref.get();
>
>              if (ar.size != ar.readsize)
>
>                  continue;                                    // this COULD happen
>
>              int[] tempar = new int[ar.size];
>
>              System.arraycopy(ar.ar, 0, tempar, 0, ar.size);
>
>              return tempar;
>
>          } while (true);
>
>      }
>
> }
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
-Nathan


From andy at stagirite.com  Tue Jan  9 14:06:05 2018
From: andy at stagirite.com (Andrew Nuss)
Date: Tue, 9 Jan 2018 11:06:05 -0800
Subject: [concurrency-interest] how to use Thread.yield to fix
 ConcurrentIntArrayList
In-Reply-To: <92a91998-47c1-f8e2-5569-ce866fcbb80b@gmail.com>
References: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
 <92a91998-47c1-f8e2-5569-ce866fcbb80b@gmail.com>
Message-ID: <ceec4d97-c585-5ff3-98bb-c68b3be4a1f1@stagirite.com>

I'm currently using this class with all threads of the same priority. 
However, given the lack of this type of concurrent arraylist in
java.util.concurrent, I was hoping for a way to generalize the add()
function which mutates the "ref" AtomicReference in such a way that the
add() can be used by both high priority and lower priority threads. 
Again, my read on the commented critical section near the end of the
add() function is that if the compareAndSet() to MyArray with space for
growth succeeds in a low priority thread, and then immediately a high
priority thread calls add(), the high priority thread will deadlock in
the beginning of add() with a busy loop, while the low priority thread
is then unable to get any cpu cycles to free the critical section by
finishing to the point of ref.set(ar3).

This must be solveable, because I assume that ConcurrentHashMap can be
used for put() by both high priority and low priority concurrent
threads, and always makes progress in the high priority thread, because
there is no way a low priority thread can deadlock it.


On 01/09/2018 09:53 AM, Nathan and Ila Reynolds via Concurrency-interest
wrote:
> Thread.yield() will not solve the problem.  The higher priority
> threads will only yield to other threads of the same priority. Hence,
> the lower priority thread will never get a chance to run on the
> processor.  You will have to block enough threads so that the lower
> priority threads are allowed to run.  In other words, you are facing
> priority inversion.
>
> Is there some way the higher priority thread can do the work that the
> lower priority thread is trying to do?
>
> -Nathan
>
> On 1/9/2018 9:48 AM, Andrew Nuss via Concurrency-interest wrote:
>> I have written a growable concurrent array list of ints.  For stats
>> accumulation.  I think its ok, except for the issue
>> that it must currently be used by threads of the same priority or
>> there could be deadlock due to a lower priority thread
>> changing the "ref" in a temporary way in the add() function, and not
>> being able to complete the final set due to a higher
>> priority thread also for example starting an add() and looping
>> waiting for the final set to occur.
>>
>> Any ideas on how to fix, such as using Thread.yield() in some
>> effective way?
>>
>> Or is there something already out there that does this?
>>
>> public class ConcurrentIntArrayList {
>>
>>     
>>      private static final int        DEFAULT_CAPACITY = 16;
>>
>>     
>>      private static class MyArray {
>>
>>         
>>          private volatile int[]        ar;            // effectively
>> final, but volatile to flush elem values between cores
>>
>>          private final int            size;
>>
>>          private final int            readsize;    // may be less
>> than the above if a value being appended
>>
>>         
>>          private MyArray (int[] ar, int size, int readsize)
>>
>>          {
>>
>>              this.ar = ar;
>>
>>              this.size = size;
>>
>>              this.readsize = readsize;
>>
>>          }
>>
>>      }
>>
>>     
>>      private final AtomicReference<MyArray>        ref;
>>
>>      private final Object                        mutex = new Object();
>>
>>     
>>      /**
>>
>>       * Construct an underlying array with initial capacity of 16
>>
>>       */
>>
>>      public ConcurrentIntArrayList ()
>>
>>      {
>>
>>          ref = new AtomicReference<>(new MyArray(new
>> int[DEFAULT_CAPACITY], 0, 0));
>>
>>      }
>>
>>     
>>      public int size ()
>>
>>      {
>>
>>          do {
>>
>>              MyArray ar = ref.get();
>>
>>              if (ar.size != ar.readsize)
>>
>>                  continue;                                    // this
>> COULD happen
>>
>>              return ar.size;
>>
>>          } while (true);
>>
>>      }
>>
>>     
>>      public int get (int index)
>>
>>      {
>>
>>          do {
>>
>>              MyArray ar = ref.get();
>>
>>              if (index >= ar.size)
>>
>>                  throw new IndexOutOfBoundsException();
>>
>>              if (index < ar.readsize)
>>
>>                  return ar.ar[index];
>>
>>          } while (true);
>>
>>      }
>>
>>     
>>      public void add (int value)
>>
>>      {
>>
>>          do {
>>
>>              MyArray ar = ref.get();
>>
>>              if (ar.size != ar.readsize)
>>
>>                  continue;                                    // this
>> COULD happen
>>
>>              if (ar.size == ar.ar.length) {
>>
>>                  synchronized (mutex) {
>>
>>                      MyArray ar2 = ref.get();
>>
>>                      if (ar == ar2) {
>>
>>                          // we're the first, grow it
>>
>>                          int[] tempar = new int[ar.size*2];
>>
>>                          System.arraycopy(ar.ar, 0, tempar, 0, ar.size);
>>
>>                          MyArray ar3 = new MyArray(tempar, ar.size,
>> ar.size);
>>
>>                          if (!ref.compareAndSet(ar, ar3)) {
>>
>>                              // since we're in the mutex with
>> synchronized lock, no other thread can
>>
>>                              // be growing the array and since this
>> MyArray "ar" as no other capacity
>>
>>                              // no one can be adding values outside
>> the synchronized block, and
>>
>>                              // since this class has no removals,
>> this should NOT happen!
>>
>>                              continue;
>>
>>                          }
>>
>>                          ar = ar3;
>>
>>                      } else {
>>
>>                          ar = ar2;
>>
>>                          if (ar.size == ar.ar.length || ar.size !=
>> ar.readsize)
>>
>>                              continue;                        // this
>> COULD happen
>>
>>                      }
>>
>>                  }
>>
>>              }
>>
>>             
>>              // reserve room for the value, if this succeeds, then
>> from after these two statements
>>
>>              // to the ref.set() below, the array is effectively
>> locked relative to other setters
>>
>>              MyArray ar2 = new MyArray(ar.ar, ar.size+1, ar.size);
>>
>>              if (!ref.compareAndSet(ar, ar2))
>>
>>                  continue;                                    // this
>> COULD happen
>>
>>             
>>              // NOTE: this critical section could fail if threads of
>> different priority
>>
>>              // use the add function.  A low priority thread could be
>> at this point in the process
>>
>>              // and a high priority thread could be stuck in add()
>> because of this.  Deadlock.
>>
>>              ar.ar[ar.size] = value;
>>
>>              MyArray ar3 = new MyArray(ar.ar, ar.size+1, ar.size+1);
>>
>>              ref.set(ar3);
>>
>>              break;
>>
>>          } while (true);
>>
>>      }
>>
>>     
>>      public int[] toArray ()
>>
>>      {
>>
>>          do {
>>
>>              MyArray ar = ref.get();
>>
>>              if (ar.size != ar.readsize)
>>
>>                  continue;                                    // this
>> COULD happen
>>
>>              int[] tempar = new int[ar.size];
>>
>>              System.arraycopy(ar.ar, 0, tempar, 0, ar.size);
>>
>>              return tempar;
>>
>>          } while (true);
>>
>>      }
>>
>> }
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



From nathanila at gmail.com  Tue Jan  9 14:23:02 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Tue, 9 Jan 2018 12:23:02 -0700
Subject: [concurrency-interest] how to use Thread.yield to fix
 ConcurrentIntArrayList
In-Reply-To: <ceec4d97-c585-5ff3-98bb-c68b3be4a1f1@stagirite.com>
References: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
 <92a91998-47c1-f8e2-5569-ce866fcbb80b@gmail.com>
 <ceec4d97-c585-5ff3-98bb-c68b3be4a1f1@stagirite.com>
Message-ID: <9f92835f-7407-871e-fd09-e13e8f204999@gmail.com>

ConcurrentHashMap uses a striped lock to protect the contents. If I 
remember right, each array element in the table gets its own lock.  
However, there are others on this list that can tell you every detail 
about ConcurrentHashMap.

-Nathan

On 1/9/2018 12:06 PM, Andrew Nuss via Concurrency-interest wrote:
> I'm currently using this class with all threads of the same priority.
> However, given the lack of this type of concurrent arraylist in
> java.util.concurrent, I was hoping for a way to generalize the add()
> function which mutates the "ref" AtomicReference in such a way that the
> add() can be used by both high priority and lower priority threads.
> Again, my read on the commented critical section near the end of the
> add() function is that if the compareAndSet() to MyArray with space for
> growth succeeds in a low priority thread, and then immediately a high
> priority thread calls add(), the high priority thread will deadlock in
> the beginning of add() with a busy loop, while the low priority thread
> is then unable to get any cpu cycles to free the critical section by
> finishing to the point of ref.set(ar3).
>
> This must be solveable, because I assume that ConcurrentHashMap can be
> used for put() by both high priority and low priority concurrent
> threads, and always makes progress in the high priority thread, because
> there is no way a low priority thread can deadlock it.
>
>
> On 01/09/2018 09:53 AM, Nathan and Ila Reynolds via Concurrency-interest
> wrote:
>> Thread.yield() will not solve the problem.  The higher priority
>> threads will only yield to other threads of the same priority. Hence,
>> the lower priority thread will never get a chance to run on the
>> processor.  You will have to block enough threads so that the lower
>> priority threads are allowed to run.  In other words, you are facing
>> priority inversion.
>>
>> Is there some way the higher priority thread can do the work that the
>> lower priority thread is trying to do?
>>
>> -Nathan
>>
>> On 1/9/2018 9:48 AM, Andrew Nuss via Concurrency-interest wrote:
>>> I have written a growable concurrent array list of ints.  For stats
>>> accumulation.  I think its ok, except for the issue
>>> that it must currently be used by threads of the same priority or
>>> there could be deadlock due to a lower priority thread
>>> changing the "ref" in a temporary way in the add() function, and not
>>> being able to complete the final set due to a higher
>>> priority thread also for example starting an add() and looping
>>> waiting for the final set to occur.
>>>
>>> Any ideas on how to fix, such as using Thread.yield() in some
>>> effective way?
>>>
>>> Or is there something already out there that does this?
>>>
>>> public class ConcurrentIntArrayList {
>>>
>>>      
>>>       private static final int        DEFAULT_CAPACITY = 16;
>>>
>>>      
>>>       private static class MyArray {
>>>
>>>          
>>>           private volatile int[]        ar;            // effectively
>>> final, but volatile to flush elem values between cores
>>>
>>>           private final int            size;
>>>
>>>           private final int            readsize;    // may be less
>>> than the above if a value being appended
>>>
>>>          
>>>           private MyArray (int[] ar, int size, int readsize)
>>>
>>>           {
>>>
>>>               this.ar = ar;
>>>
>>>               this.size = size;
>>>
>>>               this.readsize = readsize;
>>>
>>>           }
>>>
>>>       }
>>>
>>>      
>>>       private final AtomicReference<MyArray>        ref;
>>>
>>>       private final Object                        mutex = new Object();
>>>
>>>      
>>>       /**
>>>
>>>        * Construct an underlying array with initial capacity of 16
>>>
>>>        */
>>>
>>>       public ConcurrentIntArrayList ()
>>>
>>>       {
>>>
>>>           ref = new AtomicReference<>(new MyArray(new
>>> int[DEFAULT_CAPACITY], 0, 0));
>>>
>>>       }
>>>
>>>      
>>>       public int size ()
>>>
>>>       {
>>>
>>>           do {
>>>
>>>               MyArray ar = ref.get();
>>>
>>>               if (ar.size != ar.readsize)
>>>
>>>                   continue;                                    // this
>>> COULD happen
>>>
>>>               return ar.size;
>>>
>>>           } while (true);
>>>
>>>       }
>>>
>>>      
>>>       public int get (int index)
>>>
>>>       {
>>>
>>>           do {
>>>
>>>               MyArray ar = ref.get();
>>>
>>>               if (index >= ar.size)
>>>
>>>                   throw new IndexOutOfBoundsException();
>>>
>>>               if (index < ar.readsize)
>>>
>>>                   return ar.ar[index];
>>>
>>>           } while (true);
>>>
>>>       }
>>>
>>>      
>>>       public void add (int value)
>>>
>>>       {
>>>
>>>           do {
>>>
>>>               MyArray ar = ref.get();
>>>
>>>               if (ar.size != ar.readsize)
>>>
>>>                   continue;                                    // this
>>> COULD happen
>>>
>>>               if (ar.size == ar.ar.length) {
>>>
>>>                   synchronized (mutex) {
>>>
>>>                       MyArray ar2 = ref.get();
>>>
>>>                       if (ar == ar2) {
>>>
>>>                           // we're the first, grow it
>>>
>>>                           int[] tempar = new int[ar.size*2];
>>>
>>>                           System.arraycopy(ar.ar, 0, tempar, 0, ar.size);
>>>
>>>                           MyArray ar3 = new MyArray(tempar, ar.size,
>>> ar.size);
>>>
>>>                           if (!ref.compareAndSet(ar, ar3)) {
>>>
>>>                               // since we're in the mutex with
>>> synchronized lock, no other thread can
>>>
>>>                               // be growing the array and since this
>>> MyArray "ar" as no other capacity
>>>
>>>                               // no one can be adding values outside
>>> the synchronized block, and
>>>
>>>                               // since this class has no removals,
>>> this should NOT happen!
>>>
>>>                               continue;
>>>
>>>                           }
>>>
>>>                           ar = ar3;
>>>
>>>                       } else {
>>>
>>>                           ar = ar2;
>>>
>>>                           if (ar.size == ar.ar.length || ar.size !=
>>> ar.readsize)
>>>
>>>                               continue;                        // this
>>> COULD happen
>>>
>>>                       }
>>>
>>>                   }
>>>
>>>               }
>>>
>>>              
>>>               // reserve room for the value, if this succeeds, then
>>> from after these two statements
>>>
>>>               // to the ref.set() below, the array is effectively
>>> locked relative to other setters
>>>
>>>               MyArray ar2 = new MyArray(ar.ar, ar.size+1, ar.size);
>>>
>>>               if (!ref.compareAndSet(ar, ar2))
>>>
>>>                   continue;                                    // this
>>> COULD happen
>>>
>>>              
>>>               // NOTE: this critical section could fail if threads of
>>> different priority
>>>
>>>               // use the add function.  A low priority thread could be
>>> at this point in the process
>>>
>>>               // and a high priority thread could be stuck in add()
>>> because of this.  Deadlock.
>>>
>>>               ar.ar[ar.size] = value;
>>>
>>>               MyArray ar3 = new MyArray(ar.ar, ar.size+1, ar.size+1);
>>>
>>>               ref.set(ar3);
>>>
>>>               break;
>>>
>>>           } while (true);
>>>
>>>       }
>>>
>>>      
>>>       public int[] toArray ()
>>>
>>>       {
>>>
>>>           do {
>>>
>>>               MyArray ar = ref.get();
>>>
>>>               if (ar.size != ar.readsize)
>>>
>>>                   continue;                                    // this
>>> COULD happen
>>>
>>>               int[] tempar = new int[ar.size];
>>>
>>>               System.arraycopy(ar.ar, 0, tempar, 0, ar.size);
>>>
>>>               return tempar;
>>>
>>>           } while (true);
>>>
>>>       }
>>>
>>> }
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
-Nathan


From bronee at gmail.com  Tue Jan  9 15:00:46 2018
From: bronee at gmail.com (Brian S O'Neill)
Date: Tue, 9 Jan 2018 12:00:46 -0800
Subject: [concurrency-interest] how to use Thread.yield to fix
 ConcurrentIntArrayList
In-Reply-To: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
References: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
Message-ID: <94f3a7ac-b48a-7e73-aea8-fd43a2011d75@gmail.com>

Your comment on the "ar" field suggests that you expect stores into the 
array to behave as volatile stores. This isn't the case. Only the 
pointer to ar itself is volatile. I think you want a final reference to 
an AtomicIntegerArray instead.

On 2018-01-09 08:48 AM, Andrew Nuss via Concurrency-interest wrote:
> I have written a growable concurrent array list of ints.  For stats accumulation.  I think its ok, except for the issue
> that it must currently be used by threads of the same priority or there could be deadlock due to a lower priority thread
> changing the "ref" in a temporary way in the add() function, and not being able to complete the final set due to a higher
> priority thread also for example starting an add() and looping waiting for the final set to occur.
> 
> Any ideas on how to fix, such as using Thread.yield() in some effective way?
> 
> Or is there something already out there that does this?
> 
> public class ConcurrentIntArrayList {
>      private static final int DEFAULT_CAPACITY = 16;
> 
>      private static class MyArray {
>          private volatile int[] ar;  // effectively final, but volatile to flush elem values between cores
> 

From davidcholmes at aapt.net.au  Tue Jan  9 15:59:10 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 10 Jan 2018 06:59:10 +1000
Subject: [concurrency-interest] how to use Thread.yield to
	fix	ConcurrentIntArrayList
In-Reply-To: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
References: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
Message-ID: <018a01d3898c$b2d35f20$187a1d60$@aapt.net.au>

Andrew,

The priority-induced deadlock  you describe can only occur if you are running under a strict priority-preemptive scheduler (i.e SCHED_FIFO) and even then it depends on the number of processors and the number of always runnable threads and their priority. The OpenJDK is not designed to run in such environments - neither the algorithms in existing j.u.c classes, nor the VM itself. To solve your problem a thread that can't make progress must eventually block to ensure other threads can make progress.

David

> -----Original Message-----
> From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Andrew Nuss via Concurrency-
> interest
> Sent: Wednesday, January 10, 2018 2:49 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] how to use Thread.yield to fix ConcurrentIntArrayList
> 
> I have written a growable concurrent array list of ints.  For stats accumulation.  I think its ok, except for the issue that it must currently
> be used by threads of the same priority or there could be deadlock due to a lower priority thread changing the "ref" in a temporary
> way in the add() function, and not being able to complete the final set due to a higher priority thread also for example starting an
> add() and looping waiting for the final set to occur.
> 
> Any ideas on how to fix, such as using Thread.yield() in some effective way?
> 
> Or is there something already out there that does this?
> 
> public class ConcurrentIntArrayList {
> 
> 
> 
>     private static final int        DEFAULT_CAPACITY = 16;
> 
> 
> 
>     private static class MyArray {
> 
> 
> 
>         private volatile int[]        ar;            // effectively final, but volatile to flush elem values between cores
> 
>         private final int            size;
> 
>         private final int            readsize;    // may be less than the above if a value being appended
> 
> 
> 
>         private MyArray (int[] ar, int size, int readsize)
> 
>         {
> 
>             this.ar = ar;
> 
>             this.size = size;
> 
>             this.readsize = readsize;
> 
>         }
> 
>     }
> 
> 
> 
>     private final AtomicReference<MyArray>        ref;
> 
>     private final Object                        mutex = new Object();
> 
> 
> 
>     /**
> 
>      * Construct an underlying array with initial capacity of 16
> 
>      */
> 
>     public ConcurrentIntArrayList ()
> 
>     {
> 
>         ref = new AtomicReference<>(new MyArray(new int[DEFAULT_CAPACITY], 0, 0));
> 
>     }
> 
> 
> 
>     public int size ()
> 
>     {
> 
>         do {
> 
>             MyArray ar = ref.get();
> 
>             if (ar.size != ar.readsize)
> 
>                 continue;                                    // this COULD happen
> 
>             return ar.size;
> 
>         } while (true);
> 
>     }
> 
> 
> 
>     public int get (int index)
> 
>     {
> 
>         do {
> 
>             MyArray ar = ref.get();
> 
>             if (index >= ar.size)
> 
>                 throw new IndexOutOfBoundsException();
> 
>             if (index < ar.readsize)
> 
>                 return ar.ar[index];
> 
>         } while (true);
> 
>     }
> 
> 
> 
>     public void add (int value)
> 
>     {
> 
>         do {
> 
>             MyArray ar = ref.get();
> 
>             if (ar.size != ar.readsize)
> 
>                 continue;                                    // this COULD happen
> 
>             if (ar.size == ar.ar.length) {
> 
>                 synchronized (mutex) {
> 
>                     MyArray ar2 = ref.get();
> 
>                     if (ar == ar2) {
> 
>                         // we're the first, grow it
> 
>                         int[] tempar = new int[ar.size*2];
> 
>                         System.arraycopy(ar.ar, 0, tempar, 0, ar.size);
> 
>                         MyArray ar3 = new MyArray(tempar, ar.size, ar.size);
> 
>                         if (!ref.compareAndSet(ar, ar3)) {
> 
>                             // since we're in the mutex with synchronized lock, no other thread can
> 
>                             // be growing the array and since this MyArray "ar" as no other capacity
> 
>                             // no one can be adding values outside the synchronized block, and
> 
>                             // since this class has no removals, this should NOT happen!
> 
>                             continue;
> 
>                         }
> 
>                         ar = ar3;
> 
>                     } else {
> 
>                         ar = ar2;
> 
>                         if (ar.size == ar.ar.length || ar.size != ar.readsize)
> 
>                             continue;                        // this COULD happen
> 
>                     }
> 
>                 }
> 
>             }
> 
> 
> 
>             // reserve room for the value, if this succeeds, then from after these two statements
> 
>             // to the ref.set() below, the array is effectively locked relative to other setters
> 
>             MyArray ar2 = new MyArray(ar.ar, ar.size+1, ar.size);
> 
>             if (!ref.compareAndSet(ar, ar2))
> 
>                 continue;                                    // this COULD happen
> 
> 
> 
>             // NOTE: this critical section could fail if threads of different priority
> 
>             // use the add function.  A low priority thread could be at this point in the process
> 
>             // and a high priority thread could be stuck in add() because of this.  Deadlock.
> 
>             ar.ar[ar.size] = value;
> 
>             MyArray ar3 = new MyArray(ar.ar, ar.size+1, ar.size+1);
> 
>             ref.set(ar3);
> 
>             break;
> 
>         } while (true);
> 
>     }
> 
> 
> 
>     public int[] toArray ()
> 
>     {
> 
>         do {
> 
>             MyArray ar = ref.get();
> 
>             if (ar.size != ar.readsize)
> 
>                 continue;                                    // this COULD happen
> 
>             int[] tempar = new int[ar.size];
> 
>             System.arraycopy(ar.ar, 0, tempar, 0, ar.size);
> 
>             return tempar;
> 
>         } while (true);
> 
>     }
> 
> }
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From andy at stagirite.com  Tue Jan  9 18:53:25 2018
From: andy at stagirite.com (Andrew Nuss)
Date: Tue, 9 Jan 2018 15:53:25 -0800
Subject: [concurrency-interest] how to use Thread.yield to fix
 ConcurrentIntArrayList
In-Reply-To: <018a01d3898c$b2d35f20$187a1d60$@aapt.net.au>
References: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
 <018a01d3898c$b2d35f20$187a1d60$@aapt.net.au>
Message-ID: <bbdf0f80-8d37-6dc4-8dbd-1613dd7fce38@stagirite.com>

David,

Are you saying that my code works on openjdk as is, or should the loops
at the beginning of each of my functions, loop say 20 times, and then if
no progress, do a Thread.sleep?  Also Brian O'Neill responded that the
volatile marker to the effectively final "ar" member of MyArray should
be final.  Does it really matter in my use case?

Andrew

On 01/09/2018 12:59 PM, David Holmes wrote:
> Andrew,
>
> The priority-induced deadlock  you describe can only occur if you are running under a strict priority-preemptive scheduler (i.e SCHED_FIFO) and even then it depends on the number of processors and the number of always runnable threads and their priority. The OpenJDK is not designed to run in such environments - neither the algorithms in existing j.u.c classes, nor the VM itself. To solve your problem a thread that can't make progress must eventually block to ensure other threads can make progress.
>
> David
>
>



From davidcholmes at aapt.net.au  Tue Jan  9 19:08:57 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 10 Jan 2018 10:08:57 +1000
Subject: [concurrency-interest] how to use Thread.yield to fix
	ConcurrentIntArrayList
In-Reply-To: <bbdf0f80-8d37-6dc4-8dbd-1613dd7fce38@stagirite.com>
References: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
 <018a01d3898c$b2d35f20$187a1d60$@aapt.net.au>
 <bbdf0f80-8d37-6dc4-8dbd-1613dd7fce38@stagirite.com>
Message-ID: <01ac01d389a7$359e8000$a0db8000$@aapt.net.au>

Hi Andrew,

I have not analysed the correctness, or otherwise, of your code. I'm only pointing out that you only need to be concerned about issues around thread priority if running in a full priority-preemptive scheduling environment - in which case you have even more to worry about as OpenJDK is not designed for such an environment and may itself contain livelocks either in library code or the VM (and of course suffer from priority inversion). If you want to write your code in a way that is immune to priority issues then you can't spin indefinitely if that might prevent the thread you are waiting for from ever running - hence you need to degrade from a spin to a block.

Declaring an array reference as volatile is seldom what you need because it has no effect on accessing the array elements - a read of ar[i] will be a volatile read of ar but not of ar[i]. A write to ar[i] will be a volatile read of ar, and a plain write to ar[i].

David

> -----Original Message-----
> From: Andrew Nuss [mailto:andy at stagirite.com]
> Sent: Wednesday, January 10, 2018 9:53 AM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] how to use Thread.yield to fix ConcurrentIntArrayList
> 
> David,
> 
> Are you saying that my code works on openjdk as is, or should the loops at the beginning of each of my functions, loop say 20 times,
> and then if no progress, do a Thread.sleep?  Also Brian O'Neill responded that the volatile marker to the effectively final "ar" member
> of MyArray should be final.  Does it really matter in my use case?
> 
> Andrew
> 
> On 01/09/2018 12:59 PM, David Holmes wrote:
> > Andrew,
> >
> > The priority-induced deadlock  you describe can only occur if you are running under a strict priority-preemptive scheduler (i.e
> SCHED_FIFO) and even then it depends on the number of processors and the number of always runnable threads and their priority.
> The OpenJDK is not designed to run in such environments - neither the algorithms in existing j.u.c classes, nor the VM itself. To solve
> your problem a thread that can't make progress must eventually block to ensure other threads can make progress.
> >
> > David
> >
> >



From nathanila at gmail.com  Tue Jan  9 19:27:30 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Tue, 9 Jan 2018 17:27:30 -0700
Subject: [concurrency-interest] how to use Thread.yield to fix
 ConcurrentIntArrayList
In-Reply-To: <01ac01d389a7$359e8000$a0db8000$@aapt.net.au>
References: <31468240-82fb-4343-7d40-e87f92e81159@stagirite.com>
 <018a01d3898c$b2d35f20$187a1d60$@aapt.net.au>
 <bbdf0f80-8d37-6dc4-8dbd-1613dd7fce38@stagirite.com>
 <01ac01d389a7$359e8000$a0db8000$@aapt.net.au>
Message-ID: <9b4b4846-c7ff-2563-8ac0-c399190035a2@gmail.com>

I highly encourage you to not use Thread.sleep() for blocking. The 
thread may sleep for longer than is needful and the performance may 
suffer.  I have fixed a lot of performance issues where Thread.sleep() 
was the problem.  When the code was adapted so that the thread could be 
woken up, then the performance issues went away.

-Nathan

On 1/9/2018 5:08 PM, David Holmes via Concurrency-interest wrote:
> Hi Andrew,
>
> I have not analysed the correctness, or otherwise, of your code. I'm only pointing out that you only need to be concerned about issues around thread priority if running in a full priority-preemptive scheduling environment - in which case you have even more to worry about as OpenJDK is not designed for such an environment and may itself contain livelocks either in library code or the VM (and of course suffer from priority inversion). If you want to write your code in a way that is immune to priority issues then you can't spin indefinitely if that might prevent the thread you are waiting for from ever running - hence you need to degrade from a spin to a block.
>
> Declaring an array reference as volatile is seldom what you need because it has no effect on accessing the array elements - a read of ar[i] will be a volatile read of ar but not of ar[i]. A write to ar[i] will be a volatile read of ar, and a plain write to ar[i].
>
> David
>
>> -----Original Message-----
>> From: Andrew Nuss [mailto:andy at stagirite.com]
>> Sent: Wednesday, January 10, 2018 9:53 AM
>> To: dholmes at ieee.org
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] how to use Thread.yield to fix ConcurrentIntArrayList
>>
>> David,
>>
>> Are you saying that my code works on openjdk as is, or should the loops at the beginning of each of my functions, loop say 20 times,
>> and then if no progress, do a Thread.sleep?  Also Brian O'Neill responded that the volatile marker to the effectively final "ar" member
>> of MyArray should be final.  Does it really matter in my use case?
>>
>> Andrew
>>
>> On 01/09/2018 12:59 PM, David Holmes wrote:
>>> Andrew,
>>>
>>> The priority-induced deadlock  you describe can only occur if you are running under a strict priority-preemptive scheduler (i.e
>> SCHED_FIFO) and even then it depends on the number of processors and the number of always runnable threads and their priority.
>> The OpenJDK is not designed to run in such environments - neither the algorithms in existing j.u.c classes, nor the VM itself. To solve
>> your problem a thread that can't make progress must eventually block to ensure other threads can make progress.
>>> David
>>>
>>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
-Nathan


From alarmnummer at gmail.com  Fri Jan 12 06:27:07 2018
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Fri, 12 Jan 2018 13:27:07 +0200
Subject: [concurrency-interest] RecursiveTask and asynchronous waiting for
	completion
Message-ID: <CAGuAWdDYvbLg7HU3UwkbmbLuE5ZjnrV1WENdWUO7rJwJUpQ8Ng@mail.gmail.com>

If has a RecursiveTask and on completion I need to take an action. However,
I don't want to block the thread by calling task.join.

So how can one listen to the completion of a RecursiveTask and then take an
action? So comparable like the CompletableFuture.thenApply.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180112/fd22ae40/attachment.html>

From dl at cs.oswego.edu  Fri Jan 12 09:37:10 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 12 Jan 2018 09:37:10 -0500
Subject: [concurrency-interest] RecursiveTask and asynchronous waiting
 for completion
In-Reply-To: <CAGuAWdDYvbLg7HU3UwkbmbLuE5ZjnrV1WENdWUO7rJwJUpQ8Ng@mail.gmail.com>
References: <CAGuAWdDYvbLg7HU3UwkbmbLuE5ZjnrV1WENdWUO7rJwJUpQ8Ng@mail.gmail.com>
Message-ID: <07e887fc-d893-31e7-f34c-7cde7a16c08c@cs.oswego.edu>

On 01/12/2018 06:27 AM, Peter Veentjer via Concurrency-interest wrote:
> If has a RecursiveTask and on completion I need to take an action.
> However, I don't want to block the thread by calling task.join.

You could use CountedCompleter rather than RecursiveTask.
It provides simple and efficient but hard-to-use completion chaining.
Or recast as CompletableFuture, which was introduced in part because
CountedCompleters are hard to use.

-Doug

> 
> So how can one listen to the completion of a RecursiveTask and then take
> an action? So comparable like the CompletableFuture.thenApply.


From brianfromoregon at gmail.com  Fri Jan 12 09:48:44 2018
From: brianfromoregon at gmail.com (Brian Harris)
Date: Fri, 12 Jan 2018 06:48:44 -0800
Subject: [concurrency-interest] offheap plain store reordering
Message-ID: <CAFtUM9YW2rno4+XMO2OB+X8_D6OKKLf5u1NRw2spp3XqK97a-w@mail.gmail.com>

Hi

A question on Vitaly's comment here:
http://cs.oswego.edu/pipermail/concurrency-interest/2013-August/011674.html
"whereas if you have two plain stores, the compiler can technically reorder
them as it sees fit"

Can these offheap plain stores also be reordered by the compiler?
unsafe.putLong(null, addr1, 1)
unsafe.putLong(null, addr2, 2)

If so, I'd want to use putOrderedLong as a compiler directive to prevent
that.
unsafe.putLong(null, addr1, 1)
unsafe.putOrderedLong(null, addr2, 2)

Thanks
Brian
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180112/f0ea415a/attachment.html>

From shade at redhat.com  Fri Jan 12 09:57:28 2018
From: shade at redhat.com (Aleksey Shipilev)
Date: Fri, 12 Jan 2018 15:57:28 +0100
Subject: [concurrency-interest] offheap plain store reordering
In-Reply-To: <CAFtUM9YW2rno4+XMO2OB+X8_D6OKKLf5u1NRw2spp3XqK97a-w@mail.gmail.com>
References: <CAFtUM9YW2rno4+XMO2OB+X8_D6OKKLf5u1NRw2spp3XqK97a-w@mail.gmail.com>
Message-ID: <85ffa450-4f16-02ea-c5fa-fe4aa386eaa7@redhat.com>

On 01/12/2018 03:48 PM, Brian Harris via Concurrency-interest wrote:
> A question on Vitaly's comment
> here: http://cs.oswego.edu/pipermail/concurrency-interest/2013-August/011674.html
> "whereas if you have two plain stores, the compiler can technically reorder them as it sees fit"
> 
> Can these offheap plain stores also be reordered by the compiler?
> unsafe.putLong(null, addr1, 1)
> unsafe.putLong(null, addr2, 2)

Technically, if addr1 and addr2 do not overlap/alias, it is allowed for compiler to issue them in
whatever order. But at least Hotspot's handling of Unsafe intrinsics makes such reordering
impossible for off-heap accesses -- issues CPUOrder membar for them:
 http://hg.openjdk.java.net/jdk/jdk/file/7f57c5908c57/src/hotspot/share/opto/library_call.cpp#l2467

> If so, I'd want to use putOrderedLong as a compiler directive to prevent that.
> unsafe.putLong(null, addr1, 1)
> unsafe.putOrderedLong(null, addr2, 2)

I would say VH.putOpaque gives a bit more efficient code for the intent as stated (prevent compiler
reordering), but it would probably not matter on x86.

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180112/bc99add9/attachment.sig>

From dl at cs.oswego.edu  Tue Jan 16 12:40:36 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 16 Jan 2018 12:40:36 -0500
Subject: [concurrency-interest] juc and GC (plus FJ updates)
Message-ID: <361c7411-791e-af58-1ce7-48a5b908f06b@cs.oswego.edu>


As nicely pointed out recently by Nitsan Wakart (with contributions
from Aleksey Shipilev)
(http://psy-lob-saw.blogspot.com/2018/01/what-difference-jvm-makes.html)
performance of concurrent data structures can vary a lot across
garbage collectors. Of which there are at least nine: ParallelGC,
ConcMarkSweepGC, G1, Shenandoah, ZGC, Zing, IBM-J9 GC, Android, each
with different low-level implementations across processors (X86, ARM,
etc), and each with many tuning options.  Different properties of
collectors interact with concurrency: Write barriers may add ordering
and low-level fencing, read-barriers may widen CAS windows, GC pausing
interacts with blocking for synchronization, safepoints interact with
progress, and placing and moving objects interact with cache pollution
and thread-to-memory affinity.  We'd like j.u.c components to work
reasonably under different collectors (and different JVMs). Which is
challenging if not impossible in general, but it is worth the effort
to do the best we can for some of the most commonly used classes like
ConcurrentHashMap and ForkJoin.  (Some others contain too many
user-selected options to systematically deal with; for example
ThreadPoolExecutor allows any kind of Queue to be used.)

One precaution is to cope with the fact that nearly all collectors use
cardmarks, amplifying false-sharing effects by creating memory
contention for reference writes that happen to be nearby in memory
even if not on same cacheline.  The performance impact is just as bad
as when multiple threads are repeatedly writing the same location (8X
slowdowns are not rare).  GCs also tend to move elements of linked
data structures closer together after a garbage collection, which
sometimes causes programs to get slower over time.  For array-based
collections, one workaround is to over-allocate such that different
arrays that are frequently written by different threads are far enough
apart.  The target value is such that bytes marking addresses from
different arrays are on different cachelines. Emprically, 8Kbytes of
separation works across at least most collectors. Under ParallelGC,
you can band-aid this with -XX:+UseCondCardMark, which reduces most
cases of write-write contention to read-write, leading to less cache
traffic.  Even though FJ guards workstealing arrays by overallocating,
(and normally-populated ConcurrentHashMaps intrinsically do so for
their arrays) UseCondCardMark usually improves performance by reducing
impact of accessing bookeeping fields of other objects like executed
FJ tasks. (It doesn't always improve because of extra per-write
overhead that might not have been needed, but even if so, impact is
small.)  Collectors that don't have this option are more prone to
occasional mysterious big slowdowns.

Contemplation of other GC mechanics sometimes reduces other negative
impact. For example, if a reference write might sometimes entail a
memory fence anyway, then you might prefer basing explicitly atomic
operations on them vs surrounding them with locks. ConcurrentHashMap,
ConcurrentLinkedQueue, CompletableFuture, SubmissionPublisher, and
ForkJoin, mostly do this. I just locally (jsr166 CVS) committed some
FJ updates that do this in more cases and add a few other tweaky
adjustments.  The impact on ParallelGC is small, but others now seem
more well-behaved.

Across various tests that mainly exercise concurrency (without doing
very much else), in hostpot, -XX:+UseParallelGC -XX:+UseCondCardMark
nearly always gives best throughput, even better than no-GC (Epsilon),
but with occasional long GC pauses. In most cases, G1 is around 10%
slower; sometimes moreso. I don't have enough empirical experience with
newer in-progress collectors or other JVMs to say anything about them
yet in general.  As always, the main determiners of performance in
ForkJoin (as well as ThreadPoolExecutor and other async frameworks)
are task issue rate (granularity) and isolation (including
not blocking within tasks).  If async tasks contain around 10,000
thread-local instructions, GC choices depend less on concurrency per
se, and more on throughput/latency choices etc.

Please try out tentative updates in
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar (running with java
--patch-module java.base="$DIR/jsr166.jar"). We also are always
looking to add more performance tests that are informative about basic
functionality or interactions with other frameworks.  Contributions
welcome.  If you are interested in further exploring impact on
hotspot, try out Aleksey's builds with some of the new in-progress
GCs, at https://builds.shipilev.net/

Aside: All of the above emphasize that we are in the era of the "killer
microsecond", where most performance challenges are not handled well
by nanosecod-level (usually CPU-based, like instruction parallelism)
or millisecond-level (usually OS-based, like IO) techniques.  See
https://cacm.acm.org/magazines/2017/4/215032-attack-of-the-killer-microseconds/fulltext

-Doug


From gil at azul.com  Tue Jan 16 15:34:02 2018
From: gil at azul.com (Gil Tene)
Date: Tue, 16 Jan 2018 20:34:02 +0000
Subject: [concurrency-interest] juc and GC (plus FJ updates)
In-Reply-To: <361c7411-791e-af58-1ce7-48a5b908f06b@cs.oswego.edu>
References: <361c7411-791e-af58-1ce7-48a5b908f06b@cs.oswego.edu>
Message-ID: <9BEB7749-5BD9-45A2-A918-EA82991171AF@azul.com>

I agree that some level of conditional card marking (like -XX:+UseCondCardMark in ParallelGC) can be critical to avoid sharing contention on card mark. With HotSpot's current byte-per-card and card-per-512-bytes-of-heap ratios, there is real sharing of individual card marks [representing 512 bytes of heap each], false sharing on a card mark line [representing 64 cards on x86] and false sharing for heap reference stores [all stores to the same 32KB-aligned region of heap (in x86) represent a store to the same cardmark cache line]. Arguably, in a world where the likelihood of multiple threads mutating references that are stored in the same 32KB of heap is pretty high, conditional card marking should arguably be a default.

Conditional card marking (in the sense that the mark does not dirty an already dirty card) is [currently] more expensive primarily because it involves reading the card value for evaluation. When this extra cost is paid on every reference mutation, it can offset much of the scaling gains, and presents an annoying tradeoff choice between single-threaded performance and multi-threaded scalability.

But the extra cost of conditional card marking can be combatted fairly effectively. For reference, in Zing we've worked hard to eliminate these tradeoffs (our designs started off on 384-way Vega SMPs, so false sharing on card marks was an obvious non-starter). Zing's card marking scheme uses conditional marking at all times, which avoids line contention at one level (writers to the same card or card cache line). We also use precise card marking [a card bit per heap word rather than a card byte per 512 heap bytes], which completely eliminates all card-related false sharing. In addition, we reduce the cost of the read-and-maybe-write: our conditional card modification check (which requires a card read) is much cheaper in practice because it is fronted by a conditional newgen-ref-being-stored-into-oldgen-location check which is purely based on addresses, does not require any reads, and eliminates 95%-99%+ of card modification attempts.

Historically, the main design reason against bit-based card marking had been that unconditional card marks are faster when using a byte per card (it's a fire-and-forget byte store operation, with no need for a read-modify-write thing). However, once you buy into the conditional card marking notion (with or without generation-based filter ahead of it) , you've introduced a read-check-and-maybe-write thing anyway, making a read-check-and-maybe-modify-and-write thing not that big a deal. The steps from there to precise (1 card per heap word) card marking then become pretty obvious.

I'd encourage other card-marking collectors (which is pretty much all current and most future generational schemes) to look at similar notions for the card table as they can help reduce multi-threaded scaling sensitivity of most workloads. Specifically, reducing the cost of conditional card marking (by e.g. fronting it with a cross-generation-contitional filter) can help eliminate the notion that a tradeoff is involved when turning on -XX:+UseCondCardMark. With the current 32KB-wide reference-mutation-contention window in HotSpot, false-card-line-sharing contention can easily hit unsuspecting and even unrelated data structure, and the obvious healthy default should avoid that.

— Gil.

> On Jan 16, 2018, at 9:40 AM, Doug Lea via Concurrency-interest <Concurrency-interest at cs.oswego.edu> wrote:
> 
> 
> As nicely pointed out recently by Nitsan Wakart (with contributions
> from Aleksey Shipilev)
> (http://psy-lob-saw.blogspot.com/2018/01/what-difference-jvm-makes.html)
> performance of concurrent data structures can vary a lot across
> garbage collectors. Of which there are at least nine: ParallelGC,
> ConcMarkSweepGC, G1, Shenandoah, ZGC, Zing, IBM-J9 GC, Android, each
> with different low-level implementations across processors (X86, ARM,
> etc), and each with many tuning options.  Different properties of
> collectors interact with concurrency: Write barriers may add ordering
> and low-level fencing, read-barriers may widen CAS windows, GC pausing
> interacts with blocking for synchronization, safepoints interact with
> progress, and placing and moving objects interact with cache pollution
> and thread-to-memory affinity.  We'd like j.u.c components to work
> reasonably under different collectors (and different JVMs). Which is
> challenging if not impossible in general, but it is worth the effort
> to do the best we can for some of the most commonly used classes like
> ConcurrentHashMap and ForkJoin.  (Some others contain too many
> user-selected options to systematically deal with; for example
> ThreadPoolExecutor allows any kind of Queue to be used.)
> 
> One precaution is to cope with the fact that nearly all collectors use
> cardmarks, amplifying false-sharing effects by creating memory
> contention for reference writes that happen to be nearby in memory
> even if not on same cacheline.  The performance impact is just as bad
> as when multiple threads are repeatedly writing the same location (8X
> slowdowns are not rare).  GCs also tend to move elements of linked
> data structures closer together after a garbage collection, which
> sometimes causes programs to get slower over time.  For array-based
> collections, one workaround is to over-allocate such that different
> arrays that are frequently written by different threads are far enough
> apart.  The target value is such that bytes marking addresses from
> different arrays are on different cachelines. Emprically, 8Kbytes of
> separation works across at least most collectors. Under ParallelGC,
> you can band-aid this with -XX:+UseCondCardMark, which reduces most
> cases of write-write contention to read-write, leading to less cache
> traffic.  Even though FJ guards workstealing arrays by overallocating,
> (and normally-populated ConcurrentHashMaps intrinsically do so for
> their arrays) UseCondCardMark usually improves performance by reducing
> impact of accessing bookeeping fields of other objects like executed
> FJ tasks. (It doesn't always improve because of extra per-write
> overhead that might not have been needed, but even if so, impact is
> small.)  Collectors that don't have this option are more prone to
> occasional mysterious big slowdowns.
> 
> Contemplation of other GC mechanics sometimes reduces other negative
> impact. For example, if a reference write might sometimes entail a
> memory fence anyway, then you might prefer basing explicitly atomic
> operations on them vs surrounding them with locks. ConcurrentHashMap,
> ConcurrentLinkedQueue, CompletableFuture, SubmissionPublisher, and
> ForkJoin, mostly do this. I just locally (jsr166 CVS) committed some
> FJ updates that do this in more cases and add a few other tweaky
> adjustments.  The impact on ParallelGC is small, but others now seem
> more well-behaved.
> 
> Across various tests that mainly exercise concurrency (without doing
> very much else), in hostpot, -XX:+UseParallelGC -XX:+UseCondCardMark
> nearly always gives best throughput, even better than no-GC (Epsilon),
> but with occasional long GC pauses. In most cases, G1 is around 10%
> slower; sometimes moreso. I don't have enough empirical experience with
> newer in-progress collectors or other JVMs to say anything about them
> yet in general.  As always, the main determiners of performance in
> ForkJoin (as well as ThreadPoolExecutor and other async frameworks)
> are task issue rate (granularity) and isolation (including
> not blocking within tasks).  If async tasks contain around 10,000
> thread-local instructions, GC choices depend less on concurrency per
> se, and more on throughput/latency choices etc.
> 
> Please try out tentative updates in
> http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar (running with java
> --patch-module java.base="$DIR/jsr166.jar"). We also are always
> looking to add more performance tests that are informative about basic
> functionality or interactions with other frameworks.  Contributions
> welcome.  If you are interested in further exploring impact on
> hotspot, try out Aleksey's builds with some of the new in-progress
> GCs, at https://builds.shipilev.net/
> 
> Aside: All of the above emphasize that we are in the era of the "killer
> microsecond", where most performance challenges are not handled well
> by nanosecod-level (usually CPU-based, like instruction parallelism)
> or millisecond-level (usually OS-based, like IO) techniques.  See
> https://cacm.acm.org/magazines/2017/4/215032-attack-of-the-killer-microseconds/fulltext
> 
> -Doug
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 874 bytes
Desc: Message signed with OpenPGP
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180116/fa399d1e/attachment.sig>

From pavel.rappo at gmail.com  Wed Jan 17 15:28:21 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Wed, 17 Jan 2018 20:28:21 +0000
Subject: [concurrency-interest] Exception handling with CompletionStage
Message-ID: <CAChcVu=9VcXmJP70vYpvHkMUJrW9KwjinsFBbqPudsYmFKAiwQ@mail.gmail.com>

Hello,

I have a question regarding a case I ran into while handling an exception
relayed through a sequence of stages. Consider the following scenarios:

1.

    public static void main(String[] args) {
        CompletableFuture.failedFuture(new RuntimeException("hello")).join();
    }

-- stdout --
Exception in thread "main" java.util.concurrent.CompletionException:
java.lang.RuntimeException: hello
   at java.base/java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:412)
   at java.base/java.util.concurrent.CompletableFuture.join(CompletableFuture.java:2044)
   at CF.main(CF.java:6)
Caused by: java.lang.RuntimeException: hello
   ... 1 more

2.

    public static void main(String[] args) {
        CompletableFuture.failedFuture(new RuntimeException("hello"))
                .whenComplete((r, e) -> System.out.println("Error: " + e))
                .join();
    }

-- stdout --

Error: java.lang.RuntimeException: hello
Exception in thread "main" java.util.concurrent.CompletionException:
java.lang.RuntimeException: hello
   at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
   at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
   at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:870)
   at java.base/java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:883)
   at java.base/java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2251)
   at CF.main(CF.java:7)
Caused by: java.lang.RuntimeException: hello
   at CF.main(CF.java:6)

3.

    public static void main(String[] args) {
        CompletableFuture.failedFuture(new RuntimeException("hello"))
                .whenComplete((r, e) -> System.out.println("Error 1: " + e))
                .whenComplete((r, e) -> System.out.println("Error 2: " + e))
                .join();
    }

-- stdout --

Error 1: java.lang.RuntimeException: hello
Error 2: java.util.concurrent.CompletionException:
java.lang.RuntimeException: hello
Exception in thread "main" java.util.concurrent.CompletionException:
java.lang.RuntimeException: hello
   at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
   at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
   at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:870)
   at java.base/java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:883)
   at java.base/java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2251)
   at CF.main(CF.java:7)
Caused by: java.lang.RuntimeException: hello
   at CF.main(CF.java:6)

-------------

I guess the difference in these behaviors could be (somewhat) explained by this
passage in the javadoc for CompletionStage:

 * In all other cases, if a stage's computation terminates abruptly
 * with an (unchecked) exception or error, then all dependent stages
 * requiring its completion complete exceptionally as well, with a
 * {@link CompletionException} holding the exception as its cause.

What was the rationale behind passing a wrapped exception to the dependant
stage? Why is it not wrapped in the first place (in the failedFuture method)?

And finally. How would one organise exception handling not being sure if the
target exception is wrapped or not?

Thanks,
-Pavel

From akarnokd at gmail.com  Wed Jan 17 15:47:32 2018
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Wed, 17 Jan 2018 21:47:32 +0100
Subject: [concurrency-interest] Exception handling with CompletionStage
In-Reply-To: <CAChcVu=9VcXmJP70vYpvHkMUJrW9KwjinsFBbqPudsYmFKAiwQ@mail.gmail.com>
References: <CAChcVu=9VcXmJP70vYpvHkMUJrW9KwjinsFBbqPudsYmFKAiwQ@mail.gmail.com>
Message-ID: <CAAWwtm8pMGcTLqKPL7rV+xhBjmTTofazOETMbS23n0HjRj-uXQ@mail.gmail.com>

I can't answer the why, but I have experience with how.

In my recent async-enumerable library, (anticipating an async-await world
:) I used CompletionStage as the means to communicate the next value is
ready. This wrapping behavior forced me to use a helper CompletableFuture
at processing stages as simply forwarding with a whenComplete or the other
CompletionStage operators made the errors wrapped:

public CompletionStage<Boolean> moveNext() {
   CompletableFuture<Boolean> cf = new CompletableFuture<>();

   source.moveNext().whenComplete((hasValue, error) -> {
      if (error != null) {
         cf.completeExceptionally(error);
         return;
      }

      if (hasValue) {
         current = mapperFunction.apply(source.current());
         cf.complete(true);
      } else {
         cf.complete(false);
      }
   });

   return cf;
}



2018-01-17 21:28 GMT+01:00 Pavel Rappo via Concurrency-interest <
concurrency-interest at cs.oswego.edu>:

> Hello,
>
> I have a question regarding a case I ran into while handling an exception
> relayed through a sequence of stages. Consider the following scenarios:
>
> 1.
>
>     public static void main(String[] args) {
>         CompletableFuture.failedFuture(new RuntimeException("hello")).
> join();
>     }
>
> -- stdout --
> Exception in thread "main" java.util.concurrent.CompletionException:
> java.lang.RuntimeException: hello
>    at java.base/java.util.concurrent.CompletableFuture.
> reportJoin(CompletableFuture.java:412)
>    at java.base/java.util.concurrent.CompletableFuture.
> join(CompletableFuture.java:2044)
>    at CF.main(CF.java:6)
> Caused by: java.lang.RuntimeException: hello
>    ... 1 more
>
> 2.
>
>     public static void main(String[] args) {
>         CompletableFuture.failedFuture(new RuntimeException("hello"))
>                 .whenComplete((r, e) -> System.out.println("Error: " + e))
>                 .join();
>     }
>
> -- stdout --
>
> Error: java.lang.RuntimeException: hello
> Exception in thread "main" java.util.concurrent.CompletionException:
> java.lang.RuntimeException: hello
>    at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(
> CompletableFuture.java:331)
>    at java.base/java.util.concurrent.CompletableFuture.completeThrowable(
> CompletableFuture.java:346)
>    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(
> CompletableFuture.java:870)
>    at java.base/java.util.concurrent.CompletableFuture.
> uniWhenCompleteStage(CompletableFuture.java:883)
>    at java.base/java.util.concurrent.CompletableFuture.whenComplete(
> CompletableFuture.java:2251)
>    at CF.main(CF.java:7)
> Caused by: java.lang.RuntimeException: hello
>    at CF.main(CF.java:6)
>
> 3.
>
>     public static void main(String[] args) {
>         CompletableFuture.failedFuture(new RuntimeException("hello"))
>                 .whenComplete((r, e) -> System.out.println("Error 1: " +
> e))
>                 .whenComplete((r, e) -> System.out.println("Error 2: " +
> e))
>                 .join();
>     }
>
> -- stdout --
>
> Error 1: java.lang.RuntimeException: hello
> Error 2: java.util.concurrent.CompletionException:
> java.lang.RuntimeException: hello
> Exception in thread "main" java.util.concurrent.CompletionException:
> java.lang.RuntimeException: hello
>    at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(
> CompletableFuture.java:331)
>    at java.base/java.util.concurrent.CompletableFuture.completeThrowable(
> CompletableFuture.java:346)
>    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(
> CompletableFuture.java:870)
>    at java.base/java.util.concurrent.CompletableFuture.
> uniWhenCompleteStage(CompletableFuture.java:883)
>    at java.base/java.util.concurrent.CompletableFuture.whenComplete(
> CompletableFuture.java:2251)
>    at CF.main(CF.java:7)
> Caused by: java.lang.RuntimeException: hello
>    at CF.main(CF.java:6)
>
> -------------
>
> I guess the difference in these behaviors could be (somewhat) explained by
> this
> passage in the javadoc for CompletionStage:
>
>  * In all other cases, if a stage's computation terminates abruptly
>  * with an (unchecked) exception or error, then all dependent stages
>  * requiring its completion complete exceptionally as well, with a
>  * {@link CompletionException} holding the exception as its cause.
>
> What was the rationale behind passing a wrapped exception to the dependant
> stage? Why is it not wrapped in the first place (in the failedFuture
> method)?
>
> And finally. How would one organise exception handling not being sure if
> the
> target exception is wrapped or not?
>
> Thanks,
> -Pavel
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180117/ee63ace2/attachment.html>

From pavel.rappo at gmail.com  Wed Jan 17 16:35:37 2018
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Wed, 17 Jan 2018 21:35:37 +0000
Subject: [concurrency-interest] Exception handling with CompletionStage
In-Reply-To: <CAAWwtm8pMGcTLqKPL7rV+xhBjmTTofazOETMbS23n0HjRj-uXQ@mail.gmail.com>
References: <CAChcVu=9VcXmJP70vYpvHkMUJrW9KwjinsFBbqPudsYmFKAiwQ@mail.gmail.com>
 <CAAWwtm8pMGcTLqKPL7rV+xhBjmTTofazOETMbS23n0HjRj-uXQ@mail.gmail.com>
Message-ID: <CAChcVu=m+ANL3XVA7m0ya7e+nAgK3kcg=06dPts2mvmKuHN5Vw@mail.gmail.com>

Yeah, it seems like we have a couple of options here:

1) To unwrap the exception (like you did) by making a dependant stage which is
then completed manually

2) To wrap any directly passed exception into CompletionException

    CompletableFuture.failedFuture(
            new CompletionException(
                    new RuntimeException("hello")))

        or

    CompletableFuture<Object> cf = new CompletableFuture<>();
    cf.completeExceptionally(new CompletionException(new
RuntimeException("hello")));

Option (1) requires extra CompletionStage and the related boilerplate on the
producing site. Option (2) requires extra instanceof/getClass handling
boilerplate on each of the consuming sites.

Given this, I'll probably stick with option (1).

On Wed, Jan 17, 2018 at 8:47 PM, Dávid Karnok <akarnokd at gmail.com> wrote:
> I can't answer the why, but I have experience with how.
>
> In my recent async-enumerable library, (anticipating an async-await world :)
> I used CompletionStage as the means to communicate the next value is ready.
> This wrapping behavior forced me to use a helper CompletableFuture at
> processing stages as simply forwarding with a whenComplete or the other
> CompletionStage operators made the errors wrapped:
>
> public CompletionStage<Boolean> moveNext() {
>    CompletableFuture<Boolean> cf = new CompletableFuture<>();
>
>    source.moveNext().whenComplete((hasValue, error) -> {
>       if (error != null) {
>          cf.completeExceptionally(error);
>          return;
>       }
>
>       if (hasValue) {
>          current = mapperFunction.apply(source.current());
>          cf.complete(true);
>       } else {
>          cf.complete(false);
>       }
>    });
>
>    return cf;
> }
>
>
>
> 2018-01-17 21:28 GMT+01:00 Pavel Rappo via Concurrency-interest
> <concurrency-interest at cs.oswego.edu>:
>>
>> Hello,
>>
>> I have a question regarding a case I ran into while handling an exception
>> relayed through a sequence of stages. Consider the following scenarios:
>>
>> 1.
>>
>>     public static void main(String[] args) {
>>         CompletableFuture.failedFuture(new
>> RuntimeException("hello")).join();
>>     }
>>
>> -- stdout --
>> Exception in thread "main" java.util.concurrent.CompletionException:
>> java.lang.RuntimeException: hello
>>    at
>> java.base/java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:412)
>>    at
>> java.base/java.util.concurrent.CompletableFuture.join(CompletableFuture.java:2044)
>>    at CF.main(CF.java:6)
>> Caused by: java.lang.RuntimeException: hello
>>    ... 1 more
>>
>> 2.
>>
>>     public static void main(String[] args) {
>>         CompletableFuture.failedFuture(new RuntimeException("hello"))
>>                 .whenComplete((r, e) -> System.out.println("Error: " + e))
>>                 .join();
>>     }
>>
>> -- stdout --
>>
>> Error: java.lang.RuntimeException: hello
>> Exception in thread "main" java.util.concurrent.CompletionException:
>> java.lang.RuntimeException: hello
>>    at
>> java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
>>    at
>> java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
>>    at
>> java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:870)
>>    at
>> java.base/java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:883)
>>    at
>> java.base/java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2251)
>>    at CF.main(CF.java:7)
>> Caused by: java.lang.RuntimeException: hello
>>    at CF.main(CF.java:6)
>>
>> 3.
>>
>>     public static void main(String[] args) {
>>         CompletableFuture.failedFuture(new RuntimeException("hello"))
>>                 .whenComplete((r, e) -> System.out.println("Error 1: " +
>> e))
>>                 .whenComplete((r, e) -> System.out.println("Error 2: " +
>> e))
>>                 .join();
>>     }
>>
>> -- stdout --
>>
>> Error 1: java.lang.RuntimeException: hello
>> Error 2: java.util.concurrent.CompletionException:
>> java.lang.RuntimeException: hello
>> Exception in thread "main" java.util.concurrent.CompletionException:
>> java.lang.RuntimeException: hello
>>    at
>> java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
>>    at
>> java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
>>    at
>> java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:870)
>>    at
>> java.base/java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:883)
>>    at
>> java.base/java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2251)
>>    at CF.main(CF.java:7)
>> Caused by: java.lang.RuntimeException: hello
>>    at CF.main(CF.java:6)
>>
>> -------------
>>
>> I guess the difference in these behaviors could be (somewhat) explained by
>> this
>> passage in the javadoc for CompletionStage:
>>
>>  * In all other cases, if a stage's computation terminates abruptly
>>  * with an (unchecked) exception or error, then all dependent stages
>>  * requiring its completion complete exceptionally as well, with a
>>  * {@link CompletionException} holding the exception as its cause.
>>
>> What was the rationale behind passing a wrapped exception to the dependant
>> stage? Why is it not wrapped in the first place (in the failedFuture
>> method)?
>>
>> And finally. How would one organise exception handling not being sure if
>> the
>> target exception is wrapped or not?
>>
>> Thanks,
>> -Pavel
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> --
> Best regards,
> David Karnok

