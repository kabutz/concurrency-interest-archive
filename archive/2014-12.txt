From viktor.klang at gmail.com  Mon Dec  1 04:59:56 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 1 Dec 2014 10:59:56 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-vV-hm2ivu0ge33y-5UXViWXYpuPwE8C=g2V0CXqAi5aw@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<CANPzfU9fnMvdkbnKWb35hg5RXFnDgH3dQTGegeSSPAXVpYSPug@mail.gmail.com>
	<CAHJZN-vCAsHnw+GJ=oGp1LH_OOAYXTVxgrYihGtu9pJhObeJUg@mail.gmail.com>
	<CANPzfU9RBqX3zF40F7ZV4oPQGMVtBjo7dqQsZbt3Bw4P=qUyVQ@mail.gmail.com>
	<CAHJZN-vV-hm2ivu0ge33y-5UXViWXYpuPwE8C=g2V0CXqAi5aw@mail.gmail.com>
Message-ID: <CANPzfU8JfO6o9NyRzSQduc+G0_WnaR6zMJbzck4ZdsMOL4AdqA@mail.gmail.com>

Hi Josh,

apologies for the delayed response.

On Fri, Nov 28, 2014 at 7:50 PM, Josh Humphries <jh at squareup.com> wrote:

> Thanks for your input, Viktor. It looks like our use cases are very
> different, hence our differences of opinion.
>
> We have plenty of services that use blocking I/O, JDBC, etc.
>

We avoid blocking at all cost, since the cost of pushing around contexts
and the thread wakeup lag on modern CPUs does not scale.


> We have no problem scaling over 4 cores (perhaps I don't understand what
> you mean by that expression, but we can easily saturate much greater than 4
> cores/host across multiple hosts). Because so many tools we use involve
> blocking, we use normal thread pools (ThreadPoolExecutorService), not
> ForkJoinPools.
>

Saturate cores is easy, having them do meaning ful work with all that is
hard (especially if you have any kind of blocking: see Amdahl's Law and
Neil Gunther's Universal Scalability Law)


>
> Since synchronous code is often easier to write and maintain and has not
> posed an issue for scalability for us,
>

The first part of the sentence does not really connect to the second one.
Synchronous code is often time easier to write initially and then much
harder to maintain because when one has scalability-issues and is not
familiar with how to swrite non-blocking asynchronous code, it is hard to
overcome performance/scalability issues imposed by the synchronous code.
At least this is my experience.


> the blocking option is fine for our purposes. But we tend to use
> asynchronous idioms under-the-hood in frameworks/libraries, to maximize
> parallelism and reduce latency.
>

In my experience it is rather fruitless to keep the innards async and
non-blocking if the first thing that end-users are going to do is to create
their own deadlocks, sprinkle timeouts everwhere or even worse, do
unbounded blocking.


>
> So, as a client of an asynchronous task, being able to do both blocking
> and non-blocking consumption is valuable as it means we don't need
> different APIs (or verbose wrappers/adapters from async->blocking) for
> typical usages, but it also still allows "power users" to easily do things
> asynchronously.
>

I strongly disagree. If it is -easier or as-easy- to do the wrong thing,
users will, without even thinking about it?do it.
I tried very hard to eliminate -all- blocking operations except for one,
external, way of doing it, and I don't know how much time has been wasted
helping users out who have blocked themselves into a corner.


>
> However, as the implementor of an asynchronous task, I don't like the fact
> that CompletableFuture gives the consumer a chance to actually produce the
> value (via the complete* and obtrude*).
>

This I -completely- agree with, which is why you'd never return a
CompletableFuture unless you wanted the user to cancel it, obtrude it or
otherwise modify it?instead you'd return a CompletionStage.


> CompletableFuture is an implementation detail of the *producer* -- it's
> one approach to fulfilling the result.
>

Absolutely! :)


> So having that in the public API that *clients* use (e.g. CompletionStage)
> is leaky.
>

Agreed. I'd prefer to have a static method on CompletableFuture that
optionally "wraps" the CompletionStage into a CompletableFuture or, if it
indeed is a CompletableFuture, casts and returns it.


>
> So to me, splitting imperative completion and task-based implicit
> completion into different interfaces is a different concern than splitting
> blocking and non-blocking forms of consumption.
>

I don't see how you reach this conclusion based on the previous arguments,
could you elaborate in what way they are so different that it is not mixing
concerns in the latter case?


>
> BTW, I agree 100% about CompletionStage's API being way too broad. Keeping
> the API broad would probably be okay if they had default implementations,
> so that interface implementors need only worry about a handful of methods
> to actually implement instead of umpteen.
>

Ending the email on a happy note then! :-)


>
>
>
>
> ----
> *Josh Humphries*
> Manager, Shared Systems  |  Platform Engineering
> Atlanta, GA  |  678-400-4867
> *Square* (www.squareup.com)
>
> On Fri, Nov 28, 2014 at 11:57 AM, ?iktor ?lang <viktor.klang at gmail.com>
> wrote:
>
>> Thanks for your reply, Josh!
>> Answers inline
>>
>> On Fri, Nov 28, 2014 at 3:29 PM, Josh Humphries <jh at squareup.com> wrote:
>>
>>> Below are some of my nits about the Java8 APIs:
>>>>>
>>>>> I disagree with how the CompletionStage interface is so tightly
>>>>> coupled to the CompletableFuture implementation. Why does CompletionStage
>>>>> have a toCompletableFuture() method? It feels like the interface should
>>>>> instead have just extended Future.
>>>>>
>>>>
>>>> I agree that `toCompletableFuture` is less than ideal, but having
>>>> CompletionStage extend Future would be way worse, there is not a single
>>>> method on j.u.c.Future that I'd ever want to call, or ever want any of my
>>>> users to call.
>>>>
>>>
>>> CompletionStage provides no way to actually get the result of
>>> computation,
>>>
>>
>> Most of the methods on it gives you the result. But I assume you mean
>> block to get the result.
>>
>>
>>> to block until it's done (or even query if it's done),
>>>
>>
>> I argue, based on experience, that blocking shouldn't be done in 99% of
>> the cases (blocking is the leading cause to why most software does not
>> scale beyond 4 threads), and if you really must do so, and know that it is
>> OK to block the current thread of execution, you can very easily:
>>
>> val cf = new CompletableFuture<R>()
>> stage.whenComplete((r, e) -> if (e != null) cf.completeExceptionally(e)
>> else cf.complete(r))
>> cf.get()
>>
>> or, in the current API: stage.toCompletableFuture().get()
>>
>>
>>> attempt to cancel the task,
>>>
>>
>> This is also something that should be avoided, in principle because it is
>> inherently racy, and also because if -all- consumers are allowed to cancel,
>> then you cannot safely share it among several consumers anymore because any
>> one of them may cancel it for everyone else, so this leads us into
>> defensive copying, which I think we all agree is an antipattern.
>>
>>
>>> etc. In my mind, those are the only reasons you'd ever call
>>> CompletionStage.toCompletableFuture, and I think having that behavior in
>>> the CompletionStage interface would have been better.
>>>
>>
>> I hope my arguments against it changes that conclusion :)
>>
>>
>>>
>>> I think a big benefit of ListenableFuture (and now CompletableFuture) is
>>> that you can use them in both blocking and fully asynchronous idioms. So
>>> being able to do things like get the result (or block for it) are valuable
>>> to me.
>>>
>>
>> In my experience, offing both blocking and non-blocking is the worst
>> option, because the ones that don't want to play nice will do blocking
>> everywhere, and then you're back to "won't scale beyond 4 cores". (I don't
>> know how much time I have wasted trying to convert blocking code to
>> non-blocking. And very, very rarely have I ever done the opposite.)
>>
>>
>>>
>>>
>>>
>>>> I dislike that the only CompletionStage implementation in the JRE,
>>>>> CompletableFuture, is like Guava's SettableFuture -- where the result is
>>>>> set imperatively. This has its uses for sure. But an implementation whose
>>>>> result comes from executing a unit of logic, like FutureTask
>>>>> (CompletionStageTask?), is a glaring omission.
>>>>>
>>>>
>>>> This is a one-liner tho: unitCompletableFuture.theApplyAsync( unit ->
>>>> createResult, executor)
>>>>
>>>
>>>
>>> Sorry if it wasn't clear. But my nit is about API purity, not
>>> functionality. Sure, the functionality is there (and
>>> CompletableFuture.supplyAsync is even more concise that your example). But
>>> the result is an object (CompletableFuture) that conflates the two styles
>>> of use for futures -- 1) completion being implicitly tied to the execution
>>> of a task and 2) imperative completion.
>>>
>>
>> I couldn't help but notice that you're objecting against having 2 styles
>> here, but argue that CompletionStage should support both blocking and
>> non-blocking operations. Is this intentional?
>>
>> In my experience, the more behavior an interface exposes (exhibit A:
>> java.util.Collection), the harder, and more costly, it will be for
>> implementors to implement it efficiently. I think CompletionStage offers a
>> pragmatic interface for multiple Future-style APIs to conform to and
>> implement that does not tie interface to implementation (like
>> CompletableFuture would do).
>>
>> Now, my preference would be to have removed the 3 versions of every
>> method (x, xAsync, xAsync + Executor) and instead only have `x` + Executor
>> and then you can supply a CallingThreadExecutor, ForkJoinPool.commonPool()
>> or <custom> and you get exactly the same behavior without the interface
>> pollution.
>>
>> Regarding CompletableFuture, I would probably personally not use it
>> because of `cancel`, `get`, `obtrude`, `complete` etc, and thanks to
>> CompletionStage I can avoid all that :)
>>
>> In scala.concurrent, we have Promise, which is the writer-handle
>> (write-once), and Future, which is the read-handle (read-many), we don't
>> support cancellation because it is not read-only. The only officially
>> supported way of blocking on the result of a Future is to go through the
>> auxiliary "Await.result(future, timeout)"-method (which makes it simple to
>> outlaw if one wants to avoid blocking). Noteworthy is that
>> "Await.result(?)" uses `scala.concurrent.blocking` demarcation so for
>> ForkJoinPools it can play nice with ManagedBlocker transparent to the user
>> code.
>>
>> So in the end, we encourage async transformation, clear separation
>> between readers and writer and if blocking is -mandated- it can play nice
>> with managed blocking. It has turned out very well, I'd say!
>>
>> If I have one regret, it is that I introduced "Await" at all. Blocking is
>> just bad.
>>
>>
>>>
>>>
>>>
>>>> On that note, an ExecutorService that returns CompletionStages (or lack
>>>>> thereof) is another obvious gap in the APIs in Java8 -- something like
>>>>> Guava's ListeningExecutorService. CompletableFuture's runAsync and
>>>>> supplyAsync methods do not suffice for a few reasons:
>>>>>
>>>>>    1. They don't allow interruption of the task when the future is
>>>>>    cancelled.
>>>>>    2. They can't throw checked exceptions. A callAsync method (taking
>>>>>    a Callable) would have been nice to avoid the try/catch boiler-plate in the
>>>>>    Supplier -- especially since CompletableFuture.completeExceptionally
>>>>>    readily supports checked exceptions.
>>>>>    3. They don't support scheduling the task for deferred execution
>>>>>    (like in a ScheduledExecutorService).
>>>>>
>>>>> Finally, I'm not fond of the presence of the obtrude* methods in
>>>>> CompletableFuture's public API. I feel like they should be protected (or
>>>>> maybe just not exist). Mutating a future's value, after it has completed
>>>>> and possibly already been inspected, seems like a potential source of
>>>>> hard-to-track-down bugs.
>>>>>
>>>>
>>>> TBH I think ExecutorService needs retirement, it mixes too many
>>>> concerns already. It would be extremely simple to introduce a new branch in
>>>> the Executor hierarchy and build on top of that instead.
>>>>
>>>>
>>>>>
>>>>>
>>>>> ----
>>>>> *Josh Humphries*
>>>>> Manager, Shared Systems  |  Platform Engineering
>>>>> Atlanta, GA  |  678-400-4867
>>>>> *Square* (www.squareup.com)
>>>>>
>>>>> On Thu, Nov 27, 2014 at 6:43 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>
>>>>>> If you use ListenableFuture in Java8, it does support lambda
>>>>>> (they're independent). What I mean is ListenableFuture is pre-Java8,
>>>>>> so it's probably not designed to take advantage of lambda. To me,
>>>>>> async task chaining by Guava "Futures.transform" is harder to read than
>>>>>> CompletableFuture (with or without lambda).
>>>>>>
>>>>>> Josh, when you plan to jump to Java8, why do you use adapters from ListenableFuture
>>>>>> -> CompletionStage, why not change all ListenableFutures to CompletableFutures
>>>>>> (is it much harder than using adapters)? Adapter is just a wrapper
>>>>>> and makes the code more verbose.
>>>>>>
>>>>>> Also, how do you compare ListenableFuture and CompletableFuture? I'm
>>>>>> still trying to know the differences/advantages between them.
>>>>>>
>>>>>> On Thu, Nov 27, 2014 at 8:54 AM, Josh Humphries <jh at squareup.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Why do you say ListenableFuture doesn't support lambdas? In Java8,
>>>>>>> any single-abstract-method interface can be the target of a lambda
>>>>>>> expressions, so Guava's Function is just as suited as the Function
>>>>>>> interface added in Java8.
>>>>>>>
>>>>>>> Perhaps you're only looking at the ListenableFuture interface --
>>>>>>> which just has addListener(Runnable,Executor). Take a look at Futures,
>>>>>>> which is full of static helper methods. They include additional methods for
>>>>>>> working with ListenableFutures and callbacks, including a way to
>>>>>>> "transform" a ListenableFuture with a Function. I'm pretty sure this was
>>>>>>> done so the ListenableFuture interface itself could be narrow without
>>>>>>> requiring implementations provide the full breadth (since it's pre-Java8,
>>>>>>> there is no luxury of default and static methods on the interface).
>>>>>>>
>>>>>>> At Square, we use ListenableFutures heavily in framework code (to
>>>>>>> support pre-Java8 apps). But we also have a lot of stuff moving to Java8.
>>>>>>> So I wrote adapters from ListenableFuture -> CompletionStage and vice
>>>>>>> versa. All of the building blocks needed are present in ListenableFuture.
>>>>>>> With these adapters, CompletableFutures (more importantly, the
>>>>>>> CompletionStage API) can still be used even though framework APIs expect
>>>>>>> and return ListenableFutures.
>>>>>>>
>>>>>>> But we actually have very little use of CompletableFutures. A lot of
>>>>>>> projects prefer serial code (with blocking operations) because the code is
>>>>>>> generally more readable (thus easier to maintain) and the performance is
>>>>>>> adequate. Code that is async and parallel is often encapsulated behind a
>>>>>>> library, which generally uses Java7 (and ListenableFutures) to support the
>>>>>>> various projects that haven't yet made the jump to Java8.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ----
>>>>>>> *Josh Humphries*
>>>>>>> Manager, Shared Systems  |  Platform Engineering
>>>>>>> Atlanta, GA  |  678-400-4867
>>>>>>> *Square* (www.squareup.com)
>>>>>>>
>>>>>>> On Thu, Nov 27, 2014 at 2:24 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>>>
>>>>>>>> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys
>>>>>>>> know what's the major differences among RxJava, ListenableFuture and
>>>>>>>> CompletableFuture. What's the advantage of each?
>>>>>>>>
>>>>>>>> It seems to me all three support callback listeners and async task
>>>>>>>> chaining. But ListenableFuture doesn't support lambda while RxJava support
>>>>>>>> more operations for Observable. How to choose which construct to use when
>>>>>>>> doing asynchronous programming?
>>>>>>>>
>>>>>>>> Thanks,
>>>>>>>> Yu
>>>>>>>>
>>>>>>>> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
>>>>>>>> radhakrishnan.mohan at gmail.com> wrote:
>>>>>>>>
>>>>>>>>> Don't you think Netflix should be using it ?
>>>>>>>>> http://netflix.github.io/#repo
>>>>>>>>>
>>>>>>>>> Thanks,
>>>>>>>>> Mohan
>>>>>>>>>
>>>>>>>>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com>
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>>> I guess David does mean RxJava. There're some projects use RxJava.
>>>>>>>>>> David, in which project do you use it? May I take a look if it's
>>>>>>>>>> open-source?
>>>>>>>>>>
>>>>>>>>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com>
>>>>>>>>>> wrote:
>>>>>>>>>>
>>>>>>>>>>> OK, thanks for all your detailed analysis. I guess the slow
>>>>>>>>>>> adaption of a new version is why I can't find the use of it. I myself is
>>>>>>>>>>> using Java 6? Other similar libraries you mentioned seem to be interesting.
>>>>>>>>>>>
>>>>>>>>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <
>>>>>>>>>>> akarnokd at gmail.com> wrote:
>>>>>>>>>>>
>>>>>>>>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student
>>>>>>>>>>>> myself, I see a few shortcomings:- Slow adaptation of Java 8,- the feature
>>>>>>>>>>>> lag in a well known mobile platform and- the alternatives available as
>>>>>>>>>>>> library for some time.In addition, it may compose better than Future, but
>>>>>>>>>>>> IMHO, it doesn't go far enough. Therefore, I use a FOSS library (with
>>>>>>>>>>>> increasing popularity) that allows one to compose asynchronous data streams
>>>>>>>>>>>> with much less worry on concurrency and continuation than CF. I use it in
>>>>>>>>>>>> 50k loc projects, but I read/heard a well known company is switching almost
>>>>>>>>>>>> all of its software stack to the reactive/asynchronous programming idiom
>>>>>>>>>>>> this library enables. Excellent material is available (including videos and
>>>>>>>>>>>> conference talk) on this subject (and the library).
>>>>>>>>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>>>>>>>>
>>>>>>>>>>>>> Hi all,
>>>>>>>>>>>>>
>>>>>>>>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing
>>>>>>>>>>>>> research on the uses of Java concurrent/asynchronous tasks. A new construct
>>>>>>>>>>>>> introduced in Java 8, CompletableFuture, seems to be a very nice and easy
>>>>>>>>>>>>> to use concurrent task.
>>>>>>>>>>>>>
>>>>>>>>>>>>> I'm looking for projects that use CompletableFuture. But since
>>>>>>>>>>>>> it's very new, I didn't find any real-world projects use it (only found
>>>>>>>>>>>>> many toy examples/demos).
>>>>>>>>>>>>>
>>>>>>>>>>>>> Does anyone know any projects that use it?
>>>>>>>>>>>>>
>>>>>>>>>>>>> Thanks,
>>>>>>>>>>>>> Yu Lin
>>>>>>>>>>>>>
>>>>>>>>>>>>> _______________________________________________
>>>>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>>
>>>> --
>>>> Cheers,
>>>> ?
>>>>
>>>
>>>
>>
>>
>> --
>> Cheers,
>> ?
>>
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/f32cf7ec/attachment-0001.html>

From viktor.klang at gmail.com  Mon Dec  1 05:07:35 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 1 Dec 2014 11:07:35 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <547B3ADD.4070700@cs.oswego.edu>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
Message-ID: <CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>

On Sun, Nov 30, 2014 at 4:42 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 11/27/2014 10:45 PM, Josh Humphries wrote:
>
> A few notes not covered in other responses...
>
>
>> Below are some of my nits about the Java8 APIs:
>>
>> I disagree with how the CompletionStage interface is so tightly coupled to
>> the CompletableFuture implementation. Why does CompletionStage have a
>> toCompletableFuture() method? It feels like the interface should instead
>> have
>>  just extended Future.
>>
>
> Viktor recapped the good arguments for not extending Future. But
> we still needed a standard means for interoperable implementations to
> extract values. Method toCompletableFuture() is the simplest way to get
> this effect by anyCompletableFuture developer (usually a one-liner),
> and is marked as optional in case they don't care about interoperability.


I agree from a discoverability-perspective, even if I'd rather had seen a
CompletableFuture.fromStage(stage) method as to keep the generic part
oblivious of the implementation.


>
>
>
>> I dislike that the only CompletionStage implementation in the JRE,
>> CompletableFuture, is like Guava's SettableFuture -- where the result is
>> set
>>  imperatively. This has its uses for sure. But an implementation whose
>> result
>>  comes from executing a unit of logic, like FutureTask
>> (CompletionStageTask?), is a glaring omission.
>>
>
> The goal was to provide a robust, efficient base implementation supporting
> all reasonable usage policies. We expected more cases of delegation
> for frameworks with restricted policies (similarly for obtrude*).
> This hasn't happened much yet.
>

You mean delegation as in wrapping?


>
>
>  1. They don't allow interruption of the task when the future is cancelled.
>>
>
>  3. They don't support scheduling the task for deferred execution (like in
>> a
>> ScheduledExecutorService).
>>
>
> In most cases, this is the same issue: You'd like to
> somehow remove/suppress a cancelled timeout or timer-generated action
> upon cancellation, mainly for the sake of resource-control.
> Most other cases reduce to the mixed sync/async usage issues
> mentioned in other posts, that we cannot do much about in j.u.c itself.
>
> In retrospect, maybe we should have added a few static methods and
> a default internal static ScheduledExecutor to support these usages,
> since everyone seems to need them. One reason we didn't is that
> these further invite defining methods producing streams of periodic
> CFs, which leads to further APIs bridging CFs with streams/observables,
> which we triaged out of jdk8 (see my other posts on this.)
> Still, something along these lines (perhaps just a class with
> static CF utility methods) should be added.


Perhaps this is the perfect time for a j.u.c2? With all we've learned the
past 10-15 years :-)


>
>
> On 11/28/2014 11:57 AM, ?iktor ?lang wrote:
>
>  Now, my preference would be to have removed the 3 versions of every method
>> (x, xAsync, xAsync + Executor) and instead only have `x` + Executor and
>> then
>> you can supply a CallingThreadExecutor, ForkJoinPool.commonPool() or
>> <custom>
>> and you get exactly the same behavior without the interface pollution.
>>
>
> If the API were expressed in a language with implicits, this might have
> been more tempting. (Well, except that everyone would then instead
> be complaining about implicits :-)


Lol! Well, that is certainly true, however, we seem to fare pretty well
with a Java API that takes an Executor  (ExecutionContext) for all
"deferred" operations. There's also the choise of having a bind-method so
you could treat it as a Stream of sorts:

future.bind(executor).operation1(?).operation2(?).terminalOperation(/* no
evaluation happens until here*/)



> Also, it there were a Unit type,
> the API would shrink by almost a factor of three
> (Runnable as Unit->Unit etc).
>

Yes, sadly Void has no value :) (the cause of much irregularities since day
1)


>
> But the variants are regular enough that users don't seem to
> complain about any of them in particular, only of the resulting
> too-bigness feeling.
> On the other hand, it does look overly imposing to developers of
> layered/delegated classes. Adding default implementations using
> lambda-based function adaptors might be a good idea, but it is
> probably too late for that in CompletionStage itself. We could
> add class AbstractCompletionStage though.


AbstractCompletionStage sounds like a great idea to accelerate adoption of
CompletionStage,
we should explore that option.


>
>
>
> -Doug
>
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/cf6f2238/attachment.html>

From radhakrishnan.mohan at gmail.com  Mon Dec  1 06:13:46 2014
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Mon, 1 Dec 2014 16:43:46 +0530
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
Message-ID: <CAOoXFP9GdvesKPYOqOhxk+b8O7meeTdtoccJez1YNa465R+CCQ@mail.gmail.com>

Hi viktor,
               I read what Gunther has published in his 'Guerilla' books.
Did you work with any USL calculations relating cores and the j.u.c.
threads ?

Thanks,
Mohan

On Mon, Dec 1, 2014 at 3:37 PM, ?iktor ?lang <viktor.klang at gmail.com> wrote:

>
>
> On Sun, Nov 30, 2014 at 4:42 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 11/27/2014 10:45 PM, Josh Humphries wrote:
>>
>> A few notes not covered in other responses...
>>
>>
>>> Below are some of my nits about the Java8 APIs:
>>>
>>> I disagree with how the CompletionStage interface is so tightly coupled
>>> to
>>> the CompletableFuture implementation. Why does CompletionStage have a
>>> toCompletableFuture() method? It feels like the interface should instead
>>> have
>>>  just extended Future.
>>>
>>
>> Viktor recapped the good arguments for not extending Future. But
>> we still needed a standard means for interoperable implementations to
>> extract values. Method toCompletableFuture() is the simplest way to get
>> this effect by anyCompletableFuture developer (usually a one-liner),
>> and is marked as optional in case they don't care about interoperability.
>
>
> I agree from a discoverability-perspective, even if I'd rather had seen a
> CompletableFuture.fromStage(stage) method as to keep the generic part
> oblivious of the implementation.
>
>
>>
>>
>>
>>> I dislike that the only CompletionStage implementation in the JRE,
>>> CompletableFuture, is like Guava's SettableFuture -- where the result is
>>> set
>>>  imperatively. This has its uses for sure. But an implementation whose
>>> result
>>>  comes from executing a unit of logic, like FutureTask
>>> (CompletionStageTask?), is a glaring omission.
>>>
>>
>> The goal was to provide a robust, efficient base implementation supporting
>> all reasonable usage policies. We expected more cases of delegation
>> for frameworks with restricted policies (similarly for obtrude*).
>> This hasn't happened much yet.
>>
>
> You mean delegation as in wrapping?
>
>
>>
>>
>>  1. They don't allow interruption of the task when the future is
>>> cancelled.
>>>
>>
>>  3. They don't support scheduling the task for deferred execution (like
>>> in a
>>> ScheduledExecutorService).
>>>
>>
>> In most cases, this is the same issue: You'd like to
>> somehow remove/suppress a cancelled timeout or timer-generated action
>> upon cancellation, mainly for the sake of resource-control.
>> Most other cases reduce to the mixed sync/async usage issues
>> mentioned in other posts, that we cannot do much about in j.u.c itself.
>>
>> In retrospect, maybe we should have added a few static methods and
>> a default internal static ScheduledExecutor to support these usages,
>> since everyone seems to need them. One reason we didn't is that
>> these further invite defining methods producing streams of periodic
>> CFs, which leads to further APIs bridging CFs with streams/observables,
>> which we triaged out of jdk8 (see my other posts on this.)
>> Still, something along these lines (perhaps just a class with
>> static CF utility methods) should be added.
>
>
> Perhaps this is the perfect time for a j.u.c2? With all we've learned the
> past 10-15 years :-)
>
>
>>
>>
>> On 11/28/2014 11:57 AM, ?iktor ?lang wrote:
>>
>>  Now, my preference would be to have removed the 3 versions of every
>>> method
>>> (x, xAsync, xAsync + Executor) and instead only have `x` + Executor and
>>> then
>>> you can supply a CallingThreadExecutor, ForkJoinPool.commonPool() or
>>> <custom>
>>> and you get exactly the same behavior without the interface pollution.
>>>
>>
>> If the API were expressed in a language with implicits, this might have
>> been more tempting. (Well, except that everyone would then instead
>> be complaining about implicits :-)
>
>
> Lol! Well, that is certainly true, however, we seem to fare pretty well
> with a Java API that takes an Executor  (ExecutionContext) for all
> "deferred" operations. There's also the choise of having a bind-method so
> you could treat it as a Stream of sorts:
>
> future.bind(executor).operation1(?).operation2(?).terminalOperation(/* no
> evaluation happens until here*/)
>
>
>
>> Also, it there were a Unit type,
>> the API would shrink by almost a factor of three
>> (Runnable as Unit->Unit etc).
>>
>
> Yes, sadly Void has no value :) (the cause of much irregularities since
> day 1)
>
>
>>
>> But the variants are regular enough that users don't seem to
>> complain about any of them in particular, only of the resulting
>> too-bigness feeling.
>> On the other hand, it does look overly imposing to developers of
>> layered/delegated classes. Adding default implementations using
>> lambda-based function adaptors might be a good idea, but it is
>> probably too late for that in CompletionStage itself. We could
>> add class AbstractCompletionStage though.
>
>
> AbstractCompletionStage sounds like a great idea to accelerate adoption of
> CompletionStage,
> we should explore that option.
>
>
>>
>>
>>
>> -Doug
>>
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Cheers,
> ?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/b1d87d71/attachment-0001.html>

From tim at peierls.net  Mon Dec  1 08:54:28 2014
From: tim at peierls.net (Tim Peierls)
Date: Mon, 1 Dec 2014 08:54:28 -0500
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <547BBF1D.1020908@bbs.darktech.org>
References: <1417232551381-11504.post@n7.nabble.com>
	<547B1EEC.3060306@cs.oswego.edu>
	<CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>
	<547B3FC1.5060801@cs.oswego.edu>
	<CA+F8eeQHddfkfgsnNrfFpDCma7nPs3ad6_EtAzwK0hNtmbkzbg@mail.gmail.com>
	<547BBF1D.1020908@bbs.darktech.org>
Message-ID: <CA+F8eeQo5rrWf-Om6AtroOcvyjD7aznxBHui7ycOmtuLdRie-g@mail.gmail.com>

On Sun, Nov 30, 2014 at 7:59 PM, cowwoc <cowwoc at bbs.darktech.org> wrote:

> The CompletableFuture documentation is extremely difficult to comprehend
> (much more so than other j.u.c classes). Keep in mind that I am coming at
> this with full comprehension of Javascript Promises (having used them daily
> for over 6 months). I can't imagine people with no prior exposure to
> Promises managing to get this working.
>

I actually think the CompletionStage/CompletableFuture docs are pretty
good, barring the few glitches mentioned in this thread. They provide
neither a from-scratch tutorial nor a how-to for Promises users, nor should
they.



> On a related note, I find Generics compiler error messages to be extremely
> cryptic ...
>

You're not alone, but this discussion belongs elsewhere.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/867f697a/attachment.html>

From dl at cs.oswego.edu  Mon Dec  1 08:58:35 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 01 Dec 2014 08:58:35 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
Message-ID: <547C740B.1090903@cs.oswego.edu>

On 12/01/2014 05:07 AM, ?iktor ?lang wrote:

>     Viktor recapped the good arguments for not extending Future. But
>     we still needed a standard means for interoperable implementations to
>     extract values. Method toCompletableFuture() is the simplest way to get
>     this effect by anyCompletableFuture developer (usually a one-liner),
>     and is marked as optional in case they don't care about interoperability.
>
>
> I agree from a discoverability-perspective, even if I'd rather had seen a
> CompletableFuture.fromStage(stage) method as to keep the generic part oblivious
> of the implementation.

Right, except that this method must be virtual and overridden
by a CompletionStage implementation. If interfaces could have
"protected" methods, it might have been a slightly better
indication that this method is solely for the sake of
interoperability and shouldn't ordinarily be called by
framework users. This is a common API design problem:
Sometimes saying "shouldn't call" is the best you can do
when you ideally want to restrict calls to special contexts.

This is the model we had in mind with CompletionStage:
It doesn't include any way of getting or setting or waiting for
results, so is a preferable return type for most
user-level methods in layered frameworks. But it still
has the toCompletableFuture escape hatch that must
be discouraged by policy vs type-checkers. Doing
more than this requires wrapping/delegation to
distinguish internal vs external usages in a statically
enforceable way. If people are going to do this frequently,
we ought to make it easier, by creating AbstractCompletionStage.


>
> Perhaps this is the perfect time for a j.u.c2? With all we've learned the past
> 10-15 years :-)

This is almost never possible in core language/libraries.
See Brian Goetz's recent talk:
   https://www.youtube.com/watch?v=2y5Pv4yN0b0&app=desktop

But we can/do add new better stuff that we encourage people to use
instead of old stuff when it becomes less useful as the world changes.

-Doug




From Sebastian.Millies at softwareag.com  Mon Dec  1 10:38:55 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Mon, 1 Dec 2014 15:38:55 +0000
Subject: [concurrency-interest] CompletableFuture in Java 8
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD8891D7@HQMBX5.eur.ad.sag>



> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-
> bounces at cs.oswego.edu] On Behalf Of Doug Lea
> Sent: Monday, December 01, 2014 2:59 PM
> To: concurrency-interest
> Subject: Re: [concurrency-interest] CompletableFuture in Java 8
>
[snip]

> This is the model we had in mind with CompletionStage:
> It doesn't include any way of getting or setting or waiting for results, so is a
> preferable return type for most user-level methods in layered frameworks.

[snip]

Perhaps this is somewhat at a tangent, but why are allOf(), anyOf() not in CompletionStage ?
They don't wait, do they?

Example: Tomasz Nurkiewicz has proposed the following
code for aggregating the results of multiple CompletableFutures in a List:

  /**
   * We'd like to utilize existing CompletableFuture.allOf(). Unfortunately, it has two minor drawbacks - it takes
   * vararg instead of Collection and doesn't return a future of aggregated results but Void instead. By aggregated
   * results I mean: if we provide List<CompletableFuture<Double>>, it should return CompletableFuture<List<Double>>,
   * not CompletableFuture<Void>! Luckily it's easy to fix with a bit of glue code.
   */
  public static <T> CompletableFuture<List<T>> sequence(List<CompletableFuture<T>> futures) {
    // the trick is to use existing allOf() but when allDoneFuture completes (which means all/ underlying futures are done),
    // simply iterate overall futures and join() (blocking wait) on each. However this call is guaranteed not to block because
    // by now all futures completed!
    CompletableFuture<Void> allDoneFuture = CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures
        .size()]));
    return allDoneFuture.thenApply(v -> futures.stream().map(CompletableFuture::join).collect(toList()));
  }

Would it not be nicer if this method could be called with List<CompletionStage> argument (of course also returning a
CompletionStage)? I find myself inserting map(s->s.toCompletionStage()) in my streams in order to call this
method, and then continue working with CompletionStage methods (thenApply() etc.) on the "aggregated" result.

I'd like to avoid CompletableFuture showing up in the types of the intermediate variables. Is there a way?

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com



From viktor.klang at gmail.com  Mon Dec  1 10:58:11 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 1 Dec 2014 16:58:11 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <547C740B.1090903@cs.oswego.edu>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
Message-ID: <CANPzfU95EhS1gmB7eVTckxyWRa_=iAotTmN8StjsWn6s9DaVMA@mail.gmail.com>

On Mon, Dec 1, 2014 at 2:58 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/01/2014 05:07 AM, ?iktor ?lang wrote:
>
>      Viktor recapped the good arguments for not extending Future. But
>>     we still needed a standard means for interoperable implementations to
>>     extract values. Method toCompletableFuture() is the simplest way to
>> get
>>     this effect by anyCompletableFuture developer (usually a one-liner),
>>     and is marked as optional in case they don't care about
>> interoperability.
>>
>>
>> I agree from a discoverability-perspective, even if I'd rather had seen a
>> CompletableFuture.fromStage(stage) method as to keep the generic part
>> oblivious
>> of the implementation.
>>
>
> Right, except that this method must be virtual and overridden
> by a CompletionStage implementation.


I guess it depends whether one wants that feature overridable (one can go
from CompletionStage to CompletableFuture with the combinators)


> If interfaces could have
> "protected" methods, it might have been a slightly better
> indication that this method is solely for the sake of
> interoperability and shouldn't ordinarily be called by
> framework users. This is a common API design problem:
> Sometimes saying "shouldn't call" is the best you can do
> when you ideally want to restrict calls to special contexts.
>

Agreed.


>
> This is the model we had in mind with CompletionStage:
> It doesn't include any way of getting or setting or waiting for
> results, so is a preferable return type for most
> user-level methods in layered frameworks. But it still
> has the toCompletableFuture escape hatch that must
> be discouraged by policy vs type-checkers. Doing
> more than this requires wrapping/delegation to
> distinguish internal vs external usages in a statically
> enforceable way. If people are going to do this frequently,
> we ought to make it easier, by creating AbstractCompletionStage.
>
>
>
>> Perhaps this is the perfect time for a j.u.c2? With all we've learned the
>> past
>> 10-15 years :-)
>>
>
> This is almost never possible in core language/libraries.
> See Brian Goetz's recent talk:
>   https://www.youtube.com/watch?v=2y5Pv4yN0b0&app=desktop
>
> But we can/do add new better stuff that we encourage people to use
> instead of old stuff when it becomes less useful as the world changes.


Hmmm, if only there was some kind of successor-like language to Java... :-)


>
>
> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/a82a35e2/attachment.html>

From viktor.klang at gmail.com  Mon Dec  1 11:16:08 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 1 Dec 2014 17:16:08 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD8891D7@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD8891D7@HQMBX5.eur.ad.sag>
Message-ID: <CANPzfU_DTdOVjXh+PO9G3xvMCfASbLRQkD0V6N7EHs49_goBBQ@mail.gmail.com>

Hi Sebastian!

It is indeed possible to implement a whole host of interesting utility
methods both asynchronously and non-blocking (could be
CompletionStage-based):
http://www.scala-lang.org/api/current/#scala.concurrent.Future$

On Mon, Dec 1, 2014 at 4:38 PM, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:

>
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-
> > bounces at cs.oswego.edu] On Behalf Of Doug Lea
> > Sent: Monday, December 01, 2014 2:59 PM
> > To: concurrency-interest
> > Subject: Re: [concurrency-interest] CompletableFuture in Java 8
> >
> [snip]
>
> > This is the model we had in mind with CompletionStage:
> > It doesn't include any way of getting or setting or waiting for results,
> so is a
> > preferable return type for most user-level methods in layered frameworks.
>
> [snip]
>
> Perhaps this is somewhat at a tangent, but why are allOf(), anyOf() not in
> CompletionStage ?
> They don't wait, do they?
>
> Example: Tomasz Nurkiewicz has proposed the following
> code for aggregating the results of multiple CompletableFutures in a List:
>
>   /**
>    * We'd like to utilize existing CompletableFuture.allOf().
> Unfortunately, it has two minor drawbacks - it takes
>    * vararg instead of Collection and doesn't return a future of
> aggregated results but Void instead. By aggregated
>    * results I mean: if we provide List<CompletableFuture<Double>>, it
> should return CompletableFuture<List<Double>>,
>    * not CompletableFuture<Void>! Luckily it's easy to fix with a bit of
> glue code.
>    */
>   public static <T> CompletableFuture<List<T>>
> sequence(List<CompletableFuture<T>> futures) {
>     // the trick is to use existing allOf() but when allDoneFuture
> completes (which means all/ underlying futures are done),
>     // simply iterate overall futures and join() (blocking wait) on each.
> However this call is guaranteed not to block because
>     // by now all futures completed!
>     CompletableFuture<Void> allDoneFuture =
> CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures
>         .size()]));
>     return allDoneFuture.thenApply(v ->
> futures.stream().map(CompletableFuture::join).collect(toList()));
>   }
>
> Would it not be nicer if this method could be called with
> List<CompletionStage> argument (of course also returning a
> CompletionStage)? I find myself inserting map(s->s.toCompletionStage()) in
> my streams in order to call this
> method, and then continue working with CompletionStage methods
> (thenApply() etc.) on the "aggregated" result.
>
> I'd like to avoid CompletableFuture showing up in the types of the
> intermediate variables. Is there a way?
>
> Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt,
> Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 -
> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - http://www.softwareag.com
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/a4f8a8bf/attachment-0001.html>

From cowwoc at bbs.darktech.org  Mon Dec  1 12:18:27 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Mon, 1 Dec 2014 10:18:27 -0700 (MST)
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <CA+F8eeQo5rrWf-Om6AtroOcvyjD7aznxBHui7ycOmtuLdRie-g@mail.gmail.com>
References: <1417232551381-11504.post@n7.nabble.com>
	<547B1EEC.3060306@cs.oswego.edu>
	<CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>
	<547B3FC1.5060801@cs.oswego.edu>
	<CA+F8eeQHddfkfgsnNrfFpDCma7nPs3ad6_EtAzwK0hNtmbkzbg@mail.gmail.com>
	<547BBF1D.1020908@bbs.darktech.org>
	<CA+F8eeQo5rrWf-Om6AtroOcvyjD7aznxBHui7ycOmtuLdRie-g@mail.gmail.com>
Message-ID: <547CA4A0.5060506@bbs.darktech.org>

On 01/12/2014 8:59 AM, tpeierls [via JSR166 Concurrency] wrote:
>
>     On a related note, I find Generics compiler error messages to be
>     extremely cryptic ...
>
>
> You're not alone, but this discussion belongs elsewhere.
>
> --tim
>

Any idea where "elsewhere" is?

Gili




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Please-clarify-Javadoc-of-CompletableFuture-thenCompose-tp11504p11530.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/adf2f2a4/attachment.html>

From martinrb at google.com  Mon Dec  1 12:27:46 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 09:27:46 -0800
Subject: [concurrency-interest] Packing 2 data points into 1 field in
	ThreadPoolExecutor
In-Reply-To: <0F081861-ACBA-484E-9FA7-784D2EE4A023@icloud.com>
References: <F20EAFC5-2BD8-4787-901B-413B9D4A84E2@icloud.com>
	<0F081861-ACBA-484E-9FA7-784D2EE4A023@icloud.com>
Message-ID: <CA+kOe08odwpUrUeeniY+j8SyraATDLbBSHWF+S2=1ECoT_R1wA@mail.gmail.com>

The only way to use atomic compare and set is to pack all your state
into a single primitive unit.  The largest we have is "long".  So we
regularly pack what the C folks would call bitfields into longs.

On Sat, Nov 29, 2014 at 8:13 AM, Alex Yursha <alexyursha at gmail.com> wrote:
> Hi all,
>
> According to javadoc current implementation of ThreadPoolExecutor packs two conceptual fields ?workerCount? and ?runState? into one actual field ?ctl? of type AtomicInteger.
>
> Could you please explain are there any performance or other benefits for this? It seems to complicate the class design and I can?t find the positive side of this.
>
> Thanks,
> Alex
>


From martinrb at google.com  Mon Dec  1 13:24:12 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 10:24:12 -0800
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <CA+F8eeQHddfkfgsnNrfFpDCma7nPs3ad6_EtAzwK0hNtmbkzbg@mail.gmail.com>
References: <1417232551381-11504.post@n7.nabble.com>
	<547B1EEC.3060306@cs.oswego.edu>
	<CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>
	<547B3FC1.5060801@cs.oswego.edu>
	<CA+F8eeQHddfkfgsnNrfFpDCma7nPs3ad6_EtAzwK0hNtmbkzbg@mail.gmail.com>
Message-ID: <CA+kOe09Rr8cZqkPr7tv+03ENJ7iRWxSx=zxnKpx22gw990N7Rw@mail.gmail.com>

On Sun, Nov 30, 2014 at 8:59 AM, Tim Peierls <tim at peierls.net> wrote:
> On Sun, Nov 30, 2014 at 11:03 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> On 11/30/2014 09:30 AM, Tim Peierls wrote:
>>>
>>>
>>> Confusing, because of the inherited CompletionStage doc: The type of
>>> "this" and
>>> the return type promised by the doc are both referred to as "stage" when
>>> in fact
>>> both are specifically CompletableFuture. That's imprecise, not wrong, but
>>> it
>>> makes the reader stumble right off.
>>
>>
>> OK, but we do this uniformly in java.util/j.u.c.
>>
>> For example, see the many
>> mentions of "this collection" in Collection implementations.
>
>
>
> Well... Two seconds of looking and I found an inconsistency: type parameter
> E for Set is "type of elements maintained by this set" and for Queue is
> "type of elements held in this collection" (compare E in List, SortedSet,
> Deque, ...). Further brief manual scan picked up no other instances of "this
> collection" in the docs for commonly used Collection subtypes in j.u.
>
> And just because we do it, doesn't mean it's a good thing. In most cases we
> are getting away with it, but read on for my reasoning about when it becomes
> inappropriate.

I actually made an effort many years ago to make the javadoc for
collection classes better by changing "this collection" to "this set",
etc.  The most readable is for both Set and HashSet to use "this set",
I think.  For CompletableFuture, it is better to change occurrences of
"this stage".  Readability trumps maintainability for core
libraries.... except that here the amount of copy-paste that would be
required is unprecedented ... so I'm hesitating ....

But while we're thinking about that, do we have agreement that the
following is progress?

Index: AbstractQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/AbstractQueue.java,v
retrieving revision 1.38
diff -u -U 0 -r1.38 AbstractQueue.java
--- AbstractQueue.java 16 Jan 2013 01:59:47 -0000 1.38
+++ AbstractQueue.java 1 Dec 2014 18:21:02 -0000
@@ -33 +33 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: ArrayDeque.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/ArrayDeque.java,v
retrieving revision 1.59
diff -u -U 0 -r1.59 ArrayDeque.java
--- ArrayDeque.java 23 Nov 2014 18:06:50 -0000 1.59
+++ ArrayDeque.java 1 Dec 2014 18:21:02 -0000
@@ -56 +56 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this deque
Index: Deque.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/Deque.java,v
retrieving revision 1.29
diff -u -U 0 -r1.29 Deque.java
--- Deque.java 23 Nov 2014 18:46:47 -0000 1.29
+++ Deque.java 1 Dec 2014 18:21:02 -0000
@@ -162 +162 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this deque
Index: PriorityQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/PriorityQueue.java,v
retrieving revision 1.100
diff -u -U 0 -r1.100 PriorityQueue.java
--- PriorityQueue.java 29 Aug 2014 21:42:37 -0000 1.100
+++ PriorityQueue.java 1 Dec 2014 18:21:03 -0000
@@ -80 +80 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: Queue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/Queue.java,v
retrieving revision 1.41
diff -u -U 0 -r1.41 Queue.java
--- Queue.java 12 Nov 2013 23:23:05 -0000 1.41
+++ Queue.java 1 Dec 2014 18:21:03 -0000
@@ -113 +113 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: concurrent/ArrayBlockingQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ArrayBlockingQueue.java,v
retrieving revision 1.108
diff -u -U 0 -r1.108 ArrayBlockingQueue.java
--- concurrent/ArrayBlockingQueue.java 11 Apr 2014 21:15:44 -0000 1.108
+++ concurrent/ArrayBlockingQueue.java 1 Dec 2014 18:21:03 -0000
@@ -53 +53 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: concurrent/BlockingDeque.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/BlockingDeque.java,v
retrieving revision 1.27
diff -u -U 0 -r1.27 BlockingDeque.java
--- concurrent/BlockingDeque.java 12 Nov 2013 23:23:05 -0000 1.27
+++ concurrent/BlockingDeque.java 1 Dec 2014 18:21:03 -0000
@@ -169 +169 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this deque
Index: concurrent/BlockingQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/BlockingQueue.java,v
retrieving revision 1.53
diff -u -U 0 -r1.53 BlockingQueue.java
--- concurrent/BlockingQueue.java 12 Nov 2013 23:23:05 -0000 1.53
+++ concurrent/BlockingQueue.java 1 Dec 2014 18:21:03 -0000
@@ -149 +149 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: concurrent/ConcurrentLinkedDeque.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedDeque.java,v
retrieving revision 1.54
diff -u -U 0 -r1.54 ConcurrentLinkedDeque.java
--- concurrent/ConcurrentLinkedDeque.java 23 Nov 2014 21:39:30 -0000 1.54
+++ concurrent/ConcurrentLinkedDeque.java 1 Dec 2014 18:21:04 -0000
@@ -63 +63 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this deque
Index: concurrent/ConcurrentLinkedQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
retrieving revision 1.104
diff -u -U 0 -r1.104 ConcurrentLinkedQueue.java
--- concurrent/ConcurrentLinkedQueue.java 23 Nov 2014 18:20:26 -0000 1.104
+++ concurrent/ConcurrentLinkedQueue.java 1 Dec 2014 18:21:04 -0000
@@ -77 +77 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: concurrent/CopyOnWriteArrayList.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CopyOnWriteArrayList.java,v
retrieving revision 1.117
diff -u -U 0 -r1.117 CopyOnWriteArrayList.java
--- concurrent/CopyOnWriteArrayList.java 23 Nov 2014 21:40:08 -0000 1.117
+++ concurrent/CopyOnWriteArrayList.java 1 Dec 2014 18:21:05 -0000
@@ -71 +71 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this list
Index: concurrent/CopyOnWriteArraySet.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CopyOnWriteArraySet.java,v
retrieving revision 1.59
diff -u -U 0 -r1.59 CopyOnWriteArraySet.java
--- concurrent/CopyOnWriteArraySet.java 23 Nov 2014 21:40:08 -0000 1.59
+++ concurrent/CopyOnWriteArraySet.java 1 Dec 2014 18:21:05 -0000
@@ -66 +66 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this set
Index: concurrent/DelayQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/DelayQueue.java,v
retrieving revision 1.66
diff -u -U 0 -r1.66 DelayQueue.java
--- concurrent/DelayQueue.java 11 Apr 2014 21:15:44 -0000 1.66
+++ concurrent/DelayQueue.java 1 Dec 2014 18:21:05 -0000
@@ -39 +39 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: concurrent/LinkedBlockingDeque.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/LinkedBlockingDeque.java,v
retrieving revision 1.51
diff -u -U 0 -r1.51 LinkedBlockingDeque.java
--- concurrent/LinkedBlockingDeque.java 8 Aug 2013 20:12:10 -0000 1.51
+++ concurrent/LinkedBlockingDeque.java 1 Dec 2014 18:21:05 -0000
@@ -48 +48 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this deque
Index: concurrent/LinkedBlockingQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/LinkedBlockingQueue.java,v
retrieving revision 1.87
diff -u -U 0 -r1.87 LinkedBlockingQueue.java
--- concurrent/LinkedBlockingQueue.java 8 Aug 2013 20:12:10 -0000 1.87
+++ concurrent/LinkedBlockingQueue.java 1 Dec 2014 18:21:05 -0000
@@ -51 +51 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: concurrent/LinkedTransferQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/LinkedTransferQueue.java,v
retrieving revision 1.74
diff -u -U 0 -r1.74 LinkedTransferQueue.java
--- concurrent/LinkedTransferQueue.java 23 Nov 2014 17:44:51 -0000 1.74
+++ concurrent/LinkedTransferQueue.java 1 Dec 2014 18:21:05 -0000
@@ -60 +60 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: concurrent/PriorityBlockingQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/PriorityBlockingQueue.java,v
retrieving revision 1.103
diff -u -U 0 -r1.103 PriorityBlockingQueue.java
--- concurrent/PriorityBlockingQueue.java 8 Aug 2013 20:12:10 -0000 1.103
+++ concurrent/PriorityBlockingQueue.java 1 Dec 2014 18:21:05 -0000
@@ -79 +79 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: concurrent/SynchronousQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/SynchronousQueue.java,v
retrieving revision 1.105
diff -u -U 0 -r1.105 SynchronousQueue.java
--- concurrent/SynchronousQueue.java 29 Nov 2014 03:03:14 -0000 1.105
+++ concurrent/SynchronousQueue.java 1 Dec 2014 18:21:05 -0000
@@ -55 +55 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue
Index: concurrent/TransferQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/TransferQueue.java,v
retrieving revision 1.5
diff -u -U 0 -r1.5 TransferQueue.java
--- concurrent/TransferQueue.java 15 Mar 2011 19:47:04 -0000 1.5
+++ concurrent/TransferQueue.java 1 Dec 2014 18:21:05 -0000
@@ -37 +37 @@
- * @param <E> the type of elements held in this collection
+ * @param <E> the type of elements held in this queue

From cowwoc at bbs.darktech.org  Mon Dec  1 13:28:34 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Mon, 1 Dec 2014 11:28:34 -0700 (MST)
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <CA+kOe09Rr8cZqkPr7tv+03ENJ7iRWxSx=zxnKpx22gw990N7Rw@mail.gmail.com>
References: <1417232551381-11504.post@n7.nabble.com>
	<547B1EEC.3060306@cs.oswego.edu>
	<CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>
	<547B3FC1.5060801@cs.oswego.edu>
	<CA+F8eeQHddfkfgsnNrfFpDCma7nPs3ad6_EtAzwK0hNtmbkzbg@mail.gmail.com>
	<CA+kOe09Rr8cZqkPr7tv+03ENJ7iRWxSx=zxnKpx22gw990N7Rw@mail.gmail.com>
Message-ID: <547CB508.2000505@bbs.darktech.org>

+1 that readability trumps maintainability. "Written a few times, read 
many more times" :)

Gili

On 01/12/2014 1:26 PM, Martin Buchholz-3 [via JSR166 Concurrency] wrote:
> On Sun, Nov 30, 2014 at 8:59 AM, Tim Peierls <[hidden email] 
> </user/SendEmail.jtp?type=node&node=11532&i=0>> wrote:
>
> > On Sun, Nov 30, 2014 at 11:03 AM, Doug Lea <[hidden email] 
> </user/SendEmail.jtp?type=node&node=11532&i=1>> wrote:
> >>
> >> On 11/30/2014 09:30 AM, Tim Peierls wrote:
> >>>
> >>>
> >>> Confusing, because of the inherited CompletionStage doc: The type of
> >>> "this" and
> >>> the return type promised by the doc are both referred to as 
> "stage" when
> >>> in fact
> >>> both are specifically CompletableFuture. That's imprecise, not 
> wrong, but
> >>> it
> >>> makes the reader stumble right off.
> >>
> >>
> >> OK, but we do this uniformly in java.util/j.u.c.
> >>
> >> For example, see the many
> >> mentions of "this collection" in Collection implementations.
> >
> >
> >
> > Well... Two seconds of looking and I found an inconsistency: type 
> parameter
> > E for Set is "type of elements maintained by this set" and for Queue is
> > "type of elements held in this collection" (compare E in List, 
> SortedSet,
> > Deque, ...). Further brief manual scan picked up no other instances 
> of "this
> > collection" in the docs for commonly used Collection subtypes in j.u.
> >
> > And just because we do it, doesn't mean it's a good thing. In most 
> cases we
> > are getting away with it, but read on for my reasoning about when it 
> becomes
> > inappropriate.
>
> I actually made an effort many years ago to make the javadoc for
> collection classes better by changing "this collection" to "this set",
> etc.  The most readable is for both Set and HashSet to use "this set",
> I think.  For CompletableFuture, it is better to change occurrences of
> "this stage".  Readability trumps maintainability for core
> libraries.... except that here the amount of copy-paste that would be
> required is unprecedented ... so I'm hesitating ....
>
> But while we're thinking about that, do we have agreement that the
> following is progress?
>
> Index: AbstractQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/AbstractQueue.java,v
> retrieving revision 1.38
> diff -u -U 0 -r1.38 AbstractQueue.java
> --- AbstractQueue.java 16 Jan 2013 01:59:47 -0000 1.38
> +++ AbstractQueue.java 1 Dec 2014 18:21:02 -0000
> @@ -33 +33 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: ArrayDeque.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/ArrayDeque.java,v
> retrieving revision 1.59
> diff -u -U 0 -r1.59 ArrayDeque.java
> --- ArrayDeque.java 23 Nov 2014 18:06:50 -0000 1.59
> +++ ArrayDeque.java 1 Dec 2014 18:21:02 -0000
> @@ -56 +56 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this deque
> Index: Deque.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/Deque.java,v
> retrieving revision 1.29
> diff -u -U 0 -r1.29 Deque.java
> --- Deque.java 23 Nov 2014 18:46:47 -0000 1.29
> +++ Deque.java 1 Dec 2014 18:21:02 -0000
> @@ -162 +162 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this deque
> Index: PriorityQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/PriorityQueue.java,v
> retrieving revision 1.100
> diff -u -U 0 -r1.100 PriorityQueue.java
> --- PriorityQueue.java 29 Aug 2014 21:42:37 -0000 1.100
> +++ PriorityQueue.java 1 Dec 2014 18:21:03 -0000
> @@ -80 +80 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: Queue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/Queue.java,v
> retrieving revision 1.41
> diff -u -U 0 -r1.41 Queue.java
> --- Queue.java 12 Nov 2013 23:23:05 -0000 1.41
> +++ Queue.java 1 Dec 2014 18:21:03 -0000
> @@ -113 +113 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: concurrent/ArrayBlockingQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ArrayBlockingQueue.java,v
> retrieving revision 1.108
> diff -u -U 0 -r1.108 ArrayBlockingQueue.java
> --- concurrent/ArrayBlockingQueue.java 11 Apr 2014 21:15:44 -0000 1.108
> +++ concurrent/ArrayBlockingQueue.java 1 Dec 2014 18:21:03 -0000
> @@ -53 +53 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: concurrent/BlockingDeque.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/BlockingDeque.java,v
> retrieving revision 1.27
> diff -u -U 0 -r1.27 BlockingDeque.java
> --- concurrent/BlockingDeque.java 12 Nov 2013 23:23:05 -0000 1.27
> +++ concurrent/BlockingDeque.java 1 Dec 2014 18:21:03 -0000
> @@ -169 +169 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this deque
> Index: concurrent/BlockingQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/BlockingQueue.java,v
> retrieving revision 1.53
> diff -u -U 0 -r1.53 BlockingQueue.java
> --- concurrent/BlockingQueue.java 12 Nov 2013 23:23:05 -0000 1.53
> +++ concurrent/BlockingQueue.java 1 Dec 2014 18:21:03 -0000
> @@ -149 +149 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: concurrent/ConcurrentLinkedDeque.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedDeque.java,v
> retrieving revision 1.54
> diff -u -U 0 -r1.54 ConcurrentLinkedDeque.java
> --- concurrent/ConcurrentLinkedDeque.java 23 Nov 2014 21:39:30 -0000 1.54
> +++ concurrent/ConcurrentLinkedDeque.java 1 Dec 2014 18:21:04 -0000
> @@ -63 +63 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this deque
> Index: concurrent/ConcurrentLinkedQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
> retrieving revision 1.104
> diff -u -U 0 -r1.104 ConcurrentLinkedQueue.java
> --- concurrent/ConcurrentLinkedQueue.java 23 Nov 2014 18:20:26 -0000 
> 1.104
> +++ concurrent/ConcurrentLinkedQueue.java 1 Dec 2014 18:21:04 -0000
> @@ -77 +77 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: concurrent/CopyOnWriteArrayList.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CopyOnWriteArrayList.java,v
> retrieving revision 1.117
> diff -u -U 0 -r1.117 CopyOnWriteArrayList.java
> --- concurrent/CopyOnWriteArrayList.java 23 Nov 2014 21:40:08 -0000 1.117
> +++ concurrent/CopyOnWriteArrayList.java 1 Dec 2014 18:21:05 -0000
> @@ -71 +71 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this list
> Index: concurrent/CopyOnWriteArraySet.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CopyOnWriteArraySet.java,v
> retrieving revision 1.59
> diff -u -U 0 -r1.59 CopyOnWriteArraySet.java
> --- concurrent/CopyOnWriteArraySet.java 23 Nov 2014 21:40:08 -0000 1.59
> +++ concurrent/CopyOnWriteArraySet.java 1 Dec 2014 18:21:05 -0000
> @@ -66 +66 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this set
> Index: concurrent/DelayQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/DelayQueue.java,v
> retrieving revision 1.66
> diff -u -U 0 -r1.66 DelayQueue.java
> --- concurrent/DelayQueue.java 11 Apr 2014 21:15:44 -0000 1.66
> +++ concurrent/DelayQueue.java 1 Dec 2014 18:21:05 -0000
> @@ -39 +39 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: concurrent/LinkedBlockingDeque.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/LinkedBlockingDeque.java,v
> retrieving revision 1.51
> diff -u -U 0 -r1.51 LinkedBlockingDeque.java
> --- concurrent/LinkedBlockingDeque.java 8 Aug 2013 20:12:10 -0000 1.51
> +++ concurrent/LinkedBlockingDeque.java 1 Dec 2014 18:21:05 -0000
> @@ -48 +48 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this deque
> Index: concurrent/LinkedBlockingQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/LinkedBlockingQueue.java,v
> retrieving revision 1.87
> diff -u -U 0 -r1.87 LinkedBlockingQueue.java
> --- concurrent/LinkedBlockingQueue.java 8 Aug 2013 20:12:10 -0000 1.87
> +++ concurrent/LinkedBlockingQueue.java 1 Dec 2014 18:21:05 -0000
> @@ -51 +51 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: concurrent/LinkedTransferQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/LinkedTransferQueue.java,v
> retrieving revision 1.74
> diff -u -U 0 -r1.74 LinkedTransferQueue.java
> --- concurrent/LinkedTransferQueue.java 23 Nov 2014 17:44:51 -0000 1.74
> +++ concurrent/LinkedTransferQueue.java 1 Dec 2014 18:21:05 -0000
> @@ -60 +60 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: concurrent/PriorityBlockingQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/PriorityBlockingQueue.java,v
> retrieving revision 1.103
> diff -u -U 0 -r1.103 PriorityBlockingQueue.java
> --- concurrent/PriorityBlockingQueue.java 8 Aug 2013 20:12:10 -0000 1.103
> +++ concurrent/PriorityBlockingQueue.java 1 Dec 2014 18:21:05 -0000
> @@ -79 +79 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: concurrent/SynchronousQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/SynchronousQueue.java,v
> retrieving revision 1.105
> diff -u -U 0 -r1.105 SynchronousQueue.java
> --- concurrent/SynchronousQueue.java 29 Nov 2014 03:03:14 -0000 1.105
> +++ concurrent/SynchronousQueue.java 1 Dec 2014 18:21:05 -0000
> @@ -55 +55 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> Index: concurrent/TransferQueue.java
> ===================================================================
> RCS file: 
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/TransferQueue.java,v
> retrieving revision 1.5
> diff -u -U 0 -r1.5 TransferQueue.java
> --- concurrent/TransferQueue.java 15 Mar 2011 19:47:04 -0000 1.5
> +++ concurrent/TransferQueue.java 1 Dec 2014 18:21:05 -0000
> @@ -37 +37 @@
> - * @param <E> the type of elements held in this collection
> + * @param <E> the type of elements held in this queue
> _______________________________________________
> Concurrency-interest mailing list
> [hidden email] </user/SendEmail.jtp?type=node&node=11532&i=2>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> ------------------------------------------------------------------------
> If you reply to this email, your message will be added to the 
> discussion below:
> http://jsr166-concurrency.10961.n7.nabble.com/Please-clarify-Javadoc-of-CompletableFuture-thenCompose-tp11504p11532.html 
>
> To unsubscribe from Please clarify Javadoc of 
> CompletableFuture.thenCompose(), click here 
> <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=11504&code=Y293d29jQGJicy5kYXJrdGVjaC5vcmd8MTE1MDR8MTU3NDMyMTI0Nw==>.
> NAML 
> <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml> 
>





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Please-clarify-Javadoc-of-CompletableFuture-thenCompose-tp11504p11533.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/374b6860/attachment-0001.html>

From vitalyd at gmail.com  Mon Dec  1 13:37:51 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 1 Dec 2014 13:37:51 -0500
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <547CA4A0.5060506@bbs.darktech.org>
References: <1417232551381-11504.post@n7.nabble.com>
	<547B1EEC.3060306@cs.oswego.edu>
	<CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>
	<547B3FC1.5060801@cs.oswego.edu>
	<CA+F8eeQHddfkfgsnNrfFpDCma7nPs3ad6_EtAzwK0hNtmbkzbg@mail.gmail.com>
	<547BBF1D.1020908@bbs.darktech.org>
	<CA+F8eeQo5rrWf-Om6AtroOcvyjD7aznxBHui7ycOmtuLdRie-g@mail.gmail.com>
	<547CA4A0.5060506@bbs.darktech.org>
Message-ID: <CAHjP37Gtd8J_v47PgwaZC2FKMgDC8PP-NvOu++jDryj-BRAZMQ@mail.gmail.com>

compiler-dev at openjdk.java.net

Sent from my phone
On Dec 1, 2014 12:49 PM, "cowwoc" <cowwoc at bbs.darktech.org> wrote:

> On 01/12/2014 8:59 AM, tpeierls [via JSR166 Concurrency] wrote:
>
>   On a related note, I find Generics compiler error messages to be
>> extremely cryptic ...
>>
>
>  You're not alone, but this discussion belongs elsewhere.
>
>  --tim
>
>
> Any idea where "elsewhere" is?
>
> Gili
>
> ------------------------------
> View this message in context: Re: Please clarify Javadoc of
> CompletableFuture.thenCompose()
> <http://jsr166-concurrency.10961.n7.nabble.com/Please-clarify-Javadoc-of-CompletableFuture-thenCompose-tp11504p11530.html>
> Sent from the JSR166 Concurrency mailing list archive
> <http://jsr166-concurrency.10961.n7.nabble.com/> at Nabble.com.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/01b7e206/attachment.html>

From headius at headius.com  Mon Dec  1 13:50:18 2014
From: headius at headius.com (Charles Oliver Nutter)
Date: Mon, 1 Dec 2014 12:50:18 -0600
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others) not
	waking on interrupt?
Message-ID: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>

I have been trying to find a bug in JRuby that involves a queue wait
hanging when it should be successfully interrupted. I'm starting to
believe it's a bug in either ArrayBlockingQueue or classes downstream
from #take (e.g. LockSupport). I have so far been unable to reproduce
with simple Java code because it requires a lot of overhead. It fails
once out of 1000 loops in Ruby code.

The scenario is that thread A is waiting on an ArrayBlockingQueue of
size 1, and thread B comes along and calls Thread#interrupt on it.
Most of the time this succeeds, but when it fails I see thread A
happily sitting in LockSupport.park logic even though it should have
been woken up.

I can only think of a few possible scenarios for this:

* LockSupport.park is not receiving the thread interrupt, or otherwise
does not (can not?) atomically check it before descheduling the
thread.
* The interrupt is getting improperly cleared by something in the
ArrayBlockingQueue or downstream before it gets into the park call.
* I'm doing this wrong.

Help?

- Charlie

From headius at headius.com  Mon Dec  1 14:08:46 2014
From: headius at headius.com (Charles Oliver Nutter)
Date: Mon, 1 Dec 2014 13:08:46 -0600
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
	not waking on interrupt?
In-Reply-To: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
Message-ID: <CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>

To confirm I'm actually sending the interrupt, I added logging output
to my test. The log line is immediately before a call to
Thread#interrupt. I do not understand how the thread could still be
parked (about five seconds later when I triggered the thread dump):

interrupting in take: Thread[Ruby-0-Thread-49: blah.rb:1,5,main]

^\2014-12-01 13:00:57

Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.40-b14 mixed mode):


"Ruby-0-Thread-49: blah.rb:1" #60 daemon prio=5 os_prio=31
tid=0x00007fd43b74f000 nid=0x5bbf waiting on condition
[0x000000011a951000]

   java.lang.Thread.State: WAITING (parking)

at sun.misc.Unsafe.park(Native Method)

- parking to wait for  <0x00000007b6c7c890> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)

at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)

at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)

at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)

- Charlie

On Mon, Dec 1, 2014 at 12:50 PM, Charles Oliver Nutter
<headius at headius.com> wrote:
> I have been trying to find a bug in JRuby that involves a queue wait
> hanging when it should be successfully interrupted. I'm starting to
> believe it's a bug in either ArrayBlockingQueue or classes downstream
> from #take (e.g. LockSupport). I have so far been unable to reproduce
> with simple Java code because it requires a lot of overhead. It fails
> once out of 1000 loops in Ruby code.
>
> The scenario is that thread A is waiting on an ArrayBlockingQueue of
> size 1, and thread B comes along and calls Thread#interrupt on it.
> Most of the time this succeeds, but when it fails I see thread A
> happily sitting in LockSupport.park logic even though it should have
> been woken up.
>
> I can only think of a few possible scenarios for this:
>
> * LockSupport.park is not receiving the thread interrupt, or otherwise
> does not (can not?) atomically check it before descheduling the
> thread.
> * The interrupt is getting improperly cleared by something in the
> ArrayBlockingQueue or downstream before it gets into the park call.
> * I'm doing this wrong.
>
> Help?
>
> - Charlie

From martinrb at google.com  Mon Dec  1 14:29:35 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 11:29:35 -0800
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
Message-ID: <CA+kOe09E30dBnfLAsYk8j+QW1sC3bSryQJDXAk7WZBALAcEyOg@mail.gmail.com>

Doesn't seem like a known problem...
You could try varying the parameters trying to track down the root cause.
- make the ABQ capacity > 1 ?  capacity 1 is unusual.
- use another blocking queue implementation?
- you could instrument your code to make 100% sure that the queue is
currently empty and the taker is currently blocked by checking its
Thread.State before sending the interrupt

Us non-rubyists are likely to be looking for a 100% java repro recipe.

of course, if you check for empty, then send an interrupt, there is a
race where the queue may receive an element in between.

On Mon, Dec 1, 2014 at 10:50 AM, Charles Oliver Nutter
<headius at headius.com> wrote:
> I have been trying to find a bug in JRuby that involves a queue wait
> hanging when it should be successfully interrupted. I'm starting to
> believe it's a bug in either ArrayBlockingQueue or classes downstream
> from #take (e.g. LockSupport). I have so far been unable to reproduce
> with simple Java code because it requires a lot of overhead. It fails
> once out of 1000 loops in Ruby code.
>
> The scenario is that thread A is waiting on an ArrayBlockingQueue of
> size 1, and thread B comes along and calls Thread#interrupt on it.
> Most of the time this succeeds, but when it fails I see thread A
> happily sitting in LockSupport.park logic even though it should have
> been woken up.
>
> I can only think of a few possible scenarios for this:
>
> * LockSupport.park is not receiving the thread interrupt, or otherwise
> does not (can not?) atomically check it before descheduling the
> thread.
> * The interrupt is getting improperly cleared by something in the
> ArrayBlockingQueue or downstream before it gets into the park call.
> * I'm doing this wrong.
>
> Help?
>
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From martinrb at google.com  Mon Dec  1 14:47:01 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 11:47:01 -0800
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
Message-ID: <CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>

It seems unlikely that you would be the first person to find a
missing-interrupt bug in either ABQ or AQS or park.  From your
description you are not making 100% sure that the thread is already in
take when the interrupt is delivered.  Maybe it got swallowed before?

Have you tried different jdk versions?

On Mon, Dec 1, 2014 at 11:08 AM, Charles Oliver Nutter
<headius at headius.com> wrote:
> To confirm I'm actually sending the interrupt, I added logging output
> to my test. The log line is immediately before a call to
> Thread#interrupt. I do not understand how the thread could still be
> parked (about five seconds later when I triggered the thread dump):
>
> interrupting in take: Thread[Ruby-0-Thread-49: blah.rb:1,5,main]
>
> ^\2014-12-01 13:00:57
>
> Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.40-b14 mixed mode):
>
>
> "Ruby-0-Thread-49: blah.rb:1" #60 daemon prio=5 os_prio=31
> tid=0x00007fd43b74f000 nid=0x5bbf waiting on condition
> [0x000000011a951000]
>
>    java.lang.Thread.State: WAITING (parking)
>
> at sun.misc.Unsafe.park(Native Method)
>
> - parking to wait for  <0x00000007b6c7c890> (a
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
>
> at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
>
> at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
>
> at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
>
> - Charlie
>
> On Mon, Dec 1, 2014 at 12:50 PM, Charles Oliver Nutter
> <headius at headius.com> wrote:
>> I have been trying to find a bug in JRuby that involves a queue wait
>> hanging when it should be successfully interrupted. I'm starting to
>> believe it's a bug in either ArrayBlockingQueue or classes downstream
>> from #take (e.g. LockSupport). I have so far been unable to reproduce
>> with simple Java code because it requires a lot of overhead. It fails
>> once out of 1000 loops in Ruby code.
>>
>> The scenario is that thread A is waiting on an ArrayBlockingQueue of
>> size 1, and thread B comes along and calls Thread#interrupt on it.
>> Most of the time this succeeds, but when it fails I see thread A
>> happily sitting in LockSupport.park logic even though it should have
>> been woken up.
>>
>> I can only think of a few possible scenarios for this:
>>
>> * LockSupport.park is not receiving the thread interrupt, or otherwise
>> does not (can not?) atomically check it before descheduling the
>> thread.
>> * The interrupt is getting improperly cleared by something in the
>> ArrayBlockingQueue or downstream before it gets into the park call.
>> * I'm doing this wrong.
>>
>> Help?
>>
>> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From headius at headius.com  Mon Dec  1 14:50:54 2014
From: headius at headius.com (Charles Oliver Nutter)
Date: Mon, 1 Dec 2014 13:50:54 -0600
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
Message-ID: <CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>

I'm working to narrow that down now. You're right...I don't expect to
have found a bug, but I'm having trouble explaining the behavior I'm
seeing.

Just to clarify the key point, though: LockSupport.park should be 100%
wakeable using Thread#interrupt, yes? There's no situation where a
thread in park would *not* be expected to wake up in response to
Thread#interrupt?

- Charlie

On Mon, Dec 1, 2014 at 1:47 PM, Martin Buchholz <martinrb at google.com> wrote:
> It seems unlikely that you would be the first person to find a
> missing-interrupt bug in either ABQ or AQS or park.  From your
> description you are not making 100% sure that the thread is already in
> take when the interrupt is delivered.  Maybe it got swallowed before?
>
> Have you tried different jdk versions?
>
> On Mon, Dec 1, 2014 at 11:08 AM, Charles Oliver Nutter
> <headius at headius.com> wrote:
>> To confirm I'm actually sending the interrupt, I added logging output
>> to my test. The log line is immediately before a call to
>> Thread#interrupt. I do not understand how the thread could still be
>> parked (about five seconds later when I triggered the thread dump):
>>
>> interrupting in take: Thread[Ruby-0-Thread-49: blah.rb:1,5,main]
>>
>> ^\2014-12-01 13:00:57
>>
>> Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.40-b14 mixed mode):
>>
>>
>> "Ruby-0-Thread-49: blah.rb:1" #60 daemon prio=5 os_prio=31
>> tid=0x00007fd43b74f000 nid=0x5bbf waiting on condition
>> [0x000000011a951000]
>>
>>    java.lang.Thread.State: WAITING (parking)
>>
>> at sun.misc.Unsafe.park(Native Method)
>>
>> - parking to wait for  <0x00000007b6c7c890> (a
>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
>>
>> at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
>>
>> at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
>>
>> at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
>>
>> - Charlie
>>
>> On Mon, Dec 1, 2014 at 12:50 PM, Charles Oliver Nutter
>> <headius at headius.com> wrote:
>>> I have been trying to find a bug in JRuby that involves a queue wait
>>> hanging when it should be successfully interrupted. I'm starting to
>>> believe it's a bug in either ArrayBlockingQueue or classes downstream
>>> from #take (e.g. LockSupport). I have so far been unable to reproduce
>>> with simple Java code because it requires a lot of overhead. It fails
>>> once out of 1000 loops in Ruby code.
>>>
>>> The scenario is that thread A is waiting on an ArrayBlockingQueue of
>>> size 1, and thread B comes along and calls Thread#interrupt on it.
>>> Most of the time this succeeds, but when it fails I see thread A
>>> happily sitting in LockSupport.park logic even though it should have
>>> been woken up.
>>>
>>> I can only think of a few possible scenarios for this:
>>>
>>> * LockSupport.park is not receiving the thread interrupt, or otherwise
>>> does not (can not?) atomically check it before descheduling the
>>> thread.
>>> * The interrupt is getting improperly cleared by something in the
>>> ArrayBlockingQueue or downstream before it gets into the park call.
>>> * I'm doing this wrong.
>>>
>>> Help?
>>>
>>> - Charlie
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From martinrb at google.com  Mon Dec  1 15:02:46 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 12:02:46 -0800
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>
Message-ID: <CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>

On Mon, Dec 1, 2014 at 11:50 AM, Charles Oliver Nutter
<headius at headius.com> wrote:
> I'm working to narrow that down now. You're right...I don't expect to
> have found a bug, but I'm having trouble explaining the behavior I'm
> seeing.
>
> Just to clarify the key point, though: LockSupport.park should be 100%
> wakeable using Thread#interrupt, yes? There's no situation where a
> thread in park would *not* be expected to wake up in response to
> Thread#interrupt?

Right - park() is allowed to return spuriously, but not allowed to
spuriously fail to return !
But these methods are usually called in a loop, and if a caller failed
to check or notice the interrupt then it would call park again.

From martinrb at google.com  Mon Dec  1 15:13:24 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 12:13:24 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <5473A344.8090302@oracle.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
Message-ID: <CA+kOe0-mVZAJtFvT0jSU9+oLy9K4GdArXUSYpMOinuXdaTrjfw@mail.gmail.com>

On Mon, Nov 24, 2014 at 1:29 PM, Aleksey Shipilev
<aleksey.shipilev at oracle.com> wrote:
> I think "implies the effect of C++11" is too strong wording. "related"
> might be more appropriate.

I've been struggling to come up with better wording.  The latest webrev says

+     * Corresponds to C11 atomic_thread_fence(memory_order_acquire).

but I'm not totally happy with that either. "Essentially equivalent to" ?

From headius at headius.com  Mon Dec  1 15:17:53 2014
From: headius at headius.com (Charles Oliver Nutter)
Date: Mon, 1 Dec 2014 14:17:53 -0600
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>
	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
Message-ID: <CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>

Sense and sanity prevail! Given the new information that this is
probably my fault, I dug a bit deeper...and found a race within my own
code. The thread interrupt logic in JRuby was firing before an
unblocker function was installed, causing the interrupt to just set
flags and not forcibly interrupt the thread. The blocking call then
proceeded and the flags were not checked again until after it had
completed. Fixed it by adding a second flag check *after* the
unblocker has been set but *before* making the blocking call, and it
seems solid now.

Incidentally, I have a question...

This logic is being used to emulate fibers (coroutines) in JRuby. A
fiber belongs to a given thread, and there may be many active fibers
within a thread, but none of them run concurrently. Every transfer
into a fiber is an explicit hand-off. If you're familiar with Python
generators or Go's goroutines, you get the idea. I've struggled to
find the right data structure to efficiently implement this hand-off.

* Exchanger was just about the slowest way. I never figured out why.
* Manually implementing it with park/unpark worked, but I could never
get it perfect. Might have been this race or another race, so I may
revisit.
* new ArrayBlockingQueue(1) ultimately ended up being the fastest by a
wide margin.

I believe I've asked this before, but what's the fastest way to
*explicitly* transfer control to another thread in an interruptible
way?

Thanks for clarifying things :-)

- Charlie

On Mon, Dec 1, 2014 at 2:02 PM, Martin Buchholz <martinrb at google.com> wrote:
> On Mon, Dec 1, 2014 at 11:50 AM, Charles Oliver Nutter
> <headius at headius.com> wrote:
>> I'm working to narrow that down now. You're right...I don't expect to
>> have found a bug, but I'm having trouble explaining the behavior I'm
>> seeing.
>>
>> Just to clarify the key point, though: LockSupport.park should be 100%
>> wakeable using Thread#interrupt, yes? There's no situation where a
>> thread in park would *not* be expected to wake up in response to
>> Thread#interrupt?
>
> Right - park() is allowed to return spuriously, but not allowed to
> spuriously fail to return !
> But these methods are usually called in a loop, and if a caller failed
> to check or notice the interrupt then it would call park again.

From martinrb at google.com  Mon Dec  1 15:40:51 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 12:40:51 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CAPUmR1aZNSdk5znSfp4ei1M+ZK1M06uEYujH3Z2_B1wtOqMMhw@mail.gmail.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
	<CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>
	<B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>
	<21DC5F06-9597-4CCB-8A25-5CC354A0BCE5@flyingtroika.com>
	<CAPUmR1aZNSdk5znSfp4ei1M+ZK1M06uEYujH3Z2_B1wtOqMMhw@mail.gmail.com>
Message-ID: <CA+kOe0-FYxQ0_EgQULwuy_eJi5YfvVZ_rGYj0BGkj_fxWK7SvQ@mail.gmail.com>

Hans,

(Thanks for your excellent work on C/C++ 11 and your eternal patience)

On Tue, Nov 25, 2014 at 11:15 AM, Hans Boehm <boehm at acm.org> wrote:
> It seems to me that a (dubiuously named) loadFence is intended to have
> essentially the same semantics as the (perhaps slightly less dubiously
> named) C++ atomic_thread_fence(memory_order_acquire), and a storeFence
> matches atomic_thread_fence(memory_order_release).  The C++ standard and,
> even more so, Mark Batty's work have a precise definition of what those mean
> in terms of implied "synchronizes with" relationships.
>
> It looks to me like this whole implementation model for volatiles in terms
> of fences is fundamentally doomed, and it probably makes more sense to get
> rid of it rather than spending time on renaming it (though we just did the
> latter in Android to avoid similar confusion about semantics).  It's

I would also like to see alignment to leverage the technical and
cultural work done on C11.  I would like to see Unsafe get
load-acquire and store-release methods and these should be used in
preference to fences where possible.  I'd like to see the C11 wording
reused as much as possible.  The meanings of the words "acquire" and
"release" are now "owned" by the C11 community and we should tag
along.

A better API for Unsafe would be

putOrdered -> storeRelease
put -> storeRelaxed
(ordinary volatile write) -> store (default is sequential consistent)

etc ...

but the high cost of renaming methods in Unsafe probably makes this a
no-go, even though Unsafe is not a public API in theory.

At least the documentation of all the methods should indicate what the
memory effects and the corresponding C++11 memory model interpretation
is.

E.g. Unsafe.compareAndSwap should document the memory effects, i.e.
sequential consistency.

Unsafe doesn't currently have a readAcquire method (mirror of
putOrdered) probably because volatile read is _almost_ the same (but
not on ppc!).

> fundamentally incompatible with the way volatiles/atomics are intended to be
> implemented on ARMv8 (and Itanium).  Which I think fundamentally get this
> much closer to right than traditional fence-based ISAs.
>
> I'm no hardware architect, but fundamentally it seems to me that
>
> load x
> acquire_fence
>
> imposes a much more stringent constraint than
>
> load_acquire x
>
> Consider the case in which the load from x is an L1 hit, but a preceding
> load (from say y) is a long-latency miss.  If we enforce ordering by just
> waiting for completion of prior operation, the former has to wait for the
> load from y to complete; while the latter doesn't.  I find it hard to
> believe that this doesn't leave an appreciable amount of performance on the
> table, at least for some interesting microarchitectures.

I agree.  Fences should be used rarely.

From martinrb at google.com  Mon Dec  1 15:46:00 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 12:46:00 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
Message-ID: <CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>

David, Paul (i.e. Reviewers) and Doug,

I'd like to commit corrections so we make progress.

I think the current webrev is simple progress with the exception of my
attempt to translate volatiles into fences, which is marginal (but was
a good learning exercise for me).

From vitalyd at gmail.com  Mon Dec  1 16:02:06 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 1 Dec 2014 16:02:06 -0500
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>
	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
	<CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
Message-ID: <CAHjP37HLWYJaqgBYCRAf4uKqems57oR=eVHcT=uSLmqXcvaeEw@mail.gmail.com>

Have you tried SynchronousQueue for the handoff?

On Mon, Dec 1, 2014 at 3:17 PM, Charles Oliver Nutter <headius at headius.com>
wrote:

> Sense and sanity prevail! Given the new information that this is
> probably my fault, I dug a bit deeper...and found a race within my own
> code. The thread interrupt logic in JRuby was firing before an
> unblocker function was installed, causing the interrupt to just set
> flags and not forcibly interrupt the thread. The blocking call then
> proceeded and the flags were not checked again until after it had
> completed. Fixed it by adding a second flag check *after* the
> unblocker has been set but *before* making the blocking call, and it
> seems solid now.
>
> Incidentally, I have a question...
>
> This logic is being used to emulate fibers (coroutines) in JRuby. A
> fiber belongs to a given thread, and there may be many active fibers
> within a thread, but none of them run concurrently. Every transfer
> into a fiber is an explicit hand-off. If you're familiar with Python
> generators or Go's goroutines, you get the idea. I've struggled to
> find the right data structure to efficiently implement this hand-off.
>
> * Exchanger was just about the slowest way. I never figured out why.
> * Manually implementing it with park/unpark worked, but I could never
> get it perfect. Might have been this race or another race, so I may
> revisit.
> * new ArrayBlockingQueue(1) ultimately ended up being the fastest by a
> wide margin.
>
> I believe I've asked this before, but what's the fastest way to
> *explicitly* transfer control to another thread in an interruptible
> way?
>
> Thanks for clarifying things :-)
>
> - Charlie
>
> On Mon, Dec 1, 2014 at 2:02 PM, Martin Buchholz <martinrb at google.com>
> wrote:
> > On Mon, Dec 1, 2014 at 11:50 AM, Charles Oliver Nutter
> > <headius at headius.com> wrote:
> >> I'm working to narrow that down now. You're right...I don't expect to
> >> have found a bug, but I'm having trouble explaining the behavior I'm
> >> seeing.
> >>
> >> Just to clarify the key point, though: LockSupport.park should be 100%
> >> wakeable using Thread#interrupt, yes? There's no situation where a
> >> thread in park would *not* be expected to wake up in response to
> >> Thread#interrupt?
> >
> > Right - park() is allowed to return spuriously, but not allowed to
> > spuriously fail to return !
> > But these methods are usually called in a loop, and if a caller failed
> > to check or notice the interrupt then it would call park again.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/74c1455d/attachment-0001.html>

From martinrb at google.com  Mon Dec  1 16:04:08 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 13:04:08 -0800
Subject: [concurrency-interest] Condition.awaitNanos(Long.MIN_VALUE)
Message-ID: <CA+kOe098Q2mi+FM9s5QFU7BHZAqD7qy47F939pea2sWkTUOrXw@mail.gmail.com>

I just finished telling Charlie that it's impossible to find
concurrency bugs in old j.u.c. code, but here I am claiming this is
one:
awaitNanos is at risk of underflow:

import java.util.concurrent.locks.*;

public class NegativeInfinityWait {
    public static void main(String[] args) throws Throwable {
        ReentrantLock lock = new ReentrantLock();
        Condition condition = lock.newCondition();
        lock.lock();
        System.out.println(condition.awaitNanos(Long.MIN_VALUE));
    }
}
==> javac -source 1.9 -Xlint:all NegativeInfinityWait.java
==> java -esa -ea NegativeInfinityWait
9223372036854686346

From boehm at acm.org  Mon Dec  1 16:51:15 2014
From: boehm at acm.org (Hans Boehm)
Date: Mon, 1 Dec 2014 13:51:15 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe0-FYxQ0_EgQULwuy_eJi5YfvVZ_rGYj0BGkj_fxWK7SvQ@mail.gmail.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
	<CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>
	<B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>
	<21DC5F06-9597-4CCB-8A25-5CC354A0BCE5@flyingtroika.com>
	<CAPUmR1aZNSdk5znSfp4ei1M+ZK1M06uEYujH3Z2_B1wtOqMMhw@mail.gmail.com>
	<CA+kOe0-FYxQ0_EgQULwuy_eJi5YfvVZ_rGYj0BGkj_fxWK7SvQ@mail.gmail.com>
Message-ID: <CAPUmR1bzdeMoDmwdZUtPYjnSZRijCo+9byJiVPfwdrCS3SFfpQ@mail.gmail.com>

Needless to say, I would clearly also like to see a simple correspondence.

But this does raise the interesting question of whether put/get and
store(..., memory_order_relaxed)/load(memory_order_relaxed) are intended to
have similar semantics.  I would guess not, in that the former don't
satisfy coherence; accesses to the same variable can be reordered as for
normal variable accesses, while the C++11/C11 variants do provide those
guarantees.  On most, but not all, architectures that's entirely a compiler
issue; the hardware claims to provide that guarantee.

This affects, for example, whether a variable that is only ever incremented
by one thread can appear to another thread to decrease in value.  Or if a
reference set to a non-null value exactly once can appear to change back to
null after appearing non-null.  In my opinion, it makes sense to always
provide coherence for atomics, since the overhead is small, and so are the
odds of getting code relying on non-coherent racing accesses correct.  But
for ordinary variables whose accesses are not intended to race the
trade-offs are very different.



Hans

On Mon, Dec 1, 2014 at 12:40 PM, Martin Buchholz <martinrb at google.com>
wrote:

> Hans,
>
> (Thanks for your excellent work on C/C++ 11 and your eternal patience)
>
> On Tue, Nov 25, 2014 at 11:15 AM, Hans Boehm <boehm at acm.org> wrote:
> > It seems to me that a (dubiuously named) loadFence is intended to have
> > essentially the same semantics as the (perhaps slightly less dubiously
> > named) C++ atomic_thread_fence(memory_order_acquire), and a storeFence
> > matches atomic_thread_fence(memory_order_release).  The C++ standard and,
> > even more so, Mark Batty's work have a precise definition of what those
> mean
> > in terms of implied "synchronizes with" relationships.
> >
> > It looks to me like this whole implementation model for volatiles in
> terms
> > of fences is fundamentally doomed, and it probably makes more sense to
> get
> > rid of it rather than spending time on renaming it (though we just did
> the
> > latter in Android to avoid similar confusion about semantics).  It's
>
> I would also like to see alignment to leverage the technical and
> cultural work done on C11.  I would like to see Unsafe get
> load-acquire and store-release methods and these should be used in
> preference to fences where possible.  I'd like to see the C11 wording
> reused as much as possible.  The meanings of the words "acquire" and
> "release" are now "owned" by the C11 community and we should tag
> along.
>
> A better API for Unsafe would be
>
> putOrdered -> storeRelease
> put -> storeRelaxed
> (ordinary volatile write) -> store (default is sequential consistent)
>
> etc ...
>
> but the high cost of renaming methods in Unsafe probably makes this a
> no-go, even though Unsafe is not a public API in theory.
>
> At least the documentation of all the methods should indicate what the
> memory effects and the corresponding C++11 memory model interpretation
> is.
>
> E.g. Unsafe.compareAndSwap should document the memory effects, i.e.
> sequential consistency.
>
> Unsafe doesn't currently have a readAcquire method (mirror of
> putOrdered) probably because volatile read is _almost_ the same (but
> not on ppc!).
>
> > fundamentally incompatible with the way volatiles/atomics are intended
> to be
> > implemented on ARMv8 (and Itanium).  Which I think fundamentally get this
> > much closer to right than traditional fence-based ISAs.
> >
> > I'm no hardware architect, but fundamentally it seems to me that
> >
> > load x
> > acquire_fence
> >
> > imposes a much more stringent constraint than
> >
> > load_acquire x
> >
> > Consider the case in which the load from x is an L1 hit, but a preceding
> > load (from say y) is a long-latency miss.  If we enforce ordering by just
> > waiting for completion of prior operation, the former has to wait for the
> > load from y to complete; while the latter doesn't.  I find it hard to
> > believe that this doesn't leave an appreciable amount of performance on
> the
> > table, at least for some interesting microarchitectures.
>
> I agree.  Fences should be used rarely.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/1cf5a6e8/attachment.html>

From dt at flyingtroika.com  Mon Dec  1 17:01:06 2014
From: dt at flyingtroika.com (DT)
Date: Mon, 1 Dec 2014 14:01:06 -0800
Subject: [concurrency-interest] Default thread scheduler mechanism ,
	what are pros and cons?
Message-ID: <6D84BA06-BA08-406B-9D6F-2037EAE71284@flyingtroika.com>

Would it be realistically to introduce a default scheduler with extension mechanism ?  
Currently threads are scheduled by os kernel in Java and same thing applies to latest C++14 standard. As we had experience in the past when a thread pool executors were introduced 1-st time there was a push on having a default thread pool executor. It was not easy to come up with default behavior that can be extended and  be robust enough meet majority  of engineering requirements.

Some reasons to have a default thread scheduler inside jvm:
- difficulty to handle a combination of short and long lived threads
- entirely have to trust on os scheduling algorithms (by far don't have capability for modification, unless a developer recompiled a kernel)
- more requirements come from concurrent / distributed in nature systems to be able to schedule tasks / threads on multiple processors / cores locally and remotely having control of different thread executors
- don' t have a capability to switch / adjust a scheduling algorithm during execution in run time ( FIFO, capacity, fair scheduling)
  



From david.lloyd at redhat.com  Mon Dec  1 17:12:51 2014
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Mon, 01 Dec 2014 16:12:51 -0600
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
	<CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
Message-ID: <547CE7E3.2050401@redhat.com>

On 12/01/2014 02:17 PM, Charles Oliver Nutter wrote:
> Sense and sanity prevail! Given the new information that this is
> probably my fault, I dug a bit deeper...and found a race within my own
> code. The thread interrupt logic in JRuby was firing before an
> unblocker function was installed, causing the interrupt to just set
> flags and not forcibly interrupt the thread. The blocking call then
> proceeded and the flags were not checked again until after it had
> completed. Fixed it by adding a second flag check *after* the
> unblocker has been set but *before* making the blocking call, and it
> seems solid now.
>
> Incidentally, I have a question...
>
> This logic is being used to emulate fibers (coroutines) in JRuby. A
> fiber belongs to a given thread, and there may be many active fibers
> within a thread, but none of them run concurrently. Every transfer
> into a fiber is an explicit hand-off. If you're familiar with Python
> generators or Go's goroutines, you get the idea. I've struggled to
> find the right data structure to efficiently implement this hand-off.
>
> * Exchanger was just about the slowest way. I never figured out why.
> * Manually implementing it with park/unpark worked, but I could never
> get it perfect. Might have been this race or another race, so I may
> revisit.
> * new ArrayBlockingQueue(1) ultimately ended up being the fastest by a
> wide margin.
>
> I believe I've asked this before, but what's the fastest way to
> *explicitly* transfer control to another thread in an interruptible
> way?

I think probably writing a work item to a thread-specific field and 
unparking the thread must be the fastest possible way to give a specific 
waiting thread some specific work.  I don't think that unpark does 
anything in terms of memory visibility though (in any event I'm not 
seeing anything in the javadoc for LockSupport) so the field probably 
would have to be volatile for this to work in its simplest form. 
Tracking and deciding which among multiple threads (if you have more 
than one in a pool situation) are ready for work adds a bunch of 
complexity though, as does coping with having no workers available (e.g. 
do you make the producer wait, or employ a pending tasks queue?). 
Follow this path a surprisingly short distance and you're facing the 
same problems that ThreadPoolExecutor attempts to solve.

-- 
- DML

From headius at headius.com  Mon Dec  1 17:34:46 2014
From: headius at headius.com (Charles Oliver Nutter)
Date: Mon, 1 Dec 2014 16:34:46 -0600
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAHjP37HLWYJaqgBYCRAf4uKqems57oR=eVHcT=uSLmqXcvaeEw@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>
	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
	<CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
	<CAHjP37HLWYJaqgBYCRAf4uKqems57oR=eVHcT=uSLmqXcvaeEw@mail.gmail.com>
Message-ID: <CAE-f1xRHZG8CLAoN+3yCTw79_p5En6PaAueOD73i_EfQL0B45Q@mail.gmail.com>

Ahh yes, that's another one I tried. Sadly it was *also* still slower
than using a single-element ArrayBlockingQueue.

I will try to explore the options again and report back, so I know
I've got my facts straight.

- Charlie

On Mon, Dec 1, 2014 at 3:02 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> Have you tried SynchronousQueue for the handoff?
>
> On Mon, Dec 1, 2014 at 3:17 PM, Charles Oliver Nutter <headius at headius.com>
> wrote:
>>
>> Sense and sanity prevail! Given the new information that this is
>> probably my fault, I dug a bit deeper...and found a race within my own
>> code. The thread interrupt logic in JRuby was firing before an
>> unblocker function was installed, causing the interrupt to just set
>> flags and not forcibly interrupt the thread. The blocking call then
>> proceeded and the flags were not checked again until after it had
>> completed. Fixed it by adding a second flag check *after* the
>> unblocker has been set but *before* making the blocking call, and it
>> seems solid now.
>>
>> Incidentally, I have a question...
>>
>> This logic is being used to emulate fibers (coroutines) in JRuby. A
>> fiber belongs to a given thread, and there may be many active fibers
>> within a thread, but none of them run concurrently. Every transfer
>> into a fiber is an explicit hand-off. If you're familiar with Python
>> generators or Go's goroutines, you get the idea. I've struggled to
>> find the right data structure to efficiently implement this hand-off.
>>
>> * Exchanger was just about the slowest way. I never figured out why.
>> * Manually implementing it with park/unpark worked, but I could never
>> get it perfect. Might have been this race or another race, so I may
>> revisit.
>> * new ArrayBlockingQueue(1) ultimately ended up being the fastest by a
>> wide margin.
>>
>> I believe I've asked this before, but what's the fastest way to
>> *explicitly* transfer control to another thread in an interruptible
>> way?
>>
>> Thanks for clarifying things :-)
>>
>> - Charlie
>>
>> On Mon, Dec 1, 2014 at 2:02 PM, Martin Buchholz <martinrb at google.com>
>> wrote:
>> > On Mon, Dec 1, 2014 at 11:50 AM, Charles Oliver Nutter
>> > <headius at headius.com> wrote:
>> >> I'm working to narrow that down now. You're right...I don't expect to
>> >> have found a bug, but I'm having trouble explaining the behavior I'm
>> >> seeing.
>> >>
>> >> Just to clarify the key point, though: LockSupport.park should be 100%
>> >> wakeable using Thread#interrupt, yes? There's no situation where a
>> >> thread in park would *not* be expected to wake up in response to
>> >> Thread#interrupt?
>> >
>> > Right - park() is allowed to return spuriously, but not allowed to
>> > spuriously fail to return !
>> > But these methods are usually called in a loop, and if a caller failed
>> > to check or notice the interrupt then it would call park again.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From martinrb at google.com  Mon Dec  1 17:41:37 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 14:41:37 -0800
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAE-f1xRHZG8CLAoN+3yCTw79_p5En6PaAueOD73i_EfQL0B45Q@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>
	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
	<CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
	<CAHjP37HLWYJaqgBYCRAf4uKqems57oR=eVHcT=uSLmqXcvaeEw@mail.gmail.com>
	<CAE-f1xRHZG8CLAoN+3yCTw79_p5En6PaAueOD73i_EfQL0B45Q@mail.gmail.com>
Message-ID: <CA+kOe0_zPsfBKf1O+N4LSt4YXVQe_MqPO4MyYDb+ZCZ66ue4qA@mail.gmail.com>

If all you want to do is to have one thread produce a single result
while another waits, that sounds a lot like Future, and both recent
FutureTask and CompletableFuture ought to be more efficient than a
1-element ABQ.

On Mon, Dec 1, 2014 at 2:34 PM, Charles Oliver Nutter
<headius at headius.com> wrote:
> Ahh yes, that's another one I tried. Sadly it was *also* still slower
> than using a single-element ArrayBlockingQueue.
>
> I will try to explore the options again and report back, so I know
> I've got my facts straight.
>
> - Charlie
>
> On Mon, Dec 1, 2014 at 3:02 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>> Have you tried SynchronousQueue for the handoff?
>>
>> On Mon, Dec 1, 2014 at 3:17 PM, Charles Oliver Nutter <headius at headius.com>
>> wrote:
>>>
>>> Sense and sanity prevail! Given the new information that this is
>>> probably my fault, I dug a bit deeper...and found a race within my own
>>> code. The thread interrupt logic in JRuby was firing before an
>>> unblocker function was installed, causing the interrupt to just set
>>> flags and not forcibly interrupt the thread. The blocking call then
>>> proceeded and the flags were not checked again until after it had
>>> completed. Fixed it by adding a second flag check *after* the
>>> unblocker has been set but *before* making the blocking call, and it
>>> seems solid now.
>>>
>>> Incidentally, I have a question...
>>>
>>> This logic is being used to emulate fibers (coroutines) in JRuby. A
>>> fiber belongs to a given thread, and there may be many active fibers
>>> within a thread, but none of them run concurrently. Every transfer
>>> into a fiber is an explicit hand-off. If you're familiar with Python
>>> generators or Go's goroutines, you get the idea. I've struggled to
>>> find the right data structure to efficiently implement this hand-off.
>>>
>>> * Exchanger was just about the slowest way. I never figured out why.
>>> * Manually implementing it with park/unpark worked, but I could never
>>> get it perfect. Might have been this race or another race, so I may
>>> revisit.
>>> * new ArrayBlockingQueue(1) ultimately ended up being the fastest by a
>>> wide margin.
>>>
>>> I believe I've asked this before, but what's the fastest way to
>>> *explicitly* transfer control to another thread in an interruptible
>>> way?
>>>
>>> Thanks for clarifying things :-)
>>>
>>> - Charlie
>>>
>>> On Mon, Dec 1, 2014 at 2:02 PM, Martin Buchholz <martinrb at google.com>
>>> wrote:
>>> > On Mon, Dec 1, 2014 at 11:50 AM, Charles Oliver Nutter
>>> > <headius at headius.com> wrote:
>>> >> I'm working to narrow that down now. You're right...I don't expect to
>>> >> have found a bug, but I'm having trouble explaining the behavior I'm
>>> >> seeing.
>>> >>
>>> >> Just to clarify the key point, though: LockSupport.park should be 100%
>>> >> wakeable using Thread#interrupt, yes? There's no situation where a
>>> >> thread in park would *not* be expected to wake up in response to
>>> >> Thread#interrupt?
>>> >
>>> > Right - park() is allowed to return spuriously, but not allowed to
>>> > spuriously fail to return !
>>> > But these methods are usually called in a loop, and if a caller failed
>>> > to check or notice the interrupt then it would call park again.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>

From thurston at nomagicsoftware.com  Mon Dec  1 18:03:04 2014
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 1 Dec 2014 16:03:04 -0700 (MST)
Subject: [concurrency-interest] FJ stack bounds
Message-ID: <1417474984151-11552.post@n7.nabble.com>

Hello, 

I was wondering if there was any way to compute a maximum bound for the
stack size/height (in frames, not bytes) for a given FJ problem.

So given n = # of FJT created.
Let's keep the math simple and say n = 256

and your RecursiveTasks just split "themselves" into 2 separate
RecursiveTasks which are then forked and joined (idiomatic FJ)

At first glance, it would seem that this would be classic log(n) - but is
it?  In the simple example that would mean a max stack height of 8.

Given that worker threads engage in work-stealing (and combined with the
fact that I can't tell from looking at the code what invariants there are
(if any) in determining which actual FJT will be #exec()'d), is it possible
that (given some unlucky and unlikely "timing"), that a single worker thread
could effect a stack height much > 8? even, perhaps 256? (I realize this is
unlikely, I'm just asking if it's possible)

As best I could tell, there seems to be some logic (#doJoin/#tryUnpush) that
"defaults" to executing the joined task only if it is at the top of the
current thread's task stack (workerqueue), else . . . try to "help" by
executing some other (arbitrarily chosen?) task.
Does that mean that:
left = . . .fork()
right = . . . fork()  //Top of this thread's worker queue

return left.join() + right.join()
is different than
return right.join() + left.join()
in terms of the possible stack heights that can result?

I want to make it clear that I haven't encountered any runtime problems with
stack overflow errors, just wanted to know if there was some theoretical
bound that can be statically computed





  



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/FJ-stack-bounds-tp11552.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jh at squareup.com  Mon Dec  1 18:23:33 2014
From: jh at squareup.com (Josh Humphries)
Date: Mon, 1 Dec 2014 18:23:33 -0500
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CA+kOe0_zPsfBKf1O+N4LSt4YXVQe_MqPO4MyYDb+ZCZ66ue4qA@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>
	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
	<CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
	<CAHjP37HLWYJaqgBYCRAf4uKqems57oR=eVHcT=uSLmqXcvaeEw@mail.gmail.com>
	<CAE-f1xRHZG8CLAoN+3yCTw79_p5En6PaAueOD73i_EfQL0B45Q@mail.gmail.com>
	<CA+kOe0_zPsfBKf1O+N4LSt4YXVQe_MqPO4MyYDb+ZCZ66ue4qA@mail.gmail.com>
Message-ID: <CAHJZN-ve2kufowHzTz9_U_v6ckCAWOxDPv8H5FeGPuWTK+iVjg@mail.gmail.com>

A future probably won't do since it's single use, but for this case,
control needs to keep swapping between consumer and co-routine and back. I
wonder if a there's a way to get a Phaser to work (plus a volatile field to
actually store the item being transferred).

I've actually toyed with exactly this sort of thing, emulating
generators/co-routines in Java using multiple threads. My experiments used
a SynchronousQueue, and it was painfully slow. BTW, it seems like one issue
with ArrayBlockingQueue(1) is that it's not a perfect exchange. You can
have a single element in the queue, unconsumed, instead of a synchronous
hand-off. I think that means that the generator thread is always one ahead
of the consumer thread. (Although maybe I'm imagining the rest of your
implementation incorrectly. And perhaps this fact doesn't strictly matter.)


----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Mon, Dec 1, 2014 at 5:41 PM, Martin Buchholz <martinrb at google.com> wrote:

> If all you want to do is to have one thread produce a single result
> while another waits, that sounds a lot like Future, and both recent
> FutureTask and CompletableFuture ought to be more efficient than a
> 1-element ABQ.
>
> On Mon, Dec 1, 2014 at 2:34 PM, Charles Oliver Nutter
> <headius at headius.com> wrote:
> > Ahh yes, that's another one I tried. Sadly it was *also* still slower
> > than using a single-element ArrayBlockingQueue.
> >
> > I will try to explore the options again and report back, so I know
> > I've got my facts straight.
> >
> > - Charlie
> >
> > On Mon, Dec 1, 2014 at 3:02 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
> >> Have you tried SynchronousQueue for the handoff?
> >>
> >> On Mon, Dec 1, 2014 at 3:17 PM, Charles Oliver Nutter <
> headius at headius.com>
> >> wrote:
> >>>
> >>> Sense and sanity prevail! Given the new information that this is
> >>> probably my fault, I dug a bit deeper...and found a race within my own
> >>> code. The thread interrupt logic in JRuby was firing before an
> >>> unblocker function was installed, causing the interrupt to just set
> >>> flags and not forcibly interrupt the thread. The blocking call then
> >>> proceeded and the flags were not checked again until after it had
> >>> completed. Fixed it by adding a second flag check *after* the
> >>> unblocker has been set but *before* making the blocking call, and it
> >>> seems solid now.
> >>>
> >>> Incidentally, I have a question...
> >>>
> >>> This logic is being used to emulate fibers (coroutines) in JRuby. A
> >>> fiber belongs to a given thread, and there may be many active fibers
> >>> within a thread, but none of them run concurrently. Every transfer
> >>> into a fiber is an explicit hand-off. If you're familiar with Python
> >>> generators or Go's goroutines, you get the idea. I've struggled to
> >>> find the right data structure to efficiently implement this hand-off.
> >>>
> >>> * Exchanger was just about the slowest way. I never figured out why.
> >>> * Manually implementing it with park/unpark worked, but I could never
> >>> get it perfect. Might have been this race or another race, so I may
> >>> revisit.
> >>> * new ArrayBlockingQueue(1) ultimately ended up being the fastest by a
> >>> wide margin.
> >>>
> >>> I believe I've asked this before, but what's the fastest way to
> >>> *explicitly* transfer control to another thread in an interruptible
> >>> way?
> >>>
> >>> Thanks for clarifying things :-)
> >>>
> >>> - Charlie
> >>>
> >>> On Mon, Dec 1, 2014 at 2:02 PM, Martin Buchholz <martinrb at google.com>
> >>> wrote:
> >>> > On Mon, Dec 1, 2014 at 11:50 AM, Charles Oliver Nutter
> >>> > <headius at headius.com> wrote:
> >>> >> I'm working to narrow that down now. You're right...I don't expect
> to
> >>> >> have found a bug, but I'm having trouble explaining the behavior I'm
> >>> >> seeing.
> >>> >>
> >>> >> Just to clarify the key point, though: LockSupport.park should be
> 100%
> >>> >> wakeable using Thread#interrupt, yes? There's no situation where a
> >>> >> thread in park would *not* be expected to wake up in response to
> >>> >> Thread#interrupt?
> >>> >
> >>> > Right - park() is allowed to return spuriously, but not allowed to
> >>> > spuriously fail to return !
> >>> > But these methods are usually called in a loop, and if a caller
> failed
> >>> > to check or notice the interrupt then it would call park again.
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/6c8e831b/attachment.html>

From dl at cs.oswego.edu  Mon Dec  1 19:21:04 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 01 Dec 2014 19:21:04 -0500
Subject: [concurrency-interest] Condition.awaitNanos(Long.MIN_VALUE)
In-Reply-To: <CA+kOe098Q2mi+FM9s5QFU7BHZAqD7qy47F939pea2sWkTUOrXw@mail.gmail.com>
References: <CA+kOe098Q2mi+FM9s5QFU7BHZAqD7qy47F939pea2sWkTUOrXw@mail.gmail.com>
Message-ID: <547D05F0.3000301@cs.oswego.edu>

On 12/01/2014 04:04 PM, Martin Buchholz wrote:
> I just finished telling Charlie that it's impossible to find
> concurrency bugs in old j.u.c. code, but here I am claiming this is
> one:
> awaitNanos is at risk of underflow:

Thanks. The method is correct in not waiting, but
with extreme negative arguments (about -500 years), can return
an overflowed (not saturated) remaining time estimate.
A simple fix for AbstractQueued{Long}Synchronizer classes is
now in jsr166 CVS. Feel free to report and sync.

-Doug


>
> import java.util.concurrent.locks.*;
>
> public class NegativeInfinityWait {
>      public static void main(String[] args) throws Throwable {
>          ReentrantLock lock = new ReentrantLock();
>          Condition condition = lock.newCondition();
>          lock.lock();
>          System.out.println(condition.awaitNanos(Long.MIN_VALUE));
>      }
> }
> ==> javac -source 1.9 -Xlint:all NegativeInfinityWait.java
> ==> java -esa -ea NegativeInfinityWait
> 9223372036854686346
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From headius at headius.com  Mon Dec  1 19:45:29 2014
From: headius at headius.com (Charles Oliver Nutter)
Date: Mon, 1 Dec 2014 18:45:29 -0600
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAHJZN-ve2kufowHzTz9_U_v6ckCAWOxDPv8H5FeGPuWTK+iVjg@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>
	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
	<CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
	<CAHjP37HLWYJaqgBYCRAf4uKqems57oR=eVHcT=uSLmqXcvaeEw@mail.gmail.com>
	<CAE-f1xRHZG8CLAoN+3yCTw79_p5En6PaAueOD73i_EfQL0B45Q@mail.gmail.com>
	<CA+kOe0_zPsfBKf1O+N4LSt4YXVQe_MqPO4MyYDb+ZCZ66ue4qA@mail.gmail.com>
	<CAHJZN-ve2kufowHzTz9_U_v6ckCAWOxDPv8H5FeGPuWTK+iVjg@mail.gmail.com>
Message-ID: <CAE-f1xTEUrSEbSuo95tN25fB14q8NN3GvaOdhY2+LLZCrRa2RA@mail.gmail.com>

On Mon, Dec 1, 2014 at 5:23 PM, Josh Humphries <jh at squareup.com> wrote:
> A future probably won't do since it's single use, but for this case, control
> needs to keep swapping between consumer and co-routine and back. I wonder if
> a there's a way to get a Phaser to work (plus a volatile field to actually
> store the item being transferred).

Right, future is one-shot and I need this to be a generator.

I tried to think through Phaser but at the time I could not use it
without introducing an additional library dependency.

> I've actually toyed with exactly this sort of thing, emulating
> generators/co-routines in Java using multiple threads. My experiments used a
> SynchronousQueue, and it was painfully slow.

My experience as well. It was better than Exchanger, but not by much.
ABQ was *much* faster.

> BTW, it seems like one issue
> with ArrayBlockingQueue(1) is that it's not a perfect exchange. You can have
> a single element in the queue, unconsumed, instead of a synchronous
> hand-off. I think that means that the generator thread is always one ahead
> of the consumer thread. (Although maybe I'm imagining the rest of your
> implementation incorrectly. And perhaps this fact doesn't strictly matter.)

Every fiber round-trip involves two queues (basically, every
transferrable entity has their own queue). The calling thread triggers
the receiving thread by feeding it the next value via the receiving
thread's queue (Fiber#resume in Ruby). It then waits on its own queue
for a response. This allows one-to-one round trips as well as fiber
"transfers" where a chain of them call each other, eventually ending
up back at the initiator.

I think I'll dig up my park/unpark-based impl and give it another
shot. It works basically like you describe, with a volatile transfer
field and explicit parking/unparking. I agree it will probably be the
fastest option, if I can get it to play nicely with some other Ruby
threading oddities (Thread#kill, Thread#raise... sigh).

- Charlie

From dl at cs.oswego.edu  Mon Dec  1 19:58:26 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 01 Dec 2014 19:58:26 -0500
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
Message-ID: <547D0EB2.3070802@cs.oswego.edu>

On 12/01/2014 03:46 PM, Martin Buchholz wrote:
> David, Paul (i.e. Reviewers) and Doug,
>
> I'd like to commit corrections so we make progress.

The current one looks OK to me.
(http://cr.openjdk.java.net/~martin/webrevs/openjdk9/fence-intrinsics/)

-Doug


>
> I think the current webrev is simple progress with the exception of my
> attempt to translate volatiles into fences, which is marginal (but was
> a good learning exercise for me).
>


From martinrb at google.com  Mon Dec  1 20:05:14 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 1 Dec 2014 17:05:14 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CAPUmR1bzdeMoDmwdZUtPYjnSZRijCo+9byJiVPfwdrCS3SFfpQ@mail.gmail.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
	<CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>
	<B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>
	<21DC5F06-9597-4CCB-8A25-5CC354A0BCE5@flyingtroika.com>
	<CAPUmR1aZNSdk5znSfp4ei1M+ZK1M06uEYujH3Z2_B1wtOqMMhw@mail.gmail.com>
	<CA+kOe0-FYxQ0_EgQULwuy_eJi5YfvVZ_rGYj0BGkj_fxWK7SvQ@mail.gmail.com>
	<CAPUmR1bzdeMoDmwdZUtPYjnSZRijCo+9byJiVPfwdrCS3SFfpQ@mail.gmail.com>
Message-ID: <CA+kOe09BkdvpEEGJAo0WgA02Syx0KW18mkeEhDBSR2kc5PLvgg@mail.gmail.com>

On Mon, Dec 1, 2014 at 1:51 PM, Hans Boehm <boehm at acm.org> wrote:
> Needless to say, I would clearly also like to see a simple correspondence.
>
> But this does raise the interesting question of whether put/get and
> store(..., memory_order_relaxed)/load(memory_order_relaxed) are intended to
> have similar semantics.  I would guess not, in that the former don't satisfy
> coherence; accesses to the same variable can be reordered as for normal
> variable accesses, while the C++11/C11 variants do provide those guarantees.
> On most, but not all, architectures that's entirely a compiler issue; the
> hardware claims to provide that guarantee.
>
> This affects, for example, whether a variable that is only ever incremented
> by one thread can appear to another thread to decrease in value.  Or if a
> reference set to a non-null value exactly once can appear to change back to
> null after appearing non-null.  In my opinion, it makes sense to always
> provide coherence for atomics, since the overhead is small, and so are the
> odds of getting code relying on non-coherent racing accesses correct.  But
> for ordinary variables whose accesses are not intended to race the
> trade-offs are very different.

It would be nice to pretend that ordinary java loads and stores map
perfectly to C11 relaxed loads and stores.  This maps well to the lack
of undefined behavior for data races in Java.  But this fails also
with lack of atomicity of Java longs and doubles.  I have no intuition
as to whether always requiring per-variable sequential consistency
would be a performance problem.  Introducing an explicit relaxed
memory order mode in Java when the distinction between ordinary access
is smaller than in C/C++ 11 would be confusing.

Despite all that, it would be clean, consistent and seemingly
straightforward to simply add all of the C/C++ atomic loads, stores
and fences to sun.misc.Unsafe (with the possible exception of consume,
which is still under a cloud).  If that works out for jdk-internal
code, we can add them to a public API.  Providing the full set will
help with interoperability with C code running in another thread
accessing a direct buffer.

From boehm at acm.org  Mon Dec  1 20:54:49 2014
From: boehm at acm.org (Hans Boehm)
Date: Mon, 1 Dec 2014 17:54:49 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe09BkdvpEEGJAo0WgA02Syx0KW18mkeEhDBSR2kc5PLvgg@mail.gmail.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
	<CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>
	<B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>
	<21DC5F06-9597-4CCB-8A25-5CC354A0BCE5@flyingtroika.com>
	<CAPUmR1aZNSdk5znSfp4ei1M+ZK1M06uEYujH3Z2_B1wtOqMMhw@mail.gmail.com>
	<CA+kOe0-FYxQ0_EgQULwuy_eJi5YfvVZ_rGYj0BGkj_fxWK7SvQ@mail.gmail.com>
	<CAPUmR1bzdeMoDmwdZUtPYjnSZRijCo+9byJiVPfwdrCS3SFfpQ@mail.gmail.com>
	<CA+kOe09BkdvpEEGJAo0WgA02Syx0KW18mkeEhDBSR2kc5PLvgg@mail.gmail.com>
Message-ID: <CAPUmR1b54fc7ZEP-x8nDgyppmx-TrwA6f84uttm-wOY46+jzDA@mail.gmail.com>

I think that requiring coherence for ordinary Java accesses would be a
performance problem.  The pre-2005 Java memory model actually promised it,
but implementations ignored that requirement.  That was one significant
motivation of the 2005 memory model overhaul.

The basic problem is that if you have

r1 = x.f;
r2 = y.f;
r3 = x.f;

the compiler can no longer perform common subexpression elimination  on the
two loads from x.f unless it can prove that x and y do not alias, which is
probably rare.  Loads kill available expressions.  Clearly this can
significantly reduce the effectiveness of CSE and similar basic
optimizations.  Enforcing coherence also turns out to be somewhat expensive
on Itanium, and rather expensive on some slightly older ARM processors.

Those arguments probably don't apply to sun.misc.unsafe.  But no matter
what you do for sun.misc.unsafe, something will be inconsistent.

(The other problem of course is that we still don't really know how to
define memory_order_relaxed any better than we know how to define ordinary
Java memory references.)

On Mon, Dec 1, 2014 at 5:05 PM, Martin Buchholz <martinrb at google.com> wrote:

> On Mon, Dec 1, 2014 at 1:51 PM, Hans Boehm <boehm at acm.org> wrote:
> > Needless to say, I would clearly also like to see a simple
> correspondence.
> >
> > But this does raise the interesting question of whether put/get and
> > store(..., memory_order_relaxed)/load(memory_order_relaxed) are intended
> to
> > have similar semantics.  I would guess not, in that the former don't
> satisfy
> > coherence; accesses to the same variable can be reordered as for normal
> > variable accesses, while the C++11/C11 variants do provide those
> guarantees.
> > On most, but not all, architectures that's entirely a compiler issue; the
> > hardware claims to provide that guarantee.
> >
> > This affects, for example, whether a variable that is only ever
> incremented
> > by one thread can appear to another thread to decrease in value.  Or if a
> > reference set to a non-null value exactly once can appear to change back
> to
> > null after appearing non-null.  In my opinion, it makes sense to always
> > provide coherence for atomics, since the overhead is small, and so are
> the
> > odds of getting code relying on non-coherent racing accesses correct.
> But
> > for ordinary variables whose accesses are not intended to race the
> > trade-offs are very different.
>
> It would be nice to pretend that ordinary java loads and stores map
> perfectly to C11 relaxed loads and stores.  This maps well to the lack
> of undefined behavior for data races in Java.  But this fails also
> with lack of atomicity of Java longs and doubles.  I have no intuition
> as to whether always requiring per-variable sequential consistency
> would be a performance problem.  Introducing an explicit relaxed
> memory order mode in Java when the distinction between ordinary access
> is smaller than in C/C++ 11 would be confusing.
>
> Despite all that, it would be clean, consistent and seemingly
> straightforward to simply add all of the C/C++ atomic loads, stores
> and fences to sun.misc.Unsafe (with the possible exception of consume,
> which is still under a cloud).  If that works out for jdk-internal
> code, we can add them to a public API.  Providing the full set will
> help with interoperability with C code running in another thread
> accessing a direct buffer.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/98fa4faf/attachment.html>

From dt at flyingtroika.com  Mon Dec  1 21:54:02 2014
From: dt at flyingtroika.com (DT)
Date: Mon, 01 Dec 2014 18:54:02 -0800
Subject: [concurrency-interest] RFR:
 8065804:JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <1417028404335.73215@devexperts.com>
References: <c806b219426c4c8aa9e5bfa79fc9901d@exchmb02.office.devexperts.com>	<NFBBKALFDCPFIDBNKAPCIECNKLAA.davidcholmes@aapt.net.au>
	<74ff42b06c314fc9998d512725bae69a@exchmb02.office.devexperts.com>,
	<54760CD7.9070704@flyingtroika.com>
	<1417028404335.73215@devexperts.com>
Message-ID: <547D29CA.2000905@flyingtroika.com>

Roman, thank you. As it has been mentioned before from practical 
perspective its quite difficult to incorporate. Though as I understood , 
can be wrong of course that volatile variables and immutable objects 
represent  lineariazable objects because they satisfy those concurrent 
conditions.

Dmitry

On 11/26/2014 11:00 AM, Roman Elizarov wrote:
>
> I'd suggest to start with the original paper by Herlihy who had come 
> up with the concept of Linearizability in 1990:
>
> Linearizability: A Correctness Condition for Concurrent Objects
>
> http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.142.5315
>
>
> There were lot of reasearch about linearizability since then (there 
> are almost a thouthand citations for this arcticle) expanding and 
> improving proof techniquies and applying it. There were no 
> breakthroughs of the comparable magnitude since then. All 
> "thread-safe" objects that you enconter in the modern word are 
> linearizable. It is a defacto "golden standard" correctness condition 
> for concurrent objects.
>
>
> This position is well deserved, because having lineariazable objects 
> as your building blocks makes it super-easy to formally reason about 
> correctness of your code. You will rarely encouter concurrent 
> algorithms that provide weaker guarantees (like quescient 
> consistency), because they all too hard to reason about -- they are 
> either not composable or not local. But when all your 
> concurrent objects are linearizable, then you can ditch 
> happens-before, forget that everything is actually parallel and 
> simply reason about your code in terms of interleaving of "atomic" 
> operations that happen in some global order. That is the beauty of 
> linearizability.
>
>
> But Linearizability is indeed a pretty strong requirement. 
> Linearizability of your shared memory requires that Independent Reads 
> of Independent Writes (IRIW) are consistent. Can you get away with 
> some weaker requirement and still get all the same goodies as 
> linearizability gets you? I have not seen anything promising in this 
> direction. Whoever makes this breakthrough will surely reap the 
> world's recognition and respect.
>
>
> /Roman
>
>
> ------------------------------------------------------------------------
> *??:* DT <dt at flyingtroika.com>
> *??????????:* 26 ?????? 2014 ?. 20:24
> *????:* Roman Elizarov; dholmes at ieee.org; Hans Boehm
> *?????:* core-libs-dev; concurrency-interest at cs.oswego.edu
> *????:* Re: [concurrency-interest] RFR: 
> 8065804:JEP171:Clarifications/corrections for fence intrinsics
> Roman,
> Can you point to any specific article providing the concurrency 
> problem statement with a further proof using linearizability to reason 
> about solution.
>
> Thanks,
> DT
>
> On 11/26/2014 2:59 AM, Roman Elizarov wrote:
>>
>> Whether IRIW has any _/practical/_ uses is definitely subject to 
>> debate. However, there is no tractable way for formal reasoning about 
>> properties of large concurrent systems, but via linearizability. 
>> Linearizability is the only property that is both local and 
>> hierarchical. It lets you build more complex linearizable algorithms 
>> from simpler ones, having quite succinct and compelling proofs at 
>> each step.
>>
>> In other words, if you want to be able to construct a formal proof 
>> that your [large] concurrent system if correct, then you must have 
>> IRIW consistency. Do you need a formal proof of correctness? Maybe 
>> not. In many applications hand-waving is enough,  but there are many 
>> other applications where hand-waving does not count as a proof. It 
>> may be possible to construct formal correctness proofs for some very 
>> simple algorithms even on a system that does not provide IRIW, but 
>> this is beyond the state of the art of formal verification for 
>> anything sufficiently complex.
>>
>> /Roman
>>
>> *From:*David Holmes [mailto:davidcholmes at aapt.net.au]
>> *Sent:* Wednesday, November 26, 2014 11:54 AM
>> *To:* Roman Elizarov; Hans Boehm
>> *Cc:* concurrency-interest at cs.oswego.edu; core-libs-dev
>> *Subject:* RE: [concurrency-interest] RFR: 
>> 8065804:JEP171:Clarifications/corrections for fence intrinsics
>>
>> Can you expand on that please. All previous discussion of IRIW I have 
>> seen indicated that the property, while a consequence of existing JMM 
>> rules, had no practical use.
>>
>> Thanks,
>>
>> David
>>
>>     -----Original Message-----
>>     *From:* Roman Elizarov [mailto:elizarov at devexperts.com]
>>     *Sent:* Wednesday, 26 November 2014 6:49 PM
>>     *To:* dholmes at ieee.org <mailto:dholmes at ieee.org>; Hans Boehm
>>     *Cc:* concurrency-interest at cs.oswego.edu
>>     <mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
>>     *Subject:* RE: [concurrency-interest] RFR:
>>     8065804:JEP171:Clarifications/corrections for fence intrinsics
>>
>>     There is no conceivable way to kill IRIW consistency requirement
>>     while retaining ability to prove correctness of large software
>>     systems. If IRIW of volatile variables are not consistent, then
>>     volatile reads and writes are not linearizable, which breaks
>>     linearizabiliy of all higher-level primitives build on top of
>>     them and makes formal reasoning about behavior of concurrent
>>     systems practically impossible. There are many fields where this
>>     is not acceptable.
>>
>>     /Roman
>>
>>     *From:*concurrency-interest-bounces at cs.oswego.edu
>>     <mailto:concurrency-interest-bounces at cs.oswego.edu>
>>     [mailto:concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of
>>     *David Holmes
>>     *Sent:* Wednesday, November 26, 2014 5:11 AM
>>     *To:* Hans Boehm
>>     *Cc:* concurrency-interest at cs.oswego.edu
>>     <mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
>>     *Subject:* Re: [concurrency-interest] RFR: 8065804:
>>     JEP171:Clarifications/corrections for fence intrinsics
>>
>>     Hi Hans,
>>
>>     Given IRIW is a thorn in everyone's side and has no known useful
>>     benefit, and can hopefully be killed off in the future, lets not
>>     get bogged down in IRIW. But none of what you say below relates
>>     to multi-copy-atomicity.
>>
>>     Cheers,
>>
>>     David
>>
>>         -----Original Message-----
>>         *From:* hjkhboehm at gmail.com
>>         <mailto:hjkhboehm at gmail.com>[mailto:hjkhboehm at gmail.com]*On
>>         Behalf Of *Hans Boehm
>>         *Sent:* Wednesday, 26 November 2014 12:04 PM
>>         *To:* dholmes at ieee.org <mailto:dholmes at ieee.org>
>>         *Cc:* Stephan Diestelhorst;
>>         concurrency-interest at cs.oswego.edu
>>         <mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
>>         *Subject:* Re: [concurrency-interest] RFR: 8065804:
>>         JEP171:Clarifications/corrections for fence intrinsics
>>
>>         To be concrete here, on Power, loads can normally be ordered
>>         by an address dependency or light-weight fence (lwsync).
>>         However, neither is enough to prevent the questionable
>>         outcome for IRIW, since it doesn't ensure that the stores in
>>         T1 and T2 will be made visible to other threads in a
>>         consistent order. That outcome can be prevented by using
>>         heavyweight fences (sync) instructions between the loads
>>         instead.  Peter Sewell's group concluded that to enforce
>>         correct volatile behavior on Power, you essentially need a a
>>         heavyweight fence between every pair of volatile operations
>>         on Power.  That cannot be understood based on simple ordering
>>         constraints.
>>
>>         As Stephan pointed out, there are similar issues on ARM, but
>>         they're less commonly encountered in a Java implementation.
>>         If you're lucky, you can get to the right implementation
>>         recipe by looking at only reordering, I think.
>>
>>         On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
>>         <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>>
>>         wrote:
>>
>>             Stephan Diestelhorst writes:
>>             >
>>             > David Holmes wrote:
>>             > > Stephan Diestelhorst writes:
>>             > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb
>>             Hans Boehm:
>>             > > > > I'm no hardware architect, but fundamentally it
>>             seems to me that
>>             > > > >
>>             > > > > load x
>>             > > > > acquire_fence
>>             > > > >
>>             > > > > imposes a much more stringent constraint than
>>             > > > >
>>             > > > > load_acquire x
>>             > > > >
>>             > > > > Consider the case in which the load from x is an
>>             L1 hit, but a
>>             > > > > preceding load (from say y) is a long-latency
>>             miss.  If we enforce
>>             > > > > ordering by just waiting for completion of prior
>>             operation, the
>>             > > > > former has to wait for the load from y to
>>             complete; while the
>>             > > > > latter doesn't.  I find it hard to believe that
>>             this doesn't leave
>>             > > > > an appreciable amount of performance on the
>>             table, at least for
>>             > > > > some interesting microarchitectures.
>>             > > >
>>             > > > I agree, Hans, that this is a reasonable
>>             assumption.  Load_acquire x
>>             > > > does allow roach motel, whereas the acquire fence
>>             does not.
>>             > > >
>>             > > > >  In addition, for better or worse, fencing
>>             requirements on at least
>>             > > > >  Power are actually driven as much by store
>>             atomicity issues, as by
>>             > > > >  the ordering issues discussed in the cookbook. 
>>             This was not
>>             > > > >  understood in 2005, and unfortunately doesn't
>>             seem to be
>>             > amenable to
>>             > > > >  the kind of straightforward explanation as in
>>             Doug's cookbook.
>>             > > >
>>             > > > Coming from a strongly ordered architecture to a
>>             weakly ordered one
>>             > > > myself, I also needed some mental adjustment about
>>             store (multi-copy)
>>             > > > atomicity.  I can imagine others will be unaware of
>>             this difference,
>>             > > > too, even in 2014.
>>             > >
>>             > > Sorry I'm missing the connection between fences and
>>             multi-copy
>>             > atomicity.
>>             >
>>             > One example is the classic IRIW. With non-multi copy
>>             atomic stores, but
>>             > ordered (say through a dependency) loads in the
>>             following example:
>>             >
>>             > Memory: foo = bar = 0
>>             > _T1_         _T2_         _T3_                       _T4_
>>             > st (foo),1   st (bar),1   ld r1, (bar)                 
>>                 ld r3,(foo)
>>             >                           <addr dep / local "fence"
>>             here>   <addr dep>
>>             >                           ld r2, (foo)                 
>>                 ld r4, (bar)
>>             >
>>             > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
>>             non-multi-copy atomic
>>             > machines.  On TSO boxes, this is not possible.  That
>>             means that the
>>             > memory fence that will prevent such a behaviour (DMB on
>>             ARM) needs to
>>             > carry some additional oomph in ensuring multi-copy
>>             atomicity, or rather
>>             > prevent you from seeing it (which is the same thing).
>>
>>             I take it as given that any code for which you may have
>>             ordering
>>             constraints, must first have basic atomicity properties
>>             for loads and
>>             stores. I would not expect any kind of fence to add
>>             multi-copy-atomicity
>>             where there was none.
>>
>>             David
>>
>>
>>             > Stephan
>>             >
>>             > _______________________________________________
>>             > Concurrency-interest mailing list
>>             > Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141201/800e3d3f/attachment-0001.html>

From dt at flyingtroika.com  Mon Dec  1 22:09:33 2014
From: dt at flyingtroika.com (DT)
Date: Mon, 01 Dec 2014 19:09:33 -0800
Subject: [concurrency-interest] check that volatile stores are not reordered
	with volatile loads
Message-ID: <547D2D6D.3090401@flyingtroika.com>

Folks,
How can we check/prove (use case) that java guarantees that volatile stores are not reordered with volatile loads?
Any points to the code will be grateful.

Thanks,
Dmitry


From dawid.weiss at gmail.com  Tue Dec  2 03:03:10 2014
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Tue, 2 Dec 2014 09:03:10 +0100
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAE-f1xTEUrSEbSuo95tN25fB14q8NN3GvaOdhY2+LLZCrRa2RA@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>
	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>
	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>
	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>
	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
	<CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
	<CAHjP37HLWYJaqgBYCRAf4uKqems57oR=eVHcT=uSLmqXcvaeEw@mail.gmail.com>
	<CAE-f1xRHZG8CLAoN+3yCTw79_p5En6PaAueOD73i_EfQL0B45Q@mail.gmail.com>
	<CA+kOe0_zPsfBKf1O+N4LSt4YXVQe_MqPO4MyYDb+ZCZ66ue4qA@mail.gmail.com>
	<CAHJZN-ve2kufowHzTz9_U_v6ckCAWOxDPv8H5FeGPuWTK+iVjg@mail.gmail.com>
	<CAE-f1xTEUrSEbSuo95tN25fB14q8NN3GvaOdhY2+LLZCrRa2RA@mail.gmail.com>
Message-ID: <CAM21Rt9UcAGnnFF82qYGorn+K4o2YbEqmoNa-1SY8b0uVm8Dbw@mail.gmail.com>

Allowing myself to CC Aleksey since all the suggestions in this
discussion would constitute a very interesting (and non-trivial?) use
case example for JMH....

Dawid

On Tue, Dec 2, 2014 at 1:45 AM, Charles Oliver Nutter
<headius at headius.com> wrote:
> On Mon, Dec 1, 2014 at 5:23 PM, Josh Humphries <jh at squareup.com> wrote:
>> A future probably won't do since it's single use, but for this case, control
>> needs to keep swapping between consumer and co-routine and back. I wonder if
>> a there's a way to get a Phaser to work (plus a volatile field to actually
>> store the item being transferred).
>
> Right, future is one-shot and I need this to be a generator.
>
> I tried to think through Phaser but at the time I could not use it
> without introducing an additional library dependency.
>
>> I've actually toyed with exactly this sort of thing, emulating
>> generators/co-routines in Java using multiple threads. My experiments used a
>> SynchronousQueue, and it was painfully slow.
>
> My experience as well. It was better than Exchanger, but not by much.
> ABQ was *much* faster.
>
>> BTW, it seems like one issue
>> with ArrayBlockingQueue(1) is that it's not a perfect exchange. You can have
>> a single element in the queue, unconsumed, instead of a synchronous
>> hand-off. I think that means that the generator thread is always one ahead
>> of the consumer thread. (Although maybe I'm imagining the rest of your
>> implementation incorrectly. And perhaps this fact doesn't strictly matter.)
>
> Every fiber round-trip involves two queues (basically, every
> transferrable entity has their own queue). The calling thread triggers
> the receiving thread by feeding it the next value via the receiving
> thread's queue (Fiber#resume in Ruby). It then waits on its own queue
> for a response. This allows one-to-one round trips as well as fiber
> "transfers" where a chain of them call each other, eventually ending
> up back at the initiator.
>
> I think I'll dig up my park/unpark-based impl and give it another
> shot. It works basically like you describe, with a volatile transfer
> field and explicit parking/unparking. I agree it will probably be the
> fastest option, if I can get it to play nicely with some other Ruby
> threading oddities (Thread#kill, Thread#raise... sigh).
>
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From paul.sandoz at oracle.com  Tue Dec  2 05:41:31 2014
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Tue, 2 Dec 2014 11:41:31 +0100
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <547D0EB2.3070802@cs.oswego.edu>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<547D0EB2.3070802@cs.oswego.edu>
Message-ID: <C8007E24-9D2F-4909-9340-C3E57E37F9BE@oracle.com>


On Dec 2, 2014, at 1:58 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/01/2014 03:46 PM, Martin Buchholz wrote:
>> David, Paul (i.e. Reviewers) and Doug,
>> 
>> I'd like to commit corrections so we make progress.
> 
> The current one looks OK to me.
> (http://cr.openjdk.java.net/~martin/webrevs/openjdk9/fence-intrinsics/)
> 

Same here, looks ok.

I anticipate we will be revisiting this area with the enhanced volatiles [1] work and related JMM updates, where there will be a public API for low-level enhanced field/array access [2].

As you rightly observed Unsafe does not currently have a get/read-acquire method. Implementations of [2] currently emulate that with a relaxed read + Unsafe.loadFence. It's something we need to add.

Paul.

[1] http://openjdk.java.net/jeps/193
[2] http://hg.openjdk.java.net/valhalla/valhalla/jdk/file/2d4531473a89/src/java.base/share/classes/java/lang/invoke/VarHandle.java
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141202/2336fdb2/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141202/2336fdb2/attachment.bin>

From oleksandr.otenko at oracle.com  Tue Dec  2 10:50:24 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 02 Dec 2014 15:50:24 +0000
Subject: [concurrency-interest] check that volatile stores are not
 reordered with volatile loads
In-Reply-To: <547D2D6D.3090401@flyingtroika.com>
References: <547D2D6D.3090401@flyingtroika.com>
Message-ID: <547DDFC0.4080806@oracle.com>

The following proves volatile stores and loads form a total order.

Thread 1:
r1=y;
r2=x;

Thread 2:
r3=x;
r4=y;

Thread 3: x=1
Thread 4: y=1

You shall never observe (r1,r2,r3,r4) = (1,0,1,0)


You can build a similar proof for intra-thread reordering:

Thread 1:
y=1;
r1=x;
z=1;

Thread 2:
r3=z;
x=1;
r2=y;

You shall never observe (r1,r2)=(0,0)
You shall never observe (r1,r3)=(1,1)


Alex


On 02/12/2014 03:09, DT wrote:
> Folks,
> How can we check/prove (use case) that java guarantees that volatile 
> stores are not reordered with volatile loads?
> Any points to the code will be grateful.
>
> Thanks,
> Dmitry
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Tue Dec  2 13:30:35 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 02 Dec 2014 13:30:35 -0500
Subject: [concurrency-interest] FJ stack bounds
In-Reply-To: <1417474984151-11552.post@n7.nabble.com>
References: <1417474984151-11552.post@n7.nabble.com>
Message-ID: <547E054B.3070407@cs.oswego.edu>

On 12/01/2014 06:03 PM, thurstonn wrote:
> Hello,
>
> I was wondering if there was any way to compute a maximum bound for the
> stack size/height (in frames, not bytes) for a given FJ problem.
>
> So given n = # of FJT created.
> Let's keep the math simple and say n = 256
>
> and your RecursiveTasks just split "themselves" into 2 separate
> RecursiveTasks which are then forked and joined (idiomatic FJ)
>
> At first glance, it would seem that this would be classic log(n) - but is
> it?  In the simple example that would mean a max stack height of 8.
>
> Given that worker threads engage in work-stealing (and combined with the
> fact that I can't tell from looking at the code what invariants there are
> (if any) in determining which actual FJT will be #exec()'d), is it possible
> that (given some unlucky and unlikely "timing"), that a single worker thread
> could effect a stack height much > 8? even, perhaps 256? (I realize this is
> unlikely, I'm just asking if it's possible)

The worst case under such scenarios is approximately
(lg(N) * (lg(N) - 1)) / 2 (base 2 logs). In your example, about
28 = 8 + 7 + 6 + ... 1).  A helping worker can steal a task
with depth greater than current task to join, then expand to
max depth before helping again, and so on up through max depth.
The "approximately" is due to ignoring possible transient
unrelated tasks.

>
> As best I could tell, there seems to be some logic (#doJoin/#tryUnpush) that
> "defaults" to executing the joined task only if it is at the top of the
> current thread's task stack (workerqueue), else . . . try to "help" by
> executing some other (arbitrarily chosen?) task.
> Does that mean that:
> left = . . .fork()
> right = . . . fork()  //Top of this thread's worker queue
>
> return left.join() + right.join()
> is different than
> return right.join() + left.join()
> in terms of the possible stack heights that can result?

Yes, but the main impact is on #threads not #tasks per queue.

FJ efficiently implements parallel dag computation. When
joins are performed in non-LIFO order of forks, performance
can suffer: The caller cannot necessarily help perform the
topmost task needed to uncover second, so may need to block
until some other thread does, which may require generating
extra spare threads, which can then cascade if new threads
also encounter misordered joins.

So, always join in LIFO order. If you forget which order is
which, use FJT.invoke(task1, task2), and/or its extensions
for multiple tasks, that remember this for you.

-Doug

From aleksey.shipilev at oracle.com  Wed Dec  3 04:41:26 2014
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 03 Dec 2014 12:41:26 +0300
Subject: [concurrency-interest] check that volatile stores are not
 reordered with volatile loads
In-Reply-To: <547DDFC0.4080806@oracle.com>
References: <547D2D6D.3090401@flyingtroika.com> <547DDFC0.4080806@oracle.com>
Message-ID: <547EDAC6.8000703@oracle.com>

On 12/02/2014 06:50 PM, Oleksandr Otenko wrote:
> The following proves volatile stores and loads form a total order.
> 
> Thread 1:
> r1=y;
> r2=x;
> 
> Thread 2:
> r3=x;
> r4=y;
> 
> Thread 3: x=1
> Thread 4: y=1
> 
> You shall never observe (r1,r2,r3,r4) = (1,0,1,0)

This is IRIW. A relevant test case:
 http://hg.openjdk.java.net/code-tools/jcstress/file/3b6a31d64cd3/tests-custom/src/main/java/org/openjdk/jcstress/tests/volatiles/VolatileIRIWTest.java

> You can build a similar proof for intra-thread reordering:
> 
> Thread 1:
> y=1;
> r1=x;
> z=1;
> 
> Thread 2:
> r3=z;
> x=1;
> r2=y;
> 
> You shall never observe (r1,r2)=(0,0)

This seems to be a variation of Dekker idiom (if you drop "z"), a
relevant test case:
 http://hg.openjdk.java.net/code-tools/jcstress/file/3b6a31d64cd3/tests-custom/src/main/java/org/openjdk/jcstress/tests/volatiles/DekkerTest.java

> You shall never observe (r1,r3)=(1,1)

True, this is the inverse of Dekker's idiom. jcstress has the
autogenerated test cases for volatile sequential consistency. The
pattern in question is L1_S2_L3__S3_L2_S1__Test.

-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141203/70a4e0d4/attachment.bin>

From dl at cs.oswego.edu  Wed Dec  3 07:10:24 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 03 Dec 2014 07:10:24 -0500
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>
	<CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
Message-ID: <547EFDB0.3080902@cs.oswego.edu>

On 12/01/2014 03:17 PM, Charles Oliver Nutter wrote:
> This logic is being used to emulate fibers (coroutines) in JRuby. A
> fiber belongs to a given thread, and there may be many active fibers
> within a thread, but none of them run concurrently. Every transfer
> into a fiber is an explicit hand-off.

Continuously blocking and unblocking a bunch of threads such
that only one of them is running at any given time is among
the slowest things you can do on a multicore these days, no
matter what synchronization scheme you use. I suspect that
even the slowest of the exception-based coroutine conversion
techniques would be a lot faster. (Googling "java coroutine"
finds some.) Better options should be available if/when JVMs
include mechanics for tail-calls that allow reuse of stack frames.
Until such support exists, there's no much we can do in j.u.c,
or anywhere in JDK, to help. But if you are operating at
the compiler/tool level (as you are for Ruby), you can still
perform code transformations that get most of these effects.

-Doug


From dl at cs.oswego.edu  Wed Dec  3 09:07:17 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 03 Dec 2014 09:07:17 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU95EhS1gmB7eVTckxyWRa_=iAotTmN8StjsWn6s9DaVMA@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>	<547B3ADD.4070700@cs.oswego.edu>	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>	<547C740B.1090903@cs.oswego.edu>
	<CANPzfU95EhS1gmB7eVTckxyWRa_=iAotTmN8StjsWn6s9DaVMA@mail.gmail.com>
Message-ID: <547F1915.40109@cs.oswego.edu>

On 12/01/2014 10:58 AM, ?iktor ?lang wrote:

> I guess it depends whether one wants that feature overridable (one can go from
> CompletionStage to CompletableFuture with the combinators)

Right. While overriding toCompletableFuture allows CompletionStage
authors to disable direct conversion, it doesn't prevent their
CompletionStage  users from finding less efficient ways of
performing blocking joins in particular. For example:
   CountdownLatch latch = ...;
   CompletableFuture cf = ....thenRun(() -> latch.countDown());
   latch.await();

Which leads me to re-wonder why any CompletionStage implementor
would ever bother to disable.

Digressing: I noticed while googling around for such constructions
that people sometimes use them to ensure that their main()
program using default-ayncs doesn't exit before computation
is complete. It would be better to use
ForkJoinPool.commonPool().awaitQuiescence(timeout, unit).
But this method has timeout arguments that people
probably don't want to think about in this case.
And the need for it is non-obvious to new users.
We should consider doing something about this.


>     But we can/do add new better stuff that we encourage people to use
>     instead of old stuff when it becomes less useful as the world changes.
>
>
> Hmmm, if only there was some kind of successor-like language to Java... :-)

Although any such language that specifically disallowed access to
JDK classes that you don't happen to like might not fare so well :-)

-Doug





From viktor.klang at gmail.com  Wed Dec  3 09:21:45 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 3 Dec 2014 15:21:45 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <547F1915.40109@cs.oswego.edu>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<CANPzfU95EhS1gmB7eVTckxyWRa_=iAotTmN8StjsWn6s9DaVMA@mail.gmail.com>
	<547F1915.40109@cs.oswego.edu>
Message-ID: <CANPzfU93WsQUsZ6faUOBitopRNmj0vf+PAWu-dEmMggR76PUJg@mail.gmail.com>

On Wed, Dec 3, 2014 at 3:07 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/01/2014 10:58 AM, ?iktor ?lang wrote:
>
>  I guess it depends whether one wants that feature overridable (one can go
>> from
>> CompletionStage to CompletableFuture with the combinators)
>>
>
> Right. While overriding toCompletableFuture allows CompletionStage
> authors to disable direct conversion, it doesn't prevent their
> CompletionStage  users from finding less efficient ways of
> performing blocking joins in particular. For example:
>   CountdownLatch latch = ...;
>   CompletableFuture cf = ....thenRun(() -> latch.countDown());
>   latch.await();
>
> Which leads me to re-wonder why any CompletionStage implementor
> would ever bother to disable.
>

Default impl would just create a -new- CompletableFuture and
CompletableFuture would override to return this?


>
> Digressing: I noticed while googling around for such constructions
> that people sometimes use them to ensure that their main()
> program using default-ayncs doesn't exit before computation
> is complete. It would be better to use
> ForkJoinPool.commonPool().awaitQuiescence(timeout, unit).
> But this method has timeout arguments that people
> probably don't want to think about in this case.
> And the need for it is non-obvious to new users.
> We should consider doing something about this.


I'm working on some hobby-research in how to supersede hardcoded timeouts.


>
>
>
>      But we can/do add new better stuff that we encourage people to use
>>     instead of old stuff when it becomes less useful as the world changes.
>>
>>
>> Hmmm, if only there was some kind of successor-like language to Java...
>> :-)
>>
>
> Although any such language that specifically disallowed access to
> JDK classes that you don't happen to like might not fare so well :-)


haha, true


>
>
> -Doug
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141203/9c02bbd4/attachment.html>

From thurston at nomagicsoftware.com  Wed Dec  3 17:33:29 2014
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 3 Dec 2014 15:33:29 -0700 (MST)
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <547C740B.1090903@cs.oswego.edu>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
Message-ID: <1417646009375-11569.post@n7.nabble.com>

So, I'll put in my 2c (might not even be worth that).

Would it really be so . . . unprincipled . . . to add a get() method to
CompletionStage (and overloaded variants)?  Or at least an isDone() method
(I'm assuming that that would *not* need to block, or wait)

I think that is what Josh is essentially advocating (cancel() is more
problematic I agree).

I guess I just think about writing tests; if you're going to expose
CompletionStage as return types from API methods, the most simple and
obvious way to test the implementation is:


CompletionStage do(some-input)


assert expected == do(...).get()

Viktor's suggested work-around:
val cf = new CompletableFuture<R>()
stage.whenComplete((r, e) -> if (e != null) cf.completeExceptionally(e) else
cf.complete(r))
cf.get()

is just evil IMHO.
Although the CDL code that Doug references is even worse.
What should the test code do, put in arbitrary Thread.sleeps()?  Blecch


And I think Josh's point that blocking (invoking get()) is *orthogonal* to
the ability for a reader/consumer to *write* the value of a computation,
which is what the admittedly "escape hatch" of #toCompletableFuture()
exposes, is correct.

I'm not dismissing Viktor's concerns, and you can be sure that there would
be engineers who would end up doing:

CompletionStage cs1 = . . .
result = cs1.get()

CompletionStage cs2 = . . . (result)
cs2.get()

and so on (which might result in inter-engineer violent crimes).

Maybe I just can't let go of bad habits, but although I admire the goal of
*never block*, at least in the applications I write, I'm not sure if it's
realistic (I'd love to eliminate side effects as well, but I need a
database, or to write to a socket, etc).

Even Erlang apps block sometimes (cf. gen_server:call())







--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/CompletableFuture-in-Java-8-tp11414p11569.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From viktor.klang at gmail.com  Wed Dec  3 18:32:50 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 4 Dec 2014 00:32:50 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <1417646009375-11569.post@n7.nabble.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
Message-ID: <CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>

Hi thurstonn!

On Wed, Dec 3, 2014 at 11:33 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> So, I'll put in my 2c (might not even be worth that).
>
> Would it really be so . . . unprincipled . . . to add a get() method to
> CompletionStage (and overloaded variants)?


I'd say so, given the experience I have with trying to get people of the
blocking-addiction.
And if we didn't have `toCompletableFuture` and you sorely required it, it
is 1 utility method await:

def toCompletableFuture[R](stage: CompletionStage[R]): CompletableFuture[R]
= {
  val cf = new CompletableFuture<R>()
  stage.whenComplete((r, e) => if (e != null) cf.completeExceptionally(e)
else cf.complete(r))
  cf
}

toCompletableFuture(stage).get()


> Or at least an isDone() method
> (I'm assuming that that would *not* need to block, or wait)
>

What's the use-case, if you don't mind?


>
> I think that is what Josh is essentially advocating (cancel() is more
> problematic I agree).
>
> I guess I just think about writing tests; if you're going to expose
> CompletionStage as return types from API methods, the most simple and
> obvious way to test the implementation is:
>
>
> CompletionStage do(some-input)
>
>
> assert expected == do(...).get()
>

I hope that test cases don't use `assert` (keyword) but some kind of test
framework (JUnit, TestNG, or any other similar),
in which case it is, if not already present, straight-forward to do:

awaitAssert(5, SECONDS, do(?), equals(expected))

or even:

within(5, SECONDS, do(?), (result) -> equals(expected, result))


>
> Viktor's suggested work-around:
> val cf = new CompletableFuture<R>()
> stage.whenComplete((r, e) -> if (e != null) cf.completeExceptionally(e)
> else
> cf.complete(r))
> cf.get()
>
> is just evil IMHO.
>

Out of curiosity, what about it do you find evil?


> Although the CDL code that Doug references is even worse.
> What should the test code do, put in arbitrary Thread.sleeps()?  Blecch
>

(Thread.sleep _is_ evil! :) ), No what it should do is to register a
completion signal to the CompletionStage and then add a timeout when to
fail the test-case. Does not need any blocking whatsoever SFAICS.


> And I think Josh's point that blocking (invoking get()) is *orthogonal* to
> the ability for a reader/consumer to *write* the value of a computation,
>

Now that I think we can all agree on. But that was not how he phrased it
AFAICT.


> which is what the admittedly "escape hatch" of #toCompletableFuture()
> exposes, is correct.
>
> I'm not dismissing Viktor's concerns, and you can be sure that there would
> be engineers who would end up doing:
>

Trust me, they are more common than you think!


>
> CompletionStage cs1 = . . .
> result = cs1.get()
>
> CompletionStage cs2 = . . . (result)
> cs2.get()
>
> and so on (which might result in inter-engineer violent crimes).
>

With those unbounded waits you'll probably see an Ops-person or two join
into the fray.


>
> Maybe I just can't let go of bad habits, but although I admire the goal of
> *never block*, at least in the applications I write, I'm not sure if it's
> realistic (I'd love to eliminate side effects as well, but I need a
> database, or to write to a socket, etc).
>

I'm not sure there's any isomorphism between the necessity of side-effects
and blocking.

(keep in mind that the definition of blocking we're using here is limited
to putting a Thread into BLOCKED, WAITING or TIMED_WAITING states)


>
> Even Erlang apps block sometimes (cf. gen_server:call())
>

I'm unsure whether anybody has ever claimed Erlang to be fully non-blocking
:-)


>
>
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/CompletableFuture-in-Java-8-tp11414p11569.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141204/a248fd1b/attachment.html>

From jh at squareup.com  Thu Dec  4 13:11:41 2014
From: jh at squareup.com (Josh Humphries)
Date: Thu, 4 Dec 2014 13:11:41 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
Message-ID: <CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>

>
>
> And I think Josh's point that blocking (invoking get()) is *orthogonal* to
>> the ability for a reader/consumer to *write* the value of a computation,
>>
>
> Now that I think we can all agree on. But that was not how he phrased it
> AFAICT.
>

Admittedly, I didn't use exactly that phrase. But that is precisely what I
meant when I wrote this:

"So to me, splitting imperative completion and task-based implicit
completion into different interfaces is a different concern than splitting
blocking and non-blocking forms of consumption."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141204/354eff62/attachment.html>

From viktor.klang at gmail.com  Thu Dec  4 13:13:55 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 4 Dec 2014 19:13:55 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
Message-ID: <CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>

On Thu, Dec 4, 2014 at 7:11 PM, Josh Humphries <jh at squareup.com> wrote:

>
>> And I think Josh's point that blocking (invoking get()) is *orthogonal* to
>>> the ability for a reader/consumer to *write* the value of a computation,
>>>
>>
>> Now that I think we can all agree on. But that was not how he phrased it
>> AFAICT.
>>
>
> Admittedly, I didn't use exactly that phrase. But that is precisely what I
> meant when I wrote this:
>
> "So to me, splitting imperative completion and task-based implicit
> completion into different interfaces is a different concern than splitting
> blocking and non-blocking forms of consumption."
>

Thanks for clarifying. What I commented on was that mixing concerns seemed
appropriate in one case, and discouraged in the other, without any
rationale as to why that was OK for one thing but not the other. (I'm still
very much interested in this)




-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141204/f1819aba/attachment.html>

From jh at squareup.com  Thu Dec  4 23:30:13 2014
From: jh at squareup.com (Josh Humphries)
Date: Thu, 4 Dec 2014 23:30:13 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
Message-ID: <CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>

Hey, Viktor. I think I've touched on some of this already. But, since you
said you're very much interested, I'll elaborate on my thinking.


Every decision is a trade-off. Mixing concerns can bloat the API and
increase the cognitive burden of using it, but it can also provide greater
functionality or make certain patterns of use easier. While the two
concerns we're discussing may seem similar, they are very different (at
least to me) regarding what they are actually providing to the developer,
so the trade-offs are different.


Concern #1: Exposing methods to imperatively complete the future vs. having
the future's value be provided implicitly (by the running of some unit of
logic). We're not really talking about mixing the two here. My objection
was that CompletionStage#toCompletableFuture leaks the imperative style in
a way that is simply inappropriate. So my objection here is about poor
encapsulation/abstraction. If the API had returned FutureTask, that too
would have been bad. (I also griped about the lack of a FutureTask-like
implementation of CompletionStage, but that is really a nit; not a major
complaint.)

As far as inter-op with legacy APIs, a #toFuture() method would have been
much better for a few reasons:

   1. Future is an interface, so a *view* could be returned instead of
   having to create a new stateful object that must be kept in sync with the
   original.
   2. Future provides inter-op but doesn't leak complete*/obtrude* methods
   (the heart of my objections)
   3. It could have been trivially implemented as a default method that
   just returns a CompletableFuture that is set from a #whenComplete stage,
   just as you've described.

(I'm pretty certain we agree on #1. At least most of it.)


Concern #2: Exposing non-blocking mechanisms to consume/use a future value
vs. blocking mechanisms. Mixing these is a very different beast from the
above. It isn't poor encapsulation/abstraction as neither has anything to
do with how the value is actually produced. Instead, one is a simple
derivative of the other (e.g. given a non-blocking mechanism, a blocking
mechanism can always be implemented on top of it). Providing both
facilitates simple, synchronous usage where it's appropriate (without
boiler-plate) and asynchronous non-blocking usage where it isn't.

You've already clearly expressed the opinion that blocking code is never
appropriate. I think that's a reasonable assertion for many contexts, just
not the JRE. Avoiding blocking altogether in core Java APIs is neither
realistic nor (again, IMO) desirable.

BTW, a #toFuture method, as I described above, would have been fine with me
as a means of achieving the properties I want in the API. It allows
blocking consumption without boiler-plate (stage.toFuture().get()) and it
does not leak implementation details of the Future to clients.


(Long-winded tangent ahead. Apologies if it sounds like a lecture.)

There is a spectrum. On one end (let's call it "simple"), you want a
programming model that makes it easier to write correct code and that is
easy to read, write, understand, and troubleshoot (at the extreme: all
synchronous, all blocking flows -- very simple to understand but will often
have poor performance and is incapable of taking advantage of today's
multi-core computers). On the other end ("performance"), you want a
programming model that enables taking maximum advantage of hardware,
provides greater efficiency, and facilitates better performance (greater
throughput, lower latency).

If we're being pragmatic, economics is the real decider for where on the
spectrum will strike the right balance. On the simple side, there's an
advantage to spending less in engineer-hours: most developers are more
productive writing simple synchronous code, and that style of code is much
easier to debug. But this can incur greater capital costs since it may
require more hardware to do the same job. On the performance side, it's the
converse: get more out of the money spent on compute resources, but
potentially spend more in engineering effort. (There are obviously other
constraints, too, like whether a particular piece of software has any value
at all if it can't meet certain performance requirements.)

My experience is that most organizations find their maxima close to the
middle, but nearer to the simple side. So there is an economic advantage
for them to focus a little more on developer productivity than on software
efficiency and performance.

I want my users to be as productive as possible, even if it means they
write blocking code. (And, let's face it, some of them will commit
atrocities far worse than just using a blocking API.)


(A bit of an aside: I know it's from 2008, but still relevant:
http://www.mailinator.com/tymaPaulMultithreaded.pdf)



----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Thu, Dec 4, 2014 at 1:13 PM, ?iktor ?lang <viktor.klang at gmail.com> wrote:

>
>
> On Thu, Dec 4, 2014 at 7:11 PM, Josh Humphries <jh at squareup.com> wrote:
>
>>
>>> And I think Josh's point that blocking (invoking get()) is *orthogonal*
>>>> to
>>>> the ability for a reader/consumer to *write* the value of a computation,
>>>>
>>>
>>> Now that I think we can all agree on. But that was not how he phrased it
>>> AFAICT.
>>>
>>
>> Admittedly, I didn't use exactly that phrase. But that is precisely what
>> I meant when I wrote this:
>>
>> "So to me, splitting imperative completion and task-based implicit
>> completion into different interfaces is a different concern than splitting
>> blocking and non-blocking forms of consumption."
>>
>
> Thanks for clarifying. What I commented on was that mixing concerns seemed
> appropriate in one case, and discouraged in the other, without any
> rationale as to why that was OK for one thing but not the other. (I'm still
> very much interested in this)
>
>
>
>
> --
> Cheers,
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141204/52fe7aae/attachment-0001.html>

From viktor.klang at gmail.com  Fri Dec  5 08:41:34 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 5 Dec 2014 14:41:34 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
Message-ID: <CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>

Hey Josh,

On Fri, Dec 5, 2014 at 5:30 AM, Josh Humphries <jh at squareup.com> wrote:
>
> Hey, Viktor. I think I've touched on some of this already. But, since you
said you're very much interested, I'll elaborate on my thinking.
>

Thanks for taking the time and spending the effort to elaborate, Josh, I
really appreciate it!

Rereading my reply I notice that we have strayed a bit from the initial
discussion, but it is an interesting topic so I'll share my thoughts on the
topic.

TL; DR: I think we both agree but have different cutoff points. :)

>
>
> Every decision is a trade-off. Mixing concerns can bloat the API and
increase the cognitive burden of using it, but it can also provide greater
functionality or make certain patterns of use easier. While the two
concerns we're discussing may seem similar, they are very different (at
least to me) regarding what they are actually providing to the developer,
so the trade-offs are different.

Agreed. My stance is to error on the side of Single Responsibility
Principle, and it is easier to add API if defensible than deprecate and
remove (remove has never happened in the JDK AFAICT).

>
>
>
> Concern #1: Exposing methods to imperatively complete the future vs.
having the future's value be provided implicitly (by the running of some
unit of logic). We're not really talking about mixing the two here. My
objection was that CompletionStage#toCompletableFuture leaks the imperative
style in a way that is simply inappropriate.

I think both I and Doug(?) agree here, the problem is that there's no
protected scope for interface methods, so CompletableFuture would have to
wrap every CompletionStage that isn't a CompletableFuture, leading to a lot
of allocations for the worst case. Doug would be able to share more about
that.

>
> So my objection here is about poor encapsulation/abstraction. If the API
had returned FutureTask, that too would have been bad. (I also griped about
the lack of a FutureTask-like implementation of CompletionStage, but that
is really a nit; not a major complaint.)

Personally I don't mind here, it's beyond trivial to "submit" a
CompletableFuture. But YYMV.

And with CompletableFuture you have a choice if you want to expose it,
CompletionStage or Future depending on what capabilities you want to
expose, which does sound quite flexible?

>
>
> As far as inter-op with legacy APIs, a #toFuture() method would have been
much better for a few reasons:

I think again that the toCompletableFuture, as far as I can see, was
primarily needed for CompletableFuture.

>
> Future is an interface, so a *view* could be returned instead of having
to create a new stateful object that must be kept in sync with the
original.
>
> Future provides inter-op but doesn't leak complete*/obtrude* methods (the
heart of my objections)
> It could have been trivially implemented as a default method that just
returns a CompletableFuture that is set from a #whenComplete stage, just as
you've described.
> (I'm pretty certain we agree on #1. At least most of it.)

I would have much preferred to have a static method on Future called
"fromCompletionStage" so that CompletionStages do not need to know about
"the world". :-)

>
>
>
> Concern #2: Exposing non-blocking mechanisms to consume/use a future
value vs. blocking mechanisms. Mixing these is a very different beast from
the above. It isn't poor encapsulation/abstraction as neither has anything
to do with how the value is actually produced.

This I completely agree with, with the exception that I see mixing APIs
(blocking APIs and non-blocking APIs) as mixing concerns.

>
> Instead, one is a simple derivative of the other (e.g. given a
non-blocking mechanism, a blocking mechanism can always be implemented on
top of it).

Yes, and as such, I argue that if one wants to block, one can always do
that when given non-blocking APIs. Especially in the face of
Future.fromCompletionStage.

>
> Providing both facilitates simple, synchronous usage where it's
appropriate (without boiler-plate) and asynchronous non-blocking usage
where it isn't.

I think the problem here, for me, is "when appropriate". I'd argue that it
is so rarely appropriate that making it more of a hassle is worth it.
Blocking needs to be justified as it can lead to liveness problems (and
performance problems).

>
>
> You've already clearly expressed the opinion that blocking code is never
appropriate. I think that's a reasonable assertion for many contexts, just
not the JRE. Avoiding blocking altogether in core Java APIs is neither
realistic nor (again, IMO) desirable.

Would you mind expanding on "just not the JRE"?
My view is that java.util.concurrent is about tools to facilitate
concurrent programming mainly targeted towards advanced users and library
writers.
Perhaps it is here we have different views?

>
>
> BTW, a #toFuture method, as I described above, would have been fine with
me as a means of achieving the properties I want in the API. It allows
blocking consumption without boiler-plate (stage.toFuture().get()) and it
does not leak implementation details of the Future to clients.

def toFuture[T](stage: CompletionStage[T]): Future[T] =
stage.toCompletableFuture()

Now it doesn't leak? (I would have preferred to not have the "to"-method on
CompletionStage at all)

>
>
>
> (Long-winded tangent ahead. Apologies if it sounds like a lecture.)

Don't worry about it, I'll take lectures any day over silence :)

>
>
> There is a spectrum. On one end (let's call it "simple"), you want a
programming model that makes it easier to write correct code and that is
easy to read, write, understand, and troubleshoot
>
> (at the extreme: all synchronous, all blocking flows -- very simple to
understand but will often have poor performance and is incapable of taking
advantage of today's multi-core computers). On the other end
("performance"), you want a programming model that enables taking maximum
advantage of hardware, provides greater efficiency, and facilitates better
performance (greater throughput, lower latency).

I think you may be conflating "simple" and "easy":
http://www.infoq.com/presentations/Simple-Made-Easy

To me, personally, it is mostly about performance, because that's what I
need. But for my users, it is important that one can reason about how the
code will behave.

I'll argue that async monadic-style programming is -simpler- than the
blocking equivalent, yes, it may sound extremely weird at first thought but
hear me out:

Let's take these two sections of code:

def addSync(f1: j.u.c.Future[Int], f2: j.u.c.Future[Int]): Int = f1.get() +
f2.get()

Questions:
1) When is it safe to call `addSync`?
2) How do I, as the caller of `addSync` know when it is safe to call
`addSync`?
3) Will my program be able to run to completion if I call `addSync`?
4) How much code do I need to change if `addSync` causes performance or
liveness problems?

def addAsync(f1: AsyncFuture[Int], f2: AsyncFuture[Int))(implicit e:
Executor): AsyncFuture[Int] = f1 zip f2 map {_ + _}

Questions:
1) When is it safe to call `addAsync`?
2) How do I, as the caller of `addAsync` know when it is safe to call
`addAsync`?
3) Will my program be able to run to completion if I call `addAsync`?

In my experience (as a contributor to Akka for 5 years, and the co-author
of Futures & Promises for Scala), the biggest risk with adding blocking
APIs (Akka Futures had blocking APIs -on- the Future itself, Scala has it
externally on a utility called Await, which fortunately employs managed
blocking to try to reduce the risk of liveness problems at the expense of
performance) I can safely say that most people will fall back on what they
know, if that is easier (less effort) than learning something new. There's
nothing -wrong- about that, it's just human nature! However, knowing that,
we must take that into consideration and make it easier (less of an effort)
to learn new things, especially if it leads to better programs
(maintainability, performance etc).

When the blocking methods were built into the Future itself (in Akka
originally), it was one of the biggest sources of problems reported
(related to Futures).
When the blocking methods were externalized (in scala.concurrent), it is
still one of the biggest sources of problems reported (related to Futures).

Again, this is just my experience on the topic, so YMMV!

>
>
> If we're being pragmatic, economics is the real decider for where on the
spectrum will strike the right balance. On the simple side, there's an
advantage to spending less in engineer-hours: most developers are more
productive writing simple synchronous code, and that style of code is much
easier to debug. But this can incur greater capital costs since it may
require more hardware to do the same job. On the performance side, it's the
converse: get more out of the money spent on compute resources, but
potentially spend more in engineering effort. (There are obviously other
constraints, too, like whether a particular piece of software has any value
at all if it can't meet certain performance requirements.)

I understand and can sympathize with this view. But I think that it is more
complex than that, it is essentially trading away quick short term gain for
long-term loss. (Debugging deadlocks and concurrency issues more than often
cost more in developer time, and disrupting production systems than the
gain in initially writing the code.)

"It is easier to serve people desserts than greens, the question is what is
healthier." :)

>
>
> My experience is that most organizations find their maxima close to the
middle, but nearer to the simple side. So there is an economic advantage
for them to focus a little more on developer productivity than on software
efficiency and performance.

For the user, I worry more about correctness and liveness than performance.
The performance concerns is for me as a library writer (as the users can't
opt out of bad library performance).

>
>
> I want my users to be as productive as possible, even if it means they
write blocking code. (And, let's face it, some of them will commit
atrocities far worse than just using a blocking API.)

I understand this line of reasoning but it always has to be qualified.
For example, the allure of RoR was that of high initial productivity, but
what was sacrificed was both performance, maintainability and scalability.
So we need to not only consider short term "gains" but also long term
"losses".


>
>
>
> (A bit of an aside: I know it's from 2008, but still relevant:
http://www.mailinator.com/tymaPaulMultithreaded.pdf)
>
>
>
> ----
> Josh Humphries
> Manager, Shared Systems  |  Platform Engineering
> Atlanta, GA  |  678-400-4867
> Square (www.squareup.com)
>
> On Thu, Dec 4, 2014 at 1:13 PM, ?iktor ?lang <viktor.klang at gmail.com>
wrote:
>>
>>
>>
>> On Thu, Dec 4, 2014 at 7:11 PM, Josh Humphries <jh at squareup.com> wrote:
>>>>
>>>>
>>>>> And I think Josh's point that blocking (invoking get()) is
*orthogonal* to
>>>>> the ability for a reader/consumer to *write* the value of a
computation,
>>>>
>>>>
>>>> Now that I think we can all agree on. But that was not how he phrased
it AFAICT.
>>>
>>>
>>> Admittedly, I didn't use exactly that phrase. But that is precisely
what I meant when I wrote this:
>>>
>>> "So to me, splitting imperative completion and task-based implicit
completion into different interfaces is a different concern than splitting
blocking and non-blocking forms of consumption."
>>
>>
>> Thanks for clarifying. What I commented on was that mixing concerns
seemed appropriate in one case, and discouraged in the other, without any
rationale as to why that was OK for one thing but not the other. (I'm still
very much interested in this)
>>
>>
>>
>>
>> --
>> Cheers,
>> ?
>
>

-- 
Cheers,
?
Hey, Viktor. I think I've touched on some of this already. But, since you
said you're very much interested, I'll elaborate on my thinking.


Every decision is a trade-off. Mixing concerns can bloat the API and
increase the cognitive burden of using it, but it can also provide greater
functionality or make certain patterns of use easier. While the two
concerns we're discussing may seem similar, they are very different (at
least to me) regarding what they are actually providing to the developer,
so the trade-offs are different.


Concern #1: Exposing methods to imperatively complete the future vs. having
the future's value be provided implicitly (by the running of some unit of
logic). We're not really talking about mixing the two here. My objection
was that CompletionStage#toCompletableFuture leaks the imperative style in
a way that is simply inappropriate. So my objection here is about poor
encapsulation/abstraction. If the API had returned FutureTask, that too
would have been bad. (I also griped about the lack of a FutureTask-like
implementation of CompletionStage, but that is really a nit; not a major
complaint.)

As far as inter-op with legacy APIs, a #toFuture() method would have been
much better for a few reasons:

   1. Future is an interface, so a *view* could be returned instead of
   having to create a new stateful object that must be kept in sync with the
   original.
   2. Future provides inter-op but doesn't leak complete*/obtrude* methods
   (the heart of my objections)
   3. It could have been trivially implemented as a default method that
   just returns a CompletableFuture that is set from a #whenComplete stage,
   just as you've described.

(I'm pretty certain we agree on #1. At least most of it.)


Concern #2: Exposing non-blocking mechanisms to consume/use a future value
vs. blocking mechanisms. Mixing these is a very different beast from the
above. It isn't poor encapsulation/abstraction as neither has anything to
do with how the value is actually produced. Instead, one is a simple
derivative of the other (e.g. given a non-blocking mechanism, a blocking
mechanism can always be implemented on top of it). Providing both
facilitates simple, synchronous usage where it's appropriate (without
boiler-plate) and asynchronous non-blocking usage where it isn't.

You've already clearly expressed the opinion that blocking code is never
appropriate. I think that's a reasonable assertion for many contexts, just
not the JRE. Avoiding blocking altogether in core Java APIs is neither
realistic nor (again, IMO) desirable.

BTW, a #toFuture method, as I described above, would have been fine with me
as a means of achieving the properties I want in the API. It allows
blocking consumption without boiler-plate (stage.toFuture().get()) and it
does not leak implementation details of the Future to clients.


(Long-winded tangent ahead. Apologies if it sounds like a lecture.)

There is a spectrum. On one end (let's call it "simple"), you want a
programming model that makes it easier to write correct code and that is
easy to read, write, understand, and troubleshoot (at the extreme: all
synchronous, all blocking flows -- very simple to understand but will often
have poor performance and is incapable of taking advantage of today's
multi-core computers). On the other end ("performance"), you want a
programming model that enables taking maximum advantage of hardware,
provides greater efficiency, and facilitates better performance (greater
throughput, lower latency).

If we're being pragmatic, economics is the real decider for where on the
spectrum will strike the right balance. On the simple side, there's an
advantage to spending less in engineer-hours: most developers are more
productive writing simple synchronous code, and that style of code is much
easier to debug. But this can incur greater capital costs since it may
require more hardware to do the same job. On the performance side, it's the
converse: get more out of the money spent on compute resources, but
potentially spend more in engineering effort. (There are obviously other
constraints, too, like whether a particular piece of software has any value
at all if it can't meet certain performance requirements.)

My experience is that most organizations find their maxima close to the
middle, but nearer to the simple side. So there is an economic advantage
for them to focus a little more on developer productivity than on software
efficiency and performance.

I want my users to be as productive as possible, even if it means they
write blocking code. (And, let's face it, some of them will commit
atrocities far worse than just using a blocking API.)


(A bit of an aside: I know it's from 2008, but still relevant:
http://www.mailinator.com/tymaPaulMultithreaded.pdf)



----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Thu, Dec 4, 2014 at 1:13 PM, ?iktor ?lang <viktor.klang at gmail.com> wrote:

>
>
> On Thu, Dec 4, 2014 at 7:11 PM, Josh Humphries <jh at squareup.com> wrote:
>
>>
>>> And I think Josh's point that blocking (invoking get()) is *orthogonal*
>>>> to
>>>> the ability for a reader/consumer to *write* the value of a computation,
>>>>
>>>
>>> Now that I think we can all agree on. But that was not how he phrased it
>>> AFAICT.
>>>
>>
>> Admittedly, I didn't use exactly that phrase. But that is precisely what
>> I meant when I wrote this:
>>
>> "So to me, splitting imperative completion and task-based implicit
>> completion into different interfaces is a different concern than splitting
>> blocking and non-blocking forms of consumption."
>>
>
> Thanks for clarifying. What I commented on was that mixing concerns seemed
> appropriate in one case, and discouraged in the other, without any
> rationale as to why that was OK for one thing but not the other. (I'm still
> very much interested in this)
>
>
>
>
> --
> Cheers,
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141205/c5a36cf7/attachment-0001.html>

From dl at cs.oswego.edu  Fri Dec  5 09:41:05 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 05 Dec 2014 09:41:05 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>	<547B3ADD.4070700@cs.oswego.edu>	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>	<547C740B.1090903@cs.oswego.edu>	<1417646009375-11569.post@n7.nabble.com>	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
Message-ID: <5481C401.6080207@cs.oswego.edu>

On 12/04/2014 11:30 PM, Josh Humphries wrote:
> I'll elaborate on my thinking....

Thanks. It hadn't occurred to me that anyone would consider
CompletionStage.toCompletableFuture to be any less desirable
than Collection.toArray (which motivated toCompletableFuture).
Both provide an efficient base interoperation mechanism.
As mentioned, we considered other ways to do this, but all
seem worse except perhaps on their attitudinal impact,
which I hadn't considered. I'm not sure what we can or
should do about this now though.


> There is a spectrum. On one end (let's call it "simple"), you want a programming
> model that makes it easier to write correct code and that is easy to read,
> write, understand, and troubleshoot (at the extreme: all synchronous, all
> blocking flows -- very simple to understand

The goal of the "structure async" reactive approach (of which
CompletableFuture is one part) is to make these even simpler and
easier to use than traditional constructions, at least once
you get used to them. (Which seems plausible: Notice the success
of Node.js, based on impoverished forms of promises/futures
in JavaScript.)

As mentioned in other posts, development is still chaotic:
there are multiple frameworks with overlapping functionality,
and uncoordinated development of structured-async versions
of blocking components. Plus we face continuing challenges to
address resource problems when people mix blocking and
async components.

-Doug


From Sebastian.Millies at softwareag.com  Fri Dec  5 09:46:43 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Fri, 5 Dec 2014 14:46:43 +0000
Subject: [concurrency-interest] CompletableFuture in Java 8
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD88B75A@HQMBX5.eur.ad.sag>

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Josh Humphries
Sent: Friday, December 05, 2014 5:30 AM
To: ?iktor ?lang
Cc: thurstonn; concurrency-interest
Subject: Re: [concurrency-interest] CompletableFuture in Java 8

[snip]
As far as inter-op with legacy APIs, a #toFuture() method would have been much better for a few reasons:

  1.  Future is an interface, so a *view* could be returned instead of having to create a new stateful object that must be kept in sync with the original.
  2.  Future provides inter-op but doesn't leak complete*/obtrude* methods (the heart of my objections)
  3.  It could have been trivially implemented as a default method that just returns a CompletableFuture that is set from a #whenComplete stage, just as you've described.
[snip
In this context it would have been nice if Future had been provided with a default definition for join(). The exception handling with get() is a pain.

n  Sebastian




Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141205/07f288cf/attachment.html>

From jh at squareup.com  Fri Dec  5 11:07:32 2014
From: jh at squareup.com (Josh Humphries)
Date: Fri, 5 Dec 2014 11:07:32 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
Message-ID: <CAHJZN-sGQzJFrWnKeoZONjOWiHYeVD=hZ4k6rZMV1ez3cUQ=aw@mail.gmail.com>

On Fri, Dec 5, 2014 at 8:41 AM, ?iktor ?lang <viktor.klang at gmail.com> wrote:

> Hey Josh,
>
> On Fri, Dec 5, 2014 at 5:30 AM, Josh Humphries <jh at squareup.com> wrote:
> >
> > Hey, Viktor. I think I've touched on some of this already. But, since
> you said you're very much interested, I'll elaborate on my thinking.
> >
>
> Thanks for taking the time and spending the effort to elaborate, Josh, I
> really appreciate it!
>
> Rereading my reply I notice that we have strayed a bit from the initial
> discussion, but it is an interesting topic so I'll share my thoughts on the
> topic.
>
> TL; DR: I think we both agree but have different cutoff points. :)
>

Indeed. I think that sums up the difference pretty well :)


> > Every decision is a trade-off. Mixing concerns can bloat the API and
> increase the cognitive burden of using it, but it can also provide greater
> functionality or make certain patterns of use easier. While the two
> concerns we're discussing may seem similar, they are very different (at
> least to me) regarding what they are actually providing to the developer,
> so the trade-offs are different.
>
> Agreed. My stance is to error on the side of Single Responsibility
> Principle, and it is easier to add API if defensible than deprecate and
> remove (remove has never happened in the JDK AFAICT).
>
> >
> > Concern #1: Exposing methods to imperatively complete the future vs.
> having the future's value be provided implicitly (by the running of some
> unit of logic). We're not really talking about mixing the two here. My
> objection was that CompletionStage#toCompletableFuture leaks the imperative
> style in a way that is simply inappropriate.
>
> I think both I and Doug(?) agree here, the problem is that there's no
> protected scope for interface methods, so CompletableFuture would have to
> wrap every CompletionStage that isn't a CompletableFuture, leading to a lot
> of allocations for the worst case. Doug would be able to share more about
> that.
>

But CompletableFuture is a class, not an interface. So if the
CompletionStage is not a CompletableFuture, you're still back to having to
wrap the CompletionStage. You've just moved the responsibility out of
CompletableFuture and into every other implementation of CompletionStage,
which seems like an unusual choice.

>
> > So my objection here is about poor encapsulation/abstraction. If the API
> had returned FutureTask, that too would have been bad. (I also griped about
> the lack of a FutureTask-like implementation of CompletionStage, but that
> is really a nit; not a major complaint.)
>
> Personally I don't mind here, it's beyond trivial to "submit" a
> CompletableFuture. But YYMV.
>
> And with CompletableFuture you have a choice if you want to expose it,
> CompletionStage or Future depending on what capabilities you want to
> expose, which does sound quite flexible?
>

Sure. I had prefaced my whole original rant with the fact that these are
nits and acknowledged that everything that is needed is there. In fact,
I've written everything I need on top of the existing APIs (but found it
annoying that my implementation of CompletionStage has a method that
requires wrapping the stage in a CompletableFuture).

So my points are really about the aesthetics of the API (which, admittedly,
are often subjective).

> >
> > As far as inter-op with legacy APIs, a #toFuture() method would have
> been much better for a few reasons:
>
> I think again that the toCompletableFuture, as far as I can see, was
> primarily needed for CompletableFuture.
>
> >
> > Future is an interface, so a *view* could be returned instead of having
> to create a new stateful object that must be kept in sync with the
> original.
> >
> > Future provides inter-op but doesn't leak complete*/obtrude* methods
> (the heart of my objections)
> > It could have been trivially implemented as a default method that just
> returns a CompletableFuture that is set from a #whenComplete stage, just as
> you've described.
> > (I'm pretty certain we agree on #1. At least most of it.)
>
> I would have much preferred to have a static method on Future called
> "fromCompletionStage" so that CompletionStages do not need to know about
> "the world". :-)
>
That, too, is completely reasonable :)
I think the discoverability point has already been brought up. But that's
what @see tags in Javadoc are for, right?


> >
> > You've already clearly expressed the opinion that blocking code is never
> appropriate. I think that's a reasonable assertion for many contexts, just
> not the JRE. Avoiding blocking altogether in core Java APIs is neither
> realistic nor (again, IMO) desirable.
>
> Would you mind expanding on "just not the JRE"?
> My view is that java.util.concurrent is about tools to facilitate
> concurrent programming mainly targeted towards advanced users and library
> writers.
> Perhaps it is here we have different views?
>
I'll back-pedal a little and qualify "not in the JRE" with "not in certain
parts of the JRE".

Many of the APIs in that package really *are* for advanced users and
library writers, just like you said. But ExecutorService,
ScheduledExecutorService, and Future are really basic and are the kind of
building blocks that even developers writing business logic will need/want
to use. There are plenty of cases where it makes sense to provide
purpose-built libraries that wrap them. But there are plenty that don't
deserve that treatment, too. So. for the latter, I think these APIs in
particular need to be approachable and flexible.

You've already mentioned that you'd like to see ExecutorService retired. It
definitely has plenty of sharp corners. But I'll hold judgement on whether
or not its use should be retired for when I see its replacement (sorry,
FJP, you're not it.)

> >
> > There is a spectrum. On one end (let's call it "simple"), you want a
> programming model that makes it easier to write correct code and that is
> easy to read, write, understand, and troubleshoot
> >
> > (at the extreme: all synchronous, all blocking flows -- very simple to
> understand but will often have poor performance and is incapable of taking
> advantage of today's multi-core computers). On the other end
> ("performance"), you want a programming model that enables taking maximum
> advantage of hardware, provides greater efficiency, and facilitates better
> performance (greater throughput, lower latency).
>
> I think you may be conflating "simple" and "easy":
> http://www.infoq.com/presentations/Simple-Made-Easy
>
> To me, personally, it is mostly about performance, because that's what I
> need. But for my users, it is important that one can reason about how the
> code will behave.
>
> I'll argue that async monadic-style programming is -simpler- than the
> blocking equivalent, yes, it may sound extremely weird at first thought but
> hear me out:
>
I agree that this style makes certain things simpler -- *and* easier :)

That's why I like CompletionStage and wrote an adapter for users that want
to take advantage of this style with our (still on Java 7) frameworks.

(It's also why we use ListenableFuture exclusively, never plain ol'
j.u.c.Future.)


> Let's take these two sections of code:
>
> def addSync(f1: j.u.c.Future[Int], f2: j.u.c.Future[Int]): Int = f1.get()
> + f2.get()
>
> Questions:
> 1) When is it safe to call `addSync`?
> 2) How do I, as the caller of `addSync` know when it is safe to call
> `addSync`?
> 3) Will my program be able to run to completion if I call `addSync`?
> 4) How much code do I need to change if `addSync` causes performance or
> liveness problems?
>
def addAsync(f1: AsyncFuture[Int], f2: AsyncFuture[Int))(implicit e:
> Executor): AsyncFuture[Int] = f1 zip f2 map {_ + _}
>
> Questions:
> 1) When is it safe to call `addAsync`?
> 2) How do I, as the caller of `addAsync` know when it is safe to call
> `addAsync`?
> 3) Will my program be able to run to completion if I call `addAsync`?
>
> In my experience (as a contributor to Akka for 5 years, and the co-author
> of Futures & Promises for Scala), the biggest risk with adding blocking
> APIs (Akka Futures had blocking APIs -on- the Future itself, Scala has it
> externally on a utility called Await, which fortunately employs managed
> blocking to try to reduce the risk of liveness problems at the expense of
> performance) I can safely say that most people will fall back on what they
> know, if that is easier (less effort) than learning something new. There's
> nothing -wrong- about that, it's just human nature! However, knowing that,
> we must take that into consideration and make it easier (less of an effort)
> to learn new things, especially if it leads to better programs
> (maintainability, performance etc).
>
> When the blocking methods were built into the Future itself (in Akka
> originally), it was one of the biggest sources of problems reported
> (related to Futures).
> When the blocking methods were externalized (in scala.concurrent), it is
> still one of the biggest sources of problems reported (related to Futures).
>
> Again, this is just my experience on the topic, so YMMV!
>
> >
> >
> > If we're being pragmatic, economics is the real decider for where on the
> spectrum will strike the right balance. On the simple side, there's an
> advantage to spending less in engineer-hours: most developers are more
> productive writing simple synchronous code, and that style of code is much
> easier to debug. But this can incur greater capital costs since it may
> require more hardware to do the same job. On the performance side, it's the
> converse: get more out of the money spent on compute resources, but
> potentially spend more in engineering effort. (There are obviously other
> constraints, too, like whether a particular piece of software has any value
> at all if it can't meet certain performance requirements.)
>
> I understand and can sympathize with this view. But I think that it is
> more complex than that, it is essentially trading away quick short term
> gain for long-term loss. (Debugging deadlocks and concurrency issues more
> than often cost more in developer time, and disrupting production systems
> than the gain in initially writing the code.)
>
> "It is easier to serve people desserts than greens, the question is what
> is healthier." :)
>
>
I agree with most of this. I'm aware of the liveness pitfalls with
blocking, but the frameworks our app developers use and the places where we
lean on ListenableFuture don't really encounter these scenarios. There are
other potential hazards for sure. But, for us, it's been worth it. We don't
ever have app developers running into concurrency/liveness problems that
arise from blocking.

Though *I* occasionally have fun debugging that kind of stuff, in the
bowels of frameworks :)

It's why I I try to use non-blocking concurrency techniques wherever
possible. (But I work at a lower-level than I expect my users / app
developers to work.) For practical reasons, there are plenty of places
where simple blocking techniques (like synchronized blocks) were used
because it sufficed. (And I get to go debug it and refactor it if we get to
a point where it no longer does.)

> >
> >
> > My experience is that most organizations find their maxima close to the
> middle, but nearer to the simple side. So there is an economic advantage
> for them to focus a little more on developer productivity than on software
> efficiency and performance.
>
> For the user, I worry more about correctness and liveness than
> performance. The performance concerns is for me as a library writer (as the
> users can't opt out of bad library performance).
>
>
Agree completely. Except I haven't encountered the same level of problems
with deadlock arising from blocking code so maybe that's why we're split on
that point. Perhaps there are other aspects of architectures I've worked
with that have limited my exposure to it, or maybe my time just hasn't come
yet :)

>
> >
> > I want my users to be as productive as possible, even if it means they
> write blocking code. (And, let's face it, some of them will commit
> atrocities far worse than just using a blocking API.)
>
> I understand this line of reasoning but it always has to be qualified.
> For example, the allure of RoR was that of high initial productivity, but
> what was sacrificed was both performance, maintainability and scalability.
> So we need to not only consider short term "gains" but also long term
> "losses".
>
Absolutely. But I feel I need to be careful in limiting the "easy" choices
just because they lead to technical debt, because that is often totally
appropriate (as long as you understand the trade-offs and acknowledge when
you're accumulating technical debt).

RoR is a great example. Many companies today probably wouldn't exist if it
weren't for RoR. Many have had to undergo major architectural change to
later achieve those three things (performance, scalability, and
maintainability). But for plenty (including Square), it was absolutely the
right decision in order to get a product out quickly and at low cost. That
velocity, even if it means future technical pain, can be critical for a
young business in order to get early feedback and validate their business
model as well as start generating revenue.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141205/2f9dda5a/attachment-0001.html>

From thurston at nomagicsoftware.com  Fri Dec  5 13:16:00 2014
From: thurston at nomagicsoftware.com (thurstonn)
Date: Fri, 5 Dec 2014 11:16:00 -0700 (MST)
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
References: <CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
Message-ID: <1417803360836-11578.post@n7.nabble.com>

Hello Viktor,

Although this may seem a digression, I'm curious whether you object to
something like Collectors#toList() in j.u.stream?

Because this facilitates (I wouldn't say encourages) code like the
following:


List<T> l1 = . . .
List<T> l2 = l1.stream().filter(...).collect(toList())

List<T> l3 = l2.stream().map(...).collect(toList())

for (T : l3)
  do something

Now, we can all agree that the code above is awful, even wrong, but is it's
possibility (well, probably more than possibility - I've actually seen such
code) enough to warrant the exclusion of Collectors.toList()?
Of course the issue isn't exactly one of "encouraging blocking" (unless the
streams were parallel) . . .

I guess I think that "discouragement by difficulty" in API design more often
than not leads to grief (which is not to say that I've never done it)







--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/CompletableFuture-in-Java-8-tp11414p11578.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From martinrb at google.com  Fri Dec  5 16:29:44 2014
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 5 Dec 2014 13:29:44 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <54810C0C.2080901@oracle.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<54810C0C.2080901@oracle.com>
Message-ID: <CA+kOe0_LU1-RUJ0pNdkR9k1rbCb16mRd-x1ebAd8dwpoQneE2g@mail.gmail.com>

On Thu, Dec 4, 2014 at 5:36 PM, David Holmes <david.holmes at oracle.com> wrote:
> Martin,
>
> On 2/12/2014 6:46 AM, Martin Buchholz wrote:

> Is this finalized then? You can only make one commit per CR.

Right.  I'd like to commit and then perhaps do another round of clarifications.

> I still find this entire comment block to be misguided and misplaced:
>
> !     // Fences, also known as memory barriers, or membars.
> !     // See hotspot sources for more details:
> !     // orderAccess.hpp memnode.hpp unsafe.cpp
> !     //
> !     // One way of implementing Java language-level volatile variables
> using
> !     // fences (but there is often a better way without) is by:
> !     // translating a volatile store into the sequence:
> !     // - storeFence()
> !     // - relaxed store
> !     // - fullFence()
> !     // and translating a volatile load into the sequence:
> !     // - if (CPU_NOT_MULTIPLE_COPY_ATOMIC) fullFence()
> !     // - relaxed load
> !     // - loadFence()
> !     // The full fence on volatile stores ensures the memory model
> guarantee of
> !     // sequential consistency on most platforms.  On some platforms (ppc)
> we
> !     // need an additional full fence between volatile loads as well (see
> !     // hotspot's CPU_NOT_MULTIPLE_COPY_ATOMIC).

Even I think this comment is marginal - I will delete it.  But
consider this a plea for better documentation of the hotspot
internals.

> why do want this description here - it has no relevance to the API itself,
> nor to how volatiles are implemented in the VM. And as I said in the bug
> report CPU_NOT_MULTIPLE_COPY_ATOMIC exists only for platforms that want to
> implement IRIW (none of our platforms are multiple-copy-atomic, but only PPC
> sets this so that it employs IRIW).

I believe the comment _does_ reflect hotspot's current implementation
(entirely from exploring the sources).
I believe it's correct to say "all of the platforms are
multiple-copy-atomic except PPC".
I believe hotspot must implement IRIW correctly to fulfil the promise
of sequential consistency for standard Java, so on ppc volatile reads
get a full fence, which leads us back to the ppc pointer chasing
performance problem that started all of this.

From martinrb at google.com  Fri Dec  5 16:49:23 2014
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 5 Dec 2014 13:49:23 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5481108A.3030605@oracle.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<5481108A.3030605@oracle.com>
Message-ID: <CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>

On Thu, Dec 4, 2014 at 5:55 PM, David Holmes <david.holmes at oracle.com> wrote:

> In general phrasing like: " also known as a LoadLoad plus LoadStore barrier
> ..." is misleading to me as these are not "aliases"- the loadFence (in this
> case) is being specified to have the same semantics as the
> loadload|storeload. It should say "corresponds to a LoadLoad plus LoadStore
> barrier"

+     * Ensures that loads before the fence will not be reordered with loads and
+     * stores after the fence; also known as a LoadLoad plus LoadStore barrier,

I don't understand this.  I believe they _are_ aliases.  The first
clause perfectly describes a "LoadLoad plus LoadStore barrier".

 - as per the "Corresponds to a C11 ...". And referring to things
> like "load-acquire fence" is meaningless without some reference to a
> definition - who defines a load-acquire fence? Is there a universal
> definition? I would be okay with something looser eg:

Well, I'm defining "load-acquire fence" here in the javadoc - I'm
claiming that loadFence is also known via other terminology, including
"load-acquire fence".  Although it's true that "load-acquire fence" is
also used to refer to the corresponding C11 fence, which has subtly
different semantics.

> /**
>   * Ensures that loads before the fence will not be reordered with loads and
>   * stores after the fence. Corresponds to a LoadLoad plus LoadStore
> barrier,
>   * and also to the C11 atomic_thread_fence(memory_order_acquire).
>   * Sometimes referred to as a "load-acquire fence".
>   *
>
> Also I find this comment strange:
>
> !      * A pure StoreStore fence is not provided, since the addition of
> LoadStore
> !      * is almost always desired, and most current hardware instructions
> that
> !      * provide a StoreStore barrier also provide a LoadStore barrier for
> free.
>
> because inside hotspot we use storeStore barriers a lot, without any
> loadStore at the same point.

I believe the use of e.g. OrderAccess::storestore in the hotspot
sources is unfortunate.

The actual implementations of storestore (see below) seem to
universally give you the stronger ::release barrier, and it seems
likely that hotspot engineers are implicitly relying on that, that
some uses of ::storestore in the hotspot sources are bugs (should be
::release instead) and that there is very little potential performance
benefit from using ::storestore instead of ::release, precisely
because the additional loadstore barrier is very close to free on all
current hardware.  Writing correct code using ::storestore is harder
than ::release, which is already difficult enough. C11 doesn't provide
a corresponding fence, which is a strong hint.


./bsd_zero/vm/orderAccess_bsd_zero.inline.hpp:71:inline void
OrderAccess::storestore() { release(); }
./linux_sparc/vm/orderAccess_linux_sparc.inline.hpp:35:inline void
OrderAccess::storestore() { release(); }
./aix_ppc/vm/orderAccess_aix_ppc.inline.hpp:73:inline void
OrderAccess::storestore() { inlasm_lwsync();  }
./linux_zero/vm/orderAccess_linux_zero.inline.hpp:70:inline void
OrderAccess::storestore() { release(); }
./solaris_sparc/vm/orderAccess_solaris_sparc.inline.hpp:40:inline void
OrderAccess::storestore() { release(); }
./linux_ppc/vm/orderAccess_linux_ppc.inline.hpp:75:inline void
OrderAccess::storestore() { inlasm_lwsync();  }
./solaris_x86/vm/orderAccess_solaris_x86.inline.hpp:40:inline void
OrderAccess::storestore() { release(); }
./linux_x86/vm/orderAccess_linux_x86.inline.hpp:35:inline void
OrderAccess::storestore() { release(); }
./bsd_x86/vm/orderAccess_bsd_x86.inline.hpp:35:inline void
OrderAccess::storestore() { release(); }
./windows_x86/vm/orderAccess_windows_x86.inline.hpp:35:inline void
OrderAccess::storestore() { release(); }

From martinrb at google.com  Mon Dec  8 03:42:11 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 8 Dec 2014 00:42:11 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5484DB7E.6080505@oracle.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<54810C0C.2080901@oracle.com>
	<CA+kOe0_LU1-RUJ0pNdkR9k1rbCb16mRd-x1ebAd8dwpoQneE2g@mail.gmail.com>
	<5484DB7E.6080505@oracle.com>
Message-ID: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>

On Sun, Dec 7, 2014 at 2:58 PM, David Holmes <david.holmes at oracle.com> wrote:

>> I believe the comment _does_ reflect hotspot's current implementation
>> (entirely from exploring the sources).
>> I believe it's correct to say "all of the platforms are
>> multiple-copy-atomic except PPC".

... current hotspot sources don't contain ARM support.

> Here is the definition of multi-copy atomicity from the ARM architecture
> manual:
>
> "In a multiprocessing system, writes to a memory location are multi-copy
> atomic if the following conditions are both true:
> ? All writes to the same location are serialized, meaning they are observed
> in the same order by all observers, although some observers might not
> observe all of the writes.
> ? A read of a location does not return the value of a write until all
> observers observe that write."

The hotspot sources give

"""
// To assure the IRIW property on processors that are not multiple copy
// atomic, sync instructions must be issued between volatile reads to
// assure their ordering, instead of after volatile stores.
// (See "A Tutorial Introduction to the ARM and POWER Relaxed Memory Models"
// by Luc Maranget, Susmit Sarkar and Peter Sewell, INRIA/Cambridge)
#ifdef CPU_NOT_MULTIPLE_COPY_ATOMIC
const bool support_IRIW_for_not_multiple_copy_atomic_cpu = true;
"""

and the referenced paper gives

"""
on POWER and ARM, two threads can observe writes to different
locations in different orders, even in
the absence of any thread-local reordering. In other words, the
architectures are not multiple-copy atomic [Col92].
"""

which strongly suggests that x86 and sparc are OK.

> The first condition is met by Total-Store-Order (TSO) systems like x86 and
> sparc; and not by relaxed-memory-order (RMO) systems like ARM and PPC.
> However the second condition is not met simply by having TSO. If the local
> processor can see a write from the local store buffer prior to it being
> visible to other processors, then we do not have multi-copy atomicity and I
> believe that is true for x86 and sparc. Hence none of our supported
> platforms are multi-copy-atomic as far as I can see.
>
>> I believe hotspot must implement IRIW correctly to fulfil the promise
>> of sequential consistency for standard Java, so on ppc volatile reads
>> get a full fence, which leads us back to the ppc pointer chasing
>> performance problem that started all of this.
>
>
> Note that nothing in the JSR-133 cookbook allows for IRIW, even on x86 and
> sparc. The key feature needed for IRIW is a load barrier that forces global
> memory synchronization to ensure that all processors see writes at the same
> time. I'm not even sure we can force that on x86 and sparc! Such a load
> barrier negates the need for some store barriers as defined in the cookbook.
>
> My understanding, which could be wrong, is that the JMM implies
> linearizability of volatile accesses, which in turn provides the IRIW
> property. It is also my understanding that linearizability is a necessary
> property for current proof systems to be applicable. However absence of
> proof is not proof of absence, and it doesn't follow that code that doesn't
> rely on IRIW is incorrect if IRIW is not ensured on a system. As has been
> stated many times now, in the literature no practical lock-free algorithm
> seems to rely on IRIW. So I still hope that IRIW can somehow be removed
> because implementing it will impact everything related to the JMM in
> hotspot.


From davidcholmes at aapt.net.au  Mon Dec  8 03:46:50 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 8 Dec 2014 18:46:50 +1000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEJJKLAA.davidcholmes@aapt.net.au>

Martin,

The paper you cite is about ARM and Power architectures - why do you think the lack of mention of x86/sparc implies those architectures are multiple-copy-atomic?

David

> -----Original Message-----
> From: Martin Buchholz [mailto:martinrb at google.com]
> Sent: Monday, 8 December 2014 6:42 PM
> To: David Holmes
> Cc: David Holmes; Vladimir Kozlov; core-libs-dev; concurrency-interest
> Subject: Re: [concurrency-interest] RFR: 8065804: JEP 171:
> Clarifications/corrections for fence intrinsics
> 
> 
> On Sun, Dec 7, 2014 at 2:58 PM, David Holmes 
> <david.holmes at oracle.com> wrote:
> 
> >> I believe the comment _does_ reflect hotspot's current implementation
> >> (entirely from exploring the sources).
> >> I believe it's correct to say "all of the platforms are
> >> multiple-copy-atomic except PPC".
> 
> ... current hotspot sources don't contain ARM support.
> 
> > Here is the definition of multi-copy atomicity from the ARM architecture
> > manual:
> >
> > "In a multiprocessing system, writes to a memory location are multi-copy
> > atomic if the following conditions are both true:
> > ? All writes to the same location are serialized, meaning they 
> are observed
> > in the same order by all observers, although some observers might not
> > observe all of the writes.
> > ? A read of a location does not return the value of a write until all
> > observers observe that write."
> 
> The hotspot sources give
> 
> """
> // To assure the IRIW property on processors that are not multiple copy
> // atomic, sync instructions must be issued between volatile reads to
> // assure their ordering, instead of after volatile stores.
> // (See "A Tutorial Introduction to the ARM and POWER Relaxed 
> Memory Models"
> // by Luc Maranget, Susmit Sarkar and Peter Sewell, INRIA/Cambridge)
> #ifdef CPU_NOT_MULTIPLE_COPY_ATOMIC
> const bool support_IRIW_for_not_multiple_copy_atomic_cpu = true;
> """
> 
> and the referenced paper gives
> 
> """
> on POWER and ARM, two threads can observe writes to different
> locations in different orders, even in
> the absence of any thread-local reordering. In other words, the
> architectures are not multiple-copy atomic [Col92].
> """
> 
> which strongly suggests that x86 and sparc are OK.
> 
> > The first condition is met by Total-Store-Order (TSO) systems 
> like x86 and
> > sparc; and not by relaxed-memory-order (RMO) systems like ARM and PPC.
> > However the second condition is not met simply by having TSO. 
> If the local
> > processor can see a write from the local store buffer prior to it being
> > visible to other processors, then we do not have multi-copy 
> atomicity and I
> > believe that is true for x86 and sparc. Hence none of our supported
> > platforms are multi-copy-atomic as far as I can see.
> >
> >> I believe hotspot must implement IRIW correctly to fulfil the promise
> >> of sequential consistency for standard Java, so on ppc volatile reads
> >> get a full fence, which leads us back to the ppc pointer chasing
> >> performance problem that started all of this.
> >
> >
> > Note that nothing in the JSR-133 cookbook allows for IRIW, even 
> on x86 and
> > sparc. The key feature needed for IRIW is a load barrier that 
> forces global
> > memory synchronization to ensure that all processors see writes 
> at the same
> > time. I'm not even sure we can force that on x86 and sparc! Such a load
> > barrier negates the need for some store barriers as defined in 
> the cookbook.
> >
> > My understanding, which could be wrong, is that the JMM implies
> > linearizability of volatile accesses, which in turn provides the IRIW
> > property. It is also my understanding that linearizability is a 
> necessary
> > property for current proof systems to be applicable. However absence of
> > proof is not proof of absence, and it doesn't follow that code 
> that doesn't
> > rely on IRIW is incorrect if IRIW is not ensured on a system. 
> As has been
> > stated many times now, in the literature no practical lock-free 
> algorithm
> > seems to rely on IRIW. So I still hope that IRIW can somehow be removed
> > because implementing it will impact everything related to the JMM in
> > hotspot.
> 



From jh at squareup.com  Mon Dec  8 09:15:40 2014
From: jh at squareup.com (Josh Humphries)
Date: Mon, 8 Dec 2014 09:15:40 -0500
Subject: [concurrency-interest] question about spec for
	AtomicIntegerFieldUpdater
Message-ID: <CAHJZN-tURM_yTkEagre9JJyp0Sgf3oT1gabUfrVg4oPaiaaFbA@mail.gmail.com>

Hey, all (particularly Doug Lea, who's in the @author tag for this class):

The second paragraph of the class doc for this class:

Note that the guarantees of the {@code compareAndSet} method in this class
> are weaker than in other atomic classes. Because this class cannot ensure
> that all uses of the field are appropriate for purposes of atomic access,
> it can guarantee atomicity only with respect to other invocations of {@code
> compareAndSet} and {@code set} on the same updater.


The implementations for these methods (compareAndSet() and set()) use
sun.misc.Unsafe to do compareAndSwap for the first and volatile write for
the second.

But in AbstractQueuedSynchronizer, the same unsafe semantics are used for
the CAS. However, setState() does a standard field assignment to get the
volatile write (instead of using Unsafe, like AtomicIntegerFieldUpdater).

If it's okay to mix standard field assignment for the volatile write and
unsafe for CAS, what is the potential issue to which
AtomicIntegerFieldUpdater doc refers, about weaker guarantees?

At first, I saw this in the AtomicLongFieldUpdater and assumed it was
related to whether a JVM could do an atomic 64-bit CAS (and has a
lock-based implementation if not). So it makes sense for this case why
callers might need to use updater.set() for the CAS operation to work
correctly.

But I was confused to see it for its 32-bit sibling. Also,
AtomicLongQueuedSynchronizer doesn't check whether the VM can do atomic
64-bit swaps and does the same thing as AtomicQueuedSynchronizer -- always
use unsafe to do the CAS and standard field assignment to achieve the
volatile write.

In the end, I'm curious because I don't want to pay the overhead of runtime
type checks in calling AtomicIntegerFieldUpdater.set(). But I do want to
use the CAS operation, prefer the idea of laying out the variable into the
class (as opposed to using AtomicInteger and chasing a pointer) and would
prefer to never rely on sun.misc.Unsafe.

So, what are the hazards of using AtomicIntegerFieldUpdater (and
AtomicLongFieldUpdater) for compareAndset() mixed with normal field
assignment to set?

----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141208/29d319a7/attachment.html>

From viktor.klang at gmail.com  Mon Dec  8 09:25:10 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 8 Dec 2014 15:25:10 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-sGQzJFrWnKeoZONjOWiHYeVD=hZ4k6rZMV1ez3cUQ=aw@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
	<CAHJZN-sGQzJFrWnKeoZONjOWiHYeVD=hZ4k6rZMV1ez3cUQ=aw@mail.gmail.com>
Message-ID: <CANPzfU8Mj0d6_SPV08OttzkbGgVK9ppwic_4wMuDKnWEj6OJ1A@mail.gmail.com>

Hi Josh,

sorry for the delayed response!

On Fri, Dec 5, 2014 at 5:07 PM, Josh Humphries <jh at squareup.com> wrote:

> On Fri, Dec 5, 2014 at 8:41 AM, ?iktor ?lang <viktor.klang at gmail.com>
> wrote:
>
>> Hey Josh,
>>
>> On Fri, Dec 5, 2014 at 5:30 AM, Josh Humphries <jh at squareup.com> wrote:
>> >
>> > Hey, Viktor. I think I've touched on some of this already. But, since
>> you said you're very much interested, I'll elaborate on my thinking.
>> >
>>
>> Thanks for taking the time and spending the effort to elaborate, Josh, I
>> really appreciate it!
>>
>> Rereading my reply I notice that we have strayed a bit from the initial
>> discussion, but it is an interesting topic so I'll share my thoughts on the
>> topic.
>>
>> TL; DR: I think we both agree but have different cutoff points. :)
>>
>
> Indeed. I think that sums up the difference pretty well :)
>
>
>> > Every decision is a trade-off. Mixing concerns can bloat the API and
>> increase the cognitive burden of using it, but it can also provide greater
>> functionality or make certain patterns of use easier. While the two
>> concerns we're discussing may seem similar, they are very different (at
>> least to me) regarding what they are actually providing to the developer,
>> so the trade-offs are different.
>>
>> Agreed. My stance is to error on the side of Single Responsibility
>> Principle, and it is easier to add API if defensible than deprecate and
>> remove (remove has never happened in the JDK AFAICT).
>>
>> >
>> > Concern #1: Exposing methods to imperatively complete the future vs.
>> having the future's value be provided implicitly (by the running of some
>> unit of logic). We're not really talking about mixing the two here. My
>> objection was that CompletionStage#toCompletableFuture leaks the imperative
>> style in a way that is simply inappropriate.
>>
>> I think both I and Doug(?) agree here, the problem is that there's no
>> protected scope for interface methods, so CompletableFuture would have to
>> wrap every CompletionStage that isn't a CompletableFuture, leading to a lot
>> of allocations for the worst case. Doug would be able to share more about
>> that.
>>
>
> But CompletableFuture is a class, not an interface. So if the
> CompletionStage is not a CompletableFuture, you're still back to having to
> wrap the CompletionStage. You've just moved the responsibility out of
> CompletableFuture and into every other implementation of CompletionStage,
> which seems like an unusual choice.
>

Unusual: yes, unheard of: no. :) that is the same for java.util.Collection
which has a toArray (Collection is an interface, Array is highly concrete).
To be clear, I concede that it is suboptimal and if I could go without I'd
prefer not to have it!


>
> >
>> > So my objection here is about poor encapsulation/abstraction. If the
>> API had returned FutureTask, that too would have been bad. (I also griped
>> about the lack of a FutureTask-like implementation of CompletionStage, but
>> that is really a nit; not a major complaint.)
>>
>> Personally I don't mind here, it's beyond trivial to "submit" a
>> CompletableFuture. But YYMV.
>>
>> And with CompletableFuture you have a choice if you want to expose it,
>> CompletionStage or Future depending on what capabilities you want to
>> expose, which does sound quite flexible?
>>
>
> Sure. I had prefaced my whole original rant with the fact that these are
> nits and acknowledged that everything that is needed is there. In fact,
> I've written everything I need on top of the existing APIs (but found it
> annoying that my implementation of CompletionStage has a method that
> requires wrapping the stage in a CompletableFuture).
>

Fair enough! :)


>
> So my points are really about the aesthetics of the API (which,
> admittedly, are often subjective).
>

I'm highly impressed that we're having a perfectly civilized discussion
about aesthetics on the Internet :-)


> >
>> > As far as inter-op with legacy APIs, a #toFuture() method would have
>> been much better for a few reasons:
>>
>> I think again that the toCompletableFuture, as far as I can see, was
>> primarily needed for CompletableFuture.
>>
>> >
>> > Future is an interface, so a *view* could be returned instead of having
>> to create a new stateful object that must be kept in sync with the
>> original.
>> >
>> > Future provides inter-op but doesn't leak complete*/obtrude* methods
>> (the heart of my objections)
>> > It could have been trivially implemented as a default method that just
>> returns a CompletableFuture that is set from a #whenComplete stage, just as
>> you've described.
>> > (I'm pretty certain we agree on #1. At least most of it.)
>>
>> I would have much preferred to have a static method on Future called
>> "fromCompletionStage" so that CompletionStages do not need to know about
>> "the world". :-)
>>
> That, too, is completely reasonable :)
> I think the discoverability point has already been brought up. But that's
> what @see tags in Javadoc are for, right?
>

+1!


>
>
>> >
>> > You've already clearly expressed the opinion that blocking code is
>> never appropriate. I think that's a reasonable assertion for many contexts,
>> just not the JRE. Avoiding blocking altogether in core Java APIs is neither
>> realistic nor (again, IMO) desirable.
>>
>> Would you mind expanding on "just not the JRE"?
>> My view is that java.util.concurrent is about tools to facilitate
>> concurrent programming mainly targeted towards advanced users and library
>> writers.
>> Perhaps it is here we have different views?
>>
> I'll back-pedal a little and qualify "not in the JRE" with "not in certain
> parts of the JRE".
>
> Many of the APIs in that package really *are* for advanced users and
> library writers, just like you said. But ExecutorService,
> ScheduledExecutorService, and Future are really basic and are the kind of
> building blocks that even developers writing business logic will need/want
> to use.
>
There are plenty of cases where it makes sense to provide purpose-built
> libraries that wrap them. But there are plenty that don't deserve that
> treatment, too. So. for the latter, I think these APIs in particular need
> to be approachable and flexible.
>
> You've already mentioned that you'd like to see ExecutorService retired.
> It definitely has plenty of sharp corners. But I'll hold judgement on
> whether or not its use should be retired for when I see its replacement
> (sorry, FJP, you're not it.)
>

I've pitched the idea to Doug that I'd love to see a FJP implementation
without the FJ, please chime in :)


> >
>> > There is a spectrum. On one end (let's call it "simple"), you want a
>> programming model that makes it easier to write correct code and that is
>> easy to read, write, understand, and troubleshoot
>> >
>> > (at the extreme: all synchronous, all blocking flows -- very simple to
>> understand but will often have poor performance and is incapable of taking
>> advantage of today's multi-core computers). On the other end
>> ("performance"), you want a programming model that enables taking maximum
>> advantage of hardware, provides greater efficiency, and facilitates better
>> performance (greater throughput, lower latency).
>>
>> I think you may be conflating "simple" and "easy":
>> http://www.infoq.com/presentations/Simple-Made-Easy
>>
>> To me, personally, it is mostly about performance, because that's what I
>> need. But for my users, it is important that one can reason about how the
>> code will behave.
>>
>> I'll argue that async monadic-style programming is -simpler- than the
>> blocking equivalent, yes, it may sound extremely weird at first thought but
>> hear me out:
>>
> I agree that this style makes certain things simpler -- *and* easier :)
>
> That's why I like CompletionStage and wrote an adapter for users that want
> to take advantage of this style with our (still on Java 7) frameworks.
>
> (It's also why we use ListenableFuture exclusively, never plain ol'
> j.u.c.Future.)
>

+1 for this! :)


>
>
>> Let's take these two sections of code:
>>
>> def addSync(f1: j.u.c.Future[Int], f2: j.u.c.Future[Int]): Int = f1.get()
>> + f2.get()
>>
>> Questions:
>> 1) When is it safe to call `addSync`?
>> 2) How do I, as the caller of `addSync` know when it is safe to call
>> `addSync`?
>> 3) Will my program be able to run to completion if I call `addSync`?
>> 4) How much code do I need to change if `addSync` causes performance or
>> liveness problems?
>>
> def addAsync(f1: AsyncFuture[Int], f2: AsyncFuture[Int))(implicit e:
>> Executor): AsyncFuture[Int] = f1 zip f2 map {_ + _}
>>
>> Questions:
>> 1) When is it safe to call `addAsync`?
>> 2) How do I, as the caller of `addAsync` know when it is safe to call
>> `addAsync`?
>> 3) Will my program be able to run to completion if I call `addAsync`?
>>
>> In my experience (as a contributor to Akka for 5 years, and the co-author
>> of Futures & Promises for Scala), the biggest risk with adding blocking
>> APIs (Akka Futures had blocking APIs -on- the Future itself, Scala has it
>> externally on a utility called Await, which fortunately employs managed
>> blocking to try to reduce the risk of liveness problems at the expense of
>> performance) I can safely say that most people will fall back on what they
>> know, if that is easier (less effort) than learning something new. There's
>> nothing -wrong- about that, it's just human nature! However, knowing that,
>> we must take that into consideration and make it easier (less of an effort)
>> to learn new things, especially if it leads to better programs
>> (maintainability, performance etc).
>>
>> When the blocking methods were built into the Future itself (in Akka
>> originally), it was one of the biggest sources of problems reported
>> (related to Futures).
>> When the blocking methods were externalized (in scala.concurrent), it is
>> still one of the biggest sources of problems reported (related to Futures).
>>
>> Again, this is just my experience on the topic, so YMMV!
>>
>> >
>> >
>> > If we're being pragmatic, economics is the real decider for where on
>> the spectrum will strike the right balance. On the simple side, there's an
>> advantage to spending less in engineer-hours: most developers are more
>> productive writing simple synchronous code, and that style of code is much
>> easier to debug. But this can incur greater capital costs since it may
>> require more hardware to do the same job. On the performance side, it's the
>> converse: get more out of the money spent on compute resources, but
>> potentially spend more in engineering effort. (There are obviously other
>> constraints, too, like whether a particular piece of software has any value
>> at all if it can't meet certain performance requirements.)
>>
>> I understand and can sympathize with this view. But I think that it is
>> more complex than that, it is essentially trading away quick short term
>> gain for long-term loss. (Debugging deadlocks and concurrency issues more
>> than often cost more in developer time, and disrupting production systems
>> than the gain in initially writing the code.)
>>
>> "It is easier to serve people desserts than greens, the question is what
>> is healthier." :)
>>
>>
> I agree with most of this. I'm aware of the liveness pitfalls with
> blocking, but the frameworks our app developers use and the places where we
> lean on ListenableFuture don't really encounter these scenarios. There are
> other potential hazards for sure. But, for us, it's been worth it. We don't
> ever have app developers running into concurrency/liveness problems that
> arise from blocking.
>

Perhaps we just have different user bases.


>
> Though *I* occasionally have fun debugging that kind of stuff, in the
> bowels of frameworks :)
>

Fortunately, I don't need to do that very often due to not blocking :)


>
> It's why I I try to use non-blocking concurrency techniques wherever
> possible. (But I work at a lower-level than I expect my users / app
> developers to work.) For practical reasons, there are plenty of places
> where simple blocking techniques (like synchronized blocks) were used
> because it sufficed. (And I get to go debug it and refactor it if we get to
> a point where it no longer does.)
>

I consider *synchronized*-blocks to be a potential bugs, since they never
time out. Completely the wrong default IMO.


> >
>> >
>> > My experience is that most organizations find their maxima close to the
>> middle, but nearer to the simple side. So there is an economic advantage
>> for them to focus a little more on developer productivity than on software
>> efficiency and performance.
>>
>> For the user, I worry more about correctness and liveness than
>> performance. The performance concerns is for me as a library writer (as the
>> users can't opt out of bad library performance).
>>
>>
> Agree completely. Except I haven't encountered the same level of problems
> with deadlock arising from blocking code so maybe that's why we're split on
> that point. Perhaps there are other aspects of architectures I've worked
> with that have limited my exposure to it, or maybe my time just hasn't come
> yet :)
>

Well, I hope you never have to encounter them. :)


>
> >
>> >
>> > I want my users to be as productive as possible, even if it means they
>> write blocking code. (And, let's face it, some of them will commit
>> atrocities far worse than just using a blocking API.)
>>
>> I understand this line of reasoning but it always has to be qualified.
>> For example, the allure of RoR was that of high initial productivity, but
>> what was sacrificed was both performance, maintainability and scalability.
>> So we need to not only consider short term "gains" but also long term
>> "losses".
>>
> Absolutely. But I feel I need to be careful in limiting the "easy" choices
> just because they lead to technical debt, because that is often totally
> appropriate (as long as you understand the trade-offs and acknowledge when
> you're accumulating technical debt).
>
> RoR is a great example. Many companies today probably wouldn't exist if it
> weren't for RoR. Many have had to undergo major architectural change to
> later achieve those three things (performance, scalability, and
> maintainability). But for plenty (including Square), it was absolutely the
> right decision in order to get a product out quickly and at low cost. That
> velocity, even if it means future technical pain, can be critical for a
> young business in order to get early feedback and validate their business
> model as well as start generating revenue.
>

Absolutely. But I want to build technology that scales, so that one does
not have to rebuild once successful, and one can become successful quickly.
:)



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141208/6fc742d3/attachment-0001.html>

From dl at cs.oswego.edu  Mon Dec  8 09:47:15 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 08 Dec 2014 09:47:15 -0500
Subject: [concurrency-interest] question about spec for
	AtomicIntegerFieldUpdater
In-Reply-To: <CAHJZN-tURM_yTkEagre9JJyp0Sgf3oT1gabUfrVg4oPaiaaFbA@mail.gmail.com>
References: <CAHJZN-tURM_yTkEagre9JJyp0Sgf3oT1gabUfrVg4oPaiaaFbA@mail.gmail.com>
Message-ID: <5485B9F3.1020809@cs.oswego.edu>

On 12/08/2014 09:15 AM, Josh Humphries wrote:
> Hey, all (particularly Doug Lea, who's in the @author tag for this class):
>
> The second paragraph of the class doc for this class:
>
>     Note that the guarantees of the {@code compareAndSet} method in this class
>     are weaker than in other atomic classes. Because this class cannot ensure
>     that all uses of the field are appropriate for purposes of atomic access, it
>     can guarantee atomicity only with respect to other invocations of {@code
>     compareAndSet} and {@code set} on the same updater.
>

This disclaimer arose to cover mappings to processors that
do not have atomic stores of widths corresponding to atomic
CAS APIs, in which case a lock-based scheme must be used across
both set and CAS. Officially, you need to use
FieldUpdater.set(val) if you use CAS, so that the
set can be intercepted on platforms requiring it.

However, all hotspot-targeted processors have at least 32bit
atomic set and CAS, so j.u.c and other internal JDK code
have always been lax about this for ints. But there is still
at least one supported processor (Power32) that needs this treatment
for 64bit longs, so you still see explicit use internally
(for example AbstractQueuedLongSynchronizer.setState.)

-Doug

From vitalyd at gmail.com  Mon Dec  8 10:05:08 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 8 Dec 2014 10:05:08 -0500
Subject: [concurrency-interest] question about spec for
	AtomicIntegerFieldUpdater
In-Reply-To: <CAHJZN-tURM_yTkEagre9JJyp0Sgf3oT1gabUfrVg4oPaiaaFbA@mail.gmail.com>
References: <CAHJZN-tURM_yTkEagre9JJyp0Sgf3oT1gabUfrVg4oPaiaaFbA@mail.gmail.com>
Message-ID: <CAHjP37GUr7-Gg1RRzNCuKxF9eC8oVaPX_ZNJi8jJb+rc8fZ+4g@mail.gmail.com>

I think it's trying to highlight the fact that atomic classes wrap their
underlying primitive and control all access to it (I.e all accesses are
through the atomic class).  With the updater, other code may be accessing
the same primitive "naked" and thus it can't promise the same thing unless
all access to that variable is done through the updater.  For example, some
uses of the underlying field directly may involve a test-then-act sequence
or some other multi step process (I.e. not atomic).

Sent from my phone
On Dec 8, 2014 9:46 AM, "Josh Humphries" <jh at squareup.com> wrote:

> Hey, all (particularly Doug Lea, who's in the @author tag for this class):
>
> The second paragraph of the class doc for this class:
>
> Note that the guarantees of the {@code compareAndSet} method in this class
>> are weaker than in other atomic classes. Because this class cannot ensure
>> that all uses of the field are appropriate for purposes of atomic access,
>> it can guarantee atomicity only with respect to other invocations of {@code
>> compareAndSet} and {@code set} on the same updater.
>
>
> The implementations for these methods (compareAndSet() and set()) use
> sun.misc.Unsafe to do compareAndSwap for the first and volatile write for
> the second.
>
> But in AbstractQueuedSynchronizer, the same unsafe semantics are used for
> the CAS. However, setState() does a standard field assignment to get the
> volatile write (instead of using Unsafe, like AtomicIntegerFieldUpdater).
>
> If it's okay to mix standard field assignment for the volatile write and
> unsafe for CAS, what is the potential issue to which
> AtomicIntegerFieldUpdater doc refers, about weaker guarantees?
>
> At first, I saw this in the AtomicLongFieldUpdater and assumed it was
> related to whether a JVM could do an atomic 64-bit CAS (and has a
> lock-based implementation if not). So it makes sense for this case why
> callers might need to use updater.set() for the CAS operation to work
> correctly.
>
> But I was confused to see it for its 32-bit sibling. Also,
> AtomicLongQueuedSynchronizer doesn't check whether the VM can do atomic
> 64-bit swaps and does the same thing as AtomicQueuedSynchronizer -- always
> use unsafe to do the CAS and standard field assignment to achieve the
> volatile write.
>
> In the end, I'm curious because I don't want to pay the overhead of
> runtime type checks in calling AtomicIntegerFieldUpdater.set(). But I do
> want to use the CAS operation, prefer the idea of laying out the variable
> into the class (as opposed to using AtomicInteger and chasing a pointer)
> and would prefer to never rely on sun.misc.Unsafe.
>
> So, what are the hazards of using AtomicIntegerFieldUpdater (and
> AtomicLongFieldUpdater) for compareAndset() mixed with normal field
> assignment to set?
>
> ----
> *Josh Humphries*
> Manager, Shared Systems  |  Platform Engineering
> Atlanta, GA  |  678-400-4867
> *Square* (www.squareup.com)
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141208/e3643d47/attachment.html>

From jh at squareup.com  Mon Dec  8 10:33:43 2014
From: jh at squareup.com (Josh Humphries)
Date: Mon, 8 Dec 2014 10:33:43 -0500
Subject: [concurrency-interest] question about spec for
	AtomicIntegerFieldUpdater
In-Reply-To: <5485B9F3.1020809@cs.oswego.edu>
References: <CAHJZN-tURM_yTkEagre9JJyp0Sgf3oT1gabUfrVg4oPaiaaFbA@mail.gmail.com>
	<5485B9F3.1020809@cs.oswego.edu>
Message-ID: <CAHJZN-tjOwdO-qqRqZwxpXW3uUo91ZLGZWmywz_DvraG4Hy1oQ@mail.gmail.com>

>
> However, all hotspot-targeted processors have at least 32bit
> atomic set and CAS, so j.u.c and other internal JDK code
> have always been lax about this for ints. But there is still
> at least one supported processor (Power32) that needs this treatment
> for 64bit longs, so you still see explicit use internally
> (for example AbstractQueuedLongSynchronizer.setState.)


Ok. That sounds about like what I was expecting. So I'm safe using a
standard field write for 32-bit values as long as I'm okay with the
potential loss of portability to non-Oracle JVMs.

For Power32, how do volatile semantics work for long and double values
without word-tearing?

Also, I'm looking at the source for AbstractQueuedLongSynchronizer.setState
(in Java SE 1.8.0_20):

    /**
     * Sets the value of synchronization state.
     * This operation has memory semantics of a {@code volatile} write.
     * @param newState the new state value
     */
    protected final void setState(long newState) {
        state = newState;
    }

It's just using a simple volatile write. Am I missing something?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141208/633e06e1/attachment.html>

From viktor.klang at gmail.com  Mon Dec  8 10:37:16 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 8 Dec 2014 16:37:16 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <1417803360836-11578.post@n7.nabble.com>
References: <CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
	<1417803360836-11578.post@n7.nabble.com>
Message-ID: <CANPzfU-iL1=OrgdTGT4zf_uR7yq=DSgrpXG+dEQPBmSfL1Cu0g@mail.gmail.com>

Hi Thurstonn,

Apologies for the late reply,

On Fri, Dec 5, 2014 at 7:16 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> Hello Viktor,
>
> Although this may seem a digression, I'm curious whether you object to
> something like Collectors#toList() in j.u.stream?
>
> Because this facilitates (I wouldn't say encourages) code like the
> following:
>
>
> List<T> l1 = . . .
> List<T> l2 = l1.stream().filter(...).collect(toList())
>
> List<T> l3 = l2.stream().map(...).collect(toList())
>
> for (T : l3)
>   do something
>
> Now, we can all agree that the code above is awful, even wrong, but is it's
> possibility (well, probably more than possibility - I've actually seen such
> code) enough to warrant the exclusion of Collectors.toList()?
>

At least that code doesn't impact liveness of the system. (well, you can
still OOME, but you can do that by allocating an array, so that's not
really the same)


> Of course the issue isn't exactly one of "encouraging blocking" (unless the
> streams were parallel) . . .
>
> I guess I think that "discouragement by difficulty" in API design more
> often
> than not leads to grief (which is not to say that I've never done it)
>

Do you have any example?


>
>
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/CompletableFuture-in-Java-8-tp11414p11578.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141208/7a6b905d/attachment.html>

From dl at cs.oswego.edu  Mon Dec  8 10:45:02 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 08 Dec 2014 10:45:02 -0500
Subject: [concurrency-interest] question about spec for
	AtomicIntegerFieldUpdater
In-Reply-To: <CAHJZN-tjOwdO-qqRqZwxpXW3uUo91ZLGZWmywz_DvraG4Hy1oQ@mail.gmail.com>
References: <CAHJZN-tURM_yTkEagre9JJyp0Sgf3oT1gabUfrVg4oPaiaaFbA@mail.gmail.com>
	<5485B9F3.1020809@cs.oswego.edu>
	<CAHJZN-tjOwdO-qqRqZwxpXW3uUo91ZLGZWmywz_DvraG4Hy1oQ@mail.gmail.com>
Message-ID: <5485C77E.5050402@cs.oswego.edu>

On 12/08/2014 10:33 AM, Josh Humphries wrote:

> For Power32, how do volatile semantics work for long and double values without
> word-tearing?

On Power32, there are ways to get 64bit atomic load and store, but
not those that work with 32bit-only CAS emulation. So if you use
CAS, you need more expensive lock-based forms.

>
> Also, I'm looking at the source for AbstractQueuedLongSynchronizer.setState (in
> Java SE 1.8.0_20):

Try our jsr166 sources from which you can see in CVS history that
the need for putVolatileLong even here has been forgotten and
re-remembered a few times :-)

-Doug




From jh at squareup.com  Mon Dec  8 11:35:10 2014
From: jh at squareup.com (Josh Humphries)
Date: Mon, 8 Dec 2014 11:35:10 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU8Mj0d6_SPV08OttzkbGgVK9ppwic_4wMuDKnWEj6OJ1A@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
	<CAHJZN-sGQzJFrWnKeoZONjOWiHYeVD=hZ4k6rZMV1ez3cUQ=aw@mail.gmail.com>
	<CANPzfU8Mj0d6_SPV08OttzkbGgVK9ppwic_4wMuDKnWEj6OJ1A@mail.gmail.com>
Message-ID: <CAHJZN-uM03ovA2gLiE7CfvNy4oa1W1kF-CbRG6+Q6OHqYmZHRg@mail.gmail.com>

>
> But CompletableFuture is a class, not an interface. So if the
>> CompletionStage is not a CompletableFuture, you're still back to having to
>> wrap the CompletionStage. You've just moved the responsibility out of
>> CompletableFuture and into every other implementation of CompletionStage,
>> which seems like an unusual choice.
>>
>
> Unusual: yes, unheard of: no. :) that is the same for java.util.Collection
> which has a toArray (Collection is an interface, Array is highly concrete).
> To be clear, I concede that it is suboptimal and if I could go without I'd
> prefer not to have it!
>

Right, Doug pointed out the same. But for the purpose of inter-op, Future
(instead of CompletableFuture) would have sufficed.

BTW, Doug, I saw your other reply. I'm not suggesting that anything needs
be done about the API. I was just listing some of my nits after Yu Lin
asked about API differences between Guava's ListenableFuture and Java 8's
CompletableFuture. I concede that my complaints are largely cosmetic -- the
API doesn't prevent me from doing what I need to.

You've already mentioned that you'd like to see ExecutorService retired. It
>> definitely has plenty of sharp corners. But I'll hold judgement on whether
>> or not its use should be retired for when I see its replacement (sorry,
>> FJP, you're not it.)
>>
>
> I've pitched the idea to Doug that I'd love to see a FJP implementation
> without the FJ, please chime in :)
>

Yeah, that sounds nice, but I'd really have to hear and see more to have a
strong opinion.

I'm actually toying with something that might be kinda-sorta like this now.
It's like an ExecutorService, but you give each submission a "key" (think
Actor or Listener) so submissions from outside of the pool (like non-FJ
code) can try to pin the task to "the right thread" instead of a random
thread (and hopefully get better L1 cache hit rate). At the same time, it
also employs work-stealing to improve throughput and fairness.

One difference in what I'm trying to do is that it is specifically for
sequential delivery of events to listeners (and could be used for executing
operations on behalf of an actor in something like Akka). So its
implementation is quite different from FJP since each "actor" has a
separate queue. So each worker thread can have multiple queues (one per
actor pinned to that worker) instead of a single work queue.

I suspect there might be something something similar in Akka or Play (I'm
just guessing -- I have only a passing familiarity with these frameworks).

I'm suddenly kind of curious what folks on this list think of it:
https://code.google.com/p/bluegosling/source/browse/src/com/apriori/concurrent/ActorThreadPool.java


RoR is a great example. Many companies today probably wouldn't exist if it
>> weren't for RoR. Many have had to undergo major architectural change to
>> later achieve those three things (performance, scalability, and
>> maintainability). But for plenty (including Square), it was absolutely the
>> right decision in order to get a product out quickly and at low cost. That
>> velocity, even if it means future technical pain, can be critical for a
>> young business in order to get early feedback and validate their business
>> model as well as start generating revenue.
>>
>
> Absolutely. But I want to build technology that scales, so that one does
> not have to rebuild once successful, and one can become successful quickly.
> :)
>

A virtuous objective indeed! And I think the Play framework is pretty
attractive for this, although (I must confess) I haven't really used it,
only looked into a little.

For now, RoR seem to be more attractive to many largely, I think, due to
Java's reputation (a lot of it due to language's verbosity vs. terseness
for many idioms, some due its static typing and compile phase which slows
down iteration [at least it is perceived to slow it down]).

Scala has a bit of an advantage here over Java in that it doesn't have the
same reputation scars -- at least that's the impression I get from Rubyists
:)
But I really hope Java 8 can recover some of that reputation. (It certainly
deserves to!)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141208/3fe86460/attachment-0001.html>

From paul.sandoz at oracle.com  Mon Dec  8 12:45:54 2014
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Mon, 8 Dec 2014 18:45:54 +0100
Subject: [concurrency-interest] question about spec for
	AtomicIntegerFieldUpdater
In-Reply-To: <5485C77E.5050402@cs.oswego.edu>
References: <CAHJZN-tURM_yTkEagre9JJyp0Sgf3oT1gabUfrVg4oPaiaaFbA@mail.gmail.com>
	<5485B9F3.1020809@cs.oswego.edu>
	<CAHJZN-tjOwdO-qqRqZwxpXW3uUo91ZLGZWmywz_DvraG4Hy1oQ@mail.gmail.com>
	<5485C77E.5050402@cs.oswego.edu>
Message-ID: <06645EB3-3FC9-4C0E-8B56-C0EA556ADA0A@oracle.com>

On Dec 8, 2014, at 4:45 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>> 
>> Also, I'm looking at the source for AbstractQueuedLongSynchronizer.setState (in
>> Java SE 1.8.0_20):
> 
> Try our jsr166 sources from which you can see in CVS history that
> the need for putVolatileLong even here has been forgotten and
> re-remembered a few times :-)
> 

And there is this open bug logged to fix associated areas in the JDK:

  https://bugs.openjdk.java.net/browse/JDK-8044616

Which basically requires someone to create a patch and push in the JDK, which i suppose means i am volunteering :-)

Paul.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141208/d19c24f5/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141208/d19c24f5/attachment.bin>

From martinrb at google.com  Mon Dec  8 14:17:47 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 8 Dec 2014 11:17:47 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEJJKLAA.davidcholmes@aapt.net.au>
References: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEJJKLAA.davidcholmes@aapt.net.au>
Message-ID: <CA+kOe09b_3-08pOt171PYedGoT0Bp5X1HOA7B+PHzCSRMkx6wQ@mail.gmail.com>

On Mon, Dec 8, 2014 at 12:46 AM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Martin,
>
> The paper you cite is about ARM and Power architectures - why do you think the lack of mention of x86/sparc implies those architectures are multiple-copy-atomic?

Reading some more in the same paper, I see:

"""Returning to the two properties above, in TSO a thread can see its
own writes before they become visible to other
threads (by reading them from its write buffer), but any write becomes
visible to all other threads simultaneously: TSO
is a multiple-copy atomic model, in the terminology of Collier
[Col92]. One can also see the possibility of reading
from the local write buffer as allowing a specific kind of local
reordering. A program that writes one location x then
reads another location y might execute by adding the write to x to the
thread?s buffer, then reading y from memory,
before finally making the write to x visible to other threads by
flushing it from the buffer. In this case the thread reads
the value of y that was in the memory before the new write of x hits memory."""

So (as you say) with TSO you don't have a total order of stores if you
read your own writes out of your own CPU's write buffer.  However, my
interpretation of "multiple-copy atomic" is that the initial
publishing thread can choose to use an instruction with sufficiently
strong memory barrier attached (e.g. LOCK;XXX on x86) to write to
memory so that the write buffer is flushed and then use plain relaxed
loads everywhere else to read those memory locations and this explains
the situation on x86 and sparc where volatile writes are expensive and
volatile reads are "free" and you get sequential consistency for Java
volatiles.

http://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf


From martinrb at google.com  Mon Dec  8 14:25:46 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 8 Dec 2014 11:25:46 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5484D769.5030609@oracle.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<5481108A.3030605@oracle.com>
	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>
	<5484D769.5030609@oracle.com>
Message-ID: <CA+kOe0-UvzsMBC1FVv9C69tUwvbYgt5W4F5YvbzeDBtstQzFXg@mail.gmail.com>

Webrev updated to remove the comparison with volatile loads and stores.

On Sun, Dec 7, 2014 at 2:40 PM, David Holmes <david.holmes at oracle.com> wrote:
> On 6/12/2014 7:49 AM, Martin Buchholz wrote:
>>
>> On Thu, Dec 4, 2014 at 5:55 PM, David Holmes <david.holmes at oracle.com>
>> wrote:
>>
>>> In general phrasing like: " also known as a LoadLoad plus LoadStore
>>> barrier
>>> ..." is misleading to me as these are not "aliases"- the loadFence (in
>>> this
>>> case) is being specified to have the same semantics as the
>>> loadload|storeload. It should say "corresponds to a LoadLoad plus
>>> LoadStore
>>> barrier"
>>
>>
>> +     * Ensures that loads before the fence will not be reordered with
>> loads and
>> +     * stores after the fence; also known as a LoadLoad plus LoadStore
>> barrier,
>>
>> I don't understand this.  I believe they _are_ aliases.  The first
>> clause perfectly describes a "LoadLoad plus LoadStore barrier".
>
>
> I find the language use inappropriate - you are defining the first to be the
> second.

Am I missing something?  Is there something else that "LoadLoad plus
LoadStore barrier" (as used in hotspot sources and elsewhere) could
possibly mean?

>>   - as per the "Corresponds to a C11 ...". And referring to things
>>>
>>> like "load-acquire fence" is meaningless without some reference to a
>>> definition - who defines a load-acquire fence? Is there a universal
>>> definition? I would be okay with something looser eg:
>>
>>
>> Well, I'm defining "load-acquire fence" here in the javadoc - I'm
>> claiming that loadFence is also known via other terminology, including
>> "load-acquire fence".  Although it's true that "load-acquire fence" is
>> also used to refer to the corresponding C11 fence, which has subtly
>> different semantics.
>
>
> When you say "also known as XXX" it means that XXX is already defined
> elsewhere. Unless there is a generally accepted definition of XXX then this
> doesn't add much value.

Everything about this topic is confusing, but I continue to think that
"load-acquire fence" is a common industry term.  One of the reasons I
want to include them in the javadoc is precisely because these
different terms are generally used synonymously.

From viktor.klang at gmail.com  Tue Dec  9 05:05:08 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 9 Dec 2014 11:05:08 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-uM03ovA2gLiE7CfvNy4oa1W1kF-CbRG6+Q6OHqYmZHRg@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<547B3ADD.4070700@cs.oswego.edu>
	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
	<CAHJZN-sGQzJFrWnKeoZONjOWiHYeVD=hZ4k6rZMV1ez3cUQ=aw@mail.gmail.com>
	<CANPzfU8Mj0d6_SPV08OttzkbGgVK9ppwic_4wMuDKnWEj6OJ1A@mail.gmail.com>
	<CAHJZN-uM03ovA2gLiE7CfvNy4oa1W1kF-CbRG6+Q6OHqYmZHRg@mail.gmail.com>
Message-ID: <CANPzfU9JCJByyF3uddK_TS4LB4aieLgrk0HyjVOgs-w+pUCt7w@mail.gmail.com>

On Mon, Dec 8, 2014 at 5:35 PM, Josh Humphries <jh at squareup.com> wrote:

> But CompletableFuture is a class, not an interface. So if the
>>> CompletionStage is not a CompletableFuture, you're still back to having to
>>> wrap the CompletionStage. You've just moved the responsibility out of
>>> CompletableFuture and into every other implementation of CompletionStage,
>>> which seems like an unusual choice.
>>>
>>
>> Unusual: yes, unheard of: no. :) that is the same for
>> java.util.Collection which has a toArray (Collection is an interface, Array
>> is highly concrete). To be clear, I concede that it is suboptimal and if I
>> could go without I'd prefer not to have it!
>>
>
> Right, Doug pointed out the same. But for the purpose of inter-op, Future
> (instead of CompletableFuture) would have sufficed.
>

True, but the difference is that if you throw away the stage after you get
`toFuture` your options are: downcasting to CompletionStage (if possible)
or blocking. Not a great tradeoff, right?


>
> BTW, Doug, I saw your other reply. I'm not suggesting that anything needs
> be done about the API. I was just listing some of my nits after Yu Lin
> asked about API differences between Guava's ListenableFuture and Java 8's
> CompletableFuture. I concede that my complaints are largely cosmetic -- the
> API doesn't prevent me from doing what I need to.
>
> You've already mentioned that you'd like to see ExecutorService retired.
>>> It definitely has plenty of sharp corners. But I'll hold judgement on
>>> whether or not its use should be retired for when I see its replacement
>>> (sorry, FJP, you're not it.)
>>>
>>
>> I've pitched the idea to Doug that I'd love to see a FJP implementation
>> without the FJ, please chime in :)
>>
>
> Yeah, that sounds nice, but I'd really have to hear and see more to have a
> strong opinion.
>

Collecting requirements would be a great start :)


>
> I'm actually toying with something that might be kinda-sorta like this
> now. It's like an ExecutorService, but you give each submission a "key"
> (think Actor or Listener) so submissions from outside of the pool (like
> non-FJ code) can try to pin the task to "the right thread" instead of a
> random thread (and hopefully get better L1 cache hit rate). At the same
> time, it also employs work-stealing to improve throughput and fairness.
>

I guess this is another of these things were application-level CPU
scheduling would be beneficial, since thread-pinning to core isn't
JVM-supported.


>
> One difference in what I'm trying to do is that it is specifically for
> sequential delivery of events to listeners (and could be used for executing
> operations on behalf of an actor in something like Akka). So its
> implementation is quite different from FJP since each "actor" has a
> separate queue. So each worker thread can have multiple queues (one per
> actor pinned to that worker) instead of a single work queue.
>

What we do in Akka is that we have a Runnable mailbox for each actor, so
that keeps contention down on the pool.


>
> I suspect there might be something something similar in Akka or Play (I'm
> just guessing -- I have only a passing familiarity with these frameworks).
>
> I'm suddenly kind of curious what folks on this list think of it:
>
> https://code.google.com/p/bluegosling/source/browse/src/com/apriori/concurrent/ActorThreadPool.java
>
>
That's going to take a while to digest :)


>
> RoR is a great example. Many companies today probably wouldn't exist if it
>>> weren't for RoR. Many have had to undergo major architectural change to
>>> later achieve those three things (performance, scalability, and
>>> maintainability). But for plenty (including Square), it was absolutely the
>>> right decision in order to get a product out quickly and at low cost. That
>>> velocity, even if it means future technical pain, can be critical for a
>>> young business in order to get early feedback and validate their business
>>> model as well as start generating revenue.
>>>
>>
>> Absolutely. But I want to build technology that scales, so that one does
>> not have to rebuild once successful, and one can become successful quickly.
>> :)
>>
>
> A virtuous objective indeed! And I think the Play framework is pretty
> attractive for this, although (I must confess) I haven't really used it,
> only looked into a little.
>
> For now, RoR seem to be more attractive to many largely, I think, due to
> Java's reputation (a lot of it due to language's verbosity vs. terseness
> for many idioms, some due its static typing and compile phase which slows
> down iteration [at least it is perceived to slow it down]).
>
> Scala has a bit of an advantage here over Java in that it doesn't have the
> same reputation scars -- at least that's the impression I get from Rubyists
> :)
> But I really hope Java 8 can recover some of that reputation. (It
> certainly deserves to!)
>

I think the main challenge for Java is that it is statement oriented rather
than expression oriented, means that there's a lot of mutation and
verboseness. I'm not sure that's fixable. Would be interesting to see
whether Java gets local type inference or not!

-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141209/1b4f843a/attachment-0001.html>

From martinrb at google.com  Tue Dec  9 12:06:24 2014
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 9 Dec 2014 09:06:24 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <54867FEC.2020802@oracle.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<5481108A.3030605@oracle.com>
	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>
	<5484D769.5030609@oracle.com>
	<CA+kOe0-UvzsMBC1FVv9C69tUwvbYgt5W4F5YvbzeDBtstQzFXg@mail.gmail.com>
	<54867FEC.2020802@oracle.com>
Message-ID: <CA+kOe09Pjg3WCziLmbdgcYHnmxv93M1aXKdsZBEL9Hue1pAuqg@mail.gmail.com>

On Mon, Dec 8, 2014 at 8:51 PM, David Holmes <david.holmes at oracle.com> wrote:

> Then please point me to the common industry definition of it because I
> couldn't find anything definitive. And as you state yourself above one
> definition of it - the corresponding C11 fence - does not in fact have the
> same semantics!

I changed to the terminology "acquire fence" and "release fence" as
popularized by preshing
http://preshing.com/20130922/acquire-and-release-fences/

     * Ensures that loads and stores before the fence will not be reordered with
     * stores after the fence; also referred to as a StoreStore plus LoadStore
     * barrier, or as a "release fence".

From martinrb at google.com  Tue Dec  9 13:15:03 2014
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 9 Dec 2014 10:15:03 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <54867C1F.1090106@oracle.com>
References: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEJJKLAA.davidcholmes@aapt.net.au>
	<CA+kOe09b_3-08pOt171PYedGoT0Bp5X1HOA7B+PHzCSRMkx6wQ@mail.gmail.com>
	<54867C1F.1090106@oracle.com>
Message-ID: <CA+kOe09aURqjRRk_gF8JtCAaJk+Fja8pTYmNY-B=DS7wuWXr1w@mail.gmail.com>

On Mon, Dec 8, 2014 at 8:35 PM, David Holmes <david.holmes at oracle.com> wrote:

>> So (as you say) with TSO you don't have a total order of stores if you
>> read your own writes out of your own CPU's write buffer.  However, my
>> interpretation of "multiple-copy atomic" is that the initial
>> publishing thread can choose to use an instruction with sufficiently
>> strong memory barrier attached (e.g. LOCK;XXX on x86) to write to
>> memory so that the write buffer is flushed and then use plain relaxed
>> loads everywhere else to read those memory locations and this explains
>> the situation on x86 and sparc where volatile writes are expensive and
>> volatile reads are "free" and you get sequential consistency for Java
>> volatiles.
>
>
> We don't use lock'd instructions for volatile stores on x86, but the
> trailing mfence achieves the "flushing".
>
> However this still raised some questions for me. Using a mfence on x86 or
> equivalent on sparc, is no different to issuing a "DMB SYNC" on ARM, or a
> SYNC on PowerPC. They each ensure TSO for volatile stores with global
> visibility. So when such fences are used the resulting system should be
> multiple-copy atomic - no? (No!**) And there seems to be an equivalence
> between being multiple-copy atomic and providing the IRIW property. Yet we
> know that on ARM/Power, as per the paper, TSO with global visibility is not

ARM/Power don't have TSO.

> sufficient to achieve IRIW. So what is it that x86 and sparc have in
> addition to TSO that provides for IRIW?

We have both been learning.... to think in new ways.
 I found the second section of Peter Sewell's tutorial
2 From Sequential Consistency to Relaxed Memory Models
to be most useful, especially the diagrams.

> I pondered this for quite a while before realizing that the mfence on x86
> (or equivalent on sparc) is not in fact playing the same role as the
> DMB/SYNC on ARM/PPC. The key property that x86 and sparc have (and we can
> ignore the store buffers) is that stores become globally visible - if any
> other thread sees a store then all other threads see the same store. Whereas
> on ARM/PPC you can imagine a store casually making its way through the
> system, gradually becoming visible to more and more threads - unless there
> is a DMB/SYNC to force a globally consistent memory view. Hence for IRIW
> placing the DMB/SYNC after the store does not suffice because prior to the
> DMB/SYNC the store may be visible to an arbitrary subset of threads.
> Consequently IRIW requires the DMB/SYNC between the loads - to ensure that
> each thread on their second load, must see the value that the other thread
> saw on its first load (ref Section 6.1 of the paper).
>
> ** So using DMB/SYNC does not achieve multiple-copy atomicity, because until
> the DMB/SYNC happens different threads can have different views of memory.

To me, the most desirable property of x86-style TSO is that barriers
are only necessary on stores to achieve sequential consistency - the
publisher gets to decide.  Volatile reads can then be close to free.

> All of which reinforces to me that IRIW is an undesirable property to have
> to implement. YMMV. (And I also need to re-examine the PPC64 implementation
> to see exactly where they add/remove barriers when IRIW is enabled.)

I believe you get a full sync between volatile reads.

#define GET_FIELD_VOLATILE(obj, offset, type_name, v) \
  oop p = JNIHandles::resolve(obj); \
  if (support_IRIW_for_not_multiple_copy_atomic_cpu) { \
    OrderAccess::fence(); \
  } \


> Cheers,
> David
>
>> http://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf

From oleksandr.otenko at oracle.com  Tue Dec  9 15:04:16 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 09 Dec 2014 20:04:16 +0000
Subject: [concurrency-interest] RFR: 8065804:
 JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <CAPUmR1YdWyemKveLA0eHpv_MC=5BjVJtfLkj2RTZAhz_NFivog@mail.gmail.com>
References: <2458774.qkf0xEMZGz@d-allen>	<NFBBKALFDCPFIDBNKAPCAECMKLAA.davidcholmes@aapt.net.au>
	<CAPUmR1YdWyemKveLA0eHpv_MC=5BjVJtfLkj2RTZAhz_NFivog@mail.gmail.com>
Message-ID: <548755C0.5040104@oracle.com>

On 26/11/2014 02:04, Hans Boehm wrote:
> To be concrete here, on Power, loads can normally be ordered by an 
> address dependency or light-weight fence (lwsync).  However, neither 
> is enough to prevent the questionable outcome for IRIW, since it 
> doesn't ensure that the stores in T1 and T2 will be made visible to 
> other threads in a consistent order.  That outcome can be prevented by 
> using heavyweight fences (sync) instructions between the loads instead.

Why would they need fences between loads instead of syncing the order of 
stores?


Alex


> Peter Sewell's group concluded that to enforce correct volatile 
> behavior on Power, you essentially need a a heavyweight fence between 
> every pair of volatile operations on Power.  That cannot be understood 
> based on simple ordering constraints.
>
> As Stephan pointed out, there are similar issues on ARM, but they're 
> less commonly encountered in a Java implementation.  If you're lucky, 
> you can get to the right implementation recipe by looking at only 
> reordering, I think.
>
>
> On Tue, Nov 25, 2014 at 4:36 PM, David Holmes 
> <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>
>     Stephan Diestelhorst writes:
>     >
>     > David Holmes wrote:
>     > > Stephan Diestelhorst writes:
>     > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
>     > > > > I'm no hardware architect, but fundamentally it seems to
>     me that
>     > > > >
>     > > > > load x
>     > > > > acquire_fence
>     > > > >
>     > > > > imposes a much more stringent constraint than
>     > > > >
>     > > > > load_acquire x
>     > > > >
>     > > > > Consider the case in which the load from x is an L1 hit, but a
>     > > > > preceding load (from say y) is a long-latency miss.  If we
>     enforce
>     > > > > ordering by just waiting for completion of prior
>     operation, the
>     > > > > former has to wait for the load from y to complete; while the
>     > > > > latter doesn't.  I find it hard to believe that this
>     doesn't leave
>     > > > > an appreciable amount of performance on the table, at
>     least for
>     > > > > some interesting microarchitectures.
>     > > >
>     > > > I agree, Hans, that this is a reasonable assumption. 
>     Load_acquire x
>     > > > does allow roach motel, whereas the acquire fence does not.
>     > > >
>     > > > >  In addition, for better or worse, fencing requirements on
>     at least
>     > > > >  Power are actually driven as much by store atomicity
>     issues, as by
>     > > > >  the ordering issues discussed in the cookbook.  This was not
>     > > > >  understood in 2005, and unfortunately doesn't seem to be
>     > amenable to
>     > > > >  the kind of straightforward explanation as in Doug's
>     cookbook.
>     > > >
>     > > > Coming from a strongly ordered architecture to a weakly
>     ordered one
>     > > > myself, I also needed some mental adjustment about store
>     (multi-copy)
>     > > > atomicity.  I can imagine others will be unaware of this
>     difference,
>     > > > too, even in 2014.
>     > >
>     > > Sorry I'm missing the connection between fences and multi-copy
>     > atomicity.
>     >
>     > One example is the classic IRIW.  With non-multi copy atomic
>     stores, but
>     > ordered (say through a dependency) loads in the following example:
>     >
>     > Memory: foo = bar = 0
>     > _T1_         _T2_         _T3_         _T4_
>     > st (foo),1   st (bar),1   ld r1, (bar)         ld r3,(foo)
>     >                           <addr dep / local "fence" here> 
>      <addr dep>
>     >                           ld r2, (foo)         ld r4, (bar)
>     >
>     > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy
>     atomic
>     > machines.  On TSO boxes, this is not possible. That means that the
>     > memory fence that will prevent such a behaviour (DMB on ARM)
>     needs to
>     > carry some additional oomph in ensuring multi-copy atomicity, or
>     rather
>     > prevent you from seeing it (which is the same thing).
>
>     I take it as given that any code for which you may have ordering
>     constraints, must first have basic atomicity properties for loads and
>     stores. I would not expect any kind of fence to add
>     multi-copy-atomicity
>     where there was none.
>
>     David
>
>     > Stephan
>     >
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141209/54c8853a/attachment.html>

From martinrb at google.com  Tue Dec  9 15:23:17 2014
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 9 Dec 2014 12:23:17 -0800
Subject: [concurrency-interest] RFR: 8065804:
 JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <548755C0.5040104@oracle.com>
References: <2458774.qkf0xEMZGz@d-allen>
	<NFBBKALFDCPFIDBNKAPCAECMKLAA.davidcholmes@aapt.net.au>
	<CAPUmR1YdWyemKveLA0eHpv_MC=5BjVJtfLkj2RTZAhz_NFivog@mail.gmail.com>
	<548755C0.5040104@oracle.com>
Message-ID: <CA+kOe09GDM61sqfyZ6CPmP=ptc1d7siJcxnteinz8_xN2WvDFg@mail.gmail.com>

On Tue, Dec 9, 2014 at 12:04 PM, Oleksandr Otenko
<oleksandr.otenko at oracle.com> wrote:
> On 26/11/2014 02:04, Hans Boehm wrote:
>>
>> To be concrete here, on Power, loads can normally be ordered by an address
>> dependency or light-weight fence (lwsync).  However, neither is enough to
>> prevent the questionable outcome for IRIW, since it doesn't ensure that the
>> stores in T1 and T2 will be made visible to other threads in a consistent
>> order.  That outcome can be prevented by using heavyweight fences (sync)
>> instructions between the loads instead.
>
>
> Why would they need fences between loads instead of syncing the order of
> stores?

Well explained by
http://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf

From oleksandr.otenko at oracle.com  Tue Dec  9 15:34:25 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 09 Dec 2014 20:34:25 +0000
Subject: [concurrency-interest] RFR: 8065804:
 JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIECMKLAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCIECMKLAA.davidcholmes@aapt.net.au>
Message-ID: <54875CD1.4010801@oracle.com>

Is the thorn the many allowed outcomes, or the single disallowed 
outcome? (eg order consistency is too strict for stores with no 
synchronizes-with between them?)

Alex


On 26/11/2014 02:10, David Holmes wrote:
> Hi Hans,
> Given IRIW is a thorn in everyone's side and has no known useful 
> benefit, and can hopefully be killed off in the future, lets not get 
> bogged down in IRIW. But none of what you say below relates to 
> multi-copy-atomicity.
> Cheers,
> David
>
>     -----Original Message-----
>     *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf
>     Of *Hans Boehm
>     *Sent:* Wednesday, 26 November 2014 12:04 PM
>     *To:* dholmes at ieee.org
>     *Cc:* Stephan Diestelhorst; concurrency-interest at cs.oswego.edu;
>     core-libs-dev
>     *Subject:* Re: [concurrency-interest] RFR: 8065804:
>     JEP171:Clarifications/corrections for fence intrinsics
>
>     To be concrete here, on Power, loads can normally be ordered by an
>     address dependency or light-weight fence (lwsync).  However,
>     neither is enough to prevent the questionable outcome for IRIW,
>     since it doesn't ensure that the stores in T1 and T2 will be made
>     visible to other threads in a consistent order.  That outcome can
>     be prevented by using heavyweight fences (sync) instructions
>     between the loads instead.  Peter Sewell's group concluded that to
>     enforce correct volatile behavior on Power, you essentially need a
>     a heavyweight fence between every pair of volatile operations on
>     Power.  That cannot be understood based on simple ordering
>     constraints.
>
>     As Stephan pointed out, there are similar issues on ARM, but
>     they're less commonly encountered in a Java implementation.  If
>     you're lucky, you can get to the right implementation recipe by
>     looking at only reordering, I think.
>
>
>     On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
>     <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>
>         Stephan Diestelhorst writes:
>         >
>         > David Holmes wrote:
>         > > Stephan Diestelhorst writes:
>         > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
>         > > > > I'm no hardware architect, but fundamentally it seems
>         to me that
>         > > > >
>         > > > > load x
>         > > > > acquire_fence
>         > > > >
>         > > > > imposes a much more stringent constraint than
>         > > > >
>         > > > > load_acquire x
>         > > > >
>         > > > > Consider the case in which the load from x is an L1
>         hit, but a
>         > > > > preceding load (from say y) is a long-latency miss. 
>         If we enforce
>         > > > > ordering by just waiting for completion of prior
>         operation, the
>         > > > > former has to wait for the load from y to complete;
>         while the
>         > > > > latter doesn't.  I find it hard to believe that this
>         doesn't leave
>         > > > > an appreciable amount of performance on the table, at
>         least for
>         > > > > some interesting microarchitectures.
>         > > >
>         > > > I agree, Hans, that this is a reasonable assumption. 
>         Load_acquire x
>         > > > does allow roach motel, whereas the acquire fence does not.
>         > > >
>         > > > >  In addition, for better or worse, fencing
>         requirements on at least
>         > > > >  Power are actually driven as much by store atomicity
>         issues, as by
>         > > > >  the ordering issues discussed in the cookbook.  This
>         was not
>         > > > >  understood in 2005, and unfortunately doesn't seem to be
>         > amenable to
>         > > > >  the kind of straightforward explanation as in Doug's
>         cookbook.
>         > > >
>         > > > Coming from a strongly ordered architecture to a weakly
>         ordered one
>         > > > myself, I also needed some mental adjustment about store
>         (multi-copy)
>         > > > atomicity.  I can imagine others will be unaware of this
>         difference,
>         > > > too, even in 2014.
>         > >
>         > > Sorry I'm missing the connection between fences and multi-copy
>         > atomicity.
>         >
>         > One example is the classic IRIW.  With non-multi copy atomic
>         stores, but
>         > ordered (say through a dependency) loads in the following
>         example:
>         >
>         > Memory: foo = bar = 0
>         > _T1_         _T2_         _T3_           _T4_
>         > st (foo),1   st (bar),1   ld r1, (bar)           ld r3,(foo)
>         >                           <addr dep / local "fence" here> 
>          <addr dep>
>         >                           ld r2, (foo)           ld r4, (bar)
>         >
>         > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
>         non-multi-copy atomic
>         > machines.  On TSO boxes, this is not possible. That means
>         that the
>         > memory fence that will prevent such a behaviour (DMB on ARM)
>         needs to
>         > carry some additional oomph in ensuring multi-copy
>         atomicity, or rather
>         > prevent you from seeing it (which is the same thing).
>
>         I take it as given that any code for which you may have ordering
>         constraints, must first have basic atomicity properties for
>         loads and
>         stores. I would not expect any kind of fence to add
>         multi-copy-atomicity
>         where there was none.
>
>         David
>
>         > Stephan
>         >
>         > _______________________________________________
>         > Concurrency-interest mailing list
>         > Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141209/d8715297/attachment.html>

From davidcholmes at aapt.net.au  Tue Dec  9 16:36:30 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 10 Dec 2014 07:36:30 +1000
Subject: [concurrency-interest] RFR: 8065804:
	JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <54875CD1.4010801@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEKDKLAA.davidcholmes@aapt.net.au>

The "thorn" is the need for the barriers in the readers not the writers. (or
perhaps as well as the writers in some cases - that is part of the problem.)

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr
Otenko
  Sent: Wednesday, 10 December 2014 6:34 AM
  To: dholmes at ieee.org; Hans Boehm
  Cc: core-libs-dev; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


  Is the thorn the many allowed outcomes, or the single disallowed outcome?
(eg order consistency is too strict for stores with no synchronizes-with
between them?)

  Alex



  On 26/11/2014 02:10, David Holmes wrote:

    Hi Hans,
    Given IRIW is a thorn in everyone's side and has no known useful
benefit, and can hopefully be killed off in the future, lets not get bogged
down in IRIW. But none of what you say below relates to
multi-copy-atomicity.

    Cheers,
    David
      -----Original Message-----
      From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of
Hans Boehm
      Sent: Wednesday, 26 November 2014 12:04 PM
      To: dholmes at ieee.org
      Cc: Stephan Diestelhorst; concurrency-interest at cs.oswego.edu;
core-libs-dev
      Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


      To be concrete here, on Power, loads can normally be ordered by an
address dependency or light-weight fence (lwsync).  However, neither is
enough to prevent the questionable outcome for IRIW, since it doesn't ensure
that the stores in T1 and T2 will be made visible to other threads in a
consistent order.  That outcome can be prevented by using heavyweight fences
(sync) instructions between the loads instead.  Peter Sewell's group
concluded that to enforce correct volatile behavior on Power, you
essentially need a a heavyweight fence between every pair of volatile
operations on Power.  That cannot be understood based on simple ordering
constraints.


      As Stephan pointed out, there are similar issues on ARM, but they're
less commonly encountered in a Java implementation.  If you're lucky, you
can get to the right implementation recipe by looking at only reordering, I
think.




      On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
<davidcholmes at aapt.net.au> wrote:

        Stephan Diestelhorst writes:
        >
        > David Holmes wrote:
        > > Stephan Diestelhorst writes:
        > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
        > > > > I'm no hardware architect, but fundamentally it seems to me
that
        > > > >
        > > > > load x
        > > > > acquire_fence
        > > > >
        > > > > imposes a much more stringent constraint than
        > > > >
        > > > > load_acquire x
        > > > >
        > > > > Consider the case in which the load from x is an L1 hit, but
a
        > > > > preceding load (from say y) is a long-latency miss.  If we
enforce
        > > > > ordering by just waiting for completion of prior operation,
the
        > > > > former has to wait for the load from y to complete; while
the
        > > > > latter doesn't.  I find it hard to believe that this doesn't
leave
        > > > > an appreciable amount of performance on the table, at least
for
        > > > > some interesting microarchitectures.
        > > >
        > > > I agree, Hans, that this is a reasonable assumption.
Load_acquire x
        > > > does allow roach motel, whereas the acquire fence does not.
        > > >
        > > > >  In addition, for better or worse, fencing requirements on
at least
        > > > >  Power are actually driven as much by store atomicity
issues, as by
        > > > >  the ordering issues discussed in the cookbook.  This was
not
        > > > >  understood in 2005, and unfortunately doesn't seem to be
        > amenable to
        > > > >  the kind of straightforward explanation as in Doug's
cookbook.
        > > >
        > > > Coming from a strongly ordered architecture to a weakly
ordered one
        > > > myself, I also needed some mental adjustment about store
(multi-copy)
        > > > atomicity.  I can imagine others will be unaware of this
difference,
        > > > too, even in 2014.
        > >
        > > Sorry I'm missing the connection between fences and multi-copy
        > atomicity.
        >
        > One example is the classic IRIW.  With non-multi copy atomic
stores, but
        > ordered (say through a dependency) loads in the following example:
        >
        > Memory: foo = bar = 0
        > _T1_         _T2_         _T3_                              _T4_
        > st (foo),1   st (bar),1   ld r1, (bar)                      ld
r3,(foo)
        >                           <addr dep / local "fence" here>   <addr
dep>
        >                           ld r2, (foo)                      ld r4,
(bar)
        >
        > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy
atomic
        > machines.  On TSO boxes, this is not possible.  That means that
the
        > memory fence that will prevent such a behaviour (DMB on ARM) needs
to
        > carry some additional oomph in ensuring multi-copy atomicity, or
rather
        > prevent you from seeing it (which is the same thing).


        I take it as given that any code for which you may have ordering
        constraints, must first have basic atomicity properties for loads
and
        stores. I would not expect any kind of fence to add
multi-copy-atomicity
        where there was none.

        David


        > Stephan
        >
        > _______________________________________________
        > Concurrency-interest mailing list
        > Concurrency-interest at cs.oswego.edu
        > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

        _______________________________________________
        Concurrency-interest mailing list
        Concurrency-interest at cs.oswego.edu
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141210/a5d78588/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Dec  9 16:37:55 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 10 Dec 2014 07:37:55 +1000
Subject: [concurrency-interest] RFR: 8065804:
	JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <548755C0.5040104@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEKDKLAA.davidcholmes@aapt.net.au>

See my earlier response to Martin. The reader has to force a consistent view
of memory - the writer can't as the write escapes before it can issue the
barrier.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr
Otenko
  Sent: Wednesday, 10 December 2014 6:04 AM
  To: Hans Boehm; dholmes at ieee.org
  Cc: core-libs-dev; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


  On 26/11/2014 02:04, Hans Boehm wrote:

    To be concrete here, on Power, loads can normally be ordered by an
address dependency or light-weight fence (lwsync).  However, neither is
enough to prevent the questionable outcome for IRIW, since it doesn't ensure
that the stores in T1 and T2 will be made visible to other threads in a
consistent order.  That outcome can be prevented by using heavyweight fences
(sync) instructions between the loads instead.


  Why would they need fences between loads instead of syncing the order of
stores?


  Alex



    Peter Sewell's group concluded that to enforce correct volatile behavior
on Power, you essentially need a a heavyweight fence between every pair of
volatile operations on Power.  That cannot be understood based on simple
ordering constraints.


    As Stephan pointed out, there are similar issues on ARM, but they're
less commonly encountered in a Java implementation.  If you're lucky, you
can get to the right implementation recipe by looking at only reordering, I
think.




    On Tue, Nov 25, 2014 at 4:36 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

      Stephan Diestelhorst writes:
      >
      > David Holmes wrote:
      > > Stephan Diestelhorst writes:
      > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
      > > > > I'm no hardware architect, but fundamentally it seems to me
that
      > > > >
      > > > > load x
      > > > > acquire_fence
      > > > >
      > > > > imposes a much more stringent constraint than
      > > > >
      > > > > load_acquire x
      > > > >
      > > > > Consider the case in which the load from x is an L1 hit, but a
      > > > > preceding load (from say y) is a long-latency miss.  If we
enforce
      > > > > ordering by just waiting for completion of prior operation,
the
      > > > > former has to wait for the load from y to complete; while the
      > > > > latter doesn't.  I find it hard to believe that this doesn't
leave
      > > > > an appreciable amount of performance on the table, at least
for
      > > > > some interesting microarchitectures.
      > > >
      > > > I agree, Hans, that this is a reasonable assumption.
Load_acquire x
      > > > does allow roach motel, whereas the acquire fence does not.
      > > >
      > > > >  In addition, for better or worse, fencing requirements on at
least
      > > > >  Power are actually driven as much by store atomicity issues,
as by
      > > > >  the ordering issues discussed in the cookbook.  This was not
      > > > >  understood in 2005, and unfortunately doesn't seem to be
      > amenable to
      > > > >  the kind of straightforward explanation as in Doug's
cookbook.
      > > >
      > > > Coming from a strongly ordered architecture to a weakly ordered
one
      > > > myself, I also needed some mental adjustment about store
(multi-copy)
      > > > atomicity.  I can imagine others will be unaware of this
difference,
      > > > too, even in 2014.
      > >
      > > Sorry I'm missing the connection between fences and multi-copy
      > atomicity.
      >
      > One example is the classic IRIW.  With non-multi copy atomic stores,
but
      > ordered (say through a dependency) loads in the following example:
      >
      > Memory: foo = bar = 0
      > _T1_         _T2_         _T3_                              _T4_
      > st (foo),1   st (bar),1   ld r1, (bar)                      ld
r3,(foo)
      >                           <addr dep / local "fence" here>   <addr
dep>
      >                           ld r2, (foo)                      ld r4,
(bar)
      >
      > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy
atomic
      > machines.  On TSO boxes, this is not possible.  That means that the
      > memory fence that will prevent such a behaviour (DMB on ARM) needs
to
      > carry some additional oomph in ensuring multi-copy atomicity, or
rather
      > prevent you from seeing it (which is the same thing).


      I take it as given that any code for which you may have ordering
      constraints, must first have basic atomicity properties for loads and
      stores. I would not expect any kind of fence to add
multi-copy-atomicity
      where there was none.

      David


      > Stephan
      >
      > _______________________________________________
      > Concurrency-interest mailing list
      > Concurrency-interest at cs.oswego.edu
      > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141210/ff53f85e/attachment.html>

From oleksandr.otenko at oracle.com  Tue Dec  9 16:53:32 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 09 Dec 2014 21:53:32 +0000
Subject: [concurrency-interest] RFR: 8065804:
 JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEKDKLAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOEKDKLAA.davidcholmes@aapt.net.au>
Message-ID: <54876F5C.5070800@oracle.com>

Yes, I do understand the reader needs barriers, too. I guess I was 
wondering more why the reader would need something stronger than what 
dependencies etc could enforce. I guess I'll read what Martin forwarded 
first.

Alex


On 09/12/2014 21:37, David Holmes wrote:
> See my earlier response to Martin. The reader has to force a 
> consistent view of memory - the writer can't as the write escapes 
> before it can issue the barrier.
> David
>
>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Oleksandr Otenko
>     *Sent:* Wednesday, 10 December 2014 6:04 AM
>     *To:* Hans Boehm; dholmes at ieee.org
>     *Cc:* core-libs-dev; concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] RFR: 8065804:
>     JEP171:Clarifications/corrections for fence intrinsics
>
>     On 26/11/2014 02:04, Hans Boehm wrote:
>>     To be concrete here, on Power, loads can normally be ordered by
>>     an address dependency or light-weight fence (lwsync).  However,
>>     neither is enough to prevent the questionable outcome for IRIW,
>>     since it doesn't ensure that the stores in T1 and T2 will be made
>>     visible to other threads in a consistent order.  That outcome can
>>     be prevented by using heavyweight fences (sync) instructions
>>     between the loads instead.
>
>     Why would they need fences between loads instead of syncing the
>     order of stores?
>
>
>     Alex
>
>
>>     Peter Sewell's group concluded that to enforce correct volatile
>>     behavior on Power, you essentially need a a heavyweight fence
>>     between every pair of volatile operations on Power.  That cannot
>>     be understood based on simple ordering constraints.
>>
>>     As Stephan pointed out, there are similar issues on ARM, but
>>     they're less commonly encountered in a Java implementation.  If
>>     you're lucky, you can get to the right implementation recipe by
>>     looking at only reordering, I think.
>>
>>
>>     On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
>>     <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>>
>>         Stephan Diestelhorst writes:
>>         >
>>         > David Holmes wrote:
>>         > > Stephan Diestelhorst writes:
>>         > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans
>>         Boehm:
>>         > > > > I'm no hardware architect, but fundamentally it seems
>>         to me that
>>         > > > >
>>         > > > > load x
>>         > > > > acquire_fence
>>         > > > >
>>         > > > > imposes a much more stringent constraint than
>>         > > > >
>>         > > > > load_acquire x
>>         > > > >
>>         > > > > Consider the case in which the load from x is an L1
>>         hit, but a
>>         > > > > preceding load (from say y) is a long-latency miss. 
>>         If we enforce
>>         > > > > ordering by just waiting for completion of prior
>>         operation, the
>>         > > > > former has to wait for the load from y to complete;
>>         while the
>>         > > > > latter doesn't.  I find it hard to believe that this
>>         doesn't leave
>>         > > > > an appreciable amount of performance on the table, at
>>         least for
>>         > > > > some interesting microarchitectures.
>>         > > >
>>         > > > I agree, Hans, that this is a reasonable assumption. 
>>         Load_acquire x
>>         > > > does allow roach motel, whereas the acquire fence does not.
>>         > > >
>>         > > > >  In addition, for better or worse, fencing
>>         requirements on at least
>>         > > > >  Power are actually driven as much by store atomicity
>>         issues, as by
>>         > > > >  the ordering issues discussed in the cookbook.  This
>>         was not
>>         > > > >  understood in 2005, and unfortunately doesn't seem to be
>>         > amenable to
>>         > > > >  the kind of straightforward explanation as in Doug's
>>         cookbook.
>>         > > >
>>         > > > Coming from a strongly ordered architecture to a weakly
>>         ordered one
>>         > > > myself, I also needed some mental adjustment about
>>         store (multi-copy)
>>         > > > atomicity.  I can imagine others will be unaware of
>>         this difference,
>>         > > > too, even in 2014.
>>         > >
>>         > > Sorry I'm missing the connection between fences and
>>         multi-copy
>>         > atomicity.
>>         >
>>         > One example is the classic IRIW.  With non-multi copy
>>         atomic stores, but
>>         > ordered (say through a dependency) loads in the following
>>         example:
>>         >
>>         > Memory: foo = bar = 0
>>         > _T1_         _T2_         _T3_             _T4_
>>         > st (foo),1   st (bar),1   ld r1, (bar)             ld r3,(foo)
>>         >                           <addr dep / local "fence" here> 
>>          <addr dep>
>>         >                           ld r2, (foo)             ld r4, (bar)
>>         >
>>         > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
>>         non-multi-copy atomic
>>         > machines.  On TSO boxes, this is not possible. That means
>>         that the
>>         > memory fence that will prevent such a behaviour (DMB on
>>         ARM) needs to
>>         > carry some additional oomph in ensuring multi-copy
>>         atomicity, or rather
>>         > prevent you from seeing it (which is the same thing).
>>
>>         I take it as given that any code for which you may have ordering
>>         constraints, must first have basic atomicity properties for
>>         loads and
>>         stores. I would not expect any kind of fence to add
>>         multi-copy-atomicity
>>         where there was none.
>>
>>         David
>>
>>         > Stephan
>>         >
>>         > _______________________________________________
>>         > Concurrency-interest mailing list
>>         > Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141209/32cd8b3e/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Dec  9 16:59:11 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 10 Dec 2014 07:59:11 +1000
Subject: [concurrency-interest] RFR: 8065804:
	JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <54876F5C.5070800@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEKEKLAA.davidcholmes@aapt.net.au>

In this case the issue is not ordering per-se (which is what dependencies
help with) but global visibility. After performing the first read each
thread must ensure that its second read will return what the other thread
saw for the first read - hence a full dmb/sync between the reads; or
generalizing a full dmb/sync after every volatile read.

David
  -----Original Message-----
  From: Oleksandr Otenko [mailto:oleksandr.otenko at oracle.com]
  Sent: Wednesday, 10 December 2014 7:54 AM
  To: dholmes at ieee.org; Hans Boehm
  Cc: core-libs-dev; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


  Yes, I do understand the reader needs barriers, too. I guess I was
wondering more why the reader would need something stronger than what
dependencies etc could enforce. I guess I'll read what Martin forwarded
first.

  Alex



  On 09/12/2014 21:37, David Holmes wrote:

    See my earlier response to Martin. The reader has to force a consistent
view of memory - the writer can't as the write escapes before it can issue
the barrier.

    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr
Otenko
      Sent: Wednesday, 10 December 2014 6:04 AM
      To: Hans Boehm; dholmes at ieee.org
      Cc: core-libs-dev; concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


      On 26/11/2014 02:04, Hans Boehm wrote:

        To be concrete here, on Power, loads can normally be ordered by an
address dependency or light-weight fence (lwsync).  However, neither is
enough to prevent the questionable outcome for IRIW, since it doesn't ensure
that the stores in T1 and T2 will be made visible to other threads in a
consistent order.  That outcome can be prevented by using heavyweight fences
(sync) instructions between the loads instead.


      Why would they need fences between loads instead of syncing the order
of stores?


      Alex



        Peter Sewell's group concluded that to enforce correct volatile
behavior on Power, you essentially need a a heavyweight fence between every
pair of volatile operations on Power.  That cannot be understood based on
simple ordering constraints.


        As Stephan pointed out, there are similar issues on ARM, but they're
less commonly encountered in a Java implementation.  If you're lucky, you
can get to the right implementation recipe by looking at only reordering, I
think.




        On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
<davidcholmes at aapt.net.au> wrote:

          Stephan Diestelhorst writes:
          >
          > David Holmes wrote:
          > > Stephan Diestelhorst writes:
          > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
          > > > > I'm no hardware architect, but fundamentally it seems to
me that
          > > > >
          > > > > load x
          > > > > acquire_fence
          > > > >
          > > > > imposes a much more stringent constraint than
          > > > >
          > > > > load_acquire x
          > > > >
          > > > > Consider the case in which the load from x is an L1 hit,
but a
          > > > > preceding load (from say y) is a long-latency miss.  If we
enforce
          > > > > ordering by just waiting for completion of prior
operation, the
          > > > > former has to wait for the load from y to complete; while
the
          > > > > latter doesn't.  I find it hard to believe that this
doesn't leave
          > > > > an appreciable amount of performance on the table, at
least for
          > > > > some interesting microarchitectures.
          > > >
          > > > I agree, Hans, that this is a reasonable assumption.
Load_acquire x
          > > > does allow roach motel, whereas the acquire fence does not.
          > > >
          > > > >  In addition, for better or worse, fencing requirements on
at least
          > > > >  Power are actually driven as much by store atomicity
issues, as by
          > > > >  the ordering issues discussed in the cookbook.  This was
not
          > > > >  understood in 2005, and unfortunately doesn't seem to be
          > amenable to
          > > > >  the kind of straightforward explanation as in Doug's
cookbook.
          > > >
          > > > Coming from a strongly ordered architecture to a weakly
ordered one
          > > > myself, I also needed some mental adjustment about store
(multi-copy)
          > > > atomicity.  I can imagine others will be unaware of this
difference,
          > > > too, even in 2014.
          > >
          > > Sorry I'm missing the connection between fences and multi-copy
          > atomicity.
          >
          > One example is the classic IRIW.  With non-multi copy atomic
stores, but
          > ordered (say through a dependency) loads in the following
example:
          >
          > Memory: foo = bar = 0
          > _T1_         _T2_         _T3_                              _T4_
          > st (foo),1   st (bar),1   ld r1, (bar)                      ld
r3,(foo)
          >                           <addr dep / local "fence" here>
<addr dep>
          >                           ld r2, (foo)                      ld
r4, (bar)
          >
          > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy
atomic
          > machines.  On TSO boxes, this is not possible.  That means that
the
          > memory fence that will prevent such a behaviour (DMB on ARM)
needs to
          > carry some additional oomph in ensuring multi-copy atomicity, or
rather
          > prevent you from seeing it (which is the same thing).


          I take it as given that any code for which you may have ordering
          constraints, must first have basic atomicity properties for loads
and
          stores. I would not expect any kind of fence to add
multi-copy-atomicity
          where there was none.

          David


          > Stephan
          >
          > _______________________________________________
          > Concurrency-interest mailing list
          > Concurrency-interest at cs.oswego.edu
          > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

          _______________________________________________
          Concurrency-interest mailing list
          Concurrency-interest at cs.oswego.edu
          http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141210/ae4720ee/attachment.html>

From oleksandr.otenko at oracle.com  Tue Dec  9 17:21:09 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 09 Dec 2014 22:21:09 +0000
Subject: [concurrency-interest] RFR: 8065804:
 JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEKDKLAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEKDKLAA.davidcholmes@aapt.net.au>
Message-ID: <548775D5.6080809@oracle.com>

In that case I must say I can't see why you mentioned "no known useful 
benefit". The known useful benefit from ordering reads can be seen here:

store in one order:
Thread 1:
x=1
y=1

load in reverse order:
Thread 2:
r1=y;
r2=x;

This is a common pattern, so ordering loads is already useful. Here, 
even though JMM talks about total order of all volatile operations, in 
practice the order of loads is weaker, as long as this weakening cannot 
be observed - eg on x86 enforcing order of loads among themselves is an 
entirely local matter.

IRIW extends the store part to many threads, thus guaranteeing total 
store order for volatiles. I thought the total ordering of stores would 
be a more contentious point (but I agree with the point Hans makes about 
easier reasoning).

Alex

On 09/12/2014 21:36, David Holmes wrote:
> The "thorn" is the need for the barriers in the readers not the 
> writers. (or perhaps as well as the writers in some cases - that is 
> part of the problem.)
> David
>
>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Oleksandr Otenko
>     *Sent:* Wednesday, 10 December 2014 6:34 AM
>     *To:* dholmes at ieee.org; Hans Boehm
>     *Cc:* core-libs-dev; concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] RFR: 8065804:
>     JEP171:Clarifications/corrections for fence intrinsics
>
>     Is the thorn the many allowed outcomes, or the single disallowed
>     outcome? (eg order consistency is too strict for stores with no
>     synchronizes-with between them?)
>
>     Alex
>
>
>     On 26/11/2014 02:10, David Holmes wrote:
>>     Hi Hans,
>>     Given IRIW is a thorn in everyone's side and has no known useful
>>     benefit, and can hopefully be killed off in the future, lets not
>>     get bogged down in IRIW. But none of what you say below relates
>>     to multi-copy-atomicity.
>>     Cheers,
>>     David
>>
>>         -----Original Message-----
>>         *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On
>>         Behalf Of *Hans Boehm
>>         *Sent:* Wednesday, 26 November 2014 12:04 PM
>>         *To:* dholmes at ieee.org
>>         *Cc:* Stephan Diestelhorst;
>>         concurrency-interest at cs.oswego.edu; core-libs-dev
>>         *Subject:* Re: [concurrency-interest] RFR: 8065804:
>>         JEP171:Clarifications/corrections for fence intrinsics
>>
>>         To be concrete here, on Power, loads can normally be ordered
>>         by an address dependency or light-weight fence (lwsync). 
>>         However, neither is enough to prevent the questionable
>>         outcome for IRIW, since it doesn't ensure that the stores in
>>         T1 and T2 will be made visible to other threads in a
>>         consistent order.  That outcome can be prevented by using
>>         heavyweight fences (sync) instructions between the loads
>>         instead.  Peter Sewell's group concluded that to enforce
>>         correct volatile behavior on Power, you essentially need a a
>>         heavyweight fence between every pair of volatile operations
>>         on Power. That cannot be understood based on simple ordering
>>         constraints.
>>
>>         As Stephan pointed out, there are similar issues on ARM, but
>>         they're less commonly encountered in a Java implementation. 
>>         If you're lucky, you can get to the right implementation
>>         recipe by looking at only reordering, I think.
>>
>>
>>         On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
>>         <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>>
>>         wrote:
>>
>>             Stephan Diestelhorst writes:
>>             >
>>             > David Holmes wrote:
>>             > > Stephan Diestelhorst writes:
>>             > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb
>>             Hans Boehm:
>>             > > > > I'm no hardware architect, but fundamentally it
>>             seems to me that
>>             > > > >
>>             > > > > load x
>>             > > > > acquire_fence
>>             > > > >
>>             > > > > imposes a much more stringent constraint than
>>             > > > >
>>             > > > > load_acquire x
>>             > > > >
>>             > > > > Consider the case in which the load from x is an
>>             L1 hit, but a
>>             > > > > preceding load (from say y) is a long-latency
>>             miss.  If we enforce
>>             > > > > ordering by just waiting for completion of prior
>>             operation, the
>>             > > > > former has to wait for the load from y to
>>             complete; while the
>>             > > > > latter doesn't.  I find it hard to believe that
>>             this doesn't leave
>>             > > > > an appreciable amount of performance on the
>>             table, at least for
>>             > > > > some interesting microarchitectures.
>>             > > >
>>             > > > I agree, Hans, that this is a reasonable
>>             assumption.  Load_acquire x
>>             > > > does allow roach motel, whereas the acquire fence
>>             does not.
>>             > > >
>>             > > > >  In addition, for better or worse, fencing
>>             requirements on at least
>>             > > > >  Power are actually driven as much by store
>>             atomicity issues, as by
>>             > > > >  the ordering issues discussed in the cookbook. 
>>             This was not
>>             > > > >  understood in 2005, and unfortunately doesn't
>>             seem to be
>>             > amenable to
>>             > > > >  the kind of straightforward explanation as in
>>             Doug's cookbook.
>>             > > >
>>             > > > Coming from a strongly ordered architecture to a
>>             weakly ordered one
>>             > > > myself, I also needed some mental adjustment about
>>             store (multi-copy)
>>             > > > atomicity.  I can imagine others will be unaware of
>>             this difference,
>>             > > > too, even in 2014.
>>             > >
>>             > > Sorry I'm missing the connection between fences and
>>             multi-copy
>>             > atomicity.
>>             >
>>             > One example is the classic IRIW.  With non-multi copy
>>             atomic stores, but
>>             > ordered (say through a dependency) loads in the
>>             following example:
>>             >
>>             > Memory: foo = bar = 0
>>             > _T1_         _T2_         _T3_               _T4_
>>             > st (foo),1   st (bar),1   ld r1, (bar)               ld
>>             r3,(foo)
>>             >                           <addr dep / local "fence"
>>             here>   <addr dep>
>>             >                           ld r2, (foo)               ld
>>             r4, (bar)
>>             >
>>             > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
>>             non-multi-copy atomic
>>             > machines.  On TSO boxes, this is not possible.  That
>>             means that the
>>             > memory fence that will prevent such a behaviour (DMB on
>>             ARM) needs to
>>             > carry some additional oomph in ensuring multi-copy
>>             atomicity, or rather
>>             > prevent you from seeing it (which is the same thing).
>>
>>             I take it as given that any code for which you may have
>>             ordering
>>             constraints, must first have basic atomicity properties
>>             for loads and
>>             stores. I would not expect any kind of fence to add
>>             multi-copy-atomicity
>>             where there was none.
>>
>>             David
>>
>>             > Stephan
>>             >
>>             > _______________________________________________
>>             > Concurrency-interest mailing list
>>             > Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141209/4261c1eb/attachment-0001.html>

From oleksandr.otenko at oracle.com  Tue Dec  9 17:58:40 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 09 Dec 2014 22:58:40 +0000
Subject: [concurrency-interest] RFR: 8065804:
 JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEKEKLAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEKEKLAA.davidcholmes@aapt.net.au>
Message-ID: <54877EA0.9010306@oracle.com>

I see it differently. The issue is ordering - the inability of non-TSO 
platforms enforce total order of independent stores. The first loads are 
also independent and their ordering can neither be enforced, nor 
detected. But the following load can detect the lack of total ordering 
of stores and loads, so it is enforced through a heavyweight barrier.

But I understood now why other barriers won't work. Thank you.

Alex


On 09/12/2014 21:59, David Holmes wrote:
> In this case the issue is not ordering per-se (which is what 
> dependencies help with) but global visibility. After performing the 
> first read each thread must ensure that its second read will return 
> what the other thread saw for the first read - hence a full dmb/sync 
> between the reads; or generalizing a full dmb/sync after every 
> volatile read.
> David
>
>     -----Original Message-----
>     *From:* Oleksandr Otenko [mailto:oleksandr.otenko at oracle.com]
>     *Sent:* Wednesday, 10 December 2014 7:54 AM
>     *To:* dholmes at ieee.org; Hans Boehm
>     *Cc:* core-libs-dev; concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] RFR: 8065804:
>     JEP171:Clarifications/corrections for fence intrinsics
>
>     Yes, I do understand the reader needs barriers, too. I guess I was
>     wondering more why the reader would need something stronger than
>     what dependencies etc could enforce. I guess I'll read what Martin
>     forwarded first.
>
>     Alex
>
>
>     On 09/12/2014 21:37, David Holmes wrote:
>>     See my earlier response to Martin. The reader has to force a
>>     consistent view of memory - the writer can't as the write escapes
>>     before it can issue the barrier.
>>     David
>>
>>         -----Original Message-----
>>         *From:* concurrency-interest-bounces at cs.oswego.edu
>>         [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf
>>         Of *Oleksandr Otenko
>>         *Sent:* Wednesday, 10 December 2014 6:04 AM
>>         *To:* Hans Boehm; dholmes at ieee.org
>>         *Cc:* core-libs-dev; concurrency-interest at cs.oswego.edu
>>         *Subject:* Re: [concurrency-interest] RFR: 8065804:
>>         JEP171:Clarifications/corrections for fence intrinsics
>>
>>         On 26/11/2014 02:04, Hans Boehm wrote:
>>>         To be concrete here, on Power, loads can normally be ordered
>>>         by an address dependency or light-weight fence (lwsync). 
>>>         However, neither is enough to prevent the questionable
>>>         outcome for IRIW, since it doesn't ensure that the stores in
>>>         T1 and T2 will be made visible to other threads in a
>>>         consistent order.  That outcome can be prevented by using
>>>         heavyweight fences (sync) instructions between the loads
>>>         instead.
>>
>>         Why would they need fences between loads instead of syncing
>>         the order of stores?
>>
>>
>>         Alex
>>
>>
>>>         Peter Sewell's group concluded that to enforce correct
>>>         volatile behavior on Power, you essentially need a a
>>>         heavyweight fence between every pair of volatile operations
>>>         on Power.  That cannot be understood based on simple
>>>         ordering constraints.
>>>
>>>         As Stephan pointed out, there are similar issues on ARM, but
>>>         they're less commonly encountered in a Java implementation. 
>>>         If you're lucky, you can get to the right implementation
>>>         recipe by looking at only reordering, I think.
>>>
>>>
>>>         On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
>>>         <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>>
>>>         wrote:
>>>
>>>             Stephan Diestelhorst writes:
>>>             >
>>>             > David Holmes wrote:
>>>             > > Stephan Diestelhorst writes:
>>>             > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb
>>>             Hans Boehm:
>>>             > > > > I'm no hardware architect, but fundamentally it
>>>             seems to me that
>>>             > > > >
>>>             > > > > load x
>>>             > > > > acquire_fence
>>>             > > > >
>>>             > > > > imposes a much more stringent constraint than
>>>             > > > >
>>>             > > > > load_acquire x
>>>             > > > >
>>>             > > > > Consider the case in which the load from x is an
>>>             L1 hit, but a
>>>             > > > > preceding load (from say y) is a long-latency
>>>             miss.  If we enforce
>>>             > > > > ordering by just waiting for completion of prior
>>>             operation, the
>>>             > > > > former has to wait for the load from y to
>>>             complete; while the
>>>             > > > > latter doesn't.  I find it hard to believe that
>>>             this doesn't leave
>>>             > > > > an appreciable amount of performance on the
>>>             table, at least for
>>>             > > > > some interesting microarchitectures.
>>>             > > >
>>>             > > > I agree, Hans, that this is a reasonable
>>>             assumption.  Load_acquire x
>>>             > > > does allow roach motel, whereas the acquire fence
>>>             does not.
>>>             > > >
>>>             > > > >  In addition, for better or worse, fencing
>>>             requirements on at least
>>>             > > > >  Power are actually driven as much by store
>>>             atomicity issues, as by
>>>             > > > >  the ordering issues discussed in the cookbook. 
>>>             This was not
>>>             > > > >  understood in 2005, and unfortunately doesn't
>>>             seem to be
>>>             > amenable to
>>>             > > > >  the kind of straightforward explanation as in
>>>             Doug's cookbook.
>>>             > > >
>>>             > > > Coming from a strongly ordered architecture to a
>>>             weakly ordered one
>>>             > > > myself, I also needed some mental adjustment about
>>>             store (multi-copy)
>>>             > > > atomicity.  I can imagine others will be unaware
>>>             of this difference,
>>>             > > > too, even in 2014.
>>>             > >
>>>             > > Sorry I'm missing the connection between fences and
>>>             multi-copy
>>>             > atomicity.
>>>             >
>>>             > One example is the classic IRIW.  With non-multi copy
>>>             atomic stores, but
>>>             > ordered (say through a dependency) loads in the
>>>             following example:
>>>             >
>>>             > Memory: foo = bar = 0
>>>             > _T1_         _T2_         _T3_                 _T4_
>>>             > st (foo),1   st (bar),1   ld r1, (bar)                
>>>             ld r3,(foo)
>>>             >                           <addr dep / local "fence"
>>>             here>   <addr dep>
>>>             >                           ld r2, (foo)                
>>>             ld r4, (bar)
>>>             >
>>>             > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
>>>             non-multi-copy atomic
>>>             > machines.  On TSO boxes, this is not possible.  That
>>>             means that the
>>>             > memory fence that will prevent such a behaviour (DMB
>>>             on ARM) needs to
>>>             > carry some additional oomph in ensuring multi-copy
>>>             atomicity, or rather
>>>             > prevent you from seeing it (which is the same thing).
>>>
>>>             I take it as given that any code for which you may have
>>>             ordering
>>>             constraints, must first have basic atomicity properties
>>>             for loads and
>>>             stores. I would not expect any kind of fence to add
>>>             multi-copy-atomicity
>>>             where there was none.
>>>
>>>             David
>>>
>>>             > Stephan
>>>             >
>>>             > _______________________________________________
>>>             > Concurrency-interest mailing list
>>>             > Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141209/b0dea931/attachment.html>

From davidcholmes at aapt.net.au  Tue Dec  9 18:08:32 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 10 Dec 2014 09:08:32 +1000
Subject: [concurrency-interest] RFR: 8065804:
	JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <54877EA0.9010306@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEKFKLAA.davidcholmes@aapt.net.au>

Yes you are right - forcing global visibility does ensure ordering.

David
  -----Original Message-----
  From: Oleksandr Otenko [mailto:oleksandr.otenko at oracle.com]
  Sent: Wednesday, 10 December 2014 8:59 AM
  To: dholmes at ieee.org; Hans Boehm
  Cc: core-libs-dev; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


  I see it differently. The issue is ordering - the inability of non-TSO
platforms enforce total order of independent stores. The first loads are
also independent and their ordering can neither be enforced, nor detected.
But the following load can detect the lack of total ordering of stores and
loads, so it is enforced through a heavyweight barrier.

  But I understood now why other barriers won't work. Thank you.

  Alex



  On 09/12/2014 21:59, David Holmes wrote:

    In this case the issue is not ordering per-se (which is what
dependencies help with) but global visibility. After performing the first
read each thread must ensure that its second read will return what the other
thread saw for the first read - hence a full dmb/sync between the reads; or
generalizing a full dmb/sync after every volatile read.

    David
      -----Original Message-----
      From: Oleksandr Otenko [mailto:oleksandr.otenko at oracle.com]
      Sent: Wednesday, 10 December 2014 7:54 AM
      To: dholmes at ieee.org; Hans Boehm
      Cc: core-libs-dev; concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


      Yes, I do understand the reader needs barriers, too. I guess I was
wondering more why the reader would need something stronger than what
dependencies etc could enforce. I guess I'll read what Martin forwarded
first.

      Alex



      On 09/12/2014 21:37, David Holmes wrote:

        See my earlier response to Martin. The reader has to force a
consistent view of memory - the writer can't as the write escapes before it
can issue the barrier.

        David
          -----Original Message-----
          From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr
Otenko
          Sent: Wednesday, 10 December 2014 6:04 AM
          To: Hans Boehm; dholmes at ieee.org
          Cc: core-libs-dev; concurrency-interest at cs.oswego.edu
          Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


          On 26/11/2014 02:04, Hans Boehm wrote:

            To be concrete here, on Power, loads can normally be ordered by
an address dependency or light-weight fence (lwsync).  However, neither is
enough to prevent the questionable outcome for IRIW, since it doesn't ensure
that the stores in T1 and T2 will be made visible to other threads in a
consistent order.  That outcome can be prevented by using heavyweight fences
(sync) instructions between the loads instead.


          Why would they need fences between loads instead of syncing the
order of stores?


          Alex



            Peter Sewell's group concluded that to enforce correct volatile
behavior on Power, you essentially need a a heavyweight fence between every
pair of volatile operations on Power.  That cannot be understood based on
simple ordering constraints.


            As Stephan pointed out, there are similar issues on ARM, but
they're less commonly encountered in a Java implementation.  If you're
lucky, you can get to the right implementation recipe by looking at only
reordering, I think.




            On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
<davidcholmes at aapt.net.au> wrote:

              Stephan Diestelhorst writes:
              >
              > David Holmes wrote:
              > > Stephan Diestelhorst writes:
              > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans
Boehm:
              > > > > I'm no hardware architect, but fundamentally it seems
to me that
              > > > >
              > > > > load x
              > > > > acquire_fence
              > > > >
              > > > > imposes a much more stringent constraint than
              > > > >
              > > > > load_acquire x
              > > > >
              > > > > Consider the case in which the load from x is an L1
hit, but a
              > > > > preceding load (from say y) is a long-latency miss.
If we enforce
              > > > > ordering by just waiting for completion of prior
operation, the
              > > > > former has to wait for the load from y to complete;
while the
              > > > > latter doesn't.  I find it hard to believe that this
doesn't leave
              > > > > an appreciable amount of performance on the table, at
least for
              > > > > some interesting microarchitectures.
              > > >
              > > > I agree, Hans, that this is a reasonable assumption.
Load_acquire x
              > > > does allow roach motel, whereas the acquire fence does
not.
              > > >
              > > > >  In addition, for better or worse, fencing
requirements on at least
              > > > >  Power are actually driven as much by store atomicity
issues, as by
              > > > >  the ordering issues discussed in the cookbook.  This
was not
              > > > >  understood in 2005, and unfortunately doesn't seem to
be
              > amenable to
              > > > >  the kind of straightforward explanation as in Doug's
cookbook.
              > > >
              > > > Coming from a strongly ordered architecture to a weakly
ordered one
              > > > myself, I also needed some mental adjustment about store
(multi-copy)
              > > > atomicity.  I can imagine others will be unaware of this
difference,
              > > > too, even in 2014.
              > >
              > > Sorry I'm missing the connection between fences and
multi-copy
              > atomicity.
              >
              > One example is the classic IRIW.  With non-multi copy atomic
stores, but
              > ordered (say through a dependency) loads in the following
example:
              >
              > Memory: foo = bar = 0
              > _T1_         _T2_         _T3_
_T4_
              > st (foo),1   st (bar),1   ld r1, (bar)
ld r3,(foo)
              >                           <addr dep / local "fence" here>
<addr dep>
              >                           ld r2, (foo)
ld r4, (bar)
              >
              > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
non-multi-copy atomic
              > machines.  On TSO boxes, this is not possible.  That means
that the
              > memory fence that will prevent such a behaviour (DMB on ARM)
needs to
              > carry some additional oomph in ensuring multi-copy
atomicity, or rather
              > prevent you from seeing it (which is the same thing).


              I take it as given that any code for which you may have
ordering
              constraints, must first have basic atomicity properties for
loads and
              stores. I would not expect any kind of fence to add
multi-copy-atomicity
              where there was none.

              David


              > Stephan
              >
              > _______________________________________________
              > Concurrency-interest mailing list
              > Concurrency-interest at cs.oswego.edu
              > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

              _______________________________________________
              Concurrency-interest mailing list
              Concurrency-interest at cs.oswego.edu
              http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141210/75c655b4/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Dec  9 19:00:14 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 10 Dec 2014 10:00:14 +1000
Subject: [concurrency-interest] RFR: 8065804:
	JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <548775D5.6080809@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEKHKLAA.davidcholmes@aapt.net.au>

The "no known useful benefit" is based on the paper which states "we are not
aware of any cases where IRIW arises as a natural programming idiom".

I think your example would be written:

Thread 1:
x =1; storestore; y=1;

Thread 2:
r1 = y; r2 =x.

Or more clearly, the most common pattern would be:

Thread1:
data = 1; storestore; dataReady = true;

Thread 2:
if dataReady
  r2 = data

The above does not require IRIW. Conversely if you have IRIW you don't need
the storestore.

David

  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr
Otenko
  Sent: Wednesday, 10 December 2014 8:21 AM
  Cc: concurrency-interest at cs.oswego.edu; core-libs-dev
  Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


  In that case I must say I can't see why you mentioned "no known useful
benefit". The known useful benefit from ordering reads can be seen here:

  store in one order:
  Thread 1:
  x=1
  y=1

  load in reverse order:
  Thread 2:
  r1=y;
  r2=x;

  This is a common pattern, so ordering loads is already useful. Here, even
though JMM talks about total order of all volatile operations, in practice
the order of loads is weaker, as long as this weakening cannot be observed -
eg on x86 enforcing order of loads among themselves is an entirely local
matter.

  IRIW extends the store part to many threads, thus guaranteeing total store
order for volatiles. I thought the total ordering of stores would be a more
contentious point (but I agree with the point Hans makes about easier
reasoning).

  Alex


  On 09/12/2014 21:36, David Holmes wrote:

    The "thorn" is the need for the barriers in the readers not the writers.
(or perhaps as well as the writers in some cases - that is part of the
problem.)

    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr
Otenko
      Sent: Wednesday, 10 December 2014 6:34 AM
      To: dholmes at ieee.org; Hans Boehm
      Cc: core-libs-dev; concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


      Is the thorn the many allowed outcomes, or the single disallowed
outcome? (eg order consistency is too strict for stores with no
synchronizes-with between them?)

      Alex



      On 26/11/2014 02:10, David Holmes wrote:

        Hi Hans,
        Given IRIW is a thorn in everyone's side and has no known useful
benefit, and can hopefully be killed off in the future, lets not get bogged
down in IRIW. But none of what you say below relates to
multi-copy-atomicity.

        Cheers,
        David
          -----Original Message-----
          From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of
Hans Boehm
          Sent: Wednesday, 26 November 2014 12:04 PM
          To: dholmes at ieee.org
          Cc: Stephan Diestelhorst; concurrency-interest at cs.oswego.edu;
core-libs-dev
          Subject: Re: [concurrency-interest] RFR: 8065804:
JEP171:Clarifications/corrections for fence intrinsics


          To be concrete here, on Power, loads can normally be ordered by an
address dependency or light-weight fence (lwsync).  However, neither is
enough to prevent the questionable outcome for IRIW, since it doesn't ensure
that the stores in T1 and T2 will be made visible to other threads in a
consistent order.  That outcome can be prevented by using heavyweight fences
(sync) instructions between the loads instead.  Peter Sewell's group
concluded that to enforce correct volatile behavior on Power, you
essentially need a a heavyweight fence between every pair of volatile
operations on Power.  That cannot be understood based on simple ordering
constraints.


          As Stephan pointed out, there are similar issues on ARM, but
they're less commonly encountered in a Java implementation.  If you're
lucky, you can get to the right implementation recipe by looking at only
reordering, I think.




          On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
<davidcholmes at aapt.net.au> wrote:

            Stephan Diestelhorst writes:
            >
            > David Holmes wrote:
            > > Stephan Diestelhorst writes:
            > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans
Boehm:
            > > > > I'm no hardware architect, but fundamentally it seems to
me that
            > > > >
            > > > > load x
            > > > > acquire_fence
            > > > >
            > > > > imposes a much more stringent constraint than
            > > > >
            > > > > load_acquire x
            > > > >
            > > > > Consider the case in which the load from x is an L1 hit,
but a
            > > > > preceding load (from say y) is a long-latency miss.  If
we enforce
            > > > > ordering by just waiting for completion of prior
operation, the
            > > > > former has to wait for the load from y to complete;
while the
            > > > > latter doesn't.  I find it hard to believe that this
doesn't leave
            > > > > an appreciable amount of performance on the table, at
least for
            > > > > some interesting microarchitectures.
            > > >
            > > > I agree, Hans, that this is a reasonable assumption.
Load_acquire x
            > > > does allow roach motel, whereas the acquire fence does
not.
            > > >
            > > > >  In addition, for better or worse, fencing requirements
on at least
            > > > >  Power are actually driven as much by store atomicity
issues, as by
            > > > >  the ordering issues discussed in the cookbook.  This
was not
            > > > >  understood in 2005, and unfortunately doesn't seem to
be
            > amenable to
            > > > >  the kind of straightforward explanation as in Doug's
cookbook.
            > > >
            > > > Coming from a strongly ordered architecture to a weakly
ordered one
            > > > myself, I also needed some mental adjustment about store
(multi-copy)
            > > > atomicity.  I can imagine others will be unaware of this
difference,
            > > > too, even in 2014.
            > >
            > > Sorry I'm missing the connection between fences and
multi-copy
            > atomicity.
            >
            > One example is the classic IRIW.  With non-multi copy atomic
stores, but
            > ordered (say through a dependency) loads in the following
example:
            >
            > Memory: foo = bar = 0
            > _T1_         _T2_         _T3_
_T4_
            > st (foo),1   st (bar),1   ld r1, (bar)                      ld
r3,(foo)
            >                           <addr dep / local "fence" here>
<addr dep>
            >                           ld r2, (foo)                      ld
r4, (bar)
            >
            > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
non-multi-copy atomic
            > machines.  On TSO boxes, this is not possible.  That means
that the
            > memory fence that will prevent such a behaviour (DMB on ARM)
needs to
            > carry some additional oomph in ensuring multi-copy atomicity,
or rather
            > prevent you from seeing it (which is the same thing).


            I take it as given that any code for which you may have ordering
            constraints, must first have basic atomicity properties for
loads and
            stores. I would not expect any kind of fence to add
multi-copy-atomicity
            where there was none.

            David


            > Stephan
            >
            > _______________________________________________
            > Concurrency-interest mailing list
            > Concurrency-interest at cs.oswego.edu
            > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

            _______________________________________________
            Concurrency-interest mailing list
            Concurrency-interest at cs.oswego.edu
            http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141210/2c81a03e/attachment.html>

From oleksandr.otenko at oracle.com  Tue Dec  9 19:24:40 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 10 Dec 2014 00:24:40 +0000
Subject: [concurrency-interest] RFR: 8065804:
 JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEKHKLAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCEKHKLAA.davidcholmes@aapt.net.au>
Message-ID: <548792C8.6060202@oracle.com>

Yes, I read the part of that paper about IRIW.

My thinking that the ordering of stores would be the more contentious 
point also appears about the same thing.

In IRIW we have two parts of chain that's reasonable to expect to work:

x=1 <-- sw -- r1=x <-- po -- r2=y
y=1 <-- sw -- r3=y <-- po -- r4=x

Suppose r2==0. Then to show the outcome with r4==0 is forbidden need to 
show an edge from r4=x to x=1. Total ordering of stores provides such an 
edge: x=1 <-- so -- y=1 (if chosen otherwise, then we can prove r2!=0).

It looked like enforcing sw and po is reasonable and cheap. But total 
ordering of stores isn't justified in the spec - it just is there, hence 
I was wondering.

On a non-TSO there is no edge between the stores. So to construct the 
proof the outcome r4==0 is forbidden they had to enforce a ordering 
between other instructions, where r2==0 implies the preceding barrier 
also precedes r3=y, which observed r3==1; then it follows that r1=x <-- 
so -- r4=x, and a contradiction shows r4==0 is forbidden.

The effort (and the need) to order the reads is tremendous, and doesn't 
seem right, so I see why it is raising questions.

Alex

On 10/12/2014 00:00, David Holmes wrote:
> The "no known useful benefit" is based on the paper which states "we 
> are not aware of any cases where IRIW arises as a natural programming 
> idiom".
> I think your example would be written:
> Thread 1:
> x =1; storestore; y=1;
> Thread 2:
> r1 = y; r2 =x.
> Or more clearly, the most common pattern would be:
> Thread1:
> data = 1; storestore; dataReady = true;
> Thread 2:
> if dataReady
>   r2 = data
> The above does not require IRIW. Conversely if you have IRIW you don't 
> need the storestore.
> David
>
>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Oleksandr Otenko
>     *Sent:* Wednesday, 10 December 2014 8:21 AM
>     *Cc:* concurrency-interest at cs.oswego.edu; core-libs-dev
>     *Subject:* Re: [concurrency-interest] RFR: 8065804:
>     JEP171:Clarifications/corrections for fence intrinsics
>
>     In that case I must say I can't see why you mentioned "no known
>     useful benefit". The known useful benefit from ordering reads can
>     be seen here:
>
>     store in one order:
>     Thread 1:
>     x=1
>     y=1
>
>     load in reverse order:
>     Thread 2:
>     r1=y;
>     r2=x;
>
>     This is a common pattern, so ordering loads is already useful.
>     Here, even though JMM talks about total order of all volatile
>     operations, in practice the order of loads is weaker, as long as
>     this weakening cannot be observed - eg on x86 enforcing order of
>     loads among themselves is an entirely local matter.
>
>     IRIW extends the store part to many threads, thus guaranteeing
>     total store order for volatiles. I thought the total ordering of
>     stores would be a more contentious point (but I agree with the
>     point Hans makes about easier reasoning).
>
>     Alex
>
>     On 09/12/2014 21:36, David Holmes wrote:
>>     The "thorn" is the need for the barriers in the readers not the
>>     writers. (or perhaps as well as the writers in some cases - that
>>     is part of the problem.)
>>     David
>>
>>         -----Original Message-----
>>         *From:* concurrency-interest-bounces at cs.oswego.edu
>>         [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf
>>         Of *Oleksandr Otenko
>>         *Sent:* Wednesday, 10 December 2014 6:34 AM
>>         *To:* dholmes at ieee.org; Hans Boehm
>>         *Cc:* core-libs-dev; concurrency-interest at cs.oswego.edu
>>         *Subject:* Re: [concurrency-interest] RFR: 8065804:
>>         JEP171:Clarifications/corrections for fence intrinsics
>>
>>         Is the thorn the many allowed outcomes, or the single
>>         disallowed outcome? (eg order consistency is too strict for
>>         stores with no synchronizes-with between them?)
>>
>>         Alex
>>
>>
>>         On 26/11/2014 02:10, David Holmes wrote:
>>>         Hi Hans,
>>>         Given IRIW is a thorn in everyone's side and has no known
>>>         useful benefit, and can hopefully be killed off in the
>>>         future, lets not get bogged down in IRIW. But none of what
>>>         you say below relates to multi-copy-atomicity.
>>>         Cheers,
>>>         David
>>>
>>>             -----Original Message-----
>>>             *From:* hjkhboehm at gmail.com
>>>             [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans Boehm
>>>             *Sent:* Wednesday, 26 November 2014 12:04 PM
>>>             *To:* dholmes at ieee.org
>>>             *Cc:* Stephan Diestelhorst;
>>>             concurrency-interest at cs.oswego.edu; core-libs-dev
>>>             *Subject:* Re: [concurrency-interest] RFR: 8065804:
>>>             JEP171:Clarifications/corrections for fence intrinsics
>>>
>>>             To be concrete here, on Power, loads can normally be
>>>             ordered by an address dependency or light-weight fence
>>>             (lwsync).  However, neither is enough to prevent the
>>>             questionable outcome for IRIW, since it doesn't ensure
>>>             that the stores in T1 and T2 will be made visible to
>>>             other threads in a consistent order.  That outcome can
>>>             be prevented by using heavyweight fences (sync)
>>>             instructions between the loads instead.  Peter Sewell's
>>>             group concluded that to enforce correct volatile
>>>             behavior on Power, you essentially need a a heavyweight
>>>             fence between every pair of volatile operations on
>>>             Power.  That cannot be understood based on simple
>>>             ordering constraints.
>>>
>>>             As Stephan pointed out, there are similar issues on ARM,
>>>             but they're less commonly encountered in a Java
>>>             implementation.  If you're lucky, you can get to the
>>>             right implementation recipe by looking at only
>>>             reordering, I think.
>>>
>>>
>>>             On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
>>>             <davidcholmes at aapt.net.au
>>>             <mailto:davidcholmes at aapt.net.au>> wrote:
>>>
>>>                 Stephan Diestelhorst writes:
>>>                 >
>>>                 > David Holmes wrote:
>>>                 > > Stephan Diestelhorst writes:
>>>                 > > > Am Dienstag, 25. November 2014, 11:15:36
>>>                 schrieb Hans Boehm:
>>>                 > > > > I'm no hardware architect, but fundamentally
>>>                 it seems to me that
>>>                 > > > >
>>>                 > > > > load x
>>>                 > > > > acquire_fence
>>>                 > > > >
>>>                 > > > > imposes a much more stringent constraint than
>>>                 > > > >
>>>                 > > > > load_acquire x
>>>                 > > > >
>>>                 > > > > Consider the case in which the load from x
>>>                 is an L1 hit, but a
>>>                 > > > > preceding load (from say y) is a
>>>                 long-latency miss.  If we enforce
>>>                 > > > > ordering by just waiting for completion of
>>>                 prior operation, the
>>>                 > > > > former has to wait for the load from y to
>>>                 complete; while the
>>>                 > > > > latter doesn't.  I find it hard to believe
>>>                 that this doesn't leave
>>>                 > > > > an appreciable amount of performance on the
>>>                 table, at least for
>>>                 > > > > some interesting microarchitectures.
>>>                 > > >
>>>                 > > > I agree, Hans, that this is a reasonable
>>>                 assumption.  Load_acquire x
>>>                 > > > does allow roach motel, whereas the acquire
>>>                 fence does not.
>>>                 > > >
>>>                 > > > >  In addition, for better or worse, fencing
>>>                 requirements on at least
>>>                 > > > >  Power are actually driven as much by store
>>>                 atomicity issues, as by
>>>                 > > > >  the ordering issues discussed in the
>>>                 cookbook.  This was not
>>>                 > > > >  understood in 2005, and unfortunately
>>>                 doesn't seem to be
>>>                 > amenable to
>>>                 > > > >  the kind of straightforward explanation as
>>>                 in Doug's cookbook.
>>>                 > > >
>>>                 > > > Coming from a strongly ordered architecture to
>>>                 a weakly ordered one
>>>                 > > > myself, I also needed some mental adjustment
>>>                 about store (multi-copy)
>>>                 > > > atomicity.  I can imagine others will be
>>>                 unaware of this difference,
>>>                 > > > too, even in 2014.
>>>                 > >
>>>                 > > Sorry I'm missing the connection between fences
>>>                 and multi-copy
>>>                 > atomicity.
>>>                 >
>>>                 > One example is the classic IRIW.  With non-multi
>>>                 copy atomic stores, but
>>>                 > ordered (say through a dependency) loads in the
>>>                 following example:
>>>                 >
>>>                 > Memory: foo = bar = 0
>>>                 > _T1_         _T2_         _T3_                   _T4_
>>>                 > st (foo),1   st (bar),1   ld r1, (bar)            
>>>                       ld r3,(foo)
>>>                 >                           <addr dep / local
>>>                 "fence" here>   <addr dep>
>>>                 >                           ld r2, (foo)            
>>>                       ld r4, (bar)
>>>                 >
>>>                 > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
>>>                 non-multi-copy atomic
>>>                 > machines.  On TSO boxes, this is not possible. 
>>>                 That means that the
>>>                 > memory fence that will prevent such a behaviour
>>>                 (DMB on ARM) needs to
>>>                 > carry some additional oomph in ensuring multi-copy
>>>                 atomicity, or rather
>>>                 > prevent you from seeing it (which is the same thing).
>>>
>>>                 I take it as given that any code for which you may
>>>                 have ordering
>>>                 constraints, must first have basic atomicity
>>>                 properties for loads and
>>>                 stores. I would not expect any kind of fence to add
>>>                 multi-copy-atomicity
>>>                 where there was none.
>>>
>>>                 David
>>>
>>>                 > Stephan
>>>                 >
>>>                 > _______________________________________________
>>>                 > Concurrency-interest mailing list
>>>                 > Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>                 >
>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>                 _______________________________________________
>>>                 Concurrency-interest mailing list
>>>                 Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141210/8aaae1cd/attachment-0001.html>

From aph at redhat.com  Wed Dec 10 13:12:51 2014
From: aph at redhat.com (Andrew Haley)
Date: Wed, 10 Dec 2014 18:12:51 +0000
Subject: [concurrency-interest] ArrayBlockingQueue (and possibly others)
 not waking on interrupt?
In-Reply-To: <547EFDB0.3080902@cs.oswego.edu>
References: <CAE-f1xQ39AGxaO_xD8um_-Bw-xSw8z8tcW3zPuzbepNce+U9NA@mail.gmail.com>	<CAE-f1xSLmpqxrbo_1Aj7ERYGrdimC=wNRDUEPkf94CwdYuTLJw@mail.gmail.com>	<CA+kOe08uyhNoJP2aW=gT94g-HQDa_vgjReOU_sKVoBRCoG87_g@mail.gmail.com>	<CAE-f1xSUCrROxjsa1i65QYkaxKB1P9z16hNfgbLy12Gm52mu3A@mail.gmail.com>	<CA+kOe0_w0kmrF3JyoP+bv06p88oObs+J_2ou=3ZAsssjOz+ADQ@mail.gmail.com>	<CAE-f1xRfeVGfP006+PS9b_WVWkekA=DzrOxUx_K+qFqExPcCqQ@mail.gmail.com>
	<547EFDB0.3080902@cs.oswego.edu>
Message-ID: <54888D23.5000804@redhat.com>

On 12/03/2014 12:10 PM, Doug Lea wrote:
> Continuously blocking and unblocking a bunch of threads such
> that only one of them is running at any given time is among
> the slowest things you can do on a multicore these days, no
> matter what synchronization scheme you use.

System call time seems to dominate things.  Handoff to a thread
spinning on a shared memory location seems to take 50-75ns, but
awakening a stopped thread with a futex(2) takes at least a
microsecond.  There must be plenty of scope for research in this area,
because it's a real hindrance to use of multicore machines if it
takes longer to awaken worker threads than it does to do the work.

Andrew.

From oleksandr.otenko at oracle.com  Wed Dec 10 12:04:17 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 10 Dec 2014 17:04:17 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe09aURqjRRk_gF8JtCAaJk+Fja8pTYmNY-B=DS7wuWXr1w@mail.gmail.com>
References: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCAEJJKLAA.davidcholmes@aapt.net.au>	<CA+kOe09b_3-08pOt171PYedGoT0Bp5X1HOA7B+PHzCSRMkx6wQ@mail.gmail.com>	<54867C1F.1090106@oracle.com>
	<CA+kOe09aURqjRRk_gF8JtCAaJk+Fja8pTYmNY-B=DS7wuWXr1w@mail.gmail.com>
Message-ID: <54887D11.3000208@oracle.com>

I don't see David's email, so I'll respond here.


On 09/12/2014 18:15, Martin Buchholz wrote:
> On Mon, Dec 8, 2014 at 8:35 PM, David Holmes <david.holmes at oracle.com> wrote:
>
>> ..TSO with global visibility is not
>> sufficient to achieve IRIW. So what is it that x86 and sparc have in
>> addition to TSO that provides for IRIW?
>

It is precisely TSO that enables IRIW on x86.

TSO means all threads agree on the order in which the stores appear.

Write buffers relax this to mean "all other threads agree on the order 
in which the stores appear". To make it strong again, it is sufficient 
to preclude loads in the same thread that stores values, to go ahead 
before the stores become visible to the others. mfence then is just such 
a mechanism - it is a fence that does not allow the loads to go ahead 
before the preceding stores became visible to other threads. So all 
threads agree on the order of volatile stores.

TSO then is the synchronization order of stores from JMM. It seems 
possible to show that the total order of loads is not needed on TSO - 
program order of loads and synchronization order of loads with stores is 
sufficient.

ARM/Power then are different. They don't have a way to provide TSO - 
they don't provide synchronization order from JMM. Instead, they can 
only provide program order of stores. The tiny difference in the 
semantics of DMB and MFENCE is that DMB only gives /local/ guarantees - 
the loads and stores in the same thread do not commit until the 
preceding stores were flushed; but MFENCE additionally provides a 
/global/ guarantee that /all threads agree on the order/ of the stores.


Alex


PS
sketch of the argument that TSO is enough would go like this.

Programs can detect JMM is not adhered to, if they can observe a 
condition for which there is no total order of volatile operations 
*/consistent with program order and synchronizes-with/*. (That's how 
valid executions are defined anyway). But each thread already guarantees 
issuing both loads and stores consistent with program order and 
synchronizes-with. The stores already form a total order, so we only 
need to add loads to the total order. Since there are no rules in JMM 
precluding the ordering of loads from different threads, just place all 
loads that synchronize-with this store or any preceding stores from 
Thread 0 preserving program order, then add loads from Thread 1, etc (in 
contrast, stores do have rules - eg a store to x can't appear between 
another store to x and the load that synchronizes-with it).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141210/6d0682e1/attachment-0001.html>

From martinrb at google.com  Wed Dec 10 12:31:42 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 10 Dec 2014 09:31:42 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5487E1B2.6050409@oracle.com>
References: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEJJKLAA.davidcholmes@aapt.net.au>
	<CA+kOe09b_3-08pOt171PYedGoT0Bp5X1HOA7B+PHzCSRMkx6wQ@mail.gmail.com>
	<54867C1F.1090106@oracle.com>
	<CA+kOe09aURqjRRk_gF8JtCAaJk+Fja8pTYmNY-B=DS7wuWXr1w@mail.gmail.com>
	<5487E1B2.6050409@oracle.com>
Message-ID: <CA+kOe0-4ar_QmcaO=Ac_n3yHPjG+yA=AzSZSsyTuRxPehQQffg@mail.gmail.com>

Today I Leaned
that "release fence" and "acquire fence" are technical terms defined
in the C/C++ 11 standards.

So my latest version reads instead:

     * Ensures that loads and stores before the fence will not be reordered with
     * stores after the fence; a "StoreStore plus LoadStore barrier".
     *
     * Corresponds to C11 atomic_thread_fence(memory_order_release)
     * (a "release fence").

From aph at redhat.com  Wed Dec 10 16:02:39 2014
From: aph at redhat.com (Andrew Haley)
Date: Wed, 10 Dec 2014 21:02:39 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>	<5481108A.3030605@oracle.com>
	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>
Message-ID: <5488B4EF.6060103@redhat.com>

On 12/05/2014 09:49 PM, Martin Buchholz wrote:
> The actual implementations of storestore (see below) seem to
> universally give you the stronger ::release barrier, and it seems
> likely that hotspot engineers are implicitly relying on that, that
> some uses of ::storestore in the hotspot sources are bugs (should be
> ::release instead) and that there is very little potential performance
> benefit from using ::storestore instead of ::release, precisely
> because the additional loadstore barrier is very close to free on all
> current hardware.

That's not really true for ARM, where the additional loadstore requires
a full barrier.  There is a good use for a storestore, which is when
zeroing a newly-created object.

Andrew.


From oleksandr.otenko at oracle.com  Wed Dec 10 17:58:33 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 10 Dec 2014 22:58:33 +0000
Subject: [concurrency-interest] RFR:
 8065804:JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <1417028404335.73215@devexperts.com>
References: <c806b219426c4c8aa9e5bfa79fc9901d@exchmb02.office.devexperts.com>	<NFBBKALFDCPFIDBNKAPCIECNKLAA.davidcholmes@aapt.net.au>	<74ff42b06c314fc9998d512725bae69a@exchmb02.office.devexperts.com>,
	<54760CD7.9070704@flyingtroika.com>
	<1417028404335.73215@devexperts.com>
Message-ID: <5488D019.2020200@oracle.com>

This surely does not mean that on non-TSO architectures it is not 
possible to implement linearizable algorithms.

Nor does it mean that the algorithm will contain /independent/ writes.

Count the proofs that rely on synchronization order. That's the measure 
of how common IRIW pattern is. And yet, even then there is hope.

Suppose, a<b means a happens before b. There are only a few axioms 
coming from memory model:

order consistency: a<b -> b<a -> _|_    -- if you can prove a happens 
before b and b happens before a, it implies absurd

(semi-formally)
read consistency: (r v x) -> (exists (w v x) such that (w v x) < (r v x) 
&& ((w v y) -> (w v y) < (w v x) || (r v x) < (w v y)))  -- for every 
read r of variable v with value x there is a write w of value x such 
that the write happens before the read and all other writes either 
happen before this write, or happen after the read.

That's about it.

Then totality of order axiom is:
total order : a -> b -> ((a < b || b < a) && (a < b -> _|_) -> b < a && 
(b < a -> _|_) -> a < b)   -- for any two volatile operations, they are 
ordered in some way, and if you can prove a happens before b is absurd, 
then b happens before a, and converse. (the latter two things follow 
from order consistency, but stated here explicitly for comparison with 
the following)

DMB barrier on non-TSO is a different axiom:
dmb : a -> b -> c -> (a `po` b) -> ((a < c || c < b) && (c < b -> _|_) 
-> a < c && (a < c -> _|_) -> c < b)  -- for any two operations a and b 
with program order between them, a dmb barrier for any other operation c 
either proves a happens before c, or c happens before b, and if you can 
prove c happens before b is absurd, then it implies a happens before c, 
and the converse. Slight difference in reasoning from total order is 
that proving a<c does not exclude the possibility that in some other way 
c<b.

You can see many similarities between total order and DMB. The 
difficulty with IRIW case is only that there is just one write in each 
thread, so DMB has to enforce ordering between reads instead. And the 
difficulty with DMB is that, unlike volatile stores, there is no 
language-level idiom for it - you need to consider more than one entity 
to understand the effect, and more than one place in code. Total order, 
of course, is far easier to reason about - any two operations are 
ordered, and their ordering is exclusive, so you only need to look in 
one place.


Alex


On 26/11/2014 19:00, Roman Elizarov wrote:
>
> I'd suggest to start with the original paper by Herlihy who had come 
> up with the concept of Linearizability in 1990:
>
> Linearizability: A Correctness Condition for Concurrent Objects
>
> http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.142.5315
>
>
> There were lot of reasearch about linearizability since then (there 
> are almost a thouthand citations for this arcticle) expanding and 
> improving proof techniquies and applying it. There were no 
> breakthroughs of the comparable magnitude since then. All 
> "thread-safe" objects that you enconter in the modern word are 
> linearizable. It is a defacto "golden standard" correctness condition 
> for concurrent objects.
>
>
> This position is well deserved, because having lineariazable objects 
> as your building blocks makes it super-easy to formally reason about 
> correctness of your code. You will rarely encouter concurrent 
> algorithms that provide weaker guarantees (like quescient 
> consistency), because they all too hard to reason about -- they are 
> either not composable or not local. But when all your 
> concurrent objects are linearizable, then you can ditch 
> happens-before, forget that everything is actually parallel and 
> simply reason about your code in terms of interleaving of "atomic" 
> operations that happen in some global order. That is the beauty of 
> linearizability.
>
>
> But Linearizability is indeed a pretty strong requirement. 
> Linearizability of your shared memory requires that Independent Reads 
> of Independent Writes (IRIW) are consistent. Can you get away with 
> some weaker requirement and still get all the same goodies as 
> linearizability gets you? I have not seen anything promising in this 
> direction. Whoever makes this breakthrough will surely reap the 
> world's recognition and respect.
>
>
> /Roman
>
>
> ------------------------------------------------------------------------
> *??:* DT <dt at flyingtroika.com>
> *??????????:* 26 ?????? 2014 ?. 20:24
> *????:* Roman Elizarov; dholmes at ieee.org; Hans Boehm
> *?????:* core-libs-dev; concurrency-interest at cs.oswego.edu
> *????:* Re: [concurrency-interest] RFR: 
> 8065804:JEP171:Clarifications/corrections for fence intrinsics
> Roman,
> Can you point to any specific article providing the concurrency 
> problem statement with a further proof using linearizability to reason 
> about solution.
>
> Thanks,
> DT
>
> On 11/26/2014 2:59 AM, Roman Elizarov wrote:
>>
>> Whether IRIW has any _/practical/_ uses is definitely subject to 
>> debate. However, there is no tractable way for formal reasoning about 
>> properties of large concurrent systems, but via linearizability. 
>> Linearizability is the only property that is both local and 
>> hierarchical. It lets you build more complex linearizable algorithms 
>> from simpler ones, having quite succinct and compelling proofs at 
>> each step.
>>
>> In other words, if you want to be able to construct a formal proof 
>> that your [large] concurrent system if correct, then you must have 
>> IRIW consistency. Do you need a formal proof of correctness? Maybe 
>> not. In many applications hand-waving is enough,  but there are many 
>> other applications where hand-waving does not count as a proof. It 
>> may be possible to construct formal correctness proofs for some very 
>> simple algorithms even on a system that does not provide IRIW, but 
>> this is beyond the state of the art of formal verification for 
>> anything sufficiently complex.
>>
>> /Roman
>>
>> *From:*David Holmes [mailto:davidcholmes at aapt.net.au]
>> *Sent:* Wednesday, November 26, 2014 11:54 AM
>> *To:* Roman Elizarov; Hans Boehm
>> *Cc:* concurrency-interest at cs.oswego.edu; core-libs-dev
>> *Subject:* RE: [concurrency-interest] RFR: 
>> 8065804:JEP171:Clarifications/corrections for fence intrinsics
>>
>> Can you expand on that please. All previous discussion of IRIW I have 
>> seen indicated that the property, while a consequence of existing JMM 
>> rules, had no practical use.
>>
>> Thanks,
>>
>> David
>>
>>     -----Original Message-----
>>     *From:* Roman Elizarov [mailto:elizarov at devexperts.com]
>>     *Sent:* Wednesday, 26 November 2014 6:49 PM
>>     *To:* dholmes at ieee.org <mailto:dholmes at ieee.org>; Hans Boehm
>>     *Cc:* concurrency-interest at cs.oswego.edu
>>     <mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
>>     *Subject:* RE: [concurrency-interest] RFR:
>>     8065804:JEP171:Clarifications/corrections for fence intrinsics
>>
>>     There is no conceivable way to kill IRIW consistency requirement
>>     while retaining ability to prove correctness of large software
>>     systems. If IRIW of volatile variables are not consistent, then
>>     volatile reads and writes are not linearizable, which breaks
>>     linearizabiliy of all higher-level primitives build on top of
>>     them and makes formal reasoning about behavior of concurrent
>>     systems practically impossible. There are many fields where this
>>     is not acceptable.
>>
>>     /Roman
>>
>>     *From:*concurrency-interest-bounces at cs.oswego.edu
>>     <mailto:concurrency-interest-bounces at cs.oswego.edu>
>>     [mailto:concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of
>>     *David Holmes
>>     *Sent:* Wednesday, November 26, 2014 5:11 AM
>>     *To:* Hans Boehm
>>     *Cc:* concurrency-interest at cs.oswego.edu
>>     <mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
>>     *Subject:* Re: [concurrency-interest] RFR: 8065804:
>>     JEP171:Clarifications/corrections for fence intrinsics
>>
>>     Hi Hans,
>>
>>     Given IRIW is a thorn in everyone's side and has no known useful
>>     benefit, and can hopefully be killed off in the future, lets not
>>     get bogged down in IRIW. But none of what you say below relates
>>     to multi-copy-atomicity.
>>
>>     Cheers,
>>
>>     David
>>
>>         -----Original Message-----
>>         *From:* hjkhboehm at gmail.com
>>         <mailto:hjkhboehm at gmail.com>[mailto:hjkhboehm at gmail.com]*On
>>         Behalf Of *Hans Boehm
>>         *Sent:* Wednesday, 26 November 2014 12:04 PM
>>         *To:* dholmes at ieee.org <mailto:dholmes at ieee.org>
>>         *Cc:* Stephan Diestelhorst;
>>         concurrency-interest at cs.oswego.edu
>>         <mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
>>         *Subject:* Re: [concurrency-interest] RFR: 8065804:
>>         JEP171:Clarifications/corrections for fence intrinsics
>>
>>         To be concrete here, on Power, loads can normally be ordered
>>         by an address dependency or light-weight fence (lwsync).
>>         However, neither is enough to prevent the questionable
>>         outcome for IRIW, since it doesn't ensure that the stores in
>>         T1 and T2 will be made visible to other threads in a
>>         consistent order. That outcome can be prevented by using
>>         heavyweight fences (sync) instructions between the loads
>>         instead.  Peter Sewell's group concluded that to enforce
>>         correct volatile behavior on Power, you essentially need a a
>>         heavyweight fence between every pair of volatile operations
>>         on Power.  That cannot be understood based on simple ordering
>>         constraints.
>>
>>         As Stephan pointed out, there are similar issues on ARM, but
>>         they're less commonly encountered in a Java implementation.
>>         If you're lucky, you can get to the right implementation
>>         recipe by looking at only reordering, I think.
>>
>>         On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
>>         <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>>
>>         wrote:
>>
>>             Stephan Diestelhorst writes:
>>             >
>>             > David Holmes wrote:
>>             > > Stephan Diestelhorst writes:
>>             > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb
>>             Hans Boehm:
>>             > > > > I'm no hardware architect, but fundamentally it
>>             seems to me that
>>             > > > >
>>             > > > > load x
>>             > > > > acquire_fence
>>             > > > >
>>             > > > > imposes a much more stringent constraint than
>>             > > > >
>>             > > > > load_acquire x
>>             > > > >
>>             > > > > Consider the case in which the load from x is an
>>             L1 hit, but a
>>             > > > > preceding load (from say y) is a long-latency
>>             miss.  If we enforce
>>             > > > > ordering by just waiting for completion of prior
>>             operation, the
>>             > > > > former has to wait for the load from y to
>>             complete; while the
>>             > > > > latter doesn't.  I find it hard to believe that
>>             this doesn't leave
>>             > > > > an appreciable amount of performance on the
>>             table, at least for
>>             > > > > some interesting microarchitectures.
>>             > > >
>>             > > > I agree, Hans, that this is a reasonable
>>             assumption.  Load_acquire x
>>             > > > does allow roach motel, whereas the acquire fence
>>             does not.
>>             > > >
>>             > > > >  In addition, for better or worse, fencing
>>             requirements on at least
>>             > > > >  Power are actually driven as much by store
>>             atomicity issues, as by
>>             > > > >  the ordering issues discussed in the cookbook. 
>>             This was not
>>             > > > >  understood in 2005, and unfortunately doesn't
>>             seem to be
>>             > amenable to
>>             > > > >  the kind of straightforward explanation as in
>>             Doug's cookbook.
>>             > > >
>>             > > > Coming from a strongly ordered architecture to a
>>             weakly ordered one
>>             > > > myself, I also needed some mental adjustment about
>>             store (multi-copy)
>>             > > > atomicity.  I can imagine others will be unaware of
>>             this difference,
>>             > > > too, even in 2014.
>>             > >
>>             > > Sorry I'm missing the connection between fences and
>>             multi-copy
>>             > atomicity.
>>             >
>>             > One example is the classic IRIW. With non-multi copy
>>             atomic stores, but
>>             > ordered (say through a dependency) loads in the
>>             following example:
>>             >
>>             > Memory: foo = bar = 0
>>             > _T1_         _T2_         _T3_                       _T4_
>>             > st (foo),1   st (bar),1   ld r1, (bar)                 
>>                 ld r3,(foo)
>>             >                           <addr dep / local "fence"
>>             here>   <addr dep>
>>             >                           ld r2, (foo)                 
>>                 ld r4, (bar)
>>             >
>>             > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
>>             non-multi-copy atomic
>>             > machines.  On TSO boxes, this is not possible.  That
>>             means that the
>>             > memory fence that will prevent such a behaviour (DMB on
>>             ARM) needs to
>>             > carry some additional oomph in ensuring multi-copy
>>             atomicity, or rather
>>             > prevent you from seeing it (which is the same thing).
>>
>>             I take it as given that any code for which you may have
>>             ordering
>>             constraints, must first have basic atomicity properties
>>             for loads and
>>             stores. I would not expect any kind of fence to add
>>             multi-copy-atomicity
>>             where there was none.
>>
>>             David
>>
>>
>>             > Stephan
>>             >
>>             > _______________________________________________
>>             > Concurrency-interest mailing list
>>             > Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141210/b8499892/attachment-0001.html>

From martinrb at google.com  Wed Dec 10 22:28:07 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 10 Dec 2014 19:28:07 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5488B4EF.6060103@redhat.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<5481108A.3030605@oracle.com>
	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>
	<5488B4EF.6060103@redhat.com>
Message-ID: <CA+kOe0-88aotGAJiZynp2EhcHmh-7FKhvJWzdwijHEMEk63spg@mail.gmail.com>

On Wed, Dec 10, 2014 at 1:02 PM, Andrew Haley <aph at redhat.com> wrote:
> On 12/05/2014 09:49 PM, Martin Buchholz wrote:
>> The actual implementations of storestore (see below) seem to
>> universally give you the stronger ::release barrier, and it seems
>> likely that hotspot engineers are implicitly relying on that, that
>> some uses of ::storestore in the hotspot sources are bugs (should be
>> ::release instead) and that there is very little potential performance
>> benefit from using ::storestore instead of ::release, precisely
>> because the additional loadstore barrier is very close to free on all
>> current hardware.
>
> That's not really true for ARM, where the additional loadstore requires
> a full barrier.  There is a good use for a storestore, which is when
> zeroing a newly-created object.

Andrew and David,

I agree that given ARM's decision to provide the choice of StoreStore
and full fences, hotspot is right to use them in a few carefully
chosen places, like object initializers.

But ARM's decision seems poor to me. No mainstream language (like
C/C++ or Java) is likely to support those in any way accessible to
user programs (not even via Java Unsafe).  Making their StoreStore
barrier also do LoadStore would dramatically increase the
applicability of their instruction with low cost.  Maybe it's not too
late for ARM to do so.

From aph at redhat.com  Thu Dec 11 04:08:11 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 11 Dec 2014 09:08:11 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5488EB09.9000501@oracle.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>	<5481108A.3030605@oracle.com>
	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>
	<5488B4EF.6060103@redhat.com> <5488EB09.9000501@oracle.com>
Message-ID: <54895EFB.3060204@redhat.com>

On 11/12/14 00:53, David Holmes wrote:
> On 11/12/2014 7:02 AM, Andrew Haley wrote:
>> On 12/05/2014 09:49 PM, Martin Buchholz wrote:
>>> The actual implementations of storestore (see below) seem to
>>> universally give you the stronger ::release barrier, and it seems
>>> likely that hotspot engineers are implicitly relying on that, that
>>> some uses of ::storestore in the hotspot sources are bugs (should be
>>> ::release instead) and that there is very little potential performance
>>> benefit from using ::storestore instead of ::release, precisely
>>> because the additional loadstore barrier is very close to free on all
>>> current hardware.
>>
>> That's not really true for ARM, where the additional loadstore requires
>> a full barrier.  There is a good use for a storestore, which is when
>> zeroing a newly-created object.
> 
> There are many good uses of storestore in the various lock-free 
> algorithms inside hotspot.

There may be many uses, but I am extremely suspicious of how good
they are.  I wonder if we went through all the uses of storestore in
hotspot how many bugs we'd find.  As far as I can see (in the absence
of other barriers) the only things you can write before a storestore
are constants.

Andrew.

From martinrb at google.com  Wed Dec 10 22:07:52 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 10 Dec 2014 19:07:52 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5488EAB2.9030502@oracle.com>
References: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEJJKLAA.davidcholmes@aapt.net.au>
	<CA+kOe09b_3-08pOt171PYedGoT0Bp5X1HOA7B+PHzCSRMkx6wQ@mail.gmail.com>
	<54867C1F.1090106@oracle.com>
	<CA+kOe09aURqjRRk_gF8JtCAaJk+Fja8pTYmNY-B=DS7wuWXr1w@mail.gmail.com>
	<5487E1B2.6050409@oracle.com>
	<CA+kOe0-4ar_QmcaO=Ac_n3yHPjG+yA=AzSZSsyTuRxPehQQffg@mail.gmail.com>
	<5488EAB2.9030502@oracle.com>
Message-ID: <CA+kOe08hoqBjoDxJCyydemPed1icSQs_ix754V8hs2Pz1WeMQw@mail.gmail.com>

On Wed, Dec 10, 2014 at 4:52 PM, David Holmes <david.holmes at oracle.com> wrote:

> For the email record, as I have written in the bug report, I think the
> "correction" of the semantics for storeFence have resulted in problematic
> naming where storeFence and loadFence have opposite directionality
> constraints but the names suggest the same directionality constraints. Had
> the original API suggested these names with the revised semantics I would
> have argued against them. This area is confusing enough without adding to
> that confusion with names that don't suggest the action.

I also dislike the names of the "atomic" methods in Unsafe and would
like to align them as much as possible with C/C++ 11 atomics
nomenclature.

From stephan.diestelhorst at gmail.com  Thu Dec 11 09:54:12 2014
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Thu, 11 Dec 2014 14:54:12 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <54887D11.3000208@oracle.com>
References: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>
	<CA+kOe09aURqjRRk_gF8JtCAaJk+Fja8pTYmNY-B=DS7wuWXr1w@mail.gmail.com>
	<54887D11.3000208@oracle.com>
Message-ID: <1819053.ulmOa4fy3k@mymac-ubuntu>

On Wednesday 10 December 2014 17:04:17 Oleksandr Otenko wrote:
> ARM/Power then are different. They don't have a way to provide TSO -
> they don't provide synchronization order from JMM. Instead, they can
> only provide program order of stores. The tiny difference in the
> semantics of DMB and MFENCE is that DMB only gives /local/ guarantees -
> the loads and stores in the same thread do not commit until the
> preceding stores were flushed; but MFENCE additionally provides a
> /global/ guarantee that /all threads agree on the order/ of the stores.

You pretty much swapped DMBs and MFENCEs ;)  So MFENCEs are local, in
that they need to drain the write-buffer before allowing a later load
access to the data.  DMBs, on the other hand, at least conceptually need
to make sure that stores from other cores have become visible everywhere
when the local core has seen them before the DMB.

Cheers,
  Stephan


From aph at redhat.com  Thu Dec 11 10:36:27 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 11 Dec 2014 15:36:27 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <1819053.ulmOa4fy3k@mymac-ubuntu>
References: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>	<CA+kOe09aURqjRRk_gF8JtCAaJk+Fja8pTYmNY-B=DS7wuWXr1w@mail.gmail.com>	<54887D11.3000208@oracle.com>
	<1819053.ulmOa4fy3k@mymac-ubuntu>
Message-ID: <5489B9FB.6030208@redhat.com>

On 12/11/2014 02:54 PM, Stephan Diestelhorst wrote:
> You pretty much swapped DMBs and MFENCEs ;)  So MFENCEs are local, in
> that they need to drain the write-buffer before allowing a later load
> access to the data.  DMBs, on the other hand, at least conceptually need
> to make sure that stores from other cores have become visible everywhere
> when the local core has seen them before the DMB.

Excuse me?  Now I'm even more confused.

If this core has seen a store from another core, then a DMB on this
core makes that store visible to all other cores, even if the store
had no memory fence?

So the simple act of reading a memory location and then doing a DMB
causes a previous store to that memory location to become visible
to all cores.

Andrew.

From stephan.diestelhorst at gmail.com  Thu Dec 11 17:01:34 2014
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Thu, 11 Dec 2014 22:01:34 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <5489B9FB.6030208@redhat.com>
References: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>
	<1819053.ulmOa4fy3k@mymac-ubuntu> <5489B9FB.6030208@redhat.com>
Message-ID: <2018352.MmPC2QgYHV@d-allen>

Am Donnerstag, 11. Dezember 2014, 15:36:27 schrieb Andrew Haley:
> On 12/11/2014 02:54 PM, Stephan Diestelhorst wrote:
> > You pretty much swapped DMBs and MFENCEs ;)  So MFENCEs are local, in
> > that they need to drain the write-buffer before allowing a later load
> > access to the data.  DMBs, on the other hand, at least conceptually need
> > to make sure that stores from other cores have become visible everywhere
> > when the local core has seen them before the DMB.
> 
> Excuse me?  Now I'm even more confused.
> 
> If this core has seen a store from another core, then a DMB on this
> core makes that store visible to all other cores, even if the store
> had no memory fence?

Yep.  Conceptually it has to.  Imagine in the IRIW example, there is not
even a fence behind the store:

IRIW:
Memory: foo = bar = 0

T1          T2        T3        T4
foo := 1    bar := 1  r1 = foo  r3 = bar
                      DMB       DMB
                      r2 = bar  r4 = foo


r1 == 1 && r2 == 0 && r3 ==1 && r4 == 0 ?

> So the simple act of reading a memory location and then doing a DMB
> causes a previous store to that memory location to become visible
> to all cores.

Yes, if you read the updated store value with a load before the DMB.  If
you look through the ARM documentation, this is precisely the reason for
the somewhat complex description with the recursive definition of what
really is before and after the barrier.  The description tells you that
the barrier not only orders things that were on the same core before /
after, but also things that were read by instructions before the
barrier, likewise for things happening after the barrier.  This is
necessary, as otherwise there would be no way to enforce a consistent
global store order in a weak memory model.

The reason for fences being so simple on TSO(-like) architectures is
precisely, that stores are magically globally ordered already, so the
fence does not have to influence them.

Stephan



From richard.warburton at gmail.com  Thu Dec 11 21:16:21 2014
From: richard.warburton at gmail.com (Richard Warburton)
Date: Thu, 11 Dec 2014 21:16:21 -0500
Subject: [concurrency-interest] Fork/Join Use cases
Message-ID: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>

Hi,

Something I have noticed with fork/join is the absence of clear-cut and
common motivating examples and use cases. Many times people provide
examples of fork/join code that only solves a simple problem. For example
calculates Fibonacci numbers or performs a merge sort. This is great for
the educational purpose of understanding the API but isn't so convincing of
the efficacy of fork/join as a general parallel programming framework.

What I would like to see is an example of a more complex use of fork join.
For example something like training a neural network. Now its that I can't
see how such a problem don't fit into the fork/join model - but the
question is why one would conclude it to be such a general model as to
warrant inclusion into Java SE. Other than problems whose algorithms
naturally follow a recursive structure (eg fibonacci) everytime I try to
fit a parallel programming problem into the fork/join model in my head it
doesn't seem to offer me much over the ability to run a series of parallel
threads.

I'm ruling out the simpler problems alone because implementing such tasks
in parallel is often not difficult enough to warrant a framework or of
particular use. I'm not looking for:

a. Other parallel programming frameworks which depend upon fork/join (eg:
parallel streams). I'm asking for actual direct application level usage of
fork/join.

b. Other models - I'm just asking about fork/join.

c. Fitting problems into fork/join which can be made to work but either
have as code that's as complex as using threads or have poor performance.
The ideal example would have also some kind of open source code with
comparisons in other approaches, but I appreciate I'm asking for a lot here
when trying to satisfy my own understanding.

regards,

  Richard Warburton

  http://insightfullogic.com
  @RichardWarburto <http://twitter.com/richardwarburto>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141211/aae6b5b9/attachment.html>

From kirk at kodewerk.com  Fri Dec 12 02:16:19 2014
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Fri, 12 Dec 2014 08:16:19 +0100
Subject: [concurrency-interest] Fork/Join Use cases
In-Reply-To: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
References: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
Message-ID: <AD1ED690-144E-4C0C-9B2D-56D0B5B4ED1A@kodewerk.com>

Hi Richard,

Good questions and one that I?ve not been able to answer myself.

Another question.. Spliterator on Streams.  I can see use cases which implies it?s a nice abstraction over Fork-Join. However, that you need a terminating condition to trigger the action certainly makes them far less useful than I initially hoped for. React seems far more useful but the API in RxJava seem unfortunately like something that only the elite could every possibly understand!

Regards,
Kirk

On Dec 12, 2014, at 3:16 AM, Richard Warburton <richard.warburton at gmail.com> wrote:

> Hi,
> 
> Something I have noticed with fork/join is the absence of clear-cut and common motivating examples and use cases. Many times people provide examples of fork/join code that only solves a simple problem. For example calculates Fibonacci numbers or performs a merge sort. This is great for the educational purpose of understanding the API but isn't so convincing of the efficacy of fork/join as a general parallel programming framework.
> 
> What I would like to see is an example of a more complex use of fork join. For example something like training a neural network. Now its that I can't see how such a problem don't fit into the fork/join model - but the question is why one would conclude it to be such a general model as to warrant inclusion into Java SE. Other than problems whose algorithms naturally follow a recursive structure (eg fibonacci) everytime I try to fit a parallel programming problem into the fork/join model in my head it doesn't seem to offer me much over the ability to run a series of parallel threads.
> 
> I'm ruling out the simpler problems alone because implementing such tasks in parallel is often not difficult enough to warrant a framework or of particular use. I'm not looking for:
> 
> a. Other parallel programming frameworks which depend upon fork/join (eg: parallel streams). I'm asking for actual direct application level usage of fork/join.
> 
> b. Other models - I'm just asking about fork/join.
> 
> c. Fitting problems into fork/join which can be made to work but either have as code that's as complex as using threads or have poor performance.
> 
> The ideal example would have also some kind of open source code with comparisons in other approaches, but I appreciate I'm asking for a lot here when trying to satisfy my own understanding.
> 
> regards,
> 
>   Richard Warburton
> 
>   http://insightfullogic.com
>   @RichardWarburto
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141212/e099b12f/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141212/e099b12f/attachment.bin>

From aleksandar.prokopec at gmail.com  Fri Dec 12 03:37:11 2014
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Fri, 12 Dec 2014 09:37:11 +0100
Subject: [concurrency-interest] Fork/Join Use cases
In-Reply-To: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
References: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
Message-ID: <db9b4f0d-ee8c-4f02-a1db-49cbf374988b@email.android.com>

Hi Richard,

I always found fork/join to be a very foundational, and serving as a solid thread pool implementation that works really well, and doesn't blow up, which the JDK otherwise lacked. In many cases it serves as a building block for other frameworks, since one of the advantages is that all the other abstractions can optionally share the same computational resources. But since you said that you're not interested in futures, parallel collections, Rx, actors...

For concrete code, one of the applications that comes to mind is parallel sorting. You might want to search for Doug Lea's parallel array implementation to see it.
Otherwise, I'm sure you could easily use fj to implement minimax algorithms for chess AI, a constraint solving problem, typechecking in a compiler, or concurrently issuing a request to several web services.
It's maybe not the best for the continuously running applications such as simulation engines, or streaming applications, which need to react to requests or continuously produce output as part of a pipeline, but as soon as you can identify a bulk of computation with finite batches of work that can be parallelized, it's likely to be a good candidate for fork/join parallelization.

To find more use cases, it might make sense to search for online examples of Cilk code, as it was one of the first to employ the fork/join model.

Richard Warburton <richard.warburton at gmail.com> wrote:
>Hi,
>
>Something I have noticed with fork/join is the absence of clear-cut and
>common motivating examples and use cases. Many times people provide
>examples of fork/join code that only solves a simple problem. For
>example
>calculates Fibonacci numbers or performs a merge sort. This is great
>for
>the educational purpose of understanding the API but isn't so
>convincing of
>the efficacy of fork/join as a general parallel programming framework.
>
>What I would like to see is an example of a more complex use of fork
>join.
>For example something like training a neural network. Now its that I
>can't
>see how such a problem don't fit into the fork/join model - but the
>question is why one would conclude it to be such a general model as to
>warrant inclusion into Java SE. Other than problems whose algorithms
>naturally follow a recursive structure (eg fibonacci) everytime I try
>to
>fit a parallel programming problem into the fork/join model in my head
>it
>doesn't seem to offer me much over the ability to run a series of
>parallel
>threads.
>
>I'm ruling out the simpler problems alone because implementing such
>tasks
>in parallel is often not difficult enough to warrant a framework or of
>particular use. I'm not looking for:
>
>a. Other parallel programming frameworks which depend upon fork/join
>(eg:
>parallel streams). I'm asking for actual direct application level usage
>of
>fork/join.
>
>b. Other models - I'm just asking about fork/join.
>
>c. Fitting problems into fork/join which can be made to work but either
>have as code that's as complex as using threads or have poor
>performance.
>The ideal example would have also some kind of open source code with
>comparisons in other approaches, but I appreciate I'm asking for a lot
>here
>when trying to satisfy my own understanding.
>
>regards,
>
>  Richard Warburton
>
>  http://insightfullogic.com
>  @RichardWarburto <http://twitter.com/richardwarburto>
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
Sent from my Android phone with K-9 Mail. Please excuse my brevity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141212/fa304d80/attachment-0001.html>

From joe.bowbeer at gmail.com  Fri Dec 12 03:38:25 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 12 Dec 2014 00:38:25 -0800
Subject: [concurrency-interest] Fork/Join Use cases
In-Reply-To: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
References: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
Message-ID: <CAHzJPEoY_+XUa3jS73YCUociqxr3JXwtUxkxcjGEJROasdsdBQ@mail.gmail.com>

Richard,

My example is real-world, though it's not particularly elegant.  I ported
Hector Yee's Perceptual Diff from C to Java, and yet, after I tried all the
optimization tricks I could manage, the C code still outperformed the Java
code.  Darn!  So the only avenue left for me was to call on FJP, and the
resulting code outperformed C even on a dual-core laptop:

https://bitbucket.org/joebowbeer/perceptualdiff

Check out the RecursiveTask implementation in the PerceptualDiff class.

--Joe

On Thu, Dec 11, 2014 at 6:16 PM, Richard Warburton <
richard.warburton at gmail.com> wrote:

> Hi,
>
> Something I have noticed with fork/join is the absence of clear-cut and
> common motivating examples and use cases. Many times people provide
> examples of fork/join code that only solves a simple problem. For example
> calculates Fibonacci numbers or performs a merge sort. This is great for
> the educational purpose of understanding the API but isn't so convincing of
> the efficacy of fork/join as a general parallel programming framework.
>
> What I would like to see is an example of a more complex use of fork join.
> For example something like training a neural network. Now its that I can't
> see how such a problem don't fit into the fork/join model - but the
> question is why one would conclude it to be such a general model as to
> warrant inclusion into Java SE. Other than problems whose algorithms
> naturally follow a recursive structure (eg fibonacci) everytime I try to
> fit a parallel programming problem into the fork/join model in my head it
> doesn't seem to offer me much over the ability to run a series of parallel
> threads.
>
> I'm ruling out the simpler problems alone because implementing such tasks
> in parallel is often not difficult enough to warrant a framework or of
> particular use. I'm not looking for:
>
> a. Other parallel programming frameworks which depend upon fork/join (eg:
> parallel streams). I'm asking for actual direct application level usage of
> fork/join.
>
> b. Other models - I'm just asking about fork/join.
>
> c. Fitting problems into fork/join which can be made to work but either
> have as code that's as complex as using threads or have poor performance.
> The ideal example would have also some kind of open source code with
> comparisons in other approaches, but I appreciate I'm asking for a lot here
> when trying to satisfy my own understanding.
>
> regards,
>
>   Richard Warburton
>
>   http://insightfullogic.com
>   @RichardWarburto <http://twitter.com/richardwarburto>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141212/fa546ba1/attachment.html>

From aph at redhat.com  Fri Dec 12 05:57:28 2014
From: aph at redhat.com (Andrew Haley)
Date: Fri, 12 Dec 2014 10:57:28 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <2018352.MmPC2QgYHV@d-allen>
References: <CA+kOe0-kaVGdNyaLN2Yo0H4KnM1AU3N=nFryazZP1VbqK9t=Kw@mail.gmail.com>	<1819053.ulmOa4fy3k@mymac-ubuntu>
	<5489B9FB.6030208@redhat.com> <2018352.MmPC2QgYHV@d-allen>
Message-ID: <548ACA18.40706@redhat.com>

On 11/12/14 22:01, Stephan Diestelhorst wrote:
> Am Donnerstag, 11. Dezember 2014, 15:36:27 schrieb Andrew Haley:
>> On 12/11/2014 02:54 PM, Stephan Diestelhorst wrote:
>>> You pretty much swapped DMBs and MFENCEs ;)  So MFENCEs are local, in
>>> that they need to drain the write-buffer before allowing a later load
>>> access to the data.  DMBs, on the other hand, at least conceptually need
>>> to make sure that stores from other cores have become visible everywhere
>>> when the local core has seen them before the DMB.
>>
>> Excuse me?  Now I'm even more confused.
>>
>> If this core has seen a store from another core, then a DMB on this
>> core makes that store visible to all other cores, even if the store
>> had no memory fence?
> 
> Yep.  Conceptually it has to.  Imagine in the IRIW example, there is not
> even a fence behind the store:
> 
> IRIW:
> Memory: foo = bar = 0
> 
> T1          T2        T3        T4
> foo := 1    bar := 1  r1 = foo  r3 = bar
>                       DMB       DMB
>                       r2 = bar  r4 = foo
> 
> 
> r1 == 1 && r2 == 0 && r3 ==1 && r4 == 0 ?

Okay, I get that: it enforces a global visibility ordering on the memory
system.

>> So the simple act of reading a memory location and then doing a DMB
>> causes a previous store to that memory location to become visible
>> to all cores.
> 
> Yes, if you read the updated store value with a load before the DMB.  If
> you look through the ARM documentation, this is precisely the reason for
> the somewhat complex description with the recursive definition of what
> really is before and after the barrier. 

Aha!  Thank you.

> The description tells you that the barrier not only orders things
> that were on the same core before / after, but also things that were
> read by instructions before the barrier, likewise for things
> happening after the barrier.  This is necessary, as otherwise there
> would be no way to enforce a consistent global store order in a weak
> memory model.

The problem with "as if, conceptually" descriptions is that they give
programmers no intuitively understandable model they can get a grip
on.  I think I understand MESI/MOESI protocols, write buffers,
invalidate queues, and so on.  I suppose that what happens here is
that the processor executing a DMB has to send an invalidate message
to the other processors for the data in its cache, but that sounds
prohibitively expensive to me.

I am aware that the specification is a model, it's not exactly what
happens.  However, there is a real problem in the industry
(particularly in Java) that people have a false understanding of
memory barriers and their cost.  I am playing an endless game of
whack-a-mole rebutting influential programmers who tell the world to
avoid volatile variables because accesses "flush the cache" and so are
very costly.  And I have to reply no, that's not what happens, memory
barriers are a local operation and the cache-coherence protocol does
the rest.

It's the "somewhat complex" (i.e. utterly baffling) description in the
ARM ARM which is perhaps the real problem in this particular case.

Just to get on my soapbox, for a moment.  It's not enough these days
to say to the programmer "do this and your program is correct";
rather, people need to have an intuitive model which gives them a way
to reason informally (and reasonably accurately) about the cost of an
action on a multicore system.  I accept that we don't want to
overspecify anything, so I'm not suggesting that the specifications
should be changed; I would like to see a better public discourse
around all this, though.  I am aware that we're not even at the point
where programmers can reason correctly about relaxed memory, and that
has to come first.

> The reason for fences being so simple on TSO(-like) architectures is
> precisely, that stores are magically globally ordered already, so the
> fence does not have to influence them.

Right.

Andrew.

From dl at cs.oswego.edu  Fri Dec 12 08:37:31 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 12 Dec 2014 08:37:31 -0500
Subject: [concurrency-interest] Fork/Join Use cases
In-Reply-To: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
References: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
Message-ID: <548AEF9B.5040905@cs.oswego.edu>

On 12/11/2014 09:16 PM, Richard Warburton wrote:
> Something I have noticed with fork/join is the absence of clear-cut and
> common motivating examples and use cases.

One realm is aggregate operations; many of the most common ones
(map, filter, reduce, collect, etc) now (in jdk8) prepackaged in.
Stream.parallel(). But also used by others for custom in-core BigData
algorithms and the like. (Plus, the underlying work-stealing
scheduler is useful in other contexts.)

>
> everytime I try to fit a parallel programming problem into the fork/join
> model in my head it doesn't seem to offer me much over the ability to run a
> series of parallel threads.

Right. The main reason FJ exists in the JDK is to provide a framework
for developing portable general-purpose versions of such programs,
that includes ways of controlling how many threads are used,
expressing and managing granularity thresholds,  load
balancing when some cores are busy doing other things,
nested parallelism, combining/reducing results,  and so on.

Some people who don't care about any of these issues, and are
content with ad-hoc programs that run well on particular computers
don't have much motivation to use FJ. On the other hand, FJ can help
in the discovery of parallel techniques by encouraging design
via structured parallel divide-and-conquer, which is often a
productive approach.

-Doug



From thurston at nomagicsoftware.com  Fri Dec 12 15:44:02 2014
From: thurston at nomagicsoftware.com (thurstonn)
Date: Fri, 12 Dec 2014 13:44:02 -0700 (MST)
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU-iL1=OrgdTGT4zf_uR7yq=DSgrpXG+dEQPBmSfL1Cu0g@mail.gmail.com>
References: <CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
	<1417803360836-11578.post@n7.nabble.com>
	<CANPzfU-iL1=OrgdTGT4zf_uR7yq=DSgrpXG+dEQPBmSfL1Cu0g@mail.gmail.com>
Message-ID: <1418417042495-11626.post@n7.nabble.com>

?iktor ?lang wrote
>> Hello Viktor,
>>
>> Although this may seem a digression, I'm curious whether you object to
>> something like Collectors#toList() in j.u.stream?
>>
>> Because this facilitates (I wouldn't say encourages) code like the
>> following:
>>
>>
>> List
> <T>
>  l1 = . . .
>> List
> <T>
>  l2 = l1.stream().filter(...).collect(toList())
>>
>> List
> <T>
>  l3 = l2.stream().map(...).collect(toList())
>>
>> for (T : l3)
>>   do something
>>
>> Now, we can all agree that the code above is awful, even wrong, but is
>> it's
>> possibility (well, probably more than possibility - I've actually seen
>> such
>> code) enough to warrant the exclusion of Collectors.toList()?
>>
> 
> At least that code doesn't impact liveness of the system. (well, you can
> still OOME, but you can do that by allocating an array, so that's not
> really the same)

Frankly, I don't understand "impact liveness of the system" - the classical
definition of liveness is "a guarantee that each thread (process, etc) will
*eventually* succeed" -
Doesn't a (hypothetical) CompletionStage#get() offer the same liveness
guarantee as any of the "terminal operations" on Stream?
In fact, IFAICT, Stream#collect() can indeed block (probably does for
parallel streams), that's an implementation detail, and doesn't really
change the semantics of the program (of course the JVM could crash, or the
scheduler might never again schedule a thread, etc., but let's hand-wave
those away)

What I was really asking (not very well in retrospect), is if you wouldn't
prefer the Stream api to have its terminal operations return CompletionStage
(or some rump)?, i.e. your objection isn't restricted to just narrow
questions of blocking/not-blocking, but mixing "reactive"-style programming
(or as you termed it "async monadic-style programming") with imperative
style  


?iktor ?lang wrote
>> Of course the issue isn't exactly one of "encouraging blocking" (unless
>> the
>> streams were parallel) . . .
>>
>> I guess I think that "discouragement by difficulty" in API design more
>> often
>> than not leads to grief (which is not to say that I've never done it)
>>
> 
> Do you have any example?

Not sure what you're asking here -- but (assuming there was no
#toCompletableFuture()), your code on implementing a "blocking get" on a
CompletionStage is a prime example; that's certainly not the only way (as an
aside I played around with a cursory alternative approach and soon got
myself into trouble).

The point being, that if we can agree that many developers will want it (I
think that's undeniable), it's better to leave the implementation
encapsulated in the implementing CompletionStage class than have the
numerous adhoc approaches that inevitably result.
And put in the necessary admonitions in the JavaDoc



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/CompletableFuture-in-Java-8-tp11414p11626.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From viktor.klang at gmail.com  Fri Dec 12 17:04:36 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 12 Dec 2014 23:04:36 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <1418417042495-11626.post@n7.nabble.com>
References: <CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
	<1417803360836-11578.post@n7.nabble.com>
	<CANPzfU-iL1=OrgdTGT4zf_uR7yq=DSgrpXG+dEQPBmSfL1Cu0g@mail.gmail.com>
	<1418417042495-11626.post@n7.nabble.com>
Message-ID: <CANPzfU_Qu6jpssDpOsXg74VoUY=Jop_Z8Qojg58k_Kuh=Lgd+Q@mail.gmail.com>

Hi Thurston,

On Fri, Dec 12, 2014 at 9:44 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> ?iktor ?lang wrote
> >> Hello Viktor,
> >>
> >> Although this may seem a digression, I'm curious whether you object to
> >> something like Collectors#toList() in j.u.stream?
> >>
> >> Because this facilitates (I wouldn't say encourages) code like the
> >> following:
> >>
> >>
> >> List
> > <T>
> >  l1 = . . .
> >> List
> > <T>
> >  l2 = l1.stream().filter(...).collect(toList())
> >>
> >> List
> > <T>
> >  l3 = l2.stream().map(...).collect(toList())
> >>
> >> for (T : l3)
> >>   do something
> >>
> >> Now, we can all agree that the code above is awful, even wrong, but is
> >> it's
> >> possibility (well, probably more than possibility - I've actually seen
> >> such
> >> code) enough to warrant the exclusion of Collectors.toList()?
> >>
> >
> > At least that code doesn't impact liveness of the system. (well, you can
> > still OOME, but you can do that by allocating an array, so that's not
> > really the same)
>
> Frankly, I don't understand "impact liveness of the system" - the classical
> definition of liveness is "a guarantee that each thread (process, etc) will
> *eventually* succeed" -
> Doesn't a (hypothetical) CompletionStage#get() offer the same liveness
> guarantee as any of the "terminal operations" on Stream?
>

I misunderstood what you tried to convey in your example, to me it just
looked like wasteful copying.

So to answer (sort of) your question: I am not a fan of the blocking
terminal operations in Stream and would have preferred to only expose async
terminal operations (returning a CompletionStage). Perhaps we can get
`async` versions of them for Java 9? (I assume deprecating the blocking
ones is going to be a hard sell)



> In fact, IFAICT, Stream#collect() can indeed block (probably does for
> parallel streams), that's an implementation detail, and doesn't really
> change the semantics of the program (of course the JVM could crash, or the
> scheduler might never again schedule a thread, etc., but let's hand-wave
> those away)
>

There's already been reports of deadlocks when nesting terminal operations
for parallel streams.


>
> What I was really asking (not very well in retrospect), is if you wouldn't
> prefer the Stream api to have its terminal operations return
> CompletionStage
> (or some rump)?, i.e. your objection isn't restricted to just narrow
> questions of blocking/not-blocking, but mixing "reactive"-style programming
> (or as you termed it "async monadic-style programming") with imperative
> style
>

Evidently I should have read your email before starting to reply, but "yes"
:-)


>
>
> ?iktor ?lang wrote
> >> Of course the issue isn't exactly one of "encouraging blocking" (unless
> >> the
> >> streams were parallel) . . .
> >>
> >> I guess I think that "discouragement by difficulty" in API design more
> >> often
> >> than not leads to grief (which is not to say that I've never done it)
> >>
> >
> > Do you have any example?
>
> Not sure what you're asking here -- but (assuming there was no
> #toCompletableFuture()), your code on implementing a "blocking get" on a
> CompletionStage is a prime example; that's certainly not the only way (as
> an
> aside I played around with a cursory alternative approach and soon got
> myself into trouble).
>

Tongue in cheek but you want yet another way of doing blocking because all
the other ones were not good enough? :-)


>
> The point being, that if we can agree that many developers will want it (I
> think that's undeniable),


That raises the question: should API be create on a "want" basis or on a
"need" basis?

 The good thing about "Await" is that it is easy to define rules to fail
the build if someone calls it,
but given how many have gotten themselves into problems by using Await, I'd
probably not add it if I could do it all over again.


> it's better to leave the implementation
> encapsulated in the implementing CompletionStage class than have the
> numerous adhoc approaches that inevitably result.
> And put in the necessary admonitions in the JavaDoc
>

I'd have to disagree., and I am sure you aren't surprised by that!

The least damaging approach I think would be to -not- have blocking in the
API and add documentation on how to achieve blocking but show how to avoid
blocking, and put appropriate warnings all around the blocking approach.
This prevents it from showing up in IntelliSense?and thus encourages the
reading of the documentation, and hopefully understanding why it is most
likely a bad idea, and also how to "do the right thing".


>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/CompletableFuture-in-Java-8-tp11414p11626.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141212/7d69836b/attachment.html>

From dl at cs.oswego.edu  Sat Dec 13 09:26:20 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 13 Dec 2014 09:26:20 -0500
Subject: [concurrency-interest] Fork/Join Use cases
In-Reply-To: <AD1ED690-144E-4C0C-9B2D-56D0B5B4ED1A@kodewerk.com>
References: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
	<AD1ED690-144E-4C0C-9B2D-56D0B5B4ED1A@kodewerk.com>
Message-ID: <548C4C8C.4060700@cs.oswego.edu>

On 12/12/2014 02:16 AM, Kirk Pepperdine wrote:

> Another question.. Spliterator on Streams.  I can see use cases which implies
> it?s a nice abstraction over Fork-Join. However, that you need a terminating
> condition to trigger the action certainly makes them far less useful than I
> initially hoped for.

By which you mean that a user/caller must choose a parallel granularity
threshold to control splitting? All implementable suggestions for
automating this would be welcome. The best we could do for j.u.Stream
usages of Spliterators is reduce it to a yes/no decision (stream() vs
parallelStream()).

> React seems far more useful but the API in RxJava seem
> unfortunately like something that only the elite could every possibly understand!
>

Similar issues arise whether you use push-style APIs (as in Rx
and CompletionStages) vs lazy/pull-style (as in j.u.Stream and
most divide-and-conquer FJ designs), which can make them harder
to understand.

In all cases, the current state of the art is that programmers
(possibly with the help of tools) must know something about
expected data sizes, rates, access patterns, etc to use them
effectively. This is not hugely different than choosing to
use LinkedList vs ArrayList, or HashMap vs TreeMap; but applied
to less familiar settings.

-Doug




From dl at cs.oswego.edu  Sat Dec 13 10:17:05 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 13 Dec 2014 10:17:05 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU_Qu6jpssDpOsXg74VoUY=Jop_Z8Qojg58k_Kuh=Lgd+Q@mail.gmail.com>
References: <CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>	<547C740B.1090903@cs.oswego.edu>	<1417646009375-11569.post@n7.nabble.com>	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>	<1417803360836-11578.post@n7.nabble.com>	<CANPzfU-iL1=OrgdTGT4zf_uR7yq=DSgrpXG+dEQPBmSfL1Cu0g@mail.gmail.com>	<1418417042495-11626.post@n7.nabble.com>
	<CANPzfU_Qu6jpssDpOsXg74VoUY=Jop_Z8Qojg58k_Kuh=Lgd+Q@mail.gmail.com>
Message-ID: <548C5871.4000300@cs.oswego.edu>

On 12/12/2014 05:04 PM, ?iktor ?lang wrote:
> On Fri, Dec 12, 2014 at 9:44 PM, thurstonn <thurston at nomagicsoftware.com
> <mailto:thurston at nomagicsoftware.com>> wrote:
>
> So to answer (sort of) your question: I am not a fan of the blocking terminal
> operations in Stream and would have preferred to only expose async terminal
> operations (returning a CompletionStage). Perhaps we can get `async` versions of
> them for Java 9? (I assume deprecating the blocking ones is going to be a hard sell)
>

It would be nice to simplify mixed usages across the various styles
discussed on this and related threads, but this one is relatively
straightforward even now:

CompletableFuture.supplyAsync(() -> data.parallelStream().reduce(...)).
   thenApply(...);

(And some variants.)

Any further integration would require internal conversions from
the pull-style demand-driven lazy pipelines in j.u.Stream to
push-style async. Considering that the manual version is straightforward,
there's not much motivation for it. Although possibly some sugaring
in the still-under-contemplation CompletionStages util class would
be worthwhile.

As I've mentioned, accommodating the opposite direction of push-style
reactive to j.u.Stream is more important but also more challenging,
in part because so much of it is out of our hands in j.u.c:
For each kind of source, you need a reactive async alternative to
standard blocking APIs. At the moment, it seems that the best we
can do is add some utilities to simplify their creation:
Simpler ties from nio2 to CompletionStages: standardized
ways of batching (the opposite of splitting), and so on.

-Doug




From dl at cs.oswego.edu  Sat Dec 13 11:51:56 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 13 Dec 2014 11:51:56 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-uM03ovA2gLiE7CfvNy4oa1W1kF-CbRG6+Q6OHqYmZHRg@mail.gmail.com>
References: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>	<547B3ADD.4070700@cs.oswego.edu>	<CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>	<547C740B.1090903@cs.oswego.edu>	<1417646009375-11569.post@n7.nabble.com>	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>	<CAHJZN-sGQzJFrWnKeoZONjOWiHYeVD=hZ4k6rZMV1ez3cUQ=aw@mail.gmail.com>	<CANPzfU8Mj0d6_SPV08OttzkbGgVK9ppwic_4wMuDKnWEj6OJ1A@mail.gmail.com>
	<CAHJZN-uM03ovA2gLiE7CfvNy4oa1W1kF-CbRG6+Q6OHqYmZHRg@mail.gmail.com>
Message-ID: <548C6EAC.80501@cs.oswego.edu>

On 12/08/2014 11:35 AM, Josh Humphries wrote:

>         You've already mentioned that you'd like to see ExecutorService retired.
>         It definitely has plenty of sharp corners. But I'll hold judgement on
>         whether or not its use should be retired for when I see its replacement
>         (sorry, FJP, you're not it.)

Suggestions are always welcome.
In principle, it is always possible to build a custom scheduler
that will work better than any possible general-purpose j.u.c Executor
for a known collection of activities with known dependencies, But
in practice, the combination of centralized ThreadPoolExecutor and
decentralized ForkJoinPool have evolved to cover enough of the
territory that we focus on improving rather than replacing.
A few further improvements of each might occur relatively soon.
Plus, probably less soon, better ways to address the memory locality,
contention, and affinity issues common to all multicore schedulers.


> I'm actually toying with something that might be kinda-sorta like this now. It's
> like an ExecutorService, but you give each submission a "key" (think Actor or
> Listener) so submissions from outside of the pool (like non-FJ code) can try to
> pin the task to "the right thread" instead of a random thread (and hopefully get
> better L1 cache hit rate). At the same time, it also employs work-stealing to
> improve throughput and fairness.

This is what FJ does, but the "key" is a hidden thread-local hash-based
queue index that changes only if contended with another. This avoids
unbounded growth in worker threads, at the expense of residual
low likelihood of continued contention. Empirically, this seems
to be a good tradeoff.

>
> One difference in what I'm trying to do is that it is specifically for
> sequential delivery of events to listeners (and could be used for executing
> operations on behalf of an actor in something like Akka).

If a later task B cannot start before an earlier task A completes,
then sequential execution would be a better approach than treating
as async with B dependent on completion of (i.e., joining) A.
In other words, (A; B) should be batched as a single task.
It seems that this is what your queue-stealing is in effect
doing. We might be able to generalize this into more
flexible adaptive support for batching, with possible dependencies
across batches. (This was the subject of some experiments we did
on work-stealing-based graph algorithms a few years ago that didn't
make it into any FJ release, but there are now several reasons to
revisit.)

Anyway, it may well be the case that you have enough extra
context and constraints to make a specialized Executor worthwhile,
but we are also always trying to improve the general-purpose ones.

> https://code.google.com/p/bluegosling/source/browse/src/com/apriori/concurrent/ActorThreadPool.java
>

-Doug




From viktor.klang at gmail.com  Sat Dec 13 15:05:50 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sat, 13 Dec 2014 21:05:50 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <548C5871.4000300@cs.oswego.edu>
References: <CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
	<1417803360836-11578.post@n7.nabble.com>
	<CANPzfU-iL1=OrgdTGT4zf_uR7yq=DSgrpXG+dEQPBmSfL1Cu0g@mail.gmail.com>
	<1418417042495-11626.post@n7.nabble.com>
	<CANPzfU_Qu6jpssDpOsXg74VoUY=Jop_Z8Qojg58k_Kuh=Lgd+Q@mail.gmail.com>
	<548C5871.4000300@cs.oswego.edu>
Message-ID: <CANPzfU92nbV9DW7JCihUYm0t_SBzDsyxgDhao00TRh9k4Y0FAQ@mail.gmail.com>

On Sat, Dec 13, 2014 at 4:17 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/12/2014 05:04 PM, ?iktor ?lang wrote:
>
>> On Fri, Dec 12, 2014 at 9:44 PM, thurstonn <thurston at nomagicsoftware.com
>> <mailto:thurston at nomagicsoftware.com>> wrote:
>>
>> So to answer (sort of) your question: I am not a fan of the blocking
>> terminal
>> operations in Stream and would have preferred to only expose async
>> terminal
>> operations (returning a CompletionStage). Perhaps we can get `async`
>> versions of
>> them for Java 9? (I assume deprecating the blocking ones is going to be a
>> hard sell)
>>
>>
> It would be nice to simplify mixed usages across the various styles
> discussed on this and related threads, but this one is relatively
> straightforward even now:
>
> CompletableFuture.supplyAsync(() -> data.parallelStream().reduce(...)).
>   thenApply(...);
>

Isn't the only reason why this works that they are both running on the
commonPool?
The question is how many uses of this to avoid blocking exists in the wild.


>
> (And some variants.)
>
> Any further integration would require internal conversions from
> the pull-style demand-driven lazy pipelines in j.u.Stream to
> push-style async. Considering that the manual version is straightforward,
> there's not much motivation for it. Although possibly some sugaring
> in the still-under-contemplation CompletionStages util class would
> be worthwhile.
>

Wouldn't it be possible to turn it around, and having the blocking terminal
methods call the async terminal methods and then block on the result?


>
> As I've mentioned, accommodating the opposite direction of push-style
> reactive to j.u.Stream is more important but also more challenging,
> in part because so much of it is out of our hands in j.u.c:
> For each kind of source, you need a reactive async alternative to
> standard blocking APIs. At the moment, it seems that the best we
> can do is add some utilities to simplify their creation:
> Simpler ties from nio2 to CompletionStages: standardized
> ways of batching (the opposite of splitting), and so on.


Perhaps a java.util.stream.nio with integration for reactive async for the
nio package?


>
>
> -Doug
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141213/f2a257d5/attachment.html>

From hanson.char at gmail.com  Sat Dec 13 23:26:00 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 13 Dec 2014 20:26:00 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
Message-ID: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>

Hi,

I am looking for a construct that can be used to efficiently enforce
ordered execution of multiple critical sections, each calling from a
different thread. The calling threads may run in parallel and may call
the execution method out of order.  The perceived construct would
therefore be responsible for re-ordering the execution of those
threads, so that their critical sections (and only the critical
section) will be executed in order.

Would something like the following API already exist?

/**
 * Used to enforce ordered execution of critical sections calling from multiple
 * threads, parking and unparking the threads as necessary.
 */
public class OrderedExecutor<T> {
    /**
     * Executes a critical section at most once with the given order, parking
     * and unparking the current thread as necessary so that all critical
     * sections executed by different threads using this executor take place in
     * the order from 1 to n consecutively.
     */
    public T execCriticalSectionInOrder(final int order,
            final Callable<T> criticalSection) throws InterruptedException;
}

Regards,
Hanson

From stephan.diestelhorst at gmail.com  Sun Dec 14 04:15:36 2014
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Sun, 14 Dec 2014 09:15:36 +0000
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
Message-ID: <CAJR39Ey7Ch-jvQnmm3vKQUB1KzT136Uzsn0d303Qt-qDezWpVQ@mail.gmail.com>

Hanson,
  a few keywords to through into Google Scholar: deterministic
multi-threading, ordered transactions.

These should give you a start as to what is there already.

Stephan

On 14 December 2014 at 04:26, Hanson Char <hanson.char at gmail.com> wrote:
> Hi,
>
> I am looking for a construct that can be used to efficiently enforce
> ordered execution of multiple critical sections, each calling from a
> different thread. The calling threads may run in parallel and may call
> the execution method out of order.  The perceived construct would
> therefore be responsible for re-ordering the execution of those
> threads, so that their critical sections (and only the critical
> section) will be executed in order.
>
> Would something like the following API already exist?
>
> /**
>  * Used to enforce ordered execution of critical sections calling from multiple
>  * threads, parking and unparking the threads as necessary.
>  */
> public class OrderedExecutor<T> {
>     /**
>      * Executes a critical section at most once with the given order, parking
>      * and unparking the current thread as necessary so that all critical
>      * sections executed by different threads using this executor take place in
>      * the order from 1 to n consecutively.
>      */
>     public T execCriticalSectionInOrder(final int order,
>             final Callable<T> criticalSection) throws InterruptedException;
> }
>
> Regards,
> Hanson
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From peter.levart at gmail.com  Sun Dec 14 05:21:40 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 14 Dec 2014 11:21:40 +0100
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
Message-ID: <548D64B4.5090505@gmail.com>

Hi Hanson,

I don't think anything like that readily exists in java.lang.concurrent, 
but what you describe should be possible to achieve with composition of 
existing primitives. You haven't given any additional hints to what your 
OrderedExecutor should behave like. Should it be a one-shot object (like 
CountDownLatch) or a re-usable one (like CyclicBarrier)? Will 
execCriticalSectionInOrder() for a particular OrderedExecutor instance 
and 'order' value be called at most once? If yes (and I think that only 
a one-shot object makes sense here), an array of CountDownLatch(es) 
could be used:

public class OrderedExecutor<T> {
     private final CountDownLatch[] latches;

     public OrderedExecutor(int n) {
         if (n < 1) throw new IllegalArgumentException("'n' should be >= 
1");
         latches = new CountDownLatch[n - 1];
         for (int i = 0; i < latches.length; i++) {
             latches[i] = new CountDownLatch(1);
         }
     }

     public T execCriticalSectionInOrder(final int order,
                                         final Supplier<T> 
criticalSection) throws InterruptedException {
         if (order < 0 || order > latches.length)
             throw new IllegalArgumentException("'order' should be 
[0..." + latches.length + "]");
         if (order > 0) {
             latches[order - 1].await();
         }
         try {
             return criticalSection.get();
         } finally {
             if (order < latches.length) {
                 latches[order].countDown();
             }
         }
     }
}


Regards, Peter

On 12/14/2014 05:26 AM, Hanson Char wrote:
> Hi,
>
> I am looking for a construct that can be used to efficiently enforce
> ordered execution of multiple critical sections, each calling from a
> different thread. The calling threads may run in parallel and may call
> the execution method out of order.  The perceived construct would
> therefore be responsible for re-ordering the execution of those
> threads, so that their critical sections (and only the critical
> section) will be executed in order.
>
> Would something like the following API already exist?
>
> /**
>   * Used to enforce ordered execution of critical sections calling from multiple
>   * threads, parking and unparking the threads as necessary.
>   */
> public class OrderedExecutor<T> {
>      /**
>       * Executes a critical section at most once with the given order, parking
>       * and unparking the current thread as necessary so that all critical
>       * sections executed by different threads using this executor take place in
>       * the order from 1 to n consecutively.
>       */
>      public T execCriticalSectionInOrder(final int order,
>              final Callable<T> criticalSection) throws InterruptedException;
> }
>
> Regards,
> Hanson
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/453c7902/attachment.html>

From joe.bowbeer at gmail.com  Sun Dec 14 09:37:12 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 14 Dec 2014 06:37:12 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
Message-ID: <CAHzJPEpsG1R8uEseUMRKBmfEP1Mw4ZCshqj6HsYieSb+W1vLtQ@mail.gmail.com>

Here's a "cyclic" version using Semaphore.  It resets when the last
callable execs.

class OrderedExecutor<T> {

  final Semaphore[] locks;

  OrderedExecutor(int n) {
    locks = new Semaphore[n];
    for (int i = n; --i >= 0; ) {
      locks[i] = new Semaphore(0);
    }
    locks[0].release();
  }

  public T execCallableInOrder(int order, Callable<T> callable)
      throws InterruptedException, Exception {
    locks[order].acquire();
    try {
      return callable.call();
    } finally {
      locks[(order + 1) % locks.length].release();
    }
  }
}


On Sat, Dec 13, 2014 at 8:26 PM, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi,
>
> I am looking for a construct that can be used to efficiently enforce
> ordered execution of multiple critical sections, each calling from a
> different thread. The calling threads may run in parallel and may call
> the execution method out of order.  The perceived construct would
> therefore be responsible for re-ordering the execution of those
> threads, so that their critical sections (and only the critical
> section) will be executed in order.
>
> Would something like the following API already exist?
>
> /**
>  * Used to enforce ordered execution of critical sections calling from
> multiple
>  * threads, parking and unparking the threads as necessary.
>  */
> public class OrderedExecutor<T> {
>     /**
>      * Executes a critical section at most once with the given order,
> parking
>      * and unparking the current thread as necessary so that all critical
>      * sections executed by different threads using this executor take
> place in
>      * the order from 1 to n consecutively.
>      */
>     public T execCriticalSectionInOrder(final int order,
>             final Callable<T> criticalSection) throws InterruptedException;
> }
>
> Regards,
> Hanson
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/8390d3bd/attachment-0001.html>

From hanson.char at gmail.com  Sun Dec 14 10:20:38 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 14 Dec 2014 07:20:38 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <548D64B4.5090505@gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<548D64B4.5090505@gmail.com>
Message-ID: <CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>

Hi Peter,

Thanks for the suggestion, and sorry about not being clear about one
important detail: "n" is not known a priori when constructing an
OrderedExecutor.  Does this mean the use of  CountDownLatch is ruled out?

You guessed right: it's a one-shot object for a particular OrderedExecutor
instance, and "order" must be called indeed at most once.

Regards,
Hanson

On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com>
wrote:
>
>  Hi Hanson,
>
> I don't think anything like that readily exists in java.lang.concurrent,
> but what you describe should be possible to achieve with composition of
> existing primitives. You haven't given any additional hints to what your
> OrderedExecutor should behave like. Should it be a one-shot object (like
> CountDownLatch) or a re-usable one (like CyclicBarrier)? Will
> execCriticalSectionInOrder() for a particular OrderedExecutor instance and
> 'order' value be called at most once? If yes (and I think that only a
> one-shot object makes sense here), an array of CountDownLatch(es) could be
> used:
>
> public class OrderedExecutor<T> {
>     private final CountDownLatch[] latches;
>
>     public OrderedExecutor(int n) {
>         if (n < 1) throw new IllegalArgumentException("'n' should be >=
> 1");
>         latches = new CountDownLatch[n - 1];
>         for (int i = 0; i < latches.length; i++) {
>             latches[i] = new CountDownLatch(1);
>         }
>     }
>
>     public T execCriticalSectionInOrder(final int order,
>                                         final Supplier<T> criticalSection)
> throws InterruptedException {
>         if (order < 0 || order > latches.length)
>             throw new IllegalArgumentException("'order' should be [0..." +
> latches.length + "]");
>         if (order > 0) {
>             latches[order - 1].await();
>         }
>         try {
>             return criticalSection.get();
>         } finally {
>             if (order < latches.length) {
>                 latches[order].countDown();
>             }
>         }
>     }
> }
>
>
> Regards, Peter
>
>
> On 12/14/2014 05:26 AM, Hanson Char wrote:
>
> Hi,
>
> I am looking for a construct that can be used to efficiently enforce
> ordered execution of multiple critical sections, each calling from a
> different thread. The calling threads may run in parallel and may call
> the execution method out of order.  The perceived construct would
> therefore be responsible for re-ordering the execution of those
> threads, so that their critical sections (and only the critical
> section) will be executed in order.
>
> Would something like the following API already exist?
>
> /**
>  * Used to enforce ordered execution of critical sections calling from multiple
>  * threads, parking and unparking the threads as necessary.
>  */
> public class OrderedExecutor<T> {
>     /**
>      * Executes a critical section at most once with the given order, parking
>      * and unparking the current thread as necessary so that all critical
>      * sections executed by different threads using this executor take place in
>      * the order from 1 to n consecutively.
>      */
>     public T execCriticalSectionInOrder(
> final int order,
>             final Callable<T> criticalSection) throws InterruptedException;
> }
>
> Regards,
> Hanson
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/a2a89462/attachment.html>

From hanson.char at gmail.com  Sun Dec 14 10:23:57 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 14 Dec 2014 07:23:57 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<548D64B4.5090505@gmail.com>
	<CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>
Message-ID: <CABWgujZUq+3zeU1ufN8bNYk=hnUVvKmfmMnkXC=N-rc2ch9p-g@mail.gmail.com>

s/"order" must be called indeed at most once/a specific "order" value must
be called indeed at most once/

On Sun, Dec 14, 2014 at 7:20 AM, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi Peter,
>
> Thanks for the suggestion, and sorry about not being clear about one
> important detail: "n" is not known a priori when constructing an
> OrderedExecutor.  Does this mean the use of  CountDownLatch is ruled out?
>
> You guessed right: it's a one-shot object for a particular OrderedExecutor
> instance, and "order" must be called indeed at most once.
>
> Regards,
> Hanson
>
> On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com>
> wrote:
>>
>>  Hi Hanson,
>>
>> I don't think anything like that readily exists in java.lang.concurrent,
>> but what you describe should be possible to achieve with composition of
>> existing primitives. You haven't given any additional hints to what your
>> OrderedExecutor should behave like. Should it be a one-shot object (like
>> CountDownLatch) or a re-usable one (like CyclicBarrier)? Will
>> execCriticalSectionInOrder() for a particular OrderedExecutor instance and
>> 'order' value be called at most once? If yes (and I think that only a
>> one-shot object makes sense here), an array of CountDownLatch(es) could be
>> used:
>>
>> public class OrderedExecutor<T> {
>>     private final CountDownLatch[] latches;
>>
>>     public OrderedExecutor(int n) {
>>         if (n < 1) throw new IllegalArgumentException("'n' should be >=
>> 1");
>>         latches = new CountDownLatch[n - 1];
>>         for (int i = 0; i < latches.length; i++) {
>>             latches[i] = new CountDownLatch(1);
>>         }
>>     }
>>
>>     public T execCriticalSectionInOrder(final int order,
>>                                         final Supplier<T>
>> criticalSection) throws InterruptedException {
>>         if (order < 0 || order > latches.length)
>>             throw new IllegalArgumentException("'order' should be [0..."
>> + latches.length + "]");
>>         if (order > 0) {
>>             latches[order - 1].await();
>>         }
>>         try {
>>             return criticalSection.get();
>>         } finally {
>>             if (order < latches.length) {
>>                 latches[order].countDown();
>>             }
>>         }
>>     }
>> }
>>
>>
>> Regards, Peter
>>
>>
>> On 12/14/2014 05:26 AM, Hanson Char wrote:
>>
>> Hi,
>>
>> I am looking for a construct that can be used to efficiently enforce
>> ordered execution of multiple critical sections, each calling from a
>> different thread. The calling threads may run in parallel and may call
>> the execution method out of order.  The perceived construct would
>> therefore be responsible for re-ordering the execution of those
>> threads, so that their critical sections (and only the critical
>> section) will be executed in order.
>>
>> Would something like the following API already exist?
>>
>> /**
>>  * Used to enforce ordered execution of critical sections calling from multiple
>>  * threads, parking and unparking the threads as necessary.
>>  */
>> public class OrderedExecutor<T> {
>>     /**
>>      * Executes a critical section at most once with the given order, parking
>>      * and unparking the current thread as necessary so that all critical
>>      * sections executed by different threads using this executor take place in
>>      * the order from 1 to n consecutively.
>>      */
>>     public T execCriticalSectionInOrder(
>> final int order,
>>             final Callable<T> criticalSection) throws InterruptedException;
>> }
>>
>> Regards,
>> Hanson
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/910f3a5e/attachment.html>

From peter.levart at gmail.com  Sun Dec 14 11:08:27 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 14 Dec 2014 17:08:27 +0100
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>	<548D64B4.5090505@gmail.com>
	<CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>
Message-ID: <548DB5FB.4060305@gmail.com>


On 12/14/2014 04:20 PM, Hanson Char wrote:
> Hi Peter,
>
> Thanks for the suggestion, and sorry about not being clear about one 
> important detail: "n" is not known a priori when constructing an 
> OrderedExecutor.  Does this mean the use of  CountDownLatch is ruled out?

If you know at least the upper bound of 'n', it can be used with such 
'n'. Otherwise something that dynamically re-sizes the array could be 
devised. Or you could simply use a ConcurrentHashMap instead of array 
where keys are 'order' values:


public class OrderedExecutor<T> {

     private final ConcurrentMap<Integer, CountDownLatch> latches = new 
ConcurrentHashMap<>();

     public T execCriticalSectionInOrder(final int order,
                                         final Supplier<T> 
criticalSection) throws InterruptedException {
         if (order > 0) {
             latches.computeIfAbsent(order - 1, o -> new 
CountDownLatch(1)).await();
         }
         try {
             return criticalSection.get();
         } finally {
             latches.computeIfAbsent(order, o -> new 
CountDownLatch(1)).countDown();
         }
     }
}


Regards, Peter

>
> You guessed right: it's a one-shot object for a particular 
> OrderedExecutor instance, and "order" must be called indeed at most once.
>
> Regards,
> Hanson
>
> On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com 
> <mailto:peter.levart at gmail.com>> wrote:
>
>     Hi Hanson,
>
>     I don't think anything like that readily exists in
>     java.lang.concurrent, but what you describe should be possible to
>     achieve with composition of existing primitives. You haven't given
>     any additional hints to what your OrderedExecutor should behave
>     like. Should it be a one-shot object (like CountDownLatch) or a
>     re-usable one (like CyclicBarrier)? Will
>     execCriticalSectionInOrder() for a particular OrderedExecutor
>     instance and 'order' value be called at most once? If yes (and I
>     think that only a one-shot object makes sense here), an array of
>     CountDownLatch(es) could be used:
>
>     public class OrderedExecutor<T> {
>         private final CountDownLatch[] latches;
>
>         public OrderedExecutor(int n) {
>             if (n < 1) throw new IllegalArgumentException("'n' should
>     be >= 1");
>             latches = new CountDownLatch[n - 1];
>             for (int i = 0; i < latches.length; i++) {
>                 latches[i] = new CountDownLatch(1);
>             }
>         }
>
>         public T execCriticalSectionInOrder(final int order,
>                                             final Supplier<T>
>     criticalSection) throws InterruptedException {
>             if (order < 0 || order > latches.length)
>                 throw new IllegalArgumentException("'order' should be
>     [0..." + latches.length + "]");
>             if (order > 0) {
>                 latches[order - 1].await();
>             }
>             try {
>                 return criticalSection.get();
>             } finally {
>                 if (order < latches.length) {
>                     latches[order].countDown();
>                 }
>             }
>         }
>     }
>
>
>     Regards, Peter
>
>
>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>     Hi, I am looking for a construct that can be used to efficiently
>>     enforce ordered execution of multiple critical sections, each
>>     calling from a different thread. The calling threads may run in
>>     parallel and may call the execution method out of order. The
>>     perceived construct would therefore be responsible for
>>     re-ordering the execution of those threads, so that their
>>     critical sections (and only the critical section) will be
>>     executed in order. Would something like the following API already
>>     exist? /** * Used to enforce ordered execution of critical
>>     sections calling from multiple * threads, parking and unparking
>>     the threads as necessary. */ public class OrderedExecutor<T> {
>>     /** * Executes a critical section at most once with the given
>>     order, parking * and unparking the current thread as necessary so
>>     that all critical * sections executed by different threads using
>>     this executor take place in * the order from 1 to n
>>     consecutively. */ public T execCriticalSectionInOrder(
>>     final int order,
>>                  final Callable<T> criticalSection) throws InterruptedException;
>>     }
>>
>>     Regards,
>>     Hanson
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/2082cf1b/attachment-0001.html>

From hanson.char at gmail.com  Sun Dec 14 11:37:02 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 14 Dec 2014 08:37:02 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <548DB5FB.4060305@gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<548D64B4.5090505@gmail.com>
	<CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>
	<548DB5FB.4060305@gmail.com>
Message-ID: <CABWgujZNjerMZLfXt4uYoxjvtcjZjzgLUi5Bi7yMX-Ruz53Z3g@mail.gmail.com>

Thanks, Peter.  I like your idea.  It seems both CountDownLatch and
Semaphore are viable options for implementing this use case.  Are you aware
of any performance difference one may have over the other (in this case)?

Regards,
Hanson
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/a9373d89/attachment.html>

From joe.bowbeer at gmail.com  Sun Dec 14 11:39:54 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 14 Dec 2014 08:39:54 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEpsG1R8uEseUMRKBmfEP1Mw4ZCshqj6HsYieSb+W1vLtQ@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<CAHzJPEpsG1R8uEseUMRKBmfEP1Mw4ZCshqj6HsYieSb+W1vLtQ@mail.gmail.com>
Message-ID: <CAHzJPEo4-DSt=f5a6y4SwhEE8hbnM4swqvAwqmWK_4d2RByUNg@mail.gmail.com>

Below is a simple implementation where 'n' is not known in advance,
assuming '0' is the starting order:

class OrderedExecutor<T> {

  private final List<Semaphore> locks = new ArrayList<>();

  public OrderedExecutor() {
    locks.add(0, new Semaphore(1));
  }

  public T execCallableInOrder(int order, Callable<T> callable)
      throws InterruptedException, Exception {
    getLock(order).acquire();
    try {
      return callable.call();
    } finally {
      getLock(order + 1).release();
    }
  }

  private synchronized Semaphore getLock(int order) {
    for (int i = locks.size(); i <= order; i++) {
      locks.add(i, new Semaphore(0));
    }
    return locks.get(order);
  }
}

If this were long-lived, I'd want to trim the locks array as they were
consumed.

On Sun, Dec 14, 2014 at 6:37 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>
> Here's a "cyclic" version using Semaphore.  It resets when the last
> callable execs.
>
> class OrderedExecutor<T> {
>
>   final Semaphore[] locks;
>
>   OrderedExecutor(int n) {
>     locks = new Semaphore[n];
>     for (int i = n; --i >= 0; ) {
>       locks[i] = new Semaphore(0);
>     }
>     locks[0].release();
>   }
>
>   public T execCallableInOrder(int order, Callable<T> callable)
>       throws InterruptedException, Exception {
>     locks[order].acquire();
>     try {
>       return callable.call();
>     } finally {
>       locks[(order + 1) % locks.length].release();
>     }
>   }
> }
>
>
> On Sat, Dec 13, 2014 at 8:26 PM, Hanson Char <hanson.char at gmail.com>
> wrote:
>>
>> Hi,
>>
>> I am looking for a construct that can be used to efficiently enforce
>> ordered execution of multiple critical sections, each calling from a
>> different thread. The calling threads may run in parallel and may call
>> the execution method out of order.  The perceived construct would
>> therefore be responsible for re-ordering the execution of those
>> threads, so that their critical sections (and only the critical
>> section) will be executed in order.
>>
>> Would something like the following API already exist?
>>
>> /**
>>  * Used to enforce ordered execution of critical sections calling from
>> multiple
>>  * threads, parking and unparking the threads as necessary.
>>  */
>> public class OrderedExecutor<T> {
>>     /**
>>      * Executes a critical section at most once with the given order,
>> parking
>>      * and unparking the current thread as necessary so that all critical
>>      * sections executed by different threads using this executor take
>> place in
>>      * the order from 1 to n consecutively.
>>      */
>>     public T execCriticalSectionInOrder(final int order,
>>             final Callable<T> criticalSection) throws
>> InterruptedException;
>> }
>>
>> Regards,
>> Hanson
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/08f45c3e/attachment.html>

From peter.levart at gmail.com  Sun Dec 14 12:01:26 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 14 Dec 2014 18:01:26 +0100
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <548DB5FB.4060305@gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>	<548D64B4.5090505@gmail.com>
	<CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>
	<548DB5FB.4060305@gmail.com>
Message-ID: <548DC266.9070705@gmail.com>

Hi Hanson,

This one is more low-level, but catches some invalid usages and is more 
resource-friendly:


public class OrderedExecutor {

     public <T> T execCriticalSectionInOrder(
         final int order,
         final Supplier<T> criticalSection
     ) throws InterruptedException {
         if (order < 0) {
             throw new IllegalArgumentException("'order' should be >= 0");
         }
         if (order > 0) {
             waitForDone(order - 1);
         }
         try {
             return criticalSection.get();
         } finally {
             notifyDone(order);
         }
     }

     private static final Object DONE = new Object();
     private final ConcurrentMap<Integer, Object> signals = new 
ConcurrentHashMap<>();

     private void waitForDone(int order) throws InterruptedException {
         Object sig = signals.putIfAbsent(order, Thread.currentThread());
         if (sig != null && sig != DONE) {
             throw new IllegalStateException();
         }
         while (sig != DONE) {
             LockSupport.park();
             if (Thread.interrupted()) {
                 throw new InterruptedException();
             }
             sig = signals.get(order);
         }
     }

     private void notifyDone(int order) {
         Object sig = signals.putIfAbsent(order, DONE);
         if (sig instanceof Thread) {
             if (!signals.replace(order, sig, DONE)) {
                 throw new IllegalStateException();
             }
             LockSupport.unpark((Thread) sig);
         } else if (sig != null) {
             throw new IllegalStateException();
         }
     }
}


Regards, Peter

On 12/14/2014 05:08 PM, Peter Levart wrote:
>
> On 12/14/2014 04:20 PM, Hanson Char wrote:
>> Hi Peter,
>>
>> Thanks for the suggestion, and sorry about not being clear about one 
>> important detail: "n" is not known a priori when constructing an 
>> OrderedExecutor.  Does this mean the use of  CountDownLatch is ruled out?
>
> If you know at least the upper bound of 'n', it can be used with such 
> 'n'. Otherwise something that dynamically re-sizes the array could be 
> devised. Or you could simply use a ConcurrentHashMap instead of array 
> where keys are 'order' values:
>
>
> public class OrderedExecutor<T> {
>
>     private final ConcurrentMap<Integer, CountDownLatch> latches = new 
> ConcurrentHashMap<>();
>
>     public T execCriticalSectionInOrder(final int order,
>                                         final Supplier<T> 
> criticalSection) throws InterruptedException {
>         if (order > 0) {
>             latches.computeIfAbsent(order - 1, o -> new 
> CountDownLatch(1)).await();
>         }
>         try {
>             return criticalSection.get();
>         } finally {
>             latches.computeIfAbsent(order, o -> new 
> CountDownLatch(1)).countDown();
>         }
>     }
> }
>
>
> Regards, Peter
>
>>
>> You guessed right: it's a one-shot object for a particular 
>> OrderedExecutor instance, and "order" must be called indeed at most once.
>>
>> Regards,
>> Hanson
>>
>> On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com 
>> <mailto:peter.levart at gmail.com>> wrote:
>>
>>     Hi Hanson,
>>
>>     I don't think anything like that readily exists in
>>     java.lang.concurrent, but what you describe should be possible to
>>     achieve with composition of existing primitives. You haven't
>>     given any additional hints to what your OrderedExecutor should
>>     behave like. Should it be a one-shot object (like CountDownLatch)
>>     or a re-usable one (like CyclicBarrier)? Will
>>     execCriticalSectionInOrder() for a particular OrderedExecutor
>>     instance and 'order' value be called at most once? If yes (and I
>>     think that only a one-shot object makes sense here), an array of
>>     CountDownLatch(es) could be used:
>>
>>     public class OrderedExecutor<T> {
>>         private final CountDownLatch[] latches;
>>
>>         public OrderedExecutor(int n) {
>>             if (n < 1) throw new IllegalArgumentException("'n' should
>>     be >= 1");
>>             latches = new CountDownLatch[n - 1];
>>             for (int i = 0; i < latches.length; i++) {
>>                 latches[i] = new CountDownLatch(1);
>>             }
>>         }
>>
>>         public T execCriticalSectionInOrder(final int order,
>>                                             final Supplier<T>
>>     criticalSection) throws InterruptedException {
>>             if (order < 0 || order > latches.length)
>>                 throw new IllegalArgumentException("'order' should be
>>     [0..." + latches.length + "]");
>>             if (order > 0) {
>>                 latches[order - 1].await();
>>             }
>>             try {
>>                 return criticalSection.get();
>>             } finally {
>>                 if (order < latches.length) {
>>                     latches[order].countDown();
>>                 }
>>             }
>>         }
>>     }
>>
>>
>>     Regards, Peter
>>
>>
>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>     Hi, I am looking for a construct that can be used to efficiently
>>>     enforce ordered execution of multiple critical sections, each
>>>     calling from a different thread. The calling threads may run in
>>>     parallel and may call the execution method out of order. The
>>>     perceived construct would therefore be responsible for
>>>     re-ordering the execution of those threads, so that their
>>>     critical sections (and only the critical section) will be
>>>     executed in order. Would something like the following API
>>>     already exist? /** * Used to enforce ordered execution of
>>>     critical sections calling from multiple * threads, parking and
>>>     unparking the threads as necessary. */ public class
>>>     OrderedExecutor<T> { /** * Executes a critical section at most
>>>     once with the given order, parking * and unparking the current
>>>     thread as necessary so that all critical * sections executed by
>>>     different threads using this executor take place in * the order
>>>     from 1 to n consecutively. */ public T execCriticalSectionInOrder(
>>>     final int order,
>>>                  final Callable<T> criticalSection) throws InterruptedException;
>>>     }
>>>
>>>     Regards,
>>>     Hanson
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/973ca46a/attachment.html>

From joe.bowbeer at gmail.com  Sun Dec 14 12:05:10 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 14 Dec 2014 09:05:10 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEo4-DSt=f5a6y4SwhEE8hbnM4swqvAwqmWK_4d2RByUNg@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<CAHzJPEpsG1R8uEseUMRKBmfEP1Mw4ZCshqj6HsYieSb+W1vLtQ@mail.gmail.com>
	<CAHzJPEo4-DSt=f5a6y4SwhEE8hbnM4swqvAwqmWK_4d2RByUNg@mail.gmail.com>
Message-ID: <CAHzJPEqqgrOavRUA3TqTTkqCLB=bF53X5FAj-5nyd-wwx7c_xg@mail.gmail.com>

Even the simple implementation has a bug :-(

Should be:

  public OrderedExecutor() {
    getLock(0).release();
  }


On Sun, Dec 14, 2014 at 8:39 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>
> Below is a simple implementation where 'n' is not known in advance,
> assuming '0' is the starting order:
>
> class OrderedExecutor<T> {
>
>   private final List<Semaphore> locks = new ArrayList<>();
>
>   public OrderedExecutor() {
>     locks.add(0, new Semaphore(1));
>   }
>
>   public T execCallableInOrder(int order, Callable<T> callable)
>       throws InterruptedException, Exception {
>     getLock(order).acquire();
>     try {
>       return callable.call();
>     } finally {
>       getLock(order + 1).release();
>     }
>   }
>
>   private synchronized Semaphore getLock(int order) {
>     for (int i = locks.size(); i <= order; i++) {
>       locks.add(i, new Semaphore(0));
>     }
>     return locks.get(order);
>   }
> }
>
> If this were long-lived, I'd want to trim the locks array as they were
> consumed.
>
> On Sun, Dec 14, 2014 at 6:37 AM, Joe Bowbeer <joe.bowbeer at gmail.com>
> wrote:
>
>> Here's a "cyclic" version using Semaphore.  It resets when the last
>> callable execs.
>>
>> class OrderedExecutor<T> {
>>
>>   final Semaphore[] locks;
>>
>>   OrderedExecutor(int n) {
>>     locks = new Semaphore[n];
>>     for (int i = n; --i >= 0; ) {
>>       locks[i] = new Semaphore(0);
>>     }
>>     locks[0].release();
>>   }
>>
>>   public T execCallableInOrder(int order, Callable<T> callable)
>>       throws InterruptedException, Exception {
>>     locks[order].acquire();
>>     try {
>>       return callable.call();
>>     } finally {
>>       locks[(order + 1) % locks.length].release();
>>     }
>>   }
>> }
>>
>>
>> On Sat, Dec 13, 2014 at 8:26 PM, Hanson Char <hanson.char at gmail.com>
>> wrote:
>>>
>>> Hi,
>>>
>>> I am looking for a construct that can be used to efficiently enforce
>>> ordered execution of multiple critical sections, each calling from a
>>> different thread. The calling threads may run in parallel and may call
>>> the execution method out of order.  The perceived construct would
>>> therefore be responsible for re-ordering the execution of those
>>> threads, so that their critical sections (and only the critical
>>> section) will be executed in order.
>>>
>>> Would something like the following API already exist?
>>>
>>> /**
>>>  * Used to enforce ordered execution of critical sections calling from
>>> multiple
>>>  * threads, parking and unparking the threads as necessary.
>>>  */
>>> public class OrderedExecutor<T> {
>>>     /**
>>>      * Executes a critical section at most once with the given order,
>>> parking
>>>      * and unparking the current thread as necessary so that all critical
>>>      * sections executed by different threads using this executor take
>>> place in
>>>      * the order from 1 to n consecutively.
>>>      */
>>>     public T execCriticalSectionInOrder(final int order,
>>>             final Callable<T> criticalSection) throws
>>> InterruptedException;
>>> }
>>>
>>> Regards,
>>> Hanson
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/e77938b4/attachment-0001.html>

From peter.levart at gmail.com  Sun Dec 14 12:11:26 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 14 Dec 2014 18:11:26 +0100
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CABWgujZNjerMZLfXt4uYoxjvtcjZjzgLUi5Bi7yMX-Ruz53Z3g@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>	<548D64B4.5090505@gmail.com>	<CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>	<548DB5FB.4060305@gmail.com>
	<CABWgujZNjerMZLfXt4uYoxjvtcjZjzgLUi5Bi7yMX-Ruz53Z3g@mail.gmail.com>
Message-ID: <548DC4BE.8070600@gmail.com>


On 12/14/2014 05:37 PM, Hanson Char wrote:
> Thanks, Peter.  I like your idea.  It seems both CountDownLatch and 
> Semaphore are viable options for implementing this use case.  Are you 
> aware of any performance difference one may have over the other (in 
> this case)?

I think they are both the same as they both use 
AbstractQueuedSynchronizer under the hood. But they are actually 
overkill for your use-case. You only ever synchronize between two 
threads at a time, so a "simplified" variant is possible (as shown in my 
previous mail). Simplified in the sense of state overhead - the 
performance should be comparable.

Regards, Peter

>
> Regards,
> Hanson
> ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/287fe158/attachment.html>

From hanson.char at gmail.com  Sun Dec 14 12:11:30 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 14 Dec 2014 09:11:30 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <548DC266.9070705@gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<548D64B4.5090505@gmail.com>
	<CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>
	<548DB5FB.4060305@gmail.com> <548DC266.9070705@gmail.com>
Message-ID: <CABWgujbiBaePn7WpysgVo=mi4DL0-M4czZazQYn0Kmzd=qQfUA@mail.gmail.com>

Hi Peter,

Thanks for this proposed idea of using LockSupport. This begs the question:
which one would you choose if you had all three (correct) implementation
available?  (Semaphore, CountDownLatch, or LockSupport)?

Regards,
Hanson

On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart <peter.levart at gmail.com>
wrote:
>
>  Hi Hanson,
>
> This one is more low-level, but catches some invalid usages and is more
> resource-friendly:
>
>
> public class OrderedExecutor {
>
>     public <T> T execCriticalSectionInOrder(
>         final int order,
>         final Supplier<T> criticalSection
>     ) throws InterruptedException {
>         if (order < 0) {
>             throw new IllegalArgumentException("'order' should be >= 0");
>         }
>         if (order > 0) {
>             waitForDone(order - 1);
>         }
>         try {
>             return criticalSection.get();
>         } finally {
>             notifyDone(order);
>         }
>     }
>
>     private static final Object DONE = new Object();
>     private final ConcurrentMap<Integer, Object> signals = new
> ConcurrentHashMap<>();
>
>     private void waitForDone(int order) throws InterruptedException {
>         Object sig = signals.putIfAbsent(order, Thread.currentThread());
>         if (sig != null && sig != DONE) {
>             throw new IllegalStateException();
>         }
>         while (sig != DONE) {
>             LockSupport.park();
>             if (Thread.interrupted()) {
>                 throw new InterruptedException();
>             }
>             sig = signals.get(order);
>         }
>     }
>
>     private void notifyDone(int order) {
>         Object sig = signals.putIfAbsent(order, DONE);
>         if (sig instanceof Thread) {
>             if (!signals.replace(order, sig, DONE)) {
>                 throw new IllegalStateException();
>             }
>             LockSupport.unpark((Thread) sig);
>         } else if (sig != null) {
>             throw new IllegalStateException();
>         }
>     }
> }
>
>
> Regards, Peter
>
>
> On 12/14/2014 05:08 PM, Peter Levart wrote:
>
>
> On 12/14/2014 04:20 PM, Hanson Char wrote:
>
> Hi Peter,
>
>  Thanks for the suggestion, and sorry about not being clear about one
> important detail: "n" is not known a priori when constructing an
> OrderedExecutor.  Does this mean the use of  CountDownLatch is ruled out?
>
>
> If you know at least the upper bound of 'n', it can be used with such 'n'.
> Otherwise something that dynamically re-sizes the array could be devised.
> Or you could simply use a ConcurrentHashMap instead of array where keys are
> 'order' values:
>
>
> public class OrderedExecutor<T> {
>
>     private final ConcurrentMap<Integer, CountDownLatch> latches = new
> ConcurrentHashMap<>();
>
>     public T execCriticalSectionInOrder(final int order,
>                                         final Supplier<T> criticalSection)
> throws InterruptedException {
>         if (order > 0) {
>             latches.computeIfAbsent(order - 1, o -> new
> CountDownLatch(1)).await();
>         }
>         try {
>             return criticalSection.get();
>         } finally {
>             latches.computeIfAbsent(order, o -> new
> CountDownLatch(1)).countDown();
>         }
>     }
> }
>
>
> Regards, Peter
>
>
>  You guessed right: it's a one-shot object for a particular
> OrderedExecutor instance, and "order" must be called indeed at most once.
>
>  Regards,
> Hanson
>
> On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com>
> wrote:
>>
>>  Hi Hanson,
>>
>> I don't think anything like that readily exists in java.lang.concurrent,
>> but what you describe should be possible to achieve with composition of
>> existing primitives. You haven't given any additional hints to what your
>> OrderedExecutor should behave like. Should it be a one-shot object (like
>> CountDownLatch) or a re-usable one (like CyclicBarrier)? Will
>> execCriticalSectionInOrder() for a particular OrderedExecutor instance and
>> 'order' value be called at most once? If yes (and I think that only a
>> one-shot object makes sense here), an array of CountDownLatch(es) could be
>> used:
>>
>> public class OrderedExecutor<T> {
>>     private final CountDownLatch[] latches;
>>
>>     public OrderedExecutor(int n) {
>>         if (n < 1) throw new IllegalArgumentException("'n' should be >=
>> 1");
>>         latches = new CountDownLatch[n - 1];
>>         for (int i = 0; i < latches.length; i++) {
>>             latches[i] = new CountDownLatch(1);
>>         }
>>     }
>>
>>     public T execCriticalSectionInOrder(final int order,
>>                                          final Supplier<T>
>> criticalSection) throws InterruptedException {
>>         if (order < 0 || order > latches.length)
>>             throw new IllegalArgumentException("'order' should be [0..."
>> + latches.length + "]");
>>         if (order > 0) {
>>             latches[order - 1].await();
>>         }
>>         try {
>>             return criticalSection.get();
>>         } finally {
>>             if (order < latches.length) {
>>                 latches[order].countDown();
>>             }
>>         }
>>     }
>> }
>>
>>
>> Regards, Peter
>>
>>
>> On 12/14/2014 05:26 AM, Hanson Char wrote:
>>
>> Hi,
>>
>> I am looking for a construct that can be used to efficiently enforce
>> ordered execution of multiple critical sections, each calling from a
>> different thread. The calling threads may run in parallel and may call
>> the execution method out of order.  The perceived construct would
>> therefore be responsible for re-ordering the execution of those
>> threads, so that their critical sections (and only the critical
>> section) will be executed in order.
>>
>> Would something like the following API already exist?
>>
>> /**
>>  * Used to enforce ordered execution of critical sections calling from multiple
>>  * threads, parking and unparking the threads as necessary.
>>  */
>> public class OrderedExecutor<T> {
>>     /**
>>      * Executes a critical section at most once with the given order, parking
>>      * and unparking the current thread as necessary so that all critical
>>      * sections executed by different threads using this executor take place in
>>      * the order from 1 to n consecutively.
>>      */
>>     public T execCriticalSectionInOrder(
>> final int order,
>>             final Callable<T> criticalSection) throws InterruptedException;
>> }
>>
>> Regards,
>> Hanson
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/693efc75/attachment.html>

From peter.levart at gmail.com  Sun Dec 14 12:31:15 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 14 Dec 2014 18:31:15 +0100
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CABWgujbiBaePn7WpysgVo=mi4DL0-M4czZazQYn0Kmzd=qQfUA@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>	<548D64B4.5090505@gmail.com>	<CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>	<548DB5FB.4060305@gmail.com>	<548DC266.9070705@gmail.com>
	<CABWgujbiBaePn7WpysgVo=mi4DL0-M4czZazQYn0Kmzd=qQfUA@mail.gmail.com>
Message-ID: <548DC963.802@gmail.com>


On 12/14/2014 06:11 PM, Hanson Char wrote:
> Hi Peter,
>
> Thanks for this proposed idea of using LockSupport. This begs the 
> question: which one would you choose if you had all three (correct) 
> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>
> Regards,
> Hanson

The Semaphore/CountDownLatch variants are equivalent if you don't need 
re-use. So any would do. They lack invalid-use detection. What happens 
if they are not used as intended? Semaphore variant acts differently 
than CountDownLatch variant. The low-level variant I proposed detects 
invalid usage. So I would probably use this one. But the low level 
variant is harder to reason about it's correctness. I think it is 
correct, but you should show it to somebody else to confirm this.

Another question is whether you actually need this kind of synchronizer. 
Maybe if you explained what you are trying to achieve, somebody could 
have an idea how to do that even more elegantly...

Regards, Peter

>
> On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart <peter.levart at gmail.com 
> <mailto:peter.levart at gmail.com>> wrote:
>
>     Hi Hanson,
>
>     This one is more low-level, but catches some invalid usages and is
>     more resource-friendly:
>
>
>     public class OrderedExecutor {
>
>         public <T> T execCriticalSectionInOrder(
>             final int order,
>             final Supplier<T> criticalSection
>         ) throws InterruptedException {
>             if (order < 0) {
>                 throw new IllegalArgumentException("'order' should be
>     >= 0");
>             }
>             if (order > 0) {
>                 waitForDone(order - 1);
>             }
>             try {
>                 return criticalSection.get();
>             } finally {
>                 notifyDone(order);
>             }
>         }
>
>         private static final Object DONE = new Object();
>         private final ConcurrentMap<Integer, Object> signals = new
>     ConcurrentHashMap<>();
>
>         private void waitForDone(int order) throws InterruptedException {
>             Object sig = signals.putIfAbsent(order,
>     Thread.currentThread());
>             if (sig != null && sig != DONE) {
>                 throw new IllegalStateException();
>             }
>             while (sig != DONE) {
>                 LockSupport.park();
>                 if (Thread.interrupted()) {
>                     throw new InterruptedException();
>                 }
>                 sig = signals.get(order);
>             }
>         }
>
>         private void notifyDone(int order) {
>             Object sig = signals.putIfAbsent(order, DONE);
>             if (sig instanceof Thread) {
>                 if (!signals.replace(order, sig, DONE)) {
>                     throw new IllegalStateException();
>                 }
>                 LockSupport.unpark((Thread) sig);
>             } else if (sig != null) {
>                 throw new IllegalStateException();
>             }
>         }
>     }
>
>
>     Regards, Peter
>
>
>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>
>>     On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>     Hi Peter,
>>>
>>>     Thanks for the suggestion, and sorry about not being clear about
>>>     one important detail: "n" is not known a priori when
>>>     constructing an OrderedExecutor.  Does this mean the use of
>>>      CountDownLatch is ruled out?
>>
>>     If you know at least the upper bound of 'n', it can be used with
>>     such 'n'. Otherwise something that dynamically re-sizes the array
>>     could be devised. Or you could simply use a ConcurrentHashMap
>>     instead of array where keys are 'order' values:
>>
>>
>>     public class OrderedExecutor<T> {
>>
>>         private final ConcurrentMap<Integer, CountDownLatch> latches
>>     = new ConcurrentHashMap<>();
>>
>>         public T execCriticalSectionInOrder(final int order,
>>                                             final Supplier<T>
>>     criticalSection) throws InterruptedException {
>>             if (order > 0) {
>>                 latches.computeIfAbsent(order - 1, o -> new
>>     CountDownLatch(1)).await();
>>             }
>>             try {
>>                 return criticalSection.get();
>>             } finally {
>>                 latches.computeIfAbsent(order, o -> new
>>     CountDownLatch(1)).countDown();
>>             }
>>         }
>>     }
>>
>>
>>     Regards, Peter
>>
>>>
>>>     You guessed right: it's a one-shot object for a particular
>>>     OrderedExecutor instance, and "order" must be called indeed at
>>>     most once.
>>>
>>>     Regards,
>>>     Hanson
>>>
>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart
>>>     <peter.levart at gmail.com <mailto:peter.levart at gmail.com>> wrote:
>>>
>>>         Hi Hanson,
>>>
>>>         I don't think anything like that readily exists in
>>>         java.lang.concurrent, but what you describe should be
>>>         possible to achieve with composition of existing primitives.
>>>         You haven't given any additional hints to what your
>>>         OrderedExecutor should behave like. Should it be a one-shot
>>>         object (like CountDownLatch) or a re-usable one (like
>>>         CyclicBarrier)? Will execCriticalSectionInOrder() for a
>>>         particular OrderedExecutor instance and 'order' value be
>>>         called at most once? If yes (and I think that only a
>>>         one-shot object makes sense here), an array of
>>>         CountDownLatch(es) could be used:
>>>
>>>         public class OrderedExecutor<T> {
>>>             private final CountDownLatch[] latches;
>>>
>>>             public OrderedExecutor(int n) {
>>>                 if (n < 1) throw new IllegalArgumentException("'n'
>>>         should be >= 1");
>>>                 latches = new CountDownLatch[n - 1];
>>>                 for (int i = 0; i < latches.length; i++) {
>>>                     latches[i] = new CountDownLatch(1);
>>>                 }
>>>             }
>>>
>>>             public T execCriticalSectionInOrder(final int order,
>>>         final Supplier<T> criticalSection) throws InterruptedException {
>>>                 if (order < 0 || order > latches.length)
>>>                     throw new IllegalArgumentException("'order'
>>>         should be [0..." + latches.length + "]");
>>>                 if (order > 0) {
>>>                     latches[order - 1].await();
>>>                 }
>>>                 try {
>>>                     return criticalSection.get();
>>>                 } finally {
>>>                     if (order < latches.length) {
>>>         latches[order].countDown();
>>>                     }
>>>                 }
>>>             }
>>>         }
>>>
>>>
>>>         Regards, Peter
>>>
>>>
>>>         On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>         Hi, I am looking for a construct that can be used to
>>>>         efficiently enforce ordered execution of multiple critical
>>>>         sections, each calling from a different thread. The calling
>>>>         threads may run in parallel and may call the execution
>>>>         method out of order. The perceived construct would
>>>>         therefore be responsible for re-ordering the execution of
>>>>         those threads, so that their critical sections (and only
>>>>         the critical section) will be executed in order. Would
>>>>         something like the following API already exist? /** * Used
>>>>         to enforce ordered execution of critical sections calling
>>>>         from multiple * threads, parking and unparking the threads
>>>>         as necessary. */ public class OrderedExecutor<T> { /** *
>>>>         Executes a critical section at most once with the given
>>>>         order, parking * and unparking the current thread as
>>>>         necessary so that all critical * sections executed by
>>>>         different threads using this executor take place in * the
>>>>         order from 1 to n consecutively. */ public T
>>>>         execCriticalSectionInOrder(
>>>>         final int order,
>>>>                      final Callable<T> criticalSection) throws InterruptedException;
>>>>         }
>>>>
>>>>         Regards,
>>>>         Hanson
>>>>         _______________________________________________
>>>>         Concurrency-interest mailing list
>>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/3f522a5c/attachment-0001.html>

From joe.bowbeer at gmail.com  Sun Dec 14 13:14:28 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 14 Dec 2014 10:14:28 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <548DC963.802@gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<548D64B4.5090505@gmail.com>
	<CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>
	<548DB5FB.4060305@gmail.com> <548DC266.9070705@gmail.com>
	<CABWgujbiBaePn7WpysgVo=mi4DL0-M4czZazQYn0Kmzd=qQfUA@mail.gmail.com>
	<548DC963.802@gmail.com>
Message-ID: <CAHzJPEq44XoqNaJP02W=70E5NnUJgy4a0wqo+4GbV10+4=qJ9g@mail.gmail.com>

I would use the latest Peter Levart version with park/unpark and a
ConcurrentMap that cleans itself.

On Sun, Dec 14, 2014 at 9:31 AM, Peter Levart <peter.levart at gmail.com>
wrote:
>
>
> On 12/14/2014 06:11 PM, Hanson Char wrote:
>
> Hi Peter,
>
>  Thanks for this proposed idea of using LockSupport. This begs the
> question: which one would you choose if you had all three (correct)
> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>
>  Regards,
> Hanson
>
>
> The Semaphore/CountDownLatch variants are equivalent if you don't need
> re-use. So any would do. They lack invalid-use detection. What happens if
> they are not used as intended? Semaphore variant acts differently than
> CountDownLatch variant. The low-level variant I proposed detects invalid
> usage. So I would probably use this one. But the low level variant is
> harder to reason about it's correctness. I think it is correct, but you
> should show it to somebody else to confirm this.
>
> Another question is whether you actually need this kind of synchronizer.
> Maybe if you explained what you are trying to achieve, somebody could have
> an idea how to do that even more elegantly...
>
> Regards, Peter
>
>
>
> On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart <peter.levart at gmail.com>
> wrote:
>>
>>  Hi Hanson,
>>
>> This one is more low-level, but catches some invalid usages and is more
>> resource-friendly:
>>
>>
>> public class OrderedExecutor {
>>
>>     public <T> T execCriticalSectionInOrder(
>>         final int order,
>>         final Supplier<T> criticalSection
>>     ) throws InterruptedException {
>>         if (order < 0) {
>>              throw new IllegalArgumentException("'order' should be >= 0");
>>         }
>>         if (order > 0) {
>>             waitForDone(order - 1);
>>         }
>>         try {
>>             return criticalSection.get();
>>         } finally {
>>             notifyDone(order);
>>         }
>>     }
>>
>>     private static final Object DONE = new Object();
>>     private final ConcurrentMap<Integer, Object> signals = new
>> ConcurrentHashMap<>();
>>
>>     private void waitForDone(int order) throws InterruptedException {
>>         Object sig = signals.putIfAbsent(order, Thread.currentThread());
>>         if (sig != null && sig != DONE) {
>>             throw new IllegalStateException();
>>         }
>>         while (sig != DONE) {
>>             LockSupport.park();
>>             if (Thread.interrupted()) {
>>                 throw new InterruptedException();
>>             }
>>             sig = signals.get(order);
>>         }
>>     }
>>
>>     private void notifyDone(int order) {
>>         Object sig = signals.putIfAbsent(order, DONE);
>>         if (sig instanceof Thread) {
>>             if (!signals.replace(order, sig, DONE)) {
>>                 throw new IllegalStateException();
>>             }
>>             LockSupport.unpark((Thread) sig);
>>         } else if (sig != null) {
>>             throw new IllegalStateException();
>>         }
>>     }
>> }
>>
>>
>> Regards, Peter
>>
>>
>> On 12/14/2014 05:08 PM, Peter Levart wrote:
>>
>>
>> On 12/14/2014 04:20 PM, Hanson Char wrote:
>>
>> Hi Peter,
>>
>>  Thanks for the suggestion, and sorry about not being clear about one
>> important detail: "n" is not known a priori when constructing an
>> OrderedExecutor.  Does this mean the use of  CountDownLatch is ruled out?
>>
>>
>> If you know at least the upper bound of 'n', it can be used with such
>> 'n'. Otherwise something that dynamically re-sizes the array could be
>> devised. Or you could simply use a ConcurrentHashMap instead of array where
>> keys are 'order' values:
>>
>>
>> public class OrderedExecutor<T> {
>>
>>     private final ConcurrentMap<Integer, CountDownLatch> latches = new
>> ConcurrentHashMap<>();
>>
>>     public T execCriticalSectionInOrder(final int order,
>>                                         final Supplier<T>
>> criticalSection) throws InterruptedException {
>>         if (order > 0) {
>>             latches.computeIfAbsent(order - 1, o -> new
>> CountDownLatch(1)).await();
>>         }
>>         try {
>>             return criticalSection.get();
>>         } finally {
>>             latches.computeIfAbsent(order, o -> new
>> CountDownLatch(1)).countDown();
>>         }
>>     }
>> }
>>
>>
>> Regards, Peter
>>
>>
>>  You guessed right: it's a one-shot object for a particular
>> OrderedExecutor instance, and "order" must be called indeed at most once.
>>
>>  Regards,
>> Hanson
>>
>> On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com>
>> wrote:
>>>
>>>  Hi Hanson,
>>>
>>> I don't think anything like that readily exists in java.lang.concurrent,
>>> but what you describe should be possible to achieve with composition of
>>> existing primitives. You haven't given any additional hints to what your
>>> OrderedExecutor should behave like. Should it be a one-shot object (like
>>> CountDownLatch) or a re-usable one (like CyclicBarrier)? Will
>>> execCriticalSectionInOrder() for a particular OrderedExecutor instance and
>>> 'order' value be called at most once? If yes (and I think that only a
>>> one-shot object makes sense here), an array of CountDownLatch(es) could be
>>> used:
>>>
>>> public class OrderedExecutor<T> {
>>>     private final CountDownLatch[] latches;
>>>
>>>     public OrderedExecutor(int n) {
>>>         if (n < 1) throw new IllegalArgumentException("'n' should be >=
>>> 1");
>>>         latches = new CountDownLatch[n - 1];
>>>         for (int i = 0; i < latches.length; i++) {
>>>             latches[i] = new CountDownLatch(1);
>>>         }
>>>     }
>>>
>>>     public T execCriticalSectionInOrder(final int order,
>>>                                          final Supplier<T>
>>> criticalSection) throws InterruptedException {
>>>         if (order < 0 || order > latches.length)
>>>             throw new IllegalArgumentException("'order' should be [0..."
>>> + latches.length + "]");
>>>         if (order > 0) {
>>>             latches[order - 1].await();
>>>         }
>>>         try {
>>>             return criticalSection.get();
>>>         } finally {
>>>             if (order < latches.length) {
>>>                 latches[order].countDown();
>>>             }
>>>         }
>>>     }
>>> }
>>>
>>>
>>> Regards, Peter
>>>
>>>
>>> On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>
>>> Hi,
>>>
>>> I am looking for a construct that can be used to efficiently enforce
>>> ordered execution of multiple critical sections, each calling from a
>>> different thread. The calling threads may run in parallel and may call
>>> the execution method out of order.  The perceived construct would
>>> therefore be responsible for re-ordering the execution of those
>>> threads, so that their critical sections (and only the critical
>>> section) will be executed in order.
>>>
>>> Would something like the following API already exist?
>>>
>>> /**
>>>  * Used to enforce ordered execution of critical sections calling from multiple
>>>  * threads, parking and unparking the threads as necessary.
>>>  */
>>> public class OrderedExecutor<T> {
>>>     /**
>>>      * Executes a critical section at most once with the given order, parking
>>>      * and unparking the current thread as necessary so that all critical
>>>      * sections executed by different threads using this executor take place in
>>>      * the order from 1 to n consecutively.
>>>      */
>>>     public T execCriticalSectionInOrder(
>>> final int order,
>>>             final Callable<T> criticalSection) throws InterruptedException;
>>> }
>>>
>>> Regards,
>>> Hanson
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/63db4738/attachment.html>

From hanson.char at gmail.com  Sun Dec 14 13:30:08 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 14 Dec 2014 10:30:08 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEq44XoqNaJP02W=70E5NnUJgy4a0wqo+4GbV10+4=qJ9g@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<548D64B4.5090505@gmail.com>
	<CABWgujb7=qVFLZ+CAGUakhWff5XzzfsPVVbUSBhjdaTk+gdCfg@mail.gmail.com>
	<548DB5FB.4060305@gmail.com> <548DC266.9070705@gmail.com>
	<CABWgujbiBaePn7WpysgVo=mi4DL0-M4czZazQYn0Kmzd=qQfUA@mail.gmail.com>
	<548DC963.802@gmail.com>
	<CAHzJPEq44XoqNaJP02W=70E5NnUJgy4a0wqo+4GbV10+4=qJ9g@mail.gmail.com>
Message-ID: <CABWgujaaMyssOoFz2W-fauz=4GQpOg-3r9h4hiB+-WYXBKYCBQ@mail.gmail.com>

I think we have a great convergence.  Thanks!

On Sun, Dec 14, 2014 at 10:14 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>
> I would use the latest Peter Levart version with park/unpark and a
> ConcurrentMap that cleans itself.
>
> On Sun, Dec 14, 2014 at 9:31 AM, Peter Levart <peter.levart at gmail.com>
> wrote:
>>
>>
>> On 12/14/2014 06:11 PM, Hanson Char wrote:
>>
>> Hi Peter,
>>
>>  Thanks for this proposed idea of using LockSupport. This begs the
>> question: which one would you choose if you had all three (correct)
>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>
>>  Regards,
>> Hanson
>>
>>
>> The Semaphore/CountDownLatch variants are equivalent if you don't need
>> re-use. So any would do. They lack invalid-use detection. What happens if
>> they are not used as intended? Semaphore variant acts differently than
>> CountDownLatch variant. The low-level variant I proposed detects invalid
>> usage. So I would probably use this one. But the low level variant is
>> harder to reason about it's correctness. I think it is correct, but you
>> should show it to somebody else to confirm this.
>>
>> Another question is whether you actually need this kind of synchronizer.
>> Maybe if you explained what you are trying to achieve, somebody could have
>> an idea how to do that even more elegantly...
>>
>> Regards, Peter
>>
>>
>>
>> On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart <peter.levart at gmail.com>
>> wrote:
>>>
>>>  Hi Hanson,
>>>
>>> This one is more low-level, but catches some invalid usages and is more
>>> resource-friendly:
>>>
>>>
>>> public class OrderedExecutor {
>>>
>>>     public <T> T execCriticalSectionInOrder(
>>>         final int order,
>>>         final Supplier<T> criticalSection
>>>     ) throws InterruptedException {
>>>         if (order < 0) {
>>>              throw new IllegalArgumentException("'order' should be >=
>>> 0");
>>>         }
>>>         if (order > 0) {
>>>             waitForDone(order - 1);
>>>         }
>>>         try {
>>>             return criticalSection.get();
>>>         } finally {
>>>             notifyDone(order);
>>>         }
>>>     }
>>>
>>>     private static final Object DONE = new Object();
>>>     private final ConcurrentMap<Integer, Object> signals = new
>>> ConcurrentHashMap<>();
>>>
>>>     private void waitForDone(int order) throws InterruptedException {
>>>         Object sig = signals.putIfAbsent(order, Thread.currentThread());
>>>         if (sig != null && sig != DONE) {
>>>             throw new IllegalStateException();
>>>         }
>>>         while (sig != DONE) {
>>>             LockSupport.park();
>>>             if (Thread.interrupted()) {
>>>                 throw new InterruptedException();
>>>             }
>>>             sig = signals.get(order);
>>>         }
>>>     }
>>>
>>>     private void notifyDone(int order) {
>>>         Object sig = signals.putIfAbsent(order, DONE);
>>>         if (sig instanceof Thread) {
>>>             if (!signals.replace(order, sig, DONE)) {
>>>                 throw new IllegalStateException();
>>>             }
>>>             LockSupport.unpark((Thread) sig);
>>>         } else if (sig != null) {
>>>             throw new IllegalStateException();
>>>         }
>>>     }
>>> }
>>>
>>>
>>> Regards, Peter
>>>
>>>
>>> On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>
>>>
>>> On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>
>>> Hi Peter,
>>>
>>>  Thanks for the suggestion, and sorry about not being clear about one
>>> important detail: "n" is not known a priori when constructing an
>>> OrderedExecutor.  Does this mean the use of  CountDownLatch is ruled out?
>>>
>>>
>>> If you know at least the upper bound of 'n', it can be used with such
>>> 'n'. Otherwise something that dynamically re-sizes the array could be
>>> devised. Or you could simply use a ConcurrentHashMap instead of array where
>>> keys are 'order' values:
>>>
>>>
>>> public class OrderedExecutor<T> {
>>>
>>>     private final ConcurrentMap<Integer, CountDownLatch> latches = new
>>> ConcurrentHashMap<>();
>>>
>>>     public T execCriticalSectionInOrder(final int order,
>>>                                         final Supplier<T>
>>> criticalSection) throws InterruptedException {
>>>         if (order > 0) {
>>>             latches.computeIfAbsent(order - 1, o -> new
>>> CountDownLatch(1)).await();
>>>         }
>>>         try {
>>>             return criticalSection.get();
>>>         } finally {
>>>             latches.computeIfAbsent(order, o -> new
>>> CountDownLatch(1)).countDown();
>>>         }
>>>     }
>>> }
>>>
>>>
>>> Regards, Peter
>>>
>>>
>>>  You guessed right: it's a one-shot object for a particular
>>> OrderedExecutor instance, and "order" must be called indeed at most once.
>>>
>>>  Regards,
>>> Hanson
>>>
>>> On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com>
>>> wrote:
>>>>
>>>>  Hi Hanson,
>>>>
>>>> I don't think anything like that readily exists in
>>>> java.lang.concurrent, but what you describe should be possible to achieve
>>>> with composition of existing primitives. You haven't given any additional
>>>> hints to what your OrderedExecutor should behave like. Should it be a
>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>> (and I think that only a one-shot object makes sense here), an array of
>>>> CountDownLatch(es) could be used:
>>>>
>>>> public class OrderedExecutor<T> {
>>>>     private final CountDownLatch[] latches;
>>>>
>>>>     public OrderedExecutor(int n) {
>>>>         if (n < 1) throw new IllegalArgumentException("'n' should be >=
>>>> 1");
>>>>         latches = new CountDownLatch[n - 1];
>>>>         for (int i = 0; i < latches.length; i++) {
>>>>             latches[i] = new CountDownLatch(1);
>>>>         }
>>>>     }
>>>>
>>>>     public T execCriticalSectionInOrder(final int order,
>>>>                                          final Supplier<T>
>>>> criticalSection) throws InterruptedException {
>>>>         if (order < 0 || order > latches.length)
>>>>             throw new IllegalArgumentException("'order' should be
>>>> [0..." + latches.length + "]");
>>>>         if (order > 0) {
>>>>             latches[order - 1].await();
>>>>         }
>>>>         try {
>>>>             return criticalSection.get();
>>>>         } finally {
>>>>             if (order < latches.length) {
>>>>                 latches[order].countDown();
>>>>             }
>>>>         }
>>>>     }
>>>> }
>>>>
>>>>
>>>> Regards, Peter
>>>>
>>>>
>>>> On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>
>>>> Hi,
>>>>
>>>> I am looking for a construct that can be used to efficiently enforce
>>>> ordered execution of multiple critical sections, each calling from a
>>>> different thread. The calling threads may run in parallel and may call
>>>> the execution method out of order.  The perceived construct would
>>>> therefore be responsible for re-ordering the execution of those
>>>> threads, so that their critical sections (and only the critical
>>>> section) will be executed in order.
>>>>
>>>> Would something like the following API already exist?
>>>>
>>>> /**
>>>>  * Used to enforce ordered execution of critical sections calling from multiple
>>>>  * threads, parking and unparking the threads as necessary.
>>>>  */
>>>> public class OrderedExecutor<T> {
>>>>     /**
>>>>      * Executes a critical section at most once with the given order, parking
>>>>      * and unparking the current thread as necessary so that all critical
>>>>      * sections executed by different threads using this executor take place in
>>>>      * the order from 1 to n consecutively.
>>>>      */
>>>>     public T execCriticalSectionInOrder(
>>>> final int order,
>>>>             final Callable<T> criticalSection) throws InterruptedException;
>>>> }
>>>>
>>>> Regards,
>>>> Hanson
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141214/bf03138f/attachment-0001.html>

From dl at cs.oswego.edu  Mon Dec 15 10:42:46 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 15 Dec 2014 10:42:46 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU92nbV9DW7JCihUYm0t_SBzDsyxgDhao00TRh9k4Y0FAQ@mail.gmail.com>
References: <CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>	<547C740B.1090903@cs.oswego.edu>	<1417646009375-11569.post@n7.nabble.com>	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>	<1417803360836-11578.post@n7.nabble.com>	<CANPzfU-iL1=OrgdTGT4zf_uR7yq=DSgrpXG+dEQPBmSfL1Cu0g@mail.gmail.com>	<1418417042495-11626.post@n7.nabble.com>	<CANPzfU_Qu6jpssDpOsXg74VoUY=Jop_Z8Qojg58k_Kuh=Lgd+Q@mail.gmail.com>	<548C5871.4000300@cs.oswego.edu>
	<CANPzfU92nbV9DW7JCihUYm0t_SBzDsyxgDhao00TRh9k4Y0FAQ@mail.gmail.com>
Message-ID: <548F0176.9010103@cs.oswego.edu>

On 12/13/2014 03:05 PM, ?iktor ?lang wrote:
> On Sat, Dec 13, 2014 at 4:17 PM, Doug Lea <dl at cs.oswego.edu
> <mailto:dl at cs.oswego.edu>> wrote: It would be nice to simplify mixed usages
> across the various styles discussed on this and related threads, but this one
> is relatively straightforward even now:
>
> CompletableFuture.supplyAsync(__() -> data.parallelStream().reduce(.__..)).
> thenApply(...);
>
> Isn't the only reason why this works that they are both running on the
> commonPool?

Well, it always "works" even if you use the non-common-pool
parallel stream idiom, or use thenApplyAsync (vs thenApply)
with some different Executor. Some of the combinations may
transiently create and/or block some threads for the sake of
join-dependencies and hand-offs. It is worth some exploration
to streamline, but these are the kinds of cases in which
occasional needs to add threads, blocking, or queuing are not
usually major concerns, so long as there are no positive-feedback
effects leading to unbounded growth.

The more challenging cases lie elsewhere. Here's some pretentious
pontification about the underlying classic framework design issues.

Concurrent, parallel, and/or distributed APIs tend to be
either "pull" or "push" oriented:

Pull: Consumers call functions (possibly calling others, possibly in
parallel) returning results that are somehow used to produce visible
effects.  These APIs tend to apply best when data sources for a
computation already exist. Fork/Join-style frameworks generalize the
idea of a function call to enable internal parallelism.

Push: Producers trigger sets of computations (possibly in turn
triggering others) when data become available. ultimately leading to
some visible effects.  Reactive completion-based frameworks generalize
the idea of interrupt-handlers etc to allow arbitrary async flows.

In Java, Scala, etc, fluent lambda-based APIs have been found to be
pretty good in helping to structure either kind of parallel
computation, allowing (we hope) more productive exploitation of
multicores with fewer errors than seen with ad-hoc uses of
threads. But mixtures across these can run into performance and
resource problems that we'd like to better address.

Pull-style frameworks can encounter unbounded stalls and resources
when function calls entail external communication via blocking IO. And
Push-style frameworks cannot readily take advantage of efficient
in-place parallel-dag scheduling and execution. (For example, you
wouldn't want to use CompletableFutures for most divide-and-conquer
parallel processing.)

At the core library/JVM level though, concurrency support for both is
roughly similar: Encapsulating as tasks held in various data
structures and scheduling/managing execution. So in principle, we can
support arbitrary combinations. We do some of this already.  The
masochist-only CountedCompleter class requires manual continuation
passing transforms of fork/join style processing that can co-exist
with reactive processing. It is used internally in j.u.Stream, as well
as mixed parallel/distributed frameworks including 0xdata H2O. But
even ignoring the hostility/unusability factor, this approach alone
does not provide a complete solution when used with user-supplied
lambdas that can arbitrarily misbehave.

Short of any grand unified scheme, one approach is to improve
support that encourages replacements of the common enemy of
both of these API styles -- blocking IO. Where near-term
"encouragement" might take several forms, including adaptors
and utilities for CompletionStages, j.u.Streams, and other APIs.

-Doug




From richard.warburton at gmail.com  Mon Dec 15 11:08:35 2014
From: richard.warburton at gmail.com (Richard Warburton)
Date: Mon, 15 Dec 2014 16:08:35 +0000
Subject: [concurrency-interest] Fork/Join Use cases
In-Reply-To: <db9b4f0d-ee8c-4f02-a1db-49cbf374988b@email.android.com>
References: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
	<db9b4f0d-ee8c-4f02-a1db-49cbf374988b@email.android.com>
Message-ID: <CAMaYbvKJTrJRxYCY1+t1bP81Vu4rnSaoFqsWC2rgBZvvJkNERg@mail.gmail.com>

Hi Aleksandar,

Thanks for your reply.

I always found fork/join to be a very foundational, and serving as a solid
> thread pool implementation that works really well, and doesn't blow up,
> which the JDK otherwise lacked. In many cases it serves as a building block
> for other frameworks, since one of the advantages is that all the other
> abstractions can optionally share the same computational resources. But
> since you said that you're not interested in futures, parallel collections,
> Rx, actors...
>

Yes - there is the argument that it can be a useful underlying abstraction
for other frameworks. I'm not trying to be too negative about fork/join as
a framework - I just want to understand what problems people have found it
solve in the application developer space.

For concrete code, one of the applications that comes to mind is parallel
> sorting. You might want to search for Doug Lea's parallel array
> implementation to see it.
> Otherwise, I'm sure you could easily use fj to implement minimax
> algorithms for chess AI, a constraint solving problem, typechecking in a
> compiler, or concurrently issuing a request to several web services.
>

I like Minimax and constraint solving - those are good examples, albeit not
the kind of problem that many Java developers do.

I'm not that convinced that type checking is often a task that is directly
parallelised, compiler internals that I've looked at often rely on more
coarse grained batch parallelism. Also, surely this isn't something that's
done often enough to be a motivating use case for a general framework?

In the case of concurrently issuing requests to several web services my
understanding is that would be a bad use case for Fork/Join. This is
because the work stealing approach is really tuned for CPU bound workloads,
not I/O bound workloads.

To find more use cases, it might make sense to search for online examples
> of Cilk code, as it was one of the first to employ the fork/join model.
>

Thanks - I'll look into that.

regards,

  Richard Warburton

  http://insightfullogic.com
  @RichardWarburto <http://twitter.com/richardwarburto>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141215/e44a454e/attachment.html>

From kirk at kodewerk.com  Mon Dec 15 11:28:11 2014
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Mon, 15 Dec 2014 17:28:11 +0100
Subject: [concurrency-interest] Fork/Join Use cases
In-Reply-To: <CAMaYbvJ==2tsx2s7uqeZkDgaPg-pxJsRz+WD8R+FZRoGBNAeUQ@mail.gmail.com>
References: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
	<AD1ED690-144E-4C0C-9B2D-56D0B5B4ED1A@kodewerk.com>
	<CAMaYbvJ==2tsx2s7uqeZkDgaPg-pxJsRz+WD8R+FZRoGBNAeUQ@mail.gmail.com>
Message-ID: <BEA01DB8-BF4A-49E4-B9CA-F983CE90FF2F@kodewerk.com>

> 
> To be honest I think the difference here between Rx and Streams is really one of use case. If you're looking to push data incrementally through your computation pipeline then Rx is the way to go. If you want to pull a batch of data out of something, then compute a value from this then streams are more appropriate.

Indeed! Most business logic is grab crap from DB and filter it? Stream with a terminator seems reasonable for this. However, pipelining with event is a much better more scalable technique IME and for that Streams don?t work.  

> 
> I've not noticed Rx to be that much more confusing than Streams from an API point of view - though I think that the way you create Observables can be confusing.

Indeed but then I wouldn?t class you as an average application developer so I?d expect you to not be put off by the complexity.

Regards,
Kirk

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141215/e14a974b/attachment.bin>

From richard.warburton at gmail.com  Mon Dec 15 11:29:00 2014
From: richard.warburton at gmail.com (Richard Warburton)
Date: Mon, 15 Dec 2014 16:29:00 +0000
Subject: [concurrency-interest] Fork/Join Use cases
In-Reply-To: <548AEF9B.5040905@cs.oswego.edu>
References: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>
	<548AEF9B.5040905@cs.oswego.edu>
Message-ID: <CAMaYbvKk1=GxQHOduSs5Dwbi7xqCr=wK3AiDu4h_b1D-F-9QMA@mail.gmail.com>

Hi,

Thanks for taking the time to reply.

Something I have noticed with fork/join is the absence of clear-cut and
>> common motivating examples and use cases.
>>
>
> One realm is aggregate operations; many of the most common ones
> (map, filter, reduce, collect, etc) now (in jdk8) prepackaged in.
> Stream.parallel(). But also used by others for custom in-core BigData
> algorithms and the like. (Plus, the underlying work-stealing
> scheduler is useful in other contexts.)


So using fork/join is one approach to implementing such algorithms. Another
would be to use a threadpool, possibly even a work-stealing style thread
pool from fork/join, but manage the problem decomposition and result
aggregation yourself. This seems to be the approach taken by the
GS-Collections framework whose benchmarks suggest is very competitive
performance-wise to parallel streams and have a similar style of API. Can
you be more specific about the benefits here? Perhaps there's something
that GS-Collections has had to reimplement multiple times that you can
point to in their architecture or source code that Streams are getting for
free.

The other side of this coin is that whenever I've seen people deploy some
kind of big-data algorithm a significant problem isn't the computational
part of that algorithm as much as it is querying the data from an external
source which may involve I/O.

everytime I try to fit a parallel programming problem into the fork/join
>> model in my head it doesn't seem to offer me much over the ability to run
>> a
>> series of parallel threads.
>>
>
> Right. The main reason FJ exists in the JDK is to provide a framework
> for developing portable general-purpose versions of such programs,
> that includes ways of controlling how many threads are used,
> expressing and managing granularity thresholds,  load
> balancing when some cores are busy doing other things,
> nested parallelism, combining/reducing results,  and so on.
>
> Some people who don't care about any of these issues, and are
> content with ad-hoc programs that run well on particular computers
> don't have much motivation to use FJ. On the other hand, FJ can help
> in the discovery of parallel techniques by encouraging design
> via structured parallel divide-and-conquer, which is often a
> productive approach.
>

Its perfectly understandable to want to provide a general framework here
and I'm sympathetic to these motivating concerns, but I'm asking about
which situations where you think delegating these responsibilities to the
framework makes things sense over managing them by hand. Often people try
to fit the wrong type of problem into a framework and its harder to manage
the impedance mismatch than solve the problem.

Again I'm not trying to be too negative on F/J here as much as satisfying
my curiosity and making sure that I'm not missing something in my
understanding.

regards,

  Richard Warburton

  http://insightfullogic.com
  @RichardWarburto <http://twitter.com/richardwarburto>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141215/4a27953a/attachment.html>

From dl at cs.oswego.edu  Mon Dec 15 12:16:43 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 15 Dec 2014 12:16:43 -0500
Subject: [concurrency-interest] Fork/Join Use cases
In-Reply-To: <CAMaYbvKk1=GxQHOduSs5Dwbi7xqCr=wK3AiDu4h_b1D-F-9QMA@mail.gmail.com>
References: <CAMaYbvJi7yeaoOLD7+ceEL=YVb4H+ARLnNEhA6JhPC0JZYg9vg@mail.gmail.com>	<548AEF9B.5040905@cs.oswego.edu>
	<CAMaYbvKk1=GxQHOduSs5Dwbi7xqCr=wK3AiDu4h_b1D-F-9QMA@mail.gmail.com>
Message-ID: <548F177B.2020408@cs.oswego.edu>

On 12/15/2014 11:29 AM, Richard Warburton wrote:

> So using fork/join is one approach to implementing such algorithms. Another
> would be to use a threadpool, possibly even a work-stealing style thread pool
>  from fork/join, but manage the problem decomposition and result aggregation
>  yourself. This seems to be the approach taken by the GS-Collections
> framework whose benchmarks suggest is very competitive performance-wise to
> parallel streams and have a similar style of API.

That's a java.util, not java.util.concurrent question :-)

>
> The other side of this coin is that whenever I've seen people deploy some
> kind of big-data algorithm a significant problem isn't the computational part
> of that algorithm as much as it is querying the data from an external source
> which may involve I/O.

Sometimes. (See my other post this morning.)

> Its perfectly understandable to want to provide a general framework here and
> I'm sympathetic to these motivating concerns, but I'm asking about which
> situations where you think delegating these responsibilities to the framework
> makes things sense over managing them by hand. Often people try to fit the
> wrong type of problem into a framework and its harder to manage the impedance
> mismatch than solve the problem.

The primary audience for j.u.c has always been infrastructure,
middleware, and framework developers. The main criterion is whether
providing a standard API (plus at least one high quality implementation)
will be helpful to those developing layered software. (Including
usages in other parts of JDK.)

Initially, I had guessed that the audience size for FJ would be about
the same as, for a random example, SynchronousQueue. As opposed to
components like ConcurrentHashMap that came to be commonly used by
application programmers as well.

In fact, FJ was initially triaged  out of (JDK5) j.u.c. Similarly for
completion-based processing APIs. But they were added when it became
clear that developers were starting to solve similar problems
in gratuitously different ways, and so deserved a common basis.

Still, there will always be people using only the lower-level
components to build alternative/custom versions of middle-level ones.
There are as many specialized variants of jdk concurrent components
out there as there are specialized non-concurrent variants of
other jdk components. (For example, GS collections includes some of
all of these.)

>
> Again I'm not trying to be too negative on F/J here as much as satisfying my
> curiosity and making sure that I'm not missing something in my
> understanding.

My sense is that you are mostly asking why more people express
concern about the existence of ForkJoinPool than SynchronousQueue.
It might just be the sexiness factor. To some people, FJP seems like
something cool that you should know about, even if you have no
reason to use it.

-Doug




From martinrb at google.com  Tue Dec 16 19:06:58 2014
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 16 Dec 2014 16:06:58 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <54895EFB.3060204@redhat.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<5481108A.3030605@oracle.com>
	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>
	<5488B4EF.6060103@redhat.com> <5488EB09.9000501@oracle.com>
	<54895EFB.3060204@redhat.com>
Message-ID: <CA+kOe0_QOXi=mvLdXvYn4EF4J54HKDJVo=S4VBDUjujtpzgYzw@mail.gmail.com>

On Thu, Dec 11, 2014 at 1:08 AM, Andrew Haley <aph at redhat.com> wrote:
> On 11/12/14 00:53, David Holmes wrote:
>> There are many good uses of storestore in the various lock-free
>> algorithms inside hotspot.
>
> There may be many uses, but I am extremely suspicious of how good
> they are.  I wonder if we went through all the uses of storestore in
> hotspot how many bugs we'd find.  As far as I can see (in the absence
> of other barriers) the only things you can write before a storestore
> are constants.

Hans has provided us with the canonical writeup opposing store-store
and load-load barriers, here:
http://www.hboehm.info/c++mm/no_write_fences.html
Few programmers will be able to deal confidently with causality
defying time paradoxes, especially loads from "the future".

From peter.levart at gmail.com  Wed Dec 17 04:28:30 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 17 Dec 2014 10:28:30 +0100
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5490EA49.2050406@oracle.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>	<5481108A.3030605@oracle.com>	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>	<5488B4EF.6060103@redhat.com>	<5488EB09.9000501@oracle.com>	<54895EFB.3060204@redhat.com>	<CA+kOe0_QOXi=mvLdXvYn4EF4J54HKDJVo=S4VBDUjujtpzgYzw@mail.gmail.com>
	<5490EA49.2050406@oracle.com>
Message-ID: <54914CBE.2000201@gmail.com>

On 12/17/2014 03:28 AM, David Holmes wrote:
> On 17/12/2014 10:06 AM, Martin Buchholz wrote:
>> On Thu, Dec 11, 2014 at 1:08 AM, Andrew Haley <aph at redhat.com> wrote:
>>> On 11/12/14 00:53, David Holmes wrote:
>>>> There are many good uses of storestore in the various lock-free
>>>> algorithms inside hotspot.
>>>
>>> There may be many uses, but I am extremely suspicious of how good
>>> they are.  I wonder if we went through all the uses of storestore in
>>> hotspot how many bugs we'd find.  As far as I can see (in the absence
>>> of other barriers) the only things you can write before a storestore
>>> are constants.
>>
>> Hans has provided us with the canonical writeup opposing store-store
>> and load-load barriers, here:
>> http://www.hboehm.info/c++mm/no_write_fences.html
>> Few programmers will be able to deal confidently with causality
>> defying time paradoxes, especially loads from "the future".
>
> Well I take that with a grain of salt - Hans dismisses ordering based 
> on dependencies which puts us into the realm of those "causality 
> defying time paradoxes" in my opinion. Given:
>
> x.a = 0;
> x.a++
> storestore
> x_init = true
>
> Hans allows for the nonsensical, in my view, possibility that the load 
> of x.a can happen after the x_init=true store and yet somehow be 
> subject to the ++ and the ensuing store that has to come before the 
> x_init = true.
>
> David
> -----
>

Perhaps, he is speaking about why it is dangerous to replace BOTH 
release with just store-store AND acquire with just load-load?

The example would then become:

T1:

store x.a <- 0
load r <- x.a
store x.a <- r+1
; store-store
store x_init <- true

T2:

load r <- x.a
; load-load
if (r)
     store x.a <- 42


Suppose a "store" on some hypothetical architecture is actually a 
two-phase execution: prepare-store, commit-store

With prepare-store imagined as speculative posting of store to the write 
buffer and commit-store just marking it in the write buffer as commited, 
so that is is written to main memory on write buffer flush. Non commited 
stores are not written to main memory, but are allowed to be visible to 
loads in some threads (executing on same core?) which are not ordered by 
load-store before the speculative prepare-store. A load-load does not 
prevent the T2 to be executed as following:

T2:

prepare-store x.a <- 42
load r <- x.a
; load-load
if (r)
     commit-store x.a <- 42

Now this speculative prepare-store can (in real time) happen long before 
T1 instructions are executed. The loads in T1 are then allowed to "see" 
this speculative prepare-store from T2, because just store-store does 
not logically order them in any way - only load-store would.


Does this make sense?

Regards, Peter


From peter.levart at gmail.com  Wed Dec 17 04:30:48 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 17 Dec 2014 10:30:48 +0100
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <54914CBE.2000201@gmail.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>	<5481108A.3030605@oracle.com>	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>	<5488B4EF.6060103@redhat.com>	<5488EB09.9000501@oracle.com>	<54895EFB.3060204@redhat.com>	<CA+kOe0_QOXi=mvLdXvYn4EF4J54HKDJVo=S4VBDUjujtpzgYzw@mail.gmail.com>
	<5490EA49.2050406@oracle.com> <54914CBE.2000201@gmail.com>
Message-ID: <54914D48.6040309@gmail.com>

On 12/17/2014 10:28 AM, Peter Levart wrote:
> The example would then become:
>
> T1:
>
> store x.a <- 0
> load r <- x.a
> store x.a <- r+1
> ; store-store
> store x_init <- true
>
> T2:
>
> load r <- x.a
> ; load-load
> if (r)
>     store x.a <- 42

Sorry the above has an error. I meant:

T2:

load r <- x_init
; load-load
if (r)
     store x.a <- 42


Peter

From viktor.klang at gmail.com  Wed Dec 17 04:44:19 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 17 Dec 2014 10:44:19 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <548F0176.9010103@cs.oswego.edu>
References: <CANPzfU-4o8EAnh+MEBciT-Q_0KYZoU2hue4wK=FHf-evk2FpZQ@mail.gmail.com>
	<547C740B.1090903@cs.oswego.edu>
	<1417646009375-11569.post@n7.nabble.com>
	<CANPzfU9YJ7A2K136-B_DHEOAgFSCvMwdybeCZNngsq=+F3G62g@mail.gmail.com>
	<CAHJZN-sp5pk-cMUF-EfRFUyt0E9xBx1a2g+kXQ1U2SBhgWc--Q@mail.gmail.com>
	<CANPzfU8YpyTfCKj_vGQktp7t5w6GONzPtgKO65baVQn_zxDVcg@mail.gmail.com>
	<CAHJZN-vKh=6gfErr6wOL4UH4g38odSER3wdKFr5tDTjhZNLU3g@mail.gmail.com>
	<CANPzfU_7J60T0z1__D3bC6DOjDsgPw9Fod+dUqw5GaycejbeRg@mail.gmail.com>
	<1417803360836-11578.post@n7.nabble.com>
	<CANPzfU-iL1=OrgdTGT4zf_uR7yq=DSgrpXG+dEQPBmSfL1Cu0g@mail.gmail.com>
	<1418417042495-11626.post@n7.nabble.com>
	<CANPzfU_Qu6jpssDpOsXg74VoUY=Jop_Z8Qojg58k_Kuh=Lgd+Q@mail.gmail.com>
	<548C5871.4000300@cs.oswego.edu>
	<CANPzfU92nbV9DW7JCihUYm0t_SBzDsyxgDhao00TRh9k4Y0FAQ@mail.gmail.com>
	<548F0176.9010103@cs.oswego.edu>
Message-ID: <CANPzfU-xHsjLL42MH7_RuwiXd5_jN6czuTLWo0_ApJOCZBG9cA@mail.gmail.com>

On Mon, Dec 15, 2014 at 4:42 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/13/2014 03:05 PM, ?iktor ?lang wrote:
>
>> On Sat, Dec 13, 2014 at 4:17 PM, Doug Lea <dl at cs.oswego.edu
>> <mailto:dl at cs.oswego.edu>> wrote: It would be nice to simplify mixed
>> usages
>> across the various styles discussed on this and related threads, but this
>> one
>> is relatively straightforward even now:
>>
>> CompletableFuture.supplyAsync(__() -> data.parallelStream().reduce(.
>> __..)).
>> thenApply(...);
>>
>> Isn't the only reason why this works that they are both running on the
>> commonPool?
>>
>
> Well, it always "works" even if you use the non-common-pool
> parallel stream idiom, or use thenApplyAsync (vs thenApply)
> with some different Executor. Some of the combinations may
> transiently create and/or block some threads for the sake of
> join-dependencies and hand-offs.


It definitely depends on the ThreadFactory, doesn't it? (from a liveness
perspective)


> It is worth some exploration
> to streamline, but these are the kinds of cases in which
> occasional needs to add threads, blocking, or queuing are not
> usually major concerns, so long as there are no positive-feedback
> effects leading to unbounded growth.
>

Even growth alone can become a problem, especially for high-concurrency
applications (Akka for instance supports -millions- of concurrent
entities?if they all block?)


>
> The more challenging cases lie elsewhere. Here's some pretentious
> pontification about the underlying classic framework design issues.
>
> Concurrent, parallel, and/or distributed APIs tend to be
> either "pull" or "push" oriented:
>
> Pull: Consumers call functions (possibly calling others, possibly in
> parallel) returning results that are somehow used to produce visible
> effects.  These APIs tend to apply best when data sources for a
> computation already exist. Fork/Join-style frameworks generalize the
> idea of a function call to enable internal parallelism.
>
> Push: Producers trigger sets of computations (possibly in turn
> triggering others) when data become available. ultimately leading to
> some visible effects.  Reactive completion-based frameworks generalize
> the idea of interrupt-handlers etc to allow arbitrary async flows.
>
> In Java, Scala, etc, fluent lambda-based APIs have been found to be
> pretty good in helping to structure either kind of parallel
> computation, allowing (we hope) more productive exploitation of
> multicores with fewer errors than seen with ad-hoc uses of
> threads. But mixtures across these can run into performance and
> resource problems that we'd like to better address.
>
> Pull-style frameworks can encounter unbounded stalls and resources
> when function calls entail external communication via blocking IO. And
> Push-style frameworks cannot readily take advantage of efficient
> in-place parallel-dag scheduling and execution. (For example, you
> wouldn't want to use CompletableFutures for most divide-and-conquer
> parallel processing.)
>
> At the core library/JVM level though, concurrency support for both is
> roughly similar: Encapsulating as tasks held in various data
> structures and scheduling/managing execution. So in principle, we can
> support arbitrary combinations. We do some of this already.  The
> masochist-only CountedCompleter class requires manual continuation
> passing transforms of fork/join style processing that can co-exist
> with reactive processing. It is used internally in j.u.Stream, as well
> as mixed parallel/distributed frameworks including 0xdata H2O. But
> even ignoring the hostility/unusability factor, this approach alone
> does not provide a complete solution when used with user-supplied
> lambdas that can arbitrarily misbehave.
>
> Short of any grand unified scheme, one approach is to improve
> support that encourages replacements of the common enemy of
> both of these API styles -- blocking IO. Where near-term
> "encouragement" might take several forms, including adaptors
> and utilities for CompletionStages, j.u.Streams, and other APIs.


And there exists a hybrid solution (not having to choose "push" OR
"pull"?with push having nasty side-effect with unbounded growth if
consumers can't keep up, or having to resort to load-shedding?leading to
potential data loss or increased latencies/cost due to retransmissions; and
pull oftentimes having issues with performance due to the increased
communication cost?ACKing and the lock-step behavior usually associated
with that) that we use in "Reactive Streams"?www.reactive-streams.org?which
at runtime dynamically switches between push and pull depending on if the
consumer can't keep up or the producer can't keep up, without having to
block or do any load-shedding.

It would be interesting to see how JDK support for this could look like?
java.io is blocking, and java.nio isn't really user friendly.


>
> -Doug
>
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141217/106c38c0/attachment.html>

From suman_krec at yahoo.com  Wed Dec 17 09:07:15 2014
From: suman_krec at yahoo.com (suman shil)
Date: Wed, 17 Dec 2014 14:07:15 +0000 (UTC)
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <548DC963.802@gmail.com>
References: <548DC963.802@gmail.com>
Message-ID: <843651815.119507.1418825235350.JavaMail.yahoo@jws10702g.mail.gq1.yahoo.com>

Hi,Following is my solution to solve this problem. Please let me know if I am missing something.
public class OrderedExecutor{ private int currentAllowedOrder = 0; private int maxLength = 0; public OrderedExecutor(int n) { ? ? ? ? this.maxLength = n; }   public synchronized Object execCriticalSectionInOrder( ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? int order, ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Callable<Object> callable) ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? throws Exception { if (order >= maxLength) { throw new Exception("Exceeds maximum order "+ maxLength); }  while(order != currentAllowedOrder) { wait(); }  try { currentAllowedOrder = currentAllowedOrder+1; return callable.call(); } finally { notify(); } }}
RegardsSuman
      From: Peter Levart <peter.levart at gmail.com>
 To: Hanson Char <hanson.char at gmail.com> 
Cc: concurrency-interest <concurrency-interest at cs.oswego.edu> 
 Sent: Sunday, December 14, 2014 11:01 PM
 Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
   
 
 On 12/14/2014 06:11 PM, Hanson Char wrote:
  
 Hi Peter, 
  Thanks for this proposed idea of using LockSupport. This begs the question: which one would you choose if you had all three (correct) implementation available? ?(Semaphore, CountDownLatch, or LockSupport)? 
  Regards, Hanson  
 
 The Semaphore/CountDownLatch variants are equivalent if you don't need re-use. So any would do. They lack invalid-use detection. What happens if they are not used as intended? Semaphore variant acts differently than CountDownLatch variant. The low-level variant I proposed detects invalid usage. So I would probably use this one. But the low level variant is harder to reason about it's correctness. I think it is correct, but you should show it to somebody else to confirm this.
 
 Another question is whether you actually need this kind of synchronizer. Maybe if you explained what you are trying to achieve, somebody could have an idea how to do that even more elegantly...
 
 Regards, Peter


 
 
 
 On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart <peter.levart at gmail.com> wrote: 
  Hi Hanson,
 
 This one is more low-level, but catches some invalid usages and is more resource-friendly:
 
 
 public class OrderedExecutor {
 
 ??? public <T> T execCriticalSectionInOrder(
 ??????? final int order,
 ??????? final Supplier<T> criticalSection
 ??? ) throws InterruptedException {
 ??????? if (order < 0) {
  ??????????? throw new IllegalArgumentException("'order' should be >= 0");
 ??????? }
 ??????? if (order > 0) {
 ??????????? waitForDone(order - 1);
 ??????? }
 ??????? try {
 ??????????? return criticalSection.get();
 ??????? } finally {
 ??????????? notifyDone(order);
 ??????? }
 ??? }
 
 ??? private static final Object DONE = new Object();
 ??? private final ConcurrentMap<Integer, Object> signals = new ConcurrentHashMap<>();
 
 ??? private void waitForDone(int order) throws InterruptedException {
 ??????? Object sig = signals.putIfAbsent(order, Thread.currentThread());
 ??????? if (sig != null && sig != DONE) {
 ??????????? throw new IllegalStateException();
 ??????? }
 ??????? while (sig != DONE) {
 ??????????? LockSupport.park();
 ??????????? if (Thread.interrupted()) {
 ??????????????? throw new InterruptedException();
 ??????????? }
 ??????????? sig = signals.get(order);
 ??????? }
 ??? }
 
 ??? private void notifyDone(int order) {
 ??????? Object sig = signals.putIfAbsent(order, DONE);
 ??????? if (sig instanceof Thread) {
 ??????????? if (!signals.replace(order, sig, DONE)) {
 ??????????????? throw new IllegalStateException();
 ??????????? }
 ??????????? LockSupport.unpark((Thread) sig);
 ??????? } else if (sig != null) {
 ??????????? throw new IllegalStateException();
 ??????? }
 ??? }
 }
 
 
 Regards, Peter  
 
 On 12/14/2014 05:08 PM, Peter Levart wrote:
  
 
 On 12/14/2014 04:20 PM, Hanson Char wrote:
  
 Hi Peter, 
  Thanks for the suggestion, and sorry about not being clear about one important detail: "n" is not known a priori when constructing an OrderedExecutor.? Does this mean the use of ?CountDownLatch is ruled out?  
 
 If you know at least the upper bound of 'n', it can be used with such 'n'. Otherwise something that dynamically re-sizes the array could be devised. Or you could simply use a ConcurrentHashMap instead of array where keys are 'order' values:
 
 
 public class OrderedExecutor<T> {
 
 ??? private final ConcurrentMap<Integer, CountDownLatch> latches = new ConcurrentHashMap<>();
 
 ??? public T execCriticalSectionInOrder(final int order,
 ??????????????????????????????????????? final Supplier<T> criticalSection) throws InterruptedException {
 ??????? if (order > 0) {
 ??????????? latches.computeIfAbsent(order - 1, o -> new CountDownLatch(1)).await();
 ??????? }
 ??????? try {
 ??????????? return criticalSection.get();
 ??????? } finally {
 ??????????? latches.computeIfAbsent(order, o -> new CountDownLatch(1)).countDown();
 ??????? }
 ??? }
 }
 
 
 Regards, Peter
 
 
  
  You guessed right: it's a one-shot object for a particular OrderedExecutor instance, and "order" must be called indeed at most once. 
  Regards, Hanson 
 On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com> wrote: 
  Hi Hanson,
 
 I don't think anything like that readily exists in java.lang.concurrent, but what you describe should be possible to achieve with composition of existing primitives. You haven't given any  additional hints to what your OrderedExecutor should behave like. Should it be a one-shot object (like CountDownLatch) or a re-usable one (like CyclicBarrier)? Will execCriticalSectionInOrder() for a particular OrderedExecutor instance and 'order' value be called at most once? If  yes (and I think that only a one-shot object makes sense here), an array of CountDownLatch(es) could be used:
 
 public class OrderedExecutor<T> {
 ??? private final CountDownLatch[] latches;
 
 ??? public OrderedExecutor(int n) {
 ??????? if (n < 1) throw new IllegalArgumentException("'n' should be >= 1");
 ??????? latches = new CountDownLatch[n - 1];
 ??????? for (int i = 0; i < latches.length; i++) {
 ??????????? latches[i] = new CountDownLatch(1);
 ??????? }
 ??? }
 
 ??? public T execCriticalSectionInOrder(final int order,
  ??????????????????????????????????????? final Supplier<T> criticalSection) throws InterruptedException {
 ??????? if (order < 0 || order > latches.length)
 ??????????? throw new IllegalArgumentException("'order' should be [0..." + latches.length + "]");
 ??????? if (order > 0) {
 ??????????? latches[order - 1].await();
 ??????? }
 ??????? try {
 ??????????? return criticalSection.get();
 ??????? } finally {
 ??????????? if (order < latches.length) {
 ??????????????? latches[order].countDown();
 ??????????? }
 ??????? }
 ??? }
 }
 
 
 Regards, Peter  
 
 On 12/14/2014 05:26 AM, Hanson Char wrote:
    
 Hi,I am looking for a construct that can be used to efficiently enforceordered execution of multiple critical sections, each calling from adifferent thread. The calling threads may run in parallel and may callthe execution method out of order. The perceived construct wouldtherefore be responsible for re-ordering the execution of thosethreads, so that their critical sections (and only the criticalsection) will be executed in order.Would something like the following API already exist?/** * Used to enforce ordered execution of critical sections calling from multiple * threads, parking and unparking the threads as necessary. */public class OrderedExecutor<T> { /** * Executes a critical section at most once with the given order, parking * and unparking the current thread as necessary so that all critical * sections executed by different threads using this executor take place in * the order from 1 to n consecutively. */ public T execCriticalSectionInOrder(final int order, final Callable<T> criticalSection) throws InterruptedException;}Regards,Hanson_______________________________________________Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest 
 
  
    
 
 
 
    
   
 
 
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141217/9e6e6067/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Dec 17 11:25:59 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 17 Dec 2014 16:25:59 +0000
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <843651815.119507.1418825235350.JavaMail.yahoo@jws10702g.mail.gq1.yahoo.com>
References: <548DC963.802@gmail.com>
	<843651815.119507.1418825235350.JavaMail.yahoo@jws10702g.mail.gq1.yahoo.com>
Message-ID: <5491AE97.4080800@oracle.com>

There is no guarantee you'll ever hand over the control to the right 
thread upon notify()

Alex

On 17/12/2014 14:07, suman shil wrote:
> Hi,
> Following is my solution to solve this problem. Please let me know if 
> I am missing something.
>
> public class OrderedExecutor
> {
> private int currentAllowedOrder = 0;
> private int maxLength = 0;
> public OrderedExecutor(int n)
> {
>     this.maxLength = n;
> }
> public synchronized Object execCriticalSectionInOrder(
>                               int order,
>                               Callable<Object> callable)
>                             throws Exception
> {
> if (order >= maxLength)
> {
> throw new Exception("Exceeds maximum order "+ maxLength);
> }
> while(order != currentAllowedOrder)
> {
> wait();
> }
> try
> {
> currentAllowedOrder = currentAllowedOrder+1;
> return callable.call();
> }
> finally
> {
> notify();
> }
> }
> }
>
> Regards
> Suman
>
> ------------------------------------------------------------------------
> *From:* Peter Levart <peter.levart at gmail.com>
> *To:* Hanson Char <hanson.char at gmail.com>
> *Cc:* concurrency-interest <concurrency-interest at cs.oswego.edu>
> *Sent:* Sunday, December 14, 2014 11:01 PM
> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of 
> critical sections?
>
>
> On 12/14/2014 06:11 PM, Hanson Char wrote:
>> Hi Peter,
>>
>> Thanks for this proposed idea of using LockSupport. This begs the 
>> question: which one would you choose if you had all three (correct) 
>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>
>> Regards,
>> Hanson
>
> The Semaphore/CountDownLatch variants are equivalent if you don't need 
> re-use. So any would do. They lack invalid-use detection. What happens 
> if they are not used as intended? Semaphore variant acts differently 
> than CountDownLatch variant. The low-level variant I proposed detects 
> invalid usage. So I would probably use this one. But the low level 
> variant is harder to reason about it's correctness. I think it is 
> correct, but you should show it to somebody else to confirm this.
>
> Another question is whether you actually need this kind of 
> synchronizer. Maybe if you explained what you are trying to achieve, 
> somebody could have an idea how to do that even more elegantly...
>
> Regards, Peter
>
>
>
>
>>
>> On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart <peter.levart at gmail.com 
>> <mailto:peter.levart at gmail.com>> wrote:
>>
>>     Hi Hanson,
>>
>>     This one is more low-level, but catches some invalid usages and
>>     is more resource-friendly:
>>
>>
>>     public class OrderedExecutor {
>>
>>         public <T> T execCriticalSectionInOrder(
>>             final int order,
>>             final Supplier<T> criticalSection
>>         ) throws InterruptedException {
>>             if (order < 0) {
>>                 throw new IllegalArgumentException("'order' should be
>>     >= 0");
>>             }
>>             if (order > 0) {
>>                 waitForDone(order - 1);
>>             }
>>             try {
>>                 return criticalSection.get();
>>             } finally {
>>                 notifyDone(order);
>>             }
>>         }
>>
>>         private static final Object DONE = new Object();
>>         private final ConcurrentMap<Integer, Object> signals = new
>>     ConcurrentHashMap<>();
>>
>>         private void waitForDone(int order) throws InterruptedException {
>>             Object sig = signals.putIfAbsent(order,
>>     Thread.currentThread());
>>             if (sig != null && sig != DONE) {
>>                 throw new IllegalStateException();
>>             }
>>             while (sig != DONE) {
>>                 LockSupport.park();
>>                 if (Thread.interrupted()) {
>>                     throw new InterruptedException();
>>                 }
>>                 sig = signals.get(order);
>>             }
>>         }
>>
>>         private void notifyDone(int order) {
>>             Object sig = signals.putIfAbsent(order, DONE);
>>             if (sig instanceof Thread) {
>>                 if (!signals.replace(order, sig, DONE)) {
>>                     throw new IllegalStateException();
>>                 }
>>                 LockSupport.unpark((Thread) sig);
>>             } else if (sig != null) {
>>                 throw new IllegalStateException();
>>             }
>>         }
>>     }
>>
>>
>>     Regards, Peter
>>
>>
>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>
>>>     On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>     Hi Peter,
>>>>
>>>>     Thanks for the suggestion, and sorry about not being clear
>>>>     about one important detail: "n" is not known a priori when
>>>>     constructing an OrderedExecutor.  Does this mean the use of
>>>>      CountDownLatch is ruled out?
>>>
>>>     If you know at least the upper bound of 'n', it can be used with
>>>     such 'n'. Otherwise something that dynamically re-sizes the
>>>     array could be devised. Or you could simply use a
>>>     ConcurrentHashMap instead of array where keys are 'order' values:
>>>
>>>
>>>     public class OrderedExecutor<T> {
>>>
>>>         private final ConcurrentMap<Integer, CountDownLatch> latches
>>>     = new ConcurrentHashMap<>();
>>>
>>>         public T execCriticalSectionInOrder(final int order,
>>>     final Supplier<T> criticalSection) throws InterruptedException {
>>>             if (order > 0) {
>>>     latches.computeIfAbsent(order - 1, o -> new
>>>     CountDownLatch(1)).await();
>>>             }
>>>             try {
>>>                 return criticalSection.get();
>>>             } finally {
>>>     latches.computeIfAbsent(order, o -> new
>>>     CountDownLatch(1)).countDown();
>>>             }
>>>         }
>>>     }
>>>
>>>
>>>     Regards, Peter
>>>
>>>>
>>>>     You guessed right: it's a one-shot object for a particular
>>>>     OrderedExecutor instance, and "order" must be called indeed at
>>>>     most once.
>>>>
>>>>     Regards,
>>>>     Hanson
>>>>
>>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart
>>>>     <peter.levart at gmail.com <mailto:peter.levart at gmail.com>> wrote:
>>>>
>>>>         Hi Hanson,
>>>>
>>>>         I don't think anything like that readily exists in
>>>>         java.lang.concurrent, but what you describe should be
>>>>         possible to achieve with composition of existing
>>>>         primitives. You haven't given any additional hints to what
>>>>         your OrderedExecutor should behave like. Should it be a
>>>>         one-shot object (like CountDownLatch) or a re-usable one
>>>>         (like CyclicBarrier)? Will execCriticalSectionInOrder() for
>>>>         a particular OrderedExecutor instance and 'order' value be
>>>>         called at most once? If yes (and I think that only a
>>>>         one-shot object makes sense here), an array of
>>>>         CountDownLatch(es) could be used:
>>>>
>>>>         public class OrderedExecutor<T> {
>>>>             private final CountDownLatch[] latches;
>>>>
>>>>             public OrderedExecutor(int n) {
>>>>                 if (n < 1) throw new IllegalArgumentException("'n'
>>>>         should be >= 1");
>>>>                 latches = new CountDownLatch[n - 1];
>>>>                 for (int i = 0; i < latches.length; i++) {
>>>>                     latches[i] = new CountDownLatch(1);
>>>>                 }
>>>>             }
>>>>
>>>>             public T execCriticalSectionInOrder(final int order,
>>>>         final Supplier<T> criticalSection) throws
>>>>         InterruptedException {
>>>>                 if (order < 0 || order > latches.length)
>>>>                     throw new IllegalArgumentException("'order'
>>>>         should be [0..." + latches.length + "]");
>>>>                 if (order > 0) {
>>>>         latches[order - 1].await();
>>>>                 }
>>>>                 try {
>>>>                     return criticalSection.get();
>>>>                 } finally {
>>>>                     if (order < latches.length) {
>>>>         latches[order].countDown();
>>>>                     }
>>>>                 }
>>>>             }
>>>>         }
>>>>
>>>>
>>>>         Regards, Peter
>>>>
>>>>
>>>>         On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>         Hi, I am looking for a construct that can be used to
>>>>>         efficiently enforce ordered execution of multiple critical
>>>>>         sections, each calling from a different thread. The
>>>>>         calling threads may run in parallel and may call the
>>>>>         execution method out of order. The perceived construct
>>>>>         would therefore be responsible for re-ordering the
>>>>>         execution of those threads, so that their critical
>>>>>         sections (and only the critical section) will be executed
>>>>>         in order. Would something like the following API already
>>>>>         exist? /** * Used to enforce ordered execution of critical
>>>>>         sections calling from multiple * threads, parking and
>>>>>         unparking the threads as necessary. */ public class
>>>>>         OrderedExecutor<T> { /** * Executes a critical section at
>>>>>         most once with the given order, parking * and unparking
>>>>>         the current thread as necessary so that all critical *
>>>>>         sections executed by different threads using this executor
>>>>>         take place in * the order from 1 to n consecutively. */
>>>>>         public T execCriticalSectionInOrder(
>>>>>         final int order, final Callable<T> criticalSection) throws
>>>>>         InterruptedException; } Regards, Hanson
>>>>>         _______________________________________________
>>>>>         Concurrency-interest mailing list
>>>>>         Concurrency-interest at cs.oswego.edu
>>>>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
>>>>
>>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141217/7a290cd9/attachment-0001.html>

From suman_krec at yahoo.com  Wed Dec 17 12:46:38 2014
From: suman_krec at yahoo.com (suman shil)
Date: Wed, 17 Dec 2014 17:46:38 +0000 (UTC)
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <5491AE97.4080800@oracle.com>
References: <5491AE97.4080800@oracle.com>
Message-ID: <1271391365.81967.1418838398375.JavaMail.yahoo@jws10738.mail.gq1.yahoo.com>

Thanks for your response. Will notifyAll() instead of notify() solve the problem?

RegardsSuman
      From: Oleksandr Otenko <oleksandr.otenko at oracle.com>
 To: suman shil <suman_krec at yahoo.com>; Concurrency-interest <concurrency-interest at cs.oswego.edu> 
 Sent: Wednesday, December 17, 2014 9:55 PM
 Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
   
 There is no guarantee you'll ever hand over the control to the right thread upon notify()
 
 Alex
 
 

On 17/12/2014 14:07, suman shil wrote:
  
  Hi, Following is my solution to solve this problem. Please let me know if I am missing something. 
  public class OrderedExecutor {  private int currentAllowedOrder = 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {  ? ? ? ? this.maxLength = n;  }      public synchronized Object execCriticalSectionInOrder(  ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? int order,  ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Callable<Object> callable)  ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? throws Exception  {  if (order >= maxLength)  {  throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order != currentAllowedOrder)  {  wait();  }    try  {  currentAllowedOrder = currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();  }  } } 
  Regards Suman 
      From: Peter Levart <peter.levart at gmail.com>
 To: Hanson Char <hanson.char at gmail.com> 
 Cc: concurrency-interest <concurrency-interest at cs.oswego.edu> 
 Sent: Sunday, December 14, 2014 11:01 PM
 Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
   
   
 On 12/14/2014 06:11 PM, Hanson Char wrote:
  
 Hi Peter, 
  Thanks for this proposed idea of using LockSupport. This begs the question: which one would you choose if you had all three (correct) implementation available? ?(Semaphore, CountDownLatch, or LockSupport)? 
  Regards, Hanson  
 
 The Semaphore/CountDownLatch variants are equivalent if you don't need re-use. So any would do. They lack invalid-use detection. What happens if they are not used as intended? Semaphore variant acts differently than CountDownLatch variant. The low-level variant I  proposed detects invalid usage. So I would probably use this one. But the low level variant is harder to reason about it's correctness. I think it is correct, but you should show it to somebody else to confirm this.
 
 Another question is whether you actually need this kind of synchronizer. Maybe if you explained what you are trying to achieve, somebody could have an idea how to do that even more elegantly...
 
 Regards, Peter 
 
  
 
 
 
 On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart <peter.levart at gmail.com> wrote: 
  Hi Hanson,
 
 This one is more low-level, but catches some invalid usages and is more resource-friendly:
 
 
 public class OrderedExecutor {
 
 ??? public <T> T execCriticalSectionInOrder(
 ??????? final int order,
 ??????? final Supplier<T> criticalSection
 ??? ) throws InterruptedException {
 ??????? if (order < 0) {
  ??????????? throw new IllegalArgumentException("'order' should be >= 0");
 ??????? }
 ??????? if (order > 0) {
 ??????????? waitForDone(order - 1);
 ??????? }
 ??????? try {
 ??????????? return criticalSection.get();
 ??????? } finally {
 ??????????? notifyDone(order);
 ??????? }
 ??? }
 
 ??? private static final Object DONE = new Object();
 ??? private final ConcurrentMap<Integer, Object> signals = new ConcurrentHashMap<>();
 
 ??? private void waitForDone(int order) throws InterruptedException {
 ??????? Object sig = signals.putIfAbsent(order, Thread.currentThread());
 ??????? if (sig != null && sig != DONE) {
 ??????????? throw new IllegalStateException();
 ??????? }
 ??????? while (sig != DONE) {
 ??????????? LockSupport.park();
 ??????????? if (Thread.interrupted()) {
 ??????????????? throw new InterruptedException();
 ??????????? }
 ??????????? sig = signals.get(order);
 ??????? }
 ??? }
 
 ??? private void notifyDone(int order) {
 ??????? Object sig = signals.putIfAbsent(order, DONE);
 ??????? if (sig instanceof Thread) {
 ??????????? if (!signals.replace(order, sig, DONE)) {
 ??????????????? throw new IllegalStateException();
 ??????????? }
 ??????????? LockSupport.unpark((Thread) sig);
 ??????? } else if (sig != null) {
 ??????????? throw new IllegalStateException();
 ??????? }
 ??? }
 }
 
 
 Regards, Peter  
 
 On 12/14/2014 05:08 PM, Peter Levart wrote:
  
 
 On 12/14/2014 04:20 PM, Hanson Char wrote:
  
 Hi Peter, 
  Thanks for the suggestion, and sorry about not being clear about one important  detail: "n" is not known a priori when constructing an OrderedExecutor.? Does this mean the use of ?CountDownLatch is ruled out?  
 
 If you know at least the upper bound of 'n', it can be used with such 'n'. Otherwise something that dynamically re-sizes the array could be devised. Or you could simply use  a ConcurrentHashMap instead of array where keys are 'order' values:
 
 
 public class OrderedExecutor<T> {
 
 ??? private final ConcurrentMap<Integer, CountDownLatch> latches = new ConcurrentHashMap<>();
 
 ??? public T execCriticalSectionInOrder(final int order,
 ??????????????????????????????????????? final Supplier<T> criticalSection) throws InterruptedException {
 ??????? if (order > 0) {
 ??????????? latches.computeIfAbsent(order - 1, o -> new CountDownLatch(1)).await();
 ??????? }
 ??????? try {
 ??????????? return criticalSection.get();
 ??????? } finally {
 ??????????? latches.computeIfAbsent(order, o -> new CountDownLatch(1)).countDown();
 ??????? }
 ??? }
 }
 
 
 Regards, Peter
 
 
  
  You guessed right: it's a one-shot object for a particular OrderedExecutor  instance, and "order" must be called indeed at most once. 
  Regards, Hanson 
 On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com> wrote: 
  Hi Hanson,
 
 I don't think anything like that readily exists  in java.lang.concurrent, but what you describe should be possible to  achieve with composition of existing primitives.  You haven't given any additional hints to what your OrderedExecutor  should behave like. Should it be a one-shot  object (like CountDownLatch) or a re-usable one (like  CyclicBarrier)? Will execCriticalSectionInOrder() for a particular OrderedExecutor instance and 'order' value be  called at most once? If yes (and I think that only a one-shot object  makes sense here), an array of CountDownLatch(es) could be used:
 
 public class OrderedExecutor<T> {
 ??? private final CountDownLatch[] latches;
 
 ??? public OrderedExecutor(int n) {
 ??????? if (n < 1) throw new IllegalArgumentException("'n'  should be >= 1");
 ??????? latches = new CountDownLatch[n - 1];
 ??????? for (int i = 0; i < latches.length; i++) {
 ??????????? latches[i] = new CountDownLatch(1);
 ??????? }
 ??? }
 
 ??? public T execCriticalSectionInOrder(final int order,
  ??????????????????????????????????????? final Supplier<T> criticalSection) throws InterruptedException {
 ??????? if (order < 0 || order > latches.length)
 ??????????? throw new IllegalArgumentException("'order' should be [0..." +  latches.length + "]");
 ??????? if (order > 0) {
 ??????????? latches[order - 1].await();
 ??????? }
 ??????? try {
 ??????????? return criticalSection.get();
 ??????? } finally {
 ??????????? if (order < latches.length) {
 ??????????????? latches[order].countDown();
 ??????????? }
 ??????? }
 ??? }
 }
 
 
 Regards, Peter  
 
 On 12/14/2014 05:26 AM, Hanson Char wrote:
    
  Hi, I am looking for a construct that can  be used to efficiently enforce  ordered execution of multiple critical sections, each calling from a  different thread. The calling threads may run in  parallel and may call the execution method out of order. The  perceived construct would therefore be  responsible for re-ordering the execution of those threads, so that their critical  sections (and only the critical section) will be executed in order. Would something  like the following API already exist? /** * Used to enforce ordered execution of  critical sections calling from multiple *  threads, parking and unparking the  threads as necessary. */ public class  OrderedExecutor<T> { /** * Executes a critical section  at most once with the given order, parking * and  unparking the current thread as  necessary so that all critical * sections executed  by different threads using this  executor take place in * the order from 1 to n  consecutively. */ public T execCriticalSectionInOrder(  final int order, final Callable<T> criticalSection) throws InterruptedException; } Regards, Hanson _______________________________________________Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
 
  
    
 
 
 
    
   
 
    
 _______________________________________________
 Concurrency-interest mailing list
 Concurrency-interest at cs.oswego.edu
 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
  
 
     
  
 _______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 
 
 

  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141217/badf6e97/attachment-0001.html>

From peter.levart at gmail.com  Wed Dec 17 13:24:21 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 17 Dec 2014 19:24:21 +0100
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <1271391365.81967.1418838398375.JavaMail.yahoo@jws10738.mail.gq1.yahoo.com>
References: <5491AE97.4080800@oracle.com>
	<1271391365.81967.1418838398375.JavaMail.yahoo@jws10738.mail.gq1.yahoo.com>
Message-ID: <5491CA55.5000705@gmail.com>

On 12/17/2014 06:46 PM, suman shil wrote:
> Thanks for your response. Will notifyAll() instead of notify() solve the problem?

It will, but you should also account for "spurious" wake-ups. You should 
increment currentAllowedOrder only after return from callable.call (in 
finally block just before notifyAll()).

Otherwise a nice solution - with minimal state, providing that not many 
threads meet at the same time...

Regards, Peter

>
> RegardsSuman
>        From: Oleksandr Otenko <oleksandr.otenko at oracle.com>
>   To: suman shil <suman_krec at yahoo.com>; Concurrency-interest <concurrency-interest at cs.oswego.edu>
>   Sent: Wednesday, December 17, 2014 9:55 PM
>   Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
>     
>   There is no guarantee you'll ever hand over the control to the right thread upon notify()
>   
>   Alex
>   
>   
>
> On 17/12/2014 14:07, suman shil wrote:
>    
>    Hi, Following is my solution to solve this problem. Please let me know if I am missing something.
>    public class OrderedExecutor {  private int currentAllowedOrder = 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {          this.maxLength = n;  }      public synchronized Object execCriticalSectionInOrder(                                   int order,                                   Callable<Object> callable)                                 throws Exception  {  if (order >= maxLength)  {  throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order != currentAllowedOrder)  {  wait();  }    try  {  currentAllowedOrder = currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();  }  } }
>    Regards Suman
>        From: Peter Levart <peter.levart at gmail.com>
>   To: Hanson Char <hanson.char at gmail.com>
>   Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>
>   Sent: Sunday, December 14, 2014 11:01 PM
>   Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
>     
>     
>   On 12/14/2014 06:11 PM, Hanson Char wrote:
>    
>   Hi Peter,
>    Thanks for this proposed idea of using LockSupport. This begs the question: which one would you choose if you had all three (correct) implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>    Regards, Hanson
>   
>   The Semaphore/CountDownLatch variants are equivalent if you don't need re-use. So any would do. They lack invalid-use detection. What happens if they are not used as intended? Semaphore variant acts differently than CountDownLatch variant. The low-level variant I  proposed detects invalid usage. So I would probably use this one. But the low level variant is harder to reason about it's correctness. I think it is correct, but you should show it to somebody else to confirm this.
>   
>   Another question is whether you actually need this kind of synchronizer. Maybe if you explained what you are trying to achieve, somebody could have an idea how to do that even more elegantly...
>   
>   Regards, Peter
>   
>    
>   
>   
>   
>   On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart <peter.levart at gmail.com> wrote:
>    Hi Hanson,
>   
>   This one is more low-level, but catches some invalid usages and is more resource-friendly:
>   
>   
>   public class OrderedExecutor {
>   
>       public <T> T execCriticalSectionInOrder(
>           final int order,
>           final Supplier<T> criticalSection
>       ) throws InterruptedException {
>           if (order < 0) {
>                throw new IllegalArgumentException("'order' should be >= 0");
>           }
>           if (order > 0) {
>               waitForDone(order - 1);
>           }
>           try {
>               return criticalSection.get();
>           } finally {
>               notifyDone(order);
>           }
>       }
>   
>       private static final Object DONE = new Object();
>       private final ConcurrentMap<Integer, Object> signals = new ConcurrentHashMap<>();
>   
>       private void waitForDone(int order) throws InterruptedException {
>           Object sig = signals.putIfAbsent(order, Thread.currentThread());
>           if (sig != null && sig != DONE) {
>               throw new IllegalStateException();
>           }
>           while (sig != DONE) {
>               LockSupport.park();
>               if (Thread.interrupted()) {
>                   throw new InterruptedException();
>               }
>               sig = signals.get(order);
>           }
>       }
>   
>       private void notifyDone(int order) {
>           Object sig = signals.putIfAbsent(order, DONE);
>           if (sig instanceof Thread) {
>               if (!signals.replace(order, sig, DONE)) {
>                   throw new IllegalStateException();
>               }
>               LockSupport.unpark((Thread) sig);
>           } else if (sig != null) {
>               throw new IllegalStateException();
>           }
>       }
>   }
>   
>   
>   Regards, Peter
>   
>   On 12/14/2014 05:08 PM, Peter Levart wrote:
>    
>   
>   On 12/14/2014 04:20 PM, Hanson Char wrote:
>    
>   Hi Peter,
>    Thanks for the suggestion, and sorry about not being clear about one important  detail: "n" is not known a priori when constructing an OrderedExecutor.  Does this mean the use of  CountDownLatch is ruled out?
>   
>   If you know at least the upper bound of 'n', it can be used with such 'n'. Otherwise something that dynamically re-sizes the array could be devised. Or you could simply use  a ConcurrentHashMap instead of array where keys are 'order' values:
>   
>   
>   public class OrderedExecutor<T> {
>   
>       private final ConcurrentMap<Integer, CountDownLatch> latches = new ConcurrentHashMap<>();
>   
>       public T execCriticalSectionInOrder(final int order,
>                                           final Supplier<T> criticalSection) throws InterruptedException {
>           if (order > 0) {
>               latches.computeIfAbsent(order - 1, o -> new CountDownLatch(1)).await();
>           }
>           try {
>               return criticalSection.get();
>           } finally {
>               latches.computeIfAbsent(order, o -> new CountDownLatch(1)).countDown();
>           }
>       }
>   }
>   
>   
>   Regards, Peter
>   
>   
>    
>    You guessed right: it's a one-shot object for a particular OrderedExecutor  instance, and "order" must be called indeed at most once.
>    Regards, Hanson
>   On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com> wrote:
>    Hi Hanson,
>   
>   I don't think anything like that readily exists  in java.lang.concurrent, but what you describe should be possible to  achieve with composition of existing primitives.  You haven't given any additional hints to what your OrderedExecutor  should behave like. Should it be a one-shot  object (like CountDownLatch) or a re-usable one (like  CyclicBarrier)? Will execCriticalSectionInOrder() for a particular OrderedExecutor instance and 'order' value be  called at most once? If yes (and I think that only a one-shot object  makes sense here), an array of CountDownLatch(es) could be used:
>   
>   public class OrderedExecutor<T> {
>       private final CountDownLatch[] latches;
>   
>       public OrderedExecutor(int n) {
>           if (n < 1) throw new IllegalArgumentException("'n'  should be >= 1");
>           latches = new CountDownLatch[n - 1];
>           for (int i = 0; i < latches.length; i++) {
>               latches[i] = new CountDownLatch(1);
>           }
>       }
>   
>       public T execCriticalSectionInOrder(final int order,
>                                            final Supplier<T> criticalSection) throws InterruptedException {
>           if (order < 0 || order > latches.length)
>               throw new IllegalArgumentException("'order' should be [0..." +  latches.length + "]");
>           if (order > 0) {
>               latches[order - 1].await();
>           }
>           try {
>               return criticalSection.get();
>           } finally {
>               if (order < latches.length) {
>                   latches[order].countDown();
>               }
>           }
>       }
>   }
>   
>   
>   Regards, Peter
>   
>   On 12/14/2014 05:26 AM, Hanson Char wrote:
>      
>    Hi, I am looking for a construct that can  be used to efficiently enforce  ordered execution of multiple critical sections, each calling from a  different thread. The calling threads may run in  parallel and may call the execution method out of order. The  perceived construct would therefore be  responsible for re-ordering the execution of those threads, so that their critical  sections (and only the critical section) will be executed in order. Would something  like the following API already exist? /** * Used to enforce ordered execution of  critical sections calling from multiple *  threads, parking and unparking the  threads as necessary. */ public class  OrderedExecutor<T> { /** * Executes a critical section  at most once with the given order, parking * and  unparking the current thread as  necessary so that all critical * sections executed  by different threads using this  executor take place in * the order from 1 to n  consecutively. */ public T execCriticalSectionInOrder(  final int order, final Callable<T> criticalSection) throws InterruptedException; } Regards, Hanson _______________________________________________Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   
>    
>      
>   
>   
>   
>      
>     
>   
>      
>   _______________________________________________
>   Concurrency-interest mailing list
>   Concurrency-interest at cs.oswego.edu
>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>    
>   
>       
>    
>   _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   
>   
>   
>
>    
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141217/afec0cc8/attachment.html>

From suman_krec at yahoo.com  Wed Dec 17 13:36:45 2014
From: suman_krec at yahoo.com (suman shil)
Date: Wed, 17 Dec 2014 18:36:45 +0000 (UTC)
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <5491CA55.5000705@gmail.com>
References: <5491CA55.5000705@gmail.com>
Message-ID: <1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>

Thanks peter for your reply. You are right. I should have incremented?currentAllowedOrder? in finally block.
Suman      From: Peter Levart <peter.levart at gmail.com>
 To: suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <oleksandr.otenko at oracle.com>; Concurrency-interest <concurrency-interest at cs.oswego.edu> 
 Sent: Wednesday, December 17, 2014 11:54 PM
 Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
   
 On 12/17/2014 06:46 PM, suman shil wrote:
  
 Thanks for your response. Will notifyAll() instead of notify() solve the problem? 
 
 It will, but you should also account for "spurious" wake-ups. You should increment currentAllowedOrder only after return from callable.call (in finally block just before notifyAll()).
 
 Otherwise a nice solution - with minimal state, providing that not many threads meet at the same time...
 
 Regards, Peter
 
 
 RegardsSuman
      From: Oleksandr Otenko <oleksandr.otenko at oracle.com>
 To: suman shil <suman_krec at yahoo.com>; Concurrency-interest <concurrency-interest at cs.oswego.edu> 
 Sent: Wednesday, December 17, 2014 9:55 PM
 Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
   
 There is no guarantee you'll ever hand over the control to the right thread upon notify()
 
 Alex
 
 

On 17/12/2014 14:07, suman shil wrote:
  
  Hi, Following is my solution to solve this problem. Please let me know if I am missing something. 
  public class OrderedExecutor {  private int currentAllowedOrder = 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {  ? ? ? ? this.maxLength = n;  }      public synchronized Object execCriticalSectionInOrder(  ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? int order,  ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Callable<Object> callable)  ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? throws Exception  {  if (order >= maxLength)  {  throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order != currentAllowedOrder)  {  wait();  }    try  {  currentAllowedOrder = currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();  }  } } 
  Regards Suman 
      From: Peter Levart <peter.levart at gmail.com>
 To: Hanson Char <hanson.char at gmail.com> 
 Cc: concurrency-interest <concurrency-interest at cs.oswego.edu> 
 Sent: Sunday, December 14, 2014 11:01 PM
 Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
   
   
 On 12/14/2014 06:11 PM, Hanson Char wrote:
  
 Hi Peter, 
  Thanks for this proposed idea of using LockSupport. This begs the question: which one would you choose if you had all three (correct) implementation available? ?(Semaphore, CountDownLatch, or LockSupport)? 
  Regards, Hanson  
 
 The Semaphore/CountDownLatch variants are equivalent if you don't need re-use. So any would do. They lack invalid-use detection. What happens if they are not used as intended? Semaphore variant acts differently than CountDownLatch variant. The low-level variant I  proposed detects invalid usage. So I would probably use this one. But the low level variant is harder to reason about it's correctness. I think it is correct, but you should show it to somebody else to confirm this.
 
 Another question is whether you actually need this kind of synchronizer. Maybe if you explained what you are trying to achieve, somebody could have an idea how to do that even more elegantly...
 
 Regards, Peter 
 
  
 
 
 
 On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart <peter.levart at gmail.com> wrote: 
  Hi Hanson,
 
 This one is more low-level, but catches some invalid usages and is more resource-friendly:
 
 
 public class OrderedExecutor {
 
 ??? public <T> T execCriticalSectionInOrder(
 ??????? final int order,
 ??????? final Supplier<T> criticalSection
 ??? ) throws InterruptedException {
 ??????? if (order < 0) {
  ??????????? throw new IllegalArgumentException("'order' should be >= 0");
 ??????? }
 ??????? if (order > 0) {
 ??????????? waitForDone(order - 1);
 ??????? }
 ??????? try {
 ??????????? return criticalSection.get();
 ??????? } finally {
 ??????????? notifyDone(order);
 ??????? }
 ??? }
 
 ??? private static final Object DONE = new Object();
 ??? private final ConcurrentMap<Integer, Object> signals = new ConcurrentHashMap<>();
 
 ??? private void waitForDone(int order) throws InterruptedException {
 ??????? Object sig = signals.putIfAbsent(order, Thread.currentThread());
 ??????? if (sig != null && sig != DONE) {
 ??????????? throw new IllegalStateException();
 ??????? }
 ??????? while (sig != DONE) {
 ??????????? LockSupport.park();
 ??????????? if (Thread.interrupted()) {
 ??????????????? throw new InterruptedException();
 ??????????? }
 ??????????? sig = signals.get(order);
 ??????? }
 ??? }
 
 ??? private void notifyDone(int order) {
 ??????? Object sig = signals.putIfAbsent(order, DONE);
 ??????? if (sig instanceof Thread) {
 ??????????? if (!signals.replace(order, sig, DONE)) {
 ??????????????? throw new IllegalStateException();
 ??????????? }
 ??????????? LockSupport.unpark((Thread) sig);
 ??????? } else if (sig != null) {
 ??????????? throw new IllegalStateException();
 ??????? }
 ??? }
 }
 
 
 Regards, Peter  
 
 On 12/14/2014 05:08 PM, Peter Levart wrote:
  
 
 On 12/14/2014 04:20 PM, Hanson Char wrote:
  
 Hi Peter, 
  Thanks for the suggestion, and sorry about not being clear about one important  detail: "n" is not known a priori when constructing an OrderedExecutor.? Does this mean the use of ?CountDownLatch is ruled out?  
 
 If you know at least the upper bound of 'n', it can be used with such 'n'. Otherwise something that dynamically re-sizes the array could be devised. Or you could simply use  a ConcurrentHashMap instead of array where keys are 'order' values:
 
 
 public class OrderedExecutor<T> {
 
 ??? private final ConcurrentMap<Integer, CountDownLatch> latches = new ConcurrentHashMap<>();
 
 ??? public T execCriticalSectionInOrder(final int order,
 ??????????????????????????????????????? final Supplier<T> criticalSection) throws InterruptedException {
 ??????? if (order > 0) {
 ??????????? latches.computeIfAbsent(order - 1, o -> new CountDownLatch(1)).await();
 ??????? }
 ??????? try {
 ??????????? return criticalSection.get();
 ??????? } finally {
 ??????????? latches.computeIfAbsent(order, o -> new CountDownLatch(1)).countDown();
 ??????? }
 ??? }
 }
 
 
 Regards, Peter
 
 
  
  You guessed right: it's a one-shot object for a particular OrderedExecutor  instance, and "order" must be called indeed at most once. 
  Regards, Hanson 
 On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart <peter.levart at gmail.com> wrote: 
  Hi Hanson,
 
 I don't think anything like that readily exists  in java.lang.concurrent, but what you describe should be possible to  achieve with composition of existing primitives.  You haven't given any additional hints to what your OrderedExecutor  should behave like. Should it be a one-shot  object (like CountDownLatch) or a re-usable one (like  CyclicBarrier)? Will execCriticalSectionInOrder() for a particular OrderedExecutor instance and 'order' value be  called at most once? If yes (and I think that only a one-shot object  makes sense here), an array of CountDownLatch(es) could be used:
 
 public class OrderedExecutor<T> {
 ??? private final CountDownLatch[] latches;
 
 ??? public OrderedExecutor(int n) {
 ??????? if (n < 1) throw new IllegalArgumentException("'n'  should be >= 1");
 ??????? latches = new CountDownLatch[n - 1];
 ??????? for (int i = 0; i < latches.length; i++) {
 ??????????? latches[i] = new CountDownLatch(1);
 ??????? }
 ??? }
 
 ??? public T execCriticalSectionInOrder(final int order,
  ??????????????????????????????????????? final Supplier<T> criticalSection) throws InterruptedException {
 ??????? if (order < 0 || order > latches.length)
 ??????????? throw new IllegalArgumentException("'order' should be [0..." +  latches.length + "]");
 ??????? if (order > 0) {
 ??????????? latches[order - 1].await();
 ??????? }
 ??????? try {
 ??????????? return criticalSection.get();
 ??????? } finally {
 ??????????? if (order < latches.length) {
 ??????????????? latches[order].countDown();
 ??????????? }
 ??????? }
 ??? }
 }
 
 
 Regards, Peter  
 
 On 12/14/2014 05:26 AM, Hanson Char wrote:
    
  Hi, I am looking for a construct that can  be used to efficiently enforce  ordered execution of multiple critical sections, each calling from a  different thread. The calling threads may run in  parallel and may call the execution method out of order. The  perceived construct would therefore be  responsible for re-ordering the execution of those threads, so that their critical  sections (and only the critical section) will be executed in order. Would something  like the following API already exist? /** * Used to enforce ordered execution of  critical sections calling from multiple *  threads, parking and unparking the  threads as necessary. */ public class  OrderedExecutor<T> { /** * Executes a critical section  at most once with the given order, parking * and  unparking the current thread as  necessary so that all critical * sections executed  by different threads using this  executor take place in * the order from 1 to n  consecutively. */ public T execCriticalSectionInOrder
(  final int order, final Callable<T> criticalSection) throws InterruptedException; } Regards, Hanson _______________________________________________Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
 
  
    
 
 
 
    
   
 
    
 _______________________________________________
 Concurrency-interest mailing list
 Concurrency-interest at cs.oswego.edu
 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
  
 
     
  
 _______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 
 
 

  
 


  
 _______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 
 
 

  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141217/0b41fde6/attachment-0001.html>

From martinrb at google.com  Wed Dec 17 13:54:26 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 17 Dec 2014 10:54:26 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <54914CBE.2000201@gmail.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<5481108A.3030605@oracle.com>
	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>
	<5488B4EF.6060103@redhat.com> <5488EB09.9000501@oracle.com>
	<54895EFB.3060204@redhat.com>
	<CA+kOe0_QOXi=mvLdXvYn4EF4J54HKDJVo=S4VBDUjujtpzgYzw@mail.gmail.com>
	<5490EA49.2050406@oracle.com> <54914CBE.2000201@gmail.com>
Message-ID: <CA+kOe0-+78XNH84MMXvxUex6kRu6JhENd2WVGXbSktUj_MGd8w@mail.gmail.com>

On Wed, Dec 17, 2014 at 1:28 AM, Peter Levart <peter.levart at gmail.com> wrote:
> On 12/17/2014 03:28 AM, David Holmes wrote:
>>
>> On 17/12/2014 10:06 AM, Martin Buchholz wrote:
>> Hans allows for the nonsensical, in my view, possibility that the load of
>> x.a can happen after the x_init=true store and yet somehow be subject to the
>> ++ and the ensuing store that has to come before the x_init = true.
>
> Perhaps, he is speaking about why it is dangerous to replace BOTH release
> with just store-store AND acquire with just load-load?

I'm pretty sure he's talking about weakening EITHER.

"""Clearly, and unsurprisingly, it is unsafe to replace the
load_acquire with a version that restricts only load ordering in this
case. That would allow the store to x in thread 2 to become visible
before the initialization of x by thread 1 is complete, possibly
losing the update, or corrupting the state of x during initialization.

More interestingly, it is also generally unsafe to restrict the
release ordering constraint in thread 1 to only stores."""

(What's "clear and unsurprising" to Hans may not be to the rest of us)

From oleksandr.otenko at oracle.com  Wed Dec 17 14:15:36 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 17 Dec 2014 19:15:36 +0000
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
Message-ID: <5491D658.5060604@oracle.com>

No, there is no difference. Peter didn't spot your entire method is 
synchronized, so spurious wakeup won't make progress until the owner of 
the lock exits the method.

You could split the synchronization into two blocks - one encompassing 
the wait loop, the other in the finally block; but it may make no 
difference.

Alex

On 17/12/2014 18:36, suman shil wrote:
> Thanks peter for your reply. You are right. I should have incremented 
> currentAllowedOrder in finally block.
>
> Suman
> ------------------------------------------------------------------------
> *From:* Peter Levart <peter.levart at gmail.com>
> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko 
> <oleksandr.otenko at oracle.com>; Concurrency-interest 
> <concurrency-interest at cs.oswego.edu>
> *Sent:* Wednesday, December 17, 2014 11:54 PM
> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of 
> critical sections?
>
> On 12/17/2014 06:46 PM, suman shil wrote:
>> Thanks for your response. Will notifyAll() instead of notify() solve the problem?
>
> It will, but you should also account for "spurious" wake-ups. You 
> should increment currentAllowedOrder only after return from 
> callable.call (in finally block just before notifyAll()).
>
> Otherwise a nice solution - with minimal state, providing that not 
> many threads meet at the same time...
>
> Regards, Peter
>
>> RegardsSuman
>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com>  <mailto:oleksandr.otenko at oracle.com>
>>   To: suman shil<suman_krec at yahoo.com>  <mailto:suman_krec at yahoo.com>; Concurrency-interest<concurrency-interest at cs.oswego.edu>  <mailto:concurrency-interest at cs.oswego.edu>  
>>   Sent: Wednesday, December 17, 2014 9:55 PM
>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
>>     
>>   There is no guarantee you'll ever hand over the control to the right thread upon notify()
>>   
>>   Alex
>>   
>>   
>>
>> On 17/12/2014 14:07, suman shil wrote:
>>    
>>    Hi, Following is my solution to solve this problem. Please let me know if I am missing something.
>>    public class OrderedExecutor {  private int currentAllowedOrder = 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {          this.maxLength = n;  }      public synchronized Object execCriticalSectionInOrder(                                   int order,                                   Callable<Object> callable)                                 throws Exception  {  if (order >= maxLength)  {  throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order != currentAllowedOrder)  {  wait();  }    try  {  currentAllowedOrder = currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();  }  } }
>>    Regards Suman
>>        From: Peter Levart<peter.levart at gmail.com>  <mailto:peter.levart at gmail.com>
>>   To: Hanson Char<hanson.char at gmail.com>  <mailto:hanson.char at gmail.com>  
>>   Cc: concurrency-interest<concurrency-interest at cs.oswego.edu>  <mailto:concurrency-interest at cs.oswego.edu>  
>>   Sent: Sunday, December 14, 2014 11:01 PM
>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
>>     
>>     
>>   On 12/14/2014 06:11 PM, Hanson Char wrote:
>>    
>>   Hi Peter,
>>    Thanks for this proposed idea of using LockSupport. This begs the question: which one would you choose if you had all three (correct) implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>    Regards, Hanson
>>   
>>   The Semaphore/CountDownLatch variants are equivalent if you don't need re-use. So any would do. They lack invalid-use detection. What happens if they are not used as intended? Semaphore variant acts differently than CountDownLatch variant. The low-level variant I  proposed detects invalid usage. So I would probably use this one. But the low level variant is harder to reason about it's correctness. I think it is correct, but you should show it to somebody else to confirm this.
>>   
>>   Another question is whether you actually need this kind of synchronizer. Maybe if you explained what you are trying to achieve, somebody could have an idea how to do that even more elegantly...
>>   
>>   Regards, Peter
>>   
>>    
>>   
>>   
>>   
>>   On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<peter.levart at gmail.com>  <mailto:peter.levart at gmail.com>  wrote:
>>    Hi Hanson,
>>   
>>   This one is more low-level, but catches some invalid usages and is more resource-friendly:
>>   
>>   
>>   public class OrderedExecutor {
>>   
>>       public <T> T execCriticalSectionInOrder(
>>           final int order,
>>           final Supplier<T> criticalSection
>>       ) throws InterruptedException {
>>           if (order < 0) {
>>                throw new IllegalArgumentException("'order' should be >= 0");
>>           }
>>           if (order > 0) {
>>               waitForDone(order - 1);
>>           }
>>           try {
>>               return criticalSection.get();
>>           } finally {
>>               notifyDone(order);
>>           }
>>       }
>>   
>>       private static final Object DONE = new Object();
>>       private final ConcurrentMap<Integer, Object> signals = new ConcurrentHashMap<>();
>>   
>>       private void waitForDone(int order) throws InterruptedException {
>>           Object sig = signals.putIfAbsent(order, Thread.currentThread());
>>           if (sig != null && sig != DONE) {
>>               throw new IllegalStateException();
>>           }
>>           while (sig != DONE) {
>>               LockSupport.park();
>>               if (Thread.interrupted()) {
>>                   throw new InterruptedException();
>>               }
>>               sig = signals.get(order);
>>           }
>>       }
>>   
>>       private void notifyDone(int order) {
>>           Object sig = signals.putIfAbsent(order, DONE);
>>           if (sig instanceof Thread) {
>>               if (!signals.replace(order, sig, DONE)) {
>>                   throw new IllegalStateException();
>>               }
>>               LockSupport.unpark((Thread) sig);
>>           } else if (sig != null) {
>>               throw new IllegalStateException();
>>           }
>>       }
>>   }
>>   
>>   
>>   Regards, Peter
>>   
>>   On 12/14/2014 05:08 PM, Peter Levart wrote:
>>    
>>   
>>   On 12/14/2014 04:20 PM, Hanson Char wrote:
>>    
>>   Hi Peter,
>>    Thanks for the suggestion, and sorry about not being clear about one important  detail: "n" is not known a priori when constructing an OrderedExecutor.  Does this mean the use of  CountDownLatch is ruled out?
>>   
>>   If you know at least the upper bound of 'n', it can be used with such 'n'. Otherwise something that dynamically re-sizes the array could be devised. Or you could simply use  a ConcurrentHashMap instead of array where keys are 'order' values:
>>   
>>   
>>   public class OrderedExecutor<T> {
>>   
>>       private final ConcurrentMap<Integer, CountDownLatch> latches = new ConcurrentHashMap<>();
>>   
>>       public T execCriticalSectionInOrder(final int order,
>>                                           final Supplier<T> criticalSection) throws InterruptedException {
>>           if (order > 0) {
>>               latches.computeIfAbsent(order - 1, o -> new CountDownLatch(1)).await();
>>           }
>>           try {
>>               return criticalSection.get();
>>           } finally {
>>               latches.computeIfAbsent(order, o -> new CountDownLatch(1)).countDown();
>>           }
>>       }
>>   }
>>   
>>   
>>   Regards, Peter
>>   
>>   
>>    
>>    You guessed right: it's a one-shot object for a particular OrderedExecutor  instance, and "order" must be called indeed at most once.
>>    Regards, Hanson
>>   On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<peter.levart at gmail.com>  <mailto:peter.levart at gmail.com>  wrote:
>>    Hi Hanson,
>>   
>>   I don't think anything like that readily exists  in java.lang.concurrent, but what you describe should be possible to  achieve with composition of existing primitives.  You haven't given any additional hints to what your OrderedExecutor  should behave like. Should it be a one-shot  object (like CountDownLatch) or a re-usable one (like  CyclicBarrier)? Will execCriticalSectionInOrder() for a particular OrderedExecutor instance and 'order' value be  called at most once? If yes (and I think that only a one-shot object  makes sense here), an array of CountDownLatch(es) could be used:
>>   
>>   public class OrderedExecutor<T> {
>>       private final CountDownLatch[] latches;
>>   
>>       public OrderedExecutor(int n) {
>>           if (n < 1) throw new IllegalArgumentException("'n'  should be >= 1");
>>           latches = new CountDownLatch[n - 1];
>>           for (int i = 0; i < latches.length; i++) {
>>               latches[i] = new CountDownLatch(1);
>>           }
>>       }
>>   
>>       public T execCriticalSectionInOrder(final int order,
>>                                            final Supplier<T> criticalSection) throws InterruptedException {
>>           if (order < 0 || order > latches.length)
>>               throw new IllegalArgumentException("'order' should be [0..." +  latches.length + "]");
>>           if (order > 0) {
>>               latches[order - 1].await();
>>           }
>>           try {
>>               return criticalSection.get();
>>           } finally {
>>               if (order < latches.length) {
>>                   latches[order].countDown();
>>               }
>>           }
>>       }
>>   }
>>   
>>   
>>   Regards, Peter
>>   
>>   On 12/14/2014 05:26 AM, Hanson Char wrote:
>>      
>>    Hi, I am looking for a construct that can  be used to efficiently enforce  ordered execution of multiple critical sections, each calling from a  different thread. The calling threads may run in  parallel and may call the execution method out of order. The  perceived construct would therefore be  responsible for re-ordering the execution of those threads, so that their critical  sections (and only the critical section) will be executed in order. Would something  like the following API already exist? /** * Used to enforce ordered execution of  critical sections calling from multiple *  threads, parking and unparking the  threads as necessary. */ public class  OrderedExecutor<T> { /** * Executes a critical section  at most once with the given order, parking * and  unparking the current thread as  necessary so that all critical * sections executed  by different threads using this  executor take place in * the order from 1 to n  consecutively. */ public T execCriticalSectionInOrder
>> (  final int order, final Callable<T> criticalSection) throws InterruptedException; } Regards, Hanson _______________________________________________Concurrency-interest mailing listConcurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>  http://cs.oswego.edu/mailman/listinfo/concurrency-interest  
>>   
>>    
>>      
>>   
>>   
>>   
>>      
>>     
>>   
>>      
>>   _______________________________________________
>>   Concurrency-interest mailing list
>>   Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>    
>>   
>>       
>>    
>>   _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>   
>>   
>>   
>>
>>    
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141217/db2dbb39/attachment-0001.html>

From peter.levart at gmail.com  Thu Dec 18 02:43:44 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 18 Dec 2014 08:43:44 +0100
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <5491D658.5060604@oracle.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
	<5491D658.5060604@oracle.com>
Message-ID: <549285B0.3070801@gmail.com>

On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
> No, there is no difference. Peter didn't spot your entire method is 
> synchronized, so spurious wakeup won't make progress until the owner 
> of the lock exits the method.
>
> You could split the synchronization into two blocks - one encompassing 
> the wait loop, the other in the finally block; but it may make no 
> difference.
>
> Alex

You're right, Alex. I'm so infected with park/unpark virus that I missed 
that ;-)

Peter

>
> On 17/12/2014 18:36, suman shil wrote:
>> Thanks peter for your reply. You are right. I should have incremented 
>> currentAllowedOrder in finally block.
>>
>> Suman
>> ------------------------------------------------------------------------
>> *From:* Peter Levart <peter.levart at gmail.com>
>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko 
>> <oleksandr.otenko at oracle.com>; Concurrency-interest 
>> <concurrency-interest at cs.oswego.edu>
>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of 
>> critical sections?
>>
>> On 12/17/2014 06:46 PM, suman shil wrote:
>>> Thanks for your response. Will notifyAll() instead of notify() solve 
>>> the problem?
>>
>> It will, but you should also account for "spurious" wake-ups. You 
>> should increment currentAllowedOrder only after return from 
>> callable.call (in finally block just before notifyAll()).
>>
>> Otherwise a nice solution - with minimal state, providing that not 
>> many threads meet at the same time...
>>
>> Regards, Peter
>>
>>> RegardsSuman
>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> 
>>> <mailto:oleksandr.otenko at oracle.com>
>>>   To: suman shil<suman_krec at yahoo.com> 
>>> <mailto:suman_krec at yahoo.com>; 
>>> Concurrency-interest<concurrency-interest at cs.oswego.edu> 
>>> <mailto:concurrency-interest at cs.oswego.edu>    Sent: Wednesday, 
>>> December 17, 2014 9:55 PM
>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of 
>>> critical sections?
>>>       There is no guarantee you'll ever hand over the control to the 
>>> right thread upon notify()
>>>     Alex
>>>
>>> On 17/12/2014 14:07, suman shil wrote:
>>>       Hi, Following is my solution to solve this problem. Please let 
>>> me know if I am missing something.
>>>    public class OrderedExecutor {  private int currentAllowedOrder = 
>>> 0;  private int maxLength = 0;  public OrderedExecutor(int n)  
>>> {          this.maxLength = n;  } public synchronized Object 
>>> execCriticalSectionInOrder( int order, Callable<Object> 
>>> callable)                                 throws Exception  { if 
>>> (order >= maxLength)  {  throw new Exception("Exceeds maximum order 
>>> "+ maxLength);  }    while(order != currentAllowedOrder)  {  
>>> wait();  }    try  { currentAllowedOrder = currentAllowedOrder+1;  
>>> return callable.call();  }  finally  {  notify();  }  } }
>>>    Regards Suman
>>>        From: Peter Levart<peter.levart at gmail.com> 
>>> <mailto:peter.levart at gmail.com>
>>>   To: Hanson Char<hanson.char at gmail.com> 
>>> <mailto:hanson.char at gmail.com>    Cc: 
>>> concurrency-interest<concurrency-interest at cs.oswego.edu> 
>>> <mailto:concurrency-interest at cs.oswego.edu>    Sent: Sunday, 
>>> December 14, 2014 11:01 PM
>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of 
>>> critical sections?
>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>      Hi Peter,
>>>    Thanks for this proposed idea of using LockSupport. This begs the 
>>> question: which one would you choose if you had all three (correct) 
>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>    Regards, Hanson
>>>     The Semaphore/CountDownLatch variants are equivalent if you 
>>> don't need re-use. So any would do. They lack invalid-use detection. 
>>> What happens if they are not used as intended? Semaphore variant 
>>> acts differently than CountDownLatch variant. The low-level variant 
>>> I  proposed detects invalid usage. So I would probably use this one. 
>>> But the low level variant is harder to reason about it's 
>>> correctness. I think it is correct, but you should show it to 
>>> somebody else to confirm this.
>>>     Another question is whether you actually need this kind of 
>>> synchronizer. Maybe if you explained what you are trying to achieve, 
>>> somebody could have an idea how to do that even more elegantly...
>>>     Regards, Peter
>>>              On Sun, Dec 14, 2014 at 9:01 AM, Peter 
>>> Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>    Hi Hanson,
>>>     This one is more low-level, but catches some invalid usages and 
>>> is more resource-friendly:
>>>       public class OrderedExecutor {
>>>         public <T> T execCriticalSectionInOrder(
>>>           final int order,
>>>           final Supplier<T> criticalSection
>>>       ) throws InterruptedException {
>>>           if (order < 0) {
>>>                throw new IllegalArgumentException("'order' should be 
>>> >= 0");
>>>           }
>>>           if (order > 0) {
>>>               waitForDone(order - 1);
>>>           }
>>>           try {
>>>               return criticalSection.get();
>>>           } finally {
>>>               notifyDone(order);
>>>           }
>>>       }
>>>         private static final Object DONE = new Object();
>>>       private final ConcurrentMap<Integer, Object> signals = new 
>>> ConcurrentHashMap<>();
>>>         private void waitForDone(int order) throws 
>>> InterruptedException {
>>>           Object sig = signals.putIfAbsent(order, 
>>> Thread.currentThread());
>>>           if (sig != null && sig != DONE) {
>>>               throw new IllegalStateException();
>>>           }
>>>           while (sig != DONE) {
>>>               LockSupport.park();
>>>               if (Thread.interrupted()) {
>>>                   throw new InterruptedException();
>>>               }
>>>               sig = signals.get(order);
>>>           }
>>>       }
>>>         private void notifyDone(int order) {
>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>           if (sig instanceof Thread) {
>>>               if (!signals.replace(order, sig, DONE)) {
>>>                   throw new IllegalStateException();
>>>               }
>>>               LockSupport.unpark((Thread) sig);
>>>           } else if (sig != null) {
>>>               throw new IllegalStateException();
>>>           }
>>>       }
>>>   }
>>>       Regards, Peter
>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>      Hi Peter,
>>>    Thanks for the suggestion, and sorry about not being clear about 
>>> one important  detail: "n" is not known a priori when constructing 
>>> an OrderedExecutor.  Does this mean the use of CountDownLatch is 
>>> ruled out?
>>>     If you know at least the upper bound of 'n', it can be used with 
>>> such 'n'. Otherwise something that dynamically re-sizes the array 
>>> could be devised. Or you could simply use a ConcurrentHashMap 
>>> instead of array where keys are 'order' values:
>>>       public class OrderedExecutor<T> {
>>>         private final ConcurrentMap<Integer, CountDownLatch> latches 
>>> = new ConcurrentHashMap<>();
>>>         public T execCriticalSectionInOrder(final int order,
>>>                                           final Supplier<T> 
>>> criticalSection) throws InterruptedException {
>>>           if (order > 0) {
>>>               latches.computeIfAbsent(order - 1, o -> new 
>>> CountDownLatch(1)).await();
>>>           }
>>>           try {
>>>               return criticalSection.get();
>>>           } finally {
>>>               latches.computeIfAbsent(order, o -> new 
>>> CountDownLatch(1)).countDown();
>>>           }
>>>       }
>>>   }
>>>       Regards, Peter
>>>           You guessed right: it's a one-shot object for a particular 
>>> OrderedExecutor  instance, and "order" must be called indeed at most 
>>> once.
>>>    Regards, Hanson
>>>   On Sun, Dec 14, 2014 at 2:21 AM, Peter 
>>> Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>    Hi Hanson,
>>>     I don't think anything like that readily exists  in 
>>> java.lang.concurrent, but what you describe should be possible to  
>>> achieve with composition of existing primitives.  You haven't given 
>>> any additional hints to what your OrderedExecutor  should behave 
>>> like. Should it be a one-shot object (like CountDownLatch) or a 
>>> re-usable one (like CyclicBarrier)? Will 
>>> execCriticalSectionInOrder() for a particular OrderedExecutor 
>>> instance and 'order' value be called at most once? If yes (and I 
>>> think that only a one-shot object  makes sense here), an array of 
>>> CountDownLatch(es) could be used:
>>>     public class OrderedExecutor<T> {
>>>       private final CountDownLatch[] latches;
>>>         public OrderedExecutor(int n) {
>>>           if (n < 1) throw new IllegalArgumentException("'n'  should 
>>> be >= 1");
>>>           latches = new CountDownLatch[n - 1];
>>>           for (int i = 0; i < latches.length; i++) {
>>>               latches[i] = new CountDownLatch(1);
>>>           }
>>>       }
>>>         public T execCriticalSectionInOrder(final int order,
>>>                                            final Supplier<T> 
>>> criticalSection) throws InterruptedException {
>>>           if (order < 0 || order > latches.length)
>>>               throw new IllegalArgumentException("'order' should be 
>>> [0..." +  latches.length + "]");
>>>           if (order > 0) {
>>>               latches[order - 1].await();
>>>           }
>>>           try {
>>>               return criticalSection.get();
>>>           } finally {
>>>               if (order < latches.length) {
>>>                   latches[order].countDown();
>>>               }
>>>           }
>>>       }
>>>   }
>>>       Regards, Peter
>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>         Hi, I am looking for a construct that can  be used to 
>>> efficiently enforce  ordered execution of multiple critical 
>>> sections, each calling from a  different thread. The calling threads 
>>> may run in  parallel and may call the execution method out of order. 
>>> The  perceived construct would therefore be responsible for 
>>> re-ordering the execution of those threads, so that their critical  
>>> sections (and only the critical section) will be executed in order. 
>>> Would something  like the following API already exist? /** * Used to 
>>> enforce ordered execution of critical sections calling from multiple 
>>> *  threads, parking and unparking the  threads as necessary. */ 
>>> public class OrderedExecutor<T> { /** * Executes a critical section 
>>> at most once with the given order, parking * and  unparking the 
>>> current thread as  necessary so that all critical * sections 
>>> executed  by different threads using this  executor take place in * 
>>> the order from 1 to n  consecutively. */ public T 
>>> execCriticalSectionInOrder
>>> (  final int order, final Callable<T> criticalSection) throws 
>>> InterruptedException; } Regards, Hanson 
>>> _______________________________________________Concurrency-interest 
>>> mailing listConcurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu> 
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
>>> _______________________________________________
>>>   Concurrency-interest mailing list
>>>   Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>


From hanson.char at gmail.com  Thu Dec 18 15:21:56 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Thu, 18 Dec 2014 12:21:56 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <549285B0.3070801@gmail.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
	<5491D658.5060604@oracle.com> <549285B0.3070801@gmail.com>
Message-ID: <CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>

Less overhead and simpler are a nice properties, even though at the expense
of having to wake up all waiting threads just to find out the one with the
right order to execute.  Still, this seems like a good tradeoff.

Thanks,
Hanson

On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com>
wrote:
>
> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>
>> No, there is no difference. Peter didn't spot your entire method is
>> synchronized, so spurious wakeup won't make progress until the owner of the
>> lock exits the method.
>>
>> You could split the synchronization into two blocks - one encompassing
>> the wait loop, the other in the finally block; but it may make no
>> difference.
>>
>> Alex
>>
>
> You're right, Alex. I'm so infected with park/unpark virus that I missed
> that ;-)
>
> Peter
>
>
>> On 17/12/2014 18:36, suman shil wrote:
>>
>>> Thanks peter for your reply. You are right. I should have incremented
>>> currentAllowedOrder in finally block.
>>>
>>> Suman
>>> ------------------------------------------------------------------------
>>> *From:* Peter Levart <peter.levart at gmail.com>
>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>>> concurrency-interest at cs.oswego.edu>
>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>>> critical sections?
>>>
>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>
>>>> Thanks for your response. Will notifyAll() instead of notify() solve
>>>> the problem?
>>>>
>>>
>>> It will, but you should also account for "spurious" wake-ups. You should
>>> increment currentAllowedOrder only after return from callable.call (in
>>> finally block just before notifyAll()).
>>>
>>> Otherwise a nice solution - with minimal state, providing that not many
>>> threads meet at the same time...
>>>
>>> Regards, Peter
>>>
>>>  RegardsSuman
>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:
>>>> oleksandr.otenko at oracle.com>
>>>>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>;
>>>> Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December 17,
>>>> 2014 9:55 PM
>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>> critical sections?
>>>>       There is no guarantee you'll ever hand over the control to the
>>>> right thread upon notify()
>>>>     Alex
>>>>
>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>       Hi, Following is my solution to solve this problem. Please let me
>>>> know if I am missing something.
>>>>    public class OrderedExecutor {  private int currentAllowedOrder =
>>>> 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {
>>>> this.maxLength = n;  } public synchronized Object
>>>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>>>                        throws Exception  { if (order >= maxLength)  {
>>>> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
>>>> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
>>>> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
>>>> }  } }
>>>>    Regards Suman
>>>>        From: Peter Levart<peter.levart at gmail.com> <mailto:
>>>> peter.levart at gmail.com>
>>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:hanson.char at gmail.com>
>>>>   Cc: concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>>> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14, 2014
>>>> 11:01 PM
>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>> critical sections?
>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>      Hi Peter,
>>>>    Thanks for this proposed idea of using LockSupport. This begs the
>>>> question: which one would you choose if you had all three (correct)
>>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>    Regards, Hanson
>>>>     The Semaphore/CountDownLatch variants are equivalent if you don't
>>>> need re-use. So any would do. They lack invalid-use detection. What happens
>>>> if they are not used as intended? Semaphore variant acts differently than
>>>> CountDownLatch variant. The low-level variant I  proposed detects invalid
>>>> usage. So I would probably use this one. But the low level variant is
>>>> harder to reason about it's correctness. I think it is correct, but you
>>>> should show it to somebody else to confirm this.
>>>>     Another question is whether you actually need this kind of
>>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>>> somebody could have an idea how to do that even more elegantly...
>>>>     Regards, Peter
>>>>              On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>    Hi Hanson,
>>>>     This one is more low-level, but catches some invalid usages and is
>>>> more resource-friendly:
>>>>       public class OrderedExecutor {
>>>>         public <T> T execCriticalSectionInOrder(
>>>>           final int order,
>>>>           final Supplier<T> criticalSection
>>>>       ) throws InterruptedException {
>>>>           if (order < 0) {
>>>>                throw new IllegalArgumentException("'order' should be
>>>> >= 0");
>>>>           }
>>>>           if (order > 0) {
>>>>               waitForDone(order - 1);
>>>>           }
>>>>           try {
>>>>               return criticalSection.get();
>>>>           } finally {
>>>>               notifyDone(order);
>>>>           }
>>>>       }
>>>>         private static final Object DONE = new Object();
>>>>       private final ConcurrentMap<Integer, Object> signals = new
>>>> ConcurrentHashMap<>();
>>>>         private void waitForDone(int order) throws InterruptedException
>>>> {
>>>>           Object sig = signals.putIfAbsent(order,
>>>> Thread.currentThread());
>>>>           if (sig != null && sig != DONE) {
>>>>               throw new IllegalStateException();
>>>>           }
>>>>           while (sig != DONE) {
>>>>               LockSupport.park();
>>>>               if (Thread.interrupted()) {
>>>>                   throw new InterruptedException();
>>>>               }
>>>>               sig = signals.get(order);
>>>>           }
>>>>       }
>>>>         private void notifyDone(int order) {
>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>           if (sig instanceof Thread) {
>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>                   throw new IllegalStateException();
>>>>               }
>>>>               LockSupport.unpark((Thread) sig);
>>>>           } else if (sig != null) {
>>>>               throw new IllegalStateException();
>>>>           }
>>>>       }
>>>>   }
>>>>       Regards, Peter
>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>      Hi Peter,
>>>>    Thanks for the suggestion, and sorry about not being clear about one
>>>> important  detail: "n" is not known a priori when constructing an
>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>     If you know at least the upper bound of 'n', it can be used with
>>>> such 'n'. Otherwise something that dynamically re-sizes the array could be
>>>> devised. Or you could simply use a ConcurrentHashMap instead of array where
>>>> keys are 'order' values:
>>>>       public class OrderedExecutor<T> {
>>>>         private final ConcurrentMap<Integer, CountDownLatch> latches =
>>>> new ConcurrentHashMap<>();
>>>>         public T execCriticalSectionInOrder(final int order,
>>>>                                           final Supplier<T>
>>>> criticalSection) throws InterruptedException {
>>>>           if (order > 0) {
>>>>               latches.computeIfAbsent(order - 1, o -> new
>>>> CountDownLatch(1)).await();
>>>>           }
>>>>           try {
>>>>               return criticalSection.get();
>>>>           } finally {
>>>>               latches.computeIfAbsent(order, o -> new
>>>> CountDownLatch(1)).countDown();
>>>>           }
>>>>       }
>>>>   }
>>>>       Regards, Peter
>>>>           You guessed right: it's a one-shot object for a particular
>>>> OrderedExecutor  instance, and "order" must be called indeed at most once.
>>>>    Regards, Hanson
>>>>   On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<peter.levart at gmail.com>
>>>> <mailto:peter.levart at gmail.com>  wrote:
>>>>    Hi Hanson,
>>>>     I don't think anything like that readily exists  in
>>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>>> with composition of existing primitives.  You haven't given any additional
>>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>> (and I think that only a one-shot object  makes sense here), an array of
>>>> CountDownLatch(es) could be used:
>>>>     public class OrderedExecutor<T> {
>>>>       private final CountDownLatch[] latches;
>>>>         public OrderedExecutor(int n) {
>>>>           if (n < 1) throw new IllegalArgumentException("'n'  should be
>>>> >= 1");
>>>>           latches = new CountDownLatch[n - 1];
>>>>           for (int i = 0; i < latches.length; i++) {
>>>>               latches[i] = new CountDownLatch(1);
>>>>           }
>>>>       }
>>>>         public T execCriticalSectionInOrder(final int order,
>>>>                                            final Supplier<T>
>>>> criticalSection) throws InterruptedException {
>>>>           if (order < 0 || order > latches.length)
>>>>               throw new IllegalArgumentException("'order' should be
>>>> [0..." +  latches.length + "]");
>>>>           if (order > 0) {
>>>>               latches[order - 1].await();
>>>>           }
>>>>           try {
>>>>               return criticalSection.get();
>>>>           } finally {
>>>>               if (order < latches.length) {
>>>>                   latches[order].countDown();
>>>>               }
>>>>           }
>>>>       }
>>>>   }
>>>>       Regards, Peter
>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>         Hi, I am looking for a construct that can  be used to
>>>> efficiently enforce  ordered execution of multiple critical sections, each
>>>> calling from a  different thread. The calling threads may run in  parallel
>>>> and may call the execution method out of order. The  perceived construct
>>>> would therefore be responsible for re-ordering the execution of those
>>>> threads, so that their critical  sections (and only the critical section)
>>>> will be executed in order. Would something  like the following API already
>>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>>> most once with the given order, parking * and  unparking the current thread
>>>> as  necessary so that all critical * sections executed  by different
>>>> threads using this  executor take place in * the order from 1 to n
>>>> consecutively. */ public T execCriticalSectionInOrder
>>>> (  final int order, final Callable<T> criticalSection) throws
>>>> InterruptedException; } Regards, Hanson ______________________________
>>>> _________________Concurrency-interest mailing
>>>> listConcurrency-interest at cs.oswego.edu <mailto:Concurrency-interest@
>>>> cs.oswego.edu> http://cs.oswego.edu/mailman/
>>>> listinfo/concurrency-interest ______________________________
>>>> _________________
>>>>   Concurrency-interest mailing list
>>>>   Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest@
>>>> cs.oswego.edu>
>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest@
>>>> cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest@
>>>> cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>
>>>
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141218/2af8d2e2/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Dec 18 16:02:55 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 18 Dec 2014 21:02:55 +0000
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>
References: <5491CA55.5000705@gmail.com>	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>	<5491D658.5060604@oracle.com>	<549285B0.3070801@gmail.com>
	<CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>
Message-ID: <549340FF.5070301@oracle.com>

Yes, no one said it is a good idea to always do that. When it is 
contended, most of the threads will wake up to only go back to sleep.

The pattern you are after is usually called sequencer. You can see it 
used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe 
not that popular.

The best solution will be lock-like, but the waiter nodes will contain 
the value they are waiting for - so only the specific threads get woken 
up. The solution with concurrent map is very similar, only with larger 
overhead from storing the index the thread is waiting for.


Alex


On 18/12/2014 20:21, Hanson Char wrote:
> Less overhead and simpler are a nice properties, even though at the 
> expense of having to wake up all waiting threads just to find out the 
> one with the right order to execute.  Still, this seems like a good 
> tradeoff.
>
> Thanks,
> Hanson
>
> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com 
> <mailto:peter.levart at gmail.com>> wrote:
>
>     On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>
>         No, there is no difference. Peter didn't spot your entire
>         method is synchronized, so spurious wakeup won't make progress
>         until the owner of the lock exits the method.
>
>         You could split the synchronization into two blocks - one
>         encompassing the wait loop, the other in the finally block;
>         but it may make no difference.
>
>         Alex
>
>
>     You're right, Alex. I'm so infected with park/unpark virus that I
>     missed that ;-)
>
>     Peter
>
>
>         On 17/12/2014 18:36, suman shil wrote:
>
>             Thanks peter for your reply. You are right. I should have
>             incremented currentAllowedOrder in finally block.
>
>             Suman
>             ------------------------------------------------------------------------
>             *From:* Peter Levart <peter.levart at gmail.com
>             <mailto:peter.levart at gmail.com>>
>             *To:* suman shil <suman_krec at yahoo.com
>             <mailto:suman_krec at yahoo.com>>; Oleksandr Otenko
>             <oleksandr.otenko at oracle.com
>             <mailto:oleksandr.otenko at oracle.com>>;
>             Concurrency-interest <concurrency-interest at cs.oswego.edu
>             <mailto:concurrency-interest at cs.oswego.edu>>
>             *Sent:* Wednesday, December 17, 2014 11:54 PM
>             *Subject:* Re: [concurrency-interest] Enforcing ordered
>             execution of critical sections?
>
>             On 12/17/2014 06:46 PM, suman shil wrote:
>
>                 Thanks for your response. Will notifyAll() instead of
>                 notify() solve the problem?
>
>
>             It will, but you should also account for "spurious"
>             wake-ups. You should increment currentAllowedOrder only
>             after return from callable.call (in finally block just
>             before notifyAll()).
>
>             Otherwise a nice solution - with minimal state, providing
>             that not many threads meet at the same time...
>
>             Regards, Peter
>
>                 RegardsSuman
>                        From: Oleksandr
>                 Otenko<oleksandr.otenko at oracle.com
>                 <mailto:oleksandr.otenko at oracle.com>>
>                 <mailto:oleksandr.otenko at oracle.com
>                 <mailto:oleksandr.otenko at oracle.com>>
>                   To: suman shil<suman_krec at yahoo.com
>                 <mailto:suman_krec at yahoo.com>>
>                 <mailto:suman_krec at yahoo.com
>                 <mailto:suman_krec at yahoo.com>>;
>                 Concurrency-interest<concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu>>
>                 <mailto:concurrency-interest at cs.oswego.edu
>                 <mailto:concurrency-interest at cs.oswego.edu>>   Sent:
>                 Wednesday, December 17, 2014 9:55 PM
>                   Subject: Re: [concurrency-interest] Enforcing
>                 ordered execution of critical sections?
>                       There is no guarantee you'll ever hand over the
>                 control to the right thread upon notify()
>                     Alex
>
>                 On 17/12/2014 14:07, suman shil wrote:
>                       Hi, Following is my solution to solve this
>                 problem. Please let me know if I am missing something.
>                    public class OrderedExecutor {  private int
>                 currentAllowedOrder = 0;  private int maxLength = 0; 
>                 public OrderedExecutor(int n)  { this.maxLength = n; 
>                 } public synchronized Object
>                 execCriticalSectionInOrder( int order,
>                 Callable<Object> callable)              throws
>                 Exception  { if (order >= maxLength)  {  throw new
>                 Exception("Exceeds maximum order "+ maxLength);  }   
>                 while(order != currentAllowedOrder)  {  wait();  }   
>                 try  { currentAllowedOrder = currentAllowedOrder+1; 
>                 return callable.call();  }  finally  {  notify();  }  } }
>                    Regards Suman
>                        From: Peter Levart<peter.levart at gmail.com
>                 <mailto:peter.levart at gmail.com>>
>                 <mailto:peter.levart at gmail.com
>                 <mailto:peter.levart at gmail.com>>
>                   To: Hanson Char<hanson.char at gmail.com
>                 <mailto:hanson.char at gmail.com>>
>                 <mailto:hanson.char at gmail.com
>                 <mailto:hanson.char at gmail.com>>   Cc:
>                 concurrency-interest<concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu>>
>                 <mailto:concurrency-interest at cs.oswego.edu
>                 <mailto:concurrency-interest at cs.oswego.edu>>   Sent:
>                 Sunday, December 14, 2014 11:01 PM
>                   Subject: Re: [concurrency-interest] Enforcing
>                 ordered execution of critical sections?
>                           On 12/14/2014 06:11 PM, Hanson Char wrote:
>                      Hi Peter,
>                    Thanks for this proposed idea of using LockSupport.
>                 This begs the question: which one would you choose if
>                 you had all three (correct) implementation available? 
>                 (Semaphore, CountDownLatch, or LockSupport)?
>                    Regards, Hanson
>                     The Semaphore/CountDownLatch variants are
>                 equivalent if you don't need re-use. So any would do.
>                 They lack invalid-use detection. What happens if they
>                 are not used as intended? Semaphore variant acts
>                 differently than CountDownLatch variant. The low-level
>                 variant I  proposed detects invalid usage. So I would
>                 probably use this one. But the low level variant is
>                 harder to reason about it's correctness. I think it is
>                 correct, but you should show it to somebody else to
>                 confirm this.
>                     Another question is whether you actually need this
>                 kind of synchronizer. Maybe if you explained what you
>                 are trying to achieve, somebody could have an idea how
>                 to do that even more elegantly...
>                     Regards, Peter
>                              On Sun, Dec 14, 2014 at 9:01 AM, Peter
>                 Levart<peter.levart at gmail.com
>                 <mailto:peter.levart at gmail.com>>
>                 <mailto:peter.levart at gmail.com
>                 <mailto:peter.levart at gmail.com>> wrote:
>                    Hi Hanson,
>                     This one is more low-level, but catches some
>                 invalid usages and is more resource-friendly:
>                       public class OrderedExecutor {
>                         public <T> T execCriticalSectionInOrder(
>                           final int order,
>                           final Supplier<T> criticalSection
>                       ) throws InterruptedException {
>                           if (order < 0) {
>                                throw new
>                 IllegalArgumentException("'order' should be >= 0");
>                           }
>                           if (order > 0) {
>                               waitForDone(order - 1);
>                           }
>                           try {
>                               return criticalSection.get();
>                           } finally {
>                               notifyDone(order);
>                           }
>                       }
>                         private static final Object DONE = new Object();
>                       private final ConcurrentMap<Integer, Object>
>                 signals = new ConcurrentHashMap<>();
>                         private void waitForDone(int order) throws
>                 InterruptedException {
>                           Object sig = signals.putIfAbsent(order,
>                 Thread.currentThread());
>                           if (sig != null && sig != DONE) {
>                               throw new IllegalStateException();
>                           }
>                           while (sig != DONE) {
>                               LockSupport.park();
>                               if (Thread.interrupted()) {
>                                   throw new InterruptedException();
>                               }
>                               sig = signals.get(order);
>                           }
>                       }
>                         private void notifyDone(int order) {
>                           Object sig = signals.putIfAbsent(order, DONE);
>                           if (sig instanceof Thread) {
>                               if (!signals.replace(order, sig, DONE)) {
>                                   throw new IllegalStateException();
>                               }
>                               LockSupport.unpark((Thread) sig);
>                           } else if (sig != null) {
>                               throw new IllegalStateException();
>                           }
>                       }
>                   }
>                       Regards, Peter
>                     On 12/14/2014 05:08 PM, Peter Levart wrote:
>                        On 12/14/2014 04:20 PM, Hanson Char wrote:
>                      Hi Peter,
>                    Thanks for the suggestion, and sorry about not
>                 being clear about one important  detail: "n" is not
>                 known a priori when constructing an OrderedExecutor. 
>                 Does this mean the use of CountDownLatch is ruled out?
>                     If you know at least the upper bound of 'n', it
>                 can be used with such 'n'. Otherwise something that
>                 dynamically re-sizes the array could be devised. Or
>                 you could simply use a ConcurrentHashMap instead of
>                 array where keys are 'order' values:
>                       public class OrderedExecutor<T> {
>                         private final ConcurrentMap<Integer,
>                 CountDownLatch> latches = new ConcurrentHashMap<>();
>                         public T execCriticalSectionInOrder(final int
>                 order,
>                                                           final
>                 Supplier<T> criticalSection) throws InterruptedException {
>                           if (order > 0) {
>                               latches.computeIfAbsent(order - 1, o ->
>                 new CountDownLatch(1)).await();
>                           }
>                           try {
>                               return criticalSection.get();
>                           } finally {
>                               latches.computeIfAbsent(order, o -> new
>                 CountDownLatch(1)).countDown();
>                           }
>                       }
>                   }
>                       Regards, Peter
>                           You guessed right: it's a one-shot object
>                 for a particular OrderedExecutor  instance, and
>                 "order" must be called indeed at most once.
>                    Regards, Hanson
>                   On Sun, Dec 14, 2014 at 2:21 AM, Peter
>                 Levart<peter.levart at gmail.com
>                 <mailto:peter.levart at gmail.com>>
>                 <mailto:peter.levart at gmail.com
>                 <mailto:peter.levart at gmail.com>> wrote:
>                    Hi Hanson,
>                     I don't think anything like that readily exists in
>                 java.lang.concurrent, but what you describe should be
>                 possible to  achieve with composition of existing
>                 primitives.  You haven't given any additional hints to
>                 what your OrderedExecutor should behave like. Should
>                 it be a one-shot object (like CountDownLatch) or a
>                 re-usable one (like CyclicBarrier)? Will
>                 execCriticalSectionInOrder() for a particular
>                 OrderedExecutor instance and 'order' value be called
>                 at most once? If yes (and I think that only a one-shot
>                 object  makes sense here), an array of
>                 CountDownLatch(es) could be used:
>                     public class OrderedExecutor<T> {
>                       private final CountDownLatch[] latches;
>                         public OrderedExecutor(int n) {
>                           if (n < 1) throw new
>                 IllegalArgumentException("'n'  should be >= 1");
>                           latches = new CountDownLatch[n - 1];
>                           for (int i = 0; i < latches.length; i++) {
>                               latches[i] = new CountDownLatch(1);
>                           }
>                       }
>                         public T execCriticalSectionInOrder(final int
>                 order,
>                                                            final
>                 Supplier<T> criticalSection) throws InterruptedException {
>                           if (order < 0 || order > latches.length)
>                               throw new
>                 IllegalArgumentException("'order' should be [0..." + 
>                 latches.length + "]");
>                           if (order > 0) {
>                               latches[order - 1].await();
>                           }
>                           try {
>                               return criticalSection.get();
>                           } finally {
>                               if (order < latches.length) {
>                                   latches[order].countDown();
>                               }
>                           }
>                       }
>                   }
>                       Regards, Peter
>                     On 12/14/2014 05:26 AM, Hanson Char wrote:
>                         Hi, I am looking for a construct that can  be
>                 used to efficiently enforce  ordered execution of
>                 multiple critical sections, each calling from a
>                 different thread. The calling threads may run in
>                 parallel and may call the execution method out of
>                 order. The  perceived construct would therefore be
>                 responsible for re-ordering the execution of those
>                 threads, so that their critical  sections (and only
>                 the critical section) will be executed in order. Would
>                 something  like the following API already exist? /** *
>                 Used to enforce ordered execution of critical sections
>                 calling from multiple *  threads, parking and
>                 unparking the  threads as necessary. */ public class
>                 OrderedExecutor<T> { /** * Executes a critical section
>                 at most once with the given order, parking * and 
>                 unparking the current thread as  necessary so that all
>                 critical * sections executed  by different threads
>                 using this  executor take place in * the order from 1
>                 to n  consecutively. */ public T
>                 execCriticalSectionInOrder
>                 (  final int order, final Callable<T> criticalSection)
>                 throws InterruptedException; } Regards, Hanson
>                 _______________________________________________Concurrency-interest
>                 mailing listConcurrency-interest at cs.oswego.edu
>                 <mailto:listConcurrency-interest at cs.oswego.edu>
>                 <mailto:Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>                 _______________________________________________
>                   Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 <mailto:Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 <mailto:Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 <mailto:Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141218/95eacf9c/attachment-0001.html>

From joe.bowbeer at gmail.com  Thu Dec 18 16:32:22 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 18 Dec 2014 13:32:22 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <549340FF.5070301@oracle.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
	<5491D658.5060604@oracle.com> <549285B0.3070801@gmail.com>
	<CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>
	<549340FF.5070301@oracle.com>
Message-ID: <CAHzJPEoA87RkP1_TfjbPey29X0uJZ=gOwoBrxoh66shXSVZuoA@mail.gmail.com>

I frown on use of notify[All]/wait because they make the code hard to
maintain.

In this case, with potentially lots of waiting threads, I would check out
the "Specific Notification" pattern if I were determined to go the
wait/notify route:

Tim Cargill's paper is dated but still worth reading.

Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ and Peter
Haggar's article:

http://www.ibm.com/developerworks/java/library/j-spnotif.html

On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:
>
>  Yes, no one said it is a good idea to always do that. When it is
> contended, most of the threads will wake up to only go back to sleep.
>
> The pattern you are after is usually called sequencer. You can see it used
> in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe not that
> popular.
>
> The best solution will be lock-like, but the waiter nodes will contain the
> value they are waiting for - so only the specific threads get woken up. The
> solution with concurrent map is very similar, only with larger overhead
> from storing the index the thread is waiting for.
>
>
> Alex
>
>
>
> On 18/12/2014 20:21, Hanson Char wrote:
>
> Less overhead and simpler are a nice properties, even though at the
> expense of having to wake up all waiting threads just to find out the one
> with the right order to execute.  Still, this seems like a good tradeoff.
>
>  Thanks,
> Hanson
>
> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com>
> wrote:
>>
>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>
>>> No, there is no difference. Peter didn't spot your entire method is
>>> synchronized, so spurious wakeup won't make progress until the owner of the
>>> lock exits the method.
>>>
>>> You could split the synchronization into two blocks - one encompassing
>>> the wait loop, the other in the finally block; but it may make no
>>> difference.
>>>
>>> Alex
>>>
>>
>>  You're right, Alex. I'm so infected with park/unpark virus that I missed
>> that ;-)
>>
>> Peter
>>
>>
>>> On 17/12/2014 18:36, suman shil wrote:
>>>
>>>> Thanks peter for your reply. You are right. I should have incremented
>>>> currentAllowedOrder in finally block.
>>>>
>>>> Suman
>>>>
>>>> ------------------------------------------------------------------------
>>>> *From:* Peter Levart <peter.levart at gmail.com>
>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>>>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>>>> concurrency-interest at cs.oswego.edu>
>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>>>> critical sections?
>>>>
>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>>
>>>>> Thanks for your response. Will notifyAll() instead of notify() solve
>>>>> the problem?
>>>>>
>>>>
>>>> It will, but you should also account for "spurious" wake-ups. You
>>>> should increment currentAllowedOrder only after return from callable.call
>>>> (in finally block just before notifyAll()).
>>>>
>>>> Otherwise a nice solution - with minimal state, providing that not many
>>>> threads meet at the same time...
>>>>
>>>> Regards, Peter
>>>>
>>>>  RegardsSuman
>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:
>>>>> oleksandr.otenko at oracle.com>
>>>>>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>;
>>>>> Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>>>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December 17,
>>>>> 2014 9:55 PM
>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>>> critical sections?
>>>>>       There is no guarantee you'll ever hand over the control to the
>>>>> right thread upon notify()
>>>>>     Alex
>>>>>
>>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>>       Hi, Following is my solution to solve this problem. Please let
>>>>> me know if I am missing something.
>>>>>    public class OrderedExecutor {  private int currentAllowedOrder =
>>>>> 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {
>>>>> this.maxLength = n;  } public synchronized Object
>>>>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>>>>                        throws Exception  { if (order >= maxLength)  {
>>>>> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
>>>>> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
>>>>> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
>>>>> }  } }
>>>>>    Regards Suman
>>>>>         From: Peter Levart<peter.levart at gmail.com> <mailto:
>>>>> peter.levart at gmail.com>
>>>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:hanson.char at gmail.com>
>>>>>   Cc: concurrency-interest<concurrency-interest at cs.oswego.edu>
>>>>> <mailto:concurrency-interest at cs.oswego.edu>    Sent: Sunday, December
>>>>> 14, 2014 11:01 PM
>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>>> critical sections?
>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>      Hi Peter,
>>>>>    Thanks for this proposed idea of using LockSupport. This begs the
>>>>> question: which one would you choose if you had all three (correct)
>>>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>>    Regards, Hanson
>>>>>     The Semaphore/CountDownLatch variants are equivalent if you don't
>>>>> need re-use. So any would do. They lack invalid-use detection. What happens
>>>>> if they are not used as intended? Semaphore variant acts differently than
>>>>> CountDownLatch variant. The low-level variant I  proposed detects invalid
>>>>> usage. So I would probably use this one. But the low level variant is
>>>>> harder to reason about it's correctness. I think it is correct, but you
>>>>> should show it to somebody else to confirm this.
>>>>>     Another question is whether you actually need this kind of
>>>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>>>> somebody could have an idea how to do that even more elegantly...
>>>>>     Regards, Peter
>>>>>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>    Hi Hanson,
>>>>>     This one is more low-level, but catches some invalid usages and is
>>>>> more resource-friendly:
>>>>>       public class OrderedExecutor {
>>>>>         public <T> T execCriticalSectionInOrder(
>>>>>           final int order,
>>>>>           final Supplier<T> criticalSection
>>>>>       ) throws InterruptedException {
>>>>>           if (order < 0) {
>>>>>                throw new IllegalArgumentException("'order' should be
>>>>> >= 0");
>>>>>           }
>>>>>           if (order > 0) {
>>>>>               waitForDone(order - 1);
>>>>>           }
>>>>>           try {
>>>>>               return criticalSection.get();
>>>>>           } finally {
>>>>>               notifyDone(order);
>>>>>           }
>>>>>       }
>>>>>         private static final Object DONE = new Object();
>>>>>       private final ConcurrentMap<Integer, Object> signals = new
>>>>> ConcurrentHashMap<>();
>>>>>         private void waitForDone(int order) throws
>>>>> InterruptedException {
>>>>>           Object sig = signals.putIfAbsent(order,
>>>>> Thread.currentThread());
>>>>>           if (sig != null && sig != DONE) {
>>>>>               throw new IllegalStateException();
>>>>>           }
>>>>>           while (sig != DONE) {
>>>>>               LockSupport.park();
>>>>>               if (Thread.interrupted()) {
>>>>>                   throw new InterruptedException();
>>>>>               }
>>>>>               sig = signals.get(order);
>>>>>           }
>>>>>       }
>>>>>         private void notifyDone(int order) {
>>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>>           if (sig instanceof Thread) {
>>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>>                   throw new IllegalStateException();
>>>>>               }
>>>>>               LockSupport.unpark((Thread) sig);
>>>>>           } else if (sig != null) {
>>>>>               throw new IllegalStateException();
>>>>>           }
>>>>>       }
>>>>>   }
>>>>>       Regards, Peter
>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>      Hi Peter,
>>>>>    Thanks for the suggestion, and sorry about not being clear about
>>>>> one important  detail: "n" is not known a priori when constructing an
>>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>>     If you know at least the upper bound of 'n', it can be used with
>>>>> such 'n'. Otherwise something that dynamically re-sizes the array could be
>>>>> devised. Or you could simply use a ConcurrentHashMap instead of array where
>>>>> keys are 'order' values:
>>>>>       public class OrderedExecutor<T> {
>>>>>         private final ConcurrentMap<Integer, CountDownLatch> latches =
>>>>> new ConcurrentHashMap<>();
>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>                                           final Supplier<T>
>>>>> criticalSection) throws InterruptedException {
>>>>>           if (order > 0) {
>>>>>               latches.computeIfAbsent(order - 1, o -> new
>>>>> CountDownLatch(1)).await();
>>>>>           }
>>>>>           try {
>>>>>               return criticalSection.get();
>>>>>           } finally {
>>>>>               latches.computeIfAbsent(order, o -> new
>>>>> CountDownLatch(1)).countDown();
>>>>>           }
>>>>>       }
>>>>>   }
>>>>>       Regards, Peter
>>>>>           You guessed right: it's a one-shot object for a particular
>>>>> OrderedExecutor  instance, and "order" must be called indeed at most once.
>>>>>    Regards, Hanson
>>>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<
>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>    Hi Hanson,
>>>>>     I don't think anything like that readily exists  in
>>>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>>>> with composition of existing primitives.  You haven't given any additional
>>>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>>> (and I think that only a one-shot object  makes sense here), an array of
>>>>> CountDownLatch(es) could be used:
>>>>>     public class OrderedExecutor<T> {
>>>>>       private final CountDownLatch[] latches;
>>>>>         public OrderedExecutor(int n) {
>>>>>           if (n < 1) throw new IllegalArgumentException("'n'  should
>>>>> be >= 1");
>>>>>           latches = new CountDownLatch[n - 1];
>>>>>           for (int i = 0; i < latches.length; i++) {
>>>>>               latches[i] = new CountDownLatch(1);
>>>>>           }
>>>>>       }
>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>                                            final Supplier<T>
>>>>> criticalSection) throws InterruptedException {
>>>>>           if (order < 0 || order > latches.length)
>>>>>               throw new IllegalArgumentException("'order' should be
>>>>> [0..." +  latches.length + "]");
>>>>>           if (order > 0) {
>>>>>               latches[order - 1].await();
>>>>>           }
>>>>>           try {
>>>>>               return criticalSection.get();
>>>>>           } finally {
>>>>>               if (order < latches.length) {
>>>>>                   latches[order].countDown();
>>>>>               }
>>>>>           }
>>>>>       }
>>>>>   }
>>>>>       Regards, Peter
>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>          Hi, I am looking for a construct that can  be used to
>>>>> efficiently enforce  ordered execution of multiple critical sections, each
>>>>> calling from a  different thread. The calling threads may run in  parallel
>>>>> and may call the execution method out of order. The  perceived construct
>>>>> would therefore be responsible for re-ordering the execution of those
>>>>> threads, so that their critical  sections (and only the critical section)
>>>>> will be executed in order. Would something  like the following API already
>>>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>>>> most once with the given order, parking * and  unparking the current thread
>>>>> as  necessary so that all critical * sections executed  by different
>>>>> threads using this  executor take place in * the order from 1 to n
>>>>> consecutively. */ public T execCriticalSectionInOrder
>>>>> (  final int order, final Callable<T> criticalSection) throws
>>>>> InterruptedException; } Regards, Hanson
>>>>> _______________________________________________Concurrency-interest mailing
>>>>> listConcurrency-interest at cs.oswego.edu <mailto:
>>>>> Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>> _______________________________________________
>>>>>   Concurrency-interest mailing list
>>>>>   Concurrency-interest at cs.oswego.edu <mailto:
>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>> Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>> Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>>
>>>>
>>>>
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141218/acb4076a/attachment-0001.html>

From joe.bowbeer at gmail.com  Thu Dec 18 17:03:08 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 18 Dec 2014 14:03:08 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEoA87RkP1_TfjbPey29X0uJZ=gOwoBrxoh66shXSVZuoA@mail.gmail.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
	<5491D658.5060604@oracle.com> <549285B0.3070801@gmail.com>
	<CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>
	<549340FF.5070301@oracle.com>
	<CAHzJPEoA87RkP1_TfjbPey29X0uJZ=gOwoBrxoh66shXSVZuoA@mail.gmail.com>
Message-ID: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>

That would be "Tom" Cargill; link to paper:

http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html

On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>
> I frown on use of notify[All]/wait because they make the code hard to
> maintain.
>
> In this case, with potentially lots of waiting threads, I would check out
> the "Specific Notification" pattern if I were determined to go the
> wait/notify route:
>
> Tim Cargill's paper is dated but still worth reading.
>
> Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ and Peter
> Haggar's article:
>
> http://www.ibm.com/developerworks/java/library/j-spnotif.html
>
> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>>
>>  Yes, no one said it is a good idea to always do that. When it is
>> contended, most of the threads will wake up to only go back to sleep.
>>
>> The pattern you are after is usually called sequencer. You can see it
>> used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe not
>> that popular.
>>
>> The best solution will be lock-like, but the waiter nodes will contain
>> the value they are waiting for - so only the specific threads get woken up.
>> The solution with concurrent map is very similar, only with larger overhead
>> from storing the index the thread is waiting for.
>>
>>
>> Alex
>>
>>
>>
>> On 18/12/2014 20:21, Hanson Char wrote:
>>
>> Less overhead and simpler are a nice properties, even though at the
>> expense of having to wake up all waiting threads just to find out the one
>> with the right order to execute.  Still, this seems like a good tradeoff.
>>
>>  Thanks,
>> Hanson
>>
>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com>
>> wrote:
>>>
>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>
>>>> No, there is no difference. Peter didn't spot your entire method is
>>>> synchronized, so spurious wakeup won't make progress until the owner of the
>>>> lock exits the method.
>>>>
>>>> You could split the synchronization into two blocks - one encompassing
>>>> the wait loop, the other in the finally block; but it may make no
>>>> difference.
>>>>
>>>> Alex
>>>>
>>>
>>>  You're right, Alex. I'm so infected with park/unpark virus that I
>>> missed that ;-)
>>>
>>> Peter
>>>
>>>
>>>> On 17/12/2014 18:36, suman shil wrote:
>>>>
>>>>> Thanks peter for your reply. You are right. I should have incremented
>>>>> currentAllowedOrder in finally block.
>>>>>
>>>>> Suman
>>>>>
>>>>> ------------------------------------------------------------------------
>>>>> *From:* Peter Levart <peter.levart at gmail.com>
>>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>>>>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>>>>> concurrency-interest at cs.oswego.edu>
>>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>>>>> critical sections?
>>>>>
>>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>>>
>>>>>> Thanks for your response. Will notifyAll() instead of notify() solve
>>>>>> the problem?
>>>>>>
>>>>>
>>>>> It will, but you should also account for "spurious" wake-ups. You
>>>>> should increment currentAllowedOrder only after return from callable.call
>>>>> (in finally block just before notifyAll()).
>>>>>
>>>>> Otherwise a nice solution - with minimal state, providing that not
>>>>> many threads meet at the same time...
>>>>>
>>>>> Regards, Peter
>>>>>
>>>>>  RegardsSuman
>>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:
>>>>>> oleksandr.otenko at oracle.com>
>>>>>>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>;
>>>>>> Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December 17,
>>>>>> 2014 9:55 PM
>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>>>> critical sections?
>>>>>>       There is no guarantee you'll ever hand over the control to the
>>>>>> right thread upon notify()
>>>>>>     Alex
>>>>>>
>>>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>>>       Hi, Following is my solution to solve this problem. Please let
>>>>>> me know if I am missing something.
>>>>>>    public class OrderedExecutor {  private int currentAllowedOrder =
>>>>>> 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {
>>>>>> this.maxLength = n;  } public synchronized Object
>>>>>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>>>>>                        throws Exception  { if (order >= maxLength)  {
>>>>>> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
>>>>>> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
>>>>>> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
>>>>>> }  } }
>>>>>>    Regards Suman
>>>>>>         From: Peter Levart<peter.levart at gmail.com> <mailto:
>>>>>> peter.levart at gmail.com>
>>>>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:
>>>>>> hanson.char at gmail.com>    Cc: concurrency-interest<
>>>>>> concurrency-interest at cs.oswego.edu> <mailto:
>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14,
>>>>>> 2014 11:01 PM
>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>>>> critical sections?
>>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>>      Hi Peter,
>>>>>>    Thanks for this proposed idea of using LockSupport. This begs the
>>>>>> question: which one would you choose if you had all three (correct)
>>>>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>>>    Regards, Hanson
>>>>>>     The Semaphore/CountDownLatch variants are equivalent if you don't
>>>>>> need re-use. So any would do. They lack invalid-use detection. What happens
>>>>>> if they are not used as intended? Semaphore variant acts differently than
>>>>>> CountDownLatch variant. The low-level variant I  proposed detects invalid
>>>>>> usage. So I would probably use this one. But the low level variant is
>>>>>> harder to reason about it's correctness. I think it is correct, but you
>>>>>> should show it to somebody else to confirm this.
>>>>>>     Another question is whether you actually need this kind of
>>>>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>>>>> somebody could have an idea how to do that even more elegantly...
>>>>>>     Regards, Peter
>>>>>>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>    Hi Hanson,
>>>>>>     This one is more low-level, but catches some invalid usages and
>>>>>> is more resource-friendly:
>>>>>>       public class OrderedExecutor {
>>>>>>         public <T> T execCriticalSectionInOrder(
>>>>>>           final int order,
>>>>>>           final Supplier<T> criticalSection
>>>>>>       ) throws InterruptedException {
>>>>>>           if (order < 0) {
>>>>>>                throw new IllegalArgumentException("'order' should be
>>>>>> >= 0");
>>>>>>           }
>>>>>>           if (order > 0) {
>>>>>>               waitForDone(order - 1);
>>>>>>           }
>>>>>>           try {
>>>>>>               return criticalSection.get();
>>>>>>           } finally {
>>>>>>               notifyDone(order);
>>>>>>           }
>>>>>>       }
>>>>>>         private static final Object DONE = new Object();
>>>>>>       private final ConcurrentMap<Integer, Object> signals = new
>>>>>> ConcurrentHashMap<>();
>>>>>>         private void waitForDone(int order) throws
>>>>>> InterruptedException {
>>>>>>           Object sig = signals.putIfAbsent(order,
>>>>>> Thread.currentThread());
>>>>>>           if (sig != null && sig != DONE) {
>>>>>>               throw new IllegalStateException();
>>>>>>           }
>>>>>>           while (sig != DONE) {
>>>>>>               LockSupport.park();
>>>>>>               if (Thread.interrupted()) {
>>>>>>                   throw new InterruptedException();
>>>>>>               }
>>>>>>               sig = signals.get(order);
>>>>>>           }
>>>>>>       }
>>>>>>         private void notifyDone(int order) {
>>>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>>>           if (sig instanceof Thread) {
>>>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>>>                   throw new IllegalStateException();
>>>>>>               }
>>>>>>               LockSupport.unpark((Thread) sig);
>>>>>>           } else if (sig != null) {
>>>>>>               throw new IllegalStateException();
>>>>>>           }
>>>>>>       }
>>>>>>   }
>>>>>>       Regards, Peter
>>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>>      Hi Peter,
>>>>>>    Thanks for the suggestion, and sorry about not being clear about
>>>>>> one important  detail: "n" is not known a priori when constructing an
>>>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>>>     If you know at least the upper bound of 'n', it can be used with
>>>>>> such 'n'. Otherwise something that dynamically re-sizes the array could be
>>>>>> devised. Or you could simply use a ConcurrentHashMap instead of array where
>>>>>> keys are 'order' values:
>>>>>>       public class OrderedExecutor<T> {
>>>>>>         private final ConcurrentMap<Integer, CountDownLatch> latches
>>>>>> = new ConcurrentHashMap<>();
>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>                                           final Supplier<T>
>>>>>> criticalSection) throws InterruptedException {
>>>>>>           if (order > 0) {
>>>>>>               latches.computeIfAbsent(order - 1, o -> new
>>>>>> CountDownLatch(1)).await();
>>>>>>           }
>>>>>>           try {
>>>>>>               return criticalSection.get();
>>>>>>           } finally {
>>>>>>               latches.computeIfAbsent(order, o -> new
>>>>>> CountDownLatch(1)).countDown();
>>>>>>           }
>>>>>>       }
>>>>>>   }
>>>>>>       Regards, Peter
>>>>>>           You guessed right: it's a one-shot object for a particular
>>>>>> OrderedExecutor  instance, and "order" must be called indeed at most once.
>>>>>>    Regards, Hanson
>>>>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<
>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>    Hi Hanson,
>>>>>>     I don't think anything like that readily exists  in
>>>>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>>>>> with composition of existing primitives.  You haven't given any additional
>>>>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>>>> (and I think that only a one-shot object  makes sense here), an array of
>>>>>> CountDownLatch(es) could be used:
>>>>>>     public class OrderedExecutor<T> {
>>>>>>       private final CountDownLatch[] latches;
>>>>>>         public OrderedExecutor(int n) {
>>>>>>           if (n < 1) throw new IllegalArgumentException("'n'  should
>>>>>> be >= 1");
>>>>>>           latches = new CountDownLatch[n - 1];
>>>>>>           for (int i = 0; i < latches.length; i++) {
>>>>>>               latches[i] = new CountDownLatch(1);
>>>>>>           }
>>>>>>       }
>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>                                            final Supplier<T>
>>>>>> criticalSection) throws InterruptedException {
>>>>>>           if (order < 0 || order > latches.length)
>>>>>>               throw new IllegalArgumentException("'order' should be
>>>>>> [0..." +  latches.length + "]");
>>>>>>           if (order > 0) {
>>>>>>               latches[order - 1].await();
>>>>>>           }
>>>>>>           try {
>>>>>>               return criticalSection.get();
>>>>>>           } finally {
>>>>>>               if (order < latches.length) {
>>>>>>                   latches[order].countDown();
>>>>>>               }
>>>>>>           }
>>>>>>       }
>>>>>>   }
>>>>>>       Regards, Peter
>>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>>          Hi, I am looking for a construct that can  be used to
>>>>>> efficiently enforce  ordered execution of multiple critical sections, each
>>>>>> calling from a  different thread. The calling threads may run in  parallel
>>>>>> and may call the execution method out of order. The  perceived construct
>>>>>> would therefore be responsible for re-ordering the execution of those
>>>>>> threads, so that their critical  sections (and only the critical section)
>>>>>> will be executed in order. Would something  like the following API already
>>>>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>>>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>>>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>>>>> most once with the given order, parking * and  unparking the current thread
>>>>>> as  necessary so that all critical * sections executed  by different
>>>>>> threads using this  executor take place in * the order from 1 to n
>>>>>> consecutively. */ public T execCriticalSectionInOrder
>>>>>> (  final int order, final Callable<T> criticalSection) throws
>>>>>> InterruptedException; } Regards, Hanson
>>>>>> _______________________________________________Concurrency-interest mailing
>>>>>> listConcurrency-interest at cs.oswego.edu <mailto:
>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>> _______________________________________________
>>>>>>   Concurrency-interest mailing list
>>>>>>   Concurrency-interest at cs.oswego.edu <mailto:
>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141218/b6712de2/attachment-0001.html>

From cowwoc at bbs.darktech.org  Fri Dec 19 09:00:36 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Fri, 19 Dec 2014 07:00:36 -0700 (MST)
Subject: [concurrency-interest] More Javadoc problems
Message-ID: <1418997636377-11669.post@n7.nabble.com>

Hi guys,

The Javadoc for CompletableFuture.whenComplete() reads:

"If the supplied action itself encounters an exception, then the returned
stage exceptionally completes with this exception unless this stage also
completed exceptionally."

Huh?!

So if the original stage completed with exception1 but the handler threw
exception2, which exception does the returned CompleteableFuture complete
with? The specification says what *won't* happen if the stage also completed
exceptionally, but it doesn't say what *will* happen. Please clarify :)

Thanks,
Gili



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From cowwoc at bbs.darktech.org  Fri Dec 19 09:13:47 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Fri, 19 Dec 2014 07:13:47 -0700 (MST)
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <1418997636377-11669.post@n7.nabble.com>
References: <1418997636377-11669.post@n7.nabble.com>
Message-ID: <1418998427344-11670.post@n7.nabble.com>

cowwoc wrote
> Hi guys,
> 
> The Javadoc for CompletableFuture.whenComplete() reads:
> 
> "If the supplied action itself encounters an exception, then the returned
> stage exceptionally completes with this exception unless this stage also
> completed exceptionally."
> 
> Huh?!
> 
> So if the original stage completed with exception1 but the handler threw
> exception2, which exception does the returned CompleteableFuture complete
> with? The specification says what *won't* happen if the stage also
> completed exceptionally, but it doesn't say what *will* happen. Please
> clarify :)

It looks like in practice, exception1 is thrown and exception2 is lost.

Couldn't you at least add exception2 as a "suppressed" exception?

Gili



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11670.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jh at squareup.com  Fri Dec 19 09:42:43 2014
From: jh at squareup.com (Josh Humphries)
Date: Fri, 19 Dec 2014 09:42:43 -0500
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <1418997636377-11669.post@n7.nabble.com>
References: <1418997636377-11669.post@n7.nabble.com>
Message-ID: <CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>

Looking at the source code -- the ultimate spec ;) -- the original
exception is preferred. So in your example, the resulting CompletionStage
completes exceptionally with exception1, and exception2 is effectively
eaten.

----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Fri, Dec 19, 2014 at 9:00 AM, cowwoc <cowwoc at bbs.darktech.org> wrote:
>
> Hi guys,
>
> The Javadoc for CompletableFuture.whenComplete() reads:
>
> "If the supplied action itself encounters an exception, then the returned
> stage exceptionally completes with this exception unless this stage also
> completed exceptionally."
>
> Huh?!
>
> So if the original stage completed with exception1 but the handler threw
> exception2, which exception does the returned CompleteableFuture complete
> with? The specification says what *won't* happen if the stage also
> completed
> exceptionally, but it doesn't say what *will* happen. Please clarify :)
>
> Thanks,
> Gili
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/0dbe4674/attachment.html>

From cowwoc at bbs.darktech.org  Fri Dec 19 09:50:38 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Fri, 19 Dec 2014 07:50:38 -0700 (MST)
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>
References: <1418997636377-11669.post@n7.nabble.com>
	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>
Message-ID: <54943D9E.3010201@bbs.darktech.org>

Hi Josh,

Thanks for the follow-up.

I'd like to request two changes:

 1. Add exception2 as a suppressed exception (eating exceptions is like
    silent failures... it's bad business)
 2. Have the Javadoc to mention what will happen in this case.


Let me know what you think.

Thanks,
Gili

On 19/12/2014 9:42 AM, Josh Humphries [via JSR166 Concurrency] wrote:
> Looking at the source code -- the ultimate spec ;) -- the original 
> exception is preferred. So in your example, the resulting 
> CompletionStage completes exceptionally with exception1, and 
> exception2 is effectively eaten.
>
> ----
> *Josh Humphries*
> Manager, Shared Systems  |  Platform Engineering
> Atlanta, GA  |  678-400-4867
> *Square* (www.squareup.com <http://www.squareup.com>)
>
> On Fri, Dec 19, 2014 at 9:00 AM, cowwoc <[hidden email] 
> </user/SendEmail.jtp?type=node&node=11671&i=0>> wrote:
>
>     Hi guys,
>
>     The Javadoc for CompletableFuture.whenComplete() reads:
>
>     "If the supplied action itself encounters an exception, then the
>     returned
>     stage exceptionally completes with this exception unless this
>     stage also
>     completed exceptionally."
>
>     Huh?!
>
>     So if the original stage completed with exception1 but the handler
>     threw
>     exception2, which exception does the returned CompleteableFuture
>     complete
>     with? The specification says what *won't* happen if the stage also
>     completed
>     exceptionally, but it doesn't say what *will* happen. Please
>     clarify :)
>
>     Thanks,
>     Gili
>
>
>
>     --
>     View this message in context:
>     http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669.html
>     Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>     _______________________________________________
>     Concurrency-interest mailing list
>     [hidden email] </user/SendEmail.jtp?type=node&node=11671&i=1>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> [hidden email] </user/SendEmail.jtp?type=node&node=11671&i=2>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> ------------------------------------------------------------------------
> If you reply to this email, your message will be added to the 
> discussion below:
> http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11671.html 
>
> To unsubscribe from More Javadoc problems, click here 
> <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=11669&code=Y293d29jQGJicy5kYXJrdGVjaC5vcmd8MTE2Njl8MTU3NDMyMTI0Nw==>.
> NAML 
> <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml> 
>





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11672.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/b1173cf3/attachment.html>

From jh at squareup.com  Fri Dec 19 10:40:14 2014
From: jh at squareup.com (Josh Humphries)
Date: Fri, 19 Dec 2014 10:40:14 -0500
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <54943D9E.3010201@bbs.darktech.org>
References: <1418997636377-11669.post@n7.nabble.com>
	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>
	<54943D9E.3010201@bbs.darktech.org>
Message-ID: <CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>

I do not disagree. Though one thing to note of this approach is that
"observers" of the original completion stage (that failed with exception 1)
will now witness side effects from a *successor* completion stage via the
suppressed exception. It might be a little odd for some consumers to see
the state of the Throwable change *after* it has already been (potentially)
published to multiple threads.

I would personally vote that the later completion stage fail exceptionally
with exception2. But that's a change in behavior that is likely to be a
non-starter due to being incompatible with the current spec (even if only
slightly).

The actual authors of the library are on this list and can likely chime in
further about why it behaves that way.



----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Fri, Dec 19, 2014 at 9:50 AM, cowwoc <cowwoc at bbs.darktech.org> wrote:
>
> Hi Josh,
>
> Thanks for the follow-up.
>
> I'd like to request two changes:
>
>    1. Add exception2 as a suppressed exception (eating exceptions is like
>    silent failures... it's bad business)
>     2. Have the Javadoc to mention what will happen in this case.
>
>
> Let me know what you think.
>
> Thanks,
> Gili
>  On 19/12/2014 9:42 AM, Josh Humphries [via JSR166 Concurrency] wrote:
>
> Looking at the source code -- the ultimate spec ;) -- the original
> exception is preferred. So in your example, the resulting CompletionStage
> completes exceptionally with exception1, and exception2 is effectively
> eaten.
>
>  ----
> *Josh Humphries*
>  Manager, Shared Systems  |  Platform Engineering
>  Atlanta, GA  |  678-400-4867
>  *Square* (www.squareup.com)
>
> On Fri, Dec 19, 2014 at 9:00 AM, cowwoc <[hidden email]
> <http:///user/SendEmail.jtp?type=node&node=11671&i=0>> wrote:
>>
>> Hi guys,
>>
>> The Javadoc for CompletableFuture.whenComplete() reads:
>>
>> "If the supplied action itself encounters an exception, then the returned
>> stage exceptionally completes with this exception unless this stage also
>> completed exceptionally."
>>
>> Huh?!
>>
>> So if the original stage completed with exception1 but the handler threw
>> exception2, which exception does the returned CompleteableFuture complete
>> with? The specification says what *won't* happen if the stage also
>> completed
>> exceptionally, but it doesn't say what *will* happen. Please clarify :)
>>
>> Thanks,
>> Gili
>>
>>
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=11671&i=1>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=11671&i=2>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11671.html
>  To unsubscribe from More Javadoc problems, click here.
> NAML
> <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>
>
>
> ------------------------------
> View this message in context: Re: More Javadoc problems
> <http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11672.html>
>
> Sent from the JSR166 Concurrency mailing list archive
> <http://jsr166-concurrency.10961.n7.nabble.com/> at Nabble.com.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/5bd8bc77/attachment-0001.html>

From cowwoc at bbs.darktech.org  Fri Dec 19 11:45:38 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Fri, 19 Dec 2014 11:45:38 -0500
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
References: <1418997636377-11669.post@n7.nabble.com>
	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>
	<54943D9E.3010201@bbs.darktech.org>
	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
Message-ID: <54945632.4050302@bbs.darktech.org>

Hi Josh,

Can you give a concrete example where observers would witness side-effects?

Assuming what you say is possible, would you object to throwing a clone 
of the original exception instance with the suppressed exceptions added? 
This way only observers of the 2nd completion stage would observe these 
changes.

Gili

On 19/12/2014 10:40 AM, Josh Humphries wrote:
> I do not disagree. Though one thing to note of this approach is that 
> "observers" of the original completion stage (that failed with 
> exception 1) will now witness side effects from a *successor* 
> completion stage via the suppressed exception. It might be a little 
> odd for some consumers to see the state of the Throwable change 
> /after/ it has already been (potentially) published to multiple threads.
>
> I would personally vote that the later completion stage fail 
> exceptionally with exception2. But that's a change in behavior that is 
> likely to be a non-starter due to being incompatible with the current 
> spec (even if only slightly).
>
> The actual authors of the library are on this list and can likely 
> chime in further about why it behaves that way.
>
>
>
> ----
> *Josh Humphries*
> Manager, Shared Systems  |  Platform Engineering
> Atlanta, GA  |  678-400-4867
> *Square* (www.squareup.com <http://www.squareup.com>)
>
> On Fri, Dec 19, 2014 at 9:50 AM, cowwoc <cowwoc at bbs.darktech.org 
> <mailto:cowwoc at bbs.darktech.org>> wrote:
>
>     Hi Josh,
>
>     Thanks for the follow-up.
>
>     I'd like to request two changes:
>
>      1. Add exception2 as a suppressed exception (eating exceptions is
>         like silent failures... it's bad business)
>      2. Have the Javadoc to mention what will happen in this case.
>
>
>     Let me know what you think.
>
>     Thanks,
>     Gili
>
>     On 19/12/2014 9:42 AM, Josh Humphries [via JSR166 Concurrency] wrote:
>>     Looking at the source code -- the ultimate spec ;) -- the
>>     original exception is preferred. So in your example, the
>>     resulting CompletionStage completes exceptionally with
>>     exception1, and exception2 is effectively eaten.
>>
>>     ----
>>     *Josh Humphries*
>>     Manager, Shared Systems  |  Platform Engineering
>>     Atlanta, GA  | 678-400-4867 <tel:678-400-4867>
>>     *Square* (www.squareup.com <http://www.squareup.com>)
>>
>>     On Fri, Dec 19, 2014 at 9:00 AM, cowwoc <[hidden email]
>>     <http:///user/SendEmail.jtp?type=node&node=11671&i=0>> wrote:
>>
>>         Hi guys,
>>
>>         The Javadoc for CompletableFuture.whenComplete() reads:
>>
>>         "If the supplied action itself encounters an exception, then
>>         the returned
>>         stage exceptionally completes with this exception unless this
>>         stage also
>>         completed exceptionally."
>>
>>         Huh?!
>>
>>         So if the original stage completed with exception1 but the
>>         handler threw
>>         exception2, which exception does the returned
>>         CompleteableFuture complete
>>         with? The specification says what *won't* happen if the stage
>>         also completed
>>         exceptionally, but it doesn't say what *will* happen. Please
>>         clarify :)
>>
>>         Thanks,
>>         Gili
>>
>>
>>
>>         --
>>         View this message in context:
>>         http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669.html
>>         Sent from the JSR166 Concurrency mailing list archive at
>>         Nabble.com.
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         [hidden email]
>>         <http:///user/SendEmail.jtp?type=node&node=11671&i=1>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     [hidden email] <http:///user/SendEmail.jtp?type=node&node=11671&i=2>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>     ------------------------------------------------------------------------
>>     If you reply to this email, your message will be added to the
>>     discussion below:
>>     http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11671.html
>>
>>     To unsubscribe from More Javadoc problems, click here.
>>     NAML
>>     <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>
>
>     ------------------------------------------------------------------------
>     View this message in context: Re: More Javadoc problems
>     <http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11672.html>
>
>
>     Sent from the JSR166 Concurrency mailing list archive
>     <http://jsr166-concurrency.10961.n7.nabble.com/> at Nabble.com.
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/4647969e/attachment.html>

From peter.levart at gmail.com  Fri Dec 19 14:42:43 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 19 Dec 2014 20:42:43 +0100
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <54945632.4050302@bbs.darktech.org>
References: <1418997636377-11669.post@n7.nabble.com>	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>	<54943D9E.3010201@bbs.darktech.org>	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
	<54945632.4050302@bbs.darktech.org>
Message-ID: <54947FB3.7030501@gmail.com>


On 12/19/2014 05:45 PM, cowwoc wrote:
> Hi Josh,
>
> Can you give a concrete example where observers would witness 
> side-effects?

Try this:


public class CFTest {

     static void sleep(long millis) {
         try {
             Thread.sleep(millis);
         } catch (InterruptedException e) {
         }
     }

     public static void main(String[] args) throws Exception {

         CompletableFuture<Integer> cf1 = 
CompletableFuture.supplyAsync(() -> {
             sleep(200L);
             throw new RuntimeException("1st");
         });

         cf1.whenComplete((i, e1) -> {
             if (e1 != null) {
                 sleep(100L);
                 RuntimeException e2 = new RuntimeException("2nd");
                 e1.addSuppressed(e2); // this is what is desired right?
                 throw e2;
             }
         });

         try {
             cf1.join();
         } catch (CompletionException e) {
             System.out.println("\n1st print:\n");
             e.printStackTrace(System.out);
             sleep(200L);
             System.out.println("\n2nd print:\n");
             e.printStackTrace(System.out);
         }
     }
}

>
> Assuming what you say is possible, would you object to throwing a 
> clone of the original exception instance with the suppressed 
> exceptions added? This way only observers of the 2nd completion stage 
> would observe these changes.

Exceptions are not generally Cloneable.

Regards, Peter


> Gili
>
> On 19/12/2014 10:40 AM, Josh Humphries wrote:
>> I do not disagree. Though one thing to note of this approach is that 
>> "observers" of the original completion stage (that failed with 
>> exception 1) will now witness side effects from a *successor* 
>> completion stage via the suppressed exception. It might be a little 
>> odd for some consumers to see the state of the Throwable change 
>> /after/ it has already been (potentially) published to multiple threads.
>>
>> I would personally vote that the later completion stage fail 
>> exceptionally with exception2. But that's a change in behavior that 
>> is likely to be a non-starter due to being incompatible with the 
>> current spec (even if only slightly).
>>
>> The actual authors of the library are on this list and can likely 
>> chime in further about why it behaves that way.
>>
>>
>>
>> ----
>> *Josh Humphries*
>> Manager, Shared Systems  |  Platform Engineering
>> Atlanta, GA  |  678-400-4867
>> *Square* (www.squareup.com <http://www.squareup.com>)
>>
>> On Fri, Dec 19, 2014 at 9:50 AM, cowwoc <cowwoc at bbs.darktech.org 
>> <mailto:cowwoc at bbs.darktech.org>> wrote:
>>
>>     Hi Josh,
>>
>>     Thanks for the follow-up.
>>
>>     I'd like to request two changes:
>>
>>      1. Add exception2 as a suppressed exception (eating exceptions
>>         is like silent failures... it's bad business)
>>      2. Have the Javadoc to mention what will happen in this case.
>>
>>
>>     Let me know what you think.
>>
>>     Thanks,
>>     Gili
>>
>>     On 19/12/2014 9:42 AM, Josh Humphries [via JSR166 Concurrency] wrote:
>>>     Looking at the source code -- the ultimate spec ;) -- the
>>>     original exception is preferred. So in your example, the
>>>     resulting CompletionStage completes exceptionally with
>>>     exception1, and exception2 is effectively eaten.
>>>
>>>     ----
>>>     *Josh Humphries*
>>>     Manager, Shared Systems  |  Platform Engineering
>>>     Atlanta, GA  | 678-400-4867 <tel:678-400-4867>
>>>     *Square* (www.squareup.com <http://www.squareup.com>)
>>>
>>>     On Fri, Dec 19, 2014 at 9:00 AM, cowwoc <[hidden email]
>>>     <http:///user/SendEmail.jtp?type=node&node=11671&i=0>> wrote:
>>>
>>>         Hi guys,
>>>
>>>         The Javadoc for CompletableFuture.whenComplete() reads:
>>>
>>>         "If the supplied action itself encounters an exception, then
>>>         the returned
>>>         stage exceptionally completes with this exception unless
>>>         this stage also
>>>         completed exceptionally."
>>>
>>>         Huh?!
>>>
>>>         So if the original stage completed with exception1 but the
>>>         handler threw
>>>         exception2, which exception does the returned
>>>         CompleteableFuture complete
>>>         with? The specification says what *won't* happen if the
>>>         stage also completed
>>>         exceptionally, but it doesn't say what *will* happen. Please
>>>         clarify :)
>>>
>>>         Thanks,
>>>         Gili
>>>
>>>
>>>
>>>         --
>>>         View this message in context:
>>>         http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669.html
>>>         Sent from the JSR166 Concurrency mailing list archive at
>>>         Nabble.com.
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         [hidden email]
>>>         <http:///user/SendEmail.jtp?type=node&node=11671&i=1>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     [hidden email]
>>>     <http:///user/SendEmail.jtp?type=node&node=11671&i=2>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>     ------------------------------------------------------------------------
>>>     If you reply to this email, your message will be added to the
>>>     discussion below:
>>>     http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11671.html
>>>
>>>     To unsubscribe from More Javadoc problems, click here.
>>>     NAML
>>>     <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>>
>>
>>
>>     ------------------------------------------------------------------------
>>     View this message in context: Re: More Javadoc problems
>>     <http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11672.html>
>>
>>
>>     Sent from the JSR166 Concurrency mailing list archive
>>     <http://jsr166-concurrency.10961.n7.nabble.com/> at Nabble.com.
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/4f9284cf/attachment.html>

From cowwoc at bbs.darktech.org  Fri Dec 19 15:20:17 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Fri, 19 Dec 2014 15:20:17 -0500
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <54947FB3.7030501@gmail.com>
References: <1418997636377-11669.post@n7.nabble.com>	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>	<54943D9E.3010201@bbs.darktech.org>	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
	<54945632.4050302@bbs.darktech.org> <54947FB3.7030501@gmail.com>
Message-ID: <54948881.5020700@bbs.darktech.org>

Hi Peter,

That's not what I meant. I meant that we should "throw e1" not "throw 
e2" near the end. I say this because I am under the impression that 
join() only fires after whenComplete() completes. Is that not the case?

Assuming I'm wrong, then I'd advocate an approach similar to what you 
said: throw a new exception (say e2) but instead of invoking 
e1.addSuppressed(e2) I'd do it the other way around: 
e2.addSuppressed(e1). This way main() would see e1 (unmodified) and 
anyone listening to the new stage would see e2.addSuppressed(e1).

The only problem (as you mentioned) is that the specification/Javadoc 
took a very undesirable path (throws e1 and ignores e2). I really hope 
we can change this.

Gili

On 19/12/2014 2:42 PM, Peter Levart wrote:
>
> On 12/19/2014 05:45 PM, cowwoc wrote:
>> Hi Josh,
>>
>> Can you give a concrete example where observers would witness 
>> side-effects?
>
> Try this:
>
>
> public class CFTest {
>
>     static void sleep(long millis) {
>         try {
>             Thread.sleep(millis);
>         } catch (InterruptedException e) {
>         }
>     }
>
>     public static void main(String[] args) throws Exception {
>
>         CompletableFuture<Integer> cf1 = 
> CompletableFuture.supplyAsync(() -> {
>             sleep(200L);
>             throw new RuntimeException("1st");
>         });
>
>         cf1.whenComplete((i, e1) -> {
>             if (e1 != null) {
>                 sleep(100L);
>                 RuntimeException e2 = new RuntimeException("2nd");
>                 e1.addSuppressed(e2); // this is what is desired right?
>                 throw e2;
>             }
>         });
>
>         try {
>             cf1.join();
>         } catch (CompletionException e) {
>             System.out.println("\n1st print:\n");
>             e.printStackTrace(System.out);
>             sleep(200L);
>             System.out.println("\n2nd print:\n");
>             e.printStackTrace(System.out);
>         }
>     }
> }
>
>>
>> Assuming what you say is possible, would you object to throwing a 
>> clone of the original exception instance with the suppressed 
>> exceptions added? This way only observers of the 2nd completion stage 
>> would observe these changes.
>
> Exceptions are not generally Cloneable.
>
> Regards, Peter
>
>
>> Gili
>>
>> On 19/12/2014 10:40 AM, Josh Humphries wrote:
>>> I do not disagree. Though one thing to note of this approach is that 
>>> "observers" of the original completion stage (that failed with 
>>> exception 1) will now witness side effects from a *successor* 
>>> completion stage via the suppressed exception. It might be a little 
>>> odd for some consumers to see the state of the Throwable change 
>>> /after/ it has already been (potentially) published to multiple 
>>> threads.
>>>
>>> I would personally vote that the later completion stage fail 
>>> exceptionally with exception2. But that's a change in behavior that 
>>> is likely to be a non-starter due to being incompatible with the 
>>> current spec (even if only slightly).
>>>
>>> The actual authors of the library are on this list and can likely 
>>> chime in further about why it behaves that way.
>>>
>>>
>>>
>>> ----
>>> *Josh Humphries*
>>> Manager, Shared Systems  |  Platform Engineering
>>> Atlanta, GA  |  678-400-4867
>>> *Square* (www.squareup.com <http://www.squareup.com>)
>>>
>>> On Fri, Dec 19, 2014 at 9:50 AM, cowwoc <cowwoc at bbs.darktech.org 
>>> <mailto:cowwoc at bbs.darktech.org>> wrote:
>>>
>>>     Hi Josh,
>>>
>>>     Thanks for the follow-up.
>>>
>>>     I'd like to request two changes:
>>>
>>>      1. Add exception2 as a suppressed exception (eating exceptions
>>>         is like silent failures... it's bad business)
>>>      2. Have the Javadoc to mention what will happen in this case.
>>>
>>>
>>>     Let me know what you think.
>>>
>>>     Thanks,
>>>     Gili
>>>
>>>     On 19/12/2014 9:42 AM, Josh Humphries [via JSR166 Concurrency]
>>>     wrote:
>>>>     Looking at the source code -- the ultimate spec ;) -- the
>>>>     original exception is preferred. So in your example, the
>>>>     resulting CompletionStage completes exceptionally with
>>>>     exception1, and exception2 is effectively eaten.
>>>>
>>>>     ----
>>>>     *Josh Humphries*
>>>>     Manager, Shared Systems  |  Platform Engineering
>>>>     Atlanta, GA  | 678-400-4867 <tel:678-400-4867>
>>>>     *Square* (www.squareup.com <http://www.squareup.com>)
>>>>
>>>>     On Fri, Dec 19, 2014 at 9:00 AM, cowwoc <[hidden email]
>>>>     <http:///user/SendEmail.jtp?type=node&node=11671&i=0>> wrote:
>>>>
>>>>         Hi guys,
>>>>
>>>>         The Javadoc for CompletableFuture.whenComplete() reads:
>>>>
>>>>         "If the supplied action itself encounters an exception,
>>>>         then the returned
>>>>         stage exceptionally completes with this exception unless
>>>>         this stage also
>>>>         completed exceptionally."
>>>>
>>>>         Huh?!
>>>>
>>>>         So if the original stage completed with exception1 but the
>>>>         handler threw
>>>>         exception2, which exception does the returned
>>>>         CompleteableFuture complete
>>>>         with? The specification says what *won't* happen if the
>>>>         stage also completed
>>>>         exceptionally, but it doesn't say what *will* happen.
>>>>         Please clarify :)
>>>>
>>>>         Thanks,
>>>>         Gili
>>>>
>>>>
>>>>
>>>>         --
>>>>         View this message in context:
>>>>         http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669.html
>>>>         Sent from the JSR166 Concurrency mailing list archive at
>>>>         Nabble.com.
>>>>         _______________________________________________
>>>>         Concurrency-interest mailing list
>>>>         [hidden email]
>>>>         <http:///user/SendEmail.jtp?type=node&node=11671&i=1>
>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>     _______________________________________________
>>>>     Concurrency-interest mailing list
>>>>     [hidden email]
>>>>     <http:///user/SendEmail.jtp?type=node&node=11671&i=2>
>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>     ------------------------------------------------------------------------
>>>>     If you reply to this email, your message will be added to the
>>>>     discussion below:
>>>>     http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11671.html
>>>>
>>>>     To unsubscribe from More Javadoc problems, click here.
>>>>     NAML
>>>>     <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>>>
>>>
>>>
>>>     ------------------------------------------------------------------------
>>>     View this message in context: Re: More Javadoc problems
>>>     <http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11672.html>
>>>
>>>
>>>     Sent from the JSR166 Concurrency mailing list archive
>>>     <http://jsr166-concurrency.10961.n7.nabble.com/> at Nabble.com.
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu
>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/9e992a8b/attachment-0001.html>

From peter.levart at gmail.com  Fri Dec 19 16:10:47 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 19 Dec 2014 22:10:47 +0100
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <54948881.5020700@bbs.darktech.org>
References: <1418997636377-11669.post@n7.nabble.com>	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>	<54943D9E.3010201@bbs.darktech.org>	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
	<54945632.4050302@bbs.darktech.org> <54947FB3.7030501@gmail.com>
	<54948881.5020700@bbs.darktech.org>
Message-ID: <54949457.7080602@gmail.com>


On 12/19/2014 09:20 PM, cowwoc wrote:
> Hi Peter,
>
> That's not what I meant. I meant that we should "throw e1" not "throw 
> e2" near the end. I say this because I am under the impression that 
> join() only fires after whenComplete() completes. Is that not the case?

join() returns (or throws) when cf1 completes (cf1 has been 
asynchronously executing a Supplier in background thread - see 
supplyAsync). At the same time (but in another thread - the thread that 
just completed 'cf1') a whenComplete() BiConsumer is executed. So you 
have two threads (main thread and background thread) both accessing the 
same instance of exception 'e1' at the same time - the outcome of cf1. 
So you can't add 'e2' to the suppressed exceptions of 'e1' in the 
background thread without observing 'e1' being changed in main thread.

>
> Assuming I'm wrong, then I'd advocate an approach similar to what you 
> said: throw a new exception (say e2) but instead of invoking 
> e1.addSuppressed(e2) I'd do it the other way around: 
> e2.addSuppressed(e1). This way main() would see e1 (unmodified) and 
> anyone listening to the new stage would see e2.addSuppressed(e1).
>
> The only problem (as you mentioned) is that the specification/Javadoc 
> took a very undesirable path (throws e1 and ignores e2). I really hope 
> we can change this.

I think the intention of whenComplete is to provide a means of 
post-cleanup. Like implicit close in try-with-resources statement where 
the exception that gets propagated is the main exception from the body 
block. Since completion results can be consumed right-away, it is 
impossible to attach suppressed exceptions later when post-cleanup 
completes.

Why the 2nd stage completes with the same exception as 1st? I think 
because it is designed to complete with the same (exceptional OR 
nonexceptional) result as the 1st. If would be inconsistent to expect 
the same nonexceptional result but different exceptional.

If you want to change the exceptional OR nonexceptional outcome of 
preceeding stage, then use handle() instead of whenComplete().

Regards, Peter

>
> Gili
>
> On 19/12/2014 2:42 PM, Peter Levart wrote:
>>
>> On 12/19/2014 05:45 PM, cowwoc wrote:
>>> Hi Josh,
>>>
>>> Can you give a concrete example where observers would witness 
>>> side-effects?
>>
>> Try this:
>>
>>
>> public class CFTest {
>>
>>     static void sleep(long millis) {
>>         try {
>>             Thread.sleep(millis);
>>         } catch (InterruptedException e) {
>>         }
>>     }
>>
>>     public static void main(String[] args) throws Exception {
>>
>>         CompletableFuture<Integer> cf1 = 
>> CompletableFuture.supplyAsync(() -> {
>>             sleep(200L);
>>             throw new RuntimeException("1st");
>>         });
>>
>>         cf1.whenComplete((i, e1) -> {
>>             if (e1 != null) {
>>                 sleep(100L);
>>                 RuntimeException e2 = new RuntimeException("2nd");
>>                 e1.addSuppressed(e2); // this is what is desired right?
>>                 throw e2;
>>             }
>>         });
>>
>>         try {
>>             cf1.join();
>>         } catch (CompletionException e) {
>>             System.out.println("\n1st print:\n");
>>             e.printStackTrace(System.out);
>>             sleep(200L);
>>             System.out.println("\n2nd print:\n");
>>             e.printStackTrace(System.out);
>>         }
>>     }
>> }
>>
>>>
>>> Assuming what you say is possible, would you object to throwing a 
>>> clone of the original exception instance with the suppressed 
>>> exceptions added? This way only observers of the 2nd completion 
>>> stage would observe these changes.
>>
>> Exceptions are not generally Cloneable.
>>
>> Regards, Peter
>>
>>
>>> Gili
>>>
>>> On 19/12/2014 10:40 AM, Josh Humphries wrote:
>>>> I do not disagree. Though one thing to note of this approach is 
>>>> that "observers" of the original completion stage (that failed with 
>>>> exception 1) will now witness side effects from a *successor* 
>>>> completion stage via the suppressed exception. It might be a little 
>>>> odd for some consumers to see the state of the Throwable change 
>>>> /after/ it has already been (potentially) published to multiple 
>>>> threads.
>>>>
>>>> I would personally vote that the later completion stage fail 
>>>> exceptionally with exception2. But that's a change in behavior that 
>>>> is likely to be a non-starter due to being incompatible with the 
>>>> current spec (even if only slightly).
>>>>
>>>> The actual authors of the library are on this list and can likely 
>>>> chime in further about why it behaves that way.
>>>>
>>>>
>>>>
>>>> ----
>>>> *Josh Humphries*
>>>> Manager, Shared Systems  |  Platform Engineering
>>>> Atlanta, GA  |  678-400-4867
>>>> *Square* (www.squareup.com <http://www.squareup.com>)
>>>>
>>>> On Fri, Dec 19, 2014 at 9:50 AM, cowwoc <cowwoc at bbs.darktech.org 
>>>> <mailto:cowwoc at bbs.darktech.org>> wrote:
>>>>
>>>>     Hi Josh,
>>>>
>>>>     Thanks for the follow-up.
>>>>
>>>>     I'd like to request two changes:
>>>>
>>>>      1. Add exception2 as a suppressed exception (eating exceptions
>>>>         is like silent failures... it's bad business)
>>>>      2. Have the Javadoc to mention what will happen in this case.
>>>>
>>>>
>>>>     Let me know what you think.
>>>>
>>>>     Thanks,
>>>>     Gili
>>>>
>>>>     On 19/12/2014 9:42 AM, Josh Humphries [via JSR166 Concurrency]
>>>>     wrote:
>>>>>     Looking at the source code -- the ultimate spec ;) -- the
>>>>>     original exception is preferred. So in your example, the
>>>>>     resulting CompletionStage completes exceptionally with
>>>>>     exception1, and exception2 is effectively eaten.
>>>>>
>>>>>     ----
>>>>>     *Josh Humphries*
>>>>>     Manager, Shared Systems  |  Platform Engineering
>>>>>     Atlanta, GA  | 678-400-4867 <tel:678-400-4867>
>>>>>     *Square* (www.squareup.com <http://www.squareup.com>)
>>>>>
>>>>>     On Fri, Dec 19, 2014 at 9:00 AM, cowwoc <[hidden email]
>>>>>     <http:///user/SendEmail.jtp?type=node&node=11671&i=0>> wrote:
>>>>>
>>>>>         Hi guys,
>>>>>
>>>>>         The Javadoc for CompletableFuture.whenComplete() reads:
>>>>>
>>>>>         "If the supplied action itself encounters an exception,
>>>>>         then the returned
>>>>>         stage exceptionally completes with this exception unless
>>>>>         this stage also
>>>>>         completed exceptionally."
>>>>>
>>>>>         Huh?!
>>>>>
>>>>>         So if the original stage completed with exception1 but the
>>>>>         handler threw
>>>>>         exception2, which exception does the returned
>>>>>         CompleteableFuture complete
>>>>>         with? The specification says what *won't* happen if the
>>>>>         stage also completed
>>>>>         exceptionally, but it doesn't say what *will* happen.
>>>>>         Please clarify :)
>>>>>
>>>>>         Thanks,
>>>>>         Gili
>>>>>
>>>>>
>>>>>
>>>>>         --
>>>>>         View this message in context:
>>>>>         http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669.html
>>>>>         Sent from the JSR166 Concurrency mailing list archive at
>>>>>         Nabble.com.
>>>>>         _______________________________________________
>>>>>         Concurrency-interest mailing list
>>>>>         [hidden email]
>>>>>         <http:///user/SendEmail.jtp?type=node&node=11671&i=1>
>>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>     _______________________________________________
>>>>>     Concurrency-interest mailing list
>>>>>     [hidden email]
>>>>>     <http:///user/SendEmail.jtp?type=node&node=11671&i=2>
>>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>     ------------------------------------------------------------------------
>>>>>     If you reply to this email, your message will be added to the
>>>>>     discussion below:
>>>>>     http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11671.html
>>>>>
>>>>>     To unsubscribe from More Javadoc problems, click here.
>>>>>     NAML
>>>>>     <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>>>>
>>>>
>>>>
>>>>     ------------------------------------------------------------------------
>>>>     View this message in context: Re: More Javadoc problems
>>>>     <http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11672.html>
>>>>
>>>>
>>>>     Sent from the JSR166 Concurrency mailing list archive
>>>>     <http://jsr166-concurrency.10961.n7.nabble.com/> at Nabble.com.
>>>>
>>>>     _______________________________________________
>>>>     Concurrency-interest mailing list
>>>>     Concurrency-interest at cs.oswego.edu
>>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/8752a3df/attachment-0001.html>

From cowwoc at bbs.darktech.org  Fri Dec 19 16:30:05 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Fri, 19 Dec 2014 16:30:05 -0500
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <54949457.7080602@gmail.com>
References: <1418997636377-11669.post@n7.nabble.com>	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>	<54943D9E.3010201@bbs.darktech.org>	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
	<54945632.4050302@bbs.darktech.org> <54947FB3.7030501@gmail.com>
	<54948881.5020700@bbs.darktech.org> <54949457.7080602@gmail.com>
Message-ID: <549498DD.1060800@bbs.darktech.org>

On 19/12/2014 4:10 PM, Peter Levart wrote:
> If you want to change the exceptional OR nonexceptional outcome of 
> preceeding stage, then use handle() instead of whenComplete().

I can't.

If you handle() and attempt to re-throw the same exception, you will get:

     unreported exception Throwable; must be caught or declared to be thrown

I agree that whenComplete() is meant to act as a finally block (which 
removes the need to do this funky casting). What I don't like is that 
join() returns without waiting for the result of whenComplete(). If the 
case of a real try-finally block, the code after the block does not 
execute after finally completes. I am trying to implement the same 
behavior here.

What should I be doing instead?

Gili
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/5d76cd17/attachment.html>

From cowwoc at bbs.darktech.org  Fri Dec 19 16:39:54 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Fri, 19 Dec 2014 16:39:54 -0500
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <549498DD.1060800@bbs.darktech.org>
References: <1418997636377-11669.post@n7.nabble.com>	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>	<54943D9E.3010201@bbs.darktech.org>	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
	<54945632.4050302@bbs.darktech.org> <54947FB3.7030501@gmail.com>
	<54948881.5020700@bbs.darktech.org> <54949457.7080602@gmail.com>
	<549498DD.1060800@bbs.darktech.org>
Message-ID: <54949B2A.4080803@bbs.darktech.org>

On 19/12/2014 4:30 PM, cowwoc wrote:
> On 19/12/2014 4:10 PM, Peter Levart wrote:
>> If you want to change the exceptional OR nonexceptional outcome of 
>> preceeding stage, then use handle() instead of whenComplete().
>
> I can't.
>
> If you handle() and attempt to re-throw the same exception, you will get:
>
>     unreported exception Throwable; must be caught or declared to be 
> thrown
>
> I agree that whenComplete() is meant to act as a finally block (which 
> removes the need to do this funky casting). What I don't like is that 
> join() returns without waiting for the result of whenComplete(). If 
> the case of a real try-finally block, the code after the block does 
> not execute after finally completes. I am trying to implement the same 
> behavior here.
>
> What should I be doing instead?

Actually, I think I'm fine.

I've got a function that returns 
"CompletableFuture.supplyAsync(lambda).whenComplete(lambda)" and I 
join() on that. So technically speaking, I believe this means I am 
joining against the second stage (after whenComplete() has run). Is that 
correct?

Gili
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/497384ec/attachment.html>

From gergg at cox.net  Fri Dec 19 17:03:03 2014
From: gergg at cox.net (Gregg Wonderly)
Date: Fri, 19 Dec 2014 16:03:03 -0600
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <VZXY1p00s02hR0p01ZXcwu>
References: <1418997636377-11669.post@n7.nabble.com>
	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>
	<54943D9E.3010201@bbs.darktech.org>
	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
	<54945632.4050302@bbs.darktech.org> <54947FB3.7030501@gmail.com>
	<54948881.5020700@bbs.darktech.org>
	<54949457.7080602@gmail.com> <VZXY1p00s02hR0p01ZXcwu>
Message-ID: <77F21A00-19F1-468F-8DC2-2398310E195E@cox.net>

Of course, the typical solution is to throw some variant of RuntimeException to subvert the need to have all parts of the API declare the exception chain.  I like checked exceptions, and I really do wish that more people thought about how to document exceptional behavior in their APIs.   Many people seem to believe that there is only ?one? way to use there code, and thus believe they have taken care of what needs to be taken care of.

These APIs are largely designed around ?computational? activities which just throw RuntimeException for the most part.  The problem is, that ?compute only? in the ?internet age? is really a short sighted place to be.  All of the I/O necessities in today?s internet application development really requires an Exception path in all cases.  Developers will gripe and complain about it all day long, but once you really have developer, real apps, with I/O, you quickly learn that there are multiple places to deal with exceptions, and many times, at the very top of the call chain is where you need to ?abort? the work in progress because that?s the only place that has the database transaction to cancel in view, the calling environment to report an error to, and all the other cleanup knowledge.

Gregg Wonderly

> On Dec 19, 2014, at 3:30 PM, cowwoc <cowwoc at bbs.darktech.org> wrote:
> 
> On 19/12/2014 4:10 PM, Peter Levart wrote:
>> If you want to change the exceptional OR nonexceptional outcome of preceeding stage, then use handle() instead of whenComplete().
> 
> I can't.
> 
> If you handle() and attempt to re-throw the same exception, you will get:
> 
>     unreported exception Throwable; must be caught or declared to be thrown
> 
> I agree that whenComplete() is meant to act as a finally block (which removes the need to do this funky casting). What I don't like is that join() returns without waiting for the result of whenComplete(). If the case of a real try-finally block, the code after the block does not execute after finally completes. I am trying to implement the same behavior here.
> 
> What should I be doing instead?
> 
> Gili
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From peter.levart at gmail.com  Fri Dec 19 17:15:33 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 19 Dec 2014 23:15:33 +0100
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <549498DD.1060800@bbs.darktech.org>
References: <1418997636377-11669.post@n7.nabble.com>	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>	<54943D9E.3010201@bbs.darktech.org>	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
	<54945632.4050302@bbs.darktech.org> <54947FB3.7030501@gmail.com>
	<54948881.5020700@bbs.darktech.org> <54949457.7080602@gmail.com>
	<549498DD.1060800@bbs.darktech.org>
Message-ID: <5494A385.9010808@gmail.com>


On 12/19/2014 10:30 PM, cowwoc wrote:
> On 19/12/2014 4:10 PM, Peter Levart wrote:
>> If you want to change the exceptional OR nonexceptional outcome of 
>> preceeding stage, then use handle() instead of whenComplete().
>
> I can't.
>
> If you handle() and attempt to re-throw the same exception, you will get:
>
>     unreported exception Throwable; must be caught or declared to be 
> thrown

This problem has been discussed before on the list. See here:

http://cs.oswego.edu/pipermail/concurrency-interest/2014-August/012907.html

The basic principle is to wrap the Throwable as a cause into a 
CompletionException.

>
> I agree that whenComplete() is meant to act as a finally block (which 
> removes the need to do this funky casting). What I don't like is that 
> join() returns without waiting for the result of whenComplete(). If 
> the case of a real try-finally block, the code after the block does 
> not execute after finally completes. I am trying to implement the same 
> behavior here.
>
> What should I be doing instead?
>
> Gili

If you join the 1st CompletableFuture (cf1), it returns when cf1 
completes. The completion of cf1 triggers execution of the 2nd stage 
(cf2). If you join the cf2, it will return when cf2 (the whenComplete 
block) completes. Note that the program executes the stages in a 
background thread (requested with supplyAsync for the 1st stage). The 
same thread then executes following non-async stages. You are joining in 
main thread.

Regards, Peter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/f2405773/attachment.html>

From peter.levart at gmail.com  Fri Dec 19 17:16:40 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 19 Dec 2014 23:16:40 +0100
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <54949B2A.4080803@bbs.darktech.org>
References: <1418997636377-11669.post@n7.nabble.com>	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>	<54943D9E.3010201@bbs.darktech.org>	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
	<54945632.4050302@bbs.darktech.org> <54947FB3.7030501@gmail.com>
	<54948881.5020700@bbs.darktech.org> <54949457.7080602@gmail.com>
	<549498DD.1060800@bbs.darktech.org>
	<54949B2A.4080803@bbs.darktech.org>
Message-ID: <5494A3C8.1010305@gmail.com>


On 12/19/2014 10:39 PM, cowwoc wrote:
> On 19/12/2014 4:30 PM, cowwoc wrote:
>> On 19/12/2014 4:10 PM, Peter Levart wrote:
>>> If you want to change the exceptional OR nonexceptional outcome of 
>>> preceeding stage, then use handle() instead of whenComplete().
>>
>> I can't.
>>
>> If you handle() and attempt to re-throw the same exception, you will get:
>>
>>     unreported exception Throwable; must be caught or declared to be 
>> thrown
>>
>> I agree that whenComplete() is meant to act as a finally block (which 
>> removes the need to do this funky casting). What I don't like is that 
>> join() returns without waiting for the result of whenComplete(). If 
>> the case of a real try-finally block, the code after the block does 
>> not execute after finally completes. I am trying to implement the 
>> same behavior here.
>>
>> What should I be doing instead?
>
> Actually, I think I'm fine.
>
> I've got a function that returns 
> "CompletableFuture.supplyAsync(lambda).whenComplete(lambda)" and I 
> join() on that. So technically speaking, I believe this means I am 
> joining against the second stage (after whenComplete() has run). Is 
> that correct?
>
> Gili

Correct.

Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/9ad5df4b/attachment-0001.html>

From cowwoc at bbs.darktech.org  Fri Dec 19 17:47:22 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Fri, 19 Dec 2014 17:47:22 -0500
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <5494A385.9010808@gmail.com>
References: <1418997636377-11669.post@n7.nabble.com>	<CAHJZN-u8FwawQ2HzUVZL5fQz9qU_FCZ-XER2jqEePx7Lq_Ey9g@mail.gmail.com>	<54943D9E.3010201@bbs.darktech.org>	<CAHJZN-vdhZ8rZY_-qgB=3uhXOdXftfmjC+cr9Sv6_g0ntXC3UA@mail.gmail.com>
	<54945632.4050302@bbs.darktech.org> <54947FB3.7030501@gmail.com>
	<54948881.5020700@bbs.darktech.org> <54949457.7080602@gmail.com>
	<549498DD.1060800@bbs.darktech.org> <5494A385.9010808@gmail.com>
Message-ID: <5494AAFA.7070401@bbs.darktech.org>

On 19/12/2014 5:15 PM, Peter Levart wrote:
>
> On 12/19/2014 10:30 PM, cowwoc wrote:
>> On 19/12/2014 4:10 PM, Peter Levart wrote:
>>> If you want to change the exceptional OR nonexceptional outcome of 
>>> preceeding stage, then use handle() instead of whenComplete().
>>
>> I can't.
>>
>> If you handle() and attempt to re-throw the same exception, you will get:
>>
>>     unreported exception Throwable; must be caught or declared to be 
>> thrown
>
> This problem has been discussed before on the list. See here:
>
> http://cs.oswego.edu/pipermail/concurrency-interest/2014-August/012907.html
>
> The basic principle is to wrap the Throwable as a cause into a 
> CompletionException.

This is very useful, but why is this not documented in the Javadoc? It 
could be mentioned after (or as part of) the last bullet item in the 
class-level Javadoc.

I highly doubt anyone else will run into the aforementioned mailing list 
post. This information should be easier to find.

Thanks,
Gili
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141219/9272569d/attachment.html>

From dl at cs.oswego.edu  Sat Dec 20 07:19:02 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 20 Dec 2014 07:19:02 -0500
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <1418997636377-11669.post@n7.nabble.com>
References: <1418997636377-11669.post@n7.nabble.com>
Message-ID: <54956936.7020905@cs.oswego.edu>

On 12/19/2014 09:00 AM, cowwoc wrote:
> The Javadoc for CompletableFuture.whenComplete() reads:
>
> "If the supplied action itself encounters an exception, then the returned
> stage exceptionally completes with this exception unless this stage also
> completed exceptionally."

First, thanks very much to Peter Levart for suggesting how to get
the effects you wanted.

I agree that this is terse, but I don't think it is ambiguous given
the previous sentence of the javadoc, which tells you what the
"unless" is referring to:

"Returns a new CompletionStage with the same result or exception as this stage, 
that executes the given action when this stage completes."

The policy here is the same as in most other Java constructions.
The underlying notion is that a handler for one exception should
if at all possible locally arrange processing of any unrelated
exceptions it encounters. Otherwise there is no good way to
communicate them, so the usual policy is to preserve only the
outer "causal" exception. (See for example JLS "finally: specs
http://docs.oracle.com/javase/specs/jls/se7/html/jls-14.html#jls-14.20.2)
But when try-with-resources was introduced (which often hits
this problem), the Throwable.addSuppressed method was added to
provide a means of reporting them. As Peter pointed out, we
can't/shouldn't automate this, but it is usually a good idea.
On the other hand, as Peter noted, the "handle" method, that
allows arbitrary translations of results and/or exceptions, is
often a better fit in these cases.
(CompletionStage.whenComplete" is analogous to "finally",
CompletionStage.handle" is analogous to to "catch", and
CompletableFuture.exceptionally is a CompletableFuture-specific
simplification of "handle" that triggers only in the exception case,
otherwise preserving result.)

It might be helpful to develop a document discussing techniques
and suggested practices for dealing with exceptions in
CompletableFuture/CompletionStage designs.

-Doug


From suman_krec at yahoo.com  Sat Dec 20 09:04:23 2014
From: suman_krec at yahoo.com (suman shil)
Date: Sat, 20 Dec 2014 14:04:23 +0000 (UTC)
Subject: [concurrency-interest] Enforcing ordered execution of
	critical	sections?
In-Reply-To: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
Message-ID: <589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>

I have modified my solution to avoid notifyAll(). Let me know your feedback.

public class OrderedExecutor{ private int maxOrder; private int currentAllowedOrder; private Map<Integer, Object> map = new HashMap<Integer, Object>();? ? public OrderedExecutor(int n)? ? {? ? ? ?? this.maxOrder = n;?? ? ? ? this.currentAllowedOrder = 0;? ?  for(int i = 0 ; i < this.maxOrder ; i++)? ?  {? ?  map.put(i, new Object());? ?  }? ? }? ??? ? public ?Object execCriticalSectionInOrder(int order,? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Callable<Object> callable)? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? throws Exception? ? { if (order >= this.maxOrder) { throw new Exception("Exceeds maximum order "+ maxOrder); } ? while(order != currentAllowedOrder) { synchronized (this.map.get(order)) { this.map.get(order).wait();  } }  try {  return callable.call();  }? ? ? ?finally {? ? ? ? ? ? currentAllowedOrder = currentAllowedOrder+1; synchronized (this.map.get(order+1))? ? ? ? ? ? {? ? ? ? ? ? ? ? this.map.get(order+1).notify();? ? ? ? ? ? } ? ? ? ?}? ? }} ??

      From: Joe Bowbeer <joe.bowbeer at gmail.com>
 To: concurrency-interest <concurrency-interest at cs.oswego.edu> 
 Sent: Friday, December 19, 2014 3:33 AM
 Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
   
That would be "Tom" Cargill; link to paper:
http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html



On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
I frown on use of notify[All]/wait because they make the code hard to maintain.
In this case, with potentially lots of waiting threads, I would check out the "Specific Notification" pattern if I were determined to go the wait/notify route:
Tim Cargill's paper is dated but still worth reading.
Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ and Peter Haggar's article:?
http://www.ibm.com/developerworks/java/library/j-spnotif.html

On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
  Yes, no one said it is a good idea to always do that. When it is contended, most of the threads will wake up to only go back to sleep.
 
 The pattern you are after is usually called sequencer. You can see it used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe not that popular.
 
 The best solution will be lock-like, but the waiter nodes will contain the value they are waiting for - so only the specific threads get woken up. The solution with concurrent map is very similar, only with larger overhead from storing the index the thread is waiting for.
 
 
 Alex
 
 
 On 18/12/2014 20:21, Hanson Char wrote:
  
 Less overhead and simpler are a nice properties, even though at the expense of having to wake up all waiting threads just to find out the one with the right order to execute.? Still, this seems like a good tradeoff. 
  Thanks, Hanson  
 On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com> wrote: 
On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
 
 No, there is no difference. Peter didn't spot your entire method is synchronized, so spurious wakeup won't make progress until the owner of the lock exits the method.
 
 You could split the synchronization into two blocks - one encompassing the wait loop, the other in the finally block; but it may make no difference.
 
 Alex
 
 
 You're right, Alex. I'm so infected with park/unpark virus that I missed that ;-)
 
 Peter
 
 
 
 On 17/12/2014 18:36, suman shil wrote:
  
 Thanks peter for your reply. You are right. I should have incremented currentAllowedOrder in finally block.
 
 Suman
  ------------------------------------------------------------------------
 *From:* Peter Levart <peter.levart at gmail.com>
 *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <oleksandr.otenko at oracle.com>; Concurrency-interest <concurrency-interest at cs.oswego.edu>
 *Sent:* Wednesday, December 17, 2014 11:54 PM
 *Subject:* Re: [concurrency-interest] Enforcing ordered execution of critical sections?
 
 On 12/17/2014 06:46 PM, suman shil wrote:
 
 Thanks for your response. Will notifyAll() instead of notify() solve the problem?
 
 
 It will, but you should also account for "spurious" wake-ups. You should increment currentAllowedOrder only after return from callable.call (in finally block just before notifyAll()).
 
 Otherwise a nice solution - with minimal state, providing that not many threads meet at the same time...
 
 Regards, Peter
 
 
 RegardsSuman
 ? ? ? ?From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:oleksandr.otenko at oracle.com>
 ? To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>; Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:concurrency-interest at cs.oswego.edu>? ? Sent: Wednesday, December 17, 2014 9:55 PM
 ? Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
 ? ? ? There is no guarantee you'll ever hand over the control to the right thread upon notify()
 ? ? Alex
 
 On 17/12/2014 14:07, suman shil wrote:
 ? ? ? Hi, Following is my solution to solve this problem. Please let me know if I am missing something.
 ? ?public class OrderedExecutor {? private int currentAllowedOrder = 0;? private int maxLength = 0;? public OrderedExecutor(int n)? {? ? ? ? ? this.maxLength = n;? } public synchronized Object execCriticalSectionInOrder( int order, Callable<Object> callable)? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?throws Exception? { if (order >= maxLength)? {? throw new Exception("Exceeds maximum order "+ maxLength);? }? ? while(order != currentAllowedOrder)? {? wait();? }? ? try? { currentAllowedOrder = currentAllowedOrder+1;? return callable.call();? }? finally? {? notify();? }? } }
 ? ?Regards Suman
  ? ? ? ?From: Peter Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>
 ? To: Hanson Char<hanson.char at gmail.com> <mailto:hanson.char at gmail.com>? ? Cc: concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:concurrency-interest at cs.oswego.edu>? ? Sent: Sunday, December 14, 2014 11:01 PM
 ? Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
 ? ? ? ? ? On 12/14/2014 06:11 PM, Hanson Char wrote:
 ? ? ?Hi Peter,
 ? ?Thanks for this proposed idea of using LockSupport. This begs the question: which one would you choose if you had all three (correct)  implementation available?? (Semaphore, CountDownLatch, or LockSupport)?
 ? ?Regards, Hanson
 ? ? The Semaphore/CountDownLatch variants are equivalent if you don't need re-use. So any would do. They lack invalid-use detection. What happens if they are not used as intended? Semaphore variant acts differently than CountDownLatch variant. The low-level variant I? proposed detects invalid usage. So I would probably use this one. But the low level variant is harder to reason about it's correctness.  I think it is correct, but you should show it to somebody else to confirm this.
 ? ? Another question is whether you actually need this kind of synchronizer. Maybe if you explained what you are trying to achieve, somebody could have an idea how to do that even more elegantly...
 ? ? Regards, Peter
    ? ? ? ? ? ? ?On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>? wrote:
 ? ?Hi Hanson,
 ? ? This one is more low-level, but catches some invalid usages and is more resource-friendly:
 ? ? ? public class OrderedExecutor {
 ? ? ? ? public <T> T execCriticalSectionInOrder(
 ? ? ? ? ? final int order,
 ? ? ? ? ? final Supplier<T> criticalSection
 ? ? ? ) throws InterruptedException {
 ? ? ? ? ? if (order < 0) {
 ? ? ? ? ? ? ? ?throw new IllegalArgumentException("'order' should be >= 0");
 ? ? ? ? ? }
 ? ? ? ? ? if (order > 0) {
 ? ? ? ? ? ? ? waitForDone(order - 1);
 ? ? ? ? ? }
 ? ? ? ? ? try {
 ? ? ? ? ? ? ? return criticalSection.get();
 ? ? ? ? ? } finally {
 ? ? ? ? ? ? ? notifyDone(order);
 ? ? ? ? ? }
 ? ? ? }
 ? ? ? ? private static final Object DONE = new Object();
 ? ? ? private final ConcurrentMap<Integer, Object> signals = new ConcurrentHashMap<>();
 ? ? ? ? private void waitForDone(int order) throws InterruptedException {
 ? ? ? ? ? Object sig = signals.putIfAbsent(order, Thread.currentThread());
 ? ? ? ? ? if (sig != null && sig != DONE) {
 ? ? ? ? ? ? ? throw new IllegalStateException();
 ? ? ? ? ? }
 ? ? ? ? ? while (sig != DONE) {
 ? ? ? ? ? ? ? LockSupport.park();
 ? ? ? ? ? ? ? if (Thread.interrupted()) {
 ? ? ? ? ? ? ? ? ? throw new InterruptedException();
 ? ? ? ? ? ? ? }
 ? ? ? ? ? ? ? sig = signals.get(order);
 ? ? ? ? ? }
 ? ? ? }
 ? ? ? ? private void notifyDone(int order) {
 ? ? ? ? ? Object sig = signals.putIfAbsent(order, DONE);
 ? ? ? ? ? if (sig instanceof Thread) {
 ? ? ? ? ? ? ? if (!signals.replace(order, sig, DONE)) {
 ? ? ? ? ? ? ? ? ? throw new IllegalStateException();
 ? ? ? ? ? ? ? }
 ? ? ? ? ? ? ? LockSupport.unpark((Thread) sig);
 ? ? ? ? ? } else if (sig != null) {
 ? ? ? ? ? ? ? throw new IllegalStateException();
 ? ? ? ? ? }
 ? ? ? }
 ? }
 ? ? ? Regards, Peter
 ? ? On 12/14/2014 05:08 PM, Peter Levart wrote:
 ? ? ? ?On 12/14/2014 04:20 PM, Hanson Char wrote:
 ? ? ?Hi Peter,
 ? ?Thanks for the suggestion, and sorry about not being clear about one important? detail: "n" is not known a priori when constructing an OrderedExecutor.? Does this mean the use of CountDownLatch is ruled out?
 ? ? If you know at least the upper bound of 'n', it can be used with such 'n'. Otherwise something that dynamically re-sizes the array could be devised. Or you could simply use a ConcurrentHashMap instead of array where keys are 'order' values:
 ? ? ? public class OrderedExecutor<T> {
 ? ? ? ? private final ConcurrentMap<Integer, CountDownLatch> latches = new ConcurrentHashMap<>();
 ? ? ? ? public T execCriticalSectionInOrder(final int order,
 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? final Supplier<T> criticalSection) throws InterruptedException {
 ? ? ? ? ? if (order > 0) {
 ? ? ? ? ? ? ? latches.computeIfAbsent(order - 1, o -> new CountDownLatch(1)).await();
 ? ? ? ? ? }
 ? ? ? ? ? try {
 ? ? ? ? ? ? ? return criticalSection.get();
 ? ? ? ? ? } finally {
 ? ? ? ? ? ? ? latches.computeIfAbsent(order, o -> new CountDownLatch(1)).countDown();
 ? ? ? ? ? }
 ? ? ? }
 ? }
 ? ? ? Regards, Peter
 ? ? ? ? ? You guessed right: it's a one-shot object for a particular OrderedExecutor? instance, and "order" must be called indeed at most once.
 ? ?Regards, Hanson
    ? On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>? wrote:
 ? ?Hi Hanson,
 ? ? I don't think anything like that readily exists? in java.lang.concurrent, but what you describe should be possible to? achieve with composition of existing primitives.? You haven't given any additional hints to what your OrderedExecutor? should behave like. Should it be a one-shot object (like CountDownLatch) or a re-usable one (like CyclicBarrier)? Will execCriticalSectionInOrder() for a particular OrderedExecutor instance and 'order' value be called at most once? If yes (and I  think that only a one-shot object? makes sense here), an array of CountDownLatch(es) could be used:
 ? ? public class OrderedExecutor<T> {
 ? ? ? private final CountDownLatch[] latches;
 ? ? ? ? public OrderedExecutor(int n) {
 ? ? ? ? ? if (n < 1) throw new IllegalArgumentException("'n'? should be >= 1");
 ? ? ? ? ? latches = new CountDownLatch[n - 1];
 ? ? ? ? ? for (int i = 0; i < latches.length; i++) {
 ? ? ? ? ? ? ? latches[i] = new CountDownLatch(1);
 ? ? ? ? ? }
 ? ? ? }
 ? ? ? ? public T execCriticalSectionInOrder(final int order,
 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?final Supplier<T> criticalSection) throws InterruptedException {
 ? ? ? ? ? if (order < 0 || order > latches.length)
 ? ? ? ? ? ? ? throw new IllegalArgumentException("'order' should be [0..." +? latches.length + "]");
 ? ? ? ? ? if (order > 0) {
 ? ? ? ? ? ? ? latches[order - 1].await();
 ? ? ? ? ? }
 ? ? ? ? ? try {
 ? ? ? ? ? ? ? return criticalSection.get();
 ? ? ? ? ? } finally {
 ? ? ? ? ? ? ? if (order < latches.length) {
 ? ? ? ? ? ? ? ? ? latches[order].countDown();
 ? ? ? ? ? ? ? }
 ? ? ? ? ? }
 ? ? ? }
 ? }
 ? ? ? Regards, Peter
 ? ? On 12/14/2014 05:26 AM, Hanson Char wrote:
  ? ? ? ? Hi, I am looking for a construct that can? be used to efficiently enforce? ordered execution of multiple critical sections, each calling from a? different thread. The calling threads may run in? parallel and may call the execution method out of order. The? perceived construct would therefore be responsible for re-ordering the execution of those threads, so that their critical? sections (and only the critical section) will be executed in order. Would something? like the following API already exist? /** * Used to enforce ordered execution of critical sections calling from multiple *? threads, parking and unparking the? threads as necessary. */ public class OrderedExecutor<T> { /** * Executes a critical section at most once with the given order, parking * and? unparking the current thread as? necessary so that all critical * sections executed? by different threads using this? executor take place in * the order from 1 to n? consecutively. */ public T execCriticalSectionInOrder
 (? final int order, final Callable<T> criticalSection) throws InterruptedException; } Regards, Hanson _______________________________________________Concurrency-interest  mailing listConcurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> http://cs.oswego.edu/mailman/listinfo/concurrency-interest _______________________________________________
 ? Concurrency-interest mailing list
 ? Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
 ? http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 _______________________________________________
 Concurrency-interest mailing list
 Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 
 
 
 
 
 _______________________________________________
 Concurrency-interest mailing list
 Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 
 
 
 
 
 
 
 
   
 _______________________________________________
 Concurrency-interest mailing list
 Concurrency-interest at cs.oswego.edu
 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
   
   
 
 
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141220/39b3ebc5/attachment-0001.html>

From dl at cs.oswego.edu  Sat Dec 20 10:24:14 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 20 Dec 2014 10:24:14 -0500
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>
	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
Message-ID: <5495949E.1030500@cs.oswego.edu>

[CCing c-i list]

On 12/18/2014 02:33 PM, Benjamin Manes wrote:
> I'm starting down the long road of writing a JDK8-based cache - effectively a
>  rewrite of Guava's to resolve performance issues and the unbounded memory
> growth problem that occurs in highly concurrent usages. In the process, I
> found out that ConcurrentHashMap's computeIfAbsent is slower then it should
> be and a live-lock failure was reintroduced.
>
>
> The live-lock occurs when a recursive computation is performed for the same
> key. This is a failure that has occurred in practice, as I fixed a similar
> deadlock scenario in Guava when we were triaging bug. When feedback was asked
> for CHMv8 back in 2011, I caught the issue then and it was fixed [1].

The follow-up story on this (at some point briefly mentioned
in some post) is that when we moved to relying on builtin (reentrant)
bin locks without tryLock support, then only a subset of cross-bin
deadlocks/livelocks become detectable (because of reentrancy) and even
then with false-positives. So the most defensible plan is to instead
rely on general-purpose JVM tools/debuggers to help developers with this
along with any other liveness problems possible under arbitrary ifAbsent
functions. On the other hand, we didn't want to rule out all
special-case detection: If an efficient internal tryLock
intrinsic became available, we might do more, because throwing here
helps diagnose unrelated-looking problems in other threads,
and performing checks only when lock is unavailable costs almost nothing.
The main undesirable byproduct is that because the method spec
still mentions the possibility of IllegalStateExceptions, some users
expect them in cases that are not currently handled. (There is an
openJDK bug report on this (JDK-8062841) that I'm not sure what
to do about.)

> Performance-wise, computeIfAbsent pessimistically locks instead of
> optimistically trying to return an existing entry.

There are lots of tradeoffs. With the current implementation,
if you are implementing a cache, it may be better to code cache.get
to itself do a pre-screen, as in:
   V v = map.get(key);
   return (v != null) ? v : map.computeIfAbsent(key, function);

However, the exact benefit depends on access patterns.
For example, I reran your benchmark cases (urls below) on a
32way x86, and got throughputs (ops/sec) that are dramatically
better with pre-screen for the case of a single key,
but worse with your Zipf-distributed keys. As an intermediate
test, I measured the impact of adding a single-node prescreen
("1cif") before locking inside CHM.computeIfAbsent, that is similar
to what was done in some pre-release versions:

Same key

cif:        1402559
get+cif: 3775886700
1cif:    1760916148

Zipf-distributed keys

cif:     1414945003
get+cif:  882477874
1cif:     618668961

One might think (I did until running similar experiments)
that the "1cif" version would be the best compromise.
But currently it isn't.
This is in part due to interactions with biased locking,
that in some cases basically provide "free" prescreens, but
in other cases add safepoint/GC pressure in addition
to lock contention. This is all up for re-consideration though.

-Doug


> I did this (2008) in a JCiP-style map of futures, later Guava did the same on
> its CHM fork, and even Java does this in CSLM and ConcurrentMap's default
> method. When the entry's lock is not contended the impact is minor, but hot
> entries in a cache suffer unnecessarily. The results of a JMH benchmark [3]
> below shows a 10x gain when adding this check (Caffeine). Admittedly the
> impact is minor for applications as Guava's performance is not much better,
> but that is why infrastructure projects still use ConcurrentLinkedHashMap for
> performance sensitive code even though Guava was intended to be its
> successor.
>
>
> [1]
> http://cs.oswego.edu/pipermail/concurrency-interest/2011-August/008188.html
>
> [2]
> https://github.com/ben-manes/caffeine/blob/master/src/test/java/com/github/benmanes/caffeine/cache/ComputingTest.java
>
>  [3]
> https://github.com/ben-manes/caffeine/blob/master/src/jmh/java/com/github/benmanes/caffeine/cache/ComputeBenchmark.java
>
>
>
> Benchmark                                       (computingType)   Mode
> Samples Score          Error  Units
>
> c.g.b.c.c.ComputeBenchmark.compute_sameKey    ConcurrentHashMap  thrpt
> 10 17729056.323 ?   557476.404  ops/s
>
> c.g.b.c.c.ComputeBenchmark.compute_sameKey             Caffeine  thrpt
> 10 347007236.316 ? 24370724.293  ops/s
>
> c.g.b.c.c.ComputeBenchmark.compute_sameKey                Guava  thrpt
> 10 29712031.905 ?   272916.744  ops/s
>
> c.g.b.c.c.ComputeBenchmark.compute_spread     ConcurrentHashMap  thrpt
> 10 104565034.688 ?  4207350.038  ops/s
>
> c.g.b.c.c.ComputeBenchmark.compute_spread              Caffeine  thrpt
> 10 132953599.579 ? 13705263.521  ops/s
>
> c.g.b.c.c.ComputeBenchmark.compute_spread                 Guava  thrpt
> 10 61794001.850 ?  1864056.437  ops/s
>
>




From cowwoc at bbs.darktech.org  Sat Dec 20 10:28:46 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Sat, 20 Dec 2014 08:28:46 -0700 (MST)
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <54956936.7020905@cs.oswego.edu>
References: <1418997636377-11669.post@n7.nabble.com>
	<54956936.7020905@cs.oswego.edu>
Message-ID: <54959818.6050801@bbs.darktech.org>

On 20/12/2014 7:19 AM, Doug Lea [via JSR166 Concurrency] wrote:
> On 12/19/2014 09:00 AM, cowwoc wrote:
> > The Javadoc for CompletableFuture.whenComplete() reads:
> >
> > "If the supplied action itself encounters an exception, then the 
> returned
> > stage exceptionally completes with this exception unless this stage 
> also
> > completed exceptionally."
>
> First, thanks very much to Peter Levart for suggesting how to get
> the effects you wanted.
>
> I agree that this is terse, but I don't think it is ambiguous given
> the previous sentence of the javadoc, which tells you what the
> "unless" is referring to:
>
> "Returns a new CompletionStage with the same result or exception as 
> this stage,
> that executes the given action when this stage completes."

Doug, the "unless" and the paragraph you referred to are very distant 
from one another. It's hard to draw the conclusion you mentioned unless 
"unless" is one sentence away from what it is referring to. I suggest 
changing:

     "If the supplied action itself encounters an exception, then the 
returned stage exceptionally completes with this exception unless this 
stage also completed exceptionally."

to:

     "If the supplied action itself encounters an exception, then the 
returned stage exceptionally completes with this exception unless this 
stage also completed exceptionally (in which case, the returned stage 
exceptionally completes with the original exception)."

> The policy here is the same as in most other Java constructions.
> The underlying notion is that a handler for one exception should
> if at all possible locally arrange processing of any unrelated
> exceptions it encounters. Otherwise there is no good way to
> communicate them, so the usual policy is to preserve only the
> outer "causal" exception. (See for example JLS "finally: specs
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-14.html#jls-14.20.2)
> But when try-with-resources was introduced (which often hits
> this problem), the Throwable.addSuppressed method was added to
> provide a means of reporting them. As Peter pointed out, we
> can't/shouldn't automate this, but it is usually a good idea.
> On the other hand, as Peter noted, the "handle" method, that
> allows arbitrary translations of results and/or exceptions, is
> often a better fit in these cases.
> (CompletionStage.whenComplete" is analogous to "finally",
> CompletionStage.handle" is analogous to to "catch", and
> CompletableFuture.exceptionally is a CompletableFuture-specific
> simplification of "handle" that triggers only in the exception case,
> otherwise preserving result.)
>
> It might be helpful to develop a document discussing techniques
> and suggested practices for dealing with exceptions in
> CompletableFuture/CompletionStage designs.
>
> -Doug

I would actually love it for whenComplete() to behave like finally() 
because if it did it would throw exception2.addSuppressed(exception1) 
instead of exception1. Would it not? I think this would be a lot less 
confusing and would prevent the kind of information loss we are 
currently seeing.

In any case, It would help if the Javadoc provided this information or 
linked to a document that did. It would be great to read that 
"whenComplete" is analogous to "finally", "handle" is analogous to 
"catch" and so on. And also to read that one can use CompletionException 
to wrap checked exceptions without having to do any manual unwrapping 
later on.

PS: When is the online Javadoc updated? If you look at 
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#whenComplete-java.util.function.BiConsumer- 
you will notice that it doesn't contain any of this information. I'm 
guessing that it corresponds to Java 8 update 0.

Thanks,
Gili




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11686.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141220/bcf0134d/attachment.html>

From cowwoc at bbs.darktech.org  Sat Dec 20 12:57:19 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Sat, 20 Dec 2014 10:57:19 -0700 (MST)
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <54956936.7020905@cs.oswego.edu>
References: <1418997636377-11669.post@n7.nabble.com>
	<54956936.7020905@cs.oswego.edu>
Message-ID: <5495BAF1.5000401@bbs.darktech.org>

On 20/12/2014 10:39 AM, cowwoc wrote:
> I would actually love it for whenComplete() to behave like finally() 
> because if it did it would throw exception2.addSuppressed(exception1) 
> instead of exception1. Would it not? I think this would be a lot less 
> confusing and would prevent the kind of information loss we are 
> currently seeing.
>
> In any case, It would help if the Javadoc provided this information or 
> linked to a document that did. It would be great to read that 
> "whenComplete" is analogous to "finally", "handle" is analogous to 
> "catch" and so on. And also to read that one can use 
> CompletionException to wrap checked exceptions without having to do 
> any manual unwrapping later on.
>
> PS: When is the online Javadoc updated? If you look at 
> http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#whenComplete-java.util.function.BiConsumer- 
> you will notice that it doesn't contain any of this information. I'm 
> guessing that it corresponds to Java 8 update 0.
>
> Thanks,
> Gili

Now that I think about it: if the original Java8 specification did not 
mandate what should happen if the action throws an exception, does that 
mean we can change that specification now with minimal harm? Meaning, is 
there a chance we can mandate exception2.addSuppressed(exception1) be 
returned?

When was the Javadoc updated with the paragraph in question? Was it recent?

Gili




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11688.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141220/cbc7ebcd/attachment.html>

From dl at cs.oswego.edu  Sat Dec 20 13:24:24 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 20 Dec 2014 13:24:24 -0500
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <54959818.6050801@bbs.darktech.org>
References: <1418997636377-11669.post@n7.nabble.com>	<54956936.7020905@cs.oswego.edu>
	<54959818.6050801@bbs.darktech.org>
Message-ID: <5495BED8.2060703@cs.oswego.edu>

On 12/20/2014 10:28 AM, cowwoc wrote:

> "unless" is one sentence away from what it is referring to. I suggest
> changing:
>
>       "If the supplied action itself encounters an exception, then the
> returned stage exceptionally completes with this exception unless this
> stage also completed exceptionally."
>
> to:
>
>       "If the supplied action itself encounters an exception, then the
> returned stage exceptionally completes with this exception unless this
> stage also completed exceptionally (in which case, the returned stage
> exceptionally completes with the original exception)."

Thanks. This should save other people some problems in decoding.
(This will probably take a while to propagate to OpenJDK.)

> I would actually love it for whenComplete() to behave like finally()

It does!

> because if it did it would throw exception2.addSuppressed(exception1)
> instead of exception1. Would it not?

Not. This is only done in try-with-resources.
See the (adjacent) JLS sections:

http://docs.oracle.com/javase/specs/jls/se7/html/jls-14.html#jls-14.20.2

http://docs.oracle.com/javase/specs/jls/se7/html/jls-14.html#jls-14.20.3

I think that the best we can do here is to echo the standard strong
advice to localize handling of exceptions thrown in exception handlers,
and when this is impossible to use addSuppressed to alert downstream
completions of unrelated exceptions while handling the reported
exceptions. IDEs and tools might be helpful in spreading this advice.

> PS: When is the online Javadoc updated?

The only answer I know is "eventually" :-)

-Doug


From joe.bowbeer at gmail.com  Sat Dec 20 14:21:25 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 20 Dec 2014 11:21:25 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
Message-ID: <CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>

Suman,

I would advise against using notify/wait.  It raises a red flag for a lot
of reviewers, including me.

The problems I see in this implementation are:

1. Pre-allocation of locks is prohibited by (revised) problem statement.

Note that if pre-allocation were allowed, then an array would be more
efficient than a Map.

2. Access to currentAllowedOrder is not thread-safe but should be.


On Sat, Dec 20, 2014 at 6:04 AM, suman shil <suman_krec at yahoo.com> wrote:
>
> I have modified my solution to avoid notifyAll(). Let me know your
> feedback.
>
>
> public class OrderedExecutor
> {
> private int maxOrder;
> private int currentAllowedOrder;
> private Map<Integer, Object> map = new HashMap<Integer, Object>();
>     public OrderedExecutor(int n)
>     {
>          this.maxOrder = n;
>          this.currentAllowedOrder = 0;
>     for(int i = 0 ; i < this.maxOrder ; i++)
>     {
>     map.put(i, new Object());
>     }
>     }
>
>     public  Object execCriticalSectionInOrder(int order,
>                                               Callable<Object> callable)
>                                               throws Exception
>     {
> if (order >= this.maxOrder)
> {
> throw new Exception("Exceeds maximum order "+ maxOrder);
> }
>
> while(order != currentAllowedOrder)
> {
> synchronized (this.map.get(order))
> {
> this.map.get(order).wait();
> }
> }
>  try
> {
> return callable.call();
> }
>        finally
> {
>             currentAllowedOrder = currentAllowedOrder+1;
> synchronized (this.map.get(order+1))
>             {
>                 this.map.get(order+1).notify();
>             }
>        }
>     }
> }
>
>
>   ------------------------------
>  *From:* Joe Bowbeer <joe.bowbeer at gmail.com>
> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
> *Sent:* Friday, December 19, 2014 3:33 AM
>
> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
> critical sections?
>
> That would be "Tom" Cargill; link to paper:
>
> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>
>
>
> On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
> wrote:
>
> I frown on use of notify[All]/wait because they make the code hard to
> maintain.
>
> In this case, with potentially lots of waiting threads, I would check out
> the "Specific Notification" pattern if I were determined to go the
> wait/notify route:
>
> Tim Cargill's paper is dated but still worth reading.
>
> Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ and Peter
> Haggar's article:
>
> http://www.ibm.com/developerworks/java/library/j-spnotif.html
>
> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>  Yes, no one said it is a good idea to always do that. When it is
> contended, most of the threads will wake up to only go back to sleep.
>
> The pattern you are after is usually called sequencer. You can see it used
> in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe not that
> popular.
>
> The best solution will be lock-like, but the waiter nodes will contain the
> value they are waiting for - so only the specific threads get woken up. The
> solution with concurrent map is very similar, only with larger overhead
> from storing the index the thread is waiting for.
>
>
> Alex
>
>
>
> On 18/12/2014 20:21, Hanson Char wrote:
>
> Less overhead and simpler are a nice properties, even though at the
> expense of having to wake up all waiting threads just to find out the one
> with the right order to execute.  Still, this seems like a good tradeoff.
>
>  Thanks,
> Hanson
>
> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com>
> wrote:
>
> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>
> No, there is no difference. Peter didn't spot your entire method is
> synchronized, so spurious wakeup won't make progress until the owner of the
> lock exits the method.
>
> You could split the synchronization into two blocks - one encompassing the
> wait loop, the other in the finally block; but it may make no difference.
>
> Alex
>
>
> You're right, Alex. I'm so infected with park/unpark virus that I missed
> that ;-)
>
> Peter
>
>
> On 17/12/2014 18:36, suman shil wrote:
>
> Thanks peter for your reply. You are right. I should have incremented
> currentAllowedOrder in finally block.
>
> Suman
>  ------------------------------------------------------------------------
> *From:* Peter Levart <peter.levart at gmail.com>
> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
> oleksandr.otenko at oracle.com>; Concurrency-interest <
> concurrency-interest at cs.oswego.edu>
> *Sent:* Wednesday, December 17, 2014 11:54 PM
> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
> critical sections?
>
> On 12/17/2014 06:46 PM, suman shil wrote:
>
> Thanks for your response. Will notifyAll() instead of notify() solve the
> problem?
>
>
> It will, but you should also account for "spurious" wake-ups. You should
> increment currentAllowedOrder only after return from callable.call (in
> finally block just before notifyAll()).
>
> Otherwise a nice solution - with minimal state, providing that not many
> threads meet at the same time...
>
> Regards, Peter
>
>  RegardsSuman
>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:
> oleksandr.otenko at oracle.com>
>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>;
> Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December 17, 2014
> 9:55 PM
>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
> critical sections?
>       There is no guarantee you'll ever hand over the control to the right
> thread upon notify()
>     Alex
>
> On 17/12/2014 14:07, suman shil wrote:
>       Hi, Following is my solution to solve this problem. Please let me
> know if I am missing something.
>    public class OrderedExecutor {  private int currentAllowedOrder = 0;
> private int maxLength = 0;  public OrderedExecutor(int n)  {
> this.maxLength = n;  } public synchronized Object
> execCriticalSectionInOrder( int order, Callable<Object> callable)
>                        throws Exception  { if (order >= maxLength)  {
> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
> }  } }
>    Regards Suman
>         From: Peter Levart<peter.levart at gmail.com> <mailto:
> peter.levart at gmail.com>
>   To: Hanson Char<hanson.char at gmail.com> <mailto:hanson.char at gmail.com>
>   Cc: concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14, 2014
> 11:01 PM
>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
> critical sections?
>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>      Hi Peter,
>    Thanks for this proposed idea of using LockSupport. This begs the
> question: which one would you choose if you had all three (correct)
> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>    Regards, Hanson
>     The Semaphore/CountDownLatch variants are equivalent if you don't need
> re-use. So any would do. They lack invalid-use detection. What happens if
> they are not used as intended? Semaphore variant acts differently than
> CountDownLatch variant. The low-level variant I  proposed detects invalid
> usage. So I would probably use this one. But the low level variant is
> harder to reason about it's correctness. I think it is correct, but you
> should show it to somebody else to confirm this.
>     Another question is whether you actually need this kind of
> synchronizer. Maybe if you explained what you are trying to achieve,
> somebody could have an idea how to do that even more elegantly...
>     Regards, Peter
>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>    Hi Hanson,
>     This one is more low-level, but catches some invalid usages and is
> more resource-friendly:
>       public class OrderedExecutor {
>         public <T> T execCriticalSectionInOrder(
>           final int order,
>           final Supplier<T> criticalSection
>       ) throws InterruptedException {
>           if (order < 0) {
>                throw new IllegalArgumentException("'order' should be >=
> 0");
>           }
>           if (order > 0) {
>               waitForDone(order - 1);
>           }
>           try {
>               return criticalSection.get();
>           } finally {
>               notifyDone(order);
>           }
>       }
>         private static final Object DONE = new Object();
>       private final ConcurrentMap<Integer, Object> signals = new
> ConcurrentHashMap<>();
>         private void waitForDone(int order) throws InterruptedException {
>           Object sig = signals.putIfAbsent(order, Thread.currentThread());
>           if (sig != null && sig != DONE) {
>               throw new IllegalStateException();
>           }
>           while (sig != DONE) {
>               LockSupport.park();
>               if (Thread.interrupted()) {
>                   throw new InterruptedException();
>               }
>               sig = signals.get(order);
>           }
>       }
>         private void notifyDone(int order) {
>           Object sig = signals.putIfAbsent(order, DONE);
>           if (sig instanceof Thread) {
>               if (!signals.replace(order, sig, DONE)) {
>                   throw new IllegalStateException();
>               }
>               LockSupport.unpark((Thread) sig);
>           } else if (sig != null) {
>               throw new IllegalStateException();
>           }
>       }
>   }
>       Regards, Peter
>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>      Hi Peter,
>    Thanks for the suggestion, and sorry about not being clear about one
> important  detail: "n" is not known a priori when constructing an
> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>     If you know at least the upper bound of 'n', it can be used with such
> 'n'. Otherwise something that dynamically re-sizes the array could be
> devised. Or you could simply use a ConcurrentHashMap instead of array where
> keys are 'order' values:
>       public class OrderedExecutor<T> {
>         private final ConcurrentMap<Integer, CountDownLatch> latches = new
> ConcurrentHashMap<>();
>         public T execCriticalSectionInOrder(final int order,
>                                           final Supplier<T>
> criticalSection) throws InterruptedException {
>           if (order > 0) {
>               latches.computeIfAbsent(order - 1, o -> new
> CountDownLatch(1)).await();
>           }
>           try {
>               return criticalSection.get();
>           } finally {
>               latches.computeIfAbsent(order, o -> new
> CountDownLatch(1)).countDown();
>           }
>       }
>   }
>       Regards, Peter
>           You guessed right: it's a one-shot object for a particular
> OrderedExecutor  instance, and "order" must be called indeed at most once.
>    Regards, Hanson
>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<peter.levart at gmail.com>
> <mailto:peter.levart at gmail.com>  wrote:
>    Hi Hanson,
>     I don't think anything like that readily exists  in
> java.lang.concurrent, but what you describe should be possible to  achieve
> with composition of existing primitives.  You haven't given any additional
> hints to what your OrderedExecutor  should behave like. Should it be a
> one-shot object (like CountDownLatch) or a re-usable one (like
> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
> OrderedExecutor instance and 'order' value be called at most once? If yes
> (and I think that only a one-shot object  makes sense here), an array of
> CountDownLatch(es) could be used:
>     public class OrderedExecutor<T> {
>       private final CountDownLatch[] latches;
>         public OrderedExecutor(int n) {
>           if (n < 1) throw new IllegalArgumentException("'n'  should be >=
> 1");
>           latches = new CountDownLatch[n - 1];
>           for (int i = 0; i < latches.length; i++) {
>               latches[i] = new CountDownLatch(1);
>           }
>       }
>         public T execCriticalSectionInOrder(final int order,
>                                            final Supplier<T>
> criticalSection) throws InterruptedException {
>           if (order < 0 || order > latches.length)
>               throw new IllegalArgumentException("'order' should be [0..."
> +  latches.length + "]");
>           if (order > 0) {
>               latches[order - 1].await();
>           }
>           try {
>               return criticalSection.get();
>           } finally {
>               if (order < latches.length) {
>                   latches[order].countDown();
>               }
>           }
>       }
>   }
>       Regards, Peter
>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>          Hi, I am looking for a construct that can  be used to efficiently
> enforce  ordered execution of multiple critical sections, each calling from
> a  different thread. The calling threads may run in  parallel and may call
> the execution method out of order. The  perceived construct would therefore
> be responsible for re-ordering the execution of those threads, so that
> their critical  sections (and only the critical section) will be executed
> in order. Would something  like the following API already exist? /** * Used
> to enforce ordered execution of critical sections calling from multiple *
> threads, parking and unparking the  threads as necessary. */ public class
> OrderedExecutor<T> { /** * Executes a critical section at most once with
> the given order, parking * and  unparking the current thread as  necessary
> so that all critical * sections executed  by different threads using this
> executor take place in * the order from 1 to n  consecutively. */ public T
> execCriticalSectionInOrder
> (  final int order, final Callable<T> criticalSection) throws
> InterruptedException; } Regards, Hanson
> _______________________________________________Concurrency-interest mailing
> listConcurrency-interest at cs.oswego.edu <mailto:
> Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
>   Concurrency-interest mailing list
>   Concurrency-interest at cs.oswego.edu <mailto:
> Concurrency-interest at cs.oswego.edu>
>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:
> Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:
> Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141220/db9267bd/attachment-0001.html>

From ben.manes at gmail.com  Sat Dec 20 16:11:44 2014
From: ben.manes at gmail.com (Benjamin Manes)
Date: Sat, 20 Dec 2014 13:11:44 -0800
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <5495949E.1030500@cs.oswego.edu>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>
	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
	<5495949E.1030500@cs.oswego.edu>
Message-ID: <CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>

>
> If an efficient internal tryLock intrinsic became available, we might do
> more, because throwing here helps diagnose unrelated-looking problems in
> other threads, and performing checks only when lock is unavailable costs
> almost nothing.


Is the tryLock intrinsic necessary to be performant and safe? I tried using
Thread.holdsLock() and it halved the performance. I then tried stashing the
thread's id into an extra field only stored on ReservationNode and saw
equal performance on my 4-core/8HT macbook. Does this perform much worse on
your 32-way machine? (See ConcurrentHashMap2 in repository).

Benchmark                                        (computingType)   Mode
 Samples          Score         Error  Units
c.g.b.c.c.ComputeBenchmark.compute_sameKey     ConcurrentHashMap  thrpt
  10   18324025.789 ? 1030272.256  ops/s
c.g.b.c.c.ComputeBenchmark.compute_sameKey    ConcurrentHashMap2  thrpt
  10   19427762.264 ?  393697.377  ops/s
c.g.b.c.c.ComputeBenchmark.compute_spread      ConcurrentHashMap  thrpt
  10  102832301.745 ? 4022095.932  ops/s
c.g.b.c.c.ComputeBenchmark.compute_spread     ConcurrentHashMap2  thrpt
  10  103014534.166 ? 3176520.706  ops/s


On Sat, Dec 20, 2014 at 7:24 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> [CCing c-i list]
>
> On 12/18/2014 02:33 PM, Benjamin Manes wrote:
>
>> I'm starting down the long road of writing a JDK8-based cache -
>> effectively a
>>  rewrite of Guava's to resolve performance issues and the unbounded memory
>> growth problem that occurs in highly concurrent usages. In the process, I
>> found out that ConcurrentHashMap's computeIfAbsent is slower then it
>> should
>> be and a live-lock failure was reintroduced.
>>
>>
>> The live-lock occurs when a recursive computation is performed for the
>> same
>> key. This is a failure that has occurred in practice, as I fixed a similar
>> deadlock scenario in Guava when we were triaging bug. When feedback was
>> asked
>> for CHMv8 back in 2011, I caught the issue then and it was fixed [1].
>>
>
> The follow-up story on this (at some point briefly mentioned
> in some post) is that when we moved to relying on builtin (reentrant)
> bin locks without tryLock support, then only a subset of cross-bin
> deadlocks/livelocks become detectable (because of reentrancy) and even
> then with false-positives. So the most defensible plan is to instead
> rely on general-purpose JVM tools/debuggers to help developers with this
> along with any other liveness problems possible under arbitrary ifAbsent
> functions. On the other hand, we didn't want to rule out all
> special-case detection: If an efficient internal tryLock
> intrinsic became available, we might do more, because throwing here
> helps diagnose unrelated-looking problems in other threads,
> and performing checks only when lock is unavailable costs almost nothing.
> The main undesirable byproduct is that because the method spec
> still mentions the possibility of IllegalStateExceptions, some users
> expect them in cases that are not currently handled. (There is an
> openJDK bug report on this (JDK-8062841) that I'm not sure what
> to do about.)
>
>  Performance-wise, computeIfAbsent pessimistically locks instead of
>> optimistically trying to return an existing entry.
>>
>
> There are lots of tradeoffs. With the current implementation,
> if you are implementing a cache, it may be better to code cache.get
> to itself do a pre-screen, as in:
>   V v = map.get(key);
>   return (v != null) ? v : map.computeIfAbsent(key, function);
>
> However, the exact benefit depends on access patterns.
> For example, I reran your benchmark cases (urls below) on a
> 32way x86, and got throughputs (ops/sec) that are dramatically
> better with pre-screen for the case of a single key,
> but worse with your Zipf-distributed keys. As an intermediate
> test, I measured the impact of adding a single-node prescreen
> ("1cif") before locking inside CHM.computeIfAbsent, that is similar
> to what was done in some pre-release versions:
>
> Same key
>
> cif:        1402559
> get+cif: 3775886700
> 1cif:    1760916148
>
> Zipf-distributed keys
>
> cif:     1414945003
> get+cif:  882477874
> 1cif:     618668961
>
> One might think (I did until running similar experiments)
> that the "1cif" version would be the best compromise.
> But currently it isn't.
> This is in part due to interactions with biased locking,
> that in some cases basically provide "free" prescreens, but
> in other cases add safepoint/GC pressure in addition
> to lock contention. This is all up for re-consideration though.
>
> -Doug
>
>
>
>  I did this (2008) in a JCiP-style map of futures, later Guava did the
>> same on
>> its CHM fork, and even Java does this in CSLM and ConcurrentMap's default
>> method. When the entry's lock is not contended the impact is minor, but
>> hot
>> entries in a cache suffer unnecessarily. The results of a JMH benchmark
>> [3]
>> below shows a 10x gain when adding this check (Caffeine). Admittedly the
>> impact is minor for applications as Guava's performance is not much
>> better,
>> but that is why infrastructure projects still use ConcurrentLinkedHashMap
>> for
>> performance sensitive code even though Guava was intended to be its
>> successor.
>>
>>
>> [1]
>> http://cs.oswego.edu/pipermail/concurrency-interest/2011-August/008188.
>> html
>>
>> [2]
>> https://github.com/ben-manes/caffeine/blob/master/src/test/
>> java/com/github/benmanes/caffeine/cache/ComputingTest.java
>>
>>  [3]
>> https://github.com/ben-manes/caffeine/blob/master/src/jmh/
>> java/com/github/benmanes/caffeine/cache/ComputeBenchmark.java
>>
>>
>>
>> Benchmark                                       (computingType)   Mode
>> Samples Score          Error  Units
>>
>> c.g.b.c.c.ComputeBenchmark.compute_sameKey    ConcurrentHashMap  thrpt
>> 10 17729056.323 ?   557476.404  ops/s
>>
>> c.g.b.c.c.ComputeBenchmark.compute_sameKey             Caffeine  thrpt
>> 10 347007236.316 ? 24370724.293  ops/s
>>
>> c.g.b.c.c.ComputeBenchmark.compute_sameKey                Guava  thrpt
>> 10 29712031.905 ?   272916.744  ops/s
>>
>> c.g.b.c.c.ComputeBenchmark.compute_spread     ConcurrentHashMap  thrpt
>> 10 104565034.688 ?  4207350.038  ops/s
>>
>> c.g.b.c.c.ComputeBenchmark.compute_spread              Caffeine  thrpt
>> 10 132953599.579 ? 13705263.521  ops/s
>>
>> c.g.b.c.c.ComputeBenchmark.compute_spread                 Guava  thrpt
>> 10 61794001.850 ?  1864056.437  ops/s
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141220/33488e21/attachment.html>

From joe.bowbeer at gmail.com  Sat Dec 20 18:12:18 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 20 Dec 2014 15:12:18 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
Message-ID: <CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>

To round out the selection, here's an implementation using a PriorityQueue
of Conditions:

class OrderedExecutor<T> {

  static class Waiter implements Comparable<Waiter> {

    final int order;
    final Condition condition;

    Waiter(int order, Condition condition) {
      this.order = order;
      this.condition = condition;
    }

    @Override
    public int compareTo(Waiter waiter) {
      return order - waiter.order;
    }
  }

  final Lock lock = new ReentrantLock();
  final Queue<Waiter> queue = new PriorityQueue<>();
  int nextOrder; // 0 is next

  public T execCallableInOrder(int order, Callable<T> callable) throws
Exception {
    assert order >= 0;
    awaitTurn(order);
    try {
      return callable.call();
    } finally {
      signalNext(order + 1);
    }
  }

  void awaitTurn(int order) {
    lock.lock();
    try {
      Condition condition = null;
      while (nextOrder != order) {
        if (condition == null) {
          condition = lock.newCondition();
          queue.add(new Waiter(order, condition));
        }
        condition.awaitUninterruptibly();
      }
    } finally {
      lock.unlock();
    }
  }

  void signalNext(int nextOrder) {
    lock.lock();
    try {
      this.nextOrder = nextOrder;
      Waiter waiter = queue.peek();
      if (waiter != null && waiter.order == nextOrder) {
        queue.remove();
        waiter.condition.signal();
      }
    } finally {
      lock.unlock();
    }
  }
}

On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>
> Suman,
>
> I would advise against using notify/wait.  It raises a red flag for a lot
> of reviewers, including me.
>
> The problems I see in this implementation are:
>
> 1. Pre-allocation of locks is prohibited by (revised) problem statement.
>
> Note that if pre-allocation were allowed, then an array would be more
> efficient than a Map.
>
> 2. Access to currentAllowedOrder is not thread-safe but should be.
>
>
> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <suman_krec at yahoo.com> wrote:
>>
>> I have modified my solution to avoid notifyAll(). Let me know your
>> feedback.
>>
>>
>> public class OrderedExecutor
>> {
>> private int maxOrder;
>> private int currentAllowedOrder;
>> private Map<Integer, Object> map = new HashMap<Integer, Object>();
>>     public OrderedExecutor(int n)
>>     {
>>          this.maxOrder = n;
>>          this.currentAllowedOrder = 0;
>>     for(int i = 0 ; i < this.maxOrder ; i++)
>>     {
>>     map.put(i, new Object());
>>     }
>>     }
>>
>>     public  Object execCriticalSectionInOrder(int order,
>>                                               Callable<Object> callable)
>>                                               throws Exception
>>     {
>> if (order >= this.maxOrder)
>> {
>> throw new Exception("Exceeds maximum order "+ maxOrder);
>> }
>>
>> while(order != currentAllowedOrder)
>> {
>> synchronized (this.map.get(order))
>> {
>> this.map.get(order).wait();
>> }
>> }
>>  try
>> {
>> return callable.call();
>> }
>>        finally
>> {
>>             currentAllowedOrder = currentAllowedOrder+1;
>> synchronized (this.map.get(order+1))
>>             {
>>                 this.map.get(order+1).notify();
>>             }
>>        }
>>     }
>> }
>>
>>
>>   ------------------------------
>>  *From:* Joe Bowbeer <joe.bowbeer at gmail.com>
>> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
>> *Sent:* Friday, December 19, 2014 3:33 AM
>>
>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>> critical sections?
>>
>> That would be "Tom" Cargill; link to paper:
>>
>> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>>
>>
>>
>> On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>> wrote:
>>
>> I frown on use of notify[All]/wait because they make the code hard to
>> maintain.
>>
>> In this case, with potentially lots of waiting threads, I would check out
>> the "Specific Notification" pattern if I were determined to go the
>> wait/notify route:
>>
>> Tim Cargill's paper is dated but still worth reading.
>>
>> Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ and
>> Peter Haggar's article:
>>
>> http://www.ibm.com/developerworks/java/library/j-spnotif.html
>>
>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>  Yes, no one said it is a good idea to always do that. When it is
>> contended, most of the threads will wake up to only go back to sleep.
>>
>> The pattern you are after is usually called sequencer. You can see it
>> used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe not
>> that popular.
>>
>> The best solution will be lock-like, but the waiter nodes will contain
>> the value they are waiting for - so only the specific threads get woken up.
>> The solution with concurrent map is very similar, only with larger overhead
>> from storing the index the thread is waiting for.
>>
>>
>> Alex
>>
>>
>>
>> On 18/12/2014 20:21, Hanson Char wrote:
>>
>> Less overhead and simpler are a nice properties, even though at the
>> expense of having to wake up all waiting threads just to find out the one
>> with the right order to execute.  Still, this seems like a good tradeoff.
>>
>>  Thanks,
>> Hanson
>>
>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com>
>> wrote:
>>
>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>
>> No, there is no difference. Peter didn't spot your entire method is
>> synchronized, so spurious wakeup won't make progress until the owner of the
>> lock exits the method.
>>
>> You could split the synchronization into two blocks - one encompassing
>> the wait loop, the other in the finally block; but it may make no
>> difference.
>>
>> Alex
>>
>>
>> You're right, Alex. I'm so infected with park/unpark virus that I missed
>> that ;-)
>>
>> Peter
>>
>>
>> On 17/12/2014 18:36, suman shil wrote:
>>
>> Thanks peter for your reply. You are right. I should have incremented
>> currentAllowedOrder in finally block.
>>
>> Suman
>>  ------------------------------------------------------------------------
>> *From:* Peter Levart <peter.levart at gmail.com>
>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>> concurrency-interest at cs.oswego.edu>
>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>> critical sections?
>>
>> On 12/17/2014 06:46 PM, suman shil wrote:
>>
>> Thanks for your response. Will notifyAll() instead of notify() solve the
>> problem?
>>
>>
>> It will, but you should also account for "spurious" wake-ups. You should
>> increment currentAllowedOrder only after return from callable.call (in
>> finally block just before notifyAll()).
>>
>> Otherwise a nice solution - with minimal state, providing that not many
>> threads meet at the same time...
>>
>> Regards, Peter
>>
>>  RegardsSuman
>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:
>> oleksandr.otenko at oracle.com>
>>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>;
>> Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December 17,
>> 2014 9:55 PM
>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>> critical sections?
>>       There is no guarantee you'll ever hand over the control to the
>> right thread upon notify()
>>     Alex
>>
>> On 17/12/2014 14:07, suman shil wrote:
>>       Hi, Following is my solution to solve this problem. Please let me
>> know if I am missing something.
>>    public class OrderedExecutor {  private int currentAllowedOrder = 0;
>> private int maxLength = 0;  public OrderedExecutor(int n)  {
>> this.maxLength = n;  } public synchronized Object
>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>                        throws Exception  { if (order >= maxLength)  {
>> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
>> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
>> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
>> }  } }
>>    Regards Suman
>>         From: Peter Levart<peter.levart at gmail.com> <mailto:
>> peter.levart at gmail.com>
>>   To: Hanson Char<hanson.char at gmail.com> <mailto:hanson.char at gmail.com>
>>   Cc: concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14, 2014
>> 11:01 PM
>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>> critical sections?
>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>      Hi Peter,
>>    Thanks for this proposed idea of using LockSupport. This begs the
>> question: which one would you choose if you had all three (correct)
>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>    Regards, Hanson
>>     The Semaphore/CountDownLatch variants are equivalent if you don't
>> need re-use. So any would do. They lack invalid-use detection. What happens
>> if they are not used as intended? Semaphore variant acts differently than
>> CountDownLatch variant. The low-level variant I  proposed detects invalid
>> usage. So I would probably use this one. But the low level variant is
>> harder to reason about it's correctness. I think it is correct, but you
>> should show it to somebody else to confirm this.
>>     Another question is whether you actually need this kind of
>> synchronizer. Maybe if you explained what you are trying to achieve,
>> somebody could have an idea how to do that even more elegantly...
>>     Regards, Peter
>>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>    Hi Hanson,
>>     This one is more low-level, but catches some invalid usages and is
>> more resource-friendly:
>>       public class OrderedExecutor {
>>         public <T> T execCriticalSectionInOrder(
>>           final int order,
>>           final Supplier<T> criticalSection
>>       ) throws InterruptedException {
>>           if (order < 0) {
>>                throw new IllegalArgumentException("'order' should be >=
>> 0");
>>           }
>>           if (order > 0) {
>>               waitForDone(order - 1);
>>           }
>>           try {
>>               return criticalSection.get();
>>           } finally {
>>               notifyDone(order);
>>           }
>>       }
>>         private static final Object DONE = new Object();
>>       private final ConcurrentMap<Integer, Object> signals = new
>> ConcurrentHashMap<>();
>>         private void waitForDone(int order) throws InterruptedException {
>>           Object sig = signals.putIfAbsent(order, Thread.currentThread());
>>           if (sig != null && sig != DONE) {
>>               throw new IllegalStateException();
>>           }
>>           while (sig != DONE) {
>>               LockSupport.park();
>>               if (Thread.interrupted()) {
>>                   throw new InterruptedException();
>>               }
>>               sig = signals.get(order);
>>           }
>>       }
>>         private void notifyDone(int order) {
>>           Object sig = signals.putIfAbsent(order, DONE);
>>           if (sig instanceof Thread) {
>>               if (!signals.replace(order, sig, DONE)) {
>>                   throw new IllegalStateException();
>>               }
>>               LockSupport.unpark((Thread) sig);
>>           } else if (sig != null) {
>>               throw new IllegalStateException();
>>           }
>>       }
>>   }
>>       Regards, Peter
>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>      Hi Peter,
>>    Thanks for the suggestion, and sorry about not being clear about one
>> important  detail: "n" is not known a priori when constructing an
>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>     If you know at least the upper bound of 'n', it can be used with such
>> 'n'. Otherwise something that dynamically re-sizes the array could be
>> devised. Or you could simply use a ConcurrentHashMap instead of array where
>> keys are 'order' values:
>>       public class OrderedExecutor<T> {
>>         private final ConcurrentMap<Integer, CountDownLatch> latches =
>> new ConcurrentHashMap<>();
>>         public T execCriticalSectionInOrder(final int order,
>>                                           final Supplier<T>
>> criticalSection) throws InterruptedException {
>>           if (order > 0) {
>>               latches.computeIfAbsent(order - 1, o -> new
>> CountDownLatch(1)).await();
>>           }
>>           try {
>>               return criticalSection.get();
>>           } finally {
>>               latches.computeIfAbsent(order, o -> new
>> CountDownLatch(1)).countDown();
>>           }
>>       }
>>   }
>>       Regards, Peter
>>           You guessed right: it's a one-shot object for a particular
>> OrderedExecutor  instance, and "order" must be called indeed at most once.
>>    Regards, Hanson
>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<peter.levart at gmail.com>
>> <mailto:peter.levart at gmail.com>  wrote:
>>    Hi Hanson,
>>     I don't think anything like that readily exists  in
>> java.lang.concurrent, but what you describe should be possible to  achieve
>> with composition of existing primitives.  You haven't given any additional
>> hints to what your OrderedExecutor  should behave like. Should it be a
>> one-shot object (like CountDownLatch) or a re-usable one (like
>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>> OrderedExecutor instance and 'order' value be called at most once? If yes
>> (and I think that only a one-shot object  makes sense here), an array of
>> CountDownLatch(es) could be used:
>>     public class OrderedExecutor<T> {
>>       private final CountDownLatch[] latches;
>>         public OrderedExecutor(int n) {
>>           if (n < 1) throw new IllegalArgumentException("'n'  should be
>> >= 1");
>>           latches = new CountDownLatch[n - 1];
>>           for (int i = 0; i < latches.length; i++) {
>>               latches[i] = new CountDownLatch(1);
>>           }
>>       }
>>         public T execCriticalSectionInOrder(final int order,
>>                                            final Supplier<T>
>> criticalSection) throws InterruptedException {
>>           if (order < 0 || order > latches.length)
>>               throw new IllegalArgumentException("'order' should be
>> [0..." +  latches.length + "]");
>>           if (order > 0) {
>>               latches[order - 1].await();
>>           }
>>           try {
>>               return criticalSection.get();
>>           } finally {
>>               if (order < latches.length) {
>>                   latches[order].countDown();
>>               }
>>           }
>>       }
>>   }
>>       Regards, Peter
>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>          Hi, I am looking for a construct that can  be used to
>> efficiently enforce  ordered execution of multiple critical sections, each
>> calling from a  different thread. The calling threads may run in  parallel
>> and may call the execution method out of order. The  perceived construct
>> would therefore be responsible for re-ordering the execution of those
>> threads, so that their critical  sections (and only the critical section)
>> will be executed in order. Would something  like the following API already
>> exist? /** * Used to enforce ordered execution of critical sections calling
>> from multiple *  threads, parking and unparking the  threads as necessary.
>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>> most once with the given order, parking * and  unparking the current thread
>> as  necessary so that all critical * sections executed  by different
>> threads using this  executor take place in * the order from 1 to n
>> consecutively. */ public T execCriticalSectionInOrder
>> (  final int order, final Callable<T> criticalSection) throws
>> InterruptedException; } Regards, Hanson
>> _______________________________________________Concurrency-interest mailing
>> listConcurrency-interest at cs.oswego.edu <mailto:
>> Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>>   Concurrency-interest mailing list
>>   Concurrency-interest at cs.oswego.edu <mailto:
>> Concurrency-interest at cs.oswego.edu>
>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:
>> Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:
>> Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141220/23b8b8d3/attachment-0001.html>

From hanson.char at gmail.com  Sat Dec 20 18:59:02 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 20 Dec 2014 15:59:02 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
Message-ID: <CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>

This one looks interesting and educational.  The use of condition enables
us to suspend and resume specifically targeted threads without resorting to
the use of LockSupport.{part, unpark}.  Instead of using a PriorityQueue,
perhaps we can use a simple Map<Integer, Condition> instead so we can get
away without the extra Waiter inner class?

Regards,
Hanson

On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> To round out the selection, here's an implementation using a PriorityQueue
> of Conditions:
>
> class OrderedExecutor<T> {
>
>   static class Waiter implements Comparable<Waiter> {
>
>     final int order;
>     final Condition condition;
>
>     Waiter(int order, Condition condition) {
>       this.order = order;
>       this.condition = condition;
>     }
>
>     @Override
>     public int compareTo(Waiter waiter) {
>       return order - waiter.order;
>     }
>   }
>
>   final Lock lock = new ReentrantLock();
>   final Queue<Waiter> queue = new PriorityQueue<>();
>   int nextOrder; // 0 is next
>
>   public T execCallableInOrder(int order, Callable<T> callable) throws
> Exception {
>     assert order >= 0;
>     awaitTurn(order);
>     try {
>       return callable.call();
>     } finally {
>       signalNext(order + 1);
>     }
>   }
>
>   void awaitTurn(int order) {
>     lock.lock();
>     try {
>       Condition condition = null;
>       while (nextOrder != order) {
>         if (condition == null) {
>           condition = lock.newCondition();
>           queue.add(new Waiter(order, condition));
>         }
>         condition.awaitUninterruptibly();
>       }
>     } finally {
>       lock.unlock();
>     }
>   }
>
>   void signalNext(int nextOrder) {
>     lock.lock();
>     try {
>       this.nextOrder = nextOrder;
>       Waiter waiter = queue.peek();
>       if (waiter != null && waiter.order == nextOrder) {
>         queue.remove();
>         waiter.condition.signal();
>       }
>     } finally {
>       lock.unlock();
>     }
>   }
> }
>
> On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer <joe.bowbeer at gmail.com>
> wrote:
>>
>> Suman,
>>
>> I would advise against using notify/wait.  It raises a red flag for a lot
>> of reviewers, including me.
>>
>> The problems I see in this implementation are:
>>
>> 1. Pre-allocation of locks is prohibited by (revised) problem statement.
>>
>> Note that if pre-allocation were allowed, then an array would be more
>> efficient than a Map.
>>
>> 2. Access to currentAllowedOrder is not thread-safe but should be.
>>
>>
>> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <suman_krec at yahoo.com> wrote:
>>>
>>> I have modified my solution to avoid notifyAll(). Let me know your
>>> feedback.
>>>
>>>
>>> public class OrderedExecutor
>>> {
>>> private int maxOrder;
>>> private int currentAllowedOrder;
>>> private Map<Integer, Object> map = new HashMap<Integer, Object>();
>>>     public OrderedExecutor(int n)
>>>     {
>>>          this.maxOrder = n;
>>>          this.currentAllowedOrder = 0;
>>>     for(int i = 0 ; i < this.maxOrder ; i++)
>>>     {
>>>     map.put(i, new Object());
>>>     }
>>>     }
>>>
>>>     public  Object execCriticalSectionInOrder(int order,
>>>                                               Callable<Object> callable)
>>>                                               throws Exception
>>>     {
>>> if (order >= this.maxOrder)
>>> {
>>> throw new Exception("Exceeds maximum order "+ maxOrder);
>>> }
>>>
>>> while(order != currentAllowedOrder)
>>> {
>>> synchronized (this.map.get(order))
>>> {
>>> this.map.get(order).wait();
>>> }
>>> }
>>>  try
>>> {
>>> return callable.call();
>>> }
>>>        finally
>>> {
>>>             currentAllowedOrder = currentAllowedOrder+1;
>>> synchronized (this.map.get(order+1))
>>>             {
>>>                 this.map.get(order+1).notify();
>>>             }
>>>        }
>>>     }
>>> }
>>>
>>>
>>>   ------------------------------
>>>  *From:* Joe Bowbeer <joe.bowbeer at gmail.com>
>>> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
>>> *Sent:* Friday, December 19, 2014 3:33 AM
>>>
>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>>> critical sections?
>>>
>>> That would be "Tom" Cargill; link to paper:
>>>
>>> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>>>
>>>
>>>
>>> On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>> wrote:
>>>
>>> I frown on use of notify[All]/wait because they make the code hard to
>>> maintain.
>>>
>>> In this case, with potentially lots of waiting threads, I would check
>>> out the "Specific Notification" pattern if I were determined to go the
>>> wait/notify route:
>>>
>>> Tim Cargill's paper is dated but still worth reading.
>>>
>>> Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ and
>>> Peter Haggar's article:
>>>
>>> http://www.ibm.com/developerworks/java/library/j-spnotif.html
>>>
>>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
>>> oleksandr.otenko at oracle.com> wrote:
>>>
>>>  Yes, no one said it is a good idea to always do that. When it is
>>> contended, most of the threads will wake up to only go back to sleep.
>>>
>>> The pattern you are after is usually called sequencer. You can see it
>>> used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe not
>>> that popular.
>>>
>>> The best solution will be lock-like, but the waiter nodes will contain
>>> the value they are waiting for - so only the specific threads get woken up.
>>> The solution with concurrent map is very similar, only with larger overhead
>>> from storing the index the thread is waiting for.
>>>
>>>
>>> Alex
>>>
>>>
>>>
>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>
>>> Less overhead and simpler are a nice properties, even though at the
>>> expense of having to wake up all waiting threads just to find out the one
>>> with the right order to execute.  Still, this seems like a good tradeoff.
>>>
>>>  Thanks,
>>> Hanson
>>>
>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com>
>>> wrote:
>>>
>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>
>>> No, there is no difference. Peter didn't spot your entire method is
>>> synchronized, so spurious wakeup won't make progress until the owner of the
>>> lock exits the method.
>>>
>>> You could split the synchronization into two blocks - one encompassing
>>> the wait loop, the other in the finally block; but it may make no
>>> difference.
>>>
>>> Alex
>>>
>>>
>>> You're right, Alex. I'm so infected with park/unpark virus that I missed
>>> that ;-)
>>>
>>> Peter
>>>
>>>
>>> On 17/12/2014 18:36, suman shil wrote:
>>>
>>> Thanks peter for your reply. You are right. I should have incremented
>>> currentAllowedOrder in finally block.
>>>
>>> Suman
>>>  ------------------------------------------------------------------------
>>> *From:* Peter Levart <peter.levart at gmail.com>
>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>>> concurrency-interest at cs.oswego.edu>
>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>>> critical sections?
>>>
>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>
>>> Thanks for your response. Will notifyAll() instead of notify() solve the
>>> problem?
>>>
>>>
>>> It will, but you should also account for "spurious" wake-ups. You should
>>> increment currentAllowedOrder only after return from callable.call (in
>>> finally block just before notifyAll()).
>>>
>>> Otherwise a nice solution - with minimal state, providing that not many
>>> threads meet at the same time...
>>>
>>> Regards, Peter
>>>
>>>  RegardsSuman
>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:
>>> oleksandr.otenko at oracle.com>
>>>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>;
>>> Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December 17,
>>> 2014 9:55 PM
>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>> critical sections?
>>>       There is no guarantee you'll ever hand over the control to the
>>> right thread upon notify()
>>>     Alex
>>>
>>> On 17/12/2014 14:07, suman shil wrote:
>>>       Hi, Following is my solution to solve this problem. Please let me
>>> know if I am missing something.
>>>    public class OrderedExecutor {  private int currentAllowedOrder = 0;
>>> private int maxLength = 0;  public OrderedExecutor(int n)  {
>>> this.maxLength = n;  } public synchronized Object
>>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>>                        throws Exception  { if (order >= maxLength)  {
>>> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
>>> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
>>> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
>>> }  } }
>>>    Regards Suman
>>>         From: Peter Levart<peter.levart at gmail.com> <mailto:
>>> peter.levart at gmail.com>
>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:hanson.char at gmail.com>
>>>   Cc: concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14, 2014
>>> 11:01 PM
>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>> critical sections?
>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>      Hi Peter,
>>>    Thanks for this proposed idea of using LockSupport. This begs the
>>> question: which one would you choose if you had all three (correct)
>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>    Regards, Hanson
>>>     The Semaphore/CountDownLatch variants are equivalent if you don't
>>> need re-use. So any would do. They lack invalid-use detection. What happens
>>> if they are not used as intended? Semaphore variant acts differently than
>>> CountDownLatch variant. The low-level variant I  proposed detects invalid
>>> usage. So I would probably use this one. But the low level variant is
>>> harder to reason about it's correctness. I think it is correct, but you
>>> should show it to somebody else to confirm this.
>>>     Another question is whether you actually need this kind of
>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>> somebody could have an idea how to do that even more elegantly...
>>>     Regards, Peter
>>>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>    Hi Hanson,
>>>     This one is more low-level, but catches some invalid usages and is
>>> more resource-friendly:
>>>       public class OrderedExecutor {
>>>         public <T> T execCriticalSectionInOrder(
>>>           final int order,
>>>           final Supplier<T> criticalSection
>>>       ) throws InterruptedException {
>>>           if (order < 0) {
>>>                throw new IllegalArgumentException("'order' should be >=
>>> 0");
>>>           }
>>>           if (order > 0) {
>>>               waitForDone(order - 1);
>>>           }
>>>           try {
>>>               return criticalSection.get();
>>>           } finally {
>>>               notifyDone(order);
>>>           }
>>>       }
>>>         private static final Object DONE = new Object();
>>>       private final ConcurrentMap<Integer, Object> signals = new
>>> ConcurrentHashMap<>();
>>>         private void waitForDone(int order) throws InterruptedException {
>>>           Object sig = signals.putIfAbsent(order,
>>> Thread.currentThread());
>>>           if (sig != null && sig != DONE) {
>>>               throw new IllegalStateException();
>>>           }
>>>           while (sig != DONE) {
>>>               LockSupport.park();
>>>               if (Thread.interrupted()) {
>>>                   throw new InterruptedException();
>>>               }
>>>               sig = signals.get(order);
>>>           }
>>>       }
>>>         private void notifyDone(int order) {
>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>           if (sig instanceof Thread) {
>>>               if (!signals.replace(order, sig, DONE)) {
>>>                   throw new IllegalStateException();
>>>               }
>>>               LockSupport.unpark((Thread) sig);
>>>           } else if (sig != null) {
>>>               throw new IllegalStateException();
>>>           }
>>>       }
>>>   }
>>>       Regards, Peter
>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>      Hi Peter,
>>>    Thanks for the suggestion, and sorry about not being clear about one
>>> important  detail: "n" is not known a priori when constructing an
>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>     If you know at least the upper bound of 'n', it can be used with
>>> such 'n'. Otherwise something that dynamically re-sizes the array could be
>>> devised. Or you could simply use a ConcurrentHashMap instead of array where
>>> keys are 'order' values:
>>>       public class OrderedExecutor<T> {
>>>         private final ConcurrentMap<Integer, CountDownLatch> latches =
>>> new ConcurrentHashMap<>();
>>>         public T execCriticalSectionInOrder(final int order,
>>>                                           final Supplier<T>
>>> criticalSection) throws InterruptedException {
>>>           if (order > 0) {
>>>               latches.computeIfAbsent(order - 1, o -> new
>>> CountDownLatch(1)).await();
>>>           }
>>>           try {
>>>               return criticalSection.get();
>>>           } finally {
>>>               latches.computeIfAbsent(order, o -> new
>>> CountDownLatch(1)).countDown();
>>>           }
>>>       }
>>>   }
>>>       Regards, Peter
>>>           You guessed right: it's a one-shot object for a particular
>>> OrderedExecutor  instance, and "order" must be called indeed at most once.
>>>    Regards, Hanson
>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<peter.levart at gmail.com>
>>> <mailto:peter.levart at gmail.com>  wrote:
>>>    Hi Hanson,
>>>     I don't think anything like that readily exists  in
>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>> with composition of existing primitives.  You haven't given any additional
>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>> (and I think that only a one-shot object  makes sense here), an array of
>>> CountDownLatch(es) could be used:
>>>     public class OrderedExecutor<T> {
>>>       private final CountDownLatch[] latches;
>>>         public OrderedExecutor(int n) {
>>>           if (n < 1) throw new IllegalArgumentException("'n'  should be
>>> >= 1");
>>>           latches = new CountDownLatch[n - 1];
>>>           for (int i = 0; i < latches.length; i++) {
>>>               latches[i] = new CountDownLatch(1);
>>>           }
>>>       }
>>>         public T execCriticalSectionInOrder(final int order,
>>>                                            final Supplier<T>
>>> criticalSection) throws InterruptedException {
>>>           if (order < 0 || order > latches.length)
>>>               throw new IllegalArgumentException("'order' should be
>>> [0..." +  latches.length + "]");
>>>           if (order > 0) {
>>>               latches[order - 1].await();
>>>           }
>>>           try {
>>>               return criticalSection.get();
>>>           } finally {
>>>               if (order < latches.length) {
>>>                   latches[order].countDown();
>>>               }
>>>           }
>>>       }
>>>   }
>>>       Regards, Peter
>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>          Hi, I am looking for a construct that can  be used to
>>> efficiently enforce  ordered execution of multiple critical sections, each
>>> calling from a  different thread. The calling threads may run in  parallel
>>> and may call the execution method out of order. The  perceived construct
>>> would therefore be responsible for re-ordering the execution of those
>>> threads, so that their critical  sections (and only the critical section)
>>> will be executed in order. Would something  like the following API already
>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>> most once with the given order, parking * and  unparking the current thread
>>> as  necessary so that all critical * sections executed  by different
>>> threads using this  executor take place in * the order from 1 to n
>>> consecutively. */ public T execCriticalSectionInOrder
>>> (  final int order, final Callable<T> criticalSection) throws
>>> InterruptedException; } Regards, Hanson
>>> _______________________________________________Concurrency-interest mailing
>>> listConcurrency-interest at cs.oswego.edu <mailto:
>>> Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>>   Concurrency-interest mailing list
>>>   Concurrency-interest at cs.oswego.edu <mailto:
>>> Concurrency-interest at cs.oswego.edu>
>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:
>>> Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:
>>> Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141220/361665f8/attachment-0001.html>

From joe.bowbeer at gmail.com  Sat Dec 20 19:39:25 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 20 Dec 2014 16:39:25 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
Message-ID: <CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>

I think PriorityQueue more clearly represents the function than Map does in
this case.

A Queue should also retain less memory. A Map will tend to accumulate both
Entry and Integer instances over time.

The ConcurrentMap does have the advantage of lock striping, which might be
more performant if there is high contention.

I think this is an interesting problem because there are quite a few
different solutions, each with advantages and disadvantages in terms of
performance and maintainability.

If there were a PriorityQueue version of AbstractQueuedSynchronizer, then
we could have yet another solution using the AQS's state to represent the
next available order.

On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char <hanson.char at gmail.com> wrote:
>
> This one looks interesting and educational.  The use of condition enables
> us to suspend and resume specifically targeted threads without resorting to
> the use of LockSupport.{part, unpark}.  Instead of using a PriorityQueue,
> perhaps we can use a simple Map<Integer, Condition> instead so we can get
> away without the extra Waiter inner class?
>
> Regards,
> Hanson
>
> On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
> wrote:
>
>> To round out the selection, here's an implementation using a
>> PriorityQueue of Conditions:
>>
>> class OrderedExecutor<T> {
>>
>>   static class Waiter implements Comparable<Waiter> {
>>
>>     final int order;
>>     final Condition condition;
>>
>>     Waiter(int order, Condition condition) {
>>       this.order = order;
>>       this.condition = condition;
>>     }
>>
>>     @Override
>>     public int compareTo(Waiter waiter) {
>>       return order - waiter.order;
>>     }
>>   }
>>
>>   final Lock lock = new ReentrantLock();
>>   final Queue<Waiter> queue = new PriorityQueue<>();
>>   int nextOrder; // 0 is next
>>
>>   public T execCallableInOrder(int order, Callable<T> callable) throws
>> Exception {
>>     assert order >= 0;
>>     awaitTurn(order);
>>     try {
>>       return callable.call();
>>     } finally {
>>       signalNext(order + 1);
>>     }
>>   }
>>
>>   void awaitTurn(int order) {
>>     lock.lock();
>>     try {
>>       Condition condition = null;
>>       while (nextOrder != order) {
>>         if (condition == null) {
>>           condition = lock.newCondition();
>>           queue.add(new Waiter(order, condition));
>>         }
>>         condition.awaitUninterruptibly();
>>       }
>>     } finally {
>>       lock.unlock();
>>     }
>>   }
>>
>>   void signalNext(int nextOrder) {
>>     lock.lock();
>>     try {
>>       this.nextOrder = nextOrder;
>>       Waiter waiter = queue.peek();
>>       if (waiter != null && waiter.order == nextOrder) {
>>         queue.remove();
>>         waiter.condition.signal();
>>       }
>>     } finally {
>>       lock.unlock();
>>     }
>>   }
>> }
>>
>> On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer <joe.bowbeer at gmail.com>
>> wrote:
>>>
>>> Suman,
>>>
>>> I would advise against using notify/wait.  It raises a red flag for a
>>> lot of reviewers, including me.
>>>
>>> The problems I see in this implementation are:
>>>
>>> 1. Pre-allocation of locks is prohibited by (revised) problem statement.
>>>
>>> Note that if pre-allocation were allowed, then an array would be more
>>> efficient than a Map.
>>>
>>> 2. Access to currentAllowedOrder is not thread-safe but should be.
>>>
>>>
>>> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <suman_krec at yahoo.com>
>>> wrote:
>>>>
>>>> I have modified my solution to avoid notifyAll(). Let me know your
>>>> feedback.
>>>>
>>>>
>>>> public class OrderedExecutor
>>>> {
>>>> private int maxOrder;
>>>> private int currentAllowedOrder;
>>>> private Map<Integer, Object> map = new HashMap<Integer, Object>();
>>>>     public OrderedExecutor(int n)
>>>>     {
>>>>          this.maxOrder = n;
>>>>          this.currentAllowedOrder = 0;
>>>>     for(int i = 0 ; i < this.maxOrder ; i++)
>>>>     {
>>>>     map.put(i, new Object());
>>>>     }
>>>>     }
>>>>
>>>>     public  Object execCriticalSectionInOrder(int order,
>>>>                                               Callable<Object> callable)
>>>>                                               throws Exception
>>>>     {
>>>> if (order >= this.maxOrder)
>>>> {
>>>> throw new Exception("Exceeds maximum order "+ maxOrder);
>>>> }
>>>>
>>>> while(order != currentAllowedOrder)
>>>> {
>>>> synchronized (this.map.get(order))
>>>> {
>>>> this.map.get(order).wait();
>>>> }
>>>> }
>>>>  try
>>>> {
>>>> return callable.call();
>>>> }
>>>>        finally
>>>> {
>>>>             currentAllowedOrder = currentAllowedOrder+1;
>>>> synchronized (this.map.get(order+1))
>>>>             {
>>>>                 this.map.get(order+1).notify();
>>>>             }
>>>>        }
>>>>     }
>>>> }
>>>>
>>>>
>>>>   ------------------------------
>>>>  *From:* Joe Bowbeer <joe.bowbeer at gmail.com>
>>>> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
>>>> *Sent:* Friday, December 19, 2014 3:33 AM
>>>>
>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>>>> critical sections?
>>>>
>>>> That would be "Tom" Cargill; link to paper:
>>>>
>>>>
>>>> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>>>>
>>>>
>>>>
>>>> On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>> wrote:
>>>>
>>>> I frown on use of notify[All]/wait because they make the code hard to
>>>> maintain.
>>>>
>>>> In this case, with potentially lots of waiting threads, I would check
>>>> out the "Specific Notification" pattern if I were determined to go the
>>>> wait/notify route:
>>>>
>>>> Tim Cargill's paper is dated but still worth reading.
>>>>
>>>> Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ and
>>>> Peter Haggar's article:
>>>>
>>>> http://www.ibm.com/developerworks/java/library/j-spnotif.html
>>>>
>>>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
>>>> oleksandr.otenko at oracle.com> wrote:
>>>>
>>>>  Yes, no one said it is a good idea to always do that. When it is
>>>> contended, most of the threads will wake up to only go back to sleep.
>>>>
>>>> The pattern you are after is usually called sequencer. You can see it
>>>> used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe not
>>>> that popular.
>>>>
>>>> The best solution will be lock-like, but the waiter nodes will contain
>>>> the value they are waiting for - so only the specific threads get woken up.
>>>> The solution with concurrent map is very similar, only with larger overhead
>>>> from storing the index the thread is waiting for.
>>>>
>>>>
>>>> Alex
>>>>
>>>>
>>>>
>>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>>
>>>> Less overhead and simpler are a nice properties, even though at the
>>>> expense of having to wake up all waiting threads just to find out the one
>>>> with the right order to execute.  Still, this seems like a good tradeoff.
>>>>
>>>>  Thanks,
>>>> Hanson
>>>>
>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com>
>>>> wrote:
>>>>
>>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>>
>>>> No, there is no difference. Peter didn't spot your entire method is
>>>> synchronized, so spurious wakeup won't make progress until the owner of the
>>>> lock exits the method.
>>>>
>>>> You could split the synchronization into two blocks - one encompassing
>>>> the wait loop, the other in the finally block; but it may make no
>>>> difference.
>>>>
>>>> Alex
>>>>
>>>>
>>>> You're right, Alex. I'm so infected with park/unpark virus that I
>>>> missed that ;-)
>>>>
>>>> Peter
>>>>
>>>>
>>>> On 17/12/2014 18:36, suman shil wrote:
>>>>
>>>> Thanks peter for your reply. You are right. I should have incremented
>>>> currentAllowedOrder in finally block.
>>>>
>>>> Suman
>>>>
>>>> ------------------------------------------------------------------------
>>>> *From:* Peter Levart <peter.levart at gmail.com>
>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>>>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>>>> concurrency-interest at cs.oswego.edu>
>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>>>> critical sections?
>>>>
>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>>
>>>> Thanks for your response. Will notifyAll() instead of notify() solve
>>>> the problem?
>>>>
>>>>
>>>> It will, but you should also account for "spurious" wake-ups. You
>>>> should increment currentAllowedOrder only after return from callable.call
>>>> (in finally block just before notifyAll()).
>>>>
>>>> Otherwise a nice solution - with minimal state, providing that not many
>>>> threads meet at the same time...
>>>>
>>>> Regards, Peter
>>>>
>>>>  RegardsSuman
>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:
>>>> oleksandr.otenko at oracle.com>
>>>>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>;
>>>> Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December 17,
>>>> 2014 9:55 PM
>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>> critical sections?
>>>>       There is no guarantee you'll ever hand over the control to the
>>>> right thread upon notify()
>>>>     Alex
>>>>
>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>       Hi, Following is my solution to solve this problem. Please let me
>>>> know if I am missing something.
>>>>    public class OrderedExecutor {  private int currentAllowedOrder =
>>>> 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {
>>>> this.maxLength = n;  } public synchronized Object
>>>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>>>                        throws Exception  { if (order >= maxLength)  {
>>>> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
>>>> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
>>>> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
>>>> }  } }
>>>>    Regards Suman
>>>>         From: Peter Levart<peter.levart at gmail.com> <mailto:
>>>> peter.levart at gmail.com>
>>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:hanson.char at gmail.com>
>>>>   Cc: concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>>> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14, 2014
>>>> 11:01 PM
>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>> critical sections?
>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>      Hi Peter,
>>>>    Thanks for this proposed idea of using LockSupport. This begs the
>>>> question: which one would you choose if you had all three (correct)
>>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>    Regards, Hanson
>>>>     The Semaphore/CountDownLatch variants are equivalent if you don't
>>>> need re-use. So any would do. They lack invalid-use detection. What happens
>>>> if they are not used as intended? Semaphore variant acts differently than
>>>> CountDownLatch variant. The low-level variant I  proposed detects invalid
>>>> usage. So I would probably use this one. But the low level variant is
>>>> harder to reason about it's correctness. I think it is correct, but you
>>>> should show it to somebody else to confirm this.
>>>>     Another question is whether you actually need this kind of
>>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>>> somebody could have an idea how to do that even more elegantly...
>>>>     Regards, Peter
>>>>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>    Hi Hanson,
>>>>     This one is more low-level, but catches some invalid usages and is
>>>> more resource-friendly:
>>>>       public class OrderedExecutor {
>>>>         public <T> T execCriticalSectionInOrder(
>>>>           final int order,
>>>>           final Supplier<T> criticalSection
>>>>       ) throws InterruptedException {
>>>>           if (order < 0) {
>>>>                throw new IllegalArgumentException("'order' should be >=
>>>> 0");
>>>>           }
>>>>           if (order > 0) {
>>>>               waitForDone(order - 1);
>>>>           }
>>>>           try {
>>>>               return criticalSection.get();
>>>>           } finally {
>>>>               notifyDone(order);
>>>>           }
>>>>       }
>>>>         private static final Object DONE = new Object();
>>>>       private final ConcurrentMap<Integer, Object> signals = new
>>>> ConcurrentHashMap<>();
>>>>         private void waitForDone(int order) throws InterruptedException
>>>> {
>>>>           Object sig = signals.putIfAbsent(order,
>>>> Thread.currentThread());
>>>>           if (sig != null && sig != DONE) {
>>>>               throw new IllegalStateException();
>>>>           }
>>>>           while (sig != DONE) {
>>>>               LockSupport.park();
>>>>               if (Thread.interrupted()) {
>>>>                   throw new InterruptedException();
>>>>               }
>>>>               sig = signals.get(order);
>>>>           }
>>>>       }
>>>>         private void notifyDone(int order) {
>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>           if (sig instanceof Thread) {
>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>                   throw new IllegalStateException();
>>>>               }
>>>>               LockSupport.unpark((Thread) sig);
>>>>           } else if (sig != null) {
>>>>               throw new IllegalStateException();
>>>>           }
>>>>       }
>>>>   }
>>>>       Regards, Peter
>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>      Hi Peter,
>>>>    Thanks for the suggestion, and sorry about not being clear about one
>>>> important  detail: "n" is not known a priori when constructing an
>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>     If you know at least the upper bound of 'n', it can be used with
>>>> such 'n'. Otherwise something that dynamically re-sizes the array could be
>>>> devised. Or you could simply use a ConcurrentHashMap instead of array where
>>>> keys are 'order' values:
>>>>       public class OrderedExecutor<T> {
>>>>         private final ConcurrentMap<Integer, CountDownLatch> latches =
>>>> new ConcurrentHashMap<>();
>>>>         public T execCriticalSectionInOrder(final int order,
>>>>                                           final Supplier<T>
>>>> criticalSection) throws InterruptedException {
>>>>           if (order > 0) {
>>>>               latches.computeIfAbsent(order - 1, o -> new
>>>> CountDownLatch(1)).await();
>>>>           }
>>>>           try {
>>>>               return criticalSection.get();
>>>>           } finally {
>>>>               latches.computeIfAbsent(order, o -> new
>>>> CountDownLatch(1)).countDown();
>>>>           }
>>>>       }
>>>>   }
>>>>       Regards, Peter
>>>>           You guessed right: it's a one-shot object for a particular
>>>> OrderedExecutor  instance, and "order" must be called indeed at most once.
>>>>    Regards, Hanson
>>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<
>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>    Hi Hanson,
>>>>     I don't think anything like that readily exists  in
>>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>>> with composition of existing primitives.  You haven't given any additional
>>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>> (and I think that only a one-shot object  makes sense here), an array of
>>>> CountDownLatch(es) could be used:
>>>>     public class OrderedExecutor<T> {
>>>>       private final CountDownLatch[] latches;
>>>>         public OrderedExecutor(int n) {
>>>>           if (n < 1) throw new IllegalArgumentException("'n'  should be
>>>> >= 1");
>>>>           latches = new CountDownLatch[n - 1];
>>>>           for (int i = 0; i < latches.length; i++) {
>>>>               latches[i] = new CountDownLatch(1);
>>>>           }
>>>>       }
>>>>         public T execCriticalSectionInOrder(final int order,
>>>>                                            final Supplier<T>
>>>> criticalSection) throws InterruptedException {
>>>>           if (order < 0 || order > latches.length)
>>>>               throw new IllegalArgumentException("'order' should be
>>>> [0..." +  latches.length + "]");
>>>>           if (order > 0) {
>>>>               latches[order - 1].await();
>>>>           }
>>>>           try {
>>>>               return criticalSection.get();
>>>>           } finally {
>>>>               if (order < latches.length) {
>>>>                   latches[order].countDown();
>>>>               }
>>>>           }
>>>>       }
>>>>   }
>>>>       Regards, Peter
>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>          Hi, I am looking for a construct that can  be used to
>>>> efficiently enforce  ordered execution of multiple critical sections, each
>>>> calling from a  different thread. The calling threads may run in  parallel
>>>> and may call the execution method out of order. The  perceived construct
>>>> would therefore be responsible for re-ordering the execution of those
>>>> threads, so that their critical  sections (and only the critical section)
>>>> will be executed in order. Would something  like the following API already
>>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>>> most once with the given order, parking * and  unparking the current thread
>>>> as  necessary so that all critical * sections executed  by different
>>>> threads using this  executor take place in * the order from 1 to n
>>>> consecutively. */ public T execCriticalSectionInOrder
>>>> (  final int order, final Callable<T> criticalSection) throws
>>>> InterruptedException; } Regards, Hanson
>>>> _______________________________________________Concurrency-interest mailing
>>>> listConcurrency-interest at cs.oswego.edu <mailto:
>>>> Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> _______________________________________________
>>>>   Concurrency-interest mailing list
>>>>   Concurrency-interest at cs.oswego.edu <mailto:
>>>> Concurrency-interest at cs.oswego.edu>
>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>> Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>> Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141220/250d5695/attachment-0001.html>

From hanson.char at gmail.com  Sat Dec 20 20:35:29 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 20 Dec 2014 17:35:29 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
Message-ID: <CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>

Thanks, Joe. That's an interesting point about the long term memory impact
of TreeMap vs HashMap - similar to LinkedList vs ArrayList.

Regards,
Hanson

On Sat, Dec 20, 2014 at 5:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> I see.  Yes, that addresses the removal problem, though note that HashMap
> does not shrink when data is removed. Even if all keys are removed from
> HashMap, the inner size of its table does not change.
>
> So in this case maybe I would choose TreeMap as the implementation. Its
> remove() method *does* remove all the memory associated with an entry.
>
> In general, I don't like Map<Integer, *> because it seems like a hack, but
> really in this case it's a small trade-off.  Map is more concise.
> PriorityQueue is a little clearer.
>
> On Sat, Dec 20, 2014 at 4:50 PM, Hanson Char <hanson.char at gmail.com>
> wrote:
>>
>> Hi Joe,
>>
>> >A Queue should also retain less memory. A Map will tend to accumulate
>> both Entry and Integer instances over time.
>>
>> Using a Map, the signalNext would look something like below.  Doesn't
>> each entry get eventually removed from the map?  Why would they accumulate
>> over time?
>>
>> Regards,
>> Hanson
>>
>>     void signalNext(final int nextOrder) {
>>       lock.lock();
>>       try {
>>         this.nextOrder = nextOrder;
>>         Condition cond = map.remove(nextOrder);
>>         if (cond != null) {
>>             cond.signal();
>>         }
>>       } finally {
>>         lock.unlock();
>>       }
>>     }
>>   }
>>
>>
>> On Sat, Dec 20, 2014 at 4:39 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>> wrote:
>>
>>> I think PriorityQueue more clearly represents the function than Map does
>>> in this case.
>>>
>>> A Queue should also retain less memory. A Map will tend to accumulate
>>> both Entry and Integer instances over time.
>>>
>>> The ConcurrentMap does have the advantage of lock striping, which might
>>> be more performant if there is high contention.
>>>
>>> I think this is an interesting problem because there are quite a few
>>> different solutions, each with advantages and disadvantages in terms of
>>> performance and maintainability.
>>>
>>> If there were a PriorityQueue version of AbstractQueuedSynchronizer,
>>> then we could have yet another solution using the AQS's state to represent
>>> the next available order.
>>>
>>> On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char <hanson.char at gmail.com>
>>> wrote:
>>>>
>>>> This one looks interesting and educational.  The use of condition
>>>> enables us to suspend and resume specifically targeted threads without
>>>> resorting to the use of LockSupport.{part, unpark}.  Instead of using a
>>>> PriorityQueue, perhaps we can use a simple Map<Integer, Condition> instead
>>>> so we can get away without the extra Waiter inner class?
>>>>
>>>> Regards,
>>>> Hanson
>>>>
>>>> On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>> wrote:
>>>>
>>>>> To round out the selection, here's an implementation using a
>>>>> PriorityQueue of Conditions:
>>>>>
>>>>> class OrderedExecutor<T> {
>>>>>
>>>>>   static class Waiter implements Comparable<Waiter> {
>>>>>
>>>>>     final int order;
>>>>>     final Condition condition;
>>>>>
>>>>>     Waiter(int order, Condition condition) {
>>>>>       this.order = order;
>>>>>       this.condition = condition;
>>>>>     }
>>>>>
>>>>>     @Override
>>>>>     public int compareTo(Waiter waiter) {
>>>>>       return order - waiter.order;
>>>>>     }
>>>>>   }
>>>>>
>>>>>   final Lock lock = new ReentrantLock();
>>>>>   final Queue<Waiter> queue = new PriorityQueue<>();
>>>>>   int nextOrder; // 0 is next
>>>>>
>>>>>   public T execCallableInOrder(int order, Callable<T> callable) throws
>>>>> Exception {
>>>>>     assert order >= 0;
>>>>>     awaitTurn(order);
>>>>>     try {
>>>>>       return callable.call();
>>>>>     } finally {
>>>>>       signalNext(order + 1);
>>>>>     }
>>>>>   }
>>>>>
>>>>>   void awaitTurn(int order) {
>>>>>     lock.lock();
>>>>>     try {
>>>>>       Condition condition = null;
>>>>>       while (nextOrder != order) {
>>>>>         if (condition == null) {
>>>>>           condition = lock.newCondition();
>>>>>           queue.add(new Waiter(order, condition));
>>>>>         }
>>>>>         condition.awaitUninterruptibly();
>>>>>       }
>>>>>     } finally {
>>>>>       lock.unlock();
>>>>>     }
>>>>>   }
>>>>>
>>>>>   void signalNext(int nextOrder) {
>>>>>     lock.lock();
>>>>>     try {
>>>>>       this.nextOrder = nextOrder;
>>>>>       Waiter waiter = queue.peek();
>>>>>       if (waiter != null && waiter.order == nextOrder) {
>>>>>         queue.remove();
>>>>>         waiter.condition.signal();
>>>>>       }
>>>>>     } finally {
>>>>>       lock.unlock();
>>>>>     }
>>>>>   }
>>>>> }
>>>>>
>>>>> On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>> wrote:
>>>>>>
>>>>>> Suman,
>>>>>>
>>>>>> I would advise against using notify/wait.  It raises a red flag for a
>>>>>> lot of reviewers, including me.
>>>>>>
>>>>>> The problems I see in this implementation are:
>>>>>>
>>>>>> 1. Pre-allocation of locks is prohibited by (revised) problem
>>>>>> statement.
>>>>>>
>>>>>> Note that if pre-allocation were allowed, then an array would be more
>>>>>> efficient than a Map.
>>>>>>
>>>>>> 2. Access to currentAllowedOrder is not thread-safe but should be.
>>>>>>
>>>>>>
>>>>>> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <suman_krec at yahoo.com>
>>>>>> wrote:
>>>>>>>
>>>>>>> I have modified my solution to avoid notifyAll(). Let me know your
>>>>>>> feedback.
>>>>>>>
>>>>>>>
>>>>>>> public class OrderedExecutor
>>>>>>> {
>>>>>>> private int maxOrder;
>>>>>>> private int currentAllowedOrder;
>>>>>>> private Map<Integer, Object> map = new HashMap<Integer, Object>();
>>>>>>>     public OrderedExecutor(int n)
>>>>>>>     {
>>>>>>>          this.maxOrder = n;
>>>>>>>          this.currentAllowedOrder = 0;
>>>>>>>     for(int i = 0 ; i < this.maxOrder ; i++)
>>>>>>>     {
>>>>>>>     map.put(i, new Object());
>>>>>>>     }
>>>>>>>     }
>>>>>>>
>>>>>>>     public  Object execCriticalSectionInOrder(int order,
>>>>>>>                                               Callable<Object>
>>>>>>> callable)
>>>>>>>                                               throws Exception
>>>>>>>     {
>>>>>>> if (order >= this.maxOrder)
>>>>>>> {
>>>>>>> throw new Exception("Exceeds maximum order "+ maxOrder);
>>>>>>> }
>>>>>>>
>>>>>>> while(order != currentAllowedOrder)
>>>>>>> {
>>>>>>> synchronized (this.map.get(order))
>>>>>>> {
>>>>>>> this.map.get(order).wait();
>>>>>>> }
>>>>>>> }
>>>>>>>  try
>>>>>>> {
>>>>>>> return callable.call();
>>>>>>> }
>>>>>>>        finally
>>>>>>> {
>>>>>>>             currentAllowedOrder = currentAllowedOrder+1;
>>>>>>> synchronized (this.map.get(order+1))
>>>>>>>             {
>>>>>>>                 this.map.get(order+1).notify();
>>>>>>>             }
>>>>>>>        }
>>>>>>>     }
>>>>>>> }
>>>>>>>
>>>>>>>
>>>>>>>   ------------------------------
>>>>>>>  *From:* Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>>> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
>>>>>>> *Sent:* Friday, December 19, 2014 3:33 AM
>>>>>>>
>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution
>>>>>>> of critical sections?
>>>>>>>
>>>>>>> That would be "Tom" Cargill; link to paper:
>>>>>>>
>>>>>>>
>>>>>>> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>> I frown on use of notify[All]/wait because they make the code hard
>>>>>>> to maintain.
>>>>>>>
>>>>>>> In this case, with potentially lots of waiting threads, I would
>>>>>>> check out the "Specific Notification" pattern if I were determined to go
>>>>>>> the wait/notify route:
>>>>>>>
>>>>>>> Tim Cargill's paper is dated but still worth reading.
>>>>>>>
>>>>>>> Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ and
>>>>>>> Peter Haggar's article:
>>>>>>>
>>>>>>> http://www.ibm.com/developerworks/java/library/j-spnotif.html
>>>>>>>
>>>>>>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
>>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>>
>>>>>>>  Yes, no one said it is a good idea to always do that. When it is
>>>>>>> contended, most of the threads will wake up to only go back to sleep.
>>>>>>>
>>>>>>> The pattern you are after is usually called sequencer. You can see
>>>>>>> it used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe
>>>>>>> not that popular.
>>>>>>>
>>>>>>> The best solution will be lock-like, but the waiter nodes will
>>>>>>> contain the value they are waiting for - so only the specific threads get
>>>>>>> woken up. The solution with concurrent map is very similar, only with
>>>>>>> larger overhead from storing the index the thread is waiting for.
>>>>>>>
>>>>>>>
>>>>>>> Alex
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>>>>>
>>>>>>> Less overhead and simpler are a nice properties, even though at the
>>>>>>> expense of having to wake up all waiting threads just to find out the one
>>>>>>> with the right order to execute.  Still, this seems like a good tradeoff.
>>>>>>>
>>>>>>>  Thanks,
>>>>>>> Hanson
>>>>>>>
>>>>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <
>>>>>>> peter.levart at gmail.com> wrote:
>>>>>>>
>>>>>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>>>>>
>>>>>>> No, there is no difference. Peter didn't spot your entire method is
>>>>>>> synchronized, so spurious wakeup won't make progress until the owner of the
>>>>>>> lock exits the method.
>>>>>>>
>>>>>>> You could split the synchronization into two blocks - one
>>>>>>> encompassing the wait loop, the other in the finally block; but it may make
>>>>>>> no difference.
>>>>>>>
>>>>>>> Alex
>>>>>>>
>>>>>>>
>>>>>>> You're right, Alex. I'm so infected with park/unpark virus that I
>>>>>>> missed that ;-)
>>>>>>>
>>>>>>> Peter
>>>>>>>
>>>>>>>
>>>>>>> On 17/12/2014 18:36, suman shil wrote:
>>>>>>>
>>>>>>> Thanks peter for your reply. You are right. I should have
>>>>>>> incremented currentAllowedOrder in finally block.
>>>>>>>
>>>>>>> Suman
>>>>>>>
>>>>>>> ------------------------------------------------------------------------
>>>>>>> *From:* Peter Levart <peter.levart at gmail.com>
>>>>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>>>>>>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>>>>>>> concurrency-interest at cs.oswego.edu>
>>>>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of
>>>>>>> critical sections?
>>>>>>>
>>>>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>>>>>
>>>>>>> Thanks for your response. Will notifyAll() instead of notify() solve
>>>>>>> the problem?
>>>>>>>
>>>>>>>
>>>>>>> It will, but you should also account for "spurious" wake-ups. You
>>>>>>> should increment currentAllowedOrder only after return from callable.call
>>>>>>> (in finally block just before notifyAll()).
>>>>>>>
>>>>>>> Otherwise a nice solution - with minimal state, providing that not
>>>>>>> many threads meet at the same time...
>>>>>>>
>>>>>>> Regards, Peter
>>>>>>>
>>>>>>>  RegardsSuman
>>>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:
>>>>>>> oleksandr.otenko at oracle.com>
>>>>>>>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>;
>>>>>>> Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December
>>>>>>> 17, 2014 9:55 PM
>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>>>>> critical sections?
>>>>>>>       There is no guarantee you'll ever hand over the control to the
>>>>>>> right thread upon notify()
>>>>>>>     Alex
>>>>>>>
>>>>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>>>>       Hi, Following is my solution to solve this problem. Please let
>>>>>>> me know if I am missing something.
>>>>>>>    public class OrderedExecutor {  private int currentAllowedOrder =
>>>>>>> 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {
>>>>>>> this.maxLength = n;  } public synchronized Object
>>>>>>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>>>>>>                        throws Exception  { if (order >= maxLength)  {
>>>>>>> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
>>>>>>> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
>>>>>>> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
>>>>>>> }  } }
>>>>>>>    Regards Suman
>>>>>>>         From: Peter Levart<peter.levart at gmail.com> <mailto:
>>>>>>> peter.levart at gmail.com>
>>>>>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:
>>>>>>> hanson.char at gmail.com>    Cc: concurrency-interest<
>>>>>>> concurrency-interest at cs.oswego.edu> <mailto:
>>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14,
>>>>>>> 2014 11:01 PM
>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>>>>> critical sections?
>>>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>>>      Hi Peter,
>>>>>>>    Thanks for this proposed idea of using LockSupport. This begs the
>>>>>>> question: which one would you choose if you had all three (correct)
>>>>>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>>>>    Regards, Hanson
>>>>>>>     The Semaphore/CountDownLatch variants are equivalent if you
>>>>>>> don't need re-use. So any would do. They lack invalid-use detection. What
>>>>>>> happens if they are not used as intended? Semaphore variant acts
>>>>>>> differently than CountDownLatch variant. The low-level variant I  proposed
>>>>>>> detects invalid usage. So I would probably use this one. But the low level
>>>>>>> variant is harder to reason about it's correctness. I think it is correct,
>>>>>>> but you should show it to somebody else to confirm this.
>>>>>>>     Another question is whether you actually need this kind of
>>>>>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>>>>>> somebody could have an idea how to do that even more elegantly...
>>>>>>>     Regards, Peter
>>>>>>>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>    Hi Hanson,
>>>>>>>     This one is more low-level, but catches some invalid usages and
>>>>>>> is more resource-friendly:
>>>>>>>       public class OrderedExecutor {
>>>>>>>         public <T> T execCriticalSectionInOrder(
>>>>>>>           final int order,
>>>>>>>           final Supplier<T> criticalSection
>>>>>>>       ) throws InterruptedException {
>>>>>>>           if (order < 0) {
>>>>>>>                throw new IllegalArgumentException("'order' should be
>>>>>>> >= 0");
>>>>>>>           }
>>>>>>>           if (order > 0) {
>>>>>>>               waitForDone(order - 1);
>>>>>>>           }
>>>>>>>           try {
>>>>>>>               return criticalSection.get();
>>>>>>>           } finally {
>>>>>>>               notifyDone(order);
>>>>>>>           }
>>>>>>>       }
>>>>>>>         private static final Object DONE = new Object();
>>>>>>>       private final ConcurrentMap<Integer, Object> signals = new
>>>>>>> ConcurrentHashMap<>();
>>>>>>>         private void waitForDone(int order) throws
>>>>>>> InterruptedException {
>>>>>>>           Object sig = signals.putIfAbsent(order,
>>>>>>> Thread.currentThread());
>>>>>>>           if (sig != null && sig != DONE) {
>>>>>>>               throw new IllegalStateException();
>>>>>>>           }
>>>>>>>           while (sig != DONE) {
>>>>>>>               LockSupport.park();
>>>>>>>               if (Thread.interrupted()) {
>>>>>>>                   throw new InterruptedException();
>>>>>>>               }
>>>>>>>               sig = signals.get(order);
>>>>>>>           }
>>>>>>>       }
>>>>>>>         private void notifyDone(int order) {
>>>>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>>>>           if (sig instanceof Thread) {
>>>>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>>>>                   throw new IllegalStateException();
>>>>>>>               }
>>>>>>>               LockSupport.unpark((Thread) sig);
>>>>>>>           } else if (sig != null) {
>>>>>>>               throw new IllegalStateException();
>>>>>>>           }
>>>>>>>       }
>>>>>>>   }
>>>>>>>       Regards, Peter
>>>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>>>      Hi Peter,
>>>>>>>    Thanks for the suggestion, and sorry about not being clear about
>>>>>>> one important  detail: "n" is not known a priori when constructing an
>>>>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>>>>     If you know at least the upper bound of 'n', it can be used with
>>>>>>> such 'n'. Otherwise something that dynamically re-sizes the array could be
>>>>>>> devised. Or you could simply use a ConcurrentHashMap instead of array where
>>>>>>> keys are 'order' values:
>>>>>>>       public class OrderedExecutor<T> {
>>>>>>>         private final ConcurrentMap<Integer, CountDownLatch> latches
>>>>>>> = new ConcurrentHashMap<>();
>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>                                           final Supplier<T>
>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>           if (order > 0) {
>>>>>>>               latches.computeIfAbsent(order - 1, o -> new
>>>>>>> CountDownLatch(1)).await();
>>>>>>>           }
>>>>>>>           try {
>>>>>>>               return criticalSection.get();
>>>>>>>           } finally {
>>>>>>>               latches.computeIfAbsent(order, o -> new
>>>>>>> CountDownLatch(1)).countDown();
>>>>>>>           }
>>>>>>>       }
>>>>>>>   }
>>>>>>>       Regards, Peter
>>>>>>>           You guessed right: it's a one-shot object for a particular
>>>>>>> OrderedExecutor  instance, and "order" must be called indeed at most once.
>>>>>>>    Regards, Hanson
>>>>>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<
>>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>    Hi Hanson,
>>>>>>>     I don't think anything like that readily exists  in
>>>>>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>>>>>> with composition of existing primitives.  You haven't given any additional
>>>>>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>>>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>>>>> (and I think that only a one-shot object  makes sense here), an array of
>>>>>>> CountDownLatch(es) could be used:
>>>>>>>     public class OrderedExecutor<T> {
>>>>>>>       private final CountDownLatch[] latches;
>>>>>>>         public OrderedExecutor(int n) {
>>>>>>>           if (n < 1) throw new IllegalArgumentException("'n'  should
>>>>>>> be >= 1");
>>>>>>>           latches = new CountDownLatch[n - 1];
>>>>>>>           for (int i = 0; i < latches.length; i++) {
>>>>>>>               latches[i] = new CountDownLatch(1);
>>>>>>>           }
>>>>>>>       }
>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>                                            final Supplier<T>
>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>           if (order < 0 || order > latches.length)
>>>>>>>               throw new IllegalArgumentException("'order' should be
>>>>>>> [0..." +  latches.length + "]");
>>>>>>>           if (order > 0) {
>>>>>>>               latches[order - 1].await();
>>>>>>>           }
>>>>>>>           try {
>>>>>>>               return criticalSection.get();
>>>>>>>           } finally {
>>>>>>>               if (order < latches.length) {
>>>>>>>                   latches[order].countDown();
>>>>>>>               }
>>>>>>>           }
>>>>>>>       }
>>>>>>>   }
>>>>>>>       Regards, Peter
>>>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>>>          Hi, I am looking for a construct that can  be used to
>>>>>>> efficiently enforce  ordered execution of multiple critical sections, each
>>>>>>> calling from a  different thread. The calling threads may run in  parallel
>>>>>>> and may call the execution method out of order. The  perceived construct
>>>>>>> would therefore be responsible for re-ordering the execution of those
>>>>>>> threads, so that their critical  sections (and only the critical section)
>>>>>>> will be executed in order. Would something  like the following API already
>>>>>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>>>>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>>>>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>>>>>> most once with the given order, parking * and  unparking the current thread
>>>>>>> as  necessary so that all critical * sections executed  by different
>>>>>>> threads using this  executor take place in * the order from 1 to n
>>>>>>> consecutively. */ public T execCriticalSectionInOrder
>>>>>>> (  final int order, final Callable<T> criticalSection) throws
>>>>>>> InterruptedException; } Regards, Hanson
>>>>>>> _______________________________________________Concurrency-interest mailing
>>>>>>> listConcurrency-interest at cs.oswego.edu <mailto:
>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>> _______________________________________________
>>>>>>>   Concurrency-interest mailing list
>>>>>>>   Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141220/6bd1a2e0/attachment-0001.html>

From peter.levart at gmail.com  Sun Dec 21 06:37:34 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 21 Dec 2014 12:37:34 +0100
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
Message-ID: <5496B0FE.7070502@gmail.com>

Hi,

Here's a simple implementation (based on Suman Shil's idea) that stripes 
the waiting threads into multiple buckets:

public class StripedOrderedExecutor {

     private final MeetingPoint[] meetingPoints;

     public StripedOrderedExecutor(int stripeSize) {
         assert stripeSize > 0;
         meetingPoints = new MeetingPoint[stripeSize];
         for (int i = 0; i < stripeSize; i++) {
             meetingPoints[i] = new MeetingPoint();
         }
     }

     public <T> T execCriticalSectionInOrder(
         final int order,
         final Supplier<T> criticalSection
     ) throws InterruptedException {
         assert order >= 0;

         meetingPoints[order % meetingPoints.length].waitForGreen(order);
         try {
             return criticalSection.get();
         } finally {
             meetingPoints[(order + 1) % 
meetingPoints.length].notifyGreen(order + 1);
         }
     }

     private static class MeetingPoint {
         private int lastGreen;

         synchronized void waitForGreen(int order) throws 
InterruptedException {
             while (lastGreen != order) {
                 wait();
             }
         }

         synchronized void notifyGreen(int order) {
             lastGreen = order;
             notifyAll();
         }
     }
}


Regards, Peter





On 12/21/2014 02:35 AM, Hanson Char wrote:
> Thanks, Joe. That's an interesting point about the long term memory 
> impact of TreeMap vs HashMap - similar to LinkedList vs ArrayList.
>
> Regards,
> Hanson
>
> On Sat, Dec 20, 2014 at 5:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com 
> <mailto:joe.bowbeer at gmail.com>> wrote:
>
>     I see.  Yes, that addresses the removal problem, though note that
>     HashMap does not shrink when data is removed. Even if all keys are
>     removed from HashMap, the inner size of its table does not change.
>
>     So in this case maybe I would choose TreeMap as the
>     implementation. Its remove() method *does* remove all the memory
>     associated with an entry.
>
>     In general, I don't like Map<Integer, *> because it seems like a
>     hack, but really in this case it's a small trade-off.  Map is more
>     concise. PriorityQueue is a little clearer.
>
>     On Sat, Dec 20, 2014 at 4:50 PM, Hanson Char
>     <hanson.char at gmail.com <mailto:hanson.char at gmail.com>> wrote:
>
>         Hi Joe,
>
>         >A Queue should also retain less memory. A Map will tend to accumulate both Entry and Integer
>         instances over time.
>
>         Using a Map, the signalNext would look something like below. 
>         Doesn't each entry get eventually removed from the map?  Why
>         would they accumulate over time?
>
>         Regards,
>         Hanson
>
>             void signalNext(final int nextOrder) {
>               lock.lock();
>               try {
>                 this.nextOrder = nextOrder;
>                 Condition cond = map.remove(nextOrder);
>                 if (cond != null) {
>                     cond.signal();
>                 }
>               } finally {
>                 lock.unlock();
>               }
>             }
>           }
>
>
>         On Sat, Dec 20, 2014 at 4:39 PM, Joe Bowbeer
>         <joe.bowbeer at gmail.com <mailto:joe.bowbeer at gmail.com>> wrote:
>
>             I think PriorityQueue more clearly represents the function
>             than Map does in this case.
>
>             A Queue should also retain less memory. A Map will tend to
>             accumulate both Entry and Integer instances over time.
>
>             The ConcurrentMap does have the advantage of lock
>             striping, which might be more performant if there is high
>             contention.
>
>             I think this is an interesting problem because there are
>             quite a few different solutions, each with advantages and
>             disadvantages in terms of performance and maintainability.
>
>             If there were a PriorityQueue version of
>             AbstractQueuedSynchronizer, then we could have yet another
>             solution using the AQS's state to represent the next
>             available order.
>
>             On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char
>             <hanson.char at gmail.com <mailto:hanson.char at gmail.com>> wrote:
>
>                 This one looks interesting and educational.  The use
>                 of condition enables us to suspend and resume
>                 specifically targeted threads without resorting to the
>                 use of LockSupport.{part, unpark}.  Instead of using a
>                 PriorityQueue, perhaps we can use a simple
>                 Map<Integer, Condition> instead so we can get away
>                 without the extra Waiter inner class?
>
>                 Regards,
>                 Hanson
>
>                 On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer
>                 <joe.bowbeer at gmail.com <mailto:joe.bowbeer at gmail.com>>
>                 wrote:
>
>                     To round out the selection, here's an
>                     implementation using a PriorityQueue of Conditions:
>
>                     class OrderedExecutor<T> {
>
>                       static class Waiter implements Comparable<Waiter> {
>
>                         final int order;
>                         final Condition condition;
>
>                     Waiter(int order, Condition condition) {
>                     this.order = order;
>                     this.condition = condition;
>                         }
>
>                     @Override
>                     public int compareTo(Waiter waiter) {
>                     return order - waiter.order;
>                         }
>                       }
>
>                       final Lock lock = new ReentrantLock();
>                       final Queue<Waiter> queue = new PriorityQueue<>();
>                       int nextOrder; // 0 is next
>
>                       public T execCallableInOrder(int order,
>                     Callable<T> callable) throws Exception {
>                     assert order >= 0;
>                     awaitTurn(order);
>                         try {
>                     return callable.call();
>                         } finally {
>                     signalNext(order + 1);
>                         }
>                       }
>
>                       void awaitTurn(int order) {
>                     lock.lock();
>                         try {
>                     Condition condition = null;
>                     while (nextOrder != order) {
>                     if (condition == null) {
>                     condition = lock.newCondition();
>                     queue.add(new Waiter(order, condition));
>                             }
>                     condition.awaitUninterruptibly();
>                           }
>                         } finally {
>                     lock.unlock();
>                         }
>                       }
>
>                       void signalNext(int nextOrder) {
>                     lock.lock();
>                         try {
>                     this.nextOrder = nextOrder;
>                     Waiter waiter = queue.peek();
>                           if (waiter != null && waiter.order ==
>                     nextOrder) {
>                     queue.remove();
>                     waiter.condition.signal();
>                           }
>                         } finally {
>                     lock.unlock();
>                         }
>                       }
>                     }
>
>                     On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer
>                     <joe.bowbeer at gmail.com
>                     <mailto:joe.bowbeer at gmail.com>> wrote:
>
>                         Suman,
>
>                         I would advise against using notify/wait. It
>                         raises a red flag for a lot of reviewers,
>                         including me.
>
>                         The problems I see in this implementation are:
>
>                         1. Pre-allocation of locks is prohibited by
>                         (revised) problem statement.
>
>                         Note that if pre-allocation were allowed, then
>                         an array would be more efficient than a Map.
>
>                         2. Access to currentAllowedOrder is not
>                         thread-safe but should be.
>
>
>                         On Sat, Dec 20, 2014 at 6:04 AM, suman shil
>                         <suman_krec at yahoo.com
>                         <mailto:suman_krec at yahoo.com>> wrote:
>
>                             I have modified my solution to avoid
>                             notifyAll(). Let me know your feedback.
>
>
>                             public class OrderedExecutor
>                             {
>                             private int maxOrder;
>                             private int currentAllowedOrder;
>                             private Map<Integer, Object> map = new
>                             HashMap<Integer, Object>();
>                             public OrderedExecutor(int n)
>                                 {
>                             this.maxOrder = n;
>                             this.currentAllowedOrder = 0;
>                             for(int i = 0 ; i < this.maxOrder ; i++)
>                             {
>                             map.put(i, new Object());
>                             }
>                                 }
>                             public  Object
>                             execCriticalSectionInOrder(int order,
>                             Callable<Object> callable)
>                                     throws Exception
>                                 {
>                             if (order >= this.maxOrder)
>                             {
>                             throw new Exception("Exceeds maximum order
>                             "+ maxOrder);
>                             }
>                             while(order != currentAllowedOrder)
>                             {
>                             synchronized (this.map.get(order))
>                             {
>                             this.map.get(order).wait();
>                             }
>                             }
>                             try
>                             {
>                             return callable.call();
>                             }
>                              finally
>                             {
>                             currentAllowedOrder = currentAllowedOrder+1;
>                             synchronized (this.map.get(order+1))
>                               {
>                             this.map.get(order+1).notify();
>                               }
>                              }
>                                 }
>                             }
>
>
>                             ------------------------------------------------------------------------
>                             *From:* Joe Bowbeer <joe.bowbeer at gmail.com
>                             <mailto:joe.bowbeer at gmail.com>>
>                             *To:* concurrency-interest
>                             <concurrency-interest at cs.oswego.edu
>                             <mailto:concurrency-interest at cs.oswego.edu>>
>                             *Sent:* Friday, December 19, 2014 3:33 AM
>
>                             *Subject:* Re: [concurrency-interest]
>                             Enforcing ordered execution of critical
>                             sections?
>
>                             That would be "Tom" Cargill; link to paper:
>
>                             http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>
>
>
>                             On Thu, Dec 18, 2014 at 1:32 PM, Joe
>                             Bowbeer <joe.bowbeer at gmail.com
>                             <mailto:joe.bowbeer at gmail.com>> wrote:
>
>                                 I frown on use of notify[All]/wait
>                                 because they make the code hard to
>                                 maintain.
>
>                                 In this case, with potentially lots of
>                                 waiting threads, I would check out the
>                                 "Specific Notification" pattern if I
>                                 were determined to go the wait/notify
>                                 route:
>
>                                 Tim Cargill's paper is dated but still
>                                 worth reading.
>
>                                 Also see chapter 3.7.3 Specific
>                                 Notifications in Doug Lea's CPiJ and
>                                 Peter Haggar's article:
>
>                                 http://www.ibm.com/developerworks/java/library/j-spnotif.html
>
>                                 On Thu, Dec 18, 2014 at 1:02 PM,
>                                 Oleksandr Otenko
>                                 <oleksandr.otenko at oracle.com
>                                 <mailto:oleksandr.otenko at oracle.com>>
>                                 wrote:
>
>                                     Yes, no one said it is a good idea
>                                     to always do that. When it is
>                                     contended, most of the threads
>                                     will wake up to only go back to sleep.
>
>                                     The pattern you are after is
>                                     usually called sequencer. You can
>                                     see it used in TCP. I am not sure
>                                     why it wasn't implemented in
>                                     j.u.c. - maybe not that popular.
>
>                                     The best solution will be
>                                     lock-like, but the waiter nodes
>                                     will contain the value they are
>                                     waiting for - so only the specific
>                                     threads get woken up. The solution
>                                     with concurrent map is very
>                                     similar, only with larger overhead
>                                     from storing the index the thread
>                                     is waiting for.
>
>
>                                     Alex
>
>
>
>                                     On 18/12/2014 20:21, Hanson Char
>                                     wrote:
>>                                     Less overhead and simpler are a
>>                                     nice properties, even though at
>>                                     the expense of having to wake up
>>                                     all waiting threads just to find
>>                                     out the one with the right order
>>                                     to execute. Still, this seems
>>                                     like a good tradeoff.
>>
>>                                     Thanks,
>>                                     Hanson
>>
>>                                     On Wed, Dec 17, 2014 at 11:43 PM,
>>                                     Peter Levart
>>                                     <peter.levart at gmail.com
>>                                     <mailto:peter.levart at gmail.com>>
>>                                     wrote:
>>
>>                                         On 12/17/2014 08:15 PM,
>>                                         Oleksandr Otenko wrote:
>>
>>                                             No, there is no
>>                                             difference. Peter didn't
>>                                             spot your entire method
>>                                             is synchronized, so
>>                                             spurious wakeup won't
>>                                             make progress until the
>>                                             owner of the lock exits
>>                                             the method.
>>
>>                                             You could split the
>>                                             synchronization into two
>>                                             blocks - one encompassing
>>                                             the wait loop, the other
>>                                             in the finally block; but
>>                                             it may make no difference.
>>
>>                                             Alex
>>
>>
>>                                         You're right, Alex. I'm so
>>                                         infected with park/unpark
>>                                         virus that I missed that ;-)
>>
>>                                         Peter
>>
>>
>>                                             On 17/12/2014 18:36,
>>                                             suman shil wrote:
>>
>>                                                 Thanks peter for your
>>                                                 reply. You are right.
>>                                                 I should have
>>                                                 incremented
>>                                                 currentAllowedOrder
>>                                                 in finally block.
>>
>>                                                 Suman
>>                                                 ------------------------------------------------------------------------
>>                                                 *From:* Peter Levart
>>                                                 <peter.levart at gmail.com
>>                                                 <mailto:peter.levart at gmail.com>>
>>                                                 *To:* suman shil
>>                                                 <suman_krec at yahoo.com
>>                                                 <mailto:suman_krec at yahoo.com>>;
>>                                                 Oleksandr Otenko
>>                                                 <oleksandr.otenko at oracle.com
>>                                                 <mailto:oleksandr.otenko at oracle.com>>;
>>                                                 Concurrency-interest
>>                                                 <concurrency-interest at cs.oswego.edu
>>                                                 <mailto:concurrency-interest at cs.oswego.edu>>
>>                                                 *Sent:* Wednesday,
>>                                                 December 17, 2014
>>                                                 11:54 PM
>>                                                 *Subject:* Re:
>>                                                 [concurrency-interest] Enforcing
>>                                                 ordered execution of
>>                                                 critical sections?
>>
>>                                                 On 12/17/2014 06:46
>>                                                 PM, suman shil wrote:
>>
>>                                                     Thanks for your
>>                                                     response. Will
>>                                                     notifyAll()
>>                                                     instead of
>>                                                     notify() solve
>>                                                     the problem?
>>
>>
>>                                                 It will, but you
>>                                                 should also account
>>                                                 for "spurious"
>>                                                 wake-ups. You should
>>                                                 increment
>>                                                 currentAllowedOrder
>>                                                 only after return
>>                                                 from callable.call
>>                                                 (in finally block
>>                                                 just before notifyAll()).
>>
>>                                                 Otherwise a nice
>>                                                 solution - with
>>                                                 minimal state,
>>                                                 providing that not
>>                                                 many threads meet at
>>                                                 the same time...
>>
>>                                                 Regards, Peter
>>
>>                                                     RegardsSuman
>>                                                            From:
>>                                                     Oleksandr
>>                                                     Otenko<oleksandr.otenko at oracle.com
>>                                                     <mailto:oleksandr.otenko at oracle.com>>
>>                                                     <mailto:oleksandr.otenko at oracle.com
>>                                                     <mailto:oleksandr.otenko at oracle.com>>
>>                                                       To: suman
>>                                                     shil<suman_krec at yahoo.com
>>                                                     <mailto:suman_krec at yahoo.com>>
>>                                                     <mailto:suman_krec at yahoo.com
>>                                                     <mailto:suman_krec at yahoo.com>>;
>>                                                     Concurrency-interest<concurrency-interest at cs.oswego.edu
>>                                                     <mailto:concurrency-interest at cs.oswego.edu>>
>>                                                     <mailto:concurrency-interest at cs.oswego.edu
>>                                                     <mailto:concurrency-interest at cs.oswego.edu>> 
>>                                                       Sent:
>>                                                     Wednesday,
>>                                                     December 17, 2014
>>                                                     9:55 PM
>>                                                       Subject: Re:
>>                                                     [concurrency-interest]
>>                                                     Enforcing ordered
>>                                                     execution of
>>                                                     critical sections?
>>                                                           There is no
>>                                                     guarantee you'll
>>                                                     ever hand over
>>                                                     the control to
>>                                                     the right thread
>>                                                     upon notify()
>>                                                         Alex
>>
>>                                                     On 17/12/2014
>>                                                     14:07, suman shil
>>                                                     wrote:
>>                                                           Hi,
>>                                                     Following is my
>>                                                     solution to solve
>>                                                     this problem.
>>                                                     Please let me
>>                                                     know if I am
>>                                                     missing something.
>>                                                        public class
>>                                                     OrderedExecutor
>>                                                     {  private int
>>                                                     currentAllowedOrder
>>                                                     = 0;  private int
>>                                                     maxLength = 0; 
>>                                                     public
>>                                                     OrderedExecutor(int
>>                                                     n)  {
>>                                                     this.maxLength =
>>                                                     n;  } public
>>                                                     synchronized
>>                                                     Object
>>                                                     execCriticalSectionInOrder(
>>                                                     int order,
>>                                                     Callable<Object>
>>                                                     callable)  throws
>>                                                     Exception  { if
>>                                                     (order >=
>>                                                     maxLength)  {
>>                                                     throw new
>>                                                     Exception("Exceeds maximum
>>                                                     order "+
>>                                                     maxLength); }
>>                                                     while(order !=
>>                                                     currentAllowedOrder)
>>                                                     {  wait();  }  
>>                                                     try  {
>>                                                     currentAllowedOrder
>>                                                     =
>>                                                     currentAllowedOrder+1;
>>                                                     return
>>                                                     callable.call();
>>                                                     }  finally  {
>>                                                     notify();  } } }
>>                                                        Regards Suman
>>                                                      From: Peter
>>                                                     Levart<peter.levart at gmail.com
>>                                                     <mailto:peter.levart at gmail.com>>
>>                                                     <mailto:peter.levart at gmail.com
>>                                                     <mailto:peter.levart at gmail.com>>
>>                                                       To: Hanson
>>                                                     Char<hanson.char at gmail.com
>>                                                     <mailto:hanson.char at gmail.com>>
>>                                                     <mailto:hanson.char at gmail.com
>>                                                     <mailto:hanson.char at gmail.com>> 
>>                                                       Cc:
>>                                                     concurrency-interest<concurrency-interest at cs.oswego.edu
>>                                                     <mailto:concurrency-interest at cs.oswego.edu>>
>>                                                     <mailto:concurrency-interest at cs.oswego.edu
>>                                                     <mailto:concurrency-interest at cs.oswego.edu>> 
>>                                                       Sent: Sunday,
>>                                                     December 14, 2014
>>                                                     11:01 PM
>>                                                       Subject: Re:
>>                                                     [concurrency-interest]
>>                                                     Enforcing ordered
>>                                                     execution of
>>                                                     critical sections?
>>                                                               On
>>                                                     12/14/2014 06:11
>>                                                     PM, Hanson Char
>>                                                     wrote:
>>                                                          Hi Peter,
>>                                                        Thanks for
>>                                                     this proposed
>>                                                     idea of using
>>                                                     LockSupport. This
>>                                                     begs the
>>                                                     question: which
>>                                                     one would you
>>                                                     choose if you had
>>                                                     all three
>>                                                     (correct)
>>                                                     implementation
>>                                                     available?
>>                                                     (Semaphore,
>>                                                     CountDownLatch,
>>                                                     or LockSupport)?
>>                                                        Regards, Hanson
>>                                                         The
>>                                                     Semaphore/CountDownLatch
>>                                                     variants are
>>                                                     equivalent if you
>>                                                     don't need
>>                                                     re-use. So any
>>                                                     would do. They
>>                                                     lack invalid-use
>>                                                     detection. What
>>                                                     happens if they
>>                                                     are not used as
>>                                                     intended?
>>                                                     Semaphore variant
>>                                                     acts differently
>>                                                     than
>>                                                     CountDownLatch
>>                                                     variant. The
>>                                                     low-level variant
>>                                                     I proposed
>>                                                     detects invalid
>>                                                     usage. So I would
>>                                                     probably use this
>>                                                     one. But the low
>>                                                     level variant is
>>                                                     harder to reason
>>                                                     about it's
>>                                                     correctness. I
>>                                                     think it is
>>                                                     correct, but you
>>                                                     should show it to
>>                                                     somebody else to
>>                                                     confirm this.
>>                                                         Another
>>                                                     question is
>>                                                     whether you
>>                                                     actually need
>>                                                     this kind of
>>                                                     synchronizer.
>>                                                     Maybe if you
>>                                                     explained what
>>                                                     you are trying to
>>                                                     achieve, somebody
>>                                                     could have an
>>                                                     idea how to do
>>                                                     that even more
>>                                                     elegantly...
>>                                                         Regards, Peter
>>                                                          On Sun, Dec
>>                                                     14, 2014 at 9:01
>>                                                     AM, Peter
>>                                                     Levart<peter.levart at gmail.com
>>                                                     <mailto:peter.levart at gmail.com>>
>>                                                     <mailto:peter.levart at gmail.com
>>                                                     <mailto:peter.levart at gmail.com>> 
>>                                                     wrote:
>>                                                        Hi Hanson,
>>                                                         This one is
>>                                                     more low-level,
>>                                                     but catches some
>>                                                     invalid usages
>>                                                     and is more
>>                                                     resource-friendly:
>>                                                           public
>>                                                     class
>>                                                     OrderedExecutor {
>>                                                             public
>>                                                     <T> T
>>                                                     execCriticalSectionInOrder(
>>                                                     final int order,
>>                                                     final Supplier<T>
>>                                                     criticalSection
>>                                                           ) throws
>>                                                     InterruptedException
>>                                                     {
>>                                                               if
>>                                                     (order < 0) {
>>                                                      throw new
>>                                                     IllegalArgumentException("'order'
>>                                                     should be >= 0");
>>                                                               }
>>                                                               if
>>                                                     (order > 0) {
>>                                                     waitForDone(order
>>                                                     - 1);
>>                                                               }
>>                                                               try {
>>                                                     return
>>                                                     criticalSection.get();
>>                                                               } finally {
>>                                                     notifyDone(order);
>>                                                               }
>>                                                           }
>>                                                     private static
>>                                                     final Object DONE
>>                                                     = new Object();
>>                                                           private
>>                                                     final
>>                                                     ConcurrentMap<Integer,
>>                                                     Object> signals =
>>                                                     new
>>                                                     ConcurrentHashMap<>();
>>                                                     private void
>>                                                     waitForDone(int
>>                                                     order) throws
>>                                                     InterruptedException
>>                                                     {
>>                                                     Object sig =
>>                                                     signals.putIfAbsent(order,
>>                                                     Thread.currentThread());
>>                                                               if (sig
>>                                                     != null && sig !=
>>                                                     DONE) {
>>                                                     throw new
>>                                                     IllegalStateException();
>>                                                               }
>>                                                     while (sig != DONE) {
>>                                                     LockSupport.park();
>>                                                     if
>>                                                     (Thread.interrupted())
>>                                                     {
>>                                                         throw new
>>                                                     InterruptedException();
>>                                                     }
>>                                                     sig =
>>                                                     signals.get(order);
>>                                                               }
>>                                                           }
>>                                                     private void
>>                                                     notifyDone(int
>>                                                     order) {
>>                                                     Object sig =
>>                                                     signals.putIfAbsent(order,
>>                                                     DONE);
>>                                                               if (sig
>>                                                     instanceof Thread) {
>>                                                     if
>>                                                     (!signals.replace(order,
>>                                                     sig, DONE)) {
>>                                                         throw new
>>                                                     IllegalStateException();
>>                                                     }
>>                                                     LockSupport.unpark((Thread)
>>                                                     sig);
>>                                                               } else
>>                                                     if (sig != null) {
>>                                                     throw new
>>                                                     IllegalStateException();
>>                                                               }
>>                                                           }
>>                                                       }
>>                                                           Regards, Peter
>>                                                         On 12/14/2014
>>                                                     05:08 PM, Peter
>>                                                     Levart wrote:
>>                                                            On
>>                                                     12/14/2014 04:20
>>                                                     PM, Hanson Char
>>                                                     wrote:
>>                                                          Hi Peter,
>>                                                        Thanks for the
>>                                                     suggestion, and
>>                                                     sorry about not
>>                                                     being clear about
>>                                                     one important
>>                                                     detail: "n" is
>>                                                     not known a
>>                                                     priori when
>>                                                     constructing an
>>                                                     OrderedExecutor.
>>                                                     Does this mean
>>                                                     the use of
>>                                                     CountDownLatch is
>>                                                     ruled out?
>>                                                         If you know
>>                                                     at least the
>>                                                     upper bound of
>>                                                     'n', it can be
>>                                                     used with such
>>                                                     'n'. Otherwise
>>                                                     something that
>>                                                     dynamically
>>                                                     re-sizes the
>>                                                     array could be
>>                                                     devised. Or you
>>                                                     could simply use
>>                                                     a
>>                                                     ConcurrentHashMap
>>                                                     instead of array
>>                                                     where keys are
>>                                                     'order' values:
>>                                                           public
>>                                                     class
>>                                                     OrderedExecutor<T> {
>>                                                     private final
>>                                                     ConcurrentMap<Integer,
>>                                                     CountDownLatch>
>>                                                     latches = new
>>                                                     ConcurrentHashMap<>();
>>                                                             public T
>>                                                     execCriticalSectionInOrder(final
>>                                                     int order,
>>                                                     final Supplier<T>
>>                                                     criticalSection)
>>                                                     throws
>>                                                     InterruptedException
>>                                                     {
>>                                                               if
>>                                                     (order > 0) {
>>                                                     latches.computeIfAbsent(order
>>                                                     - 1, o -> new
>>                                                     CountDownLatch(1)).await();
>>                                                               }
>>                                                               try {
>>                                                     return
>>                                                     criticalSection.get();
>>                                                               } finally {
>>                                                     latches.computeIfAbsent(order,
>>                                                     o -> new
>>                                                     CountDownLatch(1)).countDown();
>>                                                               }
>>                                                           }
>>                                                       }
>>                                                           Regards, Peter
>>                                                               You
>>                                                     guessed right:
>>                                                     it's a one-shot
>>                                                     object for a
>>                                                     particular
>>                                                     OrderedExecutor
>>                                                     instance, and
>>                                                     "order" must be
>>                                                     called indeed at
>>                                                     most once.
>>                                                        Regards, Hanson
>>                                                       On Sun, Dec 14,
>>                                                     2014 at 2:21 AM,
>>                                                     Peter
>>                                                     Levart<peter.levart at gmail.com
>>                                                     <mailto:peter.levart at gmail.com>>
>>                                                     <mailto:peter.levart at gmail.com
>>                                                     <mailto:peter.levart at gmail.com>> 
>>                                                     wrote:
>>                                                        Hi Hanson,
>>                                                         I don't think
>>                                                     anything like
>>                                                     that readily
>>                                                     exists  in
>>                                                     java.lang.concurrent,
>>                                                     but what you
>>                                                     describe should
>>                                                     be possible to
>>                                                     achieve with
>>                                                     composition of
>>                                                     existing
>>                                                     primitives. You
>>                                                     haven't given any
>>                                                     additional hints
>>                                                     to what your
>>                                                     OrderedExecutor
>>                                                     should behave
>>                                                     like. Should it
>>                                                     be a one-shot
>>                                                     object (like
>>                                                     CountDownLatch)
>>                                                     or a re-usable
>>                                                     one (like
>>                                                     CyclicBarrier)?
>>                                                     Will
>>                                                     execCriticalSectionInOrder()
>>                                                     for a particular
>>                                                     OrderedExecutor
>>                                                     instance and
>>                                                     'order' value be
>>                                                     called at most
>>                                                     once? If yes (and
>>                                                     I think that only
>>                                                     a one-shot
>>                                                     object  makes
>>                                                     sense here), an
>>                                                     array of
>>                                                     CountDownLatch(es) could
>>                                                     be used:
>>                                                         public class
>>                                                     OrderedExecutor<T> {
>>                                                           private
>>                                                     final
>>                                                     CountDownLatch[]
>>                                                     latches;
>>                                                             public
>>                                                     OrderedExecutor(int
>>                                                     n) {
>>                                                               if (n <
>>                                                     1) throw new
>>                                                     IllegalArgumentException("'n'
>>                                                     should be >= 1");
>>                                                     latches = new
>>                                                     CountDownLatch[n
>>                                                     - 1];
>>                                                               for
>>                                                     (int i = 0; i <
>>                                                     latches.length;
>>                                                     i++) {
>>                                                     latches[i] = new
>>                                                     CountDownLatch(1);
>>                                                               }
>>                                                           }
>>                                                             public T
>>                                                     execCriticalSectionInOrder(final
>>                                                     int order,
>>                                                      final
>>                                                     Supplier<T>
>>                                                     criticalSection)
>>                                                     throws
>>                                                     InterruptedException
>>                                                     {
>>                                                               if
>>                                                     (order < 0 ||
>>                                                     order >
>>                                                     latches.length)
>>                                                     throw new
>>                                                     IllegalArgumentException("'order'
>>                                                     should be [0..."
>>                                                     + latches.length
>>                                                     + "]");
>>                                                               if
>>                                                     (order > 0) {
>>                                                     latches[order -
>>                                                     1].await();
>>                                                               }
>>                                                               try {
>>                                                     return
>>                                                     criticalSection.get();
>>                                                               } finally {
>>                                                     if (order <
>>                                                     latches.length) {
>>                                                     latches[order].countDown();
>>                                                     }
>>                                                               }
>>                                                           }
>>                                                       }
>>                                                           Regards, Peter
>>                                                         On 12/14/2014
>>                                                     05:26 AM, Hanson
>>                                                     Char wrote:
>>                                                       Hi, I am
>>                                                     looking for a
>>                                                     construct that
>>                                                     can  be used to
>>                                                     efficiently
>>                                                     enforce ordered
>>                                                     execution of
>>                                                     multiple critical
>>                                                     sections, each
>>                                                     calling from a 
>>                                                     different thread.
>>                                                     The calling
>>                                                     threads may run
>>                                                     in parallel and
>>                                                     may call the
>>                                                     execution method
>>                                                     out of order. The
>>                                                     perceived
>>                                                     construct would
>>                                                     therefore be
>>                                                     responsible for
>>                                                     re-ordering the
>>                                                     execution of
>>                                                     those threads, so
>>                                                     that their
>>                                                     critical sections
>>                                                     (and only the
>>                                                     critical section)
>>                                                     will be executed
>>                                                     in order. Would
>>                                                     something like
>>                                                     the following API
>>                                                     already exist?
>>                                                     /** * Used to
>>                                                     enforce ordered
>>                                                     execution of
>>                                                     critical sections
>>                                                     calling from
>>                                                     multiple *
>>                                                     threads, parking
>>                                                     and unparking the
>>                                                     threads as
>>                                                     necessary. */
>>                                                     public class
>>                                                     OrderedExecutor<T> {
>>                                                     /** * Executes a
>>                                                     critical section
>>                                                     at most once with
>>                                                     the given order,
>>                                                     parking * and
>>                                                     unparking the
>>                                                     current thread
>>                                                     as  necessary so
>>                                                     that all critical
>>                                                     * sections
>>                                                     executed  by
>>                                                     different threads
>>                                                     using this 
>>                                                     executor take
>>                                                     place in * the
>>                                                     order from 1 to n
>>                                                     consecutively. */
>>                                                     public T
>>                                                     execCriticalSectionInOrder
>>                                                     (  final int
>>                                                     order, final
>>                                                     Callable<T>
>>                                                     criticalSection)
>>                                                     throws
>>                                                     InterruptedException;
>>                                                     } Regards, Hanson
>>                                                     _______________________________________________Concurrency-interest
>>                                                     mailing
>>                                                     listConcurrency-interest at cs.oswego.edu
>>                                                     <mailto:listConcurrency-interest at cs.oswego.edu>
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu>>
>>                                                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>                                                     _______________________________________________
>>                                                     Concurrency-interest
>>                                                     mailing list
>>                                                     Concurrency-interest at cs.oswego.edu
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu>>
>>                                                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>                                                     _______________________________________________
>>                                                     Concurrency-interest
>>                                                     mailing list
>>                                                     Concurrency-interest at cs.oswego.edu
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu>>
>>                                                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>>                                                     _______________________________________________
>>                                                     Concurrency-interest
>>                                                     mailing list
>>                                                     Concurrency-interest at cs.oswego.edu
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu
>>                                                     <mailto:Concurrency-interest at cs.oswego.edu>>
>>                                                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>>
>>
>>                                         _______________________________________________
>>                                         Concurrency-interest mailing list
>>                                         Concurrency-interest at cs.oswego.edu
>>                                         <mailto:Concurrency-interest at cs.oswego.edu>
>>                                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>                                     _______________________________________________
>                                     Concurrency-interest mailing list
>                                     Concurrency-interest at cs.oswego.edu
>                                     <mailto:Concurrency-interest at cs.oswego.edu>
>                                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>                             _______________________________________________
>                             Concurrency-interest mailing list
>                             Concurrency-interest at cs.oswego.edu
>                             <mailto:Concurrency-interest at cs.oswego.edu>
>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>                             _______________________________________________
>                             Concurrency-interest mailing list
>                             Concurrency-interest at cs.oswego.edu
>                             <mailto:Concurrency-interest at cs.oswego.edu>
>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>                     Concurrency-interest at cs.oswego.edu
>                     <mailto:Concurrency-interest at cs.oswego.edu>
>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/d86e3678/attachment-0001.html>

From dl at cs.oswego.edu  Sun Dec 21 09:20:34 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 21 Dec 2014 09:20:34 -0500
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>
	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
	<5495949E.1030500@cs.oswego.edu>
	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
Message-ID: <5496D732.3010308@cs.oswego.edu>

On 12/20/2014 04:11 PM, Benjamin Manes wrote:
>     If an efficient internal tryLock intrinsic became available, we might do
>     more, because throwing here helps diagnose unrelated-looking problems in
>     other threads, and performing checks only when lock is unavailable costs
>     almost nothing.
>
>
> Is the tryLock intrinsic necessary to be performant and safe? I tried using
> Thread.holdsLock() and it halved the performance. I then tried stashing the
> thread's id into an extra field only stored on ReservationNode and saw equal
> performance on my 4-core/8HT macbook.

This is worth considering, but it only detects one form of
misbehavior when the bin doesn't hold any other nodes.
Otherwise, the first node need not be a ReservationNode. Also,
it doesn't alone deal with interactions with other
lambda-accepting methods.

Here is some background context in case others have any
thoughts about improving support:

By popular demand, CHM strengthens ConcurrentMap.computeIfAbsent
specs (similarly for computeIfPresent etc) to guarantee atomicity
while the function is executed. This requires that we hold a lock
(for the bin in which the entry will be placed) during the call
to the supplied function. We tell users not to use functions
that have any other potential side effects on the map,
but we cannot enforce it. Holding a lock adds to the possible
bad outcomes of violating the side-effect-freedom requirement.
We'd like to be helpful in diagnosing violations, because
these outcomes can be very mysterious-looking.

CHM itself cannot directly help with some potential
lock-based errors. For example, if the function for
computeIfAbsent(key1) can invoke computeIfAbsent(key2)
and vice versa, and the keys hash to different bins, then they
can encounter a classic deadlock. But this and related cases
can be diagnosed using existing JVM cycle-detection tools.

The nature of other potential lock-based errors depends
on the kind of lock used. Some preliminary versions of
jdk8 CHM used non-reentrant locks. (It now uses reentrant
locks.)

With non-reentrant locks, cases of infinite recursion
(i.e., m.computeIfAbsent(k,f) somehow calling itself) result in
lockups rather than infinite loops. Similarly for mutually
recursive computeIfAbsent calls for key1 and key2 that happen
to hash into the same bin. However these cases can be detected
with very low performance impact (by doing so only if a tryLock
fails). Which led us to add CHM method throws specs to allow
this. Even though jdk8 CHM now uses reentrant locks, we want to
keep open the ability to use potentially faster non-reentrant
approaches in the future.

With reentrant locks, infinite computeIfAbsent recursions
on the same key are not all that different than any other
case of infinite recursion. This is "detectable" only by
throwing after the number of calls exceeds a threshold.
But the threshold can arguably be set to 1 because of the
no-side-effects rule, and one special case of this check
(as above, via thread IDs in ReservationNodes) can be done
with only small impact. But it's not clear to me whether
this is worthwhile by itself.

The remaining mysterious cases are nested computeIfAbsent
calls for different keys that happen to hash to the same bin.
These conceptually ought to deadlock. Instead they will
sometimes happen to "work" but other times loop.
(This appears to be the underlying undetected user bug in
JDK-8062841.) It seems that this is beyond our current ability
to diagnose, but suggestions would be welcome.

-Doug


From viktor.klang at gmail.com  Sun Dec 21 09:59:05 2014
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 21 Dec 2014 15:59:05 +0100
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <5496D732.3010308@cs.oswego.edu>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>
	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
	<5495949E.1030500@cs.oswego.edu>
	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
	<5496D732.3010308@cs.oswego.edu>
Message-ID: <CANPzfU9qoJyUgPwK3RtPeca6n9Vif6kq1yg98QbVByw58sn3aA@mail.gmail.com>

For "computeIfAbsent"-style maps which are intended for multithreaded use,
I tend to prefer to use ConcurrentMap[Key, CompletionStage[Value]], this
means that writers are not blocked and readers aren't blocked unless they
choose to block on the returned CompletionStage. This also has the added
benefit of making it easy to designate where the computation should be
executed.

On Sun, Dec 21, 2014 at 3:20 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/20/2014 04:11 PM, Benjamin Manes wrote:
>
>>     If an efficient internal tryLock intrinsic became available, we might
>> do
>>     more, because throwing here helps diagnose unrelated-looking problems
>> in
>>     other threads, and performing checks only when lock is unavailable
>> costs
>>     almost nothing.
>>
>>
>> Is the tryLock intrinsic necessary to be performant and safe? I tried
>> using
>> Thread.holdsLock() and it halved the performance. I then tried stashing
>> the
>> thread's id into an extra field only stored on ReservationNode and saw
>> equal
>> performance on my 4-core/8HT macbook.
>>
>
> This is worth considering, but it only detects one form of
> misbehavior when the bin doesn't hold any other nodes.
> Otherwise, the first node need not be a ReservationNode. Also,
> it doesn't alone deal with interactions with other
> lambda-accepting methods.
>
> Here is some background context in case others have any
> thoughts about improving support:
>
> By popular demand, CHM strengthens ConcurrentMap.computeIfAbsent
> specs (similarly for computeIfPresent etc) to guarantee atomicity
> while the function is executed. This requires that we hold a lock
> (for the bin in which the entry will be placed) during the call
> to the supplied function. We tell users not to use functions
> that have any other potential side effects on the map,
> but we cannot enforce it. Holding a lock adds to the possible
> bad outcomes of violating the side-effect-freedom requirement.
> We'd like to be helpful in diagnosing violations, because
> these outcomes can be very mysterious-looking.
>
> CHM itself cannot directly help with some potential
> lock-based errors. For example, if the function for
> computeIfAbsent(key1) can invoke computeIfAbsent(key2)
> and vice versa, and the keys hash to different bins, then they
> can encounter a classic deadlock. But this and related cases
> can be diagnosed using existing JVM cycle-detection tools.
>
> The nature of other potential lock-based errors depends
> on the kind of lock used. Some preliminary versions of
> jdk8 CHM used non-reentrant locks. (It now uses reentrant
> locks.)
>
> With non-reentrant locks, cases of infinite recursion
> (i.e., m.computeIfAbsent(k,f) somehow calling itself) result in
> lockups rather than infinite loops. Similarly for mutually
> recursive computeIfAbsent calls for key1 and key2 that happen
> to hash into the same bin. However these cases can be detected
> with very low performance impact (by doing so only if a tryLock
> fails). Which led us to add CHM method throws specs to allow
> this. Even though jdk8 CHM now uses reentrant locks, we want to
> keep open the ability to use potentially faster non-reentrant
> approaches in the future.
>
> With reentrant locks, infinite computeIfAbsent recursions
> on the same key are not all that different than any other
> case of infinite recursion. This is "detectable" only by
> throwing after the number of calls exceeds a threshold.
> But the threshold can arguably be set to 1 because of the
> no-side-effects rule, and one special case of this check
> (as above, via thread IDs in ReservationNodes) can be done
> with only small impact. But it's not clear to me whether
> this is worthwhile by itself.
>
> The remaining mysterious cases are nested computeIfAbsent
> calls for different keys that happen to hash to the same bin.
> These conceptually ought to deadlock. Instead they will
> sometimes happen to "work" but other times loop.
> (This appears to be the underlying undetected user bug in
> JDK-8062841.) It seems that this is beyond our current ability
> to diagnose, but suggestions would be welcome.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/ed5573db/attachment.html>

From dl at cs.oswego.edu  Sun Dec 21 10:47:29 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 21 Dec 2014 10:47:29 -0500
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <CANPzfU9qoJyUgPwK3RtPeca6n9Vif6kq1yg98QbVByw58sn3aA@mail.gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>	<5495949E.1030500@cs.oswego.edu>	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>	<5496D732.3010308@cs.oswego.edu>
	<CANPzfU9qoJyUgPwK3RtPeca6n9Vif6kq1yg98QbVByw58sn3aA@mail.gmail.com>
Message-ID: <5496EB91.9090504@cs.oswego.edu>

On 12/21/2014 09:59 AM, Viktor Klang wrote:
> For "computeIfAbsent"-style maps which are intended for multithreaded use, I
> tend to prefer to use ConcurrentMap[Key, CompletionStage[Value]],

Right. Prior to jdk8, we suggested variations of this and other
techniques that remain preferable in many cases. However, the
most common CHM user error was trying (but failing) to emulate
what is now supported as computeIfAbsent. This is an improvement,
but leads to new (much less common) kinds of potential errors.
We cannot automatically eliminate or trap all of them. It
might be possible to do more than we do now, either internally
(inside CHM) or, more likely, with improved developer tools.

-Doug


From hanson.char at gmail.com  Sun Dec 21 13:03:19 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 21 Dec 2014 10:03:19 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <5496B0FE.7070502@gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
	<5496B0FE.7070502@gmail.com>
Message-ID: <CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>

Interesting - now that each order is associated with a different object
monitor, the wait/notifyAll would at most wake up one thread instead of
all.  In fact, it seems the use of notify (instead of notifyAll) would
suffice.  Simple and clever, but it does require the "stripeSize" to be
known a priori.

Regards,
Hanson

On Sun, Dec 21, 2014 at 3:37 AM, Peter Levart <peter.levart at gmail.com>
wrote:

>  Hi,
>
> Here's a simple implementation (based on Suman Shil's idea) that stripes
> the waiting threads into multiple buckets:
>
> public class StripedOrderedExecutor {
>
>     private final MeetingPoint[] meetingPoints;
>
>     public StripedOrderedExecutor(int stripeSize) {
>         assert stripeSize > 0;
>         meetingPoints = new MeetingPoint[stripeSize];
>         for (int i = 0; i < stripeSize; i++) {
>             meetingPoints[i] = new MeetingPoint();
>         }
>     }
>
>     public <T> T execCriticalSectionInOrder(
>         final int order,
>         final Supplier<T> criticalSection
>     ) throws InterruptedException {
>         assert order >= 0;
>
>         meetingPoints[order % meetingPoints.length].waitForGreen(order);
>         try {
>             return criticalSection.get();
>         } finally {
>             meetingPoints[(order + 1) %
> meetingPoints.length].notifyGreen(order + 1);
>         }
>     }
>
>     private static class MeetingPoint {
>         private int lastGreen;
>
>         synchronized void waitForGreen(int order) throws
> InterruptedException {
>             while (lastGreen != order) {
>                 wait();
>             }
>         }
>
>         synchronized void notifyGreen(int order) {
>             lastGreen = order;
>             notifyAll();
>         }
>     }
> }
>
>
> Regards, Peter
>
>
>
>
>
> On 12/21/2014 02:35 AM, Hanson Char wrote:
>
> Thanks, Joe. That's an interesting point about the long term memory impact
> of TreeMap vs HashMap - similar to LinkedList vs ArrayList.
>
>  Regards,
> Hanson
>
> On Sat, Dec 20, 2014 at 5:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
> wrote:
>
>> I see.  Yes, that addresses the removal problem, though note that HashMap
>> does not shrink when data is removed. Even if all keys are removed from
>> HashMap, the inner size of its table does not change.
>>
>>  So in this case maybe I would choose TreeMap as the implementation. Its
>> remove() method *does* remove all the memory associated with an entry.
>>
>>  In general, I don't like Map<Integer, *> because it seems like a hack,
>> but really in this case it's a small trade-off.  Map is more concise.
>> PriorityQueue is a little clearer.
>>
>> On Sat, Dec 20, 2014 at 4:50 PM, Hanson Char <hanson.char at gmail.com>
>> wrote:
>>>
>>> Hi Joe,
>>>
>>>  >A Queue should also retain less memory. A Map will tend to accumulate
>>> both Entry and Integer instances over time.
>>>
>>>  Using a Map, the signalNext would look something like below.  Doesn't
>>> each entry get eventually removed from the map?  Why would they accumulate
>>> over time?
>>>
>>>  Regards,
>>> Hanson
>>>
>>>      void signalNext(final int nextOrder) {
>>>        lock.lock();
>>>       try {
>>>         this.nextOrder = nextOrder;
>>>          Condition cond = map.remove(nextOrder);
>>>         if (cond != null) {
>>>             cond.signal();
>>>         }
>>>       } finally {
>>>         lock.unlock();
>>>       }
>>>     }
>>>   }
>>>
>>>
>>> On Sat, Dec 20, 2014 at 4:39 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>> wrote:
>>>
>>>>  I think PriorityQueue more clearly represents the function than Map
>>>> does in this case.
>>>>
>>>>  A Queue should also retain less memory. A Map will tend to accumulate
>>>> both Entry and Integer instances over time.
>>>>
>>>>  The ConcurrentMap does have the advantage of lock striping, which
>>>> might be more performant if there is high contention.
>>>>
>>>>  I think this is an interesting problem because there are quite a few
>>>> different solutions, each with advantages and disadvantages in terms of
>>>> performance and maintainability.
>>>>
>>>>  If there were a PriorityQueue version of AbstractQueuedSynchronizer,
>>>> then we could have yet another solution using the AQS's state to represent
>>>> the next available order.
>>>>
>>>> On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char <hanson.char at gmail.com>
>>>> wrote:
>>>>>
>>>>> This one looks interesting and educational.  The use of condition
>>>>> enables us to suspend and resume specifically targeted threads without
>>>>> resorting to the use of LockSupport.{part, unpark}.  Instead of using a
>>>>> PriorityQueue, perhaps we can use a simple Map<Integer, Condition> instead
>>>>> so we can get away without the extra Waiter inner class?
>>>>>
>>>>>  Regards,
>>>>> Hanson
>>>>>
>>>>> On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>> wrote:
>>>>>
>>>>>> To round out the selection, here's an implementation using a
>>>>>> PriorityQueue of Conditions:
>>>>>>
>>>>>>  class OrderedExecutor<T> {
>>>>>>
>>>>>>    static class Waiter implements Comparable<Waiter> {
>>>>>>
>>>>>>      final int order;
>>>>>>     final Condition condition;
>>>>>>
>>>>>>      Waiter(int order, Condition condition) {
>>>>>>       this.order = order;
>>>>>>       this.condition = condition;
>>>>>>     }
>>>>>>
>>>>>>      @Override
>>>>>>     public int compareTo(Waiter waiter) {
>>>>>>       return order - waiter.order;
>>>>>>     }
>>>>>>   }
>>>>>>
>>>>>>    final Lock lock = new ReentrantLock();
>>>>>>   final Queue<Waiter> queue = new PriorityQueue<>();
>>>>>>   int nextOrder; // 0 is next
>>>>>>
>>>>>>    public T execCallableInOrder(int order, Callable<T> callable)
>>>>>> throws Exception {
>>>>>>     assert order >= 0;
>>>>>>     awaitTurn(order);
>>>>>>     try {
>>>>>>       return callable.call();
>>>>>>     } finally {
>>>>>>       signalNext(order + 1);
>>>>>>     }
>>>>>>   }
>>>>>>
>>>>>>    void awaitTurn(int order) {
>>>>>>     lock.lock();
>>>>>>     try {
>>>>>>       Condition condition = null;
>>>>>>       while (nextOrder != order) {
>>>>>>         if (condition == null) {
>>>>>>           condition = lock.newCondition();
>>>>>>           queue.add(new Waiter(order, condition));
>>>>>>         }
>>>>>>         condition.awaitUninterruptibly();
>>>>>>       }
>>>>>>     } finally {
>>>>>>       lock.unlock();
>>>>>>     }
>>>>>>   }
>>>>>>
>>>>>>    void signalNext(int nextOrder) {
>>>>>>     lock.lock();
>>>>>>     try {
>>>>>>       this.nextOrder = nextOrder;
>>>>>>       Waiter waiter = queue.peek();
>>>>>>       if (waiter != null && waiter.order == nextOrder) {
>>>>>>         queue.remove();
>>>>>>         waiter.condition.signal();
>>>>>>       }
>>>>>>     } finally {
>>>>>>       lock.unlock();
>>>>>>     }
>>>>>>   }
>>>>>> }
>>>>>>
>>>>>> On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>> wrote:
>>>>>>>
>>>>>>> Suman,
>>>>>>>
>>>>>>>  I would advise against using notify/wait.  It raises a red flag
>>>>>>> for a lot of reviewers, including me.
>>>>>>>
>>>>>>>  The problems I see in this implementation are:
>>>>>>>
>>>>>>>  1. Pre-allocation of locks is prohibited by (revised) problem
>>>>>>> statement.
>>>>>>>
>>>>>>>  Note that if pre-allocation were allowed, then an array would be
>>>>>>> more efficient than a Map.
>>>>>>>
>>>>>>>  2. Access to currentAllowedOrder is not thread-safe but should be.
>>>>>>>
>>>>>>>
>>>>>>> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <suman_krec at yahoo.com>
>>>>>>> wrote:
>>>>>>>>
>>>>>>>>  I have modified my solution to avoid notifyAll(). Let me know
>>>>>>>> your feedback.
>>>>>>>>
>>>>>>>>
>>>>>>>>  public class OrderedExecutor
>>>>>>>> {
>>>>>>>>  private int maxOrder;
>>>>>>>>  private int currentAllowedOrder;
>>>>>>>>  private Map<Integer, Object> map = new HashMap<Integer, Object>();
>>>>>>>>     public OrderedExecutor(int n)
>>>>>>>>     {
>>>>>>>>          this.maxOrder = n;
>>>>>>>>          this.currentAllowedOrder = 0;
>>>>>>>>     for(int i = 0 ; i < this.maxOrder ; i++)
>>>>>>>>     {
>>>>>>>>     map.put(i, new Object());
>>>>>>>>     }
>>>>>>>>     }
>>>>>>>>
>>>>>>>>     public  Object execCriticalSectionInOrder(int order,
>>>>>>>>                                                Callable<Object>
>>>>>>>> callable)
>>>>>>>>                                               throws Exception
>>>>>>>>     {
>>>>>>>>  if (order >= this.maxOrder)
>>>>>>>>  {
>>>>>>>>  throw new Exception("Exceeds maximum order "+ maxOrder);
>>>>>>>>  }
>>>>>>>>
>>>>>>>>  while(order != currentAllowedOrder)
>>>>>>>>  {
>>>>>>>>  synchronized (this.map.get(order))
>>>>>>>>  {
>>>>>>>>  this.map.get(order).wait();
>>>>>>>>  }
>>>>>>>>  }
>>>>>>>>   try
>>>>>>>>  {
>>>>>>>>  return callable.call();
>>>>>>>>  }
>>>>>>>>        finally
>>>>>>>>  {
>>>>>>>>             currentAllowedOrder = currentAllowedOrder+1;
>>>>>>>>  synchronized (this.map.get(order+1))
>>>>>>>>             {
>>>>>>>>                 this.map.get(order+1).notify();
>>>>>>>>             }
>>>>>>>>         }
>>>>>>>>     }
>>>>>>>> }
>>>>>>>>
>>>>>>>>
>>>>>>>>   ------------------------------
>>>>>>>>  *From:* Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>>>> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
>>>>>>>> *Sent:* Friday, December 19, 2014 3:33 AM
>>>>>>>>
>>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>> of critical sections?
>>>>>>>>
>>>>>>>>  That would be "Tom" Cargill; link to paper:
>>>>>>>>
>>>>>>>>
>>>>>>>> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>  On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <
>>>>>>>> joe.bowbeer at gmail.com> wrote:
>>>>>>>>
>>>>>>>> I frown on use of notify[All]/wait because they make the code hard
>>>>>>>> to maintain.
>>>>>>>>
>>>>>>>>  In this case, with potentially lots of waiting threads, I would
>>>>>>>> check out the "Specific Notification" pattern if I were determined to go
>>>>>>>> the wait/notify route:
>>>>>>>>
>>>>>>>>  Tim Cargill's paper is dated but still worth reading.
>>>>>>>>
>>>>>>>>  Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ
>>>>>>>> and Peter Haggar's article:
>>>>>>>>
>>>>>>>>  http://www.ibm.com/developerworks/java/library/j-spnotif.html
>>>>>>>>
>>>>>>>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
>>>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>>>
>>>>>>>>  Yes, no one said it is a good idea to always do that. When it is
>>>>>>>> contended, most of the threads will wake up to only go back to sleep.
>>>>>>>>
>>>>>>>> The pattern you are after is usually called sequencer. You can see
>>>>>>>> it used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe
>>>>>>>> not that popular.
>>>>>>>>
>>>>>>>> The best solution will be lock-like, but the waiter nodes will
>>>>>>>> contain the value they are waiting for - so only the specific threads get
>>>>>>>> woken up. The solution with concurrent map is very similar, only with
>>>>>>>> larger overhead from storing the index the thread is waiting for.
>>>>>>>>
>>>>>>>>
>>>>>>>> Alex
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>>>>>>
>>>>>>>> Less overhead and simpler are a nice properties, even though at the
>>>>>>>> expense of having to wake up all waiting threads just to find out the one
>>>>>>>> with the right order to execute.  Still, this seems like a good tradeoff.
>>>>>>>>
>>>>>>>>  Thanks,
>>>>>>>> Hanson
>>>>>>>>
>>>>>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <
>>>>>>>> peter.levart at gmail.com> wrote:
>>>>>>>>
>>>>>>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>>>>>>
>>>>>>>> No, there is no difference. Peter didn't spot your entire method is
>>>>>>>> synchronized, so spurious wakeup won't make progress until the owner of the
>>>>>>>> lock exits the method.
>>>>>>>>
>>>>>>>> You could split the synchronization into two blocks - one
>>>>>>>> encompassing the wait loop, the other in the finally block; but it may make
>>>>>>>> no difference.
>>>>>>>>
>>>>>>>> Alex
>>>>>>>>
>>>>>>>>
>>>>>>>> You're right, Alex. I'm so infected with park/unpark virus that I
>>>>>>>> missed that ;-)
>>>>>>>>
>>>>>>>> Peter
>>>>>>>>
>>>>>>>>
>>>>>>>> On 17/12/2014 18:36, suman shil wrote:
>>>>>>>>
>>>>>>>> Thanks peter for your reply. You are right. I should have
>>>>>>>> incremented currentAllowedOrder in finally block.
>>>>>>>>
>>>>>>>> Suman
>>>>>>>>
>>>>>>>> ------------------------------------------------------------------------
>>>>>>>> *From:* Peter Levart <peter.levart at gmail.com>
>>>>>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>>>>>>>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>>>>>>>> concurrency-interest at cs.oswego.edu>
>>>>>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>> of critical sections?
>>>>>>>>
>>>>>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>>>>>>
>>>>>>>> Thanks for your response. Will notifyAll() instead of notify()
>>>>>>>> solve the problem?
>>>>>>>>
>>>>>>>>
>>>>>>>> It will, but you should also account for "spurious" wake-ups. You
>>>>>>>> should increment currentAllowedOrder only after return from callable.call
>>>>>>>> (in finally block just before notifyAll()).
>>>>>>>>
>>>>>>>> Otherwise a nice solution - with minimal state, providing that not
>>>>>>>> many threads meet at the same time...
>>>>>>>>
>>>>>>>> Regards, Peter
>>>>>>>>
>>>>>>>>  RegardsSuman
>>>>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:
>>>>>>>> oleksandr.otenko at oracle.com>
>>>>>>>>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>;
>>>>>>>> Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:
>>>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December
>>>>>>>> 17, 2014 9:55 PM
>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>> of critical sections?
>>>>>>>>       There is no guarantee you'll ever hand over the control to
>>>>>>>> the right thread upon notify()
>>>>>>>>     Alex
>>>>>>>>
>>>>>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>>>>>       Hi, Following is my solution to solve this problem. Please
>>>>>>>> let me know if I am missing something.
>>>>>>>>    public class OrderedExecutor {  private int currentAllowedOrder
>>>>>>>> = 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {
>>>>>>>> this.maxLength = n;  } public synchronized Object
>>>>>>>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>>>>>>>                        throws Exception  { if (order >= maxLength)  {
>>>>>>>> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
>>>>>>>> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
>>>>>>>> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
>>>>>>>> }  } }
>>>>>>>>    Regards Suman
>>>>>>>>         From: Peter Levart<peter.levart at gmail.com> <mailto:
>>>>>>>> peter.levart at gmail.com>
>>>>>>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:
>>>>>>>> hanson.char at gmail.com>    Cc: concurrency-interest<
>>>>>>>> concurrency-interest at cs.oswego.edu> <mailto:
>>>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14,
>>>>>>>> 2014 11:01 PM
>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>> of critical sections?
>>>>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>>>>      Hi Peter,
>>>>>>>>    Thanks for this proposed idea of using LockSupport. This begs
>>>>>>>> the question: which one would you choose if you had all three (correct)
>>>>>>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>>>>>    Regards, Hanson
>>>>>>>>     The Semaphore/CountDownLatch variants are equivalent if you
>>>>>>>> don't need re-use. So any would do. They lack invalid-use detection. What
>>>>>>>> happens if they are not used as intended? Semaphore variant acts
>>>>>>>> differently than CountDownLatch variant. The low-level variant I  proposed
>>>>>>>> detects invalid usage. So I would probably use this one. But the low level
>>>>>>>> variant is harder to reason about it's correctness. I think it is correct,
>>>>>>>> but you should show it to somebody else to confirm this.
>>>>>>>>     Another question is whether you actually need this kind of
>>>>>>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>>>>>>> somebody could have an idea how to do that even more elegantly...
>>>>>>>>     Regards, Peter
>>>>>>>>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>>>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>>    Hi Hanson,
>>>>>>>>     This one is more low-level, but catches some invalid usages and
>>>>>>>> is more resource-friendly:
>>>>>>>>       public class OrderedExecutor {
>>>>>>>>         public <T> T execCriticalSectionInOrder(
>>>>>>>>           final int order,
>>>>>>>>           final Supplier<T> criticalSection
>>>>>>>>       ) throws InterruptedException {
>>>>>>>>           if (order < 0) {
>>>>>>>>                throw new IllegalArgumentException("'order' should
>>>>>>>> be >= 0");
>>>>>>>>           }
>>>>>>>>           if (order > 0) {
>>>>>>>>               waitForDone(order - 1);
>>>>>>>>           }
>>>>>>>>           try {
>>>>>>>>               return criticalSection.get();
>>>>>>>>           } finally {
>>>>>>>>               notifyDone(order);
>>>>>>>>           }
>>>>>>>>       }
>>>>>>>>         private static final Object DONE = new Object();
>>>>>>>>       private final ConcurrentMap<Integer, Object> signals = new
>>>>>>>> ConcurrentHashMap<>();
>>>>>>>>         private void waitForDone(int order) throws
>>>>>>>> InterruptedException {
>>>>>>>>           Object sig = signals.putIfAbsent(order,
>>>>>>>> Thread.currentThread());
>>>>>>>>           if (sig != null && sig != DONE) {
>>>>>>>>               throw new IllegalStateException();
>>>>>>>>           }
>>>>>>>>           while (sig != DONE) {
>>>>>>>>               LockSupport.park();
>>>>>>>>               if (Thread.interrupted()) {
>>>>>>>>                   throw new InterruptedException();
>>>>>>>>               }
>>>>>>>>               sig = signals.get(order);
>>>>>>>>           }
>>>>>>>>       }
>>>>>>>>         private void notifyDone(int order) {
>>>>>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>>>>>           if (sig instanceof Thread) {
>>>>>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>>>>>                   throw new IllegalStateException();
>>>>>>>>               }
>>>>>>>>               LockSupport.unpark((Thread) sig);
>>>>>>>>           } else if (sig != null) {
>>>>>>>>               throw new IllegalStateException();
>>>>>>>>           }
>>>>>>>>       }
>>>>>>>>   }
>>>>>>>>       Regards, Peter
>>>>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>>>>      Hi Peter,
>>>>>>>>    Thanks for the suggestion, and sorry about not being clear about
>>>>>>>> one important  detail: "n" is not known a priori when constructing an
>>>>>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>>>>>     If you know at least the upper bound of 'n', it can be used
>>>>>>>> with such 'n'. Otherwise something that dynamically re-sizes the array
>>>>>>>> could be devised. Or you could simply use a ConcurrentHashMap instead of
>>>>>>>> array where keys are 'order' values:
>>>>>>>>       public class OrderedExecutor<T> {
>>>>>>>>         private final ConcurrentMap<Integer, CountDownLatch>
>>>>>>>> latches = new ConcurrentHashMap<>();
>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>>                                           final Supplier<T>
>>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>>           if (order > 0) {
>>>>>>>>               latches.computeIfAbsent(order - 1, o -> new
>>>>>>>> CountDownLatch(1)).await();
>>>>>>>>           }
>>>>>>>>           try {
>>>>>>>>               return criticalSection.get();
>>>>>>>>           } finally {
>>>>>>>>               latches.computeIfAbsent(order, o -> new
>>>>>>>> CountDownLatch(1)).countDown();
>>>>>>>>           }
>>>>>>>>       }
>>>>>>>>   }
>>>>>>>>       Regards, Peter
>>>>>>>>           You guessed right: it's a one-shot object for a
>>>>>>>> particular OrderedExecutor  instance, and "order" must be called indeed at
>>>>>>>> most once.
>>>>>>>>    Regards, Hanson
>>>>>>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<
>>>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>>    Hi Hanson,
>>>>>>>>     I don't think anything like that readily exists  in
>>>>>>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>>>>>>> with composition of existing primitives.  You haven't given any additional
>>>>>>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>>>>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>>>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>>>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>>>>>> (and I think that only a one-shot object  makes sense here), an array of
>>>>>>>> CountDownLatch(es) could be used:
>>>>>>>>     public class OrderedExecutor<T> {
>>>>>>>>       private final CountDownLatch[] latches;
>>>>>>>>         public OrderedExecutor(int n) {
>>>>>>>>           if (n < 1) throw new IllegalArgumentException("'n'
>>>>>>>> should be >= 1");
>>>>>>>>           latches = new CountDownLatch[n - 1];
>>>>>>>>           for (int i = 0; i < latches.length; i++) {
>>>>>>>>               latches[i] = new CountDownLatch(1);
>>>>>>>>           }
>>>>>>>>       }
>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>>                                            final Supplier<T>
>>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>>           if (order < 0 || order > latches.length)
>>>>>>>>               throw new IllegalArgumentException("'order' should be
>>>>>>>> [0..." +  latches.length + "]");
>>>>>>>>           if (order > 0) {
>>>>>>>>               latches[order - 1].await();
>>>>>>>>           }
>>>>>>>>           try {
>>>>>>>>               return criticalSection.get();
>>>>>>>>           } finally {
>>>>>>>>               if (order < latches.length) {
>>>>>>>>                   latches[order].countDown();
>>>>>>>>               }
>>>>>>>>           }
>>>>>>>>       }
>>>>>>>>   }
>>>>>>>>       Regards, Peter
>>>>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>>>>          Hi, I am looking for a construct that can  be used to
>>>>>>>> efficiently enforce  ordered execution of multiple critical sections, each
>>>>>>>> calling from a  different thread. The calling threads may run in  parallel
>>>>>>>> and may call the execution method out of order. The  perceived construct
>>>>>>>> would therefore be responsible for re-ordering the execution of those
>>>>>>>> threads, so that their critical  sections (and only the critical section)
>>>>>>>> will be executed in order. Would something  like the following API already
>>>>>>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>>>>>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>>>>>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>>>>>>> most once with the given order, parking * and  unparking the current thread
>>>>>>>> as  necessary so that all critical * sections executed  by different
>>>>>>>> threads using this  executor take place in * the order from 1 to n
>>>>>>>> consecutively. */ public T execCriticalSectionInOrder
>>>>>>>> (  final int order, final Callable<T> criticalSection) throws
>>>>>>>> InterruptedException; } Regards, Hanson
>>>>>>>> _______________________________________________Concurrency-interest mailing
>>>>>>>> listConcurrency-interest at cs.oswego.edu <mailto:
>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>> _______________________________________________
>>>>>>>>   Concurrency-interest mailing list
>>>>>>>>   Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/001ce273/attachment-0001.html>

From viktor.klang at gmail.com  Sun Dec 21 13:07:55 2014
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 21 Dec 2014 19:07:55 +0100
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <5496EB91.9090504@cs.oswego.edu>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>
	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
	<5495949E.1030500@cs.oswego.edu>
	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
	<5496D732.3010308@cs.oswego.edu>
	<CANPzfU9qoJyUgPwK3RtPeca6n9Vif6kq1yg98QbVByw58sn3aA@mail.gmail.com>
	<5496EB91.9090504@cs.oswego.edu>
Message-ID: <CANPzfU-w0piYhehLAG9bJh5W4K+U2rA2EkymvwHWsksLO5t4sQ@mail.gmail.com>

On Sun, Dec 21, 2014 at 4:47 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/21/2014 09:59 AM, Viktor Klang wrote:
>
>> For "computeIfAbsent"-style maps which are intended for multithreaded
>> use, I
>> tend to prefer to use ConcurrentMap[Key, CompletionStage[Value]],
>>
>
> Right. Prior to jdk8, we suggested variations of this and other
> techniques that remain preferable in many cases. However, the
> most common CHM user error was trying (but failing) to emulate
> what is now supported as computeIfAbsent. This is an improvement,
> but leads to new (much less common) kinds of potential errors.
> We cannot automatically eliminate or trap all of them. It
> might be possible to do more than we do now, either internally
> (inside CHM) or, more likely, with improved developer tools.


Absolutely, and the fact that there was only blocking Futures in the JDK at
that time.
Having "monadic" Futures like CompletableFuture means you can avoid
blocking completely (assuming that the CHM impl is non-blocking). This is
highly useful when using it as a cache, and especially if there's a very
skewed read distribution, paired with a multitude of readers.


>
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/4822eadc/attachment.html>

From peter.levart at gmail.com  Sun Dec 21 14:33:58 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 21 Dec 2014 20:33:58 +0100
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <5496D732.3010308@cs.oswego.edu>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>	<5495949E.1030500@cs.oswego.edu>	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
	<5496D732.3010308@cs.oswego.edu>
Message-ID: <549720A6.70208@gmail.com>

Hi,

If we wanted to detect any re-entrance to the whole CHM by the same 
thread, then the state would have to be placed outside the lock object 
(outside the head of the bucket). Conceptually it would have to be a 
ThreadLocal<Boolean> instance (per CHM instance).

As I understand, the problematic re-entrances are those that re-enter 
synchronized blocks. All other code (out of synchronized blocks) is safe 
to reenter, since it is safe to execute by multiple threads. Re-entering 
synchronized block causes most hard to diagnose effects (what's left are 
only deadlocks). If locks were non-reentrant and threw 
IllegalStateException on trying to re-enter, it would be fine.

One way to get such locks is to check holdsLock() before locking:

if (Thread.holdsLock(lock)) throw new IllegalStateException();
synchronized (lock) {
...
}

... but measuring on my i7 Linux PC shows that holdsLock() amounts to an 
addition of 2/3 the overhead of non-contended locking.

If we added an int field to Node class, we would extend it for 4 bytes 
on 32 bit JVM, but on 64 bit JVM it would be packed together with hash 
field and Node footprint would not increase. Current sizes of Nodes on 
32 bit JVM are: (Node/ReservationNode: 28 bytes, TreeNode: 48 bytes, 
TreeBin: 44 bytes, ForwardingNode: 32 bytes). Given that plain Nodes are 
in majority, this would amount in approx. 14% increase of heap usage for 
nodes on 32 bit JVMs, but no increase on 64 bit JVMs.

If this is a no-go, then I'm out of ideas short of checking whether 
holdsLock() could be made any faster. Perhaps it just needs to be 
intrinsified (if it's not already)? If Node increase is acceptable then 
read further...

Since thread id is a long value, packing it into an int would give us 
just a kind of hash. But good-enough hash so that additional checking 
for holdsLock() would be needed from purely theoretical perspective.

Here's to illustrate what I mean:

http://cr.openjdk.java.net/~plevart/misc/CHM.nonreentrantLocks/webrev.01/

Changes I did are purely mechanical. I inserted node.checkNotEntered() 
at start of each synchronized block that doesn't contain a call-back to 
user code:

synchronized (node) {
     node.checkNotEntered();
     // body that doesn't call-back user code
}

... and wrapped the body of synchronized blocks that call-back user code 
into:

synchronized (node) {
     node.enter();
     try {
         // body that calls-back user code
     } finally {
         node.exit();
     }
}


What do you think? Is space/diagnosability trade-of worth it?

Regards, Peter


On 12/21/2014 03:20 PM, Doug Lea wrote:
> On 12/20/2014 04:11 PM, Benjamin Manes wrote:
>> If an efficient internal tryLock intrinsic became available, we might do
>>     more, because throwing here helps diagnose unrelated-looking 
>> problems in
>>     other threads, and performing checks only when lock is 
>> unavailable costs
>>     almost nothing.
>>
>>
>> Is the tryLock intrinsic necessary to be performant and safe? I tried 
>> using
>> Thread.holdsLock() and it halved the performance. I then tried 
>> stashing the
>> thread's id into an extra field only stored on ReservationNode and 
>> saw equal
>> performance on my 4-core/8HT macbook.
>
> This is worth considering, but it only detects one form of
> misbehavior when the bin doesn't hold any other nodes.
> Otherwise, the first node need not be a ReservationNode. Also,
> it doesn't alone deal with interactions with other
> lambda-accepting methods.
>
> Here is some background context in case others have any
> thoughts about improving support:
>
> By popular demand, CHM strengthens ConcurrentMap.computeIfAbsent
> specs (similarly for computeIfPresent etc) to guarantee atomicity
> while the function is executed. This requires that we hold a lock
> (for the bin in which the entry will be placed) during the call
> to the supplied function. We tell users not to use functions
> that have any other potential side effects on the map,
> but we cannot enforce it. Holding a lock adds to the possible
> bad outcomes of violating the side-effect-freedom requirement.
> We'd like to be helpful in diagnosing violations, because
> these outcomes can be very mysterious-looking.
>
> CHM itself cannot directly help with some potential
> lock-based errors. For example, if the function for
> computeIfAbsent(key1) can invoke computeIfAbsent(key2)
> and vice versa, and the keys hash to different bins, then they
> can encounter a classic deadlock. But this and related cases
> can be diagnosed using existing JVM cycle-detection tools.
>
> The nature of other potential lock-based errors depends
> on the kind of lock used. Some preliminary versions of
> jdk8 CHM used non-reentrant locks. (It now uses reentrant
> locks.)
>
> With non-reentrant locks, cases of infinite recursion
> (i.e., m.computeIfAbsent(k,f) somehow calling itself) result in
> lockups rather than infinite loops. Similarly for mutually
> recursive computeIfAbsent calls for key1 and key2 that happen
> to hash into the same bin. However these cases can be detected
> with very low performance impact (by doing so only if a tryLock
> fails). Which led us to add CHM method throws specs to allow
> this. Even though jdk8 CHM now uses reentrant locks, we want to
> keep open the ability to use potentially faster non-reentrant
> approaches in the future.
>
> With reentrant locks, infinite computeIfAbsent recursions
> on the same key are not all that different than any other
> case of infinite recursion. This is "detectable" only by
> throwing after the number of calls exceeds a threshold.
> But the threshold can arguably be set to 1 because of the
> no-side-effects rule, and one special case of this check
> (as above, via thread IDs in ReservationNodes) can be done
> with only small impact. But it's not clear to me whether
> this is worthwhile by itself.
>
> The remaining mysterious cases are nested computeIfAbsent
> calls for different keys that happen to hash to the same bin.
> These conceptually ought to deadlock. Instead they will
> sometimes happen to "work" but other times loop.
> (This appears to be the underlying undetected user bug in
> JDK-8062841.) It seems that this is beyond our current ability
> to diagnose, but suggestions would be welcome.
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/552d5d79/attachment.html>

From ben.manes at gmail.com  Sun Dec 21 14:36:11 2014
From: ben.manes at gmail.com (Benjamin Manes)
Date: Sun, 21 Dec 2014 11:36:11 -0800
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <CANPzfU-w0piYhehLAG9bJh5W4K+U2rA2EkymvwHWsksLO5t4sQ@mail.gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>
	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
	<5495949E.1030500@cs.oswego.edu>
	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
	<5496D732.3010308@cs.oswego.edu>
	<CANPzfU9qoJyUgPwK3RtPeca6n9Vif6kq1yg98QbVByw58sn3aA@mail.gmail.com>
	<5496EB91.9090504@cs.oswego.edu>
	<CANPzfU-w0piYhehLAG9bJh5W4K+U2rA2EkymvwHWsksLO5t4sQ@mail.gmail.com>
Message-ID: <CAGu0=MNxmOhgtC_mX2MmMZsc6C64fC3eiiY7AaX4v-amk3P6Hw@mail.gmail.com>

>
> This is worth considering, but it only detects one form of
> misbehavior when the bin doesn't hold any other nodes.
> Otherwise, the first node need not be a ReservationNode.


I'm not comfortable enough with the code to experiment, but reading it I
keep wondering if inserting a node prior to computation would provide
detection. The value would be null, to be filled in if the computation
succeeds. If a reentrant computation occurs then when the existing node is
found and we observe that the value is unset, then we have detected a cycle
since we already hold the necessary lock. This would require skipping those
nodes for methods that traverse the entire map, like iterators and size().

Internally, this would make computations feel more like the JCiP-style
future based approach Viktor mentioned and allow for supporting batch
computations. It has the nice property of not wasting more memory with
values wrapped in futures, unless an asynchronous cache was explicitly
requested. That can be built on-top of CHM, as was always possible, and an
API option that I plan on providing.

The remaining mysterious cases are nested computeIfAbsent
> calls for different keys that happen to hash to the same bin.
> These conceptually ought to deadlock. Instead they will
> sometimes happen to "work" but other times loop.


If changes like the above removed the live lock issue, then I'd be much
happier with a resulting deadlock remain unsolved. That's much easier for
developers to notice with jdk tooling when debugging server issues and
recognize the problem as their fault.

On Sun, Dec 21, 2014 at 10:07 AM, Viktor Klang <viktor.klang at gmail.com>
wrote:

>
>
> On Sun, Dec 21, 2014 at 4:47 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 12/21/2014 09:59 AM, Viktor Klang wrote:
>>
>>> For "computeIfAbsent"-style maps which are intended for multithreaded
>>> use, I
>>> tend to prefer to use ConcurrentMap[Key, CompletionStage[Value]],
>>>
>>
>> Right. Prior to jdk8, we suggested variations of this and other
>> techniques that remain preferable in many cases. However, the
>> most common CHM user error was trying (but failing) to emulate
>> what is now supported as computeIfAbsent. This is an improvement,
>> but leads to new (much less common) kinds of potential errors.
>> We cannot automatically eliminate or trap all of them. It
>> might be possible to do more than we do now, either internally
>> (inside CHM) or, more likely, with improved developer tools.
>
>
> Absolutely, and the fact that there was only blocking Futures in the JDK
> at that time.
> Having "monadic" Futures like CompletableFuture means you can avoid
> blocking completely (assuming that the CHM impl is non-blocking). This is
> highly useful when using it as a cache, and especially if there's a very
> skewed read distribution, paired with a multitude of readers.
>
>
>>
>>
>> -Doug
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Cheers,
> ?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/0b8f4f5c/attachment-0001.html>

From peter.levart at gmail.com  Sun Dec 21 14:55:58 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 21 Dec 2014 20:55:58 +0100
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <549720A6.70208@gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>	<5495949E.1030500@cs.oswego.edu>	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
	<5496D732.3010308@cs.oswego.edu> <549720A6.70208@gmail.com>
Message-ID: <549725CE.7050101@gmail.com>


On 12/21/2014 08:33 PM, Peter Levart wrote:
> Here's to illustrate what I mean:
>
> http://cr.openjdk.java.net/~plevart/misc/CHM.nonreentrantLocks/webrev.01/
>
> Changes I did are purely mechanical.


Sorry, changes I did to code at last moment were not correct. This is 
what I had before and was correct:

http://cr.openjdk.java.net/~plevart/misc/CHM.nonreentrantLocks/webrev.02/

The node.checkNotEntered() must be called before synchronized block, 
since it contains a check for Thread.holdsLock(). node.enter() does not 
contain a check, so checkNotEntered() must be called before synchronized 
blocks that call-back user code too (where necessary).


Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/61e2e14a/attachment.html>

From peter.levart at gmail.com  Sun Dec 21 15:53:11 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 21 Dec 2014 21:53:11 +0100
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <CAGu0=MNxmOhgtC_mX2MmMZsc6C64fC3eiiY7AaX4v-amk3P6Hw@mail.gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>	<5495949E.1030500@cs.oswego.edu>	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>	<5496D732.3010308@cs.oswego.edu>	<CANPzfU9qoJyUgPwK3RtPeca6n9Vif6kq1yg98QbVByw58sn3aA@mail.gmail.com>	<5496EB91.9090504@cs.oswego.edu>	<CANPzfU-w0piYhehLAG9bJh5W4K+U2rA2EkymvwHWsksLO5t4sQ@mail.gmail.com>
	<CAGu0=MNxmOhgtC_mX2MmMZsc6C64fC3eiiY7AaX4v-amk3P6Hw@mail.gmail.com>
Message-ID: <54973337.30701@gmail.com>

Hi Benjamin,

Doug is certainly the one to give you the best explanation, but let me 
try...

On 12/21/2014 08:36 PM, Benjamin Manes wrote:
>
>     This is worth considering, but it only detects one form of
>     misbehavior when the bin doesn't hold any other nodes.
>     Otherwise, the first node need not be a ReservationNode.
>
> I'm not comfortable enough with the code to experiment, but reading it 
> I keep wondering if inserting a node prior to computation would 
> provide detection. The value would be null, to be filled in if the 
> computation succeeds. If a reentrant computation occurs then when the 
> existing node is found and we observe that the value is unset, then we 
> have detected a cycle since we already hold the necessary lock. This 
> would require skipping those nodes for methods that traverse the 
> entire map, like iterators and size().

...ReservationNode is such node. But is only inserted (with CAS) as 1st 
node of the bucket when the bucket is still empty. This is to provide 
the object to use as the lock when changing anything in the bucket. It 
is replaced with normal Node when the function's result is obtained. 
Logically it is skipped on look-ups and traversals. If a node is 
inserted into the bucket that already contains nodes, no ReservationNode 
is used.

And it's not only inserting (computeIfAbsent) that calls-back user code. 
See also compute() method. This method overwrites the old value with new 
value returned from function in existing node or even removes a Node 
from the bucket (if function returns null). How would you detect that 
such process is taking place when you re-enter?

I thought about it for some time, but didn't find any existing state 
combination that could be used for such checks. Perhaps a 
ReservationNode could be inserted just after 1st node in bucket 
temporarily just to mark the fact that we entered synchronization block 
and later remove it before exiting the block. This would most probably 
work, but would also mean that we produce one object of garbage for each 
such call. Maybe this is better than 4 bytes of state overhead for each 
node (see my experiment).

>
> Internally, this would make computations feel more like the JCiP-style 
> future based approach Viktor mentioned and allow for supporting batch 
> computations. It has the nice property of not wasting more memory with 
> values wrapped in futures, unless an asynchronous cache was explicitly 
> requested. That can be built on-top of CHM, as was always possible, 
> and an API option that I plan on providing.

Don't forget that CHM provides a lock-free lookup. We would not like to 
loose this feature. Lazy evaluation (or waiting on computation) can be 
added on top of CHM if one needs it.

Regards, Peter

>
>     The remaining mysterious cases are nested computeIfAbsent
>     calls for different keys that happen to hash to the same bin.
>     These conceptually ought to deadlock. Instead they will
>     sometimes happen to "work" but other times loop.
>
>
> If changes like the above removed the live lock issue, then I'd be 
> much happier with a resulting deadlock remain unsolved. That's much 
> easier for developers to notice with jdk tooling when debugging server 
> issues and recognize the problem as their fault.
>
> On Sun, Dec 21, 2014 at 10:07 AM, Viktor Klang <viktor.klang at gmail.com 
> <mailto:viktor.klang at gmail.com>> wrote:
>
>
>
>     On Sun, Dec 21, 2014 at 4:47 PM, Doug Lea <dl at cs.oswego.edu
>     <mailto:dl at cs.oswego.edu>> wrote:
>
>         On 12/21/2014 09:59 AM, Viktor Klang wrote:
>
>             For "computeIfAbsent"-style maps which are intended for
>             multithreaded use, I
>             tend to prefer to use ConcurrentMap[Key,
>             CompletionStage[Value]],
>
>
>         Right. Prior to jdk8, we suggested variations of this and other
>         techniques that remain preferable in many cases. However, the
>         most common CHM user error was trying (but failing) to emulate
>         what is now supported as computeIfAbsent. This is an improvement,
>         but leads to new (much less common) kinds of potential errors.
>         We cannot automatically eliminate or trap all of them. It
>         might be possible to do more than we do now, either internally
>         (inside CHM) or, more likely, with improved developer tools.
>
>
>     Absolutely, and the fact that there was only blocking Futures in
>     the JDK at that time.
>     Having "monadic" Futures like CompletableFuture means you can
>     avoid blocking completely (assuming that the CHM impl is
>     non-blocking). This is highly useful when using it as a cache, and
>     especially if there's a very skewed read distribution, paired with
>     a multitude of readers.
>
>
>
>         -Doug
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>     -- 
>     Cheers,
>     ?
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/0f32bb32/attachment.html>

From peter.levart at gmail.com  Sun Dec 21 16:12:11 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 21 Dec 2014 22:12:11 +0100
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>	<5496B0FE.7070502@gmail.com>
	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
Message-ID: <549737AB.8060508@gmail.com>


On 12/21/2014 07:03 PM, Hanson Char wrote:
> Interesting - now that each order is associated with a different 
> object monitor, the wait/notifyAll would at most wake up one thread 
> instead of all.  In fact, it seems the use of notify (instead of 
> notifyAll) would suffice.  Simple and clever, but it does require the 
> "stripeSize" to be known a priori.

Not exactly. Each object monitor is associated with multiple orders. For 
example, if stripeSize is 4, the 1st object monitor is associated with 
orders: 0, 4, 8, 12, 16, ... the 2nd with orders 1, 5, 9, 13, 17, ... etc.

But it also means that each object monitor is associated statistically 
with just N/stripeSize threads that wait instead of N. So notifyAll is 
still needed, but it wakes up less threads - how much less depends on 
chosen stripeSize.

Regards, Peter

>
> Regards,
> Hanson
>
> On Sun, Dec 21, 2014 at 3:37 AM, Peter Levart <peter.levart at gmail.com 
> <mailto:peter.levart at gmail.com>> wrote:
>
>     Hi,
>
>     Here's a simple implementation (based on Suman Shil's idea) that
>     stripes the waiting threads into multiple buckets:
>
>     public class StripedOrderedExecutor {
>
>         private final MeetingPoint[] meetingPoints;
>
>         public StripedOrderedExecutor(int stripeSize) {
>             assert stripeSize > 0;
>             meetingPoints = new MeetingPoint[stripeSize];
>             for (int i = 0; i < stripeSize; i++) {
>                 meetingPoints[i] = new MeetingPoint();
>             }
>         }
>
>         public <T> T execCriticalSectionInOrder(
>             final int order,
>             final Supplier<T> criticalSection
>         ) throws InterruptedException {
>             assert order >= 0;
>
>             meetingPoints[order %
>     meetingPoints.length].waitForGreen(order);
>             try {
>                 return criticalSection.get();
>             } finally {
>                 meetingPoints[(order + 1) %
>     meetingPoints.length].notifyGreen(order + 1);
>             }
>         }
>
>         private static class MeetingPoint {
>             private int lastGreen;
>
>             synchronized void waitForGreen(int order) throws
>     InterruptedException {
>                 while (lastGreen != order) {
>                     wait();
>                 }
>             }
>
>             synchronized void notifyGreen(int order) {
>                 lastGreen = order;
>                 notifyAll();
>             }
>         }
>     }
>
>
>     Regards, Peter
>
>
>
>
>
>     On 12/21/2014 02:35 AM, Hanson Char wrote:
>>     Thanks, Joe. That's an interesting point about the long term
>>     memory impact of TreeMap vs HashMap - similar to LinkedList vs
>>     ArrayList.
>>
>>     Regards,
>>     Hanson
>>
>>     On Sat, Dec 20, 2014 at 5:17 PM, Joe Bowbeer
>>     <joe.bowbeer at gmail.com <mailto:joe.bowbeer at gmail.com>> wrote:
>>
>>         I see.  Yes, that addresses the removal problem, though note
>>         that HashMap does not shrink when data is removed. Even if
>>         all keys are removed from HashMap, the inner size of its
>>         table does not change.
>>
>>         So in this case maybe I would choose TreeMap as the
>>         implementation. Its remove() method *does* remove all the
>>         memory associated with an entry.
>>
>>         In general, I don't like Map<Integer, *> because it seems
>>         like a hack, but really in this case it's a small trade-off. 
>>         Map is more concise. PriorityQueue is a little clearer.
>>
>>         On Sat, Dec 20, 2014 at 4:50 PM, Hanson Char
>>         <hanson.char at gmail.com <mailto:hanson.char at gmail.com>> wrote:
>>
>>             Hi Joe,
>>
>>             >A Queue should also retain less memory. A Map will tend to accumulate both
>>             Entry and Integer instances over time.
>>
>>             Using a Map, the signalNext would look something like
>>             below. Doesn't each entry get eventually removed from the
>>             map?  Why would they accumulate over time?
>>
>>             Regards,
>>             Hanson
>>
>>                 void signalNext(final int nextOrder) {
>>                   lock.lock();
>>                   try {
>>                     this.nextOrder = nextOrder;
>>                     Condition cond = map.remove(nextOrder);
>>                     if (cond != null) {
>>                         cond.signal();
>>                     }
>>                   } finally {
>>                     lock.unlock();
>>                   }
>>                 }
>>               }
>>
>>
>>             On Sat, Dec 20, 2014 at 4:39 PM, Joe Bowbeer
>>             <joe.bowbeer at gmail.com <mailto:joe.bowbeer at gmail.com>> wrote:
>>
>>                 I think PriorityQueue more clearly represents the
>>                 function than Map does in this case.
>>
>>                 A Queue should also retain less memory. A Map will
>>                 tend to accumulate both Entry and Integer instances
>>                 over time.
>>
>>                 The ConcurrentMap does have the advantage of lock
>>                 striping, which might be more performant if there is
>>                 high contention.
>>
>>                 I think this is an interesting problem because there
>>                 are quite a few different solutions, each with
>>                 advantages and disadvantages in terms of performance
>>                 and maintainability.
>>
>>                 If there were a PriorityQueue version of
>>                 AbstractQueuedSynchronizer, then we could have yet
>>                 another solution using the AQS's state to represent
>>                 the next available order.
>>
>>                 On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char
>>                 <hanson.char at gmail.com
>>                 <mailto:hanson.char at gmail.com>> wrote:
>>
>>                     This one looks interesting and educational. The
>>                     use of condition enables us to suspend and resume
>>                     specifically targeted threads without resorting
>>                     to the use of LockSupport.{part, unpark}. Instead
>>                     of using a PriorityQueue, perhaps we can use a
>>                     simple Map<Integer, Condition> instead so we can
>>                     get away without the extra Waiter inner class?
>>
>>                     Regards,
>>                     Hanson
>>
>>                     On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer
>>                     <joe.bowbeer at gmail.com
>>                     <mailto:joe.bowbeer at gmail.com>> wrote:
>>
>>                         To round out the selection, here's an
>>                         implementation using a PriorityQueue of
>>                         Conditions:
>>
>>                         class OrderedExecutor<T> {
>>
>>                           static class Waiter implements
>>                         Comparable<Waiter> {
>>
>>                             final int order;
>>                             final Condition condition;
>>
>>                         Waiter(int order, Condition condition) {
>>                         this.order = order;
>>                         this.condition = condition;
>>                             }
>>
>>                         @Override
>>                         public int compareTo(Waiter waiter) {
>>                         return order - waiter.order;
>>                             }
>>                           }
>>
>>                           final Lock lock = new ReentrantLock();
>>                           final Queue<Waiter> queue = new
>>                         PriorityQueue<>();
>>                           int nextOrder; // 0 is next
>>
>>                           public T execCallableInOrder(int order,
>>                         Callable<T> callable) throws Exception {
>>                         assert order >= 0;
>>                         awaitTurn(order);
>>                             try {
>>                         return callable.call();
>>                             } finally {
>>                         signalNext(order + 1);
>>                             }
>>                           }
>>
>>                           void awaitTurn(int order) {
>>                         lock.lock();
>>                             try {
>>                         Condition condition = null;
>>                         while (nextOrder != order) {
>>                         if (condition == null) {
>>                         condition = lock.newCondition();
>>                         queue.add(new Waiter(order, condition));
>>                                 }
>>                         condition.awaitUninterruptibly();
>>                               }
>>                             } finally {
>>                         lock.unlock();
>>                             }
>>                           }
>>
>>                           void signalNext(int nextOrder) {
>>                         lock.lock();
>>                             try {
>>                         this.nextOrder = nextOrder;
>>                         Waiter waiter = queue.peek();
>>                               if (waiter != null && waiter.order ==
>>                         nextOrder) {
>>                         queue.remove();
>>                         waiter.condition.signal();
>>                               }
>>                             } finally {
>>                         lock.unlock();
>>                             }
>>                           }
>>                         }
>>
>>                         On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer
>>                         <joe.bowbeer at gmail.com
>>                         <mailto:joe.bowbeer at gmail.com>> wrote:
>>
>>                             Suman,
>>
>>                             I would advise against using notify/wait.
>>                             It raises a red flag for a lot of
>>                             reviewers, including me.
>>
>>                             The problems I see in this implementation
>>                             are:
>>
>>                             1. Pre-allocation of locks is prohibited
>>                             by (revised) problem statement.
>>
>>                             Note that if pre-allocation were allowed,
>>                             then an array would be more efficient
>>                             than a Map.
>>
>>                             2. Access to currentAllowedOrder is not
>>                             thread-safe but should be.
>>
>>
>>                             On Sat, Dec 20, 2014 at 6:04 AM, suman
>>                             shil <suman_krec at yahoo.com
>>                             <mailto:suman_krec at yahoo.com>> wrote:
>>
>>                                 I have modified my solution to avoid
>>                                 notifyAll(). Let me know your feedback.
>>
>>
>>                                 public class OrderedExecutor
>>                                 {
>>                                 private int maxOrder;
>>                                 private int currentAllowedOrder;
>>                                 private Map<Integer, Object> map =
>>                                 new HashMap<Integer, Object>();
>>                                 public OrderedExecutor(int n)
>>                                     {
>>                                 this.maxOrder = n;
>>                                 this.currentAllowedOrder = 0;
>>                                 for(int i = 0 ; i < this.maxOrder ; i++)
>>                                 {
>>                                 map.put(i, new Object());
>>                                 }
>>                                     }
>>                                 public  Object
>>                                 execCriticalSectionInOrder(int order,
>>                                 Callable<Object> callable)
>>                                         throws Exception
>>                                     {
>>                                 if (order >= this.maxOrder)
>>                                 {
>>                                 throw new Exception("Exceeds maximum
>>                                 order "+ maxOrder);
>>                                 }
>>                                 while(order != currentAllowedOrder)
>>                                 {
>>                                 synchronized (this.map.get(order))
>>                                 {
>>                                 this.map.get(order).wait();
>>                                 }
>>                                 }
>>                                 try
>>                                 {
>>                                 return callable.call();
>>                                 }
>>                                  finally
>>                                 {
>>                                 currentAllowedOrder =
>>                                 currentAllowedOrder+1;
>>                                 synchronized (this.map.get(order+1))
>>                                   {
>>                                 this.map.get(order+1).notify();
>>                                   }
>>                                  }
>>                                     }
>>                                 }
>>
>>
>>                                 ------------------------------------------------------------------------
>>                                 *From:* Joe Bowbeer
>>                                 <joe.bowbeer at gmail.com
>>                                 <mailto:joe.bowbeer at gmail.com>>
>>                                 *To:* concurrency-interest
>>                                 <concurrency-interest at cs.oswego.edu
>>                                 <mailto:concurrency-interest at cs.oswego.edu>>
>>
>>                                 *Sent:* Friday, December 19, 2014
>>                                 3:33 AM
>>
>>                                 *Subject:* Re: [concurrency-interest]
>>                                 Enforcing ordered execution of
>>                                 critical sections?
>>
>>                                 That would be "Tom" Cargill; link to
>>                                 paper:
>>
>>                                 http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>>
>>
>>
>>                                 On Thu, Dec 18, 2014 at 1:32 PM, Joe
>>                                 Bowbeer <joe.bowbeer at gmail.com
>>                                 <mailto:joe.bowbeer at gmail.com>> wrote:
>>
>>                                     I frown on use of
>>                                     notify[All]/wait because they
>>                                     make the code hard to maintain.
>>
>>                                     In this case, with potentially
>>                                     lots of waiting threads, I would
>>                                     check out the "Specific
>>                                     Notification" pattern if I were
>>                                     determined to go the wait/notify
>>                                     route:
>>
>>                                     Tim Cargill's paper is dated but
>>                                     still worth reading.
>>
>>                                     Also see chapter 3.7.3 Specific
>>                                     Notifications in Doug Lea's CPiJ
>>                                     and Peter Haggar's article:
>>
>>                                     http://www.ibm.com/developerworks/java/library/j-spnotif.html
>>
>>                                     On Thu, Dec 18, 2014 at 1:02 PM,
>>                                     Oleksandr Otenko
>>                                     <oleksandr.otenko at oracle.com
>>                                     <mailto:oleksandr.otenko at oracle.com>>
>>                                     wrote:
>>
>>                                         Yes, no one said it is a good
>>                                         idea to always do that. When
>>                                         it is contended, most of the
>>                                         threads will wake up to only
>>                                         go back to sleep.
>>
>>                                         The pattern you are after is
>>                                         usually called sequencer. You
>>                                         can see it used in TCP. I am
>>                                         not sure why it wasn't
>>                                         implemented in j.u.c. - maybe
>>                                         not that popular.
>>
>>                                         The best solution will be
>>                                         lock-like, but the waiter
>>                                         nodes will contain the value
>>                                         they are waiting for - so
>>                                         only the specific threads get
>>                                         woken up. The solution with
>>                                         concurrent map is very
>>                                         similar, only with larger
>>                                         overhead from storing the
>>                                         index the thread is waiting for.
>>
>>
>>                                         Alex
>>
>>
>>
>>                                         On 18/12/2014 20:21, Hanson
>>                                         Char wrote:
>>>                                         Less overhead and simpler
>>>                                         are a nice properties, even
>>>                                         though at the expense of
>>>                                         having to wake up all
>>>                                         waiting threads just to find
>>>                                         out the one with the right
>>>                                         order to execute. Still,
>>>                                         this seems like a good
>>>                                         tradeoff.
>>>
>>>                                         Thanks,
>>>                                         Hanson
>>>
>>>                                         On Wed, Dec 17, 2014 at
>>>                                         11:43 PM, Peter Levart
>>>                                         <peter.levart at gmail.com
>>>                                         <mailto:peter.levart at gmail.com>>
>>>                                         wrote:
>>>
>>>                                             On 12/17/2014 08:15 PM,
>>>                                             Oleksandr Otenko wrote:
>>>
>>>                                                 No, there is no
>>>                                                 difference. Peter
>>>                                                 didn't spot your
>>>                                                 entire method is
>>>                                                 synchronized, so
>>>                                                 spurious wakeup
>>>                                                 won't make progress
>>>                                                 until the owner of
>>>                                                 the lock exits the
>>>                                                 method.
>>>
>>>                                                 You could split the
>>>                                                 synchronization into
>>>                                                 two blocks - one
>>>                                                 encompassing the
>>>                                                 wait loop, the other
>>>                                                 in the finally
>>>                                                 block; but it may
>>>                                                 make no difference.
>>>
>>>                                                 Alex
>>>
>>>
>>>                                             You're right, Alex. I'm
>>>                                             so infected with
>>>                                             park/unpark virus that I
>>>                                             missed that ;-)
>>>
>>>                                             Peter
>>>
>>>
>>>                                                 On 17/12/2014 18:36,
>>>                                                 suman shil wrote:
>>>
>>>                                                     Thanks peter for
>>>                                                     your reply. You
>>>                                                     are right. I
>>>                                                     should have
>>>                                                     incremented
>>>                                                     currentAllowedOrder
>>>                                                     in finally block.
>>>
>>>                                                     Suman
>>>                                                     ------------------------------------------------------------------------
>>>                                                     *From:* Peter
>>>                                                     Levart
>>>                                                     <peter.levart at gmail.com
>>>                                                     <mailto:peter.levart at gmail.com>>
>>>                                                     *To:* suman shil
>>>                                                     <suman_krec at yahoo.com
>>>                                                     <mailto:suman_krec at yahoo.com>>;
>>>                                                     Oleksandr Otenko
>>>                                                     <oleksandr.otenko at oracle.com
>>>                                                     <mailto:oleksandr.otenko at oracle.com>>;
>>>                                                     Concurrency-interest
>>>                                                     <concurrency-interest at cs.oswego.edu
>>>                                                     <mailto:concurrency-interest at cs.oswego.edu>>
>>>                                                     *Sent:*
>>>                                                     Wednesday,
>>>                                                     December 17,
>>>                                                     2014 11:54 PM
>>>                                                     *Subject:* Re:
>>>                                                     [concurrency-interest]
>>>                                                     Enforcing
>>>                                                     ordered
>>>                                                     execution of
>>>                                                     critical sections?
>>>
>>>                                                     On 12/17/2014
>>>                                                     06:46 PM, suman
>>>                                                     shil wrote:
>>>
>>>                                                         Thanks for
>>>                                                         your
>>>                                                         response.
>>>                                                         Will
>>>                                                         notifyAll()
>>>                                                         instead of
>>>                                                         notify()
>>>                                                         solve the
>>>                                                         problem?
>>>
>>>
>>>                                                     It will, but you
>>>                                                     should also
>>>                                                     account for
>>>                                                     "spurious"
>>>                                                     wake-ups. You
>>>                                                     should increment
>>>                                                     currentAllowedOrder
>>>                                                     only after
>>>                                                     return from
>>>                                                     callable.call
>>>                                                     (in finally
>>>                                                     block just
>>>                                                     before notifyAll()).
>>>
>>>                                                     Otherwise a nice
>>>                                                     solution - with
>>>                                                     minimal state,
>>>                                                     providing that
>>>                                                     not many threads
>>>                                                     meet at the same
>>>                                                     time...
>>>
>>>                                                     Regards, Peter
>>>
>>>                                                         RegardsSuman
>>>                                                                From:
>>>                                                         Oleksandr
>>>                                                         Otenko<oleksandr.otenko at oracle.com
>>>                                                         <mailto:oleksandr.otenko at oracle.com>>
>>>                                                         <mailto:oleksandr.otenko at oracle.com
>>>                                                         <mailto:oleksandr.otenko at oracle.com>>
>>>                                                           To: suman
>>>                                                         shil<suman_krec at yahoo.com
>>>                                                         <mailto:suman_krec at yahoo.com>>
>>>                                                         <mailto:suman_krec at yahoo.com
>>>                                                         <mailto:suman_krec at yahoo.com>>;
>>>                                                         Concurrency-interest<concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:concurrency-interest at cs.oswego.edu>>
>>>                                                         <mailto:concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:concurrency-interest at cs.oswego.edu>> 
>>>                                                           Sent:
>>>                                                         Wednesday,
>>>                                                         December 17,
>>>                                                         2014 9:55 PM
>>>                                                           Subject:
>>>                                                         Re:
>>>                                                         [concurrency-interest]
>>>                                                         Enforcing
>>>                                                         ordered
>>>                                                         execution of
>>>                                                         critical
>>>                                                         sections?
>>>                                                               There
>>>                                                         is no
>>>                                                         guarantee
>>>                                                         you'll ever
>>>                                                         hand over
>>>                                                         the control
>>>                                                         to the right
>>>                                                         thread upon
>>>                                                         notify()
>>>                                                             Alex
>>>
>>>                                                         On
>>>                                                         17/12/2014
>>>                                                         14:07, suman
>>>                                                         shil wrote:
>>>                                                               Hi,
>>>                                                         Following is
>>>                                                         my solution
>>>                                                         to solve
>>>                                                         this
>>>                                                         problem.
>>>                                                         Please let
>>>                                                         me know if I
>>>                                                         am missing
>>>                                                         something.
>>>                                                            public
>>>                                                         class
>>>                                                         OrderedExecutor
>>>                                                         {  private
>>>                                                         int
>>>                                                         currentAllowedOrder
>>>                                                         = 0; 
>>>                                                         private int
>>>                                                         maxLength =
>>>                                                         0;  public
>>>                                                         OrderedExecutor(int
>>>                                                         n)  {
>>>                                                         this.maxLength
>>>                                                         = n;  }
>>>                                                         public
>>>                                                         synchronized
>>>                                                         Object
>>>                                                         execCriticalSectionInOrder(
>>>                                                         int order,
>>>                                                         Callable<Object>
>>>                                                         callable)
>>>                                                          throws
>>>                                                         Exception  {
>>>                                                         if (order >=
>>>                                                         maxLength) 
>>>                                                         { throw new
>>>                                                         Exception("Exceeds
>>>                                                         maximum
>>>                                                         order "+
>>>                                                         maxLength);
>>>                                                         }
>>>                                                         while(order
>>>                                                         !=
>>>                                                         currentAllowedOrder)
>>>                                                         {  wait(); 
>>>                                                         }   try  {
>>>                                                         currentAllowedOrder
>>>                                                         =
>>>                                                         currentAllowedOrder+1;
>>>                                                         return
>>>                                                         callable.call();
>>>                                                         }  finally 
>>>                                                         { notify(); 
>>>                                                         } } }
>>>                                                            Regards Suman
>>>                                                          From: Peter
>>>                                                         Levart<peter.levart at gmail.com
>>>                                                         <mailto:peter.levart at gmail.com>>
>>>                                                         <mailto:peter.levart at gmail.com
>>>                                                         <mailto:peter.levart at gmail.com>>
>>>                                                           To: Hanson
>>>                                                         Char<hanson.char at gmail.com
>>>                                                         <mailto:hanson.char at gmail.com>>
>>>                                                         <mailto:hanson.char at gmail.com
>>>                                                         <mailto:hanson.char at gmail.com>> 
>>>                                                           Cc:
>>>                                                         concurrency-interest<concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:concurrency-interest at cs.oswego.edu>>
>>>                                                         <mailto:concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:concurrency-interest at cs.oswego.edu>> 
>>>                                                           Sent:
>>>                                                         Sunday,
>>>                                                         December 14,
>>>                                                         2014 11:01 PM
>>>                                                           Subject:
>>>                                                         Re:
>>>                                                         [concurrency-interest]
>>>                                                         Enforcing
>>>                                                         ordered
>>>                                                         execution of
>>>                                                         critical
>>>                                                         sections?
>>>                                                                   On
>>>                                                         12/14/2014
>>>                                                         06:11 PM,
>>>                                                         Hanson Char
>>>                                                         wrote:
>>>                                                              Hi Peter,
>>>                                                            Thanks
>>>                                                         for this
>>>                                                         proposed
>>>                                                         idea of
>>>                                                         using
>>>                                                         LockSupport.
>>>                                                         This begs
>>>                                                         the
>>>                                                         question:
>>>                                                         which one
>>>                                                         would you
>>>                                                         choose if
>>>                                                         you had all
>>>                                                         three
>>>                                                         (correct)
>>>                                                         implementation
>>>                                                         available?
>>>                                                         (Semaphore,
>>>                                                         CountDownLatch,
>>>                                                         or LockSupport)?
>>>                                                            Regards,
>>>                                                         Hanson
>>>                                                             The
>>>                                                         Semaphore/CountDownLatch
>>>                                                         variants are
>>>                                                         equivalent
>>>                                                         if you don't
>>>                                                         need re-use.
>>>                                                         So any would
>>>                                                         do. They
>>>                                                         lack
>>>                                                         invalid-use
>>>                                                         detection.
>>>                                                         What happens
>>>                                                         if they are
>>>                                                         not used as
>>>                                                         intended?
>>>                                                         Semaphore
>>>                                                         variant acts
>>>                                                         differently
>>>                                                         than
>>>                                                         CountDownLatch
>>>                                                         variant. The
>>>                                                         low-level
>>>                                                         variant I
>>>                                                         proposed
>>>                                                         detects
>>>                                                         invalid
>>>                                                         usage. So I
>>>                                                         would
>>>                                                         probably use
>>>                                                         this one.
>>>                                                         But the low
>>>                                                         level
>>>                                                         variant is
>>>                                                         harder to
>>>                                                         reason about
>>>                                                         it's
>>>                                                         correctness.
>>>                                                         I think it
>>>                                                         is correct,
>>>                                                         but you
>>>                                                         should show
>>>                                                         it to
>>>                                                         somebody
>>>                                                         else to
>>>                                                         confirm this.
>>>                                                             Another
>>>                                                         question is
>>>                                                         whether you
>>>                                                         actually
>>>                                                         need this
>>>                                                         kind of
>>>                                                         synchronizer. Maybe
>>>                                                         if you
>>>                                                         explained
>>>                                                         what you are
>>>                                                         trying to
>>>                                                         achieve,
>>>                                                         somebody
>>>                                                         could have
>>>                                                         an idea how
>>>                                                         to do that
>>>                                                         even more
>>>                                                         elegantly...
>>>                                                             Regards,
>>>                                                         Peter
>>>                                                              On Sun,
>>>                                                         Dec 14, 2014
>>>                                                         at 9:01 AM,
>>>                                                         Peter
>>>                                                         Levart<peter.levart at gmail.com
>>>                                                         <mailto:peter.levart at gmail.com>>
>>>                                                         <mailto:peter.levart at gmail.com
>>>                                                         <mailto:peter.levart at gmail.com>> 
>>>                                                         wrote:
>>>                                                            Hi Hanson,
>>>                                                             This one
>>>                                                         is more
>>>                                                         low-level,
>>>                                                         but catches
>>>                                                         some invalid
>>>                                                         usages and
>>>                                                         is more
>>>                                                         resource-friendly:
>>>                                                               public
>>>                                                         class
>>>                                                         OrderedExecutor
>>>                                                         {
>>>                                                                
>>>                                                         public <T> T
>>>                                                         execCriticalSectionInOrder(
>>>                                                         final int order,
>>>                                                         final
>>>                                                         Supplier<T>
>>>                                                         criticalSection
>>>                                                               )
>>>                                                         throws
>>>                                                         InterruptedException
>>>                                                         {
>>>                                                                   if
>>>                                                         (order < 0) {
>>>                                                          throw new
>>>                                                         IllegalArgumentException("'order'
>>>                                                         should be >=
>>>                                                         0");
>>>                                                                   }
>>>                                                                   if
>>>                                                         (order > 0) {
>>>                                                         waitForDone(order
>>>                                                         - 1);
>>>                                                                   }
>>>                                                                   try {
>>>                                                         return
>>>                                                         criticalSection.get();
>>>                                                                   }
>>>                                                         finally {
>>>                                                         notifyDone(order);
>>>                                                                   }
>>>                                                               }
>>>                                                         private
>>>                                                         static final
>>>                                                         Object DONE
>>>                                                         = new Object();
>>>                                                              
>>>                                                         private
>>>                                                         final
>>>                                                         ConcurrentMap<Integer,
>>>                                                         Object>
>>>                                                         signals =
>>>                                                         new
>>>                                                         ConcurrentHashMap<>();
>>>                                                         private void
>>>                                                         waitForDone(int
>>>                                                         order)
>>>                                                         throws
>>>                                                         InterruptedException
>>>                                                         {
>>>                                                         Object sig =
>>>                                                         signals.putIfAbsent(order,
>>>                                                         Thread.currentThread());
>>>                                                                   if
>>>                                                         (sig != null
>>>                                                         && sig !=
>>>                                                         DONE) {
>>>                                                         throw new
>>>                                                         IllegalStateException();
>>>                                                                   }
>>>                                                         while (sig
>>>                                                         != DONE) {
>>>                                                         LockSupport.park();
>>>                                                         if
>>>                                                         (Thread.interrupted())
>>>                                                         {
>>>                                                             throw
>>>                                                         new
>>>                                                         InterruptedException();
>>>                                                         }
>>>                                                         sig =
>>>                                                         signals.get(order);
>>>                                                                   }
>>>                                                               }
>>>                                                         private void
>>>                                                         notifyDone(int
>>>                                                         order) {
>>>                                                         Object sig =
>>>                                                         signals.putIfAbsent(order,
>>>                                                         DONE);
>>>                                                                   if
>>>                                                         (sig
>>>                                                         instanceof
>>>                                                         Thread) {
>>>                                                         if
>>>                                                         (!signals.replace(order,
>>>                                                         sig, DONE)) {
>>>                                                             throw
>>>                                                         new
>>>                                                         IllegalStateException();
>>>                                                         }
>>>                                                         LockSupport.unpark((Thread)
>>>                                                         sig);
>>>                                                                   }
>>>                                                         else if (sig
>>>                                                         != null) {
>>>                                                         throw new
>>>                                                         IllegalStateException();
>>>                                                                   }
>>>                                                               }
>>>                                                           }
>>>                                                              
>>>                                                         Regards, Peter
>>>                                                             On
>>>                                                         12/14/2014
>>>                                                         05:08 PM,
>>>                                                         Peter Levart
>>>                                                         wrote:
>>>                                                                On
>>>                                                         12/14/2014
>>>                                                         04:20 PM,
>>>                                                         Hanson Char
>>>                                                         wrote:
>>>                                                              Hi Peter,
>>>                                                            Thanks
>>>                                                         for the
>>>                                                         suggestion,
>>>                                                         and sorry
>>>                                                         about not
>>>                                                         being clear
>>>                                                         about one
>>>                                                         important
>>>                                                         detail: "n"
>>>                                                         is not known
>>>                                                         a priori
>>>                                                         when
>>>                                                         constructing
>>>                                                         an
>>>                                                         OrderedExecutor.
>>>                                                         Does this
>>>                                                         mean the use
>>>                                                         of
>>>                                                         CountDownLatch
>>>                                                         is ruled out?
>>>                                                             If you
>>>                                                         know at
>>>                                                         least the
>>>                                                         upper bound
>>>                                                         of 'n', it
>>>                                                         can be used
>>>                                                         with such
>>>                                                         'n'.
>>>                                                         Otherwise
>>>                                                         something
>>>                                                         that
>>>                                                         dynamically
>>>                                                         re-sizes the
>>>                                                         array could
>>>                                                         be devised.
>>>                                                         Or you could
>>>                                                         simply use a
>>>                                                         ConcurrentHashMap
>>>                                                         instead of
>>>                                                         array where
>>>                                                         keys are
>>>                                                         'order' values:
>>>                                                               public
>>>                                                         class
>>>                                                         OrderedExecutor<T>
>>>                                                         {
>>>                                                         private
>>>                                                         final
>>>                                                         ConcurrentMap<Integer,
>>>                                                         CountDownLatch>
>>>                                                         latches =
>>>                                                         new
>>>                                                         ConcurrentHashMap<>();
>>>                                                                
>>>                                                         public T
>>>                                                         execCriticalSectionInOrder(final
>>>                                                         int order,
>>>                                                         final
>>>                                                         Supplier<T>
>>>                                                         criticalSection)
>>>                                                         throws
>>>                                                         InterruptedException
>>>                                                         {
>>>                                                                   if
>>>                                                         (order > 0) {
>>>                                                         latches.computeIfAbsent(order
>>>                                                         - 1, o ->
>>>                                                         new
>>>                                                         CountDownLatch(1)).await();
>>>                                                                   }
>>>                                                                   try {
>>>                                                         return
>>>                                                         criticalSection.get();
>>>                                                                   }
>>>                                                         finally {
>>>                                                         latches.computeIfAbsent(order,
>>>                                                         o -> new
>>>                                                         CountDownLatch(1)).countDown();
>>>                                                                   }
>>>                                                               }
>>>                                                           }
>>>                                                              
>>>                                                         Regards, Peter
>>>                                                                  
>>>                                                         You guessed
>>>                                                         right: it's
>>>                                                         a one-shot
>>>                                                         object for a
>>>                                                         particular
>>>                                                         OrderedExecutor
>>>                                                         instance,
>>>                                                         and "order"
>>>                                                         must be
>>>                                                         called
>>>                                                         indeed at
>>>                                                         most once.
>>>                                                            Regards,
>>>                                                         Hanson
>>>                                                           On Sun,
>>>                                                         Dec 14, 2014
>>>                                                         at 2:21 AM,
>>>                                                         Peter
>>>                                                         Levart<peter.levart at gmail.com
>>>                                                         <mailto:peter.levart at gmail.com>>
>>>                                                         <mailto:peter.levart at gmail.com
>>>                                                         <mailto:peter.levart at gmail.com>> 
>>>                                                         wrote:
>>>                                                            Hi Hanson,
>>>                                                             I don't
>>>                                                         think
>>>                                                         anything
>>>                                                         like that
>>>                                                         readily
>>>                                                         exists  in
>>>                                                         java.lang.concurrent,
>>>                                                         but what you
>>>                                                         describe
>>>                                                         should be
>>>                                                         possible to
>>>                                                         achieve with
>>>                                                         composition
>>>                                                         of existing
>>>                                                         primitives.
>>>                                                         You haven't
>>>                                                         given any
>>>                                                         additional
>>>                                                         hints to
>>>                                                         what your
>>>                                                         OrderedExecutor
>>>                                                         should
>>>                                                         behave like.
>>>                                                         Should it be
>>>                                                         a one-shot
>>>                                                         object (like
>>>                                                         CountDownLatch)
>>>                                                         or a
>>>                                                         re-usable
>>>                                                         one (like
>>>                                                         CyclicBarrier)?
>>>                                                         Will
>>>                                                         execCriticalSectionInOrder()
>>>                                                         for a
>>>                                                         particular
>>>                                                         OrderedExecutor
>>>                                                         instance and
>>>                                                         'order'
>>>                                                         value be
>>>                                                         called at
>>>                                                         most once?
>>>                                                         If yes (and
>>>                                                         I think that
>>>                                                         only a
>>>                                                         one-shot
>>>                                                         object 
>>>                                                         makes sense
>>>                                                         here), an
>>>                                                         array of
>>>                                                         CountDownLatch(es)
>>>                                                         could be used:
>>>                                                             public
>>>                                                         class
>>>                                                         OrderedExecutor<T>
>>>                                                         {
>>>                                                              
>>>                                                         private
>>>                                                         final
>>>                                                         CountDownLatch[]
>>>                                                         latches;
>>>                                                                
>>>                                                         public
>>>                                                         OrderedExecutor(int
>>>                                                         n) {
>>>                                                                   if
>>>                                                         (n < 1)
>>>                                                         throw new
>>>                                                         IllegalArgumentException("'n'
>>>                                                         should be >=
>>>                                                         1");
>>>                                                         latches =
>>>                                                         new
>>>                                                         CountDownLatch[n
>>>                                                         - 1];
>>>                                                                  
>>>                                                         for (int i =
>>>                                                         0; i <
>>>                                                         latches.length;
>>>                                                         i++) {
>>>                                                         latches[i] =
>>>                                                         new
>>>                                                         CountDownLatch(1);
>>>                                                                   }
>>>                                                               }
>>>                                                                
>>>                                                         public T
>>>                                                         execCriticalSectionInOrder(final
>>>                                                         int order,
>>>                                                          final
>>>                                                         Supplier<T>
>>>                                                         criticalSection)
>>>                                                         throws
>>>                                                         InterruptedException
>>>                                                         {
>>>                                                                   if
>>>                                                         (order < 0
>>>                                                         || order >
>>>                                                         latches.length)
>>>                                                         throw new
>>>                                                         IllegalArgumentException("'order'
>>>                                                         should be
>>>                                                         [0..." +
>>>                                                         latches.length
>>>                                                         + "]");
>>>                                                                   if
>>>                                                         (order > 0) {
>>>                                                         latches[order -
>>>                                                         1].await();
>>>                                                                   }
>>>                                                                   try {
>>>                                                         return
>>>                                                         criticalSection.get();
>>>                                                                   }
>>>                                                         finally {
>>>                                                         if (order <
>>>                                                         latches.length)
>>>                                                         {
>>>                                                         latches[order].countDown();
>>>                                                         }
>>>                                                                   }
>>>                                                               }
>>>                                                           }
>>>                                                              
>>>                                                         Regards, Peter
>>>                                                             On
>>>                                                         12/14/2014
>>>                                                         05:26 AM,
>>>                                                         Hanson Char
>>>                                                         wrote:
>>>                                                           Hi, I am
>>>                                                         looking for
>>>                                                         a construct
>>>                                                         that can  be
>>>                                                         used to
>>>                                                         efficiently
>>>                                                         enforce
>>>                                                         ordered
>>>                                                         execution of
>>>                                                         multiple
>>>                                                         critical
>>>                                                         sections,
>>>                                                         each calling
>>>                                                         from a 
>>>                                                         different
>>>                                                         thread. The
>>>                                                         calling
>>>                                                         threads may
>>>                                                         run in
>>>                                                         parallel and
>>>                                                         may call the
>>>                                                         execution
>>>                                                         method out
>>>                                                         of order.
>>>                                                         The
>>>                                                         perceived
>>>                                                         construct
>>>                                                         would
>>>                                                         therefore be
>>>                                                         responsible
>>>                                                         for
>>>                                                         re-ordering
>>>                                                         the
>>>                                                         execution of
>>>                                                         those
>>>                                                         threads, so
>>>                                                         that their
>>>                                                         critical
>>>                                                         sections
>>>                                                         (and only
>>>                                                         the critical
>>>                                                         section)
>>>                                                         will be
>>>                                                         executed in
>>>                                                         order. Would
>>>                                                         something
>>>                                                         like the
>>>                                                         following
>>>                                                         API already
>>>                                                         exist? /** *
>>>                                                         Used to
>>>                                                         enforce
>>>                                                         ordered
>>>                                                         execution of
>>>                                                         critical
>>>                                                         sections
>>>                                                         calling from
>>>                                                         multiple *
>>>                                                         threads,
>>>                                                         parking and
>>>                                                         unparking
>>>                                                         the threads
>>>                                                         as
>>>                                                         necessary.
>>>                                                         */ public
>>>                                                         class
>>>                                                         OrderedExecutor<T>
>>>                                                         { /** *
>>>                                                         Executes a
>>>                                                         critical
>>>                                                         section at
>>>                                                         most once
>>>                                                         with the
>>>                                                         given order,
>>>                                                         parking *
>>>                                                         and
>>>                                                         unparking
>>>                                                         the current
>>>                                                         thread as 
>>>                                                         necessary so
>>>                                                         that all
>>>                                                         critical *
>>>                                                         sections
>>>                                                         executed  by
>>>                                                         different
>>>                                                         threads
>>>                                                         using this 
>>>                                                         executor
>>>                                                         take place
>>>                                                         in * the
>>>                                                         order from 1
>>>                                                         to n
>>>                                                         consecutively.
>>>                                                         */ public T
>>>                                                         execCriticalSectionInOrder
>>>                                                         (  final int
>>>                                                         order, final
>>>                                                         Callable<T>
>>>                                                         criticalSection)
>>>                                                         throws
>>>                                                         InterruptedException;
>>>                                                         } Regards,
>>>                                                         Hanson
>>>                                                         _______________________________________________Concurrency-interest
>>>                                                         mailing
>>>                                                         listConcurrency-interest at cs.oswego.edu
>>>                                                         <mailto:listConcurrency-interest at cs.oswego.edu>
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu>>
>>>                                                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>                                                         _______________________________________________
>>>                                                         Concurrency-interest
>>>                                                         mailing list
>>>                                                         Concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu>>
>>>                                                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>                                                         _______________________________________________
>>>                                                         Concurrency-interest
>>>                                                         mailing list
>>>                                                         Concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu>>
>>>                                                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>>                                                         _______________________________________________
>>>                                                         Concurrency-interest
>>>                                                         mailing list
>>>                                                         Concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu
>>>                                                         <mailto:Concurrency-interest at cs.oswego.edu>>
>>>                                                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>                                             _______________________________________________
>>>                                             Concurrency-interest
>>>                                             mailing list
>>>                                             Concurrency-interest at cs.oswego.edu
>>>                                             <mailto:Concurrency-interest at cs.oswego.edu>
>>>                                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>                                         _______________________________________________
>>                                         Concurrency-interest mailing list
>>                                         Concurrency-interest at cs.oswego.edu
>>                                         <mailto:Concurrency-interest at cs.oswego.edu>
>>                                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>                                 _______________________________________________
>>                                 Concurrency-interest mailing list
>>                                 Concurrency-interest at cs.oswego.edu
>>                                 <mailto:Concurrency-interest at cs.oswego.edu>
>>                                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>                                 _______________________________________________
>>                                 Concurrency-interest mailing list
>>                                 Concurrency-interest at cs.oswego.edu
>>                                 <mailto:Concurrency-interest at cs.oswego.edu>
>>                                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>                         _______________________________________________
>>                         Concurrency-interest mailing list
>>                         Concurrency-interest at cs.oswego.edu
>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/0921452f/attachment-0001.html>

From ben.manes at gmail.com  Sun Dec 21 16:20:45 2014
From: ben.manes at gmail.com (Benjamin Manes)
Date: Sun, 21 Dec 2014 13:20:45 -0800
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <54973337.30701@gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>
	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
	<5495949E.1030500@cs.oswego.edu>
	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
	<5496D732.3010308@cs.oswego.edu>
	<CANPzfU9qoJyUgPwK3RtPeca6n9Vif6kq1yg98QbVByw58sn3aA@mail.gmail.com>
	<5496EB91.9090504@cs.oswego.edu>
	<CANPzfU-w0piYhehLAG9bJh5W4K+U2rA2EkymvwHWsksLO5t4sQ@mail.gmail.com>
	<CAGu0=MNxmOhgtC_mX2MmMZsc6C64fC3eiiY7AaX4v-amk3P6Hw@mail.gmail.com>
	<54973337.30701@gmail.com>
Message-ID: <CAGu0=MMkdBe8hjs=wJyK+TS_cj6kQwCiw=Ao9PfQVN5Cqvew=w@mail.gmail.com>

Thanks Peter,

I don't think compute() changes my suggestion. If the bucket count is
larger than one, then I was suggesting inserting a regular Node with a null
value that is updated when the mapping function completes successfully. The
node is scavenged when the computation fails, but since we know the
instance that extra scan should be cheap. By updating a regular node I had
hoped one could avoid garbage in the common case.

Experimenting to determine the feasibility of this type of change is too
invasive for me to feel comfortable trying given the interactions with all
the other map operations that I may not fully consider at the moment. Since
ReservationNode is a special type, it seems likely that there are scenarios
that I'm not considering.

On Sun, Dec 21, 2014 at 12:53 PM, Peter Levart <peter.levart at gmail.com>
wrote:

>  Hi Benjamin,
>
> Doug is certainly the one to give you the best explanation, but let me
> try...
>
> On 12/21/2014 08:36 PM, Benjamin Manes wrote:
>
>  This is worth considering, but it only detects one form of
>> misbehavior when the bin doesn't hold any other nodes.
>> Otherwise, the first node need not be a ReservationNode.
>
>
> I'm not comfortable enough with the code to experiment, but reading it I
> keep wondering if inserting a node prior to computation would provide
> detection. The value would be null, to be filled in if the computation
> succeeds. If a reentrant computation occurs then when the existing node is
> found and we observe that the value is unset, then we have detected a cycle
> since we already hold the necessary lock. This would require skipping those
> nodes for methods that traverse the entire map, like iterators and size().
>
>
> ...ReservationNode is such node. But is only inserted (with CAS) as 1st
> node of the bucket when the bucket is still empty. This is to provide the
> object to use as the lock when changing anything in the bucket. It is
> replaced with normal Node when the function's result is obtained. Logically
> it is skipped on look-ups and traversals. If a node is inserted into the
> bucket that already contains nodes, no ReservationNode is used.
>
> And it's not only inserting (computeIfAbsent) that calls-back user code.
> See also compute() method. This method overwrites the old value with new
> value returned from function in existing node or even removes a Node from
> the bucket (if function returns null). How would you detect that such
> process is taking place when you re-enter?
>
> I thought about it for some time, but didn't find any existing state
> combination that could be used for such checks. Perhaps a ReservationNode
> could be inserted just after 1st node in bucket temporarily just to mark
> the fact that we entered synchronization block and later remove it before
> exiting the block. This would most probably work, but would also mean that
> we produce one object of garbage for each such call. Maybe this is better
> than 4 bytes of state overhead for each node (see my experiment).
>
>
>  Internally, this would make computations feel more like the JCiP-style
> future based approach Viktor mentioned and allow for supporting batch
> computations. It has the nice property of not wasting more memory with
> values wrapped in futures, unless an asynchronous cache was explicitly
> requested. That can be built on-top of CHM, as was always possible, and an
> API option that I plan on providing.
>
>
> Don't forget that CHM provides a lock-free lookup. We would not like to
> loose this feature. Lazy evaluation (or waiting on computation) can be
> added on top of CHM if one needs it.
>
> Regards, Peter
>
>
>  The remaining mysterious cases are nested computeIfAbsent
>> calls for different keys that happen to hash to the same bin.
>> These conceptually ought to deadlock. Instead they will
>> sometimes happen to "work" but other times loop.
>
>
>  If changes like the above removed the live lock issue, then I'd be much
> happier with a resulting deadlock remain unsolved. That's much easier for
> developers to notice with jdk tooling when debugging server issues and
> recognize the problem as their fault.
>
>  On Sun, Dec 21, 2014 at 10:07 AM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>>
>>
>> On Sun, Dec 21, 2014 at 4:47 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>>> On 12/21/2014 09:59 AM, Viktor Klang wrote:
>>>
>>>> For "computeIfAbsent"-style maps which are intended for multithreaded
>>>> use, I
>>>> tend to prefer to use ConcurrentMap[Key, CompletionStage[Value]],
>>>>
>>>
>>>  Right. Prior to jdk8, we suggested variations of this and other
>>> techniques that remain preferable in many cases. However, the
>>> most common CHM user error was trying (but failing) to emulate
>>> what is now supported as computeIfAbsent. This is an improvement,
>>> but leads to new (much less common) kinds of potential errors.
>>> We cannot automatically eliminate or trap all of them. It
>>> might be possible to do more than we do now, either internally
>>> (inside CHM) or, more likely, with improved developer tools.
>>
>>
>>  Absolutely, and the fact that there was only blocking Futures in the
>> JDK at that time.
>> Having "monadic" Futures like CompletableFuture means you can avoid
>> blocking completely (assuming that the CHM impl is non-blocking). This is
>> highly useful when using it as a cache, and especially if there's a very
>> skewed read distribution, paired with a multitude of readers.
>>
>>
>>>
>>>
>>> -Doug
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>>  --
>>   Cheers,
>> ?
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/61ec0ea4/attachment.html>

From dl at cs.oswego.edu  Sun Dec 21 17:32:14 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 21 Dec 2014 17:32:14 -0500
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <549720A6.70208@gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>	<5495949E.1030500@cs.oswego.edu>	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
	<5496D732.3010308@cs.oswego.edu> <549720A6.70208@gmail.com>
Message-ID: <54974A6E.5010305@cs.oswego.edu>

On 12/21/2014 02:33 PM, Peter Levart wrote:

> ... but measuring on my i7 Linux PC shows that holdsLock() amounts to an
> addition of 2/3 the overhead of non-contended locking.
>
> If we added an int field to Node class...

... then we'd probably do a lot more with it than this :-)

Among the requests/goals for jdk8 CHM was to reduce, not increase,
footprint compared to previous versions. Introducing space overhead
for the sake of small improvements in helping to diagnose uncommon
user errors is not a very defensible tradeoff.

But there may be some ways to more narrowly address the actual
user error scenarios; i.e., for m.computeIfAbsent(k1, f) where
function f calls m.computeIfAbsent(k2, ...),
across three cases: (1) k1 != k2 and they reside in different bins,
or (2) k1 != k2 but they reside in the same bin, or (3) k1 == k2.
(Further mixtures with computeIfPresent, compute, and/or merge
don't seem to change the basic problem.)

We should write off coping with deadlock and infinite recursion. But
we can still add some (cheap) integrity checks that can only be
violated in the remaining user-error scenarios. In particular
the one below, which will only trigger after-the-fact, but
still effective. I'll explore others too.

*** ConcurrentHashMap.java.~1.258.~	2014-12-20 11:16:27.835140276 -0500
--- ConcurrentHashMap.java	2014-12-21 17:23:43.233685637 -0500
***************
*** 1633,1638 ****
--- 1633,1639 ----
                           } finally {
                               setTabAt(tab, i, node);
                           }
+                         if (r.next != null) throw new IllegalStateException();
                       }
                   }
                   if (binCount != 0)


From peter.levart at gmail.com  Sun Dec 21 17:48:08 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 21 Dec 2014 23:48:08 +0100
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <CAGu0=MMkdBe8hjs=wJyK+TS_cj6kQwCiw=Ao9PfQVN5Cqvew=w@mail.gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>
	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
	<5495949E.1030500@cs.oswego.edu>
	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
	<5496D732.3010308@cs.oswego.edu>
	<CANPzfU9qoJyUgPwK3RtPeca6n9Vif6kq1yg98QbVByw58sn3aA@mail.gmail.com>
	<5496EB91.9090504@cs.oswego.edu>
	<CANPzfU-w0piYhehLAG9bJh5W4K+U2rA2EkymvwHWsksLO5t4sQ@mail.gmail.com>
	<CAGu0=MNxmOhgtC_mX2MmMZsc6C64fC3eiiY7AaX4v-amk3P6Hw@mail.gmail.com>
	<54973337.30701@gmail.com>
	<CAGu0=MMkdBe8hjs=wJyK+TS_cj6kQwCiw=Ao9PfQVN5Cqvew=w@mail.gmail.com>
Message-ID: <54974E28.1090200@gmail.com>


On 12/21/2014 10:20 PM, Benjamin Manes wrote:
> Thanks Peter,
>
> I don't think compute() changes my suggestion. If the bucket count is 
> larger than one, then I was suggesting inserting a regular Node with a 
> null value that is updated when the mapping function completes 
> successfully.

You mean "a Node that is *removed*" when computation ends. Yes, as I 
said, that is a possibility. A "marker" node inserted just after 1st 
node (or before 1st node?) in bucket and later removed. This would mean 
three additional volatile writes though (next field is volatile). Not to 
mention what to do when nodes are forming a tree (yes, that's possible).

> The node is scavenged when the computation fails, but since we know 
> the instance that extra scan should be cheap. By updating a regular 
> node I had hoped one could avoid garbage in the common case.

If common case was inserting, then perhaps, yes. But currently the 
skipping detection is designed by checking the hash field which is final.


>
> Experimenting to determine the feasibility of this type of change is 
> too invasive for me to feel comfortable trying given the interactions 
> with all the other map operations that I may not fully consider at the 
> moment. Since ReservationNode is a special type, it seems likely that 
> there are scenarios that I'm not considering.

A change using "marker" nodes would be very invasive. Each operation 
itself traverses the node chain or tree and either inserts, modifies or 
removes a node (possibly the 1st node of the bucket). Adding insertion 
of "marker" node and later removal would complicate the 
traversal/insertion/removal logic of each individual operation with user 
call-backs in it's own unique way. I don't know if such change is 
warranted just to get better diagnostics.

Regards, Peter

>
> On Sun, Dec 21, 2014 at 12:53 PM, Peter Levart <peter.levart at gmail.com 
> <mailto:peter.levart at gmail.com>> wrote:
>
>     Hi Benjamin,
>
>     Doug is certainly the one to give you the best explanation, but
>     let me try...
>
>     On 12/21/2014 08:36 PM, Benjamin Manes wrote:
>>
>>         This is worth considering, but it only detects one form of
>>         misbehavior when the bin doesn't hold any other nodes.
>>         Otherwise, the first node need not be a ReservationNode.
>>
>>     I'm not comfortable enough with the code to experiment, but
>>     reading it I keep wondering if inserting a node prior to
>>     computation would provide detection. The value would be null, to
>>     be filled in if the computation succeeds. If a reentrant
>>     computation occurs then when the existing node is found and we
>>     observe that the value is unset, then we have detected a cycle
>>     since we already hold the necessary lock. This would require
>>     skipping those nodes for methods that traverse the entire map,
>>     like iterators and size().
>
>     ...ReservationNode is such node. But is only inserted (with CAS)
>     as 1st node of the bucket when the bucket is still empty. This is
>     to provide the object to use as the lock when changing anything in
>     the bucket. It is replaced with normal Node when the function's
>     result is obtained. Logically it is skipped on look-ups and
>     traversals. If a node is inserted into the bucket that already
>     contains nodes, no ReservationNode is used.
>
>     And it's not only inserting (computeIfAbsent) that calls-back user
>     code. See also compute() method. This method overwrites the old
>     value with new value returned from function in existing node or
>     even removes a Node from the bucket (if function returns null).
>     How would you detect that such process is taking place when you
>     re-enter?
>
>     I thought about it for some time, but didn't find any existing
>     state combination that could be used for such checks. Perhaps a
>     ReservationNode could be inserted just after 1st node in bucket
>     temporarily just to mark the fact that we entered synchronization
>     block and later remove it before exiting the block. This would
>     most probably work, but would also mean that we produce one object
>     of garbage for each such call. Maybe this is better than 4 bytes
>     of state overhead for each node (see my experiment).
>
>>
>>     Internally, this would make computations feel more like the
>>     JCiP-style future based approach Viktor mentioned and allow for
>>     supporting batch computations. It has the nice property of not
>>     wasting more memory with values wrapped in futures, unless an
>>     asynchronous cache was explicitly requested. That can be built
>>     on-top of CHM, as was always possible, and an API option that I
>>     plan on providing.
>
>     Don't forget that CHM provides a lock-free lookup. We would not
>     like to loose this feature. Lazy evaluation (or waiting on
>     computation) can be added on top of CHM if one needs it.
>
>     Regards, Peter
>
>>
>>         The remaining mysterious cases are nested computeIfAbsent
>>         calls for different keys that happen to hash to the same bin.
>>         These conceptually ought to deadlock. Instead they will
>>         sometimes happen to "work" but other times loop.
>>
>>
>>     If changes like the above removed the live lock issue, then I'd
>>     be much happier with a resulting deadlock remain unsolved. That's
>>     much easier for developers to notice with jdk tooling when
>>     debugging server issues and recognize the problem as their fault.
>>
>>     On Sun, Dec 21, 2014 at 10:07 AM, Viktor Klang
>>     <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com>> wrote:
>>
>>
>>
>>         On Sun, Dec 21, 2014 at 4:47 PM, Doug Lea <dl at cs.oswego.edu
>>         <mailto:dl at cs.oswego.edu>> wrote:
>>
>>             On 12/21/2014 09:59 AM, Viktor Klang wrote:
>>
>>                 For "computeIfAbsent"-style maps which are intended
>>                 for multithreaded use, I
>>                 tend to prefer to use ConcurrentMap[Key,
>>                 CompletionStage[Value]],
>>
>>
>>             Right. Prior to jdk8, we suggested variations of this and
>>             other
>>             techniques that remain preferable in many cases. However, the
>>             most common CHM user error was trying (but failing) to
>>             emulate
>>             what is now supported as computeIfAbsent. This is an
>>             improvement,
>>             but leads to new (much less common) kinds of potential
>>             errors.
>>             We cannot automatically eliminate or trap all of them. It
>>             might be possible to do more than we do now, either
>>             internally
>>             (inside CHM) or, more likely, with improved developer tools.
>>
>>
>>         Absolutely, and the fact that there was only blocking Futures
>>         in the JDK at that time.
>>         Having "monadic" Futures like CompletableFuture means you can
>>         avoid blocking completely (assuming that the CHM impl is
>>         non-blocking). This is highly useful when using it as a
>>         cache, and especially if there's a very skewed read
>>         distribution, paired with a multitude of readers.
>>
>>
>>
>>             -Doug
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>         -- 
>>         Cheers,
>>         ?
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/d0d28e51/attachment-0001.html>

From cowwoc at bbs.darktech.org  Sun Dec 21 17:51:35 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Sun, 21 Dec 2014 15:51:35 -0700 (MST)
Subject: [concurrency-interest] More Javadoc problems
In-Reply-To: <5495BED8.2060703@cs.oswego.edu>
References: <1418997636377-11669.post@n7.nabble.com>
	<54956936.7020905@cs.oswego.edu>
	<54959818.6050801@bbs.darktech.org>
	<5495BED8.2060703@cs.oswego.edu>
Message-ID: <5497517A.2000301@bbs.darktech.org>

On 20/12/2014 1:23 PM, Doug Lea [via JSR166 Concurrency] wrote:
> > because if it did it would throw exception2.addSuppressed(exception1)
> > instead of exception1. Would it not?
>
> Not. This is only done in try-with-resources.
> See the (adjacent) JLS sections:
>
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-14.html#jls-14.20.2
>
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-14.html#jls-14.20.3
>

My interpretation of the first link is that:

try
{
   throw X;
}
finally
{
   throw Y;
}

will end up throwing Y. My interpretation of the second link is that:

try (resource)
{
   throw X;
}
finally
{
   throw Y;
}

will end up throwing X.addSuppressed(Y). But according to our 
discussion, CompletableFuture ends up throwing X, which does not match 
the behavior of try-finally or try-with-resources. Am I missing anything?

Thanks,
Gili




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/More-Javadoc-problems-tp11669p11710.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/748b36f0/attachment.html>

From ben.manes at gmail.com  Sun Dec 21 18:02:37 2014
From: ben.manes at gmail.com (Benjamin Manes)
Date: Sun, 21 Dec 2014 15:02:37 -0800
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <54974E28.1090200@gmail.com>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>
	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>
	<5495949E.1030500@cs.oswego.edu>
	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>
	<5496D732.3010308@cs.oswego.edu>
	<CANPzfU9qoJyUgPwK3RtPeca6n9Vif6kq1yg98QbVByw58sn3aA@mail.gmail.com>
	<5496EB91.9090504@cs.oswego.edu>
	<CANPzfU-w0piYhehLAG9bJh5W4K+U2rA2EkymvwHWsksLO5t4sQ@mail.gmail.com>
	<CAGu0=MNxmOhgtC_mX2MmMZsc6C64fC3eiiY7AaX4v-amk3P6Hw@mail.gmail.com>
	<54973337.30701@gmail.com>
	<CAGu0=MMkdBe8hjs=wJyK+TS_cj6kQwCiw=Ao9PfQVN5Cqvew=w@mail.gmail.com>
	<54974E28.1090200@gmail.com>
Message-ID: <CAGu0=MNBcaGA9Zs5p-G2e=k2BBtf5BvB2bHk5UWr31CUwc3rSg@mail.gmail.com>

>
> A "marker" node inserted just after 1st node (or before 1st node?) in
> bucket and later removed. This would mean three additional volatile writes
> though (next field is volatile). Not to mention what to do when nodes are
> forming a tree (yes, that's possible).


Yes, but since writes are done in a synchronized block, the extra writes
could piggyback their visibility on exiting that block.

But currently the skipping detection is designed by checking the hash field
> which is final.


Yes, and so this would incur an additional volatile read during writes only
if the hash and key checks passed, and a read per value node on full
traversal operations. The common case of lock free per-entry reads would be
unaffected.

On Sun, Dec 21, 2014 at 2:48 PM, Peter Levart <peter.levart at gmail.com>
wrote:

>
> On 12/21/2014 10:20 PM, Benjamin Manes wrote:
>
> Thanks Peter,
>
>  I don't think compute() changes my suggestion. If the bucket count is
> larger than one, then I was suggesting inserting a regular Node with a null
> value that is updated when the mapping function completes successfully.
>
>
> You mean "a Node that is *removed*" when computation ends. Yes, as I said,
> that is a possibility. A "marker" node inserted just after 1st node (or
> before 1st node?) in bucket and later removed. This would mean three
> additional volatile writes though (next field is volatile). Not to mention
> what to do when nodes are forming a tree (yes, that's possible).
>
>  The node is scavenged when the computation fails, but since we know the
> instance that extra scan should be cheap. By updating a regular node I had
> hoped one could avoid garbage in the common case.
>
>
> If common case was inserting, then perhaps, yes. But currently the
> skipping detection is designed by checking the hash field which is final.
>
>
>
>  Experimenting to determine the feasibility of this type of change is too
> invasive for me to feel comfortable trying given the interactions with all
> the other map operations that I may not fully consider at the moment. Since
> ReservationNode is a special type, it seems likely that there are scenarios
> that I'm not considering.
>
>
> A change using "marker" nodes would be very invasive. Each operation
> itself traverses the node chain or tree and either inserts, modifies or
> removes a node (possibly the 1st node of the bucket). Adding insertion of
> "marker" node and later removal would complicate the
> traversal/insertion/removal logic of each individual operation with user
> call-backs in it's own unique way. I don't know if such change is warranted
> just to get better diagnostics.
>
> Regards, Peter
>
>
> On Sun, Dec 21, 2014 at 12:53 PM, Peter Levart <peter.levart at gmail.com>
> wrote:
>
>>  Hi Benjamin,
>>
>> Doug is certainly the one to give you the best explanation, but let me
>> try...
>>
>> On 12/21/2014 08:36 PM, Benjamin Manes wrote:
>>
>>  This is worth considering, but it only detects one form of
>>> misbehavior when the bin doesn't hold any other nodes.
>>> Otherwise, the first node need not be a ReservationNode.
>>
>>
>> I'm not comfortable enough with the code to experiment, but reading it I
>> keep wondering if inserting a node prior to computation would provide
>> detection. The value would be null, to be filled in if the computation
>> succeeds. If a reentrant computation occurs then when the existing node is
>> found and we observe that the value is unset, then we have detected a cycle
>> since we already hold the necessary lock. This would require skipping those
>> nodes for methods that traverse the entire map, like iterators and size().
>>
>>
>> ...ReservationNode is such node. But is only inserted (with CAS) as 1st
>> node of the bucket when the bucket is still empty. This is to provide the
>> object to use as the lock when changing anything in the bucket. It is
>> replaced with normal Node when the function's result is obtained. Logically
>> it is skipped on look-ups and traversals. If a node is inserted into the
>> bucket that already contains nodes, no ReservationNode is used.
>>
>> And it's not only inserting (computeIfAbsent) that calls-back user code.
>> See also compute() method. This method overwrites the old value with new
>> value returned from function in existing node or even removes a Node from
>> the bucket (if function returns null). How would you detect that such
>> process is taking place when you re-enter?
>>
>> I thought about it for some time, but didn't find any existing state
>> combination that could be used for such checks. Perhaps a ReservationNode
>> could be inserted just after 1st node in bucket temporarily just to mark
>> the fact that we entered synchronization block and later remove it before
>> exiting the block. This would most probably work, but would also mean that
>> we produce one object of garbage for each such call. Maybe this is better
>> than 4 bytes of state overhead for each node (see my experiment).
>>
>>
>>  Internally, this would make computations feel more like the JCiP-style
>> future based approach Viktor mentioned and allow for supporting batch
>> computations. It has the nice property of not wasting more memory with
>> values wrapped in futures, unless an asynchronous cache was explicitly
>> requested. That can be built on-top of CHM, as was always possible, and an
>> API option that I plan on providing.
>>
>>
>> Don't forget that CHM provides a lock-free lookup. We would not like to
>> loose this feature. Lazy evaluation (or waiting on computation) can be
>> added on top of CHM if one needs it.
>>
>> Regards, Peter
>>
>>
>>  The remaining mysterious cases are nested computeIfAbsent
>>> calls for different keys that happen to hash to the same bin.
>>> These conceptually ought to deadlock. Instead they will
>>> sometimes happen to "work" but other times loop.
>>
>>
>>  If changes like the above removed the live lock issue, then I'd be much
>> happier with a resulting deadlock remain unsolved. That's much easier for
>> developers to notice with jdk tooling when debugging server issues and
>> recognize the problem as their fault.
>>
>>  On Sun, Dec 21, 2014 at 10:07 AM, Viktor Klang <viktor.klang at gmail.com>
>> wrote:
>>
>>>
>>>
>>> On Sun, Dec 21, 2014 at 4:47 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>
>>>> On 12/21/2014 09:59 AM, Viktor Klang wrote:
>>>>
>>>>> For "computeIfAbsent"-style maps which are intended for multithreaded
>>>>> use, I
>>>>> tend to prefer to use ConcurrentMap[Key, CompletionStage[Value]],
>>>>>
>>>>
>>>>  Right. Prior to jdk8, we suggested variations of this and other
>>>> techniques that remain preferable in many cases. However, the
>>>> most common CHM user error was trying (but failing) to emulate
>>>> what is now supported as computeIfAbsent. This is an improvement,
>>>> but leads to new (much less common) kinds of potential errors.
>>>> We cannot automatically eliminate or trap all of them. It
>>>> might be possible to do more than we do now, either internally
>>>> (inside CHM) or, more likely, with improved developer tools.
>>>
>>>
>>>  Absolutely, and the fact that there was only blocking Futures in the
>>> JDK at that time.
>>> Having "monadic" Futures like CompletableFuture means you can avoid
>>> blocking completely (assuming that the CHM impl is non-blocking). This is
>>> highly useful when using it as a cache, and especially if there's a very
>>> skewed read distribution, paired with a multitude of readers.
>>>
>>>
>>>>
>>>>
>>>> -Doug
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>
>>>  --
>>>   Cheers,
>>> ?
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/c5ec7e51/attachment-0001.html>

From joe.bowbeer at gmail.com  Sun Dec 21 19:25:48 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 21 Dec 2014 16:25:48 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <549737AB.8060508@gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
	<5496B0FE.7070502@gmail.com>
	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
	<549737AB.8060508@gmail.com>
Message-ID: <CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>

Below is the most concise implementation I can imagine, using a single
Semaphore, which is legal AFAICT according to the javadoc, but which
deadlocks in my tests.

class OrderedExecutor<T> {

  private final Semaphore semaphore = new Semaphore(1);

  @Override
  public T execCallableInOrder(int order, Callable<T> callable)
      throws InterruptedException, Exception {
    assert order >= 0;
    int acquires = order + 1;
    semaphore.acquire(acquires);
    try {
      return callable.call();
    } finally {
      semaphore.release(acquires + 1);
    }
  }
}


testOrderedExecutorSynchronizer
Waiting 2
Waiting 1
Waiting 8
Waiting 7
Waiting 9
Waiting 0
Executing 0
Executing 1
Executing 2
Waiting 6
Waiting 4
Waiting 3
Waiting 5
Executing 3
... crickets ...


A bug?

On Sun, Dec 21, 2014 at 1:12 PM, Peter Levart <peter.levart at gmail.com>
wrote:
>
>
> On 12/21/2014 07:03 PM, Hanson Char wrote:
>
>  Interesting - now that each order is associated with a different object
> monitor, the wait/notifyAll would at most wake up one thread instead of
> all.  In fact, it seems the use of notify (instead of notifyAll) would
> suffice.  Simple and clever, but it does require the "stripeSize" to be
> known a priori.
>
>
> Not exactly. Each object monitor is associated with multiple orders. For
> example, if stripeSize is 4, the 1st object monitor is associated with
> orders: 0, 4, 8, 12, 16, ... the 2nd with orders 1, 5, 9, 13, 17, ... etc.
>
> But it also means that each object monitor is associated statistically
> with just N/stripeSize threads that wait instead of N. So notifyAll is
> still needed, but it wakes up less threads - how much less depends on
> chosen stripeSize.
>
> Regards, Peter
>
>
>  Regards,
> Hanson
>
> On Sun, Dec 21, 2014 at 3:37 AM, Peter Levart <peter.levart at gmail.com>
> wrote:
>
>>  Hi,
>>
>> Here's a simple implementation (based on Suman Shil's idea) that stripes
>> the waiting threads into multiple buckets:
>>
>> public class StripedOrderedExecutor {
>>
>>     private final MeetingPoint[] meetingPoints;
>>
>>     public StripedOrderedExecutor(int stripeSize) {
>>         assert stripeSize > 0;
>>         meetingPoints = new MeetingPoint[stripeSize];
>>         for (int i = 0; i < stripeSize; i++) {
>>             meetingPoints[i] = new MeetingPoint();
>>         }
>>     }
>>
>>     public <T> T execCriticalSectionInOrder(
>>         final int order,
>>         final Supplier<T> criticalSection
>>     ) throws InterruptedException {
>>         assert order >= 0;
>>
>>         meetingPoints[order % meetingPoints.length].waitForGreen(order);
>>         try {
>>             return criticalSection.get();
>>         } finally {
>>             meetingPoints[(order + 1) %
>> meetingPoints.length].notifyGreen(order + 1);
>>         }
>>     }
>>
>>     private static class MeetingPoint {
>>         private int lastGreen;
>>
>>         synchronized void waitForGreen(int order) throws
>> InterruptedException {
>>             while (lastGreen != order) {
>>                 wait();
>>             }
>>         }
>>
>>         synchronized void notifyGreen(int order) {
>>             lastGreen = order;
>>             notifyAll();
>>         }
>>     }
>> }
>>
>>
>> Regards, Peter
>>
>>
>>
>>
>>
>> On 12/21/2014 02:35 AM, Hanson Char wrote:
>>
>> Thanks, Joe. That's an interesting point about the long term memory
>> impact of TreeMap vs HashMap - similar to LinkedList vs ArrayList.
>>
>>  Regards,
>> Hanson
>>
>> On Sat, Dec 20, 2014 at 5:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>> wrote:
>>
>>> I see.  Yes, that addresses the removal problem, though note that
>>> HashMap does not shrink when data is removed. Even if all keys are removed
>>> from HashMap, the inner size of its table does not change.
>>>
>>>  So in this case maybe I would choose TreeMap as the implementation.
>>> Its remove() method *does* remove all the memory associated with an entry.
>>>
>>>  In general, I don't like Map<Integer, *> because it seems like a hack,
>>> but really in this case it's a small trade-off.  Map is more concise.
>>> PriorityQueue is a little clearer.
>>>
>>> On Sat, Dec 20, 2014 at 4:50 PM, Hanson Char <hanson.char at gmail.com>
>>> wrote:
>>>>
>>>> Hi Joe,
>>>>
>>>>  >A Queue should also retain less memory. A Map will tend to
>>>> accumulate both Entry and Integer instances over time.
>>>>
>>>>  Using a Map, the signalNext would look something like below.  Doesn't
>>>> each entry get eventually removed from the map?  Why would they accumulate
>>>> over time?
>>>>
>>>>  Regards,
>>>> Hanson
>>>>
>>>>      void signalNext(final int nextOrder) {
>>>>        lock.lock();
>>>>       try {
>>>>         this.nextOrder = nextOrder;
>>>>          Condition cond = map.remove(nextOrder);
>>>>         if (cond != null) {
>>>>             cond.signal();
>>>>         }
>>>>       } finally {
>>>>         lock.unlock();
>>>>       }
>>>>     }
>>>>   }
>>>>
>>>>
>>>> On Sat, Dec 20, 2014 at 4:39 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>> wrote:
>>>>
>>>>>  I think PriorityQueue more clearly represents the function than Map
>>>>> does in this case.
>>>>>
>>>>>  A Queue should also retain less memory. A Map will tend to
>>>>> accumulate both Entry and Integer instances over time.
>>>>>
>>>>>  The ConcurrentMap does have the advantage of lock striping, which
>>>>> might be more performant if there is high contention.
>>>>>
>>>>>  I think this is an interesting problem because there are quite a few
>>>>> different solutions, each with advantages and disadvantages in terms of
>>>>> performance and maintainability.
>>>>>
>>>>>  If there were a PriorityQueue version of AbstractQueuedSynchronizer,
>>>>> then we could have yet another solution using the AQS's state to represent
>>>>> the next available order.
>>>>>
>>>>> On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char <hanson.char at gmail.com>
>>>>> wrote:
>>>>>>
>>>>>> This one looks interesting and educational.  The use of condition
>>>>>> enables us to suspend and resume specifically targeted threads without
>>>>>> resorting to the use of LockSupport.{part, unpark}.  Instead of using a
>>>>>> PriorityQueue, perhaps we can use a simple Map<Integer, Condition> instead
>>>>>> so we can get away without the extra Waiter inner class?
>>>>>>
>>>>>>  Regards,
>>>>>> Hanson
>>>>>>
>>>>>> On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>>> To round out the selection, here's an implementation using a
>>>>>>> PriorityQueue of Conditions:
>>>>>>>
>>>>>>>  class OrderedExecutor<T> {
>>>>>>>
>>>>>>>    static class Waiter implements Comparable<Waiter> {
>>>>>>>
>>>>>>>      final int order;
>>>>>>>     final Condition condition;
>>>>>>>
>>>>>>>      Waiter(int order, Condition condition) {
>>>>>>>       this.order = order;
>>>>>>>       this.condition = condition;
>>>>>>>     }
>>>>>>>
>>>>>>>      @Override
>>>>>>>     public int compareTo(Waiter waiter) {
>>>>>>>       return order - waiter.order;
>>>>>>>     }
>>>>>>>   }
>>>>>>>
>>>>>>>    final Lock lock = new ReentrantLock();
>>>>>>>   final Queue<Waiter> queue = new PriorityQueue<>();
>>>>>>>   int nextOrder; // 0 is next
>>>>>>>
>>>>>>>    public T execCallableInOrder(int order, Callable<T> callable)
>>>>>>> throws Exception {
>>>>>>>     assert order >= 0;
>>>>>>>     awaitTurn(order);
>>>>>>>     try {
>>>>>>>       return callable.call();
>>>>>>>     } finally {
>>>>>>>       signalNext(order + 1);
>>>>>>>     }
>>>>>>>   }
>>>>>>>
>>>>>>>    void awaitTurn(int order) {
>>>>>>>     lock.lock();
>>>>>>>     try {
>>>>>>>       Condition condition = null;
>>>>>>>       while (nextOrder != order) {
>>>>>>>         if (condition == null) {
>>>>>>>           condition = lock.newCondition();
>>>>>>>           queue.add(new Waiter(order, condition));
>>>>>>>         }
>>>>>>>         condition.awaitUninterruptibly();
>>>>>>>       }
>>>>>>>     } finally {
>>>>>>>       lock.unlock();
>>>>>>>     }
>>>>>>>   }
>>>>>>>
>>>>>>>    void signalNext(int nextOrder) {
>>>>>>>     lock.lock();
>>>>>>>     try {
>>>>>>>       this.nextOrder = nextOrder;
>>>>>>>       Waiter waiter = queue.peek();
>>>>>>>       if (waiter != null && waiter.order == nextOrder) {
>>>>>>>         queue.remove();
>>>>>>>         waiter.condition.signal();
>>>>>>>       }
>>>>>>>     } finally {
>>>>>>>       lock.unlock();
>>>>>>>     }
>>>>>>>   }
>>>>>>> }
>>>>>>>
>>>>>>> On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer <joe.bowbeer at gmail.com
>>>>>>> > wrote:
>>>>>>>>
>>>>>>>> Suman,
>>>>>>>>
>>>>>>>>  I would advise against using notify/wait.  It raises a red flag
>>>>>>>> for a lot of reviewers, including me.
>>>>>>>>
>>>>>>>>  The problems I see in this implementation are:
>>>>>>>>
>>>>>>>>  1. Pre-allocation of locks is prohibited by (revised) problem
>>>>>>>> statement.
>>>>>>>>
>>>>>>>>  Note that if pre-allocation were allowed, then an array would be
>>>>>>>> more efficient than a Map.
>>>>>>>>
>>>>>>>>  2. Access to currentAllowedOrder is not thread-safe but should be.
>>>>>>>>
>>>>>>>>
>>>>>>>> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <suman_krec at yahoo.com>
>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>>  I have modified my solution to avoid notifyAll(). Let me know
>>>>>>>>> your feedback.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>  public class OrderedExecutor
>>>>>>>>> {
>>>>>>>>>  private int maxOrder;
>>>>>>>>>  private int currentAllowedOrder;
>>>>>>>>>  private Map<Integer, Object> map = new HashMap<Integer,
>>>>>>>>> Object>();
>>>>>>>>>     public OrderedExecutor(int n)
>>>>>>>>>     {
>>>>>>>>>          this.maxOrder = n;
>>>>>>>>>          this.currentAllowedOrder = 0;
>>>>>>>>>     for(int i = 0 ; i < this.maxOrder ; i++)
>>>>>>>>>     {
>>>>>>>>>     map.put(i, new Object());
>>>>>>>>>     }
>>>>>>>>>     }
>>>>>>>>>
>>>>>>>>>     public  Object execCriticalSectionInOrder(int order,
>>>>>>>>>                                                Callable<Object>
>>>>>>>>> callable)
>>>>>>>>>                                               throws Exception
>>>>>>>>>     {
>>>>>>>>>  if (order >= this.maxOrder)
>>>>>>>>>  {
>>>>>>>>>  throw new Exception("Exceeds maximum order "+ maxOrder);
>>>>>>>>>  }
>>>>>>>>>
>>>>>>>>>  while(order != currentAllowedOrder)
>>>>>>>>>  {
>>>>>>>>>  synchronized (this.map.get(order))
>>>>>>>>>  {
>>>>>>>>>  this.map.get(order).wait();
>>>>>>>>>  }
>>>>>>>>>  }
>>>>>>>>>   try
>>>>>>>>>  {
>>>>>>>>>  return callable.call();
>>>>>>>>>  }
>>>>>>>>>        finally
>>>>>>>>>  {
>>>>>>>>>             currentAllowedOrder = currentAllowedOrder+1;
>>>>>>>>>  synchronized (this.map.get(order+1))
>>>>>>>>>             {
>>>>>>>>>                 this.map.get(order+1).notify();
>>>>>>>>>             }
>>>>>>>>>         }
>>>>>>>>>     }
>>>>>>>>> }
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>   ------------------------------
>>>>>>>>>  *From:* Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>>>>> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
>>>>>>>>> *Sent:* Friday, December 19, 2014 3:33 AM
>>>>>>>>>
>>>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>> of critical sections?
>>>>>>>>>
>>>>>>>>>  That would be "Tom" Cargill; link to paper:
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>  On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <
>>>>>>>>> joe.bowbeer at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>> I frown on use of notify[All]/wait because they make the code hard
>>>>>>>>> to maintain.
>>>>>>>>>
>>>>>>>>>  In this case, with potentially lots of waiting threads, I would
>>>>>>>>> check out the "Specific Notification" pattern if I were determined to go
>>>>>>>>> the wait/notify route:
>>>>>>>>>
>>>>>>>>>  Tim Cargill's paper is dated but still worth reading.
>>>>>>>>>
>>>>>>>>>  Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ
>>>>>>>>> and Peter Haggar's article:
>>>>>>>>>
>>>>>>>>>  http://www.ibm.com/developerworks/java/library/j-spnotif.html
>>>>>>>>>
>>>>>>>>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
>>>>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>>>>
>>>>>>>>>  Yes, no one said it is a good idea to always do that. When it is
>>>>>>>>> contended, most of the threads will wake up to only go back to sleep.
>>>>>>>>>
>>>>>>>>> The pattern you are after is usually called sequencer. You can see
>>>>>>>>> it used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe
>>>>>>>>> not that popular.
>>>>>>>>>
>>>>>>>>> The best solution will be lock-like, but the waiter nodes will
>>>>>>>>> contain the value they are waiting for - so only the specific threads get
>>>>>>>>> woken up. The solution with concurrent map is very similar, only with
>>>>>>>>> larger overhead from storing the index the thread is waiting for.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Alex
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>>>>>>>
>>>>>>>>> Less overhead and simpler are a nice properties, even though at
>>>>>>>>> the expense of having to wake up all waiting threads just to find out the
>>>>>>>>> one with the right order to execute.  Still, this seems like a good
>>>>>>>>> tradeoff.
>>>>>>>>>
>>>>>>>>>  Thanks,
>>>>>>>>> Hanson
>>>>>>>>>
>>>>>>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <
>>>>>>>>> peter.levart at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>>>>>>>
>>>>>>>>> No, there is no difference. Peter didn't spot your entire method
>>>>>>>>> is synchronized, so spurious wakeup won't make progress until the owner of
>>>>>>>>> the lock exits the method.
>>>>>>>>>
>>>>>>>>> You could split the synchronization into two blocks - one
>>>>>>>>> encompassing the wait loop, the other in the finally block; but it may make
>>>>>>>>> no difference.
>>>>>>>>>
>>>>>>>>> Alex
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> You're right, Alex. I'm so infected with park/unpark virus that I
>>>>>>>>> missed that ;-)
>>>>>>>>>
>>>>>>>>> Peter
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On 17/12/2014 18:36, suman shil wrote:
>>>>>>>>>
>>>>>>>>> Thanks peter for your reply. You are right. I should have
>>>>>>>>> incremented currentAllowedOrder in finally block.
>>>>>>>>>
>>>>>>>>> Suman
>>>>>>>>>
>>>>>>>>> ------------------------------------------------------------------------
>>>>>>>>> *From:* Peter Levart <peter.levart at gmail.com>
>>>>>>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>>>>>>>>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>>>>>>>>> concurrency-interest at cs.oswego.edu>
>>>>>>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>> of critical sections?
>>>>>>>>>
>>>>>>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>>>>>>>
>>>>>>>>> Thanks for your response. Will notifyAll() instead of notify()
>>>>>>>>> solve the problem?
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> It will, but you should also account for "spurious" wake-ups. You
>>>>>>>>> should increment currentAllowedOrder only after return from callable.call
>>>>>>>>> (in finally block just before notifyAll()).
>>>>>>>>>
>>>>>>>>> Otherwise a nice solution - with minimal state, providing that not
>>>>>>>>> many threads meet at the same time...
>>>>>>>>>
>>>>>>>>> Regards, Peter
>>>>>>>>>
>>>>>>>>>  RegardsSuman
>>>>>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com>
>>>>>>>>> <mailto:oleksandr.otenko at oracle.com>
>>>>>>>>>   To: suman shil<suman_krec at yahoo.com> <mailto:
>>>>>>>>> suman_krec at yahoo.com>; Concurrency-interest<
>>>>>>>>> concurrency-interest at cs.oswego.edu> <mailto:
>>>>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December
>>>>>>>>> 17, 2014 9:55 PM
>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>> of critical sections?
>>>>>>>>>       There is no guarantee you'll ever hand over the control to
>>>>>>>>> the right thread upon notify()
>>>>>>>>>     Alex
>>>>>>>>>
>>>>>>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>>>>>>       Hi, Following is my solution to solve this problem. Please
>>>>>>>>> let me know if I am missing something.
>>>>>>>>>    public class OrderedExecutor {  private int currentAllowedOrder
>>>>>>>>> = 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {
>>>>>>>>> this.maxLength = n;  } public synchronized Object
>>>>>>>>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>>>>>>>>                        throws Exception  { if (order >= maxLength)  {
>>>>>>>>> throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order
>>>>>>>>> != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder =
>>>>>>>>> currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();
>>>>>>>>> }  } }
>>>>>>>>>    Regards Suman
>>>>>>>>>         From: Peter Levart<peter.levart at gmail.com> <mailto:
>>>>>>>>> peter.levart at gmail.com>
>>>>>>>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:
>>>>>>>>> hanson.char at gmail.com>    Cc: concurrency-interest<
>>>>>>>>> concurrency-interest at cs.oswego.edu> <mailto:
>>>>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14,
>>>>>>>>> 2014 11:01 PM
>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>> of critical sections?
>>>>>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>>>>>      Hi Peter,
>>>>>>>>>    Thanks for this proposed idea of using LockSupport. This begs
>>>>>>>>> the question: which one would you choose if you had all three (correct)
>>>>>>>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>>>>>>    Regards, Hanson
>>>>>>>>>     The Semaphore/CountDownLatch variants are equivalent if you
>>>>>>>>> don't need re-use. So any would do. They lack invalid-use detection. What
>>>>>>>>> happens if they are not used as intended? Semaphore variant acts
>>>>>>>>> differently than CountDownLatch variant. The low-level variant I  proposed
>>>>>>>>> detects invalid usage. So I would probably use this one. But the low level
>>>>>>>>> variant is harder to reason about it's correctness. I think it is correct,
>>>>>>>>> but you should show it to somebody else to confirm this.
>>>>>>>>>     Another question is whether you actually need this kind of
>>>>>>>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>>>>>>>> somebody could have an idea how to do that even more elegantly...
>>>>>>>>>     Regards, Peter
>>>>>>>>>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>>>>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>>>    Hi Hanson,
>>>>>>>>>     This one is more low-level, but catches some invalid usages
>>>>>>>>> and is more resource-friendly:
>>>>>>>>>       public class OrderedExecutor {
>>>>>>>>>         public <T> T execCriticalSectionInOrder(
>>>>>>>>>           final int order,
>>>>>>>>>           final Supplier<T> criticalSection
>>>>>>>>>       ) throws InterruptedException {
>>>>>>>>>           if (order < 0) {
>>>>>>>>>                throw new IllegalArgumentException("'order' should
>>>>>>>>> be >= 0");
>>>>>>>>>           }
>>>>>>>>>           if (order > 0) {
>>>>>>>>>               waitForDone(order - 1);
>>>>>>>>>           }
>>>>>>>>>           try {
>>>>>>>>>               return criticalSection.get();
>>>>>>>>>           } finally {
>>>>>>>>>               notifyDone(order);
>>>>>>>>>           }
>>>>>>>>>       }
>>>>>>>>>         private static final Object DONE = new Object();
>>>>>>>>>       private final ConcurrentMap<Integer, Object> signals = new
>>>>>>>>> ConcurrentHashMap<>();
>>>>>>>>>         private void waitForDone(int order) throws
>>>>>>>>> InterruptedException {
>>>>>>>>>           Object sig = signals.putIfAbsent(order,
>>>>>>>>> Thread.currentThread());
>>>>>>>>>           if (sig != null && sig != DONE) {
>>>>>>>>>               throw new IllegalStateException();
>>>>>>>>>           }
>>>>>>>>>           while (sig != DONE) {
>>>>>>>>>               LockSupport.park();
>>>>>>>>>               if (Thread.interrupted()) {
>>>>>>>>>                   throw new InterruptedException();
>>>>>>>>>               }
>>>>>>>>>               sig = signals.get(order);
>>>>>>>>>           }
>>>>>>>>>       }
>>>>>>>>>         private void notifyDone(int order) {
>>>>>>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>>>>>>           if (sig instanceof Thread) {
>>>>>>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>>>>>>                   throw new IllegalStateException();
>>>>>>>>>               }
>>>>>>>>>               LockSupport.unpark((Thread) sig);
>>>>>>>>>           } else if (sig != null) {
>>>>>>>>>               throw new IllegalStateException();
>>>>>>>>>           }
>>>>>>>>>       }
>>>>>>>>>   }
>>>>>>>>>       Regards, Peter
>>>>>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>>>>>      Hi Peter,
>>>>>>>>>    Thanks for the suggestion, and sorry about not being clear
>>>>>>>>> about one important  detail: "n" is not known a priori when constructing an
>>>>>>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>>>>>>     If you know at least the upper bound of 'n', it can be used
>>>>>>>>> with such 'n'. Otherwise something that dynamically re-sizes the array
>>>>>>>>> could be devised. Or you could simply use a ConcurrentHashMap instead of
>>>>>>>>> array where keys are 'order' values:
>>>>>>>>>       public class OrderedExecutor<T> {
>>>>>>>>>         private final ConcurrentMap<Integer, CountDownLatch>
>>>>>>>>> latches = new ConcurrentHashMap<>();
>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>>>                                           final Supplier<T>
>>>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>>>           if (order > 0) {
>>>>>>>>>               latches.computeIfAbsent(order - 1, o -> new
>>>>>>>>> CountDownLatch(1)).await();
>>>>>>>>>           }
>>>>>>>>>           try {
>>>>>>>>>               return criticalSection.get();
>>>>>>>>>           } finally {
>>>>>>>>>               latches.computeIfAbsent(order, o -> new
>>>>>>>>> CountDownLatch(1)).countDown();
>>>>>>>>>           }
>>>>>>>>>       }
>>>>>>>>>   }
>>>>>>>>>       Regards, Peter
>>>>>>>>>           You guessed right: it's a one-shot object for a
>>>>>>>>> particular OrderedExecutor  instance, and "order" must be called indeed at
>>>>>>>>> most once.
>>>>>>>>>    Regards, Hanson
>>>>>>>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<
>>>>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>>>    Hi Hanson,
>>>>>>>>>     I don't think anything like that readily exists  in
>>>>>>>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>>>>>>>> with composition of existing primitives.  You haven't given any additional
>>>>>>>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>>>>>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>>>>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>>>>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>>>>>>> (and I think that only a one-shot object  makes sense here), an array of
>>>>>>>>> CountDownLatch(es) could be used:
>>>>>>>>>     public class OrderedExecutor<T> {
>>>>>>>>>       private final CountDownLatch[] latches;
>>>>>>>>>         public OrderedExecutor(int n) {
>>>>>>>>>           if (n < 1) throw new IllegalArgumentException("'n'
>>>>>>>>> should be >= 1");
>>>>>>>>>           latches = new CountDownLatch[n - 1];
>>>>>>>>>           for (int i = 0; i < latches.length; i++) {
>>>>>>>>>               latches[i] = new CountDownLatch(1);
>>>>>>>>>           }
>>>>>>>>>       }
>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>>>                                            final Supplier<T>
>>>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>>>           if (order < 0 || order > latches.length)
>>>>>>>>>               throw new IllegalArgumentException("'order' should
>>>>>>>>> be [0..." +  latches.length + "]");
>>>>>>>>>           if (order > 0) {
>>>>>>>>>               latches[order - 1].await();
>>>>>>>>>           }
>>>>>>>>>           try {
>>>>>>>>>               return criticalSection.get();
>>>>>>>>>           } finally {
>>>>>>>>>               if (order < latches.length) {
>>>>>>>>>                   latches[order].countDown();
>>>>>>>>>               }
>>>>>>>>>           }
>>>>>>>>>       }
>>>>>>>>>   }
>>>>>>>>>       Regards, Peter
>>>>>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>>>>>          Hi, I am looking for a construct that can  be used to
>>>>>>>>> efficiently enforce  ordered execution of multiple critical sections, each
>>>>>>>>> calling from a  different thread. The calling threads may run in  parallel
>>>>>>>>> and may call the execution method out of order. The  perceived construct
>>>>>>>>> would therefore be responsible for re-ordering the execution of those
>>>>>>>>> threads, so that their critical  sections (and only the critical section)
>>>>>>>>> will be executed in order. Would something  like the following API already
>>>>>>>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>>>>>>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>>>>>>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>>>>>>>> most once with the given order, parking * and  unparking the current thread
>>>>>>>>> as  necessary so that all critical * sections executed  by different
>>>>>>>>> threads using this  executor take place in * the order from 1 to n
>>>>>>>>> consecutively. */ public T execCriticalSectionInOrder
>>>>>>>>> (  final int order, final Callable<T> criticalSection) throws
>>>>>>>>> InterruptedException; } Regards, Hanson
>>>>>>>>> _______________________________________________Concurrency-interest mailing
>>>>>>>>> listConcurrency-interest at cs.oswego.edu <mailto:
>>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>> _______________________________________________
>>>>>>>>>   Concurrency-interest mailing list
>>>>>>>>>   Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/5ab5b25b/attachment-0001.html>

From hanson.char at gmail.com  Sun Dec 21 20:39:52 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 21 Dec 2014 17:39:52 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
	<5496B0FE.7070502@gmail.com>
	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
	<549737AB.8060508@gmail.com>
	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
Message-ID: <CABWgujYoFV=F4g+Z1PG+qc1DDhK9CO3i26UjkdAtw-oheyUE3g@mail.gmail.com>

Hi Joe,

Looks like a bug with Semaphore to me too. Even though a sufficient number
of permits are released for one of the threads already waiting, the waiting
thread still cannot make progress.  Otherwise, I think this is the niftiest
solution so far.  Sad it doesn't work out.

Regards,
Hanson

On Sun, Dec 21, 2014 at 4:25 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> Below is the most concise implementation I can imagine, using a single
> Semaphore, which is legal AFAICT according to the javadoc, but which
> deadlocks in my tests.
>
> class OrderedExecutor<T> {
>
>   private final Semaphore semaphore = new Semaphore(1);
>
>   @Override
>   public T execCallableInOrder(int order, Callable<T> callable)
>       throws InterruptedException, Exception {
>     assert order >= 0;
>     int acquires = order + 1;
>     semaphore.acquire(acquires);
>     try {
>       return callable.call();
>     } finally {
>       semaphore.release(acquires + 1);
>     }
>   }
> }
>
>
> testOrderedExecutorSynchronizer
> Waiting 2
> Waiting 1
> Waiting 8
> Waiting 7
> Waiting 9
> Waiting 0
> Executing 0
> Executing 1
> Executing 2
> Waiting 6
> Waiting 4
> Waiting 3
> Waiting 5
> Executing 3
> ... crickets ...
>
>
> A bug?
>
> On Sun, Dec 21, 2014 at 1:12 PM, Peter Levart <peter.levart at gmail.com>
> wrote:
>>
>>
>> On 12/21/2014 07:03 PM, Hanson Char wrote:
>>
>>  Interesting - now that each order is associated with a different object
>> monitor, the wait/notifyAll would at most wake up one thread instead of
>> all.  In fact, it seems the use of notify (instead of notifyAll) would
>> suffice.  Simple and clever, but it does require the "stripeSize" to be
>> known a priori.
>>
>>
>> Not exactly. Each object monitor is associated with multiple orders. For
>> example, if stripeSize is 4, the 1st object monitor is associated with
>> orders: 0, 4, 8, 12, 16, ... the 2nd with orders 1, 5, 9, 13, 17, ... etc.
>>
>> But it also means that each object monitor is associated statistically
>> with just N/stripeSize threads that wait instead of N. So notifyAll is
>> still needed, but it wakes up less threads - how much less depends on
>> chosen stripeSize.
>>
>> Regards, Peter
>>
>>
>>  Regards,
>> Hanson
>>
>> On Sun, Dec 21, 2014 at 3:37 AM, Peter Levart <peter.levart at gmail.com>
>> wrote:
>>
>>>  Hi,
>>>
>>> Here's a simple implementation (based on Suman Shil's idea) that stripes
>>> the waiting threads into multiple buckets:
>>>
>>> public class StripedOrderedExecutor {
>>>
>>>     private final MeetingPoint[] meetingPoints;
>>>
>>>     public StripedOrderedExecutor(int stripeSize) {
>>>         assert stripeSize > 0;
>>>         meetingPoints = new MeetingPoint[stripeSize];
>>>         for (int i = 0; i < stripeSize; i++) {
>>>             meetingPoints[i] = new MeetingPoint();
>>>         }
>>>     }
>>>
>>>     public <T> T execCriticalSectionInOrder(
>>>         final int order,
>>>         final Supplier<T> criticalSection
>>>     ) throws InterruptedException {
>>>         assert order >= 0;
>>>
>>>         meetingPoints[order % meetingPoints.length].waitForGreen(order);
>>>         try {
>>>             return criticalSection.get();
>>>         } finally {
>>>             meetingPoints[(order + 1) %
>>> meetingPoints.length].notifyGreen(order + 1);
>>>         }
>>>     }
>>>
>>>     private static class MeetingPoint {
>>>         private int lastGreen;
>>>
>>>         synchronized void waitForGreen(int order) throws
>>> InterruptedException {
>>>             while (lastGreen != order) {
>>>                 wait();
>>>             }
>>>         }
>>>
>>>         synchronized void notifyGreen(int order) {
>>>             lastGreen = order;
>>>             notifyAll();
>>>         }
>>>     }
>>> }
>>>
>>>
>>> Regards, Peter
>>>
>>>
>>>
>>>
>>>
>>> On 12/21/2014 02:35 AM, Hanson Char wrote:
>>>
>>> Thanks, Joe. That's an interesting point about the long term memory
>>> impact of TreeMap vs HashMap - similar to LinkedList vs ArrayList.
>>>
>>>  Regards,
>>> Hanson
>>>
>>> On Sat, Dec 20, 2014 at 5:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>> wrote:
>>>
>>>> I see.  Yes, that addresses the removal problem, though note that
>>>> HashMap does not shrink when data is removed. Even if all keys are removed
>>>> from HashMap, the inner size of its table does not change.
>>>>
>>>>  So in this case maybe I would choose TreeMap as the implementation.
>>>> Its remove() method *does* remove all the memory associated with an entry.
>>>>
>>>>  In general, I don't like Map<Integer, *> because it seems like a
>>>> hack, but really in this case it's a small trade-off.  Map is more concise.
>>>> PriorityQueue is a little clearer.
>>>>
>>>> On Sat, Dec 20, 2014 at 4:50 PM, Hanson Char <hanson.char at gmail.com>
>>>> wrote:
>>>>>
>>>>> Hi Joe,
>>>>>
>>>>>  >A Queue should also retain less memory. A Map will tend to
>>>>> accumulate both Entry and Integer instances over time.
>>>>>
>>>>>  Using a Map, the signalNext would look something like below.
>>>>> Doesn't each entry get eventually removed from the map?  Why would they
>>>>> accumulate over time?
>>>>>
>>>>>  Regards,
>>>>> Hanson
>>>>>
>>>>>      void signalNext(final int nextOrder) {
>>>>>        lock.lock();
>>>>>       try {
>>>>>         this.nextOrder = nextOrder;
>>>>>          Condition cond = map.remove(nextOrder);
>>>>>         if (cond != null) {
>>>>>             cond.signal();
>>>>>         }
>>>>>       } finally {
>>>>>         lock.unlock();
>>>>>       }
>>>>>     }
>>>>>   }
>>>>>
>>>>>
>>>>> On Sat, Dec 20, 2014 at 4:39 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>> wrote:
>>>>>
>>>>>>  I think PriorityQueue more clearly represents the function than Map
>>>>>> does in this case.
>>>>>>
>>>>>>  A Queue should also retain less memory. A Map will tend to
>>>>>> accumulate both Entry and Integer instances over time.
>>>>>>
>>>>>>  The ConcurrentMap does have the advantage of lock striping, which
>>>>>> might be more performant if there is high contention.
>>>>>>
>>>>>>  I think this is an interesting problem because there are quite a
>>>>>> few different solutions, each with advantages and disadvantages in terms of
>>>>>> performance and maintainability.
>>>>>>
>>>>>>  If there were a PriorityQueue version of
>>>>>> AbstractQueuedSynchronizer, then we could have yet another solution using
>>>>>> the AQS's state to represent the next available order.
>>>>>>
>>>>>> On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char <hanson.char at gmail.com>
>>>>>> wrote:
>>>>>>>
>>>>>>> This one looks interesting and educational.  The use of condition
>>>>>>> enables us to suspend and resume specifically targeted threads without
>>>>>>> resorting to the use of LockSupport.{part, unpark}.  Instead of using a
>>>>>>> PriorityQueue, perhaps we can use a simple Map<Integer, Condition> instead
>>>>>>> so we can get away without the extra Waiter inner class?
>>>>>>>
>>>>>>>  Regards,
>>>>>>> Hanson
>>>>>>>
>>>>>>> On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>>> To round out the selection, here's an implementation using a
>>>>>>>> PriorityQueue of Conditions:
>>>>>>>>
>>>>>>>>  class OrderedExecutor<T> {
>>>>>>>>
>>>>>>>>    static class Waiter implements Comparable<Waiter> {
>>>>>>>>
>>>>>>>>      final int order;
>>>>>>>>     final Condition condition;
>>>>>>>>
>>>>>>>>      Waiter(int order, Condition condition) {
>>>>>>>>       this.order = order;
>>>>>>>>       this.condition = condition;
>>>>>>>>     }
>>>>>>>>
>>>>>>>>      @Override
>>>>>>>>     public int compareTo(Waiter waiter) {
>>>>>>>>       return order - waiter.order;
>>>>>>>>     }
>>>>>>>>   }
>>>>>>>>
>>>>>>>>    final Lock lock = new ReentrantLock();
>>>>>>>>   final Queue<Waiter> queue = new PriorityQueue<>();
>>>>>>>>   int nextOrder; // 0 is next
>>>>>>>>
>>>>>>>>    public T execCallableInOrder(int order, Callable<T> callable)
>>>>>>>> throws Exception {
>>>>>>>>     assert order >= 0;
>>>>>>>>     awaitTurn(order);
>>>>>>>>     try {
>>>>>>>>       return callable.call();
>>>>>>>>     } finally {
>>>>>>>>       signalNext(order + 1);
>>>>>>>>     }
>>>>>>>>   }
>>>>>>>>
>>>>>>>>    void awaitTurn(int order) {
>>>>>>>>     lock.lock();
>>>>>>>>     try {
>>>>>>>>       Condition condition = null;
>>>>>>>>       while (nextOrder != order) {
>>>>>>>>         if (condition == null) {
>>>>>>>>           condition = lock.newCondition();
>>>>>>>>           queue.add(new Waiter(order, condition));
>>>>>>>>         }
>>>>>>>>         condition.awaitUninterruptibly();
>>>>>>>>       }
>>>>>>>>     } finally {
>>>>>>>>       lock.unlock();
>>>>>>>>     }
>>>>>>>>   }
>>>>>>>>
>>>>>>>>    void signalNext(int nextOrder) {
>>>>>>>>     lock.lock();
>>>>>>>>     try {
>>>>>>>>       this.nextOrder = nextOrder;
>>>>>>>>       Waiter waiter = queue.peek();
>>>>>>>>       if (waiter != null && waiter.order == nextOrder) {
>>>>>>>>         queue.remove();
>>>>>>>>         waiter.condition.signal();
>>>>>>>>       }
>>>>>>>>     } finally {
>>>>>>>>       lock.unlock();
>>>>>>>>     }
>>>>>>>>   }
>>>>>>>> }
>>>>>>>>
>>>>>>>> On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer <
>>>>>>>> joe.bowbeer at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>> Suman,
>>>>>>>>>
>>>>>>>>>  I would advise against using notify/wait.  It raises a red flag
>>>>>>>>> for a lot of reviewers, including me.
>>>>>>>>>
>>>>>>>>>  The problems I see in this implementation are:
>>>>>>>>>
>>>>>>>>>  1. Pre-allocation of locks is prohibited by (revised) problem
>>>>>>>>> statement.
>>>>>>>>>
>>>>>>>>>  Note that if pre-allocation were allowed, then an array would be
>>>>>>>>> more efficient than a Map.
>>>>>>>>>
>>>>>>>>>  2. Access to currentAllowedOrder is not thread-safe but should
>>>>>>>>> be.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <suman_krec at yahoo.com>
>>>>>>>>> wrote:
>>>>>>>>>>
>>>>>>>>>>  I have modified my solution to avoid notifyAll(). Let me know
>>>>>>>>>> your feedback.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>  public class OrderedExecutor
>>>>>>>>>> {
>>>>>>>>>>  private int maxOrder;
>>>>>>>>>>  private int currentAllowedOrder;
>>>>>>>>>>  private Map<Integer, Object> map = new HashMap<Integer,
>>>>>>>>>> Object>();
>>>>>>>>>>     public OrderedExecutor(int n)
>>>>>>>>>>     {
>>>>>>>>>>          this.maxOrder = n;
>>>>>>>>>>          this.currentAllowedOrder = 0;
>>>>>>>>>>     for(int i = 0 ; i < this.maxOrder ; i++)
>>>>>>>>>>     {
>>>>>>>>>>     map.put(i, new Object());
>>>>>>>>>>     }
>>>>>>>>>>     }
>>>>>>>>>>
>>>>>>>>>>     public  Object execCriticalSectionInOrder(int order,
>>>>>>>>>>                                                Callable<Object>
>>>>>>>>>> callable)
>>>>>>>>>>                                               throws Exception
>>>>>>>>>>     {
>>>>>>>>>>  if (order >= this.maxOrder)
>>>>>>>>>>  {
>>>>>>>>>>  throw new Exception("Exceeds maximum order "+ maxOrder);
>>>>>>>>>>  }
>>>>>>>>>>
>>>>>>>>>>  while(order != currentAllowedOrder)
>>>>>>>>>>  {
>>>>>>>>>>  synchronized (this.map.get(order))
>>>>>>>>>>  {
>>>>>>>>>>  this.map.get(order).wait();
>>>>>>>>>>  }
>>>>>>>>>>  }
>>>>>>>>>>   try
>>>>>>>>>>  {
>>>>>>>>>>  return callable.call();
>>>>>>>>>>  }
>>>>>>>>>>        finally
>>>>>>>>>>  {
>>>>>>>>>>             currentAllowedOrder = currentAllowedOrder+1;
>>>>>>>>>>  synchronized (this.map.get(order+1))
>>>>>>>>>>             {
>>>>>>>>>>                 this.map.get(order+1).notify();
>>>>>>>>>>             }
>>>>>>>>>>         }
>>>>>>>>>>     }
>>>>>>>>>> }
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>   ------------------------------
>>>>>>>>>>  *From:* Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>>>>>> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
>>>>>>>>>> *Sent:* Friday, December 19, 2014 3:33 AM
>>>>>>>>>>
>>>>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered
>>>>>>>>>> execution of critical sections?
>>>>>>>>>>
>>>>>>>>>>  That would be "Tom" Cargill; link to paper:
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>  On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer <
>>>>>>>>>> joe.bowbeer at gmail.com> wrote:
>>>>>>>>>>
>>>>>>>>>> I frown on use of notify[All]/wait because they make the code
>>>>>>>>>> hard to maintain.
>>>>>>>>>>
>>>>>>>>>>  In this case, with potentially lots of waiting threads, I would
>>>>>>>>>> check out the "Specific Notification" pattern if I were determined to go
>>>>>>>>>> the wait/notify route:
>>>>>>>>>>
>>>>>>>>>>  Tim Cargill's paper is dated but still worth reading.
>>>>>>>>>>
>>>>>>>>>>  Also see chapter 3.7.3 Specific Notifications in Doug Lea's
>>>>>>>>>> CPiJ and Peter Haggar's article:
>>>>>>>>>>
>>>>>>>>>>  http://www.ibm.com/developerworks/java/library/j-spnotif.html
>>>>>>>>>>
>>>>>>>>>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko <
>>>>>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>>>>>
>>>>>>>>>>  Yes, no one said it is a good idea to always do that. When it is
>>>>>>>>>> contended, most of the threads will wake up to only go back to sleep.
>>>>>>>>>>
>>>>>>>>>> The pattern you are after is usually called sequencer. You can
>>>>>>>>>> see it used in TCP. I am not sure why it wasn't implemented in j.u.c. -
>>>>>>>>>> maybe not that popular.
>>>>>>>>>>
>>>>>>>>>> The best solution will be lock-like, but the waiter nodes will
>>>>>>>>>> contain the value they are waiting for - so only the specific threads get
>>>>>>>>>> woken up. The solution with concurrent map is very similar, only with
>>>>>>>>>> larger overhead from storing the index the thread is waiting for.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> Alex
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>>>>>>>>
>>>>>>>>>> Less overhead and simpler are a nice properties, even though at
>>>>>>>>>> the expense of having to wake up all waiting threads just to find out the
>>>>>>>>>> one with the right order to execute.  Still, this seems like a good
>>>>>>>>>> tradeoff.
>>>>>>>>>>
>>>>>>>>>>  Thanks,
>>>>>>>>>> Hanson
>>>>>>>>>>
>>>>>>>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <
>>>>>>>>>> peter.levart at gmail.com> wrote:
>>>>>>>>>>
>>>>>>>>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>>>>>>>>
>>>>>>>>>> No, there is no difference. Peter didn't spot your entire method
>>>>>>>>>> is synchronized, so spurious wakeup won't make progress until the owner of
>>>>>>>>>> the lock exits the method.
>>>>>>>>>>
>>>>>>>>>> You could split the synchronization into two blocks - one
>>>>>>>>>> encompassing the wait loop, the other in the finally block; but it may make
>>>>>>>>>> no difference.
>>>>>>>>>>
>>>>>>>>>> Alex
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> You're right, Alex. I'm so infected with park/unpark virus that I
>>>>>>>>>> missed that ;-)
>>>>>>>>>>
>>>>>>>>>> Peter
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On 17/12/2014 18:36, suman shil wrote:
>>>>>>>>>>
>>>>>>>>>> Thanks peter for your reply. You are right. I should have
>>>>>>>>>> incremented currentAllowedOrder in finally block.
>>>>>>>>>>
>>>>>>>>>> Suman
>>>>>>>>>>
>>>>>>>>>> ------------------------------------------------------------------------
>>>>>>>>>> *From:* Peter Levart <peter.levart at gmail.com>
>>>>>>>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <
>>>>>>>>>> oleksandr.otenko at oracle.com>; Concurrency-interest <
>>>>>>>>>> concurrency-interest at cs.oswego.edu>
>>>>>>>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>>> of critical sections?
>>>>>>>>>>
>>>>>>>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>>>>>>>>
>>>>>>>>>> Thanks for your response. Will notifyAll() instead of notify()
>>>>>>>>>> solve the problem?
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> It will, but you should also account for "spurious" wake-ups. You
>>>>>>>>>> should increment currentAllowedOrder only after return from callable.call
>>>>>>>>>> (in finally block just before notifyAll()).
>>>>>>>>>>
>>>>>>>>>> Otherwise a nice solution - with minimal state, providing that
>>>>>>>>>> not many threads meet at the same time...
>>>>>>>>>>
>>>>>>>>>> Regards, Peter
>>>>>>>>>>
>>>>>>>>>>  RegardsSuman
>>>>>>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com>
>>>>>>>>>> <mailto:oleksandr.otenko at oracle.com>
>>>>>>>>>>   To: suman shil<suman_krec at yahoo.com> <mailto:
>>>>>>>>>> suman_krec at yahoo.com>; Concurrency-interest<
>>>>>>>>>> concurrency-interest at cs.oswego.edu> <mailto:
>>>>>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December
>>>>>>>>>> 17, 2014 9:55 PM
>>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>>> of critical sections?
>>>>>>>>>>       There is no guarantee you'll ever hand over the control to
>>>>>>>>>> the right thread upon notify()
>>>>>>>>>>     Alex
>>>>>>>>>>
>>>>>>>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>>>>>>>       Hi, Following is my solution to solve this problem. Please
>>>>>>>>>> let me know if I am missing something.
>>>>>>>>>>    public class OrderedExecutor {  private int
>>>>>>>>>> currentAllowedOrder = 0;  private int maxLength = 0;  public
>>>>>>>>>> OrderedExecutor(int n)  {          this.maxLength = n;  } public
>>>>>>>>>> synchronized Object execCriticalSectionInOrder( int order, Callable<Object>
>>>>>>>>>> callable)                                 throws Exception  { if (order >=
>>>>>>>>>> maxLength)  {  throw new Exception("Exceeds maximum order "+ maxLength);
>>>>>>>>>> }    while(order != currentAllowedOrder)  {  wait();  }    try  {
>>>>>>>>>> currentAllowedOrder = currentAllowedOrder+1;  return callable.call();  }
>>>>>>>>>> finally  {  notify();  }  } }
>>>>>>>>>>    Regards Suman
>>>>>>>>>>         From: Peter Levart<peter.levart at gmail.com> <mailto:
>>>>>>>>>> peter.levart at gmail.com>
>>>>>>>>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:
>>>>>>>>>> hanson.char at gmail.com>    Cc: concurrency-interest<
>>>>>>>>>> concurrency-interest at cs.oswego.edu> <mailto:
>>>>>>>>>> concurrency-interest at cs.oswego.edu>    Sent: Sunday, December
>>>>>>>>>> 14, 2014 11:01 PM
>>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>>> of critical sections?
>>>>>>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>>>>>>      Hi Peter,
>>>>>>>>>>    Thanks for this proposed idea of using LockSupport. This begs
>>>>>>>>>> the question: which one would you choose if you had all three (correct)
>>>>>>>>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>>>>>>>    Regards, Hanson
>>>>>>>>>>     The Semaphore/CountDownLatch variants are equivalent if you
>>>>>>>>>> don't need re-use. So any would do. They lack invalid-use detection. What
>>>>>>>>>> happens if they are not used as intended? Semaphore variant acts
>>>>>>>>>> differently than CountDownLatch variant. The low-level variant I  proposed
>>>>>>>>>> detects invalid usage. So I would probably use this one. But the low level
>>>>>>>>>> variant is harder to reason about it's correctness. I think it is correct,
>>>>>>>>>> but you should show it to somebody else to confirm this.
>>>>>>>>>>     Another question is whether you actually need this kind of
>>>>>>>>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>>>>>>>>> somebody could have an idea how to do that even more elegantly...
>>>>>>>>>>     Regards, Peter
>>>>>>>>>>                On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<
>>>>>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>>>>    Hi Hanson,
>>>>>>>>>>     This one is more low-level, but catches some invalid usages
>>>>>>>>>> and is more resource-friendly:
>>>>>>>>>>       public class OrderedExecutor {
>>>>>>>>>>         public <T> T execCriticalSectionInOrder(
>>>>>>>>>>           final int order,
>>>>>>>>>>           final Supplier<T> criticalSection
>>>>>>>>>>       ) throws InterruptedException {
>>>>>>>>>>           if (order < 0) {
>>>>>>>>>>                throw new IllegalArgumentException("'order' should
>>>>>>>>>> be >= 0");
>>>>>>>>>>           }
>>>>>>>>>>           if (order > 0) {
>>>>>>>>>>               waitForDone(order - 1);
>>>>>>>>>>           }
>>>>>>>>>>           try {
>>>>>>>>>>               return criticalSection.get();
>>>>>>>>>>           } finally {
>>>>>>>>>>               notifyDone(order);
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>         private static final Object DONE = new Object();
>>>>>>>>>>       private final ConcurrentMap<Integer, Object> signals = new
>>>>>>>>>> ConcurrentHashMap<>();
>>>>>>>>>>         private void waitForDone(int order) throws
>>>>>>>>>> InterruptedException {
>>>>>>>>>>           Object sig = signals.putIfAbsent(order,
>>>>>>>>>> Thread.currentThread());
>>>>>>>>>>           if (sig != null && sig != DONE) {
>>>>>>>>>>               throw new IllegalStateException();
>>>>>>>>>>           }
>>>>>>>>>>           while (sig != DONE) {
>>>>>>>>>>               LockSupport.park();
>>>>>>>>>>               if (Thread.interrupted()) {
>>>>>>>>>>                   throw new InterruptedException();
>>>>>>>>>>               }
>>>>>>>>>>               sig = signals.get(order);
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>         private void notifyDone(int order) {
>>>>>>>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>>>>>>>           if (sig instanceof Thread) {
>>>>>>>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>>>>>>>                   throw new IllegalStateException();
>>>>>>>>>>               }
>>>>>>>>>>               LockSupport.unpark((Thread) sig);
>>>>>>>>>>           } else if (sig != null) {
>>>>>>>>>>               throw new IllegalStateException();
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>   }
>>>>>>>>>>       Regards, Peter
>>>>>>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>>>>>>      Hi Peter,
>>>>>>>>>>    Thanks for the suggestion, and sorry about not being clear
>>>>>>>>>> about one important  detail: "n" is not known a priori when constructing an
>>>>>>>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>>>>>>>     If you know at least the upper bound of 'n', it can be used
>>>>>>>>>> with such 'n'. Otherwise something that dynamically re-sizes the array
>>>>>>>>>> could be devised. Or you could simply use a ConcurrentHashMap instead of
>>>>>>>>>> array where keys are 'order' values:
>>>>>>>>>>       public class OrderedExecutor<T> {
>>>>>>>>>>         private final ConcurrentMap<Integer, CountDownLatch>
>>>>>>>>>> latches = new ConcurrentHashMap<>();
>>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>>>>                                           final Supplier<T>
>>>>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>>>>           if (order > 0) {
>>>>>>>>>>               latches.computeIfAbsent(order - 1, o -> new
>>>>>>>>>> CountDownLatch(1)).await();
>>>>>>>>>>           }
>>>>>>>>>>           try {
>>>>>>>>>>               return criticalSection.get();
>>>>>>>>>>           } finally {
>>>>>>>>>>               latches.computeIfAbsent(order, o -> new
>>>>>>>>>> CountDownLatch(1)).countDown();
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>   }
>>>>>>>>>>       Regards, Peter
>>>>>>>>>>           You guessed right: it's a one-shot object for a
>>>>>>>>>> particular OrderedExecutor  instance, and "order" must be called indeed at
>>>>>>>>>> most once.
>>>>>>>>>>    Regards, Hanson
>>>>>>>>>>     On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<
>>>>>>>>>> peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>>>>    Hi Hanson,
>>>>>>>>>>     I don't think anything like that readily exists  in
>>>>>>>>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>>>>>>>>> with composition of existing primitives.  You haven't given any additional
>>>>>>>>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>>>>>>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>>>>>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>>>>>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>>>>>>>> (and I think that only a one-shot object  makes sense here), an array of
>>>>>>>>>> CountDownLatch(es) could be used:
>>>>>>>>>>     public class OrderedExecutor<T> {
>>>>>>>>>>       private final CountDownLatch[] latches;
>>>>>>>>>>         public OrderedExecutor(int n) {
>>>>>>>>>>           if (n < 1) throw new IllegalArgumentException("'n'
>>>>>>>>>> should be >= 1");
>>>>>>>>>>           latches = new CountDownLatch[n - 1];
>>>>>>>>>>           for (int i = 0; i < latches.length; i++) {
>>>>>>>>>>               latches[i] = new CountDownLatch(1);
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>>>>                                            final Supplier<T>
>>>>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>>>>           if (order < 0 || order > latches.length)
>>>>>>>>>>               throw new IllegalArgumentException("'order' should
>>>>>>>>>> be [0..." +  latches.length + "]");
>>>>>>>>>>           if (order > 0) {
>>>>>>>>>>               latches[order - 1].await();
>>>>>>>>>>           }
>>>>>>>>>>           try {
>>>>>>>>>>               return criticalSection.get();
>>>>>>>>>>           } finally {
>>>>>>>>>>               if (order < latches.length) {
>>>>>>>>>>                   latches[order].countDown();
>>>>>>>>>>               }
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>   }
>>>>>>>>>>       Regards, Peter
>>>>>>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>>>>>>          Hi, I am looking for a construct that can  be used to
>>>>>>>>>> efficiently enforce  ordered execution of multiple critical sections, each
>>>>>>>>>> calling from a  different thread. The calling threads may run in  parallel
>>>>>>>>>> and may call the execution method out of order. The  perceived construct
>>>>>>>>>> would therefore be responsible for re-ordering the execution of those
>>>>>>>>>> threads, so that their critical  sections (and only the critical section)
>>>>>>>>>> will be executed in order. Would something  like the following API already
>>>>>>>>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>>>>>>>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>>>>>>>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>>>>>>>>> most once with the given order, parking * and  unparking the current thread
>>>>>>>>>> as  necessary so that all critical * sections executed  by different
>>>>>>>>>> threads using this  executor take place in * the order from 1 to n
>>>>>>>>>> consecutively. */ public T execCriticalSectionInOrder
>>>>>>>>>> (  final int order, final Callable<T> criticalSection) throws
>>>>>>>>>> InterruptedException; } Regards, Hanson
>>>>>>>>>> _______________________________________________Concurrency-interest mailing
>>>>>>>>>> listConcurrency-interest at cs.oswego.edu <mailto:
>>>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>> _______________________________________________
>>>>>>>>>>   Concurrency-interest mailing list
>>>>>>>>>>   Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu <mailto:
>>>>>>>>>> Concurrency-interest at cs.oswego.edu>
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/fd168ab4/attachment-0001.html>

From lukeisandberg at gmail.com  Sun Dec 21 21:35:29 2014
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Sun, 21 Dec 2014 18:35:29 -0800
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
Message-ID: <CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>

I have come across a few situations where i am looking for a datastructure
and i feel like i keep coming up short.

The situation is analogous to the 'waiters' list in
java.util.concurrent.FutureTask.

Basically, I need to be able to publish a non-null object reference into a
data structure where
* order doesn't matter (insertion order would be nice, but i don't care
that much)
* add and remove identity semantics
* concurrent iteration (weakly consistent is fine)
* add/remove are non-blocking

CLQ is an obvious choice but removal is O(n), another choice would a simple
synchronized identity hash set which is fine but the lock + high entry
overhead is a deal breaker.

An AtomicReferenceArray would be super easy, but i can't put a bound on the
number of items.

Also, it would be fine for the code that adds items, to store additional
state (an index, a 'Node' reference), in order to facilitate removal.

The best thing i have seen from looking around appears to be something like
what FutureTask does to implement 'awaitDone', but even there
removeWaitier() is O(n).  that seems like a pretty good compromise when
lists are short, but what would be a better solution when lists are long?

Just to motivate this a little bit, the two cases I am looking at in
particular are:

* maintaining a set of threads associated with a server 'request' (threads
enter/exit when they start executing tasks associated with the request)
* maintaining a set of futures to be cancelled (futures are removed when
they complete to avoid pinning their referents).

Any pointers or ideas?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/3ea92696/attachment.html>

From martinrb at google.com  Mon Dec 22 00:57:37 2014
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 21 Dec 2014 21:57:37 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
	<5496B0FE.7070502@gmail.com>
	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
	<549737AB.8060508@gmail.com>
	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
Message-ID: <CA+kOe0-OX_h1oJ5mZnSh8WNxoOgpNo9P1kbe7fVnbWqg973HSA@mail.gmail.com>

It looks to me as well like a bug in Semaphore or AQS.
Where can we find the complete test program so we can debug?

On Sun, Dec 21, 2014 at 4:25 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> Below is the most concise implementation I can imagine, using a single
> Semaphore, which is legal AFAICT according to the javadoc, but which
> deadlocks in my tests.
>
> class OrderedExecutor<T> {
>
>   private final Semaphore semaphore = new Semaphore(1);
>
>   @Override
>   public T execCallableInOrder(int order, Callable<T> callable)
>       throws InterruptedException, Exception {
>     assert order >= 0;
>     int acquires = order + 1;
>     semaphore.acquire(acquires);
>     try {
>       return callable.call();
>     } finally {
>       semaphore.release(acquires + 1);
>     }
>   }
> }
>
>
> testOrderedExecutorSynchronizer
> Waiting 2
> Waiting 1
> Waiting 8
> Waiting 7
> Waiting 9
> Waiting 0
> Executing 0
> Executing 1
> Executing 2
> Waiting 6
> Waiting 4
> Waiting 3
> Waiting 5
> Executing 3
> ... crickets ...
>
>
> A bug?
>
> On Sun, Dec 21, 2014 at 1:12 PM, Peter Levart <peter.levart at gmail.com>
> wrote:
>>
>>
>> On 12/21/2014 07:03 PM, Hanson Char wrote:
>>
>> Interesting - now that each order is associated with a different object
>> monitor, the wait/notifyAll would at most wake up one thread instead of all.
>> In fact, it seems the use of notify (instead of notifyAll) would suffice.
>> Simple and clever, but it does require the "stripeSize" to be known a
>> priori.
>>
>>
>> Not exactly. Each object monitor is associated with multiple orders. For
>> example, if stripeSize is 4, the 1st object monitor is associated with
>> orders: 0, 4, 8, 12, 16, ... the 2nd with orders 1, 5, 9, 13, 17, ... etc.
>>
>> But it also means that each object monitor is associated statistically
>> with just N/stripeSize threads that wait instead of N. So notifyAll is still
>> needed, but it wakes up less threads - how much less depends on chosen
>> stripeSize.
>>
>> Regards, Peter
>>
>>
>> Regards,
>> Hanson
>>
>> On Sun, Dec 21, 2014 at 3:37 AM, Peter Levart <peter.levart at gmail.com>
>> wrote:
>>>
>>> Hi,
>>>
>>> Here's a simple implementation (based on Suman Shil's idea) that stripes
>>> the waiting threads into multiple buckets:
>>>
>>> public class StripedOrderedExecutor {
>>>
>>>     private final MeetingPoint[] meetingPoints;
>>>
>>>     public StripedOrderedExecutor(int stripeSize) {
>>>         assert stripeSize > 0;
>>>         meetingPoints = new MeetingPoint[stripeSize];
>>>         for (int i = 0; i < stripeSize; i++) {
>>>             meetingPoints[i] = new MeetingPoint();
>>>         }
>>>     }
>>>
>>>     public <T> T execCriticalSectionInOrder(
>>>         final int order,
>>>         final Supplier<T> criticalSection
>>>     ) throws InterruptedException {
>>>         assert order >= 0;
>>>
>>>         meetingPoints[order % meetingPoints.length].waitForGreen(order);
>>>         try {
>>>             return criticalSection.get();
>>>         } finally {
>>>             meetingPoints[(order + 1) %
>>> meetingPoints.length].notifyGreen(order + 1);
>>>         }
>>>     }
>>>
>>>     private static class MeetingPoint {
>>>         private int lastGreen;
>>>
>>>         synchronized void waitForGreen(int order) throws
>>> InterruptedException {
>>>             while (lastGreen != order) {
>>>                 wait();
>>>             }
>>>         }
>>>
>>>         synchronized void notifyGreen(int order) {
>>>             lastGreen = order;
>>>             notifyAll();
>>>         }
>>>     }
>>> }
>>>
>>>
>>> Regards, Peter
>>>
>>>
>>>
>>>
>>>
>>> On 12/21/2014 02:35 AM, Hanson Char wrote:
>>>
>>> Thanks, Joe. That's an interesting point about the long term memory
>>> impact of TreeMap vs HashMap - similar to LinkedList vs ArrayList.
>>>
>>> Regards,
>>> Hanson
>>>
>>> On Sat, Dec 20, 2014 at 5:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>> wrote:
>>>>
>>>> I see.  Yes, that addresses the removal problem, though note that
>>>> HashMap does not shrink when data is removed. Even if all keys are removed
>>>> from HashMap, the inner size of its table does not change.
>>>>
>>>> So in this case maybe I would choose TreeMap as the implementation. Its
>>>> remove() method *does* remove all the memory associated with an entry.
>>>>
>>>> In general, I don't like Map<Integer, *> because it seems like a hack,
>>>> but really in this case it's a small trade-off.  Map is more concise.
>>>> PriorityQueue is a little clearer.
>>>>
>>>> On Sat, Dec 20, 2014 at 4:50 PM, Hanson Char <hanson.char at gmail.com>
>>>> wrote:
>>>>>
>>>>> Hi Joe,
>>>>>
>>>>> >A Queue should also retain less memory. A Map will tend to accumulate
>>>>> > both Entry and Integer instances over time.
>>>>>
>>>>> Using a Map, the signalNext would look something like below.  Doesn't
>>>>> each entry get eventually removed from the map?  Why would they accumulate
>>>>> over time?
>>>>>
>>>>> Regards,
>>>>> Hanson
>>>>>
>>>>>     void signalNext(final int nextOrder) {
>>>>>       lock.lock();
>>>>>       try {
>>>>>         this.nextOrder = nextOrder;
>>>>>         Condition cond = map.remove(nextOrder);
>>>>>         if (cond != null) {
>>>>>             cond.signal();
>>>>>         }
>>>>>       } finally {
>>>>>         lock.unlock();
>>>>>       }
>>>>>     }
>>>>>   }
>>>>>
>>>>>
>>>>> On Sat, Dec 20, 2014 at 4:39 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>> wrote:
>>>>>>
>>>>>> I think PriorityQueue more clearly represents the function than Map
>>>>>> does in this case.
>>>>>>
>>>>>> A Queue should also retain less memory. A Map will tend to accumulate
>>>>>> both Entry and Integer instances over time.
>>>>>>
>>>>>> The ConcurrentMap does have the advantage of lock striping, which
>>>>>> might be more performant if there is high contention.
>>>>>>
>>>>>> I think this is an interesting problem because there are quite a few
>>>>>> different solutions, each with advantages and disadvantages in terms of
>>>>>> performance and maintainability.
>>>>>>
>>>>>> If there were a PriorityQueue version of AbstractQueuedSynchronizer,
>>>>>> then we could have yet another solution using the AQS's state to represent
>>>>>> the next available order.
>>>>>>
>>>>>> On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char <hanson.char at gmail.com>
>>>>>> wrote:
>>>>>>>
>>>>>>> This one looks interesting and educational.  The use of condition
>>>>>>> enables us to suspend and resume specifically targeted threads without
>>>>>>> resorting to the use of LockSupport.{part, unpark}.  Instead of using a
>>>>>>> PriorityQueue, perhaps we can use a simple Map<Integer, Condition> instead
>>>>>>> so we can get away without the extra Waiter inner class?
>>>>>>>
>>>>>>> Regards,
>>>>>>> Hanson
>>>>>>>
>>>>>>> On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>>> wrote:
>>>>>>>>
>>>>>>>> To round out the selection, here's an implementation using a
>>>>>>>> PriorityQueue of Conditions:
>>>>>>>>
>>>>>>>> class OrderedExecutor<T> {
>>>>>>>>
>>>>>>>>   static class Waiter implements Comparable<Waiter> {
>>>>>>>>
>>>>>>>>     final int order;
>>>>>>>>     final Condition condition;
>>>>>>>>
>>>>>>>>     Waiter(int order, Condition condition) {
>>>>>>>>       this.order = order;
>>>>>>>>       this.condition = condition;
>>>>>>>>     }
>>>>>>>>
>>>>>>>>     @Override
>>>>>>>>     public int compareTo(Waiter waiter) {
>>>>>>>>       return order - waiter.order;
>>>>>>>>     }
>>>>>>>>   }
>>>>>>>>
>>>>>>>>   final Lock lock = new ReentrantLock();
>>>>>>>>   final Queue<Waiter> queue = new PriorityQueue<>();
>>>>>>>>   int nextOrder; // 0 is next
>>>>>>>>
>>>>>>>>   public T execCallableInOrder(int order, Callable<T> callable)
>>>>>>>> throws Exception {
>>>>>>>>     assert order >= 0;
>>>>>>>>     awaitTurn(order);
>>>>>>>>     try {
>>>>>>>>       return callable.call();
>>>>>>>>     } finally {
>>>>>>>>       signalNext(order + 1);
>>>>>>>>     }
>>>>>>>>   }
>>>>>>>>
>>>>>>>>   void awaitTurn(int order) {
>>>>>>>>     lock.lock();
>>>>>>>>     try {
>>>>>>>>       Condition condition = null;
>>>>>>>>       while (nextOrder != order) {
>>>>>>>>         if (condition == null) {
>>>>>>>>           condition = lock.newCondition();
>>>>>>>>           queue.add(new Waiter(order, condition));
>>>>>>>>         }
>>>>>>>>         condition.awaitUninterruptibly();
>>>>>>>>       }
>>>>>>>>     } finally {
>>>>>>>>       lock.unlock();
>>>>>>>>     }
>>>>>>>>   }
>>>>>>>>
>>>>>>>>   void signalNext(int nextOrder) {
>>>>>>>>     lock.lock();
>>>>>>>>     try {
>>>>>>>>       this.nextOrder = nextOrder;
>>>>>>>>       Waiter waiter = queue.peek();
>>>>>>>>       if (waiter != null && waiter.order == nextOrder) {
>>>>>>>>         queue.remove();
>>>>>>>>         waiter.condition.signal();
>>>>>>>>       }
>>>>>>>>     } finally {
>>>>>>>>       lock.unlock();
>>>>>>>>     }
>>>>>>>>   }
>>>>>>>> }
>>>>>>>>
>>>>>>>> On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer
>>>>>>>> <joe.bowbeer at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>> Suman,
>>>>>>>>>
>>>>>>>>> I would advise against using notify/wait.  It raises a red flag for
>>>>>>>>> a lot of reviewers, including me.
>>>>>>>>>
>>>>>>>>> The problems I see in this implementation are:
>>>>>>>>>
>>>>>>>>> 1. Pre-allocation of locks is prohibited by (revised) problem
>>>>>>>>> statement.
>>>>>>>>>
>>>>>>>>> Note that if pre-allocation were allowed, then an array would be
>>>>>>>>> more efficient than a Map.
>>>>>>>>>
>>>>>>>>> 2. Access to currentAllowedOrder is not thread-safe but should be.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <suman_krec at yahoo.com>
>>>>>>>>> wrote:
>>>>>>>>>>
>>>>>>>>>> I have modified my solution to avoid notifyAll(). Let me know your
>>>>>>>>>> feedback.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> public class OrderedExecutor
>>>>>>>>>> {
>>>>>>>>>> private int maxOrder;
>>>>>>>>>> private int currentAllowedOrder;
>>>>>>>>>> private Map<Integer, Object> map = new HashMap<Integer, Object>();
>>>>>>>>>>     public OrderedExecutor(int n)
>>>>>>>>>>     {
>>>>>>>>>>          this.maxOrder = n;
>>>>>>>>>>          this.currentAllowedOrder = 0;
>>>>>>>>>>     for(int i = 0 ; i < this.maxOrder ; i++)
>>>>>>>>>>     {
>>>>>>>>>>     map.put(i, new Object());
>>>>>>>>>>     }
>>>>>>>>>>     }
>>>>>>>>>>
>>>>>>>>>>     public  Object execCriticalSectionInOrder(int order,
>>>>>>>>>>                                               Callable<Object>
>>>>>>>>>> callable)
>>>>>>>>>>                                               throws Exception
>>>>>>>>>>     {
>>>>>>>>>> if (order >= this.maxOrder)
>>>>>>>>>> {
>>>>>>>>>> throw new Exception("Exceeds maximum order "+ maxOrder);
>>>>>>>>>> }
>>>>>>>>>>
>>>>>>>>>> while(order != currentAllowedOrder)
>>>>>>>>>> {
>>>>>>>>>> synchronized (this.map.get(order))
>>>>>>>>>> {
>>>>>>>>>> this.map.get(order).wait();
>>>>>>>>>> }
>>>>>>>>>> }
>>>>>>>>>> try
>>>>>>>>>> {
>>>>>>>>>> return callable.call();
>>>>>>>>>> }
>>>>>>>>>>        finally
>>>>>>>>>> {
>>>>>>>>>>             currentAllowedOrder = currentAllowedOrder+1;
>>>>>>>>>> synchronized (this.map.get(order+1))
>>>>>>>>>>             {
>>>>>>>>>>                 this.map.get(order+1).notify();
>>>>>>>>>>             }
>>>>>>>>>>        }
>>>>>>>>>>     }
>>>>>>>>>> }
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> ________________________________
>>>>>>>>>> From: Joe Bowbeer <joe.bowbeer at gmail.com>
>>>>>>>>>> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
>>>>>>>>>> Sent: Friday, December 19, 2014 3:33 AM
>>>>>>>>>>
>>>>>>>>>> Subject: Re: [concurrency-interest] Enforcing ordered execution of
>>>>>>>>>> critical sections?
>>>>>>>>>>
>>>>>>>>>> That would be "Tom" Cargill; link to paper:
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer
>>>>>>>>>> <joe.bowbeer at gmail.com> wrote:
>>>>>>>>>>
>>>>>>>>>> I frown on use of notify[All]/wait because they make the code hard
>>>>>>>>>> to maintain.
>>>>>>>>>>
>>>>>>>>>> In this case, with potentially lots of waiting threads, I would
>>>>>>>>>> check out the "Specific Notification" pattern if I were determined to go the
>>>>>>>>>> wait/notify route:
>>>>>>>>>>
>>>>>>>>>> Tim Cargill's paper is dated but still worth reading.
>>>>>>>>>>
>>>>>>>>>> Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ
>>>>>>>>>> and Peter Haggar's article:
>>>>>>>>>>
>>>>>>>>>> http://www.ibm.com/developerworks/java/library/j-spnotif.html
>>>>>>>>>>
>>>>>>>>>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko
>>>>>>>>>> <oleksandr.otenko at oracle.com> wrote:
>>>>>>>>>>
>>>>>>>>>> Yes, no one said it is a good idea to always do that. When it is
>>>>>>>>>> contended, most of the threads will wake up to only go back to sleep.
>>>>>>>>>>
>>>>>>>>>> The pattern you are after is usually called sequencer. You can see
>>>>>>>>>> it used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe
>>>>>>>>>> not that popular.
>>>>>>>>>>
>>>>>>>>>> The best solution will be lock-like, but the waiter nodes will
>>>>>>>>>> contain the value they are waiting for - so only the specific threads get
>>>>>>>>>> woken up. The solution with concurrent map is very similar, only with larger
>>>>>>>>>> overhead from storing the index the thread is waiting for.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> Alex
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>>>>>>>>
>>>>>>>>>> Less overhead and simpler are a nice properties, even though at
>>>>>>>>>> the expense of having to wake up all waiting threads just to find out the
>>>>>>>>>> one with the right order to execute.  Still, this seems like a good
>>>>>>>>>> tradeoff.
>>>>>>>>>>
>>>>>>>>>> Thanks,
>>>>>>>>>> Hanson
>>>>>>>>>>
>>>>>>>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart
>>>>>>>>>> <peter.levart at gmail.com> wrote:
>>>>>>>>>>
>>>>>>>>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>>>>>>>>
>>>>>>>>>> No, there is no difference. Peter didn't spot your entire method
>>>>>>>>>> is synchronized, so spurious wakeup won't make progress until the owner of
>>>>>>>>>> the lock exits the method.
>>>>>>>>>>
>>>>>>>>>> You could split the synchronization into two blocks - one
>>>>>>>>>> encompassing the wait loop, the other in the finally block; but it may make
>>>>>>>>>> no difference.
>>>>>>>>>>
>>>>>>>>>> Alex
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> You're right, Alex. I'm so infected with park/unpark virus that I
>>>>>>>>>> missed that ;-)
>>>>>>>>>>
>>>>>>>>>> Peter
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On 17/12/2014 18:36, suman shil wrote:
>>>>>>>>>>
>>>>>>>>>> Thanks peter for your reply. You are right. I should have
>>>>>>>>>> incremented currentAllowedOrder in finally block.
>>>>>>>>>>
>>>>>>>>>> Suman
>>>>>>>>>>
>>>>>>>>>> ------------------------------------------------------------------------
>>>>>>>>>> *From:* Peter Levart <peter.levart at gmail.com>
>>>>>>>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko
>>>>>>>>>> <oleksandr.otenko at oracle.com>; Concurrency-interest
>>>>>>>>>> <concurrency-interest at cs.oswego.edu>
>>>>>>>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>>> of critical sections?
>>>>>>>>>>
>>>>>>>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>>>>>>>>
>>>>>>>>>> Thanks for your response. Will notifyAll() instead of notify()
>>>>>>>>>> solve the problem?
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> It will, but you should also account for "spurious" wake-ups. You
>>>>>>>>>> should increment currentAllowedOrder only after return from callable.call
>>>>>>>>>> (in finally block just before notifyAll()).
>>>>>>>>>>
>>>>>>>>>> Otherwise a nice solution - with minimal state, providing that not
>>>>>>>>>> many threads meet at the same time...
>>>>>>>>>>
>>>>>>>>>> Regards, Peter
>>>>>>>>>>
>>>>>>>>>> RegardsSuman
>>>>>>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com>
>>>>>>>>>> <mailto:oleksandr.otenko at oracle.com>
>>>>>>>>>>   To: suman shil<suman_krec at yahoo.com>
>>>>>>>>>> <mailto:suman_krec at yahoo.com>;
>>>>>>>>>> Concurrency-interest<concurrency-interest at cs.oswego.edu>
>>>>>>>>>> <mailto:concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December 17,
>>>>>>>>>> 2014 9:55 PM
>>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>>> of critical sections?
>>>>>>>>>>       There is no guarantee you'll ever hand over the control to
>>>>>>>>>> the right thread upon notify()
>>>>>>>>>>     Alex
>>>>>>>>>>
>>>>>>>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>>>>>>>       Hi, Following is my solution to solve this problem. Please
>>>>>>>>>> let me know if I am missing something.
>>>>>>>>>>    public class OrderedExecutor {  private int currentAllowedOrder
>>>>>>>>>> = 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {
>>>>>>>>>> this.maxLength = n;  } public synchronized Object
>>>>>>>>>> execCriticalSectionInOrder( int order, Callable<Object> callable)
>>>>>>>>>> throws Exception  { if (order >= maxLength)  {  throw new Exception("Exceeds
>>>>>>>>>> maximum order "+ maxLength);  }    while(order != currentAllowedOrder)  {
>>>>>>>>>> wait();  }    try  { currentAllowedOrder = currentAllowedOrder+1;  return
>>>>>>>>>> callable.call();  }  finally  {  notify();  }  } }
>>>>>>>>>>    Regards Suman
>>>>>>>>>>        From: Peter Levart<peter.levart at gmail.com>
>>>>>>>>>> <mailto:peter.levart at gmail.com>
>>>>>>>>>>   To: Hanson Char<hanson.char at gmail.com>
>>>>>>>>>> <mailto:hanson.char at gmail.com>    Cc:
>>>>>>>>>> concurrency-interest<concurrency-interest at cs.oswego.edu>
>>>>>>>>>> <mailto:concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14,
>>>>>>>>>> 2014 11:01 PM
>>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution
>>>>>>>>>> of critical sections?
>>>>>>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>>>>>>      Hi Peter,
>>>>>>>>>>    Thanks for this proposed idea of using LockSupport. This begs
>>>>>>>>>> the question: which one would you choose if you had all three (correct)
>>>>>>>>>> implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>>>>>>>    Regards, Hanson
>>>>>>>>>>     The Semaphore/CountDownLatch variants are equivalent if you
>>>>>>>>>> don't need re-use. So any would do. They lack invalid-use detection. What
>>>>>>>>>> happens if they are not used as intended? Semaphore variant acts differently
>>>>>>>>>> than CountDownLatch variant. The low-level variant I  proposed detects
>>>>>>>>>> invalid usage. So I would probably use this one. But the low level variant
>>>>>>>>>> is harder to reason about it's correctness. I think it is correct, but you
>>>>>>>>>> should show it to somebody else to confirm this.
>>>>>>>>>>     Another question is whether you actually need this kind of
>>>>>>>>>> synchronizer. Maybe if you explained what you are trying to achieve,
>>>>>>>>>> somebody could have an idea how to do that even more elegantly...
>>>>>>>>>>     Regards, Peter
>>>>>>>>>>              On Sun, Dec 14, 2014 at 9:01 AM, Peter
>>>>>>>>>> Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>>>>    Hi Hanson,
>>>>>>>>>>     This one is more low-level, but catches some invalid usages
>>>>>>>>>> and is more resource-friendly:
>>>>>>>>>>       public class OrderedExecutor {
>>>>>>>>>>         public <T> T execCriticalSectionInOrder(
>>>>>>>>>>           final int order,
>>>>>>>>>>           final Supplier<T> criticalSection
>>>>>>>>>>       ) throws InterruptedException {
>>>>>>>>>>           if (order < 0) {
>>>>>>>>>>                throw new IllegalArgumentException("'order' should
>>>>>>>>>> be >= 0");
>>>>>>>>>>           }
>>>>>>>>>>           if (order > 0) {
>>>>>>>>>>               waitForDone(order - 1);
>>>>>>>>>>           }
>>>>>>>>>>           try {
>>>>>>>>>>               return criticalSection.get();
>>>>>>>>>>           } finally {
>>>>>>>>>>               notifyDone(order);
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>         private static final Object DONE = new Object();
>>>>>>>>>>       private final ConcurrentMap<Integer, Object> signals = new
>>>>>>>>>> ConcurrentHashMap<>();
>>>>>>>>>>         private void waitForDone(int order) throws
>>>>>>>>>> InterruptedException {
>>>>>>>>>>           Object sig = signals.putIfAbsent(order,
>>>>>>>>>> Thread.currentThread());
>>>>>>>>>>           if (sig != null && sig != DONE) {
>>>>>>>>>>               throw new IllegalStateException();
>>>>>>>>>>           }
>>>>>>>>>>           while (sig != DONE) {
>>>>>>>>>>               LockSupport.park();
>>>>>>>>>>               if (Thread.interrupted()) {
>>>>>>>>>>                   throw new InterruptedException();
>>>>>>>>>>               }
>>>>>>>>>>               sig = signals.get(order);
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>         private void notifyDone(int order) {
>>>>>>>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>>>>>>>           if (sig instanceof Thread) {
>>>>>>>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>>>>>>>                   throw new IllegalStateException();
>>>>>>>>>>               }
>>>>>>>>>>               LockSupport.unpark((Thread) sig);
>>>>>>>>>>           } else if (sig != null) {
>>>>>>>>>>               throw new IllegalStateException();
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>   }
>>>>>>>>>>       Regards, Peter
>>>>>>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>>>>>>      Hi Peter,
>>>>>>>>>>    Thanks for the suggestion, and sorry about not being clear
>>>>>>>>>> about one important  detail: "n" is not known a priori when constructing an
>>>>>>>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>>>>>>>     If you know at least the upper bound of 'n', it can be used
>>>>>>>>>> with such 'n'. Otherwise something that dynamically re-sizes the array could
>>>>>>>>>> be devised. Or you could simply use a ConcurrentHashMap instead of array
>>>>>>>>>> where keys are 'order' values:
>>>>>>>>>>       public class OrderedExecutor<T> {
>>>>>>>>>>         private final ConcurrentMap<Integer, CountDownLatch>
>>>>>>>>>> latches = new ConcurrentHashMap<>();
>>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>>>>                                           final Supplier<T>
>>>>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>>>>           if (order > 0) {
>>>>>>>>>>               latches.computeIfAbsent(order - 1, o -> new
>>>>>>>>>> CountDownLatch(1)).await();
>>>>>>>>>>           }
>>>>>>>>>>           try {
>>>>>>>>>>               return criticalSection.get();
>>>>>>>>>>           } finally {
>>>>>>>>>>               latches.computeIfAbsent(order, o -> new
>>>>>>>>>> CountDownLatch(1)).countDown();
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>   }
>>>>>>>>>>       Regards, Peter
>>>>>>>>>>           You guessed right: it's a one-shot object for a
>>>>>>>>>> particular OrderedExecutor  instance, and "order" must be called indeed at
>>>>>>>>>> most once.
>>>>>>>>>>    Regards, Hanson
>>>>>>>>>>   On Sun, Dec 14, 2014 at 2:21 AM, Peter
>>>>>>>>>> Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>>>>>    Hi Hanson,
>>>>>>>>>>     I don't think anything like that readily exists  in
>>>>>>>>>> java.lang.concurrent, but what you describe should be possible to  achieve
>>>>>>>>>> with composition of existing primitives.  You haven't given any additional
>>>>>>>>>> hints to what your OrderedExecutor  should behave like. Should it be a
>>>>>>>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>>>>>>>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a particular
>>>>>>>>>> OrderedExecutor instance and 'order' value be called at most once? If yes
>>>>>>>>>> (and I think that only a one-shot object  makes sense here), an array of
>>>>>>>>>> CountDownLatch(es) could be used:
>>>>>>>>>>     public class OrderedExecutor<T> {
>>>>>>>>>>       private final CountDownLatch[] latches;
>>>>>>>>>>         public OrderedExecutor(int n) {
>>>>>>>>>>           if (n < 1) throw new IllegalArgumentException("'n'
>>>>>>>>>> should be >= 1");
>>>>>>>>>>           latches = new CountDownLatch[n - 1];
>>>>>>>>>>           for (int i = 0; i < latches.length; i++) {
>>>>>>>>>>               latches[i] = new CountDownLatch(1);
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>>>>>                                            final Supplier<T>
>>>>>>>>>> criticalSection) throws InterruptedException {
>>>>>>>>>>           if (order < 0 || order > latches.length)
>>>>>>>>>>               throw new IllegalArgumentException("'order' should
>>>>>>>>>> be [0..." +  latches.length + "]");
>>>>>>>>>>           if (order > 0) {
>>>>>>>>>>               latches[order - 1].await();
>>>>>>>>>>           }
>>>>>>>>>>           try {
>>>>>>>>>>               return criticalSection.get();
>>>>>>>>>>           } finally {
>>>>>>>>>>               if (order < latches.length) {
>>>>>>>>>>                   latches[order].countDown();
>>>>>>>>>>               }
>>>>>>>>>>           }
>>>>>>>>>>       }
>>>>>>>>>>   }
>>>>>>>>>>       Regards, Peter
>>>>>>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>>>>>>         Hi, I am looking for a construct that can  be used to
>>>>>>>>>> efficiently enforce  ordered execution of multiple critical sections, each
>>>>>>>>>> calling from a  different thread. The calling threads may run in  parallel
>>>>>>>>>> and may call the execution method out of order. The  perceived construct
>>>>>>>>>> would therefore be responsible for re-ordering the execution of those
>>>>>>>>>> threads, so that their critical  sections (and only the critical section)
>>>>>>>>>> will be executed in order. Would something  like the following API already
>>>>>>>>>> exist? /** * Used to enforce ordered execution of critical sections calling
>>>>>>>>>> from multiple *  threads, parking and unparking the  threads as necessary.
>>>>>>>>>> */ public class OrderedExecutor<T> { /** * Executes a critical section at
>>>>>>>>>> most once with the given order, parking * and  unparking the current thread
>>>>>>>>>> as  necessary so that all critical * sections executed  by different threads
>>>>>>>>>> using this  executor take place in * the order from 1 to n  consecutively.
>>>>>>>>>> */ public T execCriticalSectionInOrder
>>>>>>>>>> (  final int order, final Callable<T> criticalSection) throws
>>>>>>>>>> InterruptedException; } Regards, Hanson
>>>>>>>>>> _______________________________________________Concurrency-interest mailing
>>>>>>>>>> listConcurrency-interest at cs.oswego.edu
>>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>> _______________________________________________
>>>>>>>>>>   Concurrency-interest mailing list
>>>>>>>>>>   Concurrency-interest at cs.oswego.edu
>>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>
>>>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From hanson.char at gmail.com  Mon Dec 22 01:15:48 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 21 Dec 2014 22:15:48 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CA+kOe0-OX_h1oJ5mZnSh8WNxoOgpNo9P1kbe7fVnbWqg973HSA@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
	<5496B0FE.7070502@gmail.com>
	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
	<549737AB.8060508@gmail.com>
	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
	<CA+kOe0-OX_h1oJ5mZnSh8WNxoOgpNo9P1kbe7fVnbWqg973HSA@mail.gmail.com>
Message-ID: <CABWgujYjpkgdcJWW5wriiZDBugvNNBWfbpfob95uXb=dsdmyCQ@mail.gmail.com>

Hi Martin,

Please find attached a modified version Joe's OrderedExecutor.java which
assumes the order to start at 1 (rather than zero), and an
OrderedExecutorTest.java that can be used to sanity check the correct
behavior.

Hope this helps.

Regards,
Hanson

On Sun, Dec 21, 2014 at 9:57 PM, Martin Buchholz <martinrb at google.com>
wrote:

> It looks to me as well like a bug in Semaphore or AQS.
> Where can we find the complete test program so we can debug?
>
> On Sun, Dec 21, 2014 at 4:25 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
> wrote:
> > Below is the most concise implementation I can imagine, using a single
> > Semaphore, which is legal AFAICT according to the javadoc, but which
> > deadlocks in my tests.
> >
> > class OrderedExecutor<T> {
> >
> >   private final Semaphore semaphore = new Semaphore(1);
> >
> >   @Override
> >   public T execCallableInOrder(int order, Callable<T> callable)
> >       throws InterruptedException, Exception {
> >     assert order >= 0;
> >     int acquires = order + 1;
> >     semaphore.acquire(acquires);
> >     try {
> >       return callable.call();
> >     } finally {
> >       semaphore.release(acquires + 1);
> >     }
> >   }
> > }
> >
> >
> > testOrderedExecutorSynchronizer
> > Waiting 2
> > Waiting 1
> > Waiting 8
> > Waiting 7
> > Waiting 9
> > Waiting 0
> > Executing 0
> > Executing 1
> > Executing 2
> > Waiting 6
> > Waiting 4
> > Waiting 3
> > Waiting 5
> > Executing 3
> > ... crickets ...
> >
> >
> > A bug?
> >
> > On Sun, Dec 21, 2014 at 1:12 PM, Peter Levart <peter.levart at gmail.com>
> > wrote:
> >>
> >>
> >> On 12/21/2014 07:03 PM, Hanson Char wrote:
> >>
> >> Interesting - now that each order is associated with a different object
> >> monitor, the wait/notifyAll would at most wake up one thread instead of
> all.
> >> In fact, it seems the use of notify (instead of notifyAll) would
> suffice.
> >> Simple and clever, but it does require the "stripeSize" to be known a
> >> priori.
> >>
> >>
> >> Not exactly. Each object monitor is associated with multiple orders. For
> >> example, if stripeSize is 4, the 1st object monitor is associated with
> >> orders: 0, 4, 8, 12, 16, ... the 2nd with orders 1, 5, 9, 13, 17, ...
> etc.
> >>
> >> But it also means that each object monitor is associated statistically
> >> with just N/stripeSize threads that wait instead of N. So notifyAll is
> still
> >> needed, but it wakes up less threads - how much less depends on chosen
> >> stripeSize.
> >>
> >> Regards, Peter
> >>
> >>
> >> Regards,
> >> Hanson
> >>
> >> On Sun, Dec 21, 2014 at 3:37 AM, Peter Levart <peter.levart at gmail.com>
> >> wrote:
> >>>
> >>> Hi,
> >>>
> >>> Here's a simple implementation (based on Suman Shil's idea) that
> stripes
> >>> the waiting threads into multiple buckets:
> >>>
> >>> public class StripedOrderedExecutor {
> >>>
> >>>     private final MeetingPoint[] meetingPoints;
> >>>
> >>>     public StripedOrderedExecutor(int stripeSize) {
> >>>         assert stripeSize > 0;
> >>>         meetingPoints = new MeetingPoint[stripeSize];
> >>>         for (int i = 0; i < stripeSize; i++) {
> >>>             meetingPoints[i] = new MeetingPoint();
> >>>         }
> >>>     }
> >>>
> >>>     public <T> T execCriticalSectionInOrder(
> >>>         final int order,
> >>>         final Supplier<T> criticalSection
> >>>     ) throws InterruptedException {
> >>>         assert order >= 0;
> >>>
> >>>         meetingPoints[order %
> meetingPoints.length].waitForGreen(order);
> >>>         try {
> >>>             return criticalSection.get();
> >>>         } finally {
> >>>             meetingPoints[(order + 1) %
> >>> meetingPoints.length].notifyGreen(order + 1);
> >>>         }
> >>>     }
> >>>
> >>>     private static class MeetingPoint {
> >>>         private int lastGreen;
> >>>
> >>>         synchronized void waitForGreen(int order) throws
> >>> InterruptedException {
> >>>             while (lastGreen != order) {
> >>>                 wait();
> >>>             }
> >>>         }
> >>>
> >>>         synchronized void notifyGreen(int order) {
> >>>             lastGreen = order;
> >>>             notifyAll();
> >>>         }
> >>>     }
> >>> }
> >>>
> >>>
> >>> Regards, Peter
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> On 12/21/2014 02:35 AM, Hanson Char wrote:
> >>>
> >>> Thanks, Joe. That's an interesting point about the long term memory
> >>> impact of TreeMap vs HashMap - similar to LinkedList vs ArrayList.
> >>>
> >>> Regards,
> >>> Hanson
> >>>
> >>> On Sat, Dec 20, 2014 at 5:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
> >>> wrote:
> >>>>
> >>>> I see.  Yes, that addresses the removal problem, though note that
> >>>> HashMap does not shrink when data is removed. Even if all keys are
> removed
> >>>> from HashMap, the inner size of its table does not change.
> >>>>
> >>>> So in this case maybe I would choose TreeMap as the implementation.
> Its
> >>>> remove() method *does* remove all the memory associated with an entry.
> >>>>
> >>>> In general, I don't like Map<Integer, *> because it seems like a hack,
> >>>> but really in this case it's a small trade-off.  Map is more concise.
> >>>> PriorityQueue is a little clearer.
> >>>>
> >>>> On Sat, Dec 20, 2014 at 4:50 PM, Hanson Char <hanson.char at gmail.com>
> >>>> wrote:
> >>>>>
> >>>>> Hi Joe,
> >>>>>
> >>>>> >A Queue should also retain less memory. A Map will tend to
> accumulate
> >>>>> > both Entry and Integer instances over time.
> >>>>>
> >>>>> Using a Map, the signalNext would look something like below.  Doesn't
> >>>>> each entry get eventually removed from the map?  Why would they
> accumulate
> >>>>> over time?
> >>>>>
> >>>>> Regards,
> >>>>> Hanson
> >>>>>
> >>>>>     void signalNext(final int nextOrder) {
> >>>>>       lock.lock();
> >>>>>       try {
> >>>>>         this.nextOrder = nextOrder;
> >>>>>         Condition cond = map.remove(nextOrder);
> >>>>>         if (cond != null) {
> >>>>>             cond.signal();
> >>>>>         }
> >>>>>       } finally {
> >>>>>         lock.unlock();
> >>>>>       }
> >>>>>     }
> >>>>>   }
> >>>>>
> >>>>>
> >>>>> On Sat, Dec 20, 2014 at 4:39 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
> >>>>> wrote:
> >>>>>>
> >>>>>> I think PriorityQueue more clearly represents the function than Map
> >>>>>> does in this case.
> >>>>>>
> >>>>>> A Queue should also retain less memory. A Map will tend to
> accumulate
> >>>>>> both Entry and Integer instances over time.
> >>>>>>
> >>>>>> The ConcurrentMap does have the advantage of lock striping, which
> >>>>>> might be more performant if there is high contention.
> >>>>>>
> >>>>>> I think this is an interesting problem because there are quite a few
> >>>>>> different solutions, each with advantages and disadvantages in
> terms of
> >>>>>> performance and maintainability.
> >>>>>>
> >>>>>> If there were a PriorityQueue version of AbstractQueuedSynchronizer,
> >>>>>> then we could have yet another solution using the AQS's state to
> represent
> >>>>>> the next available order.
> >>>>>>
> >>>>>> On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char <hanson.char at gmail.com
> >
> >>>>>> wrote:
> >>>>>>>
> >>>>>>> This one looks interesting and educational.  The use of condition
> >>>>>>> enables us to suspend and resume specifically targeted threads
> without
> >>>>>>> resorting to the use of LockSupport.{part, unpark}.  Instead of
> using a
> >>>>>>> PriorityQueue, perhaps we can use a simple Map<Integer, Condition>
> instead
> >>>>>>> so we can get away without the extra Waiter inner class?
> >>>>>>>
> >>>>>>> Regards,
> >>>>>>> Hanson
> >>>>>>>
> >>>>>>> On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer <
> joe.bowbeer at gmail.com>
> >>>>>>> wrote:
> >>>>>>>>
> >>>>>>>> To round out the selection, here's an implementation using a
> >>>>>>>> PriorityQueue of Conditions:
> >>>>>>>>
> >>>>>>>> class OrderedExecutor<T> {
> >>>>>>>>
> >>>>>>>>   static class Waiter implements Comparable<Waiter> {
> >>>>>>>>
> >>>>>>>>     final int order;
> >>>>>>>>     final Condition condition;
> >>>>>>>>
> >>>>>>>>     Waiter(int order, Condition condition) {
> >>>>>>>>       this.order = order;
> >>>>>>>>       this.condition = condition;
> >>>>>>>>     }
> >>>>>>>>
> >>>>>>>>     @Override
> >>>>>>>>     public int compareTo(Waiter waiter) {
> >>>>>>>>       return order - waiter.order;
> >>>>>>>>     }
> >>>>>>>>   }
> >>>>>>>>
> >>>>>>>>   final Lock lock = new ReentrantLock();
> >>>>>>>>   final Queue<Waiter> queue = new PriorityQueue<>();
> >>>>>>>>   int nextOrder; // 0 is next
> >>>>>>>>
> >>>>>>>>   public T execCallableInOrder(int order, Callable<T> callable)
> >>>>>>>> throws Exception {
> >>>>>>>>     assert order >= 0;
> >>>>>>>>     awaitTurn(order);
> >>>>>>>>     try {
> >>>>>>>>       return callable.call();
> >>>>>>>>     } finally {
> >>>>>>>>       signalNext(order + 1);
> >>>>>>>>     }
> >>>>>>>>   }
> >>>>>>>>
> >>>>>>>>   void awaitTurn(int order) {
> >>>>>>>>     lock.lock();
> >>>>>>>>     try {
> >>>>>>>>       Condition condition = null;
> >>>>>>>>       while (nextOrder != order) {
> >>>>>>>>         if (condition == null) {
> >>>>>>>>           condition = lock.newCondition();
> >>>>>>>>           queue.add(new Waiter(order, condition));
> >>>>>>>>         }
> >>>>>>>>         condition.awaitUninterruptibly();
> >>>>>>>>       }
> >>>>>>>>     } finally {
> >>>>>>>>       lock.unlock();
> >>>>>>>>     }
> >>>>>>>>   }
> >>>>>>>>
> >>>>>>>>   void signalNext(int nextOrder) {
> >>>>>>>>     lock.lock();
> >>>>>>>>     try {
> >>>>>>>>       this.nextOrder = nextOrder;
> >>>>>>>>       Waiter waiter = queue.peek();
> >>>>>>>>       if (waiter != null && waiter.order == nextOrder) {
> >>>>>>>>         queue.remove();
> >>>>>>>>         waiter.condition.signal();
> >>>>>>>>       }
> >>>>>>>>     } finally {
> >>>>>>>>       lock.unlock();
> >>>>>>>>     }
> >>>>>>>>   }
> >>>>>>>> }
> >>>>>>>>
> >>>>>>>> On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer
> >>>>>>>> <joe.bowbeer at gmail.com> wrote:
> >>>>>>>>>
> >>>>>>>>> Suman,
> >>>>>>>>>
> >>>>>>>>> I would advise against using notify/wait.  It raises a red flag
> for
> >>>>>>>>> a lot of reviewers, including me.
> >>>>>>>>>
> >>>>>>>>> The problems I see in this implementation are:
> >>>>>>>>>
> >>>>>>>>> 1. Pre-allocation of locks is prohibited by (revised) problem
> >>>>>>>>> statement.
> >>>>>>>>>
> >>>>>>>>> Note that if pre-allocation were allowed, then an array would be
> >>>>>>>>> more efficient than a Map.
> >>>>>>>>>
> >>>>>>>>> 2. Access to currentAllowedOrder is not thread-safe but should
> be.
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <
> suman_krec at yahoo.com>
> >>>>>>>>> wrote:
> >>>>>>>>>>
> >>>>>>>>>> I have modified my solution to avoid notifyAll(). Let me know
> your
> >>>>>>>>>> feedback.
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> public class OrderedExecutor
> >>>>>>>>>> {
> >>>>>>>>>> private int maxOrder;
> >>>>>>>>>> private int currentAllowedOrder;
> >>>>>>>>>> private Map<Integer, Object> map = new HashMap<Integer,
> Object>();
> >>>>>>>>>>     public OrderedExecutor(int n)
> >>>>>>>>>>     {
> >>>>>>>>>>          this.maxOrder = n;
> >>>>>>>>>>          this.currentAllowedOrder = 0;
> >>>>>>>>>>     for(int i = 0 ; i < this.maxOrder ; i++)
> >>>>>>>>>>     {
> >>>>>>>>>>     map.put(i, new Object());
> >>>>>>>>>>     }
> >>>>>>>>>>     }
> >>>>>>>>>>
> >>>>>>>>>>     public  Object execCriticalSectionInOrder(int order,
> >>>>>>>>>>                                               Callable<Object>
> >>>>>>>>>> callable)
> >>>>>>>>>>                                               throws Exception
> >>>>>>>>>>     {
> >>>>>>>>>> if (order >= this.maxOrder)
> >>>>>>>>>> {
> >>>>>>>>>> throw new Exception("Exceeds maximum order "+ maxOrder);
> >>>>>>>>>> }
> >>>>>>>>>>
> >>>>>>>>>> while(order != currentAllowedOrder)
> >>>>>>>>>> {
> >>>>>>>>>> synchronized (this.map.get(order))
> >>>>>>>>>> {
> >>>>>>>>>> this.map.get(order).wait();
> >>>>>>>>>> }
> >>>>>>>>>> }
> >>>>>>>>>> try
> >>>>>>>>>> {
> >>>>>>>>>> return callable.call();
> >>>>>>>>>> }
> >>>>>>>>>>        finally
> >>>>>>>>>> {
> >>>>>>>>>>             currentAllowedOrder = currentAllowedOrder+1;
> >>>>>>>>>> synchronized (this.map.get(order+1))
> >>>>>>>>>>             {
> >>>>>>>>>>                 this.map.get(order+1).notify();
> >>>>>>>>>>             }
> >>>>>>>>>>        }
> >>>>>>>>>>     }
> >>>>>>>>>> }
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> ________________________________
> >>>>>>>>>> From: Joe Bowbeer <joe.bowbeer at gmail.com>
> >>>>>>>>>> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> >>>>>>>>>> Sent: Friday, December 19, 2014 3:33 AM
> >>>>>>>>>>
> >>>>>>>>>> Subject: Re: [concurrency-interest] Enforcing ordered execution
> of
> >>>>>>>>>> critical sections?
> >>>>>>>>>>
> >>>>>>>>>> That would be "Tom" Cargill; link to paper:
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer
> >>>>>>>>>> <joe.bowbeer at gmail.com> wrote:
> >>>>>>>>>>
> >>>>>>>>>> I frown on use of notify[All]/wait because they make the code
> hard
> >>>>>>>>>> to maintain.
> >>>>>>>>>>
> >>>>>>>>>> In this case, with potentially lots of waiting threads, I would
> >>>>>>>>>> check out the "Specific Notification" pattern if I were
> determined to go the
> >>>>>>>>>> wait/notify route:
> >>>>>>>>>>
> >>>>>>>>>> Tim Cargill's paper is dated but still worth reading.
> >>>>>>>>>>
> >>>>>>>>>> Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ
> >>>>>>>>>> and Peter Haggar's article:
> >>>>>>>>>>
> >>>>>>>>>> http://www.ibm.com/developerworks/java/library/j-spnotif.html
> >>>>>>>>>>
> >>>>>>>>>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko
> >>>>>>>>>> <oleksandr.otenko at oracle.com> wrote:
> >>>>>>>>>>
> >>>>>>>>>> Yes, no one said it is a good idea to always do that. When it is
> >>>>>>>>>> contended, most of the threads will wake up to only go back to
> sleep.
> >>>>>>>>>>
> >>>>>>>>>> The pattern you are after is usually called sequencer. You can
> see
> >>>>>>>>>> it used in TCP. I am not sure why it wasn't implemented in
> j.u.c. - maybe
> >>>>>>>>>> not that popular.
> >>>>>>>>>>
> >>>>>>>>>> The best solution will be lock-like, but the waiter nodes will
> >>>>>>>>>> contain the value they are waiting for - so only the specific
> threads get
> >>>>>>>>>> woken up. The solution with concurrent map is very similar,
> only with larger
> >>>>>>>>>> overhead from storing the index the thread is waiting for.
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> Alex
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> On 18/12/2014 20:21, Hanson Char wrote:
> >>>>>>>>>>
> >>>>>>>>>> Less overhead and simpler are a nice properties, even though at
> >>>>>>>>>> the expense of having to wake up all waiting threads just to
> find out the
> >>>>>>>>>> one with the right order to execute.  Still, this seems like a
> good
> >>>>>>>>>> tradeoff.
> >>>>>>>>>>
> >>>>>>>>>> Thanks,
> >>>>>>>>>> Hanson
> >>>>>>>>>>
> >>>>>>>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart
> >>>>>>>>>> <peter.levart at gmail.com> wrote:
> >>>>>>>>>>
> >>>>>>>>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
> >>>>>>>>>>
> >>>>>>>>>> No, there is no difference. Peter didn't spot your entire method
> >>>>>>>>>> is synchronized, so spurious wakeup won't make progress until
> the owner of
> >>>>>>>>>> the lock exits the method.
> >>>>>>>>>>
> >>>>>>>>>> You could split the synchronization into two blocks - one
> >>>>>>>>>> encompassing the wait loop, the other in the finally block; but
> it may make
> >>>>>>>>>> no difference.
> >>>>>>>>>>
> >>>>>>>>>> Alex
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> You're right, Alex. I'm so infected with park/unpark virus that
> I
> >>>>>>>>>> missed that ;-)
> >>>>>>>>>>
> >>>>>>>>>> Peter
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> On 17/12/2014 18:36, suman shil wrote:
> >>>>>>>>>>
> >>>>>>>>>> Thanks peter for your reply. You are right. I should have
> >>>>>>>>>> incremented currentAllowedOrder in finally block.
> >>>>>>>>>>
> >>>>>>>>>> Suman
> >>>>>>>>>>
> >>>>>>>>>>
> ------------------------------------------------------------------------
> >>>>>>>>>> *From:* Peter Levart <peter.levart at gmail.com>
> >>>>>>>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko
> >>>>>>>>>> <oleksandr.otenko at oracle.com>; Concurrency-interest
> >>>>>>>>>> <concurrency-interest at cs.oswego.edu>
> >>>>>>>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
> >>>>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered
> execution
> >>>>>>>>>> of critical sections?
> >>>>>>>>>>
> >>>>>>>>>> On 12/17/2014 06:46 PM, suman shil wrote:
> >>>>>>>>>>
> >>>>>>>>>> Thanks for your response. Will notifyAll() instead of notify()
> >>>>>>>>>> solve the problem?
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> It will, but you should also account for "spurious" wake-ups.
> You
> >>>>>>>>>> should increment currentAllowedOrder only after return from
> callable.call
> >>>>>>>>>> (in finally block just before notifyAll()).
> >>>>>>>>>>
> >>>>>>>>>> Otherwise a nice solution - with minimal state, providing that
> not
> >>>>>>>>>> many threads meet at the same time...
> >>>>>>>>>>
> >>>>>>>>>> Regards, Peter
> >>>>>>>>>>
> >>>>>>>>>> RegardsSuman
> >>>>>>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com>
> >>>>>>>>>> <mailto:oleksandr.otenko at oracle.com>
> >>>>>>>>>>   To: suman shil<suman_krec at yahoo.com>
> >>>>>>>>>> <mailto:suman_krec at yahoo.com>;
> >>>>>>>>>> Concurrency-interest<concurrency-interest at cs.oswego.edu>
> >>>>>>>>>> <mailto:concurrency-interest at cs.oswego.edu>    Sent:
> Wednesday, December 17,
> >>>>>>>>>> 2014 9:55 PM
> >>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered
> execution
> >>>>>>>>>> of critical sections?
> >>>>>>>>>>       There is no guarantee you'll ever hand over the control to
> >>>>>>>>>> the right thread upon notify()
> >>>>>>>>>>     Alex
> >>>>>>>>>>
> >>>>>>>>>> On 17/12/2014 14:07, suman shil wrote:
> >>>>>>>>>>       Hi, Following is my solution to solve this problem. Please
> >>>>>>>>>> let me know if I am missing something.
> >>>>>>>>>>    public class OrderedExecutor {  private int
> currentAllowedOrder
> >>>>>>>>>> = 0;  private int maxLength = 0;  public OrderedExecutor(int
> n)  {
> >>>>>>>>>> this.maxLength = n;  } public synchronized Object
> >>>>>>>>>> execCriticalSectionInOrder( int order, Callable<Object>
> callable)
> >>>>>>>>>> throws Exception  { if (order >= maxLength)  {  throw new
> Exception("Exceeds
> >>>>>>>>>> maximum order "+ maxLength);  }    while(order !=
> currentAllowedOrder)  {
> >>>>>>>>>> wait();  }    try  { currentAllowedOrder =
> currentAllowedOrder+1;  return
> >>>>>>>>>> callable.call();  }  finally  {  notify();  }  } }
> >>>>>>>>>>    Regards Suman
> >>>>>>>>>>        From: Peter Levart<peter.levart at gmail.com>
> >>>>>>>>>> <mailto:peter.levart at gmail.com>
> >>>>>>>>>>   To: Hanson Char<hanson.char at gmail.com>
> >>>>>>>>>> <mailto:hanson.char at gmail.com>    Cc:
> >>>>>>>>>> concurrency-interest<concurrency-interest at cs.oswego.edu>
> >>>>>>>>>> <mailto:concurrency-interest at cs.oswego.edu>    Sent: Sunday,
> December 14,
> >>>>>>>>>> 2014 11:01 PM
> >>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered
> execution
> >>>>>>>>>> of critical sections?
> >>>>>>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
> >>>>>>>>>>      Hi Peter,
> >>>>>>>>>>    Thanks for this proposed idea of using LockSupport. This begs
> >>>>>>>>>> the question: which one would you choose if you had all three
> (correct)
> >>>>>>>>>> implementation available?  (Semaphore, CountDownLatch, or
> LockSupport)?
> >>>>>>>>>>    Regards, Hanson
> >>>>>>>>>>     The Semaphore/CountDownLatch variants are equivalent if you
> >>>>>>>>>> don't need re-use. So any would do. They lack invalid-use
> detection. What
> >>>>>>>>>> happens if they are not used as intended? Semaphore variant
> acts differently
> >>>>>>>>>> than CountDownLatch variant. The low-level variant I  proposed
> detects
> >>>>>>>>>> invalid usage. So I would probably use this one. But the low
> level variant
> >>>>>>>>>> is harder to reason about it's correctness. I think it is
> correct, but you
> >>>>>>>>>> should show it to somebody else to confirm this.
> >>>>>>>>>>     Another question is whether you actually need this kind of
> >>>>>>>>>> synchronizer. Maybe if you explained what you are trying to
> achieve,
> >>>>>>>>>> somebody could have an idea how to do that even more
> elegantly...
> >>>>>>>>>>     Regards, Peter
> >>>>>>>>>>              On Sun, Dec 14, 2014 at 9:01 AM, Peter
> >>>>>>>>>> Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>
> wrote:
> >>>>>>>>>>    Hi Hanson,
> >>>>>>>>>>     This one is more low-level, but catches some invalid usages
> >>>>>>>>>> and is more resource-friendly:
> >>>>>>>>>>       public class OrderedExecutor {
> >>>>>>>>>>         public <T> T execCriticalSectionInOrder(
> >>>>>>>>>>           final int order,
> >>>>>>>>>>           final Supplier<T> criticalSection
> >>>>>>>>>>       ) throws InterruptedException {
> >>>>>>>>>>           if (order < 0) {
> >>>>>>>>>>                throw new IllegalArgumentException("'order'
> should
> >>>>>>>>>> be >= 0");
> >>>>>>>>>>           }
> >>>>>>>>>>           if (order > 0) {
> >>>>>>>>>>               waitForDone(order - 1);
> >>>>>>>>>>           }
> >>>>>>>>>>           try {
> >>>>>>>>>>               return criticalSection.get();
> >>>>>>>>>>           } finally {
> >>>>>>>>>>               notifyDone(order);
> >>>>>>>>>>           }
> >>>>>>>>>>       }
> >>>>>>>>>>         private static final Object DONE = new Object();
> >>>>>>>>>>       private final ConcurrentMap<Integer, Object> signals = new
> >>>>>>>>>> ConcurrentHashMap<>();
> >>>>>>>>>>         private void waitForDone(int order) throws
> >>>>>>>>>> InterruptedException {
> >>>>>>>>>>           Object sig = signals.putIfAbsent(order,
> >>>>>>>>>> Thread.currentThread());
> >>>>>>>>>>           if (sig != null && sig != DONE) {
> >>>>>>>>>>               throw new IllegalStateException();
> >>>>>>>>>>           }
> >>>>>>>>>>           while (sig != DONE) {
> >>>>>>>>>>               LockSupport.park();
> >>>>>>>>>>               if (Thread.interrupted()) {
> >>>>>>>>>>                   throw new InterruptedException();
> >>>>>>>>>>               }
> >>>>>>>>>>               sig = signals.get(order);
> >>>>>>>>>>           }
> >>>>>>>>>>       }
> >>>>>>>>>>         private void notifyDone(int order) {
> >>>>>>>>>>           Object sig = signals.putIfAbsent(order, DONE);
> >>>>>>>>>>           if (sig instanceof Thread) {
> >>>>>>>>>>               if (!signals.replace(order, sig, DONE)) {
> >>>>>>>>>>                   throw new IllegalStateException();
> >>>>>>>>>>               }
> >>>>>>>>>>               LockSupport.unpark((Thread) sig);
> >>>>>>>>>>           } else if (sig != null) {
> >>>>>>>>>>               throw new IllegalStateException();
> >>>>>>>>>>           }
> >>>>>>>>>>       }
> >>>>>>>>>>   }
> >>>>>>>>>>       Regards, Peter
> >>>>>>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
> >>>>>>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
> >>>>>>>>>>      Hi Peter,
> >>>>>>>>>>    Thanks for the suggestion, and sorry about not being clear
> >>>>>>>>>> about one important  detail: "n" is not known a priori when
> constructing an
> >>>>>>>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is
> ruled out?
> >>>>>>>>>>     If you know at least the upper bound of 'n', it can be used
> >>>>>>>>>> with such 'n'. Otherwise something that dynamically re-sizes
> the array could
> >>>>>>>>>> be devised. Or you could simply use a ConcurrentHashMap instead
> of array
> >>>>>>>>>> where keys are 'order' values:
> >>>>>>>>>>       public class OrderedExecutor<T> {
> >>>>>>>>>>         private final ConcurrentMap<Integer, CountDownLatch>
> >>>>>>>>>> latches = new ConcurrentHashMap<>();
> >>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
> >>>>>>>>>>                                           final Supplier<T>
> >>>>>>>>>> criticalSection) throws InterruptedException {
> >>>>>>>>>>           if (order > 0) {
> >>>>>>>>>>               latches.computeIfAbsent(order - 1, o -> new
> >>>>>>>>>> CountDownLatch(1)).await();
> >>>>>>>>>>           }
> >>>>>>>>>>           try {
> >>>>>>>>>>               return criticalSection.get();
> >>>>>>>>>>           } finally {
> >>>>>>>>>>               latches.computeIfAbsent(order, o -> new
> >>>>>>>>>> CountDownLatch(1)).countDown();
> >>>>>>>>>>           }
> >>>>>>>>>>       }
> >>>>>>>>>>   }
> >>>>>>>>>>       Regards, Peter
> >>>>>>>>>>           You guessed right: it's a one-shot object for a
> >>>>>>>>>> particular OrderedExecutor  instance, and "order" must be
> called indeed at
> >>>>>>>>>> most once.
> >>>>>>>>>>    Regards, Hanson
> >>>>>>>>>>   On Sun, Dec 14, 2014 at 2:21 AM, Peter
> >>>>>>>>>> Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>
> wrote:
> >>>>>>>>>>    Hi Hanson,
> >>>>>>>>>>     I don't think anything like that readily exists  in
> >>>>>>>>>> java.lang.concurrent, but what you describe should be possible
> to  achieve
> >>>>>>>>>> with composition of existing primitives.  You haven't given any
> additional
> >>>>>>>>>> hints to what your OrderedExecutor  should behave like. Should
> it be a
> >>>>>>>>>> one-shot object (like CountDownLatch) or a re-usable one (like
> >>>>>>>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a
> particular
> >>>>>>>>>> OrderedExecutor instance and 'order' value be called at most
> once? If yes
> >>>>>>>>>> (and I think that only a one-shot object  makes sense here), an
> array of
> >>>>>>>>>> CountDownLatch(es) could be used:
> >>>>>>>>>>     public class OrderedExecutor<T> {
> >>>>>>>>>>       private final CountDownLatch[] latches;
> >>>>>>>>>>         public OrderedExecutor(int n) {
> >>>>>>>>>>           if (n < 1) throw new IllegalArgumentException("'n'
> >>>>>>>>>> should be >= 1");
> >>>>>>>>>>           latches = new CountDownLatch[n - 1];
> >>>>>>>>>>           for (int i = 0; i < latches.length; i++) {
> >>>>>>>>>>               latches[i] = new CountDownLatch(1);
> >>>>>>>>>>           }
> >>>>>>>>>>       }
> >>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
> >>>>>>>>>>                                            final Supplier<T>
> >>>>>>>>>> criticalSection) throws InterruptedException {
> >>>>>>>>>>           if (order < 0 || order > latches.length)
> >>>>>>>>>>               throw new IllegalArgumentException("'order' should
> >>>>>>>>>> be [0..." +  latches.length + "]");
> >>>>>>>>>>           if (order > 0) {
> >>>>>>>>>>               latches[order - 1].await();
> >>>>>>>>>>           }
> >>>>>>>>>>           try {
> >>>>>>>>>>               return criticalSection.get();
> >>>>>>>>>>           } finally {
> >>>>>>>>>>               if (order < latches.length) {
> >>>>>>>>>>                   latches[order].countDown();
> >>>>>>>>>>               }
> >>>>>>>>>>           }
> >>>>>>>>>>       }
> >>>>>>>>>>   }
> >>>>>>>>>>       Regards, Peter
> >>>>>>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
> >>>>>>>>>>         Hi, I am looking for a construct that can  be used to
> >>>>>>>>>> efficiently enforce  ordered execution of multiple critical
> sections, each
> >>>>>>>>>> calling from a  different thread. The calling threads may run
> in  parallel
> >>>>>>>>>> and may call the execution method out of order. The  perceived
> construct
> >>>>>>>>>> would therefore be responsible for re-ordering the execution of
> those
> >>>>>>>>>> threads, so that their critical  sections (and only the
> critical section)
> >>>>>>>>>> will be executed in order. Would something  like the following
> API already
> >>>>>>>>>> exist? /** * Used to enforce ordered execution of critical
> sections calling
> >>>>>>>>>> from multiple *  threads, parking and unparking the  threads as
> necessary.
> >>>>>>>>>> */ public class OrderedExecutor<T> { /** * Executes a critical
> section at
> >>>>>>>>>> most once with the given order, parking * and  unparking the
> current thread
> >>>>>>>>>> as  necessary so that all critical * sections executed  by
> different threads
> >>>>>>>>>> using this  executor take place in * the order from 1 to n
> consecutively.
> >>>>>>>>>> */ public T execCriticalSectionInOrder
> >>>>>>>>>> (  final int order, final Callable<T> criticalSection) throws
> >>>>>>>>>> InterruptedException; } Regards, Hanson
> >>>>>>>>>>
> _______________________________________________Concurrency-interest mailing
> >>>>>>>>>> listConcurrency-interest at cs.oswego.edu
> >>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>>>>> _______________________________________________
> >>>>>>>>>>   Concurrency-interest mailing list
> >>>>>>>>>>   Concurrency-interest at cs.oswego.edu
> >>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
> >>>>>>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>>>>> _______________________________________________
> >>>>>>>>>> Concurrency-interest mailing list
> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
> >>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> _______________________________________________
> >>>>>>>>>> Concurrency-interest mailing list
> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
> >>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> _______________________________________________
> >>>>>>>>>> Concurrency-interest mailing list
> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> _______________________________________________
> >>>>>>>>>> Concurrency-interest mailing list
> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> _______________________________________________
> >>>>>>>>>> Concurrency-interest mailing list
> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> _______________________________________________
> >>>>>>>>>> Concurrency-interest mailing list
> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>>>>>
> >>>>>>>>
> >>>>>>>> _______________________________________________
> >>>>>>>> Concurrency-interest mailing list
> >>>>>>>> Concurrency-interest at cs.oswego.edu
> >>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>>>
> >>>>>>>
> >>>>>
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>>
> >>
> >>
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/d89eca75/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: OrderedExecutor.java
Type: application/octet-stream
Size: 1052 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/d89eca75/attachment-0002.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: OrderedExecutorTest.java
Type: application/octet-stream
Size: 3486 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/d89eca75/attachment-0003.obj>

From martinrb at google.com  Mon Dec 22 02:13:32 2014
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 21 Dec 2014 23:13:32 -0800
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
Message-ID: <CA+kOe0_iyzavsMozRGNK07jJf0Nk7SszyU_vvvg-Y825rP7-Nw@mail.gmail.com>

It seems reasonable for the user of CLQ to retain a "handle" to the
Node inserted into a CLQ, so that remove or contains can be O(1).  An
Iterator is vaguely close to such a handle (but you don't need to be
able to iterate).  You could experiment with adding such a thing to
CLQ - an insert method that returns a wrapper around the internal
Node.  Latest version in jsr166 CVS.

The remove method would look something like:

          return casItem(p, item, null);

You can't physically unlink the Node because you don't have access to
the predecessor Node, as remove(Object) does ... so garbage deleted
Nodes might build up.  But you can skip over any deleted successor
Nodes, somewhat mitigating this problem.

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java?view=markup

NodeList seems vaguely relevant.
http://code.google.com/p/guava-libraries/issues/detail?id=1285

On Sun, Dec 21, 2014 at 6:35 PM, Luke Sandberg <lukeisandberg at gmail.com> wrote:
> I have come across a few situations where i am looking for a datastructure
> and i feel like i keep coming up short.
>
> The situation is analogous to the 'waiters' list in
> java.util.concurrent.FutureTask.
>
> Basically, I need to be able to publish a non-null object reference into a
> data structure where
> * order doesn't matter (insertion order would be nice, but i don't care that
> much)
> * add and remove identity semantics
> * concurrent iteration (weakly consistent is fine)
> * add/remove are non-blocking
>
> CLQ is an obvious choice but removal is O(n), another choice would a simple
> synchronized identity hash set which is fine but the lock + high entry
> overhead is a deal breaker.
>
> An AtomicReferenceArray would be super easy, but i can't put a bound on the
> number of items.
>
> Also, it would be fine for the code that adds items, to store additional
> state (an index, a 'Node' reference), in order to facilitate removal.
>
> The best thing i have seen from looking around appears to be something like
> what FutureTask does to implement 'awaitDone', but even there
> removeWaitier() is O(n).  that seems like a pretty good compromise when
> lists are short, but what would be a better solution when lists are long?
>
> Just to motivate this a little bit, the two cases I am looking at in
> particular are:
>
> * maintaining a set of threads associated with a server 'request' (threads
> enter/exit when they start executing tasks associated with the request)
> * maintaining a set of futures to be cancelled (futures are removed when
> they complete to avoid pinning their referents).
>
> Any pointers or ideas?
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From joe.bowbeer at gmail.com  Mon Dec 22 02:31:59 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 21 Dec 2014 23:31:59 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CABWgujYjpkgdcJWW5wriiZDBugvNNBWfbpfob95uXb=dsdmyCQ@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
	<5496B0FE.7070502@gmail.com>
	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
	<549737AB.8060508@gmail.com>
	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
	<CA+kOe0-OX_h1oJ5mZnSh8WNxoOgpNo9P1kbe7fVnbWqg973HSA@mail.gmail.com>
	<CABWgujYjpkgdcJWW5wriiZDBugvNNBWfbpfob95uXb=dsdmyCQ@mail.gmail.com>
Message-ID: <CAHzJPEqoY7JYP8pETNVJOiDQ8kd7tsWkVd=ZwOPTfbg0mOSuGg@mail.gmail.com>

I pushed some test code to:

https://bitbucket.org/joebowbeer/semaphorebug


On Sun, Dec 21, 2014 at 10:15 PM, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi Martin,
>
> Please find attached a modified version Joe's OrderedExecutor.java which
> assumes the order to start at 1 (rather than zero), and an
> OrderedExecutorTest.java that can be used to sanity check the correct
> behavior.
>
> Hope this helps.
>
> Regards,
> Hanson
>
>
> On Sun, Dec 21, 2014 at 9:57 PM, Martin Buchholz <martinrb at google.com>
> wrote:
>
>> It looks to me as well like a bug in Semaphore or AQS.
>> Where can we find the complete test program so we can debug?
>>
>> On Sun, Dec 21, 2014 at 4:25 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>> wrote:
>> > Below is the most concise implementation I can imagine, using a single
>> > Semaphore, which is legal AFAICT according to the javadoc, but which
>> > deadlocks in my tests.
>> >
>> > class OrderedExecutor<T> {
>> >
>> >   private final Semaphore semaphore = new Semaphore(1);
>> >
>> >   @Override
>> >   public T execCallableInOrder(int order, Callable<T> callable)
>> >       throws InterruptedException, Exception {
>> >     assert order >= 0;
>> >     int acquires = order + 1;
>> >     semaphore.acquire(acquires);
>> >     try {
>> >       return callable.call();
>> >     } finally {
>> >       semaphore.release(acquires + 1);
>> >     }
>> >   }
>> > }
>> >
>> >
>> > testOrderedExecutorSynchronizer
>> > Waiting 2
>> > Waiting 1
>> > Waiting 8
>> > Waiting 7
>> > Waiting 9
>> > Waiting 0
>> > Executing 0
>> > Executing 1
>> > Executing 2
>> > Waiting 6
>> > Waiting 4
>> > Waiting 3
>> > Waiting 5
>> > Executing 3
>> > ... crickets ...
>> >
>> >
>> > A bug?
>> >
>> > On Sun, Dec 21, 2014 at 1:12 PM, Peter Levart <peter.levart at gmail.com>
>> > wrote:
>> >>
>> >>
>> >> On 12/21/2014 07:03 PM, Hanson Char wrote:
>> >>
>> >> Interesting - now that each order is associated with a different object
>> >> monitor, the wait/notifyAll would at most wake up one thread instead
>> of all.
>> >> In fact, it seems the use of notify (instead of notifyAll) would
>> suffice.
>> >> Simple and clever, but it does require the "stripeSize" to be known a
>> >> priori.
>> >>
>> >>
>> >> Not exactly. Each object monitor is associated with multiple orders.
>> For
>> >> example, if stripeSize is 4, the 1st object monitor is associated with
>> >> orders: 0, 4, 8, 12, 16, ... the 2nd with orders 1, 5, 9, 13, 17, ...
>> etc.
>> >>
>> >> But it also means that each object monitor is associated statistically
>> >> with just N/stripeSize threads that wait instead of N. So notifyAll is
>> still
>> >> needed, but it wakes up less threads - how much less depends on chosen
>> >> stripeSize.
>> >>
>> >> Regards, Peter
>> >>
>> >>
>> >> Regards,
>> >> Hanson
>> >>
>> >> On Sun, Dec 21, 2014 at 3:37 AM, Peter Levart <peter.levart at gmail.com>
>> >> wrote:
>> >>>
>> >>> Hi,
>> >>>
>> >>> Here's a simple implementation (based on Suman Shil's idea) that
>> stripes
>> >>> the waiting threads into multiple buckets:
>> >>>
>> >>> public class StripedOrderedExecutor {
>> >>>
>> >>>     private final MeetingPoint[] meetingPoints;
>> >>>
>> >>>     public StripedOrderedExecutor(int stripeSize) {
>> >>>         assert stripeSize > 0;
>> >>>         meetingPoints = new MeetingPoint[stripeSize];
>> >>>         for (int i = 0; i < stripeSize; i++) {
>> >>>             meetingPoints[i] = new MeetingPoint();
>> >>>         }
>> >>>     }
>> >>>
>> >>>     public <T> T execCriticalSectionInOrder(
>> >>>         final int order,
>> >>>         final Supplier<T> criticalSection
>> >>>     ) throws InterruptedException {
>> >>>         assert order >= 0;
>> >>>
>> >>>         meetingPoints[order %
>> meetingPoints.length].waitForGreen(order);
>> >>>         try {
>> >>>             return criticalSection.get();
>> >>>         } finally {
>> >>>             meetingPoints[(order + 1) %
>> >>> meetingPoints.length].notifyGreen(order + 1);
>> >>>         }
>> >>>     }
>> >>>
>> >>>     private static class MeetingPoint {
>> >>>         private int lastGreen;
>> >>>
>> >>>         synchronized void waitForGreen(int order) throws
>> >>> InterruptedException {
>> >>>             while (lastGreen != order) {
>> >>>                 wait();
>> >>>             }
>> >>>         }
>> >>>
>> >>>         synchronized void notifyGreen(int order) {
>> >>>             lastGreen = order;
>> >>>             notifyAll();
>> >>>         }
>> >>>     }
>> >>> }
>> >>>
>> >>>
>> >>> Regards, Peter
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>> On 12/21/2014 02:35 AM, Hanson Char wrote:
>> >>>
>> >>> Thanks, Joe. That's an interesting point about the long term memory
>> >>> impact of TreeMap vs HashMap - similar to LinkedList vs ArrayList.
>> >>>
>> >>> Regards,
>> >>> Hanson
>> >>>
>> >>> On Sat, Dec 20, 2014 at 5:17 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
>> >>> wrote:
>> >>>>
>> >>>> I see.  Yes, that addresses the removal problem, though note that
>> >>>> HashMap does not shrink when data is removed. Even if all keys are
>> removed
>> >>>> from HashMap, the inner size of its table does not change.
>> >>>>
>> >>>> So in this case maybe I would choose TreeMap as the implementation.
>> Its
>> >>>> remove() method *does* remove all the memory associated with an
>> entry.
>> >>>>
>> >>>> In general, I don't like Map<Integer, *> because it seems like a
>> hack,
>> >>>> but really in this case it's a small trade-off.  Map is more concise.
>> >>>> PriorityQueue is a little clearer.
>> >>>>
>> >>>> On Sat, Dec 20, 2014 at 4:50 PM, Hanson Char <hanson.char at gmail.com>
>> >>>> wrote:
>> >>>>>
>> >>>>> Hi Joe,
>> >>>>>
>> >>>>> >A Queue should also retain less memory. A Map will tend to
>> accumulate
>> >>>>> > both Entry and Integer instances over time.
>> >>>>>
>> >>>>> Using a Map, the signalNext would look something like below.
>> Doesn't
>> >>>>> each entry get eventually removed from the map?  Why would they
>> accumulate
>> >>>>> over time?
>> >>>>>
>> >>>>> Regards,
>> >>>>> Hanson
>> >>>>>
>> >>>>>     void signalNext(final int nextOrder) {
>> >>>>>       lock.lock();
>> >>>>>       try {
>> >>>>>         this.nextOrder = nextOrder;
>> >>>>>         Condition cond = map.remove(nextOrder);
>> >>>>>         if (cond != null) {
>> >>>>>             cond.signal();
>> >>>>>         }
>> >>>>>       } finally {
>> >>>>>         lock.unlock();
>> >>>>>       }
>> >>>>>     }
>> >>>>>   }
>> >>>>>
>> >>>>>
>> >>>>> On Sat, Dec 20, 2014 at 4:39 PM, Joe Bowbeer <joe.bowbeer at gmail.com
>> >
>> >>>>> wrote:
>> >>>>>>
>> >>>>>> I think PriorityQueue more clearly represents the function than Map
>> >>>>>> does in this case.
>> >>>>>>
>> >>>>>> A Queue should also retain less memory. A Map will tend to
>> accumulate
>> >>>>>> both Entry and Integer instances over time.
>> >>>>>>
>> >>>>>> The ConcurrentMap does have the advantage of lock striping, which
>> >>>>>> might be more performant if there is high contention.
>> >>>>>>
>> >>>>>> I think this is an interesting problem because there are quite a
>> few
>> >>>>>> different solutions, each with advantages and disadvantages in
>> terms of
>> >>>>>> performance and maintainability.
>> >>>>>>
>> >>>>>> If there were a PriorityQueue version of
>> AbstractQueuedSynchronizer,
>> >>>>>> then we could have yet another solution using the AQS's state to
>> represent
>> >>>>>> the next available order.
>> >>>>>>
>> >>>>>> On Sat, Dec 20, 2014 at 3:59 PM, Hanson Char <
>> hanson.char at gmail.com>
>> >>>>>> wrote:
>> >>>>>>>
>> >>>>>>> This one looks interesting and educational.  The use of condition
>> >>>>>>> enables us to suspend and resume specifically targeted threads
>> without
>> >>>>>>> resorting to the use of LockSupport.{part, unpark}.  Instead of
>> using a
>> >>>>>>> PriorityQueue, perhaps we can use a simple Map<Integer,
>> Condition> instead
>> >>>>>>> so we can get away without the extra Waiter inner class?
>> >>>>>>>
>> >>>>>>> Regards,
>> >>>>>>> Hanson
>> >>>>>>>
>> >>>>>>> On Sat, Dec 20, 2014 at 3:12 PM, Joe Bowbeer <
>> joe.bowbeer at gmail.com>
>> >>>>>>> wrote:
>> >>>>>>>>
>> >>>>>>>> To round out the selection, here's an implementation using a
>> >>>>>>>> PriorityQueue of Conditions:
>> >>>>>>>>
>> >>>>>>>> class OrderedExecutor<T> {
>> >>>>>>>>
>> >>>>>>>>   static class Waiter implements Comparable<Waiter> {
>> >>>>>>>>
>> >>>>>>>>     final int order;
>> >>>>>>>>     final Condition condition;
>> >>>>>>>>
>> >>>>>>>>     Waiter(int order, Condition condition) {
>> >>>>>>>>       this.order = order;
>> >>>>>>>>       this.condition = condition;
>> >>>>>>>>     }
>> >>>>>>>>
>> >>>>>>>>     @Override
>> >>>>>>>>     public int compareTo(Waiter waiter) {
>> >>>>>>>>       return order - waiter.order;
>> >>>>>>>>     }
>> >>>>>>>>   }
>> >>>>>>>>
>> >>>>>>>>   final Lock lock = new ReentrantLock();
>> >>>>>>>>   final Queue<Waiter> queue = new PriorityQueue<>();
>> >>>>>>>>   int nextOrder; // 0 is next
>> >>>>>>>>
>> >>>>>>>>   public T execCallableInOrder(int order, Callable<T> callable)
>> >>>>>>>> throws Exception {
>> >>>>>>>>     assert order >= 0;
>> >>>>>>>>     awaitTurn(order);
>> >>>>>>>>     try {
>> >>>>>>>>       return callable.call();
>> >>>>>>>>     } finally {
>> >>>>>>>>       signalNext(order + 1);
>> >>>>>>>>     }
>> >>>>>>>>   }
>> >>>>>>>>
>> >>>>>>>>   void awaitTurn(int order) {
>> >>>>>>>>     lock.lock();
>> >>>>>>>>     try {
>> >>>>>>>>       Condition condition = null;
>> >>>>>>>>       while (nextOrder != order) {
>> >>>>>>>>         if (condition == null) {
>> >>>>>>>>           condition = lock.newCondition();
>> >>>>>>>>           queue.add(new Waiter(order, condition));
>> >>>>>>>>         }
>> >>>>>>>>         condition.awaitUninterruptibly();
>> >>>>>>>>       }
>> >>>>>>>>     } finally {
>> >>>>>>>>       lock.unlock();
>> >>>>>>>>     }
>> >>>>>>>>   }
>> >>>>>>>>
>> >>>>>>>>   void signalNext(int nextOrder) {
>> >>>>>>>>     lock.lock();
>> >>>>>>>>     try {
>> >>>>>>>>       this.nextOrder = nextOrder;
>> >>>>>>>>       Waiter waiter = queue.peek();
>> >>>>>>>>       if (waiter != null && waiter.order == nextOrder) {
>> >>>>>>>>         queue.remove();
>> >>>>>>>>         waiter.condition.signal();
>> >>>>>>>>       }
>> >>>>>>>>     } finally {
>> >>>>>>>>       lock.unlock();
>> >>>>>>>>     }
>> >>>>>>>>   }
>> >>>>>>>> }
>> >>>>>>>>
>> >>>>>>>> On Sat, Dec 20, 2014 at 11:21 AM, Joe Bowbeer
>> >>>>>>>> <joe.bowbeer at gmail.com> wrote:
>> >>>>>>>>>
>> >>>>>>>>> Suman,
>> >>>>>>>>>
>> >>>>>>>>> I would advise against using notify/wait.  It raises a red flag
>> for
>> >>>>>>>>> a lot of reviewers, including me.
>> >>>>>>>>>
>> >>>>>>>>> The problems I see in this implementation are:
>> >>>>>>>>>
>> >>>>>>>>> 1. Pre-allocation of locks is prohibited by (revised) problem
>> >>>>>>>>> statement.
>> >>>>>>>>>
>> >>>>>>>>> Note that if pre-allocation were allowed, then an array would be
>> >>>>>>>>> more efficient than a Map.
>> >>>>>>>>>
>> >>>>>>>>> 2. Access to currentAllowedOrder is not thread-safe but should
>> be.
>> >>>>>>>>>
>> >>>>>>>>>
>> >>>>>>>>> On Sat, Dec 20, 2014 at 6:04 AM, suman shil <
>> suman_krec at yahoo.com>
>> >>>>>>>>> wrote:
>> >>>>>>>>>>
>> >>>>>>>>>> I have modified my solution to avoid notifyAll(). Let me know
>> your
>> >>>>>>>>>> feedback.
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> public class OrderedExecutor
>> >>>>>>>>>> {
>> >>>>>>>>>> private int maxOrder;
>> >>>>>>>>>> private int currentAllowedOrder;
>> >>>>>>>>>> private Map<Integer, Object> map = new HashMap<Integer,
>> Object>();
>> >>>>>>>>>>     public OrderedExecutor(int n)
>> >>>>>>>>>>     {
>> >>>>>>>>>>          this.maxOrder = n;
>> >>>>>>>>>>          this.currentAllowedOrder = 0;
>> >>>>>>>>>>     for(int i = 0 ; i < this.maxOrder ; i++)
>> >>>>>>>>>>     {
>> >>>>>>>>>>     map.put(i, new Object());
>> >>>>>>>>>>     }
>> >>>>>>>>>>     }
>> >>>>>>>>>>
>> >>>>>>>>>>     public  Object execCriticalSectionInOrder(int order,
>> >>>>>>>>>>                                               Callable<Object>
>> >>>>>>>>>> callable)
>> >>>>>>>>>>                                               throws Exception
>> >>>>>>>>>>     {
>> >>>>>>>>>> if (order >= this.maxOrder)
>> >>>>>>>>>> {
>> >>>>>>>>>> throw new Exception("Exceeds maximum order "+ maxOrder);
>> >>>>>>>>>> }
>> >>>>>>>>>>
>> >>>>>>>>>> while(order != currentAllowedOrder)
>> >>>>>>>>>> {
>> >>>>>>>>>> synchronized (this.map.get(order))
>> >>>>>>>>>> {
>> >>>>>>>>>> this.map.get(order).wait();
>> >>>>>>>>>> }
>> >>>>>>>>>> }
>> >>>>>>>>>> try
>> >>>>>>>>>> {
>> >>>>>>>>>> return callable.call();
>> >>>>>>>>>> }
>> >>>>>>>>>>        finally
>> >>>>>>>>>> {
>> >>>>>>>>>>             currentAllowedOrder = currentAllowedOrder+1;
>> >>>>>>>>>> synchronized (this.map.get(order+1))
>> >>>>>>>>>>             {
>> >>>>>>>>>>                 this.map.get(order+1).notify();
>> >>>>>>>>>>             }
>> >>>>>>>>>>        }
>> >>>>>>>>>>     }
>> >>>>>>>>>> }
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> ________________________________
>> >>>>>>>>>> From: Joe Bowbeer <joe.bowbeer at gmail.com>
>> >>>>>>>>>> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
>> >>>>>>>>>> Sent: Friday, December 19, 2014 3:33 AM
>> >>>>>>>>>>
>> >>>>>>>>>> Subject: Re: [concurrency-interest] Enforcing ordered
>> execution of
>> >>>>>>>>>> critical sections?
>> >>>>>>>>>>
>> >>>>>>>>>> That would be "Tom" Cargill; link to paper:
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> http://www.profcon.com/profcon/cargill/jgf/9809/SpecificNotification.html
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> On Thu, Dec 18, 2014 at 1:32 PM, Joe Bowbeer
>> >>>>>>>>>> <joe.bowbeer at gmail.com> wrote:
>> >>>>>>>>>>
>> >>>>>>>>>> I frown on use of notify[All]/wait because they make the code
>> hard
>> >>>>>>>>>> to maintain.
>> >>>>>>>>>>
>> >>>>>>>>>> In this case, with potentially lots of waiting threads, I would
>> >>>>>>>>>> check out the "Specific Notification" pattern if I were
>> determined to go the
>> >>>>>>>>>> wait/notify route:
>> >>>>>>>>>>
>> >>>>>>>>>> Tim Cargill's paper is dated but still worth reading.
>> >>>>>>>>>>
>> >>>>>>>>>> Also see chapter 3.7.3 Specific Notifications in Doug Lea's
>> CPiJ
>> >>>>>>>>>> and Peter Haggar's article:
>> >>>>>>>>>>
>> >>>>>>>>>> http://www.ibm.com/developerworks/java/library/j-spnotif.html
>> >>>>>>>>>>
>> >>>>>>>>>> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko
>> >>>>>>>>>> <oleksandr.otenko at oracle.com> wrote:
>> >>>>>>>>>>
>> >>>>>>>>>> Yes, no one said it is a good idea to always do that. When it
>> is
>> >>>>>>>>>> contended, most of the threads will wake up to only go back to
>> sleep.
>> >>>>>>>>>>
>> >>>>>>>>>> The pattern you are after is usually called sequencer. You can
>> see
>> >>>>>>>>>> it used in TCP. I am not sure why it wasn't implemented in
>> j.u.c. - maybe
>> >>>>>>>>>> not that popular.
>> >>>>>>>>>>
>> >>>>>>>>>> The best solution will be lock-like, but the waiter nodes will
>> >>>>>>>>>> contain the value they are waiting for - so only the specific
>> threads get
>> >>>>>>>>>> woken up. The solution with concurrent map is very similar,
>> only with larger
>> >>>>>>>>>> overhead from storing the index the thread is waiting for.
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> Alex
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> On 18/12/2014 20:21, Hanson Char wrote:
>> >>>>>>>>>>
>> >>>>>>>>>> Less overhead and simpler are a nice properties, even though at
>> >>>>>>>>>> the expense of having to wake up all waiting threads just to
>> find out the
>> >>>>>>>>>> one with the right order to execute.  Still, this seems like a
>> good
>> >>>>>>>>>> tradeoff.
>> >>>>>>>>>>
>> >>>>>>>>>> Thanks,
>> >>>>>>>>>> Hanson
>> >>>>>>>>>>
>> >>>>>>>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart
>> >>>>>>>>>> <peter.levart at gmail.com> wrote:
>> >>>>>>>>>>
>> >>>>>>>>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>> >>>>>>>>>>
>> >>>>>>>>>> No, there is no difference. Peter didn't spot your entire
>> method
>> >>>>>>>>>> is synchronized, so spurious wakeup won't make progress until
>> the owner of
>> >>>>>>>>>> the lock exits the method.
>> >>>>>>>>>>
>> >>>>>>>>>> You could split the synchronization into two blocks - one
>> >>>>>>>>>> encompassing the wait loop, the other in the finally block;
>> but it may make
>> >>>>>>>>>> no difference.
>> >>>>>>>>>>
>> >>>>>>>>>> Alex
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> You're right, Alex. I'm so infected with park/unpark virus
>> that I
>> >>>>>>>>>> missed that ;-)
>> >>>>>>>>>>
>> >>>>>>>>>> Peter
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> On 17/12/2014 18:36, suman shil wrote:
>> >>>>>>>>>>
>> >>>>>>>>>> Thanks peter for your reply. You are right. I should have
>> >>>>>>>>>> incremented currentAllowedOrder in finally block.
>> >>>>>>>>>>
>> >>>>>>>>>> Suman
>> >>>>>>>>>>
>> >>>>>>>>>>
>> ------------------------------------------------------------------------
>> >>>>>>>>>> *From:* Peter Levart <peter.levart at gmail.com>
>> >>>>>>>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko
>> >>>>>>>>>> <oleksandr.otenko at oracle.com>; Concurrency-interest
>> >>>>>>>>>> <concurrency-interest at cs.oswego.edu>
>> >>>>>>>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>> >>>>>>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered
>> execution
>> >>>>>>>>>> of critical sections?
>> >>>>>>>>>>
>> >>>>>>>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>> >>>>>>>>>>
>> >>>>>>>>>> Thanks for your response. Will notifyAll() instead of notify()
>> >>>>>>>>>> solve the problem?
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> It will, but you should also account for "spurious" wake-ups.
>> You
>> >>>>>>>>>> should increment currentAllowedOrder only after return from
>> callable.call
>> >>>>>>>>>> (in finally block just before notifyAll()).
>> >>>>>>>>>>
>> >>>>>>>>>> Otherwise a nice solution - with minimal state, providing that
>> not
>> >>>>>>>>>> many threads meet at the same time...
>> >>>>>>>>>>
>> >>>>>>>>>> Regards, Peter
>> >>>>>>>>>>
>> >>>>>>>>>> RegardsSuman
>> >>>>>>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com>
>> >>>>>>>>>> <mailto:oleksandr.otenko at oracle.com>
>> >>>>>>>>>>   To: suman shil<suman_krec at yahoo.com>
>> >>>>>>>>>> <mailto:suman_krec at yahoo.com>;
>> >>>>>>>>>> Concurrency-interest<concurrency-interest at cs.oswego.edu>
>> >>>>>>>>>> <mailto:concurrency-interest at cs.oswego.edu>    Sent:
>> Wednesday, December 17,
>> >>>>>>>>>> 2014 9:55 PM
>> >>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered
>> execution
>> >>>>>>>>>> of critical sections?
>> >>>>>>>>>>       There is no guarantee you'll ever hand over the control
>> to
>> >>>>>>>>>> the right thread upon notify()
>> >>>>>>>>>>     Alex
>> >>>>>>>>>>
>> >>>>>>>>>> On 17/12/2014 14:07, suman shil wrote:
>> >>>>>>>>>>       Hi, Following is my solution to solve this problem.
>> Please
>> >>>>>>>>>> let me know if I am missing something.
>> >>>>>>>>>>    public class OrderedExecutor {  private int
>> currentAllowedOrder
>> >>>>>>>>>> = 0;  private int maxLength = 0;  public OrderedExecutor(int
>> n)  {
>> >>>>>>>>>> this.maxLength = n;  } public synchronized Object
>> >>>>>>>>>> execCriticalSectionInOrder( int order, Callable<Object>
>> callable)
>> >>>>>>>>>> throws Exception  { if (order >= maxLength)  {  throw new
>> Exception("Exceeds
>> >>>>>>>>>> maximum order "+ maxLength);  }    while(order !=
>> currentAllowedOrder)  {
>> >>>>>>>>>> wait();  }    try  { currentAllowedOrder =
>> currentAllowedOrder+1;  return
>> >>>>>>>>>> callable.call();  }  finally  {  notify();  }  } }
>> >>>>>>>>>>    Regards Suman
>> >>>>>>>>>>        From: Peter Levart<peter.levart at gmail.com>
>> >>>>>>>>>> <mailto:peter.levart at gmail.com>
>> >>>>>>>>>>   To: Hanson Char<hanson.char at gmail.com>
>> >>>>>>>>>> <mailto:hanson.char at gmail.com>    Cc:
>> >>>>>>>>>> concurrency-interest<concurrency-interest at cs.oswego.edu>
>> >>>>>>>>>> <mailto:concurrency-interest at cs.oswego.edu>    Sent: Sunday,
>> December 14,
>> >>>>>>>>>> 2014 11:01 PM
>> >>>>>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered
>> execution
>> >>>>>>>>>> of critical sections?
>> >>>>>>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>> >>>>>>>>>>      Hi Peter,
>> >>>>>>>>>>    Thanks for this proposed idea of using LockSupport. This
>> begs
>> >>>>>>>>>> the question: which one would you choose if you had all three
>> (correct)
>> >>>>>>>>>> implementation available?  (Semaphore, CountDownLatch, or
>> LockSupport)?
>> >>>>>>>>>>    Regards, Hanson
>> >>>>>>>>>>     The Semaphore/CountDownLatch variants are equivalent if you
>> >>>>>>>>>> don't need re-use. So any would do. They lack invalid-use
>> detection. What
>> >>>>>>>>>> happens if they are not used as intended? Semaphore variant
>> acts differently
>> >>>>>>>>>> than CountDownLatch variant. The low-level variant I  proposed
>> detects
>> >>>>>>>>>> invalid usage. So I would probably use this one. But the low
>> level variant
>> >>>>>>>>>> is harder to reason about it's correctness. I think it is
>> correct, but you
>> >>>>>>>>>> should show it to somebody else to confirm this.
>> >>>>>>>>>>     Another question is whether you actually need this kind of
>> >>>>>>>>>> synchronizer. Maybe if you explained what you are trying to
>> achieve,
>> >>>>>>>>>> somebody could have an idea how to do that even more
>> elegantly...
>> >>>>>>>>>>     Regards, Peter
>> >>>>>>>>>>              On Sun, Dec 14, 2014 at 9:01 AM, Peter
>> >>>>>>>>>> Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>
>> wrote:
>> >>>>>>>>>>    Hi Hanson,
>> >>>>>>>>>>     This one is more low-level, but catches some invalid usages
>> >>>>>>>>>> and is more resource-friendly:
>> >>>>>>>>>>       public class OrderedExecutor {
>> >>>>>>>>>>         public <T> T execCriticalSectionInOrder(
>> >>>>>>>>>>           final int order,
>> >>>>>>>>>>           final Supplier<T> criticalSection
>> >>>>>>>>>>       ) throws InterruptedException {
>> >>>>>>>>>>           if (order < 0) {
>> >>>>>>>>>>                throw new IllegalArgumentException("'order'
>> should
>> >>>>>>>>>> be >= 0");
>> >>>>>>>>>>           }
>> >>>>>>>>>>           if (order > 0) {
>> >>>>>>>>>>               waitForDone(order - 1);
>> >>>>>>>>>>           }
>> >>>>>>>>>>           try {
>> >>>>>>>>>>               return criticalSection.get();
>> >>>>>>>>>>           } finally {
>> >>>>>>>>>>               notifyDone(order);
>> >>>>>>>>>>           }
>> >>>>>>>>>>       }
>> >>>>>>>>>>         private static final Object DONE = new Object();
>> >>>>>>>>>>       private final ConcurrentMap<Integer, Object> signals =
>> new
>> >>>>>>>>>> ConcurrentHashMap<>();
>> >>>>>>>>>>         private void waitForDone(int order) throws
>> >>>>>>>>>> InterruptedException {
>> >>>>>>>>>>           Object sig = signals.putIfAbsent(order,
>> >>>>>>>>>> Thread.currentThread());
>> >>>>>>>>>>           if (sig != null && sig != DONE) {
>> >>>>>>>>>>               throw new IllegalStateException();
>> >>>>>>>>>>           }
>> >>>>>>>>>>           while (sig != DONE) {
>> >>>>>>>>>>               LockSupport.park();
>> >>>>>>>>>>               if (Thread.interrupted()) {
>> >>>>>>>>>>                   throw new InterruptedException();
>> >>>>>>>>>>               }
>> >>>>>>>>>>               sig = signals.get(order);
>> >>>>>>>>>>           }
>> >>>>>>>>>>       }
>> >>>>>>>>>>         private void notifyDone(int order) {
>> >>>>>>>>>>           Object sig = signals.putIfAbsent(order, DONE);
>> >>>>>>>>>>           if (sig instanceof Thread) {
>> >>>>>>>>>>               if (!signals.replace(order, sig, DONE)) {
>> >>>>>>>>>>                   throw new IllegalStateException();
>> >>>>>>>>>>               }
>> >>>>>>>>>>               LockSupport.unpark((Thread) sig);
>> >>>>>>>>>>           } else if (sig != null) {
>> >>>>>>>>>>               throw new IllegalStateException();
>> >>>>>>>>>>           }
>> >>>>>>>>>>       }
>> >>>>>>>>>>   }
>> >>>>>>>>>>       Regards, Peter
>> >>>>>>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>> >>>>>>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>> >>>>>>>>>>      Hi Peter,
>> >>>>>>>>>>    Thanks for the suggestion, and sorry about not being clear
>> >>>>>>>>>> about one important  detail: "n" is not known a priori when
>> constructing an
>> >>>>>>>>>> OrderedExecutor.  Does this mean the use of CountDownLatch is
>> ruled out?
>> >>>>>>>>>>     If you know at least the upper bound of 'n', it can be used
>> >>>>>>>>>> with such 'n'. Otherwise something that dynamically re-sizes
>> the array could
>> >>>>>>>>>> be devised. Or you could simply use a ConcurrentHashMap
>> instead of array
>> >>>>>>>>>> where keys are 'order' values:
>> >>>>>>>>>>       public class OrderedExecutor<T> {
>> >>>>>>>>>>         private final ConcurrentMap<Integer, CountDownLatch>
>> >>>>>>>>>> latches = new ConcurrentHashMap<>();
>> >>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>> >>>>>>>>>>                                           final Supplier<T>
>> >>>>>>>>>> criticalSection) throws InterruptedException {
>> >>>>>>>>>>           if (order > 0) {
>> >>>>>>>>>>               latches.computeIfAbsent(order - 1, o -> new
>> >>>>>>>>>> CountDownLatch(1)).await();
>> >>>>>>>>>>           }
>> >>>>>>>>>>           try {
>> >>>>>>>>>>               return criticalSection.get();
>> >>>>>>>>>>           } finally {
>> >>>>>>>>>>               latches.computeIfAbsent(order, o -> new
>> >>>>>>>>>> CountDownLatch(1)).countDown();
>> >>>>>>>>>>           }
>> >>>>>>>>>>       }
>> >>>>>>>>>>   }
>> >>>>>>>>>>       Regards, Peter
>> >>>>>>>>>>           You guessed right: it's a one-shot object for a
>> >>>>>>>>>> particular OrderedExecutor  instance, and "order" must be
>> called indeed at
>> >>>>>>>>>> most once.
>> >>>>>>>>>>    Regards, Hanson
>> >>>>>>>>>>   On Sun, Dec 14, 2014 at 2:21 AM, Peter
>> >>>>>>>>>> Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>
>> wrote:
>> >>>>>>>>>>    Hi Hanson,
>> >>>>>>>>>>     I don't think anything like that readily exists  in
>> >>>>>>>>>> java.lang.concurrent, but what you describe should be possible
>> to  achieve
>> >>>>>>>>>> with composition of existing primitives.  You haven't given
>> any additional
>> >>>>>>>>>> hints to what your OrderedExecutor  should behave like. Should
>> it be a
>> >>>>>>>>>> one-shot object (like CountDownLatch) or a re-usable one (like
>> >>>>>>>>>> CyclicBarrier)? Will execCriticalSectionInOrder() for a
>> particular
>> >>>>>>>>>> OrderedExecutor instance and 'order' value be called at most
>> once? If yes
>> >>>>>>>>>> (and I think that only a one-shot object  makes sense here),
>> an array of
>> >>>>>>>>>> CountDownLatch(es) could be used:
>> >>>>>>>>>>     public class OrderedExecutor<T> {
>> >>>>>>>>>>       private final CountDownLatch[] latches;
>> >>>>>>>>>>         public OrderedExecutor(int n) {
>> >>>>>>>>>>           if (n < 1) throw new IllegalArgumentException("'n'
>> >>>>>>>>>> should be >= 1");
>> >>>>>>>>>>           latches = new CountDownLatch[n - 1];
>> >>>>>>>>>>           for (int i = 0; i < latches.length; i++) {
>> >>>>>>>>>>               latches[i] = new CountDownLatch(1);
>> >>>>>>>>>>           }
>> >>>>>>>>>>       }
>> >>>>>>>>>>         public T execCriticalSectionInOrder(final int order,
>> >>>>>>>>>>                                            final Supplier<T>
>> >>>>>>>>>> criticalSection) throws InterruptedException {
>> >>>>>>>>>>           if (order < 0 || order > latches.length)
>> >>>>>>>>>>               throw new IllegalArgumentException("'order'
>> should
>> >>>>>>>>>> be [0..." +  latches.length + "]");
>> >>>>>>>>>>           if (order > 0) {
>> >>>>>>>>>>               latches[order - 1].await();
>> >>>>>>>>>>           }
>> >>>>>>>>>>           try {
>> >>>>>>>>>>               return criticalSection.get();
>> >>>>>>>>>>           } finally {
>> >>>>>>>>>>               if (order < latches.length) {
>> >>>>>>>>>>                   latches[order].countDown();
>> >>>>>>>>>>               }
>> >>>>>>>>>>           }
>> >>>>>>>>>>       }
>> >>>>>>>>>>   }
>> >>>>>>>>>>       Regards, Peter
>> >>>>>>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>> >>>>>>>>>>         Hi, I am looking for a construct that can  be used to
>> >>>>>>>>>> efficiently enforce  ordered execution of multiple critical
>> sections, each
>> >>>>>>>>>> calling from a  different thread. The calling threads may run
>> in  parallel
>> >>>>>>>>>> and may call the execution method out of order. The  perceived
>> construct
>> >>>>>>>>>> would therefore be responsible for re-ordering the execution
>> of those
>> >>>>>>>>>> threads, so that their critical  sections (and only the
>> critical section)
>> >>>>>>>>>> will be executed in order. Would something  like the following
>> API already
>> >>>>>>>>>> exist? /** * Used to enforce ordered execution of critical
>> sections calling
>> >>>>>>>>>> from multiple *  threads, parking and unparking the  threads
>> as necessary.
>> >>>>>>>>>> */ public class OrderedExecutor<T> { /** * Executes a critical
>> section at
>> >>>>>>>>>> most once with the given order, parking * and  unparking the
>> current thread
>> >>>>>>>>>> as  necessary so that all critical * sections executed  by
>> different threads
>> >>>>>>>>>> using this  executor take place in * the order from 1 to n
>> consecutively.
>> >>>>>>>>>> */ public T execCriticalSectionInOrder
>> >>>>>>>>>> (  final int order, final Callable<T> criticalSection) throws
>> >>>>>>>>>> InterruptedException; } Regards, Hanson
>> >>>>>>>>>>
>> _______________________________________________Concurrency-interest mailing
>> >>>>>>>>>> listConcurrency-interest at cs.oswego.edu
>> >>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>>>>>>> _______________________________________________
>> >>>>>>>>>>   Concurrency-interest mailing list
>> >>>>>>>>>>   Concurrency-interest at cs.oswego.edu
>> >>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>> >>>>>>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>>>>>>> _______________________________________________
>> >>>>>>>>>> Concurrency-interest mailing list
>> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
>> >>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> _______________________________________________
>> >>>>>>>>>> Concurrency-interest mailing list
>> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
>> >>>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> _______________________________________________
>> >>>>>>>>>> Concurrency-interest mailing list
>> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
>> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> _______________________________________________
>> >>>>>>>>>> Concurrency-interest mailing list
>> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
>> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> _______________________________________________
>> >>>>>>>>>> Concurrency-interest mailing list
>> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
>> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>>
>> >>>>>>>>>> _______________________________________________
>> >>>>>>>>>> Concurrency-interest mailing list
>> >>>>>>>>>> Concurrency-interest at cs.oswego.edu
>> >>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>>>>>>>
>> >>>>>>>>
>> >>>>>>>> _______________________________________________
>> >>>>>>>> Concurrency-interest mailing list
>> >>>>>>>> Concurrency-interest at cs.oswego.edu
>> >>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>>>>>>
>> >>>>>>>
>> >>>>>
>> >>>
>> >>>
>> >>>
>> >>> _______________________________________________
>> >>> Concurrency-interest mailing list
>> >>> Concurrency-interest at cs.oswego.edu
>> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>
>> >>>
>> >>
>> >>
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141221/58a32e2d/attachment-0001.html>

From martinrb at google.com  Mon Dec 22 02:52:41 2014
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 21 Dec 2014 23:52:41 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEqoY7JYP8pETNVJOiDQ8kd7tsWkVd=ZwOPTfbg0mOSuGg@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
	<5496B0FE.7070502@gmail.com>
	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
	<549737AB.8060508@gmail.com>
	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
	<CA+kOe0-OX_h1oJ5mZnSh8WNxoOgpNo9P1kbe7fVnbWqg973HSA@mail.gmail.com>
	<CABWgujYjpkgdcJWW5wriiZDBugvNNBWfbpfob95uXb=dsdmyCQ@mail.gmail.com>
	<CAHzJPEqoY7JYP8pETNVJOiDQ8kd7tsWkVd=ZwOPTfbg0mOSuGg@mail.gmail.com>
Message-ID: <CA+kOe09abL5DDy5=S5mUb=-kU_AOs3VCR4=-UGCP=Me3kS2Lrw@mail.gmail.com>

It may be a bug in the signal/propagate part of AQS, one of the trickiest parts.
I can reproduce this and hacky-ported the bug to jsr166 CVS:

Index: SemaphoreTest.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/test/tck/SemaphoreTest.java,v
retrieving revision 1.31
diff -u -r1.31 SemaphoreTest.java
--- SemaphoreTest.java    25 Jul 2011 19:01:08 -0000    1.31
+++ SemaphoreTest.java    22 Dec 2014 07:49:03 -0000
@@ -8,6 +8,7 @@

 import junit.framework.*;
 import java.util.*;
+import java.util.concurrent.*;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Semaphore;
 import static java.util.concurrent.TimeUnit.MILLISECONDS;
@@ -630,4 +631,95 @@
         assertTrue(s.toString().contains("Permits = -2"));
     }

+    static class OrderedExecutor<T> {
+
+        private final Semaphore semaphore = new Semaphore(2);
+
+        public T execCallableInOrder(int order, Callable<T> callable)
+            throws Exception {
+            assert order >= 1;
+            final int acquires = order + 1;
+            syserr("Acquiring " + acquires + " permits with " +
semaphore.availablePermits() + "  available");
+            semaphore.acquire(acquires);
+            syserr("Acquired " + acquires + " permits with " +
semaphore.availablePermits() + "  available");
+            try {
+                return callable.call();
+            } finally {
+                syserr("Releasing " + (acquires+1) + " permits with
"+ semaphore.availablePermits() + "  available");
+                semaphore.release(acquires + 1);
+                syserr("Released " + (acquires+1) + " permits with "+
semaphore.availablePermits() + "  available");
+            }
+        }
+
+        private synchronized static void syserr(Object o) {
+            System.err.println(String.valueOf(o));
+        }
+    }
+
+    public void testFoo() throws InterruptedException, ExecutionException {
+        Random rand = new Random();
+        final OrderedExecutor<Integer> oe = new OrderedExecutor<Integer>();
+        final int[] counter = new int[1];
+        // Number of critical sections to be executed
+        Integer[] a = new Integer[10];
+        for (int i=0; i< a.length; i++)
+            a[i] = i+1;
+        List<Integer> orders = new ArrayList<Integer>(Arrays.asList(a));
+        // Shuffle them so they will be submitted in random order for execution
+        Collections.shuffle(orders);
+        ExecutorService es = Executors.newCachedThreadPool();
+        List<Future<Integer>> futures = new ArrayList<Future<Integer>>();
+        for (final Integer order: orders) {
+            sysout("Submitting task " + order);
+            // Submit a task that involves both critical and
non-critical sections
+            futures.add(es.submit(new Callable<Integer>() {
+                @Override
+                public Integer call() throws Exception {
+                    Callable<Integer> criticalSection = new
Callable<Integer>() {
+                        @Override
+                        public Integer call() {
+                          syserr("executing critical section " + order);
+                          assertTrue(++counter[0] == order);
+                          try {
+                            Thread.sleep(rand.nextInt(200));
+                          } catch (InterruptedException e) {
+                            throw new Error(e);
+                          }
+                          return order;
+                        }
+                    };
+                    // Executes the critical section in order,
blocking as necessary
+                    Integer ret = oe.execCallableInOrder(order,
criticalSection);
+                    // The rest is non-critical and can be executed
with no order imposed
+                    sysout("start non-critical section " + order);
+                    try {
+                        Thread.sleep(rand.nextInt(1000));
+                    } catch (InterruptedException e) {
+                        throw new Error(e);
+                    }
+                    sysout("end non-critical section " + order);
+                    return ret;
+                }
+            }));
+        }
+        int count = 0;
+        Iterator<Integer> expected = orders.iterator();
+        for (Future<Integer> future: futures) {
+            final int val = future.get();
+            assertTrue("count="+count + ", val=" + val, val ==
expected.next().intValue());
+            count++;
+        }
+        assertTrue(a.length == count);
+        assertTrue(a.length == counter[0]);
+    }
+
+    private synchronized static void sysout(Object o) {
+        System.out.println(String.valueOf(o));
+    }
+
+    private synchronized static void syserr(Object o) {
+        System.err.println(String.valueOf(o));
+    }
+
+
 }

From peter.levart at gmail.com  Mon Dec 22 07:15:21 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 22 Dec 2014 13:15:21 +0100
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
Message-ID: <54980B59.2080202@gmail.com>

On 12/22/2014 03:35 AM, Luke Sandberg wrote:
> Basically, I need to be able to publish a non-null object reference into a
> data structure where
> * order doesn't matter (insertion order would be nice, but i don't care
> that much)
> * add and remove identity semantics
> * concurrent iteration (weakly consistent is fine)
> * add/remove are non-blocking

If order really doesn't matter, ConcurrentHashMap is a data structure 
with plenty of uses:


public class ConcurrentIdentityHashSet<E> extends AbstractSet<E> {

     private static final class Key {
         private final Object ref;

         Key(Object ref) {
             this.ref = ref;
         }

         public int hashCode() {
             return System.identityHashCode(ref);
         }

         public boolean equals(Object obj) {
             return (obj instanceof Key) &&
                 this.ref == ((Key) obj).ref;
         }
     }

     private final ConcurrentHashMap<Key, E> map = new 
ConcurrentHashMap<>();

     public int size() {
         return map.size();
     }

     public boolean isEmpty() {
         return map.isEmpty();
     }

     public boolean contains(Object o) {
         return map.containsKey(new Key(o));
     }

     public Iterator<E> iterator() {
         return map.values().iterator();
     }

     public Object[] toArray() {
         return map.values().toArray();
     }

     public <T> T[] toArray(T[] a) {
         return map.values().toArray(a);
     }

     public boolean add(E e) {
         return map.putIfAbsent(new Key(e), e) == null;
     }

     public boolean remove(Object o) {
         return map.remove(new Key(o)) != null;
     }

     public void clear() {
         map.clear();
     }
}





Regards, Peter


From peter.levart at gmail.com  Mon Dec 22 07:21:08 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 22 Dec 2014 13:21:08 +0100
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
Message-ID: <54980CB4.4050709@gmail.com>

On 12/22/2014 03:35 AM, Luke Sandberg wrote:
> high entry
> overhead is a deal breaker.

Ah, I see. No CHM then.

Peter.

From dl at cs.oswego.edu  Mon Dec 22 08:34:18 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 22 Dec 2014 08:34:18 -0500
Subject: [concurrency-interest] ConcurrentHashMap computeIfAbsent
In-Reply-To: <54974A6E.5010305@cs.oswego.edu>
References: <CAGu0=MMZ-2h1wEm4-nk8J9_acjCG-Jy17H54o7RNdKTmSa0Maw@mail.gmail.com>	<CAGu0=MNVCWdCYeCJKMGjnvrUuZ26qxYiADM4qTgTQFenroQotA@mail.gmail.com>	<5495949E.1030500@cs.oswego.edu>	<CAGu0=MMqeaATSCte_LY6q=BsFUwhUweNi6uTp1GMWMJX9fxwWg@mail.gmail.com>	<5496D732.3010308@cs.oswego.edu>
	<549720A6.70208@gmail.com> <54974A6E.5010305@cs.oswego.edu>
Message-ID: <54981DDA.3010708@cs.oswego.edu>

On 12/21/2014 05:32 PM, Doug Lea wrote:
> We should write off coping with deadlock and infinite recursion. But
> we can still add some (cheap) integrity checks that can only be
> violated in the remaining user-error scenarios.

I added some very low overhead checks that improve diagnostic
coverage enough to catch most of these remaining potential user
errors. Given that we cannot in principle catch all of them
(and say so in the specs) this seems to be the best tradeoff.
Adding noticeable time or space overhead would provide only
almost-unnoticeably better diagnosis.

This is committed into jsr166 CVS, and will probably be
integrated as a fix for JDK-8062841.

-Doug



From aaron.grunthal at infinite-source.de  Mon Dec 22 09:24:39 2014
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Mon, 22 Dec 2014 15:24:39 +0100
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
Message-ID: <549829A7.1020604@infinite-source.de>

On 22.12.2014 03:35, Luke Sandberg wrote:
> I have come across a few situations where i am looking for a
> datastructure and i feel like i keep coming up short.
>
> The situation is analogous to the 'waiters' list in
> java.util.concurrent.FutureTask.
>
> Basically, I need to be able to publish a non-null object reference into
> a data structure where
> * order doesn't matter (insertion order would be nice, but i don't care
> that much)
> * add and remove identity semantics
> * concurrent iteration (weakly consistent is fine)
> * add/remove are non-blocking
>
> CLQ is an obvious choice but removal is O(n), another choice would a
> simple synchronized identity hash set which is fine but the lock + high
> entry overhead is a deal breaker.
>
> An AtomicReferenceArray would be super easy, but i can't put a bound on
> the number of items.

You say that it's super easy, but how would you handle finding a free 
slot for insertion in constant or logarithmic time? Since you mention 
that order doesn't matter that seems to suggest it doesn't behave in a 
stack-like manner.


> Also, it would be fine for the code that adds items, to store additional
> state (an index, a 'Node' reference), in order to facilitate removal.
>
> The best thing i have seen from looking around appears to be something
> like what FutureTask does to implement 'awaitDone', but even there
> removeWaitier() is O(n).  that seems like a pretty good compromise when
> lists are short, but what would be a better solution when lists are long?
>
> Just to motivate this a little bit, the two cases I am looking at in
> particular are:
>
> * maintaining a set of threads associated with a server 'request'
> (threads enter/exit when they start executing tasks associated with the
> request)
> * maintaining a set of futures to be cancelled (futures are removed when
> they complete to avoid pinning their referents).
>
> Any pointers or ideas?
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From dl at cs.oswego.edu  Mon Dec 22 09:36:16 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 22 Dec 2014 09:36:16 -0500
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>	<5496B0FE.7070502@gmail.com>	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>	<549737AB.8060508@gmail.com>
	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
Message-ID: <54982C60.1000008@cs.oswego.edu>

On 12/21/2014 07:25 PM, Joe Bowbeer wrote:
> Below is the most concise implementation I can imagine, using a single
> Semaphore, which is legal AFAICT according to the javadoc, but which deadlocks
> in my tests.

This makes a too-strong assumption about Semaphore.acquire(n), that
waiters do not independently try to accumulate permits.
I suppose that we don't explicitly rule this out, but do state...

"This class also provides convenience methods to acquire and release multiple 
permits at a time. Beware of the increased risk of indefinite postponement when 
these methods are used without fairness set true. "

The acquire(n) spec should probably be clarified that it promises
no more than the equivalent of
   for (int i = 0; i < n; ++i) acquire();

-Doug


>
> class OrderedExecutor<T> {
>
>    private final Semaphore semaphore = new Semaphore(1);
>
>    @Override
>    public T execCallableInOrder(int order, Callable<T> callable)
>        throws InterruptedException, Exception {
>      assert order >= 0;
>      int acquires = order + 1;
>      semaphore.acquire(acquires);
>      try {
>        return callable.call();
>      } finally {
>        semaphore.release(acquires + 1);
>      }
>    }
> }





From lukeisandberg at gmail.com  Mon Dec 22 11:10:21 2014
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Mon, 22 Dec 2014 08:10:21 -0800
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <549829A7.1020604@infinite-source.de>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
	<549829A7.1020604@infinite-source.de>
Message-ID: <CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>

Thanks for all the responses!

RE: CHM. For the thread-renaming usecase we used CHM for a while but it
consistently ended up as one of the highest contention points (as measured
by lock wait times) in some of our applications.  Also there is a fairly
high fixed overhead.

RE: AtomicReferenceArray.  If i had a strict bound on the number of items,
i would just allocate it to be that large and use an atomic int
(getAndIncrement()) to find the next available slot and then remove would
just be to assign the slot to null.  Iteration would have to read a
potentially large number of slots, but iterating an array should be
sufficiently fast that you wouldn't really notice.

RE: CLQ with some extra method.  I had thought it would have been nice to
have CLQ return a Node object for this usecase.  Though if i wouldn't be
able to physically remove the node then it seems simpler to just build my
own Trieber stack and implement remove by nulling out the 'item' field in
the Node object.


On Mon, Dec 22, 2014 at 6:24 AM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:
>
> On 22.12.2014 03:35, Luke Sandberg wrote:
>
>> I have come across a few situations where i am looking for a
>> datastructure and i feel like i keep coming up short.
>>
>> The situation is analogous to the 'waiters' list in
>> java.util.concurrent.FutureTask.
>>
>> Basically, I need to be able to publish a non-null object reference into
>> a data structure where
>> * order doesn't matter (insertion order would be nice, but i don't care
>> that much)
>> * add and remove identity semantics
>> * concurrent iteration (weakly consistent is fine)
>> * add/remove are non-blocking
>>
>> CLQ is an obvious choice but removal is O(n), another choice would a
>> simple synchronized identity hash set which is fine but the lock + high
>> entry overhead is a deal breaker.
>>
>> An AtomicReferenceArray would be super easy, but i can't put a bound on
>> the number of items.
>>
>
> You say that it's super easy, but how would you handle finding a free slot
> for insertion in constant or logarithmic time? Since you mention that order
> doesn't matter that seems to suggest it doesn't behave in a stack-like
> manner.
>
>
>  Also, it would be fine for the code that adds items, to store additional
>> state (an index, a 'Node' reference), in order to facilitate removal.
>>
>> The best thing i have seen from looking around appears to be something
>> like what FutureTask does to implement 'awaitDone', but even there
>> removeWaitier() is O(n).  that seems like a pretty good compromise when
>> lists are short, but what would be a better solution when lists are long?
>>
>> Just to motivate this a little bit, the two cases I am looking at in
>> particular are:
>>
>> * maintaining a set of threads associated with a server 'request'
>> (threads enter/exit when they start executing tasks associated with the
>> request)
>> * maintaining a set of futures to be cancelled (futures are removed when
>> they complete to avoid pinning their referents).
>>
>> Any pointers or ideas?
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/b867f9e4/attachment-0001.html>

From aleksey.shipilev at oracle.com  Mon Dec 22 11:46:02 2014
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Mon, 22 Dec 2014 19:46:02 +0300
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <54982C60.1000008@cs.oswego.edu>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>	<5496B0FE.7070502@gmail.com>	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>	<549737AB.8060508@gmail.com>	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
	<54982C60.1000008@cs.oswego.edu>
Message-ID: <54984ACA.3050401@oracle.com>

On 12/22/2014 05:36 PM, Doug Lea wrote:
> On 12/21/2014 07:25 PM, Joe Bowbeer wrote:
>> Below is the most concise implementation I can imagine, using a single
>> Semaphore, which is legal AFAICT according to the javadoc, but which
>> deadlocks
>> in my tests.
> 
> This makes a too-strong assumption about Semaphore.acquire(n), that
> waiters do not independently try to accumulate permits.
> I suppose that we don't explicitly rule this out, but do state...
> 
> "This class also provides convenience methods to acquire and release
> multiple permits at a time. Beware of the increased risk of indefinite
> postponement when these methods are used without fairness set true. "
> 
> The acquire(n) spec should probably be clarified that it promises
> no more than the equivalent of
>   for (int i = 0; i < n; ++i) acquire();

Wait, what. If fairness only applies to individual acquire() calls, that
definition would mean that even with fair Semaphore two threads can
deadlock in:

 Semaphore s = new Semaphore(5, true);

 Thread 1:
   s.acquire(5);

 Thread 2:
   s.acquire(5);

Most people I talked to assume a Semaphore does the CAS for the entire
batch when multiple permits are requested. I think we better fix the
implementation to do the same even in non-fair case, owing to the
Principle of Least Astonishment. People rightfully expect the starvation
with non-fair Semaphores, but not the deadlock.

Thanks,
-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/36d5b1e6/attachment.bin>

From dl at cs.oswego.edu  Mon Dec 22 12:11:22 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 22 Dec 2014 12:11:22 -0500
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <54984ACA.3050401@oracle.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>	<5496B0FE.7070502@gmail.com>	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>	<549737AB.8060508@gmail.com>	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
	<54982C60.1000008@cs.oswego.edu> <54984ACA.3050401@oracle.com>
Message-ID: <549850BA.8070904@cs.oswego.edu>

On 12/22/2014 11:46 AM, Aleksey Shipilev wrote:
> On 12/22/2014 05:36 PM, Doug Lea wrote:
>> On 12/21/2014 07:25 PM, Joe Bowbeer wrote:
>>> Below is the most concise implementation I can imagine, using a single
>>> Semaphore, which is legal AFAICT according to the javadoc, but which
>>> deadlocks
>>> in my tests.

>> The acquire(n) spec should probably be clarified that it promises
>> no more than the equivalent of
>>    for (int i = 0; i < n; ++i) acquire();

Sorry, this was too extreme a way of saying it.

>
> Wait, what. If fairness only applies to individual acquire() calls, that
> definition would mean that even with fair Semaphore two threads can
> deadlock in:

Suppose you have thread 1 acquire(7), and then thread 2: acquire(5).
With fairness, thread 2 waits for thread 1 to succeed before
trying. Without fairness, they could be in any order, but still
*some* order. So the net effect is not quite the same as
the above loop, but closer than implying that the semaphore
somehow sorts waiters by #permits, which it doesn't/can't.

-Doug


From hanson.char at gmail.com  Mon Dec 22 12:16:41 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 22 Dec 2014 09:16:41 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <54984ACA.3050401@oracle.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
	<5496B0FE.7070502@gmail.com>
	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
	<549737AB.8060508@gmail.com>
	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
	<54982C60.1000008@cs.oswego.edu> <54984ACA.3050401@oracle.com>
Message-ID: <CABWgujbXMJht4L6CU82TYh-VePhbGKaZ66Zj=g-MoWFF8TQYFA@mail.gmail.com>

Hi Aleksey,

In case it is not clear, in this particular case, if fairness (of the
Semaphore) is set to true, the deadlock (of running the unit test) actually
would appear much sooner than otherwise.

>Most people I talked to assume a Semaphore does the CAS for the entire
batch when multiple permits are requested.
>I think we better fix the implementation to do the same even in non-fair
case, owing to the Principle of Least Astonishment.

That would be so cool.  However, assuming the fix is not going to incur a
lot of additional overheads, this sounds like a tricky one.

Regards,
Hanson

On Mon, Dec 22, 2014 at 8:46 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 12/22/2014 05:36 PM, Doug Lea wrote:
> > On 12/21/2014 07:25 PM, Joe Bowbeer wrote:
> >> Below is the most concise implementation I can imagine, using a single
> >> Semaphore, which is legal AFAICT according to the javadoc, but which
> >> deadlocks
> >> in my tests.
> >
> > This makes a too-strong assumption about Semaphore.acquire(n), that
> > waiters do not independently try to accumulate permits.
> > I suppose that we don't explicitly rule this out, but do state...
> >
> > "This class also provides convenience methods to acquire and release
> > multiple permits at a time. Beware of the increased risk of indefinite
> > postponement when these methods are used without fairness set true. "
> >
> > The acquire(n) spec should probably be clarified that it promises
> > no more than the equivalent of
> >   for (int i = 0; i < n; ++i) acquire();
>
> Wait, what. If fairness only applies to individual acquire() calls, that
> definition would mean that even with fair Semaphore two threads can
> deadlock in:
>
>  Semaphore s = new Semaphore(5, true);
>
>  Thread 1:
>    s.acquire(5);
>
>  Thread 2:
>    s.acquire(5);
>
> Most people I talked to assume a Semaphore does the CAS for the entire
> batch when multiple permits are requested. I think we better fix the
> implementation to do the same even in non-fair case, owing to the
> Principle of Least Astonishment. People rightfully expect the starvation
> with non-fair Semaphores, but not the deadlock.
>
> Thanks,
> -Aleksey.
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/ca638972/attachment.html>

From vitalyd at gmail.com  Mon Dec 22 12:17:37 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 22 Dec 2014 12:17:37 -0500
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
	<549829A7.1020604@infinite-source.de>
	<CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>
Message-ID: <CAHjP37FQz1SqKzjei8RSz5uuM7hK4-T9Y=T-kbCbfoORe1i75Q@mail.gmail.com>

>
> RE: AtomicReferenceArray.  If i had a strict bound on the number of items,
> i would just allocate it to be that large and use an atomic int
> (getAndIncrement()) to find the next available slot and then remove would
> just be to assign the slot to null.  Iteration would have to read a
> potentially large number of slots, but iterating an array should be
> sufficiently fast that you wouldn't really notice.


How about maintaining a set of fixed size AtomicReferenceArrays that are
allocated on-demand (i.e. when you overflow capacity of existing ones)? To
facilitate iteration, you could maintain a bitmask associated with each ARA
that indicates which slots have a value (this is assuming they're sparse --
otherwise, filtered iteration should be fine).

On Mon, Dec 22, 2014 at 11:10 AM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> Thanks for all the responses!
>
> RE: CHM. For the thread-renaming usecase we used CHM for a while but it
> consistently ended up as one of the highest contention points (as measured
> by lock wait times) in some of our applications.  Also there is a fairly
> high fixed overhead.
>
> RE: AtomicReferenceArray.  If i had a strict bound on the number of items,
> i would just allocate it to be that large and use an atomic int
> (getAndIncrement()) to find the next available slot and then remove would
> just be to assign the slot to null.  Iteration would have to read a
> potentially large number of slots, but iterating an array should be
> sufficiently fast that you wouldn't really notice.
>
> RE: CLQ with some extra method.  I had thought it would have been nice to
> have CLQ return a Node object for this usecase.  Though if i wouldn't be
> able to physically remove the node then it seems simpler to just build my
> own Trieber stack and implement remove by nulling out the 'item' field in
> the Node object.
>
>
> On Mon, Dec 22, 2014 at 6:24 AM, Aaron Grunthal <
> aaron.grunthal at infinite-source.de> wrote:
>>
>> On 22.12.2014 03:35, Luke Sandberg wrote:
>>
>>> I have come across a few situations where i am looking for a
>>> datastructure and i feel like i keep coming up short.
>>>
>>> The situation is analogous to the 'waiters' list in
>>> java.util.concurrent.FutureTask.
>>>
>>> Basically, I need to be able to publish a non-null object reference into
>>> a data structure where
>>> * order doesn't matter (insertion order would be nice, but i don't care
>>> that much)
>>> * add and remove identity semantics
>>> * concurrent iteration (weakly consistent is fine)
>>> * add/remove are non-blocking
>>>
>>> CLQ is an obvious choice but removal is O(n), another choice would a
>>> simple synchronized identity hash set which is fine but the lock + high
>>> entry overhead is a deal breaker.
>>>
>>> An AtomicReferenceArray would be super easy, but i can't put a bound on
>>> the number of items.
>>>
>>
>> You say that it's super easy, but how would you handle finding a free
>> slot for insertion in constant or logarithmic time? Since you mention that
>> order doesn't matter that seems to suggest it doesn't behave in a
>> stack-like manner.
>>
>>
>>  Also, it would be fine for the code that adds items, to store additional
>>> state (an index, a 'Node' reference), in order to facilitate removal.
>>>
>>> The best thing i have seen from looking around appears to be something
>>> like what FutureTask does to implement 'awaitDone', but even there
>>> removeWaitier() is O(n).  that seems like a pretty good compromise when
>>> lists are short, but what would be a better solution when lists are long?
>>>
>>> Just to motivate this a little bit, the two cases I am looking at in
>>> particular are:
>>>
>>> * maintaining a set of threads associated with a server 'request'
>>> (threads enter/exit when they start executing tasks associated with the
>>> request)
>>> * maintaining a set of futures to be cancelled (futures are removed when
>>> they complete to avoid pinning their referents).
>>>
>>> Any pointers or ideas?
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/2c33cc41/attachment.html>

From concurrency at kuli.org  Mon Dec 22 12:31:27 2014
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Mon, 22 Dec 2014 18:31:27 +0100
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>	<549829A7.1020604@infinite-source.de>
	<CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>
Message-ID: <5498556F.7090009@kuli.org>

Am 22.12.2014 17:10, schrieb Luke Sandberg:
> RE: AtomicReferenceArray.  If i had a strict bound on the number of
> items, i would just allocate it to be that large and use an atomic int
> (getAndIncrement()) to find the next available slot and then remove
> would just be to assign the slot to null.  Iteration would have to read
> a potentially large number of slots, but iterating an array should be
> sufficiently fast that you wouldn't really notice.

What about this solution:

Your data structure has four fields:
a) The (atomic) array
b) The AtomicInteger as the counter for the next free slot
c) A possible replacement array
d) a CountDownLatch for synchronizing the enlargement of the array

Usual adds just increment b) until the length of a) is reached. In that 
case, the array has to be resized. For that, all adding threads that 
reach a)'s limit have to agree which one is resizing the array; it could 
be the one where b).getAndIncrement() is exactly a).length(), or the one 
who's able to compareAndSet a CountDownLatch of size 1 into null-d). The 
other writing threads have to wait on d) until the array is reized.

The resizing algorithm then creates a new, larger array, fills it up the 
the size of the old array with a marker value, and puts it into c). The 
it copies all values from a) to c) by setting all slots of a) to the 
mareker value via getAndSet(), and setting the taken value with a normal 
volatile write into the same position of c). When it's finished, it 
stores the replacement array c) into a), nullifies c) and d), and fires 
d) to wake up other possibly waiting threads that want to add a new value.

When a value if read from a slot, is has to be compared whether it's the 
marker value. In that case, the replacement array will be used from c) 
(if it's null, the resizing process has just finished, so start from 
new), and its value is read. If this is still the marker value, then the 
resizing thread is just copying this exact value, which is just a matter 
of nanoseconds; in that case, just start the get() method again. 
Otherwise, take the value from the replacement array.

Writes have to act in a comparable way; values mustn't just set in the 
array, call compareAndSet() with the expected old value instead to 
catch-up a possible marker state. In that case, write into the 
replacement array c) similar to the get() algorithm.

With this, you'll have a random-access data structure with only very 
little overhead (gets only have to do identity comparisons to the marker 
value, and writes must be compareAndSets). It is lock-free except for 
the resizing algorithm, which can be rather exceptional of you set the 
initial array size large enough.

-Michael


From joe.bowbeer at gmail.com  Mon Dec 22 13:05:38 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 22 Dec 2014 10:05:38 -0800
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
Message-ID: <CAHzJPErO+JTzSwKiarBg7aXYtLKq6N6nfM2UG-3D_B6tS7bX0g@mail.gmail.com>

ConcurrentSkipListSet?

On Sunday, December 21, 2014, Luke Sandberg <lukeisandberg at gmail.com> wrote:

> I have come across a few situations where i am looking for a datastructure
> and i feel like i keep coming up short.
>
> The situation is analogous to the 'waiters' list in
> java.util.concurrent.FutureTask.
>
> Basically, I need to be able to publish a non-null object reference into a
> data structure where
> * order doesn't matter (insertion order would be nice, but i don't care
> that much)
> * add and remove identity semantics
> * concurrent iteration (weakly consistent is fine)
> * add/remove are non-blocking
>
> CLQ is an obvious choice but removal is O(n), another choice would a
> simple synchronized identity hash set which is fine but the lock + high
> entry overhead is a deal breaker.
>
> An AtomicReferenceArray would be super easy, but i can't put a bound on
> the number of items.
>
> Also, it would be fine for the code that adds items, to store additional
> state (an index, a 'Node' reference), in order to facilitate removal.
>
> The best thing i have seen from looking around appears to be something
> like what FutureTask does to implement 'awaitDone', but even there
> removeWaitier() is O(n).  that seems like a pretty good compromise when
> lists are short, but what would be a better solution when lists are long?
>
> Just to motivate this a little bit, the two cases I am looking at in
> particular are:
>
> * maintaining a set of threads associated with a server 'request' (threads
> enter/exit when they start executing tasks associated with the request)
> * maintaining a set of futures to be cancelled (futures are removed when
> they complete to avoid pinning their referents).
>
> Any pointers or ideas?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/6f9b53f3/attachment.html>

From lukeisandberg at gmail.com  Mon Dec 22 13:08:34 2014
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Mon, 22 Dec 2014 10:08:34 -0800
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAHzJPErO+JTzSwKiarBg7aXYtLKq6N6nfM2UG-3D_B6tS7bX0g@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
	<CAHzJPErO+JTzSwKiarBg7aXYtLKq6N6nfM2UG-3D_B6tS7bX0g@mail.gmail.com>
Message-ID: <CAO9V1MLafTM97eXkewSK6tJr-U3i-z7TLpMiZ4aoLibHaoZ+jw@mail.gmail.com>

My items aren't necessarily comparable. (though maybe i could use guavas
Ordering#arbitrary() which uses identity hashcodes).  Also, CSLS has log(n)
add/remove.

On Mon, Dec 22, 2014 at 10:05 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>
> ConcurrentSkipListSet?
>
>
> On Sunday, December 21, 2014, Luke Sandberg <lukeisandberg at gmail.com>
> wrote:
>
>> I have come across a few situations where i am looking for a
>> datastructure and i feel like i keep coming up short.
>>
>> The situation is analogous to the 'waiters' list in
>> java.util.concurrent.FutureTask.
>>
>> Basically, I need to be able to publish a non-null object reference into
>> a data structure where
>> * order doesn't matter (insertion order would be nice, but i don't care
>> that much)
>> * add and remove identity semantics
>> * concurrent iteration (weakly consistent is fine)
>> * add/remove are non-blocking
>>
>> CLQ is an obvious choice but removal is O(n), another choice would a
>> simple synchronized identity hash set which is fine but the lock + high
>> entry overhead is a deal breaker.
>>
>> An AtomicReferenceArray would be super easy, but i can't put a bound on
>> the number of items.
>>
>> Also, it would be fine for the code that adds items, to store additional
>> state (an index, a 'Node' reference), in order to facilitate removal.
>>
>> The best thing i have seen from looking around appears to be something
>> like what FutureTask does to implement 'awaitDone', but even there
>> removeWaitier() is O(n).  that seems like a pretty good compromise when
>> lists are short, but what would be a better solution when lists are long?
>>
>> Just to motivate this a little bit, the two cases I am looking at in
>> particular are:
>>
>> * maintaining a set of threads associated with a server 'request'
>> (threads enter/exit when they start executing tasks associated with the
>> request)
>> * maintaining a set of futures to be cancelled (futures are removed when
>> they complete to avoid pinning their referents).
>>
>> Any pointers or ideas?
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/82835bb6/attachment.html>

From joe.bowbeer at gmail.com  Mon Dec 22 14:19:59 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 22 Dec 2014 11:19:59 -0800
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing ordered
	execution of critical sections?]
Message-ID: <CAHzJPEo2kPcBz4W8JF1H+Y+FS7QpQXCr_o_4Z1RyFcBkksShCg@mail.gmail.com>

Doug,

As I understand your fairness explanation, with acquire(5) waiting for
acquire(7), setting fairness=true will not cause the test to pass.  As
Hanson points out, if the Semaphore is constructed with fair=true, the test
tends to lock up even sooner.

Admittedly, this is a horrible use for a single Semaphore, and if I'd read
the source I would have known that it couldn't work -- but the javadoc
alone did not dissuade me.

I point out a few places where I think the Semaphore javadoc could be
improved below.

1. This comment in the class description needs an even strong warning?

"This class also provides convenience methods to acquire and release
multiple permits at a time. Beware of the increased risk of indefinite
postponement when these methods are used without fairness set true."

In particular, fairness isn't going to mitigate all risk.

2. The acquire(int) documentation needs more clarification.

I think an additional "and" helps readability, even though it is not proper
style.  The problem for the reader is that one of two things has to happen,
but the first thing is actually a combination of three things, all of which
have to happen:

"If insufficient permits are available then the current thread becomes
disabled for thread scheduling purposes and lies dormant until one of two
things happens:

Some other thread invokes one of the release methods for this semaphore,
[and] the current thread is next to be assigned permits and the number of
available permits satisfies this request; or ..."

3. The release(int) documentation should make it clearer that *only* one
waiting thread is selected?

"If any threads are trying to acquire permits, then one is selected and
given the permits that were just released."

4. This statement in the release(int) documentation emboldened me:

"There is no requirement that a thread that releases a permit must have
acquired that permit by calling acquire. Correct usage of a semaphore is
established by programming convention in the application."

I'm not sure what to make of the second sentence.  Would this be a good
place to advise against adding permits that cannot be acquired by any
waiting thread?


bitbucket.org/joebowbeer/semaphorebug

On Mon, Dec 22, 2014 at 9:11 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/22/2014 11:46 AM, Aleksey Shipilev wrote:
>
>> On 12/22/2014 05:36 PM, Doug Lea wrote:
>>
>>> On 12/21/2014 07:25 PM, Joe Bowbeer wrote:
>>>
>>>> Below is the most concise implementation I can imagine, using a single
>>>> Semaphore, which is legal AFAICT according to the javadoc, but which
>>>> deadlocks
>>>> in my tests.
>>>>
>>>
>  The acquire(n) spec should probably be clarified that it promises
>>> no more than the equivalent of
>>>    for (int i = 0; i < n; ++i) acquire();
>>>
>>
> Sorry, this was too extreme a way of saying it.
>
>
>> Wait, what. If fairness only applies to individual acquire() calls, that
>> definition would mean that even with fair Semaphore two threads can
>> deadlock in:
>>
>
> Suppose you have thread 1 acquire(7), and then thread 2: acquire(5).
> With fairness, thread 2 waits for thread 1 to succeed before
> trying. Without fairness, they could be in any order, but still
> *some* order. So the net effect is not quite the same as
> the above loop, but closer than implying that the semaphore
> somehow sorts waiters by #permits, which it doesn't/can't.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/a71576bb/attachment.html>

From jsampson at guidewire.com  Mon Dec 22 15:57:53 2014
From: jsampson at guidewire.com (Justin Sampson)
Date: Mon, 22 Dec 2014 20:57:53 +0000
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <549850BA.8070904@cs.oswego.edu>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>
	<589517389.381086.1419084263773.JavaMail.yahoo@jws10792.mail.gq1.yahoo.com>
	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>
	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>
	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>
	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>
	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>
	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>
	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>
	<5496B0FE.7070502@gmail.com>
	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>
	<549737AB.8060508@gmail.com>
	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>
	<54982C60.1000008@cs.oswego.edu> <54984ACA.3050401@oracle.com>
	<549850BA.8070904@cs.oswego.edu>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D7C78E@sm-ex-01-vm.guidewire.com>

Doug Lea wrote:

> Suppose you have thread 1 acquire(7), and then thread 2: acquire(5).
> With fairness, thread 2 waits for thread 1 to succeed before
> trying. Without fairness, they could be in any order, but still
> *some* order. So the net effect is not quite the same as
> the above loop, but closer than implying that the semaphore
> somehow sorts waiters by #permits, which it doesn't/can't.

It sounds like many of us might have expected for a semaphore to behave something like this:

synchronized void acquire(int permits) {
  while (state < permits) {
    wait();
  }
  state -= permits;
}

synchronized void release(int permits) {
  state += permits;
  notifyAll();
}

And it sounds like what's actually happening is more like a notify() rather than a notifyAll(), which is a perfect recipe for lost notifications whenever you have multiple threads that are actually waiting for different conditions to become true, as in the case we're discussing.

Is there any way for Semaphore's acquire() to propagate the notification if it wakes up and finds not enough permits, at least in the non-fair case?

That is, if the acquire(7) thread gets unparked and sees that there are fewer than 7 but more than 0 permits available, can it unpark the next thread before parking itself again, so that the next thread can try its acquire(5)? In the worst case you end up cycling through all the waiting threads, which is nasty, but sounds better to me than deadlocking unnecessarily, and only happens when _all_ of the threads are requesting more than 1 permit in acquire().

Of course, that would mean that using Semaphore to implement the OrderedExecutor idea in this thread ends up worse than many of the other solutions, except for its conciseness. :)

Cheers,
Justin


From dt at flyingtroika.com  Mon Dec 22 17:21:35 2014
From: dt at flyingtroika.com (DT)
Date: Mon, 22 Dec 2014 14:21:35 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <549340FF.5070301@oracle.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
	<5491D658.5060604@oracle.com> <549285B0.3070801@gmail.com>
	<CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>
	<549340FF.5070301@oracle.com>
Message-ID: <64849A49-4320-49A5-811F-842B4FA1A5B6@flyingtroika.com>

I think if the sequencer is used it would mean that both ACK/ NAK type of responses should be used and sequence has to be restarted . Restarting is ok but to deal with both ACK/NAK could bring extra complexity during contention.

Thanks,
Dmitry

Sent from my iPhone

> On Dec 18, 2014, at 1:02 PM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
> 
> Yes, no one said it is a good idea to always do that. When it is contended, most of the threads will wake up to only go back to sleep.
> 
> The pattern you are after is usually called sequencer. You can see it used in TCP. I am not sure why it wasn't implemented in j.u.c. - maybe not that popular.
> 
> The best solution will be lock-like, but the waiter nodes will contain the value they are waiting for - so only the specific threads get woken up. The solution with concurrent map is very similar, only with larger overhead from storing the index the thread is waiting for.
> 
> 
> Alex
> 
> 
>> On 18/12/2014 20:21, Hanson Char wrote:
>> Less overhead and simpler are a nice properties, even though at the expense of having to wake up all waiting threads just to find out the one with the right order to execute.  Still, this seems like a good tradeoff.
>> 
>> Thanks,
>> Hanson
>> 
>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart <peter.levart at gmail.com> wrote:           
>>>> On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>> No, there is no difference. Peter didn't spot your entire method is synchronized, so spurious wakeup won't make progress until the owner of the lock exits the method.
>>>> 
>>>> You could split the synchronization into two blocks - one encompassing the wait loop, the other in the finally block; but it may make no difference.
>>>> 
>>>> Alex
>>> 
>>> You're right, Alex. I'm so infected with park/unpark virus that I missed that ;-)
>>> 
>>> Peter
>>> 
>>>> 
>>>>> On 17/12/2014 18:36, suman shil wrote:
>>>>> Thanks peter for your reply. You are right. I should have incremented currentAllowedOrder in finally block.
>>>>> 
>>>>> Suman
>>>>> ------------------------------------------------------------------------
>>>>> *From:* Peter Levart <peter.levart at gmail.com>
>>>>> *To:* suman shil <suman_krec at yahoo.com>; Oleksandr Otenko <oleksandr.otenko at oracle.com>; Concurrency-interest <concurrency-interest at cs.oswego.edu>
>>>>> *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>> *Subject:* Re: [concurrency-interest] Enforcing ordered execution of critical sections?
>>>>> 
>>>>>> On 12/17/2014 06:46 PM, suman shil wrote:
>>>>>> Thanks for your response. Will notifyAll() instead of notify() solve the problem?
>>>>> 
>>>>> It will, but you should also account for "spurious" wake-ups. You should increment currentAllowedOrder only after return from callable.call (in finally block just before notifyAll()).
>>>>> 
>>>>> Otherwise a nice solution - with minimal state, providing that not many threads meet at the same time...
>>>>> 
>>>>> Regards, Peter
>>>>> 
>>>>>> RegardsSuman
>>>>>>        From: Oleksandr Otenko<oleksandr.otenko at oracle.com> <mailto:oleksandr.otenko at oracle.com>
>>>>>>   To: suman shil<suman_krec at yahoo.com> <mailto:suman_krec at yahoo.com>; Concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:concurrency-interest at cs.oswego.edu>    Sent: Wednesday, December 17, 2014 9:55 PM
>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
>>>>>>       There is no guarantee you'll ever hand over the control to the right thread upon notify()
>>>>>>     Alex
>>>>>> 
>>>>>> On 17/12/2014 14:07, suman shil wrote:
>>>>>>       Hi, Following is my solution to solve this problem. Please let me know if I am missing something.
>>>>>>    public class OrderedExecutor {  private int currentAllowedOrder = 0;  private int maxLength = 0;  public OrderedExecutor(int n)  {          this.maxLength = n;  } public synchronized Object execCriticalSectionInOrder( int order, Callable<Object> callable)                                 throws Exception  { if (order >= maxLength)  {  throw new Exception("Exceeds maximum order "+ maxLength);  }    while(order != currentAllowedOrder)  {  wait();  }    try  { currentAllowedOrder = currentAllowedOrder+1;  return callable.call();  }  finally  {  notify();  }  } }
>>>>>>    Regards Suman
>>>>>>        From: Peter Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>
>>>>>>   To: Hanson Char<hanson.char at gmail.com> <mailto:hanson.char at gmail.com>    Cc: concurrency-interest<concurrency-interest at cs.oswego.edu> <mailto:concurrency-interest at cs.oswego.edu>    Sent: Sunday, December 14, 2014 11:01 PM
>>>>>>   Subject: Re: [concurrency-interest] Enforcing ordered execution of critical sections?
>>>>>>           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>>      Hi Peter,
>>>>>>    Thanks for this proposed idea of using LockSupport. This begs the question: which one would you choose if you had all three (correct) implementation available?  (Semaphore, CountDownLatch, or LockSupport)?
>>>>>>    Regards, Hanson
>>>>>>     The Semaphore/CountDownLatch variants are equivalent if you don't need re-use. So any would do. They lack invalid-use detection. What happens if they are not used as intended? Semaphore variant acts differently than CountDownLatch variant. The low-level variant I  proposed detects invalid usage. So I would probably use this one. But the low level variant is harder to reason about it's correctness. I think it is correct, but you should show it to somebody else to confirm this.
>>>>>>     Another question is whether you actually need this kind of synchronizer. Maybe if you explained what you are trying to achieve, somebody could have an idea how to do that even more elegantly...
>>>>>>     Regards, Peter
>>>>>>              On Sun, Dec 14, 2014 at 9:01 AM, Peter Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>    Hi Hanson,
>>>>>>     This one is more low-level, but catches some invalid usages and is more resource-friendly:
>>>>>>       public class OrderedExecutor {
>>>>>>         public <T> T execCriticalSectionInOrder(
>>>>>>           final int order,
>>>>>>           final Supplier<T> criticalSection
>>>>>>       ) throws InterruptedException {
>>>>>>           if (order < 0) {
>>>>>>                throw new IllegalArgumentException("'order' should be >= 0");
>>>>>>           }
>>>>>>           if (order > 0) {
>>>>>>               waitForDone(order - 1);
>>>>>>           }
>>>>>>           try {
>>>>>>               return criticalSection.get();
>>>>>>           } finally {
>>>>>>               notifyDone(order);
>>>>>>           }
>>>>>>       }
>>>>>>         private static final Object DONE = new Object();
>>>>>>       private final ConcurrentMap<Integer, Object> signals = new ConcurrentHashMap<>();
>>>>>>         private void waitForDone(int order) throws InterruptedException {
>>>>>>           Object sig = signals.putIfAbsent(order, Thread.currentThread());
>>>>>>           if (sig != null && sig != DONE) {
>>>>>>               throw new IllegalStateException();
>>>>>>           }
>>>>>>           while (sig != DONE) {
>>>>>>               LockSupport.park();
>>>>>>               if (Thread.interrupted()) {
>>>>>>                   throw new InterruptedException();
>>>>>>               }
>>>>>>               sig = signals.get(order);
>>>>>>           }
>>>>>>       }
>>>>>>         private void notifyDone(int order) {
>>>>>>           Object sig = signals.putIfAbsent(order, DONE);
>>>>>>           if (sig instanceof Thread) {
>>>>>>               if (!signals.replace(order, sig, DONE)) {
>>>>>>                   throw new IllegalStateException();
>>>>>>               }
>>>>>>               LockSupport.unpark((Thread) sig);
>>>>>>           } else if (sig != null) {
>>>>>>               throw new IllegalStateException();
>>>>>>           }
>>>>>>       }
>>>>>>   }
>>>>>>       Regards, Peter
>>>>>>     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>>        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>>      Hi Peter,
>>>>>>    Thanks for the suggestion, and sorry about not being clear about one important  detail: "n" is not known a priori when constructing an OrderedExecutor.  Does this mean the use of CountDownLatch is ruled out?
>>>>>>     If you know at least the upper bound of 'n', it can be used with such 'n'. Otherwise something that dynamically re-sizes the array could be devised. Or you could simply use a ConcurrentHashMap instead of array where keys are 'order' values:
>>>>>>       public class OrderedExecutor<T> {
>>>>>>         private final ConcurrentMap<Integer, CountDownLatch> latches = new ConcurrentHashMap<>();
>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>                                           final Supplier<T> criticalSection) throws InterruptedException {
>>>>>>           if (order > 0) {
>>>>>>               latches.computeIfAbsent(order - 1, o -> new CountDownLatch(1)).await();
>>>>>>           }
>>>>>>           try {
>>>>>>               return criticalSection.get();
>>>>>>           } finally {
>>>>>>               latches.computeIfAbsent(order, o -> new CountDownLatch(1)).countDown();
>>>>>>           }
>>>>>>       }
>>>>>>   }
>>>>>>       Regards, Peter
>>>>>>           You guessed right: it's a one-shot object for a particular OrderedExecutor  instance, and "order" must be called indeed at most once.
>>>>>>    Regards, Hanson
>>>>>>   On Sun, Dec 14, 2014 at 2:21 AM, Peter Levart<peter.levart at gmail.com> <mailto:peter.levart at gmail.com>  wrote:
>>>>>>    Hi Hanson,
>>>>>>     I don't think anything like that readily exists  in java.lang.concurrent, but what you describe should be possible to  achieve with composition of existing primitives.  You haven't given any additional hints to what your OrderedExecutor                      should behave like. Should it be a one-shot object (like CountDownLatch) or a re-usable one (like CyclicBarrier)? Will execCriticalSectionInOrder() for a particular OrderedExecutor instance and 'order' value be called at most once? If yes (and I think that only a one-shot object  makes sense here), an array of CountDownLatch(es) could be used:
>>>>>>     public class OrderedExecutor<T> {
>>>>>>       private final CountDownLatch[] latches;
>>>>>>         public OrderedExecutor(int n) {
>>>>>>           if (n < 1) throw new IllegalArgumentException("'n'  should be >= 1");
>>>>>>           latches = new CountDownLatch[n - 1];
>>>>>>           for (int i = 0; i < latches.length; i++) {
>>>>>>               latches[i] = new CountDownLatch(1);
>>>>>>           }
>>>>>>       }
>>>>>>         public T execCriticalSectionInOrder(final int order,
>>>>>>                                            final Supplier<T> criticalSection) throws InterruptedException {
>>>>>>           if (order < 0 || order > latches.length)
>>>>>>               throw new IllegalArgumentException("'order' should be [0..." +  latches.length + "]");
>>>>>>           if (order > 0) {
>>>>>>               latches[order - 1].await();
>>>>>>           }
>>>>>>           try {
>>>>>>               return criticalSection.get();
>>>>>>           } finally {
>>>>>>               if (order < latches.length) {
>>>>>>                   latches[order].countDown();
>>>>>>               }
>>>>>>           }
>>>>>>       }
>>>>>>   }
>>>>>>       Regards, Peter
>>>>>>     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>>         Hi, I am looking for a construct that can  be used to efficiently enforce  ordered execution of multiple critical sections, each calling from a  different thread. The calling threads may run in  parallel and may call the execution method out of order. The  perceived construct would therefore be responsible for re-ordering the execution of those threads, so that their critical  sections (and only the critical section) will be executed in order. Would something  like the following API already exist? /** * Used to enforce ordered execution of critical sections calling from multiple *  threads, parking and unparking the  threads as necessary. */ public class                   OrderedExecutor<T> { /** * Executes a critical section at most once with the given order, parking * and  unparking the current thread as  necessary so that all critical * sections executed  by different threads using this  executor take place in * the order from 1 to n  consecutively. */ public T execCriticalSectionInOrder
>>>>>> (  final int order, final Callable<T> criticalSection) throws InterruptedException; } Regards, Hanson _______________________________________________Concurrency-interest mailing listConcurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> http://cs.oswego.edu/mailman/listinfo/concurrency-interest _______________________________________________
>>>>>>   Concurrency-interest mailing list
>>>>>>   Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/6dec4bff/attachment-0001.html>

From dl at cs.oswego.edu  Mon Dec 22 19:16:56 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 22 Dec 2014 19:16:56 -0500
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D7C78E@sm-ex-01-vm.guidewire.com>
References: <CAHzJPEro_OrYcEvFScx9QC20q32wfhyjGaMYP4b88JCkizNcqA@mail.gmail.com>	<CAHzJPEpDExXxjGA4wwtWreKtK=RqQKHEeKN+8H4EK1qDHtL+Vw@mail.gmail.com>	<CAHzJPEoCF9H2Jt=APV1jF2wEW5g88rH1p-z6+PNsDQ28UQwT4w@mail.gmail.com>	<CABWgujZzyDxggMCiL_N8gSTKN58g3703DHPL69-0u6iq+yJYYQ@mail.gmail.com>	<CAHzJPEqBnE1br0hwKwXgy0Rggsu67DdbnUHp1dsLJiasOevtuw@mail.gmail.com>	<CABWgujZO095hNjj2CenkiqhR2EJdV=CWGyuSeWma2MGCcuxavw@mail.gmail.com>	<CAHzJPEoyk-f4VMMGqn1x9QrAwNxDSWXH03LAv6uCbQonsUNUog@mail.gmail.com>	<CABWgujazrcvQT1t_rP5jC8nUKA=QK7MwOr0vhnSmsEMVPvQynA@mail.gmail.com>	<5496B0FE.7070502@gmail.com>	<CABWgujbtDFgDd7sNEYymhNLwpU56Es4tckwvyyTLFHGsD+=OoQ@mail.gmail.com>	<549737AB.8060508@gmail.com>	<CAHzJPEq8DwLtxZAEuMZfqfV2XH-i4WCmJC=C6joPqz2sQioqWg@mail.gmail.com>	<54982C60.1000008@cs.oswego.edu>
	<54984ACA.3050401@oracle.com> <549850BA.8070904@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D7C78E@sm-ex-01-vm.guidewire.com>
Message-ID: <5498B478.3060206@cs.oswego.edu>

On 12/22/2014 03:57 PM, Justin Sampson wrote:

> Is there any way for Semaphore's acquire() to propagate the notification if
> it wakes up and finds not enough permits, at least in the non-fair case?

Not any nice way.

The motivation for bulk-acquire/release methods was to
spare people from needing to write tricky/wrong one-by-one loops.
(For example those that try to acquire n, but if only m<n are
available, release m and maybe try again later.)
The need for them occasionally arises in the classic applications of
semaphores (like counting/controlling the number of available resources).
But we somehow failed to convey that these don't establish a priority
order. We'll fix.

>
> That is, if the acquire(7) thread gets unparked and sees that there are fewer
> than 7 but more than 0 permits available, can it unpark the next thread
> before parking itself again, so that the next thread can try its acquire(5)?
> In the worst case you end up cycling through all the waiting threads, which
> is nasty, but sounds better to me than deadlocking unnecessarily, and only
> happens when _all_ of the threads are requesting more than 1 permit in
> acquire().
>
> Of course, that would mean that using Semaphore to implement the
> OrderedExecutor idea in this thread ends up worse than many of the other
> solutions, except for its conciseness. :)
>

Right. If you want to wake up a lot of threads, it is already simple
enough to use monitor/notifyAll or Condition.signallAll.

-Doug


From dl at cs.oswego.edu  Mon Dec 22 19:48:47 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 22 Dec 2014 19:48:47 -0500
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <CAHzJPEo2kPcBz4W8JF1H+Y+FS7QpQXCr_o_4Z1RyFcBkksShCg@mail.gmail.com>
References: <CAHzJPEo2kPcBz4W8JF1H+Y+FS7QpQXCr_o_4Z1RyFcBkksShCg@mail.gmail.com>
Message-ID: <5498BBEF.8090708@cs.oswego.edu>

On 12/22/2014 02:19 PM, Joe Bowbeer wrote:

> I point out a few places where I think the Semaphore javadoc could be improved
> below.

Thanks!

>
> 1. This comment in the class description needs an even strong warning?
>
> "This class also provides convenience methods to acquire and release multiple
> permits at a time. Beware of the increased risk of indefinite postponement when
> these methods are used without fairness set true."

I can't think of anything better than to recast my posted example. Yes?

  * <p>This class also provides convenience methods to {@link
  * #acquire(int) acquire} and {@link #release(int) release} multiple
  * permits at a time. These methods are generally more efficient and
  * effective than loops. However, they do not establish any preference
  * order. For example, if thread A invokes @code{s.acquire(3}) and
  * thread B invokes @code{s.acquire(2)}, and two permits become
  * available, then there is no guarantee that thread B will obtain
  * them unless its acquire came first and Semaphore @code{s} is in
  * fair mode.
  *

> ... acquire(int)
> I think an additional "and" helps readability,

I agree; done.

> 3. The release(int) documentation should make it clearer that *only* one waiting
> thread is selected?
>
> "If any threads are trying to acquire permits, then one is selected and given
> the permits that were just released."

Just clarifying the referent might work at least as well ...

      * If any threads are trying to acquire permits, then one thread is

...with less risk of mis-interpreting or not reading the rest of
the method.

> 4. This statement in the release(int) documentation emboldened me:
>
> "There is no requirement that a thread that releases a permit must have acquired
> that permit by calling acquire. Correct usage of a semaphore is established by
> programming convention in the application."
>
> I'm not sure what to make of the second sentence.  Would this be a good place to
> advise against adding permits that cannot be acquired by any waiting thread?

We could just kill the second sentence if you think it does more harm
than good. I'm not sure why we included it. But I think the above
class-level explanation is a better place to put the non-prioritization
disclaimer.

-Doug


From joe.bowbeer at gmail.com  Mon Dec 22 20:17:09 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 22 Dec 2014 17:17:09 -0800
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <5498BBEF.8090708@cs.oswego.edu>
References: <CAHzJPEo2kPcBz4W8JF1H+Y+FS7QpQXCr_o_4Z1RyFcBkksShCg@mail.gmail.com>
	<5498BBEF.8090708@cs.oswego.edu>
Message-ID: <CAHzJPEow4shx6U3_kVbXKYA0xdH+pLk9ezHk46jwsY2ixKddGw@mail.gmail.com>

I like changes 1-3.  The example helps a lot.

I'd opt for leaving 4 as-is but would not mind at all if that second
sentence were removed.

On Mon, Dec 22, 2014 at 4:48 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/22/2014 02:19 PM, Joe Bowbeer wrote:
>
>  I point out a few places where I think the Semaphore javadoc could be
>> improved
>> below.
>>
>
> Thanks!
>
>
>> 1. This comment in the class description needs an even strong warning?
>>
>> "This class also provides convenience methods to acquire and release
>> multiple
>> permits at a time. Beware of the increased risk of indefinite
>> postponement when
>> these methods are used without fairness set true."
>>
>
> I can't think of anything better than to recast my posted example. Yes?
>
>  * <p>This class also provides convenience methods to {@link
>  * #acquire(int) acquire} and {@link #release(int) release} multiple
>  * permits at a time. These methods are generally more efficient and
>  * effective than loops. However, they do not establish any preference
>  * order. For example, if thread A invokes @code{s.acquire(3}) and
>  * thread B invokes @code{s.acquire(2)}, and two permits become
>  * available, then there is no guarantee that thread B will obtain
>  * them unless its acquire came first and Semaphore @code{s} is in
>  * fair mode.
>  *
>
>  ... acquire(int)
>> I think an additional "and" helps readability,
>>
>
> I agree; done.
>
>  3. The release(int) documentation should make it clearer that *only* one
>> waiting
>> thread is selected?
>>
>> "If any threads are trying to acquire permits, then one is selected and
>> given
>> the permits that were just released."
>>
>
> Just clarifying the referent might work at least as well ...
>
>      * If any threads are trying to acquire permits, then one thread is
>
> ...with less risk of mis-interpreting or not reading the rest of
> the method.
>
>  4. This statement in the release(int) documentation emboldened me:
>>
>> "There is no requirement that a thread that releases a permit must have
>> acquired
>> that permit by calling acquire. Correct usage of a semaphore is
>> established by
>> programming convention in the application."
>>
>> I'm not sure what to make of the second sentence.  Would this be a good
>> place to
>> advise against adding permits that cannot be acquired by any waiting
>> thread?
>>
>
> We could just kill the second sentence if you think it does more harm
> than good. I'm not sure why we included it. But I think the above
> class-level explanation is a better place to put the non-prioritization
> disclaimer.
>
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/9ba9053c/attachment.html>

From davidcholmes at aapt.net.au  Mon Dec 22 23:05:00 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 23 Dec 2014 14:05:00 +1000
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
	ordered execution of critical sections?]
In-Reply-To: <5498BBEF.8090708@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>

Doug Lea writes:
> On 12/22/2014 02:19 PM, Joe Bowbeer wrote:
>
> > I point out a few places where I think the Semaphore javadoc
> could be improved
> > below.
>
> Thanks!
>
> >
> > 1. This comment in the class description needs an even strong warning?
> >
> > "This class also provides convenience methods to acquire and
> release multiple
> > permits at a time. Beware of the increased risk of indefinite
> postponement when
> > these methods are used without fairness set true."
>
> I can't think of anything better than to recast my posted example. Yes?
>
>   * <p>This class also provides convenience methods to {@link
>   * #acquire(int) acquire} and {@link #release(int) release} multiple
>   * permits at a time. These methods are generally more efficient and
>   * effective than loops. However, they do not establish any preference
>   * order. For example, if thread A invokes @code{s.acquire(3}) and
>   * thread B invokes @code{s.acquire(2)}, and two permits become
>   * available, then there is no guarantee that thread B will obtain
>   * them unless its acquire came first and Semaphore @code{s} is in
>   * fair mode.
>   *
>
> > ... acquire(int)
> > I think an additional "and" helps readability,
>
> I agree; done.
>
> > 3. The release(int) documentation should make it clearer that
> *only* one waiting
> > thread is selected?
> >
> > "If any threads are trying to acquire permits, then one is
> selected and given
> > the permits that were just released."
>
> Just clarifying the referent might work at least as well ...
>
>       * If any threads are trying to acquire permits, then one thread is
>
> ...with less risk of mis-interpreting or not reading the rest of
> the method.
>
> > 4. This statement in the release(int) documentation emboldened me:
> >
> > "There is no requirement that a thread that releases a permit
> must have acquired
> > that permit by calling acquire. Correct usage of a semaphore is
> established by
> > programming convention in the application."
> >
> > I'm not sure what to make of the second sentence.  Would this
> be a good place to
> > advise against adding permits that cannot be acquired by any
> waiting thread?
>
> We could just kill the second sentence if you think it does more harm
> than good. I'm not sure why we included it. But I think the above
> class-level explanation is a better place to put the non-prioritization
> disclaimer.

We inclued it to make it clear that a Semapore is not a mutex and that there
are no rules on acquire and release in terms of which thread can do which.
It is perfectly fine for one thread to do all releases() and another to do
all acquires(). I don't understand what Joe means when he says "advise
against adding permits that cannot be acquired by any waiting thread?". Any
permit once added can be acquired.

I'm still trying to distill the essence of the current problem. It still
seems to teeter between doc bug and implementation bug to me. Will need to
check my records from when we specified this originally, as I'm sure the
atomicity of acquire(n) was subject to discussion.

David

> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From joe.bowbeer at gmail.com  Tue Dec 23 01:19:03 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 22 Dec 2014 22:19:03 -0800
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>
References: <5498BBEF.8090708@cs.oswego.edu>
	<NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>
Message-ID: <CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>

I meant to say something more like:

Advise against adding permits that cannot be acquired by every waiting
thread.  (Or maybe the "next" waiting thread?)

Anyway, I agree it is not clearly stated. I was trying to capture the idea
that is better illustrated in the example: where releasing 5 permits is not
going to help if the only thread selected needs 7.

My interpretation of what Semaphore does is now heavily influenced by what
AQS supports.  I'm glad someone is thinking about what Semaphore *should*
do in addition to what AQS *can* do.

--Joe

On Mon, Dec 22, 2014 at 8:05 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

> Doug Lea writes:
> > On 12/22/2014 02:19 PM, Joe Bowbeer wrote:
> >
> > > I point out a few places where I think the Semaphore javadoc
> > could be improved
> > > below.
> >
> > Thanks!
> >
> > >
> > > 1. This comment in the class description needs an even strong warning?
> > >
> > > "This class also provides convenience methods to acquire and
> > release multiple
> > > permits at a time. Beware of the increased risk of indefinite
> > postponement when
> > > these methods are used without fairness set true."
> >
> > I can't think of anything better than to recast my posted example. Yes?
> >
> >   * <p>This class also provides convenience methods to {@link
> >   * #acquire(int) acquire} and {@link #release(int) release} multiple
> >   * permits at a time. These methods are generally more efficient and
> >   * effective than loops. However, they do not establish any preference
> >   * order. For example, if thread A invokes @code{s.acquire(3}) and
> >   * thread B invokes @code{s.acquire(2)}, and two permits become
> >   * available, then there is no guarantee that thread B will obtain
> >   * them unless its acquire came first and Semaphore @code{s} is in
> >   * fair mode.
> >   *
> >
> > > ... acquire(int)
> > > I think an additional "and" helps readability,
> >
> > I agree; done.
> >
> > > 3. The release(int) documentation should make it clearer that
> > *only* one waiting
> > > thread is selected?
> > >
> > > "If any threads are trying to acquire permits, then one is
> > selected and given
> > > the permits that were just released."
> >
> > Just clarifying the referent might work at least as well ...
> >
> >       * If any threads are trying to acquire permits, then one thread is
> >
> > ...with less risk of mis-interpreting or not reading the rest of
> > the method.
> >
> > > 4. This statement in the release(int) documentation emboldened me:
> > >
> > > "There is no requirement that a thread that releases a permit
> > must have acquired
> > > that permit by calling acquire. Correct usage of a semaphore is
> > established by
> > > programming convention in the application."
> > >
> > > I'm not sure what to make of the second sentence.  Would this
> > be a good place to
> > > advise against adding permits that cannot be acquired by any
> > waiting thread?
> >
> > We could just kill the second sentence if you think it does more harm
> > than good. I'm not sure why we included it. But I think the above
> > class-level explanation is a better place to put the non-prioritization
> > disclaimer.
>
> We inclued it to make it clear that a Semapore is not a mutex and that
> there
> are no rules on acquire and release in terms of which thread can do which.
> It is perfectly fine for one thread to do all releases() and another to do
> all acquires(). I don't understand what Joe means when he says "advise
> against adding permits that cannot be acquired by any waiting thread?". Any
> permit once added can be acquired.
>
> I'm still trying to distill the essence of the current problem. It still
> seems to teeter between doc bug and implementation bug to me. Will need to
> check my records from when we specified this originally, as I'm sure the
> atomicity of acquire(n) was subject to discussion.
>
> David
>
> > -Doug
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141222/dbb4a785/attachment-0001.html>

From dl at cs.oswego.edu  Tue Dec 23 06:45:50 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 23 Dec 2014 06:45:50 -0500
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>
References: <5498BBEF.8090708@cs.oswego.edu>	<NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>
	<CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>
Message-ID: <549955EE.7050104@cs.oswego.edu>

On 12/23/2014 01:19 AM, Joe Bowbeer wrote:

>
> Anyway, I agree it is not clearly stated. I was trying to capture the idea that
> is better illustrated in the example: where releasing 5 permits is not going to
> help if the only thread selected needs 7.

Right. We just need to rule out any interpretation that waiters
are prioritized by counts. There is nothing in any method spec
that supports this interpretation. We had ineffectively discouraged
it by mentioning that bulk-acquire was a "convenience". But without
being explicit, prioritization is only implicitly ruled out,
for example by contemplating the impossibility of being both FIFO
and prioritized in fair mode. And so even experts
can be led to believe that the implementation might be wrong.
Hopefully the doc improvements will prevent this from happening again.

>
> My interpretation of what Semaphore does is now heavily influenced by what AQS
> supports.  I'm glad someone is thinking about what Semaphore *should* do in
> addition to what AQS *can* do.

Completely the opposite. AQS was built to simplify implementation
of a class of synchronizers, but we don't use it when it doesn't
apply.

One moral from all this is that we should contemplate
introduction of some kind of priority-based synchronizer.

-Doug



From dl at cs.oswego.edu  Tue Dec 23 08:39:38 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 23 Dec 2014 08:39:38 -0500
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <549955EE.7050104@cs.oswego.edu>
References: <5498BBEF.8090708@cs.oswego.edu>	<NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>	<CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>
	<549955EE.7050104@cs.oswego.edu>
Message-ID: <5499709A.3020709@cs.oswego.edu>

On 12/23/2014 06:45 AM, Doug Lea wrote:
> On 12/23/2014 01:19 AM, Joe Bowbeer wrote:
> Hopefully the doc improvements will prevent this from happening again.

I committed these to jsr166 CVS, plus the additional clarification in
acquire(int):

      * <p>Acquires the given number of permits, if they are available,
      * and returns immediately, reducing the number of available permits
      * by the given amount. This method has the same effect as the
      * loop {@code for (int i = 0; i < permits; ++i) acquire();} except
      * that it atomically acquires the permits all at once:

See http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Semaphore.html

-Doug




From hanson.char at gmail.com  Tue Dec 23 12:18:13 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 23 Dec 2014 09:18:13 -0800
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <5499709A.3020709@cs.oswego.edu>
References: <5498BBEF.8090708@cs.oswego.edu>
	<NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>
	<CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>
	<549955EE.7050104@cs.oswego.edu> <5499709A.3020709@cs.oswego.edu>
Message-ID: <CABWgujavOVc0WUh2+AJcu=66W9p6Tw88LNqD947mHcp3Mq-baQ@mail.gmail.com>

The modified javadoc on Semaphore looks much clearer now (about it's
limited use on multiple permits).  I especially like the added example.
Thanks !

> One moral from all this is that we should contemplate introduction of
some kind of priority-based synchronizer.

Sounds intriguing and useful.

Regards,
Hanson

On Tue, Dec 23, 2014 at 5:39 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/23/2014 06:45 AM, Doug Lea wrote:
>
>> On 12/23/2014 01:19 AM, Joe Bowbeer wrote:
>> Hopefully the doc improvements will prevent this from happening again.
>>
>
> I committed these to jsr166 CVS, plus the additional clarification in
> acquire(int):
>
>      * <p>Acquires the given number of permits, if they are available,
>      * and returns immediately, reducing the number of available permits
>      * by the given amount. This method has the same effect as the
>      * loop {@code for (int i = 0; i < permits; ++i) acquire();} except
>      * that it atomically acquires the permits all at once:
>
> See http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/
> concurrent/Semaphore.html
>
>
> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141223/20bad5e9/attachment.html>

From davidcholmes at aapt.net.au  Tue Dec 23 16:51:52 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 24 Dec 2014 07:51:52 +1000
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
	ordered execution of critical sections?]
In-Reply-To: <5499709A.3020709@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEAKKMAA.davidcholmes@aapt.net.au>

Hi Doug,

I still need to go back and check historical notes but this all seems wrong
to me.

> On 12/23/2014 06:45 AM, Doug Lea wrote:
> > On 12/23/2014 01:19 AM, Joe Bowbeer wrote:
> > Hopefully the doc improvements will prevent this from happening again.
>
> I committed these to jsr166 CVS, plus the additional clarification in
> acquire(int):
>
>       * <p>Acquires the given number of permits, if they are available,
>       * and returns immediately, reducing the number of available permits
>       * by the given amount. This method has the same effect as the
>       * loop {@code for (int i = 0; i < permits; ++i) acquire();} except
>       * that it atomically acquires the permits all at once:

To me the "except" part nullifies the initial statement. What does it mean
to acquire the permits atomically rather than in a loop? The loop can cause
permits to be "reserved" while an atomic acquire of all the permits at once
would not.

> See
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
Semaphore.html

Again the example seems wrong to me. If the Semaphore is unfair then all
permits are available to any acquirer. So as soon as two permits are
available the acquire(2) can proceed. In contrast a fair Semaphore ensures
only the head waiter can acquire permits.

David



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jsampson at guidewire.com  Tue Dec 23 17:12:33 2014
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 23 Dec 2014 22:12:33 +0000
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <549955EE.7050104@cs.oswego.edu>
References: <5498BBEF.8090708@cs.oswego.edu>
	<NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>
	<CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>
	<549955EE.7050104@cs.oswego.edu>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D7CA83@sm-ex-01-vm.guidewire.com>

Doug Lea wrote:

> Right. We just need to rule out any interpretation that waiters
> are prioritized by counts. There is nothing in any method spec
> that supports this interpretation.

Hmm, I don't think it's really an expectation of "priority" or
"preference." It's simply a (mistaken, apparently) expectation that
if _any_ threads can make progress then _some_ such thread will be
allowed to do so.

> We had ineffectively discouraged it by mentioning that
> bulk-acquire was a "convenience". But without being explicit,
> prioritization is only implicitly ruled out, for example by
> contemplating the impossibility of being both FIFO and prioritized
> in fair mode.

Maybe FIFO + "priority" is impossible, but FIFO + "progress" is not
necessarily so... The behavior _could have been_ that the earliest
thread in the queue _that can make progress_ is selected. Instead,
the earliest thread in the queue is selected, even if it can't make
progress.

And if I understand correctly, the latter is true in non-fair mode
as well, with the sole exception of the occasional barging thread
that doesn't even end up on the queue at all.

Actually, that reminds me of something else in the docs that struck
me as odd. The docs say that when a thread barges in, "logically the
new thread places itself at the head of the queue of waiting
threads." But isn't the actual behavior that a _successful_ barging
thread never touches the queue and an _unsuccessful_ barging thread
ends up at the tail of the queue just as if it hadn't tried to
barge? The head of the queue is not actually involved in either case
so it seems confusing (to me) to claim that it is "logically" so.

> And so even experts can be led to believe that the implementation
> might be wrong.

I'm not an expert, but the thing that's really bugging me, the more
that I think about this, is the claim that acquire(int) is just like
a loop but "atomic." That would seem to imply that it acquires
either all of the desired permits or none of them. That would
further seem to imply that any existing permits remain available for
other threads to acquire. The surprising thing is that the permits
are _officially_ still "available" but _effectively_ no other thread
can acquire them, except by barging!

So perhaps that's the essence of what needs to be communicated: If a
thread calls acquire(N) when M < N permits are available, it
effectively places a hold on those M permits until the entire N
permits are available, with the sole exception that in non-fair mode
another thread might acquire the held permits by barging.

Cheers,
Justin



From akarnokd at gmail.com  Tue Dec 23 17:50:09 2014
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Tue, 23 Dec 2014 23:50:09 +0100
Subject: [concurrency-interest] Can a volatile read be reordered before a
	lazySet?
Message-ID: <CAAWwtm89vvUcuzRucpVe8nnbY5Q4uPG1HvkGpV4rwBsR9A2fZw@mail.gmail.com>

Hello,

Given two atomic values held by an AtomicLong and an AtomicReference, is it
possible the following two lines may be reordered?

theLong.lazySet(theLong.get() + 1);
Object o = theRef.get();

On X86, the memory model states that reads may be reordered with older
writes to different locations, so the algorithm fragment above might be
broken. For context, this code is part of a single-producer structure wich
protects the use of theRef value with an ingress/egress counter pair.

I wonder if the same reordering might happen in a classical atomic
ping-pong example:

ping.lazySet(1);
while (pong.get() == 0);

while (ping.get() == 0);
pong.lazySet(1);

i.e., in both threads, the while loops end up before the lazySet and thus
deadlocking.

Best regards,
David Karnok


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141223/8dec8826/attachment.html>

From davidcholmes at aapt.net.au  Tue Dec 23 18:54:06 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 24 Dec 2014 09:54:06 +1000
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
	ordered execution of critical sections?]
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D7CA83@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEALKMAA.davidcholmes@aapt.net.au>

Justin Sampson writes:
> Doug Lea wrote:
>
> > Right. We just need to rule out any interpretation that waiters
> > are prioritized by counts. There is nothing in any method spec
> > that supports this interpretation.
>
> Hmm, I don't think it's really an expectation of "priority" or
> "preference." It's simply a (mistaken, apparently) expectation that
> if _any_ threads can make progress then _some_ such thread will be
> allowed to do so.

I tend to agree that referring to this as a priority issue is quite
misleading. In non-fair mode the "queue" is logically just an assemblage of
waiting threads - there is no specified order even if the implementation
constructs one. The problem we have, based on previous discussion, is that
when a waiting thread is given the opportunity to acquire M permits but
needs N, then the opportunity is not given to a second thread to acquire M
(and so on). In a simple wait/notify solution (as earlier shown) this simply
amounts to doing a notifyAll when a release is made.

> > We had ineffectively discouraged it by mentioning that
> > bulk-acquire was a "convenience". But without being explicit,
> > prioritization is only implicitly ruled out, for example by
> > contemplating the impossibility of being both FIFO and prioritized
> > in fair mode.
>
> Maybe FIFO + "priority" is impossible, but FIFO + "progress" is not
> necessarily so... The behavior _could have been_ that the earliest
> thread in the queue _that can make progress_ is selected. Instead,
> the earliest thread in the queue is selected, even if it can't make
> progress.
>
> And if I understand correctly, the latter is true in non-fair mode
> as well, with the sole exception of the occasional barging thread
> that doesn't even end up on the queue at all.

Note that barging via tryAcquire, happens regardless of fairness mode.
Otherwise IIRC barging is only an issue for non-fair mode.

> Actually, that reminds me of something else in the docs that struck
> me as odd. The docs say that when a thread barges in, "logically the
> new thread places itself at the head of the queue of waiting
> threads." But isn't the actual behavior that a _successful_ barging
> thread never touches the queue and an _unsuccessful_ barging thread
> ends up at the tail of the queue just as if it hadn't tried to
> barge? The head of the queue is not actually involved in either case
> so it seems confusing (to me) to claim that it is "logically" so.

Depends on the original logical model that is set up. You can model things
as-if a barging thread was placed at the head of the queue - but the
implementation doesn't actually do that.

> > And so even experts can be led to believe that the implementation
> > might be wrong.
>
> I'm not an expert, but the thing that's really bugging me, the more
> that I think about this, is the claim that acquire(int) is just like
> a loop but "atomic." That would seem to imply that it acquires
> either all of the desired permits or none of them. That would
> further seem to imply that any existing permits remain available for
> other threads to acquire. The surprising thing is that the permits
> are _officially_ still "available" but _effectively_ no other thread
> can acquire them, except by barging!

I don't like to think if it in terms of barging, but IIUC given 2 available
permits and a thread waiting for 3 and a thread waiting for 2, then a second
thread calling acquire(2) can proceed to get the permits. This would be
"barging" if the waiting thread had been woken up, but it wasn't.

David

> So perhaps that's the essence of what needs to be communicated: If a
> thread calls acquire(N) when M < N permits are available, it
> effectively places a hold on those M permits until the entire N
> permits are available, with the sole exception that in non-fair mode
> another thread might acquire the held permits by barging.
>
> Cheers,
> Justin
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Tue Dec 23 19:42:42 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 23 Dec 2014 19:42:42 -0500
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEAKKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEAKKMAA.davidcholmes@aapt.net.au>
Message-ID: <549A0C02.4040006@cs.oswego.edu>

On 12/23/2014 04:51 PM, David Holmes wrote:

> I still need to go back and check historical notes but this all seems wrong
> to me.

I'm confident about the original intention/spec of this method.
But I'm also sympathetic about exploring new methods/classes
so that people don't try to abuse a method designed to help
people avoid backout mechanics as if it were a priority-semaphore
or a way to cause N wakeups per release.

>>        * <p>Acquires the given number of permits, if they are available,
>>        * and returns immediately, reducing the number of available permits
>>        * by the given amount. This method has the same effect as the
>>        * loop {@code for (int i = 0; i < permits; ++i) acquire();} except
>>        * that it atomically acquires the permits all at once:
>
> To me the "except" part nullifies the initial statement. What does it mean
> to acquire the permits atomically rather than in a loop?

One way to think about it is that there is a queue of "holes"
that can be filled with permits, and that you've reserved a
consecutive range of them. But you don't know where that range lies
unless semaphore is fair and you know the invocation order.

>
> Again the example seems wrong to me. If the Semaphore is unfair then all
> permits are available to any acquirer.

But unfair just means "the implementation picks the ordering".

-Doug





From tkountis at gmail.com  Tue Dec 23 21:17:54 2014
From: tkountis at gmail.com (Thomas Kountis)
Date: Wed, 24 Dec 2014 02:17:54 +0000
Subject: [concurrency-interest] Can a volatile read be reordered before
	a lazySet?
In-Reply-To: <CAAWwtm89vvUcuzRucpVe8nnbY5Q4uPG1HvkGpV4rwBsR9A2fZw@mail.gmail.com>
References: <CAAWwtm89vvUcuzRucpVe8nnbY5Q4uPG1HvkGpV4rwBsR9A2fZw@mail.gmail.com>
Message-ID: <CAHbGfh1HutpobaRWt+yp0jk=OjSk4-2=uPGhaJFbVo5wGfUiRw@mail.gmail.com>

David,

AFAIK all the atomic* members that Java provides, pretty much provide the
same guarantees
as the volatile keyword does - which doesn't allow re-orderings on the
compiler side, neither during execution.
If you look behind the covers, the fields that those two wrappers work on,
are marked volatile as well. Also, lazy-set
is using the Unsafe.putOrdered...() which relies on a store-store barrier
and will prevent any instruction re-ordering (as the name suggests).

So, to answer your question, I believe you are safe as far as it concerns
re-orderings on your example.

t.

On Tue, Dec 23, 2014 at 10:50 PM, D?vid Karnok <akarnokd at gmail.com> wrote:
>
> Hello,
>
> Given two atomic values held by an AtomicLong and an AtomicReference, is
> it possible the following two lines may be reordered?
>
> theLong.lazySet(theLong.get() + 1);
> Object o = theRef.get();
>
> On X86, the memory model states that reads may be reordered with older
> writes to different locations, so the algorithm fragment above might be
> broken. For context, this code is part of a single-producer structure wich
> protects the use of theRef value with an ingress/egress counter pair.
>
> I wonder if the same reordering might happen in a classical atomic
> ping-pong example:
>
> ping.lazySet(1);
> while (pong.get() == 0);
>
> while (ping.get() == 0);
> pong.lazySet(1);
>
> i.e., in both threads, the while loops end up before the lazySet and thus
> deadlocking.
>
> Best regards,
> David Karnok
>
>
> --
> Best regards,
> David Karnok
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141224/b7bf5830/attachment-0001.html>

From dl at cs.oswego.edu  Wed Dec 24 07:16:53 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 24 Dec 2014 07:16:53 -0500
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D7CA83@sm-ex-01-vm.guidewire.com>
References: <5498BBEF.8090708@cs.oswego.edu>	<NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>	<CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>
	<549955EE.7050104@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D7CA83@sm-ex-01-vm.guidewire.com>
Message-ID: <549AAEB5.4000500@cs.oswego.edu>

On 12/23/2014 05:12 PM, Justin Sampson wrote:

> The behavior _could have been_ that the earliest
> thread in the queue _that can make progress_ is selected. Instead,
> the earliest thread in the queue is selected, even if it can't make
> progress.

The spec does not even guarantee that the earliest is selected
unless in fair mode. The implementation just so happens to
use a mostly-FIFO-with-barging algorithm because it provides
good throughput. But if we discovered that, say, randomized
selection had better throughput, we'd switch to it. (Aside:
one performance issue with AQS-based synchronizers
is that they haven't adapted to spin more to compensate
for increasing context-switch overhead on most platforms.
We'll probably address this.)

I agree that the spec could have made stronger promises,
and that this would have been useful in priority-based
applications. But doing so would require a new method or
more likely a new class.

> Actually, that reminds me of something else in the docs that struck
> me as odd. The docs say that when a thread barges in, "logically the
> new thread places itself at the head of the queue of waiting
> threads." But isn't the actual behavior that a _successful_ barging
> thread never touches the queue and an _unsuccessful_ barging thread
> ends up at the tail of the queue just as if it hadn't tried to
> barge? The head of the queue is not actually involved in either case
> so it seems confusing (to me) to claim that it is "logically" so.

Sorry; but I'm not sure of a better way to explain this without
overly constraining implementations. The only reason for mentioning
barging at all in spec is to explain why non-fair tryAcquire
may succeed even if there are waiting threads.

> The surprising thing is that the permits
> are _officially_ still "available" but _effectively_ no other thread
> can acquire them, except by barging!

Again, this is unlikely to be surprising to the intended
audience (that was not communicated well): Users occasionally
needing to acquire multiple permits without needing to hand-craft
backout loops.

-Doug



From dl at cs.oswego.edu  Wed Dec 24 10:05:37 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 24 Dec 2014 10:05:37 -0500
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
Message-ID: <549AD641.6020304@cs.oswego.edu>

On 12/13/2014 11:26 PM, Hanson Char wrote:
> I am looking for a construct that can be used to efficiently enforce
> ordered execution of multiple critical sections, each calling from a
> different thread. The calling threads may run in parallel and may call
> the execution method out of order.

Looking at this again while contemplating the need for
priority-based semaphores...

Have you considered creating a separate single-threaded
executor using a PriorityBlockingQueue? You'd need to
define a comparator for ordering, and then construct
via something like

Executor ex = new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS,
  new PriorityBlockingQueue<Runnable>(initialCap, taskComparator)));

You'd then use this executor only for the execCriticalSection
tasks.

-Doug

>
> /**
>   * Used to enforce ordered execution of critical sections calling from multiple
>   * threads, parking and unparking the threads as necessary.
>   */
> public class OrderedExecutor<T> {
>      /**
>       * Executes a critical section at most once with the given order, parking
>       * and unparking the current thread as necessary so that all critical
>       * sections executed by different threads using this executor take place in
>       * the order from 1 to n consecutively.
>       */
>      public T execCriticalSectionInOrder(final int order,
>              final Callable<T> criticalSection) throws InterruptedException;
> }
>
> Regards,
> Hanson
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From joe.bowbeer at gmail.com  Wed Dec 24 11:02:51 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 24 Dec 2014 08:02:51 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <549AD641.6020304@cs.oswego.edu>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<549AD641.6020304@cs.oswego.edu>
Message-ID: <CAHzJPEoKF=GXa5=kxw6J9_AKQP0-8C7K0UVquaF7nByiTL3bQA@mail.gmail.com>

Apparently the critical sections need to execute exactly in their pre
assigned orders: 1, 2, 3, etc.

An executor with an ordered queue would exec the first arrival first.

On Wednesday, December 24, 2014, Doug Lea <dl at cs.oswego.edu> wrote:

> On 12/13/2014 11:26 PM, Hanson Char wrote:
>
>> I am looking for a construct that can be used to efficiently enforce
>> ordered execution of multiple critical sections, each calling from a
>> different thread. The calling threads may run in parallel and may call
>> the execution method out of order.
>>
>
> Looking at this again while contemplating the need for
> priority-based semaphores...
>
> Have you considered creating a separate single-threaded
> executor using a PriorityBlockingQueue? You'd need to
> define a comparator for ordering, and then construct
> via something like
>
> Executor ex = new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS,
>  new PriorityBlockingQueue<Runnable>(initialCap, taskComparator)));
>
> You'd then use this executor only for the execCriticalSection
> tasks.
>
> -Doug
>
>
>> /**
>>   * Used to enforce ordered execution of critical sections calling from
>> multiple
>>   * threads, parking and unparking the threads as necessary.
>>   */
>> public class OrderedExecutor<T> {
>>      /**
>>       * Executes a critical section at most once with the given order,
>> parking
>>       * and unparking the current thread as necessary so that all critical
>>       * sections executed by different threads using this executor take
>> place in
>>       * the order from 1 to n consecutively.
>>       */
>>      public T execCriticalSectionInOrder(final int order,
>>              final Callable<T> criticalSection) throws
>> InterruptedException;
>> }
>>
>> Regards,
>> Hanson
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141224/4e623d7a/attachment.html>

From hanson.char at gmail.com  Wed Dec 24 11:20:57 2014
From: hanson.char at gmail.com (Hanson Char)
Date: Wed, 24 Dec 2014 08:20:57 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEoKF=GXa5=kxw6J9_AKQP0-8C7K0UVquaF7nByiTL3bQA@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<549AD641.6020304@cs.oswego.edu>
	<CAHzJPEoKF=GXa5=kxw6J9_AKQP0-8C7K0UVquaF7nByiTL3bQA@mail.gmail.com>
Message-ID: <CABWgujYLa9i6jNcRJQbXwZKN8nYQcDjUGjb3_CU3pEXHWWRGwQ@mail.gmail.com>

Yes, thanks for clarifying the use case.

Regards,
Hanson

On Wed, Dec 24, 2014 at 8:02 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> Apparently the critical sections need to execute exactly in their pre
> assigned orders: 1, 2, 3, etc.
>
> An executor with an ordered queue would exec the first arrival first.
>
>
> On Wednesday, December 24, 2014, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 12/13/2014 11:26 PM, Hanson Char wrote:
>>
>>> I am looking for a construct that can be used to efficiently enforce
>>> ordered execution of multiple critical sections, each calling from a
>>> different thread. The calling threads may run in parallel and may call
>>> the execution method out of order.
>>>
>>
>> Looking at this again while contemplating the need for
>> priority-based semaphores...
>>
>> Have you considered creating a separate single-threaded
>> executor using a PriorityBlockingQueue? You'd need to
>> define a comparator for ordering, and then construct
>> via something like
>>
>> Executor ex = new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS,
>>  new PriorityBlockingQueue<Runnable>(initialCap, taskComparator)));
>>
>> You'd then use this executor only for the execCriticalSection
>> tasks.
>>
>> -Doug
>>
>>
>>> /**
>>>   * Used to enforce ordered execution of critical sections calling from
>>> multiple
>>>   * threads, parking and unparking the threads as necessary.
>>>   */
>>> public class OrderedExecutor<T> {
>>>      /**
>>>       * Executes a critical section at most once with the given order,
>>> parking
>>>       * and unparking the current thread as necessary so that all
>>> critical
>>>       * sections executed by different threads using this executor take
>>> place in
>>>       * the order from 1 to n consecutively.
>>>       */
>>>      public T execCriticalSectionInOrder(final int order,
>>>              final Callable<T> criticalSection) throws
>>> InterruptedException;
>>> }
>>>
>>> Regards,
>>> Hanson
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141224/5da61824/attachment.html>

From gregg at wonderly.org  Thu Dec 25 02:26:40 2014
From: gregg at wonderly.org (Gregg Wonderly)
Date: Thu, 25 Dec 2014 01:26:40 -0600
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEALKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEALKMAA.davidcholmes@aapt.net.au>
Message-ID: <549BBC30.1080403@wonderly.org>

Why wouldn't the best implementation be something like the following?  What 
specific nuances of all
the described variations are necessary for functional code, verses side effects 
of hardware limitations or
implementations that must be dealt with by not guaranteeing progress in some 
semi fair fashion?

void acquire(int n) {
     int m;
     synchronized( this ) {
         do {
             // perform up to n acquires and return what we could get
             m = tryAcquire(n);
             if( m < n ) {
                 // release what we got since we didn't get everything we needed
                 // release() will wake up everyone
                 release(m);
                 // get in line to see the next set of changes
                 wait(this);
             }
         } while( m < n );
     }
}

Gregg

On 12/23/2014 5:54 PM, David Holmes wrote:
> Justin Sampson writes:
>> Doug Lea wrote:
>>
>>> Right. We just need to rule out any interpretation that waiters
>>> are prioritized by counts. There is nothing in any method spec
>>> that supports this interpretation.
>> Hmm, I don't think it's really an expectation of "priority" or
>> "preference." It's simply a (mistaken, apparently) expectation that
>> if _any_ threads can make progress then _some_ such thread will be
>> allowed to do so.
> I tend to agree that referring to this as a priority issue is quite
> misleading. In non-fair mode the "queue" is logically just an assemblage of
> waiting threads - there is no specified order even if the implementation
> constructs one. The problem we have, based on previous discussion, is that
> when a waiting thread is given the opportunity to acquire M permits but
> needs N, then the opportunity is not given to a second thread to acquire M
> (and so on). In a simple wait/notify solution (as earlier shown) this simply
> amounts to doing a notifyAll when a release is made.
>
>>> We had ineffectively discouraged it by mentioning that
>>> bulk-acquire was a "convenience". But without being explicit,
>>> prioritization is only implicitly ruled out, for example by
>>> contemplating the impossibility of being both FIFO and prioritized
>>> in fair mode.
>> Maybe FIFO + "priority" is impossible, but FIFO + "progress" is not
>> necessarily so... The behavior _could have been_ that the earliest
>> thread in the queue _that can make progress_ is selected. Instead,
>> the earliest thread in the queue is selected, even if it can't make
>> progress.
>>
>> And if I understand correctly, the latter is true in non-fair mode
>> as well, with the sole exception of the occasional barging thread
>> that doesn't even end up on the queue at all.
> Note that barging via tryAcquire, happens regardless of fairness mode.
> Otherwise IIRC barging is only an issue for non-fair mode.
>
>> Actually, that reminds me of something else in the docs that struck
>> me as odd. The docs say that when a thread barges in, "logically the
>> new thread places itself at the head of the queue of waiting
>> threads." But isn't the actual behavior that a _successful_ barging
>> thread never touches the queue and an _unsuccessful_ barging thread
>> ends up at the tail of the queue just as if it hadn't tried to
>> barge? The head of the queue is not actually involved in either case
>> so it seems confusing (to me) to claim that it is "logically" so.
> Depends on the original logical model that is set up. You can model things
> as-if a barging thread was placed at the head of the queue - but the
> implementation doesn't actually do that.
>
>>> And so even experts can be led to believe that the implementation
>>> might be wrong.
>> I'm not an expert, but the thing that's really bugging me, the more
>> that I think about this, is the claim that acquire(int) is just like
>> a loop but "atomic." That would seem to imply that it acquires
>> either all of the desired permits or none of them. That would
>> further seem to imply that any existing permits remain available for
>> other threads to acquire. The surprising thing is that the permits
>> are _officially_ still "available" but _effectively_ no other thread
>> can acquire them, except by barging!
> I don't like to think if it in terms of barging, but IIUC given 2 available
> permits and a thread waiting for 3 and a thread waiting for 2, then a second
> thread calling acquire(2) can proceed to get the permits. This would be
> "barging" if the waiting thread had been woken up, but it wasn't.
>
> David
>
>> So perhaps that's the essence of what needs to be communicated: If a
>> thread calls acquire(N) when M < N permits are available, it
>> effectively places a hold on those M permits until the entire N
>> permits are available, with the sole exception that in non-fair mode
>> another thread might acquire the held permits by barging.
>>
>> Cheers,
>> Justin
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From jh at squareup.com  Thu Dec 25 21:51:35 2014
From: jh at squareup.com (Josh Humphries)
Date: Thu, 25 Dec 2014 21:51:35 -0500
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHzJPEoKF=GXa5=kxw6J9_AKQP0-8C7K0UVquaF7nByiTL3bQA@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<549AD641.6020304@cs.oswego.edu>
	<CAHzJPEoKF=GXa5=kxw6J9_AKQP0-8C7K0UVquaF7nByiTL3bQA@mail.gmail.com>
Message-ID: <CAHJZN-sbTT1sB5W6t3GRRnRD5V=6_u5cOG2fLv=sah1Byk7FNw@mail.gmail.com>

In that case, you could use a DelayQueue, which is how scheduled executors
avoid executing the first arrival if it's scheduled for the future.

You could adapt submitted Runnables as instances of Delayed so that the
delay is computed as their index minus that of the next to be processed
(which starts at zero and increments after each task).

Though at that point, it may be no simpler than just a custom execution
mechanism as already discussed here...


----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Wed, Dec 24, 2014 at 11:02 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> Apparently the critical sections need to execute exactly in their pre
> assigned orders: 1, 2, 3, etc.
>
> An executor with an ordered queue would exec the first arrival first.
>
>
> On Wednesday, December 24, 2014, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 12/13/2014 11:26 PM, Hanson Char wrote:
>>
>>> I am looking for a construct that can be used to efficiently enforce
>>> ordered execution of multiple critical sections, each calling from a
>>> different thread. The calling threads may run in parallel and may call
>>> the execution method out of order.
>>>
>>
>> Looking at this again while contemplating the need for
>> priority-based semaphores...
>>
>> Have you considered creating a separate single-threaded
>> executor using a PriorityBlockingQueue? You'd need to
>> define a comparator for ordering, and then construct
>> via something like
>>
>> Executor ex = new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS,
>>  new PriorityBlockingQueue<Runnable>(initialCap, taskComparator)));
>>
>> You'd then use this executor only for the execCriticalSection
>> tasks.
>>
>> -Doug
>>
>>
>>> /**
>>>   * Used to enforce ordered execution of critical sections calling from
>>> multiple
>>>   * threads, parking and unparking the threads as necessary.
>>>   */
>>> public class OrderedExecutor<T> {
>>>      /**
>>>       * Executes a critical section at most once with the given order,
>>> parking
>>>       * and unparking the current thread as necessary so that all
>>> critical
>>>       * sections executed by different threads using this executor take
>>> place in
>>>       * the order from 1 to n consecutively.
>>>       */
>>>      public T execCriticalSectionInOrder(final int order,
>>>              final Callable<T> criticalSection) throws
>>> InterruptedException;
>>> }
>>>
>>> Regards,
>>> Hanson
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141225/0ba43d7d/attachment.html>

From joe.bowbeer at gmail.com  Thu Dec 25 22:59:07 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 25 Dec 2014 19:59:07 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
	sections?
In-Reply-To: <CAHJZN-sbTT1sB5W6t3GRRnRD5V=6_u5cOG2fLv=sah1Byk7FNw@mail.gmail.com>
References: <CABWgujaFw_B+E6A9TPWEOxkkks3GB=xSyJHLoSYo0qt_DEijJg@mail.gmail.com>
	<549AD641.6020304@cs.oswego.edu>
	<CAHzJPEoKF=GXa5=kxw6J9_AKQP0-8C7K0UVquaF7nByiTL3bQA@mail.gmail.com>
	<CAHJZN-sbTT1sB5W6t3GRRnRD5V=6_u5cOG2fLv=sah1Byk7FNw@mail.gmail.com>
Message-ID: <CAHzJPErFoiVvY0BrgLBpXd1b2M2VW411EFu3bevfARGEju-p3Q@mail.gmail.com>

Thanks for the pointer to DelayQueue.  The PriorityQueue<Condition>
synchronizer thing I wrote looks similar to a DelayQueue on the inside.
One functions like a sequenced lock, the other functions as a
time-sequenced queue.

I agree that using DelayQueue directly would be a bit of a stretch.  I
think it would be better to implement an InOrderQueue instead, and I think
its implementation would be a little simpler than DelayQueue.

On Thu, Dec 25, 2014 at 6:51 PM, Josh Humphries <jh at squareup.com> wrote:

> In that case, you could use a DelayQueue, which is how scheduled executors
> avoid executing the first arrival if it's scheduled for the future.
>
> You could adapt submitted Runnables as instances of Delayed so that the
> delay is computed as their index minus that of the next to be processed
> (which starts at zero and increments after each task).
>
> Though at that point, it may be no simpler than just a custom execution
> mechanism as already discussed here...
>
>
> ----
> *Josh Humphries*
> Manager, Shared Systems  |  Platform Engineering
> Atlanta, GA  |  678-400-4867
> *Square* (www.squareup.com)
>
> On Wed, Dec 24, 2014 at 11:02 AM, Joe Bowbeer <joe.bowbeer at gmail.com>
> wrote:
>
>> Apparently the critical sections need to execute exactly in their pre
>> assigned orders: 1, 2, 3, etc.
>>
>> An executor with an ordered queue would exec the first arrival first.
>>
>>
>> On Wednesday, December 24, 2014, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>>> On 12/13/2014 11:26 PM, Hanson Char wrote:
>>>
>>>> I am looking for a construct that can be used to efficiently enforce
>>>> ordered execution of multiple critical sections, each calling from a
>>>> different thread. The calling threads may run in parallel and may call
>>>> the execution method out of order.
>>>>
>>>
>>> Looking at this again while contemplating the need for
>>> priority-based semaphores...
>>>
>>> Have you considered creating a separate single-threaded
>>> executor using a PriorityBlockingQueue? You'd need to
>>> define a comparator for ordering, and then construct
>>> via something like
>>>
>>> Executor ex = new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS,
>>>  new PriorityBlockingQueue<Runnable>(initialCap, taskComparator)));
>>>
>>> You'd then use this executor only for the execCriticalSection
>>> tasks.
>>>
>>> -Doug
>>>
>>>
>>>> /**
>>>>   * Used to enforce ordered execution of critical sections calling from
>>>> multiple
>>>>   * threads, parking and unparking the threads as necessary.
>>>>   */
>>>> public class OrderedExecutor<T> {
>>>>      /**
>>>>       * Executes a critical section at most once with the given order,
>>>> parking
>>>>       * and unparking the current thread as necessary so that all
>>>> critical
>>>>       * sections executed by different threads using this executor take
>>>> place in
>>>>       * the order from 1 to n consecutively.
>>>>       */
>>>>      public T execCriticalSectionInOrder(final int order,
>>>>              final Callable<T> criticalSection) throws
>>>> InterruptedException;
>>>> }
>>>>
>>>> Regards,
>>>> Hanson
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141225/b87dab32/attachment.html>

From jsampson at guidewire.com  Fri Dec 26 05:59:51 2014
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 26 Dec 2014 10:59:51 +0000
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <549AAEB5.4000500@cs.oswego.edu>
References: <5498BBEF.8090708@cs.oswego.edu>
	<NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>
	<CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>
	<549955EE.7050104@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D7CA83@sm-ex-01-vm.guidewire.com>
	<549AAEB5.4000500@cs.oswego.edu>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D7DC09@sm-ex-01-vm.guidewire.com>

My point about barging was that if the queue is an implementation
detail, perhaps it's best not to mention it at all, especially in
the non-fair case.

I'm not personally advocating for any stronger promises in the
specification, I'm just jumping in with the hope of clarifying
something that seemed weird to me (and, I believe, to several other
folks in this discussion). Calling acquire(N) "merely a convenience"
or "just like a loop but atomic" really doesn't do justice to its
actual semantics in either fair or non-fair mode.

Here's an attempt at describing my own current understanding, at
this point in the discussion, of Semaphore's intended semantics, in
a minimally-constrained way, without using the words 'queue',
'atomic', 'barging', or 'convenience'. :)

1. Any acquire or tryAcquire call for N permits will either succeed
   in acquiring exactly N permits or will not acquire any permits.

2. If any acquire or tryAcquire call returns false or throws
   InterruptedException, that call will not have acquired any
   permits.

3. When fair == true and there are P > 0 available permits:

   3a. If the acquire or tryAcquire call that has been waiting the
       longest is for N <= P permits, then it will succeed.
   3b. If the acquire or tryAcquire call that has been waiting the
       longest is for N > P permits, then no acquire or tryAcquire
       call will succeed until more permits are released, except as
       described in 3c.
   3c. A zero- or one-argument tryAcquire call for N <= P permits
       may succeed immediately even if there are other acquire or
       tryAcquire calls already waiting.

4. When fair == false and there are P > 0 available permits:

   4a. If every waiting acquire or tryAcquire call is for N <= P
       permits, then one of them will succeed.
   4b. If any waiting acquire or tryAcquire call is for N > P
       permits, then it is possible that no waiting acquire or
       tryAcquire call will succeed until more permits are released.
   4c. Any acquire or tryAcquire call for N <= P permits may succeed
       immediately even if there are other acquire or tryAcquire
       calls already waiting.

Rules 3b and 4b cover the bits that had us tripped up in the
OrderedExecutor discussion.

Cheers,
Justin


From peter.levart at gmail.com  Fri Dec 26 06:18:11 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 26 Dec 2014 12:18:11 +0100
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>	<549829A7.1020604@infinite-source.de>
	<CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>
Message-ID: <549D43F3.6020104@gmail.com>

Hi Luke,

On 12/22/2014 05:10 PM, Luke Sandberg wrote:
> Thanks for all the responses!
>
> RE: CHM. For the thread-renaming usecase we used CHM for a while but 
> it consistently ended up as one of the highest contention points (as 
> measured by lock wait times) in some of our applications.  Also there 
> is a fairly high fixed overhead.

I would expect CHM to have the lowest contention for concurrent 
insertion and removal of any solution described here, since it uses 
multiple locks (one per bucket in CHM v8) and uncontended locking is 
quite fast. Unless computing identity hashCode for a Thread is a 
contention point? Were you using a pre-JDK8 CHM which uses fixed number 
of "segments" internally with one lock per segment?

The space overhead is not that much greater if your key(s) already use 
identity equals/hashCode so you don't have to wrap them with another 
object (Thread is such class). In CLQ, each element is "wrapped" into a 
node which looks like:

     private static class Node<E> {
         volatile E item;
         volatile Node<E> next;
     }

which is 20 bytes on 32 bit JVM and 32 bytes on 64 bit JVM

In CHMv8, each key/value pair (you can use a singleton object for 
values) is wrapped into a node which looks like:

     static class Node<K,V> implements Map.Entry<K,V> {
         final int hash;
         final K key;
         volatile V val;
         volatile Node<K,V> next;
     }

which is 28 bytes on 32 bit JVM and 48 bytes on 64 bit JVM

Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141226/40060d35/attachment-0001.html>

From vitalyd at gmail.com  Fri Dec 26 09:06:04 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 26 Dec 2014 09:06:04 -0500
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <549D43F3.6020104@gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
	<549829A7.1020604@infinite-source.de>
	<CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>
	<549D43F3.6020104@gmail.com>
Message-ID: <CAHjP37FK1cO9TmrybAD34CNK_RNy21==B5mkEU8gAvH4dYJ1iA@mail.gmail.com>

And if compressed oops are enabled, the sizes of the wrapper Node instances
are a bit smaller: 24 and 32 bytes, respectively, on 64bit.

Sent from my phone
On Dec 26, 2014 6:39 AM, "Peter Levart" <peter.levart at gmail.com> wrote:

>  Hi Luke,
>
> On 12/22/2014 05:10 PM, Luke Sandberg wrote:
>
>  Thanks for all the responses!
>
>  RE: CHM. For the thread-renaming usecase we used CHM for a while but it
> consistently ended up as one of the highest contention points (as measured
> by lock wait times) in some of our applications.  Also there is a fairly
> high fixed overhead.
>
>
> I would expect CHM to have the lowest contention for concurrent insertion
> and removal of any solution described here, since it uses multiple locks
> (one per bucket in CHM v8) and uncontended locking is quite fast. Unless
> computing identity hashCode for a Thread is a contention point? Were you
> using a pre-JDK8 CHM which uses fixed number of "segments" internally with
> one lock per segment?
>
> The space overhead is not that much greater if your key(s) already use
> identity equals/hashCode so you don't have to wrap them with another object
> (Thread is such class). In CLQ, each element is "wrapped" into a node which
> looks like:
>
>     private static class Node<E> {
>         volatile E item;
>         volatile Node<E> next;
>     }
>
> which is 20 bytes on 32 bit JVM and 32 bytes on 64 bit JVM
>
> In CHMv8, each key/value pair (you can use a singleton object for values)
> is wrapped into a node which looks like:
>
>     static class Node<K,V> implements Map.Entry<K,V> {
>         final int hash;
>         final K key;
>         volatile V val;
>         volatile Node<K,V> next;
>     }
>
> which is 28 bytes on 32 bit JVM and 48 bytes on 64 bit JVM
>
> Regards, Peter
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141226/0e507231/attachment.html>

From vitalyd at gmail.com  Fri Dec 26 09:11:25 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 26 Dec 2014 09:11:25 -0500
Subject: [concurrency-interest] Can a volatile read be reordered before
	a lazySet?
In-Reply-To: <CAHbGfh1HutpobaRWt+yp0jk=OjSk4-2=uPGhaJFbVo5wGfUiRw@mail.gmail.com>
References: <CAAWwtm89vvUcuzRucpVe8nnbY5Q4uPG1HvkGpV4rwBsR9A2fZw@mail.gmail.com>
	<CAHbGfh1HutpobaRWt+yp0jk=OjSk4-2=uPGhaJFbVo5wGfUiRw@mail.gmail.com>
Message-ID: <CAHjP37H+6TQgL68z7P_1m0qUcXSf1FxvSGhA-Ypk=i7JRNXrag@mail.gmail.com>

lazySet only orders stores whereas David's code has loads and stores.  As I
mentioned earlier, the ping pong example will work but the instructions may
reorder.

Sent from my phone
On Dec 23, 2014 9:40 PM, "Thomas Kountis" <tkountis at gmail.com> wrote:

> David,
>
> AFAIK all the atomic* members that Java provides, pretty much provide the
> same guarantees
> as the volatile keyword does - which doesn't allow re-orderings on the
> compiler side, neither during execution.
> If you look behind the covers, the fields that those two wrappers work on,
> are marked volatile as well. Also, lazy-set
> is using the Unsafe.putOrdered...() which relies on a store-store barrier
> and will prevent any instruction re-ordering (as the name suggests).
>
> So, to answer your question, I believe you are safe as far as it concerns
> re-orderings on your example.
>
> t.
>
> On Tue, Dec 23, 2014 at 10:50 PM, D?vid Karnok <akarnokd at gmail.com> wrote:
>>
>> Hello,
>>
>> Given two atomic values held by an AtomicLong and an AtomicReference, is
>> it possible the following two lines may be reordered?
>>
>> theLong.lazySet(theLong.get() + 1);
>> Object o = theRef.get();
>>
>> On X86, the memory model states that reads may be reordered with older
>> writes to different locations, so the algorithm fragment above might be
>> broken. For context, this code is part of a single-producer structure wich
>> protects the use of theRef value with an ingress/egress counter pair.
>>
>> I wonder if the same reordering might happen in a classical atomic
>> ping-pong example:
>>
>> ping.lazySet(1);
>> while (pong.get() == 0);
>>
>> while (ping.get() == 0);
>> pong.lazySet(1);
>>
>> i.e., in both threads, the while loops end up before the lazySet and thus
>> deadlocking.
>>
>> Best regards,
>> David Karnok
>>
>>
>> --
>> Best regards,
>> David Karnok
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141226/d8996061/attachment.html>

From nitsanw at yahoo.com  Sat Dec 27 03:09:59 2014
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Sat, 27 Dec 2014 08:09:59 +0000 (UTC)
Subject: [concurrency-interest] Can a volatile read be reordered
	before	a lazySet?
In-Reply-To: <CAHjP37H+6TQgL68z7P_1m0qUcXSf1FxvSGhA-Ypk=i7JRNXrag@mail.gmail.com>
References: <CAHjP37H+6TQgL68z7P_1m0qUcXSf1FxvSGhA-Ypk=i7JRNXrag@mail.gmail.com>
Message-ID: <402802666.1686596.1419667799102.JavaMail.yahoo@jws10685.mail.bf1.yahoo.com>

Saturday morning first cup of coffee 2 cents. T&C apply.1. In this case:
? theLong.lazySet(1L);? Object o =?theRef.get();There's technically nothing to stop reordering between the store and load. The problem is that moving the store past the load has potential implications to other sequences of stores to theLong/theObject which might race with the above. This is described in Shipilev's excellent JMM notes here:?Java Memory Model Pragmatics (transcript)
| ? |
| ? |  | ? | ? | ? | ? | ? |
| Java Memory Model Pragmatics (transcript)Happens-Before While providing a good basis to reason about programs, SO is not enough to construct a practical weak model. Here is why. Let us analyze a simple cas... |
|  |
| View on shipilev.net | Preview by Yahoo |
|  |
| ? |

 ?IIUC this means stores and volatile loads are conservatively not reordered.2. The case you quote has a further volatile read, we can break it down as:? long l = theLong.get() + 1; // LOADLOAD? theLong.lazySet(l); //STORESTORE? Object o =?theRef.get(); // LOADLOADIn this case the STORE has to happen after the first load. The first load also must happen before the second load. The store can be reordered to happen after the second load though (at least in theory, but it won't due to 1).3. The ping pong case is different, because the while loops are of indefinite length, potentially infinite. Delaying the store indefinitely would seem to break sequential consistency and so the while loop acts as an effective store barrier. If you replace the while loop with a for(int i=0;i<K;i++) loop the reordering becomes notionally feasible again.??
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141227/032f7a72/attachment.html>

From tkountis at gmail.com  Sat Dec 27 10:04:02 2014
From: tkountis at gmail.com (Thomas Kountis)
Date: Sat, 27 Dec 2014 15:04:02 +0000
Subject: [concurrency-interest] Can a volatile read be reordered before
	a lazySet?
In-Reply-To: <402802666.1686596.1419667799102.JavaMail.yahoo@jws10685.mail.bf1.yahoo.com>
References: <CAHjP37H+6TQgL68z7P_1m0qUcXSf1FxvSGhA-Ypk=i7JRNXrag@mail.gmail.com>
	<402802666.1686596.1419667799102.JavaMail.yahoo@jws10685.mail.bf1.yahoo.com>
Message-ID: <CAHbGfh0+Sf_LZpMN-_=gVScLo7Frer35VL9VrO21+t9iy=AA5Q@mail.gmail.com>

That cup of coffee must have been really good ;)
Thanks for the detailed explanation Nitsan, especially for that last piece
with the while-loop!


On Sat, Dec 27, 2014 at 8:09 AM, Nitsan Wakart <nitsanw at yahoo.com> wrote:

> Saturday morning first cup of coffee 2 cents. T&C apply.
> 1. In this case:
>   theLong.lazySet(1L);
>   Object o = theRef.get();
> There's technically nothing to stop reordering between the store and load.
> The problem is that moving the store past the load has potential
> implications to other sequences of stores to theLong/theObject which might
> race with the above. This is described in Shipilev's excellent JMM notes
> here: Java Memory Model Pragmatics (transcript)
> <http://shipilev.net/blog/2014/jmm-pragmatics/#_jmm_interpretation_roach_motel>
>
>
> [image: image]
> <http://shipilev.net/blog/2014/jmm-pragmatics/#_jmm_interpretation_roach_motel>
>
>
>
>
>
> Java Memory Model Pragmatics (transcript)
> <http://shipilev.net/blog/2014/jmm-pragmatics/#_jmm_interpretation_roach_motel>
> Happens-Before While providing a good basis to reason about programs, SO
> is not enough to construct a practical weak model. Here is why. Let us
> analyze a simple cas...
> View on shipilev.net
> <http://shipilev.net/blog/2014/jmm-pragmatics/#_jmm_interpretation_roach_motel>
> Preview by Yahoo
>
>
> IIUC this means stores and volatile loads are conservatively not reordered.
> 2. The case you quote has a further volatile read, we can break it down as:
>   long l = theLong.get() + 1; // LOADLOAD
>   theLong.lazySet(l); //STORESTORE
>   Object o = theRef.get(); // LOADLOAD
> In this case the STORE has to happen after the first load. The first load
> also must happen before the second load. The store can be reordered to
> happen after the second load though (at least in theory, but it won't due
> to 1).
> 3. The ping pong case is different, because the while loops are of
> indefinite length, potentially infinite. Delaying the store indefinitely
> would seem to break sequential consistency and so the while loop acts as an
> effective store barrier. If you replace the while loop with a for(int
> i=0;i<K;i++) loop the reordering becomes notionally feasible again.
>
>
>


-- 
Thomas Kountis
PGP: 0x069D29A3

Q: "Whats the object-oriented way to become wealthy?"
A:  Inheritance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141227/8b693187/attachment-0001.html>

From vitalyd at gmail.com  Sat Dec 27 14:37:27 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 27 Dec 2014 14:37:27 -0500
Subject: [concurrency-interest] Can a volatile read be reordered before
	a lazySet?
In-Reply-To: <402802666.1686596.1419667799102.JavaMail.yahoo@jws10685.mail.bf1.yahoo.com>
References: <CAHjP37H+6TQgL68z7P_1m0qUcXSf1FxvSGhA-Ypk=i7JRNXrag@mail.gmail.com>
	<402802666.1686596.1419667799102.JavaMail.yahoo@jws10685.mail.bf1.yahoo.com>
Message-ID: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>

Nitsan,

In #1, what do you mean exactly by "IIUC this means stores and volatile
loads are conservatively not reordered"? Are you talking about JIT code
motion only? Specifically, I'm pretty sure theLong.lazySet(1); Object o =
theRef.get() sequence compiles to machine code with no cpu fences on x86;
it's a mov immediate into theLong and a mov of theRef into register; the
cpu can reorder these (without a StoreLoad in-between).

On Sat, Dec 27, 2014 at 3:09 AM, Nitsan Wakart <nitsanw at yahoo.com> wrote:
>
> Saturday morning first cup of coffee 2 cents. T&C apply.
> 1. In this case:
>   theLong.lazySet(1L);
>   Object o = theRef.get();
> There's technically nothing to stop reordering between the store and load.
> The problem is that moving the store past the load has potential
> implications to other sequences of stores to theLong/theObject which might
> race with the above. This is described in Shipilev's excellent JMM notes
> here: Java Memory Model Pragmatics (transcript)
> <http://shipilev.net/blog/2014/jmm-pragmatics/#_jmm_interpretation_roach_motel>
>
>
> [image: image]
> <http://shipilev.net/blog/2014/jmm-pragmatics/#_jmm_interpretation_roach_motel>
>
>
>
>
>
> Java Memory Model Pragmatics (transcript)
> <http://shipilev.net/blog/2014/jmm-pragmatics/#_jmm_interpretation_roach_motel>
> Happens-Before While providing a good basis to reason about programs, SO
> is not enough to construct a practical weak model. Here is why. Let us
> analyze a simple cas...
> View on shipilev.net
> <http://shipilev.net/blog/2014/jmm-pragmatics/#_jmm_interpretation_roach_motel>
> Preview by Yahoo
>
>
> IIUC this means stores and volatile loads are conservatively not reordered.
> 2. The case you quote has a further volatile read, we can break it down as:
>   long l = theLong.get() + 1; // LOADLOAD
>   theLong.lazySet(l); //STORESTORE
>   Object o = theRef.get(); // LOADLOAD
> In this case the STORE has to happen after the first load. The first load
> also must happen before the second load. The store can be reordered to
> happen after the second load though (at least in theory, but it won't due
> to 1).
> 3. The ping pong case is different, because the while loops are of
> indefinite length, potentially infinite. Delaying the store indefinitely
> would seem to break sequential consistency and so the while loop acts as an
> effective store barrier. If you replace the while loop with a for(int
> i=0;i<K;i++) loop the reordering becomes notionally feasible again.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141227/28079660/attachment.html>

From ben_manes at yahoo.com  Sat Dec 27 15:31:57 2014
From: ben_manes at yahoo.com (Ben Manes)
Date: Sat, 27 Dec 2014 20:31:57 +0000 (UTC)
Subject: [concurrency-interest] synchronized vs
	Unsafe#monitorEnter/monitorExit
Message-ID: <770173580.1087507.1419712317470.JavaMail.yahoo@jws100170.mail.ne1.yahoo.com>

Can someone explain why using Unsafe's monitor methods are substantially worse than synchronized? I had expected them to emit equivalent monitorEnter/monitorExit instructions and have similar performance.
My use case is to support a bulk version of CHM#computeIfAbsent, where a single mapping function returns the result for computing multiple entries. I had hoped to bulk lock, insert the unfilled entries, compute, populate, and bulk unlock. An overlapping write would be blocked due to requiring an entry's lock for mutation. I had thought that using Unsafe would allow for achieving this without the memory overhead of a ReentrantLock/AQS per entry, since the synchronized keyword is not flexible enough to provide this structure.
Thanks,Ben
Benchmark ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Mode ?Samples ? ? ? ? Score ? ? ? ? Error ?Unitsc.g.b.c.SynchronizedBenchmark.monitor_contention ? ? ? ? ? ?thrpt ? ? ? 10 ? 3694951.630 ? ? 34340.707 ?ops/sc.g.b.c.SynchronizedBenchmark.monitor_noContention ? ? ? ? ?thrpt ? ? ? 10 ? 8274097.911 ? ?164356.363 ?ops/sc.g.b.c.SynchronizedBenchmark.reentrantLock_contention ? ? ?thrpt ? ? ? 10 ?31668532.247 ? ?740850.955 ?ops/sc.g.b.c.SynchronizedBenchmark.reentrantLock_noContention ? ?thrpt ? ? ? 10 ?41380163.703 ? 2270103.507 ?ops/sc.g.b.c.SynchronizedBenchmark.synchronized_contention ? ? ? thrpt ? ? ? 10 ?22905995.761 ? ?117868.968 ?ops/sc.g.b.c.SynchronizedBenchmark.synchronized_noContention ? ? thrpt ? ? ? 10 ?44891601.915 ? 1458775.665 ?ops/s
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141227/50719e9e/attachment.html>

From forax at univ-mlv.fr  Sat Dec 27 16:29:09 2014
From: forax at univ-mlv.fr (Remi Forax)
Date: Sat, 27 Dec 2014 22:29:09 +0100
Subject: [concurrency-interest] synchronized vs
	Unsafe#monitorEnter/monitorExit
In-Reply-To: <770173580.1087507.1419712317470.JavaMail.yahoo@jws100170.mail.ne1.yahoo.com>
References: <770173580.1087507.1419712317470.JavaMail.yahoo@jws100170.mail.ne1.yahoo.com>
Message-ID: <549F24A5.8070605@univ-mlv.fr>

Not all the methods of Unsafe are fast, these are in Unsafe because 
well, they are unsafe.

Note that these 3 ones will be removed soon because nobody use them.

cheers,
R?mi

On 12/27/2014 09:31 PM, Ben Manes wrote:
> Can someone explain why using Unsafe's monitor methods are 
> substantially worse than synchronized? I had expected them to emit 
> equivalent monitorEnter/monitorExit instructions and have similar 
> performance.
>
> My use case is to support a bulk version of CHM#computeIfAbsent, where 
> a single mapping function returns the result for computing multiple 
> entries. I had hoped to bulk lock, insert the unfilled entries, 
> compute, populate, and bulk unlock. An overlapping write would be 
> blocked due to requiring an entry's lock for mutation. I had thought 
> that using Unsafe would allow for achieving this without the memory 
> overhead of a ReentrantLock/AQS per entry, since the synchronized 
> keyword is not flexible enough to provide this structure.
>
> Thanks,
> Ben
>
> Benchmark                                                    Mode 
>  Samples         Score         Error  Units
> c.g.b.c.SynchronizedBenchmark.monitor_contention            thrpt     
>   10   3694951.630 ?   34340.707  ops/s
> c.g.b.c.SynchronizedBenchmark.monitor_noContention          thrpt     
>   10   8274097.911 ?  164356.363  ops/s
> c.g.b.c.SynchronizedBenchmark.reentrantLock_contention      thrpt     
>   10  31668532.247 ?  740850.955  ops/s
> c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention    thrpt     
>   10  41380163.703 ? 2270103.507  ops/s
> c.g.b.c.SynchronizedBenchmark.synchronized_contention       thrpt     
>   10  22905995.761 ?  117868.968  ops/s
> c.g.b.c.SynchronizedBenchmark.synchronized_noContention     thrpt     
>   10  44891601.915 ? 1458775.665  ops/s
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141227/4a5c58aa/attachment-0001.html>

From gil at azulsystems.com  Sat Dec 27 19:53:09 2014
From: gil at azulsystems.com (Gil Tene)
Date: Sun, 28 Dec 2014 00:53:09 +0000
Subject: [concurrency-interest] Can a volatile read be reordered
	before	a lazySet?
In-Reply-To: <CAHbGfh1HutpobaRWt+yp0jk=OjSk4-2=uPGhaJFbVo5wGfUiRw@mail.gmail.com>
References: <CAAWwtm89vvUcuzRucpVe8nnbY5Q4uPG1HvkGpV4rwBsR9A2fZw@mail.gmail.com>
	<CAHbGfh1HutpobaRWt+yp0jk=OjSk4-2=uPGhaJFbVo5wGfUiRw@mail.gmail.com>
Message-ID: <9FB24E39-B0AB-4B92-814F-8EE08FAB9C3F@azulsystems.com>

It is valid (e.g. per the JMM cookbook) for a JVM to implement a volatile read such that it prevents reordering with subsequent stores, but not with prior ones. Given this valid (and common) implementation, both cases asked about below can have a reordering happen between the lazySet and a subsequent (in program order) volatile get. Target machine (x86 or not) has nothing to do with it, as the compiler can (and will) do this reordering before any target machine code gets generated.

The only ordering that lazySet is actually known/required to provide will prevent the store operation from being reordered with any stores that came before it. There is nothing to prevent it from being pushed forward past other non-dependent stores, loads, or volatile loads. [The thing that stops it being pushed forward past a non-dependent volatile store is the volatile store's relationship with stores that precede it].

So:

theLong.lazySet(theLong.get() + 1);
Object o = theRef.get();

can be legitimately re-ordered by the compiler to:

x = theLong.get() + 1
Object o = theRef.get();
theLong.lazySet(x);

And the ping pong case:

ping.lazySet(1);
while (pong.get() == 0);

while (ping.get() == 0);
pong.lazySet(1);

can be legitimately reordered by the compiler to:

T1:
while (pong.get() == 0);
ping.lazySet(1);

T2:
while (ping.get() == 0);
pong.lazySet(1);

and can certainly result in a well deserved deadlock.

? Gil.


On Dec 23, 2014, at 6:17 PM, Thomas Kountis <tkountis at gmail.com<mailto:tkountis at gmail.com>> wrote:

David,

AFAIK all the atomic* members that Java provides, pretty much provide the same guarantees
as the volatile keyword does - which doesn't allow re-orderings on the compiler side, neither during execution.
If you look behind the covers, the fields that those two wrappers work on, are marked volatile as well. Also, lazy-set
is using the Unsafe.putOrdered...() which relies on a store-store barrier and will prevent any instruction re-ordering (as the name suggests).

So, to answer your question, I believe you are safe as far as it concerns re-orderings on your example.

t.

On Tue, Dec 23, 2014 at 10:50 PM, D?vid Karnok <akarnokd at gmail.com<mailto:akarnokd at gmail.com>> wrote:
Hello,

Given two atomic values held by an AtomicLong and an AtomicReference, is it possible the following two lines may be reordered?

theLong.lazySet(theLong.get() + 1);
Object o = theRef.get();

On X86, the memory model states that reads may be reordered with older writes to different locations, so the algorithm fragment above might be broken. For context, this code is part of a single-producer structure wich protects the use of theRef value with an ingress/egress counter pair.

I wonder if the same reordering might happen in a classical atomic ping-pong example:

ping.lazySet(1);
while (pong.get() == 0);

while (ping.get() == 0);
pong.lazySet(1);

i.e., in both threads, the while loops end up before the lazySet and thus deadlocking.

Best regards,
David Karnok


--
Best regards,
David Karnok


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141228/896100cd/attachment.html>

From gil at azulsystems.com  Sat Dec 27 20:10:01 2014
From: gil at azulsystems.com (Gil Tene)
Date: Sun, 28 Dec 2014 01:10:01 +0000
Subject: [concurrency-interest] synchronized
	vs	Unsafe#monitorEnter/monitorExit
In-Reply-To: <770173580.1087507.1419712317470.JavaMail.yahoo@jws100170.mail.ne1.yahoo.com>
References: <770173580.1087507.1419712317470.JavaMail.yahoo@jws100170.mail.ne1.yahoo.com>
Message-ID: <4DF44206-38FA-4D5A-94ED-56CDDED7FE96@azulsystems.com>

It's not "synchronized" per se that is responsible for the difference. It's the use of the monitorenter and monitorexit bytecodes. Some of the optimizations done for monitors rely on their verified behavior for correctness. The unsafe versions are not verified to adhere to the same requirements, which either makes some optimizations impossible, or just made the optimization designer not bother trying to optimize the unconfined "could do anything" case.

E.g. the fast, uncontended, unbiased monitor path devolves to fast path CAS on the object header in most JVMs (displaced headers, thin locking, Bacon bits, whatever...). But this common optimization often strongly assumes balanced use of monitors as enforced by the verifier when monitor_enter and monitor_exit byetcodes are used. E.g. HotSpot uses displaced headers for this operation, and stores a displaced mark word on the thread stack, knowing (based on the verified bytecode qualities) that the stack frame will not be rewound before a monitor_exit would occur. Since an unsafe monitor enter call may not have a matching monitor exit in the same frame, that optimization would be invalid to perform.

? Gil.

On Dec 27, 2014, at 12:31 PM, Ben Manes <ben_manes at yahoo.com<mailto:ben_manes at yahoo.com>> wrote:

Can someone explain why using Unsafe's monitor methods are substantially worse than synchronized? I had expected them to emit equivalent monitorEnter/monitorExit instructions and have similar performance.

My use case is to support a bulk version of CHM#computeIfAbsent, where a single mapping function returns the result for computing multiple entries. I had hoped to bulk lock, insert the unfilled entries, compute, populate, and bulk unlock. An overlapping write would be blocked due to requiring an entry's lock for mutation. I had thought that using Unsafe would allow for achieving this without the memory overhead of a ReentrantLock/AQS per entry, since the synchronized keyword is not flexible enough to provide this structure.

Thanks,
Ben

Benchmark                                                    Mode  Samples         Score         Error  Units
c.g.b.c.SynchronizedBenchmark.monitor_contention            thrpt       10   3694951.630 ?   34340.707  ops/s
c.g.b.c.SynchronizedBenchmark.monitor_noContention          thrpt       10   8274097.911 ?  164356.363  ops/s
c.g.b.c.SynchronizedBenchmark.reentrantLock_contention      thrpt       10  31668532.247 ?  740850.955  ops/s
c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention    thrpt       10  41380163.703 ? 2270103.507  ops/s
c.g.b.c.SynchronizedBenchmark.synchronized_contention       thrpt       10  22905995.761 ?  117868.968  ops/s
c.g.b.c.SynchronizedBenchmark.synchronized_noContention     thrpt       10  44891601.915 ? 1458775.665  ops/s



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141228/3a45b814/attachment-0001.html>

From ben_manes at yahoo.com  Sat Dec 27 21:28:20 2014
From: ben_manes at yahoo.com (Ben Manes)
Date: Sun, 28 Dec 2014 02:28:20 +0000 (UTC)
Subject: [concurrency-interest] synchronized
	vs	Unsafe#monitorEnter/monitorExit
In-Reply-To: <4DF44206-38FA-4D5A-94ED-56CDDED7FE96@azulsystems.com>
References: <4DF44206-38FA-4D5A-94ED-56CDDED7FE96@azulsystems.com>
Message-ID: <1662164010.1115063.1419733700931.JavaMail.yahoo@jws100113.mail.ne1.yahoo.com>

Thanks, that makes a lot of sense.
Not surprising, but still interesting, is that mixing both usages of the byte code results in only the Unsafe paths having a slow lock acquisition, while the verifiable paths are optimized. I'll probably have to see how heavy an AQS non-reentrant lock is per entry as an alternative approach for my use-case, or simply abandon it altogether.
Benchmark ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Mode ?Samples ? ? ? ? Score ? ? ? ? Error ?Unitsc.g.b.c.SynchronizedBenchmark.mixed ? ? ? ? ? ? ? ? ? ? ? ? ? ?thrpt ? ? ? 10 ?20893653.628 ? ? 89375.469 ?ops/sc.g.b.c.SynchronizedBenchmark.mixed:mixed_monitor ? ? ? ? ? ? ?thrpt ? ? ? 10 ? ?454111.564 ? ? ?7717.129 ?ops/sc.g.b.c.SynchronizedBenchmark.mixed:mixed_sync ? ? ? ? ? ? ? ? thrpt ? ? ? 10 ?20439542.064 ? ? 93987.110 ?ops/sc.g.b.c.SynchronizedBenchmark.monitor_contention ? ? ? ? ? ? ? thrpt ? ? ? 10 ? 3589347.939 ? ?114413.831 ?ops/sc.g.b.c.SynchronizedBenchmark.monitor_noContention ? ? ? ? ? ? thrpt ? ? ? 10 ? 7789934.551 ? ?424970.084 ?ops/sc.g.b.c.SynchronizedBenchmark.nonReentrantLock_contention ? ? ?thrpt ? ? ? 10 ?35595272.514 ? ?219754.617 ?ops/sc.g.b.c.SynchronizedBenchmark.nonReentrantLock_noContention ? ?thrpt ? ? ? 10 ?74245220.098 ? ?768665.279 ?ops/sc.g.b.c.SynchronizedBenchmark.reentrantLock_contention ? ? ? ? thrpt ? ? ? 10 ?27296079.389 ? ?787027.871 ?ops/sc.g.b.c.SynchronizedBenchmark.reentrantLock_noContention ? ? ? thrpt ? ? ? 10 ?41981666.507 ? 1374677.863 ?ops/sc.g.b.c.SynchronizedBenchmark.synchronized_contention ? ? ? ? ?thrpt ? ? ? 10 ?22512475.218 ? ?301107.363 ?ops/sc.g.b.c.SynchronizedBenchmark.synchronized_noContention ? ? ? ?thrpt ? ? ? 10 ?43245916.801 ? 1664731.804 ?ops/s 

     On Saturday, December 27, 2014 5:10 PM, Gil Tene <gil at azulsystems.com> wrote:
   

 It's not "synchronized" per se that is responsible for the difference. It's the use of the monitorenter and monitorexit bytecodes. Some of the optimizations done for monitors rely on their verified behavior for correctness. The unsafe versions are not verified to adhere to the same requirements, which either makes some optimizations impossible, or just made the optimization designer not bother trying to optimize the unconfined "could do anything" case.
E.g. the fast, uncontended, unbiased monitor path devolves to fast path CAS on the object header in most JVMs (displaced headers, thin locking, Bacon bits, whatever...). But this common optimization often strongly assumes balanced use of monitors as enforced by the verifier when monitor_enter and monitor_exit byetcodes are used. E.g. HotSpot uses displaced headers for this operation, and stores a displaced mark word on the thread stack, knowing (based on the verified bytecode qualities) that the stack frame will not be rewound before a monitor_exit would occur. Since an unsafe monitor enter call may not have a matching monitor exit in the same frame, that optimization would be invalid to perform.
? Gil.

On Dec 27, 2014, at 12:31 PM, Ben Manes <ben_manes at yahoo.com> wrote:
Can someone explain why using Unsafe's monitor methods are substantially worse than synchronized? I had expected them to emit equivalent monitorEnter/monitorExit instructions and have similar performance.
My use case is to support a bulk version of CHM#computeIfAbsent, where a single mapping function returns the result for computing multiple entries. I had hoped to bulk lock, insert the unfilled entries, compute, populate, and bulk unlock. An overlapping write would be blocked due to requiring an entry's lock for mutation. I had thought that using Unsafe would allow for achieving this without the memory overhead of a ReentrantLock/AQS per entry, since the synchronized keyword is not flexible enough to provide this structure.
Thanks,Ben
Benchmark ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Mode ?Samples ? ? ? ? Score ? ? ? ? Error ?Unitsc.g.b.c.SynchronizedBenchmark.monitor_contention ? ? ? ? ? ?thrpt ? ? ? 10 ? 3694951.630 ? ? 34340.707 ?ops/sc.g.b.c.SynchronizedBenchmark.monitor_noContention ? ? ? ? ?thrpt ? ? ? 10 ? 8274097.911 ? ?164356.363 ?ops/sc.g.b.c.SynchronizedBenchmark.reentrantLock_contention ? ? ?thrpt ? ? ? 10 ?31668532.247 ? ?740850.955 ?ops/sc.g.b.c.SynchronizedBenchmark.reentrantLock_noContention ? ?thrpt ? ? ? 10 ?41380163.703 ? 2270103.507 ?ops/sc.g.b.c.SynchronizedBenchmark.synchronized_contention ? ? ? thrpt ? ? ? 10 ?22905995.761 ? ?117868.968 ?ops/sc.g.b.c.SynchronizedBenchmark.synchronized_noContention ? ? thrpt ? ? ? 10 ?44891601.915 ? 1458775.665 ?ops/s


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141228/2d5fa98a/attachment.html>

From aph at redhat.com  Sun Dec 28 06:08:41 2014
From: aph at redhat.com (Andrew Haley)
Date: Sun, 28 Dec 2014 11:08:41 +0000
Subject: [concurrency-interest] Can a volatile read be reordered before
 a lazySet?
In-Reply-To: <9FB24E39-B0AB-4B92-814F-8EE08FAB9C3F@azulsystems.com>
References: <CAAWwtm89vvUcuzRucpVe8nnbY5Q4uPG1HvkGpV4rwBsR9A2fZw@mail.gmail.com>	<CAHbGfh1HutpobaRWt+yp0jk=OjSk4-2=uPGhaJFbVo5wGfUiRw@mail.gmail.com>
	<9FB24E39-B0AB-4B92-814F-8EE08FAB9C3F@azulsystems.com>
Message-ID: <549FE4B9.40401@redhat.com>

On 28/12/14 00:53, Gil Tene wrote:

> The only ordering that lazySet is actually known/required to provide
> will prevent the store operation from being reordered with any
> stores that came before it. There is nothing to prevent it from
> being pushed forward past other non-dependent stores, loads, or
> volatile loads. [The thing that stops it being pushed forward past a
> non-dependent volatile store is the volatile store's relationship
> with stores that precede it].

But, as Doug said:

"There are three basic strengths of write: Relaxed (normal),
releasing, and SC volatile. I got talked into naming the second
releasing version 'lazySet' which is confusing enough, but compounded
by the almost equally odd internal unsafe name 'ordered'.

"These should be less confusing in JDK9 when we provide more uniform
enhanced-volatile support."

Andrew.

From peter.levart at gmail.com  Sun Dec 28 08:18:24 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 28 Dec 2014 14:18:24 +0100
Subject: [concurrency-interest] synchronized vs
	Unsafe#monitorEnter/monitorExit
In-Reply-To: <770173580.1087507.1419712317470.JavaMail.yahoo@jws100170.mail.ne1.yahoo.com>
References: <770173580.1087507.1419712317470.JavaMail.yahoo@jws100170.mail.ne1.yahoo.com>
Message-ID: <54A00320.1050609@gmail.com>


On 12/27/2014 09:31 PM, Ben Manes wrote:
> Can someone explain why using Unsafe's monitor methods are 
> substantially worse than synchronized? I had expected them to emit 
> equivalent monitorEnter/monitorExit instructions and have similar 
> performance.
>
> My use case is to support a bulk version of CHM#computeIfAbsent, where 
> a single mapping function returns the result for computing multiple 
> entries. I had hoped to bulk lock, insert the unfilled entries, 
> compute, populate, and bulk unlock. An overlapping write would be 
> blocked due to requiring an entry's lock for mutation.

Hi Ben,

If "multiple entries" means less than say 50 or even a little more 
(using more locks than that for one bulk operation is not very optimal 
anyway), you could try constructing a recursive method to bulk-lock on 
the list of objects and execute a provided function:


     public static <T> T synchronizedAll(Iterable<?> locks, Supplier<T> 
supplier) {
         return synchronizedAll(locks.iterator(), supplier);
     }

     private static <T> T synchronizedAll(Iterator<?> locksIterator, 
Supplier<T> supplier) {
         if (locksIterator.hasNext()) {
             synchronized (locksIterator.next()) {
                 return synchronizedAll(locksIterator, supplier);
             }
         } else {
             return supplier.get();
         }
     }



Note that to avoid deadlocks, the locks list has to be sorted using a 
Comparator that is consistent with your key's equals() method...

Regards, Peter


> I had thought that using Unsafe would allow for achieving this without 
> the memory overhead of a ReentrantLock/AQS per entry, since the 
> synchronized keyword is not flexible enough to provide this structure.
>
> Thanks,
> Ben
>
> Benchmark                                                    Mode 
>  Samples         Score         Error  Units
> c.g.b.c.SynchronizedBenchmark.monitor_contention            thrpt     
>   10   3694951.630 ?   34340.707  ops/s
> c.g.b.c.SynchronizedBenchmark.monitor_noContention          thrpt     
>   10   8274097.911 ?  164356.363  ops/s
> c.g.b.c.SynchronizedBenchmark.reentrantLock_contention      thrpt     
>   10  31668532.247 ?  740850.955  ops/s
> c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention    thrpt     
>   10  41380163.703 ? 2270103.507  ops/s
> c.g.b.c.SynchronizedBenchmark.synchronized_contention       thrpt     
>   10  22905995.761 ?  117868.968  ops/s
> c.g.b.c.SynchronizedBenchmark.synchronized_noContention     thrpt     
>   10  44891601.915 ? 1458775.665  ops/s
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141228/e43102bf/attachment-0001.html>

From ben_manes at yahoo.com  Sun Dec 28 15:15:33 2014
From: ben_manes at yahoo.com (Ben Manes)
Date: Sun, 28 Dec 2014 20:15:33 +0000 (UTC)
Subject: [concurrency-interest] synchronized vs
 Unsafe#monitorEnter/monitorExit
In-Reply-To: <54A00320.1050609@gmail.com>
References: <54A00320.1050609@gmail.com>
Message-ID: <1313917327.1213625.1419797733603.JavaMail.yahoo@jws10046.mail.ne1.yahoo.com>

That's an nifty workaround but, as you said, not reliable in the case of a general api. This case is a multi-get for a cache library, so the keys and use-cases aren't known.
When adding bulk loading to Guava, due to the internal complexity we allowed it to racy by not blocking concurrent calls for shared entries. This has to be done regardless because a loadAll(keys) may return a Map<K, V> with more entries than originally requested, and those extra entries should be cached. These cases were trivial to handle when I had previously written a Map<K, Future<V>> to support multi-get, but a lot of bloat per entry. I had hoped Unsafe#monitorEnter would be a nice alternative and, if the API was stable, it arguably still could be due to the low write rate of caches. For now I'll leave it in a backlog of performance optimization ideas like we did with Guava, maybe revisiting once the rewrite stabilizes.
Thanks,-Ben 

     On Sunday, December 28, 2014 5:18 AM, Peter Levart <peter.levart at gmail.com> wrote:
   

  
 On 12/27/2014 09:31 PM, Ben Manes wrote:
  
  Can someone explain why using Unsafe's monitor methods are substantially worse than synchronized? I had expected them to emit equivalent monitorEnter/monitorExit instructions and have similar performance. 
  My use case is to support a bulk version of CHM#computeIfAbsent, where a single mapping function returns the result for computing multiple entries. I had hoped to bulk lock, insert the unfilled entries, compute, populate, and bulk unlock. An overlapping write would be blocked due to requiring an entry's  lock for mutation.   
 
 Hi Ben,
 
 If "multiple entries" means less than say 50 or even a little more (using more locks than that for one bulk operation is not very optimal anyway), you could try constructing a recursive method to bulk-lock on the list of objects and execute a provided function:
 
 
  ??? public static <T> T synchronizedAll(Iterable<?> locks, Supplier<T> supplier) {
 ??????? return synchronizedAll(locks.iterator(), supplier);
 ??? }
 
 ??? private static <T> T synchronizedAll(Iterator<?> locksIterator, Supplier<T> supplier) {
 ??????? if (locksIterator.hasNext()) {
 ??????????? synchronized (locksIterator.next()) {
 ??????????????? return synchronizedAll(locksIterator, supplier);
 ??????????? }
 ??????? } else {
 ??????????? return supplier.get();
 ??????? }
 ??? }
 
 
 
 Note that to avoid deadlocks, the locks list has to be sorted using a Comparator that is consistent with your key's equals() method...
 
 Regards, Peter
 
 
 
  I had thought that using Unsafe would allow for achieving this without the memory overhead of a ReentrantLock/AQS per entry, since the synchronized keyword is not flexible enough to provide this structure. 
  Thanks, Ben 
  Benchmark ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Mode ?Samples ? ? ? ? Score ? ? ? ? Error ?Units c.g.b.c.SynchronizedBenchmark.monitor_contention ? ? ? ? ? ?thrpt ? ? ? 10 ? 3694951.630 ? ? 34340.707 ?ops/s c.g.b.c.SynchronizedBenchmark.monitor_noContention ? ? ? ? ?thrpt ? ? ? 10 ? 8274097.911 ? ?164356.363 ?ops/s c.g.b.c.SynchronizedBenchmark.reentrantLock_contention ? ? ?thrpt ? ? ? 10 ?31668532.247 ? ?740850.955 ?ops/s c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention ? ?thrpt ? ? ? 10 ?41380163.703 ? 2270103.507 ?ops/s c.g.b.c.SynchronizedBenchmark.synchronized_contention ? ? ? thrpt ? ? ? 10 ?22905995.761 ? ?117868.968 ?ops/s c.g.b.c.SynchronizedBenchmark.synchronized_noContention ? ? thrpt ? ? ? 10 ?44891601.915 ? 1458775.665 ?ops/s 
   
  
 _______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
 
 

   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141228/6c85c3ab/attachment.html>

From oleksandr.otenko at oracle.com  Mon Dec 29 19:58:17 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 30 Dec 2014 00:58:17 +0000
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <64849A49-4320-49A5-811F-842B4FA1A5B6@flyingtroika.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
	<5491D658.5060604@oracle.com> <549285B0.3070801@gmail.com>
	<CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>
	<549340FF.5070301@oracle.com>
	<64849A49-4320-49A5-811F-842B4FA1A5B6@flyingtroika.com>
Message-ID: <54A1F8A9.3000208@oracle.com>

ACK/NAK? I only meant that data packets arrive concurrently, but get 
consumed in order.

You could see NAK as false returned from tryAcquire by the producer, but 
that is not the point. In the original thesis the sequencer is used to 
control access to the receive buffer.


Alex


On 22/12/2014 22:21, DT wrote:
> I think if the sequencer is used it would mean that both ACK/ NAK type 
> of responses should be used and sequence has to be restarted . 
> Restarting is ok but to deal with both ACK/NAK could bring extra 
> complexity during contention.
>
> Thanks,
> Dmitry
>
> Sent from my iPhone
>
> On Dec 18, 2014, at 1:02 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>> Yes, no one said it is a good idea to always do that. When it is 
>> contended, most of the threads will wake up to only go back to sleep.
>>
>> The pattern you are after is usually called sequencer. You can see it 
>> used in TCP. I am not sure why it wasn't implemented in j.u.c. - 
>> maybe not that popular.
>>
>> The best solution will be lock-like, but the waiter nodes will 
>> contain the value they are waiting for - so only the specific threads 
>> get woken up. The solution with concurrent map is very similar, only 
>> with larger overhead from storing the index the thread is waiting for.
>>
>>
>> Alex
>>
>>
>> On 18/12/2014 20:21, Hanson Char wrote:
>>> Less overhead and simpler are a nice properties, even though at the 
>>> expense of having to wake up all waiting threads just to find out 
>>> the one with the right order to execute.  Still, this seems like a 
>>> good tradeoff.
>>>
>>> Thanks,
>>> Hanson
>>>
>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart 
>>> <peter.levart at gmail.com <mailto:peter.levart at gmail.com>> wrote:
>>>
>>>     On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>
>>>         No, there is no difference. Peter didn't spot your entire
>>>         method is synchronized, so spurious wakeup won't make
>>>         progress until the owner of the lock exits the method.
>>>
>>>         You could split the synchronization into two blocks - one
>>>         encompassing the wait loop, the other in the finally block;
>>>         but it may make no difference.
>>>
>>>         Alex
>>>
>>>
>>>     You're right, Alex. I'm so infected with park/unpark virus that
>>>     I missed that ;-)
>>>
>>>     Peter
>>>
>>>
>>>         On 17/12/2014 18:36, suman shil wrote:
>>>
>>>             Thanks peter for your reply. You are right. I should
>>>             have incremented currentAllowedOrder in finally block.
>>>
>>>             Suman
>>>             ------------------------------------------------------------------------
>>>             *From:* Peter Levart <peter.levart at gmail.com
>>>             <mailto:peter.levart at gmail.com>>
>>>             *To:* suman shil <suman_krec at yahoo.com
>>>             <mailto:suman_krec at yahoo.com>>; Oleksandr Otenko
>>>             <oleksandr.otenko at oracle.com
>>>             <mailto:oleksandr.otenko at oracle.com>>;
>>>             Concurrency-interest <concurrency-interest at cs.oswego.edu
>>>             <mailto:concurrency-interest at cs.oswego.edu>>
>>>             *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>             *Subject:* Re: [concurrency-interest] Enforcing ordered
>>>             execution of critical sections?
>>>
>>>             On 12/17/2014 06:46 PM, suman shil wrote:
>>>
>>>                 Thanks for your response. Will notifyAll() instead
>>>                 of notify() solve the problem?
>>>
>>>
>>>             It will, but you should also account for "spurious"
>>>             wake-ups. You should increment currentAllowedOrder only
>>>             after return from callable.call (in finally block just
>>>             before notifyAll()).
>>>
>>>             Otherwise a nice solution - with minimal state,
>>>             providing that not many threads meet at the same time...
>>>
>>>             Regards, Peter
>>>
>>>                 RegardsSuman
>>>                        From: Oleksandr
>>>                 Otenko<oleksandr.otenko at oracle.com
>>>                 <mailto:oleksandr.otenko at oracle.com>>
>>>                 <mailto:oleksandr.otenko at oracle.com
>>>                 <mailto:oleksandr.otenko at oracle.com>>
>>>                   To: suman shil<suman_krec at yahoo.com
>>>                 <mailto:suman_krec at yahoo.com>>
>>>                 <mailto:suman_krec at yahoo.com
>>>                 <mailto:suman_krec at yahoo.com>>;
>>>                 Concurrency-interest<concurrency-interest at cs.oswego.edu
>>>                 <mailto:concurrency-interest at cs.oswego.edu>>
>>>                 <mailto:concurrency-interest at cs.oswego.edu
>>>                 <mailto:concurrency-interest at cs.oswego.edu>>   Sent:
>>>                 Wednesday, December 17, 2014 9:55 PM
>>>                   Subject: Re: [concurrency-interest] Enforcing
>>>                 ordered execution of critical sections?
>>>                       There is no guarantee you'll ever hand over
>>>                 the control to the right thread upon notify()
>>>                     Alex
>>>
>>>                 On 17/12/2014 14:07, suman shil wrote:
>>>                       Hi, Following is my solution to solve this
>>>                 problem. Please let me know if I am missing something.
>>>                    public class OrderedExecutor {  private int
>>>                 currentAllowedOrder = 0;  private int maxLength =
>>>                 0;  public OrderedExecutor(int n) {         
>>>                 this.maxLength = n;  } public synchronized Object
>>>                 execCriticalSectionInOrder( int order,
>>>                 Callable<Object> callable)                    throws
>>>                 Exception  { if (order >= maxLength)  {  throw new
>>>                 Exception("Exceeds maximum order "+ maxLength);  } 
>>>                   while(order != currentAllowedOrder)  {  wait(); 
>>>                 }    try  { currentAllowedOrder =
>>>                 currentAllowedOrder+1; return callable.call();  } 
>>>                 finally  { notify();  }  } }
>>>                    Regards Suman
>>>                        From: Peter Levart<peter.levart at gmail.com
>>>                 <mailto:peter.levart at gmail.com>>
>>>                 <mailto:peter.levart at gmail.com
>>>                 <mailto:peter.levart at gmail.com>>
>>>                   To: Hanson Char<hanson.char at gmail.com
>>>                 <mailto:hanson.char at gmail.com>>
>>>                 <mailto:hanson.char at gmail.com
>>>                 <mailto:hanson.char at gmail.com>>   Cc:
>>>                 concurrency-interest<concurrency-interest at cs.oswego.edu
>>>                 <mailto:concurrency-interest at cs.oswego.edu>>
>>>                 <mailto:concurrency-interest at cs.oswego.edu
>>>                 <mailto:concurrency-interest at cs.oswego.edu>>   Sent:
>>>                 Sunday, December 14, 2014 11:01 PM
>>>                   Subject: Re: [concurrency-interest] Enforcing
>>>                 ordered execution of critical sections?
>>>                           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>                      Hi Peter,
>>>                    Thanks for this proposed idea of using
>>>                 LockSupport. This begs the question: which one would
>>>                 you choose if you had all three (correct)
>>>                 implementation available? (Semaphore,
>>>                 CountDownLatch, or LockSupport)?
>>>                    Regards, Hanson
>>>                     The Semaphore/CountDownLatch variants are
>>>                 equivalent if you don't need re-use. So any would
>>>                 do. They lack invalid-use detection. What happens if
>>>                 they are not used as intended? Semaphore variant
>>>                 acts differently than CountDownLatch variant. The
>>>                 low-level variant I  proposed detects invalid usage.
>>>                 So I would probably use this one. But the low level
>>>                 variant is harder to reason about it's correctness.
>>>                 I think it is correct, but you should show it to
>>>                 somebody else to confirm this.
>>>                     Another question is whether you actually need
>>>                 this kind of synchronizer. Maybe if you explained
>>>                 what you are trying to achieve, somebody could have
>>>                 an idea how to do that even more elegantly...
>>>                     Regards, Peter
>>>                              On Sun, Dec 14, 2014 at 9:01 AM, Peter
>>>                 Levart<peter.levart at gmail.com
>>>                 <mailto:peter.levart at gmail.com>>
>>>                 <mailto:peter.levart at gmail.com
>>>                 <mailto:peter.levart at gmail.com>> wrote:
>>>                    Hi Hanson,
>>>                     This one is more low-level, but catches some
>>>                 invalid usages and is more resource-friendly:
>>>                       public class OrderedExecutor {
>>>                         public <T> T execCriticalSectionInOrder(
>>>                           final int order,
>>>                           final Supplier<T> criticalSection
>>>                       ) throws InterruptedException {
>>>                           if (order < 0) {
>>>                                throw new
>>>                 IllegalArgumentException("'order' should be >= 0");
>>>                           }
>>>                           if (order > 0) {
>>>                               waitForDone(order - 1);
>>>                           }
>>>                           try {
>>>                               return criticalSection.get();
>>>                           } finally {
>>>                               notifyDone(order);
>>>                           }
>>>                       }
>>>                         private static final Object DONE = new Object();
>>>                       private final ConcurrentMap<Integer, Object>
>>>                 signals = new ConcurrentHashMap<>();
>>>                         private void waitForDone(int order) throws
>>>                 InterruptedException {
>>>                           Object sig = signals.putIfAbsent(order,
>>>                 Thread.currentThread());
>>>                           if (sig != null && sig != DONE) {
>>>                               throw new IllegalStateException();
>>>                           }
>>>                           while (sig != DONE) {
>>>                               LockSupport.park();
>>>                               if (Thread.interrupted()) {
>>>                                   throw new InterruptedException();
>>>                               }
>>>                               sig = signals.get(order);
>>>                           }
>>>                       }
>>>                         private void notifyDone(int order) {
>>>                           Object sig = signals.putIfAbsent(order, DONE);
>>>                           if (sig instanceof Thread) {
>>>                               if (!signals.replace(order, sig, DONE)) {
>>>                                   throw new IllegalStateException();
>>>                               }
>>>                               LockSupport.unpark((Thread) sig);
>>>                           } else if (sig != null) {
>>>                               throw new IllegalStateException();
>>>                           }
>>>                       }
>>>                   }
>>>                       Regards, Peter
>>>                     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>                        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>                      Hi Peter,
>>>                    Thanks for the suggestion, and sorry about not
>>>                 being clear about one important detail: "n" is not
>>>                 known a priori when constructing an
>>>                 OrderedExecutor.  Does this mean the use of
>>>                 CountDownLatch is ruled out?
>>>                     If you know at least the upper bound of 'n', it
>>>                 can be used with such 'n'. Otherwise something that
>>>                 dynamically re-sizes the array could be devised. Or
>>>                 you could simply use a ConcurrentHashMap instead of
>>>                 array where keys are 'order' values:
>>>                       public class OrderedExecutor<T> {
>>>                         private final ConcurrentMap<Integer,
>>>                 CountDownLatch> latches = new ConcurrentHashMap<>();
>>>                         public T execCriticalSectionInOrder(final
>>>                 int order,
>>>                 final Supplier<T> criticalSection) throws
>>>                 InterruptedException {
>>>                           if (order > 0) {
>>>                               latches.computeIfAbsent(order - 1, o
>>>                 -> new CountDownLatch(1)).await();
>>>                           }
>>>                           try {
>>>                               return criticalSection.get();
>>>                           } finally {
>>>                               latches.computeIfAbsent(order, o ->
>>>                 new CountDownLatch(1)).countDown();
>>>                           }
>>>                       }
>>>                   }
>>>                       Regards, Peter
>>>                           You guessed right: it's a one-shot object
>>>                 for a particular OrderedExecutor instance, and
>>>                 "order" must be called indeed at most once.
>>>                    Regards, Hanson
>>>                   On Sun, Dec 14, 2014 at 2:21 AM, Peter
>>>                 Levart<peter.levart at gmail.com
>>>                 <mailto:peter.levart at gmail.com>>
>>>                 <mailto:peter.levart at gmail.com
>>>                 <mailto:peter.levart at gmail.com>> wrote:
>>>                    Hi Hanson,
>>>                     I don't think anything like that readily exists 
>>>                 in java.lang.concurrent, but what you describe
>>>                 should be possible to  achieve with composition of
>>>                 existing primitives.  You haven't given any
>>>                 additional hints to what your OrderedExecutor 
>>>                 should behave like. Should it be a one-shot object
>>>                 (like CountDownLatch) or a re-usable one (like
>>>                 CyclicBarrier)? Will execCriticalSectionInOrder()
>>>                 for a particular OrderedExecutor instance and
>>>                 'order' value be called at most once? If yes (and I
>>>                 think that only a one-shot object  makes sense
>>>                 here), an array of CountDownLatch(es) could be used:
>>>                     public class OrderedExecutor<T> {
>>>                       private final CountDownLatch[] latches;
>>>                         public OrderedExecutor(int n) {
>>>                           if (n < 1) throw new
>>>                 IllegalArgumentException("'n'  should be >= 1");
>>>                           latches = new CountDownLatch[n - 1];
>>>                           for (int i = 0; i < latches.length; i++) {
>>>                               latches[i] = new CountDownLatch(1);
>>>                           }
>>>                       }
>>>                         public T execCriticalSectionInOrder(final
>>>                 int order,
>>>                  final Supplier<T> criticalSection) throws
>>>                 InterruptedException {
>>>                           if (order < 0 || order > latches.length)
>>>                               throw new
>>>                 IllegalArgumentException("'order' should be [0..."
>>>                 +  latches.length + "]");
>>>                           if (order > 0) {
>>>                               latches[order - 1].await();
>>>                           }
>>>                           try {
>>>                               return criticalSection.get();
>>>                           } finally {
>>>                               if (order < latches.length) {
>>>                                   latches[order].countDown();
>>>                               }
>>>                           }
>>>                       }
>>>                   }
>>>                       Regards, Peter
>>>                     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>                         Hi, I am looking for a construct that can 
>>>                 be used to efficiently enforce ordered execution of
>>>                 multiple critical sections, each calling from a 
>>>                 different thread. The calling threads may run in 
>>>                 parallel and may call the execution method out of
>>>                 order. The perceived construct would therefore be
>>>                 responsible for re-ordering the execution of those
>>>                 threads, so that their critical  sections (and only
>>>                 the critical section) will be executed in order.
>>>                 Would something  like the following API already
>>>                 exist? /** * Used to enforce ordered execution of
>>>                 critical sections calling from multiple *  threads,
>>>                 parking and unparking the threads as necessary. */
>>>                 public class OrderedExecutor<T> { /** * Executes a
>>>                 critical section at most once with the given order,
>>>                 parking * and  unparking the current thread as 
>>>                 necessary so that all critical * sections executed 
>>>                 by different threads using this  executor take place
>>>                 in * the order from 1 to n  consecutively. */ public
>>>                 T execCriticalSectionInOrder
>>>                 (  final int order, final Callable<T>
>>>                 criticalSection) throws InterruptedException; }
>>>                 Regards, Hanson
>>>                 _______________________________________________Concurrency-interest
>>>                 mailing listConcurrency-interest at cs.oswego.edu
>>>                 <mailto:listConcurrency-interest at cs.oswego.edu>
>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>                 _______________________________________________
>>>                   Concurrency-interest mailing list
>>>                 Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>                 _______________________________________________
>>>                 Concurrency-interest mailing list
>>>                 Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>>                 _______________________________________________
>>>                 Concurrency-interest mailing list
>>>                 Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu
>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/960c8cbf/attachment-0001.html>

From oleksandr.otenko at oracle.com  Mon Dec 29 20:40:20 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 30 Dec 2014 01:40:20 +0000
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D7DC09@sm-ex-01-vm.guidewire.com>
References: <5498BBEF.8090708@cs.oswego.edu>	<NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>	<CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>	<549955EE.7050104@cs.oswego.edu>	<0FD072A166C6DC4C851F6115F37DDD2783D7CA83@sm-ex-01-vm.guidewire.com>	<549AAEB5.4000500@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D7DC09@sm-ex-01-vm.guidewire.com>
Message-ID: <54A20284.3030007@oracle.com>

This is a good way to put it. Still, a few more points need clarifying.

eg

if M+N permits are released in bulk, will two threads (one waiting for 
M, another for N) get woken up or not. Certainly, if they arrive in the 
nick of time (acquire without going to sleep), both will succeed. But 
what if both are asleep? If so, then the wording would need to state 
something like "the longest queue of threads the sum of whose permits is 
<=P"

Or contracted to:

fair semaphore: at any given time the longest waiting thread requires 
N>P permits, and all other threads wait until the longest waiting thread 
is allowed to proceed. This already means that if for the longest 
waiting thread N <= P, it can't be waiting.

unfair semaphore: at any given time the threads requiring N>P permits 
are waiting, and some other threads may be waiting for them to proceed 
first. Rather vague about who can proceed, but that's unfairness for 
you: can you be sure a thread makes progress? no, unless you are sure 
/all/ threads are N<=P.

semaphore not intended: at any give time /only/ threads requiring N>P 
permits are waiting [and threads requiring N<=P can't be waiting]. This 
is what some think semaphore means, but doesn't implement.


Alex


On 26/12/2014 10:59, Justin Sampson wrote:
> My point about barging was that if the queue is an implementation
> detail, perhaps it's best not to mention it at all, especially in
> the non-fair case.
>
> I'm not personally advocating for any stronger promises in the
> specification, I'm just jumping in with the hope of clarifying
> something that seemed weird to me (and, I believe, to several other
> folks in this discussion). Calling acquire(N) "merely a convenience"
> or "just like a loop but atomic" really doesn't do justice to its
> actual semantics in either fair or non-fair mode.
>
> Here's an attempt at describing my own current understanding, at
> this point in the discussion, of Semaphore's intended semantics, in
> a minimally-constrained way, without using the words 'queue',
> 'atomic', 'barging', or 'convenience'. :)
>
> 1. Any acquire or tryAcquire call for N permits will either succeed
>     in acquiring exactly N permits or will not acquire any permits.
>
> 2. If any acquire or tryAcquire call returns false or throws
>     InterruptedException, that call will not have acquired any
>     permits.
>
> 3. When fair == true and there are P > 0 available permits:
>
>     3a. If the acquire or tryAcquire call that has been waiting the
>         longest is for N <= P permits, then it will succeed.
>     3b. If the acquire or tryAcquire call that has been waiting the
>         longest is for N > P permits, then no acquire or tryAcquire
>         call will succeed until more permits are released, except as
>         described in 3c.
>     3c. A zero- or one-argument tryAcquire call for N <= P permits
>         may succeed immediately even if there are other acquire or
>         tryAcquire calls already waiting.
>
> 4. When fair == false and there are P > 0 available permits:
>
>     4a. If every waiting acquire or tryAcquire call is for N <= P
>         permits, then one of them will succeed.
>     4b. If any waiting acquire or tryAcquire call is for N > P
>         permits, then it is possible that no waiting acquire or
>         tryAcquire call will succeed until more permits are released.
>     4c. Any acquire or tryAcquire call for N <= P permits may succeed
>         immediately even if there are other acquire or tryAcquire
>         calls already waiting.
>
> Rules 3b and 4b cover the bits that had us tripped up in the
> OrderedExecutor discussion.
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/ec05f062/attachment.html>

From dt at flyingtroika.com  Mon Dec 29 22:34:22 2014
From: dt at flyingtroika.com (DT)
Date: Mon, 29 Dec 2014 19:34:22 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <54A1F8A9.3000208@oracle.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
	<5491D658.5060604@oracle.com> <549285B0.3070801@gmail.com>
	<CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>
	<549340FF.5070301@oracle.com>
	<64849A49-4320-49A5-811F-842B4FA1A5B6@flyingtroika.com>
	<54A1F8A9.3000208@oracle.com>
Message-ID: <54A21D3E.3040107@flyingtroika.com>

Every time when ordering is enforced in concurrent problems it means 
that the special attention should be devoted to the delays of threads. I 
think that's the case, but I might be wrong.
If multiple threads need to execute concurrently in order (having the 
same shared memory) using a sequence to overcome ordering and delivery  
issues as it's done in tcp/ip stack,  it would mean that introduction of 
time delays is necessarily. For some systems having such delays within 1 
sec is fine, for others it is not acceptable + over complication of the 
solution ( such as reordering, buffer size limits, reinitialization )
In my opinion,  using existing primitives such as Semaphore, 
CountDownLatch will not provide an optimal solution. All of them are 
using the same syncer design ( AbstractQueuedSynchronizer + different 
policies -> fair, unfair, etc). I have also not encountered  
AbstractQueuedSynchronizer being used broadly outside of the j.u.c. .

However,  another option  to overcome delays would be to use an event 
based approach. I am not sure why event type of executors are not 
introduced in the j.u.c. package as a some  sort of concurrency 
channels. I am inclined to the event type of design just because it 
feels more natural. Just a little bit speculation - like in the nature 
all processes are continuous , there are no discrete processes. The 
ordered concurrent execution of those processes is limited by the 
energy, whether the discrete processes are bounded by the design 
limitations.
I would use an OrderedExecutor implementation only for a system that 
requires deterministic behavior. But in this case I would try to come up 
with a new synchronization primitive that does not require deterministic 
behavior.

Thanks,
Dmitry

On 12/29/2014 4:58 PM, Oleksandr Otenko wrote:
> ACK/NAK? I only meant that data packets arrive concurrently, but get 
> consumed in order.
>
> You could see NAK as false returned from tryAcquire by the producer, 
> but that is not the point. In the original thesis the sequencer is 
> used to control access to the receive buffer.
>
>
> Alex
>
>
> On 22/12/2014 22:21, DT wrote:
>> I think if the sequencer is used it would mean that both ACK/ NAK 
>> type of responses should be used and sequence has to be restarted . 
>> Restarting is ok but to deal with both ACK/NAK could bring extra 
>> complexity during contention.
>>
>> Thanks,
>> Dmitry
>>
>> Sent from my iPhone
>>
>> On Dec 18, 2014, at 1:02 PM, Oleksandr Otenko 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>> Yes, no one said it is a good idea to always do that. When it is 
>>> contended, most of the threads will wake up to only go back to sleep.
>>>
>>> The pattern you are after is usually called sequencer. You can see 
>>> it used in TCP. I am not sure why it wasn't implemented in j.u.c. - 
>>> maybe not that popular.
>>>
>>> The best solution will be lock-like, but the waiter nodes will 
>>> contain the value they are waiting for - so only the specific 
>>> threads get woken up. The solution with concurrent map is very 
>>> similar, only with larger overhead from storing the index the thread 
>>> is waiting for.
>>>
>>>
>>> Alex
>>>
>>>
>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>> Less overhead and simpler are a nice properties, even though at the 
>>>> expense of having to wake up all waiting threads just to find out 
>>>> the one with the right order to execute.  Still, this seems like a 
>>>> good tradeoff.
>>>>
>>>> Thanks,
>>>> Hanson
>>>>
>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart 
>>>> <peter.levart at gmail.com <mailto:peter.levart at gmail.com>> wrote:
>>>>
>>>>     On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>>
>>>>         No, there is no difference. Peter didn't spot your entire
>>>>         method is synchronized, so spurious wakeup won't make
>>>>         progress until the owner of the lock exits the method.
>>>>
>>>>         You could split the synchronization into two blocks - one
>>>>         encompassing the wait loop, the other in the finally block;
>>>>         but it may make no difference.
>>>>
>>>>         Alex
>>>>
>>>>
>>>>     You're right, Alex. I'm so infected with park/unpark virus that
>>>>     I missed that ;-)
>>>>
>>>>     Peter
>>>>
>>>>
>>>>         On 17/12/2014 18:36, suman shil wrote:
>>>>
>>>>             Thanks peter for your reply. You are right. I should
>>>>             have incremented currentAllowedOrder in finally block.
>>>>
>>>>             Suman
>>>>             ------------------------------------------------------------------------
>>>>             *From:* Peter Levart <peter.levart at gmail.com
>>>>             <mailto:peter.levart at gmail.com>>
>>>>             *To:* suman shil <suman_krec at yahoo.com
>>>>             <mailto:suman_krec at yahoo.com>>; Oleksandr Otenko
>>>>             <oleksandr.otenko at oracle.com
>>>>             <mailto:oleksandr.otenko at oracle.com>>;
>>>>             Concurrency-interest
>>>>             <concurrency-interest at cs.oswego.edu
>>>>             <mailto:concurrency-interest at cs.oswego.edu>>
>>>>             *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>             *Subject:* Re: [concurrency-interest] Enforcing ordered
>>>>             execution of critical sections?
>>>>
>>>>             On 12/17/2014 06:46 PM, suman shil wrote:
>>>>
>>>>                 Thanks for your response. Will notifyAll() instead
>>>>                 of notify() solve the problem?
>>>>
>>>>
>>>>             It will, but you should also account for "spurious"
>>>>             wake-ups. You should increment currentAllowedOrder only
>>>>             after return from callable.call (in finally block just
>>>>             before notifyAll()).
>>>>
>>>>             Otherwise a nice solution - with minimal state,
>>>>             providing that not many threads meet at the same time...
>>>>
>>>>             Regards, Peter
>>>>
>>>>                 RegardsSuman
>>>>                        From: Oleksandr
>>>>                 Otenko<oleksandr.otenko at oracle.com
>>>>                 <mailto:oleksandr.otenko at oracle.com>>
>>>>                 <mailto:oleksandr.otenko at oracle.com
>>>>                 <mailto:oleksandr.otenko at oracle.com>>
>>>>                   To: suman shil<suman_krec at yahoo.com
>>>>                 <mailto:suman_krec at yahoo.com>>
>>>>                 <mailto:suman_krec at yahoo.com
>>>>                 <mailto:suman_krec at yahoo.com>>;
>>>>                 Concurrency-interest<concurrency-interest at cs.oswego.edu
>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>
>>>>                 <mailto:concurrency-interest at cs.oswego.edu
>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>  
>>>>                 Sent: Wednesday, December 17, 2014 9:55 PM
>>>>                   Subject: Re: [concurrency-interest] Enforcing
>>>>                 ordered execution of critical sections?
>>>>                       There is no guarantee you'll ever hand over
>>>>                 the control to the right thread upon notify()
>>>>                     Alex
>>>>
>>>>                 On 17/12/2014 14:07, suman shil wrote:
>>>>                       Hi, Following is my solution to solve this
>>>>                 problem. Please let me know if I am missing something.
>>>>                    public class OrderedExecutor {  private int
>>>>                 currentAllowedOrder = 0;  private int maxLength =
>>>>                 0;  public OrderedExecutor(int n)  {         
>>>>                 this.maxLength = n;  } public synchronized Object
>>>>                 execCriticalSectionInOrder( int order,
>>>>                 Callable<Object> callable)                    
>>>>                  throws Exception  { if (order >= maxLength)  { 
>>>>                 throw new Exception("Exceeds maximum order "+
>>>>                 maxLength);  }    while(order !=
>>>>                 currentAllowedOrder)  {  wait();  }    try {
>>>>                 currentAllowedOrder = currentAllowedOrder+1; 
>>>>                 return callable.call();  }  finally  {  notify();
>>>>                 }  } }
>>>>                    Regards Suman
>>>>                        From: Peter Levart<peter.levart at gmail.com
>>>>                 <mailto:peter.levart at gmail.com>>
>>>>                 <mailto:peter.levart at gmail.com
>>>>                 <mailto:peter.levart at gmail.com>>
>>>>                   To: Hanson Char<hanson.char at gmail.com
>>>>                 <mailto:hanson.char at gmail.com>>
>>>>                 <mailto:hanson.char at gmail.com
>>>>                 <mailto:hanson.char at gmail.com>>   Cc:
>>>>                 concurrency-interest<concurrency-interest at cs.oswego.edu
>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>
>>>>                 <mailto:concurrency-interest at cs.oswego.edu
>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>  
>>>>                 Sent: Sunday, December 14, 2014 11:01 PM
>>>>                   Subject: Re: [concurrency-interest] Enforcing
>>>>                 ordered execution of critical sections?
>>>>                           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>                      Hi Peter,
>>>>                    Thanks for this proposed idea of using
>>>>                 LockSupport. This begs the question: which one
>>>>                 would you choose if you had all three (correct)
>>>>                 implementation available? (Semaphore,
>>>>                 CountDownLatch, or LockSupport)?
>>>>                    Regards, Hanson
>>>>                     The Semaphore/CountDownLatch variants are
>>>>                 equivalent if you don't need re-use. So any would
>>>>                 do. They lack invalid-use detection. What happens
>>>>                 if they are not used as intended? Semaphore variant
>>>>                 acts differently than CountDownLatch variant. The
>>>>                 low-level variant I  proposed detects invalid
>>>>                 usage. So I would probably use this one. But the
>>>>                 low level variant is harder to reason about it's
>>>>                 correctness. I think it is correct, but you should
>>>>                 show it to somebody else to confirm this.
>>>>                     Another question is whether you actually need
>>>>                 this kind of synchronizer. Maybe if you explained
>>>>                 what you are trying to achieve, somebody could have
>>>>                 an idea how to do that even more elegantly...
>>>>                     Regards, Peter
>>>>                              On Sun, Dec 14, 2014 at 9:01 AM, Peter
>>>>                 Levart<peter.levart at gmail.com
>>>>                 <mailto:peter.levart at gmail.com>>
>>>>                 <mailto:peter.levart at gmail.com
>>>>                 <mailto:peter.levart at gmail.com>> wrote:
>>>>                    Hi Hanson,
>>>>                     This one is more low-level, but catches some
>>>>                 invalid usages and is more resource-friendly:
>>>>                       public class OrderedExecutor {
>>>>                         public <T> T execCriticalSectionInOrder(
>>>>                           final int order,
>>>>                           final Supplier<T> criticalSection
>>>>                       ) throws InterruptedException {
>>>>                           if (order < 0) {
>>>>                                throw new
>>>>                 IllegalArgumentException("'order' should be >= 0");
>>>>                           }
>>>>                           if (order > 0) {
>>>>                               waitForDone(order - 1);
>>>>                           }
>>>>                           try {
>>>>                               return criticalSection.get();
>>>>                           } finally {
>>>>                               notifyDone(order);
>>>>                           }
>>>>                       }
>>>>                         private static final Object DONE = new
>>>>                 Object();
>>>>                       private final ConcurrentMap<Integer, Object>
>>>>                 signals = new ConcurrentHashMap<>();
>>>>                         private void waitForDone(int order) throws
>>>>                 InterruptedException {
>>>>                           Object sig = signals.putIfAbsent(order,
>>>>                 Thread.currentThread());
>>>>                           if (sig != null && sig != DONE) {
>>>>                               throw new IllegalStateException();
>>>>                           }
>>>>                           while (sig != DONE) {
>>>>                               LockSupport.park();
>>>>                               if (Thread.interrupted()) {
>>>>                                   throw new InterruptedException();
>>>>                               }
>>>>                               sig = signals.get(order);
>>>>                           }
>>>>                       }
>>>>                         private void notifyDone(int order) {
>>>>                           Object sig = signals.putIfAbsent(order,
>>>>                 DONE);
>>>>                           if (sig instanceof Thread) {
>>>>                               if (!signals.replace(order, sig, DONE)) {
>>>>                                   throw new IllegalStateException();
>>>>                               }
>>>>                               LockSupport.unpark((Thread) sig);
>>>>                           } else if (sig != null) {
>>>>                               throw new IllegalStateException();
>>>>                           }
>>>>                       }
>>>>                   }
>>>>                       Regards, Peter
>>>>                     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>                        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>                      Hi Peter,
>>>>                    Thanks for the suggestion, and sorry about not
>>>>                 being clear about one important detail: "n" is not
>>>>                 known a priori when constructing an
>>>>                 OrderedExecutor.  Does this mean the use of
>>>>                 CountDownLatch is ruled out?
>>>>                     If you know at least the upper bound of 'n', it
>>>>                 can be used with such 'n'. Otherwise something that
>>>>                 dynamically re-sizes the array could be devised. Or
>>>>                 you could simply use a ConcurrentHashMap instead of
>>>>                 array where keys are 'order' values:
>>>>                       public class OrderedExecutor<T> {
>>>>                         private final ConcurrentMap<Integer,
>>>>                 CountDownLatch> latches = new ConcurrentHashMap<>();
>>>>                         public T execCriticalSectionInOrder(final
>>>>                 int order,
>>>>                 final Supplier<T> criticalSection) throws
>>>>                 InterruptedException {
>>>>                           if (order > 0) {
>>>>                 latches.computeIfAbsent(order - 1, o -> new
>>>>                 CountDownLatch(1)).await();
>>>>                           }
>>>>                           try {
>>>>                               return criticalSection.get();
>>>>                           } finally {
>>>>                 latches.computeIfAbsent(order, o -> new
>>>>                 CountDownLatch(1)).countDown();
>>>>                           }
>>>>                       }
>>>>                   }
>>>>                       Regards, Peter
>>>>                           You guessed right: it's a one-shot object
>>>>                 for a particular OrderedExecutor  instance, and
>>>>                 "order" must be called indeed at most once.
>>>>                    Regards, Hanson
>>>>                   On Sun, Dec 14, 2014 at 2:21 AM, Peter
>>>>                 Levart<peter.levart at gmail.com
>>>>                 <mailto:peter.levart at gmail.com>>
>>>>                 <mailto:peter.levart at gmail.com
>>>>                 <mailto:peter.levart at gmail.com>> wrote:
>>>>                    Hi Hanson,
>>>>                     I don't think anything like that readily
>>>>                 exists  in java.lang.concurrent, but what you
>>>>                 describe should be possible to  achieve with
>>>>                 composition of existing primitives. You haven't
>>>>                 given any additional hints to what your
>>>>                 OrderedExecutor  should behave like. Should it be a
>>>>                 one-shot object (like CountDownLatch) or a
>>>>                 re-usable one (like CyclicBarrier)? Will
>>>>                 execCriticalSectionInOrder() for a particular
>>>>                 OrderedExecutor instance and 'order' value be
>>>>                 called at most once? If yes (and I think that only
>>>>                 a one-shot object makes sense here), an array of
>>>>                 CountDownLatch(es) could be used:
>>>>                     public class OrderedExecutor<T> {
>>>>                       private final CountDownLatch[] latches;
>>>>                         public OrderedExecutor(int n) {
>>>>                           if (n < 1) throw new
>>>>                 IllegalArgumentException("'n'  should be >= 1");
>>>>                           latches = new CountDownLatch[n - 1];
>>>>                           for (int i = 0; i < latches.length; i++) {
>>>>                               latches[i] = new CountDownLatch(1);
>>>>                           }
>>>>                       }
>>>>                         public T execCriticalSectionInOrder(final
>>>>                 int order,
>>>>                  final Supplier<T> criticalSection) throws
>>>>                 InterruptedException {
>>>>                           if (order < 0 || order > latches.length)
>>>>                               throw new
>>>>                 IllegalArgumentException("'order' should be [0..."
>>>>                 +  latches.length + "]");
>>>>                           if (order > 0) {
>>>>                               latches[order - 1].await();
>>>>                           }
>>>>                           try {
>>>>                               return criticalSection.get();
>>>>                           } finally {
>>>>                               if (order < latches.length) {
>>>>                 latches[order].countDown();
>>>>                               }
>>>>                           }
>>>>                       }
>>>>                   }
>>>>                       Regards, Peter
>>>>                     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>                         Hi, I am looking for a construct that can 
>>>>                 be used to efficiently enforce  ordered execution
>>>>                 of multiple critical sections, each calling from a
>>>>                 different thread. The calling threads may run in 
>>>>                 parallel and may call the execution method out of
>>>>                 order. The  perceived construct would therefore be
>>>>                 responsible for re-ordering the execution of those
>>>>                 threads, so that their critical  sections (and only
>>>>                 the critical section) will be executed in order.
>>>>                 Would something  like the following API already
>>>>                 exist? /** * Used to enforce ordered execution of
>>>>                 critical sections calling from multiple * threads,
>>>>                 parking and unparking the  threads as necessary. */
>>>>                 public class OrderedExecutor<T> { /** * Executes a
>>>>                 critical section at most once with the given order,
>>>>                 parking * and  unparking the current thread as 
>>>>                 necessary so that all critical * sections executed 
>>>>                 by different threads using this  executor take
>>>>                 place in * the order from 1 to n  consecutively. */
>>>>                 public T execCriticalSectionInOrder
>>>>                 (  final int order, final Callable<T>
>>>>                 criticalSection) throws InterruptedException; }
>>>>                 Regards, Hanson
>>>>                 _______________________________________________Concurrency-interest
>>>>                 mailing listConcurrency-interest at cs.oswego.edu
>>>>                 <mailto:listConcurrency-interest at cs.oswego.edu>
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>                 _______________________________________________
>>>>                   Concurrency-interest mailing list
>>>>                 Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>                 _______________________________________________
>>>>                 Concurrency-interest mailing list
>>>>                 Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>                 _______________________________________________
>>>>                 Concurrency-interest mailing list
>>>>                 Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>     _______________________________________________
>>>>     Concurrency-interest mailing list
>>>>     Concurrency-interest at cs.oswego.edu
>>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141229/7cd01453/attachment-0001.html>

From oleksandr.otenko at oracle.com  Tue Dec 30 06:44:01 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 30 Dec 2014 11:44:01 +0000
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <54A21D3E.3040107@flyingtroika.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
	<5491D658.5060604@oracle.com> <549285B0.3070801@gmail.com>
	<CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>
	<549340FF.5070301@oracle.com>
	<64849A49-4320-49A5-811F-842B4FA1A5B6@flyingtroika.com>
	<54A1F8A9.3000208@oracle.com> <54A21D3E.3040107@flyingtroika.com>
Message-ID: <54A29001.4000408@oracle.com>

I'd say ordering is a type of consistency guarantee. The sort that 
allows to reason "if A happened-before B, then C will happen before D". 
I don't see what you are hinting at with delays etc. The ordering has 
other problems - the programs are not guaranteed to terminate, so 
arbitrary programs are not guaranteed to reach C from A - which will 
also indefinitely block progress to D from B. That's a much harder nut 
to crack. Events or no events, you still need a proof of termination 
(that C will always occur after A).

Alex


On 30/12/2014 03:34, DT wrote:
> Every time when ordering is enforced in concurrent problems it means 
> that the special attention should be devoted to the delays of threads. 
> I think that's the case, but I might be wrong.
> If multiple threads need to execute concurrently in order (having the 
> same shared memory) using a sequence to overcome ordering and 
> delivery  issues as it's done in tcp/ip stack,  it would mean that  
> introduction of time delays is necessarily. For some systems having 
> such delays within 1 sec is fine, for others it is not acceptable + 
> over complication of the solution ( such as reordering, buffer size 
> limits, reinitialization )
> In my opinion,  using existing primitives such as Semaphore, 
> CountDownLatch will not provide an optimal solution. All of them are 
> using the same syncer design ( AbstractQueuedSynchronizer + different 
> policies -> fair, unfair, etc). I have also not encountered  
> AbstractQueuedSynchronizer being used broadly outside of the j.u.c. .
>
> However,  another option  to overcome delays would be to use an event 
> based approach. I am not sure why event type of executors are not 
> introduced in the j.u.c. package as a some  sort of concurrency 
> channels. I am inclined to the event type of design just because it 
> feels more natural. Just a little bit speculation - like in the nature 
> all processes are continuous , there are no discrete processes. The 
> ordered concurrent execution of those processes is limited by the 
> energy, whether the discrete processes are bounded by the design 
> limitations.
> I would use an OrderedExecutor implementation only for a system that 
> requires deterministic behavior. But in this case I would try to come 
> up with a new synchronization primitive that does not require 
> deterministic behavior.
>
> Thanks,
> Dmitry
>
> On 12/29/2014 4:58 PM, Oleksandr Otenko wrote:
>> ACK/NAK? I only meant that data packets arrive concurrently, but get 
>> consumed in order.
>>
>> You could see NAK as false returned from tryAcquire by the producer, 
>> but that is not the point. In the original thesis the sequencer is 
>> used to control access to the receive buffer.
>>
>>
>> Alex
>>
>>
>> On 22/12/2014 22:21, DT wrote:
>>> I think if the sequencer is used it would mean that both ACK/ NAK 
>>> type of responses should be used and sequence has to be restarted . 
>>> Restarting is ok but to deal with both ACK/NAK could bring extra 
>>> complexity during contention.
>>>
>>> Thanks,
>>> Dmitry
>>>
>>> Sent from my iPhone
>>>
>>> On Dec 18, 2014, at 1:02 PM, Oleksandr Otenko 
>>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> 
>>> wrote:
>>>
>>>> Yes, no one said it is a good idea to always do that. When it is 
>>>> contended, most of the threads will wake up to only go back to sleep.
>>>>
>>>> The pattern you are after is usually called sequencer. You can see 
>>>> it used in TCP. I am not sure why it wasn't implemented in j.u.c. - 
>>>> maybe not that popular.
>>>>
>>>> The best solution will be lock-like, but the waiter nodes will 
>>>> contain the value they are waiting for - so only the specific 
>>>> threads get woken up. The solution with concurrent map is very 
>>>> similar, only with larger overhead from storing the index the 
>>>> thread is waiting for.
>>>>
>>>>
>>>> Alex
>>>>
>>>>
>>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>>> Less overhead and simpler are a nice properties, even though at 
>>>>> the expense of having to wake up all waiting threads just to find 
>>>>> out the one with the right order to execute.  Still, this seems 
>>>>> like a good tradeoff.
>>>>>
>>>>> Thanks,
>>>>> Hanson
>>>>>
>>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart 
>>>>> <peter.levart at gmail.com <mailto:peter.levart at gmail.com>> wrote:
>>>>>
>>>>>     On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>>>
>>>>>         No, there is no difference. Peter didn't spot your entire
>>>>>         method is synchronized, so spurious wakeup won't make
>>>>>         progress until the owner of the lock exits the method.
>>>>>
>>>>>         You could split the synchronization into two blocks - one
>>>>>         encompassing the wait loop, the other in the finally
>>>>>         block; but it may make no difference.
>>>>>
>>>>>         Alex
>>>>>
>>>>>
>>>>>     You're right, Alex. I'm so infected with park/unpark virus
>>>>>     that I missed that ;-)
>>>>>
>>>>>     Peter
>>>>>
>>>>>
>>>>>         On 17/12/2014 18:36, suman shil wrote:
>>>>>
>>>>>             Thanks peter for your reply. You are right. I should
>>>>>             have incremented currentAllowedOrder in finally block.
>>>>>
>>>>>             Suman
>>>>>             ------------------------------------------------------------------------
>>>>>             *From:* Peter Levart <peter.levart at gmail.com
>>>>>             <mailto:peter.levart at gmail.com>>
>>>>>             *To:* suman shil <suman_krec at yahoo.com
>>>>>             <mailto:suman_krec at yahoo.com>>; Oleksandr Otenko
>>>>>             <oleksandr.otenko at oracle.com
>>>>>             <mailto:oleksandr.otenko at oracle.com>>;
>>>>>             Concurrency-interest
>>>>>             <concurrency-interest at cs.oswego.edu
>>>>>             <mailto:concurrency-interest at cs.oswego.edu>>
>>>>>             *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>>             *Subject:* Re: [concurrency-interest] Enforcing
>>>>>             ordered execution of critical sections?
>>>>>
>>>>>             On 12/17/2014 06:46 PM, suman shil wrote:
>>>>>
>>>>>                 Thanks for your response. Will notifyAll() instead
>>>>>                 of notify() solve the problem?
>>>>>
>>>>>
>>>>>             It will, but you should also account for "spurious"
>>>>>             wake-ups. You should increment currentAllowedOrder
>>>>>             only after return from callable.call (in finally block
>>>>>             just before notifyAll()).
>>>>>
>>>>>             Otherwise a nice solution - with minimal state,
>>>>>             providing that not many threads meet at the same time...
>>>>>
>>>>>             Regards, Peter
>>>>>
>>>>>                 RegardsSuman
>>>>>                        From: Oleksandr
>>>>>                 Otenko<oleksandr.otenko at oracle.com
>>>>>                 <mailto:oleksandr.otenko at oracle.com>>
>>>>>                 <mailto:oleksandr.otenko at oracle.com
>>>>>                 <mailto:oleksandr.otenko at oracle.com>>
>>>>>                   To: suman shil<suman_krec at yahoo.com
>>>>>                 <mailto:suman_krec at yahoo.com>>
>>>>>                 <mailto:suman_krec at yahoo.com
>>>>>                 <mailto:suman_krec at yahoo.com>>;
>>>>>                 Concurrency-interest<concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>
>>>>>                 <mailto:concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>  
>>>>>                 Sent: Wednesday, December 17, 2014 9:55 PM
>>>>>                   Subject: Re: [concurrency-interest] Enforcing
>>>>>                 ordered execution of critical sections?
>>>>>                       There is no guarantee you'll ever hand over
>>>>>                 the control to the right thread upon notify()
>>>>>                     Alex
>>>>>
>>>>>                 On 17/12/2014 14:07, suman shil wrote:
>>>>>                       Hi, Following is my solution to solve this
>>>>>                 problem. Please let me know if I am missing something.
>>>>>                    public class OrderedExecutor {  private int
>>>>>                 currentAllowedOrder = 0;  private int maxLength =
>>>>>                 0;  public OrderedExecutor(int n)  {         
>>>>>                 this.maxLength = n;  } public synchronized Object
>>>>>                 execCriticalSectionInOrder( int order,
>>>>>                 Callable<Object> callable)                      
>>>>>                  throws Exception  { if (order >= maxLength)  { 
>>>>>                 throw new Exception("Exceeds maximum order "+
>>>>>                 maxLength);  }    while(order !=
>>>>>                 currentAllowedOrder)  {  wait();  } try  {
>>>>>                 currentAllowedOrder = currentAllowedOrder+1; 
>>>>>                 return callable.call();  }  finally  { notify(); 
>>>>>                 }  } }
>>>>>                    Regards Suman
>>>>>                        From: Peter Levart<peter.levart at gmail.com
>>>>>                 <mailto:peter.levart at gmail.com>>
>>>>>                 <mailto:peter.levart at gmail.com
>>>>>                 <mailto:peter.levart at gmail.com>>
>>>>>                   To: Hanson Char<hanson.char at gmail.com
>>>>>                 <mailto:hanson.char at gmail.com>>
>>>>>                 <mailto:hanson.char at gmail.com
>>>>>                 <mailto:hanson.char at gmail.com>>   Cc:
>>>>>                 concurrency-interest<concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>
>>>>>                 <mailto:concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>  
>>>>>                 Sent: Sunday, December 14, 2014 11:01 PM
>>>>>                   Subject: Re: [concurrency-interest] Enforcing
>>>>>                 ordered execution of critical sections?
>>>>>                           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>                      Hi Peter,
>>>>>                    Thanks for this proposed idea of using
>>>>>                 LockSupport. This begs the question: which one
>>>>>                 would you choose if you had all three (correct)
>>>>>                 implementation available? (Semaphore,
>>>>>                 CountDownLatch, or LockSupport)?
>>>>>                    Regards, Hanson
>>>>>                     The Semaphore/CountDownLatch variants are
>>>>>                 equivalent if you don't need re-use. So any would
>>>>>                 do. They lack invalid-use detection. What happens
>>>>>                 if they are not used as intended? Semaphore
>>>>>                 variant acts differently than CountDownLatch
>>>>>                 variant. The low-level variant I  proposed detects
>>>>>                 invalid usage. So I would probably use this one.
>>>>>                 But the low level variant is harder to reason
>>>>>                 about it's correctness. I think it is correct, but
>>>>>                 you should show it to somebody else to confirm this.
>>>>>                     Another question is whether you actually need
>>>>>                 this kind of synchronizer. Maybe if you explained
>>>>>                 what you are trying to achieve, somebody could
>>>>>                 have an idea how to do that even more elegantly...
>>>>>                     Regards, Peter
>>>>>                              On Sun, Dec 14, 2014 at 9:01 AM,
>>>>>                 Peter Levart<peter.levart at gmail.com
>>>>>                 <mailto:peter.levart at gmail.com>>
>>>>>                 <mailto:peter.levart at gmail.com
>>>>>                 <mailto:peter.levart at gmail.com>> wrote:
>>>>>                    Hi Hanson,
>>>>>                     This one is more low-level, but catches some
>>>>>                 invalid usages and is more resource-friendly:
>>>>>                       public class OrderedExecutor {
>>>>>                         public <T> T execCriticalSectionInOrder(
>>>>>                           final int order,
>>>>>                           final Supplier<T> criticalSection
>>>>>                       ) throws InterruptedException {
>>>>>                           if (order < 0) {
>>>>>                                throw new
>>>>>                 IllegalArgumentException("'order' should be >= 0");
>>>>>                           }
>>>>>                           if (order > 0) {
>>>>>                               waitForDone(order - 1);
>>>>>                           }
>>>>>                           try {
>>>>>                               return criticalSection.get();
>>>>>                           } finally {
>>>>>                               notifyDone(order);
>>>>>                           }
>>>>>                       }
>>>>>                         private static final Object DONE = new
>>>>>                 Object();
>>>>>                       private final ConcurrentMap<Integer, Object>
>>>>>                 signals = new ConcurrentHashMap<>();
>>>>>                         private void waitForDone(int order) throws
>>>>>                 InterruptedException {
>>>>>                           Object sig = signals.putIfAbsent(order,
>>>>>                 Thread.currentThread());
>>>>>                           if (sig != null && sig != DONE) {
>>>>>                               throw new IllegalStateException();
>>>>>                           }
>>>>>                           while (sig != DONE) {
>>>>>                               LockSupport.park();
>>>>>                               if (Thread.interrupted()) {
>>>>>                                   throw new InterruptedException();
>>>>>                               }
>>>>>                               sig = signals.get(order);
>>>>>                           }
>>>>>                       }
>>>>>                         private void notifyDone(int order) {
>>>>>                           Object sig = signals.putIfAbsent(order,
>>>>>                 DONE);
>>>>>                           if (sig instanceof Thread) {
>>>>>                               if (!signals.replace(order, sig,
>>>>>                 DONE)) {
>>>>>                                   throw new IllegalStateException();
>>>>>                               }
>>>>>                 LockSupport.unpark((Thread) sig);
>>>>>                           } else if (sig != null) {
>>>>>                               throw new IllegalStateException();
>>>>>                           }
>>>>>                       }
>>>>>                   }
>>>>>                       Regards, Peter
>>>>>                     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>                        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>                      Hi Peter,
>>>>>                    Thanks for the suggestion, and sorry about not
>>>>>                 being clear about one important  detail: "n" is
>>>>>                 not known a priori when constructing an
>>>>>                 OrderedExecutor.  Does this mean the use of
>>>>>                 CountDownLatch is ruled out?
>>>>>                     If you know at least the upper bound of 'n',
>>>>>                 it can be used with such 'n'. Otherwise something
>>>>>                 that dynamically re-sizes the array could be
>>>>>                 devised. Or you could simply use a
>>>>>                 ConcurrentHashMap instead of array where keys are
>>>>>                 'order' values:
>>>>>                       public class OrderedExecutor<T> {
>>>>>                         private final ConcurrentMap<Integer,
>>>>>                 CountDownLatch> latches = new ConcurrentHashMap<>();
>>>>>                         public T execCriticalSectionInOrder(final
>>>>>                 int order,
>>>>>                   final Supplier<T> criticalSection) throws
>>>>>                 InterruptedException {
>>>>>                           if (order > 0) {
>>>>>                 latches.computeIfAbsent(order - 1, o -> new
>>>>>                 CountDownLatch(1)).await();
>>>>>                           }
>>>>>                           try {
>>>>>                               return criticalSection.get();
>>>>>                           } finally {
>>>>>                 latches.computeIfAbsent(order, o -> new
>>>>>                 CountDownLatch(1)).countDown();
>>>>>                           }
>>>>>                       }
>>>>>                   }
>>>>>                       Regards, Peter
>>>>>                           You guessed right: it's a one-shot
>>>>>                 object for a particular OrderedExecutor  instance,
>>>>>                 and "order" must be called indeed at most once.
>>>>>                    Regards, Hanson
>>>>>                   On Sun, Dec 14, 2014 at 2:21 AM, Peter
>>>>>                 Levart<peter.levart at gmail.com
>>>>>                 <mailto:peter.levart at gmail.com>>
>>>>>                 <mailto:peter.levart at gmail.com
>>>>>                 <mailto:peter.levart at gmail.com>> wrote:
>>>>>                    Hi Hanson,
>>>>>                     I don't think anything like that readily
>>>>>                 exists  in java.lang.concurrent, but what you
>>>>>                 describe should be possible to  achieve with
>>>>>                 composition of existing primitives.  You haven't
>>>>>                 given any additional hints to what your
>>>>>                 OrderedExecutor  should behave like. Should it be
>>>>>                 a one-shot object (like CountDownLatch) or a
>>>>>                 re-usable one (like CyclicBarrier)? Will
>>>>>                 execCriticalSectionInOrder() for a particular
>>>>>                 OrderedExecutor instance and 'order' value be
>>>>>                 called at most once? If yes (and I think that only
>>>>>                 a one-shot object  makes sense here), an array of
>>>>>                 CountDownLatch(es) could be used:
>>>>>                     public class OrderedExecutor<T> {
>>>>>                       private final CountDownLatch[] latches;
>>>>>                         public OrderedExecutor(int n) {
>>>>>                           if (n < 1) throw new
>>>>>                 IllegalArgumentException("'n'  should be >= 1");
>>>>>                           latches = new CountDownLatch[n - 1];
>>>>>                           for (int i = 0; i < latches.length; i++) {
>>>>>                               latches[i] = new CountDownLatch(1);
>>>>>                           }
>>>>>                       }
>>>>>                         public T execCriticalSectionInOrder(final
>>>>>                 int order,
>>>>>                  final Supplier<T> criticalSection) throws
>>>>>                 InterruptedException {
>>>>>                           if (order < 0 || order > latches.length)
>>>>>                               throw new
>>>>>                 IllegalArgumentException("'order' should be [0..."
>>>>>                 +  latches.length + "]");
>>>>>                           if (order > 0) {
>>>>>                               latches[order - 1].await();
>>>>>                           }
>>>>>                           try {
>>>>>                               return criticalSection.get();
>>>>>                           } finally {
>>>>>                               if (order < latches.length) {
>>>>>                 latches[order].countDown();
>>>>>                               }
>>>>>                           }
>>>>>                       }
>>>>>                   }
>>>>>                       Regards, Peter
>>>>>                     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>                         Hi, I am looking for a construct that can 
>>>>>                 be used to efficiently enforce  ordered execution
>>>>>                 of multiple critical sections, each calling from a
>>>>>                 different thread. The calling threads may run in 
>>>>>                 parallel and may call the execution method out of
>>>>>                 order. The  perceived construct would therefore be
>>>>>                 responsible for re-ordering the execution of those
>>>>>                 threads, so that their critical  sections (and
>>>>>                 only the critical section) will be executed in
>>>>>                 order. Would something  like the following API
>>>>>                 already exist? /** * Used to enforce ordered
>>>>>                 execution of critical sections calling from
>>>>>                 multiple *  threads, parking and unparking the 
>>>>>                 threads as necessary. */ public class
>>>>>                 OrderedExecutor<T> { /** * Executes a critical
>>>>>                 section at most once with the given order, parking
>>>>>                 * and unparking the current thread as  necessary
>>>>>                 so that all critical * sections executed  by
>>>>>                 different threads using this  executor take place
>>>>>                 in * the order from 1 to n consecutively. */
>>>>>                 public T execCriticalSectionInOrder
>>>>>                 (  final int order, final Callable<T>
>>>>>                 criticalSection) throws InterruptedException; }
>>>>>                 Regards, Hanson
>>>>>                 _______________________________________________Concurrency-interest
>>>>>                 mailing listConcurrency-interest at cs.oswego.edu
>>>>>                 <mailto:listConcurrency-interest at cs.oswego.edu>
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>                 _______________________________________________
>>>>>                   Concurrency-interest mailing list
>>>>>                 Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>                 _______________________________________________
>>>>>                 Concurrency-interest mailing list
>>>>>                 Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>                 _______________________________________________
>>>>>                 Concurrency-interest mailing list
>>>>>                 Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>     _______________________________________________
>>>>>     Concurrency-interest mailing list
>>>>>     Concurrency-interest at cs.oswego.edu
>>>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu 
>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/b9ac22db/attachment-0001.html>

From oleksandr.otenko at oracle.com  Tue Dec 30 08:05:54 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 30 Dec 2014 13:05:54 +0000
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <CAHzJPEoA87RkP1_TfjbPey29X0uJZ=gOwoBrxoh66shXSVZuoA@mail.gmail.com>
References: <5491CA55.5000705@gmail.com>	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>	<5491D658.5060604@oracle.com>
	<549285B0.3070801@gmail.com>	<CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>	<549340FF.5070301@oracle.com>
	<CAHzJPEoA87RkP1_TfjbPey29X0uJZ=gOwoBrxoh66shXSVZuoA@mail.gmail.com>
Message-ID: <54A2A332.1060005@oracle.com>

What makes the code using wait/notify hard to maintain?

Alex

On 18/12/2014 21:32, Joe Bowbeer wrote:
> I frown on use of notify[All]/wait because they make the code hard to 
> maintain.
>
> In this case, with potentially lots of waiting threads, I would check 
> out the "Specific Notification" pattern if I were determined to go the 
> wait/notify route:
>
> Tim Cargill's paper is dated but still worth reading.
>
> Also see chapter 3.7.3 Specific Notifications in Doug Lea's CPiJ and 
> Peter Haggar's article:
>
> http://www.ibm.com/developerworks/java/library/j-spnotif.html
>
> On Thu, Dec 18, 2014 at 1:02 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Yes, no one said it is a good idea to always do that. When it is
>     contended, most of the threads will wake up to only go back to sleep.
>
>     The pattern you are after is usually called sequencer. You can see
>     it used in TCP. I am not sure why it wasn't implemented in j.u.c.
>     - maybe not that popular.
>
>     The best solution will be lock-like, but the waiter nodes will
>     contain the value they are waiting for - so only the specific
>     threads get woken up. The solution with concurrent map is very
>     similar, only with larger overhead from storing the index the
>     thread is waiting for.
>
>
>     Alex
>
>
>
>     On 18/12/2014 20:21, Hanson Char wrote:
>>     Less overhead and simpler are a nice properties, even though at
>>     the expense of having to wake up all waiting threads just to find
>>     out the one with the right order to execute.  Still, this seems
>>     like a good tradeoff.
>>
>>     Thanks,
>>     Hanson
>>
>>     On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart
>>     <peter.levart at gmail.com <mailto:peter.levart at gmail.com>> wrote:
>>
>>         On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>
>>             No, there is no difference. Peter didn't spot your entire
>>             method is synchronized, so spurious wakeup won't make
>>             progress until the owner of the lock exits the method.
>>
>>             You could split the synchronization into two blocks - one
>>             encompassing the wait loop, the other in the finally
>>             block; but it may make no difference.
>>
>>             Alex
>>
>>
>>         You're right, Alex. I'm so infected with park/unpark virus
>>         that I missed that ;-)
>>
>>         Peter
>>
>>
>>             On 17/12/2014 18:36, suman shil wrote:
>>
>>                 Thanks peter for your reply. You are right. I should
>>                 have incremented currentAllowedOrder in finally block.
>>
>>                 Suman
>>                 ------------------------------------------------------------------------
>>                 *From:* Peter Levart <peter.levart at gmail.com
>>                 <mailto:peter.levart at gmail.com>>
>>                 *To:* suman shil <suman_krec at yahoo.com
>>                 <mailto:suman_krec at yahoo.com>>; Oleksandr Otenko
>>                 <oleksandr.otenko at oracle.com
>>                 <mailto:oleksandr.otenko at oracle.com>>;
>>                 Concurrency-interest
>>                 <concurrency-interest at cs.oswego.edu
>>                 <mailto:concurrency-interest at cs.oswego.edu>>
>>                 *Sent:* Wednesday, December 17, 2014 11:54 PM
>>                 *Subject:* Re: [concurrency-interest] Enforcing
>>                 ordered execution of critical sections?
>>
>>                 On 12/17/2014 06:46 PM, suman shil wrote:
>>
>>                     Thanks for your response. Will notifyAll()
>>                     instead of notify() solve the problem?
>>
>>
>>                 It will, but you should also account for "spurious"
>>                 wake-ups. You should increment currentAllowedOrder
>>                 only after return from callable.call (in finally
>>                 block just before notifyAll()).
>>
>>                 Otherwise a nice solution - with minimal state,
>>                 providing that not many threads meet at the same time...
>>
>>                 Regards, Peter
>>
>>                     RegardsSuman
>>                            From: Oleksandr
>>                     Otenko<oleksandr.otenko at oracle.com
>>                     <mailto:oleksandr.otenko at oracle.com>>
>>                     <mailto:oleksandr.otenko at oracle.com
>>                     <mailto:oleksandr.otenko at oracle.com>>
>>                       To: suman shil<suman_krec at yahoo.com
>>                     <mailto:suman_krec at yahoo.com>>
>>                     <mailto:suman_krec at yahoo.com
>>                     <mailto:suman_krec at yahoo.com>>;
>>                     Concurrency-interest<concurrency-interest at cs.oswego.edu
>>                     <mailto:concurrency-interest at cs.oswego.edu>>
>>                     <mailto:concurrency-interest at cs.oswego.edu
>>                     <mailto:concurrency-interest at cs.oswego.edu>>  
>>                     Sent: Wednesday, December 17, 2014 9:55 PM
>>                       Subject: Re: [concurrency-interest] Enforcing
>>                     ordered execution of critical sections?
>>                           There is no guarantee you'll ever hand over
>>                     the control to the right thread upon notify()
>>                         Alex
>>
>>                     On 17/12/2014 14:07, suman shil wrote:
>>                           Hi, Following is my solution to solve this
>>                     problem. Please let me know if I am missing
>>                     something.
>>                        public class OrderedExecutor { private int
>>                     currentAllowedOrder = 0; private int maxLength =
>>                     0;  public OrderedExecutor(int n)  {
>>                     this.maxLength = n;  } public synchronized Object
>>                     execCriticalSectionInOrder( int order,
>>                     Callable<Object> callable)                      
>>                          throws Exception  { if (order >= maxLength) 
>>                     {  throw new Exception("Exceeds maximum order "+
>>                     maxLength);  }    while(order !=
>>                     currentAllowedOrder)  {  wait();  } try  {
>>                     currentAllowedOrder = currentAllowedOrder+1; 
>>                     return callable.call();  }  finally  { notify(); 
>>                     }  } }
>>                        Regards Suman
>>                            From: Peter Levart<peter.levart at gmail.com
>>                     <mailto:peter.levart at gmail.com>>
>>                     <mailto:peter.levart at gmail.com
>>                     <mailto:peter.levart at gmail.com>>
>>                       To: Hanson Char<hanson.char at gmail.com
>>                     <mailto:hanson.char at gmail.com>>
>>                     <mailto:hanson.char at gmail.com
>>                     <mailto:hanson.char at gmail.com>>   Cc:
>>                     concurrency-interest<concurrency-interest at cs.oswego.edu
>>                     <mailto:concurrency-interest at cs.oswego.edu>>
>>                     <mailto:concurrency-interest at cs.oswego.edu
>>                     <mailto:concurrency-interest at cs.oswego.edu>>  
>>                     Sent: Sunday, December 14, 2014 11:01 PM
>>                       Subject: Re: [concurrency-interest] Enforcing
>>                     ordered execution of critical sections?
>>                               On 12/14/2014 06:11 PM, Hanson Char wrote:
>>                          Hi Peter,
>>                        Thanks for this proposed idea of using
>>                     LockSupport. This begs the question: which one
>>                     would you choose if you had all three (correct)
>>                     implementation available?  (Semaphore,
>>                     CountDownLatch, or LockSupport)?
>>                        Regards, Hanson
>>                         The Semaphore/CountDownLatch variants are
>>                     equivalent if you don't need re-use. So any would
>>                     do. They lack invalid-use detection. What happens
>>                     if they are not used as intended? Semaphore
>>                     variant acts differently than CountDownLatch
>>                     variant. The low-level variant I proposed detects
>>                     invalid usage. So I would probably use this one.
>>                     But the low level variant is harder to reason
>>                     about it's correctness. I think it is correct,
>>                     but you should show it to somebody else to
>>                     confirm this.
>>                         Another question is whether you actually need
>>                     this kind of synchronizer. Maybe if you explained
>>                     what you are trying to achieve, somebody could
>>                     have an idea how to do that even more elegantly...
>>                         Regards, Peter
>>                                  On Sun, Dec 14, 2014 at 9:01 AM,
>>                     Peter Levart<peter.levart at gmail.com
>>                     <mailto:peter.levart at gmail.com>>
>>                     <mailto:peter.levart at gmail.com
>>                     <mailto:peter.levart at gmail.com>> wrote:
>>                        Hi Hanson,
>>                         This one is more low-level, but catches some
>>                     invalid usages and is more resource-friendly:
>>                           public class OrderedExecutor {
>>                             public <T> T execCriticalSectionInOrder(
>>                               final int order,
>>                               final Supplier<T> criticalSection
>>                           ) throws InterruptedException {
>>                               if (order < 0) {
>>                                    throw new
>>                     IllegalArgumentException("'order' should be >= 0");
>>                               }
>>                               if (order > 0) {
>>                                   waitForDone(order - 1);
>>                               }
>>                               try {
>>                                   return criticalSection.get();
>>                               } finally {
>>                                   notifyDone(order);
>>                               }
>>                           }
>>                             private static final Object DONE = new
>>                     Object();
>>                           private final ConcurrentMap<Integer,
>>                     Object> signals = new ConcurrentHashMap<>();
>>                             private void waitForDone(int order)
>>                     throws InterruptedException {
>>                               Object sig = signals.putIfAbsent(order,
>>                     Thread.currentThread());
>>                               if (sig != null && sig != DONE) {
>>                                   throw new IllegalStateException();
>>                               }
>>                               while (sig != DONE) {
>>                                   LockSupport.park();
>>                                   if (Thread.interrupted()) {
>>                                       throw new InterruptedException();
>>                                   }
>>                                   sig = signals.get(order);
>>                               }
>>                           }
>>                             private void notifyDone(int order) {
>>                               Object sig = signals.putIfAbsent(order,
>>                     DONE);
>>                               if (sig instanceof Thread) {
>>                                   if (!signals.replace(order, sig,
>>                     DONE)) {
>>                                       throw new IllegalStateException();
>>                                   }
>>                     LockSupport.unpark((Thread) sig);
>>                               } else if (sig != null) {
>>                                   throw new IllegalStateException();
>>                               }
>>                           }
>>                       }
>>                           Regards, Peter
>>                         On 12/14/2014 05:08 PM, Peter Levart wrote:
>>                            On 12/14/2014 04:20 PM, Hanson Char wrote:
>>                          Hi Peter,
>>                        Thanks for the suggestion, and sorry about not
>>                     being clear about one important  detail: "n" is
>>                     not known a priori when constructing an
>>                     OrderedExecutor.  Does this mean the use of
>>                     CountDownLatch is ruled out?
>>                         If you know at least the upper bound of 'n',
>>                     it can be used with such 'n'. Otherwise something
>>                     that dynamically re-sizes the array could be
>>                     devised. Or you could simply use a
>>                     ConcurrentHashMap instead of array where keys are
>>                     'order' values:
>>                           public class OrderedExecutor<T> {
>>                             private final ConcurrentMap<Integer,
>>                     CountDownLatch> latches = new ConcurrentHashMap<>();
>>                             public T execCriticalSectionInOrder(final
>>                     int order,
>>                           final Supplier<T> criticalSection) throws
>>                     InterruptedException {
>>                               if (order > 0) {
>>                     latches.computeIfAbsent(order - 1, o -> new
>>                     CountDownLatch(1)).await();
>>                               }
>>                               try {
>>                                   return criticalSection.get();
>>                               } finally {
>>                     latches.computeIfAbsent(order, o -> new
>>                     CountDownLatch(1)).countDown();
>>                               }
>>                           }
>>                       }
>>                           Regards, Peter
>>                               You guessed right: it's a one-shot
>>                     object for a particular OrderedExecutor 
>>                     instance, and "order" must be called indeed at
>>                     most once.
>>                        Regards, Hanson
>>                       On Sun, Dec 14, 2014 at 2:21 AM, Peter
>>                     Levart<peter.levart at gmail.com
>>                     <mailto:peter.levart at gmail.com>>
>>                     <mailto:peter.levart at gmail.com
>>                     <mailto:peter.levart at gmail.com>> wrote:
>>                        Hi Hanson,
>>                         I don't think anything like that readily
>>                     exists  in java.lang.concurrent, but what you
>>                     describe should be possible to achieve with
>>                     composition of existing primitives.  You haven't
>>                     given any additional hints to what your
>>                     OrderedExecutor  should behave like. Should it be
>>                     a one-shot object (like CountDownLatch) or a
>>                     re-usable one (like CyclicBarrier)? Will
>>                     execCriticalSectionInOrder() for a particular
>>                     OrderedExecutor instance and 'order' value be
>>                     called at most once? If yes (and I think that
>>                     only a one-shot object  makes sense here), an
>>                     array of CountDownLatch(es) could be used:
>>                         public class OrderedExecutor<T> {
>>                           private final CountDownLatch[] latches;
>>                             public OrderedExecutor(int n) {
>>                               if (n < 1) throw new
>>                     IllegalArgumentException("'n'  should be >= 1");
>>                               latches = new CountDownLatch[n - 1];
>>                               for (int i = 0; i < latches.length; i++) {
>>                                   latches[i] = new CountDownLatch(1);
>>                               }
>>                           }
>>                             public T execCriticalSectionInOrder(final
>>                     int order,
>>                          final Supplier<T> criticalSection) throws
>>                     InterruptedException {
>>                               if (order < 0 || order > latches.length)
>>                                   throw new
>>                     IllegalArgumentException("'order' should be
>>                     [0..." +  latches.length + "]");
>>                               if (order > 0) {
>>                                   latches[order - 1].await();
>>                               }
>>                               try {
>>                                   return criticalSection.get();
>>                               } finally {
>>                                   if (order < latches.length) {
>>                     latches[order].countDown();
>>                                   }
>>                               }
>>                           }
>>                       }
>>                           Regards, Peter
>>                         On 12/14/2014 05:26 AM, Hanson Char wrote:
>>                             Hi, I am looking for a construct that
>>                     can  be used to efficiently enforce  ordered
>>                     execution of multiple critical sections, each
>>                     calling from a  different thread. The calling
>>                     threads may run in  parallel and may call the
>>                     execution method out of order. The  perceived
>>                     construct would therefore be responsible for
>>                     re-ordering the execution of those threads, so
>>                     that their critical  sections (and only the
>>                     critical section) will be executed in order.
>>                     Would something  like the following API already
>>                     exist? /** * Used to enforce ordered execution of
>>                     critical sections calling from multiple *
>>                     threads, parking and unparking the threads as
>>                     necessary. */ public class OrderedExecutor<T> {
>>                     /** * Executes a critical section at most once
>>                     with the given order, parking * and unparking the
>>                     current thread as necessary so that all critical
>>                     * sections executed  by different threads using
>>                     this  executor take place in * the order from 1
>>                     to n  consecutively. */ public T
>>                     execCriticalSectionInOrder
>>                     (  final int order, final Callable<T>
>>                     criticalSection) throws InterruptedException; }
>>                     Regards, Hanson
>>                     _______________________________________________Concurrency-interest
>>                     mailing listConcurrency-interest at cs.oswego.edu
>>                     <mailto:listConcurrency-interest at cs.oswego.edu>
>>                     <mailto:Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>                     _______________________________________________
>>                       Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     <mailto:Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     <mailto:Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     <mailto:Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/8e9fd0ce/attachment-0001.html>

From oleksandr.otenko at oracle.com  Tue Dec 30 09:54:11 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 30 Dec 2014 14:54:11 +0000
Subject: [concurrency-interest] synchronized vs
	Unsafe#monitorEnter/monitorExit
In-Reply-To: <1313917327.1213625.1419797733603.JavaMail.yahoo@jws10046.mail.ne1.yahoo.com>
References: <54A00320.1050609@gmail.com>
	<1313917327.1213625.1419797733603.JavaMail.yahoo@jws10046.mail.ne1.yahoo.com>
Message-ID: <54A2BC93.5090406@oracle.com>

Why do you need synchronized specifically? (and not j.u.c.locks.Lock 
lock/unlock)

Alex

On 28/12/2014 20:15, Ben Manes wrote:
> That's an nifty workaround but, as you said, not reliable in the case 
> of a general api. This case is a multi-get for a cache library, so the 
> keys and use-cases aren't known.
>
> When adding bulk loading to Guava, due to the internal complexity we 
> allowed it to racy by not blocking concurrent calls for shared 
> entries. This has to be done regardless because a loadAll(keys) may 
> return a Map<K, V> with more entries than originally requested, and 
> those extra entries should be cached. These cases were trivial to 
> handle when I had previously written a Map<K, Future<V>> to support 
> multi-get, but a lot of bloat per entry. I had hoped 
> Unsafe#monitorEnter would be a nice alternative and, if the API was 
> stable, it arguably still could be due to the low write rate of 
> caches. For now I'll leave it in a backlog of performance optimization 
> ideas like we did with Guava, maybe revisiting once the rewrite 
> stabilizes.
>
> Thanks,
> -Ben
>
>
> On Sunday, December 28, 2014 5:18 AM, Peter Levart 
> <peter.levart at gmail.com> wrote:
>
>
>
> On 12/27/2014 09:31 PM, Ben Manes wrote:
>> Can someone explain why using Unsafe's monitor methods are 
>> substantially worse than synchronized? I had expected them to emit 
>> equivalent monitorEnter/monitorExit instructions and have similar 
>> performance.
>>
>> My use case is to support a bulk version of CHM#computeIfAbsent, 
>> where a single mapping function returns the result for computing 
>> multiple entries. I had hoped to bulk lock, insert the unfilled 
>> entries, compute, populate, and bulk unlock. An overlapping write 
>> would be blocked due to requiring an entry's lock for mutation.
>
> Hi Ben,
>
> If "multiple entries" means less than say 50 or even a little more 
> (using more locks than that for one bulk operation is not very optimal 
> anyway), you could try constructing a recursive method to bulk-lock on 
> the list of objects and execute a provided function:
>
>
>     public static <T> T synchronizedAll(Iterable<?> locks, Supplier<T> 
> supplier) {
>         return synchronizedAll(locks.iterator(), supplier);
>     }
>
>     private static <T> T synchronizedAll(Iterator<?> locksIterator, 
> Supplier<T> supplier) {
>         if (locksIterator.hasNext()) {
>             synchronized (locksIterator.next()) {
>                 return synchronizedAll(locksIterator, supplier);
>             }
>         } else {
>             return supplier.get();
>         }
>     }
>
>
>
> Note that to avoid deadlocks, the locks list has to be sorted using a 
> Comparator that is consistent with your key's equals() method...
>
> Regards, Peter
>
>
>
>> I had thought that using Unsafe would allow for achieving this 
>> without the memory overhead of a ReentrantLock/AQS per entry, since 
>> the synchronized keyword is not flexible enough to provide this 
>> structure.
>>
>> Thanks,
>> Ben
>>
>> Benchmark                          Mode  Samples   Score         
>> Error  Units
>> c.g.b.c.SynchronizedBenchmark.monitor_contention            thrpt     
>>   10   3694951.630 ? 34340.707  ops/s
>> c.g.b.c.SynchronizedBenchmark.monitor_noContention          thrpt     
>>   10   8274097.911 ?  164356.363  ops/s
>> c.g.b.c.SynchronizedBenchmark.reentrantLock_contention      thrpt     
>>   10  31668532.247 ?  740850.955  ops/s
>> c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention    thrpt     
>>   10  41380163.703 ? 2270103.507  ops/s
>> c.g.b.c.SynchronizedBenchmark.synchronized_contention       thrpt     
>>   10  22905995.761 ?  117868.968  ops/s
>> c.g.b.c.SynchronizedBenchmark.synchronized_noContention     thrpt     
>>   10  44891601.915 ? 1458775.665  ops/s
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/500e8a27/attachment.html>

From oleksandr.otenko at oracle.com  Tue Dec 30 10:22:52 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 30 Dec 2014 15:22:52 +0000
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
Message-ID: <54A2C34C.6000807@oracle.com>

Do you know a non-concurrent structure that you are looking for?

(eg you don't like O(n) for removal of arbitrary items - but is it 
possible to have /all/ operations O(1), even in a non-concurrent case?)


Alex


On 22/12/2014 02:35, Luke Sandberg wrote:
>
> I have come across a few situations where i am looking for a 
> datastructure and i feel like i keep coming up short.
>
> The situation is analogous to the 'waiters' list in 
> java.util.concurrent.FutureTask.
>
> Basically, I need to be able to publish a non-null object reference 
> into a data structure where
> * order doesn't matter (insertion order would be nice, but i don't 
> care that much)
> * add and remove identity semantics
> * concurrent iteration (weakly consistent is fine)
> * add/remove are non-blocking
>
> CLQ is an obvious choice but removal is O(n), another choice would a 
> simple synchronized identity hash set which is fine but the lock + 
> high entry overhead is a deal breaker.
>
> An AtomicReferenceArray would be super easy, but i can't put a bound 
> on the number of items.
>
> Also, it would be fine for the code that adds items, to store 
> additional state (an index, a 'Node' reference), in order to 
> facilitate removal.
>
> The best thing i have seen from looking around appears to be something 
> like what FutureTask does to implement 'awaitDone', but even there 
> removeWaitier() is O(n).  that seems like a pretty good compromise 
> when lists are short, but what would be a better solution when lists 
> are long?
>
> Just to motivate this a little bit, the two cases I am looking at in 
> particular are:
>
> * maintaining a set of threads associated with a server 'request' 
> (threads enter/exit when they start executing tasks associated with 
> the request)
> * maintaining a set of futures to be cancelled (futures are removed 
> when they complete to avoid pinning their referents).
>
> Any pointers or ideas?
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/71fe7712/attachment.html>

From martinrb at google.com  Tue Dec 30 11:49:47 2014
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 30 Dec 2014 08:49:47 -0800
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
	<549829A7.1020604@infinite-source.de>
	<CAO9V1MKRhY6=UwieS8=Gq=fv8FQNDcuC497HkycH-Ykqvcia2g@mail.gmail.com>
Message-ID: <CA+kOe0_cx-CuPeqwcNZwMTBa_19-JY9JFuom41NbmixgJgQO0g@mail.gmail.com>

On Mon, Dec 22, 2014 at 8:10 AM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

>
> RE: CLQ with some extra method.  I had thought it would have been nice to
> have CLQ return a Node object for this usecase.  Though if i wouldn't be
> able to physically remove the node then it seems simpler to just build my
> own Trieber stack and implement remove by nulling out the 'item' field in
> the Node object.
>

If you add a Node-returning method to CLQ, you can't guarantee that Nodes
can always be unlinked, but it's possible to sweep a few successor Nodes
whenever an element is removed, which is likely to be good enough in
practice. Taking another look at CLQ, I see CLQ.iterator().remove() makes
no effort to do any unlinking, leaving that task to a future traversal.  If
you have a Node-returning method, it's less likely there will be a "future
iteration".

If you use my pet ConcurrentLinkedDeque instead of ConcurrentLinkedQueue,
you get guaranteed amortized O(1) deletion with negligible garbage
accumulation, and you get an already written unlink(Node) method, so it's
pretty easy to add the "handle-returning method" without dealing with CLD's
extremely tricky unlinking mechanics.  If you want to go this way, I'm
willing to code something up.  Seems generally useful anyways, but is a
fundamental new feature in java collections.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/2ecd851b/attachment-0001.html>

From dt at flyingtroika.com  Tue Dec 30 12:37:38 2014
From: dt at flyingtroika.com (DT)
Date: Tue, 30 Dec 2014 09:37:38 -0800
Subject: [concurrency-interest] Enforcing ordered execution of critical
 sections?
In-Reply-To: <54A29001.4000408@oracle.com>
References: <5491CA55.5000705@gmail.com>
	<1008086066.91003.1418841405454.JavaMail.yahoo@jws10722.mail.gq1.yahoo.com>
	<5491D658.5060604@oracle.com> <549285B0.3070801@gmail.com>
	<CABWgujbEz8eZno_0xF_rf2G7=b2LF-7r1qOpa5Ktzt34cCy4-Q@mail.gmail.com>
	<549340FF.5070301@oracle.com>
	<64849A49-4320-49A5-811F-842B4FA1A5B6@flyingtroika.com>
	<54A1F8A9.3000208@oracle.com> <54A21D3E.3040107@flyingtroika.com>
	<54A29001.4000408@oracle.com>
Message-ID: <54A2E2E2.6070300@flyingtroika.com>

I was applying delay issues in respect to the sequencer pattern if its 
used to come up with the  OrderederdExecution . I did not think about 
delivery guarantees.
In my opinion, to have a prove of termination is not exactly necessarily 
if  threads are subscribed equally to events. Though it reminds me 
notifyAll concept.

Dmitry

On 12/30/2014 3:44 AM, Oleksandr Otenko wrote:
> I'd say ordering is a type of consistency guarantee. The sort that 
> allows to reason "if A happened-before B, then C will happen before 
> D". I don't see what you are hinting at with delays etc. The ordering 
> has other problems - the programs are not guaranteed to terminate, so 
> arbitrary programs are not guaranteed to reach C from A - which will 
> also indefinitely block progress to D from B. That's a much harder nut 
> to crack. Events or no events, you still need a proof of termination 
> (that C will always occur after A).
>
> Alex
>
>
> On 30/12/2014 03:34, DT wrote:
>> Every time when ordering is enforced in concurrent problems it means 
>> that the special attention should be devoted to the delays of 
>> threads. I think that's the case, but I might be wrong.
>> If multiple threads need to execute concurrently in order (having the 
>> same shared memory) using a sequence to overcome ordering and 
>> delivery  issues as it's done in tcp/ip stack,  it would mean that  
>> introduction of time delays is necessarily. For some systems having 
>> such delays within 1 sec is fine, for others it is not acceptable + 
>> over complication of the solution ( such as reordering, buffer size 
>> limits, reinitialization )
>> In my opinion,  using existing primitives such as Semaphore, 
>> CountDownLatch will not provide an optimal solution. All of them are 
>> using the same syncer design ( AbstractQueuedSynchronizer + different 
>> policies -> fair, unfair, etc). I have also not encountered  
>> AbstractQueuedSynchronizer being used broadly outside of the j.u.c. .
>>
>> However,  another option  to overcome delays would be to use an event 
>> based approach. I am not sure why event type of executors are not 
>> introduced in the j.u.c. package as a some  sort of concurrency 
>> channels. I am inclined to the event type of design just because it 
>> feels more natural. Just a little bit speculation - like in the 
>> nature all processes are continuous , there are no discrete 
>> processes. The ordered concurrent execution of those processes is 
>> limited by the energy, whether the discrete processes are bounded by 
>> the design limitations.
>> I would use an OrderedExecutor implementation only for a system that 
>> requires deterministic behavior. But in this case I would try to come 
>> up with a new synchronization primitive that does not require 
>> deterministic behavior.
>>
>> Thanks,
>> Dmitry
>>
>> On 12/29/2014 4:58 PM, Oleksandr Otenko wrote:
>>> ACK/NAK? I only meant that data packets arrive concurrently, but get 
>>> consumed in order.
>>>
>>> You could see NAK as false returned from tryAcquire by the producer, 
>>> but that is not the point. In the original thesis the sequencer is 
>>> used to control access to the receive buffer.
>>>
>>>
>>> Alex
>>>
>>>
>>> On 22/12/2014 22:21, DT wrote:
>>>> I think if the sequencer is used it would mean that both ACK/ NAK 
>>>> type of responses should be used and sequence has to be restarted . 
>>>> Restarting is ok but to deal with both ACK/NAK could bring extra 
>>>> complexity during contention.
>>>>
>>>> Thanks,
>>>> Dmitry
>>>>
>>>> Sent from my iPhone
>>>>
>>>> On Dec 18, 2014, at 1:02 PM, Oleksandr Otenko 
>>>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> 
>>>> wrote:
>>>>
>>>>> Yes, no one said it is a good idea to always do that. When it is 
>>>>> contended, most of the threads will wake up to only go back to sleep.
>>>>>
>>>>> The pattern you are after is usually called sequencer. You can see 
>>>>> it used in TCP. I am not sure why it wasn't implemented in j.u.c. 
>>>>> - maybe not that popular.
>>>>>
>>>>> The best solution will be lock-like, but the waiter nodes will 
>>>>> contain the value they are waiting for - so only the specific 
>>>>> threads get woken up. The solution with concurrent map is very 
>>>>> similar, only with larger overhead from storing the index the 
>>>>> thread is waiting for.
>>>>>
>>>>>
>>>>> Alex
>>>>>
>>>>>
>>>>> On 18/12/2014 20:21, Hanson Char wrote:
>>>>>> Less overhead and simpler are a nice properties, even though at 
>>>>>> the expense of having to wake up all waiting threads just to find 
>>>>>> out the one with the right order to execute.  Still, this seems 
>>>>>> like a good tradeoff.
>>>>>>
>>>>>> Thanks,
>>>>>> Hanson
>>>>>>
>>>>>> On Wed, Dec 17, 2014 at 11:43 PM, Peter Levart 
>>>>>> <peter.levart at gmail.com <mailto:peter.levart at gmail.com>> wrote:
>>>>>>
>>>>>>     On 12/17/2014 08:15 PM, Oleksandr Otenko wrote:
>>>>>>
>>>>>>         No, there is no difference. Peter didn't spot your entire
>>>>>>         method is synchronized, so spurious wakeup won't make
>>>>>>         progress until the owner of the lock exits the method.
>>>>>>
>>>>>>         You could split the synchronization into two blocks - one
>>>>>>         encompassing the wait loop, the other in the finally
>>>>>>         block; but it may make no difference.
>>>>>>
>>>>>>         Alex
>>>>>>
>>>>>>
>>>>>>     You're right, Alex. I'm so infected with park/unpark virus
>>>>>>     that I missed that ;-)
>>>>>>
>>>>>>     Peter
>>>>>>
>>>>>>
>>>>>>         On 17/12/2014 18:36, suman shil wrote:
>>>>>>
>>>>>>             Thanks peter for your reply. You are right. I should
>>>>>>             have incremented currentAllowedOrder in finally block.
>>>>>>
>>>>>>             Suman
>>>>>>             ------------------------------------------------------------------------
>>>>>>             *From:* Peter Levart <peter.levart at gmail.com
>>>>>>             <mailto:peter.levart at gmail.com>>
>>>>>>             *To:* suman shil <suman_krec at yahoo.com
>>>>>>             <mailto:suman_krec at yahoo.com>>; Oleksandr Otenko
>>>>>>             <oleksandr.otenko at oracle.com
>>>>>>             <mailto:oleksandr.otenko at oracle.com>>;
>>>>>>             Concurrency-interest
>>>>>>             <concurrency-interest at cs.oswego.edu
>>>>>>             <mailto:concurrency-interest at cs.oswego.edu>>
>>>>>>             *Sent:* Wednesday, December 17, 2014 11:54 PM
>>>>>>             *Subject:* Re: [concurrency-interest] Enforcing
>>>>>>             ordered execution of critical sections?
>>>>>>
>>>>>>             On 12/17/2014 06:46 PM, suman shil wrote:
>>>>>>
>>>>>>                 Thanks for your response. Will notifyAll()
>>>>>>                 instead of notify() solve the problem?
>>>>>>
>>>>>>
>>>>>>             It will, but you should also account for "spurious"
>>>>>>             wake-ups. You should increment currentAllowedOrder
>>>>>>             only after return from callable.call (in finally
>>>>>>             block just before notifyAll()).
>>>>>>
>>>>>>             Otherwise a nice solution - with minimal state,
>>>>>>             providing that not many threads meet at the same time...
>>>>>>
>>>>>>             Regards, Peter
>>>>>>
>>>>>>                 RegardsSuman
>>>>>>                        From: Oleksandr
>>>>>>                 Otenko<oleksandr.otenko at oracle.com
>>>>>>                 <mailto:oleksandr.otenko at oracle.com>>
>>>>>>                 <mailto:oleksandr.otenko at oracle.com
>>>>>>                 <mailto:oleksandr.otenko at oracle.com>>
>>>>>>                   To: suman shil<suman_krec at yahoo.com
>>>>>>                 <mailto:suman_krec at yahoo.com>>
>>>>>>                 <mailto:suman_krec at yahoo.com
>>>>>>                 <mailto:suman_krec at yahoo.com>>;
>>>>>>                 Concurrency-interest<concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>
>>>>>>                 <mailto:concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>  
>>>>>>                 Sent: Wednesday, December 17, 2014 9:55 PM
>>>>>>                   Subject: Re: [concurrency-interest] Enforcing
>>>>>>                 ordered execution of critical sections?
>>>>>>                       There is no guarantee you'll ever hand over
>>>>>>                 the control to the right thread upon notify()
>>>>>>                     Alex
>>>>>>
>>>>>>                 On 17/12/2014 14:07, suman shil wrote:
>>>>>>                       Hi, Following is my solution to solve this
>>>>>>                 problem. Please let me know if I am missing
>>>>>>                 something.
>>>>>>                    public class OrderedExecutor { private int
>>>>>>                 currentAllowedOrder = 0; private int maxLength =
>>>>>>                 0;  public OrderedExecutor(int n)  {
>>>>>>                 this.maxLength = n;  } public synchronized Object
>>>>>>                 execCriticalSectionInOrder( int order,
>>>>>>                 Callable<Object> callable)                      
>>>>>>                    throws Exception  { if (order >= maxLength) { 
>>>>>>                 throw new Exception("Exceeds maximum order "+
>>>>>>                 maxLength);  }    while(order !=
>>>>>>                 currentAllowedOrder)  {  wait();  }   try  {
>>>>>>                 currentAllowedOrder = currentAllowedOrder+1; 
>>>>>>                 return callable.call();  }  finally  { notify(); 
>>>>>>                 }  } }
>>>>>>                    Regards Suman
>>>>>>                        From: Peter Levart<peter.levart at gmail.com
>>>>>>                 <mailto:peter.levart at gmail.com>>
>>>>>>                 <mailto:peter.levart at gmail.com
>>>>>>                 <mailto:peter.levart at gmail.com>>
>>>>>>                   To: Hanson Char<hanson.char at gmail.com
>>>>>>                 <mailto:hanson.char at gmail.com>>
>>>>>>                 <mailto:hanson.char at gmail.com
>>>>>>                 <mailto:hanson.char at gmail.com>>   Cc:
>>>>>>                 concurrency-interest<concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>
>>>>>>                 <mailto:concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:concurrency-interest at cs.oswego.edu>>  
>>>>>>                 Sent: Sunday, December 14, 2014 11:01 PM
>>>>>>                   Subject: Re: [concurrency-interest] Enforcing
>>>>>>                 ordered execution of critical sections?
>>>>>>                           On 12/14/2014 06:11 PM, Hanson Char wrote:
>>>>>>                      Hi Peter,
>>>>>>                    Thanks for this proposed idea of using
>>>>>>                 LockSupport. This begs the question: which one
>>>>>>                 would you choose if you had all three (correct)
>>>>>>                 implementation available?  (Semaphore,
>>>>>>                 CountDownLatch, or LockSupport)?
>>>>>>                    Regards, Hanson
>>>>>>                     The Semaphore/CountDownLatch variants are
>>>>>>                 equivalent if you don't need re-use. So any would
>>>>>>                 do. They lack invalid-use detection. What happens
>>>>>>                 if they are not used as intended? Semaphore
>>>>>>                 variant acts differently than CountDownLatch
>>>>>>                 variant. The low-level variant I  proposed
>>>>>>                 detects invalid usage. So I would probably use
>>>>>>                 this one. But the low level variant is harder to
>>>>>>                 reason about it's correctness. I think it is
>>>>>>                 correct, but you should show it to somebody else
>>>>>>                 to confirm this.
>>>>>>                     Another question is whether you actually need
>>>>>>                 this kind of synchronizer. Maybe if you explained
>>>>>>                 what you are trying to achieve, somebody could
>>>>>>                 have an idea how to do that even more elegantly...
>>>>>>                     Regards, Peter
>>>>>>                              On Sun, Dec 14, 2014 at 9:01 AM,
>>>>>>                 Peter Levart<peter.levart at gmail.com
>>>>>>                 <mailto:peter.levart at gmail.com>>
>>>>>>                 <mailto:peter.levart at gmail.com
>>>>>>                 <mailto:peter.levart at gmail.com>> wrote:
>>>>>>                    Hi Hanson,
>>>>>>                     This one is more low-level, but catches some
>>>>>>                 invalid usages and is more resource-friendly:
>>>>>>                       public class OrderedExecutor {
>>>>>>                         public <T> T execCriticalSectionInOrder(
>>>>>>                           final int order,
>>>>>>                           final Supplier<T> criticalSection
>>>>>>                       ) throws InterruptedException {
>>>>>>                           if (order < 0) {
>>>>>>                                throw new
>>>>>>                 IllegalArgumentException("'order' should be >= 0");
>>>>>>                           }
>>>>>>                           if (order > 0) {
>>>>>>                               waitForDone(order - 1);
>>>>>>                           }
>>>>>>                           try {
>>>>>>                               return criticalSection.get();
>>>>>>                           } finally {
>>>>>>                               notifyDone(order);
>>>>>>                           }
>>>>>>                       }
>>>>>>                         private static final Object DONE = new
>>>>>>                 Object();
>>>>>>                       private final ConcurrentMap<Integer,
>>>>>>                 Object> signals = new ConcurrentHashMap<>();
>>>>>>                         private void waitForDone(int order)
>>>>>>                 throws InterruptedException {
>>>>>>                           Object sig = signals.putIfAbsent(order,
>>>>>>                 Thread.currentThread());
>>>>>>                           if (sig != null && sig != DONE) {
>>>>>>                               throw new IllegalStateException();
>>>>>>                           }
>>>>>>                           while (sig != DONE) {
>>>>>>                               LockSupport.park();
>>>>>>                               if (Thread.interrupted()) {
>>>>>>                                   throw new InterruptedException();
>>>>>>                               }
>>>>>>                               sig = signals.get(order);
>>>>>>                           }
>>>>>>                       }
>>>>>>                         private void notifyDone(int order) {
>>>>>>                           Object sig = signals.putIfAbsent(order,
>>>>>>                 DONE);
>>>>>>                           if (sig instanceof Thread) {
>>>>>>                               if (!signals.replace(order, sig,
>>>>>>                 DONE)) {
>>>>>>                                   throw new IllegalStateException();
>>>>>>                               }
>>>>>>                 LockSupport.unpark((Thread) sig);
>>>>>>                           } else if (sig != null) {
>>>>>>                               throw new IllegalStateException();
>>>>>>                           }
>>>>>>                       }
>>>>>>                   }
>>>>>>                       Regards, Peter
>>>>>>                     On 12/14/2014 05:08 PM, Peter Levart wrote:
>>>>>>                        On 12/14/2014 04:20 PM, Hanson Char wrote:
>>>>>>                      Hi Peter,
>>>>>>                    Thanks for the suggestion, and sorry about not
>>>>>>                 being clear about one important  detail: "n" is
>>>>>>                 not known a priori when constructing an
>>>>>>                 OrderedExecutor.  Does this mean the use of
>>>>>>                 CountDownLatch is ruled out?
>>>>>>                     If you know at least the upper bound of 'n',
>>>>>>                 it can be used with such 'n'. Otherwise something
>>>>>>                 that dynamically re-sizes the array could be
>>>>>>                 devised. Or you could simply use a
>>>>>>                 ConcurrentHashMap instead of array where keys are
>>>>>>                 'order' values:
>>>>>>                       public class OrderedExecutor<T> {
>>>>>>                         private final ConcurrentMap<Integer,
>>>>>>                 CountDownLatch> latches = new ConcurrentHashMap<>();
>>>>>>                         public T execCriticalSectionInOrder(final
>>>>>>                 int order,
>>>>>>                     final Supplier<T> criticalSection) throws
>>>>>>                 InterruptedException {
>>>>>>                           if (order > 0) {
>>>>>>                 latches.computeIfAbsent(order - 1, o -> new
>>>>>>                 CountDownLatch(1)).await();
>>>>>>                           }
>>>>>>                           try {
>>>>>>                               return criticalSection.get();
>>>>>>                           } finally {
>>>>>>                 latches.computeIfAbsent(order, o -> new
>>>>>>                 CountDownLatch(1)).countDown();
>>>>>>                           }
>>>>>>                       }
>>>>>>                   }
>>>>>>                       Regards, Peter
>>>>>>                           You guessed right: it's a one-shot
>>>>>>                 object for a particular OrderedExecutor 
>>>>>>                 instance, and "order" must be called indeed at
>>>>>>                 most once.
>>>>>>                    Regards, Hanson
>>>>>>                   On Sun, Dec 14, 2014 at 2:21 AM, Peter
>>>>>>                 Levart<peter.levart at gmail.com
>>>>>>                 <mailto:peter.levart at gmail.com>>
>>>>>>                 <mailto:peter.levart at gmail.com
>>>>>>                 <mailto:peter.levart at gmail.com>> wrote:
>>>>>>                    Hi Hanson,
>>>>>>                     I don't think anything like that readily
>>>>>>                 exists  in java.lang.concurrent, but what you
>>>>>>                 describe should be possible to  achieve with
>>>>>>                 composition of existing primitives.  You haven't
>>>>>>                 given any additional hints to what your
>>>>>>                 OrderedExecutor  should behave like. Should it be
>>>>>>                 a one-shot object (like CountDownLatch) or a
>>>>>>                 re-usable one (like CyclicBarrier)? Will
>>>>>>                 execCriticalSectionInOrder() for a particular
>>>>>>                 OrderedExecutor instance and 'order' value be
>>>>>>                 called at most once? If yes (and I think that
>>>>>>                 only a one-shot object  makes sense here), an
>>>>>>                 array of CountDownLatch(es) could be used:
>>>>>>                     public class OrderedExecutor<T> {
>>>>>>                       private final CountDownLatch[] latches;
>>>>>>                         public OrderedExecutor(int n) {
>>>>>>                           if (n < 1) throw new
>>>>>>                 IllegalArgumentException("'n'  should be >= 1");
>>>>>>                           latches = new CountDownLatch[n - 1];
>>>>>>                           for (int i = 0; i < latches.length; i++) {
>>>>>>                               latches[i] = new CountDownLatch(1);
>>>>>>                           }
>>>>>>                       }
>>>>>>                         public T execCriticalSectionInOrder(final
>>>>>>                 int order,
>>>>>>                    final Supplier<T> criticalSection) throws
>>>>>>                 InterruptedException {
>>>>>>                           if (order < 0 || order > latches.length)
>>>>>>                               throw new
>>>>>>                 IllegalArgumentException("'order' should be
>>>>>>                 [0..." +  latches.length + "]");
>>>>>>                           if (order > 0) {
>>>>>>                               latches[order - 1].await();
>>>>>>                           }
>>>>>>                           try {
>>>>>>                               return criticalSection.get();
>>>>>>                           } finally {
>>>>>>                               if (order < latches.length) {
>>>>>>                 latches[order].countDown();
>>>>>>                               }
>>>>>>                           }
>>>>>>                       }
>>>>>>                   }
>>>>>>                       Regards, Peter
>>>>>>                     On 12/14/2014 05:26 AM, Hanson Char wrote:
>>>>>>                         Hi, I am looking for a construct that
>>>>>>                 can  be used to efficiently enforce  ordered
>>>>>>                 execution of multiple critical sections, each
>>>>>>                 calling from a different thread. The calling
>>>>>>                 threads may run in  parallel and may call the
>>>>>>                 execution method out of order. The perceived
>>>>>>                 construct would therefore be responsible for
>>>>>>                 re-ordering the execution of those threads, so
>>>>>>                 that their critical sections (and only the
>>>>>>                 critical section) will be executed in order.
>>>>>>                 Would something  like the following API already
>>>>>>                 exist? /** * Used to enforce ordered execution of
>>>>>>                 critical sections calling from multiple * 
>>>>>>                 threads, parking and unparking the  threads as
>>>>>>                 necessary. */ public class OrderedExecutor<T> {
>>>>>>                 /** * Executes a critical section at most once
>>>>>>                 with the given order, parking * and unparking the
>>>>>>                 current thread as  necessary so that all critical
>>>>>>                 * sections executed by different threads using
>>>>>>                 this  executor take place in * the order from 1
>>>>>>                 to n consecutively. */ public T
>>>>>>                 execCriticalSectionInOrder
>>>>>>                 (  final int order, final Callable<T>
>>>>>>                 criticalSection) throws InterruptedException; }
>>>>>>                 Regards, Hanson
>>>>>>                 _______________________________________________Concurrency-interest
>>>>>>                 mailing listConcurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:listConcurrency-interest at cs.oswego.edu>
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>                 _______________________________________________
>>>>>>                   Concurrency-interest mailing list
>>>>>>                 Concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>                 _______________________________________________
>>>>>>                 Concurrency-interest mailing list
>>>>>>                 Concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>                 _______________________________________________
>>>>>>                 Concurrency-interest mailing list
>>>>>>                 Concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu
>>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>>
>>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>     _______________________________________________
>>>>>>     Concurrency-interest mailing list
>>>>>>     Concurrency-interest at cs.oswego.edu
>>>>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu 
>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/aaf3ba96/attachment-0001.html>

From jsampson at guidewire.com  Tue Dec 30 14:07:21 2014
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 30 Dec 2014 19:07:21 +0000
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcing
 ordered execution of critical sections?]
In-Reply-To: <54A20284.3030007@oracle.com>
References: <5498BBEF.8090708@cs.oswego.edu>
	<NFBBKALFDCPFIDBNKAPCIEPPKLAA.davidcholmes@aapt.net.au>
	<CAHzJPEqHjAKP9OtgecJiPFnco6JQYNaVawnTmH5=OhjfjoEXzw@mail.gmail.com>
	<549955EE.7050104@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D7CA83@sm-ex-01-vm.guidewire.com>
	<549AAEB5.4000500@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D7DC09@sm-ex-01-vm.guidewire.com>
	<54A20284.3030007@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D7DE6B@sm-ex-01-vm.guidewire.com>

Oleksandr Otenko wrote:

> This is a good way to put it. Still, a few more points need
> clarifying.

Thanks!

> eg
>
> if M+N permits are released in bulk, will two threads (one
> waiting for M, another for N) get woken up or not. Certainly, if
> they arrive in the nick of time (acquire without going to sleep),
> both will succeed. But what if both are asleep? If so, then the
> wording would need to state something like "the longest queue of
> threads the sum of whose permits is <=P"

Yeah, I was still leaving that somewhat implicit in my rules. If
only the acquire(M) is awoken in that case, the result is a state in
which N permits are available and the acquire(N) has now been
waiting the longest, such that my rule 3a says that it must succeed
as well.

Of course, this whole thread is about making implicit rules more
explicit in the docs!

Maybe instead of, "When fair == xxx and there are P > 0 available
permits," I could have written "*Whenever* there are P > 0 available
permits and fair == xxx," to be more explicit?

> Or contracted to: 
>
> fair semaphore: at any given time the longest waiting thread
> requires N>P permits, and all other threads wait until the longest
> waiting thread is allowed to proceed. This already means that if
> for the longest waiting thread N <= P, it can't be waiting.
>
> unfair semaphore: at any given time the threads requiring N>P
> permits are waiting, and some other threads may be waiting for
> them to proceed first. Rather vague about who can proceed, but
> that's unfairness for you: can you be sure a thread makes
> progress? no, unless you are sure all threads are N<=P.
>
> semaphore not intended: at any give time only threads requiring
> N>P permits are waiting [and threads requiring N<=P can't be
> waiting]. This is what some think semaphore means, but doesn't
> implement.

Nice summaries! Yes, that's all consistent with my understanding.

Cheers,
Justin

P.S. Most of concurrency-interest goes over my head, but I got
excited about this discussion because I figure at _least_ I should
be able to understand the semantics of a *semaphore*! :)


From ben_manes at yahoo.com  Tue Dec 30 19:12:44 2014
From: ben_manes at yahoo.com (Ben Manes)
Date: Wed, 31 Dec 2014 00:12:44 +0000 (UTC)
Subject: [concurrency-interest] synchronized vs
 Unsafe#monitorEnter/monitorExit
In-Reply-To: <54A2BC93.5090406@oracle.com>
References: <54A2BC93.5090406@oracle.com>
Message-ID: <476178422.1738456.1419984764152.JavaMail.yahoo@jws100139.mail.ne1.yahoo.com>

> Why do you need synchronized specifically? (and not j.u.c.locks.Lock lock/unlock)

To minimize memory usage, as the lock is per entry. Consider this as an extension to ConcurrentHashMap's computeIfAbsent(K key), but that instead accepts Iterable<K> keys. A full j.u.c lock is ideal, but that requires each entry embeds a lock instance and that adds significant bloat. In the case of a cache, writes are rare and even rarer to the same entries. Thus, synchronizing should use the Object's header as a think lock until contention is detected and the JVM inflates the lock to a full mutex. After inflation, the JVM could decide to deinflate the lock if it reverts back to low contention usages. That means no wasted memory to support a single use-case, which may not be a very commonly used pattern. If the Unsafe versions were optimizable, then using synchronized would be an ideal choice.
There are alternatives, of course. The approach in Guava was to be non-blocking and populate the cache after the bulk load completes. This overwrites any entries that appeared while the loadAll was computed, so it doesn't solve the thundering herd problem. This is reasonable based on the assumption that local cache misses don't cause the as much pain as, say, a memcached restart causing a database query storm. Another approach might?use lock striping, e.g. 1024 real locks that can be hashed to and are acquired in ascending order. That adds more complexity and hash collisions might occur, but it wouldn't be too bad in practice. It does add extra complexity. And yet another might be a lazy lock field per entry, but that is still wasteful in large caches.
There's probably a solution using some type of reservation entry that contains a lock reference, and after populated transitions to an implementation that doesn't have a lock. That can be done within the hash table, like CHM's ReservationNode, but is harder to do elegantly through a decorator.
Anyway, hopefully its clear now why Unsafe appeared like an attractive solution if it had performed well. 

     On Tuesday, December 30, 2014 6:54 AM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
   

  Why do you need synchronized specifically? (and not j.u.c.locks.Lock lock/unlock)
 
 Alex
 
 On 28/12/2014 20:15, Ben Manes wrote:
  
  That's an nifty workaround but, as you said, not reliable in the case of a general api. This case is a multi-get for a cache library, so the keys and use-cases aren't known. 
  When adding bulk loading to Guava, due to the internal complexity we allowed it to racy by not blocking concurrent calls for shared entries. This has to be done regardless because a loadAll(keys) may return a Map<K, V> with more entries than originally requested, and those extra entries should be cached. These cases were trivial to handle when I had previously written a Map<K, Future<V>> to support multi-get, but a lot of bloat per entry. I had hoped Unsafe#monitorEnter would be a nice alternative and, if the API was stable, it arguably still could be due to the low write rate of caches. For now I'll leave it in a backlog of performance optimization ideas like we did with Guava, maybe  revisiting once the rewrite stabilizes. 
  Thanks, -Ben 
 
       On Sunday, December 28, 2014 5:18 AM, Peter Levart <peter.levart at gmail.com> wrote:
   
 
    
 On 12/27/2014 09:31 PM, Ben Manes wrote:
  
  Can someone explain why using Unsafe's monitor methods are substantially worse than synchronized? I had expected them to emit equivalent monitorEnter/monitorExit instructions and have similar performance. 
  My use case is to support a bulk version of CHM#computeIfAbsent, where a single mapping function returns the result for computing multiple entries. I had hoped to bulk lock, insert the unfilled entries, compute,  populate, and bulk unlock. An overlapping write would be blocked due to requiring an entry's lock for mutation.   
 
 Hi Ben,
 
 If "multiple entries" means less than say 50 or even a little more (using more locks than that for one bulk operation is not very optimal anyway), you could try constructing a recursive method to bulk-lock on the list of objects and execute a  provided function:
 
 
   ??? public static <T> T synchronizedAll(Iterable<?> locks, Supplier<T> supplier) {
 ??????? return synchronizedAll(locks.iterator(), supplier);
 ??? }
 
 ??? private static <T> T synchronizedAll(Iterator<?> locksIterator, Supplier<T> supplier) {
 ??????? if (locksIterator.hasNext()) {
 ??????????? synchronized (locksIterator.next()) {
 ??????????????? return synchronizedAll(locksIterator, supplier);
 ??????????? }
 ??????? } else {
 ??????????? return supplier.get();
 ??????? }
 ??? }
 
 
 
 Note that to avoid deadlocks, the locks list has to be sorted using a Comparator that is consistent with your key's equals() method...
 
 Regards, Peter 
 
 
  
     I had thought that using Unsafe would allow for achieving this without the memory overhead of a ReentrantLock/AQS per entry, since the synchronized keyword is not flexible enough to provide this structure. 
  Thanks, Ben 
  Benchmark ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Mode ?Samples ? ? ? ? Score ? ? ? ? Error ?Units c.g.b.c.SynchronizedBenchmark.monitor_contention ? ? ? ? ? ?thrpt ? ? ? 10 ? 3694951.630 ? ? 34340.707 ?ops/s c.g.b.c.SynchronizedBenchmark.monitor_noContention ? ? ? ? ?thrpt ? ? ? 10 ? 8274097.911 ? ?164356.363 ?ops/s c.g.b.c.SynchronizedBenchmark.reentrantLock_contention ? ? ?thrpt ? ? ? 10 ?31668532.247 ? ?740850.955 ?ops/s c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention ? ?thrpt ? ? ? 10 ?41380163.703 ? 2270103.507 ?ops/s c.g.b.c.SynchronizedBenchmark.synchronized_contention ? ? ? thrpt ? ? ? 10 ?22905995.761 ? ?117868.968 ?ops/s c.g.b.c.SynchronizedBenchmark.synchronized_noContention ? ? thrpt ? ? ? 10 ?44891601.915 ? 1458775.665 ?ops/s  
   
  
 _______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest   
  
    
 
      
  
 _______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 
 
 

   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/250b9b97/attachment-0001.html>

From lukeisandberg at gmail.com  Tue Dec 30 19:22:43 2014
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Tue, 30 Dec 2014 16:22:43 -0800
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <54A2C34C.6000807@oracle.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
	<54A2C34C.6000807@oracle.com>
Message-ID: <CAO9V1MJH6v+WPcuWnhrV5jRPLzm6i2bLuCUKhwZcbZLyAwViyw@mail.gmail.com>

A doubly linked list that exposes the nodes would have that behavior.
On Dec 30, 2014 9:23 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  Do you know a non-concurrent structure that you are looking for?
>
> (eg you don't like O(n) for removal of arbitrary items - but is it
> possible to have *all* operations O(1), even in a non-concurrent case?)
>
>
> Alex
>
>
> On 22/12/2014 02:35, Luke Sandberg wrote:
>
> I have come across a few situations where i am looking for a datastructure
> and i feel like i keep coming up short.
>
> The situation is analogous to the 'waiters' list in
> java.util.concurrent.FutureTask.
>
> Basically, I need to be able to publish a non-null object reference into a
> data structure where
> * order doesn't matter (insertion order would be nice, but i don't care
> that much)
> * add and remove identity semantics
> * concurrent iteration (weakly consistent is fine)
> * add/remove are non-blocking
>
> CLQ is an obvious choice but removal is O(n), another choice would a
> simple synchronized identity hash set which is fine but the lock + high
> entry overhead is a deal breaker.
>
> An AtomicReferenceArray would be super easy, but i can't put a bound on
> the number of items.
>
> Also, it would be fine for the code that adds items, to store additional
> state (an index, a 'Node' reference), in order to facilitate removal.
>
> The best thing i have seen from looking around appears to be something
> like what FutureTask does to implement 'awaitDone', but even there
> removeWaitier() is O(n).  that seems like a pretty good compromise when
> lists are short, but what would be a better solution when lists are long?
>
> Just to motivate this a little bit, the two cases I am looking at in
> particular are:
>
> * maintaining a set of threads associated with a server 'request' (threads
> enter/exit when they start executing tasks associated with the request)
> * maintaining a set of futures to be cancelled (futures are removed when
> they complete to avoid pinning their referents).
>
> Any pointers or ideas?
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/7544284b/attachment.html>

From oleksandr.otenko at oracle.com  Tue Dec 30 19:29:11 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 31 Dec 2014 00:29:11 +0000
Subject: [concurrency-interest] synchronized vs
	Unsafe#monitorEnter/monitorExit
In-Reply-To: <476178422.1738456.1419984764152.JavaMail.yahoo@jws100139.mail.ne1.yahoo.com>
References: <54A2BC93.5090406@oracle.com>
	<476178422.1738456.1419984764152.JavaMail.yahoo@jws100139.mail.ne1.yahoo.com>
Message-ID: <54A34357.2080400@oracle.com>

ok. You must consider that using the object header means different JVMs 
will have different overheads and limits. Some JVMs will not let you 
have tens of millions of objects on which you want to synchronize - 
contended or not - just because there is not enough space in their 
object headers for all the book-keeping info.

Alex


On 31/12/2014 00:12, Ben Manes wrote:
> > Why do you need synchronized specifically? (and not j.u.c.locks.Lock 
> lock/unlock)
>
> To minimize memory usage, as the lock is per entry. Consider this as 
> an extension to ConcurrentHashMap's computeIfAbsent(K key), but that 
> instead accepts Iterable<K> keys. A full j.u.c lock is ideal, but that 
> requires each entry embeds a lock instance and that adds significant 
> bloat. In the case of a cache, writes are rare and even rarer to the 
> same entries. Thus, synchronizing should use the Object's header as a 
> think lock until contention is detected and the JVM inflates the lock 
> to a full mutex. After inflation, the JVM could decide to deinflate 
> the lock if it reverts back to low contention usages. That means no 
> wasted memory to support a single use-case, which may not be a very 
> commonly used pattern. If the Unsafe versions were optimizable, then 
> using synchronized would be an ideal choice.
>
> There are alternatives, of course. The approach in Guava was to be 
> non-blocking and populate the cache after the bulk load completes. 
> This overwrites any entries that appeared while the loadAll was 
> computed, so it doesn't solve the thundering herd problem. This is 
> reasonable based on the assumption that local cache misses don't cause 
> the as much pain as, say, a memcached restart causing a database query 
> storm. Another approach might use lock striping, e.g. 1024 real locks 
> that can be hashed to and are acquired in ascending order. That adds 
> more complexity and hash collisions might occur, but it wouldn't be 
> too bad in practice. It does add extra complexity. And yet another 
> might be a lazy lock field per entry, but that is still wasteful in 
> large caches.
>
> There's probably a solution using some type of reservation entry that 
> contains a lock reference, and after populated transitions to an 
> implementation that doesn't have a lock. That can be done within the 
> hash table, like CHM's ReservationNode, but is harder to do elegantly 
> through a decorator.
>
> Anyway, hopefully its clear now why Unsafe appeared like an attractive 
> solution if it had performed well.
>
>
> On Tuesday, December 30, 2014 6:54 AM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com> wrote:
>
>
> Why do you need synchronized specifically? (and not j.u.c.locks.Lock 
> lock/unlock)
>
> Alex
>
> On 28/12/2014 20:15, Ben Manes wrote:
>> That's an nifty workaround but, as you said, not reliable in the case 
>> of a general api. This case is a multi-get for a cache library, so 
>> the keys and use-cases aren't known.
>>
>> When adding bulk loading to Guava, due to the internal complexity we 
>> allowed it to racy by not blocking concurrent calls for shared 
>> entries. This has to be done regardless because a loadAll(keys) may 
>> return a Map<K, V> with more entries than originally requested, and 
>> those extra entries should be cached. These cases were trivial to 
>> handle when I had previously written a Map<K, Future<V>> to support 
>> multi-get, but a lot of bloat per entry. I had hoped 
>> Unsafe#monitorEnter would be a nice alternative and, if the API was 
>> stable, it arguably still could be due to the low write rate of 
>> caches. For now I'll leave it in a backlog of performance 
>> optimization ideas like we did with Guava, maybe revisiting once the 
>> rewrite stabilizes.
>>
>> Thanks,
>> -Ben
>>
>>
>> On Sunday, December 28, 2014 5:18 AM, Peter Levart 
>> <peter.levart at gmail.com> <mailto:peter.levart at gmail.com> wrote:
>>
>>
>>
>> On 12/27/2014 09:31 PM, Ben Manes wrote:
>>> Can someone explain why using Unsafe's monitor methods are 
>>> substantially worse than synchronized? I had expected them to emit 
>>> equivalent monitorEnter/monitorExit instructions and have similar 
>>> performance.
>>>
>>> My use case is to support a bulk version of CHM#computeIfAbsent, 
>>> where a single mapping function returns the result for computing 
>>> multiple entries. I had hoped to bulk lock, insert the unfilled 
>>> entries, compute, populate, and bulk unlock. An overlapping write 
>>> would be blocked due to requiring an entry's lock for mutation.
>>
>> Hi Ben,
>>
>> If "multiple entries" means less than say 50 or even a little more 
>> (using more locks than that for one bulk operation is not very 
>> optimal anyway), you could try constructing a recursive method to 
>> bulk-lock on the list of objects and execute a provided function:
>>
>>
>>     public static <T> T synchronizedAll(Iterable<?> locks, 
>> Supplier<T> supplier) {
>>         return synchronizedAll(locks.iterator(), supplier);
>>     }
>>
>>     private static <T> T synchronizedAll(Iterator<?> locksIterator, 
>> Supplier<T> supplier) {
>>         if (locksIterator.hasNext()) {
>>             synchronized (locksIterator.next()) {
>>                 return synchronizedAll(locksIterator, supplier);
>>             }
>>         } else {
>>             return supplier.get();
>>         }
>>     }
>>
>>
>>
>> Note that to avoid deadlocks, the locks list has to be sorted using a 
>> Comparator that is consistent with your key's equals() method...
>>
>> Regards, Peter
>>
>>
>>
>>> I had thought that using Unsafe would allow for achieving this 
>>> without the memory overhead of a ReentrantLock/AQS per entry, since 
>>> the synchronized keyword is not flexible enough to provide this 
>>> structure.
>>>
>>> Thanks,
>>> Ben
>>>
>>> Benchmark                  Mode  Samples         Score     Error  Units
>>> c.g.b.c.SynchronizedBenchmark.monitor_contention            thrpt   
>>>     10   3694951.630 ? 34340.707  ops/s
>>> c.g.b.c.SynchronizedBenchmark.monitor_noContention          thrpt   
>>>     10 8274097.911 ?  164356.363  ops/s
>>> c.g.b.c.SynchronizedBenchmark.reentrantLock_contention      thrpt   
>>>     10  31668532.247 ?  740850.955  ops/s
>>> c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention    thrpt   
>>>     10  41380163.703 ? 2270103.507  ops/s
>>> c.g.b.c.SynchronizedBenchmark.synchronized_contention       thrpt   
>>>     10  22905995.761 ?  117868.968  ops/s
>>> c.g.b.c.SynchronizedBenchmark.synchronized_noContention     thrpt   
>>>     10  44891601.915 ? 1458775.665  ops/s
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/aaa6601a/attachment-0001.html>

From dt at flyingtroika.com  Tue Dec 30 19:39:35 2014
From: dt at flyingtroika.com (DT)
Date: Tue, 30 Dec 2014 16:39:35 -0800
Subject: [concurrency-interest] AQS.compareAndSetHead CAS abbreviation
Message-ID: <E6D8C445-76D2-4D6B-B606-DC901A2DFABE@flyingtroika.com>

In the AQS class method compareAndSetHead has java doc : /* CAS head field ... */ does it mean compare and set or compare and swap?
It calls unsafe.compareAndSwapObject

Thanks




From ben_manes at yahoo.com  Tue Dec 30 20:04:53 2014
From: ben_manes at yahoo.com (Ben Manes)
Date: Wed, 31 Dec 2014 01:04:53 +0000 (UTC)
Subject: [concurrency-interest] synchronized vs
 Unsafe#monitorEnter/monitorExit
In-Reply-To: <54A34357.2080400@oracle.com>
References: <54A34357.2080400@oracle.com>
Message-ID: <273674174.1740396.1419987893418.JavaMail.yahoo@jws10082.mail.ne1.yahoo.com>

Sure, but here we're talking about a range more commonly in the 10s to low 1,000s. The larger the batch size means the more expensive the batch query is, so concurrent lookups might take an unacceptable delay waiting for a large batch to complete. At that point its a game of tuning the application for excessively large batch scenarios. 

     On Tuesday, December 30, 2014 4:29 PM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
   

  ok. You must consider that using the object header means different JVMs will have different overheads and limits. Some JVMs will not let you have tens of millions of objects on which you want to synchronize - contended or not - just because there is not enough space in their object headers for all the book-keeping info.
 
 Alex
 
 
 On 31/12/2014 00:12, Ben Manes wrote:
  
  > Why do you need synchronized specifically? (and not j.u.c.locks.Lock lock/unlock)
  
  To minimize memory usage, as the lock is per entry. Consider this as an extension to ConcurrentHashMap's computeIfAbsent(K key), but that instead accepts Iterable<K> keys. A full j.u.c lock is ideal, but that requires each entry embeds a lock instance and that adds significant bloat. In the case of a cache, writes are rare and even rarer to the same entries. Thus, synchronizing should use the Object's header as a think lock until contention is detected and the JVM inflates the lock to a full mutex. After inflation, the JVM could decide to deinflate the lock if it reverts back to low contention usages. That means no wasted memory to support a single use-case, which may not be a very commonly used pattern. If the Unsafe versions were optimizable, then using synchronized would be an ideal choice. 
  There are alternatives, of course. The approach in Guava was to be non-blocking and populate the cache after the bulk load completes. This overwrites any entries that appeared while the loadAll was computed, so it doesn't solve the thundering herd problem. This is reasonable based on the assumption that local cache misses don't cause the as much pain as, say, a memcached restart causing a database query storm. Another approach might?use lock striping, e.g. 1024 real locks that can be hashed to and are acquired in ascending order. That adds more complexity and hash collisions might  occur, but it wouldn't be too bad in practice. It does add extra complexity. And yet another might be a lazy lock field per entry, but that is still wasteful in large caches. 
  There's probably a solution using some type of reservation entry that contains a lock reference, and after populated transitions to an implementation that doesn't have a lock. That can be done within the hash table, like CHM's ReservationNode, but is harder to do elegantly through a decorator. 
  Anyway, hopefully its clear now why Unsafe appeared like an attractive solution if it had performed well. 
 
       On Tuesday, December 30, 2014 6:54 AM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
   
 
    Why do you need synchronized specifically? (and not j.u.c.locks.Lock lock/unlock)
 
 Alex
 
  On 28/12/2014 20:15, Ben Manes wrote:
  
  That's an nifty workaround but, as you said, not reliable in the case of a general api. This case is a multi-get for a cache library, so the keys and use-cases aren't known. 
  When adding bulk loading to Guava, due to the internal complexity we allowed it to racy by not blocking concurrent calls for shared entries. This has to be done regardless because a loadAll(keys) may return a  Map<K, V> with more entries than originally requested, and those extra entries should be cached. These cases were  trivial to handle when I had previously written a Map<K, Future<V>> to support multi-get, but a lot of bloat per entry. I had hoped Unsafe#monitorEnter would be a nice alternative and, if the API was stable, it arguably still could be due to the low write rate of caches. For now I'll  leave it in a backlog of performance optimization ideas like we did with Guava, maybe revisiting once the rewrite stabilizes. 
  Thanks, -Ben 
 
       On Sunday, December 28, 2014 5:18 AM, Peter Levart <peter.levart at gmail.com> wrote:
   
 
    
 On 12/27/2014 09:31 PM, Ben Manes wrote:
  
  Can someone explain why using Unsafe's monitor methods are  substantially worse than synchronized? I had expected them to emit equivalent  monitorEnter/monitorExit instructions and have similar performance. 
  My use case is to support a bulk version of CHM#computeIfAbsent, where a single mapping function returns the result for  computing multiple entries. I had hoped to bulk lock, insert the unfilled entries, compute, populate, and bulk unlock. An overlapping write  would be blocked due to requiring an entry's lock for mutation.   
 
 Hi Ben,
 
 If "multiple entries" means less than say 50 or even a little more  (using more locks than that for one bulk operation is not very optimal anyway), you could try  constructing a recursive method to bulk-lock on the list of objects and execute a provided function:
 
 
   ??? public static <T> T synchronizedAll(Iterable<?> locks, Supplier<T> supplier) {
 ??????? return synchronizedAll(locks.iterator(), supplier);
 ??? }
 
 ??? private static <T> T synchronizedAll(Iterator<?> locksIterator, Supplier<T> supplier) {
 ??????? if (locksIterator.hasNext()) {
 ??????????? synchronized (locksIterator.next()) {
 ??????????????? return synchronizedAll(locksIterator, supplier);
 ??????????? }
 ??????? } else {
 ??????????? return supplier.get();
 ??????? }
 ??? }
 
 
 
 Note that to avoid deadlocks, the locks list has to be sorted using  a Comparator that is consistent with your key's equals() method...
 
 Regards, Peter 
 
 
  
     I had thought that using Unsafe would allow for  achieving this without the memory overhead of a ReentrantLock/AQS per  entry, since the synchronized keyword is not flexible enough to provide this structure. 
  Thanks, Ben 
  Benchmark ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Mode ?Samples ? ? ? ? Score ? ? ? ? Error ?Units c.g.b.c.SynchronizedBenchmark.monitor_contention ? ? ? ? ? ?thrpt ? ? ? 10 ? 3694951.630 ? ? 34340.707 ?ops/s c.g.b.c.SynchronizedBenchmark.monitor_noContention ? ? ? ? ?thrpt ? ? ? 10 ? 8274097.911 ? ?164356.363 ?ops/s c.g.b.c.SynchronizedBenchmark.reentrantLock_contention ? ? ?thrpt ? ? ? 10 ?31668532.247 ? ?740850.955 ?ops/s c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention ? ?thrpt ? ? ? 10 ?41380163.703 ? 2270103.507 ?ops/s c.g.b.c.SynchronizedBenchmark.synchronized_contention ? ? ? thrpt ? ? ? 10 ?22905995.761 ? ?117868.968 ?ops/s c.g.b.c.SynchronizedBenchmark.synchronized_noContention ? ? thrpt ? ? ? 10 ?44891601.915 ? 1458775.665 ?ops/s  
   
  
 _______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest     
  
    
 
      
  
 _______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
 
  
   
 
      
 
 

   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/642ed875/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Dec 30 20:23:18 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 31 Dec 2014 11:23:18 +1000
Subject: [concurrency-interest] Semaphore doc bug [Was: Enforcingordered
	execution of critical sections?]
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEAKKMAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCKECFKMAA.davidcholmes@aapt.net.au>

On December 24 I wrote:
> Hi Doug,
>
> I still need to go back and check historical notes but this all
> seems wrong to me.

So ... originally acquire(n) was only defined for the explicit FifoSemapore
class, as I explained below back in early 2003:

> David Holmes wrote on Thursday, 6 February 2003 8:27 AM
> To: Asif Qamar; concurrency-interest at altair.cs.oswego.edu
> Subject: RE: [concurrency-interest] Should there also be an acquire
> (long n) method in Semaphore class ?
>
>
> Hello Asif,
>
> > My question is exploratory: why is there no corresponding
> > acquire (int n) method?
>
> There is no acquire(n) method in semaphore because it requires some
> sense of ordering that Semaphore does not provide. You need to decide
> whether a thread waiting for n permits prevents other threads from
> taking any available permits: if the answer is yes then you've
> effectively imposed a Fifo ordering; if no, then it probably doesn't
> do what you want.
>
> There is acquire(n) on FifoSemaphore as the semantics are clear-cut:
> acquire the next n available permits.

However, later that year Doug decided to redesign things:

> 3. Fair subclass
>
> We defined FairSemaphore because acquire(n) is not necessarily
> well-behaved unless you can guarantee ordering. However, in my
> implementation, it IS arguably well-behaved even when fairness is not
> switched on. Here (as with RL), I use "FIFO plus barging": At any
> given time only the oldest thread and (if fairness set false) any
> newly arriving thread are allowed to contend (all others block).  When
> you run problematic acquire(n) cases through this, they don't
> seem particularly bad to me. Still, when people use acquire(n)
> all the time, the almost surely want fair version.

Looking back now I'm not sure we were focussed on the right issue - it is
"arguably well-behaved" the problem is that you can't deduce what the
behaviour might be from the specification, which to me makes acquire(n)
unusable in the non-fair case (unless relying on knowledge of the
implementation).

The class doc we have now:

"This class also provides convenience methods to acquire and release
multiple permits at a time. These methods are generally more efficient and
effective than loops. However, they do not establish any preference order.
For example, if thread A invokes s.acquire(3) and thread B invokes
s.acquire(2), and two permits become available, then there is no guarantee
that thread B will obtain them unless its acquire came first and Semaphore s
is in fair mode. "

is not completely terrible, but does somewhat obscure the fact that
acquire(n) is not really usable in non-fair mode. Perhaps that should be
stated explicitly. Also for the fair case these are not simply convenience
methods - they provide semantics that are impossible to obtain using
multiple acquire() operations!

However the new addition to the method doc:

"This method has the same effect as the loop for (int i = 0; i < permits;
++i) acquire(); except that it atomically acquires the permits all at once:
" (note ending : typo)

is not good! This is a completely meaningless statement in general because
you can not infer what "atomically" means given that the acquire() in the
loop has blocking semantics. I think it would be more meaningful to say
"except that it prevents acquire()'s from other threads from being
interleaved with those in the loop" - but even that is inaccurate in the
face of barging tryAcquires.

David
-----


>
> > On 12/23/2014 06:45 AM, Doug Lea wrote:
> > > On 12/23/2014 01:19 AM, Joe Bowbeer wrote:
> > > Hopefully the doc improvements will prevent this from happening again.
> >
> > I committed these to jsr166 CVS, plus the additional clarification in
> > acquire(int):
> >
> >       * <p>Acquires the given number of permits, if they are available,
> >       * and returns immediately, reducing the number of
> available permits
> >       * by the given amount. This method has the same effect as the
> >       * loop {@code for (int i = 0; i < permits; ++i) acquire();} except
> >       * that it atomically acquires the permits all at once:
>
> To me the "except" part nullifies the initial statement. What does it mean
> to acquire the permits atomically rather than in a loop? The loop
> can cause
> permits to be "reserved" while an atomic acquire of all the
> permits at once
> would not.
>
> > See
> > http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
> Semaphore.html
>
> Again the example seems wrong to me. If the Semaphore is unfair then all
> permits are available to any acquirer. So as soon as two permits are
> available the acquire(2) can proceed. In contrast a fair Semaphore ensures
> only the head waiter can acquire permits.
>
> David
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From martinrb at google.com  Tue Dec 30 20:44:31 2014
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 30 Dec 2014 17:44:31 -0800
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1MJH6v+WPcuWnhrV5jRPLzm6i2bLuCUKhwZcbZLyAwViyw@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
	<54A2C34C.6000807@oracle.com>
	<CAO9V1MJH6v+WPcuWnhrV5jRPLzm6i2bLuCUKhwZcbZLyAwViyw@mail.gmail.com>
Message-ID: <CA+kOe0_MTyv+Ek8t7RztVF5LtvOfGU+tZzwe_gmrepVyMrvzaQ@mail.gmail.com>

On Tue, Dec 30, 2014 at 4:22 PM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> A doubly linked list that exposes the nodes would have that behavior.
>
And ConcurrentLinkedDeque is essentially a lock-free doubly linked list
that supports all operations except interior insertion, which we don't know
how to (and don't need to) implement.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141230/ca2866f2/attachment.html>

From gil at azulsystems.com  Tue Dec 30 20:44:57 2014
From: gil at azulsystems.com (Gil Tene)
Date: Wed, 31 Dec 2014 01:44:57 +0000
Subject: [concurrency-interest] synchronized
	vs	Unsafe#monitorEnter/monitorExit
In-Reply-To: <54A34357.2080400@oracle.com>
References: <54A2BC93.5090406@oracle.com>
	<476178422.1738456.1419984764152.JavaMail.yahoo@jws100139.mail.ne1.yahoo.com>
	<54A34357.2080400@oracle.com>
Message-ID: <0161A568-9AC7-4F58-84E2-5AC31428084F@azulsystems.com>

The number of objects being synchronized on doesn't really have much of an effect. There aren't that many JVMs out there, and AFAIK all the ones that run on servers will handle high counts of synchronized-on objects just fine. There are plenty of commonly used pieces of software that put JVMs through these paces every day. For example, Cassandra (under load) will tend to generate 10s of thousands of new *inflated* monitors per second. And I've seen several other apps that do that too.

JVMs (at least the ones with fixed sized object headers, which is most server JVMs) tend to have room in all object headers for the "un-contended"/deflated monitor state case, both locked and unlocked. The way they implement this varies, e.g. displaced headers in Oracle HotSpot/OpenJDK vs. thin locks or bacon bits in others, but they all fit that stuff in a single header word somehow.

It's "inflation" and "deflation" that are the actual cost, and the choices there vary. E.g. it is possible to contain the number of inflated monitors to the number of threads. Each thread can at most be blocked on one monitor at a time, so the maximum number of actual required monitors is always capped by the number of threads. It is also possible to incorporate the inflated monitor state into the thread structures based on this observation (chaining stuff through threads). But in practice HotSpot variants (including Zing) tend to maintain an inflated monitor pool. When monitors need to be inflated (due to contention or wait() calls) an inflated monitor state is allocated and connected to the object (usually by having the header refer to it somehow). This obviously means that monitors need to be deflated to balance this out. Monitor deflation is usually done as a STW (stop the world) operation as part of safepoint operations (time is usually hidden in GC pauses, and not explicitly reported). Deflation can be done concurrently without STW effects, but AFAIK Zing is the only one doing it concurrently right now. Makes quite a difference when you need to deflate at a sustained rate of 30K monitors/sec and don't want to take pauses in the process...

? Gil.

On Dec 30, 2014, at 4:29 PM, Oleksandr Otenko <oleksandr.otenko at oracle.com<mailto:oleksandr.otenko at oracle.com>> wrote:

ok. You must consider that using the object header means different JVMs will have different overheads and limits. Some JVMs will not let you have tens of millions of objects on which you want to synchronize - contended or not - just because there is not enough space in their object headers for all the book-keeping info.

Alex


On 31/12/2014 00:12, Ben Manes wrote:
> Why do you need synchronized specifically? (and not j.u.c.locks.Lock lock/unlock)

To minimize memory usage, as the lock is per entry. Consider this as an extension to ConcurrentHashMap's computeIfAbsent(K key), but that instead accepts Iterable<K> keys. A full j.u.c lock is ideal, but that requires each entry embeds a lock instance and that adds significant bloat. In the case of a cache, writes are rare and even rarer to the same entries. Thus, synchronizing should use the Object's header as a think lock until contention is detected and the JVM inflates the lock to a full mutex. After inflation, the JVM could decide to deinflate the lock if it reverts back to low contention usages. That means no wasted memory to support a single use-case, which may not be a very commonly used pattern. If the Unsafe versions were optimizable, then using synchronized would be an ideal choice.

There are alternatives, of course. The approach in Guava was to be non-blocking and populate the cache after the bulk load completes. This overwrites any entries that appeared while the loadAll was computed, so it doesn't solve the thundering herd problem. This is reasonable based on the assumption that local cache misses don't cause the as much pain as, say, a memcached restart causing a database query storm. Another approach might use lock striping, e.g. 1024 real locks that can be hashed to and are acquired in ascending order. That adds more complexity and hash collisions might occur, but it wouldn't be too bad in practice. It does add extra complexity. And yet another might be a lazy lock field per entry, but that is still wasteful in large caches.

There's probably a solution using some type of reservation entry that contains a lock reference, and after populated transitions to an implementation that doesn't have a lock. That can be done within the hash table, like CHM's ReservationNode, but is harder to do elegantly through a decorator.

Anyway, hopefully its clear now why Unsafe appeared like an attractive solution if it had performed well.


On Tuesday, December 30, 2014 6:54 AM, Oleksandr Otenko <oleksandr.otenko at oracle.com><mailto:oleksandr.otenko at oracle.com> wrote:


Why do you need synchronized specifically? (and not j.u.c.locks.Lock lock/unlock)

Alex

On 28/12/2014 20:15, Ben Manes wrote:
That's an nifty workaround but, as you said, not reliable in the case of a general api. This case is a multi-get for a cache library, so the keys and use-cases aren't known.

When adding bulk loading to Guava, due to the internal complexity we allowed it to racy by not blocking concurrent calls for shared entries. This has to be done regardless because a loadAll(keys) may return a Map<K, V> with more entries than originally requested, and those extra entries should be cached. These cases were trivial to handle when I had previously written a Map<K, Future<V>> to support multi-get, but a lot of bloat per entry. I had hoped Unsafe#monitorEnter would be a nice alternative and, if the API was stable, it arguably still could be due to the low write rate of caches. For now I'll leave it in a backlog of performance optimization ideas like we did with Guava, maybe revisiting once the rewrite stabilizes.

Thanks,
-Ben


On Sunday, December 28, 2014 5:18 AM, Peter Levart <peter.levart at gmail.com><mailto:peter.levart at gmail.com> wrote:



On 12/27/2014 09:31 PM, Ben Manes wrote:
Can someone explain why using Unsafe's monitor methods are substantially worse than synchronized? I had expected them to emit equivalent monitorEnter/monitorExit instructions and have similar performance.

My use case is to support a bulk version of CHM#computeIfAbsent, where a single mapping function returns the result for computing multiple entries. I had hoped to bulk lock, insert the unfilled entries, compute, populate, and bulk unlock. An overlapping write would be blocked due to requiring an entry's lock for mutation.

Hi Ben,

If "multiple entries" means less than say 50 or even a little more (using more locks than that for one bulk operation is not very optimal anyway), you could try constructing a recursive method to bulk-lock on the list of objects and execute a provided function:


    public static <T> T synchronizedAll(Iterable<?> locks, Supplier<T> supplier) {
        return synchronizedAll(locks.iterator(), supplier);
    }

    private static <T> T synchronizedAll(Iterator<?> locksIterator, Supplier<T> supplier) {
        if (locksIterator.hasNext()) {
            synchronized (locksIterator.next()) {
                return synchronizedAll(locksIterator, supplier);
            }
        } else {
            return supplier.get();
        }
    }



Note that to avoid deadlocks, the locks list has to be sorted using a Comparator that is consistent with your key's equals() method...

Regards, Peter



I had thought that using Unsafe would allow for achieving this without the memory overhead of a ReentrantLock/AQS per entry, since the synchronized keyword is not flexible enough to provide this structure.

Thanks,
Ben

Benchmark                                                    Mode  Samples         Score         Error  Units
c.g.b.c.SynchronizedBenchmark.monitor_contention            thrpt       10   3694951.630 ?   34340.707  ops/s
c.g.b.c.SynchronizedBenchmark.monitor_noContention          thrpt       10   8274097.911 ?  164356.363  ops/s
c.g.b.c.SynchronizedBenchmark.reentrantLock_contention      thrpt       10  31668532.247 ?  740850.955  ops/s
c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention    thrpt       10  41380163.703 ? 2270103.507  ops/s
c.g.b.c.SynchronizedBenchmark.synchronized_contention       thrpt       10  22905995.761 ?  117868.968  ops/s
c.g.b.c.SynchronizedBenchmark.synchronized_noContention     thrpt       10  44891601.915 ? 1458775.665  ops/s




_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest







_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/d5cca52f/attachment-0001.html>

From dt at flyingtroika.com  Tue Dec 30 22:14:02 2014
From: dt at flyingtroika.com (DT)
Date: Tue, 30 Dec 2014 19:14:02 -0800
Subject: [concurrency-interest] instructions reordering in java - practical
	check tools/principals/papers
Message-ID: <54A369FA.6090901@flyingtroika.com>

What would be a good way or tools to check of the reordering of 
instructions in java starting from java code-> compiler ->jvm reordering 
->processor reordering?


Thanks,
DT

From nitsanw at yahoo.com  Wed Dec 31 02:20:54 2014
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Wed, 31 Dec 2014 07:20:54 +0000 (UTC)
Subject: [concurrency-interest] Can a volatile read be reordered before
 a lazySet?
In-Reply-To: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>
References: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>
Message-ID: <1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>

Re-read the Shipilev "JMM Pragmatics" reference again last night, and discussed with Gil. I think that cup of coffee was nowhere near enough :-). In particular:
"If IUC this means stores and volatile loads are conservatively not reordered"?is FALSE because the premise "IUC" is false. I did not understand correctly which leads me to amend my comment:1. There's technically nothing to stop reordering between the store and load.
>? theLong.lazySet(1L);
>? Object o =?theRef.get();
2.?There's technically nothing to stop reordering between the store and load.>? long l = theLong.get() + 1; // LOADLOAD
>? theLong.lazySet(l); //STORESTORE
>? Object o =?theRef.get(); // LOADLOAD
3. I still think the while loop presents a different case to normal flow. So for now I still hold that the lazySet cannot be moved after the while loop.
My sincere apologies for the confusion.Happy holidays to all.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/4bbd67f7/attachment.html>

From jsampson at guidewire.com  Wed Dec 31 04:51:18 2014
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 31 Dec 2014 09:51:18 +0000
Subject: [concurrency-interest] AQS.compareAndSetHead CAS abbreviation
In-Reply-To: <E6D8C445-76D2-4D6B-B606-DC901A2DFABE@flyingtroika.com>
References: <E6D8C445-76D2-4D6B-B606-DC901A2DFABE@flyingtroika.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D7DF12@sm-ex-01-vm.guidewire.com>

DT wrote:

> In the AQS class method compareAndSetHead has java doc : /* CAS
> head field ... */ does it mean compare and set or compare and
> swap? It calls unsafe.compareAndSwapObject

Well, it appears that the Unsafe.compareAndSwapX methods are
actually compare-and-set operations anyway, aren't they? They all
return boolean to indicate success or failure, rather than returning
the old value of the field as a full compare-and-swap operation
would do.

It looks like java.util.concurrent code pretty consistently uses the
"compare and set" wording even though whoever wrote sun.misc.Unsafe
chose the "compare and swap" wording to mean exactly the same thing.
So I would presume that whoever wrote that "CAS head field" comment
was thinking "compare and set" at the time. :)

Cheers,
Justin


From aleksandar.prokopec at gmail.com  Wed Dec 31 05:22:34 2014
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Wed, 31 Dec 2014 11:22:34 +0100
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CAO9V1MJH6v+WPcuWnhrV5jRPLzm6i2bLuCUKhwZcbZLyAwViyw@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
	<54A2C34C.6000807@oracle.com>
	<CAO9V1MJH6v+WPcuWnhrV5jRPLzm6i2bLuCUKhwZcbZLyAwViyw@mail.gmail.com>
Message-ID: <e952cdbe-6797-4ee1-95bd-9681ce8e6cc5@email.android.com>

Did you consider a concurrent hash map?
It is slightly slower than a data structure based on just a linked list, but it does not sound like you would notice the difference.

Luke Sandberg <lukeisandberg at gmail.com> wrote:
>A doubly linked list that exposes the nodes would have that behavior.
>On Dec 30, 2014 9:23 AM, "Oleksandr Otenko"
><oleksandr.otenko at oracle.com>
>wrote:
>
>>  Do you know a non-concurrent structure that you are looking for?
>>
>> (eg you don't like O(n) for removal of arbitrary items - but is it
>> possible to have *all* operations O(1), even in a non-concurrent
>case?)
>>
>>
>> Alex
>>
>>
>> On 22/12/2014 02:35, Luke Sandberg wrote:
>>
>> I have come across a few situations where i am looking for a
>datastructure
>> and i feel like i keep coming up short.
>>
>> The situation is analogous to the 'waiters' list in
>> java.util.concurrent.FutureTask.
>>
>> Basically, I need to be able to publish a non-null object reference
>into a
>> data structure where
>> * order doesn't matter (insertion order would be nice, but i don't
>care
>> that much)
>> * add and remove identity semantics
>> * concurrent iteration (weakly consistent is fine)
>> * add/remove are non-blocking
>>
>> CLQ is an obvious choice but removal is O(n), another choice would a
>> simple synchronized identity hash set which is fine but the lock +
>high
>> entry overhead is a deal breaker.
>>
>> An AtomicReferenceArray would be super easy, but i can't put a bound
>on
>> the number of items.
>>
>> Also, it would be fine for the code that adds items, to store
>additional
>> state (an index, a 'Node' reference), in order to facilitate removal.
>>
>> The best thing i have seen from looking around appears to be
>something
>> like what FutureTask does to implement 'awaitDone', but even there
>> removeWaitier() is O(n).  that seems like a pretty good compromise
>when
>> lists are short, but what would be a better solution when lists are
>long?
>>
>> Just to motivate this a little bit, the two cases I am looking at in
>> particular are:
>>
>> * maintaining a set of threads associated with a server 'request'
>(threads
>> enter/exit when they start executing tasks associated with the
>request)
>> * maintaining a set of futures to be cancelled (futures are removed
>when
>> they complete to avoid pinning their referents).
>>
>> Any pointers or ideas?
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing
>listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
Sent from my Android phone with K-9 Mail. Please excuse my brevity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/5952e2fc/attachment.html>

From sub at laerad.com  Wed Dec 31 07:51:54 2014
From: sub at laerad.com (Benedict Elliott Smith)
Date: Wed, 31 Dec 2014 12:51:54 +0000
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CA+kOe0_MTyv+Ek8t7RztVF5LtvOfGU+tZzwe_gmrepVyMrvzaQ@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>
	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>
	<54A2C34C.6000807@oracle.com>
	<CAO9V1MJH6v+WPcuWnhrV5jRPLzm6i2bLuCUKhwZcbZLyAwViyw@mail.gmail.com>
	<CA+kOe0_MTyv+Ek8t7RztVF5LtvOfGU+tZzwe_gmrepVyMrvzaQ@mail.gmail.com>
Message-ID: <CACr06N2paXbS-JEyYS-TatNfB7VNevmcDUezhY+OjMTUNMjtFA@mail.gmail.com>

I have a simple wait-free collection supporting insertion, removal and
arbitrary-order traversal here
<https://github.com/belliottsmith/bes-utils/blob/master/src/bes/concurrent/collections/SimpleCollection.java>,
and a lock-free insert, wait-free remove, in-order traversible collection
here
<https://github.com/belliottsmith/bes-utils/blob/master/src/bes/concurrent/collections/SimpleWaitFreeCollection.java>
(on
which the former is built), and have attempted to blog about the latter here
<http://belliottsmith.com/eventual-consistency-concurrent-data-structures/>.

These are doubly-linked lists, much like CLDeque, but the implementation is
a little different, resulting in  wait-free worst-case constant time (and
CAS-free) deletion, but with the possibility of some garbage accumulation
under high contention. Since contention necessarily drops as the list
grows, the amount of garbage should be pretty limited, and is limited to
the nodes themselves, not the data. The lack of CAS operation means
uncontended (i.e. non-adjacent) deletes should be appreciably cheaper,
though I haven't benchmarked it extensively since I mostly use it because
it is easy to reason about.

On 31 December 2014 at 01:44, Martin Buchholz <martinrb at google.com> wrote:

>
>
> On Tue, Dec 30, 2014 at 4:22 PM, Luke Sandberg <lukeisandberg at gmail.com>
> wrote:
>
>> A doubly linked list that exposes the nodes would have that behavior.
>>
> And ConcurrentLinkedDeque is essentially a lock-free doubly linked list
> that supports all operations except interior insertion, which we don't know
> how to (and don't need to) implement.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/874cccd1/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Dec 31 09:19:52 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 31 Dec 2014 14:19:52 +0000
Subject: [concurrency-interest] Looking for a data struture
In-Reply-To: <CA+kOe0_MTyv+Ek8t7RztVF5LtvOfGU+tZzwe_gmrepVyMrvzaQ@mail.gmail.com>
References: <CAO9V1ML7Yn3W5rO4rfD9m7a1Qpetee_ytX+dPwgHUfc4g6=6-A@mail.gmail.com>	<CAO9V1M+Tw-7=iOgLcS60qO9BJSn13MC+uVCJhyzdu5LqZy1ahg@mail.gmail.com>	<54A2C34C.6000807@oracle.com>	<CAO9V1MJH6v+WPcuWnhrV5jRPLzm6i2bLuCUKhwZcbZLyAwViyw@mail.gmail.com>
	<CA+kOe0_MTyv+Ek8t7RztVF5LtvOfGU+tZzwe_gmrepVyMrvzaQ@mail.gmail.com>
Message-ID: <54A40608.3050504@oracle.com>

I suppose, the difference is that a doubly linked list that exposes the 
nodes is faster on deletion, because you'd only need to relink the 
neighbouring nodes - rather than traverse the queue to do that.

This, of course, is not a general-purpose container, because it requires 
someone to hold on to the nodes (eg item in the list should point to the 
node containing it - so can only be inserted into one list at a time)

Alex


On 31/12/2014 01:44, Martin Buchholz wrote:
>
>
> On Tue, Dec 30, 2014 at 4:22 PM, Luke Sandberg 
> <lukeisandberg at gmail.com <mailto:lukeisandberg at gmail.com>> wrote:
>
>     A doubly linked list that exposes the nodes would have that behavior.
>
> And ConcurrentLinkedDeque is essentially a lock-free doubly linked 
> list that supports all operations except interior insertion, which we 
> don't know how to (and don't need to) implement.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/f1497cea/attachment.html>

From oleksandr.otenko at oracle.com  Wed Dec 31 09:36:55 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 31 Dec 2014 14:36:55 +0000
Subject: [concurrency-interest] synchronized vs
	Unsafe#monitorEnter/monitorExit
In-Reply-To: <0161A568-9AC7-4F58-84E2-5AC31428084F@azulsystems.com>
References: <54A2BC93.5090406@oracle.com>
	<476178422.1738456.1419984764152.JavaMail.yahoo@jws100139.mail.ne1.yahoo.com>
	<54A34357.2080400@oracle.com>
	<0161A568-9AC7-4F58-84E2-5AC31428084F@azulsystems.com>
Message-ID: <54A40A07.9090401@oracle.com>

This is great summary.

Yes, I probably stumbled on the inflated locks once. It is not the 
number of locks in use at any time that mattered, but the number of 
locks in use in a certain generation that mattered - so they didn't get 
deflated until the GC for that generation would fire. In the end I 
probably ran out of inflated monitors in the pool. (tens of millions, I 
think)

No, it wasn't HotSpot, if you are wondering.

Alex

On 31/12/2014 01:44, Gil Tene wrote:
> The number of objects being synchronized on doesn't really have much 
> of an effect. There aren't that many JVMs out there, and AFAIK all the 
> ones that run on servers will handle high counts of synchronized-on 
> objects just fine. There are plenty of commonly used pieces of 
> software that put JVMs through these paces every day. For example, 
> Cassandra (under load) will tend to generate 10s of thousands of new 
> *inflated* monitors per second. And I've seen several other apps that 
> do that too.
>
> JVMs (at least the ones with fixed sized object headers, which is most 
> server JVMs) tend to have room in all object headers for the 
> "un-contended"/deflated monitor state case, both locked and unlocked. 
> The way they implement this varies, e.g. displaced headers in Oracle 
> HotSpot/OpenJDK vs. thin locks or bacon bits in others, but they all 
> fit that stuff in a single header word somehow.
>
> It's "inflation" and "deflation" that are the actual cost, and the 
> choices there vary. E.g. it is possible to contain the number of 
> inflated monitors to the number of threads. Each thread can at most be 
> blocked on one monitor at a time, so the maximum number of actual 
> required monitors is always capped by the number of threads. It is 
> also possible to incorporate the inflated monitor state into the 
> thread structures based on this observation (chaining stuff through 
> threads). But in practice HotSpot variants (including Zing) tend to 
> maintain an inflated monitor pool. When monitors need to be inflated 
> (due to contention or wait() calls) an inflated monitor state is 
> allocated and connected to the object (usually by having the header 
> refer to it somehow). This obviously means that monitors need to be 
> deflated to balance this out. Monitor deflation is usually done as a 
> STW (stop the world) operation as part of safepoint operations (time 
> is usually hidden in GC pauses, and not explicitly reported). 
> Deflation can be done concurrently without STW effects, but AFAIK Zing 
> is the only one doing it concurrently right now. Makes quite a 
> difference when you need to deflate at a sustained rate of 30K 
> monitors/sec and don't want to take pauses in the process...
>
> ? Gil.
>
>> On Dec 30, 2014, at 4:29 PM, Oleksandr Otenko 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>> ok. You must consider that using the object header means different 
>> JVMs will have different overheads and limits. Some JVMs will not let 
>> you have tens of millions of objects on which you want to synchronize 
>> - contended or not - just because there is not enough space in their 
>> object headers for all the book-keeping info.
>>
>> Alex
>>
>>
>> On 31/12/2014 00:12, Ben Manes wrote:
>>> > Why do you need synchronized specifically? (and not 
>>> j.u.c.locks.Lock lock/unlock)
>>>
>>> To minimize memory usage, as the lock is per entry. Consider this as 
>>> an extension to ConcurrentHashMap's computeIfAbsent(K key), but that 
>>> instead accepts Iterable<K> keys. A full j.u.c lock is ideal, but 
>>> that requires each entry embeds a lock instance and that adds 
>>> significant bloat. In the case of a cache, writes are rare and even 
>>> rarer to the same entries. Thus, synchronizing should use the 
>>> Object's header as a think lock until contention is detected and the 
>>> JVM inflates the lock to a full mutex. After inflation, the JVM 
>>> could decide to deinflate the lock if it reverts back to low 
>>> contention usages. That means no wasted memory to support a single 
>>> use-case, which may not be a very commonly used pattern. If the 
>>> Unsafe versions were optimizable, then using synchronized would be 
>>> an ideal choice.
>>>
>>> There are alternatives, of course. The approach in Guava was to be 
>>> non-blocking and populate the cache after the bulk load completes. 
>>> This overwrites any entries that appeared while the loadAll was 
>>> computed, so it doesn't solve the thundering herd problem. This is 
>>> reasonable based on the assumption that local cache misses don't 
>>> cause the as much pain as, say, a memcached restart causing a 
>>> database query storm. Another approach might use lock striping, e.g. 
>>> 1024 real locks that can be hashed to and are acquired in ascending 
>>> order. That adds more complexity and hash collisions might occur, 
>>> but it wouldn't be too bad in practice. It does add extra 
>>> complexity. And yet another might be a lazy lock field per entry, 
>>> but that is still wasteful in large caches.
>>>
>>> There's probably a solution using some type of reservation entry 
>>> that contains a lock reference, and after populated transitions to 
>>> an implementation that doesn't have a lock. That can be done within 
>>> the hash table, like CHM's ReservationNode, but is harder to do 
>>> elegantly through a decorator.
>>>
>>> Anyway, hopefully its clear now why Unsafe appeared like an 
>>> attractive solution if it had performed well.
>>>
>>>
>>> On Tuesday, December 30, 2014 6:54 AM, Oleksandr Otenko 
>>> <oleksandr.otenko at oracle.com> wrote:
>>>
>>>
>>> Why do you need synchronized specifically? (and not j.u.c.locks.Lock 
>>> lock/unlock)
>>>
>>> Alex
>>>
>>> On 28/12/2014 20:15, Ben Manes wrote:
>>>> That's an nifty workaround but, as you said, not reliable in the 
>>>> case of a general api. This case is a multi-get for a cache 
>>>> library, so the keys and use-cases aren't known.
>>>>
>>>> When adding bulk loading to Guava, due to the internal complexity 
>>>> we allowed it to racy by not blocking concurrent calls for shared 
>>>> entries. This has to be done regardless because a loadAll(keys) may 
>>>> return a Map<K, V> with more entries than originally requested, and 
>>>> those extra entries should be cached. These cases were trivial to 
>>>> handle when I had previously written a Map<K, Future<V>> to support 
>>>> multi-get, but a lot of bloat per entry. I had hoped 
>>>> Unsafe#monitorEnter would be a nice alternative and, if the API was 
>>>> stable, it arguably still could be due to the low write rate of 
>>>> caches. For now I'll leave it in a backlog of performance 
>>>> optimization ideas like we did with Guava, maybe revisiting once 
>>>> the rewrite stabilizes.
>>>>
>>>> Thanks,
>>>> -Ben
>>>>
>>>>
>>>> On Sunday, December 28, 2014 5:18 AM, Peter Levart 
>>>> <peter.levart at gmail.com> <mailto:peter.levart at gmail.com> wrote:
>>>>
>>>>
>>>>
>>>> On 12/27/2014 09:31 PM, Ben Manes wrote:
>>>>> Can someone explain why using Unsafe's monitor methods are 
>>>>> substantially worse than synchronized? I had expected them to emit 
>>>>> equivalent monitorEnter/monitorExit instructions and have similar 
>>>>> performance.
>>>>>
>>>>> My use case is to support a bulk version of CHM#computeIfAbsent, 
>>>>> where a single mapping function returns the result for computing 
>>>>> multiple entries. I had hoped to bulk lock, insert the unfilled 
>>>>> entries, compute, populate, and bulk unlock. An overlapping write 
>>>>> would be blocked due to requiring an entry's lock for mutation.
>>>>
>>>> Hi Ben,
>>>>
>>>> If "multiple entries" means less than say 50 or even a little more 
>>>> (using more locks than that for one bulk operation is not very 
>>>> optimal anyway), you could try constructing a recursive method to 
>>>> bulk-lock on the list of objects and execute a provided function:
>>>>
>>>>
>>>>     public static <T> T synchronizedAll(Iterable<?> locks, 
>>>> Supplier<T> supplier) {
>>>>         return synchronizedAll(locks.iterator(), supplier);
>>>>     }
>>>>
>>>>     private static <T> T synchronizedAll(Iterator<?> locksIterator, 
>>>> Supplier<T> supplier) {
>>>>         if (locksIterator.hasNext()) {
>>>>             synchronized (locksIterator.next()) {
>>>>                 return synchronizedAll(locksIterator, supplier);
>>>>             }
>>>>         } else {
>>>>             return supplier.get();
>>>>         }
>>>>     }
>>>>
>>>>
>>>>
>>>> Note that to avoid deadlocks, the locks list has to be sorted using 
>>>> a Comparator that is consistent with your key's equals() method...
>>>>
>>>> Regards, Peter
>>>>
>>>>
>>>>
>>>>> I had thought that using Unsafe would allow for achieving this 
>>>>> without the memory overhead of a ReentrantLock/AQS per entry, 
>>>>> since the synchronized keyword is not flexible enough to provide 
>>>>> this structure.
>>>>>
>>>>> Thanks,
>>>>> Ben
>>>>>
>>>>> Benchmark  Mode  Samples       Score     Error  Units
>>>>> c.g.b.c.SynchronizedBenchmark.monitor_contention            thrpt 
>>>>>       10 3694951.630 ? 34340.707  ops/s
>>>>> c.g.b.c.SynchronizedBenchmark.monitor_noContention          thrpt 
>>>>>     10 8274097.911 ?  164356.363  ops/s
>>>>> c.g.b.c.SynchronizedBenchmark.reentrantLock_contention      thrpt 
>>>>> 10  31668532.247 ?  740850.955  ops/s
>>>>> c.g.b.c.SynchronizedBenchmark.reentrantLock_noContention    thrpt 
>>>>> 10  41380163.703 ? 2270103.507  ops/s
>>>>> c.g.b.c.SynchronizedBenchmark.synchronized_contention       thrpt 
>>>>>   10  22905995.761 ?  117868.968  ops/s
>>>>> c.g.b.c.SynchronizedBenchmark.synchronized_noContention     thrpt 
>>>>> 10  44891601.915 ? 1458775.665  ops/s
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/58d86e60/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Dec 31 09:45:29 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 31 Dec 2014 14:45:29 +0000
Subject: [concurrency-interest] Can a volatile read be reordered before
 a lazySet?
In-Reply-To: <1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>
References: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>
	<1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>
Message-ID: <54A40C09.7040004@oracle.com>

Not sure why you say (3). Since lazySet is not part of JMM, I am only 
assuming it is pretty much like an ordered store on x86.

In the presence of write buffers the loop may spin indefinitely, yet the 
lazy-stored write won't become visible to the pong thread - there'd be 
nothing to cause the buffers of the ping thread to flush. So ping 
effectively will appear to perform all the loads of the indefinite loop 
ahead of the store.

In reality the only event that would cause the write buffers to flush, 
is the thread preemption.


Alex


On 31/12/2014 07:20, Nitsan Wakart wrote:
> Re-read the Shipilev "JMM Pragmatics" reference again last night, and 
> discussed with Gil. I think that cup of coffee was nowhere near enough 
> :-). In particular:
> "If IUC this means stores and volatile loads are conservatively not 
> reordered" is FALSE because the premise "IUC" is false. I did not 
> understand correctly which leads me to amend my comment:
> 1. There's technically nothing to stop reordering between the store 
> and load.
> >theLong.lazySet(1L);
> >Object o = theRef.get();
> 2. There's technically nothing to stop reordering between the store 
> and load.
> >  long l = theLong.get() + 1; // LOADLOAD
> >theLong.lazySet(l); //STORESTORE
> >Object o = theRef.get(); // LOADLOAD
> 3. I still think the while loop presents a different case to normal 
> flow. So for now I still hold that the lazySet cannot be moved after 
> the while loop.
>
> My sincere apologies for the confusion.
> Happy holidays to all.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/68da4b8b/attachment.html>

From vitalyd at gmail.com  Wed Dec 31 10:18:02 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 31 Dec 2014 10:18:02 -0500
Subject: [concurrency-interest] Can a volatile read be reordered before
	a lazySet?
In-Reply-To: <54A40C09.7040004@oracle.com>
References: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>
	<1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>
	<54A40C09.7040004@oracle.com>
Message-ID: <CAHjP37HNCBKrpgWaP5PTT0HR55C1jp7CKRTXoKwO9Fp--57osg@mail.gmail.com>

Write buffers don't *need* to be drained, they drain automatically anyway.
At least that's the case for intel.  So the issue here is more like what
Gil said - compiler can reorder the code causing a deadlock.

Sent from my phone
On Dec 31, 2014 9:45 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  Not sure why you say (3). Since lazySet is not part of JMM, I am only
> assuming it is pretty much like an ordered store on x86.
>
> In the presence of write buffers the loop may spin indefinitely, yet the
> lazy-stored write won't become visible to the pong thread - there'd be
> nothing to cause the buffers of the ping thread to flush. So ping
> effectively will appear to perform all the loads of the indefinite loop
> ahead of the store.
>
> In reality the only event that would cause the write buffers to flush, is
> the thread preemption.
>
>
> Alex
>
>
> On 31/12/2014 07:20, Nitsan Wakart wrote:
>
> Re-read the Shipilev "JMM Pragmatics" reference again last night, and
> discussed with Gil. I think that cup of coffee was nowhere near enough :-).
> In particular:
>  "If IUC this means stores and volatile loads are conservatively not
> reordered" is FALSE because the premise "IUC" is false. I did not
> understand correctly which leads me to amend my comment:
> 1. There's technically nothing to stop reordering between the store and
> load.
>  >  theLong.lazySet(1L);
> >  Object o = theRef.get();
>  2. There's technically nothing to stop reordering between the store and
> load.
> >  long l = theLong.get() + 1; // LOADLOAD
> >  theLong.lazySet(l); //STORESTORE
> >  Object o = theRef.get(); // LOADLOAD
>  3. I still think the while loop presents a different case to normal
> flow. So for now I still hold that the lazySet cannot be moved after the
> while loop.
>
>  My sincere apologies for the confusion.
> Happy holidays to all.
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/aea27b5b/attachment.html>

From oleksandr.otenko at oracle.com  Wed Dec 31 11:31:47 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 31 Dec 2014 16:31:47 +0000
Subject: [concurrency-interest] Can a volatile read be reordered before
 a lazySet?
In-Reply-To: <CAHjP37HNCBKrpgWaP5PTT0HR55C1jp7CKRTXoKwO9Fp--57osg@mail.gmail.com>
References: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>	<1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>	<54A40C09.7040004@oracle.com>
	<CAHjP37HNCBKrpgWaP5PTT0HR55C1jp7CKRTXoKwO9Fp--57osg@mail.gmail.com>
Message-ID: <54A424F3.2060107@oracle.com>

Yes, but do they drain if no one writes anything?

Alex

On 31/12/2014 15:18, Vitaly Davidovich wrote:
>
> Write buffers don't *need* to be drained, they drain automatically 
> anyway.  At least that's the case for intel.  So the issue here is 
> more like what Gil said - compiler can reorder the code causing a 
> deadlock.
>
> Sent from my phone
>
> On Dec 31, 2014 9:45 AM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Not sure why you say (3). Since lazySet is not part of JMM, I am
>     only assuming it is pretty much like an ordered store on x86.
>
>     In the presence of write buffers the loop may spin indefinitely,
>     yet the lazy-stored write won't become visible to the pong thread
>     - there'd be nothing to cause the buffers of the ping thread to
>     flush. So ping effectively will appear to perform all the loads of
>     the indefinite loop ahead of the store.
>
>     In reality the only event that would cause the write buffers to
>     flush, is the thread preemption.
>
>
>     Alex
>
>
>     On 31/12/2014 07:20, Nitsan Wakart wrote:
>>     Re-read the Shipilev "JMM Pragmatics" reference again last night,
>>     and discussed with Gil. I think that cup of coffee was nowhere
>>     near enough :-). In particular:
>>     "If IUC this means stores and volatile loads are conservatively
>>     not reordered" is FALSE because the premise "IUC" is false. I did
>>     not understand correctly which leads me to amend my comment:
>>     1. There's technically nothing to stop reordering between the
>>     store and load.
>>     >  theLong.lazySet(1L);
>>     >  Object o = theRef.get();
>>     2. There's technically nothing to stop reordering between the
>>     store and load.
>>     >  long l = theLong.get() + 1; // LOADLOAD
>>     >  theLong.lazySet(l); //STORESTORE
>>     >  Object o = theRef.get(); // LOADLOAD
>>     3. I still think the while loop presents a different case to
>>     normal flow. So for now I still hold that the lazySet cannot be
>>     moved after the while loop.
>>
>>     My sincere apologies for the confusion.
>>     Happy holidays to all.
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/972b3132/attachment.html>

From vitalyd at gmail.com  Wed Dec 31 11:36:49 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 31 Dec 2014 11:36:49 -0500
Subject: [concurrency-interest] Can a volatile read be reordered before
	a lazySet?
In-Reply-To: <54A424F3.2060107@oracle.com>
References: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>
	<1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>
	<54A40C09.7040004@oracle.com>
	<CAHjP37HNCBKrpgWaP5PTT0HR55C1jp7CKRTXoKwO9Fp--57osg@mail.gmail.com>
	<54A424F3.2060107@oracle.com>
Message-ID: <CAHjP37G7UMZOLq74Pcp51k5752REsS07t8g22A1OZLN7Pzbd4Q@mail.gmail.com>

Your previous reply talked about performing a lazy write and then possibly
having it stall indefinitely (or until preemption, as you say) because
nobody forces the buffer to drain.  I'm saying if the write actually
occurs, it'll drain on its own.

Sent from my phone
On Dec 31, 2014 11:31 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  Yes, but do they drain if no one writes anything?
>
> Alex
>
> On 31/12/2014 15:18, Vitaly Davidovich wrote:
>
> Write buffers don't *need* to be drained, they drain automatically
> anyway.  At least that's the case for intel.  So the issue here is more
> like what Gil said - compiler can reorder the code causing a deadlock.
>
> Sent from my phone
> On Dec 31, 2014 9:45 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  Not sure why you say (3). Since lazySet is not part of JMM, I am only
>> assuming it is pretty much like an ordered store on x86.
>>
>> In the presence of write buffers the loop may spin indefinitely, yet the
>> lazy-stored write won't become visible to the pong thread - there'd be
>> nothing to cause the buffers of the ping thread to flush. So ping
>> effectively will appear to perform all the loads of the indefinite loop
>> ahead of the store.
>>
>> In reality the only event that would cause the write buffers to flush, is
>> the thread preemption.
>>
>>
>> Alex
>>
>>
>> On 31/12/2014 07:20, Nitsan Wakart wrote:
>>
>> Re-read the Shipilev "JMM Pragmatics" reference again last night, and
>> discussed with Gil. I think that cup of coffee was nowhere near enough :-).
>> In particular:
>>  "If IUC this means stores and volatile loads are conservatively not
>> reordered" is FALSE because the premise "IUC" is false. I did not
>> understand correctly which leads me to amend my comment:
>> 1. There's technically nothing to stop reordering between the store and
>> load.
>>  >  theLong.lazySet(1L);
>> >  Object o = theRef.get();
>>  2. There's technically nothing to stop reordering between the store and
>> load.
>> >  long l = theLong.get() + 1; // LOADLOAD
>> >  theLong.lazySet(l); //STORESTORE
>> >  Object o = theRef.get(); // LOADLOAD
>>  3. I still think the while loop presents a different case to normal
>> flow. So for now I still hold that the lazySet cannot be moved after the
>> while loop.
>>
>>  My sincere apologies for the confusion.
>> Happy holidays to all.
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/25dd4f6a/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Dec 31 11:44:15 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 31 Dec 2014 16:44:15 +0000
Subject: [concurrency-interest] AQS.compareAndSetHead CAS abbreviation
In-Reply-To: <E6D8C445-76D2-4D6B-B606-DC901A2DFABE@flyingtroika.com>
References: <E6D8C445-76D2-4D6B-B606-DC901A2DFABE@flyingtroika.com>
Message-ID: <54A427DF.4030705@oracle.com>

Setting or Swapping is the same in this case - upon successful CAS(x, 
expected, new) you can tell that the value of x was expected, and now 
set to new. The meaning of swap is that the underlying instruction set 
may permit to load the value of x on failure - that is, if x is not 
expected, tell what it really is; swapping in this case refers to 
swapping the value of register holding expected value for the actual value.

Alex


On 31/12/2014 00:39, DT wrote:
> In the AQS class method compareAndSetHead has java doc : /* CAS head field ... */ does it mean compare and set or compare and swap?
> It calls unsafe.compareAndSwapObject
>
> Thanks
>


From oleksandr.otenko at oracle.com  Wed Dec 31 11:50:06 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 31 Dec 2014 16:50:06 +0000
Subject: [concurrency-interest] Can a volatile read be reordered before
 a lazySet?
In-Reply-To: <CAHjP37G7UMZOLq74Pcp51k5752REsS07t8g22A1OZLN7Pzbd4Q@mail.gmail.com>
References: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>	<1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>	<54A40C09.7040004@oracle.com>	<CAHjP37HNCBKrpgWaP5PTT0HR55C1jp7CKRTXoKwO9Fp--57osg@mail.gmail.com>	<54A424F3.2060107@oracle.com>
	<CAHjP37G7UMZOLq74Pcp51k5752REsS07t8g22A1OZLN7Pzbd4Q@mail.gmail.com>
Message-ID: <54A4293E.7000503@oracle.com>

yes, I got you, and my clumsy question really should have read "upon 
what condition will they drain on their own"

Like, N tics? Coherence traffic ceased? Hyperthreaded sibling does 
something?

Alex

On 31/12/2014 16:36, Vitaly Davidovich wrote:
>
> Your previous reply talked about performing a lazy write and then 
> possibly having it stall indefinitely (or until preemption, as you 
> say) because nobody forces the buffer to drain.  I'm saying if the 
> write actually occurs, it'll drain on its own.
>
> Sent from my phone
>
> On Dec 31, 2014 11:31 AM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Yes, but do they drain if no one writes anything?
>
>     Alex
>
>     On 31/12/2014 15:18, Vitaly Davidovich wrote:
>>
>>     Write buffers don't *need* to be drained, they drain
>>     automatically anyway.  At least that's the case for intel.  So
>>     the issue here is more like what Gil said - compiler can reorder
>>     the code causing a deadlock.
>>
>>     Sent from my phone
>>
>>     On Dec 31, 2014 9:45 AM, "Oleksandr Otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         Not sure why you say (3). Since lazySet is not part of JMM, I
>>         am only assuming it is pretty much like an ordered store on x86.
>>
>>         In the presence of write buffers the loop may spin
>>         indefinitely, yet the lazy-stored write won't become visible
>>         to the pong thread - there'd be nothing to cause the buffers
>>         of the ping thread to flush. So ping effectively will appear
>>         to perform all the loads of the indefinite loop ahead of the
>>         store.
>>
>>         In reality the only event that would cause the write buffers
>>         to flush, is the thread preemption.
>>
>>
>>         Alex
>>
>>
>>         On 31/12/2014 07:20, Nitsan Wakart wrote:
>>>         Re-read the Shipilev "JMM Pragmatics" reference again last
>>>         night, and discussed with Gil. I think that cup of coffee
>>>         was nowhere near enough :-). In particular:
>>>         "If IUC this means stores and volatile loads are
>>>         conservatively not reordered" is FALSE because the premise
>>>         "IUC" is false. I did not understand correctly which leads
>>>         me to amend my comment:
>>>         1. There's technically nothing to stop reordering between
>>>         the store and load.
>>>         >theLong.lazySet(1L);
>>>         >  Object o = theRef.get();
>>>         2. There's technically nothing to stop reordering between
>>>         the store and load.
>>>         >  long l = theLong.get() + 1; // LOADLOAD
>>>         >  theLong.lazySet(l); //STORESTORE
>>>         >  Object o = theRef.get(); // LOADLOAD
>>>         3. I still think the while loop presents a different case to
>>>         normal flow. So for now I still hold that the lazySet cannot
>>>         be moved after the while loop.
>>>
>>>         My sincere apologies for the confusion.
>>>         Happy holidays to all.
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/4b0fa584/attachment.html>

From vitalyd at gmail.com  Wed Dec 31 13:32:54 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 31 Dec 2014 13:32:54 -0500
Subject: [concurrency-interest] Can a volatile read be reordered before
	a lazySet?
In-Reply-To: <54A4293E.7000503@oracle.com>
References: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>
	<1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>
	<54A40C09.7040004@oracle.com>
	<CAHjP37HNCBKrpgWaP5PTT0HR55C1jp7CKRTXoKwO9Fp--57osg@mail.gmail.com>
	<54A424F3.2060107@oracle.com>
	<CAHjP37G7UMZOLq74Pcp51k5752REsS07t8g22A1OZLN7Pzbd4Q@mail.gmail.com>
	<54A4293E.7000503@oracle.com>
Message-ID: <CAHjP37GUTXi3rfo1xbBw=04LhRuX-t6--ceaVagU-7Y-KSF4YQ@mail.gmail.com>

I don't know the exact mechanics of draining, but I suspect it happens when
the cacheline is fetched and/or RFO completes on it.  The way I understand
it is core wants to write but either doesn't have the cacheline or needs to
obtain ownership of it; while that request is outstanding, the write is put
into the store buffer.  Once the line is available, the write is applied to
it from the buffer.

I'd welcome someone with more details to correct/enhance my understanding.

Sent from my phone
On Dec 31, 2014 11:50 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  yes, I got you, and my clumsy question really should have read "upon what
> condition will they drain on their own"
>
> Like, N tics? Coherence traffic ceased? Hyperthreaded sibling does
> something?
>
> Alex
>
> On 31/12/2014 16:36, Vitaly Davidovich wrote:
>
> Your previous reply talked about performing a lazy write and then possibly
> having it stall indefinitely (or until preemption, as you say) because
> nobody forces the buffer to drain.  I'm saying if the write actually
> occurs, it'll drain on its own.
>
> Sent from my phone
> On Dec 31, 2014 11:31 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  Yes, but do they drain if no one writes anything?
>>
>> Alex
>>
>> On 31/12/2014 15:18, Vitaly Davidovich wrote:
>>
>> Write buffers don't *need* to be drained, they drain automatically
>> anyway.  At least that's the case for intel.  So the issue here is more
>> like what Gil said - compiler can reorder the code causing a deadlock.
>>
>> Sent from my phone
>> On Dec 31, 2014 9:45 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>>  Not sure why you say (3). Since lazySet is not part of JMM, I am only
>>> assuming it is pretty much like an ordered store on x86.
>>>
>>> In the presence of write buffers the loop may spin indefinitely, yet the
>>> lazy-stored write won't become visible to the pong thread - there'd be
>>> nothing to cause the buffers of the ping thread to flush. So ping
>>> effectively will appear to perform all the loads of the indefinite loop
>>> ahead of the store.
>>>
>>> In reality the only event that would cause the write buffers to flush,
>>> is the thread preemption.
>>>
>>>
>>> Alex
>>>
>>>
>>> On 31/12/2014 07:20, Nitsan Wakart wrote:
>>>
>>> Re-read the Shipilev "JMM Pragmatics" reference again last night, and
>>> discussed with Gil. I think that cup of coffee was nowhere near enough :-).
>>> In particular:
>>>  "If IUC this means stores and volatile loads are conservatively not
>>> reordered" is FALSE because the premise "IUC" is false. I did not
>>> understand correctly which leads me to amend my comment:
>>> 1. There's technically nothing to stop reordering between the store and
>>> load.
>>>  >  theLong.lazySet(1L);
>>> >  Object o = theRef.get();
>>>  2. There's technically nothing to stop reordering between the store
>>> and load.
>>> >  long l = theLong.get() + 1; // LOADLOAD
>>> >  theLong.lazySet(l); //STORESTORE
>>> >  Object o = theRef.get(); // LOADLOAD
>>>  3. I still think the while loop presents a different case to normal
>>> flow. So for now I still hold that the lazySet cannot be moved after the
>>> while loop.
>>>
>>>  My sincere apologies for the confusion.
>>> Happy holidays to all.
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/873894d2/attachment.html>

From gil at azulsystems.com  Wed Dec 31 13:41:13 2014
From: gil at azulsystems.com (Gil Tene)
Date: Wed, 31 Dec 2014 18:41:13 +0000
Subject: [concurrency-interest] Can a volatile read be reordered before
 a lazySet?
In-Reply-To: <54A4293E.7000503@oracle.com>
References: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>
	<1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>
	<54A40C09.7040004@oracle.com>
	<CAHjP37HNCBKrpgWaP5PTT0HR55C1jp7CKRTXoKwO9Fp--57osg@mail.gmail.com>
	<54A424F3.2060107@oracle.com>
	<CAHjP37G7UMZOLq74Pcp51k5752REsS07t8g22A1OZLN7Pzbd4Q@mail.gmail.com>
	<54A4293E.7000503@oracle.com>
Message-ID: <95CF20E9-823C-4B70-B638-2B5F52938D86@azulsystems.com>

It would vary by processor. But in most processors, store buffer contents are usually drained as soon as their associated in-flight L1 cache line arrives so that the store can be merged into it. On processors that implicitly maintain store order, the drain order if usually a FIFO. I.e. stores that have lines ready won't drain before that were ahead of them but don't yet have lines available.

So in general stuff will be in the buffer for a time length that is on the order of an LLC miss (L3 on modern Xeons).


On Dec 31, 2014, at 8:50 AM, Oleksandr Otenko <oleksandr.otenko at oracle.com<mailto:oleksandr.otenko at oracle.com>> wrote:

yes, I got you, and my clumsy question really should have read "upon what condition will they drain on their own"

Like, N tics? Coherence traffic ceased? Hyperthreaded sibling does something?

Alex

On 31/12/2014 16:36, Vitaly Davidovich wrote:

Your previous reply talked about performing a lazy write and then possibly having it stall indefinitely (or until preemption, as you say) because nobody forces the buffer to drain.  I'm saying if the write actually occurs, it'll drain on its own.

Sent from my phone

On Dec 31, 2014 11:31 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com<mailto:oleksandr.otenko at oracle.com>> wrote:
Yes, but do they drain if no one writes anything?

Alex

On 31/12/2014 15:18, Vitaly Davidovich wrote:

Write buffers don't *need* to be drained, they drain automatically anyway.  At least that's the case for intel.  So the issue here is more like what Gil said - compiler can reorder the code causing a deadlock.

Sent from my phone

On Dec 31, 2014 9:45 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com<mailto:oleksandr.otenko at oracle.com>> wrote:
Not sure why you say (3). Since lazySet is not part of JMM, I am only assuming it is pretty much like an ordered store on x86.

In the presence of write buffers the loop may spin indefinitely, yet the lazy-stored write won't become visible to the pong thread - there'd be nothing to cause the buffers of the ping thread to flush. So ping effectively will appear to perform all the loads of the indefinite loop ahead of the store.

In reality the only event that would cause the write buffers to flush, is the thread preemption.


Alex


On 31/12/2014 07:20, Nitsan Wakart wrote:
Re-read the Shipilev "JMM Pragmatics" reference again last night, and discussed with Gil. I think that cup of coffee was nowhere near enough :-). In particular:
"If IUC this means stores and volatile loads are conservatively not reordered" is FALSE because the premise "IUC" is false. I did not understand correctly which leads me to amend my comment:
1. There's technically nothing to stop reordering between the store and load.
>  theLong.lazySet(1L);
>  Object o = theRef.get();
2. There's technically nothing to stop reordering between the store and load.
>  long l = theLong.get() + 1; // LOADLOAD
>  theLong.lazySet(l); //STORESTORE
>  Object o = theRef.get(); // LOADLOAD
3. I still think the while loop presents a different case to normal flow. So for now I still hold that the lazySet cannot be moved after the while loop.

My sincere apologies for the confusion.
Happy holidays to all.



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/a6873862/attachment.html>

From dt at flyingtroika.com  Wed Dec 31 16:37:58 2014
From: dt at flyingtroika.com (DT)
Date: Wed, 31 Dec 2014 13:37:58 -0800
Subject: [concurrency-interest] AQS.compareAndSetHead CAS abbreviation
In-Reply-To: <54A427DF.4030705@oracle.com>
References: <E6D8C445-76D2-4D6B-B606-DC901A2DFABE@flyingtroika.com>
	<54A427DF.4030705@oracle.com>
Message-ID: <3BB9A18E-C298-4B4E-85E5-3F19D1DDE401@flyingtroika.com>

Folks, thank you for the clarification.


> On Dec 31, 2014, at 8:44 AM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
> 
> Setting or Swapping is the same in this case - upon successful CAS(x, expected, new) you can tell that the value of x was expected, and now set to new. The meaning of swap is that the underlying instruction set may permit to load the value of x on failure - that is, if x is not expected, tell what it really is; swapping in this case refers to swapping the value of register holding expected value for the actual value.
> 
> Alex
> 
> 
>> On 31/12/2014 00:39, DT wrote:
>> In the AQS class method compareAndSetHead has java doc : /* CAS head field ... */ does it mean compare and set or compare and swap?
>> It calls unsafe.compareAndSwapObject
>> 
>> Thanks
> 


From boehm at acm.org  Wed Dec 31 21:41:14 2014
From: boehm at acm.org (Hans Boehm)
Date: Wed, 31 Dec 2014 18:41:14 -0800
Subject: [concurrency-interest] Can a volatile read be reordered before
	a lazySet?
In-Reply-To: <CAHjP37GUTXi3rfo1xbBw=04LhRuX-t6--ceaVagU-7Y-KSF4YQ@mail.gmail.com>
References: <CAHjP37EWeg7HH31JuQvU_ohOEpwZC8xwktawoyDPhJ1C4Prhjg@mail.gmail.com>
	<1691363577.3021979.1420010454774.JavaMail.yahoo@jws106104.mail.bf1.yahoo.com>
	<54A40C09.7040004@oracle.com>
	<CAHjP37HNCBKrpgWaP5PTT0HR55C1jp7CKRTXoKwO9Fp--57osg@mail.gmail.com>
	<54A424F3.2060107@oracle.com>
	<CAHjP37G7UMZOLq74Pcp51k5752REsS07t8g22A1OZLN7Pzbd4Q@mail.gmail.com>
	<54A4293E.7000503@oracle.com>
	<CAHjP37GUTXi3rfo1xbBw=04LhRuX-t6--ceaVagU-7Y-KSF4YQ@mail.gmail.com>
Message-ID: <CAPUmR1aFWfVzF8g_96P9QepZ4iY32MNsKKc-5zXUsM=87jnE9g@mail.gmail.com>

In my view, if we have

ping.lazySet(1);
> while (pong.get() == 0);
>

it is certainly possible that some number of the loads become visible
before the store.  But I don't believe the intent in either Java or C++,
was to allow the compiler to defer the  store until after the loop, causing
this to deadlock.

This is largely a question of which progress guarantees are provided:  This
transformation makes it look to the programmer as though visibility of the
lazySet is delayed indefinitely for no reason.  Both languages are very
careful in what they actually promise in this respect because some real
implementations do in fact provide very weak guarantees.  At least at the
time of the Java memory model, there were non-preemptive threads
implementations, in which threads could stall indefinitely for no good
reason except the existence of other runnable threads.  As earlier messages
point out, hardware architectures also typically don't provide an iron-clad
guarantee that store buffers drain in a finite amount of time under all
conditions, though in practice they tend do so quite quickly (otherwise
e.g. spin-lock releases would not become visible to other threads, which
would be bad).

So I think the somewhat unsatisfactory bottom line here is that there is no
clear-cut, always-applicable, prohibition against compiler movement of the
lazySet/release store (though 17.4.9 imposes some restrictions for Java),
but I would consider it very poor quality of implementation to actually do
do.  I don't think this would deadlock for any real implementations.

Hans

On Wed, Dec 31, 2014 at 10:32 AM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> I don't know the exact mechanics of draining, but I suspect it happens
> when the cacheline is fetched and/or RFO completes on it.  The way I
> understand it is core wants to write but either doesn't have the cacheline
> or needs to obtain ownership of it; while that request is outstanding, the
> write is put into the store buffer.  Once the line is available, the write
> is applied to it from the buffer.
>
> I'd welcome someone with more details to correct/enhance my understanding.
>
> Sent from my phone
> On Dec 31, 2014 11:50 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  yes, I got you, and my clumsy question really should have read "upon
>> what condition will they drain on their own"
>>
>> Like, N tics? Coherence traffic ceased? Hyperthreaded sibling does
>> something?
>>
>> Alex
>>
>> On 31/12/2014 16:36, Vitaly Davidovich wrote:
>>
>> Your previous reply talked about performing a lazy write and then
>> possibly having it stall indefinitely (or until preemption, as you say)
>> because nobody forces the buffer to drain.  I'm saying if the write
>> actually occurs, it'll drain on its own.
>>
>> Sent from my phone
>> On Dec 31, 2014 11:31 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>>  Yes, but do they drain if no one writes anything?
>>>
>>> Alex
>>>
>>> On 31/12/2014 15:18, Vitaly Davidovich wrote:
>>>
>>> Write buffers don't *need* to be drained, they drain automatically
>>> anyway.  At least that's the case for intel.  So the issue here is more
>>> like what Gil said - compiler can reorder the code causing a deadlock.
>>>
>>> Sent from my phone
>>> On Dec 31, 2014 9:45 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>>> wrote:
>>>
>>>>  Not sure why you say (3). Since lazySet is not part of JMM, I am only
>>>> assuming it is pretty much like an ordered store on x86.
>>>>
>>>> In the presence of write buffers the loop may spin indefinitely, yet
>>>> the lazy-stored write won't become visible to the pong thread - there'd be
>>>> nothing to cause the buffers of the ping thread to flush. So ping
>>>> effectively will appear to perform all the loads of the indefinite loop
>>>> ahead of the store.
>>>>
>>>> In reality the only event that would cause the write buffers to flush,
>>>> is the thread preemption.
>>>>
>>>>
>>>> Alex
>>>>
>>>>
>>>> On 31/12/2014 07:20, Nitsan Wakart wrote:
>>>>
>>>> Re-read the Shipilev "JMM Pragmatics" reference again last night, and
>>>> discussed with Gil. I think that cup of coffee was nowhere near enough :-).
>>>> In particular:
>>>>  "If IUC this means stores and volatile loads are conservatively not
>>>> reordered" is FALSE because the premise "IUC" is false. I did not
>>>> understand correctly which leads me to amend my comment:
>>>> 1. There's technically nothing to stop reordering between the store and
>>>> load.
>>>>  >  theLong.lazySet(1L);
>>>> >  Object o = theRef.get();
>>>>  2. There's technically nothing to stop reordering between the store
>>>> and load.
>>>> >  long l = theLong.get() + 1; // LOADLOAD
>>>> >  theLong.lazySet(l); //STORESTORE
>>>> >  Object o = theRef.get(); // LOADLOAD
>>>>  3. I still think the while loop presents a different case to normal
>>>> flow. So for now I still hold that the lazySet cannot be moved after the
>>>> while loop.
>>>>
>>>>  My sincere apologies for the confusion.
>>>> Happy holidays to all.
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/3c57999f/attachment.html>

