From jhu@glog.com  Wed Feb  2 15:17:04 2005
From: jhu@glog.com (Hu, Jinsong)
Date: Wed, 2 Feb 2005 10:17:04 -0500
Subject: [concurrency-interest] more lock modes based on AbstractQueuedSynchronizer
Message-ID: <00E2E6C68DA5D6118CE000065B3CE15526A644@mail-pa.glog.com>

This message is in MIME format. Since your mail reader does not understand
this format, some or all of this message may not be legible.

------_=_NextPart_001_01C5093A.40C347A0
Content-Type: text/plain

Hi All, 

 

I am new to jdk1.5 concurrent API, at the first glance of
AbstractQueuedSynchronizer, it appears to me that this class has built-in
support for Exclusive and Shared mode locks, if I want to support more type
of locks, such as, IX, SIX, Update locks, which are common to database
systems, can I still base on this framework, or I have to implement
something similar from scratch?

 

Thanks

 


------_=_NextPart_001_01C5093A.40C347A0
Content-Type: text/html

<html>

<head>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=us-ascii">


<meta name=Generator content="Microsoft Word 10 (filtered)">

<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:SimSun;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:"\@SimSun";
	panose-1:2 1 6 0 3 1 1 1 1 1;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman";}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{color:purple;
	text-decoration:underline;}
span.EmailStyle17
	{font-family:Arial;
	color:windowtext;}
@page Section1
	{size:8.5in 11.0in;
	margin:1.0in 1.25in 1.0in 1.25in;}
div.Section1
	{page:Section1;}
-->
</style>

</head>

<body lang=EN-US link=blue vlink=purple>

<div class=Section1>

<p class=MsoNormal><font size=2 face=Arial><span style='font-size:10.0pt;
font-family:Arial'>Hi All, </span></font></p>

<p class=MsoNormal><font size=2 face=Arial><span style='font-size:10.0pt;
font-family:Arial'>&nbsp;</span></font></p>

<p class=MsoNormal><font size=2 face=Arial><span style='font-size:10.0pt;
font-family:Arial'>I am new to jdk1.5 concurrent API, at the first glance of AbstractQueuedSynchronizer,
it appears to me that this class has built-in support for Exclusive and Shared
mode locks, if I want to support more type of locks, such as, IX, SIX, Update
locks, which are common to database systems, can I still base on this framework,
or I have to implement something similar from scratch?</span></font></p>

<p class=MsoNormal><font size=2 face=Arial><span style='font-size:10.0pt;
font-family:Arial'>&nbsp;</span></font></p>

<p class=MsoNormal><font size=2 face=Arial><span style='font-size:10.0pt;
font-family:Arial'>Thanks</span></font></p>

<p class=MsoNormal><font size=2 face=Arial><span style='font-size:10.0pt;
font-family:Arial'>&nbsp;</span></font></p>

</div>

</body>

</html>

------_=_NextPart_001_01C5093A.40C347A0--

From matthias.ernst@coremedia.com  Wed Feb  2 15:43:59 2005
From: matthias.ernst@coremedia.com (Ernst, Matthias)
Date: Wed, 2 Feb 2005 16:43:59 +0100
Subject: AW: [concurrency-interest] more lock modes based on AbstractQueuedSynchronizer
Message-ID: <F34C8A704C489B46B9E9FBDBD1B91D5FC043BE@MARS.coremedia.com>

As far as I understand:

* "shared" only means "potentially shared", i.e. two threads possibly
operating concurrently "in" the lock. The exclusive mode is more
performant since the AQS "knows better" how many waiting threads to wake
up
* the lock state must be encodable in an int
* a lock request must be encodable in an int (why is this? Well, it
can't be more information as is encodable in the state ...)

So it seems well possible to do what you're thinking about.
AQS is very powerful indeed ...

Matthias


From dl@cs.oswego.edu  Wed Feb  2 15:58:01 2005
From: dl@cs.oswego.edu (Doug Lea)
Date: Wed, 2 Feb 2005 10:58:01 -0500
Subject: AW: [concurrency-interest] more lock modes based on AbstractQueuedSynchronizer
In-Reply-To: <F34C8A704C489B46B9E9FBDBD1B91D5FC043BE@MARS.coremedia.com>
References: <F34C8A704C489B46B9E9FBDBD1B91D5FC043BE@MARS.coremedia.com>
Message-ID: <16896.63625.783072.6219@altair.cs.oswego.edu>

As an aside, we're now planning to add AbstractQueuedLongSynchronizer
(sorry for crummy name) to Mustang. This expands the range of
applicability to synchronization states requiring 64bits. In many
applications you'd still have to use bit-oriented techniques (that can
be awkward in Java) to partition these 64bits into different aspects
of sync state.  Also, the atomic updates require emulation (so won't
perform very well) on a few platforms -- the only common one is pre-G5
macs. But by the time Apple supports Mustang, we figure all mac users
will have G5s :-)

-Doug


From richard.zschech@cqrdata.com  Thu Feb  3 07:43:50 2005
From: richard.zschech@cqrdata.com (Richard Zschech)
Date: Thu, 03 Feb 2005 07:43:50 +0000
Subject: [concurrency-interest] Owned Locks
Message-ID: <4201D636.4040502@cqrdata.com>

I previously posted a question [1] over a year and a half ago (it 
doesn't seem that long)
about owned locks.
Doug Lea responded [2] saying that they were intentionally left out and 
suggested using
a ThreadLocal to track the owner of the lock and a special 
implementation of the Lock
interface which works with the ThreadLocal.
I am trying to figure out how to best leverage the current built in 
locks to achieve owned
locks. Any help or suggestions would be appreciated.

Thanks in advance,
RZ

[1] 
http://altair.cs.oswego.edu/pipermail/concurrency-interest/2003-July/000497.html
[2] 
http://altair.cs.oswego.edu/pipermail/concurrency-interest/2003-July/000498.html


From tim@peierls.net  Thu Feb  3 18:30:38 2005
From: tim@peierls.net (Tim Peierls)
Date: Thu, 03 Feb 2005 13:30:38 -0500
Subject: [concurrency-interest] more lock modes based on
 AbstractQueuedSynchronizer
In-Reply-To: <00E2E6C68DA5D6118CE000065B3CE15526A644@mail-pa.glog.com>
References: <00E2E6C68DA5D6118CE000065B3CE15526A644@mail-pa.glog.com>
Message-ID: <42026DCE.9050502@peierls.net>

Hu, Jinsong wrote:
> I am new to jdk1.5 concurrent API, at the first glance of 
> AbstractQueuedSynchronizer, it appears to me that this class has 
> built-in support for Exclusive and Shared mode locks, if I want to 
> support more type of locks, such as, IX, SIX, Update locks, which are 
> common to database systems, can I still base on this framework, or I 
> have to implement something similar from scratch?

and Matthias Ernst wrote:
> * the lock state must be encodable in an int
> * a lock request must be encodable in an int ...
>
> So it seems well possible to do what you're thinking about.


Here, for example, is an implementation of a four-mode (read, write,
intent-read, and intent-write) lock.

https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/rrlock/MMLock.java

I haven't done significant testing of this, so there are undoubtedly
bugs, but this should give you an idea of what is possible and how to
approach it.

You can also use this as a three-mode (read, write, and increment) lock.
The incrementLock() method is an alias for intentWriteLock().

I wrote this as part of a quad-tree implementation to demonstrate
intent locking. I'll post a link to it when it exists.

--tim


From tim@peierls.net  Sun Feb  6 05:31:35 2005
From: tim@peierls.net (Tim Peierls)
Date: Sun, 06 Feb 2005 00:31:35 -0500
Subject: [concurrency-interest] Owned Locks
In-Reply-To: <4201D636.4040502@cqrdata.com>
References: <4201D636.4040502@cqrdata.com>
Message-ID: <4205ABB7.7030806@peierls.net>

Richard Zschech wrote:
> I previously posted a question [1] over a year and a half ago ... about
> owned locks. Doug Lea responded [2] saying that they were intentionally
> left out and suggested using a ThreadLocal to track the owner of the
> lock and a special implementation of the Lock interface which works with
> the ThreadLocal. I am trying to figure out how to best leverage the
> current built in locks to achieve owned locks. Any help or suggestions
> would be appreciated.
> 
> http://altair.cs.oswego.edu/pipermail/concurrency-interest/2003-July/000497.html
> http://altair.cs.oswego.edu/pipermail/concurrency-interest/2003-July/000498.html

This feels very much like another example of what I called GenderLock in an
earlier posting, but have since learned is known as "room synchronization".
See http://www-2.cs.cmu.edu/afs/cs/project/pscico/www/rooms.html for more,
but the basic idea is that any number of threads can enter a room, and only
one room at a time can be entered. That sounds a lot like having a lock
that can have one owner at a time, but is not tied to one thread.

https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/rrlock/RRLock.java

(For "gender", read "room" in this code.)

But Doug's suggestion of using ThreadLocals is interesting, because
it leads to an entirely different implementation with very similar
behavior:

https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/rrlock/OwnedLock.java

Caveat lector: I have done only primitive testing of this class;
in particular, I haven't tried out the Condition support at all.

Can you expand a bit on the two applications of owned locks that
you mentioned, session ID and transaction ID?

--tim


From richard.zschech@cqrdata.com  Tue Feb  8 22:29:45 2005
From: richard.zschech@cqrdata.com (Richard Zschech)
Date: Tue, 08 Feb 2005 22:29:45 +0000
Subject: [concurrency-interest] Owned Locks
In-Reply-To: <4205ABB7.7030806@peierls.net>
References: <4201D636.4040502@cqrdata.com> <4205ABB7.7030806@peierls.net>
Message-ID: <42093D59.3030602@cqrdata.com>

Sorry I previously sent the response to Tim not the list.


Hmm, I'm not sure how the room example applies and the OwnedLock example 
dosen't seem to allow for the owner of the lock to change overtime.

The desired behavior is still a reentrant mutual exclusion the only 
problem is that I want to access the lock based on a owner not based the 
current thread.
In the Java Transaction API [1] when implementing an XAResource there 
are number of methods which take an Xid which any locking needs to be 
based around.
A rather contrived example is if I have an implementation of an 
XAResource which also has a method to update a variable pool based on an 
expression e.g. foo = foo++. This code is called as follows:

resource.start(xid);
result = resource.update("foo", "++");
resource.prepare(xid);
// "foo" is now locked
resource.commit(xid, false /*two phase commit*/);
// "foo" is now unlocked and result is now filled in

The implementation of the prepare and commit would look something like 
this:

public StringBuilder update(String variable, String expression) {
 Variable v =  variablePool.get(variable);
 StringBuilder result = new StringBuilder();
 getCurrentTransactionContext().addUpdateRequest(v, expression, result);
 return result;
}

public void prepare(Xid xid) {
 // acquire the lock
 for(all update requests associated with this xid) {
   updateRequest.getVariable().getLock().lock(xid);
 }
}

public void commit(Xid xid, boolean onePhase) {
 if(onePhase) {
   prepare(xid);
 }
 // do the update
 for(all update requests associated with this xid) {
   updateRequest.doUpdate();
 }

 // release lock
 for(all update requests associated with this xid) {
   updateRequest.getVariable().getLock().unlock(xid);
 }
}

There is more plumbing which I have omitted to associate the Xids with 
the update call. The problem is that the prepare which obtains any 
required locks and the commit doesn't have to be called on the same 
thread so the locking of foo cant be thread based.

I don't mind that the owned lock doesn't implement the Lock interface 
and has an extra owner parameter. I already have an implementation of 
the owned locks, I was just hoping I could leverage the built in locking.

[1] http://java.sun.com/products/jta/



Tim Peierls wrote:

> Richard Zschech wrote:
>
>> I previously posted a question [1] over a year and a half ago ... about
>> owned locks. Doug Lea responded [2] saying that they were intentionally
>> left out and suggested using a ThreadLocal to track the owner of the
>> lock and a special implementation of the Lock interface which works with
>> the ThreadLocal. I am trying to figure out how to best leverage the
>> current built in locks to achieve owned locks. Any help or suggestions
>> would be appreciated.
>>
>> http://altair.cs.oswego.edu/pipermail/concurrency-interest/2003-July/000497.html 
>>
>> http://altair.cs.oswego.edu/pipermail/concurrency-interest/2003-July/000498.html 
>>
>
>
>
> This feels very much like another example of what I called GenderLock 
> in an
> earlier posting, but have since learned is known as "room 
> synchronization".
> See http://www-2.cs.cmu.edu/afs/cs/project/pscico/www/rooms.html for 
> more,
> but the basic idea is that any number of threads can enter a room, and 
> only
> one room at a time can be entered. That sounds a lot like having a lock
> that can have one owner at a time, but is not tied to one thread.
>
> https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/rrlock/RRLock.java 
>
>
> (For "gender", read "room" in this code.)
>
> But Doug's suggestion of using ThreadLocals is interesting, because
> it leads to an entirely different implementation with very similar
> behavior:
>
> https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/rrlock/OwnedLock.java 
>
>
> Caveat lector: I have done only primitive testing of this class;
> in particular, I haven't tried out the Condition support at all.
>
> Can you expand a bit on the two applications of owned locks that
> you mentioned, session ID and transaction ID?
>
> --tim
>



From dholmes@dltech.com.au  Wed Feb  9 00:14:53 2005
From: dholmes@dltech.com.au (David Holmes)
Date: Wed, 9 Feb 2005 10:14:53 +1000
Subject: [concurrency-interest] Owned Locks
In-Reply-To: <42093D59.3030602@cqrdata.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEGIFFAA.dholmes@dltech.com.au>

Richard,

This seems more a token-based access control mechanism: if you pass the
right token you are allowed to perform a particular function; if you don't
have the right token then you can't. This is different to locks where one
thread may hold the lock and another tries to acquire it, blocking until it
is available--if your threads don't have the right token then blocking will
not change that (unless you have a token-passing scheme :) ). Your problem
then boils down to one of token management: how are they defined, how can
they be used (can more than one thread hold the token at a time) etc. Within
that token management system you may need to use "normal" locks, but I'm not
quite seeing where the "owned-lock" comes into play.

Doug Lea covers this sort of transactional model in Section 3.6 of
Concurrent Programming in Java 2nd Edition. You can add transaction id's as
your tokens to all method calls, or you can make the transaction id implicit
such as through a ThreadLocal value.

Apologies if I'm not seeing your problem from the right perspective.

Regards,
David Holmes

> -----Original Message-----
> From: concurrency-interest-admin@cs.oswego.edu
> [mailto:concurrency-interest-admin@cs.oswego.edu]On Behalf Of Richard
> Zschech
> Sent: Wednesday, 9 February 2005 8:30 AM
> To: concurrency-interest@altair.cs.oswego.edu
> Subject: Re: [concurrency-interest] Owned Locks
>
>
> Sorry I previously sent the response to Tim not the list.
>
>
> Hmm, I'm not sure how the room example applies and the OwnedLock example
> dosen't seem to allow for the owner of the lock to change overtime.
>
> The desired behavior is still a reentrant mutual exclusion the only
> problem is that I want to access the lock based on a owner not based the
> current thread.
> In the Java Transaction API [1] when implementing an XAResource there
> are number of methods which take an Xid which any locking needs to be
> based around.
> A rather contrived example is if I have an implementation of an
> XAResource which also has a method to update a variable pool based on an
> expression e.g. foo = foo++. This code is called as follows:
>
> resource.start(xid);
> result = resource.update("foo", "++");
> resource.prepare(xid);
> // "foo" is now locked
> resource.commit(xid, false /*two phase commit*/);
> // "foo" is now unlocked and result is now filled in
>
> The implementation of the prepare and commit would look something like
> this:
>
> public StringBuilder update(String variable, String expression) {
>  Variable v =  variablePool.get(variable);
>  StringBuilder result = new StringBuilder();
>  getCurrentTransactionContext().addUpdateRequest(v, expression, result);
>  return result;
> }
>
> public void prepare(Xid xid) {
>  // acquire the lock
>  for(all update requests associated with this xid) {
>    updateRequest.getVariable().getLock().lock(xid);
>  }
> }
>
> public void commit(Xid xid, boolean onePhase) {
>  if(onePhase) {
>    prepare(xid);
>  }
>  // do the update
>  for(all update requests associated with this xid) {
>    updateRequest.doUpdate();
>  }
>
>  // release lock
>  for(all update requests associated with this xid) {
>    updateRequest.getVariable().getLock().unlock(xid);
>  }
> }
>
> There is more plumbing which I have omitted to associate the Xids with
> the update call. The problem is that the prepare which obtains any
> required locks and the commit doesn't have to be called on the same
> thread so the locking of foo cant be thread based.
>
> I don't mind that the owned lock doesn't implement the Lock interface
> and has an extra owner parameter. I already have an implementation of
> the owned locks, I was just hoping I could leverage the built in locking.
>
> [1] http://java.sun.com/products/jta/
>
>
>


From jhu@glog.com  Wed Feb  9 17:16:04 2005
From: jhu@glog.com (Hu, Jinsong)
Date: Wed, 9 Feb 2005 12:16:04 -0500
Subject: [concurrency-interest] Is enq(Node node) safe?
Message-ID: <00E2E6C68DA5D6118CE000065B3CE15526A66B@mail-pa.glog.com>

Hi,

    Can anyone prove that I am wrong? While I review the code for
AbstractQueuedSynchronizer Node's enq method, I have a concern this may mess
up in some multi-thread scenario. Actually, this code logic is used almost
everywhere, I think I do not understand it correctly. 

    For example, if two threads do the enq at the same time, one thread
reaches line 8 compareAndSetHead(h) and succeeds, while before it reaches
line 9, for whatever reason, CPU decided this thread should sleep for a
while, let other threads to run. Then second thread comes in, since tail is
still null, reaches Line 8, also succeed, reaches line 9, changed tail and
return. Then the first thread wakes up, execute line 9 which changes the
tail, then we can see the tail is not the true tail:

    [node0]->[node1]-[node2] 
       ^        ^   
       |        |
      head     tail


    private Node enq(final Node node) {
        for (;;) {
            Node t = tail;
            if (t == null) { // Must initialize
                Node h = new Node(); // Dummy header
                h.next = node;
                node.prev = h;
                if (compareAndSetHead(h)) {      // Line 8  
                    tail = node;                 // Line 9  
                    return h;
                }
            }
            else {
                node.prev = t;     
                if (compareAndSetTail(t, node)) {    // line 15
                    t.next = node;                   // line 16
                    return t; 
                }
            }
        }
    }

From dl@cs.oswego.edu  Wed Feb  9 17:38:07 2005
From: dl@cs.oswego.edu (Doug Lea)
Date: Wed, 09 Feb 2005 12:38:07 -0500
Subject: [concurrency-interest] Is enq(Node node) safe?
In-Reply-To: <00E2E6C68DA5D6118CE000065B3CE15526A66B@mail-pa.glog.com>
References: <00E2E6C68DA5D6118CE000065B3CE15526A66B@mail-pa.glog.com>
Message-ID: <420A4A7F.6060501@cs.oswego.edu>

Hu, Jinsong wrote:
> Hi,
> 
>     Can anyone prove that I am wrong? While I review the code for
> AbstractQueuedSynchronizer Node's enq method, I have a concern this may mess
> up in some multi-thread scenario. Actually, this code logic is used almost
> everywhere, I think I do not understand it correctly. 
> 
>     For example, if two threads do the enq at the same time, one thread
> reaches line 8 compareAndSetHead(h) and succeeds, while before it reaches
> line 9, for whatever reason, CPU decided this thread should sleep for a
> while, let other threads to run. Then second thread comes in, since tail is
> still null, reaches Line 8, also succeed, reaches line 9, changed tail and
> return. Then the first thread wakes up, execute line 9 which changes the
> tail, then we can see the tail is not the true tail:

The short answer is that all the other code using nodes is aware of this
possibility and copes. My CSJP workshop paper has a brief explanation.
See http://gee.cs.oswego.edu/dl/papers/aqs.pdf

For the best careful explanation I know on this general technique, see
the DISC'04 paper "An Optimistic Approach to Lock-Free FIFO Queues" Edya 
Ladan-Mozes and Nir Shavit. Google says you can find it at
http://www.springerlink.com/index/H84DFEXJFTDAL4P4.pdf

-Doug

From blanshlu@netscape.net  Wed Feb  9 18:52:10 2005
From: blanshlu@netscape.net (Luke Blanshard)
Date: Wed, 09 Feb 2005 12:52:10 -0600
Subject: [concurrency-interest] Is enq(Node node) safe?
In-Reply-To: <00E2E6C68DA5D6118CE000065B3CE15526A66B@mail-pa.glog.com>
References: <00E2E6C68DA5D6118CE000065B3CE15526A66B@mail-pa.glog.com>
Message-ID: <420A5BDA.7090705@netscape.net>

I think the premise that both threads can succeed in setting the head is 
false.  One will succeed, the other will fail.  The compareAndSetHead 
call uses an atomic compare-and-set instruction to set the head only if 
it is currently null.  (Actually, I'm just guessing here -- if I'm wrong 
I'm sure someone will correct me.)

Luke

jhu@glog.com wrote:

>Hi,
>
>    Can anyone prove that I am wrong? While I review the code for
>AbstractQueuedSynchronizer Node's enq method, I have a concern this may mess
>up in some multi-thread scenario. Actually, this code logic is used almost
>everywhere, I think I do not understand it correctly. 
>
>    For example, if two threads do the enq at the same time, one thread
>reaches line 8 compareAndSetHead(h) and succeeds, while before it reaches
>line 9, for whatever reason, CPU decided this thread should sleep for a
>while, let other threads to run. Then second thread comes in, since tail is
>still null, reaches Line 8, also succeed, reaches line 9, changed tail and
>return. Then the first thread wakes up, execute line 9 which changes the
>tail, then we can see the tail is not the true tail:
>
>    [node0]->[node1]-[node2] 
>       ^        ^   
>       |        |
>      head     tail
>
>
>    private Node enq(final Node node) {
>        for (;;) {
>            Node t = tail;
>            if (t == null) { // Must initialize
>                Node h = new Node(); // Dummy header
>                h.next = node;
>                node.prev = h;
>                if (compareAndSetHead(h)) {      // Line 8  
>                    tail = node;                 // Line 9  
>                    return h;
>                }
>            }
>            else {
>                node.prev = t;     
>                if (compareAndSetTail(t, node)) {    // line 15
>                    t.next = node;                   // line 16
>                    return t; 
>                }
>            }
>        }
>    }
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest@altair.cs.oswego.edu
>http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>  
>

From dl@cs.oswego.edu  Wed Feb  9 20:16:58 2005
From: dl@cs.oswego.edu (Doug Lea)
Date: Wed, 09 Feb 2005 15:16:58 -0500
Subject: [concurrency-interest] Is enq(Node node) safe?
In-Reply-To: <420A5BDA.7090705@netscape.net>
References: <00E2E6C68DA5D6118CE000065B3CE15526A66B@mail-pa.glog.com> <420A5BDA.7090705@netscape.net>
Message-ID: <420A6FBA.4020909@cs.oswego.edu>

Luke Blanshard wrote:
> I think the premise that both threads can succeed in setting the head is 
> false.  One will succeed, the other will fail.  The compareAndSetHead 
> call uses an atomic compare-and-set instruction to set the head only if 
> it is currently null.  (Actually, I'm just guessing here -- if I'm wrong 
> I'm sure someone will correct me.

Thanks! Yes, this is true. I should have said this in my last mail,
to make clear that the compensation code elsewhere only needs to
cover apparently-null vs non-null cases.

-Doug


From jhu@glog.com  Wed Feb  9 20:21:52 2005
From: jhu@glog.com (Hu, Jinsong)
Date: Wed, 9 Feb 2005 15:21:52 -0500
Subject: [concurrency-interest] Is enq(Node node) safe?
Message-ID: <00E2E6C68DA5D6118CE000065B3CE15526A66D@mail-pa.glog.com>

My original posting does not assume that both threads need to successfully
compareAndSetHead at the same time, they do have orders, one at a time, the
point is, the first succeeded thread was forced to sleep after it
successfully compareAndSetHead, since the first thread already finished
this, the second thread should not fail on compareAndSetHead.
 

-----Original Message-----
From: Doug Lea [mailto:dl@cs.oswego.edu] 
Sent: Wednesday, February 09, 2005 3:17 PM
To: Luke Blanshard
Cc: jhu@glog.com; concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Is enq(Node node) safe?

Luke Blanshard wrote:
> I think the premise that both threads can succeed in setting the head is 
> false.  One will succeed, the other will fail.  The compareAndSetHead 
> call uses an atomic compare-and-set instruction to set the head only if 
> it is currently null.  (Actually, I'm just guessing here -- if I'm wrong 
> I'm sure someone will correct me.

Thanks! Yes, this is true. I should have said this in my last mail,
to make clear that the compensation code elsewhere only needs to
cover apparently-null vs non-null cases.

-Doug

From dl@cs.oswego.edu  Wed Feb  9 20:36:24 2005
From: dl@cs.oswego.edu (Doug Lea)
Date: Wed, 09 Feb 2005 15:36:24 -0500
Subject: [concurrency-interest] Is enq(Node node) safe?
In-Reply-To: <00E2E6C68DA5D6118CE000065B3CE15526A66D@mail-pa.glog.com>
References: <00E2E6C68DA5D6118CE000065B3CE15526A66D@mail-pa.glog.com>
Message-ID: <420A7448.8070403@cs.oswego.edu>

Hu, Jinsong wrote:
> My original posting does not assume that both threads need to successfully
> compareAndSetHead at the same time, they do have orders, one at a time, the
> point is, the first succeeded thread was forced to sleep after it
> successfully compareAndSetHead, since the first thread already finished
> this, the second thread should not fail on compareAndSetHead.

No. compareAndSetHead(h) expands to a compareAndSet(null, h). Only
the first thread can succeed in CAS'ing head to non-null. The other 
might spin for a while until the successful one wakes up, but since this
is a race in one-time initialization code, the potential for an 
unbounded spin is not worth specially handling. (Preliminary versions
of this class had some spin control here, but in testing even on big
multiprocessors, it was found not to have any effect so was simplified
away.)

-Doug

From jhu@glog.com  Wed Feb  9 20:42:32 2005
From: jhu@glog.com (Hu, Jinsong)
Date: Wed, 9 Feb 2005 15:42:32 -0500
Subject: [concurrency-interest] Is enq(Node node) safe?
Message-ID: <00E2E6C68DA5D6118CE000065B3CE15526A66E@mail-pa.glog.com>

Sorry, I do not notice that. 
Then how about compareAndSetTail part of this enq method, then my test
scenario does apply, doesn't it?


-----Original Message-----
From: Doug Lea [mailto:dl@cs.oswego.edu] 
Sent: Wednesday, February 09, 2005 3:36 PM
To: Hu, Jinsong
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Is enq(Node node) safe?

Hu, Jinsong wrote:
> My original posting does not assume that both threads need to successfully
> compareAndSetHead at the same time, they do have orders, one at a time,
the
> point is, the first succeeded thread was forced to sleep after it
> successfully compareAndSetHead, since the first thread already finished
> this, the second thread should not fail on compareAndSetHead.

No. compareAndSetHead(h) expands to a compareAndSet(null, h). Only
the first thread can succeed in CAS'ing head to non-null. The other 
might spin for a while until the successful one wakes up, but since this
is a race in one-time initialization code, the potential for an 
unbounded spin is not worth specially handling. (Preliminary versions
of this class had some spin control here, but in testing even on big
multiprocessors, it was found not to have any effect so was simplified
away.)

-Doug

From dholmes@dltech.com.au  Wed Feb  9 22:15:49 2005
From: dholmes@dltech.com.au (David Holmes)
Date: Thu, 10 Feb 2005 08:15:49 +1000
Subject: [concurrency-interest] Is enq(Node node) safe?
In-Reply-To: <00E2E6C68DA5D6118CE000065B3CE15526A66E@mail-pa.glog.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEHLFFAA.dholmes@dltech.com.au>

> Hu Jinsong wrote:
>
> Sorry, I do not notice that.
> Then how about compareAndSetTail part of this enq method, then my test
> scenario does apply, doesn't it?
>

No that is the part that Doug referred to where users of the code compensate
for the fact that the next field is set potentially much later from when the
tail is swapped in - so a null next field does not imply it is the tail. See
the docs on the Node.next field.

David Holmes


From jean.morissette666@videotron.ca  Thu Feb 10 23:36:02 2005
From: jean.morissette666@videotron.ca (Jean Morissette)
Date: Thu, 10 Feb 2005 18:36:02 -0500
Subject: [concurrency-interest] Reloading thread variables
Message-ID: <420BEFE2.1070005@videotron.ca>

Hi,
Suppose that we have a class that is used by many threads.  This class 
behavior is dependent of one of its fields.  This field can be modified 
by any threads and it it modified very rarely.  Finally, the class must 
offer very high performances.

So, when the field value change, I'm wondering if restarting all threads 
(stoping live threads and creating new ones) that use this class would 
ensure that they see the new field value?  If yes, I will not be forced 
to declare the field volatile (thus no overhead).

Or do you have another idea to reload thread registers?

From dholmes@dltech.com.au  Thu Feb 10 23:56:59 2005
From: dholmes@dltech.com.au (David Holmes)
Date: Fri, 11 Feb 2005 09:56:59 +1000
Subject: [concurrency-interest] Reloading thread variables
In-Reply-To: <420BEFE2.1070005@videotron.ca>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEJFFFAA.dholmes@dltech.com.au>

Jean,

This seems like excessive micro-optimization. Write the class using a
volatile and see if there is a performance problem. If so then try to figure
out ways of dealing with it.

> So, when the field value change, I'm wondering if restarting all threads
> (stoping live threads and creating new ones) that use this class would
> ensure that they see the new field value?  If yes, I will not be forced
> to declare the field volatile (thus no overhead).

If the pattern is:

    changeValue(int newval) {
       stopAllThreadsButMe();
       val = newval;
       startNewThreads();
    }

then yes the new threads will all see the new value. But the implied thread
management here would seem to impose its own significant overhead. And it
assumes you can stop all threads etc.

David Holmes


From dawidk at mathcs.emory.edu  Sat Feb 12 00:50:20 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Sat Feb 12 00:50:34 2005
Subject: [concurrency-interest] backport-util-concurrent: new release
	available
Message-ID: <420D991C.7060205@mathcs.emory.edu>

The backport-util-concurrent, version 1.1_01, has been released:

http://www.mathcs.emory.edu/dcl/util/backport-util-concurrent/

Change log:

    * Bugfix: race condition in the fair implementation of ReentrantLock
      caused it to occassionally cause IllegalMonitorState exceptions.
      Non-fair implementation was not affected, however, classes that
      depend on fair reentrant locks, namely: fair ArrayBlockingQueue,
      fair SynchronousQueue, and PriorityQueue, were affected. Thanks to
      Ramesh Nethi for reporting this bug and helping to track it down.
    * Testing: backport has been stress-tested using the "loops" tests
      (courtesy of Doug Lea and the JSR 166 Expert Group). The tests are
      included in the development source bundle.

The "loops" tests thoroughly evaluate behavior and performance of 
various types of locks, queues, maps, futures, and other API classes, 
under various conditions (contention etc.) and circumstances, including 
cancellation. The backport has withstood days of testing on Windows XP, 
Linux, and Solaris (SMP), running up to 400 concurrent threads. 

Regards,
Dawid Kurzyniec


From dawidk at mathcs.emory.edu  Sat Feb 12 01:05:07 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Sat Feb 12 01:05:12 2005
Subject: [concurrency-interest] backport-util-concurrent: this list
	permitted as a discussion forum
Message-ID: <420D9C93.4080005@mathcs.emory.edu>

Courtesy of Doug Lea and the JSR 166 Expert Group, the 
concurrency-interest list (i.e. this list) may be used as a discussion 
forum for the backport-util-concurrent 
(http://www.mathcs.emory.edu/dcl/util/backport-util-concurrent/). When 
posting a comment or question regarding the backport rather than the 
original API, please clearly say so in the e-mail. I suggest prefixing 
subject with "backport-util-concurrent:" or "backport:".

Regards,
Dawid Kurzyniec


From gdp at inf.ed.ac.uk  Sun Feb 13 07:37:05 2005
From: gdp at inf.ed.ac.uk (gdp@inf.ed.ac.uk)
Date: Sun Feb 13 07:37:09 2005
Subject: [concurrency-interest] CMSB 2005: Second Announcement 
Message-ID: <1108298225.420f49f13c024@mail.inf.ed.ac.uk>

This is a second announcement for

        Computational Methods in Systems Biology 2005

to be held in Edinburgh, April 3rd-5th, as part of ETAPS.

The Invited Speakers are:

        Hamid Bolouri, Institute for Systems Biology, Seattle (US)
        Drew Endy, MIT Biological Engineering (US)
        Stephen Muggleton, Imperial College (UK)
        Vincent Schachter, Genoscope, Evry (FR)
        Christophe Soul?, Centre National de la Recherche Scientifique,
          Institut des Hautes ?tudes Scientifiques (FR)

Titles and abstracts of their talks, as well as those of the contributed
papers and the posters can all be found at the meeting web site:

        http://homepages.inf.ed.ac.uk/v1bklin/cmsb05/

Early registration is advised, particularly as accommodation can become
scarce.

I apologise for multiple announcements.

Gordon Plotkin (Chair, CMSB 2005)

From hanson.char at gmail.com  Thu Feb 17 03:49:07 2005
From: hanson.char at gmail.com (Hanson Char)
Date: Thu Feb 17 03:49:23 2005
Subject: [concurrency-interest] volatile long
Message-ID: <ca53c8f805021700497c3d7fff@mail.gmail.com>

A question on the use of volatile in a concurrent environment. 
Consider the example:

public class  Foo {
    private volatile int count;

     public void run() {
        count++; 
        // ... do some other operations
        count--;
    }
     public int getCount() { return count; }
}

My understanding is:

1) The value returned by getCount() will never be less than zero; and 
2) count will never be in a corrupted state.

Does the above 2 statements still hold if count is type long instead of int ?

Hanson
From jozart at blarg.net  Thu Feb 17 04:10:43 2005
From: jozart at blarg.net (Joe Bowbeer)
Date: Thu Feb 17 04:11:06 2005
Subject: [concurrency-interest] volatile long
References: <ca53c8f805021700497c3d7fff@mail.gmail.com>
Message-ID: <068f01c514d0$903e1cf0$0200a8c0@REPLICANT2>

Even though though ++ and -- are each represented by a single bytecode 
(iinc), I don't think these are atomic operations in terms of the JMM.  That 
is, each is composed of a read followed by a write.  In which case, even an 
"int" count can creep very negative or very positive over time, depending on 
the interleaving of threads.  Use AtomicInteger to prevent counter creep.

Volatile protects long values from word shearing, but it doesn't turn ++ 
or -- into atomic operations either.


----- Original Message ----- 
From: "Hanson Char" <hanson.char@gmail.com>
To: <concurrency-interest@altair.cs.oswego.edu>
Sent: Thursday, February 17, 2005 12:49 AM
Subject: [concurrency-interest] volatile long


A question on the use of volatile in a concurrent environment.
Consider the example:

public class  Foo {
    private volatile int count;

     public void run() {
        count++;
        // ... do some other operations
        count--;
    }
     public int getCount() { return count; }
}

My understanding is:

1) The value returned by getCount() will never be less than zero; and
2) count will never be in a corrupted state.

Does the above 2 statements still hold if count is type long instead of int 
?

Hanson
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From hanson.char at gmail.com  Thu Feb 17 05:05:49 2005
From: hanson.char at gmail.com (Hanson Char)
Date: Thu Feb 17 05:06:01 2005
Subject: [concurrency-interest] volatile long
In-Reply-To: <068f01c514d0$903e1cf0$0200a8c0@REPLICANT2>
References: <ca53c8f805021700497c3d7fff@mail.gmail.com>
	<068f01c514d0$903e1cf0$0200a8c0@REPLICANT2>
Message-ID: <ca53c8f8050217020540141a23@mail.gmail.com>

I just found out from the JLS 2ed, Section 17.7 (p 437):

"The load, store, read and write actions on volatile variables are
atomic, even if the type of the variable is double or long"

Hanson

On Thu, 17 Feb 2005 01:10:43 -0800, Joe Bowbeer <jozart@blarg.net> wrote:
> Even though though ++ and -- are each represented by a single bytecode
> (iinc), I don't think these are atomic operations in terms of the JMM.  That
> is, each is composed of a read followed by a write.  In which case, even an
> "int" count can creep very negative or very positive over time, depending on
> the interleaving of threads.  Use AtomicInteger to prevent counter creep.
> 
> Volatile protects long values from word shearing, but it doesn't turn ++
> or -- into atomic operations either.
> 
> 
> ----- Original Message -----
> From: "Hanson Char" <hanson.char@gmail.com>
> To: <concurrency-interest@altair.cs.oswego.edu>
> Sent: Thursday, February 17, 2005 12:49 AM
> Subject: [concurrency-interest] volatile long
> 
> A question on the use of volatile in a concurrent environment.
> Consider the example:
> 
> public class  Foo {
>     private volatile int count;
> 
>      public void run() {
>         count++;
>         // ... do some other operations
>         count--;
>     }
>      public int getCount() { return count; }
> }
> 
> My understanding is:
> 
> 1) The value returned by getCount() will never be less than zero; and
> 2) count will never be in a corrupted state.
> 
> Does the above 2 statements still hold if count is type long instead of int
> ?
> 
> Hanson
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
>
From hanson.char at gmail.com  Thu Feb 17 05:14:29 2005
From: hanson.char at gmail.com (Hanson Char)
Date: Thu Feb 17 05:14:38 2005
Subject: [concurrency-interest] volatile long
In-Reply-To: <ca53c8f8050217020540141a23@mail.gmail.com>
References: <ca53c8f805021700497c3d7fff@mail.gmail.com>
	<068f01c514d0$903e1cf0$0200a8c0@REPLICANT2>
	<ca53c8f8050217020540141a23@mail.gmail.com>
Message-ID: <ca53c8f805021702147a1ddf17@mail.gmail.com>

JLS 2nd ed Section 8.3.1.4 (p.157):

"A field may be decalred volatile, in which case a thread must
reconcile its working copy of the field with the master copy every
time it accesses the variable.  Moreover, operations on the master
copies of one or more volatile variables on behalf of a thread are
performed by the main memory in exactly the order that the thread
requested."

>From this, it appears that the two statements 

> > 1) The value returned by getCount() will never be less than zero; and
> > 2) count will never be in a corrupted state.

are always true even if count is volatile long or volatile double.

Hanson

On Thu, 17 Feb 2005 21:05:49 +1100, Hanson Char <hanson.char@gmail.com> wrote:
> I just found out from the JLS 2ed, Section 17.7 (p 437):
> 
> "The load, store, read and write actions on volatile variables are
> atomic, even if the type of the variable is double or long"
> 
> Hanson
> 
> On Thu, 17 Feb 2005 01:10:43 -0800, Joe Bowbeer <jozart@blarg.net> wrote:
> > Even though though ++ and -- are each represented by a single bytecode
> > (iinc), I don't think these are atomic operations in terms of the JMM.  That
> > is, each is composed of a read followed by a write.  In which case, even an
> > "int" count can creep very negative or very positive over time, depending on
> > the interleaving of threads.  Use AtomicInteger to prevent counter creep.
> >
> > Volatile protects long values from word shearing, but it doesn't turn ++
> > or -- into atomic operations either.
> >
> >
> > ----- Original Message -----
> > From: "Hanson Char" <hanson.char@gmail.com>
> > To: <concurrency-interest@altair.cs.oswego.edu>
> > Sent: Thursday, February 17, 2005 12:49 AM
> > Subject: [concurrency-interest] volatile long
> >
> > A question on the use of volatile in a concurrent environment.
> > Consider the example:
> >
> > public class  Foo {
> >     private volatile int count;
> >
> >      public void run() {
> >         count++;
> >         // ... do some other operations
> >         count--;
> >     }
> >      public int getCount() { return count; }
> > }
> >
> > My understanding is:
> >
> > 1) The value returned by getCount() will never be less than zero; and
> > 2) count will never be in a corrupted state.
> >
> > Does the above 2 statements still hold if count is type long instead of int
> > ?
> >
> > Hanson
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest@altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
From hanson.char at gmail.com  Thu Feb 17 05:24:21 2005
From: hanson.char at gmail.com (Hanson Char)
Date: Thu Feb 17 05:24:29 2005
Subject: [concurrency-interest] volatile long
In-Reply-To: <068f01c514d0$903e1cf0$0200a8c0@REPLICANT2>
References: <ca53c8f805021700497c3d7fff@mail.gmail.com>
	<068f01c514d0$903e1cf0$0200a8c0@REPLICANT2>
Message-ID: <ca53c8f8050217022473645d09@mail.gmail.com>

> each is composed of a read followed by a write.

I think this is false.  Had the statements been something like:

if (count++) ...

Then it would be a read followed by a write.  But just count++ only
involves a write.

Hanson

On Thu, 17 Feb 2005 01:10:43 -0800, Joe Bowbeer <jozart@blarg.net> wrote:
> Even though though ++ and -- are each represented by a single bytecode
> (iinc), I don't think these are atomic operations in terms of the JMM.  That
> is, each is composed of a read followed by a write.  In which case, even an
> "int" count can creep very negative or very positive over time, depending on
> the interleaving of threads.  Use AtomicInteger to prevent counter creep.
> 
> Volatile protects long values from word shearing, but it doesn't turn ++
> or -- into atomic operations either.
> 
> 
> ----- Original Message -----
> From: "Hanson Char" <hanson.char@gmail.com>
> To: <concurrency-interest@altair.cs.oswego.edu>
> Sent: Thursday, February 17, 2005 12:49 AM
> Subject: [concurrency-interest] volatile long
> 
> A question on the use of volatile in a concurrent environment.
> Consider the example:
> 
> public class  Foo {
>     private volatile int count;
> 
>      public void run() {
>         count++;
>         // ... do some other operations
>         count--;
>     }
>      public int getCount() { return count; }
> }
> 
> My understanding is:
> 
> 1) The value returned by getCount() will never be less than zero; and
> 2) count will never be in a corrupted state.
> 
> Does the above 2 statements still hold if count is type long instead of int
> ?
> 
> Hanson
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
>
From jozart at blarg.net  Thu Feb 17 05:38:41 2005
From: jozart at blarg.net (Joe Bowbeer)
Date: Thu Feb 17 05:38:53 2005
Subject: [concurrency-interest] volatile long
References: <ca53c8f805021700497c3d7fff@mail.gmail.com>
	<068f01c514d0$903e1cf0$0200a8c0@REPLICANT2>
	<ca53c8f8050217020540141a23@mail.gmail.com>
Message-ID: <06dd01c514dc$d921f890$0200a8c0@REPLICANT2>

A similar example is presented in the article "Going Atomic" by JSR-166 EG 
member Brian Goetz:

http://www-106.ibm.com/developerworks/java/library/j-jtp11234/

where it clearly states that declaring the count variable "volatile" will 
not solve the problem.


----- Original Message ----- 
From: "Hanson Char" <hanson.char@gmail.com>
To: "Joe Bowbeer" <jozart@blarg.net>
Cc: <concurrency-interest@altair.cs.oswego.edu>
Sent: Thursday, February 17, 2005 2:05 AM
Subject: Re: [concurrency-interest] volatile long


I just found out from the JLS 2ed, Section 17.7 (p 437):

"The load, store, read and write actions on volatile variables are
atomic, even if the type of the variable is double or long"

Hanson

On Thu, 17 Feb 2005 01:10:43 -0800, Joe Bowbeer <jozart@blarg.net> wrote:
> Even though though ++ and -- are each represented by a single bytecode
> (iinc), I don't think these are atomic operations in terms of the JMM. 
> That
> is, each is composed of a read followed by a write.  In which case, even 
> an
> "int" count can creep very negative or very positive over time, depending 
> on
> the interleaving of threads.  Use AtomicInteger to prevent counter creep.
>
> Volatile protects long values from word shearing, but it doesn't turn ++
> or -- into atomic operations either.
>
>
> ----- Original Message -----
> From: "Hanson Char" <hanson.char@gmail.com>
> To: <concurrency-interest@altair.cs.oswego.edu>
> Sent: Thursday, February 17, 2005 12:49 AM
> Subject: [concurrency-interest] volatile long
>
> A question on the use of volatile in a concurrent environment.
> Consider the example:
>
> public class  Foo {
>     private volatile int count;
>
>      public void run() {
>         count++;
>         // ... do some other operations
>         count--;
>     }
>      public int getCount() { return count; }
> }
>
> My understanding is:
>
> 1) The value returned by getCount() will never be less than zero; and
> 2) count will never be in a corrupted state.
>
> Does the above 2 statements still hold if count is type long instead of 
> int
> ?
>
> Hanson

From hanson.char at gmail.com  Thu Feb 17 06:30:21 2005
From: hanson.char at gmail.com (Hanson Char)
Date: Thu Feb 17 06:30:30 2005
Subject: [concurrency-interest] volatile long
In-Reply-To: <06dd01c514dc$d921f890$0200a8c0@REPLICANT2>
References: <ca53c8f805021700497c3d7fff@mail.gmail.com>
	<068f01c514d0$903e1cf0$0200a8c0@REPLICANT2>
	<ca53c8f8050217020540141a23@mail.gmail.com>
	<06dd01c514dc$d921f890$0200a8c0@REPLICANT2>
Message-ID: <ca53c8f80502170330eb401d2@mail.gmail.com>

Thanks Joe, I did some experiment and the result is on your side.  The
++count is a read-modify-write operation, not a pure write operation. 
So volatile will never work with such construct.

Hanson

On Thu, 17 Feb 2005 02:38:41 -0800, Joe Bowbeer <jozart@blarg.net> wrote:
> A similar example is presented in the article "Going Atomic" by JSR-166 EG
> member Brian Goetz:
> 
> http://www-106.ibm.com/developerworks/java/library/j-jtp11234/
> 
> where it clearly states that declaring the count variable "volatile" will
> not solve the problem.
> 
> 
> ----- Original Message -----
> From: "Hanson Char" <hanson.char@gmail.com>
> To: "Joe Bowbeer" <jozart@blarg.net>
> Cc: <concurrency-interest@altair.cs.oswego.edu>
> Sent: Thursday, February 17, 2005 2:05 AM
> Subject: Re: [concurrency-interest] volatile long
> 
> I just found out from the JLS 2ed, Section 17.7 (p 437):
> 
> "The load, store, read and write actions on volatile variables are
> atomic, even if the type of the variable is double or long"
> 
> Hanson
> 
> On Thu, 17 Feb 2005 01:10:43 -0800, Joe Bowbeer <jozart@blarg.net> wrote:
> > Even though though ++ and -- are each represented by a single bytecode
> > (iinc), I don't think these are atomic operations in terms of the JMM.
> > That
> > is, each is composed of a read followed by a write.  In which case, even
> > an
> > "int" count can creep very negative or very positive over time, depending
> > on
> > the interleaving of threads.  Use AtomicInteger to prevent counter creep.
> >
> > Volatile protects long values from word shearing, but it doesn't turn ++
> > or -- into atomic operations either.
> >
> >
> > ----- Original Message -----
> > From: "Hanson Char" <hanson.char@gmail.com>
> > To: <concurrency-interest@altair.cs.oswego.edu>
> > Sent: Thursday, February 17, 2005 12:49 AM
> > Subject: [concurrency-interest] volatile long
> >
> > A question on the use of volatile in a concurrent environment.
> > Consider the example:
> >
> > public class  Foo {
> >     private volatile int count;
> >
> >      public void run() {
> >         count++;
> >         // ... do some other operations
> >         count--;
> >     }
> >      public int getCount() { return count; }
> > }
> >
> > My understanding is:
> >
> > 1) The value returned by getCount() will never be less than zero; and
> > 2) count will never be in a corrupted state.
> >
> > Does the above 2 statements still hold if count is type long instead of
> > int
> > ?
> >
> > Hanson
> 
>
From Victor.Luchangco at Sun.COM  Thu Feb 17 09:06:00 2005
From: Victor.Luchangco at Sun.COM (Victor Luchangco)
Date: Thu Feb 17 09:06:05 2005
Subject: [concurrency-interest] volatile long
In-Reply-To: <ca53c8f80502170330eb401d2@mail.gmail.com>
References: <ca53c8f805021700497c3d7fff@mail.gmail.com>
	<068f01c514d0$903e1cf0$0200a8c0@REPLICANT2>
	<ca53c8f8050217020540141a23@mail.gmail.com>
	<06dd01c514dc$d921f890$0200a8c0@REPLICANT2>
	<ca53c8f80502170330eb401d2@mail.gmail.com>
Message-ID: <4214A4C8.5080709@Sun.COM>

Hanson Char wrote:
> Thanks Joe, I did some experiment and the result is on your side.  The
> ++count is a read-modify-write operation, not a pure write operation. 
> So volatile will never work with such construct.
> 
> Hanson

Yup, volatile only makes the reads and writes atomic, not combinations
of reads and writes.  And ++ is definitely a read and a write (how else
will it know the value to increment?).  In case you're interested,
here's an example execution of how you can get count to be negative:

count initially 0

Thread 1                       Thread 2

read count (0)
                               read count (0)
                               write count = 1
write count = 1
read count (1)
write count = 0
                               read count (0)
                               write count = -1


- Victor

From dholmes at dltech.com.au  Thu Feb 17 18:22:46 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Thu Feb 17 18:23:24 2005
Subject: [concurrency-interest] volatile long
In-Reply-To: <ca53c8f80502170330eb401d2@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEBCFGAA.dholmes@dltech.com.au>

I'd just like to reinforce the message here as this is the second case of
this particular misunderstanding that has arisen in recent time.

The basic rules are:

1. reading or writing all 32-bit values is atomic
2. reading or writing all volatile variables is atomic
3. reading/writing volatile variables establishes specific happens-before
relationships in the memory model

volatile does not make any other operations other than a simple read or a
simple write, atomic.

While statements like v++ or --v could be implemented by a particular JVM on
a particular architecture using a single atomic update instruction, the
language itself provides no such guarantee. Indeed the semantics of ++/--
are that expressions using them yield a value and they are defined in terms
of a read-modify-write sequence. It would be a potential optimisation, in
the case where the value is not used, to perform an atomically updating
instruction, but not an optimisation that a program should ever rely upon.
If you always think of v++/v-- as syntactic shorthand for v=v+1, v=v-1 then
the read-modify-write semantics are more obvious.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces@cs.oswego.edu
> [mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of Hanson
> Char
> Sent: Thursday, 17 February 2005 9:30 PM
> To: Joe Bowbeer
> Cc: concurrency-interest@altair.cs.oswego.edu
> Subject: Re: [concurrency-interest] volatile long
>
>
> Thanks Joe, I did some experiment and the result is on your side.  The
> ++count is a read-modify-write operation, not a pure write operation.
> So volatile will never work with such construct.
>
> Hanson
>
> On Thu, 17 Feb 2005 02:38:41 -0800, Joe Bowbeer <jozart@blarg.net> wrote:
> > A similar example is presented in the article "Going Atomic" by
> JSR-166 EG
> > member Brian Goetz:
> >
> > http://www-106.ibm.com/developerworks/java/library/j-jtp11234/
> >
> > where it clearly states that declaring the count variable
> "volatile" will
> > not solve the problem.
> >
> >
> > ----- Original Message -----
> > From: "Hanson Char" <hanson.char@gmail.com>
> > To: "Joe Bowbeer" <jozart@blarg.net>
> > Cc: <concurrency-interest@altair.cs.oswego.edu>
> > Sent: Thursday, February 17, 2005 2:05 AM
> > Subject: Re: [concurrency-interest] volatile long
> >
> > I just found out from the JLS 2ed, Section 17.7 (p 437):
> >
> > "The load, store, read and write actions on volatile variables are
> > atomic, even if the type of the variable is double or long"
> >
> > Hanson
> >
> > On Thu, 17 Feb 2005 01:10:43 -0800, Joe Bowbeer
> <jozart@blarg.net> wrote:
> > > Even though though ++ and -- are each represented by a single bytecode
> > > (iinc), I don't think these are atomic operations in terms of the JMM.
> > > That
> > > is, each is composed of a read followed by a write.  In which
> case, even
> > > an
> > > "int" count can creep very negative or very positive over
> time, depending
> > > on
> > > the interleaving of threads.  Use AtomicInteger to prevent
> counter creep.
> > >
> > > Volatile protects long values from word shearing, but it
> doesn't turn ++
> > > or -- into atomic operations either.
> > >
> > >
> > > ----- Original Message -----
> > > From: "Hanson Char" <hanson.char@gmail.com>
> > > To: <concurrency-interest@altair.cs.oswego.edu>
> > > Sent: Thursday, February 17, 2005 12:49 AM
> > > Subject: [concurrency-interest] volatile long
> > >
> > > A question on the use of volatile in a concurrent environment.
> > > Consider the example:
> > >
> > > public class  Foo {
> > >     private volatile int count;
> > >
> > >      public void run() {
> > >         count++;
> > >         // ... do some other operations
> > >         count--;
> > >     }
> > >      public int getCount() { return count; }
> > > }
> > >
> > > My understanding is:
> > >
> > > 1) The value returned by getCount() will never be less than zero; and
> > > 2) count will never be in a corrupted state.
> > >
> > > Does the above 2 statements still hold if count is type long
> instead of
> > > int
> > > ?
> > >
> > > Hanson
> >
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From matthias.ernst at coremedia.com  Thu Feb 24 10:53:45 2005
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Thu Feb 24 10:53:53 2005
Subject: [concurrency-interest] Why is ReentrantLock faster than
	synchronized ?
Message-ID: <F34C8A704C489B46B9E9FBDBD1B91D5FC04416@MARS.coremedia.com>

Hi,

a recent article by Brian Goetz made me wonder about this again. Brian
demonstrates, how ReentrantLock has a performance and scalability
advantage over synchronized.

Did anyone investigate why that would be the case? Is it due to the fact
that the VM has lock-enable *any* java.lang.Object through things like
header displacement, lock inflation/deflation? Or has GC an advantage
over manual management of lock records?

Other than that I cannot think of any edge ReentrantLock could have over
synchronized: both codes are generated inline by the Hotspot compiler,
both can and probably do use the same suspension/resumption methods, the
same atomic instruction sequences, ...

Just wondering
Matthias
-- 
Meet us at CeBIT: Launching CoreMedia CMS 2005, Hall 3, D09. 
 

From dl at cs.oswego.edu  Thu Feb 24 11:45:31 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu Feb 24 11:45:32 2005
Subject: [concurrency-interest] Why is ReentrantLock faster
	than	synchronized ?
In-Reply-To: <F34C8A704C489B46B9E9FBDBD1B91D5FC04416@MARS.coremedia.com>
References: <F34C8A704C489B46B9E9FBDBD1B91D5FC04416@MARS.coremedia.com>
Message-ID: <421E04AB.4060009@cs.oswego.edu>


> 
> a recent article by Brian Goetz made me wonder about this again. Brian
> demonstrates, how ReentrantLock has a performance and scalability
> advantage over synchronized.
> 
> Did anyone investigate why that would be the case? Is it due to the fact
> that the VM has lock-enable *any* java.lang.Object through things like
> header displacement, lock inflation/deflation? Or has GC an advantage
> over manual management of lock records?
> 


There's some friendly competition among those doing Java-level sync
vs VM-level sync. The underlying algorithms are increasingly
pretty similar. So you should expect relative performance differences
in typical server applications to fluctuate across releases. The main
goal of ReentrantLock is NOT to uniformly replace builtin sync, but to
offer greater flexibility and capabilities when you need them, and to
maintain good performance in those kinds of applications.

The people doing VM-level sync also have to concentrate on issues that
we don't with ReentrantLock, like the need to use only a few bits of
object header space to avoid bloat in objects that are never
locked. This impacts impmentation details allowing ReentrantLock to
sometimes require a few cycles less overhead.

Also VM-levl support must deal with the fact that many
programs/classes do a lot of sync that is entirely useless because it
can never be contended. (We assume people hardly ever do this with
ReentrantLock so don't do anything special about it.) Techniques to
greatly cheapen this case are probably coming soon in hotspot and
other VMs. (This case is already pretty cheap on uniprocessors, but
not multiprocessors.)

There are currently still a few things that can be done at JVM level
that we can't do at Java level. For example, adapting spinning to vary
with load averages. We're working on leveling the playing field here
though :-)

-Doug
From brian at quiotix.com  Thu Feb 24 17:08:43 2005
From: brian at quiotix.com (Brian Goetz)
Date: Thu Feb 24 17:09:04 2005
Subject: [concurrency-interest] Why is ReentrantLock faster
	than	synchronized ?
In-Reply-To: <F34C8A704C489B46B9E9FBDBD1B91D5FC04416@MARS.coremedia.com>
References: <F34C8A704C489B46B9E9FBDBD1B91D5FC04416@MARS.coremedia.com>
Message-ID: <421E506B.4060605@quiotix.com>

> Did anyone investigate why that would be the case? Is it due to the fact
> that the VM has lock-enable *any* java.lang.Object through things like
> header displacement, lock inflation/deflation? Or has GC an advantage
> over manual management of lock records?
> 
> Other than that I cannot think of any edge ReentrantLock could have over
> synchronized: both codes are generated inline by the Hotspot compiler,
> both can and probably do use the same suspension/resumption methods, the
> same atomic instruction sequences, ...

Performance is a moving target.  In the first JVM, performance for 
everything sucked (locking, garbage collection, allocation, you name it) 
because the first JVM was a proof-of-concept and performance wasn't the 
goal.  Once the VM concept was proven, engineering resources were then 
allocated to improve performance, and there is no shortage of good ideas 
for making things faster, so performance in these areas improved and is 
improving with each JVM version.

So, one factor in why ReentrantLock is faster than built-in 
synchronization is that the JSR 166 team spent some effort building a 
better lock -- not because the JVM folks didn't have access to the same 
papers on lock performance, but because they had other priorities of 
where to spend their efforts.  But they will get around to it and the 
scalability gap will surely close in future JVM versions.

Interestingly, the algorithm used under the hood of ReentrantLock is 
easier to implement in Java than in C, because of garbage collection -- 
a C version of the same algorithm would be a lot more work and would 
require more bookkeeping in the algorithm.  As a result, the approach 
taken by ReentrantLock makes more garbage and uses less locking than the 
obvious C analogue, and it turns out that, given the current relative 
cost between memory management and memory synchronization, an algorithm 
that makes more garbage and uses less coordination is more scalable. 
This week.  Might be different next week.  Performance is a moving target.


From minnieh at corp.earthlink.net  Fri Feb 25 23:07:06 2005
From: minnieh at corp.earthlink.net (Minnie Haridasa)
Date: Fri Feb 25 23:07:41 2005
Subject: [concurrency-interest] How do I wait for the thread from the
	ThreadPool to finish?
Message-ID: <000401c51bb8$a27339c0$3124500a@sjclap022925>

 

Hi.

 

I am trying to use the PooledExecutor to create a thread pool. Once I create
my runnable task that needs to be executed, how does the calling thread wait
for the thread in the pool to finish, especially since I do not have the
handle to the thread that is executing the task.

 

threadPool.execute(task);

// At this point the calling thread needs to wait for the task to complete.

 

 

I am quite new to this Library and would appreciate your help.

Thanks

MH

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20050225/37eeef3f/attachment.htm
From dawidk at mathcs.emory.edu  Fri Feb 25 23:25:49 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Fri Feb 25 23:26:05 2005
Subject: [concurrency-interest] How do I wait for the thread from the
	ThreadPool to finish?
In-Reply-To: <000401c51bb8$a27339c0$3124500a@sjclap022925>
References: <000401c51bb8$a27339c0$3124500a@sjclap022925>
Message-ID: <421FFA4D.7010902@mathcs.emory.edu>

Minnie Haridasa wrote:

>  
>
> Hi.
>
>  
>
> I am trying to use the PooledExecutor to create a thread pool. Once I 
> create my runnable task that needs to be executed, how does the 
> calling thread wait for the thread in the pool to finish, especially 
> since I do not have the handle to the thread that is executing the task.
>
>  
>
> threadPool.execute(task);
>
> // At this point the calling thread needs to wait for the task to 
> complete.
>
>  
>
>
Use the submit method (in ExecutorService interface):

Future f = executor.submit(runnable, null);

then you can wait using f.get().


(You can also explicitly wrap your runnable into callable before 
submitting: Executors.callable(runnable)).

Regards,
Dawid

From jozart at blarg.net  Fri Feb 25 23:27:25 2005
From: jozart at blarg.net (Joe Bowbeer)
Date: Fri Feb 25 23:27:45 2005
Subject: [concurrency-interest] How do I wait for the thread from
	theThreadPool to finish?
References: <000401c51bb8$a27339c0$3124500a@sjclap022925>
Message-ID: <010601c51bbb$79364040$0200a8c0@REPLICANT2>

MH asks:

> Once I create my runnable task that needs to be executed,
> how does the calling thread wait for the [task] to finish

See ExecutorCompletionService

completionService.submit(task, null).take();



----- Original Message ----- 
From: "Minnie Haridasa" <minnieh@corp.earthlink.net>
To: <concurrency-interest@altair.cs.oswego.edu>
Sent: Friday, February 25, 2005 8:07 PM
Subject: [concurrency-interest] How do I wait for the thread from 
theThreadPool to finish?


I am trying to use the PooledExecutor to create a thread pool. Once I create
my runnable task that needs to be executed, how does the calling thread wait
for the thread in the pool to finish, especially since I do not have the
handle to the thread that is executing the task.

threadPool.execute(task);

// At this point the calling thread needs to wait for the task to complete.

I am quite new to this Library and would appreciate your help.

Thanks

MH

From jozart at blarg.net  Fri Feb 25 23:30:15 2005
From: jozart at blarg.net (Joe Bowbeer)
Date: Fri Feb 25 23:30:26 2005
Subject: [concurrency-interest] How do I wait for the thread from
	theThreadPool to finish?
Message-ID: <011d01c51bbb$deee4a40$0200a8c0@REPLICANT2>

Correction:

completionService.submit(task, null).get();


-or-

completionService.submit(task, null);

completionService.take();



----- Original Message ----- 
From: "Joe Bowbeer" <jozart@blarg.net>
To: <concurrency-interest@altair.cs.oswego.edu>
Sent: Friday, February 25, 2005 8:27 PM
Subject: Re: [concurrency-interest] How do I wait for the thread from 
theThreadPool to finish?


MH asks:

> Once I create my runnable task that needs to be executed,
> how does the calling thread wait for the [task] to finish

See ExecutorCompletionService

completionService.submit(task, null).take();



----- Original Message ----- 
From: "Minnie Haridasa" <minnieh@corp.earthlink.net>
To: <concurrency-interest@altair.cs.oswego.edu>
Sent: Friday, February 25, 2005 8:07 PM
Subject: [concurrency-interest] How do I wait for the thread from
theThreadPool to finish?


I am trying to use the PooledExecutor to create a thread pool. Once I create
my runnable task that needs to be executed, how does the calling thread wait
for the thread in the pool to finish, especially since I do not have the
handle to the thread that is executing the task.

threadPool.execute(task);

// At this point the calling thread needs to wait for the task to complete.

I am quite new to this Library and would appreciate your help.

Thanks

MH

From holger at wizards.de  Sun Feb 27 16:23:44 2005
From: holger at wizards.de (Holger Hoffstaette)
Date: Sun Feb 27 16:37:58 2005
Subject: [concurrency-interest] backport-concurrent vs. concurrent-1.3.4
Message-ID: <pan.2005.02.27.21.23.43.498000@wizards.de>


Hi -

I've been using util.concurrent-1.3.4 for some time and recently
discovered the backport-effort. Just as in JDK 5.0 some rather handy
classes are missing compared to concurrent-1.3.4 and I was wondering if
there are any recommendations for or against either one? I very much like
the stress-testing and the backport's obvious ease of forward-migration to
5.0 (if that becomes a requirement one day) but so far had nothing but the
best experiences with Doug Lea's classes. Also, quite a few OpenSource
packages have adopted concurrent-1.3.4 and just mixing things - while
technically possible - is IMHO not desirable. Any wise words for or
against  either one? The backport docs unfortunately do not talk about
this (or did I miss it?)

Thanks for any insights -
Holger


From mclain at halcyon.com  Sun Feb 27 16:55:17 2005
From: mclain at halcyon.com (Fred McLain)
Date: Sun Feb 27 16:55:23 2005
Subject: [concurrency-interest] Threads best practices books/sites?
Message-ID: <1109541317.6724.19.camel@glowtoy.zipcon.net>

Hey all,

I'm working on a fairly large (~1M LOC) project that far too many
threads (30+).  We're running into an occasional deadlock situation and
although I'm fairly competent in the basics (double checked locking,
groups etc) I'm on the lookout for some books and or sites describing
best practices for dealing with multi threaded Java applications.  I'm
also interested in hearing about what tooling you've used to resolve or
debug deadlocks, Eclipse just doesn't seem to be up to the task.  Most
of our threading issues have to deal with device interfaces server side.

Any recommendations would be most welcome.

Thanks,

        -Fred-



From dawidk at mathcs.emory.edu  Mon Feb 28 02:23:55 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Mon Feb 28 02:24:31 2005
Subject: [concurrency-interest] backport-concurrent vs. concurrent-1.3.4
In-Reply-To: <pan.2005.02.27.21.23.43.498000@wizards.de>
References: <pan.2005.02.27.21.23.43.498000@wizards.de>
Message-ID: <4222C70B.2040100@mathcs.emory.edu>

Holger Hoffstaette wrote:

>Hi -
>
>I've been using util.concurrent-1.3.4 for some time and recently
>discovered the backport-effort. Just as in JDK 5.0 some rather handy
>classes are missing compared to concurrent-1.3.4 and I was wondering if
>there are any recommendations for or against either one? I very much like
>the stress-testing and the backport's obvious ease of forward-migration to
>5.0 (if that becomes a requirement one day) but so far had nothing but the
>best experiences with Doug Lea's classes. Also, quite a few OpenSource
>packages have adopted concurrent-1.3.4 and just mixing things - while
>technically possible - is IMHO not desirable. Any wise words for or
>against  either one? The backport docs unfortunately do not talk about
>this (or did I miss it?)
>
>  
>
Hi Holger,

Well, you must decide what is best for your project, but here is a 
couple of pros and cons, the way I see it:

Against:

* well, stating the obvious: any code reengineering = costs of 
development, testing, and a risk of introducing new bugs into the 
software. If you have a perfectly stable software that is used in 
production environments, migration can only be justified if it offers 
benefits that outweight the costs.

* if your software depends on external libraries using dl.u.c., as you 
pointed out, mixing can be painful. (BTW, interesting project idea would 
be to develop adapter classes allowing to expose java.util.concurrent 
(and backport) classes to dl.u.c.-dependent code).

* as you pointed out, java.util.concurrent, as well as the backport, 
misses a few useful classes found in dl.u.c. If you cannot live without 
them, it may be better to wait until the functionality is supported.

* some people say that since you have to pay the migration costs anyway, 
it is better to do it all in one shot when switching to Java 5.0.

* dl.u.c. will probably work as well on future Java platforms as it 
works on current ones.

OK, now pros. First, why switching to java.util.concurrent in the long run:

* Again, stating the obvious: in the perspective of a few years, 
domination of java.util.concurrent above any other Java concurrency API 
seems inevitable. (example: see how java.util collections dominated 
preceding collection libraries).

* Even though dl.u.c. works on Java 5.0, locks in java.util.concurrent 
are  much faster due to the JVM support. In a recent mailing, Matthias 
Ernst quotes Brian Goetz reporting that java.u.c.ReentrantLock may 
outperform JVM monitors ("synchronized"). In comparison, the dl.u.c. 
(and also the backport) version measures a few times slower. Obviously, 
this means a strong motivation to migrate for applications that are 
sensitive to this.

* True, dl.u.c. still has some functionality that is not supported in 
java.u.c. But these gaps are gradually filled out. On the other hand, 
java.u.c. (and the backport) has a lot of new and useful stuff that is 
not in dl.u.c. And since dl.u.c. is no longer actively developed, the 
time will work in favor of java.u.c.

* Even though the APIs are a bit different, the principles are similar, 
making the migration relatively painless in most cases, at least from my 
experience. And as long as you don't use something dl.u.c.-exclusive.

OK, now motivating the backport:

* In many cases you cannot control the JVM at the user's side. 
Therefore, you often need to target multiple Java versions. Backport 
seems to be the easiest way to simultaneously support Java 5.0 and 1.4 
while 1) exploiting the performance advantages on 5.0 and 2) keeping the 
differences between versions as small as possible (maybe even 
automatically-generated).

* backport gives you a standard API for a nanosecond-precision timer. On 
1.4.2, out-of-the-box, its resolution will be between a few hundred ns 
and 1 us. On earlier platforms, it will gracefully degrade to 
System.currentTimeMillis. And it allows you to plug in your own native 
timer implementation.

* Switching early (before ultimately committing to Java 5.0) means that 
you are no longer developing new code that will later require migration.

* If you've come to trust Doug Lea's code, you will like the backport 
since it is mostly Doug Lea's code in one way or the other. Locks come 
directly from dl.u.c. 1.3.4, and most other things from 
java.util.concurrent. The last available version, 1.1_01, is having so 
far about 13 downloads per day on average since the release, and no bug 
reports so far (I keep fingers crossed).


Hope that helps, but of course I am biased -
Dawid Kurzyniec


From Rahul.Joshi at FMR.COM  Mon Feb 28 11:08:38 2005
From: Rahul.Joshi at FMR.COM (Joshi, Rahul)
Date: Mon Feb 28 11:08:49 2005
Subject: [concurrency-interest] Questions about ThreadPoolExecutor and
	possible loss of 'command'
Message-ID: <42BBD772AC30EA428B057864E203C999014F9269@MSGBOSCLF2WIN.DMN1.FMR.COM>

Hi,
I have couple of questions on ThreadPoolExecutor

Per documentation, ThreadPoolExecutor will not create new threads if
corePoolSize threads are running instead new tasks are queued in the
workQueue. Then there is no usage of 'maximumPoolSize'. 

Typically an application will like to use max threads and if all max
threads are busy then queue task (in unbounded queue) instead of
rejecting it. Application developers will know max concurrency possible
on their environment hence they would set maximumPoolSize accordingly.
But it will never be used in this case. 

Can I get such behavior from ThreadPoolExecutor? I do not want to use
bounded work queue.


The other question is on 'execute' method implementation
Following is the code snippet from the backport code

    public void execute(Runnable command) {
        if (command == null)
            throw new NullPointerException();
        for (;;) {
            if (runState != RUNNING) {
                reject(command);
                return;
            }
            if (poolSize < corePoolSize &&
addIfUnderCorePoolSize(command))
                return;
            if (workQueue.offer(command))
                return;
            Runnable r = addIfUnderMaximumPoolSize(command);
            if (r == command)
                return;
            if (r == null) {
                reject(command);
                return;
            }
            // else retry
        }
    }


'addIfUnderMaximumPoolSize' is called only when the workQueue is FULL.
It is called with 'command' as a parameter. Now lets look at
'addIfUnderMaximumPoolSize' method implementation as follows

    private Runnable addIfUnderMaximumPoolSize(Runnable firstTask) {
        Thread t = null;
        Runnable next = null;
        final ReentrantLock mainLock = this.mainLock;
        mainLock.lock();
        try {
            if (poolSize < maximumPoolSize) {
                next = (Runnable)workQueue.poll();
                if (next == null)
                    next = firstTask;
                t = addThread(next);
            }
        } finally {
            mainLock.unlock();
        }
        if (t == null)
            return null;
        t.start();
        return next;
    }

The method first polls the queue and as queue is FULL, it will read a
runnable task from the queue. Hence next will never be null. In this
case the method will never use 'firstTask' and it will return next which
is a runnable from the queue.

Now back in the caller i.e. execute, r will never be same as command and
it will never be null, hence no error is returned but 'command' task is
neither in the queue nor as the first task of the new thread. There is a
possibility that task 'command' is lost and is never executed.

Am I missing something here. Can someone help me understand this better?
Prompt help would be appreciated.

Thanks,
Rahul

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20050228/5f201294/attachment.htm
From dl at cs.oswego.edu  Mon Feb 28 11:26:55 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon Feb 28 11:26:57 2005
Subject: [concurrency-interest] Questions about ThreadPoolExecutor and
	possible loss of 'command'
In-Reply-To: <42BBD772AC30EA428B057864E203C999014F9269@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <42BBD772AC30EA428B057864E203C999014F9269@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <4223464F.10409@cs.oswego.edu>


> Typically an application will like to use max threads and if all max
>  threads are busy then queue task (in unbounded queue) instead of 
> rejecting it.


> Can I get such behavior from ThreadPoolExecutor?

That's what  Executors.newFixedThreadPool gives you.

Sorry you found the terminology of core vs max threads confusing.
It's the best we could come up with.


-Doug
From Rahul.Joshi at FMR.COM  Mon Feb 28 11:42:19 2005
From: Rahul.Joshi at FMR.COM (Joshi, Rahul)
Date: Mon Feb 28 11:42:30 2005
Subject: [concurrency-interest] Questions about ThreadPoolExecutor and
	possible loss of 'command'
Message-ID: <42BBD772AC30EA428B057864E203C999014F926A@MSGBOSCLF2WIN.DMN1.FMR.COM>

Hi,
Thanks for a prompt reply. 
I guess my first question was not very clear.

As an application developer, I would like to take advantage of the
coreSize, maxSize as well as thread idle timeout. I would like to see a
thread pool which grows up to maxSize on a load and shrinks to coreSize
on minimum / no load conditions. This way I can utilize the pool to the
maximum extend in the load conditions.

As 'addIfUnderMaximumPoolSize' is private I can not overload 'execute'
method. How do I implement the pool as explained above.

Also can you answer my second query about possible loss of 'command'?

Thanks,
Rahul


-----Original Message-----
From: Doug Lea [mailto:dl@cs.oswego.edu] 
Sent: Monday, February 28, 2005 11:27 AM
To: Joshi, Rahul
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Questions about ThreadPoolExecutor
and possible loss of 'command'



> Typically an application will like to use max threads and if all max
>  threads are busy then queue task (in unbounded queue) instead of 
> rejecting it.


> Can I get such behavior from ThreadPoolExecutor?

That's what  Executors.newFixedThreadPool gives you.

Sorry you found the terminology of core vs max threads confusing.
It's the best we could come up with.


-Doug

From dl at cs.oswego.edu  Mon Feb 28 12:02:14 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon Feb 28 12:02:15 2005
Subject: [concurrency-interest] Questions about ThreadPoolExecutor and
	possible loss of 'command'
In-Reply-To: <42BBD772AC30EA428B057864E203C999014F926A@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <42BBD772AC30EA428B057864E203C999014F926A@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <42234E96.7080802@cs.oswego.edu>

Joshi, Rahul wrote:

> 
> As 'addIfUnderMaximumPoolSize' is private I can not overload 'execute'
> method. How do I implement the pool as explained above.

One way to do it is to override beforeExecute and/or afterExecute
to dynamically change core, max, or timeout settings. You can
do something similar with a RejectedExecutionHandler.

Our experience though is that either you want to unboundedly add threads
(i.e., use a bounded queue but unbounded max) or unboundedly add queued
tasks (i.e., use an unbounded queue and bounded max). Other combinations
tend to be less well-behaved.

> 
> Also can you answer my second query about possible loss of 'command'?

Notice that the code is in a loop, so it is not lost; instead everything
is retried.

-Doug

