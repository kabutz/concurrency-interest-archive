From travis at tabbal.net  Wed Nov  1 15:51:57 2006
From: travis at tabbal.net (Travis Tabbal)
Date: Wed, 1 Nov 2006 13:51:57 -0700
Subject: [concurrency-interest] scheduleWithFixedDelay and exceptions
Message-ID: <391cfb8a0611011251j58bb2ba5vdf2ffec7e8bad48c@mail.gmail.com>

The JavaDoc says:

"If any execution of the task encounters an exception, subsequent
executions are suppressed."

That tells me that the periodic task will stop running if it gets an
exception. That's a reasonable thing to do, but how do I check to see
if this happened? I'm getting the executor by calling
Executors.newSingleThreadScheduledExecutor(); ...

I want to be sure I can insert some error handling in the process
should one of them throw an exception.

Thanks!

Travis Tabbal

From tim at peierls.net  Thu Nov  2 10:02:50 2006
From: tim at peierls.net (Tim Peierls)
Date: Thu, 2 Nov 2006 10:02:50 -0500
Subject: [concurrency-interest] scheduleWithFixedDelay and exceptions
In-Reply-To: <391cfb8a0611011251j58bb2ba5vdf2ffec7e8bad48c@mail.gmail.com>
References: <391cfb8a0611011251j58bb2ba5vdf2ffec7e8bad48c@mail.gmail.com>
Message-ID: <63b4e4050611020702n11988a61m7c0b3844ef18b4bd@mail.gmail.com>

On 11/1/06, Travis Tabbal <travis at tabbal.net> wrote:
>
> The JavaDoc says:
> "If any execution of the task encounters an exception, subsequent
> executions are suppressed."
>
> That tells me that the periodic task will stop running if it gets an
> exception. That's a reasonable thing to do, but how do I check to see if
> this happened? I'm getting the executor by calling
> Executors.newSingleThreadScheduledExecutor(); ...


Use the isDone() method of the ScheduledFuture returned by
ScheduledExecutorService.scheduleAtFixedRate and
ScheduledExecutorService.scheduleWithFixedDelay. It returns true if the task
is no longer running (because it threw an exception or was cancelled).

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061102/b0ceff4f/attachment.html 

From amar_nan at yahoo.com  Thu Nov  2 11:02:14 2006
From: amar_nan at yahoo.com (Amarnath Nanduri)
Date: Thu, 2 Nov 2006 08:02:14 -0800 (PST)
Subject: [concurrency-interest] question on concurrent HashMap
Message-ID: <20061102160214.1644.qmail@web31206.mail.mud.yahoo.com>

I am currently using a concurrent hashmap to act as shared memory between different threads. I think I might be doing something wrong and would appreciate your help and guidance on this.
   
  The scenario is:
  A manager class fires up a bunch of database objects (implementing the Callable interface)  every one minute (in a Timer task) and gets back the immutable data objects. 
  //sample code
  ExecutorService.invokeAll(database objects implementing Callable interface).
   
  The manager then updates a concurrent HashMap with these data objects. 
   
  The gui components request the manager for the concurrent HashMap and each gui component asks for its own data object out of the cncurrent hash map. All gui components are in a single swing timer that fires up an event once every minute and asks the gui components to get their data objects. 
   
  Both Timer Task and Swing Timer are fired every one minute. I currently guess the amount of time delay I need (set to 30 seconds)  (to get the data from the database and populate the concurrent hash map) before the swing timer fires an event for the gui components to fetch from the concurrent hash map.
   
  I have a feeling that I am doing something wrong here, but can't pinpoint the exact cause. One question that bothers me is that how can I make the Timer Task (in manager) and Swing Timer (controlling the gui components) be in step. i.e only after the concurrent hashmap is populated by the manager, do I want the swing timer firing up to allow the gui components to get their data.
   
  Is there a way for the gui components to wait till their data object is put in the concurrent hash map, before they attempt to retreive the data from the concurrent hashmap?
   
  Thank you for your time and patience.
   
  Regards,
  Amar..

 	
---------------------------------
Everyone is raving about the  all-new Yahoo! Mail.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061102/57bf0974/attachment.html 

From joe.bowbeer at gmail.com  Thu Nov  2 20:09:42 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 2 Nov 2006 17:09:42 -0800
Subject: [concurrency-interest] question on concurrent HashMap
In-Reply-To: <20061102160214.1644.qmail@web31206.mail.mud.yahoo.com>
References: <20061102160214.1644.qmail@web31206.mail.mud.yahoo.com>
Message-ID: <31f2a7bd0611021709i615e260cj1218ce91be8ada9a@mail.gmail.com>

On 11/2/06, Amarnath Nanduri <amar_nan at yahoo.com> wrote:
> [...]
> One question that bothers me is that how can I make the
> Timer Task (in manager) and Swing Timer (controlling the gui
> components) be in step. i.e only after the concurrent hashmap
> is populated by the manager, do I want the swing timer firing up
> to allow the gui components to get their data.
>

The timer task in manager can schedule a task on Swing's event thread
using EventQueue.invokeLater (aka SwingUtilities.invokeLater).  This
is the mechanism employed by SwingWorker.  In your case, the task
would query the cache for the latest results and update the UI.

If the contents of the cache are interrelated then you may need to
employ a split-model technique so that your UI depicts a valid
instance of the cached state - and not some transitional state.

More:

SwingWorker is reconstructed in JCiP.

The second example in my Swing Threads article uses the split-model technique:

http://java.sun.com/products/jfc/tsc/articles/threads/threads3.html


> Is there a way for the gui components to wait till their data object
> is put in the concurrent hash map, before they attempt to retrieve
> the data from the concurrent hashmap?
>

Not on the Swing GUI side - without blocking the event thread.
Polling in the event thread (via Swing Timer) would be the only way.

--Joe


On 11/2/06, Amarnath Nanduri <amar_nan at yahoo.com> wrote:
> I am currently using a concurrent hashmap to act as shared memory between
> different threads. I think I might be doing something wrong and would
> appreciate your help and guidance on this.
>
> The scenario is:
> A manager class fires up a bunch of database objects (implementing the
> Callable interface)  every one minute (in a Timer task) and gets back the
> immutable data objects.
> //sample code
> ExecutorService.invokeAll(database objects implementing Callable interface).
>
> The manager then updates a concurrent HashMap with these data objects.
>
> The gui components request the manager for the concurrent HashMap and each
> gui component asks for its own data object out of the cncurrent hash map.
> All gui components are in a single swing timer that fires up an event once
> every minute and asks the gui components to get their data objects.
>
> Both Timer Task and Swing Timer are fired every one minute. I currently
> guess the amount of time delay I need (set to 30 seconds)  (to get the data
> from the database and populate the concurrent hash map) before the swing
> timer fires an event for the gui components to fetch from the concurrent
> hash map.
>
> [...]

From amar_nan at yahoo.com  Fri Nov  3 00:34:29 2006
From: amar_nan at yahoo.com (Amarnath Nanduri)
Date: Thu, 2 Nov 2006 21:34:29 -0800 (PST)
Subject: [concurrency-interest] question on concurrent HashMap
In-Reply-To: <31f2a7bd0611021709i615e260cj1218ce91be8ada9a@mail.gmail.com>
Message-ID: <20061103053429.752.qmail@web31212.mail.mud.yahoo.com>

Hi Joe:

   I thank you for your reply. I use the split model (infact I had already seen the article you mentioned a few months back and use that technique in my application whenever end user demands to see detailed data and it is got from the database i.e it is by demand). 

The manager is not part of the GUI and is infact a singleton in the application. I figured a singleton to schedule the database calls in a timer, to get the data and populate the concurrent hash map would be an ideal approach. 

The way the application flows currently is:
1.   create a connection pool, and create the manager.
2.   manager creates the timer task and schedules the database calls every 60 seconds.
3    manager populates the concurrent hash map.
4    gui swing timer is created with an initial delay of 10 seconds. 
5    swing timer event fires up every 60 seconds and gui components ask the manager for the concurrent hash map.
6    gui components get their respective data and displays the information.

I feel my weak link is between 3 & 4. By trail and error I came up with the 10 second delay in 4. 

The assumed time line in minute/seconds is  :
0.00 - start the application
0.05 - create the connection pool  ( takes 5 seconds)
0.07 - create the manager and the timer. 
0.08 - manager fires up the timer for database calls. timer fires up every 60 seconds.
0.10 - manager starts populating the concurrent hash map.
0.11 - gui creates swing timer with initial delay of 10 seconds.
0.41 - fire up the swing timer. timer fires up every 60 seconds.
0.42 - gui components access the concurrent hash map
1.08 - manager timer fires up and makes the database calls.
1.10 - manager populates the concurrent hash map
1.41 - swing timer fires up 
 
Based on this timeline ( i made some assumptions) do you foresee any problems with the approach? I assume the database will be up and running all the time my application runs.

Many thanks,
Amar..

Joe Bowbeer <joe.bowbeer at gmail.com> wrote: On 11/2/06, Amarnath Nanduri  wrote:
> [...]
> One question that bothers me is that how can I make the
> Timer Task (in manager) and Swing Timer (controlling the gui
> components) be in step. i.e only after the concurrent hashmap
> is populated by the manager, do I want the swing timer firing up
> to allow the gui components to get their data.
>

The timer task in manager can schedule a task on Swing's event thread
using EventQueue.invokeLater (aka SwingUtilities.invokeLater).  This
is the mechanism employed by SwingWorker.  In your case, the task
would query the cache for the latest results and update the UI.

If the contents of the cache are interrelated then you may need to
employ a split-model technique so that your UI depicts a valid
instance of the cached state - and not some transitional state.

More:

SwingWorker is reconstructed in JCiP.

The second example in my Swing Threads article uses the split-model technique:

http://java.sun.com/products/jfc/tsc/articles/threads/threads3.html


> Is there a way for the gui components to wait till their data object
> is put in the concurrent hash map, before they attempt to retrieve
> the data from the concurrent hashmap?
>

Not on the Swing GUI side - without blocking the event thread.
Polling in the event thread (via Swing Timer) would be the only way.

--Joe


On 11/2/06, Amarnath Nanduri  wrote:
> I am currently using a concurrent hashmap to act as shared memory between
> different threads. I think I might be doing something wrong and would
> appreciate your help and guidance on this.
>
> The scenario is:
> A manager class fires up a bunch of database objects (implementing the
> Callable interface)  every one minute (in a Timer task) and gets back the
> immutable data objects.
> //sample code
> ExecutorService.invokeAll(database objects implementing Callable interface).
>
> The manager then updates a concurrent HashMap with these data objects.
>
> The gui components request the manager for the concurrent HashMap and each
> gui component asks for its own data object out of the cncurrent hash map.
> All gui components are in a single swing timer that fires up an event once
> every minute and asks the gui components to get their data objects.
>
> Both Timer Task and Swing Timer are fired every one minute. I currently
> guess the amount of time delay I need (set to 30 seconds)  (to get the data
> from the database and populate the concurrent hash map) before the swing
> timer fires an event for the gui components to fetch from the concurrent
> hash map.
>
> [...]
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


 
---------------------------------
Get your email and see which of your friends are online - Right on the  new Yahoo.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061102/e4c139be/attachment.html 

From dcholmes at optusnet.com.au  Fri Nov  3 00:58:27 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 3 Nov 2006 15:58:27 +1000
Subject: [concurrency-interest] question on concurrent HashMap
In-Reply-To: <20061103053429.752.qmail@web31212.mail.mud.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMENEHDAA.dcholmes@optusnet.com.au>

Hello Amar,

I think what Joe was suggesting was a "push" style approach whereby the
timer task, after getting updates from  the database, posts an event to the
Swing event thread that then has each component update itself with the
latest data from the map. That way there is no need for the UI updates to
wait for fresh data.

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Amarnath
Nanduri
  Sent: Friday, 3 November 2006 3:34 PM
  To: Joe Bowbeer; Concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] question on concurrent HashMap


  Hi Joe:

     I thank you for your reply. I use the split model (infact I had already
seen the article you mentioned a few months back and use that technique in
my application whenever end user demands to see detailed data and it is got
from the database i.e it is by demand).

  The manager is not part of the GUI and is infact a singleton in the
application. I figured a singleton to schedule the database calls in a
timer, to get the data and populate the concurrent hash map would be an
ideal approach.

  The way the application flows currently is:
  1.   create a connection pool, and create the manager.
  2.   manager creates the timer task and schedules the database calls every
60 seconds.
  3    manager populates the concurrent hash map.
  4    gui swing timer is created with an initial delay of 10 seconds.
  5    swing timer event fires up every 60 seconds and gui components ask
the manager for the concurrent hash map.
  6    gui components get their respective data and displays the
information.

  I feel my weak link is between 3 & 4. By trail and error I came up with
the 10 second delay in 4.

  The assumed time line in minute/seconds is  :
  0.00 - start the application
  0.05 - create the connection pool  ( takes 5 seconds)
  0.07 - create the manager and the timer.
  0.08 - manager fires up the timer for database calls. timer fires up every
60 seconds.
  0.10 - manager starts populating the concurrent hash map.
  0.11 - gui creates swing timer with initial delay of 10 seconds.
  0.41 - fire up the swing timer. timer fires up every 60 seconds.
  0.42 - gui components access the concurrent hash map
  1.08 - manager timer fires up and makes the database calls.
  1.10 - manager populates the concurrent hash map
  1.41 - swing timer fires up

  Based on this timeline ( i made some assumptions) do you foresee any
problems with the approach? I assume the database will be up and running all
the time my application runs.

  Many thanks,
  Amar..

  Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
    On 11/2/06, Amarnath Nanduri wrote:
    > [...]
    > One question that bothers me is that how can I make the
    > Timer Task (in manager) and Swing Timer (controlling the gui
    > components) be in step. i.e only after the concurrent hashmap
    > is populated by the manager, do I want the swing timer firing up
    > to allow the gui components to get their data.
    >

    The timer task in manager can schedule a task on Swing's event thread
    using EventQueue.invokeLater (aka SwingUtilities.invokeLater). This
    is the mechanism employed by SwingWorker. In your case, the task
    would query the cache for the latest results and update the UI.

    If the contents of the cache are interrelated then you may need to
    employ a split-model technique so that your UI depicts a valid
    instance of the cached state - and not some transitional state.

    More:

    SwingWorker is reconstructed in JCiP.

    The second example in my Swing Threads article uses the split-model
technique:

    http://java.sun.com/products/jfc/tsc/articles/threads/threads3.html


    > Is there a way for the gui components to wait till their data object
    > is put in the concurrent hash map, before they attempt to retrieve
    > the data from the concurrent hashmap?
    >

    Not on the Swing GUI side - without blocking the event thread.
    Polling in the event thread (via Swing Timer) would be the only way.

    --Joe


    On 11/2/06, Amarnath Nanduri wrote:
    > I am currently using a concurrent hashmap to act as shared memory
between
    > different threads. I think I might be doing something wrong and would
    > appreciate your help and guidance on this.
    >
    > The scenario is:
    > A manager class fires up a bunch of database objects (implementing the
    > Callable interface) every one minute (in a Timer task) and gets back
the
    > immutable data objects.
    > //sample code
    > ExecutorService.invokeAll(database objects implementing Callable
interface).
    >
    > The manager then updates a concurrent HashMap with these data objects.
    >
    > The gui components request the manager for the concurrent HashMap and
each
    > gui component asks for its own data object out of the cncurrent hash
map.
    > All gui components are in a single swing timer that fires up an event
once
    > every minute and asks the gui components to get their data objects.
    >
    > Both Timer Task and Swing Timer are fired every one minute. I
currently
    > guess the amount of time delay I need (set to 30 seconds) (to get the
data
    > from the database and populate the concurrent hash map) before the
swing
    > timer fires an event for the gui components to fetch from the
concurrent
    > hash map.
    >
    > [...]
    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at altair.cs.oswego.edu
    http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest





----------------------------------------------------------------------------
--
  Get your email and see which of your friends are online - Right on the new
Yahoo.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061103/c891b5da/attachment-0001.html 

From joe.bowbeer at gmail.com  Fri Nov  3 02:17:36 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 2 Nov 2006 23:17:36 -0800
Subject: [concurrency-interest] question on concurrent HashMap
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMENEHDAA.dcholmes@optusnet.com.au>
References: <20061103053429.752.qmail@web31212.mail.mud.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCMENEHDAA.dcholmes@optusnet.com.au>
Message-ID: <31f2a7bd0611022317n1edcd17ds964fc4d9a1f378b8@mail.gmail.com>

On 11/2/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
> I think what Joe was suggesting was a "push" style approach whereby
> the timer task, after getting updates from the database, posts an event
> to the Swing event thread that then has each component update itself
> with the latest data from the map. That way there is no need for the UI
> updates to wait for fresh data.
>

Yes.

One way to implement this while still preserving the beneficial
separation of UI from data model would be to add support for listeners
to the manager.

Then the UI could register a special "relaying" listener that, when
notified, would schedule (i.e., invokeLater) the appropriate update
task on the event thread.

--Joe

From tim at peierls.net  Fri Nov  3 08:42:54 2006
From: tim at peierls.net (Tim Peierls)
Date: Fri, 3 Nov 2006 08:42:54 -0500
Subject: [concurrency-interest] question on concurrent HashMap
In-Reply-To: <31f2a7bd0611022317n1edcd17ds964fc4d9a1f378b8@mail.gmail.com>
References: <20061103053429.752.qmail@web31212.mail.mud.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCMENEHDAA.dcholmes@optusnet.com.au>
	<31f2a7bd0611022317n1edcd17ds964fc4d9a1f378b8@mail.gmail.com>
Message-ID: <63b4e4050611030542i2935460yc92ab943c22441b7@mail.gmail.com>

And see section 9.4 of Java Concurrency in Practice for more on "push" and
thread-safe data models.

--tim

On 11/3/06, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>
> On 11/2/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> >
> > I think what Joe was suggesting was a "push" style approach whereby
> > the timer task, after getting updates from the database, posts an event
> > to the Swing event thread that then has each component update itself
> > with the latest data from the map. That way there is no need for the UI
> > updates to wait for fresh data.
> >
>
> Yes.
>
> One way to implement this while still preserving the beneficial
> separation of UI from data model would be to add support for listeners
> to the manager.
>
> Then the UI could register a special "relaying" listener that, when
> notified, would schedule (i.e., invokeLater) the appropriate update
> task on the event thread.
>
> --Joe
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061103/5e76d02f/attachment.html 

From amar_nan at yahoo.com  Fri Nov  3 09:48:57 2006
From: amar_nan at yahoo.com (Amarnath Nanduri)
Date: Fri, 3 Nov 2006 06:48:57 -0800 (PST)
Subject: [concurrency-interest] question on concurrent HashMap
In-Reply-To: <63b4e4050611030542i2935460yc92ab943c22441b7@mail.gmail.com>
Message-ID: <20061103144857.26007.qmail@web31215.mail.mud.yahoo.com>

This is fantastic. I will definitely modify my application to follow this approach. Many thanks to all of you for your suggestions.
   
  Regards,
  Amar..

Tim Peierls <tim at peierls.net> wrote:
  And see section 9.4 of Java Concurrency in Practice for more on "push" and thread-safe data models.

--tim

  On 11/3/06, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:  On 11/2/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
> I think what Joe was suggesting was a "push" style approach whereby
> the timer task, after getting updates from the database, posts an event 
> to the Swing event thread that then has each component update itself
> with the latest data from the map. That way there is no need for the UI
> updates to wait for fresh data.
>

Yes.

One way to implement this while still preserving the beneficial
separation of UI from data model would be to add support for listeners
to the manager.

Then the UI could register a special "relaying" listener that, when 
notified, would schedule (i.e., invokeLater) the appropriate update
task on the event thread.

--Joe
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest




 
---------------------------------
Cheap Talk? Check out Yahoo! Messenger's low  PC-to-Phone call rates.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061103/477544d0/attachment.html 

From the.mindstorm.mailinglist at gmail.com  Sat Nov  4 04:53:37 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Sat, 4 Nov 2006 11:53:37 +0200
Subject: [concurrency-interest] JCiP Condition Queues
Message-ID: <c6f400460611040153r4b26f13bneabf01073a4b7e39@mail.gmail.com>

Hi!

I have made my way through the Chapter 14 (the book is great, the time
is limited :-)) . At the end of 14.2.4 subchapter I have found the
following code:

[code - page: 304]
public synchronized void put(V v) throws InterruptedException {
    while (isFull()) wait();

     booleam wasEmpty = isEmpty();
     doPut(v);
     if(wasEmpty) {
         notifyAll(); // <=== question here
     }
}
[/code]

Considering that the chapter was detailing the usage of single
notification and conditional notification, and also that the example
comes from the BoundedBuffer (which is one-in one-out),  I am
wondering what is the benefit of having notifyAll instead of notify.

I feel I am missing something, because indeed BoundedBuffer doesn't
conform to the rules to use notify() instead of notifyAll(), but in
this case when passing from empty to 1-element queue, only one thread
will be able to take().

thanks in advance for hints,

./alex
--
.w( the_mindstorm )p.

From mailinglist.taras.tielkes at gmail.com  Sat Nov  4 06:40:33 2006
From: mailinglist.taras.tielkes at gmail.com (Taras Tielkes)
Date: Sat, 04 Nov 2006 12:40:33 +0100
Subject: [concurrency-interest] JCiP Condition Queues
In-Reply-To: <c6f400460611040153r4b26f13bneabf01073a4b7e39@mail.gmail.com>
References: <c6f400460611040153r4b26f13bneabf01073a4b7e39@mail.gmail.com>
Message-ID: <454C7C31.5060906@gmail.com>

Alexandru Popescu wrote:
> Hi!
> 
> I have made my way through the Chapter 14 (the book is great, the time
> is limited :-)) . At the end of 14.2.4 subchapter I have found the
> following code:
> 
> [code - page: 304]
> public synchronized void put(V v) throws InterruptedException {
>     while (isFull()) wait();
> 
>      booleam wasEmpty = isEmpty();
>      doPut(v);
>      if(wasEmpty) {
>          notifyAll(); // <=== question here
>      }
> }
> [/code]
> 
> Considering that the chapter was detailing the usage of single
> notification and conditional notification, and also that the example
> comes from the BoundedBuffer (which is one-in one-out),  I am
> wondering what is the benefit of having notifyAll instead of notify.
> 
> I feel I am missing something, because indeed BoundedBuffer doesn't
> conform to the rules to use notify() instead of notifyAll(), but in
> this case when passing from empty to 1-element queue, only one thread
> will be able to take().
> 

This example (as the subscript indicates) is about conditional 
notification, not single notification.
The idea is to notifyAll() only on empty->notEmpty and full->notFull 
transitions, and not on all the other state transitions.

The 'uniform waiters' condition does not hold for BoundedBuffer: having 
the number of producers/consumers exceed the buffer capacity makes it 
possible to have different threads wait for "not full" and "not empty" 
at the same time. (see pages 300-304)

-tt

From the.mindstorm.mailinglist at gmail.com  Sat Nov  4 07:46:13 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Sat, 4 Nov 2006 14:46:13 +0200
Subject: [concurrency-interest] JCiP Condition Queues
In-Reply-To: <454C7C31.5060906@gmail.com>
References: <c6f400460611040153r4b26f13bneabf01073a4b7e39@mail.gmail.com>
	<454C7C31.5060906@gmail.com>
Message-ID: <c6f400460611040446v42ed81a8j743fa21f530f8934@mail.gmail.com>

On 11/4/06, Taras Tielkes <mailinglist.taras.tielkes at gmail.com> wrote:
> Alexandru Popescu wrote:
> > Hi!
> >
> > I have made my way through the Chapter 14 (the book is great, the time
> > is limited :-)) . At the end of 14.2.4 subchapter I have found the
> > following code:
> >
> > [code - page: 304]
> > public synchronized void put(V v) throws InterruptedException {
> >     while (isFull()) wait();
> >
> >      booleam wasEmpty = isEmpty();
> >      doPut(v);
> >      if(wasEmpty) {
> >          notifyAll(); // <=== question here
> >      }
> > }
> > [/code]
> >
> > Considering that the chapter was detailing the usage of single
> > notification and conditional notification, and also that the example
> > comes from the BoundedBuffer (which is one-in one-out),  I am
> > wondering what is the benefit of having notifyAll instead of notify.
> >
> > I feel I am missing something, because indeed BoundedBuffer doesn't
> > conform to the rules to use notify() instead of notifyAll(), but in
> > this case when passing from empty to 1-element queue, only one thread
> > will be able to take().
> >
>
> This example (as the subscript indicates) is about conditional
> notification, not single notification.
> The idea is to notifyAll() only on empty->notEmpty and full->notFull
> transitions, and not on all the other state transitions.
>

Darn... I have found the note on the previous page... sorry for missing it.

> The 'uniform waiters' condition does not hold for BoundedBuffer: having
> the number of producers/consumers exceed the buffer capacity makes it
> possible to have different threads wait for "not full" and "not empty"
> at the same time. (see pages 300-304)
>

Even if I agree with the first sentence (which I have also provided in
the initial email), I am not sure I am following the above argument
:-(.  Can you please detail?
For me it looks like the code can be written like:

[code]
if(wasEmpty) {
   notify(); // was empty and now I have 1 element => only 1 consumer
can really do something
}
else {
   notifyAll(); // there is no problem to awake multiple threads... we
have plenty of elements
}
[/code]

./alex
--
.w( the_mindstorm )p.

> -tt
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From tim at peierls.net  Sat Nov  4 09:18:00 2006
From: tim at peierls.net (Tim Peierls)
Date: Sat, 4 Nov 2006 09:18:00 -0500
Subject: [concurrency-interest] JCiP Condition Queues
In-Reply-To: <c6f400460611040446v42ed81a8j743fa21f530f8934@mail.gmail.com>
References: <c6f400460611040153r4b26f13bneabf01073a4b7e39@mail.gmail.com>
	<454C7C31.5060906@gmail.com>
	<c6f400460611040446v42ed81a8j743fa21f530f8934@mail.gmail.com>
Message-ID: <63b4e4050611040618g86e2b2pce5f87cf33190c2@mail.gmail.com>

On 11/4/06, Alexandru Popescu <the.mindstorm.mailinglist at gmail.com> wrote:
>
> For me it looks like the code can be written like:
>
> [code]
> if(wasEmpty) {
>    notify(); // was empty and now I have 1 element => only 1 consumer can
> really do something
> }
> else {
>    notifyAll(); // there is no problem to awake multiple threads... we
> have plenty of elements
> }
> [/code]
>

"Only one consumer can really do something" but there's no guarantee that
the thread that is woken up will be a consumer thread, i.e., a thread doing
take() rather than put(). That's the reason for the "uniform waiters"
condition on p.303.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061104/a0b0c6c0/attachment.html 

From the.mindstorm.mailinglist at gmail.com  Sat Nov  4 11:59:45 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Sat, 4 Nov 2006 18:59:45 +0200
Subject: [concurrency-interest] JCiP Condition Queues
In-Reply-To: <63b4e4050611040618g86e2b2pce5f87cf33190c2@mail.gmail.com>
References: <c6f400460611040153r4b26f13bneabf01073a4b7e39@mail.gmail.com>
	<454C7C31.5060906@gmail.com>
	<c6f400460611040446v42ed81a8j743fa21f530f8934@mail.gmail.com>
	<63b4e4050611040618g86e2b2pce5f87cf33190c2@mail.gmail.com>
Message-ID: <c6f400460611040859v508fe3eey37f4f04f36c32751@mail.gmail.com>

On 11/4/06, Tim Peierls <tim at peierls.net> wrote:
> On 11/4/06, Alexandru Popescu
> <the.mindstorm.mailinglist at gmail.com> wrote:
> > For me it looks like the code can be written like:
> >
> > [code]
> > if(wasEmpty) {
> >    notify(); // was empty and now I have 1 element => only 1 consumer can
> really do something
> > }
> > else {
> >    notifyAll(); // there is no problem to awake multiple threads... we
> have plenty of elements
> > }
> > [/code]
> >
>
> "Only one consumer can really do something" but there's no guarantee that
> the thread that is woken up will be a consumer thread, i.e., a thread doing
> take() rather than put(). That's the reason for the "uniform waiters"
> condition on p.303.
>

That was it :-)). Once again, many many thanks Tim.

./alex
--
.w( the_mindstorm )p.


> --tim
>

From forax at univ-mlv.fr  Thu Nov  9 05:32:08 2006
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Thu, 09 Nov 2006 11:32:08 +0100
Subject: [concurrency-interest] [HS] why creation of array of parametrized
	type is unsafe
Message-ID: <455303A8.3000907@univ-mlv.fr>

It's a totally HS question, but i know that some Java gurus listen that 
list.

It's perhaps a stupid question, but i don't see why array of 
parametrized type
is unsafe.

see my blog on that :
http://weblogs.java.net/blog/forax/archive/2006/11/why_array_of_pa.html

R?mi


From josh at bloch.us  Thu Nov  9 15:09:52 2006
From: josh at bloch.us (Joshua Bloch)
Date: Thu, 9 Nov 2006 12:09:52 -0800
Subject: [concurrency-interest] [HS] why creation of array of
	parametrized type is unsafe
In-Reply-To: <455303A8.3000907@univ-mlv.fr>
References: <455303A8.3000907@univ-mlv.fr>
Message-ID: <b097ac510611091209p29e7f440wfca36f82b793750e@mail.gmail.com>

R?mi,

Because the the array that is created is generally *not* an array of T (the
type parameter), but of its bound.  Automatically generated casts at the
call site can fail.  Run this program (which compiles with a warning) and
you'll get the idea.  The program was written by Peter van der Ahe:



public class GenericVarargs {
   static <T> T[] m1(T t1, T t2) {
       return m2(t1, t2);
   }
   static <T> T[] m2(T... args) {
       return args;
   }
   public static void main(String... args) {
       String[] strings = m1("bad", "karma");
   }
}



On 11/9/06, R?mi Forax <forax at univ-mlv.fr> wrote:
>
> It's a totally HS question, but i know that some Java gurus listen that
> list.
>
> It's perhaps a stupid question, but i don't see why array of
> parametrized type
> is unsafe.
>
> see my blog on that :
> http://weblogs.java.net/blog/forax/archive/2006/11/why_array_of_pa.html
>
> R?mi
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061109/15a157d9/attachment.html 

From dhanji at gmail.com  Thu Nov  9 22:07:57 2006
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Fri, 10 Nov 2006 13:07:57 +1000
Subject: [concurrency-interest] [HS] why creation of array of
	parametrized type is unsafe
In-Reply-To: <b097ac510611091209p29e7f440wfca36f82b793750e@mail.gmail.com>
References: <455303A8.3000907@univ-mlv.fr>
	<b097ac510611091209p29e7f440wfca36f82b793750e@mail.gmail.com>
Message-ID: <aa067ea10611091907u7f4f5d04i37da89ac9de37d31@mail.gmail.com>

A similar problem exists with parameterized collections--at runtime I can
add objects not of a reified type via reflection (unless using a checkedSet
or list..which throws CCE).
Is this true?

On 11/10/06, Joshua Bloch <josh at bloch.us> wrote:
>
> R?mi,
>
> Because the the array that is created is generally *not* an array of T
> (the type parameter), but of its bound.  Automatically generated casts at
> the call site can fail.  Run this program (which compiles with a warning)
> and you'll get the idea.  The program was written by Peter van der Ahe:
>
>
>
> public class GenericVarargs {
>    static <T> T[] m1(T t1, T t2) {
>        return m2(t1, t2);
>    }
>    static <T> T[] m2(T... args) {
>        return args;
>    }
>    public static void main(String... args) {
>        String[] strings = m1("bad", "karma");
>    }
> }
>
>
>
> On 11/9/06, R?mi Forax < forax at univ-mlv.fr> wrote:
> >
> > It's a totally HS question, but i know that some Java gurus listen that
> > list.
> >
> > It's perhaps a stupid question, but i don't see why array of
> > parametrized type
> > is unsafe.
> >
> > see my blog on that :
> > http://weblogs.java.net/blog/forax/archive/2006/11/why_array_of_pa.html
> >
> > R?mi
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061110/12a8d2e3/attachment.html 

From josh at bloch.us  Fri Nov 10 00:05:35 2006
From: josh at bloch.us (Joshua Bloch)
Date: Thu, 9 Nov 2006 21:05:35 -0800
Subject: [concurrency-interest] [HS] why creation of array of
	parametrized type is unsafe
In-Reply-To: <aa067ea10611091907u7f4f5d04i37da89ac9de37d31@mail.gmail.com>
References: <455303A8.3000907@univ-mlv.fr>
	<b097ac510611091209p29e7f440wfca36f82b793750e@mail.gmail.com>
	<aa067ea10611091907u7f4f5d04i37da89ac9de37d31@mail.gmail.com>
Message-ID: <b097ac510611092105y4f454457y563182f26a9ef529@mail.gmail.com>

Dhanji,

No, collections are very different.  You can create a collections whose
elements are of a non-reified type:

   List<List<String>> lls = new ArrayList<List<String>>();

As you know, the array analogue is illegal:

    List<String>[] als = new List<String>[SIZE];

It is, however, the case that you don't get exceptions at run time is you
insert an element of the wrong type (unless you're using a checked
collection). One way to look at this is that Collections give you static
type safety but not runtime, and arrays give you runtime type safety but not
static.

        Josh

On 11/9/06, Dhanji R. Prasanna <dhanji at gmail.com> wrote:
>
> A similar problem exists with parameterized collections--at runtime I can
> add objects not of a reified type via reflection (unless using a checkedSet
> or list..which throws CCE).
> Is this true?
>
> On 11/10/06, Joshua Bloch <josh at bloch.us> wrote:
> >
> > R?mi,
> >
> > Because the the array that is created is generally *not* an array of T
> > (the type parameter), but of its bound.  Automatically generated casts at
> > the call site can fail.  Run this program (which compiles with a warning)
> > and you'll get the idea.  The program was written by Peter van der Ahe:
> >
> >
> >
> > public class GenericVarargs {
> >    static <T> T[] m1(T t1, T t2) {
> >        return m2(t1, t2);
> >    }
> >    static <T> T[] m2(T... args) {
> >        return args;
> >    }
> >    public static void main(String... args) {
> >        String[] strings = m1("bad", "karma");
> >    }
> > }
> >
> >
> >
> > On 11/9/06, R?mi Forax < forax at univ-mlv.fr> wrote:
> > >
> > > It's a totally HS question, but i know that some Java gurus listen
> > > that
> > > list.
> > >
> > > It's perhaps a stupid question, but i don't see why array of
> > > parametrized type
> > > is unsafe.
> > >
> > > see my blog on that :
> > >
> > > http://weblogs.java.net/blog/forax/archive/2006/11/why_array_of_pa.html
> > >
> > > R?mi
> > >
> > > _______________________________________________
> > > Concurrency-interest mailing list
> > > Concurrency-interest at altair.cs.oswego.edu
> > > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> > >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061109/93fa6f93/attachment.html 

From forax at univ-mlv.fr  Fri Nov 10 03:51:21 2006
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Fri, 10 Nov 2006 09:51:21 +0100
Subject: [concurrency-interest] [HS] why creation of array of
 parametrized type is unsafe
In-Reply-To: <aa067ea10611091907u7f4f5d04i37da89ac9de37d31@mail.gmail.com>
References: <455303A8.3000907@univ-mlv.fr>	
	<b097ac510611091209p29e7f440wfca36f82b793750e@mail.gmail.com>
	<aa067ea10611091907u7f4f5d04i37da89ac9de37d31@mail.gmail.com>
Message-ID: <45543D89.2090709@univ-mlv.fr>

Dhanji R. Prasanna a ?crit :
> A similar problem exists with parameterized collections--at runtime I 
> can add objects not of a reified type via reflection (unless using a 
> checkedSet or list..which throws CCE).
> Is this true?
Reflection and generics doesn't play well together, by example, the 
following code is
safe but generates a CCE at le last line.

public static void main(String[] args) throws IllegalAccessException, 
InvocationTargetException, NoSuchMethodException {
      ArrayList<String> list=new ArrayList<String>();
     
      List.class.getMethod("add",Object.class).invoke(list,3);
     
      list.get(0).charAt(0);
}

so it is another problem.

R?mi

>
> On 11/10/06, *Joshua Bloch* <josh at bloch.us <mailto:josh at bloch.us>> wrote:
>
>     R?mi,
>
>     Because the the array that is created is generally *not* an array
>     of T (the type parameter), but of its bound.  Automatically
>     generated casts at the call site can fail.  Run this program
>     (which compiles with a warning) and you'll get the idea.  The
>     program was written by Peter van der Ahe:
>
>
>
>     public class GenericVarargs {
>        static <T> T[] m1(T t1, T t2) {
>            return m2(t1, t2);
>        }
>        static <T> T[] m2(T... args) {
>            return args;
>        }
>        public static void main(String... args) {
>            String[] strings = m1("bad", "karma");
>
>        }
>     }
>
>
>
>     On 11/9/06, *R?mi Forax * < forax at univ-mlv.fr
>     <mailto:forax at univ-mlv.fr>> wrote:
>
>         It's a totally HS question, but i know that some Java gurus
>         listen that
>         list.
>
>         It's perhaps a stupid question, but i don't see why array of
>         parametrized type
>         is unsafe.
>
>         see my blog on that :
>         http://weblogs.java.net/blog/forax/archive/2006/11/why_array_of_pa.html
>
>         R?mi
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at altair.cs.oswego.edu
>         <mailto:Concurrency-interest at altair.cs.oswego.edu>
>         http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>         <http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at altair.cs.oswego.edu
>     <mailto:Concurrency-interest at altair.cs.oswego.edu>
>     http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>



From pfeiffer at tzi.de  Fri Nov 10 06:56:48 2006
From: pfeiffer at tzi.de (Oliver Pfeiffer)
Date: Fri, 10 Nov 2006 12:56:48 +0100
Subject: [concurrency-interest] ReadWriteLock implementation with extended
	policy providing upgradeable read-locks?
Message-ID: <VPOP31.4.6.20061110125656.982.5f.1.3a27a341@mythos>

I'm looking for a certain ReadWriteLock implementation following a very
similar policy as Doug Leas ReentrantReadWriteLock except that the last
reader is enabled to be upgraded to a writer. The lack of these concept in
default JDK15 distribution is the source of most threading issues I have had
to fix in the last months in foreign code.

Unfortunately it's obvious that this can't be solved by the common
read-write-lock concept as provided by the interface, since two simultaneous
threads, each holding the read-lock, may both acquire the write-lock and
therefore will consequently deadlock each other. This seems to be solvable
only by classifying read-locks as exlusive-read-locks and
non-exclusive-read-locks. The exclusive-read-lock is a normal read-lock
except that it can only be shared with other non-exclusive-read-locks (but
not with other exclusive-read-locks or write-locks). In other words: there
can be only one thread holding the exclusive-read-lock per time thus this
exclusice-read-lock can be safely upgraded to a write-lock without any risc
of deadlocking. Certainly any non-exclusive-read-lock acquiring the
write-lock or the exclusive-read-lock will block foreever, but an
exclusive-read-lock can always acquire the non-exclusive-read-lock and the
write-lock.

The benefit of such a concept is the ability to share many readers with a
single reader that _might_ be need to lazily become a writer. Or in general
there is no need to exclusively lock a model when there are time-consuming
preparations needed before a write-lock is actually acquired. This can
result in a performance gain bundeled with a less risc of deadlocking.

Does anybody knows a serious extension of default java concurrency features
providing such a concept?

-- 
Gr??e - Regards
Oliver Pfeiffer
ICQ-ID 84320006 



From tim at peierls.net  Fri Nov 10 07:13:42 2006
From: tim at peierls.net (Tim Peierls)
Date: Fri, 10 Nov 2006 07:13:42 -0500
Subject: [concurrency-interest] ReadWriteLock implementation with
	extended policy providing upgradeable read-locks?
In-Reply-To: <VPOP31.4.6.20061110125656.982.5f.1.3a27a341@mythos>
References: <VPOP31.4.6.20061110125656.982.5f.1.3a27a341@mythos>
Message-ID: <63b4e4050611100413l34120c88kbb7b5f5ed157b35d@mail.gmail.com>

You should consider using AbstractQueuedSynchronizer to build the kind of
lock you describe:

http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/locks/AbstractQueuedSynchronizer.html

Also see Section 14.5 of Java Concurrency in Practice.

I have some examples of other non-standard lock implementations using AQS,
if you're interested.

--tim

On 11/10/06, Oliver Pfeiffer <pfeiffer at tzi.de> wrote:
>
> I'm looking for a certain ReadWriteLock implementation following a very
> similar policy as Doug Leas ReentrantReadWriteLock except that the last
> reader is enabled to be upgraded to a writer. The lack of these concept in
> default JDK15 distribution is the source of most threading issues I have
> had
> to fix in the last months in foreign code.
>
> Unfortunately it's obvious that this can't be solved by the common
> read-write-lock concept as provided by the interface, since two
> simultaneous
> threads, each holding the read-lock, may both acquire the write-lock and
> therefore will consequently deadlock each other. This seems to be solvable
> only by classifying read-locks as exlusive-read-locks and
> non-exclusive-read-locks. The exclusive-read-lock is a normal read-lock
> except that it can only be shared with other non-exclusive-read-locks (but
> not with other exclusive-read-locks or write-locks). In other words: there
> can be only one thread holding the exclusive-read-lock per time thus this
> exclusice-read-lock can be safely upgraded to a write-lock without any
> risc
> of deadlocking. Certainly any non-exclusive-read-lock acquiring the
> write-lock or the exclusive-read-lock will block foreever, but an
> exclusive-read-lock can always acquire the non-exclusive-read-lock and the
> write-lock.
>
> The benefit of such a concept is the ability to share many readers with a
> single reader that _might_ be need to lazily become a writer. Or in
> general
> there is no need to exclusively lock a model when there are time-consuming
> preparations needed before a write-lock is actually acquired. This can
> result in a performance gain bundeled with a less risc of deadlocking.
>
> Does anybody knows a serious extension of default java concurrency
> features
> providing such a concept?
>
> --
> Gr??e - Regards
> Oliver Pfeiffer
> ICQ-ID 84320006
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061110/58eee591/attachment.html 

From dcholmes at optusnet.com.au  Fri Nov 10 20:34:15 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat, 11 Nov 2006 11:34:15 +1000
Subject: [concurrency-interest] ReadWriteLock implementation with
	extendedpolicy providing upgradeable read-locks?
In-Reply-To: <VPOP31.4.6.20061110125656.982.5f.1.3a27a341@mythos>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEADHEAA.dcholmes@optusnet.com.au>

Hello Oliver,

I think this makes the third request for such functionality since 2000 -
hence it hasn't been a high priority :)

Intention-locks were discussed by the EG a couple of times but basically
they didn't meet the acceptance criteria. They are quite complicated to
implement correctly, have ugly API's and don't inter-mix with normal
ReadWriteLocks. They are also rarely needed oustide transactional contexts.
But maybe it is time to consider them again for Java 7.

That said, with changes that went in to Java 6 to track read-lock ownership,
it shouldn't be too hard for you to modify ReentrantReadWriteLock to allow
the final-reader to acquire the writeLock. That isn't quite an
intention-lock but might satisfy your needs.

Can you describe the use-case for this.

Cheers,
David Holmes


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oliver
> Pfeiffer
> Sent: Friday, 10 November 2006 9:57 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] ReadWriteLock implementation with
> extendedpolicy providing upgradeable read-locks?
>
>
> I'm looking for a certain ReadWriteLock implementation following a very
> similar policy as Doug Leas ReentrantReadWriteLock except that the last
> reader is enabled to be upgraded to a writer. The lack of these concept in
> default JDK15 distribution is the source of most threading issues
> I have had
> to fix in the last months in foreign code.
>
> Unfortunately it's obvious that this can't be solved by the common
> read-write-lock concept as provided by the interface, since two
> simultaneous
> threads, each holding the read-lock, may both acquire the write-lock and
> therefore will consequently deadlock each other. This seems to be solvable
> only by classifying read-locks as exlusive-read-locks and
> non-exclusive-read-locks. The exclusive-read-lock is a normal read-lock
> except that it can only be shared with other non-exclusive-read-locks (but
> not with other exclusive-read-locks or write-locks). In other words: there
> can be only one thread holding the exclusive-read-lock per time thus this
> exclusice-read-lock can be safely upgraded to a write-lock
> without any risc
> of deadlocking. Certainly any non-exclusive-read-lock acquiring the
> write-lock or the exclusive-read-lock will block foreever, but an
> exclusive-read-lock can always acquire the non-exclusive-read-lock and the
> write-lock.
>
> The benefit of such a concept is the ability to share many readers with a
> single reader that _might_ be need to lazily become a writer. Or
> in general
> there is no need to exclusively lock a model when there are time-consuming
> preparations needed before a write-lock is actually acquired. This can
> result in a performance gain bundeled with a less risc of deadlocking.
>
> Does anybody knows a serious extension of default java
> concurrency features
> providing such a concept?
>
> --
> Gr??e - Regards
> Oliver Pfeiffer
> ICQ-ID 84320006
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>



From dawidk at mathcs.emory.edu  Fri Nov 10 20:36:18 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Fri, 10 Nov 2006 20:36:18 -0500
Subject: [concurrency-interest] who uses backport-util-concurrent?
Message-ID: <45552912.5090405@mathcs.emory.edu>

Hello everybody,

I am building the "projects that use backport-util-concurrent" section 
on the backport Web page 
(http://dcl.mathcs.emory.edu/util/backport-util-concurrent/). Currently 
I have "Retrotranslator" and "Glazed Lists", but I know that there are 
many more out there. If your project uses the backport and you'd like to 
have it linked from the backport page, please send me the following:

* project name
* URL
* 1-3 sentences of description
* if you feel like, a "user's testimonial"

BTW, new backport release is coming. It will include a surprise: a new 
version / branch, optimized for Java 5.0. (And, also, another one that 
works with Java 1.2 and 1.3).

What's the point of backporting to 5.0? Well, one thing is that the 
backport actually has 6.0 stuff. But more importantly, I've heard of 
people who decide to continue to use the backport on Java 5.0, since 
they don't want to change their applications just yet, as they want to 
keep them portable between 5.0 and 1.4. If so, why not letting them have 
native speed? Stay tuned.

Regards,
Dawid Kurzyniec


From the.mindstorm.mailinglist at gmail.com  Sat Nov 11 05:26:48 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Sat, 11 Nov 2006 12:26:48 +0200
Subject: [concurrency-interest] who uses backport-util-concurrent?
In-Reply-To: <45552912.5090405@mathcs.emory.edu>
References: <45552912.5090405@mathcs.emory.edu>
Message-ID: <4555A568.3020203@gmail.com>

Hi Dawid!

Here it is:

Project name: TestNG

URL: http://testng.org

Authors:
	Cedric Beust
	Alexandru Popescu

Description:

TestNG is a testing framework inspired from JUnit and NUnit but introducing some new functionalities 
that make it more powerful and easier to use, such as:

     * JDK 5 Annotations (JDK 1.4 is also supported with JavaDoc annotations).
     * Flexible test configuration.
     * Support for data-driven testing (with @DataProvider).
     * Support for parameters.
     * Allows distribution of tests on slave machines.
     * Powerful execution model (no more TestSuite).
     * Supported by a variety of tools and plug-ins (Eclipse, IDEA, Maven, etc...).
     * Embeds BeanShell for further flexibility.
     * Default JDK functions for runtime and logging (no dependencies).
     * Dependent methods for application server testing.

TestNG is designed to cover all categories of tests:  unit, functional, end-to-end, integration, etc...

(taken from the TestNG main page)

Backport Usage:
	TestNG enables running various configurations in "parallel" (concurrent) mode. Tests, methods or 
even individual methods can be configured to run in parallel. As TestNG is able to run on both JDK5 
and JDK1.4 (using either JDK annotated tests or javadoc-like annotated tests) it uses 
backport-concurrent-util for implementing the "parallel" strategy for JDK1.4.
	I have defined small wrapper interfaces (containing a very small subset of API that we need) for 
the concurrent part that are implemented using either JDK5 concurrent classes or 
backport-concurrent-util classes.

thanks,

./alex
--
.w( the_mindstorm )p.
   TestNG co-founder
EclipseTestNG Creator


#: Dawid Kurzyniec changed the world a bit at a time by saying (astral date: 11/11/2006 3:36 AM) :#
> Hello everybody,
> 
> I am building the "projects that use backport-util-concurrent" section 
> on the backport Web page 
> (http://dcl.mathcs.emory.edu/util/backport-util-concurrent/). Currently 
> I have "Retrotranslator" and "Glazed Lists", but I know that there are 
> many more out there. If your project uses the backport and you'd like to 
> have it linked from the backport page, please send me the following:
> 
> * project name
> * URL
> * 1-3 sentences of description
> * if you feel like, a "user's testimonial"
> 
> BTW, new backport release is coming. It will include a surprise: a new 
> version / branch, optimized for Java 5.0. (And, also, another one that 
> works with Java 1.2 and 1.3).
> 
> What's the point of backporting to 5.0? Well, one thing is that the 
> backport actually has 6.0 stuff. But more importantly, I've heard of 
> people who decide to continue to use the backport on Java 5.0, since 
> they don't want to change their applications just yet, as they want to 
> keep them portable between 5.0 and 1.4. If so, why not letting them have 
> native speed? Stay tuned.
> 
> Regards,
> Dawid Kurzyniec
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From yechielf at gigaspaces.com  Sun Nov 12 11:22:10 2006
From: yechielf at gigaspaces.com (Yechiel Feffer)
Date: Sun, 12 Nov 2006 18:22:10 +0200
Subject: [concurrency-interest] multi-core cpus & java
Message-ID: <D166C96F43D1D611B8E3000255A0C48C6BEAB1@officesrv.gspaces.com>

Hi 
 
1. What do an operating-system have to do in order to make a good use of
multi-core ?  Which OS are well-adapted for multi-core ?
2. What do a Jvm have to do in order to make a good use of multi-core ?
Which Jvms are well-adapted for multi-core ?
3  What about java applications- ?
 
If you have usefull links-that will be great
 
Thanks,
Yechiel Fefer 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061112/4e76d86b/attachment.html 

From dcholmes at optusnet.com.au  Sun Nov 12 17:24:37 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 13 Nov 2006 08:24:37 +1000
Subject: [concurrency-interest] multi-core cpus & java
In-Reply-To: <D166C96F43D1D611B8E3000255A0C48C6BEAB1@officesrv.gspaces.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEAKHEAA.dcholmes@optusnet.com.au>

Generally speaking using multi-core is fairly transparent to the OS and
completely transparent to a JVM that uses native OS threads. It is also
transparent to an application.

Performance-wise, however, the tuning that makes the OS/JVM/app run well on
a n-way SMP, is different to that needed on an n-core UP (or a k-core l-way
SMP - where k*l=n). It depends on the particular architecture as to what
hardware facilities the cores share - for example the memory interface.

Application tuning on such systems needs to be considered at deploy time as
there is no runtime facility in the VM to determine the nature of the
underlying processors.

Other's may be able to give more specific details.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Yechiel
Feffer
  Sent: Monday, 13 November 2006 2:22 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] multi-core cpus & java


  Hi

  1. What do an operating-system have to do in order to make a good use of
multi-core ?  Which OS are well-adapted for multi-core ?
  2. What do a Jvm have to do in order to make a good use of multi-core ?
Which Jvms are well-adapted for multi-core ?
  3  What about java applications- ?

  If you have usefull links-that will be great

  Thanks,
  Yechiel Fefer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061113/66d7fd0b/attachment.html 

From dhanji at gmail.com  Sun Nov 12 22:12:22 2006
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Mon, 13 Nov 2006 13:12:22 +1000
Subject: [concurrency-interest] multi-core cpus & java
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEAKHEAA.dcholmes@optusnet.com.au>
References: <D166C96F43D1D611B8E3000255A0C48C6BEAB1@officesrv.gspaces.com>
	<NFBBKALFDCPFIDBNKAPCKEAKHEAA.dcholmes@optusnet.com.au>
Message-ID: <aa067ea10611121912t373b7880w4cb90e109d174346@mail.gmail.com>

On 11/13/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>
> Application tuning on such systems needs to be considered at deploy time
> as there is no runtime facility in the VM to determine the nature of the
> underlying processors.
>

If the nature of the processor is known at deploytime, what might be some
considerations in tuning a multi-threaded app on a jvm? (such as an
appserver)


Other's may be able to give more specific details.
>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Yechiel Feffer
> *Sent:* Monday, 13 November 2006 2:22 AM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] multi-core cpus & java
>
> Hi
>
> 1. What do an operating-system have to do in order to make a good use of
> multi-core ?  Which OS are well-adapted for multi-core ?
> 2. What do a Jvm have to do in order to make a good use of multi-core ?
> Which Jvms are well-adapted for multi-core ?
> 3  What about java applications- ?
>
> If you have usefull links-that will be great
>
> Thanks,
> Yechiel Fefer
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061113/e3b71138/attachment.html 

From dcholmes at optusnet.com.au  Sun Nov 12 22:57:30 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 13 Nov 2006 13:57:30 +1000
Subject: [concurrency-interest] multi-core cpus & java
In-Reply-To: <aa067ea10611121912t373b7880w4cb90e109d174346@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEALHEAA.dcholmes@optusnet.com.au>

That would depend on the architectural differences between tne multi-core
system and the SMP one. Suppose the memory interface is a bottleneck for a
multi-core chip, then you might get better overall throughput by scaling
back the number of threads.

It all depends on the architecture - and I'm no expert on that.

Cheers,
David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dhanji R.
Prasanna
  Sent: Monday, 13 November 2006 1:12 PM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] multi-core cpus & java




  On 11/13/06, David Holmes <dcholmes at optusnet.com.au> wrote:

    Application tuning on such systems needs to be considered at deploy time
as there is no runtime facility in the VM to determine the nature of the
underlying processors.

  If the nature of the processor is known at deploytime, what might be some
considerations in tuning a multi-threaded app on a jvm? (such as an
appserver)




    Other's may be able to give more specific details.

    David Holmes
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Yechiel
Feffer
      Sent: Monday, 13 November 2006 2:22 AM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] multi-core cpus & java


      Hi

      1. What do an operating-system have to do in order to make a good use
of multi-core ?  Which OS are well-adapted for multi-core ?
      2. What do a Jvm have to do in order to make a good use of multi-core
?  Which Jvms are well-adapted for multi-core ?
      3  What about java applications- ?

      If you have usefull links-that will be great

      Thanks,
      Yechiel Fefer

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at altair.cs.oswego.edu
    http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest




-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061113/fc4cde73/attachment.html 

From mike_edwards at uk.ibm.com  Mon Nov 13 03:28:55 2006
From: mike_edwards at uk.ibm.com (Mike Edwards)
Date: Mon, 13 Nov 2006 08:28:55 +0000
Subject: [concurrency-interest] multi-core cpus & java
In-Reply-To: <mailman.1.1163350801.16732.concurrency-interest@altair.cs.oswego.edu>
Message-ID: <OF59B9FC6E.AFC87C11-ON80257225.002CF03F-80257225.002E5341@uk.ibm.com>

Yechiel,

In general, an operating system that can deal with multi-CPU systems will 
be able to deal
effectively with multi-core CPUs.  Multi-CPU systems have been around for 
a while and most
of the common operating systems are able to handle them effectively.  This 
includes Linux and
Windows on Intel / AMD platforms.  Linux and the various forms of Unix 
(AIX, Solaris, etc) handle
the various other types of CPU.  The one area of difference for some of 
the systems is the 
ability to scale to very large numbers of CPUs, where some of the more 
specialist versions 
of Unix tend to have the lead in overall scalability.

For Java VMs, the main thing that they need to do is to make effective use 
of the threading
capabilities of the underlying operating system and avoid contention 
points where multiple
threads try to access the same resource on a frequent basis.  In general, 
most of the modern
Java VMs do this very well.  To get an idea of how well, take a look at 
the SpecJBB test numbers
here:

http://www.spec.org/jbb2005/results/

This test measures the capability of Java VMs to scale with the number of 
processors (including
multi-core) and you will find a series of multi-core and single core 
scores in those results.  It is
the straightness of the initial curve that indicates good scalability.

For Java applications, the first thing is to be multi-threaded - to use 
multiple cores, the application
must have multiple threads to run on them.  This is typically easier for 
server applications which are
handling many clients.  Once the application has many threads, the main 
thing to do is to avoid hot locks 
where different threads contend on a single lock.  Synchronization in 
general should be kept to a 
minimum, but the real evil is to have some synchronization that is a) used 
simultaneously by lots of 
threads and b) used very frequently.  Such hot locks will throw away many 
of the benefits of 
multiple threads.


Yours,  Mike.

Strategist - Emerging Technologies, SCA & SDO.
IBM Hursley Park, Mail Point 146, Winchester, SO21 2JN, Great Britain.
Phone & FAX: +44-1962-818014    Mobile: +44-7802-467431 
Email:  mike_edwards at uk.ibm.com


> Date: Sun, 12 Nov 2006 18:22:10 +0200
> From: Yechiel Feffer <yechielf at gigaspaces.com>
> Subject: [concurrency-interest] multi-core cpus & java
> To: concurrency-interest at cs.oswego.edu
> Message-ID:
>    <D166C96F43D1D611B8E3000255A0C48C6BEAB1 at officesrv.gspaces.com>
> Content-Type: text/plain; charset="windows-1255"
> 
> Hi 
> 
> 1. What do an operating-system have to do in order to make a good use of
> multi-core ?  Which OS are well-adapted for multi-core ?
> 2. What do a Jvm have to do in order to make a good use of multi-core ?
> Which Jvms are well-adapted for multi-core ?
> 3  What about java applications- ?
> 
> If you have usefull links-that will be great
> 
> Thanks,
> Yechiel Fefer 
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: /pipermail/attachments/20061112/4e76d86b/attachment-0001.html 
> 
> ------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> End of Concurrency-interest Digest, Vol 22, Issue 10
> ****************************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061113/1ebd60d3/attachment-0001.html 

From Stefan.Skoglund at it-huset.se  Mon Nov 13 04:59:54 2006
From: Stefan.Skoglund at it-huset.se (Stefan Skoglund)
Date: Mon, 13 Nov 2006 10:59:54 +0100
Subject: [concurrency-interest] SIMD and java
Message-ID: <2FDD14551F535345BE46817D94D8DBEC406EDF@ripley.it-huset.se>

As the multi-core messages appear, I'll have to ask another, somewhat related question. Is it possible to more or less explicitly utilize SIMD instructions? Is there a performance gain to e.g. explicitly vectorise calculations on arrays of primitives.
 
Are there any VMs that care about such optmizations?
 
/Stefan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061113/18efde9f/attachment.html 

From gregg at cytetech.com  Mon Nov 13 09:12:36 2006
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 13 Nov 2006 08:12:36 -0600
Subject: [concurrency-interest] multi-core cpus & java
In-Reply-To: <aa067ea10611121912t373b7880w4cb90e109d174346@mail.gmail.com>
References: <D166C96F43D1D611B8E3000255A0C48C6BEAB1@officesrv.gspaces.com>	<NFBBKALFDCPFIDBNKAPCKEAKHEAA.dcholmes@optusnet.com.au>
	<aa067ea10611121912t373b7880w4cb90e109d174346@mail.gmail.com>
Message-ID: <45587D54.9090007@cytetech.com>



Dhanji R. Prasanna wrote:
> 
> On 11/13/06, *David Holmes* <dcholmes at optusnet.com.au 
> <mailto:dcholmes at optusnet.com.au>> wrote:
> 
>      
>     Application tuning on such systems needs to be considered at deploy
>     time as there is no runtime facility in the VM to determine the
>     nature of the underlying processors.
> 
> 
> If the nature of the processor is known at deploytime, what might be 
> some considerations in tuning a multi-threaded app on a jvm? (such as an 
> appserver)

One of the predominate factors in utilizing any computing system is resource 
contention.  Resources which are heavily contendend need to be considered from 
several perspectives.  Memory contention is often implicit in CPU contention, 
but that is a hardware architecture issue.  Network bandwidth contention and 
associated latency go together.  Buffering in the OS comes into play.

My general practice is, at runtime, to start with two threads for any particular 
contended latency related resource.  I add threads only if I'm not realizing the 
needed performance and then use throughput measurement per thread to see if 
adding that thread increased or decreased the throughput of the system overall. 
  I look for increased latency, per thread as an indication that resources are 
now overly contended.

For any N visible processors (multi-core or N-way), I try to use N+1 threads as 
a starting point for CPU bound paths.  If I'm not making the needed throughput, 
I look at the load average to see whether I am CPU bound, or latency bound.  If 
the CPU use is less than 30% or so, I'll add a thread and see whether the load 
average goes up or down.  If the load average is high, I'll decrease the number 
of threads to look for memory/cache thrashing.

If you have java.nio based I/O subsystem use, then more worker threads on the 
backside might be what solves a throughput issue.  But, it might be that you 
really just have an arrival rate that exceeds the memory throughput of your 
hardware.  So, you might need to do some measuring, and look for the hot spots 
using a profiling tool.  Netbeans has one built in.  I like to use the "Your 
Kit" profiler because it lets me have it available in a running JVM. I can 
switch it on, and any moment, get a snapshot of memory and/or CPU and turn it 
off again.  It also supports memory dump on overflow and VM exit.

Gregg Wonderly

From MOKEEFFE at amfam.com  Tue Nov 14 13:17:51 2006
From: MOKEEFFE at amfam.com (OKeeffe, Michael K)
Date: Tue, 14 Nov 2006 12:17:51 -0600
Subject: [concurrency-interest] multi-core cpus & java
Message-ID: <2E8B2DCB7DE50B42808F8063C4E2858F095838FD@NHQ1ACCOEX05VS1.corporate.amfam.com>

Yechiel,
 
Here's an interesting article.  Basically, in your java apps, make use
of the concurrent APIs ;)
 
 
http://www.artima.com/forums/flat.jsp?forum=276&thread=183715

	-----Original Message-----
	From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Yechiel
Feffer
	Sent: Sunday, November 12, 2006 10:22 AM
	To: concurrency-interest at cs.oswego.edu
	Subject: [concurrency-interest] multi-core cpus & java
	
	
	Hi 
	 
	1. What do an operating-system have to do in order to make a
good use of multi-core ?  Which OS are well-adapted for multi-core ?
	2. What do a Jvm have to do in order to make a good use of
multi-core ?  Which Jvms are well-adapted for multi-core ?
	3  What about java applications- ?
	 
	If you have usefull links-that will be great
	 
	Thanks,
	Yechiel Fefer 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061114/e7495b37/attachment.html 

From harshanagesh at yahoo.com  Tue Nov 14 20:45:33 2006
From: harshanagesh at yahoo.com (Harsha Nagesh)
Date: Tue, 14 Nov 2006 17:45:33 -0800 (PST)
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and stalled jobs
Message-ID: <775523.79468.qm@web50715.mail.yahoo.com>

Hi,

     I am trying to use ScheduledThreadPoolExecutor to schedule a bunch of
jobs, each of which should be scheduled every minute. I am running this on win
XP and using a poolsize = 20 (threads). However, what I see is that out of the
100 or so jobs that have been added to the scheduler, there is a subset of them
(more than 20%) which seem to be scheduled once and never again after the first
time. I am not sure how to go about debugging this issue. Any pointers will be
greatly appreciated. Here is the skeleton of code that shows the way I am using
the scheduler and I am not sure if this is the best way to achieve my objective

int poolSize = 20;
long period = 60; //1 minute
long delay = 60; //1 minute
TimeUnit timeUnit = TimeUnit.SECONDS;

ScheduledThreadPoolExecutor pool = new 
          ScheduledThreadPoolExecutor(poolSize);
for(int i = 0; i < 100; i++){
   MyTask task = new MyTask();
   pool.scheduleAtFixedRate(task,0,period,timeUnit);
}

I also tried to use 

pool.scheduleWithFixedDelay(task,0,delay,timeUnit);

instead of the fixedRate scheduling, but I found the same behavior and many of
my jobs were stuck and not executed after the first run.

Any ideas ?

Thanks,
Harsha



 
____________________________________________________________________________________
Yahoo! Music Unlimited
Access over 1 million songs.
http://music.yahoo.com/unlimited

From dcholmes at optusnet.com.au  Tue Nov 14 22:49:31 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 15 Nov 2006 13:49:31 +1000
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and stalled
	jobs
In-Reply-To: <775523.79468.qm@web50715.mail.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEBFHEAA.dcholmes@optusnet.com.au>

Harsha,

You skeleton is right - the key question is : what does a MyTask try to do?
Do they block for anything?

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Harsha
> Nagesh
> Sent: Wednesday, 15 November 2006 11:46 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] ScheduledThreadPoolExecutor and stalled
> jobs
>
>
> Hi,
>
>      I am trying to use ScheduledThreadPoolExecutor to schedule a bunch of
> jobs, each of which should be scheduled every minute. I am
> running this on win
> XP and using a poolsize = 20 (threads). However, what I see is
> that out of the
> 100 or so jobs that have been added to the scheduler, there is a
> subset of them
> (more than 20%) which seem to be scheduled once and never again
> after the first
> time. I am not sure how to go about debugging this issue. Any
> pointers will be
> greatly appreciated. Here is the skeleton of code that shows the
> way I am using
> the scheduler and I am not sure if this is the best way to
> achieve my objective
>
> int poolSize = 20;
> long period = 60; //1 minute
> long delay = 60; //1 minute
> TimeUnit timeUnit = TimeUnit.SECONDS;
>
> ScheduledThreadPoolExecutor pool = new
>           ScheduledThreadPoolExecutor(poolSize);
> for(int i = 0; i < 100; i++){
>    MyTask task = new MyTask();
>    pool.scheduleAtFixedRate(task,0,period,timeUnit);
> }
>
> I also tried to use
>
> pool.scheduleWithFixedDelay(task,0,delay,timeUnit);
>
> instead of the fixedRate scheduling, but I found the same
> behavior and many of
> my jobs were stuck and not executed after the first run.
>
> Any ideas ?
>
> Thanks,
> Harsha
>
>
>
>
> __________________________________________________________________
> __________________
> Yahoo! Music Unlimited
> Access over 1 million songs.
> http://music.yahoo.com/unlimited
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From dcholmes at optusnet.com.au  Tue Nov 14 23:00:54 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 15 Nov 2006 14:00:54 +1000
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and stalled
	jobs
In-Reply-To: <775523.79468.qm@web50715.mail.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEBGHEAA.dcholmes@optusnet.com.au>

Just had a thought ...

ScheduledThreadPoolExecutor operates by dequeing a task and executing it,
then requeuing afterwards. This means that a task is never running
concurrently with itself if it executes beyond its next release time.

A different model of periodic task execution would leave the task in the
queue so that every period a new execution of it commences, even if the
previous one had not completed.

If you expected this second model then that is not what you get with
ScheduledThreadPoolExecutor.

Hope that helps.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Harsha
> Nagesh
> Sent: Wednesday, 15 November 2006 11:46 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] ScheduledThreadPoolExecutor and stalled
> jobs
>
>
> Hi,
>
>      I am trying to use ScheduledThreadPoolExecutor to schedule a bunch of
> jobs, each of which should be scheduled every minute. I am
> running this on win
> XP and using a poolsize = 20 (threads). However, what I see is
> that out of the
> 100 or so jobs that have been added to the scheduler, there is a
> subset of them
> (more than 20%) which seem to be scheduled once and never again
> after the first
> time. I am not sure how to go about debugging this issue. Any
> pointers will be
> greatly appreciated. Here is the skeleton of code that shows the
> way I am using
> the scheduler and I am not sure if this is the best way to
> achieve my objective
>
> int poolSize = 20;
> long period = 60; //1 minute
> long delay = 60; //1 minute
> TimeUnit timeUnit = TimeUnit.SECONDS;
>
> ScheduledThreadPoolExecutor pool = new
>           ScheduledThreadPoolExecutor(poolSize);
> for(int i = 0; i < 100; i++){
>    MyTask task = new MyTask();
>    pool.scheduleAtFixedRate(task,0,period,timeUnit);
> }
>
> I also tried to use
>
> pool.scheduleWithFixedDelay(task,0,delay,timeUnit);
>
> instead of the fixedRate scheduling, but I found the same
> behavior and many of
> my jobs were stuck and not executed after the first run.
>
> Any ideas ?
>
> Thanks,
> Harsha
>
>
>
>
> __________________________________________________________________
> __________________
> Yahoo! Music Unlimited
> Access over 1 million songs.
> http://music.yahoo.com/unlimited
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From harshanagesh at yahoo.com  Wed Nov 15 12:35:27 2006
From: harshanagesh at yahoo.com (Harsha Nagesh)
Date: Wed, 15 Nov 2006 09:35:27 -0800 (PST)
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and stalled
	jobs
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEBGHEAA.dcholmes@optusnet.com.au>
Message-ID: <583451.39399.qm@web50710.mail.yahoo.com>

Hi David,

Thanks for explaining the executor's operating model. I suspected that
something similar might be happening. So I tried the same code with the
scheduleWithFixedDelay method, which, according to the API description will
schedule the next instance of the task only after the current one is completed
(with a certain delay). But I noticed the same behavior there too. Some tasks
may take longer to complete than others, but if the next execution of a task is
scheduled after a certain delay from the completion of the current run, I don't
understand why it would stop further executions of the task.

As far as MyTask is concerned, its just a simple computation. I don't think it
is blocking as I can see the end result of the task printed out, before it
stalls further scheduling.

Any more pointers or ideas to try out will be greatly helpful.

Thanks,
Harsha

--- David Holmes <dcholmes at optusnet.com.au> wrote:

> Just had a thought ...
> 
> ScheduledThreadPoolExecutor operates by dequeing a task and executing it,
> then requeuing afterwards. This means that a task is never running
> concurrently with itself if it executes beyond its next release time.
> 
> A different model of periodic task execution would leave the task in the
> queue so that every period a new execution of it commences, even if the
> previous one had not completed.
> 
> If you expected this second model then that is not what you get with
> ScheduledThreadPoolExecutor.
> 
> Hope that helps.
> 
> David Holmes
> 
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Harsha
> > Nagesh
> > Sent: Wednesday, 15 November 2006 11:46 AM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: [concurrency-interest] ScheduledThreadPoolExecutor and stalled
> > jobs
> >
> >
> > Hi,
> >
> >      I am trying to use ScheduledThreadPoolExecutor to schedule a bunch of
> > jobs, each of which should be scheduled every minute. I am
> > running this on win
> > XP and using a poolsize = 20 (threads). However, what I see is
> > that out of the
> > 100 or so jobs that have been added to the scheduler, there is a
> > subset of them
> > (more than 20%) which seem to be scheduled once and never again
> > after the first
> > time. I am not sure how to go about debugging this issue. Any
> > pointers will be
> > greatly appreciated. Here is the skeleton of code that shows the
> > way I am using
> > the scheduler and I am not sure if this is the best way to
> > achieve my objective
> >
> > int poolSize = 20;
> > long period = 60; //1 minute
> > long delay = 60; //1 minute
> > TimeUnit timeUnit = TimeUnit.SECONDS;
> >
> > ScheduledThreadPoolExecutor pool = new
> >           ScheduledThreadPoolExecutor(poolSize);
> > for(int i = 0; i < 100; i++){
> >    MyTask task = new MyTask();
> >    pool.scheduleAtFixedRate(task,0,period,timeUnit);
> > }
> >
> > I also tried to use
> >
> > pool.scheduleWithFixedDelay(task,0,delay,timeUnit);
> >
> > instead of the fixedRate scheduling, but I found the same
> > behavior and many of
> > my jobs were stuck and not executed after the first run.
> >
> > Any ideas ?
> >
> > Thanks,
> > Harsha
> >
> >
> >
> >
> > __________________________________________________________________
> > __________________
> > Yahoo! Music Unlimited
> > Access over 1 million songs.
> > http://music.yahoo.com/unlimited
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 




 
____________________________________________________________________________________
Sponsored Link

Mortgage rates near 39yr lows. 
$310k for $999/mo. Calculate new payment! 
www.LowerMyBills.com/lre

From gregg at cytetech.com  Wed Nov 15 15:11:13 2006
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 15 Nov 2006 14:11:13 -0600
Subject: [concurrency-interest] Concurrent telemetry gathering
Message-ID: <455B7461.9050600@cytetech.com>

I'm working on collecting telemetry regarding bandwidth utilization through a 
"connection".  I've defined the following class, thinking that this should 
minimize concurrency for updating the numbers.

In addition, there is a 
ConcurrentHashMap<OutboundConnection,ConnectionBandwidth> object that is 
accessed by threads of execution which are handling the I/O requests through the 
system.  That map is lazily initialized with putIfAbsent().  When the first 
entry is added, there is a ScheduledThreadPoolExecutor used to schedule a 
regular execution of "run" to create a time based running average of bandwidth 
usage.

Is there anything else I should do differently?

public class ConnectionBandwidth implements Runnable, Serializable {
	private double recv, xmit;
	private AtomicLong crecv = new AtomicLong(), cxmit = new AtomicLong();
	private transient OutboundConnection conn;
	private double avgIntv, avgIntvMinusOne;

	public ConnectionBandwidth( OutboundConnection conn, int intv ) {
		this.conn = conn;
		avgIntv = intv;
		avgIntvMinusOne = intv - 1;
	}
	
	public synchronized void setAverageInterval( int intv ) {
		avgIntv = intv;
		avgIntvMinusOne = intv - 1;
	}

	public void bytesRecv( long val ) {
		crecv.addAndGet( val );
	}

	public void bytesXmit( long val ) {
		cxmit.addAndGet( val );
	}

	public final synchronized void run() {
		recv = ( (recv * avgIntvMinusOne ) + crecv.getAndSet(0) ) / avgIntv;
		xmit = ( (xmit * avgIntvMinusOne ) + cxmit.getAndSet(0) ) / avgIntv;
	}
}

Gregg Wonderly

From tim at peierls.net  Wed Nov 15 15:55:37 2006
From: tim at peierls.net (Tim Peierls)
Date: Wed, 15 Nov 2006 15:55:37 -0500
Subject: [concurrency-interest] Concurrent telemetry gathering
In-Reply-To: <455B7461.9050600@cytetech.com>
References: <455B7461.9050600@cytetech.com>
Message-ID: <63b4e4050611151255r555a1ce4g23d7be0d5708823d@mail.gmail.com>

Do you really need avgIntvMinusOne? How about making avgIntv volatile, get
rid of avgInvMinusOne, remove the synchronized keyword from
setAverageInterval, and have run() look like this:

public final void run() {
    int a = avgIntv;
    int r = crecv.getAndSet(0);
    int x = cxmit.getAndSet(0);
    synchronized (this) {
        recv = ((recv * (a-1)) + r) / a;
        xmit = ((xmit * (a-1)) + x) / a;
    }
}

Also, make crecv and cxmit final, add @GuardedBy("this") to recv and xmit,
and make conn final in addition to transient.

What's the serializiability for? Will deserialization resubmit map entries
to the STPE?

--tim

On 11/15/06, Gregg Wonderly <gregg at cytetech.com> wrote:
>
> I'm working on collecting telemetry regarding bandwidth utilization
> through a
> "connection".  I've defined the following class, thinking that this should
> minimize concurrency for updating the numbers.
>
> In addition, there is a
> ConcurrentHashMap<OutboundConnection,ConnectionBandwidth> object that is
> accessed by threads of execution which are handling the I/O requests
> through the
> system.  That map is lazily initialized with putIfAbsent().  When the
> first
> entry is added, there is a ScheduledThreadPoolExecutor used to schedule a
> regular execution of "run" to create a time based running average of
> bandwidth
> usage.
>
> Is there anything else I should do differently?
>
> public class ConnectionBandwidth implements Runnable, Serializable {
>         private double recv, xmit;
>         private AtomicLong crecv = new AtomicLong(), cxmit = new
> AtomicLong();
>         private transient OutboundConnection conn;
>         private double avgIntv, avgIntvMinusOne;
>
>         public ConnectionBandwidth( OutboundConnection conn, int intv ) {
>                 this.conn = conn;
>                 avgIntv = intv;
>                 avgIntvMinusOne = intv - 1;
>         }
>
>         public synchronized void setAverageInterval( int intv ) {
>                 avgIntv = intv;
>                 avgIntvMinusOne = intv - 1;
>         }
>
>         public void bytesRecv( long val ) {
>                 crecv.addAndGet( val );
>         }
>
>         public void bytesXmit( long val ) {
>                 cxmit.addAndGet( val );
>         }
>
>         public final synchronized void run() {
>                 recv = ( (recv * avgIntvMinusOne ) + crecv.getAndSet(0) )
> / avgIntv;
>                 xmit = ( (xmit * avgIntvMinusOne ) + cxmit.getAndSet(0) )
> / avgIntv;
>         }
> }
>
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061115/243bef2f/attachment.html 

From dcholmes at optusnet.com.au  Wed Nov 15 17:20:01 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 16 Nov 2006 08:20:01 +1000
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and
	stalledjobs
In-Reply-To: <583451.39399.qm@web50710.mail.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEBKHEAA.dcholmes@optusnet.com.au>

I'm sorry I don't have any further suggestions. I ran your code with a
trivial MyTask that printed out its name and iteration number and it worked
fine.

I wonder whether you might simply be saturating on the CPU usage and so only
able to complete a fraction of the work. There is no guarantee of fairness
so it could be a particular subset of the tasks that is excluded.

While the programm is running, do a thread dump to see where all the pool
threads are.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Harsha
> Nagesh
> Sent: Thursday, 16 November 2006 3:35 AM
> To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] ScheduledThreadPoolExecutor and
> stalledjobs
>
>
> Hi David,
>
> Thanks for explaining the executor's operating model. I suspected that
> something similar might be happening. So I tried the same code with the
> scheduleWithFixedDelay method, which, according to the API
> description will
> schedule the next instance of the task only after the current one
> is completed
> (with a certain delay). But I noticed the same behavior there
> too. Some tasks
> may take longer to complete than others, but if the next
> execution of a task is
> scheduled after a certain delay from the completion of the
> current run, I don't
> understand why it would stop further executions of the task.
>
> As far as MyTask is concerned, its just a simple computation. I
> don't think it
> is blocking as I can see the end result of the task printed out, before it
> stalls further scheduling.
>
> Any more pointers or ideas to try out will be greatly helpful.
>
> Thanks,
> Harsha
>
> --- David Holmes <dcholmes at optusnet.com.au> wrote:
>
> > Just had a thought ...
> >
> > ScheduledThreadPoolExecutor operates by dequeing a task and
> executing it,
> > then requeuing afterwards. This means that a task is never running
> > concurrently with itself if it executes beyond its next release time.
> >
> > A different model of periodic task execution would leave the task in the
> > queue so that every period a new execution of it commences, even if the
> > previous one had not completed.
> >
> > If you expected this second model then that is not what you get with
> > ScheduledThreadPoolExecutor.
> >
> > Hope that helps.
> >
> > David Holmes
> >
> > > -----Original Message-----
> > > From: concurrency-interest-bounces at cs.oswego.edu
> > > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Harsha
> > > Nagesh
> > > Sent: Wednesday, 15 November 2006 11:46 AM
> > > To: concurrency-interest at cs.oswego.edu
> > > Subject: [concurrency-interest] ScheduledThreadPoolExecutor
> and stalled
> > > jobs
> > >
> > >
> > > Hi,
> > >
> > >      I am trying to use ScheduledThreadPoolExecutor to
> schedule a bunch of
> > > jobs, each of which should be scheduled every minute. I am
> > > running this on win
> > > XP and using a poolsize = 20 (threads). However, what I see is
> > > that out of the
> > > 100 or so jobs that have been added to the scheduler, there is a
> > > subset of them
> > > (more than 20%) which seem to be scheduled once and never again
> > > after the first
> > > time. I am not sure how to go about debugging this issue. Any
> > > pointers will be
> > > greatly appreciated. Here is the skeleton of code that shows the
> > > way I am using
> > > the scheduler and I am not sure if this is the best way to
> > > achieve my objective
> > >
> > > int poolSize = 20;
> > > long period = 60; //1 minute
> > > long delay = 60; //1 minute
> > > TimeUnit timeUnit = TimeUnit.SECONDS;
> > >
> > > ScheduledThreadPoolExecutor pool = new
> > >           ScheduledThreadPoolExecutor(poolSize);
> > > for(int i = 0; i < 100; i++){
> > >    MyTask task = new MyTask();
> > >    pool.scheduleAtFixedRate(task,0,period,timeUnit);
> > > }
> > >
> > > I also tried to use
> > >
> > > pool.scheduleWithFixedDelay(task,0,delay,timeUnit);
> > >
> > > instead of the fixedRate scheduling, but I found the same
> > > behavior and many of
> > > my jobs were stuck and not executed after the first run.
> > >
> > > Any ideas ?
> > >
> > > Thanks,
> > > Harsha
> > >
> > >
> > >
> > >
> > > __________________________________________________________________
> > > __________________
> > > Yahoo! Music Unlimited
> > > Access over 1 million songs.
> > > http://music.yahoo.com/unlimited
> > > _______________________________________________
> > > Concurrency-interest mailing list
> > > Concurrency-interest at altair.cs.oswego.edu
> > > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
>
>
>
>
> __________________________________________________________________
> __________________
> Sponsored Link
>
> Mortgage rates near 39yr lows.
> $310k for $999/mo. Calculate new payment!
> www.LowerMyBills.com/lre
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From kav at it.edu  Thu Nov 16 04:32:14 2006
From: kav at it.edu (Kasper Nielsen)
Date: Thu, 16 Nov 2006 10:32:14 +0100
Subject: [concurrency-interest] Concurrent telemetry gathering
In-Reply-To: <455B7461.9050600@cytetech.com>
References: <455B7461.9050600@cytetech.com>
Message-ID: <455C301E.4090907@it.edu>

Gregg Wonderly wrote:
> I'm working on collecting telemetry regarding bandwidth utilization through a 
> "connection".  I've defined the following class, thinking that this should 
> minimize concurrency for updating the numbers.
> 
> In addition, there is a 
> ConcurrentHashMap<OutboundConnection,ConnectionBandwidth> object that is 
> accessed by threads of execution which are handling the I/O requests through the 
> system.  That map is lazily initialized with putIfAbsent().  When the first 
> entry is added, there is a ScheduledThreadPoolExecutor used to schedule a 
> regular execution of "run" to create a time based running average of bandwidth 
> usage.
> 
> Is there anything else I should do differently?

Have you considered using (single) exponential smoothing instead. What 
are you using the numbers for?

- Kasper

From kav at it.edu  Thu Nov 16 04:50:21 2006
From: kav at it.edu (Kasper Nielsen)
Date: Thu, 16 Nov 2006 10:50:21 +0100
Subject: [concurrency-interest] Concurrent telemetry gathering
In-Reply-To: <455C301E.4090907@it.edu>
References: <455B7461.9050600@cytetech.com> <455C301E.4090907@it.edu>
Message-ID: <455C345D.6040104@it.edu>

Kasper Nielsen wrote:
> Gregg Wonderly wrote:
>> I'm working on collecting telemetry regarding bandwidth utilization through a 
>> "connection".  I've defined the following class, thinking that this should 
>> minimize concurrency for updating the numbers.
>>
>> In addition, there is a 
>> ConcurrentHashMap<OutboundConnection,ConnectionBandwidth> object that is 
>> accessed by threads of execution which are handling the I/O requests through the 
>> system.  That map is lazily initialized with putIfAbsent().  When the first 
>> entry is added, there is a ScheduledThreadPoolExecutor used to schedule a 
>> regular execution of "run" to create a time based running average of bandwidth 
>> usage.
>>
>> Is there anything else I should do differently?
> 
> Have you considered using (single) exponential smoothing instead. What 
> are you using the numbers for?
> 
Having had a second look on the code, I see that you are already using 
exponential smoothing with a smoothing constant of 1/avgIntv. Never mind 
then.

- Kasper

From gergg at cox.net  Wed Nov 15 21:27:07 2006
From: gergg at cox.net (Gregg Wonderly)
Date: Wed, 15 Nov 2006 20:27:07 -0600
Subject: [concurrency-interest] Concurrent telemetry gathering
In-Reply-To: <63b4e4050611151255r555a1ce4g23d7be0d5708823d@mail.gmail.com>
References: <455B7461.9050600@cytetech.com>
	<63b4e4050611151255r555a1ce4g23d7be0d5708823d@mail.gmail.com>
Message-ID: <455BCC7B.2060502@cox.net>

Tim Peierls wrote:
> Do you really need avgIntvMinusOne? 

I suppose not, but am trying to minimize CPU as much as possible as this will be 
a very active part of the code.

 > How about making avgIntv volatile,
> get rid of avgInvMinusOne, remove the synchronized keyword from 
> setAverageInterval, and have run() look like this:
> 
> public final void run() {
>     int a = avgIntv;
>     int r = crecv.getAndSet(0);
>     int x = cxmit.getAndSet(0);
>     synchronized (this) {
>         recv = ((recv * (a-1)) + r) / a;
>         xmit = ((xmit * (a-1)) + x) / a;
>     }
> }

I was worried about concurrent access to avgIntv in the setter.   Copying its 
value and using that as an atomically created single value should solve that 
problem!

> Also, make crecv and cxmit final, add @GuardedBy("this") to recv and 
> xmit, and make conn final in addition to transient.

Okay, all good ideas!

> What's the serializiability for? Will deserialization resubmit map 
> entries to the STPE?

It just goes via Jini/RMI to a remote console that uses the computed numbers to 
display some statistics.  It never goes back into an STPE.

Gregg Wonderly


From dawidk at mathcs.emory.edu  Thu Nov 16 13:44:00 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Thu, 16 Nov 2006 13:44:00 -0500
Subject: [concurrency-interest] backport-util-concurrent 3.0 released
Message-ID: <455CB170.40701@mathcs.emory.edu>


http://dcl.mathcs.emory.edu/util/backport-util-concurrent/

Enjoy!

Dawid


From pfeiffer at tzi.de  Thu Nov 16 17:03:02 2006
From: pfeiffer at tzi.de (Oliver Pfeiffer)
Date: Thu, 16 Nov 2006 23:03:02 +0100
Subject: [concurrency-interest] ReadWriteLock implementation with
	extendedpolicy providing upgradeable read-locks?
In-Reply-To: <13086732.1163208888748.JavaMail.pfeiffer@mailhost>
Message-ID: <VPOP31.4.6.20061116230316.283.88.1.3a27a341@mythos>

Hello David,

our use-case for an intention lock is the common "prepare for write"
operation. Our application model contains a complex time-based interval
storage for telemetry data. This storage is accessed concurrently by many
readers (acquiring old telemetry) and many writers (providing new
telemetry). Each writer must only provide data that doesn't already exist in
the model, thus the writer must only submit the delta to the model. The
submission itself is a very fast operation, but the delta creation (by
intersecting thousands of intervals) takes much longer. Since the model
isn't changed by the delta creation, there is no reason to exclude other
readers - only the other writers must be blocked. Using an intention lock
would provide a significant performance boost for this use-case.

-- 
Gr??e - Regards
Oliver Pfeiffer
ICQ-ID 84320006 

> -----Original Message-----
> From: David Holmes [mailto:dcholmes at optusnet.com.au] 
> Sent: Saturday, November 11, 2006 2:34 AM
> To: Oliver Pfeiffer; concurrency-interest at cs.oswego.edu
> Subject: RE: [concurrency-interest] ReadWriteLock 
> implementation with extendedpolicy providing upgradeable read-locks?
> 
> Hello Oliver,
> 
> I think this makes the third request for such functionality 
> since 2000 -
> hence it hasn't been a high priority :)
> 
> Intention-locks were discussed by the EG a couple of times 
> but basically
> they didn't meet the acceptance criteria. They are quite 
> complicated to
> implement correctly, have ugly API's and don't inter-mix with normal
> ReadWriteLocks. They are also rarely needed oustide 
> transactional contexts.
> But maybe it is time to consider them again for Java 7.
> 
> That said, with changes that went in to Java 6 to track 
> read-lock ownership,
> it shouldn't be too hard for you to modify 
> ReentrantReadWriteLock to allow
> the final-reader to acquire the writeLock. That isn't quite an
> intention-lock but might satisfy your needs.
> 
> Can you describe the use-case for this.
> 
> Cheers,
> David Holmes
> 
> 
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On 
> Behalf Of Oliver
> > Pfeiffer
> > Sent: Friday, 10 November 2006 9:57 PM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: [concurrency-interest] ReadWriteLock implementation with
> > extendedpolicy providing upgradeable read-locks?
> >
> >
> > I'm looking for a certain ReadWriteLock implementation 
> following a very
> > similar policy as Doug Leas ReentrantReadWriteLock except 
> that the last
> > reader is enabled to be upgraded to a writer. The lack of 
> these concept in
> > default JDK15 distribution is the source of most threading issues
> > I have had
> > to fix in the last months in foreign code.
> >
> > Unfortunately it's obvious that this can't be solved by the common
> > read-write-lock concept as provided by the interface, since two
> > simultaneous
> > threads, each holding the read-lock, may both acquire the 
> write-lock and
> > therefore will consequently deadlock each other. This seems 
> to be solvable
> > only by classifying read-locks as exlusive-read-locks and
> > non-exclusive-read-locks. The exclusive-read-lock is a 
> normal read-lock
> > except that it can only be shared with other 
> non-exclusive-read-locks (but
> > not with other exclusive-read-locks or write-locks). In 
> other words: there
> > can be only one thread holding the exclusive-read-lock per 
> time thus this
> > exclusice-read-lock can be safely upgraded to a write-lock
> > without any risc
> > of deadlocking. Certainly any non-exclusive-read-lock acquiring the
> > write-lock or the exclusive-read-lock will block foreever, but an
> > exclusive-read-lock can always acquire the 
> non-exclusive-read-lock and the
> > write-lock.
> >
> > The benefit of such a concept is the ability to share many 
> readers with a
> > single reader that _might_ be need to lazily become a writer. Or
> > in general
> > there is no need to exclusively lock a model when there are 
> time-consuming
> > preparations needed before a write-lock is actually 
> acquired. This can
> > result in a performance gain bundeled with a less risc of 
> deadlocking.
> >
> > Does anybody knows a serious extension of default java
> > concurrency features
> > providing such a concept?
> >
> > --
> > Gr??e - Regards
> > Oliver Pfeiffer
> > ICQ-ID 84320006
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >



From alarmnummer at gmail.com  Fri Nov 17 06:12:37 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Fri, 17 Nov 2006 12:12:37 +0100
Subject: [concurrency-interest] backport-util-concurrent 3.0 released
In-Reply-To: <455CB170.40701@mathcs.emory.edu>
References: <455CB170.40701@mathcs.emory.edu>
Message-ID: <1466c1d60611170312j389759c2j77a3122eae9223ed@mail.gmail.com>

Great. I haven't been able to work with Java 5 for a while, so this
backport is really a life saver. Keep up the good work!

On 11/16/06, Dawid Kurzyniec <dawidk at mathcs.emory.edu> wrote:
>
> http://dcl.mathcs.emory.edu/util/backport-util-concurrent/
>
> Enjoy!
>
> Dawid
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From alarmnummer at gmail.com  Sun Nov 19 07:45:06 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 19 Nov 2006 13:45:06 +0100
Subject: [concurrency-interest] why bytecode difference between synchronized
	block and method
Message-ID: <1466c1d60611190445r7a40b4b5s40c76e3bb5c63e6d@mail.gmail.com>

Why is there a difference in the bytecode between a synchronized block
(as first statement of a method) and a synchronized method? I don't
understand why there should be any difference but if I look at the
bytecode of the following example, there is.

example:
import java.util.*;

public class DifferenceMain{

    private List list = new LinkedList();

    public void foo(){
		synchronized(this){
			if(list.size()==0)
				list.add("foo");
		}
	}

    public synchronized void bar(){
		if(list.size()==0)
			list.add("bar");
    }

    public static void main(String s){
    }
}


bytecode:
Compiled from "DifferenceMain.java"
public class DifferenceMain extends java.lang.Object{
public DifferenceMain();
  Code:
   0:	aload_0
   1:	invokespecial	#1; //Method java/lang/Object."<init>":()V
   4:	aload_0
   5:	new	#2; //class java/util/LinkedList
   8:	dup
   9:	invokespecial	#3; //Method java/util/LinkedList."<init>":()V
   12:	putfield	#4; //Field list:Ljava/util/List;
   15:	return

public void foo();
  Code:
   0:	aload_0
   1:	dup
   2:	astore_1
   3:	monitorenter
   4:	aload_0
   5:	getfield	#4; //Field list:Ljava/util/List;
   8:	invokeinterface	#5,  1; //InterfaceMethod java/util/List.size:()I
   13:	ifne	28
   16:	aload_0
   17:	getfield	#4; //Field list:Ljava/util/List;
   20:	ldc	#6; //String foo
   22:	invokeinterface	#7,  2; //InterfaceMethod
java/util/List.add:(Ljava/lang/Object;)Z
   27:	pop
   28:	aload_1
   29:	monitorexit
   30:	goto	38
   33:	astore_2
   34:	aload_1
   35:	monitorexit
   36:	aload_2
   37:	athrow
   38:	return
  Exception table:
   from   to  target type
     4    30    33   any
    33    36    33   any

public synchronized void bar();
  Code:
   0:	aload_0
   1:	getfield	#4; //Field list:Ljava/util/List;
   4:	invokeinterface	#5,  1; //InterfaceMethod java/util/List.size:()I
   9:	ifne	24
   12:	aload_0
   13:	getfield	#4; //Field list:Ljava/util/List;
   16:	ldc	#8; //String bar
   18:	invokeinterface	#7,  2; //InterfaceMethod
java/util/List.add:(Ljava/lang/Object;)Z
   23:	pop
   24:	return

public static void main(java.lang.String);
  Code:
   0:	return

}

From tackline at tackline.plus.com  Sun Nov 19 09:21:46 2006
From: tackline at tackline.plus.com (Thomas Hawtin)
Date: Sun, 19 Nov 2006 14:21:46 +0000
Subject: [concurrency-interest] why bytecode difference between
 synchronized block and method
In-Reply-To: <1466c1d60611190445r7a40b4b5s40c76e3bb5c63e6d@mail.gmail.com>
References: <1466c1d60611190445r7a40b4b5s40c76e3bb5c63e6d@mail.gmail.com>
Message-ID: <4560687A.50904@tackline.plus.com>

Peter Veentjer wrote:
> Why is there a difference in the bytecode between a synchronized block
> (as first statement of a method) and a synchronized method? I don't
> understand why there should be any difference but if I look at the
> bytecode of the following example, there is.

For compactness, there is a special representation for synchronized 
methods (it's just a bit, together with abstract, public, protected, 
private, etc. flags). If you write it long hand, so does javac: 
monitor-enter(this); try { ... } catch (* exc) { monitor-exit(this); 
throw exc; } monitor-exit(this);.

By the time it is has been compiled into machine code, there should be 
no discernible difference.

Tom Hawtin

From thanot at infovista.com  Wed Nov 22 07:18:19 2006
From: thanot at infovista.com (Thierry Hanot)
Date: Wed, 22 Nov 2006 13:18:19 +0100
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
Message-ID: <0329C0D74DD1D64F98E6C6B43B5C78108B5420@frlulms01.iv.local>

 

A small proposal for adding a new class in the concurrent package.

All bounded collection are bounded to avoid to use to much memory.(At
least in my case :-)).

But the element put in those collections are often with different size. 

In my case  BoundedQueue is used as an event queue and we can have
composite events which contains itself many events.

What do you think about adding some bounded collection no more based on
the count but on the sum of the weight of the object inside?

 

After a quick look on the code of the LinkedBoundedQueue it seems pretty
easy to do.

 

Does somebody else can see the advantage of this kind of object and is
there enough people interested to make it a part of the concurrent
package?

 

B.R

 

 

Thierry Hanot  

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061122/90fb1313/attachment.html 

From dcholmes at optusnet.com.au  Wed Nov 22 19:26:06 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 23 Nov 2006 10:26:06 +1000
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <0329C0D74DD1D64F98E6C6B43B5C78108B5420@frlulms01.iv.local>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEDEHEAA.dcholmes@optusnet.com.au>

How do you define and measure the "weight" of an object?

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry
Hanot
  Sent: Wednesday, 22 November 2006 10:18 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue




  A small proposal for adding a new class in the concurrent package.

  All bounded collection are bounded to avoid to use to much memory.(At
least in my case J).

  But the element put in those collections are often with different size.

  In my case  BoundedQueue is used as an event queue and we can have
composite events which contains itself many events.

  What do you think about adding some bounded collection no more based on
the count but on the sum of the weight of the object inside?



  After a quick look on the code of the LinkedBoundedQueue it seems pretty
easy to do.



  Does somebody else can see the advantage of this kind of object and is
there enough people interested to make it a part of the concurrent package?



  B.R





  Thierry Hanot


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061123/a8aff019/attachment.html 

From amar_nan at yahoo.com  Wed Nov 22 21:54:29 2006
From: amar_nan at yahoo.com (Amarnath Nanduri)
Date: Wed, 22 Nov 2006 18:54:29 -0800 (PST)
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEDEHEAA.dcholmes@optusnet.com.au>
Message-ID: <734786.2130.qm@web31214.mail.mud.yahoo.com>

Defined by the end user???? through interfaces or some other means...

David Holmes <dcholmes at optusnet.com.au> wrote:         How do you define and measure the "weight" of an object?  
  
 David Holmes
    -----Original Message-----
From:    concurrency-interest-bounces at cs.oswego.edu    [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry    Hanot
Sent: Wednesday, 22 November 2006 10:18 PM
To:    concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest]    Proposal WeightedLinkedBoundedQueue


       
   A small proposal for adding a new    class in the concurrent package.
   All bounded collection are bounded    to avoid to use to much memory.(At least in my case J).
   But the element put in those    collections are often with different size. 
   In my case  BoundedQueue is    used as an event queue and we can have composite events which contains itself    many events.
   What do you think about adding    some bounded collection no more based on the count but on the sum of the    weight of the object inside?
    
   After a quick look on the code of    the LinkedBoundedQueue it seems pretty easy to  do.
    
   Does somebody else can see the    advantage of this kind of object and is there enough people interested to make    it a part of the concurrent package?
    
   B.R
    
    
   Thierry Hanot     
    

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


 
---------------------------------
Everyone is raving about the all-new Yahoo! Mail beta.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061122/9189508e/attachment.html 

From thanot at infovista.com  Thu Nov 23 06:43:25 2006
From: thanot at infovista.com (Thierry Hanot)
Date: Thu, 23 Nov 2006 12:43:25 +0100
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
Message-ID: <0329C0D74DD1D64F98E6C6B43B5C78108B5426@frlulms01.iv.local>

The weight is defined by the end user using a small interface yes.

I use the term weight but size is also relevant :-)

 

 

Thierry Hanot  

________________________________

From: David Holmes [mailto:dcholmes at optusnet.com.au] 
Sent: jeudi 23 novembre 2006 01:26
To: Thierry Hanot; concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] Proposal WeightedLinkedBoundedQueue

 

How do you define and measure the "weight" of an object? 

 

David Holmes

	-----Original Message-----
	From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry
Hanot
	Sent: Wednesday, 22 November 2006 10:18 PM
	To: concurrency-interest at cs.oswego.edu
	Subject: [concurrency-interest] Proposal
WeightedLinkedBoundedQueue

	 

	A small proposal for adding a new class in the concurrent
package.

	All bounded collection are bounded to avoid to use to much
memory.(At least in my case :-)).

	But the element put in those collections are often with
different size. 

	In my case  BoundedQueue is used as an event queue and we can
have composite events which contains itself many events.

	What do you think about adding some bounded collection no more
based on the count but on the sum of the weight of the object inside?

	 

	After a quick look on the code of the LinkedBoundedQueue it
seems pretty easy to do.

	 

	Does somebody else can see the advantage of this kind of object
and is there enough people interested to make it a part of the
concurrent package?

	 

	B.R

	 

	 

	Thierry Hanot  

	 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061123/721889dc/attachment-0001.html 

From alarmnummer at gmail.com  Thu Nov 23 15:18:33 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 23 Nov 2006 21:18:33 +0100
Subject: [concurrency-interest] lock removal by compiler together with
	monitor-lock order rule
Message-ID: <1466c1d60611231218g2073f44n459829af445e8d7@mail.gmail.com>

I have a question about the compiler removing 'useless' synchronizations.

eg:

class Foo{
    int value;

   setValue(int value){
       this.value = value;
       synchronized(this){}
   }

   int getValue(){
       synchronized(this){}
       return value;
   }
}

In this case the release of the lock in setValue and the get of the
lock in getValue. provides safe 'piggy backing on synchronization' for
the variable value.

Removal of the synchronized blocks could change program behaviour. Is
the compiler to allowed to remove these locks? My guess would be that
this isn't allowed.

From alarmnummer at gmail.com  Thu Nov 23 15:45:26 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 23 Nov 2006 21:45:26 +0100
Subject: [concurrency-interest] lock removal by compiler together with
	monitor-lock order rule
In-Reply-To: <1466c1d60611231218g2073f44n459829af445e8d7@mail.gmail.com>
References: <1466c1d60611231218g2073f44n459829af445e8d7@mail.gmail.com>
Message-ID: <1466c1d60611231245x5ec74d29w7d52841a93b49701@mail.gmail.com>

Or could it be ,that the lack of program order n the content of the
methods are the cause that piggybacking on synchronization doesn't
hold? If there is no program order, there is no piggybacking. This
means,there was no guaranteed behaviour in the first place.

On 11/23/06, Peter Veentjer <alarmnummer at gmail.com> wrote:
> I have a question about the compiler removing 'useless' synchronizations.
>
> eg:
>
> class Foo{
>     int value;
>
>    setValue(int value){
>        this.value = value;
>        synchronized(this){}
>    }
>
>    int getValue(){
>        synchronized(this){}
>        return value;
>    }
> }
>
> In this case the release of the lock in setValue and the get of the
> lock in getValue. provides safe 'piggy backing on synchronization' for
> the variable value.
>
> Removal of the synchronized blocks could change program behaviour. Is
> the compiler to allowed to remove these locks? My guess would be that
> this isn't allowed.
>

From joe.bowbeer at gmail.com  Thu Nov 23 16:32:04 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 23 Nov 2006 13:32:04 -0800
Subject: [concurrency-interest] lock removal by compiler together with
	monitor-lock order rule
In-Reply-To: <1466c1d60611231245x5ec74d29w7d52841a93b49701@mail.gmail.com>
References: <1466c1d60611231218g2073f44n459829af445e8d7@mail.gmail.com>
	<1466c1d60611231245x5ec74d29w7d52841a93b49701@mail.gmail.com>
Message-ID: <31f2a7bd0611231332y530ef1fat73ea14a239d352de@mail.gmail.com>

On 11/23/06, Peter Veentjer <alarmnummer at gmail.com> wrote:
> I have a question about the compiler removing 'useless' synchronizations.
>
> eg:
>
> class Foo{
>     int value;
>
>    setValue(int value){
>        this.value = value;
>        synchronized(this){}
>    }
>
>    int getValue(){
>        synchronized(this){}
>        return value;
>    }
> }
>

The (runtime) compiler is not allowed to remove synchronized blocks if
there's any chance that multiple threads will be executing them.  If,
for example, the compiler is smart enough to determine that these
methods are only called by the same thread, then the synchronized
blocks could be removed.

By the way, the "roach motel" semantics would allow your assignments
to move into the synchronized block from above or below, but not to
leave the synchronized block, or jump over it.

From dcholmes at optusnet.com.au  Thu Nov 23 19:13:30 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 24 Nov 2006 10:13:30 +1000
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <0329C0D74DD1D64F98E6C6B43B5C78108B5426@frlulms01.iv.local>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEDNHEAA.dcholmes@optusnet.com.au>

Ok - but practically speaking how to you calculate the size/weight? We often
talk informally about lightweight and heavyweight objects, but we don't
actually have a simple means of quantifying that. Are you just working on
the basis that if the user can define some notion of "weight" then this data
structure can impose a weight limit?

It seems rather special purpose. If weight was memory-usage, and we had a
way to measure that, then I could see use for this in some memory
constrained contexts.

As it is I would think you could do this via a fairly simple wrapper, as
you'll need to track the cumulative weight as objects are added/removed.
You'll need stronger synchronization than provided by LinkedBlockingQueue
unless the weight-limit is allowed to be violated - so adding the sync in
the wrapper seems reasonable too.

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry
Hanot
  Sent: Thursday, 23 November 2006 9:43 PM
  To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Proposal WeightedLinkedBoundedQueue


  The weight is defined by the end user using a small interface yes.

  I use the term weight but size is also relevant J





  Thierry Hanot


----------------------------------------------------------------------------
--

  From: David Holmes [mailto:dcholmes at optusnet.com.au]
  Sent: jeudi 23 novembre 2006 01:26
  To: Thierry Hanot; concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] Proposal WeightedLinkedBoundedQueue



  How do you define and measure the "weight" of an object?



  David Holmes

    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry
Hanot
    Sent: Wednesday, 22 November 2006 10:18 PM
    To: concurrency-interest at cs.oswego.edu
    Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue



    A small proposal for adding a new class in the concurrent package.

    All bounded collection are bounded to avoid to use to much memory.(At
least in my case J).

    But the element put in those collections are often with different size.

    In my case  BoundedQueue is used as an event queue and we can have
composite events which contains itself many events.

    What do you think about adding some bounded collection no more based on
the count but on the sum of the weight of the object inside?



    After a quick look on the code of the LinkedBoundedQueue it seems pretty
easy to do.



    Does somebody else can see the advantage of this kind of object and is
there enough people interested to make it a part of the concurrent package?



    B.R





    Thierry Hanot


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061124/f4e03173/attachment.html 

From thanot at infovista.com  Fri Nov 24 06:11:46 2006
From: thanot at infovista.com (Thierry Hanot)
Date: Fri, 24 Nov 2006 12:11:46 +0100
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
Message-ID: <0329C0D74DD1D64F98E6C6B43B5C78108B542E@frlulms01.iv.local>

Ok - but practically speaking how to you calculate the size/weight? We
often talk informally about lightweight and heavyweight objects, but we
don't actually have a simple means of quantifying that. Are you just
working on the basis that if the user can define some notion of "weight"
then this data structure can impose a weight limit?

 

Yes because the weight depends a lot about the architecture of what you
are doing.

For instance if you use a queue to store task, the weight of the task
can an estimation of the cost of the task ( refuse task if you want to
be sure to guarantee the execution in a certain amount of time). If for
a reason or another the task is also containing data used during the
execution, then the weight can be an estimation of its size.

In my case the queue contains data to be store in a database. Each data
is referenced by a composite id and we have some other information about
the impact of the data. To avoid to have for each event the 3 objects
(the id,the data and the impacts). We have one event which is containing
one id , n data and one compacted impact list.

Then the size of an event is not 1. And when we want to avoid using to
much memory we need to use a weight estimation which instead of the
count to bound the queue.

 

 

It seems rather special purpose. If weight was memory-usage, and we had
a way to measure that, then I could see use for this in some memory
constrained contexts.

 

I agree, it's a bit specific. But the queues are very useful to have
some kind of "resource usage control".  The resource cannot be
represented by the counts of objects in the queue or by the memory but
can be of any kind ( memory mostly but we can have network/cpu/disk
...). And most of the applications need the resource control (except if
you have unlimited resources) 

 

As it is I would think you could do this via a fairly simple wrapper, as
you'll need to track the cumulative weight as objects are added/removed.
You'll need stronger synchronization than provided by
LinkedBlockingQueue unless the weight-limit is allowed to be violated -
so adding the sync in the wrapper seems reasonable too.

 

I was thinking of extending the LinkedBoundedQueue because we can
generalize all the principle in this queue easily. But the locks and
conditions are private:-(. 

The main change in the code is: 

instead of using getAndIncrement(or decrement) we can use getAndAdd(+ or
- element.weigth) and  test the not full no anymore with an equality on
capacity but on >=. 

With this we can avoid any stronger synchronization ( even if we will
probably notify a bit too much the notFull condition).

 

 

 

 

Cheers,

David Holmes

 

 

Best regards,

Thierry Hanot

 

	-----Original Message-----
	From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry
Hanot
	Sent: Thursday, 23 November 2006 9:43 PM
	To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
	Subject: Re: [concurrency-interest] Proposal
WeightedLinkedBoundedQueue

	The weight is defined by the end user using a small interface
yes.

	I use the term weight but size is also relevant :-)

	 

	 

	Thierry Hanot  

	
________________________________


	From: David Holmes [mailto:dcholmes at optusnet.com.au] 
	Sent: jeudi 23 novembre 2006 01:26
	To: Thierry Hanot; concurrency-interest at cs.oswego.edu
	Subject: RE: [concurrency-interest] Proposal
WeightedLinkedBoundedQueue

	 

	How do you define and measure the "weight" of an object? 

	 

	David Holmes

		-----Original Message-----
		From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry
Hanot
		Sent: Wednesday, 22 November 2006 10:18 PM
		To: concurrency-interest at cs.oswego.edu
		Subject: [concurrency-interest] Proposal
WeightedLinkedBoundedQueue

		 

		A small proposal for adding a new class in the
concurrent package.

		All bounded collection are bounded to avoid to use to
much memory.(At least in my case :-)).

		But the element put in those collections are often with
different size. 

		In my case  BoundedQueue is used as an event queue and
we can have composite events which contains itself many events.

		What do you think about adding some bounded collection
no more based on the count but on the sum of the weight of the object
inside?

		 

		After a quick look on the code of the LinkedBoundedQueue
it seems pretty easy to do.

		 

		Does somebody else can see the advantage of this kind of
object and is there enough people interested to make it a part of the
concurrent package?

		 

		B.R

		 

		 

		Thierry Hanot  

		 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061124/6e3047de/attachment-0001.html 

From tim at peierls.net  Fri Nov 24 12:48:20 2006
From: tim at peierls.net (Tim Peierls)
Date: Fri, 24 Nov 2006 12:48:20 -0500
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <0329C0D74DD1D64F98E6C6B43B5C78108B542E@frlulms01.iv.local>
References: <0329C0D74DD1D64F98E6C6B43B5C78108B542E@frlulms01.iv.local>
Message-ID: <63b4e4050611240948j232cd4f1ncca4ce06c859ece1@mail.gmail.com>

If this is about resource control, couldn't you use a Semaphore to bound the
resource usage and any kind of thread-safe queue to provide the resource
ordering?

put(x) => sem.acquire(x.weight()); queue.put(x);
take() => x = queue.take(); sem.release(x.weight());

where sem is initialized with the resource bound.

--tim

On 11/24/06, Thierry Hanot <thanot at infovista.com> wrote:
>
>  Ok - but practically speaking how to you calculate the size/weight? We
> often talk informally about lightweight and heavyweight objects, but we
> don't actually have a simple means of quantifying that. Are you just working
> on the basis that if the user can define some notion of "weight" then this
> data structure can impose a weight limit?
>
>
>
> Yes because the weight depends a lot about the architecture of what you
> are doing.
>
> For instance if you use a queue to store task, the weight of the task can
> an estimation of the cost of the task ( refuse task if you want to be sure
> to guarantee the execution in a certain amount of time). If for a reason or
> another the task is also containing data used during the execution, then the
> weight can be an estimation of its size.
>
> In my case the queue contains data to be store in a database. Each data is
> referenced by a composite id and we have some other information about the
> impact of the data. To avoid to have for each event the 3 objects (the
> id,the data and the impacts). We have one event which is containing one id ,
> n data and one compacted impact list.
>
> Then the size of an event is not 1. And when we want to avoid using to
> much memory we need to use a weight estimation which instead of the count to
> bound the queue.
>
>
>
>
>
> It seems rather special purpose. If weight was memory-usage, and we had a
> way to measure that, then I could see use for this in some memory
> constrained contexts.
>
>
>
> I agree, it's a bit specific. But the queues are very useful to have some
> kind of "resource usage control".  The resource cannot be represented by the
> counts of objects in the queue or by the memory but can be of any kind (
> memory mostly but we can have network/cpu/disk ?). And most of the
> applications need the resource control (except if you have unlimited
> resources)
>
>
>
> As it is I would think you could do this via a fairly simple wrapper, as
> you'll need to track the cumulative weight as objects are added/removed.
> You'll need stronger synchronization than provided by LinkedBlockingQueue
> unless the weight-limit is allowed to be violated - so adding the sync in
> the wrapper seems reasonable too.
>
>
>
> I was thinking of extending the LinkedBoundedQueue because we can
> generalize all the principle in this queue easily. But the locks and
> conditions are privateL.
>
> The main change in the code is:
>
> instead of using getAndIncrement(or decrement) we can use getAndAdd(+ or ?
> element.weigth) and  test the not full no anymore with an equality on
> capacity but on >=.
>
> With this we can avoid any stronger synchronization ( even if we will
> probably notify a bit too much the notFull condition).
>
>
>
>
>
>
>
>
>
> Cheers,
>
> David Holmes
>
>
>
>
>
> Best regards,
>
> Thierry Hanot
>
>
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Thierry Hanot
> *Sent:* Thursday, 23 November 2006 9:43 PM
> *To:* dholmes at ieee.org; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
>
> The weight is defined by the end user using a small interface yes.
>
> I use the term weight but size is also relevant J
>
>
>
>
>
> *Thierry Hanot*
>   ------------------------------
>
> *From:* David Holmes [mailto:dcholmes at optusnet.com.au]
> *Sent:* jeudi 23 novembre 2006 01:26
> *To:* Thierry Hanot; concurrency-interest at cs.oswego.edu
> *Subject:* RE: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
>
>
>
> How do you define and measure the "weight" of an object?
>
>
>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Thierry Hanot
> *Sent:* Wednesday, 22 November 2006 10:18 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Proposal WeightedLinkedBoundedQueue
>
>
>
> A small proposal for adding a new class in the concurrent package.
>
> All bounded collection are bounded to avoid to use to much memory.(At
> least in my case J).
>
> But the element put in those collections are often with different size.
>
> In my case  BoundedQueue is used as an event queue and we can have
> composite events which contains itself many events.
>
> What do you think about adding some bounded collection no more based on
> the count but on the sum of the weight of the object inside?
>
>
>
> After a quick look on the code of the LinkedBoundedQueue it seems pretty
> easy to do.
>
>
>
> Does somebody else can see the advantage of this kind of object and is
> there enough people interested to make it a part of the concurrent package?
>
>
>
> B.R
>
>
>
>
>
> *Thierry Hanot*
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061124/2b188fc9/attachment.html 

From dcholmes at optusnet.com.au  Sat Nov 25 05:32:33 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat, 25 Nov 2006 20:32:33 +1000
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <0329C0D74DD1D64F98E6C6B43B5C78108B542E@frlulms01.iv.local>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEEDHEAA.dcholmes@optusnet.com.au>

Heh Heh! I forgot about using an atomic to maintain the weight :)

Tim's generalised semaphore approach also sounds feasible - and much simpler
in terms of getting the blocking semantics for excess weight.

Cheers,
David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry
Hanot
  Sent: Friday, 24 November 2006 9:12 PM
  To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Proposal WeightedLinkedBoundedQueue


  Ok - but practically speaking how to you calculate the size/weight? We
often talk informally about lightweight and heavyweight objects, but we
don't actually have a simple means of quantifying that. Are you just working
on the basis that if the user can define some notion of "weight" then this
data structure can impose a weight limit?



  Yes because the weight depends a lot about the architecture of what you
are doing.

  For instance if you use a queue to store task, the weight of the task can
an estimation of the cost of the task ( refuse task if you want to be sure
to guarantee the execution in a certain amount of time). If for a reason or
another the task is also containing data used during the execution, then the
weight can be an estimation of its size.

  In my case the queue contains data to be store in a database. Each data is
referenced by a composite id and we have some other information about the
impact of the data. To avoid to have for each event the 3 objects (the
id,the data and the impacts). We have one event which is containing one id ,
n data and one compacted impact list.

  Then the size of an event is not 1. And when we want to avoid using to
much memory we need to use a weight estimation which instead of the count to
bound the queue.





  It seems rather special purpose. If weight was memory-usage, and we had a
way to measure that, then I could see use for this in some memory
constrained contexts.



  I agree, it's a bit specific. But the queues are very useful to have some
kind of "resource usage control".  The resource cannot be represented by the
counts of objects in the queue or by the memory but can be of any kind (
memory mostly but we can have network/cpu/disk .). And most of the
applications need the resource control (except if you have unlimited
resources)



  As it is I would think you could do this via a fairly simple wrapper, as
you'll need to track the cumulative weight as objects are added/removed.
You'll need stronger synchronization than provided by LinkedBlockingQueue
unless the weight-limit is allowed to be violated - so adding the sync in
the wrapper seems reasonable too.



  I was thinking of extending the LinkedBoundedQueue because we can
generalize all the principle in this queue easily. But the locks and
conditions are privateL.

  The main change in the code is:

  instead of using getAndIncrement(or decrement) we can use getAndAdd(+ or -
element.weigth) and  test the not full no anymore with an equality on
capacity but on >=.

  With this we can avoid any stronger synchronization ( even if we will
probably notify a bit too much the notFull condition).









  Cheers,

  David Holmes





  Best regards,

  Thierry Hanot



    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry
Hanot
    Sent: Thursday, 23 November 2006 9:43 PM
    To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
    Subject: Re: [concurrency-interest] Proposal WeightedLinkedBoundedQueue

    The weight is defined by the end user using a small interface yes.

    I use the term weight but size is also relevant J





    Thierry Hanot


----------------------------------------------------------------------------

    From: David Holmes [mailto:dcholmes at optusnet.com.au]
    Sent: jeudi 23 novembre 2006 01:26
    To: Thierry Hanot; concurrency-interest at cs.oswego.edu
    Subject: RE: [concurrency-interest] Proposal WeightedLinkedBoundedQueue



    How do you define and measure the "weight" of an object?



    David Holmes

      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thierry
Hanot
      Sent: Wednesday, 22 November 2006 10:18 PM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue



      A small proposal for adding a new class in the concurrent package.

      All bounded collection are bounded to avoid to use to much memory.(At
least in my case J).

      But the element put in those collections are often with different
size.

      In my case  BoundedQueue is used as an event queue and we can have
composite events which contains itself many events.

      What do you think about adding some bounded collection no more based
on the count but on the sum of the weight of the object inside?



      After a quick look on the code of the LinkedBoundedQueue it seems
pretty easy to do.



      Does somebody else can see the advantage of this kind of object and is
there enough people interested to make it a part of the concurrent package?



      B.R





      Thierry Hanot


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061125/ee5587a1/attachment-0001.html 

From brian at quiotix.com  Tue Nov 28 13:50:07 2006
From: brian at quiotix.com (Brian Goetz)
Date: Tue, 28 Nov 2006 13:50:07 -0500
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEEDHEAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCAEEDHEAA.dcholmes@optusnet.com.au>
Message-ID: <456C84DF.1070100@quiotix.com>

> Tim's generalised semaphore approach also sounds feasible - and much 
> simpler in terms of getting the blocking semantics for excess weight.

This works as long as an object's weight doesn't change after permits 
are acquired.  (This property follows from effective immutability, which 
will often hold for things that are put on a shared queue.)

From thanot at infovista.com  Thu Nov 30 03:04:03 2006
From: thanot at infovista.com (Thierry Hanot)
Date: Thu, 30 Nov 2006 09:04:03 +0100
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
Message-ID: <0329C0D74DD1D64F98E6C6B43B5C78108B544F@frlulms01.iv.local>


I'll try to use the semaphore. And I agree with Tim , the weight should
be immutable once the element is in the queue. But I'm still believe
that we can avoid any synchronization over cost by simply modifying the
LinkedBoundedQueue and changing the AtomicInteger used to manage the
count :).

Thierry Hanot  

-----Original Message-----
From: Brian Goetz [mailto:brian at quiotix.com] 
Sent: mardi 28 novembre 2006 19:50
To: dholmes at ieee.org
Cc: Thierry Hanot; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Proposal WeightedLinkedBoundedQueue

> Tim's generalised semaphore approach also sounds feasible - and much 
> simpler in terms of getting the blocking semantics for excess weight.

This works as long as an object's weight doesn't change after permits 
are acquired.  (This property follows from effective immutability, which

will often hold for things that are put on a shared queue.)


From dawidk at mathcs.emory.edu  Thu Nov 30 12:45:02 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Thu, 30 Nov 2006 12:45:02 -0500
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <0329C0D74DD1D64F98E6C6B43B5C78108B544F@frlulms01.iv.local>
References: <0329C0D74DD1D64F98E6C6B43B5C78108B544F@frlulms01.iv.local>
Message-ID: <456F189E.6000900@mathcs.emory.edu>

Thierry Hanot wrote:
> I'll try to use the semaphore. And I agree with Tim , the weight should
> be immutable once the element is in the queue. But I'm still believe
> that we can avoid any synchronization over cost by simply modifying the
> LinkedBoundedQueue and changing the AtomicInteger used to manage the
> count :).
>   

Nothing can stop you from grabbing a public-domain source from:

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/

and custom-fitting it for your purpose. Which is one of many reasons why
we love this library so much :)

Regards,
Dawid


From thanot at infovista.com  Thu Nov 30 12:56:15 2006
From: thanot at infovista.com (Thierry Hanot)
Date: Thu, 30 Nov 2006 18:56:15 +0100
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
Message-ID: <0329C0D74DD1D64F98E6C6B43B5C78108B5456@frlulms01.iv.local>

That was the answer I was waiting for :)
I'm doing it now ;)

If somebody is interesting by the code feel free to ask.


Thanks a lot 



Thierry Hanot  

-----Original Message-----
From: Dawid Kurzyniec [mailto:dawidk at mathcs.emory.edu] 
Sent: jeudi 30 novembre 2006 18:45
To: Thierry Hanot
Cc: Brian Goetz; dholmes at ieee.org; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Proposal WeightedLinkedBoundedQueue

Thierry Hanot wrote:
> I'll try to use the semaphore. And I agree with Tim , the weight
should
> be immutable once the element is in the queue. But I'm still believe
> that we can avoid any synchronization over cost by simply modifying
the
> LinkedBoundedQueue and changing the AtomicInteger used to manage the
> count :).
>   

Nothing can stop you from grabbing a public-domain source from:

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/c
oncurrent/

and custom-fitting it for your purpose. Which is one of many reasons why
we love this library so much :)

Regards,
Dawid

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20061130/96d4b600/attachment.html 

From dawidk at mathcs.emory.edu  Thu Nov 30 17:40:30 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Thu, 30 Nov 2006 17:40:30 -0500
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <0329C0D74DD1D64F98E6C6B43B5C78108B5456@frlulms01.iv.local>
References: <0329C0D74DD1D64F98E6C6B43B5C78108B5456@frlulms01.iv.local>
Message-ID: <456F5DDE.1040905@mathcs.emory.edu>

Thierry Hanot wrote:
> That was the answer I was waiting for :)
> I'm doing it now ;)
>
>   

> Thierry Hanot wrote:
>   
>> But I'm still believe
>> that we can avoid any synchronization over cost by simply modifying the LinkedBoundedQueue and changing the AtomicInteger used to manage the
>> count :).
>>   
>>     
>
> Nothing can stop you from grabbing a public-domain source from:
>
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/c
> oncurrent/
>
> and custom-fitting it for your purpose. Which is one of many reasons why
> we love this library so much :)
>   



Just one more thing - I am not sure if this is going to be a problem in
your case, but are you aware of potential starvation scenarios? E.g. if
your queue is full most of the time, and if you have a requestor that
wants to put a really large object in the queue, it might never get a
chance to do so (losing competition with other requestors putting
smaller objects, so that there is never enough space available for the
big guy). In the semaphore-based approach, you could prevent this by
using a fair semaphore. If you're doing it by hand, you might want to be
careful about this.

Regards,
Dawid


From brian at quiotix.com  Thu Nov 30 19:06:30 2006
From: brian at quiotix.com (Brian Goetz)
Date: Thu, 30 Nov 2006 19:06:30 -0500
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <456F5DDE.1040905@mathcs.emory.edu>
References: <0329C0D74DD1D64F98E6C6B43B5C78108B5456@frlulms01.iv.local>
	<456F5DDE.1040905@mathcs.emory.edu>
Message-ID: <456F7206.80600@quiotix.com>

Although there's a really unfortunate deadlock interaction if you use a
fair semaphore: you have a queue with a bound of 100 and someone tries
to enqueue an object of weight 101.  With a fair semaphore, it's game
over -- no one will ever succeed in enqueuing again.

> Just one more thing - I am not sure if this is going to be a problem in
> your case, but are you aware of potential starvation scenarios? E.g. if
> your queue is full most of the time, and if you have a requestor that
> wants to put a really large object in the queue, it might never get a
> chance to do so (losing competition with other requestors putting
> smaller objects, so that there is never enough space available for the
> big guy). In the semaphore-based approach, you could prevent this by
> using a fair semaphore. If you're doing it by hand, you might want to be
> careful about this.



From dcholmes at optusnet.com.au  Thu Nov 30 20:40:42 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 1 Dec 2006 11:40:42 +1000
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <456F7206.80600@quiotix.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEFGHEAA.dcholmes@optusnet.com.au>

But if the bound of 100 is a hard-bound then the enqueue of 101 should be
rejected out of hand.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Brian
> Goetz
> Sent: Friday, 1 December 2006 10:07 AM
> To: Dawid Kurzyniec; concurrency-interest
> Subject: Re: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
>
>
> Although there's a really unfortunate deadlock interaction if you use a
> fair semaphore: you have a queue with a bound of 100 and someone tries
> to enqueue an object of weight 101.  With a fair semaphore, it's game
> over -- no one will ever succeed in enqueuing again.
>
> > Just one more thing - I am not sure if this is going to be a problem in
> > your case, but are you aware of potential starvation scenarios? E.g. if
> > your queue is full most of the time, and if you have a requestor that
> > wants to put a really large object in the queue, it might never get a
> > chance to do so (losing competition with other requestors putting
> > smaller objects, so that there is never enough space available for the
> > big guy). In the semaphore-based approach, you could prevent this by
> > using a fair semaphore. If you're doing it by hand, you might want to be
> > careful about this.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From brian at quiotix.com  Thu Nov 30 21:27:18 2006
From: brian at quiotix.com (Brian Goetz)
Date: Thu, 30 Nov 2006 21:27:18 -0500
Subject: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEFGHEAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCKEFGHEAA.dcholmes@optusnet.com.au>
Message-ID: <456F9306.1070507@quiotix.com>

That's what I meant -- the implementation _should_ reject it out of 
hand.  If it doesn't, it is subject to deadlock.

David Holmes wrote:
> But if the bound of 100 is a hard-bound then the enqueue of 101 should be
> rejected out of hand.
> 
> David Holmes
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Brian
>> Goetz
>> Sent: Friday, 1 December 2006 10:07 AM
>> To: Dawid Kurzyniec; concurrency-interest
>> Subject: Re: [concurrency-interest] Proposal WeightedLinkedBoundedQueue
>>
>>
>> Although there's a really unfortunate deadlock interaction if you use a
>> fair semaphore: you have a queue with a bound of 100 and someone tries
>> to enqueue an object of weight 101.  With a fair semaphore, it's game
>> over -- no one will ever succeed in enqueuing again.
>>
>>> Just one more thing - I am not sure if this is going to be a problem in
>>> your case, but are you aware of potential starvation scenarios? E.g. if
>>> your queue is full most of the time, and if you have a requestor that
>>> wants to put a really large object in the queue, it might never get a
>>> chance to do so (losing competition with other requestors putting
>>> smaller objects, so that there is never enough space available for the
>>> big guy). In the semaphore-based approach, you could prevent this by
>>> using a fair semaphore. If you're doing it by hand, you might want to be
>>> careful about this.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

