From TEREKHOV at de.ibm.com  Sat Oct  1 07:10:08 2011
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Sat, 1 Oct 2011 13:10:08 +0200
Subject: [concurrency-interest] AtomicXXX.lazySet
	and	happens-before	reasoning
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD203FB0A@G4W3299.americas.hpqcorp.net>
Message-ID: <OFC2601562.B185517B-ONC125791C.003B3260-C125791C.003D931F@de.ibm.com>

"Boehm, Hans" <hans.boehm at hp.com> wrote:
[...]
> C++ model . . . I do not believe you ever want to reason in terms of
> fences.

For programs targeted to run on POWER/PPC and ARM hardware, reasoning
and coding in terms of fences is the best and that is why the latest
C++11 standard does have acq/rel/acq_rel/SC fences (expressed in
terms of 'synchronizes with', see [atomics.fences]), no?

regards,
alexander.

P.S. I don't like C++11 atomics, I think that atomic loads and
stores ought to support the following 'modes':

  Whether load/store is competing (default) or not. Competing load
  means that there might be concurrent store (to the same object).
  Competing store means that there might be concurrent load or
  store. Non-competing load/store can be performed non-atomically.

  Whether competing load/store needs remote write atomicity (default
  is no remote write atomicity). A remote-write-atomicity-yes load
  triggers undefined behaivior in the case of concurrent remote-
  write-atomicity-no store.

  Whether load/store has specified reordering constraint (default
  is no constraint specified) in terms of the following reordering
  modes:

    Whether preceding loads (in program order) can be reordered
    across it (can by default).

    Whether preceding stores (in program order) can be reordered
    across it (can by default).

    Whether subsequent loads (in program order) can be reordered
    across it (can by default). For load, the set of constrained
    subsequent loads can be limited to only dependant loads (aka
    'consume' mode).

    Whether subsequent stores (in program order) can be reordered
    across it (can by default). For load, there is an implicit
    reordering constraint regarding dependent stores (no need to
    specify it).

    A fence/barrier operation can be used to specify reordering
    constraint using basically the same modes.

Re C++11 MM, I'm still missing more fine-grained memory order
labels such as in pseudo C++ example below.

(I mean mo::noncompeting, mo::ssb/ssb_t (sink store barrier, a
release not affecting preceding loads), slb/slb_t (a release not
affecting preceding stores) below, and somesuch for relaxed acquire)

// Introspection (for bool argument below) aside for a moment
template<typename T, bool copy_ctor_or_dtor_can_mutate_object>
class mutex_and_condvar_free_single_producer_single_consumer {

  typedef isolated< aligned_storage< T > > ELEM;

  size_t           m_size; // > 1
  ELEM *           m_elem; // array of elements, init'ed by ctor
  atomic< ELEM * > m_head; // initially == m_elem
  atomic< ELEM * > m_tail; // initially == m_elem

  ELEM * advance(ELEM * elem) const {
    return (++elem < m_elem + m_size) ? elem : m_elem;
  }

public:

  mutex_and_condvar_free_single_producer_single_consumer(); // ctor
 ~mutex_and_condvar_free_single_producer_single_consumer(); // dtor

  void producer(const T & value) {
    ELEM * tail = m_tail.load(mo::noncompeting); // may be nonatomic
    ELEM * next = advance(tail);
    while (next == m_head.load(mo::relaxed)) usleep(1000);
    new(tail) T(value); // placement copy ctor (make queued copy)
    m_tail.store(next, mo::ssb); // cheaper than mo::release
  }

  T consumer() {
    ELEM * head = m_head.load(mo::noncompeting); // may be nonatomic
    while (head == m_tail.load(mo::consume)) usleep(1000);
    T value(*head); // T's copy ctor (make a copy to return)
    head->~T(); // T's dtor (cleanup for queued copy)
    m_head.store(advance(head), type_list< mo::slb_t, mo::rel_t >::
      element<copy_ctor_or_dtor_can_mutate_object>::type());
    return value; // return copied T
  }

};

Note also that given that example above presumes that no more than
one thread can read from relevant atomic locations while they are
written concurrently, there is definitely no need to pay the price
of remote write atomicity even if it is run on 3+ way
multiprocessor... IOW, hwsync is unneeded even if all mo::* above
are changed to SC... but C++11 MM doesn't allow to express
no-need-for-remote-write-atomicity for SC atomics.



"Boehm, Hans" <hans.boehm at hp.com>@cs.oswego.edu on 30.09.2011 22:25:27

Sent by:    concurrency-interest-bounces at cs.oswego.edu


To:    Ruslan Cheremin <cheremin at gmail.com>, Doug Lea <dl at cs.oswego.edu>
cc:    "concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>
Subject:    Re: [concurrency-interest] AtomicXXX.lazySet and
       happens-before   reasoning


> c) Does it mean what next JMM release will throw out HB reasoning and
> move to memory barriers/fences notation?
>
Adding to Doug's answer, the answer to this question is a resounding "no".
Happens-before reasoning largely remains valid with lazySet, as you can see
in the C++ model (which is unfortunately also significantly complicated by
memory_order_consume, which currently has no Java analog, though it's
closely related to some of the final field extension discussions).  What I
believe goes away is any meaningful notion of a single total
synchronization order.

I do not believe you ever want to reason in terms of fences.  Such
reasoning is not usually sound for Java, since the memory model is
carefully designed to allow elimination of synchronization on e.g. a
volatile accessible form accessed by only a single thread.  Java volatiles
etc. do not have fence semantics.  I do not know of any language-level
memory models that have been successfully expressed in terms of fences.
See http://dl.acm.org/citation.cfm?doid=1988915.1988919 for a discussion of
why I don't consider a couple of better known attempts to be fully
successful.

Hans

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From hans.boehm at hp.com  Sat Oct  1 12:32:05 2011
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sat, 1 Oct 2011 16:32:05 +0000
Subject: [concurrency-interest] AtomicXXX.lazySet
	and	happens-before	reasoning
In-Reply-To: <OFC2601562.B185517B-ONC125791C.003B3260-C125791C.003D931F@de.ibm.com>
References: <A3E67C2071F49C4CBC4F17E6D77CDDD203FB0A@G4W3299.americas.hpqcorp.net>
	<OFC2601562.B185517B-ONC125791C.003B3260-C125791C.003D931F@de.ibm.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD203FE85@G4W3299.americas.hpqcorp.net>

>From my perspective, the main argument for fences in the C++11 memory model is to allow easy conversion of existing applications.  Another argument brought forward, especially by the original proposal, was that fences allow you to further tweak certain pieces of code, e.g. if you were releasing two custom-implemented spin-locks in a row.  Both arguments are valid to some extent, but the latter has always struck me as a very minor issue.  (IIRC, some of the examples in the original paper can be sped up slightly with fences.  Some of the other examples don't actually benefit.)  I supported the addition of fences based on the argument about legacy code.  (Also the original version was much simpler to add and, as it turned out, mostly useless.  As often happens, the actual complexity didn't appear until later.)  I would not recommend that people use C++ fences for new code.

We've had discussions about alternate C++ in atomics elsewhere.  And I'm sure we will continue to disagree about a lot of it, though I wouldn't be opposed to eventual support for noncompeting accesses to atomics.  And none of this is directly applicable to Java.

Hans 

> -----Original Message-----
> From: Alexander Terekhov [mailto:TEREKHOV at de.ibm.com]
> Sent: Saturday, October 01, 2011 4:10 AM
> To: Boehm, Hans
> Cc: Ruslan Cheremin; Doug Lea; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] AtomicXXX.lazySet and happens-
> before reasoning
> 
> "Boehm, Hans" <hans.boehm at hp.com> wrote:
> [...]
> > C++ model . . . I do not believe you ever want to reason in terms of
> > fences.
> 
> For programs targeted to run on POWER/PPC and ARM hardware, reasoning
> and coding in terms of fences is the best and that is why the latest
> C++11 standard does have acq/rel/acq_rel/SC fences (expressed in
> terms of 'synchronizes with', see [atomics.fences]), no?
> 
> regards,
> alexander.
> 
> P.S. I don't like C++11 atomics, I think that atomic loads and
> stores ought to support the following 'modes':
> 
>   Whether load/store is competing (default) or not. Competing load
>   means that there might be concurrent store (to the same object).
>   Competing store means that there might be concurrent load or
>   store. Non-competing load/store can be performed non-atomically.
> 
>   Whether competing load/store needs remote write atomicity (default
>   is no remote write atomicity). A remote-write-atomicity-yes load
>   triggers undefined behaivior in the case of concurrent remote-
>   write-atomicity-no store.
> 
>   Whether load/store has specified reordering constraint (default
>   is no constraint specified) in terms of the following reordering
>   modes:
> 
>     Whether preceding loads (in program order) can be reordered
>     across it (can by default).
> 
>     Whether preceding stores (in program order) can be reordered
>     across it (can by default).
> 
>     Whether subsequent loads (in program order) can be reordered
>     across it (can by default). For load, the set of constrained
>     subsequent loads can be limited to only dependant loads (aka
>     'consume' mode).
> 
>     Whether subsequent stores (in program order) can be reordered
>     across it (can by default). For load, there is an implicit
>     reordering constraint regarding dependent stores (no need to
>     specify it).
> 
>     A fence/barrier operation can be used to specify reordering
>     constraint using basically the same modes.
> 
> Re C++11 MM, I'm still missing more fine-grained memory order
> labels such as in pseudo C++ example below.
> 
> (I mean mo::noncompeting, mo::ssb/ssb_t (sink store barrier, a
> release not affecting preceding loads), slb/slb_t (a release not
> affecting preceding stores) below, and somesuch for relaxed acquire)
> 
> // Introspection (for bool argument below) aside for a moment
> template<typename T, bool copy_ctor_or_dtor_can_mutate_object>
> class mutex_and_condvar_free_single_producer_single_consumer {
> 
>   typedef isolated< aligned_storage< T > > ELEM;
> 
>   size_t           m_size; // > 1
>   ELEM *           m_elem; // array of elements, init'ed by ctor
>   atomic< ELEM * > m_head; // initially == m_elem
>   atomic< ELEM * > m_tail; // initially == m_elem
> 
>   ELEM * advance(ELEM * elem) const {
>     return (++elem < m_elem + m_size) ? elem : m_elem;
>   }
> 
> public:
> 
>   mutex_and_condvar_free_single_producer_single_consumer(); // ctor
>  ~mutex_and_condvar_free_single_producer_single_consumer(); // dtor
> 
>   void producer(const T & value) {
>     ELEM * tail = m_tail.load(mo::noncompeting); // may be nonatomic
>     ELEM * next = advance(tail);
>     while (next == m_head.load(mo::relaxed)) usleep(1000);
>     new(tail) T(value); // placement copy ctor (make queued copy)
>     m_tail.store(next, mo::ssb); // cheaper than mo::release
>   }
> 
>   T consumer() {
>     ELEM * head = m_head.load(mo::noncompeting); // may be nonatomic
>     while (head == m_tail.load(mo::consume)) usleep(1000);
>     T value(*head); // T's copy ctor (make a copy to return)
>     head->~T(); // T's dtor (cleanup for queued copy)
>     m_head.store(advance(head), type_list< mo::slb_t, mo::rel_t >::
>       element<copy_ctor_or_dtor_can_mutate_object>::type());
>     return value; // return copied T
>   }
> 
> };
> 
> Note also that given that example above presumes that no more than
> one thread can read from relevant atomic locations while they are
> written concurrently, there is definitely no need to pay the price
> of remote write atomicity even if it is run on 3+ way
> multiprocessor... IOW, hwsync is unneeded even if all mo::* above
> are changed to SC... but C++11 MM doesn't allow to express
> no-need-for-remote-write-atomicity for SC atomics.
> 
> 
> 
> "Boehm, Hans" <hans.boehm at hp.com>@cs.oswego.edu on 30.09.2011 22:25:27
> 
> Sent by:    concurrency-interest-bounces at cs.oswego.edu
> 
> 
> To:    Ruslan Cheremin <cheremin at gmail.com>, Doug Lea
> <dl at cs.oswego.edu>
> cc:    "concurrency-interest at cs.oswego.edu"
>        <concurrency-interest at cs.oswego.edu>
> Subject:    Re: [concurrency-interest] AtomicXXX.lazySet and
>        happens-before   reasoning
> 
> 
> > c) Does it mean what next JMM release will throw out HB reasoning and
> > move to memory barriers/fences notation?
> >
> Adding to Doug's answer, the answer to this question is a resounding
> "no".
> Happens-before reasoning largely remains valid with lazySet, as you can
> see
> in the C++ model (which is unfortunately also significantly complicated
> by
> memory_order_consume, which currently has no Java analog, though it's
> closely related to some of the final field extension discussions).
> What I
> believe goes away is any meaningful notion of a single total
> synchronization order.
> 
> I do not believe you ever want to reason in terms of fences.  Such
> reasoning is not usually sound for Java, since the memory model is
> carefully designed to allow elimination of synchronization on e.g. a
> volatile accessible form accessed by only a single thread.  Java
> volatiles
> etc. do not have fence semantics.  I do not know of any language-level
> memory models that have been successfully expressed in terms of fences.
> See http://dl.acm.org/citation.cfm?doid=1988915.1988919 for a
> discussion of
> why I don't consider a couple of better known attempts to be fully
> successful.
> 
> Hans
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 



From TEREKHOV at de.ibm.com  Sat Oct  1 13:44:02 2011
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Sat, 1 Oct 2011 19:44:02 +0200
Subject: [concurrency-interest] AtomicXXX.lazySet
	and	happens-before	reasoning
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD203FE85@G4W3299.americas.hpqcorp.net>
Message-ID: <OF6221D11D.3E6622B6-ONC125791C.00610F7E-C125791C.0061A42D@de.ibm.com>

"Boehm, Hans" <hans.boehm at hp.com> wrote:
[...]
> From my perspective, the main argument for fences in the C++11
> memory model is to allow easy conversion of existing applications.

C++11 most widely used acquire/release fences are unidirectional,
whereas fences in existing classic applications are bidirectional.

regards,
alexander.


"Boehm, Hans" <hans.boehm at hp.com> on 01.10.2011 18:32:05

To:    Alexander Terekhov/Germany/IBM at IBMDE
cc:    Ruslan Cheremin <cheremin at gmail.com>, Doug Lea <dl at cs.oswego.edu>,
       "concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>
Subject:    RE: [concurrency-interest] AtomicXXX.lazySet and
       happens-before   reasoning


>From my perspective, the main argument for fences in the C++11 memory model
is to allow easy conversion of existing applications.  Another argument
brought forward, especially by the original proposal, was that fences allow
you to further tweak certain pieces of code, e.g. if you were releasing two
custom-implemented spin-locks in a row.  Both arguments are valid to some
extent, but the latter has always struck me as a very minor issue.  (IIRC,
some of the examples in the original paper can be sped up slightly with
fences.  Some of the other examples don't actually benefit.)  I supported
the addition of fences based on the argument about legacy code.  (Also the
original version was much simpler to add and, as it turned out, mostly
useless.  As often happens, the actual complexity didn't appear until
later.)  I would not recommend that people use C++ fences for new code.

We've had discussions about alternate C++ in atomics elsewhere.  And I'm
sure we will continue to disagree about a lot of it, though I wouldn't be
opposed to eventual support for noncompeting accesses to atomics.  And none
of this is directly applicable to Java.

Hans

> -----Original Message-----
> From: Alexander Terekhov [mailto:TEREKHOV at de.ibm.com]
> Sent: Saturday, October 01, 2011 4:10 AM
> To: Boehm, Hans
> Cc: Ruslan Cheremin; Doug Lea; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] AtomicXXX.lazySet and happens-
> before reasoning
>
> "Boehm, Hans" <hans.boehm at hp.com> wrote:
> [...]
> > C++ model . . . I do not believe you ever want to reason in terms of
> > fences.
>
> For programs targeted to run on POWER/PPC and ARM hardware, reasoning
> and coding in terms of fences is the best and that is why the latest
> C++11 standard does have acq/rel/acq_rel/SC fences (expressed in
> terms of 'synchronizes with', see [atomics.fences]), no?
>
> regards,
> alexander.
>
> P.S. I don't like C++11 atomics, I think that atomic loads and
> stores ought to support the following 'modes':
>
>   Whether load/store is competing (default) or not. Competing load
>   means that there might be concurrent store (to the same object).
>   Competing store means that there might be concurrent load or
>   store. Non-competing load/store can be performed non-atomically.
>
>   Whether competing load/store needs remote write atomicity (default
>   is no remote write atomicity). A remote-write-atomicity-yes load
>   triggers undefined behaivior in the case of concurrent remote-
>   write-atomicity-no store.
>
>   Whether load/store has specified reordering constraint (default
>   is no constraint specified) in terms of the following reordering
>   modes:
>
>     Whether preceding loads (in program order) can be reordered
>     across it (can by default).
>
>     Whether preceding stores (in program order) can be reordered
>     across it (can by default).
>
>     Whether subsequent loads (in program order) can be reordered
>     across it (can by default). For load, the set of constrained
>     subsequent loads can be limited to only dependant loads (aka
>     'consume' mode).
>
>     Whether subsequent stores (in program order) can be reordered
>     across it (can by default). For load, there is an implicit
>     reordering constraint regarding dependent stores (no need to
>     specify it).
>
>     A fence/barrier operation can be used to specify reordering
>     constraint using basically the same modes.
>
> Re C++11 MM, I'm still missing more fine-grained memory order
> labels such as in pseudo C++ example below.
>
> (I mean mo::noncompeting, mo::ssb/ssb_t (sink store barrier, a
> release not affecting preceding loads), slb/slb_t (a release not
> affecting preceding stores) below, and somesuch for relaxed acquire)
>
> // Introspection (for bool argument below) aside for a moment
> template<typename T, bool copy_ctor_or_dtor_can_mutate_object>
> class mutex_and_condvar_free_single_producer_single_consumer {
>
>   typedef isolated< aligned_storage< T > > ELEM;
>
>   size_t           m_size; // > 1
>   ELEM *           m_elem; // array of elements, init'ed by ctor
>   atomic< ELEM * > m_head; // initially == m_elem
>   atomic< ELEM * > m_tail; // initially == m_elem
>
>   ELEM * advance(ELEM * elem) const {
>     return (++elem < m_elem + m_size) ? elem : m_elem;
>   }
>
> public:
>
>   mutex_and_condvar_free_single_producer_single_consumer(); // ctor
>  ~mutex_and_condvar_free_single_producer_single_consumer(); // dtor
>
>   void producer(const T & value) {
>     ELEM * tail = m_tail.load(mo::noncompeting); // may be nonatomic
>     ELEM * next = advance(tail);
>     while (next == m_head.load(mo::relaxed)) usleep(1000);
>     new(tail) T(value); // placement copy ctor (make queued copy)
>     m_tail.store(next, mo::ssb); // cheaper than mo::release
>   }
>
>   T consumer() {
>     ELEM * head = m_head.load(mo::noncompeting); // may be nonatomic
>     while (head == m_tail.load(mo::consume)) usleep(1000);
>     T value(*head); // T's copy ctor (make a copy to return)
>     head->~T(); // T's dtor (cleanup for queued copy)
>     m_head.store(advance(head), type_list< mo::slb_t, mo::rel_t >::
>       element<copy_ctor_or_dtor_can_mutate_object>::type());
>     return value; // return copied T
>   }
>
> };
>
> Note also that given that example above presumes that no more than
> one thread can read from relevant atomic locations while they are
> written concurrently, there is definitely no need to pay the price
> of remote write atomicity even if it is run on 3+ way
> multiprocessor... IOW, hwsync is unneeded even if all mo::* above
> are changed to SC... but C++11 MM doesn't allow to express
> no-need-for-remote-write-atomicity for SC atomics.
>
>
>
> "Boehm, Hans" <hans.boehm at hp.com>@cs.oswego.edu on 30.09.2011 22:25:27
>
> Sent by:    concurrency-interest-bounces at cs.oswego.edu
>
>
> To:    Ruslan Cheremin <cheremin at gmail.com>, Doug Lea
> <dl at cs.oswego.edu>
> cc:    "concurrency-interest at cs.oswego.edu"
>        <concurrency-interest at cs.oswego.edu>
> Subject:    Re: [concurrency-interest] AtomicXXX.lazySet and
>        happens-before   reasoning
>
>
> > c) Does it mean what next JMM release will throw out HB reasoning and
> > move to memory barriers/fences notation?
> >
> Adding to Doug's answer, the answer to this question is a resounding
> "no".
> Happens-before reasoning largely remains valid with lazySet, as you can
> see
> in the C++ model (which is unfortunately also significantly complicated
> by
> memory_order_consume, which currently has no Java analog, though it's
> closely related to some of the final field extension discussions).
> What I
> believe goes away is any meaningful notion of a single total
> synchronization order.
>
> I do not believe you ever want to reason in terms of fences.  Such
> reasoning is not usually sound for Java, since the memory model is
> carefully designed to allow elimination of synchronization on e.g. a
> volatile accessible form accessed by only a single thread.  Java
> volatiles
> etc. do not have fence semantics.  I do not know of any language-level
> memory models that have been successfully expressed in terms of fences.
> See http://dl.acm.org/citation.cfm?doid=1988915.1988919 for a
> discussion of
> why I don't consider a couple of better known attempts to be fully
> successful.
>
> Hans
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>




From dl at cs.oswego.edu  Sun Oct  2 18:29:39 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 02 Oct 2011 18:29:39 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E85F532.1040808@redhat.com>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com>
Message-ID: <4E88E5D3.6070502@cs.oswego.edu>


An update to jsr166e.ConcurrentHashMapV8 includes two
suggested changes that can affect usage:

* The "compute" method now supports read-modify-write
by using a function that provides the previous value.

* Both computeIfAbsent and compute now throw NullPointerException
(leaving old mapping or lack thereof intact) if the function returns
null.

Plus some other improvements.

While I'm at it...

On 09/30/11 12:58, Jason T. Greene wrote:
>>
>> [... overriding default load factor  ... ]
>
> What about just limiting the upper range? I could see someone dropping the
> default load factor to get marginally better throughput.

Although even in that case, the overrides were previously
not handled consistently, and by allowing overridden load
factor to affect initial sizing, we still support it about
as well as before, but can now clearly specify the effects.
The lack of clarity arose from a series of inconsistent
decisions (some of them mine) to more-or-less mimic
the already-inconsistent policies of the original HashTable
class. Unclear enough that if someone wanted to backport
the CHMV8 code but not the clarified specs to previous JDKs,
I don't think any user could tell the difference in the
effects of overriding load factor.

-Doug

From viktor.klang at gmail.com  Mon Oct  3 09:34:38 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 3 Oct 2011 15:34:38 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E88E5D3.6070502@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
Message-ID: <CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>

Hi Doug,

I'd love to be able to specify a Comparator in the constructor so that
conditional remove doesn't rely on reference-equality or .equals (since
oftentimes I cannot overload it)

Thoughts?

Cheers,
?

On Mon, Oct 3, 2011 at 12:29 AM, Doug Lea <dl at cs.oswego.edu> wrote:

>
> An update to jsr166e.ConcurrentHashMapV8 includes two
> suggested changes that can affect usage:
>
> * The "compute" method now supports read-modify-write
> by using a function that provides the previous value.
>
> * Both computeIfAbsent and compute now throw NullPointerException
> (leaving old mapping or lack thereof intact) if the function returns
> null.
>
> Plus some other improvements.
>
> While I'm at it...
>
> On 09/30/11 12:58, Jason T. Greene wrote:
>
>>
>>> [... overriding default load factor  ... ]
>>>
>>
>> What about just limiting the upper range? I could see someone dropping the
>> default load factor to get marginally better throughput.
>>
>
> Although even in that case, the overrides were previously
> not handled consistently, and by allowing overridden load
> factor to affect initial sizing, we still support it about
> as well as before, but can now clearly specify the effects.
> The lack of clarity arose from a series of inconsistent
> decisions (some of them mine) to more-or-less mimic
> the already-inconsistent policies of the original HashTable
> class. Unclear enough that if someone wanted to backport
> the CHMV8 code but not the clarified specs to previous JDKs,
> I don't think any user could tell the difference in the
> effects of overriding load factor.
>
> -Doug
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111003/6a9b694d/attachment.html>

From dl at cs.oswego.edu  Mon Oct  3 09:52:04 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 03 Oct 2011 09:52:04 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
Message-ID: <4E89BE04.7060907@cs.oswego.edu>

On 10/03/11 09:34, ?iktor ?lang wrote:

> I'd love to be able to specify a Comparator in the constructor so that
> conditional remove doesn't rely on reference-equality or .equals (since
> oftentimes I cannot overload it)

One reasonable way to do this interacts with a decision
I keep flip-flopping on because people make good cases for
all three of the possibilities: What should the map do when
either a mapper or remapper function returns null?
The possibilities are:
   1. treat it as a user error, so throw NullPointerException
   2. treat it as a (re)mapping failure, so ignore the outcome
   3. treat it as a removal

I originally did (2), but arguments that (1) was a safer
option led me to change to it in latest update. But yours
is another of several use cases that make (3) more
attractive -- on the plus side, it allows atomic removal
(that you could use in the above case).
But on the minus side it might lead to unintentional
atomic removal.

Further arguments welcome.

-Doug




From viktor.klang at gmail.com  Mon Oct  3 10:07:33 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 3 Oct 2011 16:07:33 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E89BE04.7060907@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu>
Message-ID: <CANPzfU9PzcXj+HDzJBSC5=VSXB4gTzhJSK0JBhXudSf6zrh8Xg@mail.gmail.com>

2011/10/3 Doug Lea <dl at cs.oswego.edu>

> On 10/03/11 09:34, ?iktor ?lang wrote:
>
>  I'd love to be able to specify a Comparator in the constructor so that
>> conditional remove doesn't rely on reference-equality or .equals (since
>> oftentimes I cannot overload it)
>>
>
> One reasonable way to do this interacts with a decision
> I keep flip-flopping on because people make good cases for
> all three of the possibilities: What should the map do when
> either a mapper or remapper function returns null?
>

TBH I think they need to be symmetrical to put(key, null), no?


> The possibilities are:
>  1. treat it as a user error, so throw NullPointerException
>  2. treat it as a (re)mapping failure, so ignore the outcome
>  3. treat it as a removal
>
> I originally did (2), but arguments that (1) was a safer
> option led me to change to it in latest update. But yours
> is another of several use cases that make (3) more
> attractive -- on the plus side, it allows atomic removal
> (that you could use in the above case).
> But on the minus side it might lead to unintentional
> atomic removal.
>
> Further arguments welcome.
>
> -Doug
>
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111003/0a8ff35d/attachment.html>

From dl at cs.oswego.edu  Mon Oct  3 10:20:31 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 03 Oct 2011 10:20:31 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CANPzfU9PzcXj+HDzJBSC5=VSXB4gTzhJSK0JBhXudSf6zrh8Xg@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>
	<CANPzfU9PzcXj+HDzJBSC5=VSXB4gTzhJSK0JBhXudSf6zrh8Xg@mail.gmail.com>
Message-ID: <4E89C4AF.4010200@cs.oswego.edu>

On 10/03/11 10:07, ?iktor ?lang wrote:
> TBH I think they need to be symmetrical to put(key, null), no?

In a library that was not constrained by 15 years of history
and conventions, map.put(key, null) would also most likely mean
to remove mapping. We can't do that though and still obey
java.util.Map spec. But the details of new methods are still
up for grabs.

>
>     The possibilities are:
>       1. treat it as a user error, so throw NullPointerException
>       2. treat it as a (re)mapping failure, so ignore the outcome
>       3. treat it as a removal
>
>     I originally did (2), but arguments that (1) was a safer
>     option led me to change to it in latest update. But yours
>     is another of several use cases that make (3) more
>     attractive -- on the plus side, it allows atomic removal
>     (that you could use in the above case).
>     But on the minus side it might lead to unintentional
>     atomic removal.



From mthornton at optrak.com  Mon Oct  3 10:22:15 2011
From: mthornton at optrak.com (Mark Thornton)
Date: Mon, 03 Oct 2011 15:22:15 +0100
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>
	<4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
Message-ID: <4E89C517.9050304@optrak.com>

On 03/10/11 14:34, ?iktor ?lang wrote:
> Hi Doug,
>
> I'd love to be able to specify a Comparator in the constructor so that 
> conditional remove doesn't rely on reference-equality or .equals 
> (since oftentimes I cannot overload it)
>
> Thoughts?
>
> Cheers,
> ?
A Comparator wouldn't be appropriate for a HashMap. What you need is 
something like:

interface Equivalence<T> {
     int hash(T value);
     boolean isEqual(T a, T b);
}

Mark


From viktor.klang at gmail.com  Mon Oct  3 10:59:08 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 3 Oct 2011 16:59:08 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E89C517.9050304@optrak.com>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89C517.9050304@optrak.com>
Message-ID: <CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>

On Mon, Oct 3, 2011 at 4:22 PM, Mark Thornton <mthornton at optrak.com> wrote:

> On 03/10/11 14:34, ?iktor ?lang wrote:
>
>> Hi Doug,
>>
>> I'd love to be able to specify a Comparator in the constructor so that
>> conditional remove doesn't rely on reference-equality or .equals (since
>> oftentimes I cannot overload it)
>>
>> Thoughts?
>>
>> Cheers,
>> ?
>>
> A Comparator wouldn't be appropriate for a HashMap.


Appropriate? You mean just like doing a DNS lookup in the equals of
java.net.URL?


> What you need is something like:
>
> interface Equivalence<T> {
>    int hash(T value);
>    boolean isEqual(T a, T b);
> }
>

What does hashes have to do with equivalence? Is that appropriate?

interface Hashable<T> {
  int computeHashFor(T value);
}

interface Equivalence<T> {
  boolean isEqual(T a, T b);
}

and then require something that implements both Hashable<K> and
Equivalence<V>

Cheers,
?


>
> Mark
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111003/16a72708/attachment-0001.html>

From mthornton at optrak.com  Mon Oct  3 11:06:05 2011
From: mthornton at optrak.com (Mark Thornton)
Date: Mon, 03 Oct 2011 16:06:05 +0100
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89C517.9050304@optrak.com>
	<CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>
Message-ID: <4E89CF5D.3020707@optrak.com>

On 03/10/11 15:59, ?iktor ?lang wrote:
>
>
>     A Comparator wouldn't be appropriate for a HashMap.
>
>     ?
>
>
> Appropriate? You mean just like doing a DNS lookup in the equals of 
> java.net.URL?

A Comparator requires a domain with an ordering whereas HashMap's can be 
used on unordered domains.
>
>     What you need is something like:
>
>     interface Equivalence<T> {
>        int hash(T value);
>        boolean isEqual(T a, T b);
>     }
>
>
> What does hashes have to do with equivalence? Is that appropriate?
>
> interface Hashable<T> {
>   int computeHashFor(T value);
> }
>
> interface Equivalence<T> {
>   boolean isEqual(T a, T b);
> }
>
> and then require something that implements both Hashable<K> and 
> Equivalence<V>
>

I think Hashable might extend Equivalence as this sort of hash is tied 
up with equivalence.

Mark

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111003/94e98c81/attachment.html>

From viktor.klang at gmail.com  Mon Oct  3 11:18:15 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 3 Oct 2011 17:18:15 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E89CF5D.3020707@optrak.com>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89C517.9050304@optrak.com>
	<CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>
	<4E89CF5D.3020707@optrak.com>
Message-ID: <CANPzfU8FC36arPPBYD8EE_W5SJJBFk-z-Wqg+K8Eq6gNtcnnTw@mail.gmail.com>

2011/10/3 Mark Thornton <mthornton at optrak.com>

> **
> On 03/10/11 15:59, ?iktor ?lang wrote:
>
>
>
>  A Comparator wouldn't be appropriate for a HashMap.
>
> ?
>>
>
> Appropriate? You mean just like doing a DNS lookup in the equals of
> java.net.URL?
>
>
> A Comparator requires a domain with an ordering whereas HashMap's can be
> used on unordered domains.
>

While I agree with your stance on this in general, the contract of
Comparator does not require the parameterized type to have any total
ordering. For the subset of functionality needed in the Comparator (only 0
results are interesting), I'd prefer that over introducing yet another Noun
that is not backported into the main API. (Closable is still not implemented
everywhere)

Now, what we'd really want here is a function from T => T => Boolean, or
uncurried (T,T) => Boolean so we didn't have to come up with new names all
the time. A man can dream, I suppose.


>
>
>
>> What you need is something like:
>>
>> interface Equivalence<T> {
>>    int hash(T value);
>>    boolean isEqual(T a, T b);
>> }
>>
>
> What does hashes have to do with equivalence? Is that appropriate?
>
> interface Hashable<T> {
>   int computeHashFor(T value);
> }
>
> interface Equivalence<T> {
>   boolean isEqual(T a, T b);
> }
>
> and then require something that implements both Hashable<K> and
> Equivalence<V>
>
>
> I think Hashable might extend Equivalence as this sort of hash is tied up
> with equivalence.
>

The tie is the parameterized type signature of ? extends Hashable<K> &
Equivalence<V>,
smashing them together would really limit the usefulness and doesn't add any
value.

Cheers,
?


>
> Mark
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111003/7f3ef5f4/attachment.html>

From kasper at kav.dk  Mon Oct  3 11:47:12 2011
From: kasper at kav.dk (Kasper Nielsen)
Date: Mon, 03 Oct 2011 17:47:12 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CANPzfU8FC36arPPBYD8EE_W5SJJBFk-z-Wqg+K8Eq6gNtcnnTw@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89C517.9050304@optrak.com>
	<CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>
	<4E89CF5D.3020707@optrak.com>
	<CANPzfU8FC36arPPBYD8EE_W5SJJBFk-z-Wqg+K8Eq6gNtcnnTw@mail.gmail.com>
Message-ID: <4E89D900.1070601@kav.dk>

On 03-10-2011 17:18, ?iktor ?lang wrote:
> The tie is the parameterized type signature of ? extends Hashable<K> &
> Equivalence<V>,
> smashing them together would really limit the usefulness and doesn't add
> any value.

I really do not see practically reason for separating these into 2 
different interfaces. Only more clutter.

First of all, 90 % of all usage will be either Equivalence.Equals or 
Equivalence.Identity. With probably Equivalence.ignoreStringCase as a 
trailing third. These can all be predefined so users do not need to 
implement anything.

Second, I think almost all usage will be in situations where you
will also need the custom hash, for example, for keys in a hash map.

See also
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/extra166y/CustomConcurrentHashMap.java 
which supports user-defined equivalence comparisons.

Guava has an identical interface com.google.common.base.Equivalence.

cheers
   Kasper

From viktor.klang at gmail.com  Mon Oct  3 12:04:06 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 3 Oct 2011 18:04:06 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E89D900.1070601@kav.dk>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89C517.9050304@optrak.com>
	<CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>
	<4E89CF5D.3020707@optrak.com>
	<CANPzfU8FC36arPPBYD8EE_W5SJJBFk-z-Wqg+K8Eq6gNtcnnTw@mail.gmail.com>
	<4E89D900.1070601@kav.dk>
Message-ID: <CANPzfU9zUQjZdS=w3M-q2vtOGcPHFs94etyP_ASOFjshVqdcdg@mail.gmail.com>

On Mon, Oct 3, 2011 at 5:47 PM, Kasper Nielsen <kasper at kav.dk> wrote:

> On 03-10-2011 17:18, ?iktor ?lang wrote:
>
>> The tie is the parameterized type signature of ? extends Hashable<K> &
>> Equivalence<V>,
>> smashing them together would really limit the usefulness and doesn't add
>> any value.
>>
>
> I really do not see practically reason for separating these into 2
> different interfaces. Only more clutter.
>

You're a firm disbeliever is the Single Responsibility Principle?

The computation of the hashCode of the Key is distinctly something else from
comparing equality of Values.


>
> First of all, 90 % of all usage will be either Equivalence.Equals or
> Equivalence.Identity. With probably Equivalence.ignoreStringCase as a
> trailing third. These can all be predefined so users do not need to
> implement anything.
>
> Second, I think almost all usage will be in situations where you
> will also need the custom hash, for example, for keys in a hash map.
>

What? As I said, how does the equivalence-checking of the Values interact
with the hash of the Keys?


>
> See also
> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**extra166y/**
> CustomConcurrentHashMap.java<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/extra166y/CustomConcurrentHashMap.java>which supports user-defined equivalence comparisons.
>

That is about the equivalence of the Keys, not of the Values.


>
> Guava has an identical interface com.google.common.base.**Equivalence.
>

I'd be surprised if there aren't thousands of similar implementations all
over the Java ecosystem due to the lack of it in the standard library.

Cheers,
?


>
> cheers
>  Kasper
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111003/ddf57a84/attachment.html>

From kasper at kav.dk  Mon Oct  3 12:36:59 2011
From: kasper at kav.dk (Kasper Nielsen)
Date: Mon, 03 Oct 2011 18:36:59 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CANPzfU9zUQjZdS=w3M-q2vtOGcPHFs94etyP_ASOFjshVqdcdg@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89C517.9050304@optrak.com>
	<CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>
	<4E89CF5D.3020707@optrak.com>
	<CANPzfU8FC36arPPBYD8EE_W5SJJBFk-z-Wqg+K8Eq6gNtcnnTw@mail.gmail.com>
	<4E89D900.1070601@kav.dk>
	<CANPzfU9zUQjZdS=w3M-q2vtOGcPHFs94etyP_ASOFjshVqdcdg@mail.gmail.com>
Message-ID: <4E89E4AB.3010108@kav.dk>

On 03-10-2011 18:04, ?iktor ?lang wrote:
>
>     I really do not see practically reason for separating these into 2
>     different interfaces. Only more clutter.
>
>
> You're a firm disbeliever is the Single Responsibility Principle?

No, I am a firm believer in pragmatic design and keeping things simple.
Adding 1 interface is better then adding 2.

Look, you probably have a pretty specific use case for why you cannot 
use the equals contract when you remove an element. And you have no need 
for a custom hash code.

But I think for most other people there is no need to separate the 
responsibility of calculating a hash code and answering whether two 
object are considered equivalent. Because you need both most of the time.

One of the first things you learn when working with Java is always 
override hashCode() if you override equals(). I think it is only natural 
that any interface defining custom policies with regards to equals() or 
hashCode() supports that notion.

I mean forcing you and a handful other people to also implement 
Equivalence.hashCode() method is not going to ruin anyone's day.

>     See also
>     http://gee.cs.oswego.edu/cgi-__bin/viewcvs.cgi/jsr166/src/__extra166y/__CustomConcurrentHashMap.java
>     which supports user-defined equivalence comparisons.
> That is about the equivalence of the Keys, not of the Values.

If you look closer it also support value equivalence.

cheers
   Kasper

From kasper at kav.dk  Mon Oct  3 13:13:57 2011
From: kasper at kav.dk (Kasper Nielsen)
Date: Mon, 03 Oct 2011 19:13:57 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E89BE04.7060907@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu>
Message-ID: <4E89ED55.4060600@kav.dk>

On 03-10-2011 15:52, Doug Lea wrote:
> On 10/03/11 09:34, ?iktor ?lang wrote:
>
>> I'd love to be able to specify a Comparator in the constructor so that
>> conditional remove doesn't rely on reference-equality or .equals (since
>> oftentimes I cannot overload it)
>
> One reasonable way to do this interacts with a decision
> I keep flip-flopping on because people make good cases for
> all three of the possibilities: What should the map do when
> either a mapper or remapper function returns null?
> The possibilities are:
> 1. treat it as a user error, so throw NullPointerException
> 2. treat it as a (re)mapping failure, so ignore the outcome
> 3. treat it as a removal

I had a similar discussion with myself a while ago for a library I was 
designing. I ended up with 2. (ignoring the outcome) for a couple of reasons

1)
I allow users to recompute all mappings for a map. A.la
computeAll(BinaryMapper<? super K, ? super V, ? extends V> remapper)
which will run through any entry in the map and optionally compute a new 
value. Since I rarely need to update all values at the same time it is 
common I do not want to do anything with the value.

Although volatile writes are not that expensive. Having to update every 
mapping is still some task for large maps.

Of course, one way to do avoid this would be to see if the existing 
value is identical to the recomputed value. In which case the mapping 
does not need to be updated.

2)
In many situations there is a difference between a no-op and updating an 
entry in place.

If you working with something such as the event handlers defined in the 
upcoming JSR 107. You only want to notify these event handlers in case
a value changes.

Doing a simple check on identify by a map/cache as I suggested in 1). Is 
often not enough since users might not create a new value in the compute 
method, but instead update the value in place. For example, changing 
some bits in a blob.

Likewise, if you are working in a distributed setting you only want to 
send new values to other peers if they actually change.

3)
Although it is difficult to generalize. In case a Remapper returned a 
null as the result of a user error. I had the feeling a no-op is 
preferred instead of deleting the entry from the map.

4)
I wanted both computeIfPresent and computeIfAbsent to behave identical 
with respect to null being returned by a mapper/remapper. So either it 
was a user error (1) or it was a no-op (2). Which left 3. out.

Even if there are only few use case for it, returning null from 
computeIfAbsent will result in no mapping being established.

5)
Since Remapper/Mapper code is invoked within atomicity control, it is 
usually short and simple code. So few errors arise. At least I have yet 
to come across one resulting from returning null as a mistake.

6)
In general, I don't like null representing anything but a mistake. But 
in my case I had some use cases where the benefits of 2. (ignoring the 
outcome) outweighed my reluctance towards using nulls.

cheers
   Kasper

From viktor.klang at gmail.com  Mon Oct  3 13:20:22 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 3 Oct 2011 19:20:22 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E89E4AB.3010108@kav.dk>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89C517.9050304@optrak.com>
	<CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>
	<4E89CF5D.3020707@optrak.com>
	<CANPzfU8FC36arPPBYD8EE_W5SJJBFk-z-Wqg+K8Eq6gNtcnnTw@mail.gmail.com>
	<4E89D900.1070601@kav.dk>
	<CANPzfU9zUQjZdS=w3M-q2vtOGcPHFs94etyP_ASOFjshVqdcdg@mail.gmail.com>
	<4E89E4AB.3010108@kav.dk>
Message-ID: <CANPzfU8oMc7D6q0PYPTe5Z4Q8-_goEqipqZWqXPo2ZRN52FfWA@mail.gmail.com>

On Mon, Oct 3, 2011 at 6:36 PM, Kasper Nielsen <kasper at kav.dk> wrote:

> On 03-10-2011 18:04, ?iktor ?lang wrote:
>
>>
>>    I really do not see practically reason for separating these into 2
>>    different interfaces. Only more clutter.
>>
>>
>> You're a firm disbeliever is the Single Responsibility Principle?
>>
>
> No, I am a firm believer in pragmatic design and keeping things simple.
>

I'm also a fan of pragmatic design, but not when it comes to standard
libraries, simply because you cannot revert later. Pragmatism thrives in
dynamic environments, a standard lib is usually anything but dynamic.

java.util.Date was also a rather pragmatic approach.


> Adding 1 interface is better then adding 2.
>

According to what definition of "better"? To be honest, if we'd have more
fine grained interfaces in Java, we would probably not be in this situation
we are in today, where the Collection interface is so broad you oftentimes
have to choose between performance or add UnsupportedOperationExceptions.
If we'd have more fine-grained interfaces (or typeclasses) we could be more
selective in what data gets exposed.


>
> Look, you probably have a pretty specific use case for why you cannot use
> the equals contract when you remove an element.


In my world equality is contextual, having equals on java.lang.Object
removes the possibility of contexts. Universal equality is one of those
things where there jury is still out.


> And you have no need for a custom hash code.
>

And so I should not have to tinker with that. So the simplest 2 solutions
are:

1) Separate the things into Hashable and Equivalence, if it is deemed that
euqality is always interesting when dealing with hashes, then Hashable
should extend Equivalence. All is fine.

2) Atleast make it so that it doesn't create pointless boilerplate and make
Equivalence an abstract class (due to the lack of mixins in Java) and have
the default implementation of hash return a.hashCode()



> But I think for most other people there is no need to separate the
> responsibility of calculating a hash code and answering whether two object
> are considered equivalent.


I'd be more than surprised to see hashCode being called at all places where
equality is checked. Atleast in Java. The other way around is probably true
tho, that when you need Hashability, you need Equality, this ties into what
I've said previously in this email.


> Because you need both most of the time.
>

I do not believe that. Equality as a concept I believe is far more used than
hashability. If you do not believe me we can have a small contest to see who
finds most usage of either equality alone or equality and hashability in a
randomly chosen Java project.


>
> One of the first things you learn when working with Java is always override
> hashCode() if you override equals(). I think it is only natural that any
> interface defining custom policies with regards to equals() or hashCode()
> supports that notion.
>

And since the majority of Java classes have mutable state, the majority of
them is broken.


>
> I mean forcing you and a handful other people to also implement
> Equivalence.hashCode() method is not going to ruin anyone's day.
>

Boilerplate ruins my day, I think it's rude to the users.


>
>     See also
>>    http://gee.cs.oswego.edu/cgi-_**_bin/viewcvs.cgi/jsr166/src/__**
>> extra166y/__**CustomConcurrentHashMap.java<http://gee.cs.oswego.edu/cgi-__bin/viewcvs.cgi/jsr166/src/__extra166y/__CustomConcurrentHashMap.java>
>>
>>    which supports user-defined equivalence comparisons.
>> That is about the equivalence of the Keys, not of the Values.
>>
>
> If you look closer it also support value equivalence.


Indeed! See my answers above.

Cheers,
?


>
> cheers
>  Kasper
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111003/6f5b7a78/attachment-0001.html>

From kasper at kav.dk  Mon Oct  3 14:07:15 2011
From: kasper at kav.dk (Kasper Nielsen)
Date: Mon, 03 Oct 2011 20:07:15 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CANPzfU8oMc7D6q0PYPTe5Z4Q8-_goEqipqZWqXPo2ZRN52FfWA@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89C517.9050304@optrak.com>
	<CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>
	<4E89CF5D.3020707@optrak.com>
	<CANPzfU8FC36arPPBYD8EE_W5SJJBFk-z-Wqg+K8Eq6gNtcnnTw@mail.gmail.com>
	<4E89D900.1070601@kav.dk>
	<CANPzfU9zUQjZdS=w3M-q2vtOGcPHFs94etyP_ASOFjshVqdcdg@mail.gmail.com>
	<4E89E4AB.3010108@kav.dk>
	<CANPzfU8oMc7D6q0PYPTe5Z4Q8-_goEqipqZWqXPo2ZRN52FfWA@mail.gmail.com>
Message-ID: <4E89F9D3.9000700@kav.dk>

On 03-10-2011 19:20, ?iktor ?lang wrote:
>     Adding 1 interface is better then adding 2.
>
>
> According to what definition of "better"? To be honest, if we'd have
> more fine grained interfaces in Java, we would probably not be in this
> situation we are in today, where the Collection interface is so broad
> you oftentimes have to choose between performance or add
> UnsupportedOperationExceptions.
> If we'd have more fine-grained interfaces (or typeclasses) we could be
> more selective in what data gets exposed.

According to the design goals of the Java collection framework.
http://download.oracle.com/javase/1.4.2/docs/guide/collections/overview.html, 
1 interface is "better" than 2.

I know things are probably different in Scala land. But i do think the 
designers of the Java collection API made the right choices with regards 
to this. Is it perfect? no. But I would rather have 10 interfaces with 
optional operations not capturing distinctions such as modifiability 
than 20 - 30 interfaces that did. Others might disagree, but I really do 
not think it was a mistake as you are implying.

Do it places a burden on implementors, yes. But for every time there is 
one person implementing the Collection interface there are probably 
thousands that are merely users of that interface.

I really think too many API designers think too much on taxonomy and to 
little of the "conceptual weight" of the APIs they design.

Hey! there are users that need to figure out how that APIs work. Whether 
there are 10 interfaces or 20 - 30 interfaces matters to them.

Anyway, this is probably not the right forum do discuss this. And we are 
probably not going to come to any kind of agreement on this anyway.

cheers
   Kasper

From viktor.klang at gmail.com  Mon Oct  3 14:23:43 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 3 Oct 2011 20:23:43 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E89F9D3.9000700@kav.dk>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89C517.9050304@optrak.com>
	<CANPzfU-k8K9j14eYxLJR9PbB1sTNsxHt_bW1=UMUku+_beO96Q@mail.gmail.com>
	<4E89CF5D.3020707@optrak.com>
	<CANPzfU8FC36arPPBYD8EE_W5SJJBFk-z-Wqg+K8Eq6gNtcnnTw@mail.gmail.com>
	<4E89D900.1070601@kav.dk>
	<CANPzfU9zUQjZdS=w3M-q2vtOGcPHFs94etyP_ASOFjshVqdcdg@mail.gmail.com>
	<4E89E4AB.3010108@kav.dk>
	<CANPzfU8oMc7D6q0PYPTe5Z4Q8-_goEqipqZWqXPo2ZRN52FfWA@mail.gmail.com>
	<4E89F9D3.9000700@kav.dk>
Message-ID: <CANPzfU-uvWOAkhhmzWpN0qGwpxkx-p9KwGEMNP6LL8+kGwupLg@mail.gmail.com>

On Mon, Oct 3, 2011 at 8:07 PM, Kasper Nielsen <kasper at kav.dk> wrote:

> On 03-10-2011 19:20, ?iktor ?lang wrote:
>
>>    Adding 1 interface is better then adding 2.
>>
>>
>> According to what definition of "better"? To be honest, if we'd have
>> more fine grained interfaces in Java, we would probably not be in this
>> situation we are in today, where the Collection interface is so broad
>> you oftentimes have to choose between performance or add
>> UnsupportedOperationExceptions**.
>> If we'd have more fine-grained interfaces (or typeclasses) we could be
>> more selective in what data gets exposed.
>>
>
>
Mate, you've completely sidetracked here, would have much appreciated if we
could stay on topic, I believe I made several points that you chose to skip
in your reply.


> According to the design goals of the Java collection framework.
> http://download.oracle.com/**javase/1.4.2/docs/guide/**
> collections/overview.html<http://download.oracle.com/javase/1.4.2/docs/guide/collections/overview.html>,
> 1 interface is "better" than 2.
>
> I know things are probably different in Scala land. But i do think the
> designers of the Java collection API made the right choices with regards to
> this. Is it perfect? no. But I would rather have 10 interfaces with optional
> operations not capturing distinctions such as modifiability than 20 - 30
> interfaces that did. Others might disagree, but I really do not think it was
> a mistake as you are implying.
>

II used to have the exact same opinion, then I found something better. We
should never be afraid of questioning the status quo out of fear of change.


>
> Do it places a burden on implementors, yes. But for every time there is one
> person implementing the Collection interface there are probably thousands
> that are merely users of that interface.
>

I think it's beneficial to let the compiler enforce contracts than having to
surrender to runtime exceptions.


>
> I really think too many API designers think too much on taxonomy and to
> little of the "conceptual weight" of the APIs they design.
>

Those two things are completely orthogonal so there's no reason not to
strive for both.


>
> Hey! there are users that need to figure out how that APIs work. Whether
> there are 10 interfaces or 20 - 30 interfaces matters to them.
>

The number of interfaces isn't nearly as interesting as the number of
methods. I'd rather have a 1000 interfaces with 10 methods than 10000
methods in 1 interface.


>
> Anyway, this is probably not the right forum do discuss this. And we are
> probably not going to come to any kind of agreement on this anyway.


I think that's a bit of a shame, since I opened the door by considering to
add hash as an optional override in Equivalence.

Cheers,
?


>
>
> cheers
>  Kasper
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111003/93294dc2/attachment.html>

From fweimer at bfk.de  Tue Oct  4 04:46:17 2011
From: fweimer at bfk.de (Florian Weimer)
Date: Tue, 04 Oct 2011 08:46:17 +0000
Subject: [concurrency-interest] Persistent data structures
Message-ID: <82fwj9p2jq.fsf@mid.bfk.de>

I'm not sure if this is the right place to ask, but anyway: I'm looking
for reasonably efficient implementations of persistent data structures,
mainly maps and list (with fast index-based access and addition at the
end).  Is there a canonical library I should choose?

-- 
Florian Weimer                <fweimer at bfk.de>
BFK edv-consulting GmbH       http://www.bfk.de/
Kriegsstra?e 100              tel: +49-721-96201-1
D-76133 Karlsruhe             fax: +49-721-96201-99


From kkr at trifork.com  Tue Oct  4 04:57:49 2011
From: kkr at trifork.com (Karl Krukow)
Date: Tue, 4 Oct 2011 10:57:49 +0200
Subject: [concurrency-interest] Persistent data structures
In-Reply-To: <82fwj9p2jq.fsf@mid.bfk.de>
References: <82fwj9p2jq.fsf@mid.bfk.de>
Message-ID: <014191E2-14CF-415B-9CA6-F80E86134C61@trifork.com>

The Clojure programming language has many persistent datastructures and it's creator Rich Hickey is very conscious about performance. So I recommend trying Clojure.

Alternatively if you are staying in Java, you can use clj-ds which is a port of Clojures data structures that I've made to work in a Java context. Clojure has just been updated to 1.3, and I'm looking to port any changes to clj-ds.

Kind Regards,
- Karl

https://github.com/krukow/clj-ds

On 04/10/2011, at 10.46, Florian Weimer wrote:

> I'm not sure if this is the right place to ask, but anyway: I'm looking
> for reasonably efficient implementations of persistent data structures,
> mainly maps and list (with fast index-based access and addition at the
> end).  Is there a canonical library I should choose?
> 
> -- 
> Florian Weimer                <fweimer at bfk.de>
> BFK edv-consulting GmbH       http://www.bfk.de/
> Kriegsstra?e 100              tel: +49-721-96201-1
> D-76133 Karlsruhe             fax: +49-721-96201-99
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From mohanr at fss.co.in  Tue Oct  4 05:01:10 2011
From: mohanr at fss.co.in (Mohan Radhakrishnan)
Date: Tue, 4 Oct 2011 14:31:10 +0530
Subject: [concurrency-interest] ConcurrentHashMap of singletons
Message-ID: <65F4195D4E2C6042BA66D4114E5BB9DC4D483F@fssbemail.fss.india>

Hi,

 

 

    private static final Map<String, Provider> providers =

        new ConcurrentHashMap<String, Provider>();

 

    public static Service newInstance(String name) {

        Provider p = providers.get(name);

        if (p == null)

            throw new IllegalArgumentException(

                "No provider registered with name: " + name);

        return p.newService();

    }

 

 I have had a doubt about this type of code in 'Effective Java' that
uses a 'ConcurrentHashMap' instead of synchronizing newInstance. This
looks like another way of implementing a set of singletons if we extend
this pattern by putting a new object if it is null. Are there any
problems here ?

 

Thanks,

Mohan



DISCLAIMER:
==========================================================================================================================================================The information contained in this e-mail message may be privileged and/or confidential and protected from disclosure under applicable law. It is intended only for the individual to whom or entity to which it is addressed as shown at the beginning of the message. If the reader of this message is not the intended recipient, or if the employee or agent responsible for delivering the message is not an employee or agent of the intended recipient, you are hereby notified that any review, dissemination,distribution, use, or copying of this message is strictly prohibited. If you have received this message in error, please notify us immediately by return e-mail and permanently delete this message and your reply to the extent it includes this message. Any views or opinions presented in this message or attachments are those of the author and do not necessarily represent those of the Company. All e-mails and attachments sent and received are subject to monitoring, reading, and archival by the Company.==========================================================================================================================================================
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111004/7bfc4558/attachment-0001.html>

From nader at aeinehchi.com  Tue Oct  4 05:59:04 2011
From: nader at aeinehchi.com (Nader Aeinehchi)
Date: Tue, 04 Oct 2011 11:59:04 +0200
Subject: [concurrency-interest] ConcurrentHashMap of singletons
In-Reply-To: <65F4195D4E2C6042BA66D4114E5BB9DC4D483F@fssbemail.fss.india>
References: <65F4195D4E2C6042BA66D4114E5BB9DC4D483F@fssbemail.fss.india>
Message-ID: <4E8AD8E8.70201@aeinehchi.com>

As long as you have correctly populated the map(providers) with provider 
objects, this pattern:
1. acts as a singleton
2. is thread safe
3. scales good

You should prefer this pattern rather synchronizing newInstance.





On 10/04/2011 11:01 AM, Mohan Radhakrishnan wrote:
>
> Hi,
>
>     private static final Map<String, Provider> providers =
>
>         new ConcurrentHashMap<String, Provider>();
>
>     public static Service newInstance(String name) {
>
>         Provider p = providers.get(name);
>
>         if (p == null)
>
>             throw new IllegalArgumentException(
>
>                 "No provider registered with name: " + name);
>
>         return p.newService();
>
>     }
>
>  I have had a doubt about this type of code in 'Effective Java' that 
> uses a 'ConcurrentHashMap' instead of synchronizing newInstance. This 
> looks like another way of implementing a set of singletons if we 
> extend this pattern by putting a new object if it is null. Are there 
> any problems here ?
>
> Thanks,
>
> Mohan
>
> DISCLAIMER:
> ========================================================================================================================================================== 
>
> The information contained in this e-mail message may be privileged 
> and/or confidential and protected from disclosure under applicable 
> law. It is intended only for the individual to whom or entity to which 
> it is addressed as shown at the beginning of the message. If the 
> reader of this message is not the intended recipient, or if the 
> employee or agent responsible for delivering the message is not an 
> employee or agent of the intended recipient, you are hereby notified 
> that any review, dissemination,distribution, use, or copying of this 
> message is strictly prohibited. If you have received this message in 
> error, please notify us immediately by return e-mail and permanently 
> delete this message and your reply to the extent it includes this 
> message. Any views or opinions presented in this message or 
> attachments are those of the author and do not necessarily represent 
> those of the Company. All e-mails and attachments sent and received 
> are subject to monitoring, reading, and archival by the Company.
> ==========================================================================================================================================================
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111004/ed0cc214/attachment.html>

From joe.bowbeer at gmail.com  Tue Oct  4 06:10:35 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 4 Oct 2011 03:10:35 -0700
Subject: [concurrency-interest] ConcurrentHashMap of singletons
In-Reply-To: <65F4195D4E2C6042BA66D4114E5BB9DC4D483F@fssbemail.fss.india>
References: <65F4195D4E2C6042BA66D4114E5BB9DC4D483F@fssbemail.fss.india>
Message-ID: <CAHzJPEq8UGbKtQdNcJ=crNAe9vEP7tXfV_-g-oNsUjXDnP22sg@mail.gmail.com>

You may want to use putIfAbsent unless you add external synchronization.

This is covered in "Java Concurrency in Practice" in several sections at the
end of chapter 5.

http://jcip.net/listings.html

By the way, a similar question was asked in June about a "Multiton"
implementation in Wikipedia, which could still use a little work:

http://en.wikipedia.org/wiki/Multiton_pattern#Java

In the next version of ConcurrentHashMap, computeIfAbsent functionality is
planned, which may be even better suited for your needs:

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/ConcurrentHashMapV8.java?view=markup

Joe

On Tue, Oct 4, 2011 at 2:01 AM, Mohan Radhakrishnan wrote:

>  Hi,****
>
> ** **
>
> ** **
>
>     private static final Map<String, Provider> providers =****
>
>         new ConcurrentHashMap<String, Provider>();****
>
> ** **
>
>     public static Service newInstance(String name) {****
>
>         Provider p = providers.get(name);****
>
>         if (p == null)****
>
>             throw new IllegalArgumentException(****
>
>                 "No provider registered with name: " + name);****
>
>         return p.newService();****
>
>     }****
>
> ** **
>
>  I have had a doubt about this type of code in ?Effective Java? that uses a
> ?ConcurrentHashMap? instead of synchronizing newInstance. This looks like
> another way of implementing a set of singletons if we extend this pattern by
> putting a new object if it is null. Are there any problems here ?****
>
> ** **
>
> Thanks,****
>
> Mohan****
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111004/72a60122/attachment.html>

From kasper at kav.dk  Tue Oct  4 10:02:44 2011
From: kasper at kav.dk (Kasper Nielsen)
Date: Tue, 04 Oct 2011 16:02:44 +0200
Subject: [concurrency-interest] ThreadLocalRandom initial seed
In-Reply-To: <BANLkTi=E2rYKmoV27W_X4Z5VWsMTKRPM6XxQ_=sMqe3aanJm2w@mail.gmail.com>
References: <BANLkTing9Vx0_zOpY_sS75e0eTReocfd+w@mail.gmail.com>
	<BANLkTin=mj1+0+8GZLkmAFiT9WmUa8qg1GsSkBjQptEyVpGRaw@mail.gmail.com>
	<BANLkTi=E2rYKmoV27W_X4Z5VWsMTKRPM6XxQ_=sMqe3aanJm2w@mail.gmail.com>
Message-ID: <4E8B1204.6080307@kav.dk>

On 06-06-2011 22:27, Martin Buchholz wrote:
> On Fri, Jun 3, 2011 at 14:35, Martin Buchholz<martinrb at google.com>  wrote:
>
> jsr166 CVS now has a fixed version of Random.java that causes
> ThreadLocalRandom to work correctly.

Was java 7 shipped without the fix?

I?m doing
public static void main(String[] args) {
     System.out.println(ThreadLocalRandom.current().nextInt());
}
and getting a 0 everytime

cheers
  Kasper

From chris.hegarty at oracle.com  Tue Oct  4 10:28:15 2011
From: chris.hegarty at oracle.com (Chris Hegarty)
Date: Tue, 04 Oct 2011 15:28:15 +0100
Subject: [concurrency-interest] ThreadLocalRandom initial seed
In-Reply-To: <4E8B1204.6080307@kav.dk>
References: <BANLkTing9Vx0_zOpY_sS75e0eTReocfd+w@mail.gmail.com>	<BANLkTin=mj1+0+8GZLkmAFiT9WmUa8qg1GsSkBjQptEyVpGRaw@mail.gmail.com>	<BANLkTi=E2rYKmoV27W_X4Z5VWsMTKRPM6XxQ_=sMqe3aanJm2w@mail.gmail.com>
	<4E8B1204.6080307@kav.dk>
Message-ID: <4E8B17FF.3040908@oracle.com>

On 10/ 4/11 03:02 PM, Kasper Nielsen wrote:
> On 06-06-2011 22:27, Martin Buchholz wrote:
>> On Fri, Jun 3, 2011 at 14:35, Martin Buchholz<martinrb at google.com> wrote:
>>
>> jsr166 CVS now has a fixed version of Random.java that causes
>> ThreadLocalRandom to work correctly.
>
> Was java 7 shipped without the fix?

Yes, JDK7 shipped with this issue. It has been fixed in JDK8 [1] and 7u2 
[2]. You will need to get the latest 7u2 pre-release binary.

-Chris.

[1] http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7051516
[2] http://bugs.sun.com/view_bug.do?bug_id=2210683

>
> I?m doing
> public static void main(String[] args) {
> System.out.println(ThreadLocalRandom.current().nextInt());
> }
> and getting a 0 everytime
>
> cheers
> Kasper
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From cheremin at gmail.com  Tue Oct  4 15:00:03 2011
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Tue, 4 Oct 2011 23:00:03 +0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <4E85EFD3.1080204@cs.oswego.edu>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
Message-ID: <CAOwENiKkQc610xh8TWoZb25arsUvX0-2h5X2iN2aRriT-dTNnA@mail.gmail.com>

Oh, thank you Doug for your patience! Seems like now it is clear for me.

Now I can see the most confusing part for me was  about "preceeding stores
can't be reordered". Primary
I do not understand does term "reordering" here is about "instruction
reordering" only, or it is about instruction
effect _visibility_ reordering. I mean that stores can be issues in one
order, but I can see their effects in
different order due to some complex mechanics inside memory cache hierarchy.
Usually JMM uses term
reordering only in discussions and examples, not in formal definitions, so I
have no feeling of "default" meaning
for reordering in this context... May be it makes sense to clarify javadocs
about this?



On 09/30/11 11:18, Ruslan Cheremin wrote:
>
>> I still do not catch it. As far as I see we have 2 question about the
>> code:
>>
>
> These are still good questions, because they have no direct answers in JLS.
> But because they don't, you need to use other existing information to
> decode.
> One more try:
>
>
>> 1) Will lazySet write actually happens in some finite time?
>>
>
> The most you can say from the spec is that it will be written
> no later than at the point that the process must write anything
> else in the Synchronization Order, if such a point exists.
> However, independently of the spec, we know that so long as
> any process makes progress,  only a finite number of writes
> can be delayed. So, yes.
>
>  2) If it happens (== we see spin-wait loop finished) -- does it mean,
>>     what all writes preceeding lazySet are also done, commited, _and
>> visible_
>> to thread 2,
>>     which finished spin-wait loop?
>>
>
> Yes, although technically, you cannot show this by reference
> to the Synchronization Order in the current JLS. A fully
> integrated account would be along the lines of that for
> the (non-existent) order{Writes,Accesses} Fences in
> http://gee.cs.oswego.edu/dl/**jsr166/dist/docs/java/util/**
> concurrent/atomic/Fences.html<http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html>
>
> -Doug
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111004/9838ea68/attachment.html>

From dl at cs.oswego.edu  Wed Oct  5 09:11:17 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 05 Oct 2011 09:11:17 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E89BE04.7060907@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu>
Message-ID: <4E8C5775.8090805@cs.oswego.edu>

I'm still contemplating changes in handling null-returns
in the new compute methods. But in the mean time, here
is another tiny corner-case issue that is handled differently
in ConcurrentHashMapV8 than in the existing j.u.c version.
I consider it just a misfeature-fix, but Joe Bowbeer remains
nervous about it, so urged me to post and solicit reaction.

This takes a fair amount of context to set up:

The Set<Map.Entry> you get from map.entrySet() has always
had some problematic specs, mainly because they are
based on those of Map.Entry. When you have a Map.Entry,
you don't know whether it is mutable vs immutable
because setValue is an optional operation. And in some
cases, you don't whether setValue actually updates
the map you got the entry from or just updates an
unbound Map.Entry object.

While it would be nice to better specify some of this,
the current Map-related specs are phrased in such a way
that implementations can normally do what users expect.
The main intent of setValue is to support only
one use case: iterating through entries one-by-one
and changing some of the mapped values in the absence
of any modifications by other threads. ConcurrentHashMap
(both the existing and V8 versions) supports this use case
in the expected way -- which of course can have surprising
effects if other threads ARE modifying the map while
you are doing this. But we don't go so far as
to disable the method because of this.

However, the write-through nature of setValue
(i.e., to actually change the mapped value in
the map) conflicts with the intent of the
entrySet.toArray() method. Even though the
Collection.toArray() spec doesn't explicitly
say so, users expect it to return an independent
copy of the elements, in array form. For example,
you would be surprised and unhappy if you did
   Object[] a = myArrayList.toArray();
   a[0] = 17;
and then found out that this caused myArrayList.get(0)
to also now return 17.

In the case of ConcurrentHashMap, this means that the
kind of Entry you get for the elements of entrySet.toArray
should NOT write back to the map you got them from.
In other words:
   Object[] a = myMap.entrySet().toArray();
   (Entry[])a[0].setValue(17);
should not change myMap.get(keyFor0) to return 17.

However, the existing j.u.c version does so. This was
changed to the safer non-write-through form for V8.
The original/existing behavior was mainly just due to
oversight -- it would have been different to begin with
if anyone had noticed this issue of establishing a
persistent aliasing path (unlike the transient path
available in one-by-one iteration).

In general the interactions of loose specs for both
Map.Entry and Collection.toArray mean that you can't
count on very much here, and it turns out that different
Maps do different things. For ConcurrentHashMap, we'd
like it to do the most sensible thing, at least in
future versions. However, because the specs are loose,
we cannot claim that this is a bug-fix; it is just
a mis-feature fix.

To me, fixing the mis-feature is more important
than retaining pure compatibility for a usage
that probably didn't do what people expected anyway.
But if you've ever done anything relying on the old j.u.c
behavior, could you let us know? Thanks!

-Doug

From studdugie at gmail.com  Wed Oct  5 09:31:06 2011
From: studdugie at gmail.com (Dane Foster)
Date: Wed, 5 Oct 2011 09:31:06 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8C5775.8090805@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu> <4E8C5775.8090805@cs.oswego.edu>
Message-ID: <CA+WxinJgXw2h7cEJzFQ8yTiXnHnxSntYfk9GajPpx82g_px+9A@mail.gmail.com>

+1 Fix it

Dane


On Wed, Oct 5, 2011 at 9:11 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> I'm still contemplating changes in handling null-returns
> in the new compute methods. But in the mean time, here
> is another tiny corner-case issue that is handled differently
> in ConcurrentHashMapV8 than in the existing j.u.c version.
> I consider it just a misfeature-fix, but Joe Bowbeer remains
> nervous about it, so urged me to post and solicit reaction.
>
> This takes a fair amount of context to set up:
>
> The Set<Map.Entry> you get from map.entrySet() has always
> had some problematic specs, mainly because they are
> based on those of Map.Entry. When you have a Map.Entry,
> you don't know whether it is mutable vs immutable
> because setValue is an optional operation. And in some
> cases, you don't whether setValue actually updates
> the map you got the entry from or just updates an
> unbound Map.Entry object.
>
> While it would be nice to better specify some of this,
> the current Map-related specs are phrased in such a way
> that implementations can normally do what users expect.
> The main intent of setValue is to support only
> one use case: iterating through entries one-by-one
> and changing some of the mapped values in the absence
> of any modifications by other threads. ConcurrentHashMap
> (both the existing and V8 versions) supports this use case
> in the expected way -- which of course can have surprising
> effects if other threads ARE modifying the map while
> you are doing this. But we don't go so far as
> to disable the method because of this.
>
> However, the write-through nature of setValue
> (i.e., to actually change the mapped value in
> the map) conflicts with the intent of the
> entrySet.toArray() method. Even though the
> Collection.toArray() spec doesn't explicitly
> say so, users expect it to return an independent
> copy of the elements, in array form. For example,
> you would be surprised and unhappy if you did
>  Object[] a = myArrayList.toArray();
>  a[0] = 17;
> and then found out that this caused myArrayList.get(0)
> to also now return 17.
>
> In the case of ConcurrentHashMap, this means that the
> kind of Entry you get for the elements of entrySet.toArray
> should NOT write back to the map you got them from.
> In other words:
>  Object[] a = myMap.entrySet().toArray();
>  (Entry[])a[0].setValue(17);
> should not change myMap.get(keyFor0) to return 17.
>
> However, the existing j.u.c version does so. This was
> changed to the safer non-write-through form for V8.
> The original/existing behavior was mainly just due to
> oversight -- it would have been different to begin with
> if anyone had noticed this issue of establishing a
> persistent aliasing path (unlike the transient path
> available in one-by-one iteration).
>
> In general the interactions of loose specs for both
> Map.Entry and Collection.toArray mean that you can't
> count on very much here, and it turns out that different
> Maps do different things. For ConcurrentHashMap, we'd
> like it to do the most sensible thing, at least in
> future versions. However, because the specs are loose,
> we cannot claim that this is a bug-fix; it is just
> a mis-feature fix.
>
> To me, fixing the mis-feature is more important
> than retaining pure compatibility for a usage
> that probably didn't do what people expected anyway.
> But if you've ever done anything relying on the old j.u.c
> behavior, could you let us know? Thanks!
>
> -Doug
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111005/38f0ad56/attachment.html>

From david.lloyd at redhat.com  Wed Oct  5 18:31:55 2011
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 05 Oct 2011 15:31:55 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8C5775.8090805@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>
	<4E8C5775.8090805@cs.oswego.edu>
Message-ID: <4E8CDADB.7030205@redhat.com>

On 10/5/11 6:11 AM, Doug Lea wrote:
> I'm still contemplating changes in handling null-returns
> in the new compute methods. But in the mean time, here
> is another tiny corner-case issue that is handled differently
> in ConcurrentHashMapV8 than in the existing j.u.c version.
> I consider it just a misfeature-fix, but Joe Bowbeer remains
> nervous about it, so urged me to post and solicit reaction.
>
> This takes a fair amount of context to set up:
> [...snip...]
> For example,
> you would be surprised and unhappy if you did
> Object[] a = myArrayList.toArray();
> a[0] = 17;
> and then found out that this caused myArrayList.get(0)
> to also now return 17.
>
> In the case of ConcurrentHashMap, this means that the
> kind of Entry you get for the elements of entrySet.toArray
> should NOT write back to the map you got them from.
> In other words:
> Object[] a = myMap.entrySet().toArray();
> (Entry[])a[0].setValue(17);
> should not change myMap.get(keyFor0) to return 17.

I disagree with this conclusion; these scenarios aren't really the same. 
  The general contract of toArray() is to return an array consisting of 
all the values in the collection - I think interpreting it such that it 
can return copies of the values is a stretch.  I can't think of any case 
where we'd actually copy the values in normal collections; I don't see 
why an entry set should be any different, magical write-through 
notwithstanding.

For example if I did:

Object[] a = myMap.entrySet().toArray();

I would expect the same result as if I did:

Object[] a = new ArrayList<Entry<Blah,Blah>>(myMap.entrySet()).toArray();

I don't think toArray should be special here.  I think this is the "most 
correct" interpretation of this requirement, and in any case is the one 
I've adhered to whenever developing collection implementations.  I think 
that there should be no difference between the values in a toArray() 
result and the values you'd get back from a set iterator.
-- 
- DML

From joe.bowbeer at gmail.com  Wed Oct  5 19:12:26 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 5 Oct 2011 16:12:26 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8CDADB.7030205@redhat.com>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu> <4E8C5775.8090805@cs.oswego.edu>
	<4E8CDADB.7030205@redhat.com>
Message-ID: <CAHzJPEq5Ep3GhW8bBSGx1ZHtoyLz5weEvUBkikD=WdbaEuhmEQ@mail.gmail.com>

On Wed, Oct 5, 2011 at 3:31 PM, David M. Lloyd wrote:

> On 10/5/11 6:11 AM, Doug Lea wrote:
>
>> I'm still contemplating changes in handling null-returns
>> in the new compute methods. But in the mean time, here
>> is another tiny corner-case issue that is handled differently
>> in ConcurrentHashMapV8 than in the existing j.u.c version.
>> I consider it just a misfeature-fix, but Joe Bowbeer remains
>> nervous about it, so urged me to post and solicit reaction.
>>
>> This takes a fair amount of context to set up:
>> [...snip...]
>>
>> For example,
>> you would be surprised and unhappy if you did
>> Object[] a = myArrayList.toArray();
>> a[0] = 17;
>> and then found out that this caused myArrayList.get(0)
>> to also now return 17.
>>
>> In the case of ConcurrentHashMap, this means that the
>> kind of Entry you get for the elements of entrySet.toArray
>> should NOT write back to the map you got them from.
>> In other words:
>> Object[] a = myMap.entrySet().toArray();
>> (Entry[])a[0].setValue(17);
>> should not change myMap.get(keyFor0) to return 17.
>>
>
> I disagree with this conclusion; these scenarios aren't really the same.
>  The general contract of toArray() is to return an array consisting of all
> the values in the collection - I think interpreting it such that it can
> return copies of the values is a stretch.  I can't think of any case where
> we'd actually copy the values in normal collections; I don't see why an
> entry set should be any different, magical write-through notwithstanding.
>
> For example if I did:
>
> Object[] a = myMap.entrySet().toArray();
>
> I would expect the same result as if I did:
>
> Object[] a = new ArrayList<Entry<Blah,Blah>>(**
> myMap.entrySet()).toArray();
>
> I don't think toArray should be special here.  I think this is the "most
> correct" interpretation of this requirement, and in any case is the one I've
> adhered to whenever developing collection implementations.  I think that
> there should be no difference between the values in a toArray() result and
> the values you'd get back from a set iterator.
> --
> - DML


I agree with your interpretation and expectation regarding toArray.  But the
Map.Entry documentation (below) limits their validity to the duration of the
iteration.  How do you reconcile the two?

http://download.oracle.com/javase/6/docs/api/java/util/Map.Entry.html

"The *only* way to obtain a reference to a map entry is from the iterator of
this [entry-set] collection-view. These Map.Entry objects are valid *only*
for the duration of the iteration; more formally, the behavior of a map
entry is undefined if the backing map has been modified after the entry was
returned by the iterator, except through the setValue operation on the map
entry."

--Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111005/67e2a88e/attachment.html>

From david.lloyd at redhat.com  Wed Oct  5 19:46:57 2011
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 05 Oct 2011 16:46:57 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CAHzJPEq5Ep3GhW8bBSGx1ZHtoyLz5weEvUBkikD=WdbaEuhmEQ@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>
	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>
	<4E8C5775.8090805@cs.oswego.edu>	<4E8CDADB.7030205@redhat.com>
	<CAHzJPEq5Ep3GhW8bBSGx1ZHtoyLz5weEvUBkikD=WdbaEuhmEQ@mail.gmail.com>
Message-ID: <4E8CEC71.8030706@redhat.com>

On 10/5/11 4:12 PM, Joe Bowbeer wrote:
> On Wed, Oct 5, 2011 at 3:31 PM, David M. Lloyd wrote:
>
>     On 10/5/11 6:11 AM, Doug Lea wrote:
>
>         I'm still contemplating changes in handling null-returns
>         in the new compute methods. But in the mean time, here
>         is another tiny corner-case issue that is handled differently
>         in ConcurrentHashMapV8 than in the existing j.u.c version.
>         I consider it just a misfeature-fix, but Joe Bowbeer remains
>         nervous about it, so urged me to post and solicit reaction.
>
>         This takes a fair amount of context to set up:
>         [...snip...]
>
>         For example,
>         you would be surprised and unhappy if you did
>         Object[] a = myArrayList.toArray();
>         a[0] = 17;
>         and then found out that this caused myArrayList.get(0)
>         to also now return 17.
>
>         In the case of ConcurrentHashMap, this means that the
>         kind of Entry you get for the elements of entrySet.toArray
>         should NOT write back to the map you got them from.
>         In other words:
>         Object[] a = myMap.entrySet().toArray();
>         (Entry[])a[0].setValue(17);
>         should not change myMap.get(keyFor0) to return 17.
>
>
>     I disagree with this conclusion; these scenarios aren't really the
>     same.  The general contract of toArray() is to return an array
>     consisting of all the values in the collection - I think
>     interpreting it such that it can return copies of the values is a
>     stretch.  I can't think of any case where we'd actually copy the
>     values in normal collections; I don't see why an entry set should be
>     any different, magical write-through notwithstanding.
>
>     For example if I did:
>
>     Object[] a = myMap.entrySet().toArray();
>
>     I would expect the same result as if I did:
>
>     Object[] a = new
>     ArrayList<Entry<Blah,Blah>>(__myMap.entrySet()).toArray();
>
>     I don't think toArray should be special here.  I think this is the
>     "most correct" interpretation of this requirement, and in any case
>     is the one I've adhered to whenever developing collection
>     implementations.  I think that there should be no difference between
>     the values in a toArray() result and the values you'd get back from
>     a set iterator.
>     --
>     - DML
>
>
> I agree with your interpretation and expectation regarding toArray.  But
> the Map.Entry documentation (below) limits their validity to the
> duration of the iteration.  How do you reconcile the two?
>
> http://download.oracle.com/javase/6/docs/api/java/util/Map.Entry.html
>
> "The *only* way to obtain a reference to a map entry is from the
> iterator of this [entry-set] collection-view. These Map.Entry objects
> are valid *only* for the duration of the iteration; more formally, the
> behavior of a map entry is undefined if the backing map has been
> modified after the entry was returned by the iterator, except through
> the setValue operation on the map entry."

Well be cautious how you define "the duration".  I think that the real 
definition as given by the "more formally" section relates to modifying 
the map after an iterator is created.  And I really interpret this as 
meaning that the entry *must* be valid before such a modification, and 
*may* be valid afterward.  For an implementer this means you are allowed 
to keep them valid for as long as is reasonable or convenient.  From a 
user perspective it means that you'd better not rely on it unless you 
know what implementation you're dealing with and what guarantees it 
makes, above and beyond what is specified.

But for ConcurrentMap in particular, this restriction needs a rethink 
given that the whole _point_ of the thing is to provide concurrent 
access; entry iterators become a lot less useful if you must freeze 
write access to the whole map while you iterate.  I think it might be a 
good idea to override entrySet() for the purpose of adding documentation 
to clarify this.

In any case, I think you can say that "during iteration" is equal to the 
time in which you have any active references to the items returned by 
the iterator, because no other definition makes sense.  So if toArray() 
is defined in terms of iteration then there is no conflict - using the 
array is the same as using the values returned by the iterator as they 
are returned; the iteration is seen to be in progress until the last 
entry is "discarded".  I don't see anything in the docs that contradict 
this.


-- 
- DML

From crazybob at crazybob.org  Wed Oct  5 20:48:15 2011
From: crazybob at crazybob.org (Bob Lee)
Date: Wed, 5 Oct 2011 17:48:15 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8CDADB.7030205@redhat.com>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu> <4E8C5775.8090805@cs.oswego.edu>
	<4E8CDADB.7030205@redhat.com>
Message-ID: <CAGmsiP4VUFbdXxNTrsFpPouk4tRotjn5g5f4o3Cyow8t3BrciQ@mail.gmail.com>

One more thing to consider: using entrySet.toArray() involves creating an
array of a generic type (or actually casting an Entry[] to Entry<K, V>[]).
That's probably not something we want to encourage or go out of our way to
enable.

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111005/a948833c/attachment.html>

From davidcholmes at aapt.net.au  Thu Oct  6 01:15:01 2011
From: davidcholmes at aapt.net.au (davidcholmes at aapt.net.au)
Date: Thu, 06 Oct 2011 16:15:01 +1100
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8CEC71.8030706@redhat.com>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<4E68AAC9.9070800@cs.oswego.edu> <4E85F532.1040808@redhat.com>
	<4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu> <4E8C5775.8090805@cs.oswego.edu>
	<4E8CDADB.7030205@redhat.com>
	<CAHzJPEq5Ep3GhW8bBSGx1ZHtoyLz5weEvUBkikD=WdbaEuhmEQ@mail.gmail.com>
	<4E8CEC71.8030706@redhat.com>
Message-ID: <20111006161501.nd9r29pny8sooww8@getonline.aapt.com.au>

Quoting "David M. Lloyd" <david.lloyd at redhat.com>:
> In any case, I think you can say that "during iteration" is equal to
> the time in which you have any active references to the items returned
> by the iterator, because no other definition makes sense.

On the contrary, "during the iteration" means to me that the iterator  
is still active and has not yet reached the end of the collection.  
Once we reach the end the iteration is over, and so an array created  
by iteration contains invalid map.entry objects the instant the  
filling of the array completes.

Though what exactly it means to have an "invalid" map.entry object is  
itself undefined.

The spec for entrySet is poorly written and the interaction with  
toArray is not clearly defined. Given that we can argue this either way.

But because we can argue it either way I'm inclined to say leave the  
existing code as-is. Neither the existing code nor the new CHMV8 code  
is "wrong" per-se.

David Holmes
------------



> So if
> toArray() is defined in terms of iteration then there is no conflict -
> using the array is the same as using the values returned by the
> iterator as they are returned; the iteration is seen to be in progress
> until the last entry is "discarded".  I don't see anything in the docs
> that contradict this.
>
>
> -- 
> - DML
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest





From dl at cs.oswego.edu  Thu Oct  6 07:30:00 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 06 Oct 2011 07:30:00 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8CDADB.7030205@redhat.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>	<4E8C5775.8090805@cs.oswego.edu>
	<4E8CDADB.7030205@redhat.com>
Message-ID: <4E8D9138.8080501@cs.oswego.edu>

On 10/05/11 18:31, David M. Lloyd wrote:
>> In the case of ConcurrentHashMap, this means that the
>> kind of Entry you get for the elements of entrySet.toArray
>> should NOT write back to the map you got them from.
>> In other words:
>> Object[] a = myMap.entrySet().toArray();
>> (Entry[])a[0].setValue(17);
>> should not change myMap.get(keyFor0) to return 17.
>
> I disagree with this conclusion; these scenarios aren't really the same. The
> general contract of toArray() is to return an array consisting of all the values
> in the collection - I think interpreting it such that it can return copies of
> the values is a stretch. I can't think of any case where we'd actually copy the
> values in normal collections;

Never deep copies. But always shallow copies. For example

Object[] a = myMap.values().toArray();
a[0] = 17;
also does not change myMap.get(keyFor0) to return 17 in
any JDK Map implementation.

Besides not wanting to set up side-channels for modifications,
the various toArray methods must have snapshot-like semantics
because they cannot possibly track additions and removals
occurring after (or concurrently with) the array creation.
This is the basis for the the lifetime disclaimers in the
Map.Entry specs.

Several people seem to have the feeling that entrySet().toArray()
should be somehow different about these policies because they know
that HashMap in particular returns internal mutable Entry objects.
Which is arguably also a bad idea. But most other Maps do not
do this. They instead return distinct Entry objects
that snapshot current mappings while usually still supporting
setValue for one-by-one traversal in the same way that CHM does,
invoking map.put. Which is a little bit crazy -- we add some
time/space overhead to entrySet operations just to imperfectly
reinforce the questionable intuition that methods operate
directly on internal Entry objects that don't actually exist
(and need not actually exist wrt any other aspects of Map specs).

-Doug

From david.lloyd at redhat.com  Thu Oct  6 17:39:14 2011
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Thu, 06 Oct 2011 16:39:14 -0500
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8D9138.8080501@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>	<4E8C5775.8090805@cs.oswego.edu>
	<4E8CDADB.7030205@redhat.com> <4E8D9138.8080501@cs.oswego.edu>
Message-ID: <4E8E2002.7090905@redhat.com>

On 10/06/2011 06:30 AM, Doug Lea wrote:
> On 10/05/11 18:31, David M. Lloyd wrote:
>>> In the case of ConcurrentHashMap, this means that the
>>> kind of Entry you get for the elements of entrySet.toArray
>>> should NOT write back to the map you got them from.
>>> In other words:
>>> Object[] a = myMap.entrySet().toArray();
>>> (Entry[])a[0].setValue(17);
>>> should not change myMap.get(keyFor0) to return 17.
>>
>> I disagree with this conclusion; these scenarios aren't really the
>> same. The
>> general contract of toArray() is to return an array consisting of all
>> the values
>> in the collection - I think interpreting it such that it can return
>> copies of
>> the values is a stretch. I can't think of any case where we'd actually
>> copy the
>> values in normal collections;
>
> Never deep copies. But always shallow copies. For example
>
> Object[] a = myMap.values().toArray();
> a[0] = 17;
> also does not change myMap.get(keyFor0) to return 17 in
> any JDK Map implementation.

Yes, but this:

Object[] a = myMap.entrySet().toArray();
a[0] = "Foo";

doesn't change the mapping that happens to be at that element either, 
which is much more analogous to your example in my opinion.

> Besides not wanting to set up side-channels for modifications,
> the various toArray methods must have snapshot-like semantics
> because they cannot possibly track additions and removals
> occurring after (or concurrently with) the array creation.
> This is the basis for the the lifetime disclaimers in the
> Map.Entry specs.

> Several people seem to have the feeling that entrySet().toArray()
> should be somehow different about these policies because they know
> that HashMap in particular returns internal mutable Entry objects.
> Which is arguably also a bad idea. But most other Maps do not
> do this. They instead return distinct Entry objects
> that snapshot current mappings while usually still supporting
> setValue for one-by-one traversal in the same way that CHM does,
> invoking map.put. Which is a little bit crazy -- we add some
> time/space overhead to entrySet operations just to imperfectly
> reinforce the questionable intuition that methods operate
> directly on internal Entry objects that don't actually exist
> (and need not actually exist wrt any other aspects of Map specs).

But if the Entry actually *did* reflect the actual physical entry, then 
the contract for the Entry interface makes a lot more sense.  You can 
retrieve the key always, and the value if it hasn't been removed (if it 
has you could give IllegalStateException as is allowed by spec).  The 
setValue() method translates more or less directly to an atomic 
getAndSet() or get+CAS (this method also may throw ISE in the event of 
removal).  You could even use the Entry objects for a primitive sort of 
polling of map state, and it's easy to define their behavior for an 
arbitrary lifespan without any hinkiness (i.e. no need to tie the 
lifespan to the iterator from whence it came, which doesn't make sense 
to me anyway, least of all in a concurrent setting).  I think it's 
perfectly reasonable to provide an Entry which is more forgiving than 
the spec requires, especially if you can give it really clear semantics.

In one of my scratchpads I have a lockless* ConcurrentMap which does 
exactly this (have its entries implement Entry, that is) implemented in 
terms of an atomic ref array of (immutable) arrays of entries.  I hope 
to have time to run it through some more testing in the next couple 
weeks, to see how it compares with what's on the table.  So it's at 
least possible... we'll see about practical. :)

The other extreme is the copy-on-write type of map where an iterator 
would reflect a snapshot and Entries would be immutable.  But this is 
also easy to make well-defined for an arbitrary lifespan.

* well, okay, it does lock to block writers during resize, so it's not 
100% lockless.
-- 
- DML

From david.lloyd at redhat.com  Thu Oct  6 19:09:51 2011
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Thu, 06 Oct 2011 18:09:51 -0500
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8E2002.7090905@redhat.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>	<4E8C5775.8090805@cs.oswego.edu>
	<4E8CDADB.7030205@redhat.com> <4E8D9138.8080501@cs.oswego.edu>
	<4E8E2002.7090905@redhat.com>
Message-ID: <4E8E353F.2080809@redhat.com>

On 10/06/2011 04:39 PM, David M. Lloyd wrote:
> In one of my scratchpads I have a lockless* ConcurrentMap which does
> exactly this (have its entries implement Entry, that is) implemented in
> terms of an atomic ref array of (immutable) arrays of entries. I hope to
> have time to run it through some more testing in the next couple weeks,
> to see how it compares with what's on the table. So it's at least
> possible... we'll see about practical. :)
>
> The other extreme is the copy-on-write type of map where an iterator
> would reflect a snapshot and Entries would be immutable. But this is
> also easy to make well-defined for an arbitrary lifespan.
>
> * well, okay, it does lock to block writers during resize, so it's not
> 100% lockless.

I put this up in 
https://github.com/dmlloyd/experimental-crap/branches/hashmaps if anyone 
is interested, or just wants a good laugh.

-- 
- DML

From dl at cs.oswego.edu  Fri Oct  7 06:41:55 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 07 Oct 2011 06:41:55 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8E2002.7090905@redhat.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>	<4E8C5775.8090805@cs.oswego.edu>	<4E8CDADB.7030205@redhat.com>
	<4E8D9138.8080501@cs.oswego.edu> <4E8E2002.7090905@redhat.com>
Message-ID: <4E8ED773.9020508@cs.oswego.edu>

On 10/06/11 17:39, David M. Lloyd wrote:
> -- we add some
>> time/space overhead to entrySet operations just to imperfectly
>> reinforce the questionable intuition that methods operate
>> directly on internal Entry objects that don't actually exist
>> (and need not actually exist wrt any other aspects of Map specs).
>
> But if the Entry actually *did* reflect the actual physical entry, then the
> contract for the Entry interface makes a lot more sense. You can retrieve the
> key always, and the value if it hasn't been removed (if it has you could give
> IllegalStateException as is allowed by spec).

We considered and rejected this approach back in JDK5 when deciding
upon iterator semantics for concurrent collections. If you allow
hasNext() to lie, claiming that an element exists but then throwing
an exception in next() because it no longer exists, then most
client iterator usages break. So instead we ensure that if
hasNext saw a valid element, then it is snapshotted as the one
returned in next(). Since these elements can concurrently come
and go at any time, receiving this (weakly consistent) snapshot
is as much as you can expect anyway. Even if you exported
a "live" entry, it could disappear as soon as client reads it
but before it acts upon it.

The underlying problem is that the non-atomic hasNext()/next()
pairing in Iterator is inherently concurrency-hostile.
But we didn't want to replace it with something else.
However, with upcoming lambdas and bulk operations, I'm
considering creating a variant of CHM that does not support
iterators at all, but instead several variants of
method forEach(entry -> action) etc.

-Doug


From cheremin at gmail.com  Fri Oct  7 09:24:20 2011
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Fri, 7 Oct 2011 17:24:20 +0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <4E85EFD3.1080204@cs.oswego.edu>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
Message-ID: <CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>

After some thinking and reading I still do not understand some issues...

As far, as I can see from description of lazySet, it is just ordinary
store with StoreStore barrier just before it. But if it is so, what is
the difference between it and ordinary volatile write? In JSR-133
implementation cookbook http://gee.cs.oswego.edu/dl/jmm/cookbook.html
you've shown volatile store implementation as store having exactly
StoreStore barrier before it.

Or the difference is what volatile store must have both LoadStore and
StoreStore barrier before?


2011/9/30 Doug Lea <dl at cs.oswego.edu>:
> On 09/30/11 11:18, Ruslan Cheremin wrote:
>>
>> I still do not catch it. As far as I see we have 2 question about the
>> code:
>
> These are still good questions, because they have no direct answers in JLS.
> But because they don't, you need to use other existing information to
> decode.
> One more try:
>
>>
>> 1) Will lazySet write actually happens in some finite time?
>
> The most you can say from the spec is that it will be written
> no later than at the point that the process must write anything
> else in the Synchronization Order, if such a point exists.
> However, independently of the spec, we know that so long as
> any process makes progress, ?only a finite number of writes
> can be delayed. So, yes.
>
>> 2) If it happens (== we see spin-wait loop finished) -- does it mean,
>> ? ? what all writes preceeding lazySet are also done, commited, _and
>> visible_
>> to thread 2,
>> ? ? which finished spin-wait loop?
>
> Yes, although technically, you cannot show this by reference
> to the Synchronization Order in the current JLS. A fully
> integrated account would be along the lines of that for
> the (non-existent) order{Writes,Accesses} Fences in
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html
>
> -Doug
>
>


From dl at cs.oswego.edu  Fri Oct  7 09:45:49 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 07 Oct 2011 09:45:49 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>	<4E85AFB2.3070804@cs.oswego.edu>	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>	<4E85CECC.3050209@cs.oswego.edu>	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
Message-ID: <4E8F028D.8030109@cs.oswego.edu>

On 10/07/11 09:24, Ruslan Cheremin wrote:
> After some thinking and reading I still do not understand some issues...
>
> As far, as I can see from description of lazySet, it is just ordinary
> store with StoreStore barrier just before it. But if it is so, what is
> the difference between it and ordinary volatile write? In JSR-133
> implementation cookbook http://gee.cs.oswego.edu/dl/jmm/cookbook.html
> you've shown volatile store implementation as store having exactly
> StoreStore barrier before it.

Plus, for a volatile, a StoreLoad fence between the write and any read.
Almost always, the only good choice for where to place it is
immediately after the write. In addition to disabling
more optimizations, StoreLoad fences translate to
instructions that are not cheap on any platform, although they are
currently a lot cheaper than they were about 5 years ago on most
platforms. But in any case, if you have a situation that is
guaranteed not to need one to preserve correctness, it is always
faster not to require one.

-Doug


From david.lloyd at redhat.com  Fri Oct  7 10:32:42 2011
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Fri, 07 Oct 2011 09:32:42 -0500
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8ED773.9020508@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>	<4E8C5775.8090805@cs.oswego.edu>	<4E8CDADB.7030205@redhat.com>
	<4E8D9138.8080501@cs.oswego.edu> <4E8E2002.7090905@redhat.com>
	<4E8ED773.9020508@cs.oswego.edu>
Message-ID: <4E8F0D8A.1080706@redhat.com>

On 10/07/2011 05:41 AM, Doug Lea wrote:
> On 10/06/11 17:39, David M. Lloyd wrote:
>> -- we add some
>>> time/space overhead to entrySet operations just to imperfectly
>>> reinforce the questionable intuition that methods operate
>>> directly on internal Entry objects that don't actually exist
>>> (and need not actually exist wrt any other aspects of Map specs).
>>
>> But if the Entry actually *did* reflect the actual physical entry,
>> then the
>> contract for the Entry interface makes a lot more sense. You can
>> retrieve the
>> key always, and the value if it hasn't been removed (if it has you
>> could give
>> IllegalStateException as is allowed by spec).
>
> We considered and rejected this approach back in JDK5 when deciding
> upon iterator semantics for concurrent collections. If you allow
> hasNext() to lie, claiming that an element exists but then throwing
> an exception in next() because it no longer exists, then most
> client iterator usages break. So instead we ensure that if
> hasNext saw a valid element, then it is snapshotted as the one
> returned in next(). Since these elements can concurrently come
> and go at any time, receiving this (weakly consistent) snapshot
> is as much as you can expect anyway. Even if you exported
> a "live" entry, it could disappear as soon as client reads it
> but before it acts upon it.

Yeah, but I think that's not outside of what should be reasonably 
expected when you iterate a (non-COW) concurrent hash map.  As you say, 
hasNext()/next() can be made consistent by actually getting a snapshot 
ref to the next entry in hasNext(), but it doesn't have to be a copy of 
the Entry, it can still be the literal Entry in the map.

Using a "real" Entry, if the user calls getValue() on the entry but it 
has since been deleted, you'd get an ISE regardless of whether it came 
from an array, iterator, or reflection right into the guts of the map, 
which is very consistent behavior.

If we wanted to really follow the precedents that exist in the 
collections lib, we'd have a ConcurrentEntry which had more atomic 
operations on it, like setValueIfAbsent(), replaceValue(), remove(), 
getValueIfPresent(), etc.  But I imagine that ship has sailed.

For the other set types, you'd probably wouldn't have to throw ISE 
because the keys don't usually go anywhere and you can train the value 
iterator to pass over missing entries and snapshot value refs, so even 
if they're deleted in the meantime the behavior is as "nice" as can be 
expected, if not more so.

> The underlying problem is that the non-atomic hasNext()/next()
> pairing in Iterator is inherently concurrency-hostile.
> But we didn't want to replace it with something else.
> However, with upcoming lambdas and bulk operations, I'm
> considering creating a variant of CHM that does not support
> iterators at all, but instead several variants of
> method forEach(entry -> action) etc.

Agreed that lambdas will greatly improve the situation, but I'm not sure 
it warrants shooting down CHM iterators altogether.

-- 
- DML

From mthornton at optrak.com  Fri Oct  7 10:45:41 2011
From: mthornton at optrak.com (Mark Thornton)
Date: Fri, 07 Oct 2011 15:45:41 +0100
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8F0D8A.1080706@redhat.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>	<4E8C5775.8090805@cs.oswego.edu>	<4E8CDADB.7030205@redhat.com>	<4E8D9138.8080501@cs.oswego.edu>
	<4E8E2002.7090905@redhat.com>	<4E8ED773.9020508@cs.oswego.edu>
	<4E8F0D8A.1080706@redhat.com>
Message-ID: <4E8F1095.9080905@optrak.com>

On 07/10/11 15:32, David M. Lloyd wrote:
> Yeah, but I think that's not outside of what should be reasonably 
> expected when you iterate a (non-COW) concurrent hash map.  As you 
> say, hasNext()/next() can be made consistent by actually getting a 
> snapshot ref to the next entry in hasNext(), but it doesn't have to be 
> a copy of the Entry, it can still be the literal Entry in the map.
I have several nonconcurrent map implementations which do not have Entry 
instances internally --- they are created on the fly as needed to 
satisfy the entrySet requirements. I usually do this where I can't 
afford the space required by literal Entry instances.

Mark Thornton


From adrian.tarau at gmail.com  Fri Oct  7 11:23:21 2011
From: adrian.tarau at gmail.com (Adrian Tarau)
Date: Fri, 07 Oct 2011 11:23:21 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8F1095.9080905@optrak.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>	<4E89BE04.7060907@cs.oswego.edu>	<4E8C5775.8090805@cs.oswego.edu>	<4E8CDADB.7030205@redhat.com>	<4E8D9138.8080501@cs.oswego.edu>	<4E8E2002.7090905@redhat.com>	<4E8ED773.9020508@cs.oswego.edu>	<4E8F0D8A.1080706@redhat.com>
	<4E8F1095.9080905@optrak.com>
Message-ID: <4E8F1969.1080903@gmail.com>

Hi Mark,

Are those implementations open source or closed source? I need a 
non-concurrent map which has minimum memory overhead per entry for a 
cache with small payloads(store a mapping between small strings).

Thanks,
Adrian Tarau.

On 10/07/2011 10:45 AM, Mark Thornton wrote:
> On 07/10/11 15:32, David M. Lloyd wrote:
>> Yeah, but I think that's not outside of what should be reasonably 
>> expected when you iterate a (non-COW) concurrent hash map.  As you 
>> say, hasNext()/next() can be made consistent by actually getting a 
>> snapshot ref to the next entry in hasNext(), but it doesn't have to 
>> be a copy of the Entry, it can still be the literal Entry in the map.
> I have several nonconcurrent map implementations which do not have 
> Entry instances internally --- they are created on the fly as needed 
> to satisfy the entrySet requirements. I usually do this where I can't 
> afford the space required by literal Entry instances.
>
> Mark Thornton
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From hans.boehm at hp.com  Fri Oct  7 14:02:19 2011
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 7 Oct 2011 18:02:19 +0000
Subject: [concurrency-interest] AtomicXXX.lazySet and
	happens-before	reasoning
In-Reply-To: <4E8F028D.8030109@cs.oswego.edu>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>

> From: Doug Lea
> 
> On 10/07/11 09:24, Ruslan Cheremin wrote:
> > After some thinking and reading I still do not understand some
> issues...
> >
> > As far, as I can see from description of lazySet, it is just ordinary
> > store with StoreStore barrier just before it. But if it is so, what
> is
> > the difference between it and ordinary volatile write? In JSR-133
> > implementation cookbook http://gee.cs.oswego.edu/dl/jmm/cookbook.html
> > you've shown volatile store implementation as store having exactly
> > StoreStore barrier before it.
It also needs a LoadStore fence, in both cases.  But even then, it's important to remember that although his may be a sufficient implementation, this is an incorrect description from the user's perspective.  In particular, if v is volatile (and certainly if it's accessed using lazySet), and x and y are ordinary variables, then the assignments to x and y in the following may be visibly reordered:

x = 1;
v = 2;
y = 3;

Volatiles are not fences.

> 
> Plus, for a volatile, a StoreLoad fence between the write and any read.
> Almost always, the only good choice for where to place it is
> immediately after the write. In addition to disabling
> more optimizations, StoreLoad fences translate to
> instructions that are not cheap on any platform, although they are
> currently a lot cheaper than they were about 5 years ago on most
> platforms. But in any case, if you have a situation that is
> guaranteed not to need one to preserve correctness, it is always
> faster not to require one.
> 
And on something like PowerPC, the implementation rules are actually currently quite unclear.  The currently preferred implementation, at least for the C++ equivalents, actually associates much of the volatile overhead with loads, though it's not completely clear that's the right choice.  Just adding a StoreLoad fence to stores isn't sufficient, for fairly complex reasons related to the fact that this view of fences is too simplistic for PowerPC.  If Java implementations follow the recommendations in http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html, then lazySet saves a relatively small percentage of the overhead, and the change amounts to weakening the fence before the store, not dropping one after it.  But that's because the StoreLoad fence is associated with the load, for which Java doesn't have a weaker form.  And having Java follow a different recipe from the C++ one is probably also a bad idea, since it essentially breaks mixed applications.

Hans


From gregg at cytetech.com  Fri Oct  7 15:57:58 2011
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 07 Oct 2011 14:57:58 -0500
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8F1095.9080905@optrak.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com>	<4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu>	<4E8C5775.8090805@cs.oswego.edu>
	<4E8CDADB.7030205@redhat.com>	<4E8D9138.8080501@cs.oswego.edu>
	<4E8E2002.7090905@redhat.com>	<4E8ED773.9020508@cs.oswego.edu>
	<4E8F0D8A.1080706@redhat.com> <4E8F1095.9080905@optrak.com>
Message-ID: <4E8F59C6.4070701@cytetech.com>

On 10/7/2011 9:45 AM, Mark Thornton wrote:
> On 07/10/11 15:32, David M. Lloyd wrote:
>> Yeah, but I think that's not outside of what should be reasonably expected
>> when you iterate a (non-COW) concurrent hash map. As you say, hasNext()/next()
>> can be made consistent by actually getting a snapshot ref to the next entry in
>> hasNext(), but it doesn't have to be a copy of the Entry, it can still be the
>> literal Entry in the map.
> I have several nonconcurrent map implementations which do not have Entry
> instances internally --- they are created on the fly as needed to satisfy the
> entrySet requirements. I usually do this where I can't afford the space required
> by literal Entry instances.

It is important for an iterator to snapshot if you can ask it for the next 
entry.  What would be better, to avoid copying and to manage concurrency easier, 
without the hasNext()/next() issue, would be to have iteration functionality in 
an expression so that there was no hasNext()/next() pairing. But rather put the 
collection in control of performing a call out to the "task" for the next 
element in the collection.  As in something like

public interface IterationPoint<T> {
	public void iterateFor( T value );
}

public interface IterationProvider<T> {
	public int iterateOver( IterationPoint<T> point );
}

This would then allow complete freedom from hasNext()/next() pairing because 
only if the collection found another value, would it need to reveal it to the 
IterationPoint.

Lambdas would make this easy, but even with inner classes, it's still usable.

Gregg Wonderly


From crazybob at crazybob.org  Fri Oct  7 16:52:02 2011
From: crazybob at crazybob.org (Bob Lee)
Date: Fri, 7 Oct 2011 13:52:02 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E8C5775.8090805@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu> <4E8C5775.8090805@cs.oswego.edu>
Message-ID: <CAGmsiP7_6x2SdiWAUBtstz4iTVJUVxPY6eDPyaXftOdtzry7rQ@mail.gmail.com>

On Wed, Oct 5, 2011 at 6:11 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> To me, fixing the mis-feature is more important
> than retaining pure compatibility for a usage
> that probably didn't do what people expected anyway.
> But if you've ever done anything relying on the old j.u.c
> behavior, could you let us know? Thanks!


Is it correct to assume that "fixing" CHM.entrySet().toArray() requires more
work on our part? If so, it's probably not worth the effort

If we assume the user wants a snapshot of the entry set, here's the code
using toArray():

    Set<Map.Entry<K, V>> original = map.entrySet();
    // This array isn't used outside of this code, so we can ignore the
warning.
    @SuppressWarnings("unchecked")
    Map.Entry<K, V>[] array = original.toArray(
        (Map.Entry<K, V>[]) new Map.Entry[0]);
    // We erroneously get an "unchecked generic array creation" warning
here.
    @SuppressWarnings("unchecked")
    Set<Map.Entry<K, V>> copy = new HashSet<Map.Entry<K, V>>(
        Arrays.asList(array));

Yuck. It requires two unsafe operations. We actually shouldn't get a warning
in the second case, and it'll go away either way with Java 7, but I do get a
warning with my current setup. Copying entries from a concurrent map to an
exact-length array is tricky and inefficient. The user can call size() and
use it to create the passed-in array, but the array size could change. It
can also change while copying the entries in which case toArray() may have
to create a copy. If the size goes down, the array needs to be truncated. If
the size goes up, any remaining entries are ignored.

It's simpler, safer, and more efficient for users to copy the entries
manually:

    Set<Map.Entry<K, V>> copy = new HashSet<Map.Entry<K, V>>(map.size());
    for (Map.Entry<K, V> entry : map.entrySet()) {
      copy.add(new AbstractMap.SimpleEntry<K, V>(entry));
    }

Thanks,
Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111007/aad6db23/attachment.html>

From cheremin at gmail.com  Fri Oct  7 17:44:52 2011
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Sat, 8 Oct 2011 01:44:52 +0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
Message-ID: <CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>

> It also needs a LoadStore fence, in both cases.

But why lazySet needs LoadStore fence? It seems what lazySet javadoc
does not put any ordering constraints on loads...

>?In particular, if v is volatile (and certainly if it's accessed using lazySet), and x and y are ordinary variables,
> then the assignments to x and y in the following may be visibly reordered:
> x = 1;
> v = 2;
> y = 3;

You mean what vstore is not "transparent" upside down, but
"transparent" downside up, so this

y=3
x=1
v=2

is allowed reordering?


From hans.boehm at hp.com  Fri Oct  7 19:22:33 2011
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 7 Oct 2011 23:22:33 +0000
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
 reasoning
In-Reply-To: <CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>

> From: Ruslan Cheremin [mailto:cheremin at gmail.com]
> > It also needs a LoadStore fence, in both cases.
> 
> But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> does not put any ordering constraints on loads...
I do read it as imposing such a constraint, though we all agree that a more precise spec would help.  Certainly C++11's memory_order_release imposes such a constraint.

If not, it would mean that e.g.

Thread 1:
x = ...;
...
r1 = x;
done_with_x.lazySet(true);

Thread 2:
if (done_with_x.get()) {
   x = ...;
   ...
   r2 = x;
}

wouldn't work as expected.

In my opinion, that's an indefensible design point, especially since I don't believe it makes lazySet appreciably cheaper on any modern architectures.


> 
> >?In particular, if v is volatile (and certainly if it's accessed using
> lazySet), and x and y are ordinary variables,
> > then the assignments to x and y in the following may be visibly
> reordered:
> > x = 1;
> > v = 2;
> > y = 3;
> 
> You mean what vstore is not "transparent" upside down, but
> "transparent" downside up, so this
> 
> y=3
> x=1
> v=2
> 
> is allowed reordering?
Correct.

Hans


From vitalyd at gmail.com  Fri Oct  7 20:09:45 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 7 Oct 2011 20:09:45 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
Message-ID: <CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>

Does it even make sense to say that lazySet needs a LoadStore fence? The
get() does but that's because it has same semantics as volatile read.
On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

> > From: Ruslan Cheremin [mailto:cheremin at gmail.com]
> > > It also needs a LoadStore fence, in both cases.
> >
> > But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> > does not put any ordering constraints on loads...
> I do read it as imposing such a constraint, though we all agree that a more
> precise spec would help.  Certainly C++11's memory_order_release imposes
> such a constraint.
>
> If not, it would mean that e.g.
>
> Thread 1:
> x = ...;
> ...
> r1 = x;
> done_with_x.lazySet(true);
>
> Thread 2:
> if (done_with_x.get()) {
>   x = ...;
>   ...
>   r2 = x;
> }
>
> wouldn't work as expected.
>
> In my opinion, that's an indefensible design point, especially since I
> don't believe it makes lazySet appreciably cheaper on any modern
> architectures.
>
>
> >
> > > In particular, if v is volatile (and certainly if it's accessed using
> > lazySet), and x and y are ordinary variables,
> > > then the assignments to x and y in the following may be visibly
> > reordered:
> > > x = 1;
> > > v = 2;
> > > y = 3;
> >
> > You mean what vstore is not "transparent" upside down, but
> > "transparent" downside up, so this
> >
> > y=3
> > x=1
> > v=2
> >
> > is allowed reordering?
> Correct.
>
> Hans
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111007/d6bd4ed5/attachment-0001.html>

From hans.boehm at hp.com  Sat Oct  8 00:06:08 2011
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sat, 8 Oct 2011 04:06:08 +0000
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
 reasoning
In-Reply-To: <CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>

LazySet() needs to prevent reordering of ordinary memory operations with a subsequent lazySet() operation.  In the JSR 133 Cookbook style, that can be implemented with a LoadStore | StoreStore fence preceding the lazySet() call.  So yes, that makes sense.

Real machines tend to require neither of those fences (x86) or combine them into a single instruction.

Hans

From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
Sent: Friday, October 07, 2011 5:10 PM
To: Boehm, Hans
Cc: concurrency-interest at cs.oswego.edu; Ruslan Cheremin
Subject: Re: [concurrency-interest] AtomicXXX.lazySet and happens-before reasoning


Does it even make sense to say that lazySet needs a LoadStore fence? The get() does but that's because it has same semantics as volatile read.
On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:
> From: Ruslan Cheremin [mailto:cheremin at gmail.com<mailto:cheremin at gmail.com>]
> > It also needs a LoadStore fence, in both cases.
>
> But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> does not put any ordering constraints on loads...
I do read it as imposing such a constraint, though we all agree that a more precise spec would help.  Certainly C++11's memory_order_release imposes such a constraint.

If not, it would mean that e.g.

Thread 1:
x = ...;
...
r1 = x;
done_with_x.lazySet(true);

Thread 2:
if (done_with_x.get()) {
  x = ...;
  ...
  r2 = x;
}

wouldn't work as expected.

In my opinion, that's an indefensible design point, especially since I don't believe it makes lazySet appreciably cheaper on any modern architectures.


>
> > In particular, if v is volatile (and certainly if it's accessed using
> lazySet), and x and y are ordinary variables,
> > then the assignments to x and y in the following may be visibly
> reordered:
> > x = 1;
> > v = 2;
> > y = 3;
>
> You mean what vstore is not "transparent" upside down, but
> "transparent" downside up, so this
>
> y=3
> x=1
> v=2
>
> is allowed reordering?
Correct.

Hans

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111008/bcf9a553/attachment.html>

From dl at cs.oswego.edu  Sat Oct  8 06:30:17 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 08 Oct 2011 06:30:17 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CAGmsiP7_6x2SdiWAUBtstz4iTVJUVxPY6eDPyaXftOdtzry7rQ@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu> <4E8C5775.8090805@cs.oswego.edu>
	<CAGmsiP7_6x2SdiWAUBtstz4iTVJUVxPY6eDPyaXftOdtzry7rQ@mail.gmail.com>
Message-ID: <4E902639.1070800@cs.oswego.edu>

On 10/07/11 16:52, Bob Lee wrote:
> On Wed, Oct 5, 2011 at 6:11 AM, Doug Lea <dl at cs.oswego.edu
> <mailto:dl at cs.oswego.edu>> wrote:
>
>     To me, fixing the mis-feature is more important
>     than retaining pure compatibility for a usage
>     that probably didn't do what people expected anyway.
>     But if you've ever done anything relying on the old j.u.c
>     behavior, could you let us know? Thanks!
>
>
> Is it correct to assume that "fixing" CHM.entrySet().toArray() requires more
> work on our part?

I'm not sure what you mean. The fix in V8 is to use non-write-through
Entry objects in entrySet.toArray to avoid unwanted side effects in
array[index].setValue().

The problematic use case is very rare, if it even exists.
In a google code search for the combination on ConcurrentHashMap,
entrySet, toArray, and setValue, I didn't see any. (Although
there are other possible ways to this encounter it.)
But because it is rare, it is a good idea to avoid surprises
when people do encounter it.

If you'd like to avoid the write-through side-effects without this
fix in existing CHM, you'd need to copy the elements of
entrySet().toArray() one by one into AbstractMap.SimpleEntry
(or some similar class) objects.

-Doug

From peter.firmstone at zeus.net.au  Sat Oct  8 06:26:10 2011
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Sat, 08 Oct 2011 20:26:10 +1000
Subject: [concurrency-interest] Subject: Re:  ConcurrentHashMapV8
In-Reply-To: <mailman.43.1318032593.6569.concurrency-interest@cs.oswego.edu>
References: <mailman.43.1318032593.6569.concurrency-interest@cs.oswego.edu>
Message-ID: <1318069569.1648.24.camel@bluto>

With Reference to Iterators, why not do something like this:

interface AtomicIterator<T> extends Iterator<T>{

    T getIfHasNext();
    
}


Example use (ignore the unchecked cast please, just to keep it simple):

AtomicIterator<T> iter = (AtomicIterator) keys.iterator<T>();

T t = iter.getIfHasNext();
while ( t != null ) {
  // Do something with t;
  t = iter.getIfHasNext();
}

Then you could back your AtomicIterator with a queue, every time there's
a mutation, add it to the end of the queue, or remove it.

Just a thought.

Cheers,

Peter.


On Sat, 2011-10-08 at 10:09, concurrency-interest-request at cs.oswego.edu
wrote:
> Send Concurrency-interest mailing list submissions to
> 	concurrency-interest at cs.oswego.edu
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
> 	concurrency-interest-request at cs.oswego.edu
> 
> You can reach the person managing the list at
> 	concurrency-interest-owner at cs.oswego.edu
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
> 
> 
> Today's Topics:
> 
>    1. Re: AtomicXXX.lazySet and	happens-before	reasoning (Boehm, Hans)
>    2. Re: ConcurrentHashMapV8 (Gregg Wonderly)
>    3. Re: ConcurrentHashMapV8 (Bob Lee)
>    4. Re: AtomicXXX.lazySet and happens-before	reasoning
>       (Ruslan Cheremin)
>    5. Re: AtomicXXX.lazySet and happens-before reasoning (Boehm, Hans)
>    6. Re: AtomicXXX.lazySet and happens-before	reasoning
>       (Vitaly Davidovich)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Fri, 7 Oct 2011 18:02:19 +0000
> From: "Boehm, Hans" <hans.boehm at hp.com>
> To: Doug Lea <dl at cs.oswego.edu>, Ruslan Cheremin <cheremin at gmail.com>
> Cc: "concurrency-interest at cs.oswego.edu"
> 	<concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] AtomicXXX.lazySet and
> 	happens-before	reasoning
> Message-ID:
> 	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB at G4W3299.americas.hpqcorp.net>
> Content-Type: text/plain; charset="us-ascii"
> 
> > From: Doug Lea
> > 
> > On 10/07/11 09:24, Ruslan Cheremin wrote:
> > > After some thinking and reading I still do not understand some
> > issues...
> > >
> > > As far, as I can see from description of lazySet, it is just ordinary
> > > store with StoreStore barrier just before it. But if it is so, what
> > is
> > > the difference between it and ordinary volatile write? In JSR-133
> > > implementation cookbook http://gee.cs.oswego.edu/dl/jmm/cookbook.html
> > > you've shown volatile store implementation as store having exactly
> > > StoreStore barrier before it.
> It also needs a LoadStore fence, in both cases.  But even then, it's important to remember that although his may be a sufficient implementation, this is an incorrect description from the user's perspective.  In particular, if v is volatile (and certainly if it's accessed using lazySet), and x and y are ordinary variables, then the assignments to x and y in the following may be visibly reordered:
> 
> x = 1;
> v = 2;
> y = 3;
> 
> Volatiles are not fences.
> 
> > 
> > Plus, for a volatile, a StoreLoad fence between the write and any read.
> > Almost always, the only good choice for where to place it is
> > immediately after the write. In addition to disabling
> > more optimizations, StoreLoad fences translate to
> > instructions that are not cheap on any platform, although they are
> > currently a lot cheaper than they were about 5 years ago on most
> > platforms. But in any case, if you have a situation that is
> > guaranteed not to need one to preserve correctness, it is always
> > faster not to require one.
> > 
> And on something like PowerPC, the implementation rules are actually currently quite unclear.  The currently preferred implementation, at least for the C++ equivalents, actually associates much of the volatile overhead with loads, though it's not completely clear that's the right choice.  Just adding a StoreLoad fence to stores isn't sufficient, for fairly complex reasons related to the fact that this view of fences is too simplistic for PowerPC.  If Java implementations follow the recommendations in http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html, then lazySet saves a relatively small percentage of the overhead, and the change amounts to weakening the fence before the store, not dropping one after it.  But that's because the StoreLoad fence is associated with the load, for which Java doesn't have a weaker form.  And having Java follow a different recipe from the C++ one is probably also a bad idea, since it essentially breaks mixed applications.
> 
> Hans
> 
> 
> 
> ------------------------------
> 
> Message: 2
> Date: Fri, 07 Oct 2011 14:57:58 -0500
> From: Gregg Wonderly <gregg at cytetech.com>
> To: Mark Thornton <mthornton at optrak.com>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] ConcurrentHashMapV8
> Message-ID: <4E8F59C6.4070701 at cytetech.com>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> On 10/7/2011 9:45 AM, Mark Thornton wrote:
> > On 07/10/11 15:32, David M. Lloyd wrote:
> >> Yeah, but I think that's not outside of what should be reasonably expected
> >> when you iterate a (non-COW) concurrent hash map. As you say, hasNext()/next()
> >> can be made consistent by actually getting a snapshot ref to the next entry in
> >> hasNext(), but it doesn't have to be a copy of the Entry, it can still be the
> >> literal Entry in the map.
> > I have several nonconcurrent map implementations which do not have Entry
> > instances internally --- they are created on the fly as needed to satisfy the
> > entrySet requirements. I usually do this where I can't afford the space required
> > by literal Entry instances.
> 
> It is important for an iterator to snapshot if you can ask it for the next 
> entry.  What would be better, to avoid copying and to manage concurrency easier, 
> without the hasNext()/next() issue, would be to have iteration functionality in 
> an expression so that there was no hasNext()/next() pairing. But rather put the 
> collection in control of performing a call out to the "task" for the next 
> element in the collection.  As in something like
> 
> public interface IterationPoint<T> {
> 	public void iterateFor( T value );
> }
> 
> public interface IterationProvider<T> {
> 	public int iterateOver( IterationPoint<T> point );
> }
> 
> This would then allow complete freedom from hasNext()/next() pairing because 
> only if the collection found another value, would it need to reveal it to the 
> IterationPoint.
> 
> Lambdas would make this easy, but even with inner classes, it's still usable.
> 
> Gregg Wonderly
> 
> 
> 
> ------------------------------
> 
> Message: 3
> Date: Fri, 7 Oct 2011 13:52:02 -0700
> From: Bob Lee <crazybob at crazybob.org>
> To: Doug Lea <dl at cs.oswego.edu>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] ConcurrentHashMapV8
> Message-ID:
> 	<CAGmsiP7_6x2SdiWAUBtstz4iTVJUVxPY6eDPyaXftOdtzry7rQ at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
> 
> On Wed, Oct 5, 2011 at 6:11 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> 
> > To me, fixing the mis-feature is more important
> > than retaining pure compatibility for a usage
> > that probably didn't do what people expected anyway.
> > But if you've ever done anything relying on the old j.u.c
> > behavior, could you let us know? Thanks!
> 
> 
> Is it correct to assume that "fixing" CHM.entrySet().toArray() requires more
> work on our part? If so, it's probably not worth the effort
> 
> If we assume the user wants a snapshot of the entry set, here's the code
> using toArray():
> 
>     Set<Map.Entry<K, V>> original = map.entrySet();
>     // This array isn't used outside of this code, so we can ignore the
> warning.
>     @SuppressWarnings("unchecked")
>     Map.Entry<K, V>[] array = original.toArray(
>         (Map.Entry<K, V>[]) new Map.Entry[0]);
>     // We erroneously get an "unchecked generic array creation" warning
> here.
>     @SuppressWarnings("unchecked")
>     Set<Map.Entry<K, V>> copy = new HashSet<Map.Entry<K, V>>(
>         Arrays.asList(array));
> 
> Yuck. It requires two unsafe operations. We actually shouldn't get a warning
> in the second case, and it'll go away either way with Java 7, but I do get a
> warning with my current setup. Copying entries from a concurrent map to an
> exact-length array is tricky and inefficient. The user can call size() and
> use it to create the passed-in array, but the array size could change. It
> can also change while copying the entries in which case toArray() may have
> to create a copy. If the size goes down, the array needs to be truncated. If
> the size goes up, any remaining entries are ignored.
> 
> It's simpler, safer, and more efficient for users to copy the entries
> manually:
> 
>     Set<Map.Entry<K, V>> copy = new HashSet<Map.Entry<K, V>>(map.size());
>     for (Map.Entry<K, V> entry : map.entrySet()) {
>       copy.add(new AbstractMap.SimpleEntry<K, V>(entry));
>     }
> 
> Thanks,
> Bob
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111007/aad6db23/attachment-0001.html>
> 
> ------------------------------
> 
> Message: 4
> Date: Sat, 8 Oct 2011 01:44:52 +0400
> From: Ruslan Cheremin <cheremin at gmail.com>
> To: "Boehm, Hans" <hans.boehm at hp.com>
> Cc: "concurrency-interest at cs.oswego.edu"
> 	<concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] AtomicXXX.lazySet and
> 	happens-before	reasoning
> Message-ID:
> 	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> > It also needs a LoadStore fence, in both cases.
> 
> But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> does not put any ordering constraints on loads...
> 
> >?In particular, if v is volatile (and certainly if it's accessed using lazySet), and x and y are ordinary variables,
> > then the assignments to x and y in the following may be visibly reordered:
> > x = 1;
> > v = 2;
> > y = 3;
> 
> You mean what vstore is not "transparent" upside down, but
> "transparent" downside up, so this
> 
> y=3
> x=1
> v=2
> 
> is allowed reordering?
> 
> 
> 
> ------------------------------
> 
> Message: 5
> Date: Fri, 7 Oct 2011 23:22:33 +0000
> From: "Boehm, Hans" <hans.boehm at hp.com>
> To: Ruslan Cheremin <cheremin at gmail.com>
> Cc: "concurrency-interest at cs.oswego.edu"
> 	<concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] AtomicXXX.lazySet and
> 	happens-before reasoning
> Message-ID:
> 	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF at G4W3299.americas.hpqcorp.net>
> Content-Type: text/plain; charset="iso-8859-1"
> 
> > From: Ruslan Cheremin [mailto:cheremin at gmail.com]
> > > It also needs a LoadStore fence, in both cases.
> > 
> > But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> > does not put any ordering constraints on loads...
> I do read it as imposing such a constraint, though we all agree that a more precise spec would help.  Certainly C++11's memory_order_release imposes such a constraint.
> 
> If not, it would mean that e.g.
> 
> Thread 1:
> x = ...;
> ...
> r1 = x;
> done_with_x.lazySet(true);
> 
> Thread 2:
> if (done_with_x.get()) {
>    x = ...;
>    ...
>    r2 = x;
> }
> 
> wouldn't work as expected.
> 
> In my opinion, that's an indefensible design point, especially since I don't believe it makes lazySet appreciably cheaper on any modern architectures.
> 
> 
> > 
> > >?In particular, if v is volatile (and certainly if it's accessed using
> > lazySet), and x and y are ordinary variables,
> > > then the assignments to x and y in the following may be visibly
> > reordered:
> > > x = 1;
> > > v = 2;
> > > y = 3;
> > 
> > You mean what vstore is not "transparent" upside down, but
> > "transparent" downside up, so this
> > 
> > y=3
> > x=1
> > v=2
> > 
> > is allowed reordering?
> Correct.
> 
> Hans
> 
> 
> 
> ------------------------------
> 
> Message: 6
> Date: Fri, 7 Oct 2011 20:09:45 -0400
> From: Vitaly Davidovich <vitalyd at gmail.com>
> To: "Boehm, Hans" <hans.boehm at hp.com>
> Cc: "concurrency-interest at cs.oswego.edu"
> 	<concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] AtomicXXX.lazySet and
> 	happens-before	reasoning
> Message-ID:
> 	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
> 
> Does it even make sense to say that lazySet needs a LoadStore fence? The
> get() does but that's because it has same semantics as volatile read.
> On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:
> 
> > > From: Ruslan Cheremin [mailto:cheremin at gmail.com]
> > > > It also needs a LoadStore fence, in both cases.
> > >
> > > But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> > > does not put any ordering constraints on loads...
> > I do read it as imposing such a constraint, though we all agree that a more
> > precise spec would help.  Certainly C++11's memory_order_release imposes
> > such a constraint.
> >
> > If not, it would mean that e.g.
> >
> > Thread 1:
> > x = ...;
> > ...
> > r1 = x;
> > done_with_x.lazySet(true);
> >
> > Thread 2:
> > if (done_with_x.get()) {
> >   x = ...;
> >   ...
> >   r2 = x;
> > }
> >
> > wouldn't work as expected.
> >
> > In my opinion, that's an indefensible design point, especially since I
> > don't believe it makes lazySet appreciably cheaper on any modern
> > architectures.
> >
> >
> > >
> > > > In particular, if v is volatile (and certainly if it's accessed using
> > > lazySet), and x and y are ordinary variables,
> > > > then the assignments to x and y in the following may be visibly
> > > reordered:
> > > > x = 1;
> > > > v = 2;
> > > > y = 3;
> > >
> > > You mean what vstore is not "transparent" upside down, but
> > > "transparent" downside up, so this
> > >
> > > y=3
> > > x=1
> > > v=2
> > >
> > > is allowed reordering?
> > Correct.
> >
> > Hans
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111007/d6bd4ed5/attachment.html>
> 
> ------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> End of Concurrency-interest Digest, Vol 81, Issue 11
> ****************************************************


From dl at cs.oswego.edu  Sat Oct  8 08:02:18 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 08 Oct 2011 08:02:18 -0400
Subject: [concurrency-interest] Subject: Re:  ConcurrentHashMapV8
In-Reply-To: <1318069569.1648.24.camel@bluto>
References: <mailman.43.1318032593.6569.concurrency-interest@cs.oswego.edu>
	<1318069569.1648.24.camel@bluto>
Message-ID: <4E903BCA.7090104@cs.oswego.edu>

On 10/08/11 06:26, Peter Firmstone wrote:
> With Reference to Iterators, why not do something like this:
>
> interface AtomicIterator<T>  extends Iterator<T>{
>      T getIfHasNext();
> }

Back in pre-JSR166 days, I considered uniformly using
a stream-like interface style along these lines for concurrent
collections instead of Iterator, to avoid the hasNext/next
non-atomicity problem in a classic way similar to IO and
channel APIs.  Recasting without the Iterator connection:

interface ElementStream<T> {
   T next(); // return next element, or null if there aren't any
}

This works in part because nulls are generally disallowed
in concurrent collections. (An ElementStream for an
array-like collection like CopyOnWriteArrayList that
can legitimately contain null gaps would need
to internally skip over them when presenting next().)

It also meshes nicely with the various Queue APIs -- Queue.poll
is a "mutative" version of next() that also removes the element.

At the time of JDK5/JSR166, it was more important to integrate
concurrent collections with plain java.util collections, so
we didn't pursue this. And even though signature-wise, you
could claim that Iterator extends ElementStream, the
null-means-exhausted rule makes them incompatible.

But with the advent of lambdas and bulk parallel operations,
it might be worth considering this as a way of supporting
stream-style processing in addition to forEach-style processing.
Which would amount to giving people two choices for how they'd
like to replace all their sequential iterator code. Or,
to allow gradual adoption at the expense of messiness,
supporting all three styles.

-Doug


From david.lloyd at redhat.com  Sat Oct  8 10:40:13 2011
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Sat, 08 Oct 2011 09:40:13 -0500
Subject: [concurrency-interest] Subject: Re:  ConcurrentHashMapV8
In-Reply-To: <4E903BCA.7090104@cs.oswego.edu>
References: <mailman.43.1318032593.6569.concurrency-interest@cs.oswego.edu>
	<1318069569.1648.24.camel@bluto> <4E903BCA.7090104@cs.oswego.edu>
Message-ID: <4E9060CD.6080002@redhat.com>

On 10/08/2011 07:02 AM, Doug Lea wrote:
> On 10/08/11 06:26, Peter Firmstone wrote:
>> With Reference to Iterators, why not do something like this:
>>
>> interface AtomicIterator<T> extends Iterator<T>{
>> T getIfHasNext();
>> }
>
> Back in pre-JSR166 days, I considered uniformly using
> a stream-like interface style along these lines for concurrent
> collections instead of Iterator, to avoid the hasNext/next
> non-atomicity problem in a classic way similar to IO and
> channel APIs. Recasting without the Iterator connection:
>
> interface ElementStream<T> {
> T next(); // return next element, or null if there aren't any
> }
>
> This works in part because nulls are generally disallowed
> in concurrent collections. (An ElementStream for an
> array-like collection like CopyOnWriteArrayList that
> can legitimately contain null gaps would need
> to internally skip over them when presenting next().)
>
> It also meshes nicely with the various Queue APIs -- Queue.poll
> is a "mutative" version of next() that also removes the element.

Of the two issues, I don't really find this one to be all that severe 
(it's easy enough to work around in most cases) - which is a good thing, 
since even if a superior alternative were implemented today, I imagine 
it would be many, many years before classical Iterators fell out of 
common usage.  In any case, it seems certain that lambdas will sand off 
this rough edge.

-- 
- DML

From crazybob at crazybob.org  Sat Oct  8 11:33:07 2011
From: crazybob at crazybob.org (Bob Lee)
Date: Sat, 8 Oct 2011 08:33:07 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E902639.1070800@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu> <4E68AAC9.9070800@cs.oswego.edu>
	<4E85F532.1040808@redhat.com> <4E88E5D3.6070502@cs.oswego.edu>
	<CANPzfU8MaAyA5rB+XDgCfiWLOZK1hBhkA0REjMqDbrVF7GXOvQ@mail.gmail.com>
	<4E89BE04.7060907@cs.oswego.edu> <4E8C5775.8090805@cs.oswego.edu>
	<CAGmsiP7_6x2SdiWAUBtstz4iTVJUVxPY6eDPyaXftOdtzry7rQ@mail.gmail.com>
	<4E902639.1070800@cs.oswego.edu>
Message-ID: <CAGmsiP6wHnhka8Fv2CVkL6hV5Ac_hZvA+acbmSF1_12ryK0TVw@mail.gmail.com>

On Sat, Oct 8, 2011 at 3:30 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> I'm not sure what you mean. The fix in V8 is to use non-write-through
> Entry objects in entrySet.toArray to avoid unwanted side effects in
> array[index].setValue().
>

If the user wants entries where setValue() doesn't modify the original map,
it's simpler, safer, and more efficient for the user to copy the entries
manually. It's also compatible across implementations. We needn't waste our
time and bytes making EntrySet.toArray() do something "sensible" because
users shouldn't use it anyway. Generics and arrays don't mix.

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111008/f8911e21/attachment.html>

From vitalyd at gmail.com  Sat Oct  8 13:11:29 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 8 Oct 2011 13:11:29 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
Message-ID: <CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>

+ rest of the group

On Sat, Oct 8, 2011 at 1:10 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Hi Hans,
>
> I was under the impression that lazySet is purely a StoreStore barrier, and
> only specifies that the lazySet cannot be reordered with prior writes -- I
> never saw mention of requiring no reordering with prior loads.  Here's
> Doug's evaluation:
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6275329, where only a
> store-store is mentioned.  If it's really a LoadStore | StoreStore, good to
> know ...
>
> Thanks
>
> On Sat, Oct 8, 2011 at 12:06 AM, Boehm, Hans <hans.boehm at hp.com> wrote:
>
>>  LazySet() needs to prevent reordering of ordinary memory operations with
>> a subsequent lazySet() operation.  In the JSR 133 Cookbook style, that can
>> be implemented with a LoadStore | StoreStore fence preceding the lazySet()
>> call.  So yes, that makes sense.****
>>
>> ** **
>>
>> Real machines tend to require neither of those fences (x86) or combine
>> them into a single instruction.****
>>
>> ** **
>>
>> Hans****
>>
>> ** **
>>
>> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
>> *Sent:* Friday, October 07, 2011 5:10 PM
>> *To:* Boehm, Hans
>> *Cc:* concurrency-interest at cs.oswego.edu; Ruslan Cheremin
>>
>> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and
>> happens-before reasoning****
>>
>>  ** **
>>
>> Does it even make sense to say that lazySet needs a LoadStore fence? The
>> get() does but that's because it has same semantics as volatile read. ***
>> *
>>
>> On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:****
>>
>> > From: Ruslan Cheremin [mailto:cheremin at gmail.com]
>> > > It also needs a LoadStore fence, in both cases.
>> >
>> > But why lazySet needs LoadStore fence? It seems what lazySet javadoc
>> > does not put any ordering constraints on loads...
>> I do read it as imposing such a constraint, though we all agree that a
>> more precise spec would help.  Certainly C++11's memory_order_release
>> imposes such a constraint.
>>
>> If not, it would mean that e.g.
>>
>> Thread 1:
>> x = ...;
>> ...
>> r1 = x;
>> done_with_x.lazySet(true);
>>
>> Thread 2:
>> if (done_with_x.get()) {
>>   x = ...;
>>   ...
>>   r2 = x;
>> }
>>
>> wouldn't work as expected.
>>
>> In my opinion, that's an indefensible design point, especially since I
>> don't believe it makes lazySet appreciably cheaper on any modern
>> architectures.
>>
>>
>> >
>> > > In particular, if v is volatile (and certainly if it's accessed using
>> > lazySet), and x and y are ordinary variables,
>> > > then the assignments to x and y in the following may be visibly
>> > reordered:
>> > > x = 1;
>> > > v = 2;
>> > > y = 3;
>> >
>> > You mean what vstore is not "transparent" upside down, but
>> > "transparent" downside up, so this
>> >
>> > y=3
>> > x=1
>> > v=2
>> >
>> > is allowed reordering?
>> Correct.
>>
>> Hans
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>>
>
>
>
> --
> Vitaly
> 617-548-7007 (mobile)
>



-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111008/9d83a775/attachment.html>

From hans.boehm at hp.com  Sat Oct  8 20:10:00 2011
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sun, 9 Oct 2011 00:10:00 +0000
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
 reasoning
In-Reply-To: <CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>

The specification states

*         lazySet has the memory effects of writing (assigning) a volatile variable except that it permits reorderings with subsequent (but not previous) memory actions that do not themselves impose reordering constraints with ordinary non-volatile writes. Among other usage contexts, lazySet may apply when nulling out, for the sake of garbage collection, a reference that is never accessed again.
in the java.util.concurrent description, which implies that it may not be reordered with previous "memory actions", not just stores.  Doug can comment more authoritatively on the intent, but that specification seems fairly unambiguous in this particular respect.

Hans



From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
Sent: Saturday, October 08, 2011 10:11 AM
To: Boehm, Hans; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] AtomicXXX.lazySet and happens-before reasoning

+ rest of the group
On Sat, Oct 8, 2011 at 1:10 PM, Vitaly Davidovich <vitalyd at gmail.com<mailto:vitalyd at gmail.com>> wrote:
Hi Hans,

I was under the impression that lazySet is purely a StoreStore barrier, and only specifies that the lazySet cannot be reordered with prior writes -- I never saw mention of requiring no reordering with prior loads.  Here's Doug's evaluation: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6275329, where only a store-store is mentioned.  If it's really a LoadStore | StoreStore, good to know ...

Thanks

On Sat, Oct 8, 2011 at 12:06 AM, Boehm, Hans <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:
LazySet() needs to prevent reordering of ordinary memory operations with a subsequent lazySet() operation.  In the JSR 133 Cookbook style, that can be implemented with a LoadStore | StoreStore fence preceding the lazySet() call.  So yes, that makes sense.

Real machines tend to require neither of those fences (x86) or combine them into a single instruction.

Hans

From: Vitaly Davidovich [mailto:vitalyd at gmail.com<mailto:vitalyd at gmail.com>]
Sent: Friday, October 07, 2011 5:10 PM
To: Boehm, Hans
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; Ruslan Cheremin

Subject: Re: [concurrency-interest] AtomicXXX.lazySet and happens-before reasoning


Does it even make sense to say that lazySet needs a LoadStore fence? The get() does but that's because it has same semantics as volatile read.
On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:
> From: Ruslan Cheremin [mailto:cheremin at gmail.com<mailto:cheremin at gmail.com>]
> > It also needs a LoadStore fence, in both cases.
>
> But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> does not put any ordering constraints on loads...
I do read it as imposing such a constraint, though we all agree that a more precise spec would help.  Certainly C++11's memory_order_release imposes such a constraint.

If not, it would mean that e.g.

Thread 1:
x = ...;
...
r1 = x;
done_with_x.lazySet(true);

Thread 2:
if (done_with_x.get()) {
  x = ...;
  ...
  r2 = x;
}

wouldn't work as expected.

In my opinion, that's an indefensible design point, especially since I don't believe it makes lazySet appreciably cheaper on any modern architectures.


>
> > In particular, if v is volatile (and certainly if it's accessed using
> lazySet), and x and y are ordinary variables,
> > then the assignments to x and y in the following may be visibly
> reordered:
> > x = 1;
> > v = 2;
> > y = 3;
>
> You mean what vstore is not "transparent" upside down, but
> "transparent" downside up, so this
>
> y=3
> x=1
> v=2
>
> is allowed reordering?
Correct.

Hans

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



--
Vitaly
617-548-7007<tel:617-548-7007> (mobile)



--
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111009/9b46f820/attachment-0001.html>

From peter.firmstone at zeus.net.au  Sun Oct  9 08:20:07 2011
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Sun, 09 Oct 2011 22:20:07 +1000
Subject: [concurrency-interest] Subject: Re:  ConcurrentHashMapV8
In-Reply-To: <mailman.1.1318089601.22085.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1318089601.22085.concurrency-interest@cs.oswego.edu>
Message-ID: <1318162806.1648.37.camel@bluto>

Thanks Doug, I don't know much about lambdas, I'm confident you'll
figure out the right solution.

The following is another similar interface which permits early
termination.

Regards,

Peter.

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership. The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.river.api.util;

import java.io.IOException;

/**
 * This interface is similar to an Enumerator, it is designed to return
 * results incrementally in loops, however unlike an Enumerator, there
is no
 * check first operation as implementors must return a null value after
 * the backing data source has been exhausted. So this terminates like a
stream
 * by returning a null value.
 * 
 * @author Peter Firmstone
 */
public interface ResultStream<T> {
    /**
     * Get next T, call from a loop until T is null;
     * @return T unless end of stream in which case null is returned.
     */
    public T get() throws IOException;
    /**
     * Close the result stream, this allows the implementer to close any
     * resources prior to deleting reference.
     */
    public void close() throws IOException;
}


> Message: 1
> Date: Sat, 08 Oct 2011 08:02:18 -0400
> From: Doug Lea <dl at cs.oswego.edu>
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Subject: Re:  ConcurrentHashMapV8
> Message-ID: <4E903BCA.7090104 at cs.oswego.edu>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> On 10/08/11 06:26, Peter Firmstone wrote:
> > With Reference to Iterators, why not do something like this:
> >
> > interface AtomicIterator<T>  extends Iterator<T>{
> >      T getIfHasNext();
> > }
> 
> Back in pre-JSR166 days, I considered uniformly using
> a stream-like interface style along these lines for concurrent
> collections instead of Iterator, to avoid the hasNext/next
> non-atomicity problem in a classic way similar to IO and
> channel APIs.  Recasting without the Iterator connection:
> 
> interface ElementStream<T> {
>    T next(); // return next element, or null if there aren't any
> }
> 
> This works in part because nulls are generally disallowed
> in concurrent collections. (An ElementStream for an
> array-like collection like CopyOnWriteArrayList that
> can legitimately contain null gaps would need
> to internally skip over them when presenting next().)
> 
> It also meshes nicely with the various Queue APIs -- Queue.poll
> is a "mutative" version of next() that also removes the element.
> 
> At the time of JDK5/JSR166, it was more important to integrate
> concurrent collections with plain java.util collections, so
> we didn't pursue this. And even though signature-wise, you
> could claim that Iterator extends ElementStream, the
> null-means-exhausted rule makes them incompatible.
> 
> But with the advent of lambdas and bulk parallel operations,
> it might be worth considering this as a way of supporting
> stream-style processing in addition to forEach-style processing.
> Which would amount to giving people two choices for how they'd
> like to replace all their sequential iterator code. Or,
> to allow gradual adoption at the expense of messiness,
> supporting all three styles.
> 
> -Doug
> 
> 
> 



From jwesleysmith at atlassian.com  Sun Oct  9 17:32:50 2011
From: jwesleysmith at atlassian.com (Jed Wesley-Smith)
Date: Mon, 10 Oct 2011 08:32:50 +1100
Subject: [concurrency-interest] Subject: Re: ConcurrentHashMapV8
In-Reply-To: <4E903BCA.7090104@cs.oswego.edu>
References: <mailman.43.1318032593.6569.concurrency-interest@cs.oswego.edu>
	<1318069569.1648.24.camel@bluto> <4E903BCA.7090104@cs.oswego.edu>
Message-ID: <CAKh+yi9Pe0=pAJ9D-B78SuqLc_cKG94+Xi=r6Rh5RZbQ7QfROw@mail.gmail.com>

The functional/persistent Stream interface is a great alternative that
doesn't rely on null being magic:

Stream<T> {
  T get(); // throws if empty aka head()
  Stream<T> next(); // aka tail()
  boolean isEmpty();
}

implementations can be strict or lazy, but each actual instance is
referentially transparent.

this interface works well in conjunction with Option/Some (or Guava's
Optional).

On Sat, Oct 8, 2011 at 11:02 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 10/08/11 06:26, Peter Firmstone wrote:
>
>> With Reference to Iterators, why not do something like this:
>>
>> interface AtomicIterator<T>  extends Iterator<T>{
>>     T getIfHasNext();
>> }
>>
>
> Back in pre-JSR166 days, I considered uniformly using
> a stream-like interface style along these lines for concurrent
> collections instead of Iterator, to avoid the hasNext/next
> non-atomicity problem in a classic way similar to IO and
> channel APIs.  Recasting without the Iterator connection:
>
> interface ElementStream<T> {
>  T next(); // return next element, or null if there aren't any
> }
>
> This works in part because nulls are generally disallowed
> in concurrent collections. (An ElementStream for an
> array-like collection like CopyOnWriteArrayList that
> can legitimately contain null gaps would need
> to internally skip over them when presenting next().)
>
> It also meshes nicely with the various Queue APIs -- Queue.poll
> is a "mutative" version of next() that also removes the element.
>
> At the time of JDK5/JSR166, it was more important to integrate
> concurrent collections with plain java.util collections, so
> we didn't pursue this. And even though signature-wise, you
> could claim that Iterator extends ElementStream, the
> null-means-exhausted rule makes them incompatible.
>
> But with the advent of lambdas and bulk parallel operations,
> it might be worth considering this as a way of supporting
> stream-style processing in addition to forEach-style processing.
> Which would amount to giving people two choices for how they'd
> like to replace all their sequential iterator code. Or,
> to allow gradual adoption at the expense of messiness,
> supporting all three styles.
>
> -Doug
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111010/30e0a251/attachment.html>

From cheremin at gmail.com  Sun Oct  9 18:03:53 2011
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Mon, 10 Oct 2011 02:03:53 +0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
Message-ID: <CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>

The farther, the better :)

Yes, from this atomic package javadoc specification it is clear that it is
not only StoreStore, but also LoadStore barriers before lazySet.

Interestingly that my meeting with lazySet started from informal definition
like "non-volatile write to volatile variable with few additional ordering
constraints", but for now it is "has the memory effects of writing
(assigning) a volatile variable except..."


That is not clear for me -- doesn't strict volatile store prevents any
reordering with subsequent memory actions? Huns, you've just present an
example of such allowed reordering.

It seems like it somekind related to StoreLoad fence which ordinary volatile
store must has after it (although I still do not understand why), but
lazySet omit?



> The specification states****
>
> ** **
>
> **?         **lazySet has the memory effects of writing (assigning) a
> volatile variable except that it permits reorderings with subsequent (but
> not previous) memory actions that do not themselves impose reordering
> constraints with ordinary non-volatile writes. Among other usage contexts,
> lazySet may apply when nulling out, for the sake of garbage collection, a
> reference that is never accessed again. ****
>
> in the java.util.concurrent description, which implies that it may not be
> reordered with previous ?memory actions?, not just stores.  Doug can comment
> more authoritatively on the intent, but that specification seems fairly
> unambiguous in this particular respect.****
>
> ** **
>
> Hans****
>
> ** **
>
> ** **
>
> ** **
>
> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com <javascript:_e({},
> 'cvml', 'vitalyd at gmail.com');>]
> *Sent:* Saturday, October 08, 2011 10:11 AM
> *To:* Boehm, Hans; concurrency-interest at cs.oswego.edu <javascript:_e({},
> 'cvml', 'concurrency-interest at cs.oswego.edu');>
> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and happens-before
> reasoning****
>
> ** **
>
> + rest of the group****
>
> On Sat, Oct 8, 2011 at 1:10 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:****
>
> Hi Hans,****
>
> ** **
>
> I was under the impression that lazySet is purely a StoreStore barrier, and
> only specifies that the lazySet cannot be reordered with prior writes -- I
> never saw mention of requiring no reordering with prior loads.  Here's
> Doug's evaluation:
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6275329, where only a
> store-store is mentioned.  If it's really a LoadStore | StoreStore, good to
> know ...****
>
> ** **
>
> Thanks****
>
> ** **
>
> On Sat, Oct 8, 2011 at 12:06 AM, Boehm, Hans <hans.boehm at hp.com> wrote:***
> *
>
> LazySet() needs to prevent reordering of ordinary memory operations with a
> subsequent lazySet() operation.  In the JSR 133 Cookbook style, that can be
> implemented with a LoadStore | StoreStore fence preceding the lazySet()
> call.  So yes, that makes sense.****
>
>  ****
>
> Real machines tend to require neither of those fences (x86) or combine them
> into a single instruction.****
>
>  ****
>
> Hans****
>
>  ****
>
> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
> *Sent:* Friday, October 07, 2011 5:10 PM
> *To:* Boehm, Hans
> *Cc:* concurrency-interest at cs.oswego.edu; Ruslan Cheremin****
>
>
> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and happens-before
> reasoning****
>
>  ****
>
> Does it even make sense to say that lazySet needs a LoadStore fence? The
> get() does but that's because it has same semantics as volatile read. ****
>
> On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:****
>
> > From: Ruslan Cheremin [mailto:cheremin at gmail.com]
> > > It also needs a LoadStore fence, in both cases.
> >
> > But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> > does not put any ordering constraints on loads...
> I do read it as imposing such a constraint, though we all agree that a more
> precise spec would help.  Certainly C++11's memory_order_release imposes
> such a constraint.
>
> If not, it would mean that e.g.
>
> Thread 1:
> x = ...;
> ...
> r1 = x;
> done_with_x.lazySet(true);
>
> Thread 2:
> if (done_with_x.get()) {
>   x = ...;
>   ...
>   r2 = x;
> }
>
> wouldn't work as expected.
>
> In my opinion, that's an indefensible design point, especially since I
> don't believe it makes lazySet appreciably cheaper on any modern
> architectures.
>
>
> >
> > > In particular, if v is volatile (and certainly if it's accessed using
> > lazySet), and x and y are ordinary variables,
> > > then the assignments to x and y in the following may be visibly
> > reordered:
> > > x = 1;
> > > v = 2;
> > > y = 3;
> >
> > You mean what vstore is not "transparent" upside down, but
> > "transparent" downside up, so this
> >
> > y=3
> > x=1
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111010/e2a450c9/attachment.html>

From vitalyd at gmail.com  Sun Oct  9 18:52:45 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sun, 9 Oct 2011 18:52:45 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
Message-ID: <CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>

>
> That is not clear for me -- doesn't strict volatile store prevents any
> reordering with subsequent memory actions? Huns, you've just present an
> example of such allowed reordering.


A volatile store does not prevent subsequent *non-volatile* store/loads from
reordering with it.  A StoreLoad is placed after the volatile store only if
followed by a volatile load of the same memory location that was stored
(this is to avoid the load from being satisfied out of the processor's write
buffer before the prior store is made globally visible).

Based on this discussion, it sounds like lazySet does not prevent subsequent
instructions from reordering with it.  Specifically, if a lazySet is
followed by a volatile load, no StoreLoad is issued in between and the
processor can fetch the data from the write buffer before other processors
see the write; this is the reason that lazySet will be much cheaper than
volatile write because it doesn't have to issue a fence instruction here and
wait for the store buffer to drain to the cache.  In my own observation,
lazySet translates to a regular MOV instruction on x86 (64) and a volatile
write issues a locked add instruction after the write (or mfence on older
Hotspot versions), which obtains the required serialization.


On Sun, Oct 9, 2011 at 6:03 PM, Ruslan Cheremin <cheremin at gmail.com> wrote:

>
> The farther, the better :)
>
> Yes, from this atomic package javadoc specification it is clear that it is
> not only StoreStore, but also LoadStore barriers before lazySet.
>
> Interestingly that my meeting with lazySet started from informal definition
> like "non-volatile write to volatile variable with few additional ordering
> constraints", but for now it is "has the memory effects of writing
> (assigning) a volatile variable except..."
>
>
> That is not clear for me -- doesn't strict volatile store prevents any
> reordering with subsequent memory actions? Huns, you've just present an
> example of such allowed reordering.
>
> It seems like it somekind related to StoreLoad fence which ordinary
> volatile store must has after it (although I still do not understand why),
> but lazySet omit?
>
>
>
>> The specification states****
>>
>> ** **
>>
>> **?         **lazySet has the memory effects of writing (assigning) a
>> volatile variable except that it permits reorderings with subsequent (but
>> not previous) memory actions that do not themselves impose reordering
>> constraints with ordinary non-volatile writes. Among other usage
>> contexts, lazySet may apply when nulling out, for the sake of garbage
>> collection, a reference that is never accessed again. ****
>>
>> in the java.util.concurrent description, which implies that it may not be
>> reordered with previous ?memory actions?, not just stores.  Doug can comment
>> more authoritatively on the intent, but that specification seems fairly
>> unambiguous in this particular respect.****
>>
>> ** **
>>
>> Hans****
>>
>> ** **
>>
>> ** **
>>
>> ** **
>>
>> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
>> *Sent:* Saturday, October 08, 2011 10:11 AM
>> *To:* Boehm, Hans; concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and
>> happens-before reasoning****
>>
>> ** **
>>
>> + rest of the group****
>>
>> On Sat, Oct 8, 2011 at 1:10 PM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:****
>>
>> Hi Hans,****
>>
>> ** **
>>
>> I was under the impression that lazySet is purely a StoreStore barrier,
>> and only specifies that the lazySet cannot be reordered with prior writes --
>> I never saw mention of requiring no reordering with prior loads.  Here's
>> Doug's evaluation:
>> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6275329, where only a
>> store-store is mentioned.  If it's really a LoadStore | StoreStore, good to
>> know ...****
>>
>> ** **
>>
>> Thanks****
>>
>> ** **
>>
>> On Sat, Oct 8, 2011 at 12:06 AM, Boehm, Hans <hans.boehm at hp.com> wrote:**
>> **
>>
>> LazySet() needs to prevent reordering of ordinary memory operations with a
>> subsequent lazySet() operation.  In the JSR 133 Cookbook style, that can be
>> implemented with a LoadStore | StoreStore fence preceding the lazySet()
>> call.  So yes, that makes sense.****
>>
>>  ****
>>
>> Real machines tend to require neither of those fences (x86) or combine
>> them into a single instruction.****
>>
>>  ****
>>
>> Hans****
>>
>>  ****
>>
>> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
>> *Sent:* Friday, October 07, 2011 5:10 PM
>> *To:* Boehm, Hans
>> *Cc:* concurrency-interest at cs.oswego.edu; Ruslan Cheremin****
>>
>>
>> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and
>> happens-before reasoning****
>>
>>  ****
>>
>> Does it even make sense to say that lazySet needs a LoadStore fence? The
>> get() does but that's because it has same semantics as volatile read. ***
>> *
>>
>> On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:****
>>
>> > From: Ruslan Cheremin [mailto:cheremin at gmail.com]
>> > > It also needs a LoadStore fence, in both cases.
>> >
>> > But why lazySet needs LoadStore fence? It seems what lazySet javadoc
>> > does not put any ordering constraints on loads...
>> I do read it as imposing such a constraint, though we all agree that a
>> more precise spec would help.  Certainly C++11's memory_order_release
>> imposes such a constraint.
>>
>> If not, it would mean that e.g.
>>
>> Thread 1:
>> x = ...;
>> ...
>> r1 = x;
>> done_with_x.lazySet(true);
>>
>> Thread 2:
>> if (done_with_x.get()) {
>>   x = ...;
>>   ...
>>   r2 = x;
>> }
>>
>> wouldn't work as expected.
>>
>> In my opinion, that's an indefensible design point, especially since I
>> don't believe it makes lazySet appreciably cheaper on any modern
>> architectures.
>>
>>
>> >
>> > > In particular, if v is volatile (and certainly if it's accessed using
>> > lazySet), and x and y are ordinary variables,
>> > > then the assignments to x and y in the following may be visibly
>> > reordered:
>> > > x = 1;
>> > > v = 2;
>> > > y = 3;
>> >
>> > You mean what vstore is not "transparent" upside down, but
>> > "transparent" downside up, so this
>> >
>> > y=3
>> > x=1
>>
>


-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111009/718c0300/attachment-0001.html>

From hans.boehm at hp.com  Sun Oct  9 22:46:44 2011
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 10 Oct 2011 02:46:44 +0000
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>

I agree with most of this.  However, the reason for the StoreLoad fence following a volatile store is typically  to prevent reordering of the store with a subsequent volatile load of a DIFFERENT VOLATILE variable.   For example, consider the standard Dekker's algorithm example, where everything is initially zero:

Thread 1:
x = 1;
r1 = y;

Thread 2:
y = 1;
r2 = x;

If x and y are volatile, r1 = r2 = 0 is not allowed.  This means that the two accesses in each thread may not be reordered.  My understanding is that if lazySet is used here, r1 = r2 = 0 is allowed, and hence the trailing StoreLoad fence is not required.

A lot of the details here are architecture specific.  In particular, the PowerPC story is quite different and more complicated, since PowerPC doesn't by default guarantee that stores become visible on all other processors at the same time.  Fences are also required to effectively obtain that guarantee.  The difference between lazySet and a volatile store is probably larger on x86 than on most other architectures.

Hans

From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
Sent: Sunday, October 09, 2011 3:53 PM
To: Ruslan Cheremin
Cc: Boehm, Hans; concurrency-interest at cs.oswego.edu
Subject: Re: AtomicXXX.lazySet and happens-before reasoning

That is not clear for me -- doesn't strict volatile store prevents any reordering with subsequent memory actions? Huns, you've just present an example of such allowed reordering.

A volatile store does not prevent subsequent non-volatile store/loads from reordering with it.  A StoreLoad is placed after the volatile store only if followed by a volatile load of the same memory location that was stored (this is to avoid the load from being satisfied out of the processor's write buffer before the prior store is made globally visible).

Based on this discussion, it sounds like lazySet does not prevent subsequent instructions from reordering with it.  Specifically, if a lazySet is followed by a volatile load, no StoreLoad is issued in between and the processor can fetch the data from the write buffer before other processors see the write; this is the reason that lazySet will be much cheaper than volatile write because it doesn't have to issue a fence instruction here and wait for the store buffer to drain to the cache.  In my own observation, lazySet translates to a regular MOV instruction on x86 (64) and a volatile write issues a locked add instruction after the write (or mfence on older Hotspot versions), which obtains the required serialization.


On Sun, Oct 9, 2011 at 6:03 PM, Ruslan Cheremin <cheremin at gmail.com<mailto:cheremin at gmail.com>> wrote:

The farther, the better :)

Yes, from this atomic package javadoc specification it is clear that it is not only StoreStore, but also LoadStore barriers before lazySet.

Interestingly that my meeting with lazySet started from informal definition like "non-volatile write to volatile variable with few additional ordering constraints", but for now it is "has the memory effects of writing (assigning) a volatile variable except..."


That is not clear for me -- doesn't strict volatile store prevents any reordering with subsequent memory actions? Huns, you've just present an example of such allowed reordering.

It seems like it somekind related to StoreLoad fence which ordinary volatile store must has after it (although I still do not understand why), but lazySet omit?


The specification states

*         lazySet has the memory effects of writing (assigning) a volatile variable except that it permits reorderings with subsequent (but not previous) memory actions that do not themselves impose reordering constraints with ordinary non-volatile writes. Among other usage contexts, lazySet may apply when nulling out, for the sake of garbage collection, a reference that is never accessed again.
in the java.util.concurrent description, which implies that it may not be reordered with previous "memory actions", not just stores.  Doug can comment more authoritatively on the intent, but that specification seems fairly unambiguous in this particular respect.

Hans



From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
Sent: Saturday, October 08, 2011 10:11 AM
To: Boehm, Hans; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] AtomicXXX.lazySet and happens-before reasoning



+ rest of the group

On Sat, Oct 8, 2011 at 1:10 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:

Hi Hans,



I was under the impression that lazySet is purely a StoreStore barrier, and only specifies that the lazySet cannot be reordered with prior writes -- I never saw mention of requiring no reordering with prior loads.  Here's Doug's evaluation: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6275329, where only a store-store is mentioned.  If it's really a LoadStore | StoreStore, good to know ...



Thanks



On Sat, Oct 8, 2011 at 12:06 AM, Boehm, Hans <hans.boehm at hp.com> wrote:

LazySet() needs to prevent reordering of ordinary memory operations with a subsequent lazySet() operation.  In the JSR 133 Cookbook style, that can be implemented with a LoadStore | StoreStore fence preceding the lazySet() call.  So yes, that makes sense.



Real machines tend to require neither of those fences (x86) or combine them into a single instruction.



Hans



From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
Sent: Friday, October 07, 2011 5:10 PM
To: Boehm, Hans
Cc: concurrency-interest at cs.oswego.edu; Ruslan Cheremin

Subject: Re: [concurrency-interest] AtomicXXX.lazySet and happens-before reasoning



Does it even make sense to say that lazySet needs a LoadStore fence? The get() does but that's because it has same semantics as volatile read.

On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

> From: Ruslan Cheremin [mailto:cheremin at gmail.com]
> > It also needs a LoadStore fence, in both cases.
>
> But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> does not put any ordering constraints on loads...
I do read it as imposing such a constraint, though we all agree that a more precise spec would help.  Certainly C++11's memory_order_release imposes such a constraint.

If not, it would mean that e.g.

Thread 1:
x = ...;
...
r1 = x;
done_with_x.lazySet(true);

Thread 2:
if (done_with_x.get()) {
  x = ...;
  ...
  r2 = x;
}

wouldn't work as expected.

In my opinion, that's an indefensible design point, especially since I don't believe it makes lazySet appreciably cheaper on any modern architectures.


>
> > In particular, if v is volatile (and certainly if it's accessed using
> lazySet), and x and y are ordinary variables,
> > then the assignments to x and y in the following may be visibly
> reordered:
> > x = 1;
> > v = 2;
> > y = 3;
>
> You mean what vstore is not "transparent" upside down, but
> "transparent" downside up, so this
>
> y=3
> x=1



--
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111010/2ca7876b/attachment.html>

From aleksey.shipilev at gmail.com  Mon Oct 10 05:59:48 2011
From: aleksey.shipilev at gmail.com (Aleksey Shipilev)
Date: Mon, 10 Oct 2011 13:59:48 +0400
Subject: [concurrency-interest] [doc] ConcurrentReferenceHashMap is missing,
	yet Javadoc is there
Message-ID: <CA+1LWGGB=1TzF8=NkB8iZW0N-2jwgDnmu=W6Zr-fjAV5EXcHHw@mail.gmail.com>

Hi,

I think extra166y has a JavaDoc glitch, there is JavaDoc for
ConcurrentReferenceHashMap [1], but appropriate Java source file [2]
is missing. Was it moved to CustomConcurrentHashMap [3], but javadoc
wasn't cleaned up?

Thanks,
Aleksey.

[1] http://gee.cs.oswego.edu/dl/jsr166/dist/extra166ydocs/extra166y/ConcurrentReferenceHashMap.html
[2] http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/extra166y/
[3] http://gee.cs.oswego.edu/dl/jsr166/dist/extra166ydocs/extra166y/CustomConcurrentHashMap.html

From dl at cs.oswego.edu  Mon Oct 10 06:14:57 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 10 Oct 2011 06:14:57 -0400
Subject: [concurrency-interest] [doc] ConcurrentReferenceHashMap is
 missing, yet Javadoc is there
In-Reply-To: <CA+1LWGGB=1TzF8=NkB8iZW0N-2jwgDnmu=W6Zr-fjAV5EXcHHw@mail.gmail.com>
References: <CA+1LWGGB=1TzF8=NkB8iZW0N-2jwgDnmu=W6Zr-fjAV5EXcHHw@mail.gmail.com>
Message-ID: <4E92C5A1.2070601@cs.oswego.edu>

On 10/10/11 05:59, Aleksey Shipilev wrote:
> Hi,
>
> I think extra166y has a JavaDoc glitch, there is JavaDoc for
> ConcurrentReferenceHashMap [1], but appropriate Java source file [2]
> is missing. Was it moved to CustomConcurrentHashMap [3], but javadoc
> wasn't cleaned up?

Yes, that is exactly what happened (about 3 years ago!) and we hadn't
noticed. Now fixed.

Thanks!

-Doug


From vitalyd at gmail.com  Mon Oct 10 08:56:40 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 10 Oct 2011 08:56:40 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
Message-ID: <CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>

Hi Hans,

I agree that the way StoreLoad is implemented ensures that volatile reads of
different memory location don't move before the store, but I think the JMM
only talks about needing this for loads of same memory location (see Doug's
cookbook web page description).  For example, it's known that in order to
create a happens-before edge, according to the JMM, by using volatile
read/write you have to read/write same volatile.  In practice this probably
isn't strictly the case because the JVM doesn't track the "pairs" (I.e.
store/load of same volatile across different methods) and thus issues fences
strong enough to make this work for unrelated volatile locations.

Please correct me if I'm wrong though.

Thanks
On Oct 9, 2011 10:48 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

>  I agree with most of this.  However, the reason for the StoreLoad fence
> following a volatile store is typically  to prevent reordering of the store
> with a subsequent volatile load of a DIFFERENT VOLATILE variable.   For
> example, consider the standard Dekker?s algorithm example, where everything
> is initially zero:****
>
> ** **
>
> Thread 1:****
>
> x = 1;****
>
> r1 = y;****
>
> ** **
>
> Thread 2:****
>
> y = 1;****
>
> r2 = x;****
>
> ** **
>
> If x and y are volatile, r1 = r2 = 0 is not allowed.  This means that the
> two accesses in each thread may not be reordered.  My understanding is that
> if lazySet is used here, r1 = r2 = 0 is allowed, and hence the trailing
> StoreLoad fence is not required.****
>
> ** **
>
> A lot of the details here are architecture specific.  In particular, the
> PowerPC story is quite different and more complicated, since PowerPC doesn?t
> by default guarantee that stores become visible on all other processors at
> the same time.  Fences are also required to effectively obtain that
> guarantee.  The difference between lazySet and a volatile store is probably
> larger on x86 than on most other architectures.****
>
> ** **
>
> Hans****
>
> ** **
>
> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
> *Sent:* Sunday, October 09, 2011 3:53 PM
> *To:* Ruslan Cheremin
> *Cc:* Boehm, Hans; concurrency-interest at cs.oswego.edu
> *Subject:* Re: AtomicXXX.lazySet and happens-before reasoning****
>
> ** **
>
> That is not clear for me -- doesn't strict volatile store prevents any
> reordering with subsequent memory actions? Huns, you've just present an
> example of such allowed reordering.****
>
>  ** **
>
> A volatile store does not prevent subsequent *non-volatile* store/loads
> from reordering with it.  A StoreLoad is placed after the volatile store
> only if followed by a volatile load of the same memory location that was
> stored (this is to avoid the load from being satisfied out of the
> processor's write buffer before the prior store is made globally visible).
> ****
>
> ** **
>
> Based on this discussion, it sounds like lazySet does not prevent
> subsequent instructions from reordering with it.  Specifically, if a lazySet
> is followed by a volatile load, no StoreLoad is issued in between and the
> processor can fetch the data from the write buffer before other processors
> see the write; this is the reason that lazySet will be much cheaper than
> volatile write because it doesn't have to issue a fence instruction here and
> wait for the store buffer to drain to the cache.  In my own observation,
> lazySet translates to a regular MOV instruction on x86 (64) and a volatile
> write issues a locked add instruction after the write (or mfence on older
> Hotspot versions), which obtains the required serialization.****
>
> ** **
>
> ** **
>
> On Sun, Oct 9, 2011 at 6:03 PM, Ruslan Cheremin <cheremin at gmail.com>
> wrote:****
>
> ** **
>
> The farther, the better :)****
>
> ** **
>
> Yes, from this atomic package javadoc specification it is clear that it is
> not only StoreStore, but also LoadStore barriers before lazySet.****
>
> ** **
>
> Interestingly that my meeting with lazySet started from informal definition
> like "non-volatile write to volatile variable with few additional ordering
> constraints", but for now it is "has the memory effects of writing
> (assigning) a volatile variable except..." ****
>
> ** **
>
> ** **
>
> That is not clear for me -- doesn't strict volatile store prevents any
> reordering with subsequent memory actions? Huns, you've just present an
> example of such allowed reordering.****
>
> ** **
>
> It seems like it somekind related to StoreLoad fence which ordinary
> volatile store must has after it (although I still do not understand why),
> but lazySet omit?****
>
> ** **
>
>  ****
>
>  The specification states****
>
>  ****
>
> ?         lazySet has the memory effects of writing (assigning) a volatilevariable except that it permits reorderings with subsequent (but not
> previous) memory actions that do not themselves impose reordering
> constraints with ordinary non-volatile writes. Among other usage contexts,
> lazySet may apply when nulling out, for the sake of garbage collection, a
> reference that is never accessed again. ****
>
> in the java.util.concurrent description, which implies that it may not be
> reordered with previous ?memory actions?, not just stores.  Doug can comment
> more authoritatively on the intent, but that specification seems fairly
> unambiguous in this particular respect.****
>
>  ****
>
> Hans****
>
>  ****
>
>  ****
>
>  ****
>
> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
> *Sent:* Saturday, October 08, 2011 10:11 AM
> *To:* Boehm, Hans; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and happens-before
> reasoning****
>
>  ****
>
> + rest of the group****
>
> On Sat, Oct 8, 2011 at 1:10 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:****
>
> Hi Hans,****
>
>  ****
>
> I was under the impression that lazySet is purely a StoreStore barrier, and
> only specifies that the lazySet cannot be reordered with prior writes -- I
> never saw mention of requiring no reordering with prior loads.  Here's
> Doug's evaluation:
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6275329, where only a
> store-store is mentioned.  If it's really a LoadStore | StoreStore, good to
> know ...****
>
>  ****
>
> Thanks****
>
>  ****
>
> On Sat, Oct 8, 2011 at 12:06 AM, Boehm, Hans <hans.boehm at hp.com> wrote:***
> *
>
> LazySet() needs to prevent reordering of ordinary memory operations with a
> subsequent lazySet() operation.  In the JSR 133 Cookbook style, that can be
> implemented with a LoadStore | StoreStore fence preceding the lazySet()
> call.  So yes, that makes sense.****
>
>  ****
>
> Real machines tend to require neither of those fences (x86) or combine them
> into a single instruction.****
>
>  ****
>
> Hans****
>
>  ****
>
> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
> *Sent:* Friday, October 07, 2011 5:10 PM
> *To:* Boehm, Hans
> *Cc:* concurrency-interest at cs.oswego.edu; Ruslan Cheremin****
>
>
> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and happens-before
> reasoning****
>
>  ****
>
> Does it even make sense to say that lazySet needs a LoadStore fence? The
> get() does but that's because it has same semantics as volatile read. ****
>
> On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:****
>
> > From: Ruslan Cheremin [mailto:cheremin at gmail.com]
> > > It also needs a LoadStore fence, in both cases.
> >
> > But why lazySet needs LoadStore fence? It seems what lazySet javadoc
> > does not put any ordering constraints on loads...
> I do read it as imposing such a constraint, though we all agree that a more
> precise spec would help.  Certainly C++11's memory_order_release imposes
> such a constraint.
>
> If not, it would mean that e.g.
>
> Thread 1:
> x = ...;
> ...
> r1 = x;
> done_with_x.lazySet(true);
>
> Thread 2:
> if (done_with_x.get()) {
>   x = ...;
>   ...
>   r2 = x;
> }
>
> wouldn't work as expected.
>
> In my opinion, that's an indefensible design point, especially since I
> don't believe it makes lazySet appreciably cheaper on any modern
> architectures.
>
>
> >
> > > In particular, if v is volatile (and certainly if it's accessed using
> > lazySet), and x and y are ordinary variables,
> > > then the assignments to x and y in the following may be visibly
> > reordered:
> > > x = 1;
> > > v = 2;
> > > y = 3;
> >
> > You mean what vstore is not "transparent" upside down, but
> > "transparent" downside up, so this
> >
> > y=3
> > x=1****
>
>
>
> ****
>
> ** **
>
> --
> Vitaly
> 617-548-7007 (mobile)****
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111010/12ef18f5/attachment-0001.html>

From vitalyd at gmail.com  Mon Oct 10 09:00:42 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 10 Oct 2011 09:00:42 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
	<CAOwENiLmWKhwaemUqg9jf6K2FP7GeGBQKqa8rait5yYf_BQgrA@mail.gmail.com>
	<4E85CECC.3050209@cs.oswego.edu>
	<CAOwENiKRpG6-5tizEfhU7vK9TbGtbLBC=4g-T93Kaw1dtHdJkw@mail.gmail.com>
	<4E85EFD3.1080204@cs.oswego.edu>
	<CAOwENiKJ2+os+dJ6OcxLCq6Vsa0uGpGsFn-uBviV2UE7DEDHjg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
	<CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
Message-ID: <CAHjP37EUG-nXVaw+yFkhZwxzwm7TaeMb-wSHDvJVhAkLruntnw@mail.gmail.com>

By the way, would be great if someone from the Hotspot runtime/compiler team
could shed some light on how Hotspot handles these, with the caveat that
people shouldn't necessarily base their code on it if it makes stronger
guarantees than the JMM :).
On Oct 10, 2011 8:56 AM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> Hi Hans,
>
> I agree that the way StoreLoad is implemented ensures that volatile reads
> of different memory location don't move before the store, but I think the
> JMM only talks about needing this for loads of same memory location (see
> Doug's cookbook web page description).  For example, it's known that in
> order to create a happens-before edge, according to the JMM, by using
> volatile read/write you have to read/write same volatile.  In practice this
> probably isn't strictly the case because the JVM doesn't track the "pairs"
> (I.e. store/load of same volatile across different methods) and thus issues
> fences strong enough to make this work for unrelated volatile locations.
>
> Please correct me if I'm wrong though.
>
> Thanks
> On Oct 9, 2011 10:48 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:
>
>>  I agree with most of this.  However, the reason for the StoreLoad fence
>> following a volatile store is typically  to prevent reordering of the store
>> with a subsequent volatile load of a DIFFERENT VOLATILE variable.   For
>> example, consider the standard Dekker?s algorithm example, where everything
>> is initially zero:****
>>
>> ** **
>>
>> Thread 1:****
>>
>> x = 1;****
>>
>> r1 = y;****
>>
>> ** **
>>
>> Thread 2:****
>>
>> y = 1;****
>>
>> r2 = x;****
>>
>> ** **
>>
>> If x and y are volatile, r1 = r2 = 0 is not allowed.  This means that the
>> two accesses in each thread may not be reordered.  My understanding is that
>> if lazySet is used here, r1 = r2 = 0 is allowed, and hence the trailing
>> StoreLoad fence is not required.****
>>
>> ** **
>>
>> A lot of the details here are architecture specific.  In particular, the
>> PowerPC story is quite different and more complicated, since PowerPC doesn?t
>> by default guarantee that stores become visible on all other processors at
>> the same time.  Fences are also required to effectively obtain that
>> guarantee.  The difference between lazySet and a volatile store is probably
>> larger on x86 than on most other architectures.****
>>
>> ** **
>>
>> Hans****
>>
>> ** **
>>
>> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
>> *Sent:* Sunday, October 09, 2011 3:53 PM
>> *To:* Ruslan Cheremin
>> *Cc:* Boehm, Hans; concurrency-interest at cs.oswego.edu
>> *Subject:* Re: AtomicXXX.lazySet and happens-before reasoning****
>>
>> ** **
>>
>> That is not clear for me -- doesn't strict volatile store prevents any
>> reordering with subsequent memory actions? Huns, you've just present an
>> example of such allowed reordering.****
>>
>>  ** **
>>
>> A volatile store does not prevent subsequent *non-volatile* store/loads
>> from reordering with it.  A StoreLoad is placed after the volatile store
>> only if followed by a volatile load of the same memory location that was
>> stored (this is to avoid the load from being satisfied out of the
>> processor's write buffer before the prior store is made globally visible).
>> ****
>>
>> ** **
>>
>> Based on this discussion, it sounds like lazySet does not prevent
>> subsequent instructions from reordering with it.  Specifically, if a lazySet
>> is followed by a volatile load, no StoreLoad is issued in between and the
>> processor can fetch the data from the write buffer before other processors
>> see the write; this is the reason that lazySet will be much cheaper than
>> volatile write because it doesn't have to issue a fence instruction here and
>> wait for the store buffer to drain to the cache.  In my own observation,
>> lazySet translates to a regular MOV instruction on x86 (64) and a volatile
>> write issues a locked add instruction after the write (or mfence on older
>> Hotspot versions), which obtains the required serialization.****
>>
>> ** **
>>
>> ** **
>>
>> On Sun, Oct 9, 2011 at 6:03 PM, Ruslan Cheremin <cheremin at gmail.com>
>> wrote:****
>>
>> ** **
>>
>> The farther, the better :)****
>>
>> ** **
>>
>> Yes, from this atomic package javadoc specification it is clear that it is
>> not only StoreStore, but also LoadStore barriers before lazySet.****
>>
>> ** **
>>
>> Interestingly that my meeting with lazySet started from informal
>> definition like "non-volatile write to volatile variable with few additional
>> ordering constraints", but for now it is "has the memory effects of
>> writing (assigning) a volatile variable except..." ****
>>
>> ** **
>>
>> ** **
>>
>> That is not clear for me -- doesn't strict volatile store prevents any
>> reordering with subsequent memory actions? Huns, you've just present an
>> example of such allowed reordering.****
>>
>> ** **
>>
>> It seems like it somekind related to StoreLoad fence which ordinary
>> volatile store must has after it (although I still do not understand why),
>> but lazySet omit?****
>>
>> ** **
>>
>>  ****
>>
>>  The specification states****
>>
>>  ****
>>
>> ?         lazySet has the memory effects of writing (assigning) a
>> volatile variable except that it permits reorderings with subsequent (but
>> not previous) memory actions that do not themselves impose reordering
>> constraints with ordinary non-volatile writes. Among other usage
>> contexts, lazySet may apply when nulling out, for the sake of garbage
>> collection, a reference that is never accessed again. ****
>>
>> in the java.util.concurrent description, which implies that it may not be
>> reordered with previous ?memory actions?, not just stores.  Doug can comment
>> more authoritatively on the intent, but that specification seems fairly
>> unambiguous in this particular respect.****
>>
>>  ****
>>
>> Hans****
>>
>>  ****
>>
>>  ****
>>
>>  ****
>>
>> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
>> *Sent:* Saturday, October 08, 2011 10:11 AM
>> *To:* Boehm, Hans; concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and
>> happens-before reasoning****
>>
>>  ****
>>
>> + rest of the group****
>>
>> On Sat, Oct 8, 2011 at 1:10 PM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:****
>>
>> Hi Hans,****
>>
>>  ****
>>
>> I was under the impression that lazySet is purely a StoreStore barrier,
>> and only specifies that the lazySet cannot be reordered with prior writes --
>> I never saw mention of requiring no reordering with prior loads.  Here's
>> Doug's evaluation:
>> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6275329, where only a
>> store-store is mentioned.  If it's really a LoadStore | StoreStore, good to
>> know ...****
>>
>>  ****
>>
>> Thanks****
>>
>>  ****
>>
>> On Sat, Oct 8, 2011 at 12:06 AM, Boehm, Hans <hans.boehm at hp.com> wrote:**
>> **
>>
>> LazySet() needs to prevent reordering of ordinary memory operations with a
>> subsequent lazySet() operation.  In the JSR 133 Cookbook style, that can be
>> implemented with a LoadStore | StoreStore fence preceding the lazySet()
>> call.  So yes, that makes sense.****
>>
>>  ****
>>
>> Real machines tend to require neither of those fences (x86) or combine
>> them into a single instruction.****
>>
>>  ****
>>
>> Hans****
>>
>>  ****
>>
>> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
>> *Sent:* Friday, October 07, 2011 5:10 PM
>> *To:* Boehm, Hans
>> *Cc:* concurrency-interest at cs.oswego.edu; Ruslan Cheremin****
>>
>>
>> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and
>> happens-before reasoning****
>>
>>  ****
>>
>> Does it even make sense to say that lazySet needs a LoadStore fence? The
>> get() does but that's because it has same semantics as volatile read. ***
>> *
>>
>> On Oct 7, 2011 7:29 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:****
>>
>> > From: Ruslan Cheremin [mailto:cheremin at gmail.com]
>> > > It also needs a LoadStore fence, in both cases.
>> >
>> > But why lazySet needs LoadStore fence? It seems what lazySet javadoc
>> > does not put any ordering constraints on loads...
>> I do read it as imposing such a constraint, though we all agree that a
>> more precise spec would help.  Certainly C++11's memory_order_release
>> imposes such a constraint.
>>
>> If not, it would mean that e.g.
>>
>> Thread 1:
>> x = ...;
>> ...
>> r1 = x;
>> done_with_x.lazySet(true);
>>
>> Thread 2:
>> if (done_with_x.get()) {
>>   x = ...;
>>   ...
>>   r2 = x;
>> }
>>
>> wouldn't work as expected.
>>
>> In my opinion, that's an indefensible design point, especially since I
>> don't believe it makes lazySet appreciably cheaper on any modern
>> architectures.
>>
>>
>> >
>> > > In particular, if v is volatile (and certainly if it's accessed using
>> > lazySet), and x and y are ordinary variables,
>> > > then the assignments to x and y in the following may be visibly
>> > reordered:
>> > > x = 1;
>> > > v = 2;
>> > > y = 3;
>> >
>> > You mean what vstore is not "transparent" upside down, but
>> > "transparent" downside up, so this
>> >
>> > y=3
>> > x=1****
>>
>>
>>
>> ****
>>
>> ** **
>>
>> --
>> Vitaly
>> 617-548-7007 (mobile)****
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111010/d470472e/attachment.html>

From dl at cs.oswego.edu  Mon Oct 10 09:12:58 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 10 Oct 2011 09:12:58 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and
	happens-before	reasoning
In-Reply-To: <CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>	<4E8F028D.8030109@cs.oswego.edu>	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>	<A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
	<CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
Message-ID: <4E92EF5A.1080901@cs.oswego.edu>

On 10/10/11 08:56, Vitaly Davidovich wrote:
> I agree that the way StoreLoad is implemented ensures that volatile reads of
>  different memory location don't move before the store, but I think the JMM
> only talks about needing this for loads of same memory location (see Doug's
> cookbook web page description).

Additionally, the "synchronization order" (roughly, any trace of
all volatile accesses) is required to be a total order, which
is the main constraint that forces the Dekker example to work.
This is not made clear enough in cookbook, which should be improved.

> By the way, would be great if someone from the Hotspot runtime/compiler team
> could shed some light on how Hotspot handles these, with the caveat that
> people shouldn't necessarily base their code on it if it makes stronger
> guarantees than the JMM :).

The main visible cases are processor- not JVM- based, and
most are only visible in giving you more consistency than required
for non-volatile accesses. In general, x86 and sparc are
stronger than POWER and ARM, with a few others
(Azul, IA64) in the middle.

-Doug







From vitalyd at gmail.com  Mon Oct 10 09:20:03 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 10 Oct 2011 09:20:03 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <4E92EF5A.1080901@cs.oswego.edu>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
	<CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
	<4E92EF5A.1080901@cs.oswego.edu>
Message-ID: <CAHjP37Har_sFe8QeM_6Aa3vQh2V648LODERt56FaL7VqjVUZVQ@mail.gmail.com>

Thanks Doug.

I'm aware that the type of instruction and whether a fence is a no-op is
arch specific, but I was curious how the compilers (c1/c2) make decisions
around these things,  such as if they track pairs of read/write of same
volatile across method bounds ( I believe they don't at the moment).
On Oct 10, 2011 9:14 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:

> On 10/10/11 08:56, Vitaly Davidovich wrote:
>
>> I agree that the way StoreLoad is implemented ensures that volatile reads
>> of
>>  different memory location don't move before the store, but I think the
>> JMM
>> only talks about needing this for loads of same memory location (see
>> Doug's
>> cookbook web page description).
>>
>
> Additionally, the "synchronization order" (roughly, any trace of
> all volatile accesses) is required to be a total order, which
> is the main constraint that forces the Dekker example to work.
> This is not made clear enough in cookbook, which should be improved.
>
>  By the way, would be great if someone from the Hotspot runtime/compiler
>> team
>> could shed some light on how Hotspot handles these, with the caveat that
>> people shouldn't necessarily base their code on it if it makes stronger
>> guarantees than the JMM :).
>>
>
> The main visible cases are processor- not JVM- based, and
> most are only visible in giving you more consistency than required
> for non-volatile accesses. In general, x86 and sparc are
> stronger than POWER and ARM, with a few others
> (Azul, IA64) in the middle.
>
> -Doug
>
>
>
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111010/20d43e04/attachment.html>

From cheremin at gmail.com  Mon Oct 10 09:50:30 2011
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Mon, 10 Oct 2011 17:50:30 +0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <CAHjP37Har_sFe8QeM_6Aa3vQh2V648LODERt56FaL7VqjVUZVQ@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
	<CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
	<4E92EF5A.1080901@cs.oswego.edu>
	<CAHjP37Har_sFe8QeM_6Aa3vQh2V648LODERt56FaL7VqjVUZVQ@mail.gmail.com>
Message-ID: <CAOwENiK13_62qGvPWQXdOZT+EbsSR6bBQgcs3TN6Z9msdzcDgw@mail.gmail.com>

> I'm aware that the type of instruction and whether a fence is a no-op is
> arch specific, but I was curious how the compilers (c1/c2) make decisions
> around these things,? such as if they track pairs of read/write of same
> volatile across method bounds ( I believe they don't at the moment).

As far, as I understand, JIT does not need to track pairs -- since
StoreLoad is required to
keep volatiles access visible in same order for any thread (in
synchronization order), so
you can't remove it anyway.

I totally agree what it'll be very helpfull and interesting to see
some kind of "improved
JSR-133 cookbook" with slightely more detailed description about
ordering fences
issued for supporting varios kinds of JMM requirements, and their roles. Keeping
in mind forecoming Fences it can help to answer many arising questions like
"what Fence should I add to lazySet to get exactly volatile store and why".



> On Oct 10, 2011 9:14 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:
>>
>> On 10/10/11 08:56, Vitaly Davidovich wrote:
>>>
>>> I agree that the way StoreLoad is implemented ensures that volatile reads
>>> of
>>> ?different memory location don't move before the store, but I think the
>>> JMM
>>> only talks about needing this for loads of same memory location (see
>>> Doug's
>>> cookbook web page description).
>>
>> Additionally, the "synchronization order" (roughly, any trace of
>> all volatile accesses) is required to be a total order, which
>> is the main constraint that forces the Dekker example to work.
>> This is not made clear enough in cookbook, which should be improved.
>>
>>> By the way, would be great if someone from the Hotspot runtime/compiler
>>> team
>>> could shed some light on how Hotspot handles these, with the caveat that
>>> people shouldn't necessarily base their code on it if it makes stronger
>>> guarantees than the JMM :).
>>
>> The main visible cases are processor- not JVM- based, and
>> most are only visible in giving you more consistency than required
>> for non-volatile accesses. In general, x86 and sparc are
>> stronger than POWER and ARM, with a few others
>> (Azul, IA64) in the middle.
>>
>> -Doug
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From vitalyd at gmail.com  Mon Oct 10 10:18:25 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 10 Oct 2011 10:18:25 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <CAOwENiK13_62qGvPWQXdOZT+EbsSR6bBQgcs3TN6Z9msdzcDgw@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
	<CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
	<4E92EF5A.1080901@cs.oswego.edu>
	<CAHjP37Har_sFe8QeM_6Aa3vQh2V648LODERt56FaL7VqjVUZVQ@mail.gmail.com>
	<CAOwENiK13_62qGvPWQXdOZT+EbsSR6bBQgcs3TN6Z9msdzcDgw@mail.gmail.com>
Message-ID: <CAHjP37F3ZRCEycNN9ooq903pWpXpek+1_a-DCo=AOFKxv2CfOQ@mail.gmail.com>

Did I miss something in this thread (or elsewhere) that said Fences API is
coming? I thought Doug's attempt at this didn't go anywhere last time.

Thanks
On Oct 10, 2011 9:50 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

> > I'm aware that the type of instruction and whether a fence is a no-op is
> > arch specific, but I was curious how the compilers (c1/c2) make decisions
> > around these things,  such as if they track pairs of read/write of same
> > volatile across method bounds ( I believe they don't at the moment).
>
> As far, as I understand, JIT does not need to track pairs -- since
> StoreLoad is required to
> keep volatiles access visible in same order for any thread (in
> synchronization order), so
> you can't remove it anyway.
>
> I totally agree what it'll be very helpfull and interesting to see
> some kind of "improved
> JSR-133 cookbook" with slightely more detailed description about
> ordering fences
> issued for supporting varios kinds of JMM requirements, and their roles.
> Keeping
> in mind forecoming Fences it can help to answer many arising questions like
> "what Fence should I add to lazySet to get exactly volatile store and why".
>
>
>
> > On Oct 10, 2011 9:14 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:
> >>
> >> On 10/10/11 08:56, Vitaly Davidovich wrote:
> >>>
> >>> I agree that the way StoreLoad is implemented ensures that volatile
> reads
> >>> of
> >>>  different memory location don't move before the store, but I think the
> >>> JMM
> >>> only talks about needing this for loads of same memory location (see
> >>> Doug's
> >>> cookbook web page description).
> >>
> >> Additionally, the "synchronization order" (roughly, any trace of
> >> all volatile accesses) is required to be a total order, which
> >> is the main constraint that forces the Dekker example to work.
> >> This is not made clear enough in cookbook, which should be improved.
> >>
> >>> By the way, would be great if someone from the Hotspot runtime/compiler
> >>> team
> >>> could shed some light on how Hotspot handles these, with the caveat
> that
> >>> people shouldn't necessarily base their code on it if it makes stronger
> >>> guarantees than the JMM :).
> >>
> >> The main visible cases are processor- not JVM- based, and
> >> most are only visible in giving you more consistency than required
> >> for non-volatile accesses. In general, x86 and sparc are
> >> stronger than POWER and ARM, with a few others
> >> (Azul, IA64) in the middle.
> >>
> >> -Doug
> >>
> >>
> >>
> >>
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111010/b8749227/attachment.html>

From cheremin at gmail.com  Mon Oct 10 10:25:09 2011
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Mon, 10 Oct 2011 18:25:09 +0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <CAHjP37F3ZRCEycNN9ooq903pWpXpek+1_a-DCo=AOFKxv2CfOQ@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
	<CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
	<4E92EF5A.1080901@cs.oswego.edu>
	<CAHjP37Har_sFe8QeM_6Aa3vQh2V648LODERt56FaL7VqjVUZVQ@mail.gmail.com>
	<CAOwENiK13_62qGvPWQXdOZT+EbsSR6bBQgcs3TN6Z9msdzcDgw@mail.gmail.com>
	<CAHjP37F3ZRCEycNN9ooq903pWpXpek+1_a-DCo=AOFKxv2CfOQ@mail.gmail.com>
Message-ID: <CAOwENiK1ANcFrogWxSDib=W8tbBMCe6FWhjmq=yx=4us4b6PWg@mail.gmail.com>

I do not know much about it. But there is already lazySet and weakCAS here (and
being used in production code!). And by any way we need some way of reasoning
about such code. So even if Fences itself is still under question, kind of JMM
enchancement introducing such fences-like constructions and how to deal
with them is definitely required. As far as I understand, such efforts currently
going around Fences API.

2011/10/10 Vitaly Davidovich <vitalyd at gmail.com>:
> Did I miss something in this thread (or elsewhere) that said Fences API is
> coming? I thought Doug's attempt at this didn't go anywhere last time.
>
> Thanks
>
> On Oct 10, 2011 9:50 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
>>
>> > I'm aware that the type of instruction and whether a fence is a no-op is
>> > arch specific, but I was curious how the compilers (c1/c2) make
>> > decisions
>> > around these things,? such as if they track pairs of read/write of same
>> > volatile across method bounds ( I believe they don't at the moment).
>>
>> As far, as I understand, JIT does not need to track pairs -- since
>> StoreLoad is required to
>> keep volatiles access visible in same order for any thread (in
>> synchronization order), so
>> you can't remove it anyway.
>>
>> I totally agree what it'll be very helpfull and interesting to see
>> some kind of "improved
>> JSR-133 cookbook" with slightely more detailed description about
>> ordering fences
>> issued for supporting varios kinds of JMM requirements, and their roles.
>> Keeping
>> in mind forecoming Fences it can help to answer many arising questions
>> like
>> "what Fence should I add to lazySet to get exactly volatile store and
>> why".
>>
>>
>>
>> > On Oct 10, 2011 9:14 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:
>> >>
>> >> On 10/10/11 08:56, Vitaly Davidovich wrote:
>> >>>
>> >>> I agree that the way StoreLoad is implemented ensures that volatile
>> >>> reads
>> >>> of
>> >>> ?different memory location don't move before the store, but I think
>> >>> the
>> >>> JMM
>> >>> only talks about needing this for loads of same memory location (see
>> >>> Doug's
>> >>> cookbook web page description).
>> >>
>> >> Additionally, the "synchronization order" (roughly, any trace of
>> >> all volatile accesses) is required to be a total order, which
>> >> is the main constraint that forces the Dekker example to work.
>> >> This is not made clear enough in cookbook, which should be improved.
>> >>
>> >>> By the way, would be great if someone from the Hotspot
>> >>> runtime/compiler
>> >>> team
>> >>> could shed some light on how Hotspot handles these, with the caveat
>> >>> that
>> >>> people shouldn't necessarily base their code on it if it makes
>> >>> stronger
>> >>> guarantees than the JMM :).
>> >>
>> >> The main visible cases are processor- not JVM- based, and
>> >> most are only visible in giving you more consistency than required
>> >> for non-volatile accesses. In general, x86 and sparc are
>> >> stronger than POWER and ARM, with a few others
>> >> (Azul, IA64) in the middle.
>> >>
>> >> -Doug
>> >>
>> >>
>> >>
>> >>
>> >>
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>


From vitalyd at gmail.com  Mon Oct 10 10:27:31 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 10 Oct 2011 10:27:31 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <CAOwENiK1ANcFrogWxSDib=W8tbBMCe6FWhjmq=yx=4us4b6PWg@mail.gmail.com>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
	<CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
	<4E92EF5A.1080901@cs.oswego.edu>
	<CAHjP37Har_sFe8QeM_6Aa3vQh2V648LODERt56FaL7VqjVUZVQ@mail.gmail.com>
	<CAOwENiK13_62qGvPWQXdOZT+EbsSR6bBQgcs3TN6Z9msdzcDgw@mail.gmail.com>
	<CAHjP37F3ZRCEycNN9ooq903pWpXpek+1_a-DCo=AOFKxv2CfOQ@mail.gmail.com>
	<CAOwENiK1ANcFrogWxSDib=W8tbBMCe6FWhjmq=yx=4us4b6PWg@mail.gmail.com>
Message-ID: <CAHjP37GQ3_OQ+WmZ2eGUCBMRcSOjTWXhON9xsYptZHMt8w+ExQ@mail.gmail.com>

I agree - more detail and thorougher coverage of these things would be
welcomed, Fences or not.
On Oct 10, 2011 10:25 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:

> I do not know much about it. But there is already lazySet and weakCAS here
> (and
> being used in production code!). And by any way we need some way of
> reasoning
> about such code. So even if Fences itself is still under question, kind of
> JMM
> enchancement introducing such fences-like constructions and how to deal
> with them is definitely required. As far as I understand, such efforts
> currently
> going around Fences API.
>
> 2011/10/10 Vitaly Davidovich <vitalyd at gmail.com>:
> > Did I miss something in this thread (or elsewhere) that said Fences API
> is
> > coming? I thought Doug's attempt at this didn't go anywhere last time.
> >
> > Thanks
> >
> > On Oct 10, 2011 9:50 AM, "Ruslan Cheremin" <cheremin at gmail.com> wrote:
> >>
> >> > I'm aware that the type of instruction and whether a fence is a no-op
> is
> >> > arch specific, but I was curious how the compilers (c1/c2) make
> >> > decisions
> >> > around these things,  such as if they track pairs of read/write of
> same
> >> > volatile across method bounds ( I believe they don't at the moment).
> >>
> >> As far, as I understand, JIT does not need to track pairs -- since
> >> StoreLoad is required to
> >> keep volatiles access visible in same order for any thread (in
> >> synchronization order), so
> >> you can't remove it anyway.
> >>
> >> I totally agree what it'll be very helpfull and interesting to see
> >> some kind of "improved
> >> JSR-133 cookbook" with slightely more detailed description about
> >> ordering fences
> >> issued for supporting varios kinds of JMM requirements, and their roles.
> >> Keeping
> >> in mind forecoming Fences it can help to answer many arising questions
> >> like
> >> "what Fence should I add to lazySet to get exactly volatile store and
> >> why".
> >>
> >>
> >>
> >> > On Oct 10, 2011 9:14 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:
> >> >>
> >> >> On 10/10/11 08:56, Vitaly Davidovich wrote:
> >> >>>
> >> >>> I agree that the way StoreLoad is implemented ensures that volatile
> >> >>> reads
> >> >>> of
> >> >>>  different memory location don't move before the store, but I think
> >> >>> the
> >> >>> JMM
> >> >>> only talks about needing this for loads of same memory location (see
> >> >>> Doug's
> >> >>> cookbook web page description).
> >> >>
> >> >> Additionally, the "synchronization order" (roughly, any trace of
> >> >> all volatile accesses) is required to be a total order, which
> >> >> is the main constraint that forces the Dekker example to work.
> >> >> This is not made clear enough in cookbook, which should be improved.
> >> >>
> >> >>> By the way, would be great if someone from the Hotspot
> >> >>> runtime/compiler
> >> >>> team
> >> >>> could shed some light on how Hotspot handles these, with the caveat
> >> >>> that
> >> >>> people shouldn't necessarily base their code on it if it makes
> >> >>> stronger
> >> >>> guarantees than the JMM :).
> >> >>
> >> >> The main visible cases are processor- not JVM- based, and
> >> >> most are only visible in giving you more consistency than required
> >> >> for non-volatile accesses. In general, x86 and sparc are
> >> >> stronger than POWER and ARM, with a few others
> >> >> (Azul, IA64) in the middle.
> >> >>
> >> >> -Doug
> >> >>
> >> >>
> >> >>
> >> >>
> >> >>
> >> >>
> >> >> _______________________________________________
> >> >> Concurrency-interest mailing list
> >> >> Concurrency-interest at cs.oswego.edu
> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111010/02e6bed1/attachment.html>

From hans.boehm at hp.com  Mon Oct 10 12:25:18 2011
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 10 Oct 2011 16:25:18 +0000
Subject: [concurrency-interest] AtomicXXX.lazySet
	and	happens-before	reasoning
In-Reply-To: <4E92EF5A.1080901@cs.oswego.edu>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E8F028D.8030109@cs.oswego.edu>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20422BB@G4W3299.americas.hpqcorp.net>
	<CAOwENiKEPcfmDP-75B4=L7k7i3TB-8fM4aMww1aR9wqGRywzaQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20424AF@G4W3299.americas.hpqcorp.net>
	<CAHjP37FCUURtwfKJNajvUnoTXxuSe7k_GV+1nN-e8wS69RPSFg@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20425D8@G4W3299.americas.hpqcorp.net>
	<CAHjP37FmpHuN9Uhnvc-wkGXHBZEVGkVTS16sgO08O3+Gqw==7w@mail.gmail.com>
	<CAHjP37HOjG24UqyZHROUXWbCwUF060FsL42BiCUdO=eWkjTtgQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD20428A0@G4W3299.americas.hpqcorp.net>
	<CAOwENiKFExi5FVOu5c_rA2eEtTuKtLj-8oZrNVaqEks0ojzwVw@mail.gmail.com>
	<CAHjP37EKgMcUbrHaCpOpnWPABquGFMy4AK5SNxvaPSzJDCTa+g@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2043923@G4W3299.americas.hpqcorp.net>
	<CAHjP37EdnZSKyRbOEv0UqO=68Hjzb-HNCey3LWd_Qy2JtEODDA@mail.gmail.com>
	<4E92EF5A.1080901@cs.oswego.edu>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2043BBE@G4W3299.americas.hpqcorp.net>

My current operating hypothesis is that the C++ rules (with Java volatile = memory_order_seq_cst and lazySet = memory_order_release) specified in http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html should apply here.  Some of these are finally backed up by formal proofs.

The major complication is that for PowerPC and ARM, there are at least two different plausible conventions, and it's unclear to me that the "leading full fence" convention used here is the right one, especially for ARM.  And the two conventions don't play together.  It wouldn't surprise me if some of the mappings on ARM and PowerPC changed in the future and/or are currently inconsistent between C++ and Java.

Hans

> From: Doug Lea
> On 10/10/11 08:56, Vitaly Davidovich wrote:
> > I agree that the way StoreLoad is implemented ensures that volatile
> reads of
> >  different memory location don't move before the store, but I think
> the JMM
> > only talks about needing this for loads of same memory location (see
> Doug's
> > cookbook web page description).
> 
> Additionally, the "synchronization order" (roughly, any trace of
> all volatile accesses) is required to be a total order, which
> is the main constraint that forces the Dekker example to work.
> This is not made clear enough in cookbook, which should be improved.
> 
> > By the way, would be great if someone from the Hotspot
> runtime/compiler team
> > could shed some light on how Hotspot handles these, with the caveat
> that
> > people shouldn't necessarily base their code on it if it makes
> stronger
> > guarantees than the JMM :).
> 
> The main visible cases are processor- not JVM- based, and
> most are only visible in giving you more consistency than required
> for non-volatile accesses. In general, x86 and sparc are
> stronger than POWER and ARM, with a few others
> (Azul, IA64) in the middle.
> 
> -Doug
> 
> 
> 
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Mon Oct 10 17:26:02 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 11 Oct 2011 07:26:02 +1000
Subject: [concurrency-interest] AtomicXXX.lazySet and
	happens-beforereasoning
In-Reply-To: <CAHjP37Har_sFe8QeM_6Aa3vQh2V648LODERt56FaL7VqjVUZVQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEHBJAAA.davidcholmes@aapt.net.au>

The VM augments volatile loads and stores with pre- and post- actions that
ensure the requirements for all pairings are met. This means that in some
cases the "barriers" between memory accesses are stronger than needed. The
VM does not track the pairings so can't issue exact barriers needed. However
C2 will combine/elide redundant barriers to some extent.

If you need more detail feel free to read the source code :)

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Vitaly
Davidovich
  Sent: Monday, 10 October 2011 11:20 PM
  To: Doug Lea
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] AtomicXXX.lazySet and
happens-beforereasoning


  Thanks Doug.

  I'm aware that the type of instruction and whether a fence is a no-op is
arch specific, but I was curious how the compilers (c1/c2) make decisions
around these things,  such as if they track pairs of read/write of same
volatile across method bounds ( I believe they don't at the moment).

  On Oct 10, 2011 9:14 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:

    On 10/10/11 08:56, Vitaly Davidovich wrote:

      I agree that the way StoreLoad is implemented ensures that volatile
reads of
       different memory location don't move before the store, but I think
the JMM
      only talks about needing this for loads of same memory location (see
Doug's
      cookbook web page description).


    Additionally, the "synchronization order" (roughly, any trace of
    all volatile accesses) is required to be a total order, which
    is the main constraint that forces the Dekker example to work.
    This is not made clear enough in cookbook, which should be improved.


      By the way, would be great if someone from the Hotspot
runtime/compiler team
      could shed some light on how Hotspot handles these, with the caveat
that
      people shouldn't necessarily base their code on it if it makes
stronger
      guarantees than the JMM :).


    The main visible cases are processor- not JVM- based, and
    most are only visible in giving you more consistency than required
    for non-volatile accesses. In general, x86 and sparc are
    stronger than POWER and ARM, with a few others
    (Azul, IA64) in the middle.

    -Doug






    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111011/5a008cf5/attachment.html>

From vitalyd at gmail.com  Mon Oct 10 19:18:13 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 10 Oct 2011 19:18:13 -0400
Subject: [concurrency-interest] AtomicXXX.lazySet and
	happens-beforereasoning
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEHBJAAA.davidcholmes@aapt.net.au>
References: <CAHjP37Har_sFe8QeM_6Aa3vQh2V648LODERt56FaL7VqjVUZVQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEHBJAAA.davidcholmes@aapt.net.au>
Message-ID: <CAHjP37EmiDd6EhUZAkFGBnSt40cRWiaiwRbwFpYuKk=qgg59qw@mail.gmail.com>

Thanks David.  I've actually seen hotspot combine two successive volatile
writes with just 1 fence after them on x86.
On Oct 10, 2011 5:25 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

> **
> The VM augments volatile loads and stores with pre- and post- actions that
> ensure the requirements for all pairings are met. This means that in some
> cases the "barriers" between memory accesses are stronger than needed. The
> VM does not track the pairings so can't issue exact barriers needed. However
> C2 will combine/elide redundant barriers to some extent.
>
> If you need more detail feel free to read the source code :)
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Vitaly
> Davidovich
> *Sent:* Monday, 10 October 2011 11:20 PM
> *To:* Doug Lea
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] AtomicXXX.lazySet and
> happens-beforereasoning
>
> Thanks Doug.
>
> I'm aware that the type of instruction and whether a fence is a no-op is
> arch specific, but I was curious how the compilers (c1/c2) make decisions
> around these things,  such as if they track pairs of read/write of same
> volatile across method bounds ( I believe they don't at the moment).
> On Oct 10, 2011 9:14 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:
>
>> On 10/10/11 08:56, Vitaly Davidovich wrote:
>>
>>> I agree that the way StoreLoad is implemented ensures that volatile reads
>>> of
>>>  different memory location don't move before the store, but I think the
>>> JMM
>>> only talks about needing this for loads of same memory location (see
>>> Doug's
>>> cookbook web page description).
>>>
>>
>> Additionally, the "synchronization order" (roughly, any trace of
>> all volatile accesses) is required to be a total order, which
>> is the main constraint that forces the Dekker example to work.
>> This is not made clear enough in cookbook, which should be improved.
>>
>> By the way, would be great if someone from the Hotspot runtime/compiler
>>> team
>>> could shed some light on how Hotspot handles these, with the caveat that
>>> people shouldn't necessarily base their code on it if it makes stronger
>>> guarantees than the JMM :).
>>>
>>
>> The main visible cases are processor- not JVM- based, and
>> most are only visible in giving you more consistency than required
>> for non-volatile accesses. In general, x86 and sparc are
>> stronger than POWER and ARM, with a few others
>> (Azul, IA64) in the middle.
>>
>> -Doug
>>
>>
>>
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111010/66f2dd65/attachment.html>

From mohanr at fss.co.in  Wed Oct 12 05:10:53 2011
From: mohanr at fss.co.in (Mohan Radhakrishnan)
Date: Wed, 12 Oct 2011 14:40:53 +0530
Subject: [concurrency-interest] Basic Fork Join questions
Message-ID: <65F4195D4E2C6042BA66D4114E5BB9DC581162@fssbemail.fss.india>

Hi,

 

       I am trying to understand  the basic work stealing algorithm
implemented in ForkJoinWorkerThread.java. There are references to some
papers and Herlihy and Shavit's book which I will read.

 

Do these two methods mainly implement the stealing part - deqTask() for
other queues and locallyDeqTask() for own queue.? 

 

I am interested in looking at the running ForkJoin code in
java.util.concurrent to get the queue size, tasks and stealing part
using most probably AspectJ. 

 

Thanks,

Mohan



DISCLAIMER:
==========================================================================================================================================================The information contained in this e-mail message may be privileged and/or confidential and protected from disclosure under applicable law. It is intended only for the individual to whom or entity to which it is addressed as shown at the beginning of the message. If the reader of this message is not the intended recipient, or if the employee or agent responsible for delivering the message is not an employee or agent of the intended recipient, you are hereby notified that any review, dissemination,distribution, use, or copying of this message is strictly prohibited. If you have received this message in error, please notify us immediately by return e-mail and permanently delete this message and your reply to the extent it includes this message. Any views or opinions presented in this message or attachments are those of the author and do not necessarily represent those of the Company. All e-mails and attachments sent and received are subject to monitoring, reading, and archival by the Company.==========================================================================================================================================================
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111012/204bd12a/attachment.html>

From dl at cs.oswego.edu  Wed Oct 12 08:17:30 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 12 Oct 2011 08:17:30 -0400
Subject: [concurrency-interest] Basic Fork Join questions
In-Reply-To: <65F4195D4E2C6042BA66D4114E5BB9DC581162@fssbemail.fss.india>
References: <65F4195D4E2C6042BA66D4114E5BB9DC581162@fssbemail.fss.india>
Message-ID: <4E95855A.4080500@cs.oswego.edu>

On 10/12/11 05:10, Mohan Radhakrishnan wrote:
> I am trying to understand the basic work stealing algorithm implemented in
> ForkJoinWorkerThread.java. There are references to some papers and Herlihy and
> Shavit?s book which I will read.

Yes. Without this background, it is probably impossible to
modify, extend or retarget the implementation.

>
> Do these two methods mainly implement the stealing part - deqTask() for other
> queues and locallyDeqTask() for own queue.?

deqTask is for steals. The standard path for processing local task
is popTask and its variants (locallyDeqTask is used only for
pools in async/fifo mode). However, there are several special
cases of these used throughout the FJ code.

>
> I am interested in looking at the running ForkJoin code in java.util.concurrent
> to get the queue size, tasks and stealing part using most probably AspectJ.

I'm not sure what this means, but like almost
everything else in java.util.concurrent, we supply enough
lower-level methods (including some "protected" ones in
ForkJoinPool and ForkJoinTask) that you should be able to
invent new ways of exposing higher-level functionality.

-Doug




From bheem at sbcglobal.net  Fri Oct 21 10:39:52 2011
From: bheem at sbcglobal.net (bhm)
Date: Fri, 21 Oct 2011 10:39:52 -0400
Subject: [concurrency-interest] memoizer cache
Message-ID: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>

I'm trying to build a cache for configuration objects for my projects-
Requirements:
 1 only one instance per key should be created, either because its absent
   for a given key or its expired (in that case it will be recreated).
 2 most common path asking for a value for a key should not block or
synchronize.
 3 when a value doesnt exists for a key, one thread should create the instance
   of value and put in the cache, other threads asking for same key should wait
   till thats complete.
 4 if a value is expired, one thread should (re)create the instance of value
   and other threads asking for same key (when one thread is recreating it)
   should use expired instance of value and should not block.

My code is largely derived from Christian's reply
http://cs.oswego.edu/pipermail/concurrency-interest/2010-November/007469.html

Please comment or suggest on following, Thanks for you help.

    final ConcurrentMap<K, Object> store = new ConcurrentHashMap<K, Object>();

    V get(K key) {
        Object ret = store.get(key);

        if (ret == null) {
            CountDownLatch latch = new CountDownLatch(1);
            ret = store.putIfAbsent(key, latch);
            if (null == ret) {
                ret = new Entry<V>(factory.create(key));
                store.replace(key, ret);
                latch.countDown();
            }
        }

        if (ret instanceof CountDownLatch) {
            try {((CountDownLatch) ret).await();} // till new value is created
            catch (InterruptedException e) {throw new RuntimeException(e);}
            ret = store.get(key); // get new value
        } else {
            Entry<V> entry = (Entry<V>) ret;

            if (evictionpolicy != null && evictionpolicy.isExpired(entry.v)) {
                final AtomicInteger sync = entry.sync;

                if (sync.compareAndSet(0, 1)) { // otherwise retrun old value
                    try {
                        entry = (Entry<V>) store.get(key); // get again
                        if (evictionpolicy.isExpired(entry.v)) { // double-check
                            Entry<V> newval = new Entry<V>(factory.create(key));
                            store.replace(key, newval);
                            ret = newval;
                        }
                    }
                    catch (Throwable catchall) {
                      catchall.printStackTrace(System.err);//return old value
                    }
                    finally {sync.set(0);/*unlock*/}
                }
            }
        }

        return ((Entry<V>) ret).v;
    }

    class Entry<V> {
        final AtomicInteger sync = new AtomicInteger(0);
        final V v;
        Entry(V v) {this.v=v;}
    }


form other discussions about similar topics in the list, I understand there are
libraries to do that (MapMaker etc.), I wanted something specific, small, just
to slove the given problem and avoid generalization.
I couldnt get jcp's Memoizer to work when cache entries could expire. Its good
for lazy one time load.

Thanks again.

From fry at google.com  Fri Oct 21 11:16:37 2011
From: fry at google.com (Charles Fry)
Date: Fri, 21 Oct 2011 11:16:37 -0400
Subject: [concurrency-interest] memoizer cache
In-Reply-To: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
References: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
Message-ID: <CAOhq40q0gL7RSPdibdP6rs-OEhtPeymPR1YNQRSq_6Vgdp9fog@mail.gmail.com>

>
> form other discussions about similar topics in the list, I understand there
> are
> libraries to do that (MapMaker etc.), I wanted something specific, small,
> just
> to slove the given problem and avoid generalization.
>

The newest thing we have in Guava is CacheBuilder:


http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/cache/CacheBuilder.html

I'd just go with it. ;-)

Charles
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111021/baa264f8/attachment.html>

From karmazilla at gmail.com  Fri Oct 21 12:11:58 2011
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Fri, 21 Oct 2011 18:11:58 +0200
Subject: [concurrency-interest] memoizer cache
In-Reply-To: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
References: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
Message-ID: <CACyP5Pdwu3yLUsr+-BefrUQd3FV7TgtgyWaiAhqBX4KAAdzK9w@mail.gmail.com>

One thing that comes to mind is this:
If you determine that the value you are about to return has expired, then
you could replace(key, entry, null) and recursively call get(key) again. If
the replace() call returns `false` then someone got ahead of you and already
put a null or a new CountDownLatch in there, and the get() call will deal
with it. Otherwise it will look, to the following get() call, as if the
entry never existed and get() will create it for you. So you don't really
need to check the return value of replace() with this approach.
If you do it this way then I guess you don't need the Entry class either. As
far as my quick glancing can tell.

On Fri, Oct 21, 2011 at 16:39, bhm <bheem at sbcglobal.net> wrote:

> I'm trying to build a cache for configuration objects for my projects-
> Requirements:
>  1 only one instance per key should be created, either because its absent
>   for a given key or its expired (in that case it will be recreated).
>  2 most common path asking for a value for a key should not block or
> synchronize.
>  3 when a value doesnt exists for a key, one thread should create the
> instance
>   of value and put in the cache, other threads asking for same key should
> wait
>   till thats complete.
>  4 if a value is expired, one thread should (re)create the instance of
> value
>   and other threads asking for same key (when one thread is recreating it)
>   should use expired instance of value and should not block.
>
> My code is largely derived from Christian's reply
>
> http://cs.oswego.edu/pipermail/concurrency-interest/2010-November/007469.html
>
> Please comment or suggest on following, Thanks for you help.
>
>    final ConcurrentMap<K, Object> store = new ConcurrentHashMap<K,
> Object>();
>
>    V get(K key) {
>        Object ret = store.get(key);
>
>        if (ret == null) {
>            CountDownLatch latch = new CountDownLatch(1);
>            ret = store.putIfAbsent(key, latch);
>            if (null == ret) {
>                ret = new Entry<V>(factory.create(key));
>                store.replace(key, ret);
>                latch.countDown();
>            }
>        }
>
>        if (ret instanceof CountDownLatch) {
>            try {((CountDownLatch) ret).await();} // till new value is
> created
>            catch (InterruptedException e) {throw new RuntimeException(e);}
>            ret = store.get(key); // get new value
>        } else {
>            Entry<V> entry = (Entry<V>) ret;
>
>            if (evictionpolicy != null && evictionpolicy.isExpired(entry.v))
> {
>                final AtomicInteger sync = entry.sync;
>
>                if (sync.compareAndSet(0, 1)) { // otherwise retrun old
> value
>                    try {
>                        entry = (Entry<V>) store.get(key); // get again
>                        if (evictionpolicy.isExpired(entry.v)) { //
> double-check
>                            Entry<V> newval = new
> Entry<V>(factory.create(key));
>                            store.replace(key, newval);
>                            ret = newval;
>                        }
>                    }
>                    catch (Throwable catchall) {
>                      catchall.printStackTrace(System.err);//return old
> value
>                    }
>                    finally {sync.set(0);/*unlock*/}
>                }
>            }
>        }
>
>        return ((Entry<V>) ret).v;
>    }
>
>    class Entry<V> {
>        final AtomicInteger sync = new AtomicInteger(0);
>        final V v;
>        Entry(V v) {this.v=v;}
>    }
>
>
> form other discussions about similar topics in the list, I understand there
> are
> libraries to do that (MapMaker etc.), I wanted something specific, small,
> just
> to slove the given problem and avoid generalization.
> I couldnt get jcp's Memoizer to work when cache entries could expire. Its
> good
> for lazy one time load.
>
> Thanks again.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111021/a2003430/attachment.html>

From karmazilla at gmail.com  Fri Oct 21 12:18:43 2011
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Fri, 21 Oct 2011 18:18:43 +0200
Subject: [concurrency-interest] memoizer cache
In-Reply-To: <CACyP5Pdwu3yLUsr+-BefrUQd3FV7TgtgyWaiAhqBX4KAAdzK9w@mail.gmail.com>
References: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
	<CACyP5Pdwu3yLUsr+-BefrUQd3FV7TgtgyWaiAhqBX4KAAdzK9w@mail.gmail.com>
Message-ID: <CACyP5PeDV4Cjaj7BmDLAe+=wTLzJvWmukzbUrz2msixk8xe-dg@mail.gmail.com>

Oh, it seems I overlooked the requirement to not block other threads when a
value is being re-created.

On Fri, Oct 21, 2011 at 18:11, Christian Vest Hansen
<karmazilla at gmail.com>wrote:

>
> One thing that comes to mind is this:
> If you determine that the value you are about to return has expired, then
> you could replace(key, entry, null) and recursively call get(key) again. If
> the replace() call returns `false` then someone got ahead of you and already
> put a null or a new CountDownLatch in there, and the get() call will deal
> with it. Otherwise it will look, to the following get() call, as if the
> entry never existed and get() will create it for you. So you don't really
> need to check the return value of replace() with this approach.
> If you do it this way then I guess you don't need the Entry class either.
> As far as my quick glancing can tell.
>
>
> On Fri, Oct 21, 2011 at 16:39, bhm <bheem at sbcglobal.net> wrote:
>
>> I'm trying to build a cache for configuration objects for my projects-
>> Requirements:
>>  1 only one instance per key should be created, either because its absent
>>   for a given key or its expired (in that case it will be recreated).
>>  2 most common path asking for a value for a key should not block or
>> synchronize.
>>  3 when a value doesnt exists for a key, one thread should create the
>> instance
>>   of value and put in the cache, other threads asking for same key should
>> wait
>>   till thats complete.
>>  4 if a value is expired, one thread should (re)create the instance of
>> value
>>   and other threads asking for same key (when one thread is recreating it)
>>   should use expired instance of value and should not block.
>>
>> My code is largely derived from Christian's reply
>>
>> http://cs.oswego.edu/pipermail/concurrency-interest/2010-November/007469.html
>>
>> Please comment or suggest on following, Thanks for you help.
>>
>>    final ConcurrentMap<K, Object> store = new ConcurrentHashMap<K,
>> Object>();
>>
>>    V get(K key) {
>>        Object ret = store.get(key);
>>
>>        if (ret == null) {
>>            CountDownLatch latch = new CountDownLatch(1);
>>            ret = store.putIfAbsent(key, latch);
>>            if (null == ret) {
>>                ret = new Entry<V>(factory.create(key));
>>                store.replace(key, ret);
>>                latch.countDown();
>>            }
>>        }
>>
>>        if (ret instanceof CountDownLatch) {
>>            try {((CountDownLatch) ret).await();} // till new value is
>> created
>>            catch (InterruptedException e) {throw new RuntimeException(e);}
>>            ret = store.get(key); // get new value
>>        } else {
>>            Entry<V> entry = (Entry<V>) ret;
>>
>>            if (evictionpolicy != null &&
>> evictionpolicy.isExpired(entry.v)) {
>>                final AtomicInteger sync = entry.sync;
>>
>>                if (sync.compareAndSet(0, 1)) { // otherwise retrun old
>> value
>>                    try {
>>                        entry = (Entry<V>) store.get(key); // get again
>>                        if (evictionpolicy.isExpired(entry.v)) { //
>> double-check
>>                            Entry<V> newval = new
>> Entry<V>(factory.create(key));
>>                            store.replace(key, newval);
>>                            ret = newval;
>>                        }
>>                    }
>>                    catch (Throwable catchall) {
>>                      catchall.printStackTrace(System.err);//return old
>> value
>>                    }
>>                    finally {sync.set(0);/*unlock*/}
>>                }
>>            }
>>        }
>>
>>        return ((Entry<V>) ret).v;
>>    }
>>
>>    class Entry<V> {
>>        final AtomicInteger sync = new AtomicInteger(0);
>>        final V v;
>>        Entry(V v) {this.v=v;}
>>    }
>>
>>
>> form other discussions about similar topics in the list, I understand
>> there are
>> libraries to do that (MapMaker etc.), I wanted something specific, small,
>> just
>> to slove the given problem and avoid generalization.
>> I couldnt get jcp's Memoizer to work when cache entries could expire. Its
>> good
>> for lazy one time load.
>>
>> Thanks again.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Venlig hilsen / Kind regards,
> Christian Vest Hansen.
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111021/c7e3be88/attachment.html>

From crazybob at crazybob.org  Fri Oct 21 12:34:20 2011
From: crazybob at crazybob.org (Bob Lee)
Date: Fri, 21 Oct 2011 09:34:20 -0700
Subject: [concurrency-interest] memoizer cache
In-Reply-To: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
References: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
Message-ID: <CAGmsiP7k3V7E=ayfnzpwEqKfJj1Smbwbb55T3f69EyasmgLyGw@mail.gmail.com>

MapMaker is just a few classes if you want to copy it out (MapMaker,
CustomConcurrentHashMap, ComputingConcurrentHashMap, and Function I think).
We used to do that with Guice.

I'm not sure if it supports #4 yet. When an entry expires, all clients
typically wait for the updated value. You could certainly compute a value
and put it in the map though--other clients would still be able to retrieve
the original value during the computation.

Bob

On Fri, Oct 21, 2011 at 7:39 AM, bhm <bheem at sbcglobal.net> wrote:

> I'm trying to build a cache for configuration objects for my projects-
> Requirements:
>  1 only one instance per key should be created, either because its absent
>   for a given key or its expired (in that case it will be recreated).
>  2 most common path asking for a value for a key should not block or
> synchronize.
>  3 when a value doesnt exists for a key, one thread should create the
> instance
>   of value and put in the cache, other threads asking for same key should
> wait
>   till thats complete.
>  4 if a value is expired, one thread should (re)create the instance of
> value
>   and other threads asking for same key (when one thread is recreating it)
>   should use expired instance of value and should not block.
>
> My code is largely derived from Christian's reply
>
> http://cs.oswego.edu/pipermail/concurrency-interest/2010-November/007469.html
>
> Please comment or suggest on following, Thanks for you help.
>
>    final ConcurrentMap<K, Object> store = new ConcurrentHashMap<K,
> Object>();
>
>    V get(K key) {
>        Object ret = store.get(key);
>
>        if (ret == null) {
>            CountDownLatch latch = new CountDownLatch(1);
>            ret = store.putIfAbsent(key, latch);
>            if (null == ret) {
>                ret = new Entry<V>(factory.create(key));
>                store.replace(key, ret);
>                latch.countDown();
>            }
>        }
>
>        if (ret instanceof CountDownLatch) {
>            try {((CountDownLatch) ret).await();} // till new value is
> created
>            catch (InterruptedException e) {throw new RuntimeException(e);}
>            ret = store.get(key); // get new value
>        } else {
>            Entry<V> entry = (Entry<V>) ret;
>
>            if (evictionpolicy != null && evictionpolicy.isExpired(entry.v))
> {
>                final AtomicInteger sync = entry.sync;
>
>                if (sync.compareAndSet(0, 1)) { // otherwise retrun old
> value
>                    try {
>                        entry = (Entry<V>) store.get(key); // get again
>                        if (evictionpolicy.isExpired(entry.v)) { //
> double-check
>                            Entry<V> newval = new
> Entry<V>(factory.create(key));
>                            store.replace(key, newval);
>                            ret = newval;
>                        }
>                    }
>                    catch (Throwable catchall) {
>                      catchall.printStackTrace(System.err);//return old
> value
>                    }
>                    finally {sync.set(0);/*unlock*/}
>                }
>            }
>        }
>
>        return ((Entry<V>) ret).v;
>    }
>
>    class Entry<V> {
>        final AtomicInteger sync = new AtomicInteger(0);
>        final V v;
>        Entry(V v) {this.v=v;}
>    }
>
>
> form other discussions about similar topics in the list, I understand there
> are
> libraries to do that (MapMaker etc.), I wanted something specific, small,
> just
> to slove the given problem and avoid generalization.
> I couldnt get jcp's Memoizer to work when cache entries could expire. Its
> good
> for lazy one time load.
>
> Thanks again.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111021/3d1ed454/attachment-0001.html>

From zhong.j.yu at gmail.com  Fri Oct 21 22:49:40 2011
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Fri, 21 Oct 2011 21:49:40 -0500
Subject: [concurrency-interest] memoizer cache
In-Reply-To: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
References: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
Message-ID: <CACuKZqHe0hT5MAwEx-i=4zP6vpa2ZukmBUK6k5Yz+4CbKsDebg@mail.gmail.com>

I think there's a small problem

[1]       entry = store.get(key);

           if ( expired(entry.v) )

               final AtomicInteger sync = entry.sync;

[2]            if (sync.compareAndSet(0, 1)) { // otherwise retrun old value
                   try {
                       entry = (Entry<V>) store.get(key); // get again
                       if ( expired(entry.v) ) { // double-check
                           Entry<V> newval = new Entry<V>(factory.create(key));
[3]                       store.replace(key, newval);
                           ret = newval;
                       }
                   }
                   finally {sync.set(0);/*unlock*/}

since you insert a new entry at [3], it's possible that two threads
see two different entries at [1], and they are trying to acquire
different locks at [2]. Both may succeed and proceed concurrently. Of
course the chance of this happening is very small. I don't see a
problem if you reuse the entry, so that [2] always sees the same sync.

Zhong Yu

From ted_yu at yahoo.com  Sun Oct 23 15:01:41 2011
From: ted_yu at yahoo.com (Ted Yu)
Date: Sun, 23 Oct 2011 12:01:41 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
Message-ID: <1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>

HBase expects ConcurrentSkipListMap to have good performance.
See the following discussion.

Comments are welcome.

---------- Forwarded message ----------
From: Akash Ashok <thehellmaker at gmail.com>
Date: Sun, Oct 23, 2011 at 2:57 AM
Subject: Re: SILT - nice keyvalue store paper
To: dev at hbase.apache.org


I
 was running some similar tests and came across a surprising finding. I 
compared reads and write on ConcurrentSkipListMap ( which the memstore 
uses) and synchronized TreeMap ( Which was literally treemap 
synchronized). Executed concurrent reads, writes and deletes on both of 
them.
Surprisingly synchronized treeMap performed better, though just slightly
 better, than ConcurrentSkipListMap which KeyValueSkipListSet uses.

Here are the output of a few runs

Sometimes the difference was considerable
Using HBaseMap it took 20438ms
Using TreeMap it took 11613ms
Time Difference:8825ms

And sometimes the difference was negligible
Using HBaseMap it took 13370ms
Using TreeMap it took 9482ms
Time Difference:3888ms

I've attaching the test? java file which I wrote to test it. 
This
 might be a very minor differece but still surprising considering the 
fact that ConcurrentSkipListMap uses fancy 2 level indexes which they 
say improves the deletion performance.

And here are the details about the test run.
100 Threads each fetching 10,000 records
100 threads each adding 10,000 records.
100 threads each deletin 10,000 records
( Reads, Writes and deletes simultaneously )

Cheers,
Akash A
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111023/a6545a32/attachment.html>

From ben_manes at yahoo.com  Sun Oct 23 15:58:11 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Sun, 23 Oct 2011 12:58:11 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
Message-ID: <1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>

There's not enough information to diagnose the cause, but you may want to try one of the CSLM replacements.


https://github.com/mspiegel/lockfreeskiptree
https://github.com/nbronson/snaptree



________________________________
From: Ted Yu <ted_yu at yahoo.com>
To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
Sent: Sunday, October 23, 2011 12:01 PM
Subject: [concurrency-interest] ConcurrentSkipListMap performance


HBase expects ConcurrentSkipListMap to have good performance.
See the following discussion.

Comments are welcome.

---------- Forwarded message ----------
From: Akash Ashok <thehellmaker at gmail.com>
Date: Sun, Oct 23, 2011 at 2:57 AM
Subject: Re: SILT - nice keyvalue store paper
To: dev at hbase.apache.org


I
 was running some similar tests and came across a surprising finding. I 
compared reads and write on ConcurrentSkipListMap ( which the memstore 
uses) and synchronized TreeMap ( Which was literally treemap 
synchronized). Executed concurrent reads, writes and deletes on both of 
them.
Surprisingly synchronized treeMap performed better, though just slightly
 better, than ConcurrentSkipListMap which KeyValueSkipListSet uses.

Here are the output of a few runs

Sometimes the difference was considerable
Using HBaseMap it took 20438ms
Using TreeMap it took 11613ms
Time Difference:8825ms

And sometimes the difference was negligible
Using HBaseMap it took 13370ms
Using TreeMap it took 9482ms
Time Difference:3888ms

I've attaching the test? java file which I wrote to test it. 
This
 might be a very minor differece but still surprising considering the 
fact that ConcurrentSkipListMap uses fancy 2 level indexes which they 
say improves the deletion performance.

And here are the details about the test run.
100 Threads each fetching 10,000 records
100 threads each adding 10,000 records.
100 threads each deletin 10,000 records
( Reads, Writes and deletes simultaneously )

Cheers,
Akash A
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111023/beec7a38/attachment.html>

From davidcholmes at aapt.net.au  Sun Oct 23 18:01:03 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 24 Oct 2011 08:01:03 +1000
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEMEJAAA.davidcholmes@aapt.net.au>

As Ben said, not enough details to be able to interpret or diagnoze. Details
on the test system - how may processors vs. cores vs HT might be useful.

I also don't find 3888ms a negligible difference.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ted Yu
  Sent: Monday, 24 October 2011 5:02 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] ConcurrentSkipListMap performance


  HBase expects ConcurrentSkipListMap to have good performance.
  See the following discussion.

  Comments are welcome.

  ---------- Forwarded message ----------
  From: Akash Ashok <thehellmaker at gmail.com>
  Date: Sun, Oct 23, 2011 at 2:57 AM
  Subject: Re: SILT - nice keyvalue store paper
  To: dev at hbase.apache.org


  I was running some similar tests and came across a surprising finding. I
compared reads and write on ConcurrentSkipListMap ( which the memstore uses)
and synchronized TreeMap ( Which was literally treemap synchronized).
Executed concurrent reads, writes and deletes on both of them.
  Surprisingly synchronized treeMap performed better, though just slightly
better, than ConcurrentSkipListMap which KeyValueSkipListSet uses.

  Here are the output of a few runs

  Sometimes the difference was considerable
  Using HBaseMap it took 20438ms
  Using TreeMap it took 11613ms
  Time Difference:8825ms

  And sometimes the difference was negligible
  Using HBaseMap it took 13370ms
  Using TreeMap it took 9482ms
  Time Difference:3888ms

  I've attaching the test  java file which I wrote to test it.
  This might be a very minor differece but still surprising considering the
fact that ConcurrentSkipListMap uses fancy 2 level indexes which they say
improves the deletion performance.

  And here are the details about the test run.
  100 Threads each fetching 10,000 records
  100 threads each adding 10,000 records.
  100 threads each deletin 10,000 records
  ( Reads, Writes and deletes simultaneously )

  Cheers,
  Akash A
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111024/e6f5e760/attachment.html>

From ted_yu at yahoo.com  Sun Oct 23 20:24:40 2011
From: ted_yu at yahoo.com (Ted Yu)
Date: Sun, 23 Oct 2011 17:24:40 -0700
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
Message-ID: <E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>

Is either or both of the following compatible with Apache license 2.0 ?

Thanks



On Oct 23, 2011, at 12:58 PM, Ben Manes <ben_manes at yahoo.com> wrote:

> There's not enough information to diagnose the cause, but you may want to try one of the CSLM replacements.
> 
> https://github.com/mspiegel/lockfreeskiptree
> https://github.com/nbronson/snaptree
> 
> From: Ted Yu <ted_yu at yahoo.com>
> To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
> Sent: Sunday, October 23, 2011 12:01 PM
> Subject: [concurrency-interest] ConcurrentSkipListMap performance
> 
> HBase expects ConcurrentSkipListMap to have good performance.
> See the following discussion.
> 
> Comments are welcome.
> 
> ---------- Forwarded message ----------
> From: Akash Ashok <thehellmaker at gmail.com>
> Date: Sun, Oct 23, 2011 at 2:57 AM
> Subject: Re: SILT - nice keyvalue store paper
> To: dev at hbase.apache.org
> 
> 
> I was running some similar tests and came across a surprising finding. I compared reads and write on ConcurrentSkipListMap ( which the memstore uses) and synchronized TreeMap ( Which was literally treemap synchronized). Executed concurrent reads, writes and deletes on both of them.
> Surprisingly synchronized treeMap performed better, though just slightly better, than ConcurrentSkipListMap which KeyValueSkipListSet uses.
> 
> Here are the output of a few runs
> 
> Sometimes the difference was considerable
> Using HBaseMap it took 20438ms
> Using TreeMap it took 11613ms
> Time Difference:8825ms
> 
> And sometimes the difference was negligible
> Using HBaseMap it took 13370ms
> Using TreeMap it took 9482ms
> Time Difference:3888ms
> 
> I've attaching the test  java file which I wrote to test it. 
> This might be a very minor differece but still surprising considering the fact that ConcurrentSkipListMap uses fancy 2 level indexes which they say improves the deletion performance.
> 
> And here are the details about the test run.
> 100 Threads each fetching 10,000 records
> 100 threads each adding 10,000 records.
> 100 threads each deletin 10,000 records
> ( Reads, Writes and deletes simultaneously )
> 
> Cheers,
> Akash A
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111023/491e3064/attachment-0001.html>

From dmitry.miltsov at oracle.com  Sun Oct 23 20:50:26 2011
From: dmitry.miltsov at oracle.com (dmitry.miltsov at oracle.com)
Date: Sun, 23 Oct 2011 17:50:26 -0700 (PDT)
Subject: [concurrency-interest] Auto Reply: Concurrency-interest Digest,
	Vol 81, Issue 27
Message-ID: <512a22b3-ac80-416a-8f91-3ed520955cea@default>

This is an auto-replied message.
I'm on vacation from October 24, returning to the office on October 31.

My backup persons are:
java.text, java.util - Yuri Gaevsky;
java.lang - Victor Rudometov;
java.security, javax.security - Paul Rank; 

Please contact my manager Pavel Klodin regarding other issues. 

Thanks,
Dmitry Miltsov


From ben_manes at yahoo.com  Sun Oct 23 21:33:35 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Sun, 23 Oct 2011 18:33:35 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>
Message-ID: <1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>

See the file headers. The Skiptree implementation is public domain. The Snaptree is copyrighted by Stanford University (http://ppl.stanford.edu) and should fall under their policy for academic research. You can contact the author or the lab for licensing details.


________________________________
From: Ted Yu <ted_yu at yahoo.com>
To: Ben Manes <ben_manes at yahoo.com>
Cc: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
Sent: Sunday, October 23, 2011 5:24 PM
Subject: Re: [concurrency-interest] ConcurrentSkipListMap performance


Is either or both of the following compatible with Apache license 2.0 ?

Thanks




On Oct 23, 2011, at 12:58 PM, Ben Manes <ben_manes at yahoo.com> wrote:


There's not enough information to diagnose the cause, but you may want to try one of the CSLM replacements.
>
>
>
>https://github.com/mspiegel/lockfreeskiptree
>https://github.com/nbronson/snaptree
>
>
>
>
>________________________________
>From: Ted Yu <ted_yu at yahoo.com>
>To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
>Sent: Sunday, October 23, 2011 12:01 PM
>Subject: [concurrency-interest] ConcurrentSkipListMap performance
>
>
>HBase expects ConcurrentSkipListMap to have good performance.
>See the following discussion.
>
>Comments are welcome.
>
>---------- Forwarded message ----------
>From: Akash Ashok <thehellmaker at gmail.com>
>Date: Sun, Oct 23, 2011 at 2:57 AM
>Subject: Re: SILT - nice keyvalue store paper
>To: dev at hbase.apache.org
>
>
>I
 was running some similar tests and came across a surprising finding. I 
compared reads and write on ConcurrentSkipListMap ( which the memstore 
uses) and synchronized TreeMap ( Which was literally treemap 
synchronized). Executed concurrent reads, writes and deletes on both of 
them.
>Surprisingly synchronized treeMap performed better, though just slightly
 better, than ConcurrentSkipListMap which KeyValueSkipListSet uses.
>
>Here are the output of a few runs
>
>Sometimes the difference was considerable
>Using HBaseMap it took 20438ms
>Using TreeMap it took 11613ms
>Time Difference:8825ms
>
>And sometimes the difference was negligible
>Using HBaseMap it took 13370ms
>Using TreeMap it took 9482ms
>Time Difference:3888ms
>
>I've attaching the test? java file which I wrote to test it. 
>This
 might be a very minor differece but still surprising considering the 
fact that ConcurrentSkipListMap uses fancy 2 level indexes which they 
say improves the deletion performance.
>
>And here are the details about the test run.
>100 Threads each fetching 10,000 records
>100 threads each adding 10,000 records.
>100 threads each deletin 10,000 records
>( Reads, Writes and deletes simultaneously )
>
>Cheers,
>Akash A
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111023/4f5f2a96/attachment.html>

From ben_manes at yahoo.com  Mon Oct 24 01:52:09 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Sun, 23 Oct 2011 22:52:09 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>
	<1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>
Message-ID: <1319435529.92794.YahooMailNeo@web38804.mail.mud.yahoo.com>

A quick glance at the benchmark [1] indicates that there are a few flaws, such as:?
- the JVM warm-up penalty is being accrued by the CSLM run
- Use thread.yield() to end a request, as otherwise the active thread may be able to run through its time slice without incurring much?lock contention.

[1]?http://osdir.com/ml/attachments/txtBAixyCerpa.txt


________________________________
From: Ben Manes <ben_manes at yahoo.com>
To: Ted Yu <ted_yu at yahoo.com>
Cc: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
Sent: Sunday, October 23, 2011 6:33 PM
Subject: Re: [concurrency-interest] ConcurrentSkipListMap performance


See the file headers. The Skiptree implementation is public domain. The Snaptree is copyrighted by Stanford University (http://ppl.stanford.edu/) and should fall under their policy for academic research. You can contact the author or the lab for licensing details.


________________________________
From: Ted Yu <ted_yu at yahoo.com>
To: Ben Manes <ben_manes at yahoo.com>
Cc: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
Sent: Sunday, October 23, 2011 5:24 PM
Subject: Re: [concurrency-interest] ConcurrentSkipListMap performance


Is either or both of the following compatible with Apache license 2.0 ?

Thanks




On Oct 23, 2011, at 12:58 PM, Ben Manes <ben_manes at yahoo.com> wrote:


There's not enough information to diagnose the cause, but you may want to try one of the CSLM replacements.
>
>
>
>https://github.com/mspiegel/lockfreeskiptree
>https://github.com/nbronson/snaptree
>
>
>
>
>________________________________
>From: Ted Yu <ted_yu at yahoo.com>
>To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
>Sent: Sunday, October 23, 2011 12:01 PM
>Subject: [concurrency-interest] ConcurrentSkipListMap performance
>
>
>HBase expects ConcurrentSkipListMap to have good performance.
>See the following discussion.
>
>Comments are welcome.
>
>---------- Forwarded message ----------
>From: Akash Ashok <thehellmaker at gmail.com>
>Date: Sun, Oct 23, 2011 at 2:57 AM
>Subject: Re: SILT - nice keyvalue store paper
>To: dev at hbase.apache.org
>
>
>I
 was running some similar tests and came across a surprising finding. I 
compared reads and write on ConcurrentSkipListMap ( which the memstore 
uses) and synchronized TreeMap ( Which was literally treemap 
synchronized). Executed concurrent reads, writes and deletes on both of 
them.
>Surprisingly synchronized treeMap performed better, though just slightly
 better, than ConcurrentSkipListMap which KeyValueSkipListSet uses.
>
>Here are the output of a few runs
>
>Sometimes the difference was considerable
>Using HBaseMap it took 20438ms
>Using TreeMap it took 11613ms
>Time Difference:8825ms
>
>And sometimes the difference was negligible
>Using HBaseMap it took 13370ms
>Using TreeMap it took 9482ms
>Time Difference:3888ms
>
>I've attaching the test? java file which I wrote to test it. 
>This
 might be a very minor differece but still surprising considering the 
fact that ConcurrentSkipListMap uses fancy 2 level indexes which they 
say improves the deletion performance.
>
>And here are the details about the test run.
>100 Threads each fetching 10,000 records
>100 threads each adding 10,000 records.
>100 threads each deletin 10,000 records
>( Reads, Writes and deletes simultaneously )
>
>Cheers,
>Akash A
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111023/00852373/attachment-0001.html>

From bheem at sbcglobal.net  Mon Oct 24 09:06:21 2011
From: bheem at sbcglobal.net (bhm)
Date: Mon, 24 Oct 2011 09:06:21 -0400
Subject: [concurrency-interest] memoizer cache
In-Reply-To: <CACwMgat1utp7a_YANi_M6gOosZXyzSW+Zvbxh+aXeGO+wE+_kw@mail.gmail.com>
References: <CACwMgas+6JZtT365hun8zcB1C8TNgOd7LCvGk7aer3rOc8qo0Q@mail.gmail.com>
	<CACuKZqHe0hT5MAwEx-i=4zP6vpa2ZukmBUK6k5Yz+4CbKsDebg@mail.gmail.com>
	<CACwMgat1utp7a_YANi_M6gOosZXyzSW+Zvbxh+aXeGO+wE+_kw@mail.gmail.com>
Message-ID: <CACwMgasFpQrQqtd6nB0vQ7bGgtpO-qSS22sWOPm-E6jXsHE+pQ@mail.gmail.com>

 Thanks everyone for comments and suggestions, I've updated code used
 same sync instance when creating post-expired entry and added another
 impl class that recurs on get and doesnt use Entry class. Thanks.

> On Fri, Oct 21, 2011 at 10:49 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>> I think there's a small problem
>>
>> [1] ? ? ? entry = store.get(key);
>>
>> ? ? ? ? ? if ( expired(entry.v) )
>>
>> ? ? ? ? ? ? ? final AtomicInteger sync = entry.sync;
>>
>> [2] ? ? ? ? ? ?if (sync.compareAndSet(0, 1)) { // otherwise retrun old value
>> ? ? ? ? ? ? ? ? ? try {
>> ? ? ? ? ? ? ? ? ? ? ? entry = (Entry<V>) store.get(key); // get again
>> ? ? ? ? ? ? ? ? ? ? ? if ( expired(entry.v) ) { // double-check
>> ? ? ? ? ? ? ? ? ? ? ? ? ? Entry<V> newval = new Entry<V>(factory.create(key));
>> [3] ? ? ? ? ? ? ? ? ? ? ? store.replace(key, newval);
>> ? ? ? ? ? ? ? ? ? ? ? ? ? ret = newval;
>> ? ? ? ? ? ? ? ? ? ? ? }
>> ? ? ? ? ? ? ? ? ? }
>> ? ? ? ? ? ? ? ? ? finally {sync.set(0);/*unlock*/}
>>
>> since you insert a new entry at [3], it's possible that two threads
>> see two different entries at [1], and they are trying to acquire
>> different locks at [2]. Both may succeed and proceed concurrently. Of
>> course the chance of this happening is very small. I don't see a
>> problem if you reuse the entry, so that [2] always sees the same sync.
>>
>> Zhong Yu
>>
>


From nbronson at cs.stanford.edu  Mon Oct 24 11:54:05 2011
From: nbronson at cs.stanford.edu (Nathan Grasso Bronson)
Date: Mon, 24 Oct 2011 08:54:05 -0700
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>
Message-ID: <CACAD769.4A6BA%nbronson@cs.stanford.edu>

SnapTree has a BSD license

https://github.com/nbronson/snaptree/blob/master/doc/LICENSE

 - Nathan

From:  Ben Manes <ben_manes at yahoo.com>
Reply-To:  Ben Manes <ben_manes at yahoo.com>
Date:  Sun, 23 Oct 2011 18:33:35 -0700 (PDT)
To:  Ted Yu <ted_yu at yahoo.com>
Cc:  "concurrency-interest at cs.oswego.edu"
<concurrency-interest at cs.oswego.edu>
Subject:  Re: [concurrency-interest] ConcurrentSkipListMap performance

See the file headers. The Skiptree implementation is public domain. The
Snaptree is copyrighted by Stanford University (http://ppl.stanford.edu
<http://ppl.stanford.edu/> ) and should fall under their policy for academic
research. You can contact the author or the lab for licensing details.


From: Ted Yu <ted_yu at yahoo.com>
To: Ben Manes <ben_manes at yahoo.com>
Cc: "concurrency-interest at cs.oswego.edu"
<concurrency-interest at cs.oswego.edu>
Sent: Sunday, October 23, 2011 5:24 PM
Subject: Re: [concurrency-interest] ConcurrentSkipListMap performance

Is either or both of the following compatible with Apache license 2.0 ?

Thanks



On Oct 23, 2011, at 12:58 PM, Ben Manes <ben_manes at yahoo.com> wrote:

> There's not enough information to diagnose the cause, but you may want to try
> one of the CSLM replacements.
> 
> https://github.com/mspiegel/lockfreeskiptree
> https://github.com/nbronson/snaptree
> 
> 
> From: Ted Yu <ted_yu at yahoo.com>
> To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
> Sent: Sunday, October 23, 2011 12:01 PM
> Subject: [concurrency-interest] ConcurrentSkipListMap performance
> 
> HBase expects ConcurrentSkipListMap to have good performance.
> See the following discussion.
> 
> Comments are welcome.
> 
> ---------- Forwarded message ----------
> From: Akash Ashok <thehellmaker at gmail.com>
> Date: Sun, Oct 23, 2011 at 2:57 AM
> Subject: Re: SILT - nice keyvalue store paper
> To: dev at hbase.apache.org
> 
> 
> I was running some similar tests and came across a surprising finding. I
> compared reads and write on ConcurrentSkipListMap ( which the memstore uses)
> and synchronized TreeMap ( Which was literally treemap synchronized). Executed
> concurrent reads, writes and deletes on both of them.
> Surprisingly synchronized treeMap performed better, though just slightly
> better, than ConcurrentSkipListMap which KeyValueSkipListSet uses.
> 
> Here are the output of a few runs
> 
> Sometimes the difference was considerable
> Using HBaseMap it took 20438ms
> Using TreeMap it took 11613ms
> Time Difference:8825ms
> 
> And sometimes the difference was negligible
> Using HBaseMap it took 13370ms
> Using TreeMap it took 9482ms
> Time Difference:3888ms
> 
> I've attaching the test  java file which I wrote to test it.
> This might be a very minor differece but still surprising considering the fact
> that ConcurrentSkipListMap uses fancy 2 level indexes which they say improves
> the deletion performance.
> 
> And here are the details about the test run.
> 100 Threads each fetching 10,000 records
> 100 threads each adding 10,000 records.
> 100 threads each deletin 10,000 records
> ( Reads, Writes and deletes simultaneously )
> 
> Cheers,
> Akash A
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


_______________________________________________ Concurrency-interest mailing
list Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111024/62b8e31a/attachment.html>

From nathan.reynolds at oracle.com  Mon Oct 24 15:58:16 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 24 Oct 2011 12:58:16 -0700
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <1319435529.92794.YahooMailNeo@web38804.mail.mud.yahoo.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>
	<1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>
	<1319435529.92794.YahooMailNeo@web38804.mail.mud.yahoo.com>
Message-ID: <4EA5C358.4030401@oracle.com>

There are a lot of important details going on here but I am not an 
expert.  I just know of a few of them.  These details make 
microbenchmarking very difficult.  I would like to hear of other ideas.

JVM warm-up can be tricky.  For example, HotSpot with the -server option 
has 2 optimization passes.  The first pass happens at 1,000 invocations 
or iterations of a loop.  The second pass happens at 10,000 (default) 
invocations or iterations of a loop.  The first pass causes the 
method(s) to be executed as native processor instructions instead of 
being interpreted Java bytecode.  This first pass adds profiling code.  
The second pass takes the profiling results and generates a fully 
optimized method or loop.  This time there isn't any profiling code and 
full speed is achieved.  The optimized code isn't executed until the 
thread exits the loop or method and then re-enters.  Note:  HotSpot does 
have an optimizer logging feature and can be used to determine if the 
optimizer is finished (i.e. by lack of further output).

The test is based on executing a fixed number of operations.  This makes 
it difficult to spot OS scheduler influences.  It also makes it 
difficult to account for ramp up and ramp down.  Both ends of the test 
will not have 300 threads trying to execute at the same time.  Instead, 
you might see some threads finishing within the first few milliseconds 
and the rest of the threads taking longer.  Then, at the end of the 
test, threads will finish and reduce the amount of concurrency.  
Instead, it is better to wait for all of the threads to start running 
and then set a flag to cause all of the threads to reset their 
counters.  Then, run the test for X minutes and set a "quit" flag.  All 
of the threads stop executing and report their counters as results.

In order to reduce noise, the threads and JVM process should be run at 
the highest CPU, disk and network priorities allowed by the OS.  
Background processes (including your own interaction) with the 
environment can skew the results.  Each test should be run multiple 
times (10 is a minimum).  Statistics on the results should be calculated 
to ensure that the results are significantly different.

Multiple passes should be done with a various number of threads.  The 
throughput graph can reveal other significant information with the test 
harness and the code.  It could be that TreeMap performs better at some 
concurrency levels than others.

A full GC should be forced before the test happens so that each test 
pass starts at the same memory usage.  This can be done by calling 
"System.gc()" in a loop and monitoring the GarbageCollectorMXBean.  The 
collection count should increase before the loop exits.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 10/23/2011 10:52 PM, Ben Manes wrote:
> A quick glance at the benchmark [1] indicates that there are a few 
> flaws, such as:
> - the JVM warm-up penalty is being accrued by the CSLM run
> - Use thread.yield() to end a request, as otherwise the active thread 
> may be able to run through its time slice without incurring much lock 
> contention.
>
> [1] http://osdir.com/ml/attachments/txtBAixyCerpa.txt
>
> ------------------------------------------------------------------------
> *From:* Ben Manes <ben_manes at yahoo.com>
> *To:* Ted Yu <ted_yu at yahoo.com>
> *Cc:* "concurrency-interest at cs.oswego.edu" 
> <concurrency-interest at cs.oswego.edu>
> *Sent:* Sunday, October 23, 2011 6:33 PM
> *Subject:* Re: [concurrency-interest] ConcurrentSkipListMap performance
>
> See the file headers. The Skiptree implementation is public domain. 
> The Snaptree is copyrighted by Stanford University 
> (http://ppl.stanford.edu/) and should fall under their policy for 
> academic research. You can contact the author or the lab for licensing 
> details.
>
> ------------------------------------------------------------------------
> *From:* Ted Yu <ted_yu at yahoo.com>
> *To:* Ben Manes <ben_manes at yahoo.com>
> *Cc:* "concurrency-interest at cs.oswego.edu" 
> <concurrency-interest at cs.oswego.edu>
> *Sent:* Sunday, October 23, 2011 5:24 PM
> *Subject:* Re: [concurrency-interest] ConcurrentSkipListMap performance
>
> Is either or both of the following compatible with Apache license 2.0 ?
>
> Thanks
>
>
>
> On Oct 23, 2011, at 12:58 PM, Ben Manes <ben_manes at yahoo.com 
> <mailto:ben_manes at yahoo.com>> wrote:
>
>> There's not enough information to diagnose the cause, but you may 
>> want to try one of the CSLM replacements.
>>
>> https://github.com/mspiegel/lockfreeskiptree
>> https://github.com/nbronson/snaptree
>>
>> ------------------------------------------------------------------------
>> *From:* Ted Yu <ted_yu at yahoo.com <mailto:ted_yu at yahoo.com>>
>> *To:* "concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>" 
>> <concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>>
>> *Sent:* Sunday, October 23, 2011 12:01 PM
>> *Subject:* [concurrency-interest] ConcurrentSkipListMap performance
>>
>> HBase expects ConcurrentSkipListMap to have good performance.
>> See the following discussion.
>>
>> Comments are welcome.
>>
>> ---------- Forwarded message ----------
>> From: *Akash Ashok* <thehellmaker at gmail.com 
>> <mailto:thehellmaker at gmail.com>>
>> Date: Sun, Oct 23, 2011 at 2:57 AM
>> Subject: Re: SILT - nice keyvalue store paper
>> To: dev at hbase.apache.org <mailto:dev at hbase.apache.org>
>>
>>
>> I was running some similar tests and came across a surprising 
>> finding. I compared reads and write on ConcurrentSkipListMap ( which 
>> the memstore uses) and synchronized TreeMap ( Which was literally 
>> treemap synchronized). Executed concurrent reads, writes and deletes 
>> on both of them.
>> Surprisingly synchronized treeMap performed better, though just 
>> slightly better, than ConcurrentSkipListMap which KeyValueSkipListSet 
>> uses.
>>
>> Here are the output of a few runs
>>
>> Sometimes the difference was considerable
>> Using HBaseMap it took 20438ms
>> Using TreeMap it took 11613ms
>> Time Difference:8825ms
>>
>> And sometimes the difference was negligible
>> Using HBaseMap it took 13370ms
>> Using TreeMap it took 9482ms
>> Time Difference:3888ms
>>
>> I've attaching the test  java file which I wrote to test it.
>> This might be a very minor differece but still surprising considering 
>> the fact that ConcurrentSkipListMap uses fancy 2 level indexes which 
>> they say improves the deletion performance.
>>
>> And here are the details about the test run.
>> 100 Threads each fetching 10,000 records
>> 100 threads each adding 10,000 records.
>> 100 threads each deletin 10,000 records
>> ( Reads, Writes and deletes simultaneously )
>>
>> Cheers,
>> Akash A
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111024/223124d8/attachment-0001.html>

From nathan.reynolds at oracle.com  Mon Oct 24 16:20:49 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 24 Oct 2011 13:20:49 -0700
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <1319435529.92794.YahooMailNeo@web38804.mail.mud.yahoo.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>
	<1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>
	<1319435529.92794.YahooMailNeo@web38804.mail.mud.yahoo.com>
Message-ID: <4EA5C8A1.3050900@oracle.com>

There are a lot of important details going on here but I am not an 
expert.  I just know of a few of them.  These details make 
microbenchmarking very difficult.  I would like to hear of other ideas.

JVM warm-up can be tricky.  For example, HotSpot with the -server option 
has 2 optimization passes.  The first pass happens at 1,000 invocations 
or iterations of a loop.  The second pass happens at 10,000 (default) 
invocations or iterations of a loop.  The first pass causes the 
method(s) to be executed as native processor instructions instead of 
being interpreted Java bytecode.  This first pass adds profiling code.  
The second pass takes the profiling results and generates a fully 
optimized method or loop.  This time there isn't any profiling code and 
full speed is achieved.  The optimized code isn't executed until the 
thread exits the loop or method and then re-enters.  Note:  HotSpot does 
have an optimizer logging feature and can be used to determine if the 
optimizer is finished (i.e. by lack of further output).

The test is based on executing a fixed number of operations.  This makes 
it difficult to spot OS scheduler influences.  It also makes it 
difficult to account for ramp up and ramp down.  Both ends of the test 
will not have 300 threads trying to execute at the same time.  Instead, 
you might see some threads finishing within the first few milliseconds 
and the rest of the threads taking longer.  Then, at the end of the 
test, threads will finish and reduce the amount of concurrency.  
Instead, it is better to wait for all of the threads to start running 
and then set a flag to cause all of the threads to reset their 
counters.  Then, run the test for X minutes and set a "quit" flag.  All 
of the threads stop executing and report their counters as results.

In order to reduce noise, the threads and JVM process should be run at 
the highest CPU, disk and network priorities allowed by the OS.  
Background processes (including your own interaction) with the 
environment can skew the results.  Each test should be run multiple 
times (10 is a minimum).  Statistics on the results should be calculated 
to ensure that the results are significantly different.

Multiple passes should be done with a various number of threads.  The 
throughput graph can reveal other significant information with the test 
harness and the code.  It could be that TreeMap performs better at some 
concurrency levels than others.

A full GC should be forced before the test happens so that each test 
pass starts at the same memory usage.  This can be done by calling 
"System.gc()" in a loop and monitoring the GarbageCollectorMXBean.  The 
collection count should increase before the loop exits.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 10/23/2011 10:52 PM, Ben Manes wrote:
> A quick glance at the benchmark [1] indicates that there are a few 
> flaws, such as:
> - the JVM warm-up penalty is being accrued by the CSLM run
> - Use thread.yield() to end a request, as otherwise the active thread 
> may be able to run through its time slice without incurring much lock 
> contention.
>
> [1] http://osdir.com/ml/attachments/txtBAixyCerpa.txt
>
> ------------------------------------------------------------------------
> *From:* Ben Manes <ben_manes at yahoo.com>
> *To:* Ted Yu <ted_yu at yahoo.com>
> *Cc:* "concurrency-interest at cs.oswego.edu" 
> <concurrency-interest at cs.oswego.edu>
> *Sent:* Sunday, October 23, 2011 6:33 PM
> *Subject:* Re: [concurrency-interest] ConcurrentSkipListMap performance
>
> See the file headers. The Skiptree implementation is public domain. 
> The Snaptree is copyrighted by Stanford University 
> (http://ppl.stanford.edu/) and should fall under their policy for 
> academic research. You can contact the author or the lab for licensing 
> details.
>
> ------------------------------------------------------------------------
> *From:* Ted Yu <ted_yu at yahoo.com>
> *To:* Ben Manes <ben_manes at yahoo.com>
> *Cc:* "concurrency-interest at cs.oswego.edu" 
> <concurrency-interest at cs.oswego.edu>
> *Sent:* Sunday, October 23, 2011 5:24 PM
> *Subject:* Re: [concurrency-interest] ConcurrentSkipListMap performance
>
> Is either or both of the following compatible with Apache license 2.0 ?
>
> Thanks
>
>
>
> On Oct 23, 2011, at 12:58 PM, Ben Manes <ben_manes at yahoo.com 
> <mailto:ben_manes at yahoo.com>> wrote:
>
>> There's not enough information to diagnose the cause, but you may 
>> want to try one of the CSLM replacements.
>>
>> https://github.com/mspiegel/lockfreeskiptree
>> https://github.com/nbronson/snaptree
>>
>> ------------------------------------------------------------------------
>> *From:* Ted Yu <ted_yu at yahoo.com <mailto:ted_yu at yahoo.com>>
>> *To:* "concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>" 
>> <concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>>
>> *Sent:* Sunday, October 23, 2011 12:01 PM
>> *Subject:* [concurrency-interest] ConcurrentSkipListMap performance
>>
>> HBase expects ConcurrentSkipListMap to have good performance.
>> See the following discussion.
>>
>> Comments are welcome.
>>
>> ---------- Forwarded message ----------
>> From: *Akash Ashok* <thehellmaker at gmail.com 
>> <mailto:thehellmaker at gmail.com>>
>> Date: Sun, Oct 23, 2011 at 2:57 AM
>> Subject: Re: SILT - nice keyvalue store paper
>> To: dev at hbase.apache.org <mailto:dev at hbase.apache.org>
>>
>>
>> I was running some similar tests and came across a surprising 
>> finding. I compared reads and write on ConcurrentSkipListMap ( which 
>> the memstore uses) and synchronized TreeMap ( Which was literally 
>> treemap synchronized). Executed concurrent reads, writes and deletes 
>> on both of them.
>> Surprisingly synchronized treeMap performed better, though just 
>> slightly better, than ConcurrentSkipListMap which KeyValueSkipListSet 
>> uses.
>>
>> Here are the output of a few runs
>>
>> Sometimes the difference was considerable
>> Using HBaseMap it took 20438ms
>> Using TreeMap it took 11613ms
>> Time Difference:8825ms
>>
>> And sometimes the difference was negligible
>> Using HBaseMap it took 13370ms
>> Using TreeMap it took 9482ms
>> Time Difference:3888ms
>>
>> I've attaching the test  java file which I wrote to test it.
>> This might be a very minor differece but still surprising considering 
>> the fact that ConcurrentSkipListMap uses fancy 2 level indexes which 
>> they say improves the deletion performance.
>>
>> And here are the details about the test run.
>> 100 Threads each fetching 10,000 records
>> 100 threads each adding 10,000 records.
>> 100 threads each deletin 10,000 records
>> ( Reads, Writes and deletes simultaneously )
>>
>> Cheers,
>> Akash A
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111024/b33d0121/attachment-0001.html>

From blair at orcaware.com  Mon Oct 24 16:40:56 2011
From: blair at orcaware.com (Blair Zajac)
Date: Mon, 24 Oct 2011 13:40:56 -0700
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <4EA5C8A1.3050900@oracle.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>
	<1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>
	<1319435529.92794.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<4EA5C8A1.3050900@oracle.com>
Message-ID: <4EA5CD58.1080403@orcaware.com>

On 10/24/2011 01:20 PM, Nathan Reynolds wrote:
> There are a lot of important details going on here but I am not an
> expert. I just know of a few of them. These details make
> microbenchmarking very difficult. I would like to hear of other ideas.
>
> JVM warm-up can be tricky. For example, HotSpot with the -server option
> has 2 optimization passes. The first pass happens at 1,000 invocations
> or iterations of a loop. The second pass happens at 10,000 (default)
> invocations or iterations of a loop. The first pass causes the method(s)
> to be executed as native processor instructions instead of being
> interpreted Java bytecode. This first pass adds profiling code. The
> second pass takes the profiling results and generates a fully optimized
> method or loop. This time there isn't any profiling code and full speed
> is achieved. The optimized code isn't executed until the thread exits
> the loop or method and then re-enters. Note: HotSpot does have an
> optimizer logging feature and can be used to determine if the optimizer
> is finished (i.e. by lack of further output).

For these issues, and running a GC as you mention below, running ones 
benchmark using Google Caliper will take care of a few of them:

http://code.google.com/p/caliper/

Regards,
Blair

From ross.mcilroy at gmail.com  Tue Oct 25 04:45:17 2011
From: ross.mcilroy at gmail.com (Ross McIlroy)
Date: Tue, 25 Oct 2011 09:45:17 +0100
Subject: [concurrency-interest] CFP: Runtime Environments, Systems,
 Layering and Virtual Environments at ASPLOS 2012
Message-ID: <CABaC1FH0225iKNHTNBwWgCZAYZPzN-tE4mJuBLmuWMA4YGS9iw@mail.gmail.com>

Please consider publishing work-in-progress OS or language based
virtual machine research at the following workshop:

--- CALL FOR PAPERS ---

Runtime Environments, Systems, Layering and Virtualized
Environments (RESoLVE'12)

Workshop at ASPLOS 2012, London, UK
March 3rd, 2012
http://www.dcs.gla.ac.uk/conferences/resolve12


The second workshop on Runtime Environments, Systems,
Layering, and Virtualized Environments (RESoLVE'12) aims to
brings together researchers in both the OS and language
level virtual machine communities to exchange ideas and
experiences and to discuss how these separate layers can
take advantage of each others' services.

This workshop will discuss work-in-progress research around
how these layers interact and complement each other, and how
best to support new software architectures. Topics of
interest include:

* better structuring / communication of services and
 divisions of labor
* trade-offs in the boundary between trusted and untrusted
 bases and mechanisms to provide information / feedback
 across the layers
* approaches for particular services (e.g. memory management
 / garbage collection / synchronization / signalling /
 scheduling)
* prototypes demonstrating combinations of SW- and HW-based
 techniques to provide better isolation, scaling, and
 quality-of-service
* enabling legacy systems to more readily take advantage of
 new hardware and software capabilities through virtual
 appliances and services
* visualization / introspection techniques for understanding
 the resulting systems

IMPORTANT DATES

Submission deadline: 3 Dec 2011
Notification: 14 Jan 2012
Camera-ready deadline: 4 Feb 2012
Workshop: 3 Mar 2012

SUBMISSIONS

We solicit papers for informal, non-archival
proceedings. Papers should be 6-8 pages, in the SIGPLAN
conference format. Full details are available on the
workshop website at
http://www.dcs.gla.ac.uk/conferences/resolve12
Also note that several travel grants are available for student
paper presentations.

PROGRAM COMMITTEE

Ross McIlroy (co-chair, Google, UK)
Jeremy Singer (co-chair, University of Glasgow, UK)
Alex Garthwaite (VMWare, USA)
Tim Harris (Microsoft Research, UK)
Chris Hawblitzel (Microsoft Research, USA)
Matt Horsnell (ARM, UK)
Tomas Kalibera (University of Kent, UK)
Stephen Kell (University of Oxford, UK)
Orran Krieger
Chandra Krintz (University of California, Santa Barbara, USA)
Geoffrey Lefebvre (Ericsson, Canada)
Ian Rogers (Google, USA)
Ian Sommerville (St Andrews, UK)
Bennet Yee (Google, USA)
Olivier Zendra (INRIA, France)

SPONSORS

VMWare - http://www.vmware.com

From thorsten.moeller at unibas.ch  Tue Oct 25 05:13:42 2011
From: thorsten.moeller at unibas.ch (=?iso-8859-1?Q?Thorsten_M=F6ller?=)
Date: Tue, 25 Oct 2011 11:13:42 +0200
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <4EA5C8A1.3050900@oracle.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>
	<1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>
	<1319435529.92794.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<4EA5C8A1.3050900@oracle.com>
Message-ID: <8CC4C6D7-1EB5-4919-9BE9-C165A7DE3FC5@unibas.ch>


Am 24.10.2011 um 22:20 schrieb Nathan Reynolds:

> Note:  HotSpot does have an optimizer logging feature and can be used to determine if the optimizer is finished (i.e. by lack of further output).

Interesting - wasn't aware of this yet. Could you detail how it can be used. Seems that it is a JVM parameter [1]?

Thorsten

[1] http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html



From dmitry.miltsov at oracle.com  Tue Oct 25 12:05:04 2011
From: dmitry.miltsov at oracle.com (dmitry.miltsov at oracle.com)
Date: Tue, 25 Oct 2011 09:05:04 -0700 (PDT)
Subject: [concurrency-interest] Auto Reply: Concurrency-interest Digest,
	Vol 81, Issue 32
Message-ID: <43683c34-b9a5-420a-befe-097c550526ed@default>

This is an auto-replied message.
I'm on vacation from October 24, returning to the office on October 31.

My backup persons are:
java.text, java.util - Yuri Gaevsky;
java.lang - Victor Rudometov;
java.security, javax.security - Paul Rank; 

Please contact my manager Pavel Klodin regarding other issues. 

Thanks,
Dmitry Miltsov


From nathan.reynolds at oracle.com  Tue Oct 25 12:15:42 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 25 Oct 2011 09:15:42 -0700
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <8CC4C6D7-1EB5-4919-9BE9-C165A7DE3FC5@unibas.ch>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>
	<1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>
	<1319435529.92794.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<4EA5C8A1.3050900@oracle.com>
	<8CC4C6D7-1EB5-4919-9BE9-C165A7DE3FC5@unibas.ch>
Message-ID: <4EA6E0AE.1060502@oracle.com>

I turn on HotSpot's -XX:-PrintCompliation and run the warmup and test.  
If I see any messages during the actual test (after warmup), then I know 
I didn't warmup the JVM long enough.

The parameter -XX:CompileThreshold (i.e. optimizer threshold) defaults 
to 10,000 when using the "-server" flag.  I find that if I run the 
method under test and all of the test harness code 11,000 times then I 
don't see any compiler (optimizer) messages during my test.  I also put 
a Thread.sleep after warmup but before running the test.  This gives 
time and CPU resources to the compiler (optimizer) threads.

Getting this right was very tricky.  I am still not confident that I 
have mastered it yet.  I should really do some harness tests to gain 
more confidence.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 10/25/2011 2:13 AM, Thorsten M?ller wrote:
> Am 24.10.2011 um 22:20 schrieb Nathan Reynolds:
>
>> Note:  HotSpot does have an optimizer logging feature and can be used to determine if the optimizer is finished (i.e. by lack of further output).
> Interesting - wasn't aware of this yet. Could you detail how it can be used. Seems that it is a JVM parameter [1]?
>
> Thorsten
>
> [1] http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111025/1cb174cb/attachment.html>

From mlists at juma.me.uk  Tue Oct 25 13:37:57 2011
From: mlists at juma.me.uk (Ismael Juma)
Date: Tue, 25 Oct 2011 18:37:57 +0100
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <4EA5C8A1.3050900@oracle.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<1319399891.95503.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<E957D9FA-0070-4285-932F-81C94E55C5AB@yahoo.com>
	<1319420015.84643.YahooMailNeo@web38802.mail.mud.yahoo.com>
	<1319435529.92794.YahooMailNeo@web38804.mail.mud.yahoo.com>
	<4EA5C8A1.3050900@oracle.com>
Message-ID: <CAD5tkZYJ0Apo9SLZ2oRHtNEwTL0i5rqgdrPcqwqmO0siphKvgA@mail.gmail.com>

On Mon, Oct 24, 2011 at 9:20 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

> JVM warm-up can be tricky.  For example, HotSpot with the -server option
> has 2 optimization passes.  The first pass happens at 1,000 invocations or
> iterations of a loop.  The second pass happens at 10,000 (default)
> invocations or iterations of a loop.
>

That sounds like TieredCompilation, which is not enabled by default with
-server as far as I know.

Best,
Ismael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111025/6079e2f7/attachment.html>

From hawkinsp at cs.stanford.edu  Tue Oct 25 21:20:09 2011
From: hawkinsp at cs.stanford.edu (Peter Hawkins)
Date: Tue, 25 Oct 2011 18:20:09 -0700
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
Message-ID: <CAHV0EmaH5e1YA4G+Tvtr5fcx7NCZ9M=LCgnksE+2PR1ZvkhhZQ@mail.gmail.com>

Hi...

Just out of interest, why does HBase need an ordered concurrent map?
Is there a description of what the relevant module does somewhere?

I was looking for users of ConcurrentSkipListMap via a Google code
search recently but didn't find an awful lot; on a cursory examination
most of the clients I found didn't actually need concurrent ordered
iteration and arguably would have performed better with a
ConcurrentHashMap.

Cheers,
Peter

On Sun, Oct 23, 2011 at 12:01 PM, Ted Yu <ted_yu at yahoo.com> wrote:
> HBase expects ConcurrentSkipListMap to have good performance.
> See the following discussion.
>
> Comments are welcome.
>
> ---------- Forwarded message ----------
> From: Akash Ashok <thehellmaker at gmail.com>
> Date: Sun, Oct 23, 2011 at 2:57 AM
> Subject: Re: SILT - nice keyvalue store paper
> To: dev at hbase.apache.org
>
>
> I was running some similar tests and came across a surprising finding. I
> compared reads and write on ConcurrentSkipListMap ( which the memstore uses)
> and synchronized TreeMap ( Which was literally treemap synchronized).
> Executed concurrent reads, writes and deletes on both of them.
> Surprisingly synchronized treeMap performed better, though just slightly
> better, than ConcurrentSkipListMap which KeyValueSkipListSet uses.
>
> Here are the output of a few runs
>
> Sometimes the difference was considerable
> Using HBaseMap it took 20438ms
> Using TreeMap it took 11613ms
> Time Difference:8825ms
>
> And sometimes the difference was negligible
> Using HBaseMap it took 13370ms
> Using TreeMap it took 9482ms
> Time Difference:3888ms
>
> I've attaching the test? java file which I wrote to test it.
> This might be a very minor differece but still surprising considering the
> fact that ConcurrentSkipListMap uses fancy 2 level indexes which they say
> improves the deletion performance.
>
> And here are the details about the test run.
> 100 Threads each fetching 10,000 records
> 100 threads each adding 10,000 records.
> 100 threads each deletin 10,000 records
> ( Reads, Writes and deletes simultaneously )
>
> Cheers,
> Akash A
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From ted_yu at yahoo.com  Tue Oct 25 22:03:17 2011
From: ted_yu at yahoo.com (Ted Yu)
Date: Tue, 25 Oct 2011 19:03:17 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentSkipListMap performance
In-Reply-To: <CAHV0EmaH5e1YA4G+Tvtr5fcx7NCZ9M=LCgnksE+2PR1ZvkhhZQ@mail.gmail.com>
References: <mailman.1.1319299200.16128.concurrency-interest@cs.oswego.edu>
	<1319396501.63980.YahooMailNeo@web30805.mail.mud.yahoo.com>
	<CAHV0EmaH5e1YA4G+Tvtr5fcx7NCZ9M=LCgnksE+2PR1ZvkhhZQ@mail.gmail.com>
Message-ID: <1319594597.62219.YahooMailNeo@web30804.mail.mud.yahoo.com>

Peter:
We use the following in MemStore so that we can narrow our search:
????? SortedSet<KeyValue> ss = kvset.tailSet(firstKv);
where kvset is KeyValueSkipListSet which is built on ConcurrentSkipListMap

If you're really interested, see http://hbase.apache.org/book/regions.arch.html 8.6.4.1
You can also get the source code by following http://hbase.apache.org/source-repository.html

Cheers



________________________________
From: Peter Hawkins <hawkinsp at cs.stanford.edu>
To: Ted Yu <ted_yu at yahoo.com>
Cc: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
Sent: Tuesday, October 25, 2011 6:20 PM
Subject: Re: [concurrency-interest] ConcurrentSkipListMap performance

Hi...

Just out of interest, why does HBase need an ordered concurrent map?
Is there a description of what the relevant module does somewhere?

I was looking for users of ConcurrentSkipListMap via a Google code
search recently but didn't find an awful lot; on a cursory examination
most of the clients I found didn't actually need concurrent ordered
iteration and arguably would have performed better with a
ConcurrentHashMap.

Cheers,
Peter

On Sun, Oct 23, 2011 at 12:01 PM, Ted Yu <ted_yu at yahoo.com> wrote:
> HBase expects ConcurrentSkipListMap to have good performance.
> See the following discussion.
>
> Comments are welcome.
>
> ---------- Forwarded message ----------
> From: Akash Ashok <thehellmaker at gmail.com>
> Date: Sun, Oct 23, 2011 at 2:57 AM
> Subject: Re: SILT - nice keyvalue store paper
> To: dev at hbase.apache.org
>
>
> I was running some similar tests and came across a surprising finding. I
> compared reads and write on ConcurrentSkipListMap ( which the memstore uses)
> and synchronized TreeMap ( Which was literally treemap synchronized).
> Executed concurrent reads, writes and deletes on both of them.
> Surprisingly synchronized treeMap performed better, though just slightly
> better, than ConcurrentSkipListMap which KeyValueSkipListSet uses.
>
> Here are the output of a few runs
>
> Sometimes the difference was considerable
> Using HBaseMap it took 20438ms
> Using TreeMap it took 11613ms
> Time Difference:8825ms
>
> And sometimes the difference was negligible
> Using HBaseMap it took 13370ms
> Using TreeMap it took 9482ms
> Time Difference:3888ms
>
> I've attaching the test? java file which I wrote to test it.
> This might be a very minor differece but still surprising considering the
> fact that ConcurrentSkipListMap uses fancy 2 level indexes which they say
> improves the deletion performance.
>
> And here are the details about the test run.
> 100 Threads each fetching 10,000 records
> 100 threads each adding 10,000 records.
> 100 threads each deletin 10,000 records
> ( Reads, Writes and deletes simultaneously )
>
> Cheers,
> Akash A
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111025/c2de4841/attachment-0001.html>

From martinrb at google.com  Sun Oct 30 00:19:39 2011
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 29 Oct 2011 21:19:39 -0700
Subject: [concurrency-interest] AtomicXXX.lazySet and happens-before
	reasoning
In-Reply-To: <4E85AFB2.3070804@cs.oswego.edu>
References: <CAOwENi+Z5=TcukpoAHp-ROaMuNuRM6iG-igOy43TjEJtRm0ytg@mail.gmail.com>
	<4E85AFB2.3070804@cs.oswego.edu>
Message-ID: <CA+kOe093pVxAK7AjzcscyAnxKT5eVxryn0KM-c=FA5EHKWt98w@mail.gmail.com>

On Fri, Sep 30, 2011 at 05:01, Doug Lea <dl at cs.oswego.edu> wrote:

> On 09/30/11 07:33, Ruslan Cheremin wrote:
>
>> What does mean AtomicXXX.lazySet(value) method in terms of happens-before
>> edges, used in most of JMM reasoning? The javadocs is pure on it, and
>> Sun bug 6275329 states:
>>
>
> Because lazySet was introduced after the JMM spec, the
> specs are not fully integrated into JLS chapter 17,
> and so had to be phrased in a legal but unsatisfying way.
> Doing this better requires a JMM revision, which
> is unlikely to happen any time soon.


But there is one obvious thing you can easily do today - update the JSR-133
Cookbook for Compiler Writers to add lazySet to all of those pretty tables,
which at least will make your intent clear (or at least, clearer (for JDK
implementers) than the j.u.c.atomic specs).

http://gee.cs.oswego.edu/dl/jmm/cookbook.html

Martin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20111029/2da4fd25/attachment.html>

