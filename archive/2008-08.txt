From invite-2349389293 at konnects.com  Sat Aug  2 01:26:36 2008
From: invite-2349389293 at konnects.com (Michael McGrady)
Date: Fri,  1 Aug 2008 22:26:36 -0700 (PDT)
Subject: [concurrency-interest] I'm inviting you to join my network
Message-ID: <167454277.1217654796913.JavaMail.tomcat4@web01.konnects.com>


   
   I've added you as a professional contact on Konnects.
   
   Konnects is a Business Networking site where you are able to network, share referrals and connect with other professionals.
   
   Here is the link: https://www.konnects.com/join/bhfcbahgaxahbaab
   
   Konnects is different than other professional sites because their platform gives you your own professional space on line and provides you more ways to stay connected with others with access to 1000's of communities based on interests, discussion groups, blogs, event postings and much more.
   
   

   
   -- Michael
   
   
   You have received this notification because Michael McGrady sent you an invitation to join Konnects.

( If you would not like to receive further invitations from Michael, then follow this link to opt-out: https://www.konnects.com/optout/bhfcbahgaxahbaab )

	Thanks,

	Michael

Konnects, Inc. P.O. Box 1642 Tacoma, WA 98402, USA

	
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080801/31c557b5/attachment.html>

From invite-8342386291 at konnects.com  Wed Aug  6 08:45:40 2008
From: invite-8342386291 at konnects.com (Michael McGrady)
Date: Wed,  6 Aug 2008 05:45:40 -0700 (PDT)
Subject: [concurrency-interest] Your invite from Michael McGrady is about to
	run out
Message-ID: <18677954.1218026740220.JavaMail.root@web02>


    Recently Michael McGrady sent you an invite to become part of Michael?s professional network on Konnects.

	Your invite is going to run out soon. 

	Here is the link to accept Michael?s invite:    
	
	https://www.konnects.com/join/hafcbahgaxbfcdbb

	Signing up is free and only takes one step.


You have received this notification because Michael McGrady sent you an invitation to join their online professional network.

( If you would not like to receive further invitations from Michael, then follow this link to opt-out: https://www.konnects.com/optout/hafcbahgaxbfcdbb )

Konnects, Inc.
P.O. Box 1642 Tacoma, WA 98402, USA

   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080806/59e3d9c0/attachment.html>

From bean at alcatel-lucent.com  Wed Aug  6 20:10:21 2008
From: bean at alcatel-lucent.com (Mike Bean)
Date: Wed, 06 Aug 2008 17:10:21 -0700
Subject: [concurrency-interest] Locking a ConcurrentHashSet in a
	ConcurrentHasHMap
Message-ID: <489A3D6D.7080404@alcatel-lucent.com>

Experts,

I was wondering if anyone had some thoughts on how to avoid a lock to 
allow removal of a ConcurrentHashSet from a ConcurrentHashMap.  Context 
is an index manager with these two calls:

    public void index( Normal name, Value value, Entry entry )
    {
        if ( !m_indexingEnabled )
            return;

        ConcurrentHashMap<String,ConcurrentHashSet<Entry>> index = 
getIndex( name );
        if ( index != null )
        {
            String valueName = value.getAsString();
            while( true )
            {
                ConcurrentHashSet<Entry> set = index.get( valueName );
                if ( set == null )
                {
                    ConcurrentHashSet<Entry> newSet = new 
ConcurrentHashSet<Entry>();
                    newSet.add( entry );
                    if ( index.putIfAbsent( valueName, newSet ) == null )
                    {
                        break;
                    }
                }
                else
                {
                    synchronized ( set )  //!!! make sure we don't add 
entry to set while another thread removes empty set from index
                    {
                        if ( set.size() == 0 )
                        {
                            continue;
                        }
                        set.add( entry );
                        break;
                    }
                }
            }
        }
    }

    public void unindex( Normal name, Value value, Entry entry )
    {
        if ( !m_indexingEnabled )
            return;

        ConcurrentHashMap<String,ConcurrentHashSet<Entry>> index = 
getIndex( name );
        if ( index != null )
        {
            String valueName = value.getAsString();
            ConcurrentHashSet<Entry> set = index.get( valueName );
            if ( set != null )
            {
                synchronized ( set )  //!!! make sure we don't remove 
set from index while another thread adds entry to set
                {
                    set.remove( entry );
                    if ( set.size() == 0 )
                    {
                        index.remove( valueName, set );
                    }
                }
            }
        }
    }

Is there a better way to prevent one thread from updating the set while 
another thread is trying to remove the set from the map than the lock?.

I'm also concerned at the memory cost of a ConcurrentHashSet for a 
single reference to an Entry object but the indexing and unindexing 
entries gets more complex with the map containing an Entry or a 
ConcurrentHashSet of Entries.

Methods not shown from our index manager include queries that never fail 
as the map and set of entries change.

Thanks for all the work in this area,

Mike

-- 
Michael Bean (Mike)
Alcatel-Lucent
AAA Product Group
3461 Robin Ln, Ste 1
Cameron Park, CA 95682
Email: bean at alcatel-lucent.com
Phone: 530 672 7577
Fax: 530 676 3442


From ben_manes at yahoo.com  Wed Aug  6 21:55:46 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Wed, 6 Aug 2008 18:55:46 -0700 (PDT)
Subject: [concurrency-interest] Locking a ConcurrentHashSet in a
	ConcurrentHasHMap
Message-ID: <179559.80130.qm@web38803.mail.mud.yahoo.com>

If the race was, for example, against volatile data like caches then the entries could be recreated cheaply and the this complexity isn't necessary.  This is easiest way to avoid locking.  :-)

If its not an acceptable loss, then you could use a conditional remove to avoid part of the race condition.
  index.remove(valueName, set, Collections.emptySet())

You would then have a race of updating a set that was just removed. To be optimistic, you could follow the add with a "get" to avoid internal locking, and then lock with an attempted put if the check fails.
  if (set == index.get(valueName) {
    return; // success - set wasn't removed while adding
  } else if (index.putIfAbset(valueName, set) == null)) {
    return; // success - set removed, but added back by us
  } else {
    continue; // failed - set removed, but added back by someone else.  Retry add against new set
  }


----- Original Message ----
From: Mike Bean <bean at alcatel-lucent.com>
To: concurrency-interest at cs.oswego.edu
Sent: Wednesday, August 6, 2008 5:10:21 PM
Subject: [concurrency-interest] Locking a ConcurrentHashSet in a ConcurrentHasHMap

Experts,

I was wondering if anyone had some thoughts on how to avoid a lock to 
allow removal of a ConcurrentHashSet from a ConcurrentHashMap.  Context 
is an index manager with these two calls:

    public void index( Normal name, Value value, Entry entry )
    {
        if ( !m_indexingEnabled )
            return;

        ConcurrentHashMap<String,ConcurrentHashSet<Entry>> index = 
getIndex( name );
        if ( index != null )
        {
            String valueName = value.getAsString();
            while( true )
            {
                ConcurrentHashSet<Entry> set = index.get( valueName );
                if ( set == null )
                {
                    ConcurrentHashSet<Entry> newSet = new 
ConcurrentHashSet<Entry>();
                    newSet.add( entry );
                    if ( index.putIfAbsent( valueName, newSet ) == null )
                    {
                        break;
                    }
                }
                else
                {
                    synchronized ( set )  //!!! make sure we don't add 
entry to set while another thread removes empty set from index
                    {
                        if ( set.size() == 0 )
                        {
                            continue;
                        }
                        set.add( entry );
                        break;
                    }
                }
            }
        }
    }

    public void unindex( Normal name, Value value, Entry entry )
    {
        if ( !m_indexingEnabled )
            return;

        ConcurrentHashMap<String,ConcurrentHashSet<Entry>> index = 
getIndex( name );
        if ( index != null )
        {
            String valueName = value.getAsString();
            ConcurrentHashSet<Entry> set = index.get( valueName );
            if ( set != null )
            {
                synchronized ( set )  //!!! make sure we don't remove 
set from index while another thread adds entry to set
                {
                    set.remove( entry );
                    if ( set.size() == 0 )
                    {
                        index.remove( valueName, set );
                    }
                }
            }
        }
    }

Is there a better way to prevent one thread from updating the set while 
another thread is trying to remove the set from the map than the lock?.

I'm also concerned at the memory cost of a ConcurrentHashSet for a 
single reference to an Entry object but the indexing and unindexing 
entries gets more complex with the map containing an Entry or a 
ConcurrentHashSet of Entries.

Methods not shown from our index manager include queries that never fail 
as the map and set of entries change.

Thanks for all the work in this area,

Mike

-- 
Michael Bean (Mike)
Alcatel-Lucent
AAA Product Group
3461 Robin Ln, Ste 1
Cameron Park, CA 95682
Email: bean at alcatel-lucent.com
Phone: 530 672 7577
Fax: 530 676 3442

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080806/035abe31/attachment.html>

From Thomas.Hawtin at Sun.COM  Thu Aug  7 09:57:27 2008
From: Thomas.Hawtin at Sun.COM (Tom Hawtin)
Date: Thu, 07 Aug 2008 14:57:27 +0100
Subject: [concurrency-interest] Locking a ConcurrentHashSet in
	a	ConcurrentHasHMap
In-Reply-To: <489A3D6D.7080404@alcatel-lucent.com>
References: <489A3D6D.7080404@alcatel-lucent.com>
Message-ID: <489AFF47.8070407@sun.com>

Mike Bean wrote:
> 
> I was wondering if anyone had some thoughts on how to avoid a lock to 
> allow removal of a ConcurrentHashSet from a ConcurrentHashMap.  Context 
> is an index manager with these two calls:

I guess you could use a doNotUse flag in a subclass of 
ConcurrentHashSet. Set the flag before removing the set. Check the flag 
after adding to it.

If only we had a standard full set of multiset collections and the like...

> I'm also concerned at the memory cost of a ConcurrentHashSet for a 
> single reference to an Entry object but the indexing and unindexing 
> entries gets more complex with the map containing an Entry or a 
> ConcurrentHashSet of Entries.

Are you sure you are going to have significant contention given the map? 
Perhaps a plain set with synchronisation would do. Or an immutable [by 
convention] set.

Tom

From Thomas.Hawtin at Sun.COM  Thu Aug  7 10:13:38 2008
From: Thomas.Hawtin at Sun.COM (Tom Hawtin)
Date: Thu, 07 Aug 2008 15:13:38 +0100
Subject: [concurrency-interest] Locking a ConcurrentHashSet in
	a	ConcurrentHasHMap
In-Reply-To: <489A3D6D.7080404@alcatel-lucent.com>
References: <489A3D6D.7080404@alcatel-lucent.com>
Message-ID: <489B0312.3080807@sun.com>

Mike Bean wrote:
> 
> I was wondering if anyone had some thoughts on how to avoid a lock to 
> allow removal of a ConcurrentHashSet from a ConcurrentHashMap.  Context 
> is an index manager with these two calls:

I guess you could use a doNotUse flag in a subclass of 
ConcurrentHashSet. Set the flag before removing the set. Check the flag 
after adding to it.

If only we had a standard full set of multiset collections and the like...

> I'm also concerned at the memory cost of a ConcurrentHashSet for a 
> single reference to an Entry object but the indexing and unindexing 
> entries gets more complex with the map containing an Entry or a 
> ConcurrentHashSet of Entries.

Are you sure you are going to have significant contention given the map? 
Perhaps a plain set with synchronisation would do. Or an immutable [by 
convention] set.

Tom

From juliusdavies at gmail.com  Thu Aug  7 19:14:37 2008
From: juliusdavies at gmail.com (Julius Davies)
Date: Thu, 7 Aug 2008 16:14:37 -0700
Subject: [concurrency-interest] FAQ for clock_gettime(CLOCK_REALTIME)
Message-ID: <598ad5b50808071614w36ad1f08jdfa38560fc81a996@mail.gmail.com>

After the discussion a few months about exposing
clock_gettime(CLOCK_REALTIME) in Java, I tried to start a FAQ.  Many
thanks to John Stultz @ IBM for providing the answers!

http://juliusdavies.ca/posix_clocks/clock_realtime_linux_faq.html


David - I'll look into your suggestions (quoted below) when I have some time.

>
> More stuff for the FAQ would be to discuss the different clock_sources
> available on Linux: tsc (avoid it), hpet, acpi, jiffies.
>



-- 
yours,

Julius Davies
250-592-2284 (Home)
250-893-4579 (Mobile)
http://juliusdavies.ca/

From dharvey at tachyoncm.com  Fri Aug  8 11:10:38 2008
From: dharvey at tachyoncm.com (Daniel Harvey)
Date: Fri, 8 Aug 2008 11:10:38 -0400
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
Message-ID: <C11E767A-841A-4376-8D96-7A2EB54A6398@tachyoncm.com>

Hi,
I'm writing a program that receives a very high message rate  
(~100,000 per second) over a TCP socket, filters out the useful  
information (about 1/5 of the above), and forwards that information  
over another TCP socket. I have been having trouble keeping up with  
the incoming messages, and I think its an oddity of the  
LinkedBlockingQueue I am using to queue the useful updates for later  
sending. The code looks something like this:

public class MessageForwarder {
	SendThread	sendThread = new SendThread();
	 ...

	public void onMessage(RapidMessage message) {
		long start = System.nanoTime();

		// Do a bunch of checks to decide if useful...
		boolean useful = ...
	
		if (useful) {
			sendThread.send(throttled.toMessageString())
		}
		processingTime += (System.nanoTime() - start);
	}

	// inner class
	public class SendThread extends Thread {
	
		private final BlockingQueue<String> queued = new  
LinkedBlockingQueue<String>(10000);
		private final CharsetEncoder              encoder = Charset.forName 
("ISO-8859-1").newEncoder();

		public void send(String message) {
			long start = System.nanoTime();
	 		boolean success = queued.offer(message);
			sendingTime +=  (System.nanoTime() - start);

			if (!success) {
				log.error("Failed to send message: queue is full");
			}
		}

		public void run() {
			while (true) {
				String message = queued.take();
				ByteBuffer buffer = encoder.encode(CharBuffer.wrap(message));
				socketChannel.write(buffer);
			}
		}
	}
...
}

When I print out periodically the value of the "processingTime" and  
the "sendingTime", they are similar: the majority of the processing  
time is taking place in the call to sendThread.send(). The checks to  
see if the message is useful are fast enough that it could run  
100,000 messages per second, but the calls to "send" are much slower,  
only 20,000 messages per second at best. Note that the queue is not  
filling up (not seeing the error message anywhere). What I really  
don't understand is that if I have the consumer (Sending) thread  
throw away the message after it takes it from the queue (ie just do  
repeated "take"s), then the call to sendThread.send() speeds up by a  
factor of around 6. I don't understand how if the queue isn't filling  
up, why would the actions of the consumer matter to the speed of the  
producer?

Any help or advice you can give would be much appreciated!

Thanks,
Dan Harvey


From bean at alcatel-lucent.com  Fri Aug  8 11:24:52 2008
From: bean at alcatel-lucent.com (Mike Bean)
Date: Fri, 08 Aug 2008 08:24:52 -0700
Subject: [concurrency-interest] Locking a ConcurrentHashSet in
	a	ConcurrentHasHMap
In-Reply-To: <489B0312.3080807@sun.com>
References: <489A3D6D.7080404@alcatel-lucent.com> <489B0312.3080807@sun.com>
Message-ID: <489C6544.504@alcatel-lucent.com>

Tom,

Using a concurrent map and set allow observers to iterate data structure 
without blocking adds and removes.  I had considered cloning set and 
using conditional replace but sets could be large.  If set is large the 
probability of collisions with other threads becomes greater slowing 
update of set due to other threads replacing set at same time.

I'm also exploring the idea of reworking a copy of ConcurrentHashMap 
code to allow for a multiple values per key.  The get method would 
return a collection of values by walking the entire list for a hash code 
looking for matching keys.  No sure if I will run into problems and I 
always worry about getting the logic right on concurrent data structures.

Thanks for your response,

Mike

Michael Bean (Mike)
Alcatel-Lucent
AAA Product Group
3461 Robin Ln, Ste 1
Cameron Park, CA 95682
Email: bean at alcatel-lucent.com
Phone: 530 672 7577
Fax: 530 676 3442



Tom Hawtin wrote:
> Mike Bean wrote:
>>
>> I was wondering if anyone had some thoughts on how to avoid a lock to 
>> allow removal of a ConcurrentHashSet from a ConcurrentHashMap.  
>> Context is an index manager with these two calls:
>
> I guess you could use a doNotUse flag in a subclass of 
> ConcurrentHashSet. Set the flag before removing the set. Check the 
> flag after adding to it.
>
> If only we had a standard full set of multiset collections and the 
> like...
>
>> I'm also concerned at the memory cost of a ConcurrentHashSet for a 
>> single reference to an Entry object but the indexing and unindexing 
>> entries gets more complex with the map containing an Entry or a 
>> ConcurrentHashSet of Entries.
>
> Are you sure you are going to have significant contention given the 
> map? Perhaps a plain set with synchronisation would do. Or an 
> immutable [by convention] set.
>
> Tom
>

From dcholmes at optusnet.com.au  Fri Aug  8 21:49:59 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat, 9 Aug 2008 11:49:59 +1000
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <C11E767A-841A-4376-8D96-7A2EB54A6398@tachyoncm.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEBOHNAA.dcholmes@optusnet.com.au>

Dan,

The simple answer is that the consumer is using a resource that the producer
needs. It could be processor cycles, or network access, or contention on
some shared object at the application, VM or OS level. Assuming you're
multi-processor I'd suspect contention somewhere in the TCP/IP stack. What
OS are you on? How many processors do you have?

At 100,000 messages per second you only have 10 microseconds to fully
process each one. That's a tall order on a non-real-time system. Even if
your best processing time is within 10us, it isn't a sustainable rate, so
you should expect to see a lot of variation in the message rate that you can
handle. When GC kicks in you're going to drop a lot of messages.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Daniel
> Harvey
> Sent: Saturday, 9 August 2008 1:11 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Single producer,single consumer:
> unexpected delays for producer
>
>
> Hi,
> I'm writing a program that receives a very high message rate
> (~100,000 per second) over a TCP socket, filters out the useful
> information (about 1/5 of the above), and forwards that information
> over another TCP socket. I have been having trouble keeping up with
> the incoming messages, and I think its an oddity of the
> LinkedBlockingQueue I am using to queue the useful updates for later
> sending. The code looks something like this:
>
> public class MessageForwarder {
> 	SendThread	sendThread = new SendThread();
> 	 ...
>
> 	public void onMessage(RapidMessage message) {
> 		long start = System.nanoTime();
>
> 		// Do a bunch of checks to decide if useful...
> 		boolean useful = ...
>
> 		if (useful) {
> 			sendThread.send(throttled.toMessageString())
> 		}
> 		processingTime += (System.nanoTime() - start);
> 	}
>
> 	// inner class
> 	public class SendThread extends Thread {
>
> 		private final BlockingQueue<String> queued = new
> LinkedBlockingQueue<String>(10000);
> 		private final CharsetEncoder              encoder =
> Charset.forName
> ("ISO-8859-1").newEncoder();
>
> 		public void send(String message) {
> 			long start = System.nanoTime();
> 	 		boolean success = queued.offer(message);
> 			sendingTime +=  (System.nanoTime() - start);
>
> 			if (!success) {
> 				log.error("Failed to send message:
> queue is full");
> 			}
> 		}
>
> 		public void run() {
> 			while (true) {
> 				String message = queued.take();
> 				ByteBuffer buffer =
> encoder.encode(CharBuffer.wrap(message));
> 				socketChannel.write(buffer);
> 			}
> 		}
> 	}
> ...
> }
>
> When I print out periodically the value of the "processingTime" and
> the "sendingTime", they are similar: the majority of the processing
> time is taking place in the call to sendThread.send(). The checks to
> see if the message is useful are fast enough that it could run
> 100,000 messages per second, but the calls to "send" are much slower,
> only 20,000 messages per second at best. Note that the queue is not
> filling up (not seeing the error message anywhere). What I really
> don't understand is that if I have the consumer (Sending) thread
> throw away the message after it takes it from the queue (ie just do
> repeated "take"s), then the call to sendThread.send() speeds up by a
> factor of around 6. I don't understand how if the queue isn't filling
> up, why would the actions of the consumer matter to the speed of the
> producer?
>
> Any help or advice you can give would be much appreciated!
>
> Thanks,
> Dan Harvey
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dharvey at tachyoncm.com  Fri Aug  8 22:32:48 2008
From: dharvey at tachyoncm.com (Daniel Harvey)
Date: Fri, 8 Aug 2008 22:32:48 -0400
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEBOHNAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCKEBOHNAA.dcholmes@optusnet.com.au>
Message-ID: <60FCF9E4-D251-4503-ABF7-D1F50D56641A@tachyoncm.com>

David, many thanks for your response. I'm running on Linux (CentOS)  
with a 2 processor machine, though am planning on moving to an 8  
processor machine in the next few days. I'm currently using jrockit  
(java 1.6) as the JVM. Today I downloaded jrockit-realtime so perhaps  
that will help reduce the amount of dropped messages.

What confuses me is where the delay is showing up on the producer.  
Its not showing up as a slowdown in reading from the socket, just in  
the call to queue.offer(). That queue is really the only shared  
object that is being used by both threads. I was wondering whether  
the thread signaling (when the message is put in the queue) is an OS  
or system level call, and whether that can get held up by the socket  
writing call? That might explain why the socket sending could hold up  
the queue insertions. If so, can you think of any way around my issue?

Thanks,
Dan

On Aug 8, 2008, at 9:49 PM, David Holmes wrote:

> Dan,
>
> The simple answer is that the consumer is using a resource that the  
> producer
> needs. It could be processor cycles, or network access, or  
> contention on
> some shared object at the application, VM or OS level. Assuming you're
> multi-processor I'd suspect contention somewhere in the TCP/IP  
> stack. What
> OS are you on? How many processors do you have?
>
> At 100,000 messages per second you only have 10 microseconds to fully
> process each one. That's a tall order on a non-real-time system.  
> Even if
> your best processing time is within 10us, it isn't a sustainable  
> rate, so
> you should expect to see a lot of variation in the message rate  
> that you can
> handle. When GC kicks in you're going to drop a lot of messages.
>
> Cheers,
> David Holmes
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of  
>> Daniel
>> Harvey
>> Sent: Saturday, 9 August 2008 1:11 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Single producer,single consumer:
>> unexpected delays for producer
>>
>>
>> Hi,
>> I'm writing a program that receives a very high message rate
>> (~100,000 per second) over a TCP socket, filters out the useful
>> information (about 1/5 of the above), and forwards that information
>> over another TCP socket. I have been having trouble keeping up with
>> the incoming messages, and I think its an oddity of the
>> LinkedBlockingQueue I am using to queue the useful updates for later
>> sending. The code looks something like this:
>>
>> public class MessageForwarder {
>> 	SendThread	sendThread = new SendThread();
>> 	 ...
>>
>> 	public void onMessage(RapidMessage message) {
>> 		long start = System.nanoTime();
>>
>> 		// Do a bunch of checks to decide if useful...
>> 		boolean useful = ...
>>
>> 		if (useful) {
>> 			sendThread.send(throttled.toMessageString())
>> 		}
>> 		processingTime += (System.nanoTime() - start);
>> 	}
>>
>> 	// inner class
>> 	public class SendThread extends Thread {
>>
>> 		private final BlockingQueue<String> queued = new
>> LinkedBlockingQueue<String>(10000);
>> 		private final CharsetEncoder              encoder =
>> Charset.forName
>> ("ISO-8859-1").newEncoder();
>>
>> 		public void send(String message) {
>> 			long start = System.nanoTime();
>> 	 		boolean success = queued.offer(message);
>> 			sendingTime +=  (System.nanoTime() - start);
>>
>> 			if (!success) {
>> 				log.error("Failed to send message:
>> queue is full");
>> 			}
>> 		}
>>
>> 		public void run() {
>> 			while (true) {
>> 				String message = queued.take();
>> 				ByteBuffer buffer =
>> encoder.encode(CharBuffer.wrap(message));
>> 				socketChannel.write(buffer);
>> 			}
>> 		}
>> 	}
>> ...
>> }
>>
>> When I print out periodically the value of the "processingTime" and
>> the "sendingTime", they are similar: the majority of the processing
>> time is taking place in the call to sendThread.send(). The checks to
>> see if the message is useful are fast enough that it could run
>> 100,000 messages per second, but the calls to "send" are much slower,
>> only 20,000 messages per second at best. Note that the queue is not
>> filling up (not seeing the error message anywhere). What I really
>> don't understand is that if I have the consumer (Sending) thread
>> throw away the message after it takes it from the queue (ie just do
>> repeated "take"s), then the call to sendThread.send() speeds up by a
>> factor of around 6. I don't understand how if the queue isn't filling
>> up, why would the actions of the consumer matter to the speed of the
>> producer?
>>
>> Any help or advice you can give would be much appreciated!
>>
>> Thanks,
>> Dan Harvey
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From rajesh.balamohan at gmail.com  Fri Aug  8 23:07:53 2008
From: rajesh.balamohan at gmail.com (Rajesh Balamohan)
Date: Sat, 9 Aug 2008 08:37:53 +0530
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <1da12e810808082007q77665e69u704c3c0f7031d5a@mail.gmail.com>
References: <C11E767A-841A-4376-8D96-7A2EB54A6398@tachyoncm.com>
	<1da12e810808082007q77665e69u704c3c0f7031d5a@mail.gmail.com>
Message-ID: <1da12e810808082007x4da28f8bhd36e2cebf3221fdd@mail.gmail.com>

Hi Dan,

This is just my view. Can you try to send/consume the messages without using
encoder.encode(*).

LinkedBlockingQueue might be really fast. However, after taking it from the
queue you seem to do some encoding which could be really expensive. Have a
look at the java.nio.CharsetEncoder and its details.

Did you try observing the GC as suggested by David?. Add -verbose:gc in the
java command and check out the performance with and without CharsetEncoder.

Basically the issue could be in the encoding rather than in
LinkedBlockingQueue.

~Rajesh.B


On Fri, Aug 8, 2008 at 8:40 PM, Daniel Harvey <dharvey at tachyoncm.com> wrote:

> Hi,
> I'm writing a program that receives a very high message rate (~100,000 per
> second) over a TCP socket, filters out the useful information (about 1/5 of
> the above), and forwards that information over another TCP socket. I have
> been having trouble keeping up with the incoming messages, and I think its
> an oddity of the LinkedBlockingQueue I am using to queue the useful updates
> for later sending. The code looks something like this:
>
> public class MessageForwarder {
>        SendThread      sendThread = new SendThread();
>         ...
>
>        public void onMessage(RapidMessage message) {
>                long start = System.nanoTime();
>
>                // Do a bunch of checks to decide if useful...
>                boolean useful = ...
>
>                if (useful) {
>                        sendThread.send(throttled.toMessageString())
>                }
>                processingTime += (System.nanoTime() - start);
>        }
>
>        // inner class
>        public class SendThread extends Thread {
>
>                private final BlockingQueue<String> queued = new
> LinkedBlockingQueue<String>(10000);
>                private final CharsetEncoder              encoder =
> Charset.forName("ISO-8859-1").newEncoder();
>
>                public void send(String message) {
>                        long start = System.nanoTime();
>                        boolean success = queued.offer(message);
>                        sendingTime +=  (System.nanoTime() - start);
>
>                        if (!success) {
>                                log.error("Failed to send message: queue is
> full");
>                        }
>                }
>
>                public void run() {
>                        while (true) {
>                                String message = queued.take();
>                                ByteBuffer buffer =
> encoder.encode(CharBuffer.wrap(message));
>                                socketChannel.write(buffer);
>                        }
>                }
>        }
> ...
> }
>
> When I print out periodically the value of the "processingTime" and the
> "sendingTime", they are similar: the majority of the processing time is
> taking place in the call to sendThread.send(). The checks to see if the
> message is useful are fast enough that it could run 100,000 messages per
> second, but the calls to "send" are much slower, only 20,000 messages per
> second at best. Note that the queue is not filling up (not seeing the error
> message anywhere). What I really don't understand is that if I have the
> consumer (Sending) thread throw away the message after it takes it from the
> queue (ie just do repeated "take"s), then the call to sendThread.send()
> speeds up by a factor of around 6. I don't understand how if the queue isn't
> filling up, why would the actions of the consumer matter to the speed of the
> producer?
>
> Any help or advice you can give would be much appreciated!
>
> Thanks,
> Dan Harvey
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
~Rajesh.B



-- 
~Rajesh.B
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080809/406282b3/attachment.html>

From dcholmes at optusnet.com.au  Sat Aug  9 00:45:58 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat, 9 Aug 2008 14:45:58 +1000
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <60FCF9E4-D251-4503-ABF7-D1F50D56641A@tachyoncm.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEBPHNAA.dcholmes@optusnet.com.au>

Dan,

Have you actually just timed the queue operations? You indicated that if the
consumer just did take() in a loop then there was a 6x speed-up in the
producer. That would seem to indicate that the queue itself is not the
problem. It could just be an unfortunate harmonic where the producer and
consumer always collide at the queue, but even so the delay shouldn't result
in a 6x slowdown. You might try tracking the queue size to see if grows and
shrinks as expected or whether it oscillates around being empty.

AS for whether system calls are involved ... probably, but I don't know how
anything in JRockit is implemented so I can't say for sure. I'm assuming it
will use both of your processors.

Can you download JDK 6 and try that for comparison? (You might also try an
evaluation download of Sun Java Real-time System if CentOS provides the
real-time POSIX API's - and presuming it's actually a real-time linux. :) )

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Daniel
> Harvey
> Sent: Saturday, 9 August 2008 12:33 PM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Single producer,single consumer:
> unexpected delays for producer
>
>
> David, many thanks for your response. I'm running on Linux (CentOS)
> with a 2 processor machine, though am planning on moving to an 8
> processor machine in the next few days. I'm currently using jrockit
> (java 1.6) as the JVM. Today I downloaded jrockit-realtime so perhaps
> that will help reduce the amount of dropped messages.
>
> What confuses me is where the delay is showing up on the producer.
> Its not showing up as a slowdown in reading from the socket, just in
> the call to queue.offer(). That queue is really the only shared
> object that is being used by both threads. I was wondering whether
> the thread signaling (when the message is put in the queue) is an OS
> or system level call, and whether that can get held up by the socket
> writing call? That might explain why the socket sending could hold up
> the queue insertions. If so, can you think of any way around my issue?
>
> Thanks,
> Dan
>
> On Aug 8, 2008, at 9:49 PM, David Holmes wrote:
>
> > Dan,
> >
> > The simple answer is that the consumer is using a resource that the
> > producer
> > needs. It could be processor cycles, or network access, or
> > contention on
> > some shared object at the application, VM or OS level. Assuming you're
> > multi-processor I'd suspect contention somewhere in the TCP/IP
> > stack. What
> > OS are you on? How many processors do you have?
> >
> > At 100,000 messages per second you only have 10 microseconds to fully
> > process each one. That's a tall order on a non-real-time system.
> > Even if
> > your best processing time is within 10us, it isn't a sustainable
> > rate, so
> > you should expect to see a lot of variation in the message rate
> > that you can
> > handle. When GC kicks in you're going to drop a lot of messages.
> >
> > Cheers,
> > David Holmes
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >> Daniel
> >> Harvey
> >> Sent: Saturday, 9 August 2008 1:11 AM
> >> To: concurrency-interest at cs.oswego.edu
> >> Subject: [concurrency-interest] Single producer,single consumer:
> >> unexpected delays for producer
> >>
> >>
> >> Hi,
> >> I'm writing a program that receives a very high message rate
> >> (~100,000 per second) over a TCP socket, filters out the useful
> >> information (about 1/5 of the above), and forwards that information
> >> over another TCP socket. I have been having trouble keeping up with
> >> the incoming messages, and I think its an oddity of the
> >> LinkedBlockingQueue I am using to queue the useful updates for later
> >> sending. The code looks something like this:
> >>
> >> public class MessageForwarder {
> >> 	SendThread	sendThread = new SendThread();
> >> 	 ...
> >>
> >> 	public void onMessage(RapidMessage message) {
> >> 		long start = System.nanoTime();
> >>
> >> 		// Do a bunch of checks to decide if useful...
> >> 		boolean useful = ...
> >>
> >> 		if (useful) {
> >> 			sendThread.send(throttled.toMessageString())
> >> 		}
> >> 		processingTime += (System.nanoTime() - start);
> >> 	}
> >>
> >> 	// inner class
> >> 	public class SendThread extends Thread {
> >>
> >> 		private final BlockingQueue<String> queued = new
> >> LinkedBlockingQueue<String>(10000);
> >> 		private final CharsetEncoder              encoder =
> >> Charset.forName
> >> ("ISO-8859-1").newEncoder();
> >>
> >> 		public void send(String message) {
> >> 			long start = System.nanoTime();
> >> 	 		boolean success = queued.offer(message);
> >> 			sendingTime +=  (System.nanoTime() - start);
> >>
> >> 			if (!success) {
> >> 				log.error("Failed to send message:
> >> queue is full");
> >> 			}
> >> 		}
> >>
> >> 		public void run() {
> >> 			while (true) {
> >> 				String message = queued.take();
> >> 				ByteBuffer buffer =
> >> encoder.encode(CharBuffer.wrap(message));
> >> 				socketChannel.write(buffer);
> >> 			}
> >> 		}
> >> 	}
> >> ...
> >> }
> >>
> >> When I print out periodically the value of the "processingTime" and
> >> the "sendingTime", they are similar: the majority of the processing
> >> time is taking place in the call to sendThread.send(). The checks to
> >> see if the message is useful are fast enough that it could run
> >> 100,000 messages per second, but the calls to "send" are much slower,
> >> only 20,000 messages per second at best. Note that the queue is not
> >> filling up (not seeing the error message anywhere). What I really
> >> don't understand is that if I have the consumer (Sending) thread
> >> throw away the message after it takes it from the queue (ie just do
> >> repeated "take"s), then the call to sendThread.send() speeds up by a
> >> factor of around 6. I don't understand how if the queue isn't filling
> >> up, why would the actions of the consumer matter to the speed of the
> >> producer?
> >>
> >> Any help or advice you can give would be much appreciated!
> >>
> >> Thanks,
> >> Dan Harvey
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From carfield at carfield.com.hk  Sat Aug  9 01:06:19 2008
From: carfield at carfield.com.hk (Carfield Yim)
Date: Sat, 9 Aug 2008 13:06:19 +0800
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <C11E767A-841A-4376-8D96-7A2EB54A6398@tachyoncm.com>
References: <C11E767A-841A-4376-8D96-7A2EB54A6398@tachyoncm.com>
Message-ID: <b4503c170808082206t66bb5782jd82fa885e2ce6a39@mail.gmail.com>

May be you can try this

http://binkley.blogspot.com/2007/09/performing-fixed-amounts-of-work-with.html

and see if that help?

On Fri, Aug 8, 2008 at 11:10 PM, Daniel Harvey <dharvey at tachyoncm.com>wrote:

> Hi,
> I'm writing a program that receives a very high message rate (~100,000 per
> second) over a TCP socket, filters out the useful information (about 1/5 of
> the above), and forwards that information over another TCP socket. I have
> been having trouble keeping up with the incoming messages, and I think its
> an oddity of the LinkedBlockingQueue I am using to queue the useful updates
> for later sending. The code looks something like this:
>
> public class MessageForwarder {
>        SendThread      sendThread = new SendThread();
>         ...
>
>        public void onMessage(RapidMessage message) {
>                long start = System.nanoTime();
>
>                // Do a bunch of checks to decide if useful...
>                boolean useful = ...
>
>                if (useful) {
>                        sendThread.send(throttled.toMessageString())
>                }
>                processingTime += (System.nanoTime() - start);
>        }
>
>        // inner class
>        public class SendThread extends Thread {
>
>                private final BlockingQueue<String> queued = new
> LinkedBlockingQueue<String>(10000);
>                private final CharsetEncoder              encoder =
> Charset.forName("ISO-8859-1").newEncoder();
>
>                public void send(String message) {
>                        long start = System.nanoTime();
>                        boolean success = queued.offer(message);
>                        sendingTime +=  (System.nanoTime() - start);
>
>                        if (!success) {
>                                log.error("Failed to send message: queue is
> full");
>                        }
>                }
>
>                public void run() {
>                        while (true) {
>                                String message = queued.take();
>                                ByteBuffer buffer =
> encoder.encode(CharBuffer.wrap(message));
>                                socketChannel.write(buffer);
>                        }
>                }
>        }
> ...
> }
>
> When I print out periodically the value of the "processingTime" and the
> "sendingTime", they are similar: the majority of the processing time is
> taking place in the call to sendThread.send(). The checks to see if the
> message is useful are fast enough that it could run 100,000 messages per
> second, but the calls to "send" are much slower, only 20,000 messages per
> second at best. Note that the queue is not filling up (not seeing the error
> message anywhere). What I really don't understand is that if I have the
> consumer (Sending) thread throw away the message after it takes it from the
> queue (ie just do repeated "take"s), then the call to sendThread.send()
> speeds up by a factor of around 6. I don't understand how if the queue isn't
> filling up, why would the actions of the consumer matter to the speed of the
> producer?
>
> Any help or advice you can give would be much appreciated!
>
> Thanks,
> Dan Harvey
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080809/b66e3409/attachment.html>

From dharvey at tachyoncm.com  Sat Aug  9 11:05:07 2008
From: dharvey at tachyoncm.com (Daniel Harvey)
Date: Sat, 9 Aug 2008 11:05:07 -0400
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEBPHNAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCMEBPHNAA.dcholmes@optusnet.com.au>
Message-ID: <1FDC0824-64A0-454C-9CC9-CCFF9D5476DE@tachyoncm.com>

David,
I have timed the entire onMessage() processing for the producer  
(which is called very often) and also just the time the producer  
spends in calling queue.offer() (via sendThread.send, called about  
1/5 as often). What is so odd to me is that 90% of the total time  
spent in onMessage() comes from time spent in queue.offer() despite  
the fact that this is only called for 20% of the messages. If I have  
the sendThread skip the socketChannel.write() command, than the  
situation changes greatly: time spent in queue.offer drops by a  
literally a factor of 6 or so.

I haven't tracked the queue size over time, but do know that it isn't  
filling up because I'm not seeing any error messages from queue.offer 
() returning false. As long as the queue is below its fixed max size,  
should its actual size affect the amount of work needed to perform a  
queue insertion?

Anyway, I will certainly try Sun jdk, and look into whether CentOS  
provides the POSIX api's.

I'll also give Carfield's suggestion a try (thanks), though in past  
experiments I have done I haven't found the drainTo to be _that_ much  
more efficient that repeated take()'s.

-Dan

On Aug 9, 2008, at 12:45 AM, David Holmes wrote:

> Dan,
>
> Have you actually just timed the queue operations? You indicated  
> that if the
> consumer just did take() in a loop then there was a 6x speed-up in the
> producer. That would seem to indicate that the queue itself is not the
> problem. It could just be an unfortunate harmonic where the  
> producer and
> consumer always collide at the queue, but even so the delay  
> shouldn't result
> in a 6x slowdown. You might try tracking the queue size to see if  
> grows and
> shrinks as expected or whether it oscillates around being empty.
>
> AS for whether system calls are involved ... probably, but I don't  
> know how
> anything in JRockit is implemented so I can't say for sure. I'm  
> assuming it
> will use both of your processors.
>
> Can you download JDK 6 and try that for comparison? (You might also  
> try an
> evaluation download of Sun Java Real-time System if CentOS provides  
> the
> real-time POSIX API's - and presuming it's actually a real-time  
> linux. :) )
>
> Cheers,
> David Holmes
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of  
>> Daniel
>> Harvey
>> Sent: Saturday, 9 August 2008 12:33 PM
>> To: dholmes at ieee.org
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Single producer,single consumer:
>> unexpected delays for producer
>>
>>
>> David, many thanks for your response. I'm running on Linux (CentOS)
>> with a 2 processor machine, though am planning on moving to an 8
>> processor machine in the next few days. I'm currently using jrockit
>> (java 1.6) as the JVM. Today I downloaded jrockit-realtime so perhaps
>> that will help reduce the amount of dropped messages.
>>
>> What confuses me is where the delay is showing up on the producer.
>> Its not showing up as a slowdown in reading from the socket, just in
>> the call to queue.offer(). That queue is really the only shared
>> object that is being used by both threads. I was wondering whether
>> the thread signaling (when the message is put in the queue) is an OS
>> or system level call, and whether that can get held up by the socket
>> writing call? That might explain why the socket sending could hold up
>> the queue insertions. If so, can you think of any way around my  
>> issue?
>>
>> Thanks,
>> Dan
>>
>> On Aug 8, 2008, at 9:49 PM, David Holmes wrote:
>>
>>> Dan,
>>>
>>> The simple answer is that the consumer is using a resource that the
>>> producer
>>> needs. It could be processor cycles, or network access, or
>>> contention on
>>> some shared object at the application, VM or OS level. Assuming  
>>> you're
>>> multi-processor I'd suspect contention somewhere in the TCP/IP
>>> stack. What
>>> OS are you on? How many processors do you have?
>>>
>>> At 100,000 messages per second you only have 10 microseconds to  
>>> fully
>>> process each one. That's a tall order on a non-real-time system.
>>> Even if
>>> your best processing time is within 10us, it isn't a sustainable
>>> rate, so
>>> you should expect to see a lot of variation in the message rate
>>> that you can
>>> handle. When GC kicks in you're going to drop a lot of messages.
>>>
>>> Cheers,
>>> David Holmes
>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>> Daniel
>>>> Harvey
>>>> Sent: Saturday, 9 August 2008 1:11 AM
>>>> To: concurrency-interest at cs.oswego.edu
>>>> Subject: [concurrency-interest] Single producer,single consumer:
>>>> unexpected delays for producer
>>>>
>>>>
>>>> Hi,
>>>> I'm writing a program that receives a very high message rate
>>>> (~100,000 per second) over a TCP socket, filters out the useful
>>>> information (about 1/5 of the above), and forwards that information
>>>> over another TCP socket. I have been having trouble keeping up with
>>>> the incoming messages, and I think its an oddity of the
>>>> LinkedBlockingQueue I am using to queue the useful updates for  
>>>> later
>>>> sending. The code looks something like this:
>>>>
>>>> public class MessageForwarder {
>>>> 	SendThread	sendThread = new SendThread();
>>>> 	 ...
>>>>
>>>> 	public void onMessage(RapidMessage message) {
>>>> 		long start = System.nanoTime();
>>>>
>>>> 		// Do a bunch of checks to decide if useful...
>>>> 		boolean useful = ...
>>>>
>>>> 		if (useful) {
>>>> 			sendThread.send(throttled.toMessageString())
>>>> 		}
>>>> 		processingTime += (System.nanoTime() - start);
>>>> 	}
>>>>
>>>> 	// inner class
>>>> 	public class SendThread extends Thread {
>>>>
>>>> 		private final BlockingQueue<String> queued = new
>>>> LinkedBlockingQueue<String>(10000);
>>>> 		private final CharsetEncoder              encoder =
>>>> Charset.forName
>>>> ("ISO-8859-1").newEncoder();
>>>>
>>>> 		public void send(String message) {
>>>> 			long start = System.nanoTime();
>>>> 	 		boolean success = queued.offer(message);
>>>> 			sendingTime +=  (System.nanoTime() - start);
>>>>
>>>> 			if (!success) {
>>>> 				log.error("Failed to send message:
>>>> queue is full");
>>>> 			}
>>>> 		}
>>>>
>>>> 		public void run() {
>>>> 			while (true) {
>>>> 				String message = queued.take();
>>>> 				ByteBuffer buffer =
>>>> encoder.encode(CharBuffer.wrap(message));
>>>> 				socketChannel.write(buffer);
>>>> 			}
>>>> 		}
>>>> 	}
>>>> ...
>>>> }
>>>>
>>>> When I print out periodically the value of the "processingTime" and
>>>> the "sendingTime", they are similar: the majority of the processing
>>>> time is taking place in the call to sendThread.send(). The  
>>>> checks to
>>>> see if the message is useful are fast enough that it could run
>>>> 100,000 messages per second, but the calls to "send" are much  
>>>> slower,
>>>> only 20,000 messages per second at best. Note that the queue is not
>>>> filling up (not seeing the error message anywhere). What I really
>>>> don't understand is that if I have the consumer (Sending) thread
>>>> throw away the message after it takes it from the queue (ie just do
>>>> repeated "take"s), then the call to sendThread.send() speeds up  
>>>> by a
>>>> factor of around 6. I don't understand how if the queue isn't  
>>>> filling
>>>> up, why would the actions of the consumer matter to the speed of  
>>>> the
>>>> producer?
>>>>
>>>> Any help or advice you can give would be much appreciated!
>>>>
>>>> Thanks,
>>>> Dan Harvey
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From dl at cs.oswego.edu  Sat Aug  9 11:16:02 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 09 Aug 2008 11:16:02 -0400
Subject: [concurrency-interest] Single producer,
 single consumer: unexpected delays for producer
In-Reply-To: <1FDC0824-64A0-454C-9CC9-CCFF9D5476DE@tachyoncm.com>
References: <NFBBKALFDCPFIDBNKAPCMEBPHNAA.dcholmes@optusnet.com.au>
	<1FDC0824-64A0-454C-9CC9-CCFF9D5476DE@tachyoncm.com>
Message-ID: <489DB4B2.1020205@cs.oswego.edu>

Daniel Harvey wrote:
> 
> What is so odd to me is that 90% of the total time spent in onMessage() 
> comes from time spent in queue.offer() despite the fact that this is 
> only called for 20% of the messages. If I have the sendThread skip the 
> socketChannel.write() command, than the situation changes greatly: time 
> spent in queue.offer drops by a literally a factor of 6 or so.
> 

This may be due to slow thread unblocking in your VM.
You can try to avoid this a bit by pre-spinning on take:

String prespinTake(BlockingQueue<String> q) {
   for (int i = 0; i < SPINS; ++i) {
     String x = q.poll();
     if (x != null)
        return x;
   }
   return q.take();
}

where SPINS is a number say between 1 and 1000.
This will avoid offer() needing to wake up a blocked take thread.

-Doug

From dcholmes at optusnet.com.au  Sun Aug 10 18:41:51 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 11 Aug 2008 08:41:51 +1000
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <1FDC0824-64A0-454C-9CC9-CCFF9D5476DE@tachyoncm.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIECDHNAA.dcholmes@optusnet.com.au>

Dan,

Those timings are very strange. The only possible contention between
producer and consumer on LinkedBlockingQueue occurs when the first item is
inserted (transition from empty) or the first space is created (transition
from full). It's possible you have a pathological case where the queue
always switches from empty to not-empty as the producer and consumer go in
lock-step ...

I wonder what version of the library code is used in JRockit? I'm assuming
they use the JDK libraries.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Daniel
> Harvey
> Sent: Sunday, 10 August 2008 1:05 AM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Single producer,single consumer:
> unexpected delays for producer
>
>
> David,
> I have timed the entire onMessage() processing for the producer
> (which is called very often) and also just the time the producer
> spends in calling queue.offer() (via sendThread.send, called about
> 1/5 as often). What is so odd to me is that 90% of the total time
> spent in onMessage() comes from time spent in queue.offer() despite
> the fact that this is only called for 20% of the messages. If I have
> the sendThread skip the socketChannel.write() command, than the
> situation changes greatly: time spent in queue.offer drops by a
> literally a factor of 6 or so.
>
> I haven't tracked the queue size over time, but do know that it isn't
> filling up because I'm not seeing any error messages from queue.offer
> () returning false. As long as the queue is below its fixed max size,
> should its actual size affect the amount of work needed to perform a
> queue insertion?
>
> Anyway, I will certainly try Sun jdk, and look into whether CentOS
> provides the POSIX api's.
>
> I'll also give Carfield's suggestion a try (thanks), though in past
> experiments I have done I haven't found the drainTo to be _that_ much
> more efficient that repeated take()'s.
>
> -Dan
>
> On Aug 9, 2008, at 12:45 AM, David Holmes wrote:
>
> > Dan,
> >
> > Have you actually just timed the queue operations? You indicated
> > that if the
> > consumer just did take() in a loop then there was a 6x speed-up in the
> > producer. That would seem to indicate that the queue itself is not the
> > problem. It could just be an unfortunate harmonic where the
> > producer and
> > consumer always collide at the queue, but even so the delay
> > shouldn't result
> > in a 6x slowdown. You might try tracking the queue size to see if
> > grows and
> > shrinks as expected or whether it oscillates around being empty.
> >
> > AS for whether system calls are involved ... probably, but I don't
> > know how
> > anything in JRockit is implemented so I can't say for sure. I'm
> > assuming it
> > will use both of your processors.
> >
> > Can you download JDK 6 and try that for comparison? (You might also
> > try an
> > evaluation download of Sun Java Real-time System if CentOS provides
> > the
> > real-time POSIX API's - and presuming it's actually a real-time
> > linux. :) )
> >
> > Cheers,
> > David Holmes
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >> Daniel
> >> Harvey
> >> Sent: Saturday, 9 August 2008 12:33 PM
> >> To: dholmes at ieee.org
> >> Cc: concurrency-interest at cs.oswego.edu
> >> Subject: Re: [concurrency-interest] Single producer,single consumer:
> >> unexpected delays for producer
> >>
> >>
> >> David, many thanks for your response. I'm running on Linux (CentOS)
> >> with a 2 processor machine, though am planning on moving to an 8
> >> processor machine in the next few days. I'm currently using jrockit
> >> (java 1.6) as the JVM. Today I downloaded jrockit-realtime so perhaps
> >> that will help reduce the amount of dropped messages.
> >>
> >> What confuses me is where the delay is showing up on the producer.
> >> Its not showing up as a slowdown in reading from the socket, just in
> >> the call to queue.offer(). That queue is really the only shared
> >> object that is being used by both threads. I was wondering whether
> >> the thread signaling (when the message is put in the queue) is an OS
> >> or system level call, and whether that can get held up by the socket
> >> writing call? That might explain why the socket sending could hold up
> >> the queue insertions. If so, can you think of any way around my
> >> issue?
> >>
> >> Thanks,
> >> Dan
> >>
> >> On Aug 8, 2008, at 9:49 PM, David Holmes wrote:
> >>
> >>> Dan,
> >>>
> >>> The simple answer is that the consumer is using a resource that the
> >>> producer
> >>> needs. It could be processor cycles, or network access, or
> >>> contention on
> >>> some shared object at the application, VM or OS level. Assuming
> >>> you're
> >>> multi-processor I'd suspect contention somewhere in the TCP/IP
> >>> stack. What
> >>> OS are you on? How many processors do you have?
> >>>
> >>> At 100,000 messages per second you only have 10 microseconds to
> >>> fully
> >>> process each one. That's a tall order on a non-real-time system.
> >>> Even if
> >>> your best processing time is within 10us, it isn't a sustainable
> >>> rate, so
> >>> you should expect to see a lot of variation in the message rate
> >>> that you can
> >>> handle. When GC kicks in you're going to drop a lot of messages.
> >>>
> >>> Cheers,
> >>> David Holmes
> >>>
> >>>> -----Original Message-----
> >>>> From: concurrency-interest-bounces at cs.oswego.edu
> >>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >>>> Daniel
> >>>> Harvey
> >>>> Sent: Saturday, 9 August 2008 1:11 AM
> >>>> To: concurrency-interest at cs.oswego.edu
> >>>> Subject: [concurrency-interest] Single producer,single consumer:
> >>>> unexpected delays for producer
> >>>>
> >>>>
> >>>> Hi,
> >>>> I'm writing a program that receives a very high message rate
> >>>> (~100,000 per second) over a TCP socket, filters out the useful
> >>>> information (about 1/5 of the above), and forwards that information
> >>>> over another TCP socket. I have been having trouble keeping up with
> >>>> the incoming messages, and I think its an oddity of the
> >>>> LinkedBlockingQueue I am using to queue the useful updates for
> >>>> later
> >>>> sending. The code looks something like this:
> >>>>
> >>>> public class MessageForwarder {
> >>>> 	SendThread	sendThread = new SendThread();
> >>>> 	 ...
> >>>>
> >>>> 	public void onMessage(RapidMessage message) {
> >>>> 		long start = System.nanoTime();
> >>>>
> >>>> 		// Do a bunch of checks to decide if useful...
> >>>> 		boolean useful = ...
> >>>>
> >>>> 		if (useful) {
> >>>> 			sendThread.send(throttled.toMessageString())
> >>>> 		}
> >>>> 		processingTime += (System.nanoTime() - start);
> >>>> 	}
> >>>>
> >>>> 	// inner class
> >>>> 	public class SendThread extends Thread {
> >>>>
> >>>> 		private final BlockingQueue<String> queued = new
> >>>> LinkedBlockingQueue<String>(10000);
> >>>> 		private final CharsetEncoder              encoder =
> >>>> Charset.forName
> >>>> ("ISO-8859-1").newEncoder();
> >>>>
> >>>> 		public void send(String message) {
> >>>> 			long start = System.nanoTime();
> >>>> 	 		boolean success = queued.offer(message);
> >>>> 			sendingTime +=  (System.nanoTime() - start);
> >>>>
> >>>> 			if (!success) {
> >>>> 				log.error("Failed to send message:
> >>>> queue is full");
> >>>> 			}
> >>>> 		}
> >>>>
> >>>> 		public void run() {
> >>>> 			while (true) {
> >>>> 				String message = queued.take();
> >>>> 				ByteBuffer buffer =
> >>>> encoder.encode(CharBuffer.wrap(message));
> >>>> 				socketChannel.write(buffer);
> >>>> 			}
> >>>> 		}
> >>>> 	}
> >>>> ...
> >>>> }
> >>>>
> >>>> When I print out periodically the value of the "processingTime" and
> >>>> the "sendingTime", they are similar: the majority of the processing
> >>>> time is taking place in the call to sendThread.send(). The
> >>>> checks to
> >>>> see if the message is useful are fast enough that it could run
> >>>> 100,000 messages per second, but the calls to "send" are much
> >>>> slower,
> >>>> only 20,000 messages per second at best. Note that the queue is not
> >>>> filling up (not seeing the error message anywhere). What I really
> >>>> don't understand is that if I have the consumer (Sending) thread
> >>>> throw away the message after it takes it from the queue (ie just do
> >>>> repeated "take"s), then the call to sendThread.send() speeds up
> >>>> by a
> >>>> factor of around 6. I don't understand how if the queue isn't
> >>>> filling
> >>>> up, why would the actions of the consumer matter to the speed of
> >>>> the
> >>>> producer?
> >>>>
> >>>> Any help or advice you can give would be much appreciated!
> >>>>
> >>>> Thanks,
> >>>> Dan Harvey
> >>>>
> >>>> _______________________________________________
> >>>> Concurrency-interest mailing list
> >>>> Concurrency-interest at cs.oswego.edu
> >>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dcholmes at optusnet.com.au  Sun Aug 10 18:46:08 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 11 Aug 2008 08:46:08 +1000
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <489DB4B2.1020205@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCMECDHNAA.dcholmes@optusnet.com.au>

Hi Doug,

But in this case removing the network load in the consumer causes the
consumer to execute take() more rapidly and so it is more likely to find the
queue empty, and so block. But this is the case that appears 6x faster. ???

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug Lea
> Sent: Sunday, 10 August 2008 1:16 AM
> To: Daniel Harvey
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Single producer, single consumer:
> unexpected delays for producer
>
>
> Daniel Harvey wrote:
> >
> > What is so odd to me is that 90% of the total time spent in onMessage()
> > comes from time spent in queue.offer() despite the fact that this is
> > only called for 20% of the messages. If I have the sendThread skip the
> > socketChannel.write() command, than the situation changes greatly: time
> > spent in queue.offer drops by a literally a factor of 6 or so.
> >
>
> This may be due to slow thread unblocking in your VM.
> You can try to avoid this a bit by pre-spinning on take:
>
> String prespinTake(BlockingQueue<String> q) {
>    for (int i = 0; i < SPINS; ++i) {
>      String x = q.poll();
>      if (x != null)
>         return x;
>    }
>    return q.take();
> }
>
> where SPINS is a number say between 1 and 1000.
> This will avoid offer() needing to wake up a blocked take thread.
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jed at atlassian.com  Mon Aug 11 03:34:26 2008
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Mon, 11 Aug 2008 17:34:26 +1000
Subject: [concurrency-interest] Single producer,
 single consumer: unexpected delays for producer
In-Reply-To: <1FDC0824-64A0-454C-9CC9-CCFF9D5476DE@tachyoncm.com>
References: <NFBBKALFDCPFIDBNKAPCMEBPHNAA.dcholmes@optusnet.com.au>
	<1FDC0824-64A0-454C-9CC9-CCFF9D5476DE@tachyoncm.com>
Message-ID: <489FEB82.20405@atlassian.com>

Daniel, I have been doing some performance tests on SRSW algorithms 
recently and found that LinkedBlockingQueue can degrade surprisingly 
(although as David says, if the size is small then threads will collide 
around the same nodes causing quite a lot of work). I found something 
like the following to be a lot quicker in my particular tests (single 
writer creating values, single reader retrieving them, neither really 
doing much else:

class SRSWBlockingQueue {
    final BooleanLatch latch = new BooleanLatch();
    final AtomicReference<List<Integer>> ref = new 
AtomicReference<List<Integer>>();

    public void put(final Integer i) {
        List<Integer> list;
        do {
            list = ref.getAndSet(null);
            if (list == null) {
                list = new ArrayList<Integer>();
            }
            list.add(i);
        }
        while (!ref.compareAndSet(null, list));
        latch.release();
    }

    public List<Integer> drain() throws InterruptedException {
        List<Integer> list;
        do {
            latch.await();
            list = ref.getAndSet(null);
        }
        while (list == null);
        return Collections.unmodifiableList(list);
    }
}

In my tests, the above was 60% faster than LBQ on Java6 (MacOSX) with 
high contention. The above example uses a class called BooleanLatch that 
I have discussed in previous threads on this list (basically a reusable 
single count CountDownLatch), but I found that using Lock/Condition 
wasn't a hell of a lot slower, particularly under high contention, and 
the Lock version might have a couple of advantages in this particular 
case as you can drain while holding the lock.

Anyway, I found it is worth batching your exchange if you can to reduce 
the contention. It is also worth considering creating your own 
concurrent structure for SRSW as the java.util.concurrent classes are 
written to be correct under MRMW which is obviously more complex. The 
caveat is of course that the java.util.concurrent classes *do actually 
work* and writing your own concurrent classes is not trivial.

cheers,
jed.

Daniel Harvey wrote:
> David,
> I have timed the entire onMessage() processing for the producer (which 
> is called very often) and also just the time the producer spends in 
> calling queue.offer() (via sendThread.send, called about 1/5 as 
> often). What is so odd to me is that 90% of the total time spent in 
> onMessage() comes from time spent in queue.offer() despite the fact 
> that this is only called for 20% of the messages. If I have the 
> sendThread skip the socketChannel.write() command, than the situation 
> changes greatly: time spent in queue.offer drops by a literally a 
> factor of 6 or so.
>
> I haven't tracked the queue size over time, but do know that it isn't 
> filling up because I'm not seeing any error messages from 
> queue.offer() returning false. As long as the queue is below its fixed 
> max size, should its actual size affect the amount of work needed to 
> perform a queue insertion?
>
> Anyway, I will certainly try Sun jdk, and look into whether CentOS 
> provides the POSIX api's.
>
> I'll also give Carfield's suggestion a try (thanks), though in past 
> experiments I have done I haven't found the drainTo to be _that_ much 
> more efficient that repeated take()'s.
>
> -Dan
>
> On Aug 9, 2008, at 12:45 AM, David Holmes wrote:
>
>> Dan,
>>
>> Have you actually just timed the queue operations? You indicated that 
>> if the
>> consumer just did take() in a loop then there was a 6x speed-up in the
>> producer. That would seem to indicate that the queue itself is not the
>> problem. It could just be an unfortunate harmonic where the producer and
>> consumer always collide at the queue, but even so the delay shouldn't 
>> result
>> in a 6x slowdown. You might try tracking the queue size to see if 
>> grows and
>> shrinks as expected or whether it oscillates around being empty.
>>
>> AS for whether system calls are involved ... probably, but I don't 
>> know how
>> anything in JRockit is implemented so I can't say for sure. I'm 
>> assuming it
>> will use both of your processors.
>>
>> Can you download JDK 6 and try that for comparison? (You might also 
>> try an
>> evaluation download of Sun Java Real-time System if CentOS provides the
>> real-time POSIX API's - and presuming it's actually a real-time 
>> linux. :) )
>>
>> Cheers,
>> David Holmes


From jed at atlassian.com  Mon Aug 11 03:55:20 2008
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Mon, 11 Aug 2008 17:55:20 +1000
Subject: [concurrency-interest] Single producer,
 single consumer: unexpected delays for producer
In-Reply-To: <489FEB82.20405@atlassian.com>
References: <NFBBKALFDCPFIDBNKAPCMEBPHNAA.dcholmes@optusnet.com.au>	<1FDC0824-64A0-454C-9CC9-CCFF9D5476DE@tachyoncm.com>
	<489FEB82.20405@atlassian.com>
Message-ID: <489FF068.3030708@atlassian.com>

Jed Wesley-Smith wrote:
> The above example uses a class called BooleanLatch that I have 
> discussed in previous threads on this list (basically a reusable 
> single count CountDownLatch), but I found that using Lock/Condition 
> wasn't a hell of a lot slower, particularly under high contention, and 
> the Lock version might have a couple of advantages in this particular 
> case as you can drain while holding the lock.

I should say that with high contention the Lock/Condition impl was a bit 
slower under Java6 (around 10%), and the BooleanLatch was significantly 
quicker under Java5 - about 30%. Under both JVMs the BooleanLatch is 
about twice as quick when uncontended.

cheers,
jed.

From dharvey at tachyoncm.com  Mon Aug 11 12:30:54 2008
From: dharvey at tachyoncm.com (Daniel Harvey)
Date: Mon, 11 Aug 2008 12:30:54 -0400
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMECDHNAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCMECDHNAA.dcholmes@optusnet.com.au>
Message-ID: <5EC48CEB-555A-40B2-AA7E-21B34E56D5A5@tachyoncm.com>

I've tried a number of the above suggestions.

1) The prespinTake degraded the performance a little further, and I  
think suggests that it is not waking the blocked thread that is  
causing the delay.
2) Draining the queue into an ArrayList (Carfield's suggestion) may  
have slightly improved things, but not significantly (a few % maybe).
3) I put some code in to keep track of how many messages are  
typically in the queue when the consumer thread finishes it's take(),  
and it is typically 0... ie the majority (around 80-90%) of the time  
a single message is being removed from the queue.

Having run the tests more times, and for longer duration, I  
apparently overstated the degradation that is caused by the consumer  
executing the socket.write() (in addition to performing the  
encoding):  it appears to be a factor of 3 and not 6. Nevertheless,  
that's still a big difference... specifically:

for (;;) {
	CharBuffer chars = CharBuffer.wrap(queue.take());
	ByteBuffer bytes = encoder.encode(chars);
	socketChannel.write(bytes);
}

causes queue.offer(message) to take 3 times longer to execute than if  
the consumer executes:

for (;;) {
	CharBuffer chars = CharBuffer.wrap(queue.take());
	ByteBuffer bytes = encoder.encode(chars);
}

I still intend to experiment with different JVMs and also to try  
Jed's SRSW code.

Many thanks for all the suggestions so far!

-Dan


On Aug 10, 2008, at 6:46 PM, David Holmes wrote:

> Hi Doug,
>
> But in this case removing the network load in the consumer causes the
> consumer to execute take() more rapidly and so it is more likely to  
> find the
> queue empty, and so block. But this is the case that appears 6x  
> faster. ???
>
> David
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of  
>> Doug Lea
>> Sent: Sunday, 10 August 2008 1:16 AM
>> To: Daniel Harvey
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Single producer, single consumer:
>> unexpected delays for producer
>>
>>
>> Daniel Harvey wrote:
>>>
>>> What is so odd to me is that 90% of the total time spent in  
>>> onMessage()
>>> comes from time spent in queue.offer() despite the fact that this is
>>> only called for 20% of the messages. If I have the sendThread  
>>> skip the
>>> socketChannel.write() command, than the situation changes  
>>> greatly: time
>>> spent in queue.offer drops by a literally a factor of 6 or so.
>>>
>>
>> This may be due to slow thread unblocking in your VM.
>> You can try to avoid this a bit by pre-spinning on take:
>>
>> String prespinTake(BlockingQueue<String> q) {
>>    for (int i = 0; i < SPINS; ++i) {
>>      String x = q.poll();
>>      if (x != null)
>>         return x;
>>    }
>>    return q.take();
>> }
>>
>> where SPINS is a number say between 1 and 1000.
>> This will avoid offer() needing to wake up a blocked take thread.
>>
>> -Doug
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From gregg at cytetech.com  Mon Aug 11 13:25:07 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 11 Aug 2008 12:25:07 -0500
Subject: [concurrency-interest] Single producer,
 single consumer: unexpected delays for producer
In-Reply-To: <5EC48CEB-555A-40B2-AA7E-21B34E56D5A5@tachyoncm.com>
References: <NFBBKALFDCPFIDBNKAPCMECDHNAA.dcholmes@optusnet.com.au>
	<5EC48CEB-555A-40B2-AA7E-21B34E56D5A5@tachyoncm.com>
Message-ID: <48A075F3.7030307@cytetech.com>

Daniel Harvey wrote:
> I've tried a number of the above suggestions.
> 
> 1) The prespinTake degraded the performance a little further, and I 
> think suggests that it is not waking the blocked thread that is causing 
> the delay.
> 2) Draining the queue into an ArrayList (Carfield's suggestion) may have 
> slightly improved things, but not significantly (a few % maybe).
> 3) I put some code in to keep track of how many messages are typically 
> in the queue when the consumer thread finishes it's take(), and it is 
> typically 0... ie the majority (around 80-90%) of the time a single 
> message is being removed from the queue.
> 
> Having run the tests more times, and for longer duration, I apparently 
> overstated the degradation that is caused by the consumer executing the 
> socket.write() (in addition to performing the encoding):  it appears to 
> be a factor of 3 and not 6. Nevertheless, that's still a big 
> difference... specifically:
> 
> for (;;) {
>     CharBuffer chars = CharBuffer.wrap(queue.take());
>     ByteBuffer bytes = encoder.encode(chars);
>     socketChannel.write(bytes);
> }

What happens if you run this in a console window and bang on "Ctrl-\" (or send 
SIGQUIT to it with kill) to see where the threads are "at"?  Do you see any 
"waiting to lock" situations in the thread dumps?

Gregg Wonderly

From dharvey at tachyoncm.com  Mon Aug 11 14:23:58 2008
From: dharvey at tachyoncm.com (Daniel Harvey)
Date: Mon, 11 Aug 2008 14:23:58 -0400
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <489FEB82.20405@atlassian.com>
References: <NFBBKALFDCPFIDBNKAPCMEBPHNAA.dcholmes@optusnet.com.au>
	<1FDC0824-64A0-454C-9CC9-CCFF9D5476DE@tachyoncm.com>
	<489FEB82.20405@atlassian.com>
Message-ID: <89C1968B-C8C7-44D3-AFDF-E8A04A4A7C98@tachyoncm.com>

Jed,
thanks for the code snippet. I tried implementing this instead of the  
LinkedBlockingQueue, but found the performance to be slower under my  
particular scenario (about 50% slower) for some reason.
Next step is to try different JVMs...
-Dan

On Aug 11, 2008, at 3:34 AM, Jed Wesley-Smith wrote:

> Daniel, I have been doing some performance tests on SRSW algorithms  
> recently and found that LinkedBlockingQueue can degrade  
> surprisingly (although as David says, if the size is small then  
> threads will collide around the same nodes causing quite a lot of  
> work). I found something like the following to be a lot quicker in  
> my particular tests (single writer creating values, single reader  
> retrieving them, neither really doing much else:
>
> class SRSWBlockingQueue {
>    final BooleanLatch latch = new BooleanLatch();
>    final AtomicReference<List<Integer>> ref = new  
> AtomicReference<List<Integer>>();
>
>    public void put(final Integer i) {
>        List<Integer> list;
>        do {
>            list = ref.getAndSet(null);
>            if (list == null) {
>                list = new ArrayList<Integer>();
>            }
>            list.add(i);
>        }
>        while (!ref.compareAndSet(null, list));
>        latch.release();
>    }
>
>    public List<Integer> drain() throws InterruptedException {
>        List<Integer> list;
>        do {
>            latch.await();
>            list = ref.getAndSet(null);
>        }
>        while (list == null);
>        return Collections.unmodifiableList(list);
>    }
> }
>
> In my tests, the above was 60% faster than LBQ on Java6 (MacOSX)  
> with high contention. The above example uses a class called  
> BooleanLatch that I have discussed in previous threads on this list  
> (basically a reusable single count CountDownLatch), but I found  
> that using Lock/Condition wasn't a hell of a lot slower,  
> particularly under high contention, and the Lock version might have  
> a couple of advantages in this particular case as you can drain  
> while holding the lock.
>
> Anyway, I found it is worth batching your exchange if you can to  
> reduce the contention. It is also worth considering creating your  
> own concurrent structure for SRSW as the java.util.concurrent  
> classes are written to be correct under MRMW which is obviously  
> more complex. The caveat is of course that the java.util.concurrent  
> classes *do actually work* and writing your own concurrent classes  
> is not trivial.
>
> cheers,
> jed.
>
> Daniel Harvey wrote:
>> David,
>> I have timed the entire onMessage() processing for the producer  
>> (which is called very often) and also just the time the producer  
>> spends in calling queue.offer() (via sendThread.send, called about  
>> 1/5 as often). What is so odd to me is that 90% of the total time  
>> spent in onMessage() comes from time spent in queue.offer()  
>> despite the fact that this is only called for 20% of the messages.  
>> If I have the sendThread skip the socketChannel.write() command,  
>> than the situation changes greatly: time spent in queue.offer  
>> drops by a literally a factor of 6 or so.
>>
>> I haven't tracked the queue size over time, but do know that it  
>> isn't filling up because I'm not seeing any error messages from  
>> queue.offer() returning false. As long as the queue is below its  
>> fixed max size, should its actual size affect the amount of work  
>> needed to perform a queue insertion?
>>
>> Anyway, I will certainly try Sun jdk, and look into whether CentOS  
>> provides the POSIX api's.
>>
>> I'll also give Carfield's suggestion a try (thanks), though in  
>> past experiments I have done I haven't found the drainTo to be  
>> _that_ much more efficient that repeated take()'s.
>>
>> -Dan
>>
>> On Aug 9, 2008, at 12:45 AM, David Holmes wrote:
>>
>>> Dan,
>>>
>>> Have you actually just timed the queue operations? You indicated  
>>> that if the
>>> consumer just did take() in a loop then there was a 6x speed-up  
>>> in the
>>> producer. That would seem to indicate that the queue itself is  
>>> not the
>>> problem. It could just be an unfortunate harmonic where the  
>>> producer and
>>> consumer always collide at the queue, but even so the delay  
>>> shouldn't result
>>> in a 6x slowdown. You might try tracking the queue size to see if  
>>> grows and
>>> shrinks as expected or whether it oscillates around being empty.
>>>
>>> AS for whether system calls are involved ... probably, but I  
>>> don't know how
>>> anything in JRockit is implemented so I can't say for sure. I'm  
>>> assuming it
>>> will use both of your processors.
>>>
>>> Can you download JDK 6 and try that for comparison? (You might  
>>> also try an
>>> evaluation download of Sun Java Real-time System if CentOS  
>>> provides the
>>> real-time POSIX API's - and presuming it's actually a real-time  
>>> linux. :) )
>>>
>>> Cheers,
>>> David Holmes
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dcholmes at optusnet.com.au  Mon Aug 11 20:29:17 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 12 Aug 2008 10:29:17 +1000
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <5EC48CEB-555A-40B2-AA7E-21B34E56D5A5@tachyoncm.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAECKHNAA.dcholmes@optusnet.com.au>

Dan,

> 3) I put some code in to keep track of how many messages are
> typically in the queue when the consumer thread finishes it's take(),
> and it is typically 0... ie the majority (around 80-90%) of the time
> a single message is being removed from the queue.

Okay so that explains the contention. The producer is expending effort
acquiring two locks and waking up the producer.

> that's still a big difference... specifically:
>
> for (;;) {
> 	CharBuffer chars = CharBuffer.wrap(queue.take());
> 	ByteBuffer bytes = encoder.encode(chars);
> 	socketChannel.write(bytes);
> }
>
> causes queue.offer(message) to take 3 times longer to execute than if
> the consumer executes:
>
> for (;;) {
> 	CharBuffer chars = CharBuffer.wrap(queue.take());
> 	ByteBuffer bytes = encoder.encode(chars);
> }

Can I assume that in the fast case the queue size is not normally zero? If
so my theory here is that when the consumer uses the socketChannel it is
introducing additional contention with the prodcuer that keeps the producer
and conumer effectively in lock-step:
- consumer blocks on take()
- producer gets item and does offer() and has to wakeup consumer
- consumer processes item and uses resources needed by producer
- producer is blocked or delayed getting next item
- consumer calls take() again and blocks
- producer produces and calls offer() and has to wakeup consumer
- repeat

However in that case I would expect:
  for(;;) queue.take();
to also impose maximum contention with the producer.

David


From dharvey at tachyoncm.com  Mon Aug 11 20:47:38 2008
From: dharvey at tachyoncm.com (Daniel Harvey)
Date: Mon, 11 Aug 2008 20:47:38 -0400
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAECKHNAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCAECKHNAA.dcholmes@optusnet.com.au>
Message-ID: <85F1DE08-3206-4B70-B46E-C19B2BD8E74D@tachyoncm.com>

I'm not sure whether the queue size is non-zero in the fast case,  
though I will test this tomorrow and report back. But something  
doesn't make sense here because in the second case -  the continuous  
take() one - there should be an even greater likelihood of  
encountering a non-empty queue... I really feel like the issue has to  
be the socket write interfering with the thread-signaling of the  
existence of a new message to process: I'm fairly sure that I could  
add other time-taking calculations to the consumer thread (in place  
of the socket.write() ) and they wouldn't slow down the producer.  
I'll add something like this tomorrow and see whether my assessment  
is accurate.

The following isn't a very elegant solution, and it might hammer the  
CPU too much, but I have experimented with having the consumer do a  
periodic sending of a batch of messages, then sleeping and checking  
for new messages every few ms (rather than waiting), and repeating.  
With this approach there's no signal between threads and I think the  
producer operates much faster (even with the necessary  
synchronization). Of course I am adding latency, but if throughput is  
the bottom line, it might be a useable solution.

-Dan

On Aug 11, 2008, at 8:29 PM, David Holmes wrote:

> Dan,
>
>> 3) I put some code in to keep track of how many messages are
>> typically in the queue when the consumer thread finishes it's take(),
>> and it is typically 0... ie the majority (around 80-90%) of the time
>> a single message is being removed from the queue.
>
> Okay so that explains the contention. The producer is expending effort
> acquiring two locks and waking up the producer.
>
>> that's still a big difference... specifically:
>>
>> for (;;) {
>> 	CharBuffer chars = CharBuffer.wrap(queue.take());
>> 	ByteBuffer bytes = encoder.encode(chars);
>> 	socketChannel.write(bytes);
>> }
>>
>> causes queue.offer(message) to take 3 times longer to execute than if
>> the consumer executes:
>>
>> for (;;) {
>> 	CharBuffer chars = CharBuffer.wrap(queue.take());
>> 	ByteBuffer bytes = encoder.encode(chars);
>> }
>
> Can I assume that in the fast case the queue size is not normally  
> zero? If
> so my theory here is that when the consumer uses the socketChannel  
> it is
> introducing additional contention with the prodcuer that keeps the  
> producer
> and conumer effectively in lock-step:
> - consumer blocks on take()
> - producer gets item and does offer() and has to wakeup consumer
> - consumer processes item and uses resources needed by producer
> - producer is blocked or delayed getting next item
> - consumer calls take() again and blocks
> - producer produces and calls offer() and has to wakeup consumer
> - repeat
>
> However in that case I would expect:
>   for(;;) queue.take();
> to also impose maximum contention with the producer.
>
> David
>


From dcholmes at optusnet.com.au  Mon Aug 11 20:50:40 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 12 Aug 2008 10:50:40 +1000
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <85F1DE08-3206-4B70-B46E-C19B2BD8E74D@tachyoncm.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIECKHNAA.dcholmes@optusnet.com.au>

Dan,

Your second case isn't a continuous take() because it does:

ByteBuffer bytes = encoder.encode(chars);

that might take sufficient time for the producer to produce the next item
(or two) with no contention.

That's why I said that I would expect:

for(;;) queue.take();

to also cause maximum contention.

David

> -----Original Message-----
> From: Daniel Harvey [mailto:dharvey at tachyoncm.com]
> Sent: Tuesday, 12 August 2008 10:48 AM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Single producer,single consumer:
> unexpected delays for producer
>
>
> I'm not sure whether the queue size is non-zero in the fast case,
> though I will test this tomorrow and report back. But something
> doesn't make sense here because in the second case -  the continuous
> take() one - there should be an even greater likelihood of
> encountering a non-empty queue... I really feel like the issue has to
> be the socket write interfering with the thread-signaling of the
> existence of a new message to process: I'm fairly sure that I could
> add other time-taking calculations to the consumer thread (in place
> of the socket.write() ) and they wouldn't slow down the producer.
> I'll add something like this tomorrow and see whether my assessment
> is accurate.
>
> The following isn't a very elegant solution, and it might hammer the
> CPU too much, but I have experimented with having the consumer do a
> periodic sending of a batch of messages, then sleeping and checking
> for new messages every few ms (rather than waiting), and repeating.
> With this approach there's no signal between threads and I think the
> producer operates much faster (even with the necessary
> synchronization). Of course I am adding latency, but if throughput is
> the bottom line, it might be a useable solution.
>
> -Dan
>
> On Aug 11, 2008, at 8:29 PM, David Holmes wrote:
>
> > Dan,
> >
> >> 3) I put some code in to keep track of how many messages are
> >> typically in the queue when the consumer thread finishes it's take(),
> >> and it is typically 0... ie the majority (around 80-90%) of the time
> >> a single message is being removed from the queue.
> >
> > Okay so that explains the contention. The producer is expending effort
> > acquiring two locks and waking up the producer.
> >
> >> that's still a big difference... specifically:
> >>
> >> for (;;) {
> >> 	CharBuffer chars = CharBuffer.wrap(queue.take());
> >> 	ByteBuffer bytes = encoder.encode(chars);
> >> 	socketChannel.write(bytes);
> >> }
> >>
> >> causes queue.offer(message) to take 3 times longer to execute than if
> >> the consumer executes:
> >>
> >> for (;;) {
> >> 	CharBuffer chars = CharBuffer.wrap(queue.take());
> >> 	ByteBuffer bytes = encoder.encode(chars);
> >> }
> >
> > Can I assume that in the fast case the queue size is not normally
> > zero? If
> > so my theory here is that when the consumer uses the socketChannel
> > it is
> > introducing additional contention with the prodcuer that keeps the
> > producer
> > and conumer effectively in lock-step:
> > - consumer blocks on take()
> > - producer gets item and does offer() and has to wakeup consumer
> > - consumer processes item and uses resources needed by producer
> > - producer is blocked or delayed getting next item
> > - consumer calls take() again and blocks
> > - producer produces and calls offer() and has to wakeup consumer
> > - repeat
> >
> > However in that case I would expect:
> >   for(;;) queue.take();
> > to also impose maximum contention with the producer.
> >
> > David
> >
>


From teck at terracottatech.com  Tue Aug 12 14:52:48 2008
From: teck at terracottatech.com (Tim Eck)
Date: Tue, 12 Aug 2008 11:52:48 -0700 (PDT)
Subject: [concurrency-interest] lock acquire/release order in CHM.size()
Message-ID: <C9AA20BF37F14CF0AD7B37A6CECCAF97@PIGGY2>

Just a mild curiosity of mine that folks on this list might have insight
into...

When operations like size() on j.u.c.ConcurrentHashMap have to resort to
locking, the locks are acquired and released like so:

  for (int i = 0; i < segments.length; ++i) 
      segments[i].lock();
  /* ... */ 
  for (int i = 0; i < segments.length; ++i)
      segments[i].unlock();

The curious bit is that the last lock obtained is not the first lock
released. I'm not suggesting there is anything wrong with that, but I was
wondering if there is a specific rationale for that? I suppose two threads
going through those loops would be less serialized since one thread could
start acquiring in lock step with the other thread doing the unlocks. 

thanks
-tim

From dharvey at tachyoncm.com  Tue Aug 12 14:55:51 2008
From: dharvey at tachyoncm.com (Daniel Harvey)
Date: Tue, 12 Aug 2008 14:55:51 -0400
Subject: [concurrency-interest] Single producer,
	single consumer: unexpected delays for producer
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIECKHNAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCIECKHNAA.dcholmes@optusnet.com.au>
Message-ID: <0EDAF098-C1F6-4E24-858E-97EF05FA3B68@tachyoncm.com>

Having tested today, I found that performing:
for (;;) queue.take();
causes very little more contention that the best scenario, and much  
less than when the socket.write() is being performed.

On Aug 11, 2008, at 8:50 PM, David Holmes wrote:

> Dan,
>
> Your second case isn't a continuous take() because it does:
>
> ByteBuffer bytes = encoder.encode(chars);
>
> that might take sufficient time for the producer to produce the  
> next item
> (or two) with no contention.
>
> That's why I said that I would expect:
>
> for(;;) queue.take();
>
> to also cause maximum contention.
>
> David
>
>> -----Original Message-----
>> From: Daniel Harvey [mailto:dharvey at tachyoncm.com]
>> Sent: Tuesday, 12 August 2008 10:48 AM
>> To: dholmes at ieee.org
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Single producer,single consumer:
>> unexpected delays for producer
>>
>>
>> I'm not sure whether the queue size is non-zero in the fast case,
>> though I will test this tomorrow and report back. But something
>> doesn't make sense here because in the second case -  the continuous
>> take() one - there should be an even greater likelihood of
>> encountering a non-empty queue... I really feel like the issue has to
>> be the socket write interfering with the thread-signaling of the
>> existence of a new message to process: I'm fairly sure that I could
>> add other time-taking calculations to the consumer thread (in place
>> of the socket.write() ) and they wouldn't slow down the producer.
>> I'll add something like this tomorrow and see whether my assessment
>> is accurate.
>>
>> The following isn't a very elegant solution, and it might hammer the
>> CPU too much, but I have experimented with having the consumer do a
>> periodic sending of a batch of messages, then sleeping and checking
>> for new messages every few ms (rather than waiting), and repeating.
>> With this approach there's no signal between threads and I think the
>> producer operates much faster (even with the necessary
>> synchronization). Of course I am adding latency, but if throughput is
>> the bottom line, it might be a useable solution.
>>
>> -Dan
>>
>> On Aug 11, 2008, at 8:29 PM, David Holmes wrote:
>>
>>> Dan,
>>>
>>>> 3) I put some code in to keep track of how many messages are
>>>> typically in the queue when the consumer thread finishes it's  
>>>> take(),
>>>> and it is typically 0... ie the majority (around 80-90%) of the  
>>>> time
>>>> a single message is being removed from the queue.
>>>
>>> Okay so that explains the contention. The producer is expending  
>>> effort
>>> acquiring two locks and waking up the producer.
>>>
>>>> that's still a big difference... specifically:
>>>>
>>>> for (;;) {
>>>> 	CharBuffer chars = CharBuffer.wrap(queue.take());
>>>> 	ByteBuffer bytes = encoder.encode(chars);
>>>> 	socketChannel.write(bytes);
>>>> }
>>>>
>>>> causes queue.offer(message) to take 3 times longer to execute  
>>>> than if
>>>> the consumer executes:
>>>>
>>>> for (;;) {
>>>> 	CharBuffer chars = CharBuffer.wrap(queue.take());
>>>> 	ByteBuffer bytes = encoder.encode(chars);
>>>> }
>>>
>>> Can I assume that in the fast case the queue size is not normally
>>> zero? If
>>> so my theory here is that when the consumer uses the socketChannel
>>> it is
>>> introducing additional contention with the prodcuer that keeps the
>>> producer
>>> and conumer effectively in lock-step:
>>> - consumer blocks on take()
>>> - producer gets item and does offer() and has to wakeup consumer
>>> - consumer processes item and uses resources needed by producer
>>> - producer is blocked or delayed getting next item
>>> - consumer calls take() again and blocks
>>> - producer produces and calls offer() and has to wakeup consumer
>>> - repeat
>>>
>>> However in that case I would expect:
>>>   for(;;) queue.take();
>>> to also impose maximum contention with the producer.
>>>
>>> David
>>>
>>
>


From gregg at cytetech.com  Tue Aug 12 16:42:14 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 12 Aug 2008 15:42:14 -0500
Subject: [concurrency-interest] Single producer,
 single consumer: unexpected delays for producer
In-Reply-To: <0EDAF098-C1F6-4E24-858E-97EF05FA3B68@tachyoncm.com>
References: <NFBBKALFDCPFIDBNKAPCIECKHNAA.dcholmes@optusnet.com.au>
	<0EDAF098-C1F6-4E24-858E-97EF05FA3B68@tachyoncm.com>
Message-ID: <48A1F5A6.4070606@cytetech.com>

Daniel Harvey wrote:
> Having tested today, I found that performing:
> for (;;) queue.take();
> causes very little more contention that the best scenario, and much less 
> than when the socket.write() is being performed.

Have you tried putting another queue in place to send the writes off to another 
thread?  This would let you have more than one thread doing encodes() and might 
actually allow some overlapping I/O and CPU too, which will be a throughput 
improvement.

I also didn't see whether you had used CTRL-\ or kill -3 <jvmpid> to get some 
stack traces to see if they revealed the threads always "stuck" at the same 
spot, or perhaps a "waiting to lock" state that might indicate, better, what the 
contentious resource is.

Gregg Wonderly

From brian at briangoetz.com  Tue Aug 12 23:53:34 2008
From: brian at briangoetz.com (Brian Goetz)
Date: Tue, 12 Aug 2008 23:53:34 -0400
Subject: [concurrency-interest] concurrency-interest now archived on MarkMail
Message-ID: <48A25ABE.6020609@briangoetz.com>

The concurrency-interest list is now archived using the most excellent 
MarkMail mail navigator, developed by Jason Hunter of MarkLogic.

   http://concurrency.markmail.org/



From fabio.veronez at gmail.com  Fri Aug 15 10:32:07 2008
From: fabio.veronez at gmail.com (Fabio Cechinel Veronez)
Date: Fri, 15 Aug 2008 11:32:07 -0300
Subject: [concurrency-interest] Doubt on happens-before and I/O operations.
Message-ID: <2ec434e70808150732q1eadd91fs2867ae18fc3b0985@mail.gmail.com>

Hello all,

I have a doubt (maybe a dummy one) about happens-before relation and
I/O operations.

In the following example code.

01: class FileItem {
02:     private final File target;
03:     private volatile boolean written = false;
04:
05:     public FileItem(File target) {
06:         this.target = target;
07:     }
08:
09:     public void initialize() throws IOException {
11:         if (written)
12:             return;
13:         OutputStream out = new FileOutputStream(target);
14:         try {
15:             byte[] content = // retrieves the content from somewhere ...
16:             out.write(content);
17:         } finally {
18:             out.close();
19:         }
20:         written = true;
21:     }
22:
23:     public InputStream stream() throws IOException {
24:         if (!written)
25:             throw new IllegalStateException("unwritten data");
26:         return new FileInputStream(target);
27:     }
28: }

Is it safe to say that once a Thread X reachs line 26 the content was
completely written into the 'target' file or is it possible to have
any reordering concerning the write on 'written' field (line 20) and
the output operation?

Thanks in advance.

-- 
Fabio Cechinel Veronez
Bacharel em Ci?ncias da Computa??o - UFSC
Sun Certified Programmer For The Java 2 Platform 1.4


From mthornton at optrak.co.uk  Fri Aug 15 11:13:55 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Fri, 15 Aug 2008 16:13:55 +0100
Subject: [concurrency-interest] Doubt on happens-before and I/O
	operations.
In-Reply-To: <2ec434e70808150732q1eadd91fs2867ae18fc3b0985@mail.gmail.com>
References: <2ec434e70808150732q1eadd91fs2867ae18fc3b0985@mail.gmail.com>
Message-ID: <48A59D33.7080909@optrak.co.uk>

Fabio Cechinel Veronez wrote:
> Hello all,
>
> I have a doubt (maybe a dummy one) about happens-before relation and
> I/O operations.
>
> In the following example code.
>
> 01: class FileItem {
> 02:     private final File target;
> 03:     private volatile boolean written = false;
> 04:
> 05:     public FileItem(File target) {
> 06:         this.target = target;
> 07:     }
> 08:
> 09:     public void initialize() throws IOException {
> 11:         if (written)
> 12:             return;
> 13:         OutputStream out = new FileOutputStream(target);
> 14:         try {
> 15:             byte[] content = // retrieves the content from somewhere ...
> 16:             out.write(content);
> 17:         } finally {
> 18:             out.close();
> 19:         }
> 20:         written = true;
> 21:     }
> 22:
> 23:     public InputStream stream() throws IOException {
> 24:         if (!written)
> 25:             throw new IllegalStateException("unwritten data");
> 26:         return new FileInputStream(target);
> 27:     }
> 28: }
>
> Is it safe to say that once a Thread X reachs line 26 the content was
> completely written into the 'target' file or is it possible to have
> any reordering concerning the write on 'written' field (line 20) and
> the output operation?
>
> Thanks in advance.
>   
It s possible for two threads (or more) to be simultaneously executing 
lines 13 - 20. The first one to reach line 20 will set written to true, 
but the second thread will still be writing data. So the file may be 
incomplete or may contain multiple copies of the data. I would not 
recommend using volatile in this way.

Mark Thornton



From gluck at gregluck.com  Sat Aug 16 07:54:53 2008
From: gluck at gregluck.com (Greg Luck)
Date: Sat, 16 Aug 2008 21:54:53 +1000
Subject: [concurrency-interest] ThreadPoolExecutor corePoolSize
	incompatibility between backport and j.u.c.
Message-ID: <FFBA6BA8-9B3C-48A5-B212-E810DB6998AD@gregluck.com>

I am looking at moving the new version of ehcache to j.u.c, from  
backport, as part of dropping support for Java 1.4.

It turns out that all my tests which test code that uses backport's  
ThreadPoolExecutor never complete. The code which supplies the  
executor is shown below:

    /**
      * @return Gets the executor service. This is not publically  
accessible.
      */
     ThreadPoolExecutor getExecutorService() {
         if (executorService == null) {
             synchronized (this) {
                 //0, 10, 60000, ?, Integer.MAXVALUE
                 executorService = new ThreadPoolExecutor(0,10,
                        60000, TimeUnit.MILLISECONDS, new  
LinkedBlockingQueue());
             }
         }
         return executorService;
     }


Without understanding exactly why, I tried changing the corePoolSize  
to 1. This fixed the problem. My question is why? There is nothing in  
the JavaDoc I can see which says you must use a minimum of 1.

And I am now worried that j.u.c. is flaky in Java 5.  Any comments  
from the list would be much appreciated.

Regards

Greg Luck

web: http://gregluck.com
skype: gregrluck
yahoo: gregrluck
mobile: +61 408 061 622



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080816/5f964188/attachment.html>

From martinrb at google.com  Sat Aug 16 08:34:14 2008
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 16 Aug 2008 05:34:14 -0700
Subject: [concurrency-interest] ThreadPoolExecutor corePoolSize
	incompatibility between backport and j.u.c.
In-Reply-To: <FFBA6BA8-9B3C-48A5-B212-E810DB6998AD@gregluck.com>
References: <FFBA6BA8-9B3C-48A5-B212-E810DB6998AD@gregluck.com>
Message-ID: <1ccfd1c10808160534h57f630fk2a1a2f90a4d136d6@mail.gmail.com>

Many bugs in ThreadPoolExecutor were fixed during late JDK6 and early JDK7.
Can you try a recent JDK7 first and see if the problem is resolved?

Martin

On Sat, Aug 16, 2008 at 4:54 AM, Greg Luck <gluck at gregluck.com> wrote:
> I am looking at moving the new version of ehcache to j.u.c, from backport,
> as part of dropping support for Java 1.4.
> It turns out that all my tests which test code that uses backport's
> ThreadPoolExecutor never complete. The code which supplies the executor is
> shown below:
>    /**
>      * @return Gets the executor service. This is not publically accessible.
>      */
>     ThreadPoolExecutor getExecutorService() {
>         if (executorService == null) {
>             synchronized (this) {
>                 //0, 10, 60000, ?, Integer.MAXVALUE
>                 executorService = new ThreadPoolExecutor(0,10,
>                        60000, TimeUnit.MILLISECONDS, new
> LinkedBlockingQueue());
>             }
>         }
>         return executorService;
>     }
>
> Without understanding exactly why, I tried changing the corePoolSize to 1.
> This fixed the problem. My question is why? There is nothing in the JavaDoc
> I can see which says you must use a minimum of 1.
> And I am now worried that j.u.c. is flaky in Java 5.  Any comments from the
> list would be much appreciated.
>
> Regards
>
> Greg Luck
>
> web: http://gregluck.com
> skype: gregrluck
> yahoo: gregrluck
> mobile: +61 408 061 622
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From takeshi10 at gmail.com  Sat Aug 16 10:53:26 2008
From: takeshi10 at gmail.com (Marcelo Fukushima)
Date: Sat, 16 Aug 2008 11:53:26 -0300
Subject: [concurrency-interest] ThreadPoolExecutor corePoolSize
	incompatibility between backport and j.u.c.
In-Reply-To: <1ccfd1c10808160534h57f630fk2a1a2f90a4d136d6@mail.gmail.com>
References: <FFBA6BA8-9B3C-48A5-B212-E810DB6998AD@gregluck.com>
	<1ccfd1c10808160534h57f630fk2a1a2f90a4d136d6@mail.gmail.com>
Message-ID: <7288749d0808160753m1e4c25bck3118b6616a940387@mail.gmail.com>

the problem is most likely due to a particular behavior on juc: it
only creates threads when the queue is full. Im not sure it needs
fixing (it suggests so) or if its already fixed in jdk7

On Sat, Aug 16, 2008 at 9:34 AM, Martin Buchholz <martinrb at google.com> wrote:
> Many bugs in ThreadPoolExecutor were fixed during late JDK6 and early JDK7.
> Can you try a recent JDK7 first and see if the problem is resolved?
>
> Martin
>
> On Sat, Aug 16, 2008 at 4:54 AM, Greg Luck <gluck at gregluck.com> wrote:
>> I am looking at moving the new version of ehcache to j.u.c, from backport,
>> as part of dropping support for Java 1.4.
>> It turns out that all my tests which test code that uses backport's
>> ThreadPoolExecutor never complete. The code which supplies the executor is
>> shown below:
>>    /**
>>      * @return Gets the executor service. This is not publically accessible.
>>      */
>>     ThreadPoolExecutor getExecutorService() {
>>         if (executorService == null) {
>>             synchronized (this) {
>>                 //0, 10, 60000, ?, Integer.MAXVALUE
>>                 executorService = new ThreadPoolExecutor(0,10,
>>                        60000, TimeUnit.MILLISECONDS, new
>> LinkedBlockingQueue());
>>             }
>>         }
>>         return executorService;
>>     }
>>
>> Without understanding exactly why, I tried changing the corePoolSize to 1.
>> This fixed the problem. My question is why? There is nothing in the JavaDoc
>> I can see which says you must use a minimum of 1.
>> And I am now worried that j.u.c. is flaky in Java 5.  Any comments from the
>> list would be much appreciated.
>>
>> Regards
>>
>> Greg Luck
>>
>> web: http://gregluck.com
>> skype: gregrluck
>> yahoo: gregrluck
>> mobile: +61 408 061 622
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
[]'s
Marcelo Takeshi Fukushima

From ben_manes at yahoo.com  Sat Aug 16 14:59:17 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Sat, 16 Aug 2008 11:59:17 -0700 (PDT)
Subject: [concurrency-interest] ThreadPoolExecutor corePoolSize
	incompatibility between backport and j.u.c.
Message-ID: <236019.74341.qm@web38808.mail.mud.yahoo.com>

It is in the JavaDoc.  Setting the core size to 1 only fixed the problem by making one thread available, but it will never grow.  If you really want this behavior, there are tricks to do it.

---

Core and maximum pool sizes
A ThreadPoolExecutor will automatically adjust the pool size (see getPoolSize()) according to the bounds set by corePoolSize (see getCorePoolSize()) and maximumPoolSize (see getMaximumPoolSize()). When a new task is submitted in method execute(java.lang.Runnable), and fewer than corePoolSize threads are running, a new thread is created to handle the request, even if other worker threads are idle. If there are more than corePoolSize but less than maximumPoolSize threads running, a new thread will be created only if the queue is full. By setting corePoolSize and maximumPoolSize the same, you create a fixed-size thread pool. By setting maximumPoolSize to an essentially unbounded value such as Integer.MAX_VALUE, you allow the pool to accommodate an arbitrary number of concurrent tasks. Most typically, core and maximum pool sizes are set only upon construction, but they may also be changed dynamically using setCorePoolSize(int) and setMaximumPoolSize(int).

Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to be queued in cases where all corePoolSize threads are busy. Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.) This may be appropriate when each task is completely independent of others, so tasks cannot affect each others execution; for example, in a web page server. While this style of queuing can be useful in smoothing out transient bursts of requests, it admits the possibility of unbounded work queue growth when commands continue to arrive on average faster than they can be processed.


----- Original Message ----
From: Greg Luck <gluck at gregluck.com>
To: concurrency-interest at cs.oswego.edu
Sent: Saturday, August 16, 2008 4:54:53 AM
Subject: [concurrency-interest] ThreadPoolExecutor corePoolSize incompatibility between backport and j.u.c.


I am looking at moving the new version of ehcache to j.u.c, from backport, as part of dropping support for Java 1.4.

It turns out that all my tests which test code that uses backport's ThreadPoolExecutor never complete. The code which supplies the executor is shown below:

   /**
     * @return Gets the executor service. This is not publically accessible.
     */
    ThreadPoolExecutor getExecutorService() {
        if (executorService == null) {
            synchronized (this) {
                //0, 10, 60000, ?, Integer.MAXVALUE
                executorService = new ThreadPoolExecutor(0,10,
                       60000, TimeUnit.MILLISECONDS, new LinkedBlockingQueue());
            }
        }
        return executorService;
    }


Without understanding exactly why, I tried changing the corePoolSize to 1. This fixed the problem. My question is why? There is nothing in the JavaDoc I can see which says you must use a minimum of 1. 

And I am now worried that j.u.c. is flaky in Java 5.  Any comments from the list would be much appreciated.

Regards

Greg Luck

web: http://gregluck.com
skype: gregrluck
yahoo: gregrluckmobile: +61 408 061 622


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080816/19fc853e/attachment.html>

From David.Biesack at sas.com  Mon Aug 18 14:32:20 2008
From: David.Biesack at sas.com (David J. Biesack)
Date: Mon, 18 Aug 2008 14:32:20 -0400
Subject: [concurrency-interest] jsr166y IndexedProcedure ?
Message-ID: <upro6dvij.fsf@sas.com>


Back in January, there was some discussion of the jsr166y
forkjoin/ParallelArray API. I had proposed the ability to pass the
element index to the procedure via a ParallelArray.apply. Doug replied

  "... the lack of an indexed-procedure version of apply is
   a bug; thanks for pointing it out! This will be added."

  http://cs.oswego.edu/pipermail/concurrency-interest/2008-January/004917.html
  http://cs.oswego.edu/pipermail/concurrency-interest/2008-January/004922.html

I've hit another case where this would be useful, so I'm just checking the status
of any API changes along these lines.

My use case: implementing ParallelDoubleArray.SummaryStatistics
and similar behaviors which do not mutate the ParallelDoubleArray.

I can't use summary() because I want new behavior: additional statistics
like variance and standard deviation, and pluggable value filtering;
i.e. do not include NaNs in size/sum/min/max/average.

I can't use a DoubleProcedure because I need the item index in order to
implement minIndex() and maxIndex().

I can use pda.replaceWithMappedIndex(Ops.IntAndDoubleToDouble) but that
performs extra work of assigning elements which I don't need. I'd like
to be able to reason about immutability, and replace* methods prevents
that.

-- 
David J. Biesack     SAS Institute Inc.
(919) 531-7771       SAS Campus Drive
http://www.sas.com   Cary, NC 27513

From dl at cs.oswego.edu  Mon Aug 18 15:40:55 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 18 Aug 2008 15:40:55 -0400
Subject: [concurrency-interest] jsr166y IndexedProcedure ?
In-Reply-To: <upro6dvij.fsf@sas.com>
References: <upro6dvij.fsf@sas.com>
Message-ID: <48A9D047.5040207@cs.oswego.edu>

David J. Biesack wrote:
> Back in January, there was some discussion of the jsr166y
> forkjoin/ParallelArray API. I had proposed the ability to pass the
> element index to the procedure via a ParallelArray.apply. Doug replied
> 
>   "... the lack of an indexed-procedure version of apply is
>    a bug; thanks for pointing it out! This will be added."
> 
> I've hit another case where this would be useful, so I'm just checking the status
> of any API changes along these lines.
> 

Still on the todo list. Sorry for the long delay. I've been
revamping some of the underlying FJ mechanics to provide
better support for extended capabilities (phasers, new forms
of Async tasks etc) that also invite use of some better algorithms
underlying ParallelArray. All this needs to come together a
bit more before checkin/release. Hopefully within a week or
so, followed soon after by business-as-usual improvements like
this one.

-Doug



From aarong at cs.cmu.edu  Wed Aug 20 13:21:20 2008
From: aarong at cs.cmu.edu (Aaron Greenhouse)
Date: Wed, 20 Aug 2008 13:21:20 -0400
Subject: [concurrency-interest] Cost of Inserting bytecodes around
	monitorenter and monitorexit
Message-ID: <C8A9E471-DCC4-456C-AEB4-599DC7793DFE@cs.cmu.edu>

(Sorry this is a long message.)

Hi everyone.  I have a question/problem that is a bit off topic, but I
need the advice of those who understand synchronization and the JVM
intimately.  I am manipulating Java classfiles (using ASM) as part of a
profiling project.  I was trying to understand the overhead created by
the inserted bytecodes, and I was surprised to see how much overhead I
have when I instrument around monitorenter and monitorexit operations.
Primarily my inserted bytecodes set up method calls to a data collection
class.  I insert a method call before and after monitorenter and after
moniterexit.  In this case, I have subsituted a data collection class
that has empty methods.

I am concerned because on my test case, the unintrumented program runs
in under 4 minutes.  If I instrument only monitorenter and monitorexit
bytecodes, the program takes over 20 minutes to run.  Before you tell me
that it is the inserted method calls that cause the problem, let me say
this: If I instead instrument only method calls which also causes 3
additional methods to be called for each original method call, the  
entire
program runs in around 5 minutes.  So I think there is something very
strange going on related to synchronization.

To be more specific, I have created a simpler test class. This class
doesn't do anything useful.  It is just designed to exercise a bunch of
synchronized blocks.  All the important stuff is in the Shared class.

public class Main {
   public static void main(final String[] args) {
     System.out.println("Main");

     final Shared s = new Shared();
     final Thread threadA = new Thread(new A(s));
     final Thread threadB = new Thread(new B(s));

     final long startTime = System.currentTimeMillis();
     threadA.start();
     threadB.start();

     while (threadA.isAlive() && threadB.isAlive()) {
       try {
         threadA.join();
         threadB.join();
       } catch (final InterruptedException e) {
         // won't happen
       }
     }

     final long endTime = System.currentTimeMillis();
     final long time = endTime - startTime;
     System.out.println("Done in " + time + "ms (" + ((time/1000.0)/ 
60.0) + " minutes)");
   }
}

class A implements Runnable {
   private static final int START = 2500000;
   private Shared s;

   public A(final Shared s) {
     this.s = s;
     s.forB(START);
   }

   public void run() {
     while (true) {
       try {
         final int v = s.fromB();
         if (v <= 0) {
           s.forB(v);
           break;
         }
         s.forB(v+1);
       } catch(final InterruptedException e) {
         // won't happen
       }
     }
   }
}

class B implements Runnable {
   private Shared s;

   public B(final Shared s) {
     this.s = s;
   }

   public void run() {
     while (true) {
       try {
         final int v = s.fromA();
         if (v <= 0) {
           s.forA(v);
           break;
         }
         s.forA(v-2);
       } catch (final InterruptedException e) {
         // won't happen
       }
     }
   }
}

class Shared {
   private boolean hasForB = false;
   private int fromAforB;

   private boolean hasForA = false;
   private int fromBforA;

   public void forB(final int v) {
     forB_impl(this, v);
   }

   private void forB_impl(final Object lock, final int v) {
     synchronized (lock) {
       fromAforB = v;
       hasForB = true;
       lock.notifyAll();
     }
   }

   public void forA(final int v) {
     forA_impl(this, v);
   }

   private void forA_impl(final Object lock, final int v) {
     synchronized (lock) {
       fromBforA = v;
       hasForA = true;
       lock.notifyAll();
     }
   }

   public int fromB() throws InterruptedException {
     return fromB_impl(this);
   }

   private int fromB_impl(final Object lock) throws  
InterruptedException {
     synchronized (lock) {
       while (!hasForA) {
         lock.wait();
       }
       hasForA = false;
       return fromBforA;
     }
   }

   public int fromA() throws InterruptedException {
     return fromA_impl(this);
   }

   private int fromA_impl(final Object lock) throws  
InterruptedException {
     synchronized (lock) {
       while (!hasForB) {
         lock.wait();
       }
       hasForB = false;
       return fromAforB;
     }
   }
}

The Shared class is using an extra layer of delegation because I was
trying to prevent the Java and JIT compilers from taking advantage of
the fact that the lock is known to be the receiver; this is important
below.

When I run this class normally on my Intel Core 2 Duo iMac, under Java
5, it takes about 33 seconds.  When I run with instrumentation added
around each monitorenter and monitorexit bytecode, it runs in about 39
seconds.  I have similar (but slower) results on an older Intel Dell
Laptop running Java 6.

Now, here is where it gets very strange.  I created a version of the
Shared class that approximates very closely the instrumentation that is
being added in the bytecode:

class Shared {
   private boolean hasForB = false;
   private int fromAforB;

   private boolean hasForA = false;
   private int fromBforA;

   public void forB(final int v) {
     forB_impl(this, v);
   }

   private void forB_impl(final Object lock, final int v) {
     try {
       Store.beforeStart(lock, lock == this, lock == Shared.class,  
Shared.class, 1);
       synchronized (lock) {
         Store.afterStart(lock, Shared.class, 1);
         fromAforB = v;
         hasForB = true;
         lock.notifyAll();
       }
     } finally {
       Store.afterEnd(this, Shared.class, 10);
     }
   }

   public void forA(final int v) {
     forA_impl(this, v);
   }

   private void forA_impl(final Object lock, final int v) {
     try {
       Store.beforeStart(lock, lock == this, lock == Shared.class,  
Shared.class, 1);
       synchronized (lock) {
         Store.afterStart(lock, Shared.class, 1);
         fromBforA = v;
         hasForA = true;
         lock.notifyAll();
       }
     } finally {
       Store.afterEnd(this, Shared.class, 10);
     }
   }

   public int fromB() throws InterruptedException {
     return fromB_impl(this);
   }

   private int fromB_impl(final Object lock) throws  
InterruptedException {
     try {
       Store.beforeStart(lock, lock == this, lock == Shared.class,  
Shared.class, 1);
       synchronized (lock) {
         Store.afterStart(lock, Shared.class, 1);
         while (!hasForA) {
           lock.wait();
         }
         hasForA = false;
         return fromBforA;
       }
     } finally {
       Store.afterEnd(this, Shared.class, 10);
     }
   }

   public int fromA() throws InterruptedException {
     return fromA_impl(this);
   }

   private int fromA_impl(final Object lock) throws  
InterruptedException {
     try {
       Store.beforeStart(lock, lock == this, lock == Shared.class,  
Shared.class, 1);
       synchronized (lock) {
         Store.afterStart(lock, Shared.class, 1);
         while (!hasForB) {
           lock.wait();
         }
         hasForB = false;
         return fromAforB;
       }
     } finally {
       Store.afterEnd(this, Shared.class, 10);
     }
   }
}

class Store {
   public static void beforeStart(
       final Object o, final boolean a, final boolean b, Class c, int  
l) {}
   public static void afterStart(final Object o, Class c, int l) {}
   public static void afterEnd(final Object o, Class c, int l) {}
}

This is where I want to avoid taking advantage of the fact that the lock
is the receiver.  Obviously, this isn't going to be exactly the same as
the instrumented bytecode, because the bytecode manipulator doens't know
what operations preceeded the monitorenter or monitorexit operations and
has to do some possibly redundant swap and dup stack manipulations.  But
it's pretty close.  When I run this version, it completes in about 33
seconds.  Actually, sometimes it seems to be slightly faster than the
original, running in 32 seconds.

This doesn't make any sense to me.  Other instrumentation that I insert
uses swap and dup, and is not heavily penalized.  I actually rewrote the
transformation to use local variables to avoid the use of swaps and
dups, but that turned out to be even slower!

To be even more specific again, here is the bytecode for the forB_impl()
method for the different versions.  (All the other methods are similar).

Original Method

   // Method descriptor #29 (Ljava/lang/Object;I)V
   // Stack: 2, Locals: 4
   private void forB_impl(java.lang.Object lock, int v);
      0  aload_1 [lock]
      1  dup
      2  astore_3
      3  monitorenter
      4  aload_0 [this]
      5  iload_2 [v]
      6  putfield test.Shared.fromAforB : int [31]
      9  aload_0 [this]
     10  iconst_1
     11  putfield test.Shared.hasForB : boolean [16]
     14  aload_1 [lock]
     15  invokevirtual java.lang.Object.notifyAll() : void [33]
     18  aload_3
     19  monitorexit
     20  goto 26
     23  aload_3
     24  monitorexit
     25  athrow
     26  return
       Exception Table:
         [pc: 4, pc: 20] -> 23 when : any
         [pc: 23, pc: 25] -> 23 when : any
       Line numbers:
         [pc: 0, line: 15]
         [pc: 4, line: 16]
         [pc: 9, line: 17]
         [pc: 14, line: 18]
         [pc: 18, line: 15]
         [pc: 26, line: 20]
       Local variable table:
         [pc: 0, pc: 27] local: this index: 0 type: test.Shared
         [pc: 0, pc: 27] local: lock index: 1 type: java.lang.Object
         [pc: 0, pc: 27] local: v index: 2 type: int

Pseudo-Instrumented Method

   // Method descriptor #29 (Ljava/lang/Object;I)V
   // Stack: 5, Locals: 5
   private void forB_impl(java.lang.Object lock, int v);
      0  aload_1 [lock]
      1  aload_1 [lock]
      2  aload_0 [this]
      3  if_acmpne 10
      6  iconst_1
      7  goto 11
     10  iconst_0
     11  aload_1 [lock]
     12  ldc <Class testInst.Shared> [1]
     14  if_acmpne 21
     17  iconst_1
     18  goto 22
     21  iconst_0
     22  ldc <Class testInst.Shared> [1]
     24  iconst_1
     25  invokestatic testInst.Store.beforeStart(java.lang.Object,  
boolean, boolean, java.lang.Class, int) : void [31]
     28  aload_1 [lock]
     29  dup
     30  astore_3
     31  monitorenter
     32  aload_1 [lock]
     33  ldc <Class testInst.Shared> [1]
     35  iconst_1
     36  invokestatic testInst.Store.afterStart(java.lang.Object,  
java.lang.Class, int) : void [37]
     39  aload_0 [this]
     40  iload_2 [v]
     41  putfield testInst.Shared.fromAforB : int [41]
     44  aload_0 [this]
     45  iconst_1
     46  putfield testInst.Shared.hasForB : boolean [16]
     49  aload_1 [lock]
     50  invokevirtual java.lang.Object.notifyAll() : void [43]
     53  aload_3
     54  monitorexit
     55  goto 74
     58  aload_3
     59  monitorexit
     60  athrow
     61  astore 4
     63  aload_0 [this]
     64  ldc <Class testInst.Shared> [1]
     66  bipush 10
     68  invokestatic testInst.Store.afterEnd(java.lang.Object,  
java.lang.Class, int) : void [46]
     71  aload 4
     73  athrow
     74  aload_0 [this]
     75  ldc <Class testInst.Shared> [1]
     77  bipush 10
     79  invokestatic testInst.Store.afterEnd(java.lang.Object,  
java.lang.Class, int) : void [46]
     82  return
       Exception Table:
         [pc: 32, pc: 55] -> 58 when : any
         [pc: 58, pc: 60] -> 58 when : any
         [pc: 0, pc: 61] -> 61 when : any
       Line numbers:
         [pc: 0, line: 16]
         [pc: 28, line: 17]
         [pc: 32, line: 18]
         [pc: 39, line: 19]
         [pc: 44, line: 20]
         [pc: 49, line: 21]
         [pc: 53, line: 17]
         [pc: 61, line: 23]
         [pc: 63, line: 24]
         [pc: 71, line: 25]
         [pc: 74, line: 24]
         [pc: 82, line: 26]
       Local variable table:
         [pc: 0, pc: 83] local: this index: 0 type: testInst.Shared
         [pc: 0, pc: 83] local: lock index: 1 type: java.lang.Object
         [pc: 0, pc: 83] local: v index: 2 type: int

Instrumented Method

   // Method descriptor #29 (Ljava/lang/Object;I)V
   // Stack: 9, Locals: 4
   private void forB_impl(java.lang.Object lock, int v);
      0  aload_1 [lock]
      1  dup
      2  astore_3
      3  dup
      4  dup
      5  ldc <Class test.Shared> [1]
      7  dup_x2
      8  swap
      9  dup_x1
     10  dup
     11  aload_0 [this]
     12  if_acmpne 19
     15  iconst_1
     16  goto 20
     19  iconst_0
     20  dup_x2
     21  pop
     22  swap
     23  dup_x1
     24  if_acmpne 31
     27  iconst_1
     28  goto 32
     31  iconst_0
     32  swap
     33  bipush 15
     35  invokestatic  
com.surelogic._flashlight.rewriter.test.EmptyStore.beforeIntrinsicLockAc 
quisition(java.lang.Object, boolean, boolean, java.lang.Class, int) :  
void [67]
     38  monitorenter
     39  bipush 15
     41  invokestatic  
com.surelogic._flashlight.rewriter.test.EmptyStore.afterIntrinsicLockAcq 
uisition(java.lang.Object, java.lang.Class, int) : void [71]
     44  aload_0 [this]
     45  iload_2 [v]
     46  putfield test.Shared.fromAforB : int [31]
     49  aload_0 [this]
     50  iconst_1
     51  putfield test.Shared.hasForB : boolean [16]
     54  aload_1 [lock]
     55  invokevirtual java.lang.Object.notifyAll() : void [33]
     58  aload_3
     59  dup
     60  monitorexit
     61  ldc <Class test.Shared> [1]
     63  bipush 15
     65  invokestatic  
com.surelogic._flashlight.rewriter.test.EmptyStore.afterIntrinsicLockRel 
ease(java.lang.Object, java.lang.Class, int) : void [74]
     68  goto 82
     71  aload_3
     72  dup
     73  monitorexit
     74  ldc <Class test.Shared> [1]
     76  bipush 15
     78  invokestatic  
com.surelogic._flashlight.rewriter.test.EmptyStore.afterIntrinsicLockRel 
ease(java.lang.Object, java.lang.Class, int) : void [74]
     81  athrow
     82  return
       Exception Table:
         [pc: 44, pc: 68] -> 71 when : any
         [pc: 71, pc: 81] -> 71 when : any
       Line numbers:
         [pc: 0, line: 15]
         [pc: 44, line: 16]
         [pc: 49, line: 17]
         [pc: 54, line: 18]
         [pc: 58, line: 15]
         [pc: 82, line: 20]
       Local variable table:
         [pc: 0, pc: 83] local: this index: 0 type: test.Shared
         [pc: 0, pc: 83] local: lock index: 1 type: java.lang.Object
         [pc: 0, pc: 83] local: v index: 2 type: int

In summary, I am completely stumped as to why there is a performance
difference between my bytecode-instrumented class (33 seconds) and my
Java-instrumented version (39 seconds).  Is the JIT doing something that
I'm not aware of?  Am I running into somekind of weird memory effect?
Can someone please shed some light on this situation?

--Aaron


From David.Biesack at sas.com  Mon Aug 25 15:59:36 2008
From: David.Biesack at sas.com (David J. Biesack)
Date: Mon, 25 Aug 2008 15:59:36 -0400
Subject: [concurrency-interest] FJ usage question: Mutable procedures
Message-ID: <u3aksc1cn.fsf@sas.com>


ParallelDoubleArray has a summary() method; but it's not implemented in
terms of the public apply/Ops.DoubleProcedure API of
ParallelDoubleArray. I'd like to make that more general and define a
Ops.DoubleProcedure to compute extended summary statistics, or perform
other operations which require updating state within the ops/procedures.
As noted earlier, there is no IntAndDoubleProcedure, so I'm using a
mutable Ops.IntAndDoubleToDouble instead

Naturally, I must add a lock around the mutating portions of the class

    @Override
    public double op(final int index, final double a) {
        // System.out.println(a);
        if (!Double.isNaN(a)) {
            synchronized (this) {
                sum += a;
                sumOfSquares += (a * a);
                n++;
                if (a > max) {
                    max = a;
                    maxIndex = index;
                }
                if (a < min) {
                    min = a;
                    minIndex = index;
                }
            }

        }
        return a;
    }

However, the PAS.FJDStats does not use synchronization but rather
creates new instances during the divide/conquer algorithms, and thus no
race conditions.

However, I don't see where this divide/conquer approach is part of the
public ParallelArray API; PAS and its kin are not public.

I would like to be able to do the divide/conquer for general purpose
mutable procedures/mappings. Is this possible? Another more simplistic
option would be to use ex.getParallelismLevel() and be able to create an
array of procedures/mappers, each of which the framework guarantees
safe, non-blocking execution

  Ops.DoubleProcedure procedures[] = new MyProcedure[ex.getParallelismLevel()];
  // initialize
  parallelArray.apply(procedures);

after which I can run a reducer over that array to combine their
intermediate results.

Or, are there better ways to achieve these goals?

-- 
David J. Biesack     SAS Institute Inc.
(919) 531-7771       SAS Campus Drive
http://www.sas.com   Cary, NC 27513

From dl at cs.oswego.edu  Tue Aug 26 08:59:59 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 26 Aug 2008 08:59:59 -0400
Subject: [concurrency-interest] FJ usage question: Mutable procedures
In-Reply-To: <u3aksc1cn.fsf@sas.com>
References: <u3aksc1cn.fsf@sas.com>
Message-ID: <48B3FE4F.3000402@cs.oswego.edu>

David J. Biesack wrote:
> ParallelDoubleArray has a summary() method; but it's not implemented in
> terms of the public apply/Ops.DoubleProcedure API of
> ParallelDoubleArray. I'd like to make that more general and define a
> Ops.DoubleProcedure to compute extended summary statistics, or perform
> other operations which require updating state within the ops/procedures.

This is one of the basic tradeoffs made in ParallelArray:
It knows how to translate fixed families of array operations
into underlying fork/join processing. While we can and should
keep adding to this family (as in supporting indexed procedures),
there will always only be a finite set of functional/procedural
forms that it understands.

When you need to go beyond them, the best recourse is to
step into the fun world of direct fork/join programming.
Often, it is fairly straightforward, as in the examples
(with more to come) in the RecursiveAction etc javadoc.
ParallelArray includes a bunch of tweaks that improve
performance over the basic templates (which it needs
in part because it must act without any programmer
supplied granularity information). The next update
(real soon now) will include documented, exposed
methods and classes that allow others to use them too,
but most of these can be ignored in everyday programming.

-Doug

From David.Biesack at sas.com  Tue Aug 26 10:53:38 2008
From: David.Biesack at sas.com (David J. Biesack)
Date: Tue, 26 Aug 2008 10:53:38 -0400
Subject: [concurrency-interest] FJ usage question: Mutable procedures
In-Reply-To: <48B3FE4F.3000402@cs.oswego.edu> (message from Doug Lea on Tue,
	26 Aug 2008 08:59:59 -0400)
References: <u3aksc1cn.fsf@sas.com> <48B3FE4F.3000402@cs.oswego.edu>
Message-ID: <uej4bakul.fsf@sas.com>


Thanks, a raw RecursiveAction does the trick for me for now.

Note: there are two bugs in the examples http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/jsr166y/forkjoin/RecursiveAction.html

In the sumOfSquares example, the compute() method iterates while i < h :

       for (int i = l; i < h; ++i) // perform leftmost base step
       sum += array[i] * array[i];

Thus, the constructor call 

   Applyer a = new Applyer(array, 0, n-1, seqSize, null);

should not subtract one from n:

   Applyer a = new Applyer(array, 0, n, seqSize, null);

Also, in Applyer in the javadoc

   int result; 

should be 

   double result;

> Date: Tue, 26 Aug 2008 08:59:59 -0400
> From: Doug Lea <dl at cs.oswego.edu>
> CC: <concurrency-interest at cs.oswego.edu>
> 
> David J. Biesack wrote:
> > ParallelDoubleArray has a summary() method; but it's not implemented in
> > terms of the public apply/Ops.DoubleProcedure API of
> > ParallelDoubleArray. I'd like to make that more general and define a
> > Ops.DoubleProcedure to compute extended summary statistics, or perform
> > other operations which require updating state within the ops/procedures.
> 
> This is one of the basic tradeoffs made in ParallelArray:
> It knows how to translate fixed families of array operations
> into underlying fork/join processing. While we can and should
> keep adding to this family (as in supporting indexed procedures),
> there will always only be a finite set of functional/procedural
> forms that it understands.
> 
> When you need to go beyond them, the best recourse is to
> step into the fun world of direct fork/join programming.
> Often, it is fairly straightforward, as in the examples
> (with more to come) in the RecursiveAction etc javadoc.
> ParallelArray includes a bunch of tweaks that improve
> performance over the basic templates (which it needs
> in part because it must act without any programmer
> supplied granularity information). The next update
> (real soon now) will include documented, exposed
> methods and classes that allow others to use them too,
> but most of these can be ignored in everyday programming.
> 
> -Doug
> 


-- 
David J. Biesack     SAS Institute Inc.
(919) 531-7771       SAS Campus Drive
http://www.sas.com   Cary, NC 27513

From gbikshan at in.ibm.com  Thu Aug 28 02:54:38 2008
From: gbikshan at in.ibm.com (Ganesh Bikshandi)
Date: Thu, 28 Aug 2008 12:24:38 +0530
Subject: [concurrency-interest] Learning  Java.util.concurrent
Message-ID: <OF4B605457.3835FD15-ON652574B3.0025CC15-652574B3.0025F807@in.ibm.com>


Dear Members,

I want to learn about java.util.concurrent. Which is the best source? If
there is a text book, that will be of great use.  Especially, I want to
lear about the fork-join stuff.

Best Regards,
Ganesh


From alarmnummer at gmail.com  Thu Aug 28 07:57:54 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 28 Aug 2008 13:57:54 +0200
Subject: [concurrency-interest] Learning Java.util.concurrent
In-Reply-To: <OF4B605457.3835FD15-ON652574B3.0025CC15-652574B3.0025F807@in.ibm.com>
References: <OF4B605457.3835FD15-ON652574B3.0025CC15-652574B3.0025F807@in.ibm.com>
Message-ID: <1466c1d60808280457nf951167p58eb9de158b9a368@mail.gmail.com>

The fork join stuff in Java is not yet final, and not described in any
books (implementation still is subject to change).

On the IBM site there are some articles about it written by Brian Goetz:
http://www.ibm.com/developerworks/java/library/j-jtp11137.html
http://www.ibm.com/developerworks/java/library/j-jtp03048.html

And there also is a paper about the fork join framework you could study:
http://gee.cs.oswego.edu/dl/papers/fj.pdf

But I don't know how much it is in sync with the current
implementation.But it is very informative anyway.

You could also check out the Intel thread building blocks:
http://www.amazon.com/Intel-Threading-Building-Blocks-Parallelism/dp/0596514808/ref=pd_bbs_sr_1?ie=UTF8&s=books&qid=1219923968&sr=8-1
There is a lot of overlap in both libraries and it never hurts to look
at fork join problems using different glasses.

For a more general approach you could also check out:
http://www.amazon.com/Patterns-Parallel-Programming-Software/dp/0321228111/ref=pd_bbs_sr_1?ie=UTF8&s=books&qid=1219924058&sr=1-1
It describes the fork join pattern and a lot of other patterns related
to parallel programming.

And for concurrency in Java the following books are my favorites:

http://www.amazon.com/Java-Concurrency-Practice-Brian-Goetz/dp/0321349601/ref=pd_bbs_sr_1?ie=UTF8&s=books&qid=1219924258&sr=1-1
Very practical and describes the stuff introduced in Java 5.

http://www.amazon.com/Concurrent-Programming-Java-TM-Principles/dp/0201310090/ref=pd_bbs_sr_1?ie=UTF8&s=books&qid=1219924428&sr=1-1
A classic book. Even though it is almost 10 years old, it is still
very valuable.

Both books complement each other.. one is more practical.. and the
other is more theoretical. And to do concurrency control well, you
need to be educated in both areas.

On Thu, Aug 28, 2008 at 8:54 AM, Ganesh Bikshandi <gbikshan at in.ibm.com> wrote:
>
> Dear Members,
>
> I want to learn about java.util.concurrent. Which is the best source? If
> there is a text book, that will be of great use.  Especially, I want to
> lear about the fork-join stuff.
>
> Best Regards,
> Ganesh
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

