From jeremy.manson at gmail.com  Sun Nov  2 01:20:18 2008
From: jeremy.manson at gmail.com (Jeremy Manson)
Date: Sat, 1 Nov 2008 22:20:18 -0700
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <4909F4B6.6030706@csd.uoc.gr>
References: <4909F4B6.6030706@csd.uoc.gr>
Message-ID: <1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>

My ignorant 2 cents --

There are MPI implementations in Java.  My understanding is that the
lack of interest is more a function of the fact that people don't
really use Java to write scientific apps.  There was some interest in
it back in the earlier days of Java, but it kind of disappeared.

The comment on Wikipedia is probably intended to describe the fact
that in C / C++, multidimensional arrays are a single contiguous chunk
of memory, whose indices are accessed by calculating an offset from
the base, whereas in Java, they are arrays of pointers to arrays,
whose indices are accessed by multiple indirections.  That is, in C++,
you access a[x][y] of an array declared a[X][Y] by accessing element i
+ (x * Y) + y, whereas in Java, you follow the pointer at a[x] and
then take the yth offset of that.  The C++ way is faster, and means
that you can copy a multi-dimensional array with memcpy in C++.

This has been a longstanding complaint from the scientific community
about Java.  A JSR to address it died a quiet death a few years back.
As of the last time I checked, the scientific community was happily
ignoring Java, for the most part, and JDK/JVM people didn't care
because these sorts of applications are something like 0.01% of the
market.

Having said that, I know some scientific programmers who use Java
quite happily, and I expect that if there were an enormous groundswell
of interest in it, the compiler writers would overcome the array
limitations.

Jeremy

On Thu, Oct 30, 2008 at 10:53 AM, Andreou Dimitris
<jim.andreou at gmail.com> wrote:
> Hi all,
>
> (Sorry for the overly general question, but)
> Is there a good reference explaining why the MPI programming model is
> important (or not) for Java? If it is important, should we expect good
> implementations coming for it? I'm particularly trying to understand the
> relation between fork/join and MPI in terms of usefulness.
>
> This is a comment from wikipedia[1], about the supposed difficulty of
> supporting MPI in Java, which I don't quite understand:
>>
>> Some of the most challenging parts of any MPI implementation for Java
>> arise from the language's own limitations and peculiarities, such as the
>> lack of explicit pointers and linear memory address space for its objects ,
>> which make transferring multi-dimensional arrays and complex objects
>> inefficient.
>
> Any opinions whether this holds true?
>
> Thanks a lot,
> Dimitris Andreou
>
> [1] http://en.wikipedia.org/wiki/Message_Passing_Interface#Java
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From ian.rogers at manchester.ac.uk  Sun Nov  2 07:46:17 2008
From: ian.rogers at manchester.ac.uk (Ian Rogers)
Date: Sun, 02 Nov 2008 04:46:17 -0800
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
References: <4909F4B6.6030706@csd.uoc.gr>
	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
Message-ID: <490DA119.7080707@manchester.ac.uk>

Jeremy Manson wrote:
> My ignorant 2 cents --
>
> There are MPI implementations in Java.  My understanding is that the
> lack of interest is more a function of the fact that people don't
> really use Java to write scientific apps.  There was some interest in
> it back in the earlier days of Java, but it kind of disappeared.
>
> The comment on Wikipedia is probably intended to describe the fact
> that in C / C++, multidimensional arrays are a single contiguous chunk
> of memory, whose indices are accessed by calculating an offset from
> the base, whereas in Java, they are arrays of pointers to arrays,
> whose indices are accessed by multiple indirections.  That is, in C++,
> you access a[x][y] of an array declared a[X][Y] by accessing element i
> + (x * Y) + y, whereas in Java, you follow the pointer at a[x] and
> then take the yth offset of that.  The C++ way is faster, and means
> that you can copy a multi-dimensional array with memcpy in C++.
>
> This has been a longstanding complaint from the scientific community
> about Java.  A JSR to address it died a quiet death a few years back.
> As of the last time I checked, the scientific community was happily
> ignoring Java, for the most part, and JDK/JVM people didn't care
> because these sorts of applications are something like 0.01% of the
> market.
>
> Having said that, I know some scientific programmers who use Java
> quite happily, and I expect that if there were an enormous groundswell
> of interest in it, the compiler writers would overcome the array
> limitations.
>
> Jeremy
>   

Hi Jeremy,

there is work in this area, specifically:
http://doi.acm.org/10.1145/1356058.1356061

Regards,
Ian Rogers
--
http://www.cs.man.ac.uk/~irogers/

> On Thu, Oct 30, 2008 at 10:53 AM, Andreou Dimitris
> <jim.andreou at gmail.com> wrote:
>   
>> Hi all,
>>
>> (Sorry for the overly general question, but)
>> Is there a good reference explaining why the MPI programming model is
>> important (or not) for Java? If it is important, should we expect good
>> implementations coming for it? I'm particularly trying to understand the
>> relation between fork/join and MPI in terms of usefulness.
>>
>> This is a comment from wikipedia[1], about the supposed difficulty of
>> supporting MPI in Java, which I don't quite understand:
>>     
>>> Some of the most challenging parts of any MPI implementation for Java
>>> arise from the language's own limitations and peculiarities, such as the
>>> lack of explicit pointers and linear memory address space for its objects ,
>>> which make transferring multi-dimensional arrays and complex objects
>>> inefficient.
>>>       
>> Any opinions whether this holds true?
>>
>> Thanks a lot,
>> Dimitris Andreou
>>
>> [1] http://en.wikipedia.org/wiki/Message_Passing_Interface#Java
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>     
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   


From forax at univ-mlv.fr  Sun Nov  2 08:46:57 2008
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Sun, 02 Nov 2008 14:46:57 +0100
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <490DA119.7080707@manchester.ac.uk>
References: <4909F4B6.6030706@csd.uoc.gr>	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
	<490DA119.7080707@manchester.ac.uk>
Message-ID: <490DAF51.8020702@univ-mlv.fr>

Ian Rogers a ?crit :
> Jeremy Manson wrote:
>> My ignorant 2 cents --
>>
>> There are MPI implementations in Java.  My understanding is that the
>> lack of interest is more a function of the fact that people don't
>> really use Java to write scientific apps.  There was some interest in
>> it back in the earlier days of Java, but it kind of disappeared.
No, lot of smart people use Java to write scientif API but they
do not use MPI but more high level libraries often based on RMI.

This paper (from James Gosling's blog)
shows some benchmarks comparing JVMs and Fortran arithmetic operations
and distributed API
http://hal.inria.fr/inria-00312039/en

>>
>> The comment on Wikipedia is probably intended to describe the fact
>> that in C / C++, multidimensional arrays are a single contiguous chunk
>> of memory, whose indices are accessed by calculating an offset from
>> the base, whereas in Java, they are arrays of pointers to arrays,
>> whose indices are accessed by multiple indirections.  That is, in C++,
>> you access a[x][y] of an array declared a[X][Y] by accessing element i
>> + (x * Y) + y, whereas in Java, you follow the pointer at a[x] and
>> then take the yth offset of that.  The C++ way is faster, and means
>> that you can copy a multi-dimensional array with memcpy in C++.
>>
>> This has been a longstanding complaint from the scientific community
>> about Java.  A JSR to address it died a quiet death a few years back.
>> As of the last time I checked, the scientific community was happily
>> ignoring Java, for the most part, and JDK/JVM people didn't care
>> because these sorts of applications are something like 0.01% of the
>> market.
>>
>> Having said that, I know some scientific programmers who use Java
>> quite happily, and I expect that if there were an enormous groundswell
>> of interest in it, the compiler writers would overcome the array
>> limitations.
>>
>> Jeremy
>>   
>
> Hi Jeremy,
>
> there is work in this area, specifically:
> http://doi.acm.org/10.1145/1356058.1356061
>
> Regards,
> Ian Rogers
> -- 
> http://www.cs.man.ac.uk/~irogers/
cheers,
R?mi Forax

From cmathieu at sophia.inria.fr  Sun Nov  2 18:00:03 2008
From: cmathieu at sophia.inria.fr (=?UTF-8?B?Q2zDqW1lbnQ=?= MATHIEU)
Date: Mon, 3 Nov 2008 00:00:03 +0100
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <490DAF51.8020702@univ-mlv.fr>
References: <4909F4B6.6030706@csd.uoc.gr>
	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
	<490DA119.7080707@manchester.ac.uk> <490DAF51.8020702@univ-mlv.fr>
Message-ID: <20081103000003.1624c2c7@sophia.inria.fr>

On Sun, 02 Nov 2008 14:46:57 +0100
R?mi Forax <forax at univ-mlv.fr> wrote:

Hi,

> This paper (from James Gosling's blog)
> shows some benchmarks comparing JVMs and Fortran arithmetic operations
> and distributed API
> http://hal.inria.fr/inria-00312039/en

I just forwarded this thread to the authors of this paper (we works in
the same team at INRIA). I'm sure it can be interesting to perform a
kind of state of art of Java for HPC[1]. 
Lot of peoples use Java based frameworks (like hadoop, gridgain or
ProActive) to write parallel and distributed applications. All of
theses frameworks hide concurrency to application developers. I wonder
if their is a niche between C/Fortran/MPI and the previous frameworks.


BTW. Brian and me will be at Super Computing in two weeks (booth 2203).
We will be happy to talk about this topic or to present you
our Open Source middleware for parallel, distributed, multi-core
computing (see http://proactive.inria.fr and
http://proactive.inria.fr/release-doc/pa/manual/multiple_html/Principles.html#d4e438 ).


regards.

[1]http://insidehpc.com/2008/10/30/is-supercomputing-just-about-performance/

-- 
Cl?ment Mathieu
Software Engineer
OASIS Team - INRIA Sophia-Antipolis
33 4 92 38 71 65 


From dcholmes at optusnet.com.au  Sun Nov  2 18:36:39 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 3 Nov 2008 09:36:39 +1000
Subject: [concurrency-interest] Concurrent Locks & Stack Traces
In-Reply-To: <19196d860810300623m50f46d42gb6fc1e30644f21c2@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEGBHOAA.dcholmes@optusnet.com.au>

Hi Sam,

Sam Berlin writes:
> I'm fairly certain the VM already supports (and the JDK already
> handles) registering Lock usage.  ThreadMXBean (in
> java.lang.management) has an isSynchronizerUsageSupported() methods,
> and stack traces already print out the number of owned synchronizers
> per thread.

Speaking for Hotspot, this isn't done by registering Lock usage, it is done
by scanning the heap for all instances of AbstractOwnableSynchronizer and
building a map of owner-threads and instances. This information is then
reported as part of the stack dump for a thread.

In addition, when a thread blocks on a Lock, implemented using park(), then
at that point the AbstractOwnableSynchronizer on which the thread is waiting
is registered. This provides the "waiting for XXX" to be reported and is
also used for deadlock detection.

So there really is no existing mechanism for registering when a Lock is
acquired - that action is nothing more than a Java-level CAS on a field with
a subsequent setting of an owner field.

> (This is different than 'monitor' usage, which has it own
> set of methods...  FWIW, I find it very confusing that the synchronize
> keyword maps to the 'monitor' nomenclature and the synchronize
> nomenclature maps to the implementations ReentrantLock &
> ReentrantReadWriteLock.. but oh well.)

"synchronizers" is the generic term for the java.util.concurrent
synchronization objects: Locks, Conditions, Barriers, Exchangers etc. But in
this case it's referring only to the AbstractOwnableSynchronizer class which
underpins some of those synchronizers.

And note this is only in JDK6+

> It also seems that the VM already supports keeping track of what
> frames lock what monitors.  It'd be a small leap (and one that has no
> founding in knowledge of how the VM works) to think that that notion
> could be extended to support ownable synchronizers.  Of course, it'd
> only be useful if it could done fast.

Acquiring a monitor is done in the VM and involves allocation of a
"stacklock" - just a record in the stack - and a CAS of that stacklock into
the object header. In that way a simple stack traversal can find all locked
monitors. The use of stacklocks is premised on the fact that monitor usage
must be properly nested - hence the monitor use can not outlive its
stacklock - not the case for a j.u.c Lock.

So to record Lock usage you'd have to make a call into the VM to update the
per-thread info; or you need to record the info in the Lock instance when it
is locked. Neither action is cheap and I don't think you'd want it enabled
in production.

> Your suggestion of recording the stack trace in the Lock itself is
> very neat.  It definitely solves this problem in managed environments,
> where you can turn on/off this kind of behavior.  Unfortunately,
> though, it doesn't seem like it would help much with the use-case I'm
> trying to solve.  I ship a desktop app that has an additional thread
> that periodically polls for deadlocks.  When there is one, it
> generates an error report and attempts to send it back for debugging.
> (It's amazing how well this has worked out.)  In order to capture the
> information I'd need, the stacktrace recording would have to be on all
> the time in all clients, and that would significantly slow down code
> that locks a lot.

How do you poll for deadlocks? Using the JMX functions? They are *very*
expensive and are performed at safe-points. You might find that this
additional overhead is not as significant as you might think - but of course
that depends on the frequency of locking and the frequency of polling.

> Given that all the info I'm looking for exists in some way or another
> for monitors, I'm hoping it's really just a matter of plugging ownable
> synchronizer data into it...  Wishful thinking?

In my view I'm afraid it is wishful thinking. Tracking where Locks are
acquired would require some significant changes to the Lock implementation.

Cheers,
David


From gregg at cytetech.com  Mon Nov  3 10:06:38 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 03 Nov 2008 09:06:38 -0600
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
References: <4909F4B6.6030706@csd.uoc.gr>
	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
Message-ID: <490F137E.9010303@cytetech.com>

Jeremy Manson wrote:
> My ignorant 2 cents --
> 
> There are MPI implementations in Java.  My understanding is that the
> lack of interest is more a function of the fact that people don't
> really use Java to write scientific apps.  There was some interest in
> it back in the earlier days of Java, but it kind of disappeared.

I think there are 2 issues for the scientific community.  One, Java's floating 
point math does not have the short cuts that many C++ implementations do for 
simplifying calculations, and which introduce small errors in the mantissa.

The other issue is that there is no declaration in Java that is equivalent to 
C++ for multi dimensional arrays.  In C++, one can write

double arr[10][30];

which is not possible in Java because the 2nd dimension is an object reference 
and thus can be any dimension.

for( int i = 0; i < 10; ++i ) {
	arr[i] = new double[5*i+10];
}

is a perfectly legal multi dimensional array.  In C++, you'd have to do

double *arr[10];
for( int i = 0; i < 10; ++i ) {
	arr[i] = calloc( sizeof(double)*(5*i*10) );
}

and then while you could write the expression

	arr[1][5]

but you could not do anything with memcpy to replicate the array
structure in one call.

So, I think it's comparing apples to oranges, but I understand the thought and 
reality that it would be nice to reference an array element with one memory load 
instead of 4 or more (array bounds check requires each "objects" length to be 
loaded too).  However, I think that array bounds checking and strict type 
management are a benefit that I'm willing to pay the price for.  If the amount 
of data is so large, that this becomes a real issue, as Remi said, distributing 
the work is another choice which is easily done with things like Jini's 
Javaspaces or something more raw, like RMI.

Gregg Wonderly

	arr[0]

From mthornton at optrak.co.uk  Mon Nov  3 10:36:58 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Mon, 03 Nov 2008 15:36:58 +0000
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <490F137E.9010303@cytetech.com>
References: <4909F4B6.6030706@csd.uoc.gr>	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
	<490F137E.9010303@cytetech.com>
Message-ID: <490F1A9A.7050803@optrak.co.uk>

Gregg Wonderly wrote:
> I think there are 2 issues for the scientific community.  One, Java's 
> floating point math does not have the short cuts that many C++ 
> implementations do for simplifying calculations, and which introduce 
> small errors in the mantissa.
>
> The other issue is that there is no declaration in Java that is 
> equivalent to C++ for multi dimensional arrays.  In C++, one can write
>
> double arr[10][30];
>
> which is not possible in Java because the 2nd dimension is an object 
> reference and thus can be any dimension.
>
> for( int i = 0; i < 10; ++i ) {
>     arr[i] = new double[5*i+10];
> }
>
> is a perfectly legal multi dimensional array.  In C++, you'd have to do
>
> double *arr[10];
> for( int i = 0; i < 10; ++i ) {
>     arr[i] = calloc( sizeof(double)*(5*i*10) );
> }

Not sure about either of those issues. Failure to use SIMD instructions 
(for more than scalars) is probably more important than the first issue. 
There have long been array packages for Java that provide 
multidimensional arrays with contiguous memory. The main annoyance with 
them is having to use get(i,j), set(i,j,value) instead of [i,j]. Complex 
values present a bigger problem for those applications that need them.

For all that, the main issue may still be reuse of existing code written 
in other languages (Fortran still isn't dead).

Mark Thornton


From gregg at cytetech.com  Mon Nov  3 17:02:56 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 03 Nov 2008 16:02:56 -0600
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <490F1A9A.7050803@optrak.co.uk>
References: <4909F4B6.6030706@csd.uoc.gr>
	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
	<490F137E.9010303@cytetech.com> <490F1A9A.7050803@optrak.co.uk>
Message-ID: <490F7510.10903@cytetech.com>

Mark Thornton wrote:
> Gregg Wonderly wrote:
>> I think there are 2 issues for the scientific community.  One, Java's 
>> floating point math does not have the short cuts that many C++ 
>> implementations do for simplifying calculations, and which introduce 
>> small errors in the mantissa.
>>
>> The other issue is that there is no declaration in Java that is 
>> equivalent to C++ for multi dimensional arrays.  In C++, one can write
>>
>> double arr[10][30];
>>
>> which is not possible in Java because the 2nd dimension is an object 
>> reference and thus can be any dimension.
>>
>> for( int i = 0; i < 10; ++i ) {
>>     arr[i] = new double[5*i+10];
>> }
>>
>> is a perfectly legal multi dimensional array.  In C++, you'd have to do
>>
>> double *arr[10];
>> for( int i = 0; i < 10; ++i ) {
>>     arr[i] = calloc( sizeof(double)*(5*i*10) );
>> }
> 
> Not sure about either of those issues. Failure to use SIMD instructions 
> (for more than scalars) is probably more important than the first issue.

I've heard people lament about the java floating point being too accurate for 
their needs, and citing error resolution as a slow down that they didn't need.
One such statement occured early in Java's life

http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf

> There have long been array packages for Java that provide 
> multidimensional arrays with contiguous memory. The main annoyance with 
> them is having to use get(i,j), set(i,j,value) instead of [i,j].

They all use to JNI interfaces because as I said, Java doesn't have a 
multi-dimensional initialization that would allow a "compiler" to inline 
everything.  The previously referenced paper cited the need for data flow 
analysis to arrive at the right "sized" array for example.

> Complex 
> values present a bigger problem for those applications that need them.
> 
> For all that, the main issue may still be reuse of existing code written 
> in other languages (Fortran still isn't dead).

Yeah, that is an issue for many.  Alas, we haven't yet figured how to have just 
have "one" programming language and so we continue to fight with all these 
issues about portability and reuse.  Things like the Java runtime start to drag 
people in that direction because of all the tools/code.  The VM also encourages 
them by virtualizing the notion of programming language.

Just need the Fortran and Cobal converters for the JVM I guess :-)

<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.3949>
<http://www.coboltojava.com/>


Gregg Wonderly

From mthornton at optrak.co.uk  Mon Nov  3 19:16:28 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 04 Nov 2008 00:16:28 +0000
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <490F7510.10903@cytetech.com>
References: <4909F4B6.6030706@csd.uoc.gr>
	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
	<490F137E.9010303@cytetech.com> <490F1A9A.7050803@optrak.co.uk>
	<490F7510.10903@cytetech.com>
Message-ID: <490F945C.1090001@optrak.co.uk>


>
> I've heard people lament about the java floating point being too 
> accurate for their needs, and citing error resolution as a slow down 
> that they didn't need.
> One such statement occured early in Java's life
>
> http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf
Despite his forecast, Sun/Java has managed to hold the line on numeric 
reproducibility, at the expense of performance in matrix multiply. The 
issue of mixed float/double is something that I suppose we are 
accustomed to --- Java does the same as the other commonly used 
languages. Long double is interesting, because Java and presumably other 
languages on x86 now frequently use the scalar forms of SIMD 
instructions and these don't have long double forms. So where long 
double was once little more expensive than double, there is now a 
significant performance premium.

The "too accurate" bit is probably a reference to the trig functions 
where the Java specification requires about 1024 bits of PI to do the 
argument reduction. The x86 type processors use the equivalent of 66 
bits. In this case Java is far more accurate than almost everyone 
requires, but at least it has a specification. Most languages say far 
too little of what can be expected from these functions.
>
>> There have long been array packages for Java that provide 
>> multidimensional arrays with contiguous memory. The main annoyance 
>> with them is having to use get(i,j), set(i,j,value) instead of [i,j].
>
> They all use to JNI interfaces because as I said, Java doesn't have a 
> multi-dimensional initialization that would allow a "compiler" to 
> inline everything.  The previously referenced paper cited the need for 
> data flow analysis to arrive at the right "sized" array for example.
>
Actually there are several which are pure Java: e.g. 
http://acs.lbl.gov/~hoschek/colt/

Mark Thornton


From gregg at cytetech.com  Tue Nov  4 02:16:58 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 04 Nov 2008 01:16:58 -0600
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <490F945C.1090001@optrak.co.uk>
References: <4909F4B6.6030706@csd.uoc.gr>
	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
	<490F137E.9010303@cytetech.com> <490F1A9A.7050803@optrak.co.uk>
	<490F7510.10903@cytetech.com> <490F945C.1090001@optrak.co.uk>
Message-ID: <490FF6EA.9050302@cytetech.com>

Mark Thornton wrote:
> 
>>
>> I've heard people lament about the java floating point being too 
>> accurate for their needs, and citing error resolution as a slow down 
>> that they didn't need.
>> One such statement occured early in Java's life
>>
>> http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf
> Despite his forecast, Sun/Java has managed to hold the line on numeric 
> reproducibility, at the expense of performance in matrix multiply. The 
> issue of mixed float/double is something that I suppose we are 
> accustomed to --- Java does the same as the other commonly used 
> languages. Long double is interesting, because Java and presumably other 
> languages on x86 now frequently use the scalar forms of SIMD 
> instructions and these don't have long double forms. So where long 
> double was once little more expensive than double, there is now a 
> significant performance premium.

Yeah, this focus on one type of use is where things get to be iffy.  The 
plethora of command line arguments to GCC and other compilers are a great 
example of what issues people tend to press for solutions to.

> The "too accurate" bit is probably a reference to the trig functions 
> where the Java specification requires about 1024 bits of PI to do the 
> argument reduction. The x86 type processors use the equivalent of 66 
> bits. In this case Java is far more accurate than almost everyone 
> requires, but at least it has a specification. Most languages say far 
> too little of what can be expected from these functions.

Yep, I enjoy the fact that there is a spec.  Many, seem to agree that there is 
too much precision in java for some uses, and the fact that the "APIs" can't 
change that precision is at least one issue with how the spec becomes a sort of 
baggage that everyone has to work around.

I don't know how interesting ragged arrays are for general purpose computing, 
given that maps provide similar capabilities in many ways (maps of lists for 
example).  But the fact that an matrix is not an allocatable item at the 
language level, it gets a bit more difficult.  Doug's work in this area provides 
a mechanism for JVM operations to actually be optimized because things are much 
more explicitly typed and confined.

>>> There have long been array packages for Java that provide 
>>> multidimensional arrays with contiguous memory. The main annoyance 
>>> with them is having to use get(i,j), set(i,j,value) instead of [i,j].
>>
>> They all use to JNI interfaces because as I said, Java doesn't have a 
>> multi-dimensional initialization that would allow a "compiler" to 
>> inline everything.  The previously referenced paper cited the need for 
>> data flow analysis to arrive at the right "sized" array for example.
>>
> Actually there are several which are pure Java: e.g. 
> http://acs.lbl.gov/~hoschek/colt/

Okay, I was suggesting JNI because you said SIMD.  If they are 100% Java, then 
it's whatever the JVM provides I guess, and optimized use of SIMD seems to be 
more prevalent than I realized based on your comments.

Thanks for the info Mark!

Gregg Wonderly


From osvaldo at visionnaire.com.br  Tue Nov  4 07:15:09 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Tue, 04 Nov 2008 10:15:09 -0200
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <490FF6EA.9050302@cytetech.com>
References: <4909F4B6.6030706@csd.uoc.gr>	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>	<490F137E.9010303@cytetech.com>
	<490F1A9A.7050803@optrak.co.uk>	<490F7510.10903@cytetech.com>
	<490F945C.1090001@optrak.co.uk> <490FF6EA.9050302@cytetech.com>
Message-ID: <49103CCD.5020105@visionnaire.com.br>

Gregg Wonderly wrote:
> Mark Thornton wrote:
>> The "too accurate" bit is probably a reference to the trig functions 
>> where the Java specification requires about 1024 bits of PI to do the 
>> argument reduction. The x86 type processors use the equivalent of 66 
>> bits. In this case Java is far more accurate than almost everyone 
>> requires, but at least it has a specification. Most languages say far 
>> too little of what can be expected from these functions.
> Yep, I enjoy the fact that there is a spec.  Many, seem to agree that 
> there is too much precision in java for some uses, and the fact that 
> the "APIs" can't change that precision is at least one issue with how 
> the spec becomes a sort of baggage that everyone has to work around.
The Java spec also covers the option of favoring speed over precision & 
portability since the introduction of strictfp and StrictMath. But these 
features haven't been supported by anybody. Sun didn't provide optimized 
versions of the non-strict maths; at least, I can see that all hardest 
functions of java.lang.Math methods simply delegate to StrictMath - 
don't know however if intrinsified functions are handled separately - 
but also I don't remember to ever see any evidence that non-strict math 
was faster. And the whole community ignored it too, I never saw a 
third-party library that was strict/non-strict-aware. and if we suggest 
that from now on (Java SE 7) HotSpot starts to optimize the hell out of 
non-strict maths and keep the current behavior only for explicit 
strictfp code, loads of people will certainly complain that their apps 
are breaking... well, perhaps this could be a JVM switch, off by default.

A+
Osvaldo

-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                        Visionnaire Virtus S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #226


From mthornton at optrak.co.uk  Tue Nov  4 07:30:58 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 04 Nov 2008 12:30:58 +0000
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <49103CCD.5020105@visionnaire.com.br>
References: <4909F4B6.6030706@csd.uoc.gr>	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>	<490F137E.9010303@cytetech.com>
	<490F1A9A.7050803@optrak.co.uk>	<490F7510.10903@cytetech.com>
	<490F945C.1090001@optrak.co.uk> <490FF6EA.9050302@cytetech.com>
	<49103CCD.5020105@visionnaire.com.br>
Message-ID: <49104082.8040804@optrak.co.uk>

Osvaldo Pinali Doederlein wrote:
> Gregg Wonderly wrote:
>> Mark Thornton wrote:
>>> The "too accurate" bit is probably a reference to the trig functions 
>>> where the Java specification requires about 1024 bits of PI to do 
>>> the argument reduction. The x86 type processors use the equivalent 
>>> of 66 bits. In this case Java is far more accurate than almost 
>>> everyone requires, but at least it has a specification. Most 
>>> languages say far too little of what can be expected from these 
>>> functions.
>> Yep, I enjoy the fact that there is a spec.  Many, seem to agree that 
>> there is too much precision in java for some uses, and the fact that 
>> the "APIs" can't change that precision is at least one issue with how 
>> the spec becomes a sort of baggage that everyone has to work around.
> The Java spec also covers the option of favoring speed over precision 
> & portability since the introduction of strictfp and StrictMath. But 
> these features haven't been supported by anybody. Sun didn't provide 
> optimized versions of the non-strict maths; at least, I can see that 
> all hardest functions of java.lang.Math methods simply delegate to 
> StrictMath - don't know however if intrinsified functions are handled 
> separately -
The intrinsic functions are handled seperately and do use the processor 
hardware after argument reduction where necessary. The source code for 
Math does not give any indication as to what the JVM actually does --- 
it can do what it likes provided it meets the specification. It used to 
be possible to easily observe the results changing when the code was 
compiled (while still interpreted the code does use the StrictMath 
version). Once the argument reduction was added on both paths it became 
harder to oberve the change (the difference can be at most 1 LSB).

Mark Thornton

From sberlin at gmail.com  Tue Nov  4 10:39:32 2008
From: sberlin at gmail.com (Sam Berlin)
Date: Tue, 4 Nov 2008 10:39:32 -0500
Subject: [concurrency-interest] Standard Practice For Future APIs?
Message-ID: <19196d860811040739w70b78e20wcdf143f386bd6bae@mail.gmail.com>

Hi All,

I'm curious as to what the standard practice is for an API that
returns futures or lists of futures.  I have a scenario where there's
a list-like class that has the following simple add method:

   /** Given a bar, creates a Foo and returns a Future from which that
Foo can be retrieved. */
   Future<Foo> add(Bar bar);

I'd like to add the ability to take a collection of Bars and return a
list of Future Foos.  I'd expect this would look something like:

  /** Given a bunch of Bars, returns a list where the Future Foos can
be retrieved. */
  List<Future<Foo>> addMany(BarCollection barCollection);

But, there's a catch... Processing the BarCollection to get a bunch
the Bars themselves can block.  So the result is:

  /** Given a bunch of Bars, returns a future from which a list of
Future Foos can be retrieved. */
  Future<List<Future<Foo>>> addMany(BarCollection barCollection);

... That looks pretty unwieldy to me, though.  What do other APIs do
in this scenario?  Is it typically dependent on how the return values
are expected to be used?  Would you expect the addMany method to just
return a Future<List<Foo>> and have the initial get block until all
values are calculated?

Thanks for any of your thoughts.

Sam

From mlists at juma.me.uk  Tue Nov  4 10:57:11 2008
From: mlists at juma.me.uk (Ismael Juma)
Date: Tue, 04 Nov 2008 15:57:11 +0000
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <49104082.8040804@optrak.co.uk>
References: <4909F4B6.6030706@csd.uoc.gr>
	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>
	<490F137E.9010303@cytetech.com> <490F1A9A.7050803@optrak.co.uk>
	<490F7510.10903@cytetech.com> <490F945C.1090001@optrak.co.uk>
	<490FF6EA.9050302@cytetech.com> <49103CCD.5020105@visionnaire.com.br>
	<49104082.8040804@optrak.co.uk>
Message-ID: <1225814231.23509.6.camel@mac.config>

On Tue, 2008-11-04 at 12:30 +0000, Mark Thornton wrote:
> The intrinsic functions are handled seperately and do use the processor 
> hardware after argument reduction where necessary. The source code for 
> Math does not give any indication as to what the JVM actually does ---

But the JVM source does:

http://www.google.com/codesearch?hl=en&q=show:d1fgenJEnOg:Aqwg-Dv3R-Y:xHnQUnd_1NQ&sa=N&ct=rd&cs_p=http://hg.openjdk.java.net/jdk7/jdk7/hotspot&cs_f=src/share/vm/classfile/vmSymbols.hpp

Search for "Here are all the intrinsics known". Of course, this is
subject to change, but it's interesting information nonetheless.

Regards,
Ismael



From ben_manes at yahoo.com  Tue Nov  4 13:11:51 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Tue, 4 Nov 2008 10:11:51 -0800 (PST)
Subject: [concurrency-interest] Standard Practice For Future APIs?
Message-ID: <688469.28640.qm@web38802.mail.mud.yahoo.com>

If the operation can be done in parallel, which indicates that the results are independent of each other, than I would expect List<Future<Foo>>.  If the results are dependent, which indicates that there's no value in consuming them separately, then I would probably favor Future<List<Foo>>.  To me, it would come down to what the operation was trying to perform and which approach is more natural for the client.  I would lean towards the first API design when possible.




________________________________
From: Sam Berlin <sberlin at gmail.com>
To: concurrency-interest <concurrency-interest at cs.oswego.edu>
Sent: Tuesday, November 4, 2008 7:39:32 AM
Subject: [concurrency-interest] Standard Practice For Future APIs?

Hi All,

I'm curious as to what the standard practice is for an API that
returns futures or lists of futures.  I have a scenario where there's
a list-like class that has the following simple add method:

   /** Given a bar, creates a Foo and returns a Future from which that
Foo can be retrieved. */
   Future<Foo> add(Bar bar);

I'd like to add the ability to take a collection of Bars and return a
list of Future Foos.  I'd expect this would look something like:

  /** Given a bunch of Bars, returns a list where the Future Foos can
be retrieved. */
  List<Future<Foo>> addMany(BarCollection barCollection);

But, there's a catch... Processing the BarCollection to get a bunch
the Bars themselves can block.  So the result is:

  /** Given a bunch of Bars, returns a future from which a list of
Future Foos can be retrieved. */
  Future<List<Future<Foo>>> addMany(BarCollection barCollection);

... That looks pretty unwieldy to me, though.  What do other APIs do
in this scenario?  Is it typically dependent on how the return values
are expected to be used?  Would you expect the addMany method to just
return a Future<List<Foo>> and have the initial get block until all
values are calculated?

Thanks for any of your thoughts.

Sam
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081104/da4410ea/attachment.html>

From mthornton at optrak.co.uk  Tue Nov  4 15:16:35 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 04 Nov 2008 20:16:35 +0000
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <49103CCD.5020105@visionnaire.com.br>
References: <4909F4B6.6030706@csd.uoc.gr>	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>	<490F137E.9010303@cytetech.com>
	<490F1A9A.7050803@optrak.co.uk>	<490F7510.10903@cytetech.com>
	<490F945C.1090001@optrak.co.uk> <490FF6EA.9050302@cytetech.com>
	<49103CCD.5020105@visionnaire.com.br>
Message-ID: <4910ADA3.10907@optrak.co.uk>

Osvaldo Pinali Doederlein wrote:
> But these features haven't been supported by anybody. Sun didn't 
> provide optimized versions of the non-strict maths; at least, I can 
> see that all hardest functions of java.lang.Math methods simply 
> delegate to StrictMath - don't know however if intrinsified functions are
I started looking for differences at x=0.1 and incremented by the 
smallest possible amount (Math.nextUp).

x=0.10000000000000202, sin(x)=0.09983341664683015, strict 
sin(x)=0.09983341664683017

java version "1.6.0_10"
Java(TM) SE Runtime Environment (build 1.6.0_10-b33)
Java HotSpot(TM) Client VM (build 11.0-b15, mixed mode, sharing)

Mark Thornton


From osvaldo at visionnaire.com.br  Wed Nov  5 07:15:19 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Wed, 05 Nov 2008 10:15:19 -0200
Subject: [concurrency-interest] MPI in Java?
In-Reply-To: <4910ADA3.10907@optrak.co.uk>
References: <4909F4B6.6030706@csd.uoc.gr>	<1631da7d0811012220ua4722baqa32af9b7d2e2b252@mail.gmail.com>	<490F137E.9010303@cytetech.com>	<490F1A9A.7050803@optrak.co.uk>	<490F7510.10903@cytetech.com>	<490F945C.1090001@optrak.co.uk>
	<490FF6EA.9050302@cytetech.com>	<49103CCD.5020105@visionnaire.com.br>
	<4910ADA3.10907@optrak.co.uk>
Message-ID: <49118E57.6000506@visionnaire.com.br>

Mark Thornton wrote:
> Osvaldo Pinali Doederlein wrote:
>> But these features haven't been supported by anybody. Sun didn't 
>> provide optimized versions of the non-strict maths; at least, I can 
>> see that all hardest functions of java.lang.Math methods simply 
>> delegate to StrictMath - don't know however if intrinsified functions 
>> are
> I started looking for differences at x=0.1 and incremented by the 
> smallest possible amount (Math.nextUp).
>
> x=0.10000000000000202, sin(x)=0.09983341664683015, strict 
> sin(x)=0.09983341664683017
>
This is good news (assuming of course, that the inferior precision of 
non-strict comes together with faster performance). Unfortunately this 
is a facet of performance that was never covered by benchmarks. It would 
be interesting to know the performance delta from strict to non-strict 
fp, remarkably on non-x86 platforms (which CPU did you test?). Comparing 
the javadocs of Math and StrictMath, StrictMath just imposes the extra 
restriction of producing thre same results as fdlibm 5.3. But I wonder 
if the non-strict functions could be even more relaxed, e.g. dropping 
requirements like 1ulp accuracy, semi-monotonic property, and support 
for special cases like NaN, Inf and signed zeroes - would this make a 
huge difference, is this what the HPC crowd wants?

A+
Osvaldo


-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                        Visionnaire Virtus S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #226


From sberlin at gmail.com  Thu Nov  6 19:11:38 2008
From: sberlin at gmail.com (Sam Berlin)
Date: Thu, 6 Nov 2008 19:11:38 -0500
Subject: [concurrency-interest] Future Done Events?
Message-ID: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>

Hi All,

I spent the past few days creating a general framework for listening
to Future events, so that listeners can be notified asynchronously of
the finishing status.  It's very much like completion handlers with
I/O (but not like the jdk CompletionService class). The basic premise
is:

 ListeningFuture<Type> future = listeningExecutorService.submit(callable);
 future.addFutureListener(new EventListener<FutureEvent<Type>>() {
     public void handleEvent(FutureEvent<Type> event) {
            // The future finished!
     }
  });

Or alternate usage of:

  ListeningRunnableFuture<Type> future = new
ListeningFutureTask<Type>(callable);
  executorService.execute(future);
  future.addFutureListener( ... );

The listener's notified whenever the future finishes.  I've found a
need for this kind of asynchronous future notification in many places
(notably because I don't want to have to spawn a thread just to wait
on a get).

Is there a reason why support for asynchronously listening to future
events was not added into Future to begin with?  It seems like a
terribly useful thing.  I've always found it strange that the
concurrent package (which by its very nature is designed to provide
excellent concurrent utilities) provides no way of asynchronously
notifying of future-finished events.

If anyone has copious amounts of spare time, I'd appreciate if someone
could look over the class @
<https://www.limewire.org/fisheye/browse/~raw,r=1.2/limecvs/components/common/src/main/java/org/limewire/concurrent/ListeningFutureTask.java>
and provide some feedback.  It's fairly general-purpose (except for
it's use of EventListenerList, which is a custom listening list we
have, but that's easy to swap out.)

Thanks.

Sam

From jed at atlassian.com  Thu Nov  6 22:45:37 2008
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Fri, 07 Nov 2008 14:45:37 +1100
Subject: [concurrency-interest] Future Done Events?
In-Reply-To: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
References: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
Message-ID: <4913B9E1.2090303@atlassian.com>

Whenever I've needed similar functionality I've simply used a delegating 
Callable/Runnable that wraps the first one in a try{delegate.run();} 
finally {complete();} style. Is there a real need for post-registerable 
listeners?

cheers,
jed.

Sam Berlin wrote:
> Hi All,
>
> I spent the past few days creating a general framework for listening
> to Future events, so that listeners can be notified asynchronously of
> the finishing status.  It's very much like completion handlers with
> I/O (but not like the jdk CompletionService class). The basic premise
> is:
>
>  ListeningFuture<Type> future = listeningExecutorService.submit(callable);
>  future.addFutureListener(new EventListener<FutureEvent<Type>>() {
>      public void handleEvent(FutureEvent<Type> event) {
>             // The future finished!
>      }
>   });
>
> Or alternate usage of:
>
>   ListeningRunnableFuture<Type> future = new
> ListeningFutureTask<Type>(callable);
>   executorService.execute(future);
>   future.addFutureListener( ... );
>
> The listener's notified whenever the future finishes.  I've found a
> need for this kind of asynchronous future notification in many places
> (notably because I don't want to have to spawn a thread just to wait
> on a get).
>
> Is there a reason why support for asynchronously listening to future
> events was not added into Future to begin with?  It seems like a
> terribly useful thing.  I've always found it strange that the
> concurrent package (which by its very nature is designed to provide
> excellent concurrent utilities) provides no way of asynchronously
> notifying of future-finished events.
>
> If anyone has copious amounts of spare time, I'd appreciate if someone
> could look over the class @
> <https://www.limewire.org/fisheye/browse/~raw,r=1.2/limecvs/components/common/src/main/java/org/limewire/concurrent/ListeningFutureTask.java>
> and provide some feedback.  It's fairly general-purpose (except for
> it's use of EventListenerList, which is a custom listening list we
> have, but that's easy to swap out.)
>
> Thanks.
>
> Sam
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   


From sberlin at gmail.com  Fri Nov  7 00:28:11 2008
From: sberlin at gmail.com (Sam Berlin)
Date: Fri, 7 Nov 2008 00:28:11 -0500
Subject: [concurrency-interest] Future Done Events?
In-Reply-To: <4913B9E1.2090303@atlassian.com>
References: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
	<4913B9E1.2090303@atlassian.com>
Message-ID: <19196d860811062128h6dd6a65as701a20abfd45120d@mail.gmail.com>

In cases where you're the one submitting the task, it works well to
create a wrapper around the Callable/Runnable.  In other cases, where
the task is submitted by another API and all it can return is a Future
of the result, you're generally at a loss to asynchronously listen to
the status.  It's very useful to be able to hide away the details of
not only when & where the task is executed, but also how the task is
executed.

An example use-case I have for this is processing something from a
file-management API, ie:

interface FileInfoRepository {
   Future<FileInfo> add(File file);
   List<Future<FileInfo>> addFolder(File folder);
}

Users of that interface can add Files & blockingly get the results of
the added file, but have no way of asynchronously getting details on
what it created.  If the API already internally takes care of
threading, the user shouldn't have to create a Thread just to
asychronously do something on the add finishing.  You can add a
listener as a parameter to the add method, but I find it very
cumbersome to overload the entire API with listeners when threading is
already built-in to the return value (in addition to bloating the
API).

It becomes even more useful when dealing with other layers.  For
example, consider you want to have a shared drag/drop handler that
adds files into the repository.  It can take in the repository it
wants to add files to, do a bunch of processing to make sure the files
are OK to add, and then iterate through & add the files, passing the
results to another party.  The other party can then take the Future,
immediately set a "processing" state, add a listener to it, and
asynchronously change it to a "finished" state.

It's certainly not something that's absolutely required -- there's
definite ways around a listening infrastructure.  It does seem,
though, that having the ability to listen to future's finishing
creates a lot of elegant results.

Sam

On Thu, Nov 6, 2008 at 10:45 PM, Jed Wesley-Smith <jed at atlassian.com> wrote:
> Whenever I've needed similar functionality I've simply used a delegating
> Callable/Runnable that wraps the first one in a try{delegate.run();} finally
> {complete();} style. Is there a real need for post-registerable listeners?
>
> cheers,
> jed.
>
> Sam Berlin wrote:
>>
>> Hi All,
>>
>> I spent the past few days creating a general framework for listening
>> to Future events, so that listeners can be notified asynchronously of
>> the finishing status.  It's very much like completion handlers with
>> I/O (but not like the jdk CompletionService class). The basic premise
>> is:
>>
>>  ListeningFuture<Type> future = listeningExecutorService.submit(callable);
>>  future.addFutureListener(new EventListener<FutureEvent<Type>>() {
>>     public void handleEvent(FutureEvent<Type> event) {
>>            // The future finished!
>>     }
>>  });
>>
>> Or alternate usage of:
>>
>>  ListeningRunnableFuture<Type> future = new
>> ListeningFutureTask<Type>(callable);
>>  executorService.execute(future);
>>  future.addFutureListener( ... );
>>
>> The listener's notified whenever the future finishes.  I've found a
>> need for this kind of asynchronous future notification in many places
>> (notably because I don't want to have to spawn a thread just to wait
>> on a get).
>>
>> Is there a reason why support for asynchronously listening to future
>> events was not added into Future to begin with?  It seems like a
>> terribly useful thing.  I've always found it strange that the
>> concurrent package (which by its very nature is designed to provide
>> excellent concurrent utilities) provides no way of asynchronously
>> notifying of future-finished events.
>>
>> If anyone has copious amounts of spare time, I'd appreciate if someone
>> could look over the class @
>>
>> <https://www.limewire.org/fisheye/browse/~raw,r=1.2/limecvs/components/common/src/main/java/org/limewire/concurrent/ListeningFutureTask.java>
>> and provide some feedback.  It's fairly general-purpose (except for
>> it's use of EventListenerList, which is a custom listening list we
>> have, but that's easy to swap out.)
>>
>> Thanks.
>>
>> Sam
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>

From Ben.Rowlands at morganstanley.com  Fri Nov  7 05:56:02 2008
From: Ben.Rowlands at morganstanley.com (Rowlands, Ben (IT))
Date: Fri, 7 Nov 2008 10:56:02 +0000
Subject: [concurrency-interest] Future Done Events?
In-Reply-To: <19196d860811062128h6dd6a65as701a20abfd45120d@mail.gmail.com>
References: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com><4913B9E1.2090303@atlassian.com>
	<19196d860811062128h6dd6a65as701a20abfd45120d@mail.gmail.com>
Message-ID: <E295C18F4AD63149810A83F7B7E842B118E012B6FB@LNWEXMBX0105.msad.ms.com>

I was looking for a similar API a while back to efficiently join on multiple futures. At present we use polling for >1 future as we need to detect execution failures eagerly:

  http://markmail.org/message/4dswxghye6b7x6yp

We also couldn't use the wrapping up of callable/runnable notification trick as we don't necessarily implement the execution of the request.

Within our group we could agree on a convention that all async APIs that return a Future should also accept a Runnable argument (or some custom "FutureListener" interface) to be run when the result is ready. But without a JDK API to implement (ideally something like the pervasive .NET AsyncCallback/IAsyncResult pattern that finds its way into all .NET Async APIs offering blocking and callback style of result delivery) we always need to use polling for async APIs that don't follow this pattern.

Ben Rowlands

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> Of Sam Berlin
> Sent: 07 November 2008 05:28
> To: Jed Wesley-Smith
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] Future Done Events?
>
> In cases where you're the one submitting the task, it works well to
> create a wrapper around the Callable/Runnable.  In other cases, where
> the task is submitted by another API and all it can return is a Future
> of the result, you're generally at a loss to asynchronously listen to
> the status.  It's very useful to be able to hide away the details of
> not only when & where the task is executed, but also how the task is
> executed.
>
> An example use-case I have for this is processing something from a
> file-management API, ie:
>
> interface FileInfoRepository {
>    Future<FileInfo> add(File file);
>    List<Future<FileInfo>> addFolder(File folder);
> }
>
> Users of that interface can add Files & blockingly get the results of
> the added file, but have no way of asynchronously getting details on
> what it created.  If the API already internally takes care of
> threading, the user shouldn't have to create a Thread just to
> asychronously do something on the add finishing.  You can add a
> listener as a parameter to the add method, but I find it very
> cumbersome to overload the entire API with listeners when threading is
> already built-in to the return value (in addition to bloating the
> API).
>
> It becomes even more useful when dealing with other layers.  For
> example, consider you want to have a shared drag/drop handler that
> adds files into the repository.  It can take in the repository it
> wants to add files to, do a bunch of processing to make sure the files
> are OK to add, and then iterate through & add the files, passing the
> results to another party.  The other party can then take the Future,
> immediately set a "processing" state, add a listener to it, and
> asynchronously change it to a "finished" state.
>
> It's certainly not something that's absolutely required -- there's
> definite ways around a listening infrastructure.  It does seem,
> though, that having the ability to listen to future's finishing
> creates a lot of elegant results.
>
> Sam
>
> On Thu, Nov 6, 2008 at 10:45 PM, Jed Wesley-Smith
> <jed at atlassian.com> wrote:
> > Whenever I've needed similar functionality I've simply used
> a delegating
> > Callable/Runnable that wraps the first one in a
> try{delegate.run();} finally
> > {complete();} style. Is there a real need for
> post-registerable listeners?
> >
> > cheers,
> > jed.
> >
> > Sam Berlin wrote:
> >>
> >> Hi All,
> >>
> >> I spent the past few days creating a general framework for
> listening
> >> to Future events, so that listeners can be notified
> asynchronously of
> >> the finishing status.  It's very much like completion handlers with
> >> I/O (but not like the jdk CompletionService class). The
> basic premise
> >> is:
> >>
> >>  ListeningFuture<Type> future =
> listeningExecutorService.submit(callable);
> >>  future.addFutureListener(new EventListener<FutureEvent<Type>>() {
> >>     public void handleEvent(FutureEvent<Type> event) {
> >>            // The future finished!
> >>     }
> >>  });
> >>
> >> Or alternate usage of:
> >>
> >>  ListeningRunnableFuture<Type> future = new
> >> ListeningFutureTask<Type>(callable);
> >>  executorService.execute(future);
> >>  future.addFutureListener( ... );
> >>
> >> The listener's notified whenever the future finishes.  I've found a
> >> need for this kind of asynchronous future notification in
> many places
> >> (notably because I don't want to have to spawn a thread
> just to wait
> >> on a get).
> >>
> >> Is there a reason why support for asynchronously listening
> to future
> >> events was not added into Future to begin with?  It seems like a
> >> terribly useful thing.  I've always found it strange that the
> >> concurrent package (which by its very nature is designed to provide
> >> excellent concurrent utilities) provides no way of asynchronously
> >> notifying of future-finished events.
> >>
> >> If anyone has copious amounts of spare time, I'd
> appreciate if someone
> >> could look over the class @
> >>
> >>
> <https://www.limewire.org/fisheye/browse/~raw,r=1.2/limecvs/co
mponents/common/src/main/java/org/limewire/concurrent/ListeningF> utureTask.java>
> >> and provide some feedback.  It's fairly general-purpose (except for
> >> it's use of EventListenerList, which is a custom listening list we
> >> have, but that's easy to swap out.)
> >>
> >> Thanks.
> >>
> >> Sam
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
--------------------------------------------------------

NOTICE: If received in error, please destroy and notify sender. Sender does not intend to waive confidentiality or privilege. Use of this email is prohibited when received in error.


From oztalip at gmail.com  Fri Nov  7 06:23:40 2008
From: oztalip at gmail.com (Talip Ozturk)
Date: Fri, 7 Nov 2008 13:23:40 +0200
Subject: [concurrency-interest] Future Done Events?
In-Reply-To: <E295C18F4AD63149810A83F7B7E842B118E012B6FB@LNWEXMBX0105.msad.ms.com>
References: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
	<4913B9E1.2090303@atlassian.com>
	<19196d860811062128h6dd6a65as701a20abfd45120d@mail.gmail.com>
	<E295C18F4AD63149810A83F7B7E842B118E012B6FB@LNWEXMBX0105.msad.ms.com>
Message-ID: <6a08fe2c0811070323v250f0fd2oac8f6ccdf0c5f907@mail.gmail.com>

I got the same 'FutureListener' type request for Hazelcast which has
distributed implementation of ExecutorService. We introduced
ExecutionCallback interface:

import java.util.concurrent.Future;
public interface ExecutionCallback<V> {
	void done (Future<V> future);
}

And here is how it is used:
http://web.hazelcast.com/doc/index.html#dist_execution_callback

But extending FutureTask and overriding the done() method looks as good to me.

Regards,
-talip


On Fri, Nov 7, 2008 at 12:56 PM, Rowlands, Ben (IT)
<Ben.Rowlands at morganstanley.com> wrote:
> I was looking for a similar API a while back to efficiently join on multiple futures. At present we use polling for >1 future as we need to detect execution failures eagerly:
>
>  http://markmail.org/message/4dswxghye6b7x6yp
>
> We also couldn't use the wrapping up of callable/runnable notification trick as we don't necessarily implement the execution of the request.
>
> Within our group we could agree on a convention that all async APIs that return a Future should also accept a Runnable argument (or some custom "FutureListener" interface) to be run when the result is ready. But without a JDK API to implement (ideally something like the pervasive .NET AsyncCallback/IAsyncResult pattern that finds its way into all .NET Async APIs offering blocking and callback style of result delivery) we always need to use polling for async APIs that don't follow this pattern.
>
> Ben Rowlands
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
>> Of Sam Berlin
>> Sent: 07 November 2008 05:28
>> To: Jed Wesley-Smith
>> Cc: concurrency-interest
>> Subject: Re: [concurrency-interest] Future Done Events?
>>
>> In cases where you're the one submitting the task, it works well to
>> create a wrapper around the Callable/Runnable.  In other cases, where
>> the task is submitted by another API and all it can return is a Future
>> of the result, you're generally at a loss to asynchronously listen to
>> the status.  It's very useful to be able to hide away the details of
>> not only when & where the task is executed, but also how the task is
>> executed.
>>
>> An example use-case I have for this is processing something from a
>> file-management API, ie:
>>
>> interface FileInfoRepository {
>>    Future<FileInfo> add(File file);
>>    List<Future<FileInfo>> addFolder(File folder);
>> }
>>
>> Users of that interface can add Files & blockingly get the results of
>> the added file, but have no way of asynchronously getting details on
>> what it created.  If the API already internally takes care of
>> threading, the user shouldn't have to create a Thread just to
>> asychronously do something on the add finishing.  You can add a
>> listener as a parameter to the add method, but I find it very
>> cumbersome to overload the entire API with listeners when threading is
>> already built-in to the return value (in addition to bloating the
>> API).
>>
>> It becomes even more useful when dealing with other layers.  For
>> example, consider you want to have a shared drag/drop handler that
>> adds files into the repository.  It can take in the repository it
>> wants to add files to, do a bunch of processing to make sure the files
>> are OK to add, and then iterate through & add the files, passing the
>> results to another party.  The other party can then take the Future,
>> immediately set a "processing" state, add a listener to it, and
>> asynchronously change it to a "finished" state.
>>
>> It's certainly not something that's absolutely required -- there's
>> definite ways around a listening infrastructure.  It does seem,
>> though, that having the ability to listen to future's finishing
>> creates a lot of elegant results.
>>
>> Sam
>>
>> On Thu, Nov 6, 2008 at 10:45 PM, Jed Wesley-Smith
>> <jed at atlassian.com> wrote:
>> > Whenever I've needed similar functionality I've simply used
>> a delegating
>> > Callable/Runnable that wraps the first one in a
>> try{delegate.run();} finally
>> > {complete();} style. Is there a real need for
>> post-registerable listeners?
>> >
>> > cheers,
>> > jed.
>> >
>> > Sam Berlin wrote:
>> >>
>> >> Hi All,
>> >>
>> >> I spent the past few days creating a general framework for
>> listening
>> >> to Future events, so that listeners can be notified
>> asynchronously of
>> >> the finishing status.  It's very much like completion handlers with
>> >> I/O (but not like the jdk CompletionService class). The
>> basic premise
>> >> is:
>> >>
>> >>  ListeningFuture<Type> future =
>> listeningExecutorService.submit(callable);
>> >>  future.addFutureListener(new EventListener<FutureEvent<Type>>() {
>> >>     public void handleEvent(FutureEvent<Type> event) {
>> >>            // The future finished!
>> >>     }
>> >>  });
>> >>
>> >> Or alternate usage of:
>> >>
>> >>  ListeningRunnableFuture<Type> future = new
>> >> ListeningFutureTask<Type>(callable);
>> >>  executorService.execute(future);
>> >>  future.addFutureListener( ... );
>> >>
>> >> The listener's notified whenever the future finishes.  I've found a
>> >> need for this kind of asynchronous future notification in
>> many places
>> >> (notably because I don't want to have to spawn a thread
>> just to wait
>> >> on a get).
>> >>
>> >> Is there a reason why support for asynchronously listening
>> to future
>> >> events was not added into Future to begin with?  It seems like a
>> >> terribly useful thing.  I've always found it strange that the
>> >> concurrent package (which by its very nature is designed to provide
>> >> excellent concurrent utilities) provides no way of asynchronously
>> >> notifying of future-finished events.
>> >>
>> >> If anyone has copious amounts of spare time, I'd
>> appreciate if someone
>> >> could look over the class @
>> >>
>> >>
>> <https://www.limewire.org/fisheye/browse/~raw,r=1.2/limecvs/co
> mponents/common/src/main/java/org/limewire/concurrent/ListeningF> utureTask.java>
>> >> and provide some feedback.  It's fairly general-purpose (except for
>> >> it's use of EventListenerList, which is a custom listening list we
>> >> have, but that's easy to swap out.)
>> >>
>> >> Thanks.
>> >>
>> >> Sam
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> >
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> --------------------------------------------------------
>
> NOTICE: If received in error, please destroy and notify sender. Sender does not intend to waive confidentiality or privilege. Use of this email is prohibited when received in error.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From sberlin at gmail.com  Fri Nov  7 08:24:34 2008
From: sberlin at gmail.com (Sam Berlin)
Date: Fri, 7 Nov 2008 08:24:34 -0500
Subject: [concurrency-interest] Future Done Events?
In-Reply-To: <E295C18F4AD63149810A83F7B7E842B118E012B6FB@LNWEXMBX0105.msad.ms.com>
References: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
	<4913B9E1.2090303@atlassian.com>
	<19196d860811062128h6dd6a65as701a20abfd45120d@mail.gmail.com>
	<E295C18F4AD63149810A83F7B7E842B118E012B6FB@LNWEXMBX0105.msad.ms.com>
Message-ID: <19196d860811070524p38310844j7cf4361861567d06@mail.gmail.com>

> Within our group we could agree on a convention that all async APIs that return a Future should also accept a Runnable argument (or some custom "FutureListener" interface) to be run when the result is ready. But without a JDK API to implement (ideally something like the pervasive .NET AsyncCallback/IAsyncResult pattern that finds its way into all .NET Async APIs offering blocking and callback style of result delivery) we always need to use polling for async APIs that don't follow this pattern.

I was going to go something like this route, but really didn't like
overloading APIs throughout the program so much (or passing in null if
you didn't want to listen).  Instead, there's a bunch of new
'Listening' interfaces/classes that extend the old ones, ie:
ListeningExecutorService, ListeningFuture, ListeningFutureTask,
RunnableListeningFuture, ScheduledListeningFutre,
RunnableScheduledListeningFuture, ScheduledListeningExecutorService,
AbstractListeningExecutorService, ThreadPoolListeningExecutor, & an
ExecutorsHelper that's much like Executors, but provides methods that
return and deal with listening executors.

This way, any class that internally used an ExecutorService can very
easily swap it out to use a ListeningExecutorService & return a
ListeningFuture (where it used to return a future), and the caller can
add listeners themselves.

Re: Neil's mention that the listenerList is not threadsafe.  That
usage is actually intentional.  The only place that changes the
reference is in the done() method, where it grabs the current copy and
sets it to null.  This is so that listeners added prior to finishing
are queued up in that list, whereas listeners added after finishing
are just immediately notified and then discarded.

Sam

From neil at swingler.ch  Fri Nov  7 09:13:19 2008
From: neil at swingler.ch (Neil Swingler)
Date: Fri, 07 Nov 2008 15:13:19 +0100
Subject: [concurrency-interest] =?utf-8?q?Future_Done_Events=3F?=
Message-ID: <3d62e23ffb90ca02be9999daa85cf459@swingler.ch>


Sent this directly to Sam at first


I agree. Asynchronous notification would fit very well with Future. IMO the
semantics should be that the listener gets called exactly one time
regardless of whether it is added before or after the value is set
(otherwise you end up with races when you add the listener around the same
time as the value is set). I guess there needs to be a corresponding
mechanism for reporting exceptions.

BTW Sam I think that your listener list is not thread safe. The
listenersRef.compareAndSet will always succeed because you always use the
same list object. To use an atomic variable to hold the list, you need to
make a new copy of the list every time you mutate it.

- Neil

On Fri, 7 Nov 2008 00:28:11 -0500, "Sam Berlin" <sberlin at gmail.com> wrote:
> In cases where you're the one submitting the task, it works well to
> create a wrapper around the Callable/Runnable.  In other cases, where
> the task is submitted by another API and all it can return is a Future
> of the result, you're generally at a loss to asynchronously listen to
> the status.  It's very useful to be able to hide away the details of
> not only when & where the task is executed, but also how the task is
> executed.
>
> An example use-case I have for this is processing something from a
> file-management API, ie:
>
> interface FileInfoRepository {
>    Future<FileInfo> add(File file);
>    List<Future<FileInfo>> addFolder(File folder);
> }
>
> Users of that interface can add Files & blockingly get the results of
> the added file, but have no way of asynchronously getting details on
> what it created.  If the API already internally takes care of
> threading, the user shouldn't have to create a Thread just to
> asychronously do something on the add finishing.  You can add a
> listener as a parameter to the add method, but I find it very
> cumbersome to overload the entire API with listeners when threading is
> already built-in to the return value (in addition to bloating the
> API).
>
> It becomes even more useful when dealing with other layers.  For
> example, consider you want to have a shared drag/drop handler that
> adds files into the repository.  It can take in the repository it
> wants to add files to, do a bunch of processing to make sure the files
> are OK to add, and then iterate through & add the files, passing the
> results to another party.  The other party can then take the Future,
> immediately set a "processing" state, add a listener to it, and
> asynchronously change it to a "finished" state.
>
> It's certainly not something that's absolutely required -- there's
> definite ways around a listening infrastructure.  It does seem,
> though, that having the ability to listen to future's finishing
> creates a lot of elegant results.
>
> Sam
>
> On Thu, Nov 6, 2008 at 10:45 PM, Jed Wesley-Smith <jed at atlassian.com>
> wrote:
>> Whenever I've needed similar functionality I've simply used a delegating
>> Callable/Runnable that wraps the first one in a try{delegate.run();}
> finally
>> {complete();} style. Is there a real need for post-registerable
> listeners?
>>
>> cheers,
>> jed.
>>
>> Sam Berlin wrote:
>>>
>>> Hi All,
>>>
>>> I spent the past few days creating a general framework for listening
>>> to Future events, so that listeners can be notified asynchronously of
>>> the finishing status.  It's very much like completion handlers with
>>> I/O (but not like the jdk CompletionService class). The basic premise
>>> is:
>>>
>>>  ListeningFuture<Type> future =
> listeningExecutorService.submit(callable);
>>>  future.addFutureListener(new EventListener<FutureEvent<Type>>() {
>>>     public void handleEvent(FutureEvent<Type> event) {
>>>            // The future finished!
>>>     }
>>>  });
>>>
>>> Or alternate usage of:
>>>
>>>  ListeningRunnableFuture<Type> future = new
>>> ListeningFutureTask<Type>(callable);
>>>  executorService.execute(future);
>>>  future.addFutureListener( ... );
>>>
>>> The listener's notified whenever the future finishes.  I've found a
>>> need for this kind of asynchronous future notification in many places
>>> (notably because I don't want to have to spawn a thread just to wait
>>> on a get).
>>>
>>> Is there a reason why support for asynchronously listening to future
>>> events was not added into Future to begin with?  It seems like a
>>> terribly useful thing.  I've always found it strange that the
>>> concurrent package (which by its very nature is designed to provide
>>> excellent concurrent utilities) provides no way of asynchronously
>>> notifying of future-finished events.
>>>
>>> If anyone has copious amounts of spare time, I'd appreciate if someone
>>> could look over the class @
>>>
>>>
>
<https://www.limewire.org/fisheye/browse/~raw,r=1.2/limecvs/components/common/src/main/java/org/limewire/concurrent/ListeningFutureTask.java>
>>> and provide some feedback.  It's fairly general-purpose (except for
>>> it's use of EventListenerList, which is a custom listening list we
>>> have, but that's easy to swap out.)
>>>
>>> Thanks.
>>>
>>> Sam
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From gregg at cytetech.com  Fri Nov  7 10:11:16 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 07 Nov 2008 09:11:16 -0600
Subject: [concurrency-interest] Future Done Events?
In-Reply-To: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
References: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
Message-ID: <49145A94.5060005@cytetech.com>

Sam Berlin wrote:
> Hi All,
> 
> I spent the past few days creating a general framework for listening
> to Future events, so that listeners can be notified asynchronously of
> the finishing status.  It's very much like completion handlers with
> I/O (but not like the jdk CompletionService class). 

...

> It's fairly general-purpose (except for
> it's use of EventListenerList, which is a custom listening list we
> have, but that's easy to swap out.)

I made a local copy of this class and it's dependents in a package for me to 
evaluate.  There were 18 classes total in the dependency tree, and I changed the 
logging use to java.util.logging.  I also renamed EventListener to 
TypedEventListener to keep from having a conflict with the AWT name.

I am most interested in this for swing use, and I need to look more at the 
annotation mechanism that seems to allow me to do the following.

TypedEventListener<FutureEvent<List<Data>>> te =
     new TypedEventListener<FutureEvent<List<Data>>>() {
	@SwingEDTEvent
	public void handleEvent( FutureEvent<List<Data>> ev ) {
	    if( ev.getType().equals( Type.SUCCESS ) ) {
		model.setContents( ev.getResult() );
	    } else if( ev.getType().equals( Type.EXCEPTION ) {
		reportException( ev.getException() );
	    }
	}
     };

final ServiceInterface srv = ...;
ListeningFutureTask<List<Data>> filler = new ListeningFutureTask<List<Data>>(
     new Callable() {
	public List<Data> call() throws RemoteException {
	    return srv.getData();
	}
     });

ThreadPoolExecutor exec = new ThreadPoolExector(...);

exec.submit( filler );
filler.addFutureListener( te );

Today, I do this as

	new ComponentUpdateThread<List<Data>>( ...components to disable... ) {
	    public List<Data> doInBackground() {
		try {
		    return srv.getData();
		} catch( Exception ex ) {
		    reportException( ex );
		}
		return null;
	    }
	    public void done() {
		List<Data> dt = getResult();
		    if( dt != null ) {
			model.setContents( dt );
		    }
	    }
	}.start();

The issue is that my old way is not a restartable instance.  So,
I have to recreate these over and over and I can't pass them around
and say "do this when the data changes".  With Sam's architecture,
there is an ability to pass around reusable instances of things
that can be "done" when something needs to be updated based on async
activitiy.

The clever use of annotations for SWING and other types of running context are 
nice.  In the world of Jini and RMI, context class loaders and JAAS subject 
contexts are important.  I think this will give me a good start on a new way of 
managing all of these contexts, allowing the constructing environment to get the 
information it needs to pass to the "execution" environment to make sure the 
right context is active.

My ComponentUpdateThread class, captures the context class loader and the active 
subject to reactivate in the doInBackground() context and other places where a 
different thread context may be active.

Having this support for asynchronous actions is important for me...

Gregg Wonderly

From sberlin at gmail.com  Fri Nov  7 10:25:09 2008
From: sberlin at gmail.com (Sam Berlin)
Date: Fri, 7 Nov 2008 10:25:09 -0500
Subject: [concurrency-interest] Future Done Events?
In-Reply-To: <49145A94.5060005@cytetech.com>
References: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
	<49145A94.5060005@cytetech.com>
Message-ID: <19196d860811070725g51dbf131qc6bfee9ee63fcfee@mail.gmail.com>

The annotation support on our EventListenerList is particularly nice.
There's also a @BlockingEvent annotation that instructs the list to
call the event on a brand new thread.

One small note on your sample code...

> ListeningFutureTask<List<Data>> filler = new
> ListeningFutureTask<List<Data>>(
>    new Callable() {
>        public List<Data> call() throws RemoteException {
>            return srv.getData();
>        }
>    });
>
> ThreadPoolExecutor exec = new ThreadPoolExector(...);
>
> exec.submit( filler );
> filler.addFutureListener( te );

That could be better written as:

ThreadPoolListeningExecutor exec = new ThreadPoolListeningExecutor(...);
filler = exec.submit(callable);
filler.addFutureListener(te);

Or, alternately:

RunnableListeningFuture<List<Data>> runner = new
   ListeningFutureTask<List<Data>(...);
ThreadPoolExecutor exec = new ThreadPoolExector(...);
exec.execute(runner);
runner.addFutureListener( te );

The first variation uses an extended form of TPE that returns a
ListeningFuture on submit, whereas the second variation uses the old
TPE but submits using execute instead of submit, to prevent creating a
useless Future around the Runnable.

Sam

On Fri, Nov 7, 2008 at 10:11 AM, Gregg Wonderly <gregg at cytetech.com> wrote:
> Sam Berlin wrote:
>>
>> Hi All,
>>
>> I spent the past few days creating a general framework for listening
>> to Future events, so that listeners can be notified asynchronously of
>> the finishing status.  It's very much like completion handlers with
>> I/O (but not like the jdk CompletionService class).
>
> ...
>
>> It's fairly general-purpose (except for
>> it's use of EventListenerList, which is a custom listening list we
>> have, but that's easy to swap out.)
>
> I made a local copy of this class and it's dependents in a package for me to
> evaluate.  There were 18 classes total in the dependency tree, and I changed
> the logging use to java.util.logging.  I also renamed EventListener to
> TypedEventListener to keep from having a conflict with the AWT name.
>
> I am most interested in this for swing use, and I need to look more at the
> annotation mechanism that seems to allow me to do the following.
>
> TypedEventListener<FutureEvent<List<Data>>> te =
>    new TypedEventListener<FutureEvent<List<Data>>>() {
>        @SwingEDTEvent
>        public void handleEvent( FutureEvent<List<Data>> ev ) {
>            if( ev.getType().equals( Type.SUCCESS ) ) {
>                model.setContents( ev.getResult() );
>            } else if( ev.getType().equals( Type.EXCEPTION ) {
>                reportException( ev.getException() );
>            }
>        }
>    };
>
> final ServiceInterface srv = ...;
> ListeningFutureTask<List<Data>> filler = new
> ListeningFutureTask<List<Data>>(
>    new Callable() {
>        public List<Data> call() throws RemoteException {
>            return srv.getData();
>        }
>    });
>
> ThreadPoolExecutor exec = new ThreadPoolExector(...);
>
> exec.submit( filler );
> filler.addFutureListener( te );
>
> Today, I do this as
>
>        new ComponentUpdateThread<List<Data>>( ...components to disable... )
> {
>            public List<Data> doInBackground() {
>                try {
>                    return srv.getData();
>                } catch( Exception ex ) {
>                    reportException( ex );
>                }
>                return null;
>            }
>            public void done() {
>                List<Data> dt = getResult();
>                    if( dt != null ) {
>                        model.setContents( dt );
>                    }
>            }
>        }.start();
>
> The issue is that my old way is not a restartable instance.  So,
> I have to recreate these over and over and I can't pass them around
> and say "do this when the data changes".  With Sam's architecture,
> there is an ability to pass around reusable instances of things
> that can be "done" when something needs to be updated based on async
> activitiy.
>
> The clever use of annotations for SWING and other types of running context
> are nice.  In the world of Jini and RMI, context class loaders and JAAS
> subject contexts are important.  I think this will give me a good start on a
> new way of managing all of these contexts, allowing the constructing
> environment to get the information it needs to pass to the "execution"
> environment to make sure the right context is active.
>
> My ComponentUpdateThread class, captures the context class loader and the
> active subject to reactivate in the doInBackground() context and other
> places where a different thread context may be active.
>
> Having this support for asynchronous actions is important for me...
>
> Gregg Wonderly
>

From elihusmails at gmail.com  Sat Nov  8 20:23:59 2008
From: elihusmails at gmail.com (Mark Webb)
Date: Sat, 8 Nov 2008 20:23:59 -0500
Subject: [concurrency-interest] Future Done Events?
In-Reply-To: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
References: <19196d860811061611i5d2a8341ofd82e46d40c486a1@mail.gmail.com>
Message-ID: <9f066ee90811081723k12b244a3m81a61fa58a22306d@mail.gmail.com>

We have something similar to this for I/O events in Apache MINA.  If
you are interested, check the IoFutureListener class.

--Mark

On Thu, Nov 6, 2008 at 7:11 PM, Sam Berlin <sberlin at gmail.com> wrote:
> Hi All,
>
> I spent the past few days creating a general framework for listening
> to Future events, so that listeners can be notified asynchronously of
> the finishing status.  It's very much like completion handlers with
> I/O (but not like the jdk CompletionService class). The basic premise
> is:
>
>  ListeningFuture<Type> future = listeningExecutorService.submit(callable);
>  future.addFutureListener(new EventListener<FutureEvent<Type>>() {
>     public void handleEvent(FutureEvent<Type> event) {
>            // The future finished!
>     }
>  });
>
> Or alternate usage of:
>
>  ListeningRunnableFuture<Type> future = new
> ListeningFutureTask<Type>(callable);
>  executorService.execute(future);
>  future.addFutureListener( ... );
>
> The listener's notified whenever the future finishes.  I've found a
> need for this kind of asynchronous future notification in many places
> (notably because I don't want to have to spawn a thread just to wait
> on a get).
>
> Is there a reason why support for asynchronously listening to future
> events was not added into Future to begin with?  It seems like a
> terribly useful thing.  I've always found it strange that the
> concurrent package (which by its very nature is designed to provide
> excellent concurrent utilities) provides no way of asynchronously
> notifying of future-finished events.
>
> If anyone has copious amounts of spare time, I'd appreciate if someone
> could look over the class @
> <https://www.limewire.org/fisheye/browse/~raw,r=1.2/limecvs/components/common/src/main/java/org/limewire/concurrent/ListeningFutureTask.java>
> and provide some feedback.  It's fairly general-purpose (except for
> it's use of EventListenerList, which is a custom listening list we
> have, but that's easy to swap out.)
>
> Thanks.
>
> Sam
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From unmesh_joshi at hotmail.com  Sun Nov  9 20:48:46 2008
From: unmesh_joshi at hotmail.com (Unmesh joshi)
Date: Mon, 10 Nov 2008 01:48:46 +0000
Subject: [concurrency-interest] Preemptive multitasking and context switching
In-Reply-To: <mailman.1.1226250001.7617.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1226250001.7617.concurrency-interest@cs.oswego.edu>
Message-ID: <BAY140-W46CA7B2C047356403A6F1CEF1A0@phx.gbl>


Hi,I have a very basic question.In preemptive multitasking operating system, processes are switched after a time slice. How does operating system get  control back, while a process is executing, to switch the context?  I was looking at the Linux kernel's switch_to macro for x86 which is as following. In the folllowing code, while saving task state, how does the process state is saved and not the OS kenel state, which must be executing while doing the context switch? Isn't the OS kernel code which is using the processor while doing context switch has its state in the registers that its trying to save?#define switch_to(prev, next, last)                                     \do {                                                                    \        /*                                                              \                                                                                                                                        * Context-switching clobbers all registers, so we clobber      \                                                                                                                                        * them explicitly, via unused output variables.                \                                                                                                                                        * (EAX and EBP is not listed because EBP is saved/restored     \                                                                                                                                        * explicitly for wchan access and EAX is the return value of   \                                                                                                                                        * __switch_to())                                               \                                                                                                                                        */                                                             \        unsigned long ebx, ecx, edx, esi, edi;                          \                                                                        \        asm volatile("pushfl\n\t"               /* save    flags */     \                     "pushl %%ebp\n\t"          /* save    EBP   */     \                     "movl %%esp,%[prev_sp]\n\t"        /* save    ESP   */ \                     "movl %[next_sp],%%esp\n\t"        /* restore ESP   */ \                     "movl $1f,%[prev_ip]\n\t"  /* save    EIP   */     \                     "pushl %[next_ip]\n\t"     /* restore EIP   */     \                     "jmp __switch_to\n"        /* regparm call  */     \                     "1:\t"                                             \                     "popl %%ebp\n\t"           /* restore EBP   */     \                     "popfl\n"                  /* restore flags */     \                                                                        \                     /* output parameters */                            \                     : [prev_sp] "=m" (prev->thread.sp),                \                       [prev_ip] "=m" (prev->thread.ip),                \                       "=a" (last),                                     \                                                                        \                       /* clobbered output registers: */                \                       "=b" (ebx), "=c" (ecx), "=d" (edx),              \                       "=S" (esi), "=D" (edi)                           \                                                                        \                       /* input parameters: */                          \                     : [next_sp]  "m" (next->thread.sp),                \                       [next_ip]  "m" (next->thread.ip),                \                      /* regparm parameters for __switch_to(): */      \                       [prev]     "a" (prev),                           \                       [next]     "d" (next));                          \} while (0)Thanks,Unmesh
_________________________________________________________________
Search for videos of Bollywood, Hollywood, Mollywood and every other wood, only on Live.com 
http://www.live.com/?scope=video&form=MICOAL
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081110/791e3cb4/attachment.html>

From joe.bowbeer at gmail.com  Sun Nov  9 21:04:13 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 9 Nov 2008 18:04:13 -0800
Subject: [concurrency-interest] Preemptive multitasking and context
	switching
In-Reply-To: <BAY140-W46CA7B2C047356403A6F1CEF1A0@phx.gbl>
References: <mailman.1.1226250001.7617.concurrency-interest@cs.oswego.edu>
	<BAY140-W46CA7B2C047356403A6F1CEF1A0@phx.gbl>
Message-ID: <31f2a7bd0811091804p3e3ec4ebr7194832f217d444b@mail.gmail.com>

Can you state this in the form of a question about concurrency in Java, or
the JVM?

On Sun, Nov 9, 2008 at 5:48 PM, Unmesh joshi wrote:

>  Hi,
> I have a very basic question.
> In preemptive multitasking operating system, processes are switched after a
> time slice. How does operating system get  control back, while a process is
> executing, to switch the context?  I was looking at the Linux kernel's
> switch_to macro for x86 which is as following. In the folllowing code, while
> saving task state, how does the process state is saved and not the OS kenel
> state, which must be executing while doing the context switch? Isn't the OS
> kernel code which is using the processor while doing context switch has its
> state in the registers that its trying to save?
>
> #define switch_to(prev, next, last)                                     \
> do {                                                                    \
>         /*                                                              \
>
>
>          * Context-switching clobbers all registers, so we clobber      \
>
>
>          * them explicitly, via unused output variables.                \
>
>
>          * (EAX and EBP is not listed because EBP is saved/restored     \
>
>
>          * explicitly for wchan access and EAX is the return value of   \
>
>
>          * __switch_to())                                               \
>
>
>          */                                                             \
>         unsigned long ebx, ecx, edx, esi, edi;                          \
>                                                                         \
>         asm volatile("pushfl\n\t"               /* save    flags */     \
>                      "pushl %%ebp\n\t"          /* save    EBP   */     \
>                      "movl %%esp,%[prev_sp]\n\t"        /* save    ESP   */
> \
>                      "movl %[next_sp],%%esp\n\t"        /* restore ESP   */
> \
>                      "movl $1f,%[prev_ip]\n\t"  /* save    EIP   */     \
>                      "pushl %[next_ip]\n\t"     /* restore EIP   */     \
>                      "jmp __switch_to\n"        /* regparm call  */     \
>                      "1:\t"                                             \
>                      "popl %%ebp\n\t"           /* restore EBP   */     \
>                      "popfl\n"                  /* restore flags */     \
>                                                                         \
>                      /* output parameters */                            \
>                      : [prev_sp] "=m" (prev->thread.sp),                \
>                        [prev_ip] "=m" (prev->thread.ip),                \
>                        "=a" (last),                                     \
>                                                                         \
>                        /* clobbered output registers: */                \
>                        "=b" (ebx), "=c" (ecx), "=d" (edx),              \
>                        "=S" (esi), "=D" (edi)                           \
>                                                                         \
>                        /* input parameters: */                          \
>                      : [next_sp]  "m" (next->thread.sp),                \
>                        [next_ip]  "m" (next->thread.ip),                \
>                       /* regparm parameters for __switch_to(): */      \
>                        [prev]     "a" (prev),                           \
>                        [next]     "d" (next));                          \
> } while (0)
>
> Thanks,
> Unmesh
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081109/fdacc6ad/attachment-0001.html>

From dcholmes at optusnet.com.au  Sun Nov  9 21:06:09 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 10 Nov 2008 12:06:09 +1000
Subject: [concurrency-interest] Preemptive multitasking and context
	switching
In-Reply-To: <BAY140-W46CA7B2C047356403A6F1CEF1A0@phx.gbl>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEIKHOAA.dcholmes@optusnet.com.au>

This is a bit off-topic for this list but typically a timer interrupt allows
the OS to regain control of the process and enforce the context switch.
Additionally, if the process makes use of system calls then these calls into
the OS also allow for the placing of "hooks" such that the OS can regain
control.

The details of saving state etc depend on the hardware and OS and are
definitely off topic here. Most of the "decent" Operating Systems texts give
simple examples for simple operating systems (Minix, Xinu, various low-level
executives) and there are probably some in-depth Unix/Linux books (or even
old DOS books!) that show the added complexity on modern systems.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Unmesh joshi
  Sent: Monday, 10 November 2008 11:49 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Preemptive multitasking and context
switching


  Hi,


  I have a very basic question.

  In preemptive multitasking operating system, processes are switched after
a time slice. How does operating system get  control back, while a process
is executing, to switch the context?  I was looking at the Linux kernel's
switch_to macro for x86 which is as following. In the folllowing code, while
saving task state, how does the process state is saved and not the OS kenel
state, which must be executing while doing the context switch? Isn't the OS
kernel code which is using the processor while doing context switch has its
state in the registers that its trying to save?


  #define switch_to(prev, next, last)                                     \
  do {                                                                    \
          /*                                                              \
           * Context-switching clobbers all registers, so we clobber      \
           * them explicitly, via unused output variables.                \
           * (EAX and EBP is not listed because EBP is saved/restored     \
           * explicitly for wchan access and EAX is the return value of   \
           * __switch_to())                                               \
           */                                                             \
          unsigned long ebx, ecx, edx, esi, edi;                          \
                                                                          \
          asm volatile("pushfl\n\t"               /* save    flags */     \
                       "pushl %%ebp\n\t"          /* save    EBP   */     \
                       "movl %%esp,%[prev_sp]\n\t"        /* save    ESP
*/ \
                       "movl %[next_sp],%%esp\n\t"        /* restore ESP
*/ \
                       "movl $1f,%[prev_ip]\n\t"  /* save    EIP   */     \
                       "pushl %[next_ip]\n\t"     /* restore EIP   */     \
                       "jmp __switch_to\n"        /* regparm call  */     \
                       "1:\t"                                             \
                       "popl %%ebp\n\t"           /* restore EBP   */     \
                       "popfl\n"                  /* restore flags */     \
                                                                          \
                       /* output parameters */                            \
                       : [prev_sp] "=m" (prev->thread.sp),                \
                         [prev_ip] "=m" (prev->thread.ip),                \
                         "=a" (last),                                     \
                                                                          \
                         /* clobbered output registers: */                \
                         "=b" (ebx), "=c" (ecx), "=d" (edx),              \
                         "=S" (esi), "=D" (edi)                           \
                                                                          \
                         /* input parameters: */                          \
                       : [next_sp]  "m" (next->thread.sp),                \
                         [next_ip]  "m" (next->thread.ip),                \
                        /* regparm parameters for __switch_to(): */      \
                         [prev]     "a" (prev),                           \
                         [next]     "d" (next));                          \
  } while (0)


  Thanks,
  Unmesh










----------------------------------------------------------------------------
--
  What's on the ramp today could be on the streets tomorrow. Keep up with
trends on MSN Lifestyle Try it!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081110/8f5476c7/attachment.html>

From unmesh_joshi at hotmail.com  Sun Nov  9 21:35:53 2008
From: unmesh_joshi at hotmail.com (Unmesh joshi)
Date: Mon, 10 Nov 2008 02:35:53 +0000
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 46,
	Issue 8
In-Reply-To: <mailman.1.1226282656.7750.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1226282656.7750.concurrency-interest@cs.oswego.edu>
Message-ID: <BAY140-W38B05D7BEAE3038D957BC1EF1A0@phx.gbl>


> Can you state this in the form of a question about concurrency in Java, or> the JVM?Well, I got this question while reading Java concurrency in Practice, and because Java threading uses OS threading support underneath, I was curious to know how preemptive multitasking actually happens at OS/CPU level.
I thought understanding this is essential to have deeper understanding of java concurrency.
_________________________________________________________________
Searching for the best deals on travel? Visit MSN Travel.
http://in.msn.com/coxandkings
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081110/e7fbd92e/attachment.html>

From sanne.grinovero at gmail.com  Tue Nov 11 06:51:21 2008
From: sanne.grinovero at gmail.com (Sanne Grinovero)
Date: Tue, 11 Nov 2008 12:51:21 +0100
Subject: [concurrency-interest] ThreadPoolExecutor corePoolSize
	incompatibility between backport and j.u.c.
In-Reply-To: <FFBA6BA8-9B3C-48A5-B212-E810DB6998AD@gregluck.com>
References: <FFBA6BA8-9B3C-48A5-B212-E810DB6998AD@gregluck.com>
Message-ID: <50e5f6250811110351l3dacfea1k8c1cf3da5fdc9b61@mail.gmail.com>

by the way, you should check for
 if (executorService == null)
also inside the synchronized block, or you might initialize the
service more than once.

Sanne


2008/8/16 Greg Luck <gluck at gregluck.com>:
> I am looking at moving the new version of ehcache to j.u.c, from backport,
> as part of dropping support for Java 1.4.
> It turns out that all my tests which test code that uses backport's
> ThreadPoolExecutor never complete. The code which supplies the executor is
> shown below:
>    /**
>      * @return Gets the executor service. This is not publically accessible.
>      */
>     ThreadPoolExecutor getExecutorService() {
>         if (executorService == null) {
>             synchronized (this) {
>                 //0, 10, 60000, ?, Integer.MAXVALUE
>                 executorService = new ThreadPoolExecutor(0,10,
>                        60000, TimeUnit.MILLISECONDS, new
> LinkedBlockingQueue());
>             }
>         }
>         return executorService;
>     }
>
> Without understanding exactly why, I tried changing the corePoolSize to 1.
> This fixed the problem. My question is why? There is nothing in the JavaDoc
> I can see which says you must use a minimum of 1.
> And I am now worried that j.u.c. is flaky in Java 5.  Any comments from the
> list would be much appreciated.
>
> Regards
>
> Greg Luck
>
> web: http://gregluck.com
> skype: gregrluck
> yahoo: gregrluck
> mobile: +61 408 061 622
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From milla.sun at gmail.com  Wed Nov 12 02:38:57 2008
From: milla.sun at gmail.com (Milla Sun)
Date: Wed, 12 Nov 2008 15:38:57 +0800
Subject: [concurrency-interest] About CyclicAction
Message-ID: <611a1ad40811112338v6ab27df8i5192b620bfe0bab0@mail.gmail.com>

I downloaded the current jar files and found that the "CyclicAction" and
"TaskBarrier" not exist....
How to get them?

And , another question:
If I want to implement and generate SPMD code, does the fork-join framework
provide any mechanism?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081112/1f95420a/attachment.html>

From dl at cs.oswego.edu  Wed Nov 12 07:42:14 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 12 Nov 2008 07:42:14 -0500
Subject: [concurrency-interest] About CyclicAction
In-Reply-To: <611a1ad40811112338v6ab27df8i5192b620bfe0bab0@mail.gmail.com>
References: <611a1ad40811112338v6ab27df8i5192b620bfe0bab0@mail.gmail.com>
Message-ID: <491ACF26.9090700@cs.oswego.edu>

Milla Sun wrote:
> I downloaded the current jar files and found that the "CyclicAction" and 
> "TaskBarrier" not exist....
> How to get them?

These classes are being held hostage; sorry.
I've been working on extensions to the basic framework
that enable support for a wider range of task synchronization,
including better support for looping tasks. These classes
will re-emerge when that is more stable.

For people wondering about upcoming changes: The main extension
is to allow tasks to block, although only in cooperative/controlled
ways, in which case the pool may create spare workers etc to maintain
the desired parallelism level when necessary. Doing this
enables much simpler mixing of different styles of parallelism.
(For example, nested loops, transactional tasks, arbitrary DAGs.)
It will also lead to some other API changes and extensions.
While these are basically functional, they are still in
some flux. I haven't committed them mainly because I don't
want early users to suffer through a bunch of likely API
changes while they stabilize.

>  
> And , another question:
> If I want to implement and generate SPMD code, does the fork-join 
> framework provide any mechanism?
> 

Hooking this to GPU/CUDA/Cell etc backends is in principal possible
but a lot of work, that I don't think anyone has taken on yet.

-Doug



From bomba at cboe.com  Thu Nov 13 12:01:37 2008
From: bomba at cboe.com (Bomba, Craig)
Date: Thu, 13 Nov 2008 11:01:37 -0600
Subject: [concurrency-interest] Phaser
References: <4BD174404CF4A34C98322DC926CF862B064B3785@MSMAIL.cboent.cboe.com>
	<49079AFD.1040807@cs.oswego.edu>
Message-ID: <4BD174404CF4A34C98322DC926CF862B064B37D1@MSMAIL.cboent.cboe.com>

If I wanted to use this phaser now and avoid including the new
jsr166y.jar in my classpath this brings me down the path of wanting to
compile the source available on the below link.  Another  option would
be to install the jar to lib/ext.  Other options?

While attempting the compile route, I am seeing warnings with class
Unsafe (from sun.misc.Unsafe).  It seems Unsafe was introduced to this
code between the 1.3 version and now.  Should I be concerned with these
messages?  Not to sure how/why this is used.

Phaser.java:11: warning: sun.misc.Unsafe is Sun proprietary API and may
be removed in a future release
import sun.misc.Unsafe;
               ^
Phaser.java:894: warning: sun.misc.Unsafe is Sun proprietary API and may
be removed in a future release
    static final Unsafe _unsafe;
                 ^
Phaser.java:900: warning: sun.misc.Unsafe is Sun proprietary API and may
be removed in a future release
                Field f = Unsafe.class.getDeclaredField("theUnsafe");
                          ^
Phaser.java:902: warning: sun.misc.Unsafe is Sun proprietary API and may
be removed in a future release
                _unsafe = (Unsafe)f.get(null);
                           ^
Phaser.java:905: warning: sun.misc.Unsafe is Sun proprietary API and may
be removed in a future release
                _unsafe = Unsafe.getUnsafe();
                          ^
5 warnings

Thanks in advance,
Craig

-----Original Message-----
From: Doug Lea [mailto:dl at cs.oswego.edu] 
Sent: Tuesday, October 28, 2008 6:07 PM
To: Bomba, Craig
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Phaser

Bomba, Craig wrote:
> Is there a mature version of Phaser that is available to use?
> 

The current jsr166y version is correct as far as we know
and performs well. The main incomplete to-do-list item is
to re-integrate with forkjoin framework (so you can use Phasers
in ForkJoinTasks, which you cannot yet), which in turn awaits
some internal FJ enhancements I've been working on but won't
be out for a while.

So please do use them (but not in FJ, and tell us if you have
suggestions for improvements, including suggestions for
improved documentation etc.

Get them at:
http://gee.cs.oswego.edu/dl/concurrency-interest/index.html

-Doug




From dl at cs.oswego.edu  Thu Nov 13 14:40:14 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 13 Nov 2008 14:40:14 -0500
Subject: [concurrency-interest] Phaser
In-Reply-To: <4BD174404CF4A34C98322DC926CF862B064B37D1@MSMAIL.cboent.cboe.com>
References: <4BD174404CF4A34C98322DC926CF862B064B3785@MSMAIL.cboent.cboe.com>
	<49079AFD.1040807@cs.oswego.edu>
	<4BD174404CF4A34C98322DC926CF862B064B37D1@MSMAIL.cboent.cboe.com>
Message-ID: <491C829E.1090805@cs.oswego.edu>

Bomba, Craig wrote:
> If I wanted to use this phaser now and avoid including the new
> jsr166y.jar in my classpath this brings me down the path of wanting to
> compile the source available on the below link.  Another  option would
> be to install the jar to lib/ext.  Other options?

Those are the only ways I know.

> 
> While attempting the compile route, I am seeing warnings with class
> Unsafe (from sun.misc.Unsafe). 

The special forms of compareAndSet etc in Unsafe are designed for
core library use, which this will eventually be, we hope (jdk7).
Until then, we use a hack that allows them to be compiled and
used, although with warnings. Just ignore the warnings.

-Doug

From trustin at gmail.com  Fri Nov 14 14:29:37 2008
From: trustin at gmail.com (Trustin Lee)
Date: Sat, 15 Nov 2008 04:29:37 +0900
Subject: [concurrency-interest] An infinite loop in LinkedTransferQueue
Message-ID: <20081114192936.GA3022@hermes.local>

Hi,

I am using a slightly modified version of LinkedTransferQueue.  All
modifications are related with serialization, so it's basically
identical with the original version: http://tinyurl.com/5u5w56

One of Netty users has reported an interesting problem with
LinkedTransferQueue.  He says that there's an edge case where
LinkedTransferQueue.clean() doesn't return the control to the caller
(i.e. an infinite loop in the for (;;) { .. } block.)

Unfortunately, I was not able to reproduce the problem in my machine, so
I am depending on his analysis.  However, we are having difficulties
tracking down the problem at this moment.

Could anyone take a look into this issue?  All the discussion so far is
archived here: http://tinyurl.com/5o4f3x

Thanks,
Trustin

PS: Netty homepage - http://www.jboss.org/netty/
-- 
Trustin Lee, Principal Software Engineer, JBoss, a division of Red Hat
--
what we call human nature is actually human habit
--
http://gleamynode.net/
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 197 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081115/0f173cff/attachment.bin>

From dl at cs.oswego.edu  Fri Nov 14 15:53:59 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 14 Nov 2008 15:53:59 -0500
Subject: [concurrency-interest] An infinite loop in LinkedTransferQueue
In-Reply-To: <20081114192936.GA3022@hermes.local>
References: <20081114192936.GA3022@hermes.local>
Message-ID: <491DE567.6020706@cs.oswego.edu>

Trustin Lee wrote:
> Hi,
> 
> I am using a slightly modified version of LinkedTransferQueue.  All
> modifications are related with serialization, so it's basically
> identical with the original version: http://tinyurl.com/5u5w56
> 
> One of Netty users has reported an interesting problem with
> LinkedTransferQueue.  He says that there's an edge case where
> LinkedTransferQueue.clean() doesn't return the control to the caller
> (i.e. an infinite loop in the for (;;) { .. } block.)
> 
> Unfortunately, I was not able to reproduce the problem in my machine, so
> I am depending on his analysis.  However, we are having difficulties
> tracking down the problem at this moment.
> 
> Could anyone take a look into this issue?  All the discussion so far is
> archived here: http://tinyurl.com/5o4f3x
> 

Thanks for the preliminary investigation! I'll
take a look and follow-up off-list to see if it is an LTQ
problem and if so fix it.

-Doug


From fw at deneb.enyo.de  Fri Nov 14 17:43:01 2008
From: fw at deneb.enyo.de (Florian Weimer)
Date: Fri, 14 Nov 2008 23:43:01 +0100
Subject: [concurrency-interest] Doubts about ReadWriteLock
Message-ID: <87tzaa6ia2.fsf@mid.deneb.enyo.de>

In JCIP, there's an example of an Map wrapper using a ReadWriteLock
(listing 13.7 on page 288).  However, whether conceptual read accesses
internally result in writes is an implementation detail which is
usually not expressed in the interface.

For instance, LinkedHashMap (which is explicitly mentioned as a
candidate for being wrapped with a ReadWriteLock) has the following
property:

| In access-ordered linked hash maps, merely querying the map with get
| is a structural modification.)

WeakHashMaps share this property, although it is not explicitly
documented.

I suppose this means that the usefulness of ReadWriteLocks is fairly
limited because it introduces a rather awkward dependency on an
implementation detail.

From hans.boehm at hp.com  Fri Nov 14 18:30:33 2008
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 14 Nov 2008 23:30:33 +0000
Subject: [concurrency-interest] Doubts about ReadWriteLock
In-Reply-To: <87tzaa6ia2.fsf@mid.deneb.enyo.de>
References: <87tzaa6ia2.fsf@mid.deneb.enyo.de>
Message-ID: <238A96A773B3934685A7269CC8A8D0423B0DE70D18@GVW0436EXB.americas.hpqcorp.net>

Library documentation has to specify what constitutes a write access.  At least where it's not obvious.  Anything else is a bug.  That's an essential part of the interface that has nothing to do with ReadWriteLock.  Even in the case of ordinary synchronized blocks, concurrent readers are allowed, while concurrent writers are not.  The programmer has to know.

The library can of course hide write accesses, e.g. for caching, by internally protecting them with a lock, or by using equivalent lock-free techniques.

I think we had a prior discussion of WeakHashMaps, which either need to document or fix the behavior described here.

Unfortunately, I suspect this is an area that no real library specification has ever gotten completely correct.  But we should at least try to move things in the right direction.

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> Of Florian Weimer
> Sent: Friday, November 14, 2008 2:43 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Doubts about ReadWriteLock
>
> In JCIP, there's an example of an Map wrapper using a
> ReadWriteLock (listing 13.7 on page 288).  However, whether
> conceptual read accesses internally result in writes is an
> implementation detail which is usually not expressed in the interface.
>
> For instance, LinkedHashMap (which is explicitly mentioned as
> a candidate for being wrapped with a ReadWriteLock) has the following
> property:
>
> | In access-ordered linked hash maps, merely querying the map
> with get
> | is a structural modification.)
>
> WeakHashMaps share this property, although it is not
> explicitly documented.
>
> I suppose this means that the usefulness of ReadWriteLocks is
> fairly limited because it introduces a rather awkward
> dependency on an implementation detail.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From fw at deneb.enyo.de  Fri Nov 14 18:40:33 2008
From: fw at deneb.enyo.de (Florian Weimer)
Date: Sat, 15 Nov 2008 00:40:33 +0100
Subject: [concurrency-interest] Doubts about ReadWriteLock
In-Reply-To: <238A96A773B3934685A7269CC8A8D0423B0DE70D18@GVW0436EXB.americas.hpqcorp.net>
	(Hans Boehm's message of "Fri, 14 Nov 2008 23:30:33 +0000")
References: <87tzaa6ia2.fsf@mid.deneb.enyo.de>
	<238A96A773B3934685A7269CC8A8D0423B0DE70D18@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <87abc16fm6.fsf@mid.deneb.enyo.de>

* Hans Boehm:

> Library documentation has to specify what constitutes a write
> access.  At least where it's not obvious.  Anything else is a bug.
> That's an essential part of the interface that has nothing to do
> with ReadWriteLock.  Even in the case of ordinary synchronized
> blocks, concurrent readers are allowed, while concurrent writers are
> not.

Do you mean unsynchronized concurrent readers without concurrent
writers (the latter synchronized or not)?  Hmm, you are right.  So
ReadWriteLock isn't the culprit, really.  I'm not sure if this is any
better, quite the contrary. 8-(

From joe.bowbeer at gmail.com  Fri Nov 14 18:42:30 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 14 Nov 2008 15:42:30 -0800
Subject: [concurrency-interest] Doubts about ReadWriteLock
In-Reply-To: <87tzaa6ia2.fsf@mid.deneb.enyo.de>
References: <87tzaa6ia2.fsf@mid.deneb.enyo.de>
Message-ID: <31f2a7bd0811141542g62bab6c5hef7c025aa59fe714@mail.gmail.com>

Florian,

Good point.

The same things re: LinkedHashMap and WeakHashMap were noted on this list on
9/16/2007 in a discussion entitled "copy on write semantics".

Note, at least, that access-ordered LinkedHashMap is not the default.

Joe

On Fri, Nov 14, 2008 at 2:43 PM, Florian Weimer wrote:

> In JCIP, there's an example of an Map wrapper using a ReadWriteLock
> (listing 13.7 on page 288).  However, whether conceptual read accesses
> internally result in writes is an implementation detail which is
> usually not expressed in the interface.
>
> For instance, LinkedHashMap (which is explicitly mentioned as a
> candidate for being wrapped with a ReadWriteLock) has the following
> property:
>
> | In access-ordered linked hash maps, merely querying the map with get
> | is a structural modification.)
>
> WeakHashMaps share this property, although it is not explicitly
> documented.
>
> I suppose this means that the usefulness of ReadWriteLocks is fairly
> limited because it introduces a rather awkward dependency on an
> implementation detail.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081114/dd51919d/attachment.html>

From dcholmes at optusnet.com.au  Sun Nov 16 22:19:28 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 17 Nov 2008 13:19:28 +1000
Subject: [concurrency-interest] Doubts about ReadWriteLock
In-Reply-To: <87tzaa6ia2.fsf@mid.deneb.enyo.de>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEKAHOAA.dcholmes@optusnet.com.au>

Hi Florian,

Any time that you apply external synchronization to an object, other than
fully synchronizing every method (assuming no public mutable fields), you
are depending on internal implementation details, as it is the
implementation of a class that defines what exclusion constraints exist for
thread-safety, and so what form of locking protocol is correct. So this
limits the applicability of external use of synchronized, external
application of ReeentrantLock, and external application of
ReentrantReadWritelock.

So the JCiP section should be updated to reaffirm this constraint and to fix
the suggestion regarding LinkedHashMap.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Florian
> Weimer
> Sent: Saturday, 15 November 2008 8:43 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Doubts about ReadWriteLock
>
>
> In JCIP, there's an example of an Map wrapper using a ReadWriteLock
> (listing 13.7 on page 288).  However, whether conceptual read accesses
> internally result in writes is an implementation detail which is
> usually not expressed in the interface.
>
> For instance, LinkedHashMap (which is explicitly mentioned as a
> candidate for being wrapped with a ReadWriteLock) has the following
> property:
>
> | In access-ordered linked hash maps, merely querying the map with get
> | is a structural modification.)
>
> WeakHashMaps share this property, although it is not explicitly
> documented.
>
> I suppose this means that the usefulness of ReadWriteLocks is fairly
> limited because it introduces a rather awkward dependency on an
> implementation detail.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From willmcqueen at yahoo.com  Mon Nov 17 02:48:28 2008
From: willmcqueen at yahoo.com (Will McQueen)
Date: Sun, 16 Nov 2008 23:48:28 -0800 (PST)
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <mailman.1.1226768401.9376.concurrency-interest@cs.oswego.edu>
Message-ID: <547175.66104.qm@web50111.mail.re2.yahoo.com>

Hi,

Is the ThreadLocal class thread-safe?

I know this class can be thought of as holding a Map<Thread,T> (p45, JCIP)... but to extend this analogy, I'm not sure whether that map field is just a HashMap, or a ConcurrentHashMap. So, do calls from multiple threads need to be externally synchronized when calling a ThreadLocal instance?

I went back in the concurrency-interest archives as far back as Dec 2006, and couldn't find an answer to this. Btw, I wish the archives had a "Search" field... I had to load each month's subject headers into my browser one-by-one and do a Find on that page for "ThreadLocal"... is there a better, more comprehensive way of searching the archives?

Thank you.

Cheers,
Will


      

From mthornton at optrak.co.uk  Mon Nov 17 03:38:44 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Mon, 17 Nov 2008 08:38:44 +0000
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <547175.66104.qm@web50111.mail.re2.yahoo.com>
References: <547175.66104.qm@web50111.mail.re2.yahoo.com>
Message-ID: <49212D94.5040706@optrak.co.uk>

Will McQueen wrote:
> Hi,
>
> Is the ThreadLocal class thread-safe?
>
> I know this class can be thought of as holding a Map<Thread,T> (p45, JCIP)... but to extend this analogy, I'm not sure whether that map field is just a HashMap, or a ConcurrentHashMap. So, do calls from multiple threads need to be externally synchronized 
While it used to be implemented like that, I seem to remember that it 
was changed so that there is a seperate map on each Thread object. This 
avoids synchronization altogether.

Mark Thornton

From dcholmes at optusnet.com.au  Mon Nov 17 04:49:07 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 17 Nov 2008 19:49:07 +1000
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <547175.66104.qm@web50111.mail.re2.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEKEHOAA.dcholmes@optusnet.com.au>

By definition the ThreadLocal class must be thread-safe, no matter how it is
implemented. And although conceptually a map from a Thread to a value, it
can be implemented in different ways. As Mark indicated, the current (and
it's been this way for quite a while now) implementation is actually a
per-thread map from a variable to a value. Hence there is no shared mutable
state and no synchronization is needed.

BTW there are searchable archives of this list but I don't have the
reference handy. Dare I suggest that you search for it ... sorry :)

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Will
> McQueen
> Sent: Monday, 17 November 2008 5:48 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] ThreadLocal thread-safe?
>
>
> Hi,
>
> Is the ThreadLocal class thread-safe?
>
> I know this class can be thought of as holding a Map<Thread,T>
> (p45, JCIP)... but to extend this analogy, I'm not sure whether
> that map field is just a HashMap, or a ConcurrentHashMap. So, do
> calls from multiple threads need to be externally synchronized
> when calling a ThreadLocal instance?
>
> I went back in the concurrency-interest archives as far back as
> Dec 2006, and couldn't find an answer to this. Btw, I wish the
> archives had a "Search" field... I had to load each month's
> subject headers into my browser one-by-one and do a Find on that
> page for "ThreadLocal"... is there a better, more comprehensive
> way of searching the archives?
>
> Thank you.
>
> Cheers,
> Will
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From mthornton at optrak.co.uk  Mon Nov 17 05:00:37 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Mon, 17 Nov 2008 10:00:37 +0000
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEKEHOAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCKEKEHOAA.dcholmes@optusnet.com.au>
Message-ID: <492140C5.2010207@optrak.co.uk>

David Holmes wrote:
> can be implemented in different ways. As Mark indicated, the current (and
> it's been this way for quite a while now) implementation is actually a
> per-thread map from a variable to a value. Hence there is no shared mutable
>   
I submitted an RFE for the performance of the previous version to be 
improved  ... in July 1999!

Mark Thornton


From josh at bloch.us  Mon Nov 17 15:03:43 2008
From: josh at bloch.us (Joshua Bloch)
Date: Mon, 17 Nov 2008 12:03:43 -0800
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <492140C5.2010207@optrak.co.uk>
References: <NFBBKALFDCPFIDBNKAPCKEKEHOAA.dcholmes@optusnet.com.au>
	<492140C5.2010207@optrak.co.uk>
Message-ID: <b097ac510811171203j4e79adc6k26c32748e3bb88dc@mail.gmail.com>

FYI,the performance of ThreadLocal has been improved three times over the
years as people beat on them more heavily.  If memory serves, I did the
original implementation and the first rewrite, and Doug Lea did the next two
rewrites. Bob Lee did a high-performance clean room implementation for
Apache Harmony.
        Josh

On Mon, Nov 17, 2008 at 2:00 AM, Mark Thornton <mthornton at optrak.co.uk>wrote:

> David Holmes wrote:
>
>> can be implemented in different ways. As Mark indicated, the current (and
>> it's been this way for quite a while now) implementation is actually a
>> per-thread map from a variable to a value. Hence there is no shared
>> mutable
>>
>>
> I submitted an RFE for the performance of the previous version to be
> improved  ... in July 1999!
>
> Mark Thornton
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081117/d123566c/attachment.html>

From crazybob at crazybob.org  Mon Nov 17 17:00:23 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Mon, 17 Nov 2008 14:00:23 -0800
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <b097ac510811171203j4e79adc6k26c32748e3bb88dc@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCKEKEHOAA.dcholmes@optusnet.com.au>
	<492140C5.2010207@optrak.co.uk>
	<b097ac510811171203j4e79adc6k26c32748e3bb88dc@mail.gmail.com>
Message-ID: <a74683f90811171400kfa12e93wdaefa8feea7c3782@mail.gmail.com>

On Mon, Nov 17, 2008 at 12:03 PM, Joshua Bloch <josh at bloch.us> wrote:

> Bob Lee did a high-performance clean room implementation for Apache
> Harmony.


Here it is in case anyone's interested:
http://svn.apache.org/viewvc/harmony/enhanced/drlvm/trunk/vm/vmcore/src/kernel_classes/javasrc/java/lang/ThreadLocal.java?view=markup&pathrev=671836

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081117/c1065fcd/attachment.html>

From markus.kohler at gmail.com  Tue Nov 18 04:35:19 2008
From: markus.kohler at gmail.com (Markus Kohler)
Date: Tue, 18 Nov 2008 10:35:19 +0100
Subject: [concurrency-interest] Concurrent BitSets/BitField
Message-ID: <771905290811180135g33075dfdl97af0362bfae75ce@mail.gmail.com>

Hi all,
The key algorithm in the Eclipse Memory Analyzer (
http://www.eclipse.org/mat/), has the need to simulate the mark phase of a
Garbage Collector.
For the single threaded marking algorithm a BitField( kind of a BitSet with
a fixed size;
http://dev.eclipse.org/viewcvs/index.cgi/trunk/plugins/org.eclipse.mat.report/src/org/eclipse/mat/collect/BitField.java?root=Technology_MAT&view=markup)
is used to track which nodes have already been marked.

For the multithreaded case (marking in parallel) a simple boolean array is
used with no(!) synchronization nor CAS.
Check
http://dev.eclipse.org/viewcvs/index.cgi/trunk/plugins/org.eclipse.mat.parser/src/org/eclipse/mat/parser/internal/snapshot/ObjectMarker.java?revision=68&root=Technology_MAT&view=markup
and look for the comment "no synchronization"

The rationale for not using synchronization is that it turned out to be much
slower than just synchronizing a BitField and since the boolean's are only
changed from false to true, marking some of them more than once doesn't seem
to hurt performance.
They main drawback is that the boolean[] consumes 8 times more memory, which
is an issue because heap dumps can contain 10's of millions of objects.

First question:
Does this approach just work by accident (on the CPU/OS that were tested),
or is it in?

I was thinking about implementing a simple ConcurrentBitField using CAS.

This is my first attempt to implement a concurrent data structure using CAS,
so please forgive me, if it's completely broken:

/*******************************************************************************
 * Copyright (c) 2008 Markus Kohler.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Eclipse Public License v1.0
 * which accompanies this distribution, and is available at
 * http://www.eclipse.org/legal/epl-v10.html
 *
 * Contributors:
 *    Markus Kohler - initial API and implementation
 *******************************************************************************/
package org.eclipse.mat.collect;

import java.io.Serializable;
import java.util.concurrent.atomic.AtomicIntegerArray;

/**
 * This class manages huge bit fields. It is much faster than {@link
 * java.util.BitSet} and was specifically developed to be used with huge bit
 * sets in ISnapshot (e.g. needed in virtual GC traces). Out of performance
 * reasons no method does any parameter checking, i.e. only valid values are
 * expected.
 */
public final class ConcurrentBitField implements Serializable
{
    private static final long serialVersionUID = 1L;

    private AtomicIntegerArray bits;

    /**
     * Creates a bit field with the given number of bits. Size is expected
to be
     * positive - out of performance reasons no checks are done!
     */
    public ConcurrentBitField(int size)
    {
        bits = new AtomicIntegerArray((((size) - 1) >>> 0x5) + 1);
    }

    /**
     * Sets the bit on the given index. Index is expected to be in range -
out
     * of performance reasons no checks are done!
     */
    public final void set(int index)
    {
     int i = index >>> 0x5;
     while (true) {
            int current = bits.get(i);
            int newValue = current | (1 << (index & 0x1f));
            if (bits.compareAndSet(i, current, newValue))
                break;
        }
    }

    /**
     * Clears the bit on the given index. Index is expected to be in range -
out
     * of performance reasons no checks are done!
     */
    public final void clear(int index)
    {

     int i = index >>> 0x5;
     while (true) {
            int current = bits.get(i);
            int newValue = current & ~(1 << (index & 0x1f));
            if (bits.compareAndSet(i, current, newValue))
                break;
        }

    }

    /**
     * Gets the bit on the given index. Index is expected to be in range -
out
     * of performance reasons no checks are done!
     */
    public final boolean get(int index)
    {
        return (bits.get(index >>> 0x5) & (1 << (index & 0x1f))) != 0;
    }
}

Any comments whether this makes any sense at all are welcome!






I would be interested to hear whether anybody here has experience with
concurrent implementations of BitSets(/Fields).

 I tried to do some microbenchmarks to figure out whether the boolean[]
approach degrades with more threads and it seems that if I put more than 2
threads (on a dual core Intel) it slows down pretty much. That does not mean
much probably because you would not use more than 2 threads on a dual core
machine. Unfortunately I did not have a  768 core Azul machine available at
my fingertips ;)

For a high number of threads actually the ConcurrentBitField seems to  be
faster than the boolean[].


Regards,
Markus
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081118/7c15efea/attachment.html>

From aph at redhat.com  Tue Nov 18 05:34:53 2008
From: aph at redhat.com (Andrew Haley)
Date: Tue, 18 Nov 2008 10:34:53 +0000
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <a74683f90811171400kfa12e93wdaefa8feea7c3782@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCKEKEHOAA.dcholmes@optusnet.com.au>	<492140C5.2010207@optrak.co.uk>	<b097ac510811171203j4e79adc6k26c32748e3bb88dc@mail.gmail.com>
	<a74683f90811171400kfa12e93wdaefa8feea7c3782@mail.gmail.com>
Message-ID: <49229A4D.201@redhat.com>

Bob Lee wrote:
> On Mon, Nov 17, 2008 at 12:03 PM, Joshua Bloch <josh at bloch.us> wrote:
> 
>> Bob Lee did a high-performance clean room implementation for Apache
>> Harmony.
> 
> 
> Here it is in case anyone's interested:
> http://svn.apache.org/viewvc/harmony/enhanced/drlvm/trunk/vm/vmcore/src/kernel_classes/javasrc/java/lang/ThreadLocal.java?view=markup&pathrev=671836

That looks nice.  It's very similar to the way that thread-local
storage works on glibc based GNU/Linux systems, although that can be
somewhat more efficient because a separate page can be mapped at the
same logical address in each thread.  In turn, this means that an
access can be simply mem ( base address + key ), where key is a small
integer. [*]

I wonder why Harmony's implementation uses a hash code that is
converted to a table index, rather than simply starting at 1 and
incrementing each time a new thread local is created.  Then, the index
could be used directly to access the table, as glibc does.  This does
require special handling for recycling thread locals when they're no
longer needed, but I don't think that happens very often.  It won't
make much difference, though: just an extra mask and compare, and I
suppose collisions are rare.  Perhaps it's done this way in Harmony
because it doesn't require locking when creating / destroying
ThreadLocals?

For even greater efficiency, there's no reason why the VM couldn't use
the glibc trick of mapping multiple pages at the same address.
Perhaps ThreadLocals just aren't important enough to make it worth the
bother.

Andrew.

[*] This is a simplification of what actually happens: if there are
more thread locals than will fit in the table a second-level table is
created.  But you get the idea.

From hans.boehm at hp.com  Tue Nov 18 13:52:15 2008
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 18 Nov 2008 18:52:15 +0000
Subject: [concurrency-interest] Concurrent BitSets/BitField
In-Reply-To: <771905290811180135g33075dfdl97af0362bfae75ce@mail.gmail.com>
References: <771905290811180135g33075dfdl97af0362bfae75ce@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D0423B0E99282F@GVW0436EXB.americas.hpqcorp.net>

As much as I would generally discourage code that relies on data races, and having not looked at this in detail, I think that using a boolean array in this way is probably OK.  That's essentially the approach we use in our collector if parallel marking is enabled.  Bit vectors and CAS are also supported, but usually don't seem to win, in spite of an appreciable cache benefit due to the smaller size.  The bit vector and CAS approach might be more competitive on newer processors with lower CAS overheads.  But the last time I looked at this I decided to stay with bytes.

Although our implementation is in C, I can't think of a reason the Java memory model would disallow it.  It probably does allow the case in which no marker thread sees any of the mark bits set by other threads until they're all done.  But that's only a performance issue, and not e very realistic one. I think the correctness argument is very similar to that for the Sieve of Eratosthenes example outlined in my 2005 PLDI paper.

It there were a way to write this with volatiles or java.util.concurrent.atomic, that would be another  interesting data point.  But I don't think ther currently is?

Hans

________________________________
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Markus Kohler
Sent: Tuesday, November 18, 2008 1:35 AM
To: concurrency-interest
Subject: [concurrency-interest] Concurrent BitSets/BitField

Hi all,

The key algorithm in the Eclipse Memory Analyzer (http://www.eclipse.org/mat/), has the need to simulate the mark phase of a Garbage Collector.
For the single threaded marking algorithm a BitField( kind of a BitSet with a fixed size;http://dev.eclipse.org/viewcvs/index.cgi/trunk/plugins/org.eclipse.mat.report/src/org/eclipse/mat/collect/BitField.java?root=Technology_MAT&view=markup) is used to track which nodes have already been marked.

For the multithreaded case (marking in parallel) a simple boolean array is used with no(!) synchronization nor CAS.
Check http://dev.eclipse.org/viewcvs/index.cgi/trunk/plugins/org.eclipse.mat.parser/src/org/eclipse/mat/parser/internal/snapshot/ObjectMarker.java?revision=68&root=Technology_MAT&view=markup
and look for the comment "no synchronization"

The rationale for not using synchronization is that it turned out to be much slower than just synchronizing a BitField and since the boolean's are only changed from false to true, marking some of them more than once doesn't seem to hurt performance.
They main drawback is that the boolean[] consumes 8 times more memory, which is an issue because heap dumps can contain 10's of millions of objects.

First question:
Does this approach just work by accident (on the CPU/OS that were tested), or is it in?

I was thinking about implementing a simple ConcurrentBitField using CAS.

This is my first attempt to implement a concurrent data structure using CAS, so please forgive me, if it's completely broken:

/*******************************************************************************
 * Copyright (c) 2008 Markus Kohler.
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Eclipse Public License v1.0
 * which accompanies this distribution, and is available at
 * http://www.eclipse.org/legal/epl-v10.html
 *
 * Contributors:
 *    Markus Kohler - initial API and implementation
 *******************************************************************************/
package org.eclipse.mat.collect;

import java.io.Serializable;
import java.util.concurrent.atomic.AtomicIntegerArray;

/**
 * This class manages huge bit fields. It is much faster than {@link
 * java.util.BitSet} and was specifically developed to be used with huge bit
 * sets in ISnapshot (e.g. needed in virtual GC traces). Out of performance
 * reasons no method does any parameter checking, i.e. only valid values are
 * expected.
 */
public final class ConcurrentBitField implements Serializable
{
    private static final long serialVersionUID = 1L;

    private AtomicIntegerArray bits;

    /**
     * Creates a bit field with the given number of bits. Size is expected to be
     * positive - out of performance reasons no checks are done!
     */
    public ConcurrentBitField(int size)
    {
        bits = new AtomicIntegerArray((((size) - 1) >>> 0x5) + 1);
    }

    /**
     * Sets the bit on the given index. Index is expected to be in range - out
     * of performance reasons no checks are done!
     */
    public final void set(int index)
    {
     int i = index >>> 0x5;
     while (true) {
            int current = bits.get(i);
            int newValue = current | (1 << (index & 0x1f));
            if (bits.compareAndSet(i, current, newValue))
                break;
        }
    }

    /**
     * Clears the bit on the given index. Index is expected to be in range - out
     * of performance reasons no checks are done!
     */
    public final void clear(int index)
    {

     int i = index >>> 0x5;
     while (true) {
            int current = bits.get(i);
            int newValue = current & ~(1 << (index & 0x1f));
            if (bits.compareAndSet(i, current, newValue))
                break;
        }

    }

    /**
     * Gets the bit on the given index. Index is expected to be in range - out
     * of performance reasons no checks are done!
     */
    public final boolean get(int index)
    {
        return (bits.get(index >>> 0x5) & (1 << (index & 0x1f))) != 0;
    }
}

Any comments whether this makes any sense at all are welcome!






I would be interested to hear whether anybody here has experience with concurrent implementations of BitSets(/Fields).

 I tried to do some microbenchmarks to figure out whether the boolean[] approach degrades with more threads and it seems that if I put more than 2 threads (on a dual core Intel) it slows down pretty much. That does not mean much probably because you would not use more than 2 threads on a dual core machine. Unfortunately I did not have a  768 core Azul machine available at my fingertips ;)

For a high number of threads actually the ConcurrentBitField seems to  be faster than the boolean[].



Regards,
Markus
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081118/2afa2099/attachment.html>

From hans.boehm at hp.com  Tue Nov 18 13:59:45 2008
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 18 Nov 2008 18:59:45 +0000
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <49229A4D.201@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCKEKEHOAA.dcholmes@optusnet.com.au>
	<492140C5.2010207@optrak.co.uk>
	<b097ac510811171203j4e79adc6k26c32748e3bb88dc@mail.gmail.com>
	<a74683f90811171400kfa12e93wdaefa8feea7c3782@mail.gmail.com>
	<49229A4D.201@redhat.com>
Message-ID: <238A96A773B3934685A7269CC8A8D0423B0E992851@GVW0436EXB.americas.hpqcorp.net>

[Somewhat off-topic:]

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> Of Andrew Haley
> Sent: Tuesday, November 18, 2008 2:35 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] ThreadLocal thread-safe?
>
> Bob Lee wrote:
> > On Mon, Nov 17, 2008 at 12:03 PM, Joshua Bloch
> <josh at bloch.us> wrote:
> >
> >> Bob Lee did a high-performance clean room implementation
> for Apache
> >> Harmony.
> >
> >
> > Here it is in case anyone's interested:
> >
> http://svn.apache.org/viewvc/harmony/enhanced/drlvm/trunk/vm/vmcore/sr
> >
> c/kernel_classes/javasrc/java/lang/ThreadLocal.java?view=markup&pathre
> > v=671836
>
> That looks nice.  It's very similar to the way that
> thread-local storage works on glibc based GNU/Linux systems,
> although that can be somewhat more efficient because a
> separate page can be mapped at the same logical address in
> each thread.  In turn, this means that an access can be
> simply mem ( base address + key ), where key is a small integer. [*]
>

My recollection is that at least on X86 and IA64, all threads see the same virtual to physical page mapping, but there is a register that always points to per-thread data.  On X86 this is one of the segment registers, which I guess sort of makes it a different mapping, depending on how you view the segment registers.

Hans


From crazybob at crazybob.org  Tue Nov 18 14:09:06 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Tue, 18 Nov 2008 11:09:06 -0800
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <49229A4D.201@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCKEKEHOAA.dcholmes@optusnet.com.au>
	<492140C5.2010207@optrak.co.uk>
	<b097ac510811171203j4e79adc6k26c32748e3bb88dc@mail.gmail.com>
	<a74683f90811171400kfa12e93wdaefa8feea7c3782@mail.gmail.com>
	<49229A4D.201@redhat.com>
Message-ID: <a74683f90811181109p4981d838u6608da52b12623c2@mail.gmail.com>

On Tue, Nov 18, 2008 at 2:34 AM, Andrew Haley <aph at redhat.com> wrote:

> This does
> require special handling for recycling thread locals when they're no
> longer needed, but I don't think that happens very often.


It can happen often. For example, ReadWriteLock has a ThreadLocal
internally.


>  It won't
> make much difference, though: just an extra mask and compare, and I
> suppose collisions are rare.  Perhaps it's done this way in Harmony
> because it doesn't require locking when creating / destroying
> ThreadLocals?
>

Exactly. One & operation is really nothing to stress about--it might as well
be a direct offset.


> For even greater efficiency, there's no reason why the VM couldn't use
> the glibc trick of mapping multiple pages at the same address.
> Perhaps ThreadLocals just aren't important enough to make it worth the
> bother.
>

The Thread has a direct reference to its thread local map, so we
theoretically have that...

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081118/f9c3eddb/attachment-0001.html>

From aph at redhat.com  Tue Nov 18 14:16:09 2008
From: aph at redhat.com (Andrew Haley)
Date: Tue, 18 Nov 2008 19:16:09 +0000
Subject: [concurrency-interest] ThreadLocal thread-safe?
In-Reply-To: <238A96A773B3934685A7269CC8A8D0423B0E992851@GVW0436EXB.americas.hpqcorp.net>
References: <NFBBKALFDCPFIDBNKAPCKEKEHOAA.dcholmes@optusnet.com.au>	<492140C5.2010207@optrak.co.uk>	<b097ac510811171203j4e79adc6k26c32748e3bb88dc@mail.gmail.com>	<a74683f90811171400kfa12e93wdaefa8feea7c3782@mail.gmail.com>
	<49229A4D.201@redhat.com>
	<238A96A773B3934685A7269CC8A8D0423B0E992851@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <49231479.2020506@redhat.com>

Boehm, Hans wrote:
> [Somewhat off-topic:]
> 

>> That looks nice.  It's very similar to the way that
>> thread-local storage works on glibc based GNU/Linux systems,
>> although that can be somewhat more efficient because a
>> separate page can be mapped at the same logical address in
>> each thread.  In turn, this means that an access can be
>> simply mem ( base address + key ), where key is a small integer. [*]
>>
> 
> My recollection is that at least on X86 and IA64, all threads see the same virtual to physical page mapping, but there is a register that always points to per-thread data.  On X86 this is one of the segment registers, which I guess sort of makes it a different mapping, depending on how you view the segment registers.

Yes, you may well be right.  Faulty memory.  I don't think it makes
any great difference to the cycle count, though: it's effectively the
same algorithm.

Andrew.

From khalil.bouhamza at gmail.com  Tue Nov 18 18:20:15 2008
From: khalil.bouhamza at gmail.com (Bouhamza Khalil)
Date: Wed, 19 Nov 2008 00:20:15 +0100
Subject: [concurrency-interest] Concurrent BitSets/BitField
In-Reply-To: <771905290811180135g33075dfdl97af0362bfae75ce@mail.gmail.com>
References: <771905290811180135g33075dfdl97af0362bfae75ce@mail.gmail.com>
Message-ID: <19507f870811181520jdb13aedya7a0bf0bcd2870e5@mail.gmail.com>

You may want to take a look at High scale lib developped by Cliff Click
http://sourceforge.net/projects/high-scale-lib/ specifcly at
NonBlockingSetInt, though the exposed interface is a Set<Integer>, under the
hood it is a bit vector with CAS operation for update, memory profile could
be what you are looking for with regard to space from the javadoc:

"Space: space is used in proportion to the largest element, as opposed to
the number of elements (as is the case with hash-table based Set
implementations). Space is approximately (largest_element/8 + 64) bytes. The
implementation is a simple bit-vector using CAS for update."

Cheers,
Khalil



On Tue, Nov 18, 2008 at 10:35 AM, Markus Kohler <markus.kohler at gmail.com>wrote:

> Hi all,
> The key algorithm in the Eclipse Memory Analyzer (
> http://www.eclipse.org/mat/), has the need to simulate the mark phase of a
> Garbage Collector.
> For the single threaded marking algorithm a BitField( kind of a BitSet with
> a fixed size;
> http://dev.eclipse.org/viewcvs/index.cgi/trunk/plugins/org.eclipse.mat.report/src/org/eclipse/mat/collect/BitField.java?root=Technology_MAT&view=markup)
> is used to track which nodes have already been marked.
>
> For the multithreaded case (marking in parallel) a simple boolean array is
> used with no(!) synchronization nor CAS.
> Check
> http://dev.eclipse.org/viewcvs/index.cgi/trunk/plugins/org.eclipse.mat.parser/src/org/eclipse/mat/parser/internal/snapshot/ObjectMarker.java?revision=68&root=Technology_MAT&view=markup
> and look for the comment "no synchronization"
>
> The rationale for not using synchronization is that it turned out to be
> much slower than just synchronizing a BitField and since the boolean's are
> only changed from false to true, marking some of them more than once doesn't
> seem to hurt performance.
> They main drawback is that the boolean[] consumes 8 times more memory,
> which is an issue because heap dumps can contain 10's of millions of
> objects.
>
> First question:
> Does this approach just work by accident (on the CPU/OS that were tested),
> or is it in?
>
> I was thinking about implementing a simple ConcurrentBitField using CAS.
>
> This is my first attempt to implement a concurrent data structure using
> CAS, so please forgive me, if it's completely broken:
>
>
> /*******************************************************************************
>  * Copyright (c) 2008 Markus Kohler.
>  * All rights reserved. This program and the accompanying materials
>  * are made available under the terms of the Eclipse Public License v1.0
>  * which accompanies this distribution, and is available at
>  * http://www.eclipse.org/legal/epl-v10.html
>  *
>  * Contributors:
>  *    Markus Kohler - initial API and implementation
>
>  *******************************************************************************/
> package org.eclipse.mat.collect;
>
> import java.io.Serializable;
> import java.util.concurrent.atomic.AtomicIntegerArray;
>
> /**
>  * This class manages huge bit fields. It is much faster than {@link
>  * java.util.BitSet} and was specifically developed to be used with huge
> bit
>  * sets in ISnapshot (e.g. needed in virtual GC traces). Out of performance
>  * reasons no method does any parameter checking, i.e. only valid values
> are
>  * expected.
>  */
> public final class ConcurrentBitField implements Serializable
> {
>     private static final long serialVersionUID = 1L;
>
>     private AtomicIntegerArray bits;
>
>     /**
>      * Creates a bit field with the given number of bits. Size is expected
> to be
>      * positive - out of performance reasons no checks are done!
>      */
>     public ConcurrentBitField(int size)
>     {
>         bits = new AtomicIntegerArray((((size) - 1) >>> 0x5) + 1);
>     }
>
>     /**
>      * Sets the bit on the given index. Index is expected to be in range -
> out
>      * of performance reasons no checks are done!
>      */
>     public final void set(int index)
>     {
>      int i = index >>> 0x5;
>      while (true) {
>             int current = bits.get(i);
>             int newValue = current | (1 << (index & 0x1f));
>             if (bits.compareAndSet(i, current, newValue))
>                 break;
>         }
>     }
>
>     /**
>      * Clears the bit on the given index. Index is expected to be in range
> - out
>      * of performance reasons no checks are done!
>      */
>     public final void clear(int index)
>     {
>
>      int i = index >>> 0x5;
>      while (true) {
>             int current = bits.get(i);
>             int newValue = current & ~(1 << (index & 0x1f));
>             if (bits.compareAndSet(i, current, newValue))
>                 break;
>         }
>
>     }
>
>     /**
>      * Gets the bit on the given index. Index is expected to be in range -
> out
>      * of performance reasons no checks are done!
>      */
>     public final boolean get(int index)
>     {
>         return (bits.get(index >>> 0x5) & (1 << (index & 0x1f))) != 0;
>     }
> }
>
> Any comments whether this makes any sense at all are welcome!
>
>
>
>
>
>
> I would be interested to hear whether anybody here has experience with
> concurrent implementations of BitSets(/Fields).
>
>  I tried to do some microbenchmarks to figure out whether the boolean[]
> approach degrades with more threads and it seems that if I put more than 2
> threads (on a dual core Intel) it slows down pretty much. That does not mean
> much probably because you would not use more than 2 threads on a dual core
> machine. Unfortunately I did not have a  768 core Azul machine available at
> my fingertips ;)
>
> For a high number of threads actually the ConcurrentBitField seems to  be
> faster than the boolean[].
>
>
> Regards,
> Markus
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081119/c7d1fead/attachment.html>

From alarmnummer at gmail.com  Thu Nov 20 12:04:06 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 20 Nov 2008 18:04:06 +0100
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
Message-ID: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>

Hi Guys,

I'm playing with the JMM and in some case you need to play on the
edge, or even fall over, to get a better understanding. The question
I'm going to ask is such a question, and not meant to use in a
production environment. But just to check my understanding.

You can make a read barrier by reading a volatile variable, a write
barrier by writing to a volatile variable, and a full barrier by read
and writing the volatile variable.

So:

class Barrier{
     volatile int x=0;

     void readBarrier(){
          int rubish = x;
     }

     void writeBarrier(){
         x = 0;
     }

      void fullBarrier(){
         x=x;
      }
}

And you could use it like this:

class Foobar{
    final Barrier barrier = new Barrier();
    int a;

   void foo(){
        a=1;
        barrier.writeBarrier();
   }

   void bar(){
       barrier.readBarrier();
      println(a);
  }
}

Where foo and bar are methods called by different threads. Since there
is a happens before relation between the write of a, and the read of a
(program order rule -> volatile variable rule -> program order rule)
this example doesn't suffer from reorderings or visibility problems.

My big question is: is the JVM allowed to remove the barrier
expressions? They have no meaning except to add the missing happens
before edges.

From jeremy.manson at gmail.com  Thu Nov 20 12:50:39 2008
From: jeremy.manson at gmail.com (Jeremy Manson)
Date: Thu, 20 Nov 2008 09:50:39 -0800
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
Message-ID: <1631da7d0811200950u6f84649bvff811f3a98412b28@mail.gmail.com>

Peter,

If you run foo() in one thread, and bar() in another, and the read
barrier sees the 0 written by the write barrier, then the
happens-before relationship must be maintained, and the VM has to
insert the proper barriers.

The VM can execute this without barriers if it determines that they
are called by the same thread, of course.

"Seeing the 0" is kind of an odd thing to say, of course.  You can
think of each 0 written to the x as being a different 0 from all of
the other 0s written to the x.

Jeremy

On Thu, Nov 20, 2008 at 9:04 AM, Peter Veentjer <alarmnummer at gmail.com> wrote:
> Hi Guys,
>
> I'm playing with the JMM and in some case you need to play on the
> edge, or even fall over, to get a better understanding. The question
> I'm going to ask is such a question, and not meant to use in a
> production environment. But just to check my understanding.
>
> You can make a read barrier by reading a volatile variable, a write
> barrier by writing to a volatile variable, and a full barrier by read
> and writing the volatile variable.
>
> So:
>
> class Barrier{
>     volatile int x=0;
>
>     void readBarrier(){
>          int rubish = x;
>     }
>
>     void writeBarrier(){
>         x = 0;
>     }
>
>      void fullBarrier(){
>         x=x;
>      }
> }
>
> And you could use it like this:
>
> class Foobar{
>    final Barrier barrier = new Barrier();
>    int a;
>
>   void foo(){
>        a=1;
>        barrier.writeBarrier();
>   }
>
>   void bar(){
>       barrier.readBarrier();
>      println(a);
>  }
> }
>
> Where foo and bar are methods called by different threads. Since there
> is a happens before relation between the write of a, and the read of a
> (program order rule -> volatile variable rule -> program order rule)
> this example doesn't suffer from reorderings or visibility problems.
>
> My big question is: is the JVM allowed to remove the barrier
> expressions? They have no meaning except to add the missing happens
> before edges.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From alarmnummer at gmail.com  Thu Nov 20 13:30:19 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 20 Nov 2008 19:30:19 +0100
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <1631da7d0811200950u6f84649bvff811f3a98412b28@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<1631da7d0811200950u6f84649bvff811f3a98412b28@mail.gmail.com>
Message-ID: <1466c1d60811201030n6f317e18k688da38174390f43@mail.gmail.com>

Hi Jeremy,

so I get the guarantee that:
Code that has no intra thread semantics, but has inter thread
semantics,  can't be removed by compiler optimisations.
Only when the jvm is sure that there are no inter thread semantics,
code can be removed (e.g. a read/read volatile variable.. or
read/write of a volatile always done by a single thread).

I couldn't imagine that the JVM would allow these optimisations, but
it is nice to see it being confirmed.

On Thu, Nov 20, 2008 at 6:50 PM, Jeremy Manson <jeremy.manson at gmail.com> wrote:
> Peter,
>
> If you run foo() in one thread, and bar() in another, and the read
> barrier sees the 0 written by the write barrier, then the
> happens-before relationship must be maintained, and the VM has to
> insert the proper barriers.
>
> The VM can execute this without barriers if it determines that they
> are called by the same thread, of course.
>
> "Seeing the 0" is kind of an odd thing to say, of course.  You can
> think of each 0 written to the x as being a different 0 from all of
> the other 0s written to the x.
>
> Jeremy
>
> On Thu, Nov 20, 2008 at 9:04 AM, Peter Veentjer <alarmnummer at gmail.com> wrote:
>> Hi Guys,
>>
>> I'm playing with the JMM and in some case you need to play on the
>> edge, or even fall over, to get a better understanding. The question
>> I'm going to ask is such a question, and not meant to use in a
>> production environment. But just to check my understanding.
>>
>> You can make a read barrier by reading a volatile variable, a write
>> barrier by writing to a volatile variable, and a full barrier by read
>> and writing the volatile variable.
>>
>> So:
>>
>> class Barrier{
>>     volatile int x=0;
>>
>>     void readBarrier(){
>>          int rubish = x;
>>     }
>>
>>     void writeBarrier(){
>>         x = 0;
>>     }
>>
>>      void fullBarrier(){
>>         x=x;
>>      }
>> }
>>
>> And you could use it like this:
>>
>> class Foobar{
>>    final Barrier barrier = new Barrier();
>>    int a;
>>
>>   void foo(){
>>        a=1;
>>        barrier.writeBarrier();
>>   }
>>
>>   void bar(){
>>       barrier.readBarrier();
>>      println(a);
>>  }
>> }
>>
>> Where foo and bar are methods called by different threads. Since there
>> is a happens before relation between the write of a, and the read of a
>> (program order rule -> volatile variable rule -> program order rule)
>> this example doesn't suffer from reorderings or visibility problems.
>>
>> My big question is: is the JVM allowed to remove the barrier
>> expressions? They have no meaning except to add the missing happens
>> before edges.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

From jeremy.manson at gmail.com  Thu Nov 20 13:56:42 2008
From: jeremy.manson at gmail.com (Jeremy Manson)
Date: Thu, 20 Nov 2008 10:56:42 -0800
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <1466c1d60811201030n6f317e18k688da38174390f43@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<1631da7d0811200950u6f84649bvff811f3a98412b28@mail.gmail.com>
	<1466c1d60811201030n6f317e18k688da38174390f43@mail.gmail.com>
Message-ID: <1631da7d0811201056p51205b94r77f88d6cd5e8dfe@mail.gmail.com>

Well (and at the risk of muddying the water while stating the
obvious), "inter-thread semantics" is slightly different from
"establishing a happens-before relationship".  For example, with
non-volatile, non-final field int f, initialized to 0:

Thread 1:
f = 1;
f = 2;

Thread 2:
r1 = f;

Both of those writes have "inter-thread semantics", in the sense that
the read of f can return 1 or 2 or 0.  However, the VM is free to
eliminate the write of 1 to f, because it is perfectly legal for that
read never to return 1.

Jeremy

On Thu, Nov 20, 2008 at 10:30 AM, Peter Veentjer <alarmnummer at gmail.com> wrote:
> Hi Jeremy,
>
> so I get the guarantee that:
> Code that has no intra thread semantics, but has inter thread
> semantics,  can't be removed by compiler optimisations.
> Only when the jvm is sure that there are no inter thread semantics,
> code can be removed (e.g. a read/read volatile variable.. or
> read/write of a volatile always done by a single thread).
>
> I couldn't imagine that the JVM would allow these optimisations, but
> it is nice to see it being confirmed.
>
> On Thu, Nov 20, 2008 at 6:50 PM, Jeremy Manson <jeremy.manson at gmail.com> wrote:
>> Peter,
>>
>> If you run foo() in one thread, and bar() in another, and the read
>> barrier sees the 0 written by the write barrier, then the
>> happens-before relationship must be maintained, and the VM has to
>> insert the proper barriers.
>>
>> The VM can execute this without barriers if it determines that they
>> are called by the same thread, of course.
>>
>> "Seeing the 0" is kind of an odd thing to say, of course.  You can
>> think of each 0 written to the x as being a different 0 from all of
>> the other 0s written to the x.
>>
>> Jeremy
>>
>> On Thu, Nov 20, 2008 at 9:04 AM, Peter Veentjer <alarmnummer at gmail.com> wrote:
>>> Hi Guys,
>>>
>>> I'm playing with the JMM and in some case you need to play on the
>>> edge, or even fall over, to get a better understanding. The question
>>> I'm going to ask is such a question, and not meant to use in a
>>> production environment. But just to check my understanding.
>>>
>>> You can make a read barrier by reading a volatile variable, a write
>>> barrier by writing to a volatile variable, and a full barrier by read
>>> and writing the volatile variable.
>>>
>>> So:
>>>
>>> class Barrier{
>>>     volatile int x=0;
>>>
>>>     void readBarrier(){
>>>          int rubish = x;
>>>     }
>>>
>>>     void writeBarrier(){
>>>         x = 0;
>>>     }
>>>
>>>      void fullBarrier(){
>>>         x=x;
>>>      }
>>> }
>>>
>>> And you could use it like this:
>>>
>>> class Foobar{
>>>    final Barrier barrier = new Barrier();
>>>    int a;
>>>
>>>   void foo(){
>>>        a=1;
>>>        barrier.writeBarrier();
>>>   }
>>>
>>>   void bar(){
>>>       barrier.readBarrier();
>>>      println(a);
>>>  }
>>> }
>>>
>>> Where foo and bar are methods called by different threads. Since there
>>> is a happens before relation between the write of a, and the read of a
>>> (program order rule -> volatile variable rule -> program order rule)
>>> this example doesn't suffer from reorderings or visibility problems.
>>>
>>> My big question is: is the JVM allowed to remove the barrier
>>> expressions? They have no meaning except to add the missing happens
>>> before edges.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>

From J.Sevcik at sms.ed.ac.uk  Thu Nov 20 13:58:51 2008
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Thu, 20 Nov 2008 18:58:51 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
Message-ID: <4925B36B.6050009@sms.ed.ac.uk>

Hi,

you are on very thin ice here. It depends what you mean by 'visibility 
problem'(?). It is hard to say from your example what you want to 
achieve, because your program can print either 0 or 1 regardless of 
whether the barriers work or not. How do you want to use your memory 
barriers?

In practice, I do not think that JVMs remove volatile variable accesses 
except the case when the variable does not escape a thread. For example, 
the program

class Foobar {
    final Barrier barrier1 = new Barrier();
    final Barrier barrier2 = new Barrier();

    int x = 0, y = 0;

   void foo(){
        x=1;
        barrier1.fullBarrier();
        y=1;
   }

   void bar(){
       int r1 = y;
       barrier2.fullBarrier();
       int r2 = x;
       println r1
       println r2
  }
}

can print 1 and then 0, because the JVM can remove the barriers and reorder. In terms of the JMM, there is no happens-before ordering.


Also note that the memory barriers are not always as strong as you might 
expect. For example, the program

class Foobar{
    final Barrier barrier = new Barrier();
    int x = 0, y = 0;

   void foo(){
        x=1;
        barrier.writeBarrier();
        barrier.readBarrier();
        y=1;
   }

   void bar(){
       int r1 = y;
       barrier.writeBarrier();
       barrier.readBarrier();
       int r2 = x;
       println r1
       println r2
  }
}

can somewhat surprisingly print 1 and then 0 in the JMM. However, if you replace the read/write barriers with your full barriers, it becomes impossible. 

Also, let me warn you that JVMs often do not implement the JMM properly (even Hotspot does not). I think you should stay away from the edge and write correctly synchronized programs unless you have extremely good reasons not to do so.

Jaroslav

Peter Veentjer wrote:
> Hi Guys,
>
> I'm playing with the JMM and in some case you need to play on the
> edge, or even fall over, to get a better understanding. The question
> I'm going to ask is such a question, and not meant to use in a
> production environment. But just to check my understanding.
>
> You can make a read barrier by reading a volatile variable, a write
> barrier by writing to a volatile variable, and a full barrier by read
> and writing the volatile variable.
>
> So:
>
> class Barrier{
>      volatile int x=0;
>
>      void readBarrier(){
>           int rubish = x;
>      }
>
>      void writeBarrier(){
>          x = 0;
>      }
>
>       void fullBarrier(){
>          x=x;
>       }
> }
>
> And you could use it like this:
>
> class Foobar{
>     final Barrier barrier = new Barrier();
>     int a;
>
>    void foo(){
>         a=1;
>         barrier.writeBarrier();
>    }
>
>    void bar(){
>        barrier.readBarrier();
>       println(a);
>   }
> }
>
> Where foo and bar are methods called by different threads. Since there
> is a happens before relation between the write of a, and the read of a
> (program order rule -> volatile variable rule -> program order rule)
> this example doesn't suffer from reorderings or visibility problems.
>
> My big question is: is the JVM allowed to remove the barrier
> expressions? They have no meaning except to add the missing happens
> before edges.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From alarmnummer at gmail.com  Thu Nov 20 14:26:48 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 20 Nov 2008 20:26:48 +0100
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <4925B36B.6050009@sms.ed.ac.uk>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
Message-ID: <1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>

Hi Jaroslav

On Thu, Nov 20, 2008 at 7:58 PM, Jaroslav Sevcik <J.Sevcik at sms.ed.ac.uk> wrote:
> Hi,
>
> you are on very thin ice here. It depends what you mean by 'visibility
> problem'(?). It is hard to say from your example what you want to achieve,
> because your program can print either 0 or 1 regardless of whether the
> barriers work or not. How do you want to use your memory barriers?

The example is a little bit lame, but I could make it more interesting.

class FooBar{
   List list;
   final Barrier barrier = new Barrier();

    void foo(){
         list = new ArrayList();
         barrier.writeBarrier();
    }

    void bar(){
       barrer.readBarrier();
       println(list);
   }
}

Without the barrier it could be that:
- you see null
- you see []
- you get an exception while printing the list because perhaps parts
are not visible or that instructions are reordened and you see a
partially constructed list.

With the barrier you don't need to worry about the last one in this
case; you will see null or a valid structure. Nothing in between.

> In practice, I do not think that JVMs remove volatile variable accesses
> except the case when the variable does not escape a thread. For example, the
> program
>
> class Foobar {
>   final Barrier barrier1 = new Barrier();
>   final Barrier barrier2 = new Barrier();
>
>   int x = 0, y = 0;
>
>  void foo(){
>       x=1;
>       barrier1.fullBarrier();
>       y=1;
>  }
>
>  void bar(){
>      int r1 = y;
>      barrier2.fullBarrier();
>      int r2 = x;
>      println r1
>      println r2
>  }
> }
>
> can print 1 and then 0, because the JVM can remove the barriers and reorder.
> In terms of the JMM, there is no happens-before ordering.

That is correct. The JMM clearly states that a happens before relation
only exist between a write and read of the same volatile (or a
unlock/lock of the same monitor).

> Also note that the memory barriers are not always as strong as you might
> expect. For example, the program
>
> class Foobar{
>   final Barrier barrier = new Barrier();
>   int x = 0, y = 0;
>
>  void foo(){
>       x=1;
>       barrier.writeBarrier();
>       barrier.readBarrier();
>       y=1;
>  }
>
>  void bar(){
>      int r1 = y;
>      barrier.writeBarrier();
>      barrier.readBarrier();
>      int r2 = x;
>      println r1
>      println r2
>  }
> }
>
> can somewhat surprisingly print 1 and then 0 in the JMM. However, if you
> replace the read/write barriers with your full barriers, it becomes
> impossible.

That is correct. There is no happens before relation between the write
and read of y.

> Also, let me warn you that JVMs often do not implement the JMM properly
> (even Hotspot does not).

Hmmm.. not good news.. we finally have a JMM, but JVM's don't
implement it correctly.

Do you have some references?

> I think you should stay away from the edge and
> write correctly synchronized programs unless you have extremely good reasons
> not to do so.

I agree. But to understand something you have to play on the edge.
This is playing on the edge.  Not something you would do in production
code. But I think such experiments are essential to understanding a
complex technology/specification.

Thanks for your feedback.

>
> Jaroslav
>
> Peter Veentjer wrote:
>>
>> Hi Guys,
>>
>> I'm playing with the JMM and in some case you need to play on the
>> edge, or even fall over, to get a better understanding. The question
>> I'm going to ask is such a question, and not meant to use in a
>> production environment. But just to check my understanding.
>>
>> You can make a read barrier by reading a volatile variable, a write
>> barrier by writing to a volatile variable, and a full barrier by read
>> and writing the volatile variable.
>>
>> So:
>>
>> class Barrier{
>>     volatile int x=0;
>>
>>     void readBarrier(){
>>          int rubish = x;
>>     }
>>
>>     void writeBarrier(){
>>         x = 0;
>>     }
>>
>>      void fullBarrier(){
>>         x=x;
>>      }
>> }
>>
>> And you could use it like this:
>>
>> class Foobar{
>>    final Barrier barrier = new Barrier();
>>    int a;
>>
>>   void foo(){
>>        a=1;
>>        barrier.writeBarrier();
>>   }
>>
>>   void bar(){
>>       barrier.readBarrier();
>>      println(a);
>>  }
>> }
>>
>> Where foo and bar are methods called by different threads. Since there
>> is a happens before relation between the write of a, and the read of a
>> (program order rule -> volatile variable rule -> program order rule)
>> this example doesn't suffer from reorderings or visibility problems.
>>
>> My big question is: is the JVM allowed to remove the barrier
>> expressions? They have no meaning except to add the missing happens
>> before edges.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>

From J.Sevcik at sms.ed.ac.uk  Thu Nov 20 14:40:33 2008
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Thu, 20 Nov 2008 19:40:33 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>	
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
Message-ID: <4925BD31.3030304@sms.ed.ac.uk>

Peter Veentjer wrote:
> Hi Jaroslav
>
> On Thu, Nov 20, 2008 at 7:58 PM, Jaroslav Sevcik <J.Sevcik at sms.ed.ac.uk> wrote:
>   
>> Hi,
>>
>> you are on very thin ice here. It depends what you mean by 'visibility
>> problem'(?). It is hard to say from your example what you want to achieve,
>> because your program can print either 0 or 1 regardless of whether the
>> barriers work or not. How do you want to use your memory barriers?
>>     
>
> The example is a little bit lame, but I could make it more interesting.
>
> class FooBar{
>    List list;
>    final Barrier barrier = new Barrier();
>
>     void foo(){
>          list = new ArrayList();
>          barrier.writeBarrier();
>     }
>
>     void bar(){
>        barrer.readBarrier();
>        println(list);
>    }
> }
>
> Without the barrier it could be that:
> - you see null
> - you see []
> - you get an exception while printing the list because perhaps parts
> are not visible or that instructions are reordened and you see a
> partially constructed list.
>
> With the barrier you don't need to worry about the last one in this
> case; you will see null or a valid structure. Nothing in between.
>   
Actually, not really, there is no guarantee that the readBarrier will be 
after the writeBarrier in the synchronisation order, so there still 
might be no ordering there and you could still see garbage.
>> Also, let me warn you that JVMs often do not implement the JMM properly
>> (even Hotspot does not).
>>     
>
> Hmmm.. not good news.. we finally have a JMM, but JVM's don't
> implement it correctly.
>
> Do you have some references?
>   
Yes, see my ECOOP paper: http://homepages.inf.ed.ac.uk/s0566973/jmmtrans.pdf

To add to the confusion you should know that it looks like it is not 
possible to implement volatile variables on x86 (by the current x86 
specification) and it also looks like x86 does not really implement its 
specification, i.e., you can see behaviours that are not allowed by the 
spec. For more info, see Peter Sewell's extremely cool stuff at 
http://www.cl.cam.ac.uk/~pes20/weakmemory/, especially the addendum of 
the x86 paper.

Jaroslav
>   
>> I think you should stay away from the edge and
>> write correctly synchronized programs unless you have extremely good reasons
>> not to do so.
>>     
>
> I agree. But to understand something you have to play on the edge.
> This is playing on the edge.  Not something you would do in production
> code. But I think such experiments are essential to understanding a
> complex technology/specification.
>
> Thanks for your feedback.
>   



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From hans.boehm at hp.com  Thu Nov 20 14:46:23 2008
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 20 Nov 2008 19:46:23 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <4925B36B.6050009@sms.ed.ac.uk>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
Message-ID: <238A96A773B3934685A7269CC8A8D0423CD9778C43@GVW0436EXB.americas.hpqcorp.net>

> From: Jaroslav Sevcik
> Sent: Thursday, November 20, 2008 10:59 AM
> To: Peter Veentjer x
> Cc: concurrency-interest x
> Subject: Re: [concurrency-interest] Playing on the edge: JMM.
> barriers and optimisations.
>
> Hi,
>
> you are on very thin ice here. It depends what you mean by
> 'visibility problem'(?). It is hard to say from your example
> what you want to achieve, because your program can print
> either 0 or 1 regardless of whether the barriers work or not.
> How do you want to use your memory barriers?
>
> In practice, I do not think that JVMs remove volatile
> variable accesses except the case when the variable does not
> escape a thread. For example, the program
>
> class Foobar {
>     final Barrier barrier1 = new Barrier();
>     final Barrier barrier2 = new Barrier();
>
>     int x = 0, y = 0;
>
>    void foo(){
>         x=1;
>         barrier1.fullBarrier();
>         y=1;
>    }
>
>    void bar(){
>        int r1 = y;
>        barrier2.fullBarrier();
>        int r2 = x;
>        println r1
>        println r2
>   }
> }
>
> can print 1 and then 0, because the JVM can remove the
> barriers and reorder. In terms of the JMM, there is no
> happens-before ordering.
>
>
> Also note that the memory barriers are not always as strong
> as you might expect. For example, the program
>
> class Foobar{
>     final Barrier barrier = new Barrier();
>     int x = 0, y = 0;
>
>    void foo(){
>         x=1;
>         barrier.writeBarrier();
>         barrier.readBarrier();
>         y=1;
>    }
>
>    void bar(){
>        int r1 = y;
>        barrier.writeBarrier();
>        barrier.readBarrier();
>        int r2 = x;
>        println r1
>        println r2
>   }
> }
>
> can somewhat surprisingly print 1 and then 0 in the JMM.
> However, if you replace the read/write barriers with your
> full barriers, it becomes impossible.
Are you sure?  I would have expected the opposite.

As you wrote the example, the "barriers" (I much prefer "fence", "barrier" means different things to different people) are totally ordered by the synchronization order.  This means one of the "readBarrier"s sees a write from the "writeBarrier" in the other thread, or a later one.  Hence one "writeBarrier" happens before the "readBarrier" in the other thread.  This means either x = 1 happens beore r2 = x or r1 = y happens before y = 1.
This outcome is impossible in either case.

If you reorder the read and write "Barriers", both "readBarriers" can see the initial value of the volatile, and there is no ordering.  This is equivalent to the "fullBarrier" case.

As part of the C++ effort, we went through an exercise to try to define "fences" in terms of something similar to Java volatiles.  The botton line is "don't go there".  For some more details on what goes wrong, see Peter Dimov's paper: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2633.html .  (I think this version still had a bug or two, and the later version dropped the rationale, but the basic observations were correct.)

Hans

>
> Also, let me warn you that JVMs often do not implement the
> JMM properly (even Hotspot does not). I think you should stay
> away from the edge and write correctly synchronized programs
> unless you have extremely good reasons not to do so.
Good advice ...

Hans

>
> Jaroslav
>
> Peter Veentjer wrote:
> > Hi Guys,
> >
> > I'm playing with the JMM and in some case you need to play on the
> > edge, or even fall over, to get a better understanding. The
> question
> > I'm going to ask is such a question, and not meant to use in a
> > production environment. But just to check my understanding.
> >
> > You can make a read barrier by reading a volatile variable, a write
> > barrier by writing to a volatile variable, and a full
> barrier by read
> > and writing the volatile variable.
> >
> > So:
> >
> > class Barrier{
> >      volatile int x=0;
> >
> >      void readBarrier(){
> >           int rubish = x;
> >      }
> >
> >      void writeBarrier(){
> >          x = 0;
> >      }
> >
> >       void fullBarrier(){
> >          x=x;
> >       }
> > }
> >
> > And you could use it like this:
> >
> > class Foobar{
> >     final Barrier barrier = new Barrier();
> >     int a;
> >
> >    void foo(){
> >         a=1;
> >         barrier.writeBarrier();
> >    }
> >
> >    void bar(){
> >        barrier.readBarrier();
> >       println(a);
> >   }
> > }
> >
> > Where foo and bar are methods called by different threads.
> Since there
> > is a happens before relation between the write of a, and
> the read of a
> > (program order rule -> volatile variable rule -> program
> order rule)
> > this example doesn't suffer from reorderings or visibility problems.
> >
> > My big question is: is the JVM allowed to remove the barrier
> > expressions? They have no meaning except to add the missing happens
> > before edges.
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>
> --
> The University of Edinburgh is a charitable body, registered
> in Scotland, with registration number SC005336.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From hans.boehm at hp.com  Thu Nov 20 14:49:59 2008
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 20 Nov 2008 19:49:59 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers
	and	optimisations.
In-Reply-To: <1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D0423CD9778C4D@GVW0436EXB.americas.hpqcorp.net>



> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> Of Peter Veentjer
> Sent: Thursday, November 20, 2008 11:27 AM
> To: Jaroslav Sevcik
> Cc: concurrency-interest x
> Subject: Re: [concurrency-interest] Playing on the edge: JMM.
> barriers and optimisations.
>
> Hi Jaroslav
>
> On Thu, Nov 20, 2008 at 7:58 PM, Jaroslav Sevcik
> <J.Sevcik at sms.ed.ac.uk> wrote:
> > Hi,
> >
> > you are on very thin ice here. It depends what you mean by
> 'visibility
> > problem'(?). It is hard to say from your example what you want to
> > achieve, because your program can print either 0 or 1 regardless of
> > whether the barriers work or not. How do you want to use
> your memory barriers?
>
> The example is a little bit lame, but I could make it more
> interesting.
>
> class FooBar{
>    List list;
>    final Barrier barrier = new Barrier();
>
>     void foo(){
>          list = new ArrayList();
>          barrier.writeBarrier();
>     }
>
>     void bar(){
>        barrer.readBarrier();
>        println(list);
>    }
> }
>
> Without the barrier it could be that:
> - you see null
> - you see []
> - you get an exception while printing the list because
> perhaps parts are not visible or that instructions are
> reordened and you see a partially constructed list.
>
> With the barrier you don't need to worry about the last one
> in this case; you will see null or a valid structure. Nothing
> in between.
>
How so?  This can be executed as

1) perform "readBarrier".
2) Run the two statements referencing "list" in parallel.
3) perform "writeBarrier".

The "Barrier"s buy you nothing.

Hans


From alarmnummer at gmail.com  Thu Nov 20 14:52:21 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 20 Nov 2008 20:52:21 +0100
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <4925BD31.3030304@sms.ed.ac.uk>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<4925BD31.3030304@sms.ed.ac.uk>
Message-ID: <1466c1d60811201152x34d8c8cdt2bfbcab29faa6b62@mail.gmail.com>

>
> Actually, not really, there is no guarantee that the readBarrier will be
> after the writeBarrier in the synchronisation order, so there still might be
> no ordering there and you could still see garbage.

Hmm.. I agree. The writeBarrier only prevents instructions that stand
in front of it,
to jump after it. It doesn't say anything about reordering instructions that
stay on front of it. So it still could be that a partially constructed (or
partially visible) object is read...

This would not be possible if the list variable was volatile (and the
barrier was dropped).

Thanks for pointing it out.

>>>
>>> Also, let me warn you that JVMs often do not implement the JMM properly
>>> (even Hotspot does not).
>>>
>>
>> Hmmm.. not good news.. we finally have a JMM, but JVM's don't
>> implement it correctly.
>>
>> Do you have some references?
>>
>
> Yes, see my ECOOP paper: http://homepages.inf.ed.ac.uk/s0566973/jmmtrans.pdf

Stuff to read :) Thanks!

> To add to the confusion you should know that it looks like it is not
> possible to implement volatile variables on x86 (by the current x86
> specification) and it also looks like x86 does not really implement its
> specification, i.e., you can see behaviours that are not allowed by the
> spec. For more info, see Peter Sewell's extremely cool stuff at
> http://www.cl.cam.ac.uk/~pes20/weakmemory/, especially the addendum of the
> x86 paper.

Thanks!!

>
> Jaroslav
>>
>>
>>>
>>> I think you should stay away from the edge and
>>> write correctly synchronized programs unless you have extremely good
>>> reasons
>>> not to do so.
>>>
>>
>> I agree. But to understand something you have to play on the edge.
>> This is playing on the edge.  Not something you would do in production
>> code. But I think such experiments are essential to understanding a
>> complex technology/specification.
>>
>> Thanks for your feedback.
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>

From alarmnummer at gmail.com  Thu Nov 20 14:53:22 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 20 Nov 2008 20:53:22 +0100
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <238A96A773B3934685A7269CC8A8D0423CD9778C4D@GVW0436EXB.americas.hpqcorp.net>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D0423CD9778C4D@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <1466c1d60811201153x3a1f8a04s43713667083710d5@mail.gmail.com>

I saw the problem as well. I was too quick with my example.

On Thu, Nov 20, 2008 at 8:49 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
>
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
>> Of Peter Veentjer
>> Sent: Thursday, November 20, 2008 11:27 AM
>> To: Jaroslav Sevcik
>> Cc: concurrency-interest x
>> Subject: Re: [concurrency-interest] Playing on the edge: JMM.
>> barriers and optimisations.
>>
>> Hi Jaroslav
>>
>> On Thu, Nov 20, 2008 at 7:58 PM, Jaroslav Sevcik
>> <J.Sevcik at sms.ed.ac.uk> wrote:
>> > Hi,
>> >
>> > you are on very thin ice here. It depends what you mean by
>> 'visibility
>> > problem'(?). It is hard to say from your example what you want to
>> > achieve, because your program can print either 0 or 1 regardless of
>> > whether the barriers work or not. How do you want to use
>> your memory barriers?
>>
>> The example is a little bit lame, but I could make it more
>> interesting.
>>
>> class FooBar{
>>    List list;
>>    final Barrier barrier = new Barrier();
>>
>>     void foo(){
>>          list = new ArrayList();
>>          barrier.writeBarrier();
>>     }
>>
>>     void bar(){
>>        barrer.readBarrier();
>>        println(list);
>>    }
>> }
>>
>> Without the barrier it could be that:
>> - you see null
>> - you see []
>> - you get an exception while printing the list because
>> perhaps parts are not visible or that instructions are
>> reordened and you see a partially constructed list.
>>
>> With the barrier you don't need to worry about the last one
>> in this case; you will see null or a valid structure. Nothing
>> in between.
>>
> How so?  This can be executed as
>
> 1) perform "readBarrier".
> 2) Run the two statements referencing "list" in parallel.
> 3) perform "writeBarrier".
>
> The "Barrier"s buy you nothing.
>
> Hans
>

From J.Sevcik at sms.ed.ac.uk  Thu Nov 20 15:02:27 2008
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Thu, 20 Nov 2008 20:02:27 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <238A96A773B3934685A7269CC8A8D0423CD9778C43@GVW0436EXB.americas.hpqcorp.net>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<238A96A773B3934685A7269CC8A8D0423CD9778C43@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4925C253.8090309@sms.ed.ac.uk>

Boehm, Hans wrote:
>> From: Jaroslav Sevcik
>> Sent: Thursday, November 20, 2008 10:59 AM
>> To: Peter Veentjer x
>> Cc: concurrency-interest x
>> Subject: Re: [concurrency-interest] Playing on the edge: JMM.
>> barriers and optimisations.
>>
>> class Foobar{
>>     final Barrier barrier = new Barrier();
>>     int x = 0, y = 0;
>>
>>    void foo(){
>>         x=1;
>>         barrier.writeBarrier();
>>         barrier.readBarrier();
>>         y=1;
>>    }
>>
>>    void bar(){
>>        int r1 = y;
>>        barrier.writeBarrier();
>>        barrier.readBarrier();
>>        int r2 = x;
>>        println r1
>>        println r2
>>   }
>> }
>>
>> can somewhat surprisingly print 1 and then 0 in the JMM.
>> However, if you replace the read/write barriers with your
>> full barriers, it becomes impossible.
>>     
> Are you sure?  I would have expected the opposite.
>
> As you wrote the example, the "barriers" (I much prefer "fence", "barrier" means different things to different people) are totally ordered by the synchronization order.  This means one of the "readBarrier"s sees a write from the "writeBarrier" in the other thread, or a later one.  Hence one "writeBarrier" happens before the "readBarrier" in the other thread.  This means either x = 1 happens beore r2 = x or r1 = y happens before y = 1.
> This outcome is impossible in either case.
>
> If you reorder the read and write "Barriers", both "readBarriers" can see the initial value of the volatile, and there is no ordering.  This is equivalent to the "fullBarrier" case.
>   
Yes, you are right, I am sorry for the confusion. I somehow swapped 
reads and writes in my head, I will double-check next time :-)
> As part of the C++ effort, we went through an exercise to try to define "fences" in terms of something similar to Java volatiles.  The botton line is "don't go there".  For some more details on what goes wrong, see Peter Dimov's paper: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2633.html .  (I think this version still had a bug or two, and the later version dropped the rationale, but the basic observations were correct.)
>   

Thanks for the pointer to the paper.

Jaroslav
> Hans
>
>   
>> Also, let me warn you that JVMs often do not implement the
>> JMM properly (even Hotspot does not). I think you should stay
>> away from the edge and write correctly synchronized programs
>> unless you have extremely good reasons not to do so.
>>     
> Good advice ...
>
> Han

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From alarmnummer at gmail.com  Thu Nov 20 15:09:12 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 20 Nov 2008 21:09:12 +0100
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <1466c1d60811201153x3a1f8a04s43713667083710d5@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D0423CD9778C4D@GVW0436EXB.americas.hpqcorp.net>
	<1466c1d60811201153x3a1f8a04s43713667083710d5@mail.gmail.com>
Message-ID: <1466c1d60811201209n6701f2v646f6aa6fa3ced34@mail.gmail.com>

I think the following (useless) code would work.

class FooBar{
  List list;
  final Barrier barrier = new Barrier();

   void foo(){
        List tmp = new ArrayList();
        barrier.fullBarrier();
        list = tmp
        barrier.writeBarrier();
   }

   void bar(){
      barrer.readBarrier();
      println(list);
  }
}

It isn't possible to see a partially constructed or partially visible
list because there is a guaranteed happens before relation between the
creation and the assignment to a global variable.

I state again: this is not something you want to do in production. But
it is a good way to gain a better understanding of the JMM (my goal
for this discussion).


On Thu, Nov 20, 2008 at 8:53 PM, Peter Veentjer <alarmnummer at gmail.com> wrote:
> I saw the problem as well. I was too quick with my example.
>
> On Thu, Nov 20, 2008 at 8:49 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
>>
>>
>>> -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu
>>> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
>>> Of Peter Veentjer
>>> Sent: Thursday, November 20, 2008 11:27 AM
>>> To: Jaroslav Sevcik
>>> Cc: concurrency-interest x
>>> Subject: Re: [concurrency-interest] Playing on the edge: JMM.
>>> barriers and optimisations.
>>>
>>> Hi Jaroslav
>>>
>>> On Thu, Nov 20, 2008 at 7:58 PM, Jaroslav Sevcik
>>> <J.Sevcik at sms.ed.ac.uk> wrote:
>>> > Hi,
>>> >
>>> > you are on very thin ice here. It depends what you mean by
>>> 'visibility
>>> > problem'(?). It is hard to say from your example what you want to
>>> > achieve, because your program can print either 0 or 1 regardless of
>>> > whether the barriers work or not. How do you want to use
>>> your memory barriers?
>>>
>>> The example is a little bit lame, but I could make it more
>>> interesting.
>>>
>>> class FooBar{
>>>    List list;
>>>    final Barrier barrier = new Barrier();
>>>
>>>     void foo(){
>>>          list = new ArrayList();
>>>          barrier.writeBarrier();
>>>     }
>>>
>>>     void bar(){
>>>        barrer.readBarrier();
>>>        println(list);
>>>    }
>>> }
>>>
>>> Without the barrier it could be that:
>>> - you see null
>>> - you see []
>>> - you get an exception while printing the list because
>>> perhaps parts are not visible or that instructions are
>>> reordened and you see a partially constructed list.
>>>
>>> With the barrier you don't need to worry about the last one
>>> in this case; you will see null or a valid structure. Nothing
>>> in between.
>>>
>> How so?  This can be executed as
>>
>> 1) perform "readBarrier".
>> 2) Run the two statements referencing "list" in parallel.
>> 3) perform "writeBarrier".
>>
>> The "Barrier"s buy you nothing.
>>
>> Hans
>>
>

From J.Sevcik at sms.ed.ac.uk  Thu Nov 20 15:49:54 2008
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Thu, 20 Nov 2008 20:49:54 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers
	and	optimisations.
Message-ID: <20081120204954.oljccu6qssowws8w@www.sms.ed.ac.uk>

Quoting Peter Veentjer <alarmnummer at gmail.com>:

> I think the following (useless) code would work.
>
> class FooBar{
>   List list;
>   final Barrier barrier = new Barrier();
>
>    void foo(){
>         List tmp = new ArrayList();
>         barrier.fullBarrier();
>         list = tmp
>         barrier.writeBarrier();
>    }
>
>    void bar(){
>       barrer.readBarrier();
>       println(list);
>   }
> }
> It isn't possible to see a partially constructed or partially visible
> list because there is a guaranteed happens before relation between the
> creation and the assignment to a global variable.

That is true, but you need happens-before relation between the  
construction and the read of the ArrayList's elements! There is a  
happens-before edge between the list creation and the assignment to  
the global variable even without any fence between them, because they  
are ordered by the program order.

I think that your fix is still insufficient. If the readBarrier is the first
in the synchronisation order, then there is still no ordering between
the read of 'list' and the constructor/the write to 'list', so println
could still theoretically see garbage. You would have to split the  
printing in two bits separated by a (full, I think) fence to make it  
work and fix your full fence so that it writes first, as Hans suggested.

Jaroslav



>
> I state again: this is not something you want to do in production. But
> it is a good way to gain a better understanding of the JMM (my goal
> for this discussion).
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.




From jeremy.manson at gmail.com  Thu Nov 20 15:56:16 2008
From: jeremy.manson at gmail.com (Jeremy Manson)
Date: Thu, 20 Nov 2008 12:56:16 -0800
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <4925BD31.3030304@sms.ed.ac.uk>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<4925BD31.3030304@sms.ed.ac.uk>
Message-ID: <1631da7d0811201256r724aa00dteb7b7f063fcd1251@mail.gmail.com>

To be fair, the part of volatile that is difficult to implement is not
the part we are talking about here.

Jeremy

On Thu, Nov 20, 2008 at 11:40 AM, Jaroslav Sevcik <J.Sevcik at sms.ed.ac.uk> wrote:
> Peter Veentjer wrote:
>>
>> Hi Jaroslav
>>
>> On Thu, Nov 20, 2008 at 7:58 PM, Jaroslav Sevcik <J.Sevcik at sms.ed.ac.uk>
>> wrote:
>>
>>>
>>> Hi,
>>>
>>> you are on very thin ice here. It depends what you mean by 'visibility
>>> problem'(?). It is hard to say from your example what you want to
>>> achieve,
>>> because your program can print either 0 or 1 regardless of whether the
>>> barriers work or not. How do you want to use your memory barriers?
>>>
>>
>> The example is a little bit lame, but I could make it more interesting.
>>
>> class FooBar{
>>   List list;
>>   final Barrier barrier = new Barrier();
>>
>>    void foo(){
>>         list = new ArrayList();
>>         barrier.writeBarrier();
>>    }
>>
>>    void bar(){
>>       barrer.readBarrier();
>>       println(list);
>>   }
>> }
>>
>> Without the barrier it could be that:
>> - you see null
>> - you see []
>> - you get an exception while printing the list because perhaps parts
>> are not visible or that instructions are reordened and you see a
>> partially constructed list.
>>
>> With the barrier you don't need to worry about the last one in this
>> case; you will see null or a valid structure. Nothing in between.
>>
>
> Actually, not really, there is no guarantee that the readBarrier will be
> after the writeBarrier in the synchronisation order, so there still might be
> no ordering there and you could still see garbage.
>>>
>>> Also, let me warn you that JVMs often do not implement the JMM properly
>>> (even Hotspot does not).
>>>
>>
>> Hmmm.. not good news.. we finally have a JMM, but JVM's don't
>> implement it correctly.
>>
>> Do you have some references?
>>
>
> Yes, see my ECOOP paper: http://homepages.inf.ed.ac.uk/s0566973/jmmtrans.pdf
>
> To add to the confusion you should know that it looks like it is not
> possible to implement volatile variables on x86 (by the current x86
> specification) and it also looks like x86 does not really implement its
> specification, i.e., you can see behaviours that are not allowed by the
> spec. For more info, see Peter Sewell's extremely cool stuff at
> http://www.cl.cam.ac.uk/~pes20/weakmemory/, especially the addendum of the
> x86 paper.
>
> Jaroslav
>>
>>
>>>
>>> I think you should stay away from the edge and
>>> write correctly synchronized programs unless you have extremely good
>>> reasons
>>> not to do so.
>>>
>>
>> I agree. But to understand something you have to play on the edge.
>> This is playing on the edge.  Not something you would do in production
>> code. But I think such experiments are essential to understanding a
>> complex technology/specification.
>>
>> Thanks for your feedback.
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From gregg at cytetech.com  Thu Nov 20 16:42:42 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 20 Nov 2008 15:42:42 -0600
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
Message-ID: <4925D9D2.2090705@cytetech.com>

Peter Veentjer wrote:
> Hi Jaroslav
> 
> On Thu, Nov 20, 2008 at 7:58 PM, Jaroslav Sevcik <J.Sevcik at sms.ed.ac.uk> wrote:
>> Hi,
>>
>> you are on very thin ice here. It depends what you mean by 'visibility
>> problem'(?). It is hard to say from your example what you want to achieve,
>> because your program can print either 0 or 1 regardless of whether the
>> barriers work or not. How do you want to use your memory barriers?
> 
> The example is a little bit lame, but I could make it more interesting.
> 
> class FooBar{
>    List list;
>    final Barrier barrier = new Barrier();
> 
>     void foo(){
>          list = new ArrayList();
>          barrier.writeBarrier();
>     }
> 
>     void bar(){
>        barrer.readBarrier();
>        println(list);
>    }
> }

Maybe I've got the wrong information, but I thought that the JMM said that 
"everything done before a barrier will be visible after the barrier is past", 
and not "nothing done before a barrier will be visible until the barrier is past".

Your example seems to say that barrier.readBarrier() can't be passed through 
until barrier.writeBarrier() has been passed through.  If println is executing 
in bar, between the execution of the construction of list and the call to 
barrier.writeBarrier(), in foo, is it not possible that parts of memory affected 
by the creation of list are still not visible because barrier.writeBarrier has 
not been passed, and list is not volatile?

Gregg Wonderly

From alarmnummer at gmail.com  Thu Nov 20 18:07:15 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Fri, 21 Nov 2008 00:07:15 +0100
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <4925D9D2.2090705@cytetech.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<4925D9D2.2090705@cytetech.com>
Message-ID: <1466c1d60811201507o6dc192aahc23fe79a1cc8257a@mail.gmail.com>

Hi Greg,

this example was faulty.

On Thu, Nov 20, 2008 at 10:42 PM, Gregg Wonderly <gregg at cytetech.com> wrote:
> Peter Veentjer wrote:
>>
>> Hi Jaroslav
>>
>> On Thu, Nov 20, 2008 at 7:58 PM, Jaroslav Sevcik <J.Sevcik at sms.ed.ac.uk>
>> wrote:
>>>
>>> Hi,
>>>
>>> you are on very thin ice here. It depends what you mean by 'visibility
>>> problem'(?). It is hard to say from your example what you want to
>>> achieve,
>>> because your program can print either 0 or 1 regardless of whether the
>>> barriers work or not. How do you want to use your memory barriers?
>>
>> The example is a little bit lame, but I could make it more interesting.
>>
>> class FooBar{
>>   List list;
>>   final Barrier barrier = new Barrier();
>>
>>    void foo(){
>>         list = new ArrayList();
>>         barrier.writeBarrier();
>>    }
>>
>>    void bar(){
>>       barrer.readBarrier();
>>       println(list);
>>   }
>> }
>
> Maybe I've got the wrong information, but I thought that the JMM said that
> "everything done before a barrier will be visible after the barrier is
> past", and not "nothing done before a barrier will be visible until the
> barrier is past".
>
> Your example seems to say that barrier.readBarrier() can't be passed through
> until barrier.writeBarrier() has been passed through.  If println is
> executing in bar, between the execution of the construction of list and the
> call to barrier.writeBarrier(), in foo, is it not possible that parts of
> memory affected by the creation of list are still not visible because
> barrier.writeBarrier has not been passed, and list is not volatile?
>
> Gregg Wonderly
>

From dl at cs.oswego.edu  Thu Nov 20 19:07:54 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 20 Nov 2008 19:07:54 -0500
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <4925BD31.3030304@sms.ed.ac.uk>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>		<4925B36B.6050009@sms.ed.ac.uk>	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<4925BD31.3030304@sms.ed.ac.uk>
Message-ID: <4925FBDA.5050903@cs.oswego.edu>

Jaroslav Sevcik wrote:
>>> Also, let me warn you that JVMs often do not implement the JMM properly
>>> (even Hotspot does not).
>>>  
> Yes, see my ECOOP paper: 
> http://homepages.inf.ed.ac.uk/s0566973/jmmtrans.pdf
> 

As you know, I think this is first-rate work, but I think the
hotspot example in that paper is best considered a JMM spec bug,
not a JVM bug. Formal memory model specs are hard to get right;
the JMM specs will need bugfixes to address probelms you and others
have found. Which won't be easy -- I don't think that there are
simple wording patches for some of them. But these problems
are cases where the formalisms don't match what most people
expect from the main English descriptions (which remain basically
accurate).

> For more info, see Peter Sewell's extremely cool stuff at 
> http://www.cl.cam.ac.uk/~pes20/weakmemory/, especially the addendum of 
> the x86 paper.

A similar story, but messier because of several changes in intent
over time by the x86 architectural folks. To the best of
our knowledge (where "us" includes Intel and AMD), current
placements/usages of fences and atomics on x86 do provide the
basic JMM guarantees.

(Aside: Despite these and other issues, memory-model-land is a better
place than it was a few years ago -- we now have models and specs that
are good enough to find particular faults with, as opposed to the old
days where the main task was divining the chicken entrails that passed
for specs to guess what processors and compilers would do.)

-Doug


From dl at cs.oswego.edu  Thu Nov 20 19:28:26 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 20 Nov 2008 19:28:26 -0500
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <238A96A773B3934685A7269CC8A8D0423CD9778C43@GVW0436EXB.americas.hpqcorp.net>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>	<4925B36B.6050009@sms.ed.ac.uk>
	<238A96A773B3934685A7269CC8A8D0423CD9778C43@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <492600AA.1040103@cs.oswego.edu>

Boehm, Hans wrote:
> As part of the C++ effort, we went through an exercise to try to define
> "fences" in terms of something similar to Java volatiles.  The botton line is
> "don't go there".  For some more details on what goes wrong, see Peter
> Dimov's paper:
> http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2633.html .  (I
> think this version still had a bug or two, and the later version dropped the
> rationale, but the basic observations were correct.)
> 

The main moral being that if you try to define fences in terms
of volatiles, then you need more conservative, heavier mappings
than if you just map them directly to hardware.
Which is one reason some of put together draft Java Fences API
(http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html)
a while back, that can't be considered for adoption until Java7
because it requires new JVM intrinsics support to be implemented
efficiently/effectively.

-Doug




From dl at cs.oswego.edu  Thu Nov 20 19:54:17 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 20 Nov 2008 19:54:17 -0500
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
Message-ID: <492606B9.6060504@cs.oswego.edu>

As everyone else has warned, this is not something
we encourage, but ...

Peter Veentjer wrote:
> class FooBar{
>    List list;
>    final Barrier barrier = new Barrier();
> 
>     void foo(){
>          list = new ArrayList();
>          barrier.writeBarrier();
>     }
> 
>     void bar(){
>        barrer.readBarrier();
>        println(list);
>    }
> }

You probably meant something like:
     void foo(){
          List l = new ArrayList();
          barrier.writeBarrier();
          list = l;
     }

     void bar(){
        List l = list;
        barrer.readBarrier();
        println(l);
    }

Among the problems with raw fences is that
they often need to be positioned in non-intuitive
ways. The idiom applying in this case is that
you must ensure all writes to fields of an object take
place before writing the reference to that object, and
vice-versa for reads.

>> In practice, I do not think that JVMs remove volatile variable accesses
>> except the case when the variable does not escape a thread.

Yes and no. On a processor in which volatile accesses entail
both a {load, store} and a fence, even if you suppress the
{load,store}, you must keep the fence. But there are other
processor-specific analyses that are used to remove
unnecessary fences. For example, on x86 the first mfence in:
"store a; mfence; store b; mfence" can be (and normally is)
removed by the  optimizer. This is another advantage of using
volatiles instead of explicit fences -- the compiler can
sometimes improve performance of volatiles, but cannot do
anything about explicit fences.

-Doug

From forax at univ-mlv.fr  Fri Nov 21 02:59:34 2008
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Fri, 21 Nov 2008 08:59:34 +0100
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <492600AA.1040103@cs.oswego.edu>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>	<4925B36B.6050009@sms.ed.ac.uk>	<238A96A773B3934685A7269CC8A8D0423CD9778C43@GVW0436EXB.americas.hpqcorp.net>
	<492600AA.1040103@cs.oswego.edu>
Message-ID: <49266A66.8080404@univ-mlv.fr>

Doug Lea a ?crit :
...
>
> The main moral being that if you try to define fences in terms
> of volatiles, then you need more conservative, heavier mappings
> than if you just map them directly to hardware.
> Which is one reason some of put together draft Java Fences API
> (http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html) 
>
> a while back, that can't be considered for adoption until Java7
> because it requires new JVM intrinsics support to be implemented
> efficiently/effectively.
Doug,
I've taken a look to Fences and all methods that take an array doesn't have
the correct signature.

by example, <T> void postLoadFence(T[] array, index)
should be written void post(Object[] array, int index)
or better void post(Object array, int index)

because array of primitive types are not subtype of Object[].||
>
> -Doug
R?mi

From J.Sevcik at sms.ed.ac.uk  Fri Nov 21 05:37:19 2008
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Fri, 21 Nov 2008 10:37:19 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <492606B9.6060504@cs.oswego.edu>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>	<4925B36B.6050009@sms.ed.ac.uk>	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<492606B9.6060504@cs.oswego.edu>
Message-ID: <49268F5F.1040404@sms.ed.ac.uk>

Doug Lea wrote:
> As everyone else has warned, this is not something
> we encourage, but ...
>
> Peter Veentjer wrote:
>> class FooBar{
>>    List list;
>>    final Barrier barrier = new Barrier();
>>
>>     void foo(){
>>          list = new ArrayList();
>>          barrier.writeBarrier();
>>     }
>>
>>     void bar(){
>>        barrer.readBarrier();
>>        println(list);
>>    }
>> }
>
> You probably meant something like:
>     void foo(){
>          List l = new ArrayList();
>          barrier.writeBarrier();
>          list = l;
>     }
>
>     void bar(){
>        List l = list;
>        barrer.readBarrier();
>        println(l);
>    }
Thanks, Doug. This is (almost) what I meant when I said that he should 
split the printing in two bits, except that this still *does not* seem 
to work. I believe that you need full fences to make this fly in the 
JMM. Just take the execution where the read barrier executes before the 
write barrier in the synchronisation order. In this execution, there is 
no happens-before ordering between the reads and writes and any read is 
free to see any write.

As for a rational explanation of the problematic behaviour, JVM could 
actually execute the program in the following order (foo and bar are 
executed in different threads, and I rename 'l's in foo and bar so that 
they are not confused with each other):

foo: List l1 = alloc();
foo: list = l1; //JVMs can reorder memory accesses with previous releases...
bar: List l2 = list;
bar: barrier.readBarrier();
bar: println(l2);  // Now you see unitialized arraylist!
foo: l1.<init>();
foo: barrier.writeBarrier();

So you actually need the full fences in between (where full fence =  
writeBarrier followed by readBarrier, not the opposite!). This is very 
nicely explained in the document alluded to by Hans Boehm (have a look 
at http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2633.html, 
Example 5).

    void foo(){
         List l = new ArrayList();
         barrier.writeBarrier();
         barrier.readBarrier();
         list = l;
    }

    void bar(){
       List l = list;
       barrier.writeBarrier();
       barrier.readBarrier();
       println(l);
   }

>
> Among the problems with raw fences is that
> they often need to be positioned in non-intuitive
> ways. The idiom applying in this case is that
> you must ensure all writes to fields of an object take
> place before writing the reference to that object, and
> vice-versa for reads.
>
>>> In practice, I do not think that JVMs remove volatile variable accesses
>>> except the case when the variable does not escape a thread.
>
> Yes and no. On a processor in which volatile accesses entail
> both a {load, store} and a fence, even if you suppress the
> {load,store}, you must keep the fence. But there are other
> processor-specific analyses that are used to remove
> unnecessary fences. For example, on x86 the first mfence in:
> "store a; mfence; store b; mfence" can be (and normally is)
> removed by the  optimizer. This is another advantage of using
> volatiles instead of explicit fences -- the compiler can
> sometimes improve performance of volatiles, but cannot do
> anything about explicit fences.
I was under the impression that these removals are not detectable by the 
program (even if it has data races), so you can assume that the fence is 
still there. Can some other thread detect that the fence between the 
stores has been removed?

Jaroslav
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From J.Sevcik at sms.ed.ac.uk  Fri Nov 21 06:13:10 2008
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Fri, 21 Nov 2008 11:13:10 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <4925FBDA.5050903@cs.oswego.edu>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>		<4925B36B.6050009@sms.ed.ac.uk>	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<4925BD31.3030304@sms.ed.ac.uk> <4925FBDA.5050903@cs.oswego.edu>
Message-ID: <492697C6.3070107@sms.ed.ac.uk>

All I wanted to say is that the situation about the memory models is 
still very messy because the implementations do not obey the 
specifications on several levels. The only real guarantees are the DRF 
guarantee with the final field guarantee.

With everything else, one must resort to intuitive arguments about 
possible optimisations and processor implementations (or just pray). 
Now, my intuition is certainly unreliable when reasoning about memory 
models and transformations, and I dare to say that no one really has a 
good intuition about these things, as witnessed by the discussions about 
the very small examples. Playing on the edge in the mist is dangerous.

Jaroslav

Doug Lea wrote:
> Jaroslav Sevcik wrote:
>>>> Also, let me warn you that JVMs often do not implement the JMM 
>>>> properly
>>>> (even Hotspot does not).
>>>>  
>> Yes, see my ECOOP paper: 
>> http://homepages.inf.ed.ac.uk/s0566973/jmmtrans.pdf
>>
>
> As you know, I think this is first-rate work, but I think the
> hotspot example in that paper is best considered a JMM spec bug,
> not a JVM bug. Formal memory model specs are hard to get right;
> the JMM specs will need bugfixes to address probelms you and others
> have found. Which won't be easy -- I don't think that there are
> simple wording patches for some of them. But these problems
> are cases where the formalisms don't match what most people
> expect from the main English descriptions (which remain basically
> accurate).
>
>> For more info, see Peter Sewell's extremely cool stuff at 
>> http://www.cl.cam.ac.uk/~pes20/weakmemory/, especially the addendum 
>> of the x86 paper.
>
> A similar story, but messier because of several changes in intent
> over time by the x86 architectural folks. To the best of
> our knowledge (where "us" includes Intel and AMD), current
> placements/usages of fences and atomics on x86 do provide the
> basic JMM guarantees.
>
> (Aside: Despite these and other issues, memory-model-land is a better
> place than it was a few years ago -- we now have models and specs that
> are good enough to find particular faults with, as opposed to the old
> days where the main task was divining the chicken entrails that passed
> for specs to guess what processors and compilers would do.)
>
> -Doug
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From dl at cs.oswego.edu  Fri Nov 21 07:56:15 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 21 Nov 2008 07:56:15 -0500
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <49268F5F.1040404@sms.ed.ac.uk>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>	<4925B36B.6050009@sms.ed.ac.uk>	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<492606B9.6060504@cs.oswego.edu> <49268F5F.1040404@sms.ed.ac.uk>
Message-ID: <4926AFEF.7030407@cs.oswego.edu>

Jaroslav Sevcik wrote:
> Doug Lea wrote:
>>
>> You probably meant something like:
>>     void foo(){
>>          List l = new ArrayList();
>>          barrier.writeBarrier();
>>          list = l;
>>     }
>>
>>     void bar(){
>>        List l = list;
>>        barrer.readBarrier();
>>        println(l);
>>    }

> As for a rational explanation of the problematic behaviour, JVM could 
> actually execute the program in the following order (foo and bar are 
> executed in different threads, and I rename 'l's in foo and bar so that 
> they are not confused with each other):
> 
> foo: List l1 = alloc();
> foo: list = l1; // *
> bar: List l2 = list;
> bar: barrier.readBarrier();
> bar: println(l2);  // Now you see unitialized arraylist!
> foo: l1.<init>();
> foo: barrier.writeBarrier();

I don't understand why you think the store at (*) can be moved
up there? (Are you saying that this store might be one delayed from a
previous invocation of foo()? In which case it would be some
different list object, not that l1.)


>> unnecessary fences. For example, on x86 the first mfence in:
>> "store a; mfence; store b; mfence" can be (and normally is)
>> removed by the  optimizer. This is another advantage of using
>> volatiles instead of explicit fences -- the compiler can
>> sometimes improve performance of volatiles, but cannot do
>> anything about explicit fences.
> I was under the impression that these removals are not detectable by the 
> program (even if it has data races), so you can assume that the fence is 
> still there.

Yes.

> Can some other thread detect that the fence between the 
> stores has been removed?

No. Except maybe if the other thread measured the run time of
first, and found that it ran faster than it could have with
the fence :-)

-Doug



From dl at cs.oswego.edu  Fri Nov 21 08:27:56 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 21 Nov 2008 08:27:56 -0500
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <49266A66.8080404@univ-mlv.fr>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>	<4925B36B.6050009@sms.ed.ac.uk>	<238A96A773B3934685A7269CC8A8D0423CD9778C43@GVW0436EXB.americas.hpqcorp.net>
	<492600AA.1040103@cs.oswego.edu> <49266A66.8080404@univ-mlv.fr>
Message-ID: <4926B75C.5070707@cs.oswego.edu>

R?mi Forax wrote:
> Doug,
> I've taken a look to Fences and all methods that take an array doesn't have
> the correct signature.
> 

Thanks for looking at this!


> by example, <T> void postLoadFence(T[] array, index)
> should be written void post(Object[] array, int index)
> or better void post(Object array, int index)
> 
> because array of primitive types are not subtype of Object[].||
>>

Well, yes and no. The lack of 8 X 3 more variants like
void postLoadFence(short[] array, index) was an intentional
limitation to avoid yet more signature bloat to deal with
cases that will probably never be used. As it is, the
array versions are on the fringes of utility --
Normally you'd order with respect to the array as a whole.
But with reference arrays, the index versions allow you
to in turn specify a fence with respect to a load/store
of the reference at that index, which doesn't arise with
scalar arrays.

In other words, this was a YAGNI call. If for some reason
there is a need for indexed scalar array versions, they could
be added later.

-Doug





From J.Sevcik at sms.ed.ac.uk  Fri Nov 21 09:08:26 2008
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Fri, 21 Nov 2008 14:08:26 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <4926AFEF.7030407@cs.oswego.edu>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>	<4925B36B.6050009@sms.ed.ac.uk>	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<492606B9.6060504@cs.oswego.edu> <49268F5F.1040404@sms.ed.ac.uk>
	<4926AFEF.7030407@cs.oswego.edu>
Message-ID: <4926C0DA.6050007@sms.ed.ac.uk>

Doug Lea wrote:
> Jaroslav Sevcik wrote:
>> Doug Lea wrote:
>>>
>>> You probably meant something like:
>>>     void foo(){
>>>          List l = new ArrayList();
>>>          barrier.writeBarrier();
>>>          list = l;
>>>     }
>>>
>>>     void bar(){
>>>        List l = list;
>>>        barrer.readBarrier();
>>>        println(l);
>>>    }
>
>> As for a rational explanation of the problematic behaviour, JVM could 
>> actually execute the program in the following order (foo and bar are 
>> executed in different threads, and I rename 'l's in foo and bar so 
>> that they are not confused with each other):
>>
>> foo: List l1 = alloc();
>> foo: list = l1; // *
>> bar: List l2 = list;
>> bar: barrier.readBarrier();
>> bar: println(l2);  // Now you see unitialized arraylist!
>> foo: l1.<init>();
>> foo: barrier.writeBarrier();
>
> I don't understand why you think the store at (*) can be moved
> up there? (Are you saying that this store might be one delayed from a
> previous invocation of foo()? In which case it would be some
> different list object, not that l1.)
Perhaps I should have said 'more rational explanation'. I do not know 
any real world compiler optimisation that could actually move the write 
here; however, the English description of the JMM promises that normal 
memory accesses can be reordered with previous releases or with later 
acquires (roach motel semantics), so a JVM *could* do it. Itanium allows 
such reordering (unless you insert full fence in addition to the 
store-release?). My wild guess is that on Itanium the write barrier and 
the constructor could be delayed, e.g., because of a cache miss, while 
the allocation and the write to 'list' would go ahead. I know that this 
is not very likely because the allocation would bring the object to the 
cache, but I think it is still theoretically possible (and legal).

Jaroslav

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From hans.boehm at hp.com  Fri Nov 21 14:47:45 2008
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 21 Nov 2008 19:47:45 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <4926C0DA.6050007@sms.ed.ac.uk>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<492606B9.6060504@cs.oswego.edu> <49268F5F.1040404@sms.ed.ac.uk>
	<4926AFEF.7030407@cs.oswego.edu> <4926C0DA.6050007@sms.ed.ac.uk>
Message-ID: <238A96A773B3934685A7269CC8A8D0423CD97793C2@GVW0436EXB.americas.hpqcorp.net>



> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> Of Jaroslav Sevcik
> Sent: Friday, November 21, 2008 6:08 AM
> To: Doug Lea
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] Playing on the edge: JMM.
> barriers and optimisations.
>
> Doug Lea wrote:
> > Jaroslav Sevcik wrote:
> >> Doug Lea wrote:
> >>>
> >>> You probably meant something like:
> >>>     void foo(){
> >>>          List l = new ArrayList();
> >>>          barrier.writeBarrier();
> >>>          list = l;
> >>>     }
> >>>
> >>>     void bar(){
> >>>        List l = list;
> >>>        barrer.readBarrier();
> >>>        println(l);
> >>>    }
> >
> >> As for a rational explanation of the problematic
> behaviour, JVM could
> >> actually execute the program in the following order (foo
> and bar are
> >> executed in different threads, and I rename 'l's in foo and bar so
> >> that they are not confused with each other):
> >>
> >> foo: List l1 = alloc();
> >> foo: list = l1; // *
> >> bar: List l2 = list;
> >> bar: barrier.readBarrier();
> >> bar: println(l2);  // Now you see unitialized arraylist!
> >> foo: l1.<init>();
> >> foo: barrier.writeBarrier();
> >
> > I don't understand why you think the store at (*) can be moved up
> > there? (Are you saying that this store might be one delayed from a
> > previous invocation of foo()? In which case it would be
> some different
> > list object, not that l1.)
> Perhaps I should have said 'more rational explanation'. I do
> not know any real world compiler optimisation that could
> actually move the write here; however, the English
> description of the JMM promises that normal memory accesses
> can be reordered with previous releases or with later
> acquires (roach motel semantics), so a JVM *could* do it.
> Itanium allows such reordering (unless you insert full fence
> in addition to the store-release?). My wild guess is that on
> Itanium the write barrier and the constructor could be
> delayed, e.g., because of a cache miss, while the allocation
> and the write to 'list' would go ahead. I know that this is
> not very likely because the allocation would bring the object
> to the cache, but I think it is still theoretically possible
> (and legal).
>
> Jaroslav
>
Are we making conflicting assumptions about readBarrier and writeBarrier semantics?  I read Doug's Fences proposal as requiring an implementation similar to what Peter Dimov's C++ fence proposal (which is now in the Committee Draft) requires.  (I think the C++ version is actually a bit more precise about the semantics, though it has been pointed out that those semantics are still weaker than what some people expect from fences.  I'd favor rephrasing this along those lines.)  These cannot be implemented as volatile loads or stores.  I think Doug's postLoadFence() requires an mf (full fence) on Itanium, though it presumably requires only on lwsync on IBMs PowerPC.

Hans


From hans.boehm at hp.com  Fri Nov 21 18:16:51 2008
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 21 Nov 2008 23:16:51 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <4925BD31.3030304@sms.ed.ac.uk>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<4925BD31.3030304@sms.ed.ac.uk>
Message-ID: <238A96A773B3934685A7269CC8A8D0423CD9779582@GVW0436EXB.americas.hpqcorp.net>

> From:  Jaroslav Sevcik
>
> To add to the confusion you should know that it looks like it
> is not possible to implement volatile variables on x86 (by
> the current x86
> specification)
Could you expand on that statement?  I finally got a chance to look more carefully at the paper you cite below, and I didn't get that impression.  My understanding is that Java volatiles can be implemented on X86 by mapping stores to "xchg" or a similar RMW instruction,
and load to an ordinary "mov" instruction.  In fact, I thought this was uncontroversial.  My impression is also that for a static compiler, this is likely to be the highest performance generic mapping, though a JVM might be able to do better on particular hardware, and any compiler might be able to do better in under specific conditions, such as when two volatile stores occur in a row.

I believe there are ongoing (unresolved?) discussions as to whether the spec should guarantee the correctness of some other mappings that seem to be used in practice by some JVMs, and which also seem to be empirically correct on existing hardware.  But I didn't think that there was a remaining fundamental question about implementability.  The only issue is whether future JVMs will have to switch to the xchg/mov implementation to deal with possible future hardware that is no longer stronger than the spec.

Hans


From TEREKHOV at de.ibm.com  Fri Nov 21 19:14:27 2008
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Sat, 22 Nov 2008 01:14:27 +0100
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <238A96A773B3934685A7269CC8A8D0423CD9779582@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <OFA220D75B.E7471160-ONC1257509.00013845-C1257509.00014586@de.ibm.com>

"Boehm, Hans" <hans.boehm at hp.com> wrote:
[...]
> My understanding is that Java volatiles can be implemented on X86 by
mapping stores
> to "xchg" or a similar RMW instruction, and load to an ordinary "mov"
instruction.

I'd still rather stick to <LOCK CMPXCHG address, 42, 42> for loads and
<XCHG address, value> (implicitly LOCKed) for stores. YMMV.

regards,
alexander.


"Boehm, Hans" <hans.boehm at hp.com>@cs.oswego.edu on 22.11.2008 00:16:51

Sent by:    concurrency-interest-bounces at cs.oswego.edu


To:    Jaroslav Sevcik <J.Sevcik at sms.ed.ac.uk>, Peter Veentjer
       <alarmnummer at gmail.com>
cc:    concurrency-interest x <concurrency-interest at cs.oswego.edu>
Subject:    Re: [concurrency-interest] Playing on the edge: JMM. barriers
       and optimisations.


> From:  Jaroslav Sevcik
>
> To add to the confusion you should know that it looks like it
> is not possible to implement volatile variables on x86 (by
> the current x86
> specification)
Could you expand on that statement?  I finally got a chance to look more
carefully at the paper you cite below, and I didn't get that impression.
My understanding is that Java volatiles can be implemented on X86 by
mapping stores to "xchg" or a similar RMW instruction,
and load to an ordinary "mov" instruction.  In fact, I thought this was
uncontroversial.  My impression is also that for a static compiler, this is
likely to be the highest performance generic mapping, though a JVM might be
able to do better on particular hardware, and any compiler might be able to
do better in under specific conditions, such as when two volatile stores
occur in a row.

I believe there are ongoing (unresolved?) discussions as to whether the
spec should guarantee the correctness of some other mappings that seem to
be used in practice by some JVMs, and which also seem to be empirically
correct on existing hardware.  But I didn't think that there was a
remaining fundamental question about implementability.  The only issue is
whether future JVMs will have to switch to the xchg/mov implementation to
deal with possible future hardware that is no longer stronger than the
spec.

Hans

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From J.Sevcik at sms.ed.ac.uk  Sat Nov 22 06:55:46 2008
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Sat, 22 Nov 2008 11:55:46 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers
	and	optimisations.
In-Reply-To: <238A96A773B3934685A7269CC8A8D0423CD97793C2@GVW0436EXB.americas.hpqcorp.net>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<492606B9.6060504@cs.oswego.edu> <49268F5F.1040404@sms.ed.ac.uk>
	<4926AFEF.7030407@cs.oswego.edu> <4926C0DA.6050007@sms.ed.ac.uk>
	<238A96A773B3934685A7269CC8A8D0423CD97793C2@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <20081122115546.uvzl1e8rtw4s0kgs@www.sms.ed.ac.uk>

Quoting "Boehm, Hans" <hans.boehm at hp.com>:

>
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
>> Of Jaroslav Sevcik
>> Sent: Friday, November 21, 2008 6:08 AM
>> To: Doug Lea
>> Cc: concurrency-interest
>> Subject: Re: [concurrency-interest] Playing on the edge: JMM.
>> barriers and optimisations.
>>
>> Doug Lea wrote:
>> > Jaroslav Sevcik wrote:
>> >> Doug Lea wrote:
>> >>>
>> >>> You probably meant something like:
>> >>>     void foo(){
>> >>>          List l = new ArrayList();
>> >>>          barrier.writeBarrier();
>> >>>          list = l;
>> >>>     }
>> >>>
>> >>>     void bar(){
>> >>>        List l = list;
>> >>>        barrer.readBarrier();
>> >>>        println(l);
>> >>>    }
>> >
>> >> As for a rational explanation of the problematic
>> behaviour, JVM could
>> >> actually execute the program in the following order (foo
>> and bar are
>> >> executed in different threads, and I rename 'l's in foo and bar so
>> >> that they are not confused with each other):
>> >>
>> >> foo: List l1 = alloc();
>> >> foo: list = l1; // *
>> >> bar: List l2 = list;
>> >> bar: barrier.readBarrier();
>> >> bar: println(l2);  // Now you see unitialized arraylist!
>> >> foo: l1.<init>();
>> >> foo: barrier.writeBarrier();
>> >
>> > I don't understand why you think the store at (*) can be moved up
>> > there? (Are you saying that this store might be one delayed from a
>> > previous invocation of foo()? In which case it would be
>> some different
>> > list object, not that l1.)
>> Perhaps I should have said 'more rational explanation'. I do
>> not know any real world compiler optimisation that could
>> actually move the write here; however, the English
>> description of the JMM promises that normal memory accesses
>> can be reordered with previous releases or with later
>> acquires (roach motel semantics), so a JVM *could* do it.
>> Itanium allows such reordering (unless you insert full fence
>> in addition to the store-release?). My wild guess is that on
>> Itanium the write barrier and the constructor could be
>> delayed, e.g., because of a cache miss, while the allocation
>> and the write to 'list' would go ahead. I know that this is
>> not very likely because the allocation would bring the object
>> to the cache, but I think it is still theoretically possible
>> (and legal).
>>
>> Jaroslav
>>
> Are we making conflicting assumptions about readBarrier and   
> writeBarrier semantics?

I was still talking about the memory barriers implemented as volatile  
variables (as shown in Peter Veentjer's example).

Jaroslav

>
> Hans
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.




From J.Sevcik at sms.ed.ac.uk  Sat Nov 22 07:07:22 2008
From: J.Sevcik at sms.ed.ac.uk (Jaroslav Sevcik)
Date: Sat, 22 Nov 2008 12:07:22 +0000
Subject: [concurrency-interest] Playing on the edge: JMM. barriers
	and	optimisations.
In-Reply-To: <238A96A773B3934685A7269CC8A8D0423CD9779582@GVW0436EXB.americas.hpqcorp.net>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<4925BD31.3030304@sms.ed.ac.uk>
	<238A96A773B3934685A7269CC8A8D0423CD9779582@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <20081122120722.ivt84z57ccg8w0w8@www.sms.ed.ac.uk>

Quoting "Boehm, Hans" <hans.boehm at hp.com>:

>> From:  Jaroslav Sevcik
>>
>> To add to the confusion you should know that it looks like it
>> is not possible to implement volatile variables on x86 (by
>> the current x86
>> specification)
> Could you expand on that statement?

I should have been more precise: it is not possible to implement  
volatile variables on x86 with fences and normal memory accesses.  
Thanks for the clarification.

Jaroslav

> I finally got a chance to look  more carefully at the paper you cite  
> below, and I didn't get that  impression.  My understanding is that  
> Java volatiles can be  implemented on X86 by mapping stores to  
> "xchg" or a similar RMW  instruction,
> and load to an ordinary "mov" instruction.  In fact, I thought this   
> was uncontroversial.  My impression is also that for a static   
> compiler, this is likely to be the highest performance generic   
> mapping, though a JVM might be able to do better on particular   
> hardware, and any compiler might be able to do better in under   
> specific conditions, such as when two volatile stores occur in a row.
>
> I believe there are ongoing (unresolved?) discussions as to whether   
> the spec should guarantee the correctness of some other mappings   
> that seem to be used in practice by some JVMs, and which also seem   
> to be empirically correct on existing hardware.  But I didn't think   
> that there was a remaining fundamental question about   
> implementability.  The only issue is whether future JVMs will have   
> to switch to the xchg/mov implementation to deal with possible   
> future hardware that is no longer stronger than the spec.
>
> Hans
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.





From ganzhi at gmail.com  Sat Nov 22 09:48:30 2008
From: ganzhi at gmail.com (James Gan)
Date: Sat, 22 Nov 2008 22:48:30 +0800
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
	optimisations.
In-Reply-To: <238A96A773B3934685A7269CC8A8D0423CD9779582@GVW0436EXB.americas.hpqcorp.net>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>
	<4925B36B.6050009@sms.ed.ac.uk>
	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>
	<4925BD31.3030304@sms.ed.ac.uk>
	<238A96A773B3934685A7269CC8A8D0423CD9779582@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <70c070d80811220648lc124f1fs86b50094040fc301@mail.gmail.com>

Is it necessary to mapping stores to "xchg"?  I read a funny blog post which
say no special instruction is needed for volatile in VC2005, whilst it seems
VC2005 has a similar semantic for volatile.

http://blogs.msdn.com/kangsu/archive/2007/07/16/volatile-acquire-release-memory-fences-and-vc2005.aspx


On Sat, Nov 22, 2008 at 7:16 AM, Boehm, Hans <hans.boehm at hp.com> wrote:

> > From:  Jaroslav Sevcik
> >
> > To add to the confusion you should know that it looks like it
> > is not possible to implement volatile variables on x86 (by
> > the current x86
> > specification)
> Could you expand on that statement?  I finally got a chance to look more
> carefully at the paper you cite below, and I didn't get that impression.  My
> understanding is that Java volatiles can be implemented on X86 by mapping
> stores to "xchg" or a similar RMW instruction,
> and load to an ordinary "mov" instruction.  In fact, I thought this was
> uncontroversial.  My impression is also that for a static compiler, this is
> likely to be the highest performance generic mapping, though a JVM might be
> able to do better on particular hardware, and any compiler might be able to
> do better in under specific conditions, such as when two volatile stores
> occur in a row.
>
> I believe there are ongoing (unresolved?) discussions as to whether the
> spec should guarantee the correctness of some other mappings that seem to be
> used in practice by some JVMs, and which also seem to be empirically correct
> on existing hardware.  But I didn't think that there was a remaining
> fundamental question about implementability.  The only issue is whether
> future JVMs will have to switch to the xchg/mov implementation to deal with
> possible future hardware that is no longer stronger than the spec.
>
> Hans
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20081122/8d0e3acc/attachment-0001.html>

From dl at cs.oswego.edu  Sat Nov 22 10:11:41 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 22 Nov 2008 10:11:41 -0500
Subject: [concurrency-interest] Playing on the edge: JMM. barriers and
 optimisations.
In-Reply-To: <70c070d80811220648lc124f1fs86b50094040fc301@mail.gmail.com>
References: <1466c1d60811200904k78a0655cqe8052b27c91665b3@mail.gmail.com>	<4925B36B.6050009@sms.ed.ac.uk>	<1466c1d60811201126i5f5d0aaeh8c95f333de15a0bf@mail.gmail.com>	<4925BD31.3030304@sms.ed.ac.uk>	<238A96A773B3934685A7269CC8A8D0423CD9779582@GVW0436EXB.americas.hpqcorp.net>
	<70c070d80811220648lc124f1fs86b50094040fc301@mail.gmail.com>
Message-ID: <4928212D.3050303@cs.oswego.edu>

James Gan wrote:
> Is it necessary to mapping stores to "xchg"?  I read a funny blog post 
> which say no special instruction is needed for volatile in VC2005, 
> whilst it seems VC2005 has a similar semantic for volatile.
> 

First, there has been some confusion about this that is slated to
be clarified in the next revisions of Intel and AMD specs (maybe
within a month or so?), as also alluded to in Peter Sewell et al's
http://www.cl.cam.ac.uk/~pes20/weakmemory/popl09.pdf addendum.
So a better answer about this and other related questions
awaits these revisions.

Second, while I know almost nothing about MS Visual C, I
think that their "release" is weaker than Java volatile guarantees.

-Doug





From jim.andreou at gmail.com  Sun Nov 23 14:15:04 2008
From: jim.andreou at gmail.com (Andreou Dimitris)
Date: Sun, 23 Nov 2008 21:15:04 +0200
Subject: [concurrency-interest] About actual implemention of happens-before
Message-ID: <4929ABB8.2020600@csd.uoc.gr>

Hi,

Here is a questing that has been bothering me for some time. Consider 
some globals and two threads:

volatile boolean a = false;
volatile boolean b = true;
int x = 0;

Thread A:
x = 10;
a = true; //volatile write

Thread B:
while (b) { //volatile read
    print(x);
}

I understand that for Thread B, there is no guarantee that it will ever 
print 10, since there is no happens-before relation between "x = 10" and 
"print(x)", even if Thread A runs first.

While this:
Thread B:
while (a) { //changed b to a, the volatile written by Thread A
    print(x);
}

is guaranteed to see the write of 10 and print it.

I don't understand how the compiler or the vm is going to track /which/ 
volatiles are read and written, so to efficiently honor the 
happens-before relationships. How the vm could induce that in the first 
case no barrier is needed?

Because without any tracking, what options are left? Just the 
conservative of treating *all* volatile reads having a happens-before 
relation with *all* prior volatile writes (i.e. even writes of different 
variables)? Can someone explain the typical underlying mechanics, or 
point out to some material that can clarify such details to me? I 
suspect there could be some bus snooping involed, like for example 
"flush the write-buffer if any processor tries to read any cached 
volatile I own", but then again, how would the hardware differentiate 
between cache lines holding "normal" variables from the ones holding 
"volatile" ones? I probably don't make much sense. Hopefully somebody 
can see where I'm confused and enlighten me. Thanks!

Dimitris Andreou

From dl at cs.oswego.edu  Sun Nov 23 18:30:47 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 23 Nov 2008 18:30:47 -0500
Subject: [concurrency-interest] About actual implemention of
	happens-before
In-Reply-To: <4929ABB8.2020600@csd.uoc.gr>
References: <4929ABB8.2020600@csd.uoc.gr>
Message-ID: <4929E7A7.5050301@cs.oswego.edu>

Andreou Dimitris wrote:
> I don't understand how the compiler or the vm is going to track /which/ 
> volatiles are read and written, so to efficiently honor the 
> happens-before relationships. How the vm could induce that in the first 
> case no barrier is needed?
> 

Platforms (== Compiler+VM+OS+processor) are allowed to deduce
any property of your program that is deducible given the language
etc specs. Often, they can't actually deduce much.
But you can't depend on them being  dumb forever even if you
catch them being dumb once, because they keep getting smarter.

See for example Cliff Click's Micro-benchmark advice talks and
and blog (like http://blogs.azulsystems.com/cliff/2008/03/another-round-o.html)
to see some analyses and transformations that most people find surprising.

In the case you list...

> volatile boolean a = false;
> volatile boolean b = true;
> int x = 0;
> 
> Thread A:
> x = 10;
> a = true; //volatile write
> 
> Thread B:
> while (b) { //volatile read
>    print(x);
> }

... a compiler might notice that b is
thread-local, which would (a few transformations later) allow
it to notice that Thread B optimizes to an empty infinite loop.
No platform I know does this today, but it is surely possible
and I expect that some will do this someday.

> 
> Because without any tracking, what options are left? Just the conservative 
> of treating *all* volatile reads having a happens-before relation with *all* 
> prior volatile writes (i.e. even writes of different variables)? 

That's basically the default/fallback strategy on most platforms.
But the processors are doing as much as they legally can behind
the scenes to prefetch reads etc while respecting processor and
fence specs.

... And the moral is, as always, always properly synchronize your
code.


-Doug




From dcholmes at optusnet.com.au  Sun Nov 23 18:31:28 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 24 Nov 2008 09:31:28 +1000
Subject: [concurrency-interest] About actual implemention of
	happens-before
In-Reply-To: <4929ABB8.2020600@csd.uoc.gr>
Message-ID: <NFBBKALFDCPFIDBNKAPCKELEHOAA.dcholmes@optusnet.com.au>

In simple terms the main mechanisms for enforcing the memory model do apply
globally. The JMM has a finer grain than that because it allows for
compilers to reason about whether variables are in fact shared across
threads, and if not to elide their memory model side-effects (something that
could not be done if the JMM defined global effects).

This discussion (by which I mean this thread and the preceding one) really
belong on the JMM list, and if you look at the archives for the JMM list and
the various related documents then you will find more detail about this than
you ever really wanted to know :)

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Andreou
> Dimitris
> Sent: Monday, 24 November 2008 5:15 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] About actual implemention of
> happens-before
>
>
> Hi,
>
> Here is a questing that has been bothering me for some time. Consider
> some globals and two threads:
>
> volatile boolean a = false;
> volatile boolean b = true;
> int x = 0;
>
> Thread A:
> x = 10;
> a = true; //volatile write
>
> Thread B:
> while (b) { //volatile read
>     print(x);
> }
>
> I understand that for Thread B, there is no guarantee that it will ever
> print 10, since there is no happens-before relation between "x = 10" and
> "print(x)", even if Thread A runs first.
>
> While this:
> Thread B:
> while (a) { //changed b to a, the volatile written by Thread A
>     print(x);
> }
>
> is guaranteed to see the write of 10 and print it.
>
> I don't understand how the compiler or the vm is going to track /which/
> volatiles are read and written, so to efficiently honor the
> happens-before relationships. How the vm could induce that in the first
> case no barrier is needed?
>
> Because without any tracking, what options are left? Just the
> conservative of treating *all* volatile reads having a happens-before
> relation with *all* prior volatile writes (i.e. even writes of different
> variables)? Can someone explain the typical underlying mechanics, or
> point out to some material that can clarify such details to me? I
> suspect there could be some bus snooping involed, like for example
> "flush the write-buffer if any processor tries to read any cached
> volatile I own", but then again, how would the hardware differentiate
> between cache lines holding "normal" variables from the ones holding
> "volatile" ones? I probably don't make much sense. Hopefully somebody
> can see where I'm confused and enlighten me. Thanks!
>
> Dimitris Andreou
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From Elias.Ross at autodesk.com  Tue Nov 25 16:17:17 2008
From: Elias.Ross at autodesk.com (Elias Ross)
Date: Tue, 25 Nov 2008 13:17:17 -0800
Subject: [concurrency-interest] NoSuchElementException for
 CopyOnWriteArrayList.toString JRocket 1.5R11
Message-ID: <155A38FD45A9BC42AACCA9C78C31748C380F3EEF4C@ADSK-NAMSG-02.MGDADSK.autodesk.com>

I've been getting this on JRocket 1.5R11 when calling toString() on the CopyOnWriteArrayList. This happens under heavy load.

java.util.NoSuchElementException
        at java.util.concurrent.CopyOnWriteArrayList$COWIterator.next(CopyOnWriteArrayList.java:910)
        at java.util.concurrent.CopyOnWriteArrayList.toString(CopyOnWriteArrayList.java:764)

I went through the release notes on the Sun Java site and Oracle-BEA's JRocket site and didn't see this. Did I miss this?

There is obviously a race between the creation of the array and the call to size(). JDK 1.6 doesn't seem to have this.

    public String toString() {
        StringBuffer buf = new StringBuffer();
        Iterator e = iterator();
        buf.append("[");
        int maxIndex = size() - 1;
        for (int i = 0; i <= maxIndex; i++) {
            buf.append(String.valueOf(e.next()));
            if (i < maxIndex)
                buf.append(", ");
        }
        buf.append("]");
        return buf.toString();
    }




From dcholmes at optusnet.com.au  Tue Nov 25 17:28:34 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 26 Nov 2008 08:28:34 +1000
Subject: [concurrency-interest] NoSuchElementException for
	CopyOnWriteArrayList.toString JRocket 1.5R11
In-Reply-To: <155A38FD45A9BC42AACCA9C78C31748C380F3EEF4C@ADSK-NAMSG-02.MGDADSK.autodesk.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKELLHOAA.dcholmes@optusnet.com.au>

The implementation of toString() was completely changed in JDK 6. This
change has not been backported to JDK 5.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Elias
> Ross
> Sent: Wednesday, 26 November 2008 7:17 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] NoSuchElementException for
> CopyOnWriteArrayList.toString JRocket 1.5R11
>
>
> I've been getting this on JRocket 1.5R11 when calling toString()
> on the CopyOnWriteArrayList. This happens under heavy load.
>
> java.util.NoSuchElementException
>         at
> java.util.concurrent.CopyOnWriteArrayList$COWIterator.next(CopyOnW
> riteArrayList.java:910)
>         at
> java.util.concurrent.CopyOnWriteArrayList.toString(CopyOnWriteArra
> yList.java:764)
>
> I went through the release notes on the Sun Java site and
> Oracle-BEA's JRocket site and didn't see this. Did I miss this?
>
> There is obviously a race between the creation of the array and
> the call to size(). JDK 1.6 doesn't seem to have this.
>
>     public String toString() {
>         StringBuffer buf = new StringBuffer();
>         Iterator e = iterator();
>         buf.append("[");
>         int maxIndex = size() - 1;
>         for (int i = 0; i <= maxIndex; i++) {
>             buf.append(String.valueOf(e.next()));
>             if (i < maxIndex)
>                 buf.append(", ");
>         }
>         buf.append("]");
>         return buf.toString();
>     }
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


